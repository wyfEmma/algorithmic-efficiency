python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_3 --overwrite=true --save_checkpoints=false --rng_seed=460736287 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_resnet_jax_02-29-2024-05-13-57.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0229 05:14:19.749966 140573303715648 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax.
I0229 05:14:20.811078 140573303715648 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0229 05:14:20.812505 140573303715648 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0229 05:14:20.812662 140573303715648 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0229 05:14:21.739432 140573303715648 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax/trial_1.
I0229 05:14:21.940429 140573303715648 submission_runner.py:206] Initializing dataset.
I0229 05:14:21.955738 140573303715648 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:14:21.965630 140573303715648 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:14:22.335850 140573303715648 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:14:23.525160 140573303715648 submission_runner.py:213] Initializing model.
I0229 05:14:33.582734 140573303715648 submission_runner.py:255] Initializing optimizer.
I0229 05:14:35.313882 140573303715648 submission_runner.py:262] Initializing metrics bundle.
I0229 05:14:35.314105 140573303715648 submission_runner.py:280] Initializing checkpoint and logger.
I0229 05:14:35.315004 140573303715648 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0229 05:14:35.315168 140573303715648 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0229 05:14:35.666248 140573303715648 logger_utils.py:220] Unable to record git information. Continuing without it.
I0229 05:14:35.998625 140573303715648 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax/trial_1/flags_0.json.
I0229 05:14:36.007384 140573303715648 submission_runner.py:314] Starting training loop.
I0229 05:15:28.371917 140409816737536 logging_writer.py:48] [0] global_step=0, grad_norm=0.6620714664459229, loss=6.922584056854248
I0229 05:15:28.389045 140573303715648 spec.py:321] Evaluating on the training split.
I0229 05:15:29.533172 140573303715648 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:29.542379 140573303715648 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:15:29.625874 140573303715648 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:43.156523 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 05:15:44.847122 140573303715648 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:44.856233 140573303715648 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:15:44.896604 140573303715648 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:16:00.567774 140573303715648 spec.py:349] Evaluating on the test split.
I0229 05:16:01.373734 140573303715648 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:16:01.378766 140573303715648 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0229 05:16:01.419231 140573303715648 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:16:05.753338 140573303715648 submission_runner.py:411] Time since start: 89.75s, 	Step: 1, 	{'train/accuracy': 0.001235650503076613, 'train/loss': 6.911959171295166, 'validation/accuracy': 0.00107999995816499, 'validation/loss': 6.912962913513184, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.913362979888916, 'test/num_examples': 10000, 'score': 52.38156604766846, 'total_duration': 89.74589896202087, 'accumulated_submission_time': 52.38156604766846, 'accumulated_eval_time': 37.36424398422241, 'accumulated_logging_time': 0}
I0229 05:16:05.770001 140388975236864 logging_writer.py:48] [1] accumulated_eval_time=37.364244, accumulated_logging_time=0, accumulated_submission_time=52.381566, global_step=1, preemption_count=0, score=52.381566, test/accuracy=0.000900, test/loss=6.913363, test/num_examples=10000, total_duration=89.745899, train/accuracy=0.001236, train/loss=6.911959, validation/accuracy=0.001080, validation/loss=6.912963, validation/num_examples=50000
I0229 05:16:39.441783 140388966844160 logging_writer.py:48] [100] global_step=100, grad_norm=0.692540168762207, loss=6.815499305725098
I0229 05:17:13.159399 140388975236864 logging_writer.py:48] [200] global_step=200, grad_norm=0.7990751266479492, loss=6.577093601226807
I0229 05:17:46.985095 140388966844160 logging_writer.py:48] [300] global_step=300, grad_norm=1.1705163717269897, loss=6.2867431640625
I0229 05:18:20.861859 140388975236864 logging_writer.py:48] [400] global_step=400, grad_norm=2.405721664428711, loss=6.004097938537598
I0229 05:18:54.740327 140388966844160 logging_writer.py:48] [500] global_step=500, grad_norm=1.9332082271575928, loss=5.811285495758057
I0229 05:19:28.590503 140388975236864 logging_writer.py:48] [600] global_step=600, grad_norm=3.895981788635254, loss=5.684521675109863
I0229 05:20:02.475262 140388966844160 logging_writer.py:48] [700] global_step=700, grad_norm=3.622304677963257, loss=5.467580795288086
I0229 05:20:36.353154 140388975236864 logging_writer.py:48] [800] global_step=800, grad_norm=2.6068127155303955, loss=5.245099067687988
I0229 05:21:10.293855 140388966844160 logging_writer.py:48] [900] global_step=900, grad_norm=2.5341005325317383, loss=5.197074890136719
I0229 05:21:44.197380 140388975236864 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.7051384449005127, loss=4.999974727630615
I0229 05:22:18.076856 140388966844160 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.712677478790283, loss=4.9583024978637695
I0229 05:22:51.966769 140388975236864 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.8295438289642334, loss=4.911057472229004
I0229 05:23:25.827781 140388966844160 logging_writer.py:48] [1300] global_step=1300, grad_norm=5.943663120269775, loss=4.669180870056152
I0229 05:23:59.714042 140388975236864 logging_writer.py:48] [1400] global_step=1400, grad_norm=5.835800647735596, loss=4.745398044586182
I0229 05:24:33.566630 140388966844160 logging_writer.py:48] [1500] global_step=1500, grad_norm=5.863056659698486, loss=4.449187278747559
I0229 05:24:36.032840 140573303715648 spec.py:321] Evaluating on the training split.
I0229 05:24:43.769153 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 05:24:52.006440 140573303715648 spec.py:349] Evaluating on the test split.
I0229 05:24:54.490549 140573303715648 submission_runner.py:411] Time since start: 618.48s, 	Step: 1509, 	{'train/accuracy': 0.17956791818141937, 'train/loss': 4.181913375854492, 'validation/accuracy': 0.16129998862743378, 'validation/loss': 4.331301212310791, 'validation/num_examples': 50000, 'test/accuracy': 0.125900000333786, 'test/loss': 4.813449382781982, 'test/num_examples': 10000, 'score': 562.5784339904785, 'total_duration': 618.4830918312073, 'accumulated_submission_time': 562.5784339904785, 'accumulated_eval_time': 55.82190465927124, 'accumulated_logging_time': 0.02535843849182129}
I0229 05:24:54.507350 140388983629568 logging_writer.py:48] [1509] accumulated_eval_time=55.821905, accumulated_logging_time=0.025358, accumulated_submission_time=562.578434, global_step=1509, preemption_count=0, score=562.578434, test/accuracy=0.125900, test/loss=4.813449, test/num_examples=10000, total_duration=618.483092, train/accuracy=0.179568, train/loss=4.181913, validation/accuracy=0.161300, validation/loss=4.331301, validation/num_examples=50000
I0229 05:25:25.625372 140388992022272 logging_writer.py:48] [1600] global_step=1600, grad_norm=6.101463794708252, loss=4.438639163970947
I0229 05:25:59.500893 140388983629568 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.5576651096343994, loss=4.258892059326172
I0229 05:26:33.344958 140388992022272 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.506561279296875, loss=4.312245845794678
I0229 05:27:07.184303 140388983629568 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.8925395011901855, loss=4.1575751304626465
I0229 05:27:41.065995 140388992022272 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.4145431518554688, loss=4.063216686248779
I0229 05:28:14.941375 140388983629568 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.113146781921387, loss=3.99088191986084
I0229 05:28:48.796089 140388992022272 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.90683913230896, loss=3.94757342338562
I0229 05:29:22.662675 140388983629568 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.8362860679626465, loss=3.944209098815918
I0229 05:29:56.551392 140388992022272 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.069746017456055, loss=3.7279787063598633
I0229 05:30:30.393918 140388983629568 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.288114070892334, loss=3.687290668487549
I0229 05:31:04.233323 140388992022272 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.069544792175293, loss=3.55552077293396
I0229 05:31:38.082374 140388983629568 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.258487224578857, loss=3.598402738571167
I0229 05:32:11.971888 140388992022272 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.9544129371643066, loss=3.4953131675720215
I0229 05:32:45.794724 140388983629568 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.67189359664917, loss=3.5627503395080566
I0229 05:33:19.621738 140388992022272 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.816506862640381, loss=3.4730958938598633
I0229 05:33:24.495326 140573303715648 spec.py:321] Evaluating on the training split.
I0229 05:33:32.039675 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 05:33:40.358886 140573303715648 spec.py:349] Evaluating on the test split.
I0229 05:33:42.674689 140573303715648 submission_runner.py:411] Time since start: 1146.67s, 	Step: 3016, 	{'train/accuracy': 0.32543447613716125, 'train/loss': 3.1697702407836914, 'validation/accuracy': 0.2942200005054474, 'validation/loss': 3.366985559463501, 'validation/num_examples': 50000, 'test/accuracy': 0.22660000622272491, 'test/loss': 3.981245994567871, 'test/num_examples': 10000, 'score': 1072.4957585334778, 'total_duration': 1146.6672384738922, 'accumulated_submission_time': 1072.4957585334778, 'accumulated_eval_time': 74.00124216079712, 'accumulated_logging_time': 0.052672624588012695}
I0229 05:33:42.691077 140411238594304 logging_writer.py:48] [3016] accumulated_eval_time=74.001242, accumulated_logging_time=0.052673, accumulated_submission_time=1072.495759, global_step=3016, preemption_count=0, score=1072.495759, test/accuracy=0.226600, test/loss=3.981246, test/num_examples=10000, total_duration=1146.667238, train/accuracy=0.325434, train/loss=3.169770, validation/accuracy=0.294220, validation/loss=3.366986, validation/num_examples=50000
I0229 05:34:11.466112 140411423135488 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.24754524230957, loss=3.344306468963623
I0229 05:34:45.285899 140411238594304 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.009617328643799, loss=3.3332061767578125
I0229 05:35:19.113012 140411423135488 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.497917413711548, loss=3.1886494159698486
I0229 05:35:52.929440 140411238594304 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.4969427585601807, loss=3.2402215003967285
I0229 05:36:26.764086 140411423135488 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.473181962966919, loss=3.2089200019836426
I0229 05:37:00.618699 140411238594304 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.9827325344085693, loss=3.006589412689209
I0229 05:37:34.460048 140411423135488 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.7398035526275635, loss=3.0332682132720947
I0229 05:38:08.264535 140411238594304 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.6491341590881348, loss=3.1060638427734375
I0229 05:38:42.113861 140411423135488 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.7471039295196533, loss=3.0167927742004395
I0229 05:39:15.947614 140411238594304 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.3151426315307617, loss=2.858444929122925
I0229 05:39:49.841344 140411423135488 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.11122727394104, loss=2.7797183990478516
I0229 05:40:23.648387 140411238594304 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.7393226623535156, loss=2.846562147140503
I0229 05:40:57.451460 140411423135488 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.609480857849121, loss=2.8371267318725586
I0229 05:41:31.279560 140411238594304 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.332937717437744, loss=2.887770175933838
I0229 05:42:05.079815 140411423135488 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.1944196224212646, loss=2.8549282550811768
I0229 05:42:12.955429 140573303715648 spec.py:321] Evaluating on the training split.
I0229 05:42:20.423946 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 05:42:28.853784 140573303715648 spec.py:349] Evaluating on the test split.
I0229 05:42:31.248571 140573303715648 submission_runner.py:411] Time since start: 1675.24s, 	Step: 4525, 	{'train/accuracy': 0.4550183117389679, 'train/loss': 2.387629985809326, 'validation/accuracy': 0.42486000061035156, 'validation/loss': 2.5835626125335693, 'validation/num_examples': 50000, 'test/accuracy': 0.3230000138282776, 'test/loss': 3.316889524459839, 'test/num_examples': 10000, 'score': 1582.6927065849304, 'total_duration': 1675.2411172389984, 'accumulated_submission_time': 1582.6927065849304, 'accumulated_eval_time': 92.29433751106262, 'accumulated_logging_time': 0.07882332801818848}
I0229 05:42:31.265554 140411473458944 logging_writer.py:48] [4525] accumulated_eval_time=92.294338, accumulated_logging_time=0.078823, accumulated_submission_time=1582.692707, global_step=4525, preemption_count=0, score=1582.692707, test/accuracy=0.323000, test/loss=3.316890, test/num_examples=10000, total_duration=1675.241117, train/accuracy=0.455018, train/loss=2.387630, validation/accuracy=0.424860, validation/loss=2.583563, validation/num_examples=50000
I0229 05:42:56.936108 140411481851648 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.8694888353347778, loss=2.7803478240966797
I0229 05:43:30.751568 140411473458944 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.392277717590332, loss=2.543057441711426
I0229 05:44:04.536613 140411481851648 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.3386552333831787, loss=2.742393970489502
I0229 05:44:38.362214 140411473458944 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.557190418243408, loss=2.521973133087158
I0229 05:45:12.141227 140411481851648 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.7361699342727661, loss=2.6280345916748047
I0229 05:45:45.976328 140411473458944 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.9416104555130005, loss=2.549105405807495
I0229 05:46:19.830068 140411481851648 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.321366786956787, loss=2.6383216381073
I0229 05:46:53.624654 140411473458944 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.7856802940368652, loss=2.609780788421631
I0229 05:47:27.403402 140411481851648 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.825610399246216, loss=2.6751134395599365
I0229 05:48:01.214570 140411473458944 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.174238443374634, loss=2.539276361465454
I0229 05:48:34.994895 140411481851648 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.875600814819336, loss=2.585177421569824
I0229 05:49:08.800348 140411473458944 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.6065003871917725, loss=2.389204978942871
I0229 05:49:42.564909 140411481851648 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.4691858291625977, loss=2.6340410709381104
I0229 05:50:16.337282 140411473458944 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.080082893371582, loss=2.4161810874938965
I0229 05:50:50.120330 140411481851648 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.259460210800171, loss=2.4600279331207275
I0229 05:51:01.331966 140573303715648 spec.py:321] Evaluating on the training split.
I0229 05:51:08.763686 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 05:51:17.107680 140573303715648 spec.py:349] Evaluating on the test split.
I0229 05:51:19.424264 140573303715648 submission_runner.py:411] Time since start: 2203.42s, 	Step: 6035, 	{'train/accuracy': 0.5336614847183228, 'train/loss': 1.9763846397399902, 'validation/accuracy': 0.49616000056266785, 'validation/loss': 2.1787710189819336, 'validation/num_examples': 50000, 'test/accuracy': 0.3857000172138214, 'test/loss': 2.9400079250335693, 'test/num_examples': 10000, 'score': 2092.6917436122894, 'total_duration': 2203.4167931079865, 'accumulated_submission_time': 2092.6917436122894, 'accumulated_eval_time': 110.38658118247986, 'accumulated_logging_time': 0.10665202140808105}
I0229 05:51:19.441413 140411448280832 logging_writer.py:48] [6035] accumulated_eval_time=110.386581, accumulated_logging_time=0.106652, accumulated_submission_time=2092.691744, global_step=6035, preemption_count=0, score=2092.691744, test/accuracy=0.385700, test/loss=2.940008, test/num_examples=10000, total_duration=2203.416793, train/accuracy=0.533661, train/loss=1.976385, validation/accuracy=0.496160, validation/loss=2.178771, validation/num_examples=50000
I0229 05:51:41.751759 140411490244352 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.3696658611297607, loss=2.390756607055664
I0229 05:52:15.556799 140411448280832 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.351480484008789, loss=2.497347831726074
I0229 05:52:49.364361 140411490244352 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.6102458238601685, loss=2.357940435409546
I0229 05:53:23.127274 140411448280832 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.9783589839935303, loss=2.341113328933716
I0229 05:53:56.914698 140411490244352 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.424567937850952, loss=2.4395694732666016
I0229 05:54:30.691637 140411448280832 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.8677903413772583, loss=2.2537083625793457
I0229 05:55:04.497812 140411490244352 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.2172796726226807, loss=2.3451688289642334
I0229 05:55:38.297535 140411448280832 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.978116750717163, loss=2.2951667308807373
I0229 05:56:12.065189 140411490244352 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.0268421173095703, loss=2.3875906467437744
I0229 05:56:45.854152 140411448280832 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.811771273612976, loss=2.387484073638916
I0229 05:57:19.644556 140411490244352 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.1692445278167725, loss=2.3757028579711914
I0229 05:57:53.451961 140411448280832 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.5760080814361572, loss=2.275660276412964
I0229 05:58:27.340099 140411490244352 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.5512161254882812, loss=2.382817506790161
I0229 05:59:01.109167 140411448280832 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.8084678649902344, loss=2.210935592651367
I0229 05:59:34.913251 140411490244352 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.386507511138916, loss=2.2670176029205322
I0229 05:59:49.567928 140573303715648 spec.py:321] Evaluating on the training split.
I0229 05:59:56.864260 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:00:05.381114 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:00:07.754111 140573303715648 submission_runner.py:411] Time since start: 2731.75s, 	Step: 7545, 	{'train/accuracy': 0.5792012214660645, 'train/loss': 1.7531155347824097, 'validation/accuracy': 0.5401399731636047, 'validation/loss': 1.9743461608886719, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.75813889503479, 'test/num_examples': 10000, 'score': 2602.7501418590546, 'total_duration': 2731.746652841568, 'accumulated_submission_time': 2602.7501418590546, 'accumulated_eval_time': 128.57271671295166, 'accumulated_logging_time': 0.13351845741271973}
I0229 06:00:07.773668 140411389597440 logging_writer.py:48] [7545] accumulated_eval_time=128.572717, accumulated_logging_time=0.133518, accumulated_submission_time=2602.750142, global_step=7545, preemption_count=0, score=2602.750142, test/accuracy=0.415800, test/loss=2.758139, test/num_examples=10000, total_duration=2731.746653, train/accuracy=0.579201, train/loss=1.753116, validation/accuracy=0.540140, validation/loss=1.974346, validation/num_examples=50000
I0229 06:00:26.661933 140411397990144 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.220508337020874, loss=2.2340645790100098
I0229 06:01:00.409820 140411389597440 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.3145149946212769, loss=2.301109552383423
I0229 06:01:34.160422 140411397990144 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.8160374164581299, loss=2.2865424156188965
I0229 06:02:07.922920 140411389597440 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.850171685218811, loss=2.175875186920166
I0229 06:02:41.675344 140411397990144 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9277327060699463, loss=2.0507893562316895
I0229 06:03:15.435769 140411389597440 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.8478299379348755, loss=2.2830498218536377
I0229 06:03:49.255601 140411397990144 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.8696295022964478, loss=2.2012412548065186
I0229 06:04:23.040357 140411389597440 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.6535130739212036, loss=2.1108832359313965
I0229 06:04:56.762460 140411397990144 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.7048662900924683, loss=2.148905038833618
I0229 06:05:30.562360 140411389597440 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.6650699377059937, loss=2.31663179397583
I0229 06:06:04.303447 140411397990144 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.8943533897399902, loss=2.0560481548309326
I0229 06:06:38.063224 140411389597440 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.456316351890564, loss=2.1254372596740723
I0229 06:07:11.822273 140411397990144 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.4190295934677124, loss=2.1183066368103027
I0229 06:07:45.564308 140411389597440 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.7317124605178833, loss=2.135807991027832
I0229 06:08:19.356736 140411397990144 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.9203370809555054, loss=2.2643282413482666
I0229 06:08:37.999430 140573303715648 spec.py:321] Evaluating on the training split.
I0229 06:08:45.371309 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:08:53.747508 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:08:56.088503 140573303715648 submission_runner.py:411] Time since start: 3260.08s, 	Step: 9057, 	{'train/accuracy': 0.6060865521430969, 'train/loss': 1.620163083076477, 'validation/accuracy': 0.5582199692726135, 'validation/loss': 1.88018000125885, 'validation/num_examples': 50000, 'test/accuracy': 0.43380001187324524, 'test/loss': 2.645254135131836, 'test/num_examples': 10000, 'score': 3112.906903028488, 'total_duration': 3260.0810463428497, 'accumulated_submission_time': 3112.906903028488, 'accumulated_eval_time': 146.66174578666687, 'accumulated_logging_time': 0.1640608310699463}
I0229 06:08:56.106499 140411498637056 logging_writer.py:48] [9057] accumulated_eval_time=146.661746, accumulated_logging_time=0.164061, accumulated_submission_time=3112.906903, global_step=9057, preemption_count=0, score=3112.906903, test/accuracy=0.433800, test/loss=2.645254, test/num_examples=10000, total_duration=3260.081046, train/accuracy=0.606087, train/loss=1.620163, validation/accuracy=0.558220, validation/loss=1.880180, validation/num_examples=50000
I0229 06:09:10.932255 140411507029760 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.6585404872894287, loss=2.083142042160034
I0229 06:09:44.676014 140411498637056 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.7573002576828003, loss=1.8674521446228027
I0229 06:10:18.483527 140411507029760 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.9714369773864746, loss=2.0393755435943604
I0229 06:10:52.291532 140411498637056 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.6478744745254517, loss=1.9677211046218872
I0229 06:11:26.100016 140411507029760 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.5748075246810913, loss=2.136183261871338
I0229 06:11:59.863048 140411498637056 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.480690360069275, loss=2.024057626724243
I0229 06:12:33.606522 140411507029760 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.1133761405944824, loss=2.077425956726074
I0229 06:13:07.362066 140411498637056 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.7837451696395874, loss=2.124507188796997
I0229 06:13:41.139808 140411507029760 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6334751844406128, loss=2.1006641387939453
I0229 06:14:14.900160 140411498637056 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.231367588043213, loss=2.2084550857543945
I0229 06:14:48.690044 140411507029760 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.534478783607483, loss=2.085439443588257
I0229 06:15:22.459497 140411498637056 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.6624863147735596, loss=1.9759957790374756
I0229 06:15:56.276278 140411507029760 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.402248501777649, loss=2.226161479949951
I0229 06:16:30.029747 140411498637056 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.6689810752868652, loss=2.194237470626831
I0229 06:17:03.880153 140411507029760 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.0092408657073975, loss=2.0191593170166016
I0229 06:17:26.231112 140573303715648 spec.py:321] Evaluating on the training split.
I0229 06:17:33.862471 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:17:42.390931 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:17:44.733486 140573303715648 submission_runner.py:411] Time since start: 3788.73s, 	Step: 10568, 	{'train/accuracy': 0.6356824040412903, 'train/loss': 1.4726837873458862, 'validation/accuracy': 0.5694800019264221, 'validation/loss': 1.8395023345947266, 'validation/num_examples': 50000, 'test/accuracy': 0.45000001788139343, 'test/loss': 2.5940630435943604, 'test/num_examples': 10000, 'score': 3622.9626841545105, 'total_duration': 3788.7260398864746, 'accumulated_submission_time': 3622.9626841545105, 'accumulated_eval_time': 165.1640853881836, 'accumulated_logging_time': 0.19446921348571777}
I0229 06:17:44.757423 140411397990144 logging_writer.py:48] [10568] accumulated_eval_time=165.164085, accumulated_logging_time=0.194469, accumulated_submission_time=3622.962684, global_step=10568, preemption_count=0, score=3622.962684, test/accuracy=0.450000, test/loss=2.594063, test/num_examples=10000, total_duration=3788.726040, train/accuracy=0.635682, train/loss=1.472684, validation/accuracy=0.569480, validation/loss=1.839502, validation/num_examples=50000
I0229 06:17:55.920734 140411406382848 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.544245719909668, loss=1.943590760231018
I0229 06:18:29.633947 140411397990144 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.3953707218170166, loss=1.9922947883605957
I0229 06:19:03.374135 140411406382848 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.7219038009643555, loss=2.048593759536743
I0229 06:19:37.132416 140411397990144 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.3047473430633545, loss=2.0653867721557617
I0229 06:20:10.873263 140411406382848 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.720388650894165, loss=2.0498175621032715
I0229 06:20:44.639643 140411397990144 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.7942984104156494, loss=2.0949432849884033
I0229 06:21:18.404683 140411406382848 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.5128192901611328, loss=1.9994536638259888
I0229 06:21:52.153411 140411397990144 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.8164664506912231, loss=2.0022289752960205
I0229 06:22:25.901527 140411406382848 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.496950626373291, loss=1.9663324356079102
I0229 06:22:59.678585 140411397990144 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.571509838104248, loss=2.1068081855773926
I0229 06:23:33.446510 140411406382848 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.4875926971435547, loss=1.920050859451294
I0229 06:24:07.202486 140411397990144 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.4138271808624268, loss=2.0018386840820312
I0229 06:24:40.978601 140411406382848 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.216326951980591, loss=2.008803367614746
I0229 06:25:14.742234 140411397990144 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.8924671411514282, loss=1.9328161478042603
I0229 06:25:48.514761 140411406382848 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.6246012449264526, loss=1.905951976776123
I0229 06:26:14.949460 140573303715648 spec.py:321] Evaluating on the training split.
I0229 06:26:22.370573 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:26:32.949237 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:26:35.217748 140573303715648 submission_runner.py:411] Time since start: 4319.21s, 	Step: 12080, 	{'train/accuracy': 0.6251793503761292, 'train/loss': 1.5146089792251587, 'validation/accuracy': 0.5725199580192566, 'validation/loss': 1.817832112312317, 'validation/num_examples': 50000, 'test/accuracy': 0.45500001311302185, 'test/loss': 2.581552028656006, 'test/num_examples': 10000, 'score': 4133.085363149643, 'total_duration': 4319.210289001465, 'accumulated_submission_time': 4133.085363149643, 'accumulated_eval_time': 185.43234658241272, 'accumulated_logging_time': 0.23001718521118164}
I0229 06:26:35.237784 140411381204736 logging_writer.py:48] [12080] accumulated_eval_time=185.432347, accumulated_logging_time=0.230017, accumulated_submission_time=4133.085363, global_step=12080, preemption_count=0, score=4133.085363, test/accuracy=0.455000, test/loss=2.581552, test/num_examples=10000, total_duration=4319.210289, train/accuracy=0.625179, train/loss=1.514609, validation/accuracy=0.572520, validation/loss=1.817832, validation/num_examples=50000
I0229 06:26:42.361770 140411389597440 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.074453353881836, loss=2.0693511962890625
I0229 06:27:16.061481 140411381204736 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.8290690183639526, loss=2.093493938446045
I0229 06:27:49.795614 140411389597440 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.5557197332382202, loss=1.9505878686904907
I0229 06:28:23.537536 140411381204736 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.6902716159820557, loss=1.9426807165145874
I0229 06:28:57.294694 140411389597440 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.5573629140853882, loss=2.012634754180908
I0229 06:29:31.105628 140411381204736 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.5117148160934448, loss=1.9615806341171265
I0229 06:30:04.888097 140411389597440 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4087283611297607, loss=1.9813940525054932
I0229 06:30:38.657968 140411381204736 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.6026029586791992, loss=1.982323169708252
I0229 06:31:12.429878 140411389597440 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6018708944320679, loss=1.9222582578659058
I0229 06:31:46.143764 140411381204736 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.72682785987854, loss=1.9579250812530518
I0229 06:32:19.899279 140411389597440 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.4103944301605225, loss=1.9468128681182861
I0229 06:32:53.682493 140411381204736 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.6245478391647339, loss=1.9835941791534424
I0229 06:33:27.405806 140411389597440 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.634112000465393, loss=1.9105879068374634
I0229 06:34:01.180923 140411381204736 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.237041711807251, loss=1.906664490699768
I0229 06:34:34.945660 140411389597440 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.6139665842056274, loss=1.875719666481018
I0229 06:35:05.453861 140573303715648 spec.py:321] Evaluating on the training split.
I0229 06:35:13.196279 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:35:23.679309 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:35:25.879748 140573303715648 submission_runner.py:411] Time since start: 4849.87s, 	Step: 13592, 	{'train/accuracy': 0.6494539380073547, 'train/loss': 1.422621726989746, 'validation/accuracy': 0.5925599932670593, 'validation/loss': 1.7074165344238281, 'validation/num_examples': 50000, 'test/accuracy': 0.46300002932548523, 'test/loss': 2.468202829360962, 'test/num_examples': 10000, 'score': 4643.2337856292725, 'total_duration': 4849.872251987457, 'accumulated_submission_time': 4643.2337856292725, 'accumulated_eval_time': 205.85814785957336, 'accumulated_logging_time': 0.26001858711242676}
I0229 06:35:25.899604 140411490244352 logging_writer.py:48] [13592] accumulated_eval_time=205.858148, accumulated_logging_time=0.260019, accumulated_submission_time=4643.233786, global_step=13592, preemption_count=0, score=4643.233786, test/accuracy=0.463000, test/loss=2.468203, test/num_examples=10000, total_duration=4849.872252, train/accuracy=0.649454, train/loss=1.422622, validation/accuracy=0.592560, validation/loss=1.707417, validation/num_examples=50000
I0229 06:35:28.955995 140411498637056 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.388547658920288, loss=1.9373071193695068
I0229 06:36:02.663922 140411490244352 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.3273447751998901, loss=1.8733657598495483
I0229 06:36:36.336819 140411498637056 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.4475153684616089, loss=1.7762351036071777
I0229 06:37:10.142521 140411490244352 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.7190765142440796, loss=1.9416933059692383
I0229 06:37:43.838902 140411498637056 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.3878488540649414, loss=1.891541838645935
I0229 06:38:17.599858 140411490244352 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5666207075119019, loss=1.92037832736969
I0229 06:38:51.341869 140411498637056 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.5271801948547363, loss=1.9070024490356445
I0229 06:39:25.080447 140411490244352 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.5660264492034912, loss=1.974847435951233
I0229 06:39:58.830976 140411498637056 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.5081268548965454, loss=1.8939721584320068
I0229 06:40:32.618668 140411490244352 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.5316168069839478, loss=1.847068190574646
I0229 06:41:06.310471 140411498637056 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.6878557205200195, loss=1.8981878757476807
I0229 06:41:40.104278 140411490244352 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.5303993225097656, loss=1.843419075012207
I0229 06:42:13.848563 140411498637056 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.7918739318847656, loss=1.7937240600585938
I0229 06:42:47.633784 140411490244352 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.5603442192077637, loss=1.9086991548538208
I0229 06:43:21.322885 140411498637056 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.2639389038085938, loss=1.881772518157959
I0229 06:43:55.112067 140411490244352 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.7147789001464844, loss=1.892196536064148
I0229 06:43:55.887656 140573303715648 spec.py:321] Evaluating on the training split.
I0229 06:44:04.254536 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:44:14.113737 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:44:16.401458 140573303715648 submission_runner.py:411] Time since start: 5380.39s, 	Step: 15104, 	{'train/accuracy': 0.6466438174247742, 'train/loss': 1.4184733629226685, 'validation/accuracy': 0.5945199728012085, 'validation/loss': 1.7023662328720093, 'validation/num_examples': 50000, 'test/accuracy': 0.4723000228404999, 'test/loss': 2.4483578205108643, 'test/num_examples': 10000, 'score': 5153.15531373024, 'total_duration': 5380.393999576569, 'accumulated_submission_time': 5153.15531373024, 'accumulated_eval_time': 226.37191343307495, 'accumulated_logging_time': 0.28925299644470215}
I0229 06:44:16.435389 140409577658112 logging_writer.py:48] [15104] accumulated_eval_time=226.371913, accumulated_logging_time=0.289253, accumulated_submission_time=5153.155314, global_step=15104, preemption_count=0, score=5153.155314, test/accuracy=0.472300, test/loss=2.448358, test/num_examples=10000, total_duration=5380.394000, train/accuracy=0.646644, train/loss=1.418473, validation/accuracy=0.594520, validation/loss=1.702366, validation/num_examples=50000
I0229 06:44:49.126005 140409586050816 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.4344518184661865, loss=1.9752328395843506
I0229 06:45:22.891921 140409577658112 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.821218490600586, loss=1.8752355575561523
I0229 06:45:56.643319 140409586050816 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.4886921644210815, loss=1.9561129808425903
I0229 06:46:30.403166 140409577658112 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.607450008392334, loss=1.8833801746368408
I0229 06:47:04.130740 140409586050816 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.9487675428390503, loss=1.891173243522644
I0229 06:47:37.947625 140409577658112 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.5084922313690186, loss=1.8160218000411987
I0229 06:48:11.691378 140409586050816 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.6441764831542969, loss=1.9404199123382568
I0229 06:48:45.460129 140409577658112 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5614221096038818, loss=1.8219796419143677
I0229 06:49:19.228505 140409586050816 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5444614887237549, loss=1.7870759963989258
I0229 06:49:52.966165 140409577658112 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.5549736022949219, loss=1.8057911396026611
I0229 06:50:26.693244 140409586050816 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.592703938484192, loss=1.9324897527694702
I0229 06:51:00.466797 140409577658112 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.7414618730545044, loss=1.879831314086914
I0229 06:51:34.231471 140409586050816 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.6637173891067505, loss=1.8852229118347168
I0229 06:52:07.969906 140409577658112 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.564829707145691, loss=1.8082647323608398
I0229 06:52:41.759216 140409586050816 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.4952465295791626, loss=1.8218191862106323
I0229 06:52:46.601832 140573303715648 spec.py:321] Evaluating on the training split.
I0229 06:52:54.347802 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 06:53:04.354553 140573303715648 spec.py:349] Evaluating on the test split.
I0229 06:53:06.616805 140573303715648 submission_runner.py:411] Time since start: 5910.61s, 	Step: 16616, 	{'train/accuracy': 0.6574457883834839, 'train/loss': 1.3805900812149048, 'validation/accuracy': 0.6076599955558777, 'validation/loss': 1.6330012083053589, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.402414321899414, 'test/num_examples': 10000, 'score': 5663.251859664917, 'total_duration': 5910.609344005585, 'accumulated_submission_time': 5663.251859664917, 'accumulated_eval_time': 246.38683366775513, 'accumulated_logging_time': 0.33504152297973633}
I0229 06:53:06.636070 140409854486272 logging_writer.py:48] [16616] accumulated_eval_time=246.386834, accumulated_logging_time=0.335042, accumulated_submission_time=5663.251860, global_step=16616, preemption_count=0, score=5663.251860, test/accuracy=0.477100, test/loss=2.402414, test/num_examples=10000, total_duration=5910.609344, train/accuracy=0.657446, train/loss=1.380590, validation/accuracy=0.607660, validation/loss=1.633001, validation/num_examples=50000
I0229 06:53:35.304693 140409862878976 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.6077244281768799, loss=1.7354613542556763
I0229 06:54:08.992257 140409854486272 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.432328701019287, loss=1.8023923635482788
I0229 06:54:42.756070 140409862878976 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.4223320484161377, loss=1.8682688474655151
I0229 06:55:16.556631 140409854486272 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.8735108375549316, loss=1.8591538667678833
I0229 06:55:50.311015 140409862878976 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.6815694570541382, loss=1.9117645025253296
I0229 06:56:24.005284 140409854486272 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.5338358879089355, loss=1.841970443725586
I0229 06:56:57.753565 140409862878976 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.5154155492782593, loss=1.8643007278442383
I0229 06:57:31.522520 140409854486272 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.4101619720458984, loss=1.7777464389801025
I0229 06:58:05.229078 140409862878976 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6006954908370972, loss=1.9022243022918701
I0229 06:58:39.025529 140409854486272 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.638486623764038, loss=1.8783950805664062
I0229 06:59:12.739886 140409862878976 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.598052740097046, loss=1.9847993850708008
I0229 06:59:46.512822 140409854486272 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.5307430028915405, loss=1.901479959487915
I0229 07:00:20.264217 140409862878976 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.4515641927719116, loss=1.873020887374878
I0229 07:00:53.980273 140409854486272 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.638105869293213, loss=1.758778691291809
I0229 07:01:27.735354 140409862878976 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.746930718421936, loss=1.8166061639785767
I0229 07:01:36.636995 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:01:45.000046 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:01:58.609107 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:02:00.782918 140573303715648 submission_runner.py:411] Time since start: 6444.78s, 	Step: 18128, 	{'train/accuracy': 0.6847695708274841, 'train/loss': 1.253440260887146, 'validation/accuracy': 0.6041199564933777, 'validation/loss': 1.6582300662994385, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.4177982807159424, 'test/num_examples': 10000, 'score': 6173.184433221817, 'total_duration': 6444.77542424202, 'accumulated_submission_time': 6173.184433221817, 'accumulated_eval_time': 270.5326895713806, 'accumulated_logging_time': 0.364840030670166}
I0229 07:02:00.813737 140408587810560 logging_writer.py:48] [18128] accumulated_eval_time=270.532690, accumulated_logging_time=0.364840, accumulated_submission_time=6173.184433, global_step=18128, preemption_count=0, score=6173.184433, test/accuracy=0.483000, test/loss=2.417798, test/num_examples=10000, total_duration=6444.775424, train/accuracy=0.684770, train/loss=1.253440, validation/accuracy=0.604120, validation/loss=1.658230, validation/num_examples=50000
I0229 07:02:25.449627 140409124681472 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.786577582359314, loss=1.8345216512680054
I0229 07:02:59.143679 140408587810560 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.3550034761428833, loss=1.7252565622329712
I0229 07:03:32.903368 140409124681472 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.7307405471801758, loss=1.9855538606643677
I0229 07:04:06.670082 140408587810560 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.5898514986038208, loss=1.761640191078186
I0229 07:04:40.414840 140409124681472 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.7692509889602661, loss=1.8084497451782227
I0229 07:05:14.159364 140408587810560 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.757508635520935, loss=1.9363391399383545
I0229 07:05:47.905066 140409124681472 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.5892385244369507, loss=1.8625324964523315
I0229 07:06:21.707241 140408587810560 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.525392770767212, loss=1.8968183994293213
I0229 07:06:55.472116 140409124681472 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.4814234972000122, loss=1.7537438869476318
I0229 07:07:29.236286 140408587810560 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.520169734954834, loss=1.5621156692504883
I0229 07:08:02.955454 140409124681472 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7342860698699951, loss=1.804443597793579
I0229 07:08:36.701820 140408587810560 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.9129524230957031, loss=1.8291397094726562
I0229 07:09:10.448276 140409124681472 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.5827549695968628, loss=1.8249006271362305
I0229 07:09:44.169595 140408587810560 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.886936902999878, loss=1.8827649354934692
I0229 07:10:17.918582 140409124681472 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.7310817241668701, loss=1.7750777006149292
I0229 07:10:30.821214 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:10:38.601568 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:10:53.098194 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:10:55.287159 140573303715648 submission_runner.py:411] Time since start: 6979.28s, 	Step: 19640, 	{'train/accuracy': 0.6862244606018066, 'train/loss': 1.2184010744094849, 'validation/accuracy': 0.6089800000190735, 'validation/loss': 1.6390219926834106, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.403407573699951, 'test/num_examples': 10000, 'score': 6683.125174283981, 'total_duration': 6979.279679298401, 'accumulated_submission_time': 6683.125174283981, 'accumulated_eval_time': 294.9985795021057, 'accumulated_logging_time': 0.404205322265625}
I0229 07:10:55.309039 140409846093568 logging_writer.py:48] [19640] accumulated_eval_time=294.998580, accumulated_logging_time=0.404205, accumulated_submission_time=6683.125174, global_step=19640, preemption_count=0, score=6683.125174, test/accuracy=0.488200, test/loss=2.403408, test/num_examples=10000, total_duration=6979.279679, train/accuracy=0.686224, train/loss=1.218401, validation/accuracy=0.608980, validation/loss=1.639022, validation/num_examples=50000
I0229 07:11:15.860872 140409854486272 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.9254893064498901, loss=1.7070327997207642
I0229 07:11:49.540393 140409846093568 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6713063716888428, loss=1.8130748271942139
I0229 07:12:23.331904 140409854486272 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.6010286808013916, loss=1.918695330619812
I0229 07:12:57.048849 140409846093568 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.868638277053833, loss=1.7836716175079346
I0229 07:13:30.729492 140409854486272 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6340289115905762, loss=1.8705729246139526
I0229 07:14:04.481585 140409846093568 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.6677495241165161, loss=1.8937995433807373
I0229 07:14:38.214477 140409854486272 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.800907850265503, loss=1.7794116735458374
I0229 07:15:11.933664 140409846093568 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.541291356086731, loss=1.7734712362289429
I0229 07:15:45.621391 140409854486272 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.781327724456787, loss=1.7111674547195435
I0229 07:16:19.364384 140409846093568 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6223649978637695, loss=1.7863874435424805
I0229 07:16:53.054813 140409854486272 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.6264894008636475, loss=1.6779234409332275
I0229 07:17:26.802299 140409846093568 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.688165307044983, loss=1.882278561592102
I0229 07:18:00.505523 140409854486272 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.9715352058410645, loss=1.8669021129608154
I0229 07:18:34.252441 140409846093568 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.8347887992858887, loss=1.8609542846679688
I0229 07:19:07.990624 140409854486272 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.6909552812576294, loss=1.8232272863388062
I0229 07:19:25.613858 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:19:33.293059 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:19:47.456515 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:19:49.638895 140573303715648 submission_runner.py:411] Time since start: 7513.63s, 	Step: 21154, 	{'train/accuracy': 0.6550542116165161, 'train/loss': 1.37153160572052, 'validation/accuracy': 0.5969399809837341, 'validation/loss': 1.7031642198562622, 'validation/num_examples': 50000, 'test/accuracy': 0.4700000286102295, 'test/loss': 2.498812675476074, 'test/num_examples': 10000, 'score': 7193.3619503974915, 'total_duration': 7513.631420373917, 'accumulated_submission_time': 7193.3619503974915, 'accumulated_eval_time': 319.0235517024994, 'accumulated_logging_time': 0.4361577033996582}
I0229 07:19:49.658759 140408587810560 logging_writer.py:48] [21154] accumulated_eval_time=319.023552, accumulated_logging_time=0.436158, accumulated_submission_time=7193.361950, global_step=21154, preemption_count=0, score=7193.361950, test/accuracy=0.470000, test/loss=2.498813, test/num_examples=10000, total_duration=7513.631420, train/accuracy=0.655054, train/loss=1.371532, validation/accuracy=0.596940, validation/loss=1.703164, validation/num_examples=50000
I0229 07:20:05.484233 140409124681472 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.614783525466919, loss=1.824197769165039
I0229 07:20:39.110979 140408587810560 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.5547374486923218, loss=1.6570281982421875
I0229 07:21:12.868433 140409124681472 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6188867092132568, loss=1.8027211427688599
I0229 07:21:46.593297 140408587810560 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.760697603225708, loss=1.7610877752304077
I0229 07:22:20.335405 140409124681472 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6958638429641724, loss=1.7417770624160767
I0229 07:22:54.092051 140408587810560 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.629913330078125, loss=1.8664073944091797
I0229 07:23:27.836356 140409124681472 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.6435279846191406, loss=1.7790615558624268
I0229 07:24:01.576625 140408587810560 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.7016819715499878, loss=1.750464916229248
I0229 07:24:35.342628 140409124681472 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.6077345609664917, loss=1.8309564590454102
I0229 07:25:09.081473 140408587810560 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.572655439376831, loss=1.8166992664337158
I0229 07:25:42.828713 140409124681472 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.6819581985473633, loss=1.7847758531570435
I0229 07:26:16.550963 140408587810560 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.8475362062454224, loss=1.8187294006347656
I0229 07:26:50.294947 140409124681472 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.4998422861099243, loss=1.782896637916565
I0229 07:27:24.043058 140408587810560 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.9703577756881714, loss=1.7650914192199707
I0229 07:27:57.777781 140409124681472 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.7443219423294067, loss=1.7570374011993408
I0229 07:28:19.820759 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:28:27.451864 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:28:41.770793 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:28:43.981103 140573303715648 submission_runner.py:411] Time since start: 8047.97s, 	Step: 22667, 	{'train/accuracy': 0.6710379123687744, 'train/loss': 1.3040506839752197, 'validation/accuracy': 0.6122399568557739, 'validation/loss': 1.612977147102356, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.3287904262542725, 'test/num_examples': 10000, 'score': 7703.452437400818, 'total_duration': 8047.973657846451, 'accumulated_submission_time': 7703.452437400818, 'accumulated_eval_time': 343.1838707923889, 'accumulated_logging_time': 0.46936583518981934}
I0229 07:28:43.998111 140408587810560 logging_writer.py:48] [22667] accumulated_eval_time=343.183871, accumulated_logging_time=0.469366, accumulated_submission_time=7703.452437, global_step=22667, preemption_count=0, score=7703.452437, test/accuracy=0.486900, test/loss=2.328790, test/num_examples=10000, total_duration=8047.973658, train/accuracy=0.671038, train/loss=1.304051, validation/accuracy=0.612240, validation/loss=1.612977, validation/num_examples=50000
I0229 07:28:55.433397 140409854486272 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.6415762901306152, loss=1.7582149505615234
I0229 07:29:29.093597 140408587810560 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.7392107248306274, loss=1.7878217697143555
I0229 07:30:02.836452 140409854486272 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.8708726167678833, loss=1.7823013067245483
I0229 07:30:36.587804 140408587810560 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.6649425029754639, loss=1.694999098777771
I0229 07:31:10.304767 140409854486272 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.688984751701355, loss=1.6772743463516235
I0229 07:31:43.991192 140408587810560 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.7187598943710327, loss=1.8261072635650635
I0229 07:32:17.754266 140409854486272 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.6597630977630615, loss=1.757733702659607
I0229 07:32:51.454448 140408587810560 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.8255866765975952, loss=1.7568953037261963
I0229 07:33:25.228159 140409854486272 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.5471113920211792, loss=1.8507860898971558
I0229 07:33:58.963788 140408587810560 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.66171395778656, loss=1.7927265167236328
I0229 07:34:32.670108 140409854486272 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.664070963859558, loss=1.7378990650177002
I0229 07:35:06.395637 140408587810560 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.5543909072875977, loss=1.764484167098999
I0229 07:35:40.093023 140409854486272 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.697873592376709, loss=1.8388683795928955
I0229 07:36:13.827119 140408587810560 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.5793328285217285, loss=1.7958548069000244
I0229 07:36:47.549149 140409854486272 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.7433339357376099, loss=1.8746105432510376
I0229 07:37:13.983900 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:37:21.729890 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:37:33.906068 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:37:36.184761 140573303715648 submission_runner.py:411] Time since start: 8580.18s, 	Step: 24180, 	{'train/accuracy': 0.6864636540412903, 'train/loss': 1.2356112003326416, 'validation/accuracy': 0.6240800023078918, 'validation/loss': 1.556673288345337, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.2998135089874268, 'test/num_examples': 10000, 'score': 8213.370545864105, 'total_duration': 8580.17731165886, 'accumulated_submission_time': 8213.370545864105, 'accumulated_eval_time': 365.3846924304962, 'accumulated_logging_time': 0.4949944019317627}
I0229 07:37:36.203374 140408579417856 logging_writer.py:48] [24180] accumulated_eval_time=365.384692, accumulated_logging_time=0.494994, accumulated_submission_time=8213.370546, global_step=24180, preemption_count=0, score=8213.370546, test/accuracy=0.505600, test/loss=2.299814, test/num_examples=10000, total_duration=8580.177312, train/accuracy=0.686464, train/loss=1.235611, validation/accuracy=0.624080, validation/loss=1.556673, validation/num_examples=50000
I0229 07:37:43.286693 140408587810560 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.636204719543457, loss=1.7512708902359009
I0229 07:38:16.933247 140408579417856 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.4731602668762207, loss=1.612399935722351
I0229 07:38:50.701746 140408587810560 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.594672441482544, loss=1.6675758361816406
I0229 07:39:24.435028 140408579417856 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.973347783088684, loss=1.8123364448547363
I0229 07:39:58.137037 140408587810560 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.7838561534881592, loss=1.6937761306762695
I0229 07:40:31.879878 140408579417856 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.7962766885757446, loss=1.749847650527954
I0229 07:41:05.544781 140408587810560 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.933305263519287, loss=1.6904770135879517
I0229 07:41:39.301051 140408579417856 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.0759165287017822, loss=1.7490262985229492
I0229 07:42:13.070313 140408587810560 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.5882601737976074, loss=1.7515357732772827
I0229 07:42:46.783201 140408579417856 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.6683745384216309, loss=1.7197736501693726
I0229 07:43:20.587582 140408587810560 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.661821961402893, loss=1.845211148262024
I0229 07:43:54.335392 140408579417856 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.9463859796524048, loss=1.8149532079696655
I0229 07:44:28.049555 140408587810560 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8164820671081543, loss=1.7300711870193481
I0229 07:45:01.800589 140408579417856 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.615979552268982, loss=1.6597453355789185
I0229 07:45:35.600429 140408587810560 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.7131198644638062, loss=1.6721947193145752
I0229 07:46:06.435370 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:46:14.545218 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:46:28.729738 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:46:30.909141 140573303715648 submission_runner.py:411] Time since start: 9114.90s, 	Step: 25693, 	{'train/accuracy': 0.6590800285339355, 'train/loss': 1.3417572975158691, 'validation/accuracy': 0.6096000075340271, 'validation/loss': 1.6214866638183594, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.368893623352051, 'test/num_examples': 10000, 'score': 8723.534445524216, 'total_duration': 9114.901677131653, 'accumulated_submission_time': 8723.534445524216, 'accumulated_eval_time': 389.8584134578705, 'accumulated_logging_time': 0.5229532718658447}
I0229 07:46:30.928033 140409846093568 logging_writer.py:48] [25693] accumulated_eval_time=389.858413, accumulated_logging_time=0.522953, accumulated_submission_time=8723.534446, global_step=25693, preemption_count=0, score=8723.534446, test/accuracy=0.483200, test/loss=2.368894, test/num_examples=10000, total_duration=9114.901677, train/accuracy=0.659080, train/loss=1.341757, validation/accuracy=0.609600, validation/loss=1.621487, validation/num_examples=50000
I0229 07:46:33.630808 140409854486272 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.7092045545578003, loss=1.796309471130371
I0229 07:47:07.326472 140409846093568 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.530655860900879, loss=1.809312105178833
I0229 07:47:41.018416 140409854486272 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.7991489171981812, loss=1.6492687463760376
I0229 07:48:14.712216 140409846093568 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.7992271184921265, loss=1.6666288375854492
I0229 07:48:48.474251 140409854486272 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7391657829284668, loss=1.6482435464859009
I0229 07:49:22.243221 140409846093568 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6634297370910645, loss=1.6654483079910278
I0229 07:49:55.966914 140409854486272 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.91972017288208, loss=1.818077802658081
I0229 07:50:29.692800 140409846093568 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.882846713066101, loss=1.676475167274475
I0229 07:51:03.474236 140409854486272 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.6188013553619385, loss=1.9128705263137817
I0229 07:51:37.197831 140409846093568 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6775634288787842, loss=1.7462239265441895
I0229 07:52:10.962402 140409854486272 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8705049753189087, loss=1.7574397325515747
I0229 07:52:44.690769 140409846093568 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.9833918809890747, loss=1.7961586713790894
I0229 07:53:18.415039 140409854486272 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.8474822044372559, loss=1.7472243309020996
I0229 07:53:52.137810 140409846093568 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6737914085388184, loss=1.77603280544281
I0229 07:54:25.928562 140409854486272 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.1414527893066406, loss=1.8446581363677979
I0229 07:54:59.646490 140409846093568 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.7536674737930298, loss=1.807034969329834
I0229 07:55:00.974925 140573303715648 spec.py:321] Evaluating on the training split.
I0229 07:55:09.241710 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 07:55:22.703521 140573303715648 spec.py:349] Evaluating on the test split.
I0229 07:55:24.946075 140573303715648 submission_runner.py:411] Time since start: 9648.94s, 	Step: 27205, 	{'train/accuracy': 0.7247090339660645, 'train/loss': 1.0631380081176758, 'validation/accuracy': 0.6251999735832214, 'validation/loss': 1.5556570291519165, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.290503978729248, 'test/num_examples': 10000, 'score': 9233.515226125717, 'total_duration': 9648.93862605095, 'accumulated_submission_time': 9233.515226125717, 'accumulated_eval_time': 413.8295383453369, 'accumulated_logging_time': 0.5503711700439453}
I0229 07:55:24.964247 140408319375104 logging_writer.py:48] [27205] accumulated_eval_time=413.829538, accumulated_logging_time=0.550371, accumulated_submission_time=9233.515226, global_step=27205, preemption_count=0, score=9233.515226, test/accuracy=0.495900, test/loss=2.290504, test/num_examples=10000, total_duration=9648.938626, train/accuracy=0.724709, train/loss=1.063138, validation/accuracy=0.625200, validation/loss=1.555657, validation/num_examples=50000
I0229 07:55:57.352049 140409124681472 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6632384061813354, loss=1.7041137218475342
I0229 07:56:31.037901 140408319375104 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.8879865407943726, loss=1.606081247329712
I0229 07:57:04.770546 140409124681472 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.4208738803863525, loss=1.821305274963379
I0229 07:57:38.515612 140408319375104 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.8485260009765625, loss=1.8159247636795044
I0229 07:58:12.246561 140409124681472 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.7929733991622925, loss=1.7791762351989746
I0229 07:58:46.015246 140408319375104 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.623396873474121, loss=1.6831246614456177
I0229 07:59:19.734105 140409124681472 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.6096082925796509, loss=1.682770013809204
I0229 07:59:53.468230 140408319375104 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.7078328132629395, loss=1.8288246393203735
I0229 08:00:27.225237 140409124681472 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.6526728868484497, loss=1.7582898139953613
I0229 08:01:00.941832 140408319375104 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.7849392890930176, loss=1.70176100730896
I0229 08:01:34.707822 140409124681472 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.7156519889831543, loss=1.8171496391296387
I0229 08:02:08.505377 140408319375104 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.179436206817627, loss=1.8236571550369263
I0229 08:02:42.250421 140409124681472 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.7122000455856323, loss=1.7076122760772705
I0229 08:03:15.996862 140408319375104 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.1266705989837646, loss=1.6688493490219116
I0229 08:03:49.742314 140409124681472 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6977936029434204, loss=1.8224399089813232
I0229 08:03:55.243146 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:04:03.333994 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:04:17.814738 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:04:19.958686 140573303715648 submission_runner.py:411] Time since start: 10183.95s, 	Step: 28718, 	{'train/accuracy': 0.6881178021430969, 'train/loss': 1.2257320880889893, 'validation/accuracy': 0.6116399765014648, 'validation/loss': 1.6099690198898315, 'validation/num_examples': 50000, 'test/accuracy': 0.48180001974105835, 'test/loss': 2.3851914405822754, 'test/num_examples': 10000, 'score': 9743.726099729538, 'total_duration': 10183.951251506805, 'accumulated_submission_time': 9743.726099729538, 'accumulated_eval_time': 438.5450539588928, 'accumulated_logging_time': 0.5776462554931641}
I0229 08:04:19.977608 140408310982400 logging_writer.py:48] [28718] accumulated_eval_time=438.545054, accumulated_logging_time=0.577646, accumulated_submission_time=9743.726100, global_step=28718, preemption_count=0, score=9743.726100, test/accuracy=0.481800, test/loss=2.385191, test/num_examples=10000, total_duration=10183.951252, train/accuracy=0.688118, train/loss=1.225732, validation/accuracy=0.611640, validation/loss=1.609969, validation/num_examples=50000
I0229 08:04:47.904098 140409846093568 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.9301893711090088, loss=1.7438018321990967
I0229 08:05:21.608027 140408310982400 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.9368728399276733, loss=1.7479681968688965
I0229 08:05:55.293764 140409846093568 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6453759670257568, loss=1.6877878904342651
I0229 08:06:29.034950 140408310982400 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.7427735328674316, loss=1.769445538520813
I0229 08:07:02.737226 140409846093568 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.8470741510391235, loss=1.7981680631637573
I0229 08:07:36.480748 140408310982400 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.7625341415405273, loss=1.7832849025726318
I0229 08:08:10.236350 140409846093568 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.6557576656341553, loss=1.6993497610092163
I0229 08:08:43.943285 140408310982400 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.9911365509033203, loss=1.7510932683944702
I0229 08:09:17.708343 140409846093568 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.7920705080032349, loss=1.789611577987671
I0229 08:09:51.403769 140408310982400 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.7723283767700195, loss=1.7097526788711548
I0229 08:10:25.186138 140409846093568 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.6819300651550293, loss=1.6741611957550049
I0229 08:10:58.946604 140408310982400 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.8501315116882324, loss=1.7404390573501587
I0229 08:11:32.637950 140409846093568 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.5451042652130127, loss=1.6378724575042725
I0229 08:12:06.382435 140408310982400 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.888946294784546, loss=1.6777540445327759
I0229 08:12:40.072193 140409846093568 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.631948709487915, loss=1.550773024559021
I0229 08:12:49.960744 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:12:57.774189 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:13:18.594998 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:13:20.894529 140573303715648 submission_runner.py:411] Time since start: 10724.89s, 	Step: 30231, 	{'train/accuracy': 0.69140625, 'train/loss': 1.2178308963775635, 'validation/accuracy': 0.6235600113868713, 'validation/loss': 1.5551875829696655, 'validation/num_examples': 50000, 'test/accuracy': 0.4962000250816345, 'test/loss': 2.3088910579681396, 'test/num_examples': 10000, 'score': 10253.642538785934, 'total_duration': 10724.887073040009, 'accumulated_submission_time': 10253.642538785934, 'accumulated_eval_time': 469.4787917137146, 'accumulated_logging_time': 0.6060595512390137}
I0229 08:13:20.918146 140409829308160 logging_writer.py:48] [30231] accumulated_eval_time=469.478792, accumulated_logging_time=0.606060, accumulated_submission_time=10253.642539, global_step=30231, preemption_count=0, score=10253.642539, test/accuracy=0.496200, test/loss=2.308891, test/num_examples=10000, total_duration=10724.887073, train/accuracy=0.691406, train/loss=1.217831, validation/accuracy=0.623560, validation/loss=1.555188, validation/num_examples=50000
I0229 08:13:44.484409 140409837700864 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.777513861656189, loss=1.7427425384521484
I0229 08:14:18.163108 140409829308160 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.9305065870285034, loss=1.7735353708267212
I0229 08:14:51.946661 140409837700864 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.7106090784072876, loss=1.6997966766357422
I0229 08:15:25.679448 140409829308160 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.9209495782852173, loss=1.709327220916748
I0229 08:15:59.384268 140409837700864 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.7200735807418823, loss=1.7255806922912598
I0229 08:16:33.117758 140409829308160 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.9536925554275513, loss=1.806785225868225
I0229 08:17:06.795887 140409837700864 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.7986217737197876, loss=1.6891694068908691
I0229 08:17:40.551192 140409829308160 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.8113176822662354, loss=1.7416563034057617
I0229 08:18:14.267641 140409837700864 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.660367488861084, loss=1.6797113418579102
I0229 08:18:47.993460 140409829308160 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.8286161422729492, loss=1.889498233795166
I0229 08:19:21.754399 140409837700864 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.770405888557434, loss=1.7132586240768433
I0229 08:19:55.503784 140409829308160 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.8420517444610596, loss=1.6631412506103516
I0229 08:20:29.275504 140409837700864 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7877463102340698, loss=1.6524758338928223
I0229 08:21:03.021726 140409829308160 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.9695526361465454, loss=1.661059021949768
I0229 08:21:36.783966 140409837700864 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.8094756603240967, loss=1.666379451751709
I0229 08:21:51.053966 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:21:59.248437 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:22:14.269199 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:22:16.489113 140573303715648 submission_runner.py:411] Time since start: 11260.48s, 	Step: 31744, 	{'train/accuracy': 0.6746252775192261, 'train/loss': 1.2774403095245361, 'validation/accuracy': 0.6144999861717224, 'validation/loss': 1.6090742349624634, 'validation/num_examples': 50000, 'test/accuracy': 0.49240002036094666, 'test/loss': 2.371462106704712, 'test/num_examples': 10000, 'score': 10763.705124855042, 'total_duration': 11260.48165845871, 'accumulated_submission_time': 10763.705124855042, 'accumulated_eval_time': 494.91389536857605, 'accumulated_logging_time': 0.6445157527923584}
I0229 08:22:16.508105 140408319375104 logging_writer.py:48] [31744] accumulated_eval_time=494.913895, accumulated_logging_time=0.644516, accumulated_submission_time=10763.705125, global_step=31744, preemption_count=0, score=10763.705125, test/accuracy=0.492400, test/loss=2.371462, test/num_examples=10000, total_duration=11260.481658, train/accuracy=0.674625, train/loss=1.277440, validation/accuracy=0.614500, validation/loss=1.609074, validation/num_examples=50000
I0229 08:22:35.746543 140409124681472 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.5690202713012695, loss=1.5562472343444824
I0229 08:23:09.406598 140408319375104 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.6859043836593628, loss=1.7996022701263428
I0229 08:23:43.193190 140409124681472 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.790940284729004, loss=1.7624245882034302
I0229 08:24:16.904211 140408319375104 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.7755541801452637, loss=1.680061936378479
I0229 08:24:50.673834 140409124681472 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6184202432632446, loss=1.7091847658157349
I0229 08:25:24.408264 140408319375104 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.9211539030075073, loss=1.7030913829803467
I0229 08:25:58.117979 140409124681472 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6786556243896484, loss=1.7718942165374756
I0229 08:26:31.874802 140408319375104 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7910171747207642, loss=1.744094729423523
I0229 08:27:05.718376 140409124681472 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.7315025329589844, loss=1.7185280323028564
I0229 08:27:39.427567 140408319375104 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.0392017364501953, loss=1.7452518939971924
I0229 08:28:13.178021 140409124681472 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.0462470054626465, loss=1.7838412523269653
I0229 08:28:46.924498 140408319375104 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.6154961585998535, loss=1.730419397354126
I0229 08:29:20.641245 140409124681472 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.8100866079330444, loss=1.6736924648284912
I0229 08:29:54.367781 140408319375104 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.8312668800354004, loss=1.6338999271392822
I0229 08:30:28.084476 140409124681472 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.119631052017212, loss=1.7084752321243286
I0229 08:30:46.765614 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:30:54.811558 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:31:09.442059 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:31:11.586445 140573303715648 submission_runner.py:411] Time since start: 11795.58s, 	Step: 33257, 	{'train/accuracy': 0.6889747977256775, 'train/loss': 1.2175207138061523, 'validation/accuracy': 0.6301999688148499, 'validation/loss': 1.5399216413497925, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.308980941772461, 'test/num_examples': 10000, 'score': 11273.895986318588, 'total_duration': 11795.578994750977, 'accumulated_submission_time': 11273.895986318588, 'accumulated_eval_time': 519.7346889972687, 'accumulated_logging_time': 0.6725142002105713}
I0229 08:31:11.605472 140407379851008 logging_writer.py:48] [33257] accumulated_eval_time=519.734689, accumulated_logging_time=0.672514, accumulated_submission_time=11273.895986, global_step=33257, preemption_count=0, score=11273.895986, test/accuracy=0.500900, test/loss=2.308981, test/num_examples=10000, total_duration=11795.578995, train/accuracy=0.688975, train/loss=1.217521, validation/accuracy=0.630200, validation/loss=1.539922, validation/num_examples=50000
I0229 08:31:26.454053 140408319375104 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.888930320739746, loss=1.7353569269180298
I0229 08:32:00.119776 140407379851008 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.6849626302719116, loss=1.7047944068908691
I0229 08:32:33.772037 140408319375104 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6499513387680054, loss=1.6601868867874146
I0229 08:33:07.540781 140407379851008 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.6932235956192017, loss=1.748650074005127
I0229 08:33:41.317234 140408319375104 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.0045106410980225, loss=1.6967164278030396
I0229 08:34:15.038579 140407379851008 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.5811207294464111, loss=1.6137464046478271
I0229 08:34:48.792846 140408319375104 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.611875295639038, loss=1.5997354984283447
I0229 08:35:22.522317 140407379851008 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7651417255401611, loss=1.7666082382202148
I0229 08:35:56.237943 140408319375104 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.7898118495941162, loss=1.767317771911621
I0229 08:36:29.956982 140407379851008 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.8181819915771484, loss=1.7386318445205688
I0229 08:37:03.659790 140408319375104 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.0798299312591553, loss=1.5874617099761963
I0229 08:37:37.361241 140407379851008 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.648339867591858, loss=1.5351916551589966
I0229 08:38:11.094575 140408319375104 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.9562586545944214, loss=1.7987256050109863
I0229 08:38:44.839408 140407379851008 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.740546703338623, loss=1.6415557861328125
I0229 08:39:18.596864 140408319375104 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.0063936710357666, loss=1.6596307754516602
I0229 08:39:41.618039 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:39:49.823535 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:40:04.688311 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:40:06.835419 140573303715648 submission_runner.py:411] Time since start: 12330.83s, 	Step: 34770, 	{'train/accuracy': 0.6912468075752258, 'train/loss': 1.2342867851257324, 'validation/accuracy': 0.6308000087738037, 'validation/loss': 1.5375585556030273, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2644104957580566, 'test/num_examples': 10000, 'score': 11783.841839790344, 'total_duration': 12330.82792854309, 'accumulated_submission_time': 11783.841839790344, 'accumulated_eval_time': 544.9519906044006, 'accumulated_logging_time': 0.7002818584442139}
I0229 08:40:06.855974 140407379851008 logging_writer.py:48] [34770] accumulated_eval_time=544.951991, accumulated_logging_time=0.700282, accumulated_submission_time=11783.841840, global_step=34770, preemption_count=0, score=11783.841840, test/accuracy=0.505500, test/loss=2.264410, test/num_examples=10000, total_duration=12330.827929, train/accuracy=0.691247, train/loss=1.234287, validation/accuracy=0.630800, validation/loss=1.537559, validation/num_examples=50000
I0229 08:40:17.289126 140409124681472 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.7315328121185303, loss=1.744022250175476
I0229 08:40:50.986459 140407379851008 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.6876511573791504, loss=1.604530930519104
I0229 08:41:24.658901 140409124681472 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.6352832317352295, loss=1.7236608266830444
I0229 08:41:58.399272 140407379851008 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.7420237064361572, loss=1.6947805881500244
I0229 08:42:32.108568 140409124681472 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.1121742725372314, loss=1.7533037662506104
I0229 08:43:05.838385 140407379851008 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.7999699115753174, loss=1.7215890884399414
I0229 08:43:39.609914 140409124681472 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.7959293127059937, loss=1.643308401107788
I0229 08:44:13.308313 140407379851008 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.6416912078857422, loss=1.7018227577209473
I0229 08:44:47.066924 140409124681472 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.9127235412597656, loss=1.586208701133728
I0229 08:45:20.820487 140407379851008 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.7546162605285645, loss=1.7238107919692993
I0229 08:45:54.494441 140409124681472 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.911301851272583, loss=1.7229442596435547
I0229 08:46:28.207941 140407379851008 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.9540358781814575, loss=1.7406564950942993
I0229 08:47:01.913933 140409124681472 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6835867166519165, loss=1.5980557203292847
I0229 08:47:35.596372 140407379851008 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.8680452108383179, loss=1.6691746711730957
I0229 08:48:09.302844 140409124681472 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6064032316207886, loss=1.6626280546188354
I0229 08:48:37.074450 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:48:45.186139 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:48:59.970200 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:49:02.164957 140573303715648 submission_runner.py:411] Time since start: 12866.16s, 	Step: 36284, 	{'train/accuracy': 0.7382612824440002, 'train/loss': 1.002962589263916, 'validation/accuracy': 0.6293799877166748, 'validation/loss': 1.5438278913497925, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.274627685546875, 'test/num_examples': 10000, 'score': 12293.991918563843, 'total_duration': 12866.157500267029, 'accumulated_submission_time': 12293.991918563843, 'accumulated_eval_time': 570.0424513816833, 'accumulated_logging_time': 0.7297089099884033}
I0229 08:49:02.191159 140407379851008 logging_writer.py:48] [36284] accumulated_eval_time=570.042451, accumulated_logging_time=0.729709, accumulated_submission_time=12293.991919, global_step=36284, preemption_count=0, score=12293.991919, test/accuracy=0.498800, test/loss=2.274628, test/num_examples=10000, total_duration=12866.157500, train/accuracy=0.738261, train/loss=1.002963, validation/accuracy=0.629380, validation/loss=1.543828, validation/num_examples=50000
I0229 08:49:07.918093 140409846093568 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.6691919565200806, loss=1.7307785749435425
I0229 08:49:41.602971 140407379851008 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.143758773803711, loss=1.692929983139038
I0229 08:50:15.300810 140409846093568 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.9637936353683472, loss=1.7366012334823608
I0229 08:50:49.032243 140407379851008 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.6034343242645264, loss=1.627143144607544
I0229 08:51:22.713264 140409846093568 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5797593593597412, loss=1.631489634513855
I0229 08:51:56.501902 140407379851008 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.1118860244750977, loss=1.673707127571106
I0229 08:52:30.258899 140409846093568 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.5988280773162842, loss=1.6916477680206299
I0229 08:53:03.938453 140407379851008 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.9962869882583618, loss=1.7170875072479248
I0229 08:53:37.700519 140409846093568 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7919448614120483, loss=1.614231824874878
I0229 08:54:11.402809 140407379851008 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.8061859607696533, loss=1.645938515663147
I0229 08:54:45.116147 140409846093568 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.9395098686218262, loss=1.6853142976760864
I0229 08:55:18.835953 140407379851008 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.7886955738067627, loss=1.6707717180252075
I0229 08:55:52.548430 140409846093568 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.734034538269043, loss=1.5858514308929443
I0229 08:56:26.296665 140407379851008 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.612946629524231, loss=1.6200735569000244
I0229 08:57:00.054674 140409846093568 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.8580180406570435, loss=1.6257609128952026
I0229 08:57:32.259998 140573303715648 spec.py:321] Evaluating on the training split.
I0229 08:57:39.479357 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 08:57:52.282303 140573303715648 spec.py:349] Evaluating on the test split.
I0229 08:57:54.526516 140573303715648 submission_runner.py:411] Time since start: 13398.52s, 	Step: 37797, 	{'train/accuracy': 0.7053372263908386, 'train/loss': 1.1382724046707153, 'validation/accuracy': 0.6288599967956543, 'validation/loss': 1.5252172946929932, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2593157291412354, 'test/num_examples': 10000, 'score': 12803.992875099182, 'total_duration': 13398.519066810608, 'accumulated_submission_time': 12803.992875099182, 'accumulated_eval_time': 592.3089263439178, 'accumulated_logging_time': 0.7659671306610107}
I0229 08:57:54.547488 140409124681472 logging_writer.py:48] [37797] accumulated_eval_time=592.308926, accumulated_logging_time=0.765967, accumulated_submission_time=12803.992875, global_step=37797, preemption_count=0, score=12803.992875, test/accuracy=0.506500, test/loss=2.259316, test/num_examples=10000, total_duration=13398.519067, train/accuracy=0.705337, train/loss=1.138272, validation/accuracy=0.628860, validation/loss=1.525217, validation/num_examples=50000
I0229 08:57:55.899338 140409829308160 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.8179059028625488, loss=1.9112728834152222
I0229 08:58:29.640380 140409124681472 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.070344924926758, loss=1.7956080436706543
I0229 08:59:03.326693 140409829308160 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.8429800271987915, loss=1.6478102207183838
I0229 08:59:36.993652 140409124681472 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.7240341901779175, loss=1.5686099529266357
I0229 09:00:10.724617 140409829308160 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.647250771522522, loss=1.6442431211471558
I0229 09:00:44.428863 140409124681472 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.8840874433517456, loss=1.7022480964660645
I0229 09:01:18.169326 140409829308160 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.764376163482666, loss=1.783146619796753
I0229 09:01:51.868803 140409124681472 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.7089481353759766, loss=1.6247161626815796
I0229 09:02:25.566811 140409829308160 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8000805377960205, loss=1.6018304824829102
I0229 09:02:59.267454 140409124681472 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.7471225261688232, loss=1.6817808151245117
I0229 09:03:32.985686 140409829308160 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.804356336593628, loss=1.6336642503738403
I0229 09:04:06.722552 140409124681472 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8423793315887451, loss=1.8024585247039795
I0229 09:04:40.368393 140409829308160 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.9688551425933838, loss=1.6918203830718994
I0229 09:05:14.041447 140409124681472 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.1881613731384277, loss=1.7293241024017334
I0229 09:05:47.780665 140409829308160 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.7297868728637695, loss=1.625193476676941
I0229 09:06:21.473621 140409124681472 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.943397879600525, loss=1.671724557876587
I0229 09:06:24.606569 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:06:31.766140 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:06:43.055177 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:06:45.344001 140573303715648 submission_runner.py:411] Time since start: 13929.34s, 	Step: 39311, 	{'train/accuracy': 0.6939771771430969, 'train/loss': 1.196521520614624, 'validation/accuracy': 0.6310999989509583, 'validation/loss': 1.5323960781097412, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.251711368560791, 'test/num_examples': 10000, 'score': 13313.982473373413, 'total_duration': 13929.336554050446, 'accumulated_submission_time': 13313.982473373413, 'accumulated_eval_time': 613.0463311672211, 'accumulated_logging_time': 0.7976090908050537}
I0229 09:06:45.363686 140408319375104 logging_writer.py:48] [39311] accumulated_eval_time=613.046331, accumulated_logging_time=0.797609, accumulated_submission_time=13313.982473, global_step=39311, preemption_count=0, score=13313.982473, test/accuracy=0.508100, test/loss=2.251711, test/num_examples=10000, total_duration=13929.336554, train/accuracy=0.693977, train/loss=1.196522, validation/accuracy=0.631100, validation/loss=1.532396, validation/num_examples=50000
I0229 09:07:15.697286 140409124681472 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.942061424255371, loss=1.6426539421081543
I0229 09:07:49.380752 140408319375104 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.9144282341003418, loss=1.6485133171081543
I0229 09:08:23.082791 140409124681472 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.5714935064315796, loss=1.5133929252624512
I0229 09:08:56.804920 140408319375104 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7427922487258911, loss=1.599937081336975
I0229 09:09:30.516374 140409124681472 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.9300031661987305, loss=1.519089698791504
I0229 09:10:04.270410 140408319375104 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.719508409500122, loss=1.6423499584197998
I0229 09:10:38.058625 140409124681472 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.8131695985794067, loss=1.7175002098083496
I0229 09:11:11.788549 140408319375104 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.0681633949279785, loss=1.6839865446090698
I0229 09:11:45.510795 140409124681472 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.858375906944275, loss=1.6716721057891846
I0229 09:12:19.258529 140408319375104 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.924594521522522, loss=1.6471530199050903
I0229 09:12:52.938828 140409124681472 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.7950359582901, loss=1.6910161972045898
I0229 09:13:26.673218 140408319375104 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.077439546585083, loss=1.6744608879089355
I0229 09:14:00.401949 140409124681472 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.7616276741027832, loss=1.6372689008712769
I0229 09:14:34.093172 140408319375104 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.8157532215118408, loss=1.5979093313217163
I0229 09:15:07.784940 140409124681472 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7028617858886719, loss=1.6189850568771362
I0229 09:15:15.666028 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:15:22.660967 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:15:31.713901 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:15:33.981704 140573303715648 submission_runner.py:411] Time since start: 14457.97s, 	Step: 40825, 	{'train/accuracy': 0.7027463316917419, 'train/loss': 1.1749341487884521, 'validation/accuracy': 0.6353999972343445, 'validation/loss': 1.4998513460159302, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.224247932434082, 'test/num_examples': 10000, 'score': 13824.217654943466, 'total_duration': 14457.974243879318, 'accumulated_submission_time': 13824.217654943466, 'accumulated_eval_time': 631.3619570732117, 'accumulated_logging_time': 0.8261539936065674}
I0229 09:15:34.005850 140409854486272 logging_writer.py:48] [40825] accumulated_eval_time=631.361957, accumulated_logging_time=0.826154, accumulated_submission_time=13824.217655, global_step=40825, preemption_count=0, score=13824.217655, test/accuracy=0.507400, test/loss=2.224248, test/num_examples=10000, total_duration=14457.974244, train/accuracy=0.702746, train/loss=1.174934, validation/accuracy=0.635400, validation/loss=1.499851, validation/num_examples=50000
I0229 09:15:59.679482 140409862878976 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.9177234172821045, loss=1.6531023979187012
I0229 09:16:33.331149 140409854486272 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.863673210144043, loss=1.624112844467163
I0229 09:17:07.030390 140409862878976 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7289406061172485, loss=1.6746797561645508
I0229 09:17:40.712504 140409854486272 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5950231552124023, loss=1.5167324542999268
I0229 09:18:14.373859 140409862878976 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.7819275856018066, loss=1.6110038757324219
I0229 09:18:48.095976 140409854486272 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.059659242630005, loss=1.6234687566757202
I0229 09:19:21.775920 140409862878976 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.017038583755493, loss=1.710228681564331
I0229 09:19:55.493955 140409854486272 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.7450282573699951, loss=1.6076105833053589
I0229 09:20:29.129755 140409862878976 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.7843751907348633, loss=1.7492259740829468
I0229 09:21:02.835092 140409854486272 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.7597696781158447, loss=1.6286288499832153
I0229 09:21:36.470327 140409862878976 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8270281553268433, loss=1.598585605621338
I0229 09:22:10.239923 140409854486272 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9678685665130615, loss=1.7088849544525146
I0229 09:22:43.957648 140409862878976 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.724977731704712, loss=1.6770004034042358
I0229 09:23:17.696181 140409854486272 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.9026892185211182, loss=1.6091053485870361
I0229 09:23:51.395113 140409862878976 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.7655713558197021, loss=1.5867363214492798
I0229 09:24:03.998460 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:24:10.975219 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:24:20.728385 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:24:22.998162 140573303715648 submission_runner.py:411] Time since start: 14986.99s, 	Step: 42339, 	{'train/accuracy': 0.6916852593421936, 'train/loss': 1.219604253768921, 'validation/accuracy': 0.6343799829483032, 'validation/loss': 1.5244389772415161, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.296604633331299, 'test/num_examples': 10000, 'score': 14334.139183282852, 'total_duration': 14986.990688800812, 'accumulated_submission_time': 14334.139183282852, 'accumulated_eval_time': 650.3616156578064, 'accumulated_logging_time': 0.861600399017334}
I0229 09:24:23.022190 140408319375104 logging_writer.py:48] [42339] accumulated_eval_time=650.361616, accumulated_logging_time=0.861600, accumulated_submission_time=14334.139183, global_step=42339, preemption_count=0, score=14334.139183, test/accuracy=0.503700, test/loss=2.296605, test/num_examples=10000, total_duration=14986.990689, train/accuracy=0.691685, train/loss=1.219604, validation/accuracy=0.634380, validation/loss=1.524439, validation/num_examples=50000
I0229 09:24:43.924468 140409124681472 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.8022246360778809, loss=1.7103976011276245
I0229 09:25:17.588981 140408319375104 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.802714228630066, loss=1.5993815660476685
I0229 09:25:51.230411 140409124681472 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.711586833000183, loss=1.6665105819702148
I0229 09:26:25.001006 140408319375104 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.8981659412384033, loss=1.6495797634124756
I0229 09:26:58.699398 140409124681472 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.7064650058746338, loss=1.6857991218566895
I0229 09:27:32.452917 140408319375104 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.8572551012039185, loss=1.6909379959106445
I0229 09:28:06.203462 140409124681472 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8375357389450073, loss=1.662766456604004
I0229 09:28:39.853315 140408319375104 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.869429349899292, loss=1.638505220413208
I0229 09:29:13.598805 140409124681472 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7564642429351807, loss=1.7111684083938599
I0229 09:29:47.277723 140408319375104 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.8458353281021118, loss=1.5270709991455078
I0229 09:30:21.017081 140409124681472 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.9415607452392578, loss=1.7493443489074707
I0229 09:30:54.723729 140408319375104 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.712491750717163, loss=1.502812147140503
I0229 09:31:28.450703 140409124681472 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.8088252544403076, loss=1.6562848091125488
I0229 09:32:02.188072 140408319375104 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.7971184253692627, loss=1.5912190675735474
I0229 09:32:35.915761 140409124681472 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.105405330657959, loss=1.6439745426177979
I0229 09:32:53.265509 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:33:00.168345 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:33:11.958823 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:33:14.254229 140573303715648 submission_runner.py:411] Time since start: 15518.25s, 	Step: 43853, 	{'train/accuracy': 0.687898576259613, 'train/loss': 1.2292335033416748, 'validation/accuracy': 0.631060004234314, 'validation/loss': 1.5233080387115479, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.2972493171691895, 'test/num_examples': 10000, 'score': 14844.313416957855, 'total_duration': 15518.24677681923, 'accumulated_submission_time': 14844.313416957855, 'accumulated_eval_time': 671.3503079414368, 'accumulated_logging_time': 0.8955569267272949}
I0229 09:33:14.275369 140407379851008 logging_writer.py:48] [43853] accumulated_eval_time=671.350308, accumulated_logging_time=0.895557, accumulated_submission_time=14844.313417, global_step=43853, preemption_count=0, score=14844.313417, test/accuracy=0.499300, test/loss=2.297249, test/num_examples=10000, total_duration=15518.246777, train/accuracy=0.687899, train/loss=1.229234, validation/accuracy=0.631060, validation/loss=1.523308, validation/num_examples=50000
I0229 09:33:30.444091 140408319375104 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.6633942127227783, loss=1.561537265777588
I0229 09:34:04.115161 140407379851008 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.9396731853485107, loss=1.667402744293213
I0229 09:34:37.843612 140408319375104 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.9820274114608765, loss=1.6353455781936646
I0229 09:35:11.549423 140407379851008 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7118372917175293, loss=1.4095033407211304
I0229 09:35:45.199725 140408319375104 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.7645736932754517, loss=1.531205177307129
I0229 09:36:18.856683 140407379851008 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.909801721572876, loss=1.6016273498535156
I0229 09:36:52.592007 140408319375104 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.9698117971420288, loss=1.6493666172027588
I0229 09:37:26.269151 140407379851008 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7261196374893188, loss=1.5401231050491333
I0229 09:37:59.944242 140408319375104 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.7943130731582642, loss=1.5260454416275024
I0229 09:38:33.703261 140407379851008 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.9496692419052124, loss=1.6745375394821167
I0229 09:39:07.382782 140408319375104 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.6781744956970215, loss=1.714818000793457
I0229 09:39:41.126691 140407379851008 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.9524102210998535, loss=1.7066850662231445
I0229 09:40:14.772031 140408319375104 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.7499080896377563, loss=1.6103867292404175
I0229 09:40:48.585611 140407379851008 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.6589776277542114, loss=1.525188684463501
I0229 09:41:22.297346 140408319375104 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8137924671173096, loss=1.686050295829773
I0229 09:41:44.336652 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:41:50.986561 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:42:01.148458 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:42:03.576519 140573303715648 submission_runner.py:411] Time since start: 16047.57s, 	Step: 45367, 	{'train/accuracy': 0.7377431392669678, 'train/loss': 0.9952830076217651, 'validation/accuracy': 0.6425999999046326, 'validation/loss': 1.4854003190994263, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.2528679370880127, 'test/num_examples': 10000, 'score': 15354.307039022446, 'total_duration': 16047.569056749344, 'accumulated_submission_time': 15354.307039022446, 'accumulated_eval_time': 690.5901215076447, 'accumulated_logging_time': 0.9257237911224365}
I0229 09:42:03.602286 140409837700864 logging_writer.py:48] [45367] accumulated_eval_time=690.590122, accumulated_logging_time=0.925724, accumulated_submission_time=15354.307039, global_step=45367, preemption_count=0, score=15354.307039, test/accuracy=0.510300, test/loss=2.252868, test/num_examples=10000, total_duration=16047.569057, train/accuracy=0.737743, train/loss=0.995283, validation/accuracy=0.642600, validation/loss=1.485400, validation/num_examples=50000
I0229 09:42:15.085603 140409846093568 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.8072822093963623, loss=1.7647308111190796
I0229 09:42:48.743367 140409837700864 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7376457452774048, loss=1.575266718864441
I0229 09:43:22.389813 140409846093568 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.0929367542266846, loss=1.708479404449463
I0229 09:43:56.111867 140409837700864 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.710851788520813, loss=1.5616333484649658
I0229 09:44:29.802998 140409846093568 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.8412096500396729, loss=1.6479698419570923
I0229 09:45:03.540950 140409837700864 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.8414033651351929, loss=1.557794213294983
I0229 09:45:37.217792 140409846093568 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7188844680786133, loss=1.509075403213501
I0229 09:46:10.949942 140409837700864 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.9176855087280273, loss=1.531620979309082
I0229 09:46:44.824477 140409846093568 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7646647691726685, loss=1.659620761871338
I0229 09:47:18.548007 140409837700864 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.8261204957962036, loss=1.644042730331421
I0229 09:47:52.271689 140409846093568 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.8860267400741577, loss=1.56425142288208
I0229 09:48:25.966232 140409837700864 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.7911955118179321, loss=1.5230337381362915
I0229 09:48:59.592379 140409846093568 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7643587589263916, loss=1.55460786819458
I0229 09:49:33.312760 140409837700864 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.224817991256714, loss=1.7174025774002075
I0229 09:50:06.985300 140409846093568 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.0279479026794434, loss=1.649505615234375
I0229 09:50:33.709809 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:50:40.283473 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:50:49.201503 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:50:51.439050 140573303715648 submission_runner.py:411] Time since start: 16575.43s, 	Step: 46881, 	{'train/accuracy': 0.7153220772743225, 'train/loss': 1.084122657775879, 'validation/accuracy': 0.6393199563026428, 'validation/loss': 1.4918371438980103, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.251056432723999, 'test/num_examples': 10000, 'score': 15864.34359741211, 'total_duration': 16575.43159174919, 'accumulated_submission_time': 15864.34359741211, 'accumulated_eval_time': 708.3193356990814, 'accumulated_logging_time': 0.9613451957702637}
I0229 09:50:51.465440 140407379851008 logging_writer.py:48] [46881] accumulated_eval_time=708.319336, accumulated_logging_time=0.961345, accumulated_submission_time=15864.343597, global_step=46881, preemption_count=0, score=15864.343597, test/accuracy=0.513800, test/loss=2.251056, test/num_examples=10000, total_duration=16575.431592, train/accuracy=0.715322, train/loss=1.084123, validation/accuracy=0.639320, validation/loss=1.491837, validation/num_examples=50000
I0229 09:50:58.228994 140409829308160 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.037182092666626, loss=1.6744970083236694
I0229 09:51:31.897516 140407379851008 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.8395898342132568, loss=1.6086125373840332
I0229 09:52:05.605022 140409829308160 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.695663571357727, loss=1.6406491994857788
I0229 09:52:39.413382 140407379851008 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.7446284294128418, loss=1.5683979988098145
I0229 09:53:13.160756 140409829308160 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7831733226776123, loss=1.5963304042816162
I0229 09:53:46.832947 140407379851008 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7720286846160889, loss=1.6349987983703613
I0229 09:54:20.571890 140409829308160 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7304707765579224, loss=1.615380048751831
I0229 09:54:54.208309 140407379851008 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.896349549293518, loss=1.7117263078689575
I0229 09:55:27.932888 140409829308160 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7215266227722168, loss=1.6312942504882812
I0229 09:56:01.617381 140407379851008 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.918033242225647, loss=1.735322117805481
I0229 09:56:35.292296 140409829308160 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7437007427215576, loss=1.611264705657959
I0229 09:57:08.991858 140407379851008 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.9097031354904175, loss=1.5427194833755493
I0229 09:57:42.703800 140409829308160 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.7729822397232056, loss=1.6761010885238647
I0229 09:58:16.428968 140407379851008 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7849708795547485, loss=1.6311111450195312
I0229 09:58:50.179473 140409829308160 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.3301403522491455, loss=1.7608222961425781
I0229 09:59:21.683074 140573303715648 spec.py:321] Evaluating on the training split.
I0229 09:59:28.303075 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 09:59:37.153921 140573303715648 spec.py:349] Evaluating on the test split.
I0229 09:59:39.461155 140573303715648 submission_runner.py:411] Time since start: 17103.45s, 	Step: 48395, 	{'train/accuracy': 0.702168345451355, 'train/loss': 1.1595250368118286, 'validation/accuracy': 0.6326000094413757, 'validation/loss': 1.5194488763809204, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.254747152328491, 'test/num_examples': 10000, 'score': 16374.491770744324, 'total_duration': 17103.453681230545, 'accumulated_submission_time': 16374.491770744324, 'accumulated_eval_time': 726.0973536968231, 'accumulated_logging_time': 0.9971561431884766}
I0229 09:59:39.487013 140408319375104 logging_writer.py:48] [48395] accumulated_eval_time=726.097354, accumulated_logging_time=0.997156, accumulated_submission_time=16374.491771, global_step=48395, preemption_count=0, score=16374.491771, test/accuracy=0.508200, test/loss=2.254747, test/num_examples=10000, total_duration=17103.453681, train/accuracy=0.702168, train/loss=1.159525, validation/accuracy=0.632600, validation/loss=1.519449, validation/num_examples=50000
I0229 09:59:41.522197 140409124681472 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.8506495952606201, loss=1.5040851831436157
I0229 10:00:15.170710 140408319375104 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.8422579765319824, loss=1.6092228889465332
I0229 10:00:48.924911 140409124681472 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.7543660402297974, loss=1.472425937652588
I0229 10:01:22.649795 140408319375104 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.92231023311615, loss=1.6901185512542725
I0229 10:01:56.359752 140409124681472 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.7436087131500244, loss=1.566951036453247
I0229 10:02:30.081771 140408319375104 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.8315523862838745, loss=1.5452057123184204
I0229 10:03:03.759948 140409124681472 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.9109776020050049, loss=1.6367509365081787
I0229 10:03:37.465448 140408319375104 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.857712745666504, loss=1.6766221523284912
I0229 10:04:11.215125 140409124681472 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.8610550165176392, loss=1.607577919960022
I0229 10:04:44.877744 140408319375104 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.6310250759124756, loss=1.5526202917099
I0229 10:05:18.803723 140409124681472 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.898464560508728, loss=1.631534457206726
I0229 10:05:52.490062 140408319375104 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8676272630691528, loss=1.604034423828125
I0229 10:06:26.234130 140409124681472 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.090120315551758, loss=1.628967523574829
I0229 10:06:59.965395 140408319375104 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.8936996459960938, loss=1.695996642112732
I0229 10:07:33.647337 140409124681472 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.9307303428649902, loss=1.5960708856582642
I0229 10:08:07.416990 140408319375104 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9561877250671387, loss=1.6136640310287476
I0229 10:08:09.572518 140573303715648 spec.py:321] Evaluating on the training split.
I0229 10:08:16.178744 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 10:08:26.188211 140573303715648 spec.py:349] Evaluating on the test split.
I0229 10:08:28.469560 140573303715648 submission_runner.py:411] Time since start: 17632.46s, 	Step: 49908, 	{'train/accuracy': 0.7033841013908386, 'train/loss': 1.1538665294647217, 'validation/accuracy': 0.6426599621772766, 'validation/loss': 1.476432204246521, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.219771146774292, 'test/num_examples': 10000, 'score': 16884.507195949554, 'total_duration': 17632.462087869644, 'accumulated_submission_time': 16884.507195949554, 'accumulated_eval_time': 744.994348526001, 'accumulated_logging_time': 1.0333445072174072}
I0229 10:08:28.494697 140408319375104 logging_writer.py:48] [49908] accumulated_eval_time=744.994349, accumulated_logging_time=1.033345, accumulated_submission_time=16884.507196, global_step=49908, preemption_count=0, score=16884.507196, test/accuracy=0.514900, test/loss=2.219771, test/num_examples=10000, total_duration=17632.462088, train/accuracy=0.703384, train/loss=1.153867, validation/accuracy=0.642660, validation/loss=1.476432, validation/num_examples=50000
I0229 10:08:59.859511 140409124681472 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.8201147317886353, loss=1.6326485872268677
I0229 10:09:33.513026 140408319375104 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.814415693283081, loss=1.5122904777526855
I0229 10:10:07.166137 140409124681472 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.0321555137634277, loss=1.639634132385254
I0229 10:10:40.859505 140408319375104 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7593562602996826, loss=1.5039420127868652
I0229 10:11:14.795871 140409124681472 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.839995265007019, loss=1.6546554565429688
I0229 10:11:48.476214 140408319375104 logging_writer.py:48] [50500] global_step=50500, grad_norm=2.063157081604004, loss=1.6360194683074951
I0229 10:12:22.232512 140409124681472 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.151749849319458, loss=1.562536358833313
I0229 10:12:55.908152 140408319375104 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.7782058715820312, loss=1.6293504238128662
I0229 10:13:29.647325 140409124681472 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8737605810165405, loss=1.6626002788543701
I0229 10:14:03.315060 140408319375104 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8128392696380615, loss=1.6836706399917603
I0229 10:14:36.996747 140409124681472 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.9382845163345337, loss=1.545161485671997
I0229 10:15:10.726841 140408319375104 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7853474617004395, loss=1.6044509410858154
I0229 10:15:44.425034 140409124681472 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.8647053241729736, loss=1.6091924905776978
I0229 10:16:18.124246 140408319375104 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9610220193862915, loss=1.522568702697754
I0229 10:16:51.885180 140409124681472 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.967115879058838, loss=1.5501985549926758
I0229 10:16:58.759142 140573303715648 spec.py:321] Evaluating on the training split.
I0229 10:17:05.352052 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 10:17:14.563376 140573303715648 spec.py:349] Evaluating on the test split.
I0229 10:17:17.133138 140573303715648 submission_runner.py:411] Time since start: 18161.13s, 	Step: 51422, 	{'train/accuracy': 0.7069913744926453, 'train/loss': 1.1432219743728638, 'validation/accuracy': 0.6446399688720703, 'validation/loss': 1.4551054239273071, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2311418056488037, 'test/num_examples': 10000, 'score': 17394.70106601715, 'total_duration': 18161.125678539276, 'accumulated_submission_time': 17394.70106601715, 'accumulated_eval_time': 763.3682973384857, 'accumulated_logging_time': 1.068270206451416}
I0229 10:17:17.158630 140407379851008 logging_writer.py:48] [51422] accumulated_eval_time=763.368297, accumulated_logging_time=1.068270, accumulated_submission_time=17394.701066, global_step=51422, preemption_count=0, score=17394.701066, test/accuracy=0.509100, test/loss=2.231142, test/num_examples=10000, total_duration=18161.125679, train/accuracy=0.706991, train/loss=1.143222, validation/accuracy=0.644640, validation/loss=1.455105, validation/num_examples=50000
I0229 10:17:43.792953 140409829308160 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.7840298414230347, loss=1.6916377544403076
I0229 10:18:17.496005 140407379851008 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.7471213340759277, loss=1.5196642875671387
I0229 10:18:51.185547 140409829308160 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.7848509550094604, loss=1.5665018558502197
I0229 10:19:24.904042 140407379851008 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.9750460386276245, loss=1.692232608795166
I0229 10:19:58.646786 140409829308160 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.2065179347991943, loss=1.627675175666809
I0229 10:20:32.375848 140407379851008 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.9777554273605347, loss=1.5885341167449951
I0229 10:21:06.079287 140409829308160 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8210563659667969, loss=1.5662883520126343
I0229 10:21:39.722018 140407379851008 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.997907280921936, loss=1.6762123107910156
I0229 10:22:13.480260 140409829308160 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.059115409851074, loss=1.6518795490264893
I0229 10:22:47.198268 140407379851008 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.805976390838623, loss=1.515425205230713
I0229 10:23:21.000650 140409829308160 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8441205024719238, loss=1.5435043573379517
I0229 10:23:54.739284 140407379851008 logging_writer.py:48] [52600] global_step=52600, grad_norm=2.001148223876953, loss=1.6351871490478516
I0229 10:24:28.505873 140409829308160 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.9395318031311035, loss=1.6101597547531128
I0229 10:25:02.211224 140407379851008 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.6542081832885742, loss=1.7586071491241455
I0229 10:25:35.951699 140409829308160 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.6918545961380005, loss=1.5049153566360474
I0229 10:25:47.195272 140573303715648 spec.py:321] Evaluating on the training split.
I0229 10:25:53.728042 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 10:26:02.745067 140573303715648 spec.py:349] Evaluating on the test split.
I0229 10:26:05.039493 140573303715648 submission_runner.py:411] Time since start: 18689.03s, 	Step: 52935, 	{'train/accuracy': 0.696707546710968, 'train/loss': 1.1853846311569214, 'validation/accuracy': 0.6376000046730042, 'validation/loss': 1.4835251569747925, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.219569206237793, 'test/num_examples': 10000, 'score': 17904.66800928116, 'total_duration': 18689.03200531006, 'accumulated_submission_time': 17904.66800928116, 'accumulated_eval_time': 781.212443113327, 'accumulated_logging_time': 1.1037437915802002}
I0229 10:26:05.067678 140408319375104 logging_writer.py:48] [52935] accumulated_eval_time=781.212443, accumulated_logging_time=1.103744, accumulated_submission_time=17904.668009, global_step=52935, preemption_count=0, score=17904.668009, test/accuracy=0.514800, test/loss=2.219569, test/num_examples=10000, total_duration=18689.032005, train/accuracy=0.696708, train/loss=1.185385, validation/accuracy=0.637600, validation/loss=1.483525, validation/num_examples=50000
I0229 10:26:27.324084 140409124681472 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.397395610809326, loss=1.6765339374542236
I0229 10:27:01.034237 140408319375104 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.0380313396453857, loss=1.6164019107818604
I0229 10:27:34.688901 140409124681472 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.0173609256744385, loss=1.5222021341323853
I0229 10:28:08.416106 140408319375104 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.0807430744171143, loss=1.6256320476531982
I0229 10:28:42.054176 140409124681472 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.9034249782562256, loss=1.6651053428649902
I0229 10:29:15.706514 140408319375104 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.188624143600464, loss=1.6017720699310303
I0229 10:29:49.450416 140409124681472 logging_writer.py:48] [53600] global_step=53600, grad_norm=2.143773317337036, loss=1.6527607440948486
I0229 10:30:23.136658 140408319375104 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.8147242069244385, loss=1.5725499391555786
I0229 10:30:56.866427 140409124681472 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.9230226278305054, loss=1.5876489877700806
I0229 10:31:30.558360 140408319375104 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.702444314956665, loss=1.4653501510620117
I0229 10:32:04.286748 140409124681472 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.7323462963104248, loss=1.53525710105896
I0229 10:32:37.974921 140408319375104 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.0315074920654297, loss=1.644892692565918
I0229 10:33:11.695920 140409124681472 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.7012182474136353, loss=1.551131248474121
I0229 10:33:45.425594 140408319375104 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.76810622215271, loss=1.5958826541900635
I0229 10:34:19.173878 140409124681472 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8237030506134033, loss=1.6355129480361938
I0229 10:34:35.158292 140573303715648 spec.py:321] Evaluating on the training split.
I0229 10:34:41.560844 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 10:34:50.515961 140573303715648 spec.py:349] Evaluating on the test split.
I0229 10:34:52.784277 140573303715648 submission_runner.py:411] Time since start: 19216.78s, 	Step: 54449, 	{'train/accuracy': 0.7293726205825806, 'train/loss': 1.0337110757827759, 'validation/accuracy': 0.6370999813079834, 'validation/loss': 1.4978829622268677, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2475028038024902, 'test/num_examples': 10000, 'score': 18414.68908238411, 'total_duration': 19216.77681660652, 'accumulated_submission_time': 18414.68908238411, 'accumulated_eval_time': 798.8383796215057, 'accumulated_logging_time': 1.1416583061218262}
I0229 10:34:52.810474 140409837700864 logging_writer.py:48] [54449] accumulated_eval_time=798.838380, accumulated_logging_time=1.141658, accumulated_submission_time=18414.689082, global_step=54449, preemption_count=0, score=18414.689082, test/accuracy=0.504500, test/loss=2.247503, test/num_examples=10000, total_duration=19216.776817, train/accuracy=0.729373, train/loss=1.033711, validation/accuracy=0.637100, validation/loss=1.497883, validation/num_examples=50000
I0229 10:35:10.309298 140409846093568 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9301918745040894, loss=1.508366584777832
I0229 10:35:44.171938 140409837700864 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.8912125825881958, loss=1.5249683856964111
I0229 10:36:17.880280 140409846093568 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.9124497175216675, loss=1.5557861328125
I0229 10:36:51.641296 140409837700864 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.6912790536880493, loss=1.6419204473495483
I0229 10:37:25.331867 140409846093568 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9646642208099365, loss=1.7522244453430176
I0229 10:37:59.103235 140409837700864 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.9364140033721924, loss=1.6629493236541748
I0229 10:38:32.833450 140409846093568 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.798431158065796, loss=1.5665571689605713
I0229 10:39:06.532515 140409837700864 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.8926408290863037, loss=1.5468404293060303
I0229 10:39:40.273236 140409846093568 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.9322115182876587, loss=1.6092860698699951
I0229 10:40:13.966360 140409837700864 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8774181604385376, loss=1.5386064052581787
I0229 10:40:47.715230 140409846093568 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.6320914030075073, loss=1.4700829982757568
I0229 10:41:21.377511 140409837700864 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.963903784751892, loss=1.6175180673599243
I0229 10:41:55.264649 140409846093568 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.813708782196045, loss=1.5016666650772095
I0229 10:42:28.916939 140409837700864 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.8581751585006714, loss=1.6466268301010132
I0229 10:43:02.661083 140409846093568 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.0356881618499756, loss=1.6386635303497314
I0229 10:43:23.018689 140573303715648 spec.py:321] Evaluating on the training split.
I0229 10:43:29.397100 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 10:43:38.252531 140573303715648 spec.py:349] Evaluating on the test split.
I0229 10:43:40.552991 140573303715648 submission_runner.py:411] Time since start: 19744.55s, 	Step: 55962, 	{'train/accuracy': 0.7280173897743225, 'train/loss': 1.0381181240081787, 'validation/accuracy': 0.6498199701309204, 'validation/loss': 1.4298878908157349, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.128363609313965, 'test/num_examples': 10000, 'score': 18924.828382968903, 'total_duration': 19744.545511245728, 'accumulated_submission_time': 18924.828382968903, 'accumulated_eval_time': 816.3726131916046, 'accumulated_logging_time': 1.1773545742034912}
I0229 10:43:40.590355 140409124681472 logging_writer.py:48] [55962] accumulated_eval_time=816.372613, accumulated_logging_time=1.177355, accumulated_submission_time=18924.828383, global_step=55962, preemption_count=0, score=18924.828383, test/accuracy=0.526300, test/loss=2.128364, test/num_examples=10000, total_duration=19744.545511, train/accuracy=0.728017, train/loss=1.038118, validation/accuracy=0.649820, validation/loss=1.429888, validation/num_examples=50000
I0229 10:43:53.754885 140409829308160 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.974029302597046, loss=1.605262279510498
I0229 10:44:27.419498 140409124681472 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8945374488830566, loss=1.65800940990448
I0229 10:45:01.084034 140409829308160 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.0858395099639893, loss=1.5283845663070679
I0229 10:45:34.825756 140409124681472 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.7142599821090698, loss=1.4118674993515015
I0229 10:46:08.499294 140409829308160 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.778964638710022, loss=1.4946123361587524
I0229 10:46:42.230130 140409124681472 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.9349716901779175, loss=1.5058842897415161
I0229 10:47:15.913869 140409829308160 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.4128434658050537, loss=1.6261916160583496
I0229 10:47:49.728106 140409124681472 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.6927939653396606, loss=1.5223006010055542
I0229 10:48:23.451076 140409829308160 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.6597037315368652, loss=1.5123963356018066
I0229 10:48:57.103260 140409124681472 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8585466146469116, loss=1.5240386724472046
I0229 10:49:30.790274 140409829308160 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8597873449325562, loss=1.6233092546463013
I0229 10:50:04.440974 140409124681472 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8428436517715454, loss=1.5527515411376953
I0229 10:50:38.135960 140409829308160 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.9746843576431274, loss=1.4678698778152466
I0229 10:51:11.857210 140409124681472 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.797572135925293, loss=1.5960726737976074
I0229 10:51:45.532203 140409829308160 logging_writer.py:48] [57400] global_step=57400, grad_norm=2.0780797004699707, loss=1.635264277458191
I0229 10:52:10.640393 140573303715648 spec.py:321] Evaluating on the training split.
I0229 10:52:17.060260 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 10:52:26.991647 140573303715648 spec.py:349] Evaluating on the test split.
I0229 10:52:29.260953 140573303715648 submission_runner.py:411] Time since start: 20273.25s, 	Step: 57476, 	{'train/accuracy': 0.7173150181770325, 'train/loss': 1.0886608362197876, 'validation/accuracy': 0.6471399664878845, 'validation/loss': 1.4522713422775269, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.188610553741455, 'test/num_examples': 10000, 'score': 19434.806651115417, 'total_duration': 20273.25348353386, 'accumulated_submission_time': 19434.806651115417, 'accumulated_eval_time': 834.9931175708771, 'accumulated_logging_time': 1.2265896797180176}
I0229 10:52:29.288154 140409846093568 logging_writer.py:48] [57476] accumulated_eval_time=834.993118, accumulated_logging_time=1.226590, accumulated_submission_time=19434.806651, global_step=57476, preemption_count=0, score=19434.806651, test/accuracy=0.520900, test/loss=2.188611, test/num_examples=10000, total_duration=20273.253484, train/accuracy=0.717315, train/loss=1.088661, validation/accuracy=0.647140, validation/loss=1.452271, validation/num_examples=50000
I0229 10:52:37.707824 140409854486272 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.6916354894638062, loss=1.5354998111724854
I0229 10:53:11.389033 140409846093568 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.7749472856521606, loss=1.6360111236572266
I0229 10:53:45.073730 140409854486272 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9226034879684448, loss=1.618754506111145
I0229 10:54:18.944122 140409846093568 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.7161580324172974, loss=1.4681065082550049
I0229 10:54:52.668971 140409854486272 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.80784010887146, loss=1.5779502391815186
I0229 10:55:26.401570 140409846093568 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.9195730686187744, loss=1.5595474243164062
I0229 10:56:00.098749 140409854486272 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.7670247554779053, loss=1.4919073581695557
I0229 10:56:33.823003 140409846093568 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.8844939470291138, loss=1.528015375137329
I0229 10:57:07.489666 140409854486272 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.753220558166504, loss=1.5911833047866821
I0229 10:57:41.213908 140409846093568 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.87578547000885, loss=1.5912519693374634
I0229 10:58:14.907202 140409854486272 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8567898273468018, loss=1.4582463502883911
I0229 10:58:48.633100 140409846093568 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.8986828327178955, loss=1.458626627922058
I0229 10:59:22.330219 140409854486272 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.7998582124710083, loss=1.5445618629455566
I0229 10:59:56.053663 140409846093568 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.117589235305786, loss=1.5597020387649536
I0229 11:00:29.867691 140409854486272 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.9422856569290161, loss=1.4097217321395874
I0229 11:00:59.347111 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:01:05.781491 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:01:18.326087 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:01:20.605664 140573303715648 submission_runner.py:411] Time since start: 20804.60s, 	Step: 58989, 	{'train/accuracy': 0.7143853306770325, 'train/loss': 1.1113851070404053, 'validation/accuracy': 0.6462999582290649, 'validation/loss': 1.4568002223968506, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.1574792861938477, 'test/num_examples': 10000, 'score': 19944.797166585922, 'total_duration': 20804.59820485115, 'accumulated_submission_time': 19944.797166585922, 'accumulated_eval_time': 856.2516252994537, 'accumulated_logging_time': 1.2634735107421875}
I0229 11:01:20.627826 140408319375104 logging_writer.py:48] [58989] accumulated_eval_time=856.251625, accumulated_logging_time=1.263474, accumulated_submission_time=19944.797167, global_step=58989, preemption_count=0, score=19944.797167, test/accuracy=0.526900, test/loss=2.157479, test/num_examples=10000, total_duration=20804.598205, train/accuracy=0.714385, train/loss=1.111385, validation/accuracy=0.646300, validation/loss=1.456800, validation/num_examples=50000
I0229 11:01:24.659979 140409124681472 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.994860053062439, loss=1.6293704509735107
I0229 11:01:58.217600 140408319375104 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.8231422901153564, loss=1.5826096534729004
I0229 11:02:31.877671 140409124681472 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9371215105056763, loss=1.4896657466888428
I0229 11:03:05.581384 140408319375104 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8006244897842407, loss=1.5888465642929077
I0229 11:03:39.291897 140409124681472 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.8061665296554565, loss=1.5518531799316406
I0229 11:04:12.967706 140408319375104 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.0002872943878174, loss=1.6716357469558716
I0229 11:04:46.720261 140409124681472 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.8181602954864502, loss=1.5446321964263916
I0229 11:05:20.393591 140408319375104 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9087605476379395, loss=1.5735905170440674
I0229 11:05:54.135901 140409124681472 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.973836064338684, loss=1.6422415971755981
I0229 11:06:27.880722 140408319375104 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.9143000841140747, loss=1.650569200515747
I0229 11:07:01.549568 140409124681472 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.8369736671447754, loss=1.5498043298721313
I0229 11:07:35.251527 140408319375104 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.939173698425293, loss=1.5694987773895264
I0229 11:08:09.017562 140409124681472 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.1191964149475098, loss=1.6406265497207642
I0229 11:08:42.679624 140408319375104 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.8270149230957031, loss=1.5825564861297607
I0229 11:09:16.343536 140409124681472 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8252942562103271, loss=1.4272356033325195
I0229 11:09:50.046938 140408319375104 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.1781225204467773, loss=1.5367099046707153
I0229 11:09:50.870110 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:09:57.084287 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:10:06.167048 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:10:08.462833 140573303715648 submission_runner.py:411] Time since start: 21332.46s, 	Step: 60504, 	{'train/accuracy': 0.7247688174247742, 'train/loss': 1.0654748678207397, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.3966306447982788, 'validation/num_examples': 50000, 'test/accuracy': 0.5327000021934509, 'test/loss': 2.1029765605926514, 'test/num_examples': 10000, 'score': 20454.971937417984, 'total_duration': 21332.455352783203, 'accumulated_submission_time': 20454.971937417984, 'accumulated_eval_time': 873.8442769050598, 'accumulated_logging_time': 1.2946317195892334}
I0229 11:10:08.492992 140409854486272 logging_writer.py:48] [60504] accumulated_eval_time=873.844277, accumulated_logging_time=1.294632, accumulated_submission_time=20454.971937, global_step=60504, preemption_count=0, score=20454.971937, test/accuracy=0.532700, test/loss=2.102977, test/num_examples=10000, total_duration=21332.455353, train/accuracy=0.724769, train/loss=1.065475, validation/accuracy=0.657780, validation/loss=1.396631, validation/num_examples=50000
I0229 11:10:41.231679 140409862878976 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.7418793439865112, loss=1.4766576290130615
I0229 11:11:14.888847 140409854486272 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9755032062530518, loss=1.6365385055541992
I0229 11:11:48.628482 140409862878976 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.267378330230713, loss=1.5185363292694092
I0229 11:12:22.325427 140409854486272 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.0249602794647217, loss=1.7033816576004028
I0229 11:12:56.089721 140409862878976 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8628480434417725, loss=1.5129388570785522
I0229 11:13:29.823734 140409854486272 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.0058748722076416, loss=1.6733452081680298
I0229 11:14:03.536585 140409862878976 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.866897463798523, loss=1.5285059213638306
I0229 11:14:37.283487 140409854486272 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.002896547317505, loss=1.5629374980926514
I0229 11:15:10.989932 140409862878976 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.9128161668777466, loss=1.5297051668167114
I0229 11:15:44.704983 140409854486272 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8312835693359375, loss=1.4794422388076782
I0229 11:16:18.385269 140409862878976 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.8678475618362427, loss=1.472144365310669
I0229 11:16:52.081519 140409854486272 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.017871618270874, loss=1.531372308731079
I0229 11:17:25.758393 140409862878976 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.120476245880127, loss=1.5497722625732422
I0229 11:17:59.507481 140409854486272 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.976605772972107, loss=1.598473072052002
I0229 11:18:33.257638 140409862878976 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.8487706184387207, loss=1.6407525539398193
I0229 11:18:38.471549 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:18:44.730017 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:18:53.763662 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:18:56.034506 140573303715648 submission_runner.py:411] Time since start: 21860.03s, 	Step: 62017, 	{'train/accuracy': 0.7164580821990967, 'train/loss': 1.0905795097351074, 'validation/accuracy': 0.6516599655151367, 'validation/loss': 1.424889326095581, 'validation/num_examples': 50000, 'test/accuracy': 0.5214000344276428, 'test/loss': 2.1913771629333496, 'test/num_examples': 10000, 'score': 20964.876480817795, 'total_duration': 21860.027040958405, 'accumulated_submission_time': 20964.876480817795, 'accumulated_eval_time': 891.4071831703186, 'accumulated_logging_time': 1.3386225700378418}
I0229 11:18:56.066192 140407379851008 logging_writer.py:48] [62017] accumulated_eval_time=891.407183, accumulated_logging_time=1.338623, accumulated_submission_time=20964.876481, global_step=62017, preemption_count=0, score=20964.876481, test/accuracy=0.521400, test/loss=2.191377, test/num_examples=10000, total_duration=21860.027041, train/accuracy=0.716458, train/loss=1.090580, validation/accuracy=0.651660, validation/loss=1.424889, validation/num_examples=50000
I0229 11:19:24.376106 140408319375104 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.8534138202667236, loss=1.5217323303222656
I0229 11:19:58.082109 140407379851008 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.9871573448181152, loss=1.4641635417938232
I0229 11:20:31.761929 140408319375104 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8776401281356812, loss=1.4782333374023438
I0229 11:21:05.480315 140407379851008 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9800827503204346, loss=1.6636124849319458
I0229 11:21:39.184541 140408319375104 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.971969723701477, loss=1.6528699398040771
I0229 11:22:12.880567 140407379851008 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.1087818145751953, loss=1.5984443426132202
I0229 11:22:46.577404 140408319375104 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.1662514209747314, loss=1.4866220951080322
I0229 11:23:20.237080 140407379851008 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.9908519983291626, loss=1.6296730041503906
I0229 11:23:53.996141 140408319375104 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8611764907836914, loss=1.5049911737442017
I0229 11:24:27.673392 140407379851008 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.2439112663269043, loss=1.5535999536514282
I0229 11:25:01.445428 140408319375104 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.82804274559021, loss=1.5694596767425537
I0229 11:25:35.175969 140407379851008 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.9254072904586792, loss=1.5132877826690674
I0229 11:26:08.888518 140408319375104 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.890487551689148, loss=1.5131263732910156
I0229 11:26:42.541090 140407379851008 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.9298734664916992, loss=1.5445961952209473
I0229 11:27:16.307042 140408319375104 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.151313543319702, loss=1.5582369565963745
I0229 11:27:26.227030 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:27:32.541644 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:27:44.249091 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:27:46.493386 140573303715648 submission_runner.py:411] Time since start: 22390.49s, 	Step: 63531, 	{'train/accuracy': 0.7502989172935486, 'train/loss': 0.9399452805519104, 'validation/accuracy': 0.6513599753379822, 'validation/loss': 1.4273583889007568, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.16554856300354, 'test/num_examples': 10000, 'score': 21474.96597290039, 'total_duration': 22390.485930919647, 'accumulated_submission_time': 21474.96597290039, 'accumulated_eval_time': 911.6735122203827, 'accumulated_logging_time': 1.3813552856445312}
I0229 11:27:46.516806 140409846093568 logging_writer.py:48] [63531] accumulated_eval_time=911.673512, accumulated_logging_time=1.381355, accumulated_submission_time=21474.965973, global_step=63531, preemption_count=0, score=21474.965973, test/accuracy=0.525700, test/loss=2.165549, test/num_examples=10000, total_duration=22390.485931, train/accuracy=0.750299, train/loss=0.939945, validation/accuracy=0.651360, validation/loss=1.427358, validation/num_examples=50000
I0229 11:28:10.098133 140409854486272 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8570380210876465, loss=1.5724081993103027
I0229 11:28:43.737687 140409846093568 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.9667712450027466, loss=1.511976957321167
I0229 11:29:17.464965 140409854486272 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.103694438934326, loss=1.5436567068099976
I0229 11:29:51.180435 140409846093568 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.800434947013855, loss=1.565966010093689
I0229 11:30:24.879882 140409854486272 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.154412269592285, loss=1.662037968635559
I0229 11:30:58.706870 140409846093568 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.246263027191162, loss=1.6358674764633179
I0229 11:31:32.428375 140409854486272 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9092121124267578, loss=1.557765245437622
I0229 11:32:06.116531 140409846093568 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.0246620178222656, loss=1.5247132778167725
I0229 11:32:39.841372 140409854486272 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.067192792892456, loss=1.5459420680999756
I0229 11:33:13.524782 140409846093568 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.0000174045562744, loss=1.5638467073440552
I0229 11:33:47.266342 140409854486272 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8662071228027344, loss=1.5207998752593994
I0229 11:34:20.982808 140409846093568 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9781250953674316, loss=1.5334995985031128
I0229 11:34:54.665821 140409854486272 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0573558807373047, loss=1.616067886352539
I0229 11:35:28.378874 140409846093568 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9285733699798584, loss=1.6695053577423096
I0229 11:36:02.071671 140409854486272 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.7863733768463135, loss=1.5076124668121338
I0229 11:36:16.698168 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:36:22.891882 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:36:33.140746 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:36:35.431065 140573303715648 submission_runner.py:411] Time since start: 22919.42s, 	Step: 65045, 	{'train/accuracy': 0.7389987111091614, 'train/loss': 0.9976285696029663, 'validation/accuracy': 0.6594399809837341, 'validation/loss': 1.386866569519043, 'validation/num_examples': 50000, 'test/accuracy': 0.5302000045776367, 'test/loss': 2.1050965785980225, 'test/num_examples': 10000, 'score': 21985.078320980072, 'total_duration': 22919.423607587814, 'accumulated_submission_time': 21985.078320980072, 'accumulated_eval_time': 930.4063663482666, 'accumulated_logging_time': 1.414006233215332}
I0229 11:36:35.459123 140407379851008 logging_writer.py:48] [65045] accumulated_eval_time=930.406366, accumulated_logging_time=1.414006, accumulated_submission_time=21985.078321, global_step=65045, preemption_count=0, score=21985.078321, test/accuracy=0.530200, test/loss=2.105097, test/num_examples=10000, total_duration=22919.423608, train/accuracy=0.738999, train/loss=0.997629, validation/accuracy=0.659440, validation/loss=1.386867, validation/num_examples=50000
I0229 11:36:54.388437 140408319375104 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.8757790327072144, loss=1.4615250825881958
I0229 11:37:28.027370 140407379851008 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.162388563156128, loss=1.5477946996688843
I0229 11:38:01.683334 140408319375104 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.0985817909240723, loss=1.5639219284057617
I0229 11:38:35.392444 140407379851008 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.2123470306396484, loss=1.6537063121795654
I0229 11:39:09.045925 140408319375104 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8703311681747437, loss=1.5272477865219116
I0229 11:39:42.722739 140407379851008 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.8657889366149902, loss=1.5239356756210327
I0229 11:40:16.437208 140408319375104 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.0266823768615723, loss=1.4943974018096924
I0229 11:40:50.130382 140407379851008 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.9795632362365723, loss=1.41664719581604
I0229 11:41:23.772567 140408319375104 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.9495717287063599, loss=1.5182193517684937
I0229 11:41:57.489935 140407379851008 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.8973329067230225, loss=1.5884231328964233
I0229 11:42:31.199628 140408319375104 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9480011463165283, loss=1.4724938869476318
I0229 11:43:05.006131 140407379851008 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.1484310626983643, loss=1.5444436073303223
I0229 11:43:38.715591 140408319375104 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.0599005222320557, loss=1.5605477094650269
I0229 11:44:12.421056 140407379851008 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9700114727020264, loss=1.536940574645996
I0229 11:44:46.137325 140408319375104 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.8211390972137451, loss=1.4980831146240234
I0229 11:45:05.529030 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:45:11.735268 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:45:21.703704 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:45:23.990014 140573303715648 submission_runner.py:411] Time since start: 23447.98s, 	Step: 66559, 	{'train/accuracy': 0.7310267686843872, 'train/loss': 1.0304648876190186, 'validation/accuracy': 0.6547399759292603, 'validation/loss': 1.406494140625, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.112121820449829, 'test/num_examples': 10000, 'score': 22495.0784740448, 'total_duration': 23447.982553720474, 'accumulated_submission_time': 22495.0784740448, 'accumulated_eval_time': 948.8673043251038, 'accumulated_logging_time': 1.4522960186004639}
I0229 11:45:24.017519 140409846093568 logging_writer.py:48] [66559] accumulated_eval_time=948.867304, accumulated_logging_time=1.452296, accumulated_submission_time=22495.078474, global_step=66559, preemption_count=0, score=22495.078474, test/accuracy=0.533000, test/loss=2.112122, test/num_examples=10000, total_duration=23447.982554, train/accuracy=0.731027, train/loss=1.030465, validation/accuracy=0.654740, validation/loss=1.406494, validation/num_examples=50000
I0229 11:45:38.160715 140409854486272 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.8304976224899292, loss=1.4834920167922974
I0229 11:46:11.869163 140409846093568 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.1109697818756104, loss=1.7218763828277588
I0229 11:46:45.569930 140409854486272 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.939096450805664, loss=1.5765655040740967
I0229 11:47:19.339866 140409846093568 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.16251802444458, loss=1.495234727859497
I0229 11:47:53.031930 140409854486272 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.009462833404541, loss=1.527911901473999
I0229 11:48:26.816879 140409846093568 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.7303541898727417, loss=1.456986665725708
I0229 11:49:00.530487 140409854486272 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.8720160722732544, loss=1.409454345703125
I0229 11:49:34.424114 140409846093568 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9042620658874512, loss=1.53780198097229
I0229 11:50:08.091090 140409854486272 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.8212196826934814, loss=1.5318515300750732
I0229 11:50:41.869985 140409846093568 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.038851261138916, loss=1.5637562274932861
I0229 11:51:15.576204 140409854486272 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.2617459297180176, loss=1.5500179529190063
I0229 11:51:49.275942 140409846093568 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0212998390197754, loss=1.5157626867294312
I0229 11:52:22.999089 140409854486272 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.099071502685547, loss=1.6158111095428467
I0229 11:52:56.694646 140409846093568 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.073793649673462, loss=1.5760231018066406
I0229 11:53:30.430476 140409854486272 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.0178585052490234, loss=1.4707040786743164
I0229 11:53:54.180671 140573303715648 spec.py:321] Evaluating on the training split.
I0229 11:54:00.474046 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 11:54:09.533836 140573303715648 spec.py:349] Evaluating on the test split.
I0229 11:54:11.818557 140573303715648 submission_runner.py:411] Time since start: 23975.81s, 	Step: 68072, 	{'train/accuracy': 0.7293127775192261, 'train/loss': 1.0425559282302856, 'validation/accuracy': 0.6594399809837341, 'validation/loss': 1.407362461090088, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.10113525390625, 'test/num_examples': 10000, 'score': 23005.171627759933, 'total_duration': 23975.81109070778, 'accumulated_submission_time': 23005.171627759933, 'accumulated_eval_time': 966.5051369667053, 'accumulated_logging_time': 1.4912090301513672}
I0229 11:54:11.850052 140409124681472 logging_writer.py:48] [68072] accumulated_eval_time=966.505137, accumulated_logging_time=1.491209, accumulated_submission_time=23005.171628, global_step=68072, preemption_count=0, score=23005.171628, test/accuracy=0.535600, test/loss=2.101135, test/num_examples=10000, total_duration=23975.811091, train/accuracy=0.729313, train/loss=1.042556, validation/accuracy=0.659440, validation/loss=1.407362, validation/num_examples=50000
I0229 11:54:21.603822 140409829308160 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.820838212966919, loss=1.5427753925323486
I0229 11:54:55.315891 140409124681472 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.498908042907715, loss=1.5247756242752075
I0229 11:55:29.149756 140409829308160 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.044942617416382, loss=1.5693364143371582
I0229 11:56:02.808609 140409124681472 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.1050667762756348, loss=1.4353382587432861
I0229 11:56:36.535392 140409829308160 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9832382202148438, loss=1.5865532159805298
I0229 11:57:10.193475 140409124681472 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.9714916944503784, loss=1.525596022605896
I0229 11:57:43.869719 140409829308160 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.0046122074127197, loss=1.5850085020065308
I0229 11:58:17.529285 140409124681472 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.8849029541015625, loss=1.4070792198181152
I0229 11:58:51.249668 140409829308160 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.8485376834869385, loss=1.4433006048202515
I0229 11:59:24.902634 140409124681472 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.1144537925720215, loss=1.482535719871521
I0229 11:59:58.599336 140409829308160 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.2748939990997314, loss=1.5220091342926025
I0229 12:00:32.264745 140409124681472 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.024146318435669, loss=1.5236821174621582
I0229 12:01:05.989375 140409829308160 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.0516555309295654, loss=1.6383992433547974
I0229 12:01:39.785818 140409124681472 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.8379771709442139, loss=1.480201005935669
I0229 12:02:13.493690 140409829308160 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.2454774379730225, loss=1.4745643138885498
I0229 12:02:41.908966 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:02:48.226583 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:02:56.938499 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:02:59.228740 140573303715648 submission_runner.py:411] Time since start: 24503.22s, 	Step: 69586, 	{'train/accuracy': 0.7391382455825806, 'train/loss': 1.0020445585250854, 'validation/accuracy': 0.6672199964523315, 'validation/loss': 1.3593355417251587, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.1139161586761475, 'test/num_examples': 10000, 'score': 23515.16109251976, 'total_duration': 24503.221274852753, 'accumulated_submission_time': 23515.16109251976, 'accumulated_eval_time': 983.824857711792, 'accumulated_logging_time': 1.5330331325531006}
I0229 12:02:59.257500 140407371458304 logging_writer.py:48] [69586] accumulated_eval_time=983.824858, accumulated_logging_time=1.533033, accumulated_submission_time=23515.161093, global_step=69586, preemption_count=0, score=23515.161093, test/accuracy=0.532000, test/loss=2.113916, test/num_examples=10000, total_duration=24503.221275, train/accuracy=0.739138, train/loss=1.002045, validation/accuracy=0.667220, validation/loss=1.359336, validation/num_examples=50000
I0229 12:03:04.303711 140409837700864 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.2346575260162354, loss=1.683748483657837
I0229 12:03:37.945018 140407371458304 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.969524621963501, loss=1.5972627401351929
I0229 12:04:11.697415 140409837700864 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.003626823425293, loss=1.5571495294570923
I0229 12:04:45.362037 140407371458304 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.2001168727874756, loss=1.531928539276123
I0229 12:05:19.123113 140409837700864 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.8767493963241577, loss=1.5896008014678955
I0229 12:05:52.848552 140407371458304 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.012315273284912, loss=1.6179544925689697
I0229 12:06:26.552592 140409837700864 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.8373908996582031, loss=1.3581950664520264
I0229 12:07:00.240000 140407371458304 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.1316184997558594, loss=1.512336015701294
I0229 12:07:34.063509 140409837700864 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.0092074871063232, loss=1.6247895956039429
I0229 12:08:07.768856 140407371458304 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.050057888031006, loss=1.4562485218048096
I0229 12:08:41.460002 140409837700864 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.966589093208313, loss=1.5271674394607544
I0229 12:09:15.124898 140407371458304 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.0510177612304688, loss=1.4686237573623657
I0229 12:09:48.871909 140409837700864 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0503957271575928, loss=1.4969003200531006
I0229 12:10:22.546860 140407371458304 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.0400264263153076, loss=1.5256284475326538
I0229 12:10:56.329234 140409837700864 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.069150447845459, loss=1.5299896001815796
I0229 12:11:29.491076 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:11:35.791812 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:11:45.705280 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:11:47.986073 140573303715648 submission_runner.py:411] Time since start: 25031.98s, 	Step: 71100, 	{'train/accuracy': 0.7205436825752258, 'train/loss': 1.0698813199996948, 'validation/accuracy': 0.6531199812889099, 'validation/loss': 1.4194097518920898, 'validation/num_examples': 50000, 'test/accuracy': 0.5295000076293945, 'test/loss': 2.145573139190674, 'test/num_examples': 10000, 'score': 24025.32474899292, 'total_duration': 25031.978607177734, 'accumulated_submission_time': 24025.32474899292, 'accumulated_eval_time': 1002.3198018074036, 'accumulated_logging_time': 1.5721306800842285}
I0229 12:11:48.016808 140408319375104 logging_writer.py:48] [71100] accumulated_eval_time=1002.319802, accumulated_logging_time=1.572131, accumulated_submission_time=24025.324749, global_step=71100, preemption_count=0, score=24025.324749, test/accuracy=0.529500, test/loss=2.145573, test/num_examples=10000, total_duration=25031.978607, train/accuracy=0.720544, train/loss=1.069881, validation/accuracy=0.653120, validation/loss=1.419410, validation/num_examples=50000
I0229 12:11:48.379866 140409124681472 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.9931138753890991, loss=1.4337702989578247
I0229 12:12:22.008984 140408319375104 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.961824655532837, loss=1.5912909507751465
I0229 12:12:55.652091 140409124681472 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.9400725364685059, loss=1.467930555343628
I0229 12:13:29.333243 140408319375104 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.9564625024795532, loss=1.5202827453613281
I0229 12:14:03.115989 140409124681472 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.9407330751419067, loss=1.503523349761963
I0229 12:14:36.827948 140408319375104 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.810646414756775, loss=1.5516351461410522
I0229 12:15:10.478124 140409124681472 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.910866379737854, loss=1.4167749881744385
I0229 12:15:44.208230 140408319375104 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.067171573638916, loss=1.5341547727584839
I0229 12:16:17.861994 140409124681472 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.0394022464752197, loss=1.6159876585006714
I0229 12:16:51.601570 140408319375104 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.9980899095535278, loss=1.5187784433364868
I0229 12:17:25.296471 140409124681472 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.0276949405670166, loss=1.4285032749176025
I0229 12:17:59.027909 140408319375104 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.902278184890747, loss=1.5113296508789062
I0229 12:18:32.721524 140409124681472 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.0637950897216797, loss=1.5139784812927246
I0229 12:19:06.455598 140408319375104 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9747782945632935, loss=1.481809377670288
I0229 12:19:40.285197 140409124681472 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0213093757629395, loss=1.5393693447113037
I0229 12:20:14.012984 140408319375104 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.0725064277648926, loss=1.5295264720916748
I0229 12:20:18.212256 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:20:24.423254 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:20:33.463388 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:20:35.887177 140573303715648 submission_runner.py:411] Time since start: 25559.88s, 	Step: 72614, 	{'train/accuracy': 0.74320387840271, 'train/loss': 0.9673858880996704, 'validation/accuracy': 0.6480799913406372, 'validation/loss': 1.4243541955947876, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.152554750442505, 'test/num_examples': 10000, 'score': 24535.45041823387, 'total_duration': 25559.879715681076, 'accumulated_submission_time': 24535.45041823387, 'accumulated_eval_time': 1019.9946706295013, 'accumulated_logging_time': 1.6126031875610352}
I0229 12:20:35.917395 140407379851008 logging_writer.py:48] [72614] accumulated_eval_time=1019.994671, accumulated_logging_time=1.612603, accumulated_submission_time=24535.450418, global_step=72614, preemption_count=0, score=24535.450418, test/accuracy=0.530800, test/loss=2.152555, test/num_examples=10000, total_duration=25559.879716, train/accuracy=0.743204, train/loss=0.967386, validation/accuracy=0.648080, validation/loss=1.424354, validation/num_examples=50000
I0229 12:21:05.222004 140408319375104 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.092878580093384, loss=1.5601447820663452
I0229 12:21:38.898485 140407379851008 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.0931200981140137, loss=1.4384173154830933
I0229 12:22:12.579486 140408319375104 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.9254584312438965, loss=1.4715338945388794
I0229 12:22:46.287613 140407379851008 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.2102625370025635, loss=1.4258003234863281
I0229 12:23:19.967058 140408319375104 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.014249563217163, loss=1.43870210647583
I0229 12:23:53.691227 140407379851008 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.8407111167907715, loss=1.4242948293685913
I0229 12:24:27.352866 140408319375104 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.0402328968048096, loss=1.527194857597351
I0229 12:25:01.098797 140407379851008 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.22951602935791, loss=1.4919683933258057
I0229 12:25:34.798213 140408319375104 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.1704394817352295, loss=1.5931650400161743
I0229 12:26:08.527686 140407379851008 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.8984712362289429, loss=1.4782702922821045
I0229 12:26:42.211692 140408319375104 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.055393934249878, loss=1.5013121366500854
I0229 12:27:15.931911 140407379851008 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.107997417449951, loss=1.5205570459365845
I0229 12:27:49.635309 140408319375104 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.038501262664795, loss=1.4497404098510742
I0229 12:28:23.331340 140407379851008 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9554804563522339, loss=1.5170059204101562
I0229 12:28:57.021132 140408319375104 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.9733479022979736, loss=1.5056195259094238
I0229 12:29:05.923649 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:29:12.278423 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:29:21.248113 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:29:23.538182 140573303715648 submission_runner.py:411] Time since start: 26087.53s, 	Step: 74128, 	{'train/accuracy': 0.7434629797935486, 'train/loss': 0.9811064600944519, 'validation/accuracy': 0.6617599725723267, 'validation/loss': 1.38577401638031, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.12503719329834, 'test/num_examples': 10000, 'score': 25045.385497808456, 'total_duration': 26087.53072452545, 'accumulated_submission_time': 25045.385497808456, 'accumulated_eval_time': 1037.6091718673706, 'accumulated_logging_time': 1.6538634300231934}
I0229 12:29:23.567991 140409846093568 logging_writer.py:48] [74128] accumulated_eval_time=1037.609172, accumulated_logging_time=1.653863, accumulated_submission_time=25045.385498, global_step=74128, preemption_count=0, score=25045.385498, test/accuracy=0.533300, test/loss=2.125037, test/num_examples=10000, total_duration=26087.530725, train/accuracy=0.743463, train/loss=0.981106, validation/accuracy=0.661760, validation/loss=1.385774, validation/num_examples=50000
I0229 12:29:48.118716 140409854486272 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.903261661529541, loss=1.4713101387023926
I0229 12:30:21.801468 140409846093568 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.1944308280944824, loss=1.459110975265503
I0229 12:30:55.477519 140409854486272 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.046501874923706, loss=1.4100104570388794
I0229 12:31:29.150642 140409846093568 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.9234360456466675, loss=1.4552229642868042
I0229 12:32:03.113326 140409854486272 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.8563212156295776, loss=1.4413822889328003
I0229 12:32:36.776587 140409846093568 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.0398964881896973, loss=1.5892466306686401
I0229 12:33:10.535857 140409854486272 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.0043113231658936, loss=1.4822826385498047
I0229 12:33:44.211377 140409846093568 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.9195319414138794, loss=1.559902310371399
I0229 12:34:17.971103 140409854486272 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.915332317352295, loss=1.4753278493881226
I0229 12:34:51.651160 140409846093568 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.387380838394165, loss=1.6084288358688354
I0229 12:35:25.397983 140409854486272 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.050259590148926, loss=1.4719386100769043
I0229 12:35:59.072641 140409846093568 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.0519654750823975, loss=1.470088243484497
I0229 12:36:32.767155 140409854486272 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.0170609951019287, loss=1.4475462436676025
I0229 12:37:06.463671 140409846093568 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.9694452285766602, loss=1.4681400060653687
I0229 12:37:40.102215 140409854486272 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.337226152420044, loss=1.429264783859253
I0229 12:37:53.721169 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:38:01.070730 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:38:11.992763 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:38:14.272139 140573303715648 submission_runner.py:411] Time since start: 26618.26s, 	Step: 75642, 	{'train/accuracy': 0.7388990521430969, 'train/loss': 0.9926463961601257, 'validation/accuracy': 0.6643999814987183, 'validation/loss': 1.3834563493728638, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1104061603546143, 'test/num_examples': 10000, 'score': 25555.467805862427, 'total_duration': 26618.264669656754, 'accumulated_submission_time': 25555.467805862427, 'accumulated_eval_time': 1058.160115480423, 'accumulated_logging_time': 1.6960022449493408}
I0229 12:38:14.301393 140408319375104 logging_writer.py:48] [75642] accumulated_eval_time=1058.160115, accumulated_logging_time=1.696002, accumulated_submission_time=25555.467806, global_step=75642, preemption_count=0, score=25555.467806, test/accuracy=0.535300, test/loss=2.110406, test/num_examples=10000, total_duration=26618.264670, train/accuracy=0.738899, train/loss=0.992646, validation/accuracy=0.664400, validation/loss=1.383456, validation/num_examples=50000
I0229 12:38:34.162728 140409124681472 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.1048669815063477, loss=1.5157968997955322
I0229 12:39:07.798401 140408319375104 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.012533187866211, loss=1.5138318538665771
I0229 12:39:41.502083 140409124681472 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.207794189453125, loss=1.5523338317871094
I0229 12:40:15.185053 140408319375104 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0238583087921143, loss=1.5415563583374023
I0229 12:40:48.899842 140409124681472 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.0596883296966553, loss=1.5331424474716187
I0229 12:41:22.625934 140408319375104 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.1904828548431396, loss=1.5136522054672241
I0229 12:41:56.355330 140409124681472 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1946539878845215, loss=1.6232408285140991
I0229 12:42:30.075064 140408319375104 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.0871105194091797, loss=1.6117271184921265
I0229 12:43:03.797411 140409124681472 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.9436724185943604, loss=1.3854948282241821
I0229 12:43:37.518458 140408319375104 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.290595054626465, loss=1.4145598411560059
I0229 12:44:11.289132 140409124681472 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.0538434982299805, loss=1.4931273460388184
I0229 12:44:44.970355 140408319375104 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.252126693725586, loss=1.4230852127075195
I0229 12:45:18.695626 140409124681472 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.0565741062164307, loss=1.447474718093872
I0229 12:45:52.375276 140408319375104 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.0132341384887695, loss=1.4475177526474
I0229 12:46:26.099011 140409124681472 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.9558128118515015, loss=1.4162755012512207
I0229 12:46:44.429990 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:46:50.659362 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:46:59.368044 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:47:01.642058 140573303715648 submission_runner.py:411] Time since start: 27145.63s, 	Step: 77156, 	{'train/accuracy': 0.7443000674247742, 'train/loss': 0.968315601348877, 'validation/accuracy': 0.6729399561882019, 'validation/loss': 1.3414160013198853, 'validation/num_examples': 50000, 'test/accuracy': 0.5426000356674194, 'test/loss': 2.0834763050079346, 'test/num_examples': 10000, 'score': 26065.52658700943, 'total_duration': 27145.634598970413, 'accumulated_submission_time': 26065.52658700943, 'accumulated_eval_time': 1075.3721516132355, 'accumulated_logging_time': 1.735579490661621}
I0229 12:47:01.670791 140409837700864 logging_writer.py:48] [77156] accumulated_eval_time=1075.372152, accumulated_logging_time=1.735579, accumulated_submission_time=26065.526587, global_step=77156, preemption_count=0, score=26065.526587, test/accuracy=0.542600, test/loss=2.083476, test/num_examples=10000, total_duration=27145.634599, train/accuracy=0.744300, train/loss=0.968316, validation/accuracy=0.672940, validation/loss=1.341416, validation/num_examples=50000
I0229 12:47:17.706256 140409846093568 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.148120880126953, loss=1.5275520086288452
I0229 12:47:51.357964 140409837700864 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.230635404586792, loss=1.4415122270584106
I0229 12:48:25.015650 140409846093568 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.376842975616455, loss=1.4108220338821411
I0229 12:48:58.743066 140409837700864 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.3283917903900146, loss=1.372127652168274
I0229 12:49:32.407761 140409846093568 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.1159634590148926, loss=1.389527678489685
I0229 12:50:06.058454 140409837700864 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.186760187149048, loss=1.3373061418533325
I0229 12:50:39.835676 140409846093568 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.154419183731079, loss=1.4095945358276367
I0229 12:51:13.488106 140409837700864 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.0544347763061523, loss=1.416661024093628
I0229 12:51:47.142408 140409846093568 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.105938673019409, loss=1.4448578357696533
I0229 12:52:20.908644 140409837700864 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.1407697200775146, loss=1.4081250429153442
I0229 12:52:54.570430 140409846093568 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.9727587699890137, loss=1.3664698600769043
I0229 12:53:28.227041 140409837700864 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1869957447052, loss=1.5103989839553833
I0229 12:54:01.914412 140409846093568 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.975103497505188, loss=1.4457734823226929
I0229 12:54:35.608603 140409837700864 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.957362174987793, loss=1.4484959840774536
I0229 12:55:09.280548 140409846093568 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.025115966796875, loss=1.4242526292800903
I0229 12:55:31.658087 140573303715648 spec.py:321] Evaluating on the training split.
I0229 12:55:37.837367 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 12:55:46.691829 140573303715648 spec.py:349] Evaluating on the test split.
I0229 12:55:48.958473 140573303715648 submission_runner.py:411] Time since start: 27672.95s, 	Step: 78668, 	{'train/accuracy': 0.73046875, 'train/loss': 1.0355678796768188, 'validation/accuracy': 0.6589999794960022, 'validation/loss': 1.397053837776184, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.1245603561401367, 'test/num_examples': 10000, 'score': 26574.551361083984, 'total_duration': 27672.950965881348, 'accumulated_submission_time': 26574.551361083984, 'accumulated_eval_time': 1092.6724405288696, 'accumulated_logging_time': 2.6665658950805664}
I0229 12:55:48.994703 140408319375104 logging_writer.py:48] [78668] accumulated_eval_time=1092.672441, accumulated_logging_time=2.666566, accumulated_submission_time=26574.551361, global_step=78668, preemption_count=0, score=26574.551361, test/accuracy=0.533600, test/loss=2.124560, test/num_examples=10000, total_duration=27672.950966, train/accuracy=0.730469, train/loss=1.035568, validation/accuracy=0.659000, validation/loss=1.397054, validation/num_examples=50000
I0229 12:56:00.142359 140409124681472 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.1356759071350098, loss=1.3672195672988892
I0229 12:56:33.845850 140408319375104 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.2068748474121094, loss=1.525557279586792
I0229 12:57:07.566186 140409124681472 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.234506130218506, loss=1.4152123928070068
I0229 12:57:41.224783 140408319375104 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.252039670944214, loss=1.5005290508270264
I0229 12:58:14.964631 140409124681472 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.3986568450927734, loss=1.4686286449432373
I0229 12:58:48.620432 140408319375104 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.1356253623962402, loss=1.4788819551467896
I0229 12:59:22.361112 140409124681472 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.956173062324524, loss=1.5074577331542969
I0229 12:59:56.028301 140408319375104 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.2504937648773193, loss=1.4624574184417725
I0229 13:00:29.705991 140409124681472 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.1058475971221924, loss=1.5314390659332275
I0229 13:01:03.434430 140408319375104 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.0141546726226807, loss=1.4686986207962036
I0229 13:01:37.098242 140409124681472 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.2857933044433594, loss=1.4395438432693481
I0229 13:02:10.837307 140408319375104 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.1209981441497803, loss=1.476449728012085
I0229 13:02:44.628180 140409124681472 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.9642674922943115, loss=1.4706871509552002
I0229 13:03:18.327229 140408319375104 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.9232919216156006, loss=1.2831436395645142
I0229 13:03:51.992033 140409124681472 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.351008892059326, loss=1.4756016731262207
I0229 13:04:19.093662 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:04:25.301179 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:04:33.913866 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:04:36.176106 140573303715648 submission_runner.py:411] Time since start: 28200.17s, 	Step: 80182, 	{'train/accuracy': 0.7581114172935486, 'train/loss': 0.9292783141136169, 'validation/accuracy': 0.6686399579048157, 'validation/loss': 1.3589168787002563, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.059293031692505, 'test/num_examples': 10000, 'score': 27084.58042907715, 'total_duration': 28200.168640851974, 'accumulated_submission_time': 27084.58042907715, 'accumulated_eval_time': 1109.7548336982727, 'accumulated_logging_time': 2.7132747173309326}
I0229 13:04:36.207625 140409837700864 logging_writer.py:48] [80182] accumulated_eval_time=1109.754834, accumulated_logging_time=2.713275, accumulated_submission_time=27084.580429, global_step=80182, preemption_count=0, score=27084.580429, test/accuracy=0.543300, test/loss=2.059293, test/num_examples=10000, total_duration=28200.168641, train/accuracy=0.758111, train/loss=0.929278, validation/accuracy=0.668640, validation/loss=1.358917, validation/num_examples=50000
I0229 13:04:42.607761 140409846093568 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.0369045734405518, loss=1.4095653295516968
I0229 13:05:16.308759 140409837700864 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.057361364364624, loss=1.3771274089813232
I0229 13:05:50.032475 140409846093568 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.9922385215759277, loss=1.3418279886245728
I0229 13:06:23.688661 140409837700864 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.341181993484497, loss=1.4844369888305664
I0229 13:06:57.454080 140409846093568 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.413424253463745, loss=1.4437003135681152
I0229 13:07:31.099436 140409837700864 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.0941922664642334, loss=1.4094310998916626
I0229 13:08:04.760050 140409846093568 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.1897361278533936, loss=1.521315336227417
I0229 13:08:38.606760 140409837700864 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.099961757659912, loss=1.4852941036224365
I0229 13:09:12.376721 140409846093568 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.1274189949035645, loss=1.3644587993621826
I0229 13:09:46.022157 140409837700864 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.968443512916565, loss=1.5773630142211914
I0229 13:10:19.684739 140409846093568 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.014636278152466, loss=1.4039782285690308
I0229 13:10:53.314848 140409837700864 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.9659154415130615, loss=1.3184723854064941
I0229 13:11:27.016966 140409846093568 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.2422869205474854, loss=1.3421844244003296
I0229 13:12:00.705408 140409837700864 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.1706676483154297, loss=1.5715270042419434
I0229 13:12:34.368517 140409846093568 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.1581006050109863, loss=1.4483054876327515
I0229 13:13:06.183842 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:13:12.380136 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:13:21.176002 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:13:23.463616 140573303715648 submission_runner.py:411] Time since start: 28727.46s, 	Step: 81696, 	{'train/accuracy': 0.76761794090271, 'train/loss': 0.8746867179870605, 'validation/accuracy': 0.671459972858429, 'validation/loss': 1.3250740766525269, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.0668623447418213, 'test/num_examples': 10000, 'score': 27594.486018419266, 'total_duration': 28727.456157445908, 'accumulated_submission_time': 27594.486018419266, 'accumulated_eval_time': 1127.0345587730408, 'accumulated_logging_time': 2.7547857761383057}
I0229 13:13:23.496483 140407379851008 logging_writer.py:48] [81696] accumulated_eval_time=1127.034559, accumulated_logging_time=2.754786, accumulated_submission_time=27594.486018, global_step=81696, preemption_count=0, score=27594.486018, test/accuracy=0.540200, test/loss=2.066862, test/num_examples=10000, total_duration=28727.456157, train/accuracy=0.767618, train/loss=0.874687, validation/accuracy=0.671460, validation/loss=1.325074, validation/num_examples=50000
I0229 13:13:25.187116 140408319375104 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.049391984939575, loss=1.4341540336608887
I0229 13:13:58.875517 140407379851008 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.1341559886932373, loss=1.4983265399932861
I0229 13:14:32.542668 140408319375104 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.280414342880249, loss=1.5361508131027222
I0229 13:15:06.351267 140407379851008 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.023632049560547, loss=1.4156745672225952
I0229 13:15:40.035337 140408319375104 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.025023937225342, loss=1.4217555522918701
I0229 13:16:13.763806 140407379851008 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.2712864875793457, loss=1.4444442987442017
I0229 13:16:47.457360 140408319375104 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.330951452255249, loss=1.4667621850967407
I0229 13:17:21.172457 140407379851008 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.3630452156066895, loss=1.3919683694839478
I0229 13:17:54.942701 140408319375104 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2104222774505615, loss=1.427990198135376
I0229 13:18:28.628523 140407379851008 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.076340675354004, loss=1.4368417263031006
I0229 13:19:02.320091 140408319375104 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.1176540851593018, loss=1.474765658378601
I0229 13:19:36.031657 140407379851008 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.2832398414611816, loss=1.4296263456344604
I0229 13:20:09.796157 140408319375104 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.2518105506896973, loss=1.4964722394943237
I0229 13:20:43.458253 140407379851008 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.177419424057007, loss=1.4006080627441406
I0229 13:21:17.234505 140408319375104 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1174535751342773, loss=1.3953248262405396
I0229 13:21:50.931761 140407379851008 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.190843105316162, loss=1.470980167388916
I0229 13:21:53.783502 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:22:00.096799 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:22:08.937423 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:22:11.256206 140573303715648 submission_runner.py:411] Time since start: 29255.25s, 	Step: 83210, 	{'train/accuracy': 0.7473094463348389, 'train/loss': 0.9471119046211243, 'validation/accuracy': 0.6668599843978882, 'validation/loss': 1.3621351718902588, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.0859215259552, 'test/num_examples': 10000, 'score': 28104.702069997787, 'total_duration': 29255.24874305725, 'accumulated_submission_time': 28104.702069997787, 'accumulated_eval_time': 1144.5072071552277, 'accumulated_logging_time': 2.79805588722229}
I0229 13:22:11.290925 140409854486272 logging_writer.py:48] [83210] accumulated_eval_time=1144.507207, accumulated_logging_time=2.798056, accumulated_submission_time=28104.702070, global_step=83210, preemption_count=0, score=28104.702070, test/accuracy=0.539800, test/loss=2.085922, test/num_examples=10000, total_duration=29255.248743, train/accuracy=0.747309, train/loss=0.947112, validation/accuracy=0.666860, validation/loss=1.362135, validation/num_examples=50000
I0229 13:22:41.967017 140409862878976 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.31284236907959, loss=1.4555480480194092
I0229 13:23:15.613347 140409854486272 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.4069080352783203, loss=1.472008228302002
I0229 13:23:49.261260 140409862878976 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2077388763427734, loss=1.5347900390625
I0229 13:24:22.978202 140409854486272 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.105739116668701, loss=1.4769690036773682
I0229 13:24:56.675382 140409862878976 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.314646005630493, loss=1.346971035003662
I0229 13:25:30.371480 140409854486272 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.3952980041503906, loss=1.5126709938049316
I0229 13:26:04.113894 140409862878976 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.2054193019866943, loss=1.4203435182571411
I0229 13:26:37.770916 140409854486272 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.252861499786377, loss=1.5265239477157593
I0229 13:27:11.535595 140409862878976 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.061002731323242, loss=1.405263900756836
I0229 13:27:45.195406 140409854486272 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.2523419857025146, loss=1.3859304189682007
I0229 13:28:18.907812 140409862878976 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.102027654647827, loss=1.3538427352905273
I0229 13:28:52.586659 140409854486272 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.268528699874878, loss=1.4538005590438843
I0229 13:29:26.305871 140409862878976 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.0080044269561768, loss=1.2745450735092163
I0229 13:29:59.984516 140409854486272 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.186457395553589, loss=1.3840503692626953
I0229 13:30:33.690548 140409862878976 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.1827008724212646, loss=1.4926142692565918
I0229 13:30:41.584852 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:30:47.816573 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:30:56.438623 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:30:58.753088 140573303715648 submission_runner.py:411] Time since start: 29782.75s, 	Step: 84725, 	{'train/accuracy': 0.7489636540412903, 'train/loss': 0.9444343447685242, 'validation/accuracy': 0.6720799803733826, 'validation/loss': 1.3304868936538696, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.061533212661743, 'test/num_examples': 10000, 'score': 28614.924313783646, 'total_duration': 29782.745623350143, 'accumulated_submission_time': 28614.924313783646, 'accumulated_eval_time': 1161.6754019260406, 'accumulated_logging_time': 2.844470977783203}
I0229 13:30:58.784923 140407379851008 logging_writer.py:48] [84725] accumulated_eval_time=1161.675402, accumulated_logging_time=2.844471, accumulated_submission_time=28614.924314, global_step=84725, preemption_count=0, score=28614.924314, test/accuracy=0.540000, test/loss=2.061533, test/num_examples=10000, total_duration=29782.745623, train/accuracy=0.748964, train/loss=0.944434, validation/accuracy=0.672080, validation/loss=1.330487, validation/num_examples=50000
I0229 13:31:24.369989 140408319375104 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.101828098297119, loss=1.5215038061141968
I0229 13:31:58.047491 140407379851008 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.315201997756958, loss=1.4964022636413574
I0229 13:32:31.720907 140408319375104 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.323911190032959, loss=1.4013279676437378
I0229 13:33:05.466241 140407379851008 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.196707010269165, loss=1.5269538164138794
I0229 13:33:39.114902 140408319375104 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.3498191833496094, loss=1.4108127355575562
I0229 13:34:12.827794 140407379851008 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.4077744483947754, loss=1.399182915687561
I0229 13:34:46.512942 140408319375104 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.0062084197998047, loss=1.3014984130859375
I0229 13:35:20.196856 140407379851008 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.245371103286743, loss=1.485569715499878
I0229 13:35:53.917462 140408319375104 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.2592945098876953, loss=1.4975500106811523
I0229 13:36:27.647557 140407379851008 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.1955678462982178, loss=1.3225687742233276
I0229 13:37:01.350527 140408319375104 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.1656525135040283, loss=1.3906989097595215
I0229 13:37:35.072827 140407379851008 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.135939836502075, loss=1.4878894090652466
I0229 13:38:08.740114 140408319375104 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.6563544273376465, loss=1.3803176879882812
I0229 13:38:42.444215 140407379851008 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.250924587249756, loss=1.3647176027297974
I0229 13:39:16.170072 140408319375104 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.1561014652252197, loss=1.3724337816238403
I0229 13:39:29.087937 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:39:35.217881 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:39:44.020165 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:39:46.343974 140573303715648 submission_runner.py:411] Time since start: 30310.34s, 	Step: 86240, 	{'train/accuracy': 0.7512555718421936, 'train/loss': 0.9321123957633972, 'validation/accuracy': 0.6727399826049805, 'validation/loss': 1.3387607336044312, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.0293350219726562, 'test/num_examples': 10000, 'score': 29125.15648293495, 'total_duration': 30310.336517095566, 'accumulated_submission_time': 29125.15648293495, 'accumulated_eval_time': 1178.9313924312592, 'accumulated_logging_time': 2.8873541355133057}
I0229 13:39:46.375567 140409854486272 logging_writer.py:48] [86240] accumulated_eval_time=1178.931392, accumulated_logging_time=2.887354, accumulated_submission_time=29125.156483, global_step=86240, preemption_count=0, score=29125.156483, test/accuracy=0.555800, test/loss=2.029335, test/num_examples=10000, total_duration=30310.336517, train/accuracy=0.751256, train/loss=0.932112, validation/accuracy=0.672740, validation/loss=1.338761, validation/num_examples=50000
I0229 13:40:06.888339 140409862878976 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.168541669845581, loss=1.369913935661316
I0229 13:40:40.486614 140409854486272 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.082472562789917, loss=1.439064621925354
I0229 13:41:14.213676 140409862878976 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.1339268684387207, loss=1.4525415897369385
I0229 13:41:47.883865 140409854486272 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.16795015335083, loss=1.4751255512237549
I0229 13:42:21.595793 140409862878976 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3339874744415283, loss=1.445774793624878
I0229 13:42:55.321290 140409854486272 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.152458429336548, loss=1.4539424180984497
I0229 13:43:28.983310 140409862878976 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.537322998046875, loss=1.4568736553192139
I0229 13:44:02.717849 140409854486272 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.17256236076355, loss=1.2884844541549683
I0229 13:44:36.387137 140409862878976 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.2392845153808594, loss=1.3857024908065796
I0229 13:45:10.177858 140409854486272 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.2436835765838623, loss=1.3782881498336792
I0229 13:45:43.905531 140409862878976 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2302768230438232, loss=1.4575340747833252
I0229 13:46:17.575844 140409854486272 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.1577489376068115, loss=1.4498727321624756
I0229 13:46:51.252315 140409862878976 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.194159984588623, loss=1.4171062707901
I0229 13:47:24.976756 140409854486272 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.483795642852783, loss=1.5132330656051636
I0229 13:47:58.632808 140409862878976 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.0750937461853027, loss=1.4207854270935059
I0229 13:48:16.650309 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:48:22.826622 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:48:31.395782 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:48:33.757465 140573303715648 submission_runner.py:411] Time since start: 30837.75s, 	Step: 87755, 	{'train/accuracy': 0.7402941584587097, 'train/loss': 0.9757063388824463, 'validation/accuracy': 0.6700800061225891, 'validation/loss': 1.3509361743927002, 'validation/num_examples': 50000, 'test/accuracy': 0.5366000533103943, 'test/loss': 2.072619676589966, 'test/num_examples': 10000, 'score': 29635.361619234085, 'total_duration': 30837.75000357628, 'accumulated_submission_time': 29635.361619234085, 'accumulated_eval_time': 1196.0384995937347, 'accumulated_logging_time': 2.9288535118103027}
I0229 13:48:33.792204 140408319375104 logging_writer.py:48] [87755] accumulated_eval_time=1196.038500, accumulated_logging_time=2.928854, accumulated_submission_time=29635.361619, global_step=87755, preemption_count=0, score=29635.361619, test/accuracy=0.536600, test/loss=2.072620, test/num_examples=10000, total_duration=30837.750004, train/accuracy=0.740294, train/loss=0.975706, validation/accuracy=0.670080, validation/loss=1.350936, validation/num_examples=50000
I0229 13:48:49.283256 140409124681472 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.1173923015594482, loss=1.4670004844665527
I0229 13:49:22.983184 140408319375104 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.2570223808288574, loss=1.3767776489257812
I0229 13:49:56.687871 140409124681472 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.2059662342071533, loss=1.531315803527832
I0229 13:50:30.398719 140408319375104 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.07956600189209, loss=1.3089923858642578
I0229 13:51:04.100828 140409124681472 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.9984959363937378, loss=1.399972915649414
I0229 13:51:37.893299 140408319375104 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.252617835998535, loss=1.4324390888214111
I0229 13:52:11.577612 140409124681472 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.2239749431610107, loss=1.4137862920761108
I0229 13:52:45.340457 140408319375104 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.4318087100982666, loss=1.4042918682098389
I0229 13:53:19.050012 140409124681472 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.152554988861084, loss=1.4012255668640137
I0229 13:53:52.759981 140408319375104 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.1764228343963623, loss=1.276541829109192
I0229 13:54:26.457350 140409124681472 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.346630096435547, loss=1.376997470855713
I0229 13:55:00.163355 140408319375104 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.33280086517334, loss=1.4314563274383545
I0229 13:55:33.831063 140409124681472 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.6179885864257812, loss=1.4113526344299316
I0229 13:56:07.560439 140408319375104 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3088958263397217, loss=1.4502758979797363
I0229 13:56:41.247475 140409124681472 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.583893299102783, loss=1.479142427444458
I0229 13:57:04.019024 140573303715648 spec.py:321] Evaluating on the training split.
I0229 13:57:10.213994 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 13:57:19.031598 140573303715648 spec.py:349] Evaluating on the test split.
I0229 13:57:21.303966 140573303715648 submission_runner.py:411] Time since start: 31365.30s, 	Step: 89269, 	{'train/accuracy': 0.7931481003761292, 'train/loss': 0.7678533792495728, 'validation/accuracy': 0.6738199591636658, 'validation/loss': 1.3302440643310547, 'validation/num_examples': 50000, 'test/accuracy': 0.5472000241279602, 'test/loss': 2.0408363342285156, 'test/num_examples': 10000, 'score': 30145.517723798752, 'total_duration': 31365.296506166458, 'accumulated_submission_time': 30145.517723798752, 'accumulated_eval_time': 1213.3233938217163, 'accumulated_logging_time': 2.9756851196289062}
I0229 13:57:21.341575 140407379851008 logging_writer.py:48] [89269] accumulated_eval_time=1213.323394, accumulated_logging_time=2.975685, accumulated_submission_time=30145.517724, global_step=89269, preemption_count=0, score=30145.517724, test/accuracy=0.547200, test/loss=2.040836, test/num_examples=10000, total_duration=31365.296506, train/accuracy=0.793148, train/loss=0.767853, validation/accuracy=0.673820, validation/loss=1.330244, validation/num_examples=50000
I0229 13:57:32.200964 140409846093568 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.8990437984466553, loss=1.446317434310913
I0229 13:58:05.823949 140407379851008 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.2719359397888184, loss=1.4126527309417725
I0229 13:58:39.496382 140409846093568 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.4437921047210693, loss=1.4455699920654297
I0229 13:59:13.167415 140407379851008 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2261109352111816, loss=1.388735294342041
I0229 13:59:46.836758 140409846093568 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.335219144821167, loss=1.330100178718567
I0229 14:00:20.558266 140407379851008 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.3211233615875244, loss=1.5002167224884033
I0229 14:00:54.223477 140409846093568 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.2835710048675537, loss=1.4906423091888428
I0229 14:01:27.967993 140407379851008 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.3112897872924805, loss=1.4588788747787476
I0229 14:02:01.674654 140409846093568 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.200658082962036, loss=1.379343032836914
I0229 14:02:35.336896 140407379851008 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.3652396202087402, loss=1.503759503364563
I0229 14:03:09.047276 140409846093568 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.567840814590454, loss=1.343502163887024
I0229 14:03:42.758867 140407379851008 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.198439598083496, loss=1.3895984888076782
I0229 14:04:16.413690 140409846093568 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.5752718448638916, loss=1.4009904861450195
I0229 14:04:50.121175 140407379851008 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.4264979362487793, loss=1.423376202583313
I0229 14:05:23.774837 140409846093568 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.126038074493408, loss=1.3759511709213257
I0229 14:05:51.477802 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:05:57.728731 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:06:06.581526 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:06:08.885661 140573303715648 submission_runner.py:411] Time since start: 31892.88s, 	Step: 90784, 	{'train/accuracy': 0.769929826259613, 'train/loss': 0.8533276915550232, 'validation/accuracy': 0.6751399636268616, 'validation/loss': 1.321486234664917, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.0378308296203613, 'test/num_examples': 10000, 'score': 30655.582379579544, 'total_duration': 31892.87816643715, 'accumulated_submission_time': 30655.582379579544, 'accumulated_eval_time': 1230.7311687469482, 'accumulated_logging_time': 3.023667097091675}
I0229 14:06:08.935792 140409829308160 logging_writer.py:48] [90784] accumulated_eval_time=1230.731169, accumulated_logging_time=3.023667, accumulated_submission_time=30655.582380, global_step=90784, preemption_count=0, score=30655.582380, test/accuracy=0.548500, test/loss=2.037831, test/num_examples=10000, total_duration=31892.878166, train/accuracy=0.769930, train/loss=0.853328, validation/accuracy=0.675140, validation/loss=1.321486, validation/num_examples=50000
I0229 14:06:14.688119 140409837700864 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.675787925720215, loss=1.3911564350128174
I0229 14:06:48.344754 140409829308160 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.353259801864624, loss=1.4241124391555786
I0229 14:07:22.107265 140409837700864 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.0975751876831055, loss=1.4051322937011719
I0229 14:07:55.783104 140409829308160 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.252230644226074, loss=1.5557448863983154
I0229 14:08:29.556221 140409837700864 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.126823902130127, loss=1.3204681873321533
I0229 14:09:03.237587 140409829308160 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.352517604827881, loss=1.4294209480285645
I0229 14:09:36.999130 140409837700864 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.2081058025360107, loss=1.3895926475524902
I0229 14:10:10.801983 140409829308160 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.354182481765747, loss=1.466841220855713
I0229 14:10:44.533223 140409837700864 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.28873348236084, loss=1.3573713302612305
I0229 14:11:18.204691 140409829308160 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3540894985198975, loss=1.380977988243103
I0229 14:11:51.931695 140409837700864 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.973273992538452, loss=1.3606730699539185
I0229 14:12:25.597097 140409829308160 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3951218128204346, loss=1.38120436668396
I0229 14:12:59.272547 140409837700864 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.1753852367401123, loss=1.4300183057785034
I0229 14:13:32.996517 140409829308160 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.216064453125, loss=1.3358980417251587
I0229 14:14:06.664736 140409837700864 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.201739549636841, loss=1.3687450885772705
I0229 14:14:39.195133 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:14:45.362397 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:14:54.200645 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:14:56.500173 140573303715648 submission_runner.py:411] Time since start: 32420.49s, 	Step: 92298, 	{'train/accuracy': 0.7577128410339355, 'train/loss': 0.9045006632804871, 'validation/accuracy': 0.671779990196228, 'validation/loss': 1.3457863330841064, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.0553572177886963, 'test/num_examples': 10000, 'score': 31165.769117355347, 'total_duration': 32420.492695093155, 'accumulated_submission_time': 31165.769117355347, 'accumulated_eval_time': 1248.036145210266, 'accumulated_logging_time': 3.087367534637451}
I0229 14:14:56.533328 140408319375104 logging_writer.py:48] [92298] accumulated_eval_time=1248.036145, accumulated_logging_time=3.087368, accumulated_submission_time=31165.769117, global_step=92298, preemption_count=0, score=31165.769117, test/accuracy=0.548900, test/loss=2.055357, test/num_examples=10000, total_duration=32420.492695, train/accuracy=0.757713, train/loss=0.904501, validation/accuracy=0.671780, validation/loss=1.345786, validation/num_examples=50000
I0229 14:14:57.549653 140409124681472 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.29622220993042, loss=1.3227289915084839
I0229 14:15:31.230911 140408319375104 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.3238630294799805, loss=1.4041101932525635
I0229 14:16:05.021469 140409124681472 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.268615484237671, loss=1.388912320137024
I0229 14:16:38.709609 140408319375104 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.6406872272491455, loss=1.433407187461853
I0229 14:17:12.356169 140409124681472 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.338735818862915, loss=1.452149748802185
I0229 14:17:46.003341 140408319375104 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.2066311836242676, loss=1.4501824378967285
I0229 14:18:19.662346 140409124681472 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.392327308654785, loss=1.3993797302246094
I0229 14:18:53.298005 140408319375104 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.239945650100708, loss=1.3781179189682007
I0229 14:19:26.949354 140409124681472 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.4631247520446777, loss=1.3694720268249512
I0229 14:20:00.628042 140408319375104 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.1427600383758545, loss=1.3370639085769653
I0229 14:20:34.373939 140409124681472 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.2765769958496094, loss=1.3153693675994873
I0229 14:21:08.058749 140408319375104 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.386598825454712, loss=1.427659034729004
I0229 14:21:41.805017 140409124681472 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.135479688644409, loss=1.3872126340866089
I0229 14:22:15.649202 140408319375104 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.2921438217163086, loss=1.333427906036377
I0229 14:22:49.404947 140409124681472 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.3176097869873047, loss=1.3899083137512207
I0229 14:23:23.086081 140408319375104 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.2790050506591797, loss=1.4329602718353271
I0229 14:23:26.594644 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:23:32.758909 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:23:41.645864 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:23:43.929069 140573303715648 submission_runner.py:411] Time since start: 32947.92s, 	Step: 93812, 	{'train/accuracy': 0.7662428021430969, 'train/loss': 0.8773167729377747, 'validation/accuracy': 0.6835599541664124, 'validation/loss': 1.2988988161087036, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.0494730472564697, 'test/num_examples': 10000, 'score': 31675.758866786957, 'total_duration': 32947.921608924866, 'accumulated_submission_time': 31675.758866786957, 'accumulated_eval_time': 1265.370517730713, 'accumulated_logging_time': 3.132223129272461}
I0229 14:23:43.961706 140407379851008 logging_writer.py:48] [93812] accumulated_eval_time=1265.370518, accumulated_logging_time=3.132223, accumulated_submission_time=31675.758867, global_step=93812, preemption_count=0, score=31675.758867, test/accuracy=0.554700, test/loss=2.049473, test/num_examples=10000, total_duration=32947.921609, train/accuracy=0.766243, train/loss=0.877317, validation/accuracy=0.683560, validation/loss=1.298899, validation/num_examples=50000
I0229 14:24:13.930526 140408319375104 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.427020311355591, loss=1.328908920288086
I0229 14:24:47.606312 140407379851008 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.4094061851501465, loss=1.4261994361877441
I0229 14:25:21.267588 140408319375104 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.729740858078003, loss=1.4786807298660278
I0229 14:25:54.993458 140407379851008 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.705198287963867, loss=1.3534083366394043
I0229 14:26:28.633760 140408319375104 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4656083583831787, loss=1.4060964584350586
I0229 14:27:02.350190 140407379851008 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.41450834274292, loss=1.3203834295272827
I0229 14:27:36.060445 140408319375104 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.252127170562744, loss=1.363332748413086
I0229 14:28:09.938739 140407379851008 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.6366209983825684, loss=1.4207428693771362
I0229 14:28:43.683814 140408319375104 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.5119707584381104, loss=1.4133086204528809
I0229 14:29:17.399490 140407379851008 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.2182419300079346, loss=1.4429192543029785
I0229 14:29:51.131351 140408319375104 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.3718016147613525, loss=1.4105550050735474
I0229 14:30:24.861829 140407379851008 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.5902597904205322, loss=1.4598464965820312
I0229 14:30:58.573405 140408319375104 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.4015870094299316, loss=1.47996985912323
I0229 14:31:32.309818 140407379851008 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.5639865398406982, loss=1.4702287912368774
I0229 14:32:06.009087 140408319375104 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.4262304306030273, loss=1.4235477447509766
I0229 14:32:14.242492 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:32:20.474783 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:32:29.276954 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:32:31.563554 140573303715648 submission_runner.py:411] Time since start: 33475.56s, 	Step: 95326, 	{'train/accuracy': 0.7669802308082581, 'train/loss': 0.8797918558120728, 'validation/accuracy': 0.6861199736595154, 'validation/loss': 1.2844245433807373, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 1.9772965908050537, 'test/num_examples': 10000, 'score': 32185.968641519547, 'total_duration': 33475.55606675148, 'accumulated_submission_time': 32185.968641519547, 'accumulated_eval_time': 1282.691505908966, 'accumulated_logging_time': 3.1755781173706055}
I0229 14:32:31.597681 140409124681472 logging_writer.py:48] [95326] accumulated_eval_time=1282.691506, accumulated_logging_time=3.175578, accumulated_submission_time=32185.968642, global_step=95326, preemption_count=0, score=32185.968642, test/accuracy=0.561100, test/loss=1.977297, test/num_examples=10000, total_duration=33475.556067, train/accuracy=0.766980, train/loss=0.879792, validation/accuracy=0.686120, validation/loss=1.284425, validation/num_examples=50000
I0229 14:32:56.907899 140409846093568 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.777554988861084, loss=1.4513740539550781
I0229 14:33:30.559692 140409124681472 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.6666457653045654, loss=1.423334002494812
I0229 14:34:04.232522 140409846093568 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.3355960845947266, loss=1.3786466121673584
I0229 14:34:37.959259 140409124681472 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.4460766315460205, loss=1.4067842960357666
I0229 14:35:11.708396 140409846093568 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.5411670207977295, loss=1.4708982706069946
I0229 14:35:45.396857 140409124681472 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.389606475830078, loss=1.3855103254318237
I0229 14:36:19.084644 140409846093568 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.2672054767608643, loss=1.3660547733306885
I0229 14:36:52.799167 140409124681472 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.2409517765045166, loss=1.22671377658844
I0229 14:37:26.502808 140409846093568 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.3522191047668457, loss=1.3873361349105835
I0229 14:38:00.209290 140409124681472 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.075956344604492, loss=1.2006590366363525
I0229 14:38:33.911210 140409846093568 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.4561877250671387, loss=1.4399323463439941
I0229 14:39:07.604238 140409124681472 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.0966882705688477, loss=1.335199236869812
I0229 14:39:41.270539 140409846093568 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.249253273010254, loss=1.3702889680862427
I0229 14:40:15.050385 140409124681472 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.331725597381592, loss=1.3175418376922607
I0229 14:40:48.740778 140409846093568 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.3626904487609863, loss=1.3835471868515015
I0229 14:41:01.746733 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:41:08.048645 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:41:16.917562 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:41:19.195912 140573303715648 submission_runner.py:411] Time since start: 34003.19s, 	Step: 96840, 	{'train/accuracy': 0.7622169852256775, 'train/loss': 0.8912380337715149, 'validation/accuracy': 0.6829599738121033, 'validation/loss': 1.2990450859069824, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.9927387237548828, 'test/num_examples': 10000, 'score': 32696.044674634933, 'total_duration': 34003.18845510483, 'accumulated_submission_time': 32696.044674634933, 'accumulated_eval_time': 1300.1406605243683, 'accumulated_logging_time': 3.221791982650757}
I0229 14:41:19.229240 140407371458304 logging_writer.py:48] [96840] accumulated_eval_time=1300.140661, accumulated_logging_time=3.221792, accumulated_submission_time=32696.044675, global_step=96840, preemption_count=0, score=32696.044675, test/accuracy=0.558900, test/loss=1.992739, test/num_examples=10000, total_duration=34003.188455, train/accuracy=0.762217, train/loss=0.891238, validation/accuracy=0.682960, validation/loss=1.299045, validation/num_examples=50000
I0229 14:41:39.746112 140407379851008 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.346597194671631, loss=1.3917746543884277
I0229 14:42:13.390138 140407371458304 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.225496292114258, loss=1.3644390106201172
I0229 14:42:47.120448 140407379851008 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3840229511260986, loss=1.3892163038253784
I0229 14:43:20.841028 140407371458304 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.6212120056152344, loss=1.3671364784240723
I0229 14:43:54.531150 140407379851008 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.543760299682617, loss=1.431564211845398
I0229 14:44:28.252513 140407371458304 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.337308168411255, loss=1.3692665100097656
I0229 14:45:01.913648 140407379851008 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.290794610977173, loss=1.302783727645874
I0229 14:45:35.635414 140407371458304 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.5013387203216553, loss=1.381290078163147
I0229 14:46:09.281922 140407379851008 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.3709239959716797, loss=1.35318124294281
I0229 14:46:43.072481 140407371458304 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.8289570808410645, loss=1.4604884386062622
I0229 14:47:16.789099 140407379851008 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.3074491024017334, loss=1.3780653476715088
I0229 14:47:50.515408 140407371458304 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.2320330142974854, loss=1.3303667306900024
I0229 14:48:24.125741 140407379851008 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.563328742980957, loss=1.5050606727600098
I0229 14:48:57.818661 140407371458304 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.4837329387664795, loss=1.4667518138885498
I0229 14:49:31.523949 140407379851008 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.422450542449951, loss=1.2645938396453857
I0229 14:49:49.521609 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:49:55.662151 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:50:04.618183 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:50:06.899268 140573303715648 submission_runner.py:411] Time since start: 34530.89s, 	Step: 98355, 	{'train/accuracy': 0.8137754797935486, 'train/loss': 0.6855299472808838, 'validation/accuracy': 0.6881600022315979, 'validation/loss': 1.2659634351730347, 'validation/num_examples': 50000, 'test/accuracy': 0.5583000183105469, 'test/loss': 1.9982116222381592, 'test/num_examples': 10000, 'score': 33206.26758170128, 'total_duration': 34530.89180493355, 'accumulated_submission_time': 33206.26758170128, 'accumulated_eval_time': 1317.5182647705078, 'accumulated_logging_time': 3.265420913696289}
I0229 14:50:06.934221 140407371458304 logging_writer.py:48] [98355] accumulated_eval_time=1317.518265, accumulated_logging_time=3.265421, accumulated_submission_time=33206.267582, global_step=98355, preemption_count=0, score=33206.267582, test/accuracy=0.558300, test/loss=1.998212, test/num_examples=10000, total_duration=34530.891805, train/accuracy=0.813775, train/loss=0.685530, validation/accuracy=0.688160, validation/loss=1.265963, validation/num_examples=50000
I0229 14:50:22.435517 140407379851008 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.6979196071624756, loss=1.3588699102401733
I0229 14:50:56.105943 140407371458304 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.5075137615203857, loss=1.3394989967346191
I0229 14:51:29.765022 140407379851008 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.3295607566833496, loss=1.2747048139572144
I0229 14:52:03.523225 140407371458304 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.3478941917419434, loss=1.3722587823867798
I0229 14:52:37.374254 140407379851008 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.505298137664795, loss=1.3708617687225342
I0229 14:53:11.047644 140407371458304 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.706164836883545, loss=1.3168950080871582
I0229 14:53:44.767835 140407379851008 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.4548184871673584, loss=1.353858470916748
I0229 14:54:18.429621 140407371458304 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.566326379776001, loss=1.4544864892959595
I0229 14:54:52.092305 140407379851008 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.3401174545288086, loss=1.350497841835022
I0229 14:55:25.736666 140407371458304 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.4675967693328857, loss=1.3612972497940063
I0229 14:55:59.390981 140407379851008 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.312009572982788, loss=1.3029433488845825
I0229 14:56:33.081701 140407371458304 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.4639432430267334, loss=1.3384243249893188
I0229 14:57:06.764410 140407379851008 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.628052234649658, loss=1.3335739374160767
I0229 14:57:40.405602 140407371458304 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.6577394008636475, loss=1.3924874067306519
I0229 14:58:14.104861 140407379851008 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.543381929397583, loss=1.338565707206726
I0229 14:58:36.921635 140573303715648 spec.py:321] Evaluating on the training split.
I0229 14:58:43.153757 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 14:58:51.683911 140573303715648 spec.py:349] Evaluating on the test split.
I0229 14:58:53.965405 140573303715648 submission_runner.py:411] Time since start: 35057.96s, 	Step: 99869, 	{'train/accuracy': 0.7844786047935486, 'train/loss': 0.7990682721138, 'validation/accuracy': 0.684939980506897, 'validation/loss': 1.276252269744873, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.0021021366119385, 'test/num_examples': 10000, 'score': 33716.18281817436, 'total_duration': 35057.95794534683, 'accumulated_submission_time': 33716.18281817436, 'accumulated_eval_time': 1334.5619978904724, 'accumulated_logging_time': 3.3108320236206055}
I0229 14:58:53.999726 140407371458304 logging_writer.py:48] [99869] accumulated_eval_time=1334.561998, accumulated_logging_time=3.310832, accumulated_submission_time=33716.182818, global_step=99869, preemption_count=0, score=33716.182818, test/accuracy=0.552900, test/loss=2.002102, test/num_examples=10000, total_duration=35057.957945, train/accuracy=0.784479, train/loss=0.799068, validation/accuracy=0.684940, validation/loss=1.276252, validation/num_examples=50000
I0229 14:59:04.760085 140409829308160 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.488154411315918, loss=1.4338150024414062
I0229 14:59:38.359585 140407371458304 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.382328510284424, loss=1.2871812582015991
I0229 15:00:12.045761 140409829308160 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.5521562099456787, loss=1.2952278852462769
I0229 15:00:45.767790 140407371458304 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.426873207092285, loss=1.3489618301391602
I0229 15:01:19.468175 140409829308160 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.3960466384887695, loss=1.3506637811660767
I0229 15:01:53.182761 140407371458304 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.463308095932007, loss=1.363008975982666
I0229 15:02:26.853546 140409829308160 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.780320882797241, loss=1.3360167741775513
I0229 15:03:00.602126 140407371458304 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.747547149658203, loss=1.2825238704681396
I0229 15:03:34.311802 140409829308160 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.8551604747772217, loss=1.3708877563476562
I0229 15:04:08.053833 140407371458304 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.489532232284546, loss=1.4628950357437134
I0229 15:04:41.835537 140409829308160 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.5966384410858154, loss=1.3781108856201172
I0229 15:05:15.506689 140407371458304 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.541975259780884, loss=1.3743518590927124
I0229 15:05:49.199726 140409829308160 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.311225652694702, loss=1.280245304107666
I0229 15:06:22.914451 140407371458304 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.379258871078491, loss=1.3530731201171875
I0229 15:06:56.605657 140409829308160 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.548752546310425, loss=1.372962474822998
I0229 15:07:24.079722 140573303715648 spec.py:321] Evaluating on the training split.
I0229 15:07:30.275552 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 15:07:39.030866 140573303715648 spec.py:349] Evaluating on the test split.
I0229 15:07:41.339127 140573303715648 submission_runner.py:411] Time since start: 35585.33s, 	Step: 101383, 	{'train/accuracy': 0.7774832248687744, 'train/loss': 0.8314793705940247, 'validation/accuracy': 0.6846199631690979, 'validation/loss': 1.2771825790405273, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.012157440185547, 'test/num_examples': 10000, 'score': 34226.19294190407, 'total_duration': 35585.33166742325, 'accumulated_submission_time': 34226.19294190407, 'accumulated_eval_time': 1351.8213539123535, 'accumulated_logging_time': 3.355522871017456}
I0229 15:07:41.378360 140407379851008 logging_writer.py:48] [101383] accumulated_eval_time=1351.821354, accumulated_logging_time=3.355523, accumulated_submission_time=34226.192942, global_step=101383, preemption_count=0, score=34226.192942, test/accuracy=0.552100, test/loss=2.012157, test/num_examples=10000, total_duration=35585.331667, train/accuracy=0.777483, train/loss=0.831479, validation/accuracy=0.684620, validation/loss=1.277183, validation/num_examples=50000
I0229 15:07:47.454408 140409124681472 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.496000051498413, loss=1.3046979904174805
I0229 15:08:21.133441 140407379851008 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.589970350265503, loss=1.4204647541046143
I0229 15:08:54.799134 140409124681472 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.3066015243530273, loss=1.3074151277542114
I0229 15:09:28.465977 140407379851008 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.44986629486084, loss=1.3354394435882568
I0229 15:10:02.183501 140409124681472 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.2931196689605713, loss=1.328163981437683
I0229 15:10:35.882753 140407379851008 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.5829217433929443, loss=1.3470499515533447
I0229 15:11:09.694151 140409124681472 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.408989429473877, loss=1.3478513956069946
I0229 15:11:43.342655 140407379851008 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.5305471420288086, loss=1.3635191917419434
I0229 15:12:17.021933 140409124681472 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.6516273021698, loss=1.4558035135269165
I0229 15:12:50.760699 140407379851008 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.5291500091552734, loss=1.3831698894500732
I0229 15:13:24.464540 140409124681472 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.4062905311584473, loss=1.3080785274505615
I0229 15:13:58.130034 140407379851008 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.4072537422180176, loss=1.3345812559127808
I0229 15:14:31.897748 140409124681472 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.675358533859253, loss=1.4099000692367554
I0229 15:15:05.554308 140407379851008 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.417856216430664, loss=1.2901926040649414
I0229 15:15:39.241788 140409124681472 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.525472402572632, loss=1.301803469657898
I0229 15:16:11.347006 140573303715648 spec.py:321] Evaluating on the training split.
I0229 15:16:17.596297 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 15:16:26.353209 140573303715648 spec.py:349] Evaluating on the test split.
I0229 15:16:28.642415 140573303715648 submission_runner.py:411] Time since start: 36112.63s, 	Step: 102897, 	{'train/accuracy': 0.7782804369926453, 'train/loss': 0.8227626085281372, 'validation/accuracy': 0.6896799802780151, 'validation/loss': 1.256480097770691, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 1.97683584690094, 'test/num_examples': 10000, 'score': 34736.09021759033, 'total_duration': 36112.63495731354, 'accumulated_submission_time': 34736.09021759033, 'accumulated_eval_time': 1369.1167182922363, 'accumulated_logging_time': 3.406175136566162}
I0229 15:16:28.677097 140408319375104 logging_writer.py:48] [102897] accumulated_eval_time=1369.116718, accumulated_logging_time=3.406175, accumulated_submission_time=34736.090218, global_step=102897, preemption_count=0, score=34736.090218, test/accuracy=0.559200, test/loss=1.976836, test/num_examples=10000, total_duration=36112.634957, train/accuracy=0.778280, train/loss=0.822763, validation/accuracy=0.689680, validation/loss=1.256480, validation/num_examples=50000
I0229 15:16:30.020002 140409829308160 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.4913218021392822, loss=1.4034783840179443
I0229 15:17:03.732610 140408319375104 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.2805979251861572, loss=1.2959446907043457
I0229 15:17:37.374511 140409829308160 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.4925711154937744, loss=1.2744041681289673
I0229 15:18:10.976543 140408319375104 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.4140753746032715, loss=1.2313697338104248
I0229 15:18:44.714476 140409829308160 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.4220175743103027, loss=1.3224267959594727
I0229 15:19:18.366149 140408319375104 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.4650046825408936, loss=1.2777087688446045
I0229 15:19:52.031511 140409829308160 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.7481231689453125, loss=1.408261775970459
I0229 15:20:25.705225 140408319375104 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.3318092823028564, loss=1.3136754035949707
I0229 15:20:59.436670 140409829308160 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.43056583404541, loss=1.2994309663772583
I0229 15:21:33.128252 140408319375104 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.3425042629241943, loss=1.2398581504821777
I0229 15:22:06.803392 140409829308160 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.487931728363037, loss=1.4212005138397217
I0229 15:22:40.524613 140408319375104 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.4875104427337646, loss=1.295410394668579
I0229 15:23:14.292551 140409829308160 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.410560369491577, loss=1.4376416206359863
I0229 15:23:47.964669 140408319375104 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.585693836212158, loss=1.264866828918457
I0229 15:24:21.702788 140409829308160 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.5156028270721436, loss=1.4107897281646729
I0229 15:24:55.405847 140408319375104 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.4055724143981934, loss=1.394336462020874
I0229 15:24:58.920929 140573303715648 spec.py:321] Evaluating on the training split.
I0229 15:25:05.296680 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 15:25:14.173036 140573303715648 spec.py:349] Evaluating on the test split.
I0229 15:25:16.512335 140573303715648 submission_runner.py:411] Time since start: 36640.50s, 	Step: 104412, 	{'train/accuracy': 0.7736367583274841, 'train/loss': 0.8423909544944763, 'validation/accuracy': 0.6892600059509277, 'validation/loss': 1.259985089302063, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.013371229171753, 'test/num_examples': 10000, 'score': 35246.263035058975, 'total_duration': 36640.50486254692, 'accumulated_submission_time': 35246.263035058975, 'accumulated_eval_time': 1386.708063840866, 'accumulated_logging_time': 3.452512502670288}
I0229 15:25:16.546885 140409846093568 logging_writer.py:48] [104412] accumulated_eval_time=1386.708064, accumulated_logging_time=3.452513, accumulated_submission_time=35246.263035, global_step=104412, preemption_count=0, score=35246.263035, test/accuracy=0.557400, test/loss=2.013371, test/num_examples=10000, total_duration=36640.504863, train/accuracy=0.773637, train/loss=0.842391, validation/accuracy=0.689260, validation/loss=1.259985, validation/num_examples=50000
I0229 15:25:46.452219 140409862878976 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.4670000076293945, loss=1.4372835159301758
I0229 15:26:20.139036 140409846093568 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.4909098148345947, loss=1.3671258687973022
I0229 15:26:53.882183 140409862878976 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.5455150604248047, loss=1.314270257949829
I0229 15:27:27.508440 140409846093568 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.305194139480591, loss=1.1902644634246826
I0229 15:28:01.242510 140409862878976 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.729849338531494, loss=1.3233659267425537
I0229 15:28:34.958442 140409846093568 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.5555593967437744, loss=1.3797290325164795
I0229 15:29:08.704404 140409862878976 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.7066307067871094, loss=1.3842142820358276
I0229 15:29:42.409103 140409846093568 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.5203909873962402, loss=1.3429138660430908
I0229 15:30:16.155626 140409862878976 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.6509976387023926, loss=1.251112937927246
I0229 15:30:49.849551 140409846093568 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.7888612747192383, loss=1.3522815704345703
I0229 15:31:23.587399 140409862878976 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.535912275314331, loss=1.3212580680847168
I0229 15:31:57.232089 140409846093568 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.5828237533569336, loss=1.2461984157562256
I0229 15:32:30.908637 140409862878976 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.5871176719665527, loss=1.2509011030197144
I0229 15:33:04.561448 140409846093568 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.5460922718048096, loss=1.294083833694458
I0229 15:33:38.286011 140409862878976 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.8007614612579346, loss=1.2629926204681396
I0229 15:33:46.515233 140573303715648 spec.py:321] Evaluating on the training split.
I0229 15:33:52.727992 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 15:34:01.576575 140573303715648 spec.py:349] Evaluating on the test split.
I0229 15:34:03.982427 140573303715648 submission_runner.py:411] Time since start: 37167.97s, 	Step: 105926, 	{'train/accuracy': 0.7770846486091614, 'train/loss': 0.8247220516204834, 'validation/accuracy': 0.6956799626350403, 'validation/loss': 1.2360202074050903, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 1.974216103553772, 'test/num_examples': 10000, 'score': 35756.16144490242, 'total_duration': 37167.974967479706, 'accumulated_submission_time': 35756.16144490242, 'accumulated_eval_time': 1404.1752064228058, 'accumulated_logging_time': 3.4970178604125977}
I0229 15:34:04.015964 140409124681472 logging_writer.py:48] [105926] accumulated_eval_time=1404.175206, accumulated_logging_time=3.497018, accumulated_submission_time=35756.161445, global_step=105926, preemption_count=0, score=35756.161445, test/accuracy=0.558600, test/loss=1.974216, test/num_examples=10000, total_duration=37167.974967, train/accuracy=0.777085, train/loss=0.824722, validation/accuracy=0.695680, validation/loss=1.236020, validation/num_examples=50000
I0229 15:34:29.261263 140409829308160 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.470043897628784, loss=1.1909308433532715
I0229 15:35:02.973659 140409124681472 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.7783005237579346, loss=1.271991491317749
I0229 15:35:36.711812 140409829308160 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.6730141639709473, loss=1.3813035488128662
I0229 15:36:10.423677 140409124681472 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.6668074131011963, loss=1.2796283960342407
I0229 15:36:44.107897 140409829308160 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.4087932109832764, loss=1.2681796550750732
I0229 15:37:17.832337 140409124681472 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.4712531566619873, loss=1.238080382347107
I0229 15:37:51.548249 140409829308160 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.627408027648926, loss=1.3732681274414062
I0229 15:38:25.254378 140409124681472 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.8257174491882324, loss=1.3167307376861572
I0229 15:38:58.963731 140409829308160 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.6840968132019043, loss=1.2757842540740967
I0229 15:39:32.677155 140409124681472 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.5657124519348145, loss=1.331762433052063
I0229 15:40:06.456805 140409829308160 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.4814743995666504, loss=1.1839853525161743
I0229 15:40:40.134784 140409124681472 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6367225646972656, loss=1.266153335571289
I0229 15:41:13.910615 140409829308160 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.4334146976470947, loss=1.2335923910140991
I0229 15:41:47.703979 140409124681472 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.654454469680786, loss=1.3109022378921509
I0229 15:42:21.373123 140409829308160 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.548187255859375, loss=1.3180357217788696
I0229 15:42:34.320326 140573303715648 spec.py:321] Evaluating on the training split.
I0229 15:42:40.463425 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 15:42:49.543163 140573303715648 spec.py:349] Evaluating on the test split.
I0229 15:42:51.805747 140573303715648 submission_runner.py:411] Time since start: 37695.80s, 	Step: 107440, 	{'train/accuracy': 0.8256935477256775, 'train/loss': 0.6410678625106812, 'validation/accuracy': 0.6988799571990967, 'validation/loss': 1.2236398458480835, 'validation/num_examples': 50000, 'test/accuracy': 0.5689000487327576, 'test/loss': 1.9630481004714966, 'test/num_examples': 10000, 'score': 36266.39616584778, 'total_duration': 37695.79827213287, 'accumulated_submission_time': 36266.39616584778, 'accumulated_eval_time': 1421.6605622768402, 'accumulated_logging_time': 3.54105544090271}
I0229 15:42:51.842964 140408319375104 logging_writer.py:48] [107440] accumulated_eval_time=1421.660562, accumulated_logging_time=3.541055, accumulated_submission_time=36266.396166, global_step=107440, preemption_count=0, score=36266.396166, test/accuracy=0.568900, test/loss=1.963048, test/num_examples=10000, total_duration=37695.798272, train/accuracy=0.825694, train/loss=0.641068, validation/accuracy=0.698880, validation/loss=1.223640, validation/num_examples=50000
I0229 15:43:12.404633 140409846093568 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.6449289321899414, loss=1.21812105178833
I0229 15:43:46.120167 140408319375104 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.8670334815979004, loss=1.3637527227401733
I0229 15:44:19.774631 140409846093568 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.681363582611084, loss=1.281435251235962
I0229 15:44:53.490279 140408319375104 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.6434717178344727, loss=1.3470262289047241
I0229 15:45:27.155910 140409846093568 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.52230167388916, loss=1.2177321910858154
I0229 15:46:00.824563 140408319375104 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.6008307933807373, loss=1.2536734342575073
I0229 15:46:34.516610 140409846093568 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.4481122493743896, loss=1.303093671798706
I0229 15:47:08.259323 140408319375104 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.6290104389190674, loss=1.3473328351974487
I0229 15:47:42.003214 140409846093568 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.497406244277954, loss=1.2277354001998901
I0229 15:48:15.723855 140408319375104 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.5399529933929443, loss=1.2375078201293945
I0229 15:48:49.441048 140409846093568 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.711747407913208, loss=1.3366589546203613
I0229 15:49:23.191093 140408319375104 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.704608678817749, loss=1.3875229358673096
I0229 15:49:56.915012 140409846093568 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.8986477851867676, loss=1.2716028690338135
I0229 15:50:30.616385 140408319375104 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.6044836044311523, loss=1.2882078886032104
I0229 15:51:04.256758 140409846093568 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.741771697998047, loss=1.2801110744476318
I0229 15:51:21.927598 140573303715648 spec.py:321] Evaluating on the training split.
I0229 15:51:28.081113 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 15:51:36.853335 140573303715648 spec.py:349] Evaluating on the test split.
I0229 15:51:39.106340 140573303715648 submission_runner.py:411] Time since start: 38223.10s, 	Step: 108954, 	{'train/accuracy': 0.8023955225944519, 'train/loss': 0.722143292427063, 'validation/accuracy': 0.694159984588623, 'validation/loss': 1.2538851499557495, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.0258400440216064, 'test/num_examples': 10000, 'score': 36776.41024065018, 'total_duration': 38223.09888339043, 'accumulated_submission_time': 36776.41024065018, 'accumulated_eval_time': 1438.8392596244812, 'accumulated_logging_time': 3.5891611576080322}
I0229 15:51:39.140680 140409124681472 logging_writer.py:48] [108954] accumulated_eval_time=1438.839260, accumulated_logging_time=3.589161, accumulated_submission_time=36776.410241, global_step=108954, preemption_count=0, score=36776.410241, test/accuracy=0.556500, test/loss=2.025840, test/num_examples=10000, total_duration=38223.098883, train/accuracy=0.802396, train/loss=0.722143, validation/accuracy=0.694160, validation/loss=1.253885, validation/num_examples=50000
I0229 15:51:54.969881 140409829308160 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.5921590328216553, loss=1.2563551664352417
I0229 15:52:28.639361 140409124681472 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.9334630966186523, loss=1.3363208770751953
I0229 15:53:02.363751 140409829308160 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6336278915405273, loss=1.3509011268615723
I0229 15:53:36.037513 140409124681472 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.6216580867767334, loss=1.2562711238861084
I0229 15:54:09.922324 140409829308160 logging_writer.py:48] [109400] global_step=109400, grad_norm=3.0090229511260986, loss=1.3368924856185913
I0229 15:54:43.553118 140409124681472 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.537179708480835, loss=1.2258514165878296
I0229 15:55:17.281672 140409829308160 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.5116891860961914, loss=1.2685906887054443
I0229 15:55:50.948914 140409124681472 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.5360968112945557, loss=1.260414958000183
I0229 15:56:24.580353 140409829308160 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.6407089233398438, loss=1.2872650623321533
I0229 15:56:58.352507 140409124681472 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.717892646789551, loss=1.1629570722579956
I0229 15:57:31.996832 140409829308160 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.660536766052246, loss=1.2894858121871948
I0229 15:58:05.663563 140409124681472 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.8803229331970215, loss=1.2380884885787964
I0229 15:58:39.380223 140409829308160 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.7448902130126953, loss=1.1905088424682617
I0229 15:59:13.011674 140409124681472 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.484837055206299, loss=1.251493215560913
I0229 15:59:46.833063 140409829308160 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.578446865081787, loss=1.297121286392212
I0229 16:00:09.247631 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:00:15.398994 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:00:24.266458 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:00:26.629614 140573303715648 submission_runner.py:411] Time since start: 38750.62s, 	Step: 110468, 	{'train/accuracy': 0.8027144074440002, 'train/loss': 0.7184869050979614, 'validation/accuracy': 0.7019400000572205, 'validation/loss': 1.2076020240783691, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 1.931430697441101, 'test/num_examples': 10000, 'score': 37286.4473862648, 'total_duration': 38750.62214708328, 'accumulated_submission_time': 37286.4473862648, 'accumulated_eval_time': 1456.2212126255035, 'accumulated_logging_time': 3.6329078674316406}
I0229 16:00:26.664412 140409854486272 logging_writer.py:48] [110468] accumulated_eval_time=1456.221213, accumulated_logging_time=3.632908, accumulated_submission_time=37286.447386, global_step=110468, preemption_count=0, score=37286.447386, test/accuracy=0.570300, test/loss=1.931431, test/num_examples=10000, total_duration=38750.622147, train/accuracy=0.802714, train/loss=0.718487, validation/accuracy=0.701940, validation/loss=1.207602, validation/num_examples=50000
I0229 16:00:37.781769 140409862878976 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.563812732696533, loss=1.099062204360962
I0229 16:01:11.488491 140409854486272 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.6092772483825684, loss=1.2382301092147827
I0229 16:01:45.189784 140409862878976 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.9663548469543457, loss=1.325415849685669
I0229 16:02:18.851949 140409854486272 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.7922611236572266, loss=1.3055862188339233
I0229 16:02:52.566468 140409862878976 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.588947057723999, loss=1.2492122650146484
I0229 16:03:26.226950 140409854486272 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.6358394622802734, loss=1.2542911767959595
I0229 16:03:59.897490 140409862878976 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.661222457885742, loss=1.2898198366165161
I0229 16:04:33.611186 140409854486272 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.7736434936523438, loss=1.3080520629882812
I0229 16:05:07.272413 140409862878976 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.477829933166504, loss=1.2707270383834839
I0229 16:05:40.920948 140409854486272 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.704866886138916, loss=1.1992696523666382
I0229 16:06:14.654543 140409862878976 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.883260488510132, loss=1.3000762462615967
I0229 16:06:48.273648 140409854486272 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.9316515922546387, loss=1.2695274353027344
I0229 16:07:21.933919 140409862878976 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.63206148147583, loss=1.245438814163208
I0229 16:07:55.670936 140409854486272 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.723454236984253, loss=1.3376200199127197
I0229 16:08:29.379153 140409862878976 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.9153599739074707, loss=1.2770202159881592
I0229 16:08:56.797135 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:09:03.052475 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:09:12.856656 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:09:15.176933 140573303715648 submission_runner.py:411] Time since start: 39279.17s, 	Step: 111983, 	{'train/accuracy': 0.7995057106018066, 'train/loss': 0.7365074753761292, 'validation/accuracy': 0.7022199630737305, 'validation/loss': 1.2109649181365967, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 1.9284131526947021, 'test/num_examples': 10000, 'score': 37796.50906038284, 'total_duration': 39279.16947221756, 'accumulated_submission_time': 37796.50906038284, 'accumulated_eval_time': 1474.6009676456451, 'accumulated_logging_time': 3.678417444229126}
I0229 16:09:15.212933 140407371458304 logging_writer.py:48] [111983] accumulated_eval_time=1474.600968, accumulated_logging_time=3.678417, accumulated_submission_time=37796.509060, global_step=111983, preemption_count=0, score=37796.509060, test/accuracy=0.570600, test/loss=1.928413, test/num_examples=10000, total_duration=39279.169472, train/accuracy=0.799506, train/loss=0.736507, validation/accuracy=0.702220, validation/loss=1.210965, validation/num_examples=50000
I0229 16:09:21.276427 140407379851008 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.568253755569458, loss=1.3276615142822266
I0229 16:09:54.905591 140407371458304 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.505718946456909, loss=1.2285218238830566
I0229 16:10:28.549629 140407379851008 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.784109115600586, loss=1.3109055757522583
I0229 16:11:02.270681 140407371458304 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.615957736968994, loss=1.2412972450256348
I0229 16:11:35.971859 140407379851008 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.5840892791748047, loss=1.2058143615722656
I0229 16:12:09.730248 140407371458304 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.7518019676208496, loss=1.305420994758606
I0229 16:12:43.416901 140407379851008 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.77154278755188, loss=1.3858121633529663
I0229 16:13:17.114686 140407371458304 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.8267085552215576, loss=1.3606841564178467
I0229 16:13:50.784738 140407379851008 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.50801420211792, loss=1.2083985805511475
I0229 16:14:24.439361 140407371458304 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.9173572063446045, loss=1.2398309707641602
I0229 16:14:58.204838 140407379851008 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.7129275798797607, loss=1.22158682346344
I0229 16:15:31.892138 140407371458304 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.989015579223633, loss=1.2991427183151245
I0229 16:16:05.643257 140407379851008 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.9633800983428955, loss=1.382328987121582
I0229 16:16:39.382710 140407371458304 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.8884522914886475, loss=1.2092634439468384
I0229 16:17:13.063138 140407379851008 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.5680243968963623, loss=1.2050137519836426
I0229 16:17:45.245967 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:17:51.324586 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:18:00.130884 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:18:02.467968 140573303715648 submission_runner.py:411] Time since start: 39806.46s, 	Step: 113497, 	{'train/accuracy': 0.7829639315605164, 'train/loss': 0.7980664968490601, 'validation/accuracy': 0.6932199597358704, 'validation/loss': 1.2627981901168823, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 2.024306297302246, 'test/num_examples': 10000, 'score': 38306.47283291817, 'total_duration': 39806.460503578186, 'accumulated_submission_time': 38306.47283291817, 'accumulated_eval_time': 1491.822915315628, 'accumulated_logging_time': 3.724119186401367}
I0229 16:18:02.506491 140407379851008 logging_writer.py:48] [113497] accumulated_eval_time=1491.822915, accumulated_logging_time=3.724119, accumulated_submission_time=38306.472833, global_step=113497, preemption_count=0, score=38306.472833, test/accuracy=0.562100, test/loss=2.024306, test/num_examples=10000, total_duration=39806.460504, train/accuracy=0.782964, train/loss=0.798066, validation/accuracy=0.693220, validation/loss=1.262798, validation/num_examples=50000
I0229 16:18:03.889257 140408319375104 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.732447624206543, loss=1.2307345867156982
I0229 16:18:37.727518 140407379851008 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8456826210021973, loss=1.3424378633499146
I0229 16:19:11.411089 140408319375104 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.837674856185913, loss=1.3267539739608765
I0229 16:19:45.137719 140407379851008 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.786938428878784, loss=1.2082959413528442
I0229 16:20:18.829230 140408319375104 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.7761411666870117, loss=1.253069281578064
I0229 16:20:52.560031 140407379851008 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.6887569427490234, loss=1.2162859439849854
I0229 16:21:26.214231 140408319375104 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.8953537940979004, loss=1.2721362113952637
I0229 16:21:59.881503 140407379851008 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.8788299560546875, loss=1.2612636089324951
I0229 16:22:33.617442 140408319375104 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.9154508113861084, loss=1.3218430280685425
I0229 16:23:07.286029 140407379851008 logging_writer.py:48] [114400] global_step=114400, grad_norm=3.106504201889038, loss=1.2196472883224487
I0229 16:23:40.954192 140408319375104 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.6226534843444824, loss=1.1440058946609497
I0229 16:24:14.775586 140407379851008 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.559283494949341, loss=1.2527554035186768
I0229 16:24:48.510885 140408319375104 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.8646771907806396, loss=1.2944121360778809
I0229 16:25:22.177014 140407379851008 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.7841200828552246, loss=1.1432299613952637
I0229 16:25:55.923259 140408319375104 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.9846129417419434, loss=1.1514451503753662
I0229 16:26:29.625932 140407379851008 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.7628819942474365, loss=1.2222321033477783
I0229 16:26:32.478630 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:26:38.775711 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:26:47.512882 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:26:49.815771 140573303715648 submission_runner.py:411] Time since start: 40333.81s, 	Step: 115010, 	{'train/accuracy': 0.7958585619926453, 'train/loss': 0.7441632151603699, 'validation/accuracy': 0.7008000016212463, 'validation/loss': 1.2077741622924805, 'validation/num_examples': 50000, 'test/accuracy': 0.5746999979019165, 'test/loss': 1.9470356702804565, 'test/num_examples': 10000, 'score': 38816.370770692825, 'total_duration': 40333.80831313133, 'accumulated_submission_time': 38816.370770692825, 'accumulated_eval_time': 1509.1600093841553, 'accumulated_logging_time': 3.776813507080078}
I0229 16:26:49.853319 140409837700864 logging_writer.py:48] [115010] accumulated_eval_time=1509.160009, accumulated_logging_time=3.776814, accumulated_submission_time=38816.370771, global_step=115010, preemption_count=0, score=38816.370771, test/accuracy=0.574700, test/loss=1.947036, test/num_examples=10000, total_duration=40333.808313, train/accuracy=0.795859, train/loss=0.744163, validation/accuracy=0.700800, validation/loss=1.207774, validation/num_examples=50000
I0229 16:27:20.531945 140409846093568 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.9035422801971436, loss=1.1949772834777832
I0229 16:27:54.194441 140409837700864 logging_writer.py:48] [115200] global_step=115200, grad_norm=3.0005342960357666, loss=1.2758841514587402
I0229 16:28:27.855791 140409846093568 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.645416736602783, loss=1.1804078817367554
I0229 16:29:01.564345 140409837700864 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.105764627456665, loss=1.2686409950256348
I0229 16:29:35.258459 140409846093568 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.6268465518951416, loss=1.136600136756897
I0229 16:30:08.943568 140409837700864 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.559391975402832, loss=1.163088083267212
I0229 16:30:42.751686 140409846093568 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.8596229553222656, loss=1.2619143724441528
I0229 16:31:16.451828 140409837700864 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.534816265106201, loss=1.1549432277679443
I0229 16:31:50.135863 140409846093568 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7132322788238525, loss=1.1651630401611328
I0229 16:32:23.856770 140409837700864 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.8825860023498535, loss=1.1953707933425903
I0229 16:32:57.545943 140409846093568 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.7027320861816406, loss=1.268416404724121
I0229 16:33:31.277148 140409837700864 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.6993308067321777, loss=1.19589364528656
I0229 16:34:04.953103 140409846093568 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.780866861343384, loss=1.2446871995925903
I0229 16:34:38.670533 140409837700864 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.8436942100524902, loss=1.2437652349472046
I0229 16:35:12.363806 140409846093568 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.903406858444214, loss=1.2087352275848389
I0229 16:35:19.929670 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:35:26.041123 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:35:34.642945 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:35:36.951155 140573303715648 submission_runner.py:411] Time since start: 40860.94s, 	Step: 116524, 	{'train/accuracy': 0.8346619606018066, 'train/loss': 0.6010602116584778, 'validation/accuracy': 0.7102000117301941, 'validation/loss': 1.1695055961608887, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 1.8854345083236694, 'test/num_examples': 10000, 'score': 39326.3773624897, 'total_duration': 40860.94369840622, 'accumulated_submission_time': 39326.3773624897, 'accumulated_eval_time': 1526.1814465522766, 'accumulated_logging_time': 3.824695587158203}
I0229 16:35:36.987861 140409846093568 logging_writer.py:48] [116524] accumulated_eval_time=1526.181447, accumulated_logging_time=3.824696, accumulated_submission_time=39326.377362, global_step=116524, preemption_count=0, score=39326.377362, test/accuracy=0.579100, test/loss=1.885435, test/num_examples=10000, total_duration=40860.943698, train/accuracy=0.834662, train/loss=0.601060, validation/accuracy=0.710200, validation/loss=1.169506, validation/num_examples=50000
I0229 16:36:02.911210 140409854486272 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.7572131156921387, loss=1.3096446990966797
I0229 16:36:36.650575 140409846093568 logging_writer.py:48] [116700] global_step=116700, grad_norm=3.3063268661499023, loss=1.1925983428955078
I0229 16:37:10.363259 140409854486272 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.9559216499328613, loss=1.3079781532287598
I0229 16:37:44.018789 140409846093568 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.4281277656555176, loss=1.2746076583862305
I0229 16:38:17.702939 140409854486272 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9866104125976562, loss=1.2830957174301147
I0229 16:38:51.413431 140409846093568 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.8085861206054688, loss=1.1470576524734497
I0229 16:39:25.066493 140409854486272 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.096611738204956, loss=1.3028008937835693
I0229 16:39:58.777862 140409846093568 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.8195595741271973, loss=1.1361560821533203
I0229 16:40:32.467884 140409854486272 logging_writer.py:48] [117400] global_step=117400, grad_norm=3.2908451557159424, loss=1.283186674118042
I0229 16:41:06.126384 140409846093568 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.9008285999298096, loss=1.2660021781921387
I0229 16:41:39.810801 140409854486272 logging_writer.py:48] [117600] global_step=117600, grad_norm=3.0002777576446533, loss=1.1738667488098145
I0229 16:42:13.516176 140409846093568 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.875789165496826, loss=1.2073378562927246
I0229 16:42:47.367661 140409854486272 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.7242419719696045, loss=1.1932809352874756
I0229 16:43:21.091612 140409846093568 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.9724369049072266, loss=1.3020654916763306
I0229 16:43:54.769497 140409854486272 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.621739149093628, loss=1.2057442665100098
I0229 16:44:07.068154 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:44:13.222929 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:44:21.862566 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:44:24.157104 140573303715648 submission_runner.py:411] Time since start: 41388.15s, 	Step: 118038, 	{'train/accuracy': 0.8148317933082581, 'train/loss': 0.6677629947662354, 'validation/accuracy': 0.7066999673843384, 'validation/loss': 1.2082501649856567, 'validation/num_examples': 50000, 'test/accuracy': 0.575700044631958, 'test/loss': 1.9446635246276855, 'test/num_examples': 10000, 'score': 39836.38484477997, 'total_duration': 41388.14964914322, 'accumulated_submission_time': 39836.38484477997, 'accumulated_eval_time': 1543.270350933075, 'accumulated_logging_time': 3.8735198974609375}
I0229 16:44:24.196816 140409124681472 logging_writer.py:48] [118038] accumulated_eval_time=1543.270351, accumulated_logging_time=3.873520, accumulated_submission_time=39836.384845, global_step=118038, preemption_count=0, score=39836.384845, test/accuracy=0.575700, test/loss=1.944664, test/num_examples=10000, total_duration=41388.149649, train/accuracy=0.814832, train/loss=0.667763, validation/accuracy=0.706700, validation/loss=1.208250, validation/num_examples=50000
I0229 16:44:45.435282 140409829308160 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.9425392150878906, loss=1.2101531028747559
I0229 16:45:19.091794 140409124681472 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.796485662460327, loss=1.1751285791397095
I0229 16:45:52.755498 140409829308160 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.084859848022461, loss=1.2138981819152832
I0229 16:46:26.473267 140409124681472 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.826536178588867, loss=1.175022006034851
I0229 16:47:00.124601 140409829308160 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.2057459354400635, loss=1.349819540977478
I0229 16:47:33.814893 140409124681472 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.972870349884033, loss=1.2676630020141602
I0229 16:48:07.471016 140409829308160 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.563436269760132, loss=1.1326724290847778
I0229 16:48:41.333163 140409124681472 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.6156985759735107, loss=1.1539807319641113
I0229 16:49:15.052332 140409829308160 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.955684185028076, loss=1.2611279487609863
I0229 16:49:48.735436 140409124681472 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.7885189056396484, loss=1.1846914291381836
I0229 16:50:22.470140 140409829308160 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.2823433876037598, loss=1.2429089546203613
I0229 16:50:56.185756 140409124681472 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.787008285522461, loss=1.2389589548110962
I0229 16:51:29.921692 140409829308160 logging_writer.py:48] [119300] global_step=119300, grad_norm=3.225043296813965, loss=1.1351431608200073
I0229 16:52:03.618688 140409124681472 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.297017812728882, loss=1.2424161434173584
I0229 16:52:37.335187 140409829308160 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.98302960395813, loss=1.1697076559066772
I0229 16:52:54.346358 140573303715648 spec.py:321] Evaluating on the training split.
I0229 16:53:00.494432 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 16:53:09.295884 140573303715648 spec.py:349] Evaluating on the test split.
I0229 16:53:11.602157 140573303715648 submission_runner.py:411] Time since start: 41915.59s, 	Step: 119552, 	{'train/accuracy': 0.8146523833274841, 'train/loss': 0.6767027378082275, 'validation/accuracy': 0.705020010471344, 'validation/loss': 1.2016663551330566, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.9262524843215942, 'test/num_examples': 10000, 'score': 40346.46527194977, 'total_duration': 41915.59468245506, 'accumulated_submission_time': 40346.46527194977, 'accumulated_eval_time': 1560.5260910987854, 'accumulated_logging_time': 3.9229581356048584}
I0229 16:53:11.641367 140409854486272 logging_writer.py:48] [119552] accumulated_eval_time=1560.526091, accumulated_logging_time=3.922958, accumulated_submission_time=40346.465272, global_step=119552, preemption_count=0, score=40346.465272, test/accuracy=0.578100, test/loss=1.926252, test/num_examples=10000, total_duration=41915.594682, train/accuracy=0.814652, train/loss=0.676703, validation/accuracy=0.705020, validation/loss=1.201666, validation/num_examples=50000
I0229 16:53:28.136479 140409862878976 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.9185147285461426, loss=1.2004380226135254
I0229 16:54:01.775227 140409854486272 logging_writer.py:48] [119700] global_step=119700, grad_norm=3.1506693363189697, loss=1.2146040201187134
I0229 16:54:35.490107 140409862878976 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.7515883445739746, loss=1.2155920267105103
I0229 16:55:09.310136 140409854486272 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.8923096656799316, loss=1.2204549312591553
I0229 16:55:42.988280 140409862878976 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.8527793884277344, loss=1.1507622003555298
I0229 16:56:16.720644 140409854486272 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.837200164794922, loss=1.1554648876190186
I0229 16:56:50.408378 140409862878976 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.773190975189209, loss=1.0892276763916016
I0229 16:57:24.079909 140409854486272 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.1050240993499756, loss=1.3265231847763062
I0229 16:57:57.717450 140409862878976 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.137244462966919, loss=1.1587839126586914
I0229 16:58:31.389179 140409854486272 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.901970148086548, loss=1.1276614665985107
I0229 16:59:05.099398 140409862878976 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.0598576068878174, loss=1.1265393495559692
I0229 16:59:38.731078 140409854486272 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.8525514602661133, loss=1.218318223953247
I0229 17:00:12.395197 140409862878976 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.1467390060424805, loss=1.1689704656600952
I0229 17:00:46.171352 140409854486272 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.898414134979248, loss=1.1967633962631226
I0229 17:01:19.983702 140409862878976 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.881058692932129, loss=1.1501778364181519
I0229 17:01:41.688416 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:01:47.903022 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:01:56.763145 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:01:59.083845 140573303715648 submission_runner.py:411] Time since start: 42443.08s, 	Step: 121066, 	{'train/accuracy': 0.8086734414100647, 'train/loss': 0.6844852566719055, 'validation/accuracy': 0.7085199952125549, 'validation/loss': 1.1973555088043213, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.921277642250061, 'test/num_examples': 10000, 'score': 40856.44110560417, 'total_duration': 42443.076384067535, 'accumulated_submission_time': 40856.44110560417, 'accumulated_eval_time': 1577.9214849472046, 'accumulated_logging_time': 3.97292423248291}
I0229 17:01:59.121029 140407379851008 logging_writer.py:48] [121066] accumulated_eval_time=1577.921485, accumulated_logging_time=3.972924, accumulated_submission_time=40856.441106, global_step=121066, preemption_count=0, score=40856.441106, test/accuracy=0.578900, test/loss=1.921278, test/num_examples=10000, total_duration=42443.076384, train/accuracy=0.808673, train/loss=0.684485, validation/accuracy=0.708520, validation/loss=1.197356, validation/num_examples=50000
I0229 17:02:10.900424 140408319375104 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.980928659439087, loss=1.1151515245437622
I0229 17:02:44.574808 140407379851008 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.9301180839538574, loss=1.1316057443618774
I0229 17:03:18.325945 140408319375104 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.0023369789123535, loss=1.1149883270263672
I0229 17:03:52.059204 140407379851008 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.9980597496032715, loss=1.168576717376709
I0229 17:04:25.780810 140408319375104 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.9651646614074707, loss=1.2215739488601685
I0229 17:04:59.447351 140407379851008 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.8629705905914307, loss=1.1412616968154907
I0229 17:05:33.167442 140408319375104 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.8890130519866943, loss=1.2188880443572998
I0229 17:06:06.826611 140407379851008 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.133335828781128, loss=1.213421106338501
I0229 17:06:40.559319 140408319375104 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.848811388015747, loss=1.1995654106140137
I0229 17:07:14.323696 140407379851008 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.821554660797119, loss=1.1406614780426025
I0229 17:07:48.017071 140408319375104 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.001339912414551, loss=1.1574070453643799
I0229 17:08:21.738025 140407379851008 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.0989184379577637, loss=1.1434504985809326
I0229 17:08:55.412051 140408319375104 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.118943452835083, loss=1.1149909496307373
I0229 17:09:29.143600 140407379851008 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.201432466506958, loss=1.1703927516937256
I0229 17:10:02.809975 140408319375104 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.0239832401275635, loss=1.1736830472946167
I0229 17:10:29.266140 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:10:35.463593 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:10:44.086481 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:10:46.451560 140573303715648 submission_runner.py:411] Time since start: 42970.44s, 	Step: 122580, 	{'train/accuracy': 0.7980110049247742, 'train/loss': 0.7274233102798462, 'validation/accuracy': 0.6998400092124939, 'validation/loss': 1.2325913906097412, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 1.985614538192749, 'test/num_examples': 10000, 'score': 41366.51425933838, 'total_duration': 42970.44410133362, 'accumulated_submission_time': 41366.51425933838, 'accumulated_eval_time': 1595.1068725585938, 'accumulated_logging_time': 4.020076274871826}
I0229 17:10:46.488656 140409124681472 logging_writer.py:48] [122580] accumulated_eval_time=1595.106873, accumulated_logging_time=4.020076, accumulated_submission_time=41366.514259, global_step=122580, preemption_count=0, score=41366.514259, test/accuracy=0.572100, test/loss=1.985615, test/num_examples=10000, total_duration=42970.444101, train/accuracy=0.798011, train/loss=0.727423, validation/accuracy=0.699840, validation/loss=1.232591, validation/num_examples=50000
I0229 17:10:53.564537 140409837700864 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.489969253540039, loss=1.2688701152801514
I0229 17:11:27.183495 140409124681472 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.413475513458252, loss=1.2720667123794556
I0229 17:12:00.852707 140409837700864 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.9917492866516113, loss=1.2145143747329712
I0229 17:12:34.524501 140409124681472 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.896320343017578, loss=1.0361099243164062
I0229 17:13:08.311726 140409837700864 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.1123297214508057, loss=1.1589436531066895
I0229 17:13:42.038788 140409124681472 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.3921680450439453, loss=1.1770575046539307
I0229 17:14:15.688299 140409837700864 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.1107099056243896, loss=1.1385014057159424
I0229 17:14:49.389958 140409124681472 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.8416714668273926, loss=1.1032710075378418
I0229 17:15:23.098037 140409837700864 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.8077783584594727, loss=1.1689211130142212
I0229 17:15:56.727376 140409124681472 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.9763519763946533, loss=1.1101806163787842
I0229 17:16:30.373992 140409837700864 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.2529919147491455, loss=1.1952158212661743
I0229 17:17:03.997685 140409124681472 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.134993076324463, loss=1.1214008331298828
I0229 17:17:37.646424 140409837700864 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.0114903450012207, loss=1.1443719863891602
I0229 17:18:11.298704 140409124681472 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.3175530433654785, loss=1.187950849533081
I0229 17:18:44.943269 140409837700864 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.9028801918029785, loss=1.205162525177002
I0229 17:19:16.490004 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:19:22.585331 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:19:31.474054 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:19:33.786912 140573303715648 submission_runner.py:411] Time since start: 43497.78s, 	Step: 124095, 	{'train/accuracy': 0.8214086294174194, 'train/loss': 0.6441144943237305, 'validation/accuracy': 0.7152599692344666, 'validation/loss': 1.1662580966949463, 'validation/num_examples': 50000, 'test/accuracy': 0.586400032043457, 'test/loss': 1.8914276361465454, 'test/num_examples': 10000, 'score': 41876.444503068924, 'total_duration': 43497.77945423126, 'accumulated_submission_time': 41876.444503068924, 'accumulated_eval_time': 1612.4037322998047, 'accumulated_logging_time': 4.06778359413147}
I0229 17:19:33.824158 140409846093568 logging_writer.py:48] [124095] accumulated_eval_time=1612.403732, accumulated_logging_time=4.067784, accumulated_submission_time=41876.444503, global_step=124095, preemption_count=0, score=41876.444503, test/accuracy=0.586400, test/loss=1.891428, test/num_examples=10000, total_duration=43497.779454, train/accuracy=0.821409, train/loss=0.644114, validation/accuracy=0.715260, validation/loss=1.166258, validation/num_examples=50000
I0229 17:19:35.860924 140409854486272 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.872682571411133, loss=1.1132980585098267
I0229 17:20:09.520066 140409846093568 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.0872817039489746, loss=1.2178130149841309
I0229 17:20:43.187432 140409854486272 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.2625696659088135, loss=1.2076822519302368
I0229 17:21:16.908883 140409846093568 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.099350690841675, loss=1.216897964477539
I0229 17:21:50.644567 140409854486272 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.07283878326416, loss=1.1061919927597046
I0229 17:22:24.355044 140409846093568 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.0292727947235107, loss=1.1041688919067383
I0229 17:22:58.038678 140409854486272 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.1790502071380615, loss=1.1861188411712646
I0229 17:23:31.778515 140409846093568 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.8602724075317383, loss=1.1649047136306763
I0229 17:24:05.508870 140409854486272 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.1812801361083984, loss=1.2273528575897217
I0229 17:24:39.213328 140409846093568 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.1886839866638184, loss=1.0669913291931152
I0229 17:25:12.959694 140409854486272 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.307480812072754, loss=1.1190086603164673
I0229 17:25:46.847247 140409846093568 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.4776010513305664, loss=1.2106478214263916
I0229 17:26:20.567894 140409854486272 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.2071752548217773, loss=1.1266353130340576
I0229 17:26:54.254875 140409846093568 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.0869619846343994, loss=1.1048307418823242
I0229 17:27:27.978096 140409854486272 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.817556858062744, loss=1.130295991897583
I0229 17:28:01.692672 140409846093568 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.1747984886169434, loss=1.2644774913787842
I0229 17:28:03.856600 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:28:09.968711 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:28:19.051327 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:28:21.317357 140573303715648 submission_runner.py:411] Time since start: 44025.31s, 	Step: 125608, 	{'train/accuracy': 0.8460817933082581, 'train/loss': 0.5478847026824951, 'validation/accuracy': 0.7125200033187866, 'validation/loss': 1.174476981163025, 'validation/num_examples': 50000, 'test/accuracy': 0.5817000269889832, 'test/loss': 1.9218659400939941, 'test/num_examples': 10000, 'score': 42386.406173706055, 'total_duration': 44025.309896469116, 'accumulated_submission_time': 42386.406173706055, 'accumulated_eval_time': 1629.8644435405731, 'accumulated_logging_time': 4.1161048412323}
I0229 17:28:21.360152 140409124681472 logging_writer.py:48] [125608] accumulated_eval_time=1629.864444, accumulated_logging_time=4.116105, accumulated_submission_time=42386.406174, global_step=125608, preemption_count=0, score=42386.406174, test/accuracy=0.581700, test/loss=1.921866, test/num_examples=10000, total_duration=44025.309896, train/accuracy=0.846082, train/loss=0.547885, validation/accuracy=0.712520, validation/loss=1.174477, validation/num_examples=50000
I0229 17:28:52.663282 140409829308160 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.9521756172180176, loss=1.118659257888794
I0229 17:29:26.300719 140409124681472 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.103980541229248, loss=1.2399101257324219
I0229 17:29:59.973989 140409829308160 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.488823413848877, loss=1.142162561416626
I0229 17:30:33.630655 140409124681472 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.0691299438476562, loss=1.1896272897720337
I0229 17:31:07.343110 140409829308160 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.143817901611328, loss=1.1272865533828735
I0229 17:31:41.166845 140409124681472 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.0756778717041016, loss=1.0855814218521118
I0229 17:32:14.856431 140409829308160 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.1681325435638428, loss=1.108826994895935
I0229 17:32:48.502225 140409124681472 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.2720212936401367, loss=1.144150972366333
I0229 17:33:22.223869 140409829308160 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.286848545074463, loss=1.1661171913146973
I0229 17:33:55.883264 140409124681472 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.1431901454925537, loss=1.1238572597503662
I0229 17:34:29.554842 140409829308160 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.2959444522857666, loss=1.070571780204773
I0229 17:35:03.261384 140409124681472 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.3759312629699707, loss=1.2090650796890259
I0229 17:35:36.963589 140409829308160 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.0362982749938965, loss=1.1176199913024902
I0229 17:36:10.666889 140409124681472 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.1393630504608154, loss=1.1560604572296143
I0229 17:36:44.385840 140409829308160 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.1032638549804688, loss=1.1837327480316162
I0229 17:36:51.596416 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:36:57.687419 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:37:06.855657 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:37:09.174726 140573303715648 submission_runner.py:411] Time since start: 44553.17s, 	Step: 127123, 	{'train/accuracy': 0.8364357352256775, 'train/loss': 0.592322587966919, 'validation/accuracy': 0.7131800055503845, 'validation/loss': 1.1688021421432495, 'validation/num_examples': 50000, 'test/accuracy': 0.5838000178337097, 'test/loss': 1.920723557472229, 'test/num_examples': 10000, 'score': 42896.569796323776, 'total_duration': 44553.16710352898, 'accumulated_submission_time': 42896.569796323776, 'accumulated_eval_time': 1647.4425375461578, 'accumulated_logging_time': 4.171696662902832}
I0229 17:37:09.216216 140409124681472 logging_writer.py:48] [127123] accumulated_eval_time=1647.442538, accumulated_logging_time=4.171697, accumulated_submission_time=42896.569796, global_step=127123, preemption_count=0, score=42896.569796, test/accuracy=0.583800, test/loss=1.920724, test/num_examples=10000, total_duration=44553.167104, train/accuracy=0.836436, train/loss=0.592323, validation/accuracy=0.713180, validation/loss=1.168802, validation/num_examples=50000
I0229 17:37:35.919857 140409846093568 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.3060882091522217, loss=1.1563136577606201
I0229 17:38:09.609302 140409124681472 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.957077741622925, loss=1.0806317329406738
I0229 17:38:43.278191 140409846093568 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.7896084785461426, loss=1.0312941074371338
I0229 17:39:16.912236 140409124681472 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.2798705101013184, loss=1.1856759786605835
I0229 17:39:50.556424 140409846093568 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.154696226119995, loss=1.1765871047973633
I0229 17:40:24.298097 140409124681472 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.0501904487609863, loss=1.0518836975097656
I0229 17:40:57.998994 140409846093568 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.2520411014556885, loss=1.1335349082946777
I0229 17:41:31.719563 140409124681472 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.1049370765686035, loss=1.109147310256958
I0229 17:42:05.407534 140409846093568 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.0582711696624756, loss=1.115388035774231
I0229 17:42:39.132520 140409124681472 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.3631486892700195, loss=1.1617121696472168
I0229 17:43:12.815381 140409846093568 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.1361758708953857, loss=1.140202283859253
I0229 17:43:46.624495 140409124681472 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.134946346282959, loss=1.0336586236953735
I0229 17:44:20.367725 140409846093568 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.9464786052703857, loss=1.0269145965576172
I0229 17:44:54.035364 140409124681472 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.1348516941070557, loss=1.183288335800171
I0229 17:45:27.776902 140409846093568 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.2702267169952393, loss=1.0734013319015503
I0229 17:45:39.380357 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:45:45.522620 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:45:54.595107 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:45:56.916300 140573303715648 submission_runner.py:411] Time since start: 45080.91s, 	Step: 128636, 	{'train/accuracy': 0.8303770422935486, 'train/loss': 0.6017466187477112, 'validation/accuracy': 0.7099599838256836, 'validation/loss': 1.1903725862503052, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.8865386247634888, 'test/num_examples': 10000, 'score': 43406.31649875641, 'total_duration': 45080.90883421898, 'accumulated_submission_time': 43406.31649875641, 'accumulated_eval_time': 1664.9784235954285, 'accumulated_logging_time': 4.570464134216309}
I0229 17:45:56.956484 140409124681472 logging_writer.py:48] [128636] accumulated_eval_time=1664.978424, accumulated_logging_time=4.570464, accumulated_submission_time=43406.316499, global_step=128636, preemption_count=0, score=43406.316499, test/accuracy=0.586800, test/loss=1.886539, test/num_examples=10000, total_duration=45080.908834, train/accuracy=0.830377, train/loss=0.601747, validation/accuracy=0.709960, validation/loss=1.190373, validation/num_examples=50000
I0229 17:46:18.860995 140409829308160 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.962822675704956, loss=1.0633341073989868
I0229 17:46:52.524809 140409124681472 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.0053112506866455, loss=1.124876856803894
I0229 17:47:26.189944 140409829308160 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.5242607593536377, loss=1.1498806476593018
I0229 17:47:59.924565 140409124681472 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.0605075359344482, loss=1.1228083372116089
I0229 17:48:33.612020 140409829308160 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.083901882171631, loss=1.0516023635864258
I0229 17:49:07.287687 140409124681472 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.065502166748047, loss=1.0087201595306396
I0229 17:49:40.959430 140409829308160 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.186091423034668, loss=1.1945888996124268
I0229 17:50:14.891180 140409124681472 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.528592109680176, loss=1.1942527294158936
I0229 17:50:48.596040 140409829308160 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.3256192207336426, loss=1.2142705917358398
I0229 17:51:22.332007 140409124681472 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.4075369834899902, loss=1.1260608434677124
I0229 17:51:56.018984 140409829308160 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.0750484466552734, loss=1.0615509748458862
I0229 17:52:29.731995 140409124681472 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.2538774013519287, loss=1.0701956748962402
I0229 17:53:03.379108 140409829308160 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.5672149658203125, loss=1.026748538017273
I0229 17:53:37.064181 140409124681472 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.6824746131896973, loss=1.1091804504394531
I0229 17:54:10.749681 140409829308160 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.7434802055358887, loss=1.1709845066070557
I0229 17:54:27.110761 140573303715648 spec.py:321] Evaluating on the training split.
I0229 17:54:33.198970 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 17:54:41.998579 140573303715648 spec.py:349] Evaluating on the test split.
I0229 17:54:44.312231 140573303715648 submission_runner.py:411] Time since start: 45608.30s, 	Step: 130150, 	{'train/accuracy': 0.8363161683082581, 'train/loss': 0.5795503854751587, 'validation/accuracy': 0.7214199900627136, 'validation/loss': 1.1396958827972412, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.8531285524368286, 'test/num_examples': 10000, 'score': 43916.40103673935, 'total_duration': 45608.30464339256, 'accumulated_submission_time': 43916.40103673935, 'accumulated_eval_time': 1682.1797287464142, 'accumulated_logging_time': 4.620298624038696}
I0229 17:54:44.352940 140407379851008 logging_writer.py:48] [130150] accumulated_eval_time=1682.179729, accumulated_logging_time=4.620299, accumulated_submission_time=43916.401037, global_step=130150, preemption_count=0, score=43916.401037, test/accuracy=0.594300, test/loss=1.853129, test/num_examples=10000, total_duration=45608.304643, train/accuracy=0.836316, train/loss=0.579550, validation/accuracy=0.721420, validation/loss=1.139696, validation/num_examples=50000
I0229 17:55:01.547566 140408319375104 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.0300378799438477, loss=1.016147494316101
I0229 17:55:35.230348 140407379851008 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.361104965209961, loss=1.1235510110855103
I0229 17:56:09.034496 140408319375104 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.5213608741760254, loss=1.1631261110305786
I0229 17:56:42.773210 140407379851008 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.200855016708374, loss=1.070888876914978
I0229 17:57:16.445050 140408319375104 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.2731590270996094, loss=1.062673807144165
I0229 17:57:50.180014 140407379851008 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.4203684329986572, loss=1.135067343711853
I0229 17:58:23.848049 140408319375104 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.198176383972168, loss=1.0858439207077026
I0229 17:58:57.619142 140407379851008 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.4872660636901855, loss=1.046830177307129
I0229 17:59:31.293549 140408319375104 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.6586451530456543, loss=1.0827816724777222
I0229 18:00:05.056352 140407379851008 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.320803642272949, loss=1.1287089586257935
I0229 18:00:38.771345 140408319375104 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.196962356567383, loss=1.0468273162841797
I0229 18:01:12.506266 140407379851008 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.0794341564178467, loss=1.090057134628296
I0229 18:01:46.224203 140408319375104 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.0595574378967285, loss=1.0398814678192139
I0229 18:02:19.964118 140407379851008 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.473127841949463, loss=1.1885132789611816
I0229 18:02:53.681515 140408319375104 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.4111456871032715, loss=1.1788004636764526
I0229 18:03:14.368081 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:03:20.435194 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:03:29.140721 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:03:31.461584 140573303715648 submission_runner.py:411] Time since start: 46135.45s, 	Step: 131663, 	{'train/accuracy': 0.84086012840271, 'train/loss': 0.5709583163261414, 'validation/accuracy': 0.7206400036811829, 'validation/loss': 1.1557066440582275, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.8866512775421143, 'test/num_examples': 10000, 'score': 44426.34621119499, 'total_duration': 46135.45411705971, 'accumulated_submission_time': 44426.34621119499, 'accumulated_eval_time': 1699.2731821537018, 'accumulated_logging_time': 4.671350479125977}
I0229 18:03:31.512017 140409829308160 logging_writer.py:48] [131663] accumulated_eval_time=1699.273182, accumulated_logging_time=4.671350, accumulated_submission_time=44426.346211, global_step=131663, preemption_count=0, score=44426.346211, test/accuracy=0.595800, test/loss=1.886651, test/num_examples=10000, total_duration=46135.454117, train/accuracy=0.840860, train/loss=0.570958, validation/accuracy=0.720640, validation/loss=1.155707, validation/num_examples=50000
I0229 18:03:44.318433 140409837700864 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.2685325145721436, loss=1.049897313117981
I0229 18:04:18.004088 140409829308160 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.29339599609375, loss=1.0596733093261719
I0229 18:04:51.710499 140409837700864 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.4529361724853516, loss=1.1243290901184082
I0229 18:05:25.330270 140409829308160 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.991262197494507, loss=1.001591682434082
I0229 18:05:59.018263 140409837700864 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.2509946823120117, loss=1.0726158618927002
I0229 18:06:32.728144 140409829308160 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.305368423461914, loss=1.0737590789794922
I0229 18:07:06.449872 140409837700864 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.2675182819366455, loss=1.0666170120239258
I0229 18:07:40.163914 140409829308160 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.263157606124878, loss=1.048211693763733
I0229 18:08:13.963878 140409837700864 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.358365535736084, loss=1.1498253345489502
I0229 18:08:47.683632 140409829308160 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.303044319152832, loss=1.0519721508026123
I0229 18:09:21.355394 140409837700864 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.3290765285491943, loss=1.0247724056243896
I0229 18:09:55.094888 140409829308160 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.4487338066101074, loss=1.0410888195037842
I0229 18:10:28.779918 140409837700864 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.286330461502075, loss=0.9947305917739868
I0229 18:11:02.506457 140409829308160 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.237340211868286, loss=1.007278561592102
I0229 18:11:36.249197 140409837700864 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.4381935596466064, loss=1.0896079540252686
I0229 18:12:01.625967 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:12:07.978885 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:12:16.812143 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:12:19.113195 140573303715648 submission_runner.py:411] Time since start: 46663.11s, 	Step: 133177, 	{'train/accuracy': 0.8496691584587097, 'train/loss': 0.540483295917511, 'validation/accuracy': 0.7239800095558167, 'validation/loss': 1.138985514640808, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.8570551872253418, 'test/num_examples': 10000, 'score': 44936.3878133297, 'total_duration': 46663.10573005676, 'accumulated_submission_time': 44936.3878133297, 'accumulated_eval_time': 1716.7603754997253, 'accumulated_logging_time': 4.733880996704102}
I0229 18:12:19.154072 140408319375104 logging_writer.py:48] [133177] accumulated_eval_time=1716.760375, accumulated_logging_time=4.733881, accumulated_submission_time=44936.387813, global_step=133177, preemption_count=0, score=44936.387813, test/accuracy=0.594100, test/loss=1.857055, test/num_examples=10000, total_duration=46663.105730, train/accuracy=0.849669, train/loss=0.540483, validation/accuracy=0.723980, validation/loss=1.138986, validation/num_examples=50000
I0229 18:12:27.219841 140409124681472 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.1078081130981445, loss=1.0064300298690796
I0229 18:13:00.874707 140408319375104 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.368532657623291, loss=1.0923043489456177
I0229 18:13:34.642566 140409124681472 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.7515366077423096, loss=1.0013490915298462
I0229 18:14:08.303386 140408319375104 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.66103458404541, loss=1.023969054222107
I0229 18:14:42.080239 140409124681472 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.4605965614318848, loss=1.0721644163131714
I0229 18:15:15.760756 140408319375104 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.334033966064453, loss=1.0446282625198364
I0229 18:15:49.497950 140409124681472 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.1881017684936523, loss=1.0394413471221924
I0229 18:16:23.238610 140408319375104 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.2700307369232178, loss=1.0379674434661865
I0229 18:16:56.930790 140409124681472 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.4464304447174072, loss=1.0311706066131592
I0229 18:17:30.627052 140408319375104 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.455052137374878, loss=1.089011311531067
I0229 18:18:04.383646 140409124681472 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.2351441383361816, loss=0.9524033069610596
I0229 18:18:38.107013 140408319375104 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.589895009994507, loss=1.117339849472046
I0229 18:19:11.806654 140409124681472 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.6912198066711426, loss=1.2012661695480347
I0229 18:19:45.538888 140408319375104 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.2155470848083496, loss=1.0668683052062988
I0229 18:20:19.224613 140409124681472 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.6240015029907227, loss=1.1436223983764648
I0229 18:20:49.448233 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:20:55.622620 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:21:04.746471 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:21:07.031206 140573303715648 submission_runner.py:411] Time since start: 47191.02s, 	Step: 134691, 	{'train/accuracy': 0.865652859210968, 'train/loss': 0.4808640778064728, 'validation/accuracy': 0.7212600111961365, 'validation/loss': 1.1499614715576172, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.8905266523361206, 'test/num_examples': 10000, 'score': 45446.60967588425, 'total_duration': 47191.0237467289, 'accumulated_submission_time': 45446.60967588425, 'accumulated_eval_time': 1734.3433163166046, 'accumulated_logging_time': 4.786406517028809}
I0229 18:21:07.072450 140407379851008 logging_writer.py:48] [134691] accumulated_eval_time=1734.343316, accumulated_logging_time=4.786407, accumulated_submission_time=45446.609676, global_step=134691, preemption_count=0, score=45446.609676, test/accuracy=0.589700, test/loss=1.890527, test/num_examples=10000, total_duration=47191.023747, train/accuracy=0.865653, train/loss=0.480864, validation/accuracy=0.721260, validation/loss=1.149961, validation/num_examples=50000
I0229 18:21:10.459364 140409837700864 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.2944443225860596, loss=1.0156489610671997
I0229 18:21:44.123167 140407379851008 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.4399917125701904, loss=1.0379751920700073
I0229 18:22:17.781445 140409837700864 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.451779842376709, loss=0.9609411954879761
I0229 18:22:51.492401 140407379851008 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.3588099479675293, loss=1.0810470581054688
I0229 18:23:25.160490 140409837700864 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.48419189453125, loss=1.115218997001648
I0229 18:23:58.842676 140407379851008 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.3716583251953125, loss=1.0543969869613647
I0229 18:24:32.570146 140409837700864 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.488323211669922, loss=1.13728928565979
I0229 18:25:06.274333 140407379851008 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.9469475746154785, loss=1.1324745416641235
I0229 18:25:39.972492 140409837700864 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.244680166244507, loss=1.1332862377166748
I0229 18:26:13.695956 140407379851008 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.310382843017578, loss=0.9679579138755798
I0229 18:26:47.473684 140409837700864 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.4188013076782227, loss=1.0282320976257324
I0229 18:27:21.177321 140407379851008 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.2162435054779053, loss=0.984584391117096
I0229 18:27:54.830536 140409837700864 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.400409460067749, loss=1.031598687171936
I0229 18:28:28.539280 140407379851008 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.383183717727661, loss=1.0336065292358398
I0229 18:29:02.205329 140409837700864 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.169571876525879, loss=1.0530842542648315
I0229 18:29:35.871726 140407379851008 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.2736001014709473, loss=0.9685028195381165
I0229 18:29:37.037946 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:29:43.085094 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:29:51.826046 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:29:54.148841 140573303715648 submission_runner.py:411] Time since start: 47718.14s, 	Step: 136205, 	{'train/accuracy': 0.8681640625, 'train/loss': 0.465312123298645, 'validation/accuracy': 0.726859986782074, 'validation/loss': 1.1234965324401855, 'validation/num_examples': 50000, 'test/accuracy': 0.6017000079154968, 'test/loss': 1.838789939880371, 'test/num_examples': 10000, 'score': 45956.50417351723, 'total_duration': 47718.14137125015, 'accumulated_submission_time': 45956.50417351723, 'accumulated_eval_time': 1751.4541499614716, 'accumulated_logging_time': 4.838630199432373}
I0229 18:29:54.189631 140409829308160 logging_writer.py:48] [136205] accumulated_eval_time=1751.454150, accumulated_logging_time=4.838630, accumulated_submission_time=45956.504174, global_step=136205, preemption_count=0, score=45956.504174, test/accuracy=0.601700, test/loss=1.838790, test/num_examples=10000, total_duration=47718.141371, train/accuracy=0.868164, train/loss=0.465312, validation/accuracy=0.726860, validation/loss=1.123497, validation/num_examples=50000
I0229 18:30:26.597232 140409846093568 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.619978904724121, loss=1.0432460308074951
I0229 18:31:00.253536 140409829308160 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.532945394515991, loss=1.1396397352218628
I0229 18:31:34.008374 140409846093568 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.4888417720794678, loss=0.9961692690849304
I0229 18:32:07.710378 140409829308160 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.442667007446289, loss=1.037771463394165
I0229 18:32:41.505865 140409846093568 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.586214780807495, loss=1.1207072734832764
I0229 18:33:15.232459 140409829308160 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.5437633991241455, loss=1.0718841552734375
I0229 18:33:48.878442 140409846093568 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.4348134994506836, loss=1.021856427192688
I0229 18:34:22.606281 140409829308160 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.432810068130493, loss=0.9912602305412292
I0229 18:34:56.280090 140409846093568 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.315105676651001, loss=1.003715991973877
I0229 18:35:29.986714 140409829308160 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.865819215774536, loss=1.0610193014144897
I0229 18:36:03.741777 140409846093568 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.794114589691162, loss=1.045698881149292
I0229 18:36:37.462919 140409829308160 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.2687928676605225, loss=0.9591845870018005
I0229 18:37:11.158712 140409846093568 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.8827521800994873, loss=0.9734585285186768
I0229 18:37:44.837666 140409829308160 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.488734483718872, loss=0.9953535795211792
I0229 18:38:18.593635 140409846093568 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.7915232181549072, loss=0.9840260148048401
I0229 18:38:24.461021 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:38:30.603690 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:38:39.219475 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:38:41.521905 140573303715648 submission_runner.py:411] Time since start: 48245.51s, 	Step: 137719, 	{'train/accuracy': 0.8539540767669678, 'train/loss': 0.5145111083984375, 'validation/accuracy': 0.7205399870872498, 'validation/loss': 1.144445538520813, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.8979063034057617, 'test/num_examples': 10000, 'score': 46466.704859256744, 'total_duration': 48245.51445031166, 'accumulated_submission_time': 46466.704859256744, 'accumulated_eval_time': 1768.5149869918823, 'accumulated_logging_time': 4.89011025428772}
I0229 18:38:41.560928 140409124681472 logging_writer.py:48] [137719] accumulated_eval_time=1768.514987, accumulated_logging_time=4.890110, accumulated_submission_time=46466.704859, global_step=137719, preemption_count=0, score=46466.704859, test/accuracy=0.590800, test/loss=1.897906, test/num_examples=10000, total_duration=48245.514450, train/accuracy=0.853954, train/loss=0.514511, validation/accuracy=0.720540, validation/loss=1.144446, validation/num_examples=50000
I0229 18:39:09.211638 140409837700864 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.6676905155181885, loss=1.0883194208145142
I0229 18:39:42.901407 140409124681472 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.579970359802246, loss=0.9761667251586914
I0229 18:40:16.577340 140409837700864 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.482855796813965, loss=1.0581698417663574
I0229 18:40:50.237913 140409124681472 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.340985059738159, loss=0.9850225448608398
I0229 18:41:24.001939 140409837700864 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.6055309772491455, loss=0.9839690327644348
I0229 18:41:57.675540 140409124681472 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.758202314376831, loss=0.9972882270812988
I0229 18:42:31.302279 140409837700864 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.960714340209961, loss=1.0526291131973267
I0229 18:43:04.955845 140409124681472 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.6682746410369873, loss=1.0244125127792358
I0229 18:43:38.613401 140409837700864 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.4884917736053467, loss=1.0432380437850952
I0229 18:44:12.316045 140409124681472 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.7608017921447754, loss=1.0848612785339355
I0229 18:44:45.976894 140409837700864 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.9729065895080566, loss=1.0054601430892944
I0229 18:45:19.773217 140409124681472 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.4858570098876953, loss=0.9758205413818359
I0229 18:45:53.442583 140409837700864 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.7286293506622314, loss=1.018059253692627
I0229 18:46:27.083603 140409124681472 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.4188923835754395, loss=0.9438563585281372
I0229 18:47:00.744260 140409837700864 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.2827460765838623, loss=0.9663358330726624
I0229 18:47:11.655130 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:47:17.792913 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:47:26.656960 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:47:28.965717 140573303715648 submission_runner.py:411] Time since start: 48772.96s, 	Step: 139234, 	{'train/accuracy': 0.8557477593421936, 'train/loss': 0.4996830224990845, 'validation/accuracy': 0.7240999937057495, 'validation/loss': 1.1292551755905151, 'validation/num_examples': 50000, 'test/accuracy': 0.5954000353813171, 'test/loss': 1.8895883560180664, 'test/num_examples': 10000, 'score': 46976.7206556797, 'total_duration': 48772.95825433731, 'accumulated_submission_time': 46976.7206556797, 'accumulated_eval_time': 1785.825518131256, 'accumulated_logging_time': 4.946522235870361}
I0229 18:47:29.006952 140407379851008 logging_writer.py:48] [139234] accumulated_eval_time=1785.825518, accumulated_logging_time=4.946522, accumulated_submission_time=46976.720656, global_step=139234, preemption_count=0, score=46976.720656, test/accuracy=0.595400, test/loss=1.889588, test/num_examples=10000, total_duration=48772.958254, train/accuracy=0.855748, train/loss=0.499683, validation/accuracy=0.724100, validation/loss=1.129255, validation/num_examples=50000
I0229 18:47:51.601992 140409829308160 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.74544620513916, loss=1.0896542072296143
I0229 18:48:25.348416 140407379851008 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.447542190551758, loss=0.9741635918617249
I0229 18:48:59.057205 140409829308160 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.5470800399780273, loss=0.9472849369049072
I0229 18:49:32.810443 140407379851008 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.080279588699341, loss=0.8940379619598389
I0229 18:50:06.502578 140409829308160 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.510171413421631, loss=0.9421473741531372
I0229 18:50:40.249899 140407379851008 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.2978222370147705, loss=0.8786624073982239
I0229 18:51:14.104018 140409829308160 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.56677508354187, loss=0.9778965711593628
I0229 18:51:47.855582 140407379851008 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.448840856552124, loss=0.9569422006607056
I0229 18:52:21.526421 140409829308160 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.457151412963867, loss=1.010763168334961
I0229 18:52:55.283814 140407379851008 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.5491209030151367, loss=0.960713803768158
I0229 18:53:28.964351 140409829308160 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.8255770206451416, loss=1.0531331300735474
I0229 18:54:02.694200 140407379851008 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.725783586502075, loss=0.9333313703536987
I0229 18:54:36.387698 140409829308160 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.58007550239563, loss=1.0082623958587646
I0229 18:55:10.109393 140407379851008 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.3998608589172363, loss=0.909339189529419
I0229 18:55:43.772045 140409829308160 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.620053768157959, loss=0.9900624752044678
I0229 18:55:59.053826 140573303715648 spec.py:321] Evaluating on the training split.
I0229 18:56:05.326800 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 18:56:14.046419 140573303715648 spec.py:349] Evaluating on the test split.
I0229 18:56:16.334311 140573303715648 submission_runner.py:411] Time since start: 49300.33s, 	Step: 140747, 	{'train/accuracy': 0.8660913109779358, 'train/loss': 0.4651607573032379, 'validation/accuracy': 0.728659987449646, 'validation/loss': 1.1148227453231812, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.8393598794937134, 'test/num_examples': 10000, 'score': 47486.69790649414, 'total_duration': 49300.32685422897, 'accumulated_submission_time': 47486.69790649414, 'accumulated_eval_time': 1803.1059713363647, 'accumulated_logging_time': 4.9977428913116455}
I0229 18:56:16.379033 140409124681472 logging_writer.py:48] [140747] accumulated_eval_time=1803.105971, accumulated_logging_time=4.997743, accumulated_submission_time=47486.697906, global_step=140747, preemption_count=0, score=47486.697906, test/accuracy=0.603700, test/loss=1.839360, test/num_examples=10000, total_duration=49300.326854, train/accuracy=0.866091, train/loss=0.465161, validation/accuracy=0.728660, validation/loss=1.114823, validation/num_examples=50000
I0229 18:56:34.542275 140409837700864 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.715021848678589, loss=1.083245873451233
I0229 18:57:08.335418 140409124681472 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.4229819774627686, loss=0.9670909643173218
I0229 18:57:41.933889 140409837700864 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.538602828979492, loss=1.028298020362854
I0229 18:58:15.653740 140409124681472 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.536440372467041, loss=0.9862726926803589
I0229 18:58:49.336812 140409837700864 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.5454161167144775, loss=0.9251716136932373
I0229 18:59:22.978861 140409124681472 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.5585925579071045, loss=0.9975465536117554
I0229 18:59:56.685641 140409837700864 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.5532314777374268, loss=0.928260326385498
I0229 19:00:30.344148 140409124681472 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.6159346103668213, loss=0.9603428244590759
I0229 19:01:03.972536 140409837700864 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.882549285888672, loss=1.0474716424942017
I0229 19:01:37.608517 140409124681472 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.6757514476776123, loss=0.9005005955696106
I0229 19:02:11.244337 140409837700864 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.8295021057128906, loss=0.9901901483535767
I0229 19:02:44.972429 140409124681472 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.448479652404785, loss=0.9872245192527771
I0229 19:03:18.810528 140409837700864 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.899898052215576, loss=1.0252723693847656
I0229 19:03:52.560707 140409124681472 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.5439507961273193, loss=0.9061157703399658
I0229 19:04:26.245421 140409837700864 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.769423723220825, loss=0.9642528295516968
I0229 19:04:46.563280 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:04:52.670624 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:05:01.286973 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:05:03.765779 140573303715648 submission_runner.py:411] Time since start: 49827.76s, 	Step: 142262, 	{'train/accuracy': 0.8884326815605164, 'train/loss': 0.3945820927619934, 'validation/accuracy': 0.7259399890899658, 'validation/loss': 1.1340726613998413, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.8927258253097534, 'test/num_examples': 10000, 'score': 47996.81061291695, 'total_duration': 49827.75827026367, 'accumulated_submission_time': 47996.81061291695, 'accumulated_eval_time': 1820.3083732128143, 'accumulated_logging_time': 5.054003477096558}
I0229 19:05:03.815540 140409124681472 logging_writer.py:48] [142262] accumulated_eval_time=1820.308373, accumulated_logging_time=5.054003, accumulated_submission_time=47996.810613, global_step=142262, preemption_count=0, score=47996.810613, test/accuracy=0.594100, test/loss=1.892726, test/num_examples=10000, total_duration=49827.758270, train/accuracy=0.888433, train/loss=0.394582, validation/accuracy=0.725940, validation/loss=1.134073, validation/num_examples=50000
I0229 19:05:16.991428 140409829308160 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.570838451385498, loss=0.8760074973106384
I0229 19:05:50.630085 140409124681472 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.5185599327087402, loss=0.9250534772872925
I0229 19:06:24.275068 140409829308160 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.822732448577881, loss=0.9108107686042786
I0229 19:06:58.004398 140409124681472 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.9001760482788086, loss=0.9272353649139404
I0229 19:07:31.701994 140409829308160 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.5944983959198, loss=0.9549921751022339
I0229 19:08:05.430684 140409124681472 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.8839213848114014, loss=0.926347017288208
I0229 19:08:39.119457 140409829308160 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.5751326084136963, loss=0.8854482173919678
I0229 19:09:12.840047 140409124681472 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.7647218704223633, loss=0.987891674041748
I0229 19:09:46.684347 140409829308160 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.9165217876434326, loss=1.0275790691375732
I0229 19:10:20.376183 140409124681472 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.973907709121704, loss=1.0190945863723755
I0229 19:10:54.093594 140409829308160 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.7186408042907715, loss=0.9149274826049805
I0229 19:11:27.789937 140409124681472 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.6240131855010986, loss=0.9751595258712769
I0229 19:12:01.516382 140409829308160 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.5032899379730225, loss=0.8822510838508606
I0229 19:12:35.169948 140409124681472 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.558621406555176, loss=0.8898153305053711
I0229 19:13:08.903214 140409829308160 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.6040637493133545, loss=0.9813193082809448
I0229 19:13:33.949075 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:13:40.059562 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:13:48.930752 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:13:51.203684 140573303715648 submission_runner.py:411] Time since start: 50355.20s, 	Step: 143776, 	{'train/accuracy': 0.8937739133834839, 'train/loss': 0.3656793534755707, 'validation/accuracy': 0.7310000061988831, 'validation/loss': 1.1196391582489014, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.8732010126113892, 'test/num_examples': 10000, 'score': 48506.87425112724, 'total_duration': 50355.19622254372, 'accumulated_submission_time': 48506.87425112724, 'accumulated_eval_time': 1837.5629630088806, 'accumulated_logging_time': 5.1136510372161865}
I0229 19:13:51.256742 140408319375104 logging_writer.py:48] [143776] accumulated_eval_time=1837.562963, accumulated_logging_time=5.113651, accumulated_submission_time=48506.874251, global_step=143776, preemption_count=0, score=48506.874251, test/accuracy=0.605300, test/loss=1.873201, test/num_examples=10000, total_duration=50355.196223, train/accuracy=0.893774, train/loss=0.365679, validation/accuracy=0.731000, validation/loss=1.119639, validation/num_examples=50000
I0229 19:13:59.710077 140409124681472 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.907540798187256, loss=1.0249123573303223
I0229 19:14:33.361867 140408319375104 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.8502824306488037, loss=0.9905380010604858
I0229 19:15:06.986161 140409124681472 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.716521739959717, loss=0.9571425914764404
I0229 19:15:40.743534 140408319375104 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.5785908699035645, loss=0.8444572687149048
I0229 19:16:14.425313 140409124681472 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.7002296447753906, loss=0.9242822527885437
I0229 19:16:48.085906 140408319375104 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.521207809448242, loss=0.9398645758628845
I0229 19:17:21.739902 140409124681472 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.7344582080841064, loss=1.0601916313171387
I0229 19:17:55.387973 140408319375104 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.5729169845581055, loss=0.9830892086029053
I0229 19:18:29.080069 140409124681472 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.415663242340088, loss=0.8207446932792664
I0229 19:19:02.771191 140408319375104 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.5043487548828125, loss=0.98816978931427
I0229 19:19:36.427712 140409124681472 logging_writer.py:48] [144800] global_step=144800, grad_norm=4.103719711303711, loss=0.9637434482574463
I0229 19:20:10.076582 140408319375104 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.921675205230713, loss=0.99349445104599
I0229 19:20:43.760953 140409124681472 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.922122001647949, loss=0.956910252571106
I0229 19:21:17.414890 140408319375104 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.7339401245117188, loss=0.8748793005943298
I0229 19:21:51.198526 140409124681472 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.7625784873962402, loss=0.9543164968490601
I0229 19:22:21.280086 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:22:27.523758 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:22:36.474001 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:22:38.784566 140573303715648 submission_runner.py:411] Time since start: 50882.78s, 	Step: 145291, 	{'train/accuracy': 0.8908242583274841, 'train/loss': 0.38063666224479675, 'validation/accuracy': 0.7371199727058411, 'validation/loss': 1.0951299667358398, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.8298053741455078, 'test/num_examples': 10000, 'score': 49016.82561182976, 'total_duration': 50882.77686429024, 'accumulated_submission_time': 49016.82561182976, 'accumulated_eval_time': 1855.067155122757, 'accumulated_logging_time': 5.179184913635254}
I0229 19:22:38.828128 140409124681472 logging_writer.py:48] [145291] accumulated_eval_time=1855.067155, accumulated_logging_time=5.179185, accumulated_submission_time=49016.825612, global_step=145291, preemption_count=0, score=49016.825612, test/accuracy=0.606700, test/loss=1.829805, test/num_examples=10000, total_duration=50882.776864, train/accuracy=0.890824, train/loss=0.380637, validation/accuracy=0.737120, validation/loss=1.095130, validation/num_examples=50000
I0229 19:22:42.218742 140409829308160 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.533262014389038, loss=0.9531967043876648
I0229 19:23:15.910628 140409124681472 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.969204902648926, loss=0.9888112545013428
I0229 19:23:49.584654 140409829308160 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.804142475128174, loss=0.9352626204490662
I0229 19:24:23.329723 140409124681472 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.7443649768829346, loss=0.9252867698669434
I0229 19:24:57.063255 140409829308160 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.7790915966033936, loss=1.0156362056732178
I0229 19:25:30.769292 140409124681472 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.8490149974823, loss=0.9399729371070862
I0229 19:26:04.516268 140409829308160 logging_writer.py:48] [145900] global_step=145900, grad_norm=4.036446571350098, loss=0.9655486345291138
I0229 19:26:38.222000 140409124681472 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.664641857147217, loss=0.9115803241729736
I0229 19:27:11.974771 140409829308160 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.9810991287231445, loss=0.9536232948303223
I0229 19:27:45.790681 140409124681472 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.8111586570739746, loss=0.8774663805961609
I0229 19:28:19.491539 140409829308160 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.77447247505188, loss=0.8502703905105591
I0229 19:28:53.203272 140409124681472 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.8385891914367676, loss=0.9662405252456665
I0229 19:29:26.906875 140409829308160 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.7598135471343994, loss=0.8580957651138306
I0229 19:30:00.590964 140409124681472 logging_writer.py:48] [146600] global_step=146600, grad_norm=4.027505874633789, loss=0.875549852848053
I0229 19:30:34.317427 140409829308160 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.830127477645874, loss=0.9154335260391235
I0229 19:31:08.019701 140409124681472 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.699052333831787, loss=0.8160474896430969
I0229 19:31:08.838310 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:31:14.898130 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:31:23.589590 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:31:25.863644 140573303715648 submission_runner.py:411] Time since start: 51409.86s, 	Step: 146804, 	{'train/accuracy': 0.8893893361091614, 'train/loss': 0.38604670763015747, 'validation/accuracy': 0.7328799962997437, 'validation/loss': 1.1125147342681885, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.860052227973938, 'test/num_examples': 10000, 'score': 49526.766122579575, 'total_duration': 51409.856170892715, 'accumulated_submission_time': 49526.766122579575, 'accumulated_eval_time': 1872.0924212932587, 'accumulated_logging_time': 5.233065366744995}
I0229 19:31:25.904058 140408319375104 logging_writer.py:48] [146804] accumulated_eval_time=1872.092421, accumulated_logging_time=5.233065, accumulated_submission_time=49526.766123, global_step=146804, preemption_count=0, score=49526.766123, test/accuracy=0.605100, test/loss=1.860052, test/num_examples=10000, total_duration=51409.856171, train/accuracy=0.889389, train/loss=0.386047, validation/accuracy=0.732880, validation/loss=1.112515, validation/num_examples=50000
I0229 19:31:58.583010 140409846093568 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.8208115100860596, loss=0.9179165363311768
I0229 19:32:32.220916 140408319375104 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.8030080795288086, loss=0.892256498336792
I0229 19:33:05.881182 140409846093568 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.414889335632324, loss=0.7702804803848267
I0229 19:33:39.523773 140408319375104 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.7944726943969727, loss=0.9358325004577637
I0229 19:34:13.242980 140409846093568 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.997140645980835, loss=0.8959679007530212
I0229 19:34:46.925417 140408319375104 logging_writer.py:48] [147400] global_step=147400, grad_norm=4.190254211425781, loss=1.0294179916381836
I0229 19:35:20.624650 140409846093568 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.353707313537598, loss=0.8785051107406616
I0229 19:35:54.252253 140408319375104 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.893221139907837, loss=0.9157838821411133
I0229 19:36:27.903403 140409846093568 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.7401111125946045, loss=0.9160952568054199
I0229 19:37:01.576562 140408319375104 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.894318103790283, loss=0.896937370300293
I0229 19:37:35.334129 140409846093568 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.4689886569976807, loss=0.8648350238800049
I0229 19:38:08.987154 140408319375104 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.708367347717285, loss=0.8777509927749634
I0229 19:38:42.728379 140409846093568 logging_writer.py:48] [148100] global_step=148100, grad_norm=4.1031999588012695, loss=0.8131829500198364
I0229 19:39:16.371395 140408319375104 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.977811098098755, loss=0.8841732144355774
I0229 19:39:50.034963 140409846093568 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.8820784091949463, loss=0.9327117800712585
I0229 19:39:55.978880 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:40:02.290128 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:40:11.024151 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:40:13.369240 140573303715648 submission_runner.py:411] Time since start: 51937.36s, 	Step: 148319, 	{'train/accuracy': 0.8917610049247742, 'train/loss': 0.3722779452800751, 'validation/accuracy': 0.7381599545478821, 'validation/loss': 1.0851082801818848, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.8237643241882324, 'test/num_examples': 10000, 'score': 50036.77022147179, 'total_duration': 51937.361750125885, 'accumulated_submission_time': 50036.77022147179, 'accumulated_eval_time': 1889.4827008247375, 'accumulated_logging_time': 5.284041404724121}
I0229 19:40:13.411484 140407379851008 logging_writer.py:48] [148319] accumulated_eval_time=1889.482701, accumulated_logging_time=5.284041, accumulated_submission_time=50036.770221, global_step=148319, preemption_count=0, score=50036.770221, test/accuracy=0.612500, test/loss=1.823764, test/num_examples=10000, total_duration=51937.361750, train/accuracy=0.891761, train/loss=0.372278, validation/accuracy=0.738160, validation/loss=1.085108, validation/num_examples=50000
I0229 19:40:41.083046 140408319375104 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.886244535446167, loss=0.8693162798881531
I0229 19:41:14.778761 140407379851008 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.724632501602173, loss=0.88285231590271
I0229 19:41:48.520746 140408319375104 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.799731731414795, loss=0.8863732218742371
I0229 19:42:22.228712 140407379851008 logging_writer.py:48] [148700] global_step=148700, grad_norm=4.058838844299316, loss=0.8755013346672058
I0229 19:42:55.975689 140408319375104 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.032425880432129, loss=0.8845362663269043
I0229 19:43:29.664503 140407379851008 logging_writer.py:48] [148900] global_step=148900, grad_norm=4.063644886016846, loss=0.9041749238967896
I0229 19:44:03.415740 140408319375104 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.063761234283447, loss=0.9028212428092957
I0229 19:44:37.122756 140407379851008 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.977926015853882, loss=0.9584933519363403
I0229 19:45:10.806691 140408319375104 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.930209159851074, loss=0.7626755833625793
I0229 19:45:44.477607 140407379851008 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.723132848739624, loss=0.8903820514678955
I0229 19:46:18.230368 140408319375104 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.7114803791046143, loss=0.8140251636505127
I0229 19:46:51.957313 140407379851008 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.9562861919403076, loss=0.9434563517570496
I0229 19:47:25.630627 140408319375104 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7844815254211426, loss=0.903900146484375
I0229 19:47:59.313261 140407379851008 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.7760133743286133, loss=0.8680840134620667
I0229 19:48:33.030491 140408319375104 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.868163585662842, loss=0.8498555421829224
I0229 19:48:43.612170 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:48:49.733967 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:48:58.680562 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:49:00.974626 140573303715648 submission_runner.py:411] Time since start: 52464.97s, 	Step: 149833, 	{'train/accuracy': 0.8892498016357422, 'train/loss': 0.37385424971580505, 'validation/accuracy': 0.7322799563407898, 'validation/loss': 1.1266218423843384, 'validation/num_examples': 50000, 'test/accuracy': 0.6081000566482544, 'test/loss': 1.8784505128860474, 'test/num_examples': 10000, 'score': 50546.90058851242, 'total_duration': 52464.96715068817, 'accumulated_submission_time': 50546.90058851242, 'accumulated_eval_time': 1906.8450901508331, 'accumulated_logging_time': 5.336570739746094}
I0229 19:49:01.016375 140409124681472 logging_writer.py:48] [149833] accumulated_eval_time=1906.845090, accumulated_logging_time=5.336571, accumulated_submission_time=50546.900589, global_step=149833, preemption_count=0, score=50546.900589, test/accuracy=0.608100, test/loss=1.878451, test/num_examples=10000, total_duration=52464.967151, train/accuracy=0.889250, train/loss=0.373854, validation/accuracy=0.732280, validation/loss=1.126622, validation/num_examples=50000
I0229 19:49:23.956818 140409837700864 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.693192481994629, loss=0.8391227126121521
I0229 19:49:57.618634 140409124681472 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.784695863723755, loss=0.8350996971130371
I0229 19:50:31.272823 140409837700864 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.76849365234375, loss=0.829715371131897
I0229 19:51:04.979392 140409124681472 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.0110650062561035, loss=0.8407410383224487
I0229 19:51:38.661750 140409837700864 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.8542778491973877, loss=0.8950371742248535
I0229 19:52:12.506947 140409124681472 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.895493268966675, loss=0.9201086163520813
I0229 19:52:46.221863 140409837700864 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.162460803985596, loss=0.8619680404663086
I0229 19:53:19.898111 140409124681472 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.6672728061676025, loss=0.7878375053405762
I0229 19:53:53.563706 140409837700864 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.169938564300537, loss=0.9581460356712341
I0229 19:54:27.256298 140409124681472 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.018804550170898, loss=0.873540997505188
I0229 19:55:00.912338 140409837700864 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.026424407958984, loss=0.9718706011772156
I0229 19:55:34.608588 140409124681472 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.895190954208374, loss=0.8367460370063782
I0229 19:56:08.258542 140409837700864 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.7710108757019043, loss=0.8644315004348755
I0229 19:56:41.971367 140409124681472 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.6907618045806885, loss=0.8355075120925903
I0229 19:57:15.639534 140409837700864 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.95027756690979, loss=0.8947086930274963
I0229 19:57:31.276830 140573303715648 spec.py:321] Evaluating on the training split.
I0229 19:57:37.388128 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 19:57:46.035178 140573303715648 spec.py:349] Evaluating on the test split.
I0229 19:57:48.354629 140573303715648 submission_runner.py:411] Time since start: 52992.35s, 	Step: 151348, 	{'train/accuracy': 0.9268175959587097, 'train/loss': 0.26678797602653503, 'validation/accuracy': 0.7402799725532532, 'validation/loss': 1.0880799293518066, 'validation/num_examples': 50000, 'test/accuracy': 0.6128000020980835, 'test/loss': 1.8366625308990479, 'test/num_examples': 10000, 'score': 51057.0911295414, 'total_duration': 52992.34717011452, 'accumulated_submission_time': 51057.0911295414, 'accumulated_eval_time': 1923.9228394031525, 'accumulated_logging_time': 5.388533115386963}
I0229 19:57:48.401629 140407379851008 logging_writer.py:48] [151348] accumulated_eval_time=1923.922839, accumulated_logging_time=5.388533, accumulated_submission_time=51057.091130, global_step=151348, preemption_count=0, score=51057.091130, test/accuracy=0.612800, test/loss=1.836663, test/num_examples=10000, total_duration=52992.347170, train/accuracy=0.926818, train/loss=0.266788, validation/accuracy=0.740280, validation/loss=1.088080, validation/num_examples=50000
I0229 19:58:06.232980 140408319375104 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.9201066493988037, loss=0.8735341429710388
I0229 19:58:40.059828 140407379851008 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.1045427322387695, loss=0.8138388395309448
I0229 19:59:13.714161 140408319375104 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.972045421600342, loss=0.775105357170105
I0229 19:59:47.461743 140407379851008 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.931863784790039, loss=0.8232882022857666
I0229 20:00:21.168560 140408319375104 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.7086570262908936, loss=0.7921960949897766
I0229 20:00:54.900181 140407379851008 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.816868543624878, loss=0.8699300289154053
I0229 20:01:28.581275 140408319375104 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.215614318847656, loss=0.8530364632606506
I0229 20:02:02.348009 140407379851008 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.063518524169922, loss=0.8857248425483704
I0229 20:02:36.027601 140408319375104 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.9959616661071777, loss=0.8531858325004578
I0229 20:03:09.782881 140407379851008 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.236201763153076, loss=0.8502673506736755
I0229 20:03:43.468621 140408319375104 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.9373018741607666, loss=0.8396711349487305
I0229 20:04:17.213121 140407379851008 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.977468729019165, loss=0.8816419243812561
I0229 20:04:51.004715 140408319375104 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.810291051864624, loss=0.8993467092514038
I0229 20:05:24.694240 140407379851008 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.910670042037964, loss=0.8394109010696411
I0229 20:05:58.410705 140408319375104 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.345388889312744, loss=0.8908224105834961
I0229 20:06:18.417449 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:06:25.338585 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:06:34.647961 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:06:36.907809 140573303715648 submission_runner.py:411] Time since start: 53520.90s, 	Step: 152861, 	{'train/accuracy': 0.9162946343421936, 'train/loss': 0.29054954648017883, 'validation/accuracy': 0.739799976348877, 'validation/loss': 1.094793677330017, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.8380128145217896, 'test/num_examples': 10000, 'score': 51567.03753566742, 'total_duration': 53520.900218486786, 'accumulated_submission_time': 51567.03753566742, 'accumulated_eval_time': 1942.4130220413208, 'accumulated_logging_time': 5.445278167724609}
I0229 20:06:36.952181 140409846093568 logging_writer.py:48] [152861] accumulated_eval_time=1942.413022, accumulated_logging_time=5.445278, accumulated_submission_time=51567.037536, global_step=152861, preemption_count=0, score=51567.037536, test/accuracy=0.617300, test/loss=1.838013, test/num_examples=10000, total_duration=53520.900218, train/accuracy=0.916295, train/loss=0.290550, validation/accuracy=0.739800, validation/loss=1.094794, validation/num_examples=50000
I0229 20:06:50.460938 140409854486272 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.230513572692871, loss=0.8676671385765076
I0229 20:07:24.101354 140409846093568 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.227662563323975, loss=0.8759673237800598
I0229 20:07:57.797686 140409854486272 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.1576385498046875, loss=0.8095771074295044
I0229 20:08:31.474420 140409846093568 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.434243679046631, loss=0.9095630645751953
I0229 20:09:05.214136 140409854486272 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.091745376586914, loss=0.812684953212738
I0229 20:09:38.870195 140409846093568 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.339093208312988, loss=0.8558083772659302
I0229 20:10:12.573508 140409854486272 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.112999439239502, loss=0.8514226675033569
I0229 20:10:46.432064 140409846093568 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.21473503112793, loss=0.7995238304138184
I0229 20:11:20.153148 140409854486272 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.935774803161621, loss=0.8314270973205566
I0229 20:11:53.815904 140409846093568 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.8327291011810303, loss=0.8222126960754395
I0229 20:12:27.514168 140409854486272 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.8549928665161133, loss=0.8373894095420837
I0229 20:13:01.184836 140409846093568 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.087899684906006, loss=0.8343759179115295
I0229 20:13:34.920238 140409854486272 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.9497463703155518, loss=0.7335143089294434
I0229 20:14:08.623888 140409846093568 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.273890495300293, loss=0.7995662689208984
I0229 20:14:42.362049 140409854486272 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.17854118347168, loss=0.7637269496917725
I0229 20:15:07.089576 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:15:13.137393 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:15:22.192312 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:15:24.467330 140573303715648 submission_runner.py:411] Time since start: 54048.46s, 	Step: 154375, 	{'train/accuracy': 0.9135442972183228, 'train/loss': 0.30003729462623596, 'validation/accuracy': 0.7407799959182739, 'validation/loss': 1.092759132385254, 'validation/num_examples': 50000, 'test/accuracy': 0.6141000390052795, 'test/loss': 1.8534924983978271, 'test/num_examples': 10000, 'score': 52077.10262107849, 'total_duration': 54048.459849357605, 'accumulated_submission_time': 52077.10262107849, 'accumulated_eval_time': 1959.7907156944275, 'accumulated_logging_time': 5.500972747802734}
I0229 20:15:24.510587 140407371458304 logging_writer.py:48] [154375] accumulated_eval_time=1959.790716, accumulated_logging_time=5.500973, accumulated_submission_time=52077.102621, global_step=154375, preemption_count=0, score=52077.102621, test/accuracy=0.614100, test/loss=1.853492, test/num_examples=10000, total_duration=54048.459849, train/accuracy=0.913544, train/loss=0.300037, validation/accuracy=0.740780, validation/loss=1.092759, validation/num_examples=50000
I0229 20:15:33.272234 140407379851008 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.054640293121338, loss=0.8002833127975464
I0229 20:16:06.901751 140407371458304 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.085105895996094, loss=0.752485990524292
I0229 20:16:40.714494 140407379851008 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.76108980178833, loss=0.8691089749336243
I0229 20:17:14.389969 140407371458304 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.009883403778076, loss=0.7847602367401123
I0229 20:17:48.152294 140407379851008 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.884819269180298, loss=0.8467289209365845
I0229 20:18:21.828254 140407371458304 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.8235676288604736, loss=0.7767347693443298
I0229 20:18:55.612154 140407379851008 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.013032913208008, loss=0.8052259087562561
I0229 20:19:29.292643 140407371458304 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.131110191345215, loss=0.8533557653427124
I0229 20:20:03.073874 140407379851008 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.271182537078857, loss=0.8555444478988647
I0229 20:20:36.736071 140407371458304 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.30649471282959, loss=0.8508642315864563
I0229 20:21:10.511547 140407379851008 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.104698657989502, loss=0.8158823847770691
I0229 20:21:44.220878 140407371458304 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.9362716674804688, loss=0.8262087106704712
I0229 20:22:17.946650 140407379851008 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.554673671722412, loss=0.8514685034751892
I0229 20:22:51.692917 140407371458304 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.040656089782715, loss=0.7666909694671631
I0229 20:23:25.365910 140407379851008 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.2552571296691895, loss=0.8345404267311096
I0229 20:23:54.508334 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:24:00.515339 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:24:09.686879 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:24:11.912454 140573303715648 submission_runner.py:411] Time since start: 54575.90s, 	Step: 155888, 	{'train/accuracy': 0.9125877022743225, 'train/loss': 0.2956430912017822, 'validation/accuracy': 0.7436400055885315, 'validation/loss': 1.0900652408599854, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.8527776002883911, 'test/num_examples': 10000, 'score': 52587.03049516678, 'total_duration': 54575.904990911484, 'accumulated_submission_time': 52587.03049516678, 'accumulated_eval_time': 1977.1947882175446, 'accumulated_logging_time': 5.554906845092773}
I0229 20:24:11.959319 140407379851008 logging_writer.py:48] [155888] accumulated_eval_time=1977.194788, accumulated_logging_time=5.554907, accumulated_submission_time=52587.030495, global_step=155888, preemption_count=0, score=52587.030495, test/accuracy=0.617800, test/loss=1.852778, test/num_examples=10000, total_duration=54575.904991, train/accuracy=0.912588, train/loss=0.295643, validation/accuracy=0.743640, validation/loss=1.090065, validation/num_examples=50000
I0229 20:24:16.356775 140408319375104 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.85953950881958, loss=0.7825013399124146
I0229 20:24:50.009511 140407379851008 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.9925708770751953, loss=0.729171872138977
I0229 20:25:23.713459 140408319375104 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.029836654663086, loss=0.7820931077003479
I0229 20:25:57.420644 140407379851008 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.1962995529174805, loss=0.8001549243927002
I0229 20:26:31.073147 140408319375104 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.427110195159912, loss=0.8142982721328735
I0229 20:27:04.747169 140407379851008 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.490960597991943, loss=0.7882514595985413
I0229 20:27:38.391170 140408319375104 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.415529251098633, loss=0.8178660869598389
I0229 20:28:12.066963 140407379851008 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.3373517990112305, loss=0.7924700379371643
I0229 20:28:45.791420 140408319375104 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.265309810638428, loss=0.8297250866889954
I0229 20:29:19.594668 140407379851008 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.173422813415527, loss=0.8383088111877441
I0229 20:29:53.247645 140408319375104 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.080790996551514, loss=0.8399373292922974
I0229 20:30:26.918202 140407379851008 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.014216899871826, loss=0.7835111618041992
I0229 20:31:00.639715 140408319375104 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.189878940582275, loss=0.7372515201568604
I0229 20:31:34.294226 140407379851008 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.004934787750244, loss=0.7535946369171143
I0229 20:32:08.045220 140408319375104 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.231588363647461, loss=0.7604688405990601
I0229 20:32:41.779921 140407379851008 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.068929672241211, loss=0.7858010530471802
I0229 20:32:41.942235 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:32:48.099063 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:32:56.789741 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:32:59.148437 140573303715648 submission_runner.py:411] Time since start: 55103.14s, 	Step: 157402, 	{'train/accuracy': 0.9200215339660645, 'train/loss': 0.2833128273487091, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.0706331729888916, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.8253930807113647, 'test/num_examples': 10000, 'score': 53096.94408249855, 'total_duration': 55103.1409778595, 'accumulated_submission_time': 53096.94408249855, 'accumulated_eval_time': 1994.400959968567, 'accumulated_logging_time': 5.611454725265503}
I0229 20:32:59.192148 140407379851008 logging_writer.py:48] [157402] accumulated_eval_time=1994.400960, accumulated_logging_time=5.611455, accumulated_submission_time=53096.944082, global_step=157402, preemption_count=0, score=53096.944082, test/accuracy=0.619100, test/loss=1.825393, test/num_examples=10000, total_duration=55103.140978, train/accuracy=0.920022, train/loss=0.283313, validation/accuracy=0.745540, validation/loss=1.070633, validation/num_examples=50000
I0229 20:33:32.506016 140409124681472 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.962416648864746, loss=0.7220712900161743
I0229 20:34:06.162161 140407379851008 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.968061685562134, loss=0.735828161239624
I0229 20:34:39.931529 140409124681472 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.7955715656280518, loss=0.7648530006408691
I0229 20:35:13.681032 140407379851008 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.37814474105835, loss=0.7767360806465149
I0229 20:35:47.366676 140409124681472 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.295192718505859, loss=0.7298151850700378
I0229 20:36:21.067064 140407379851008 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.440749168395996, loss=0.8423799276351929
I0229 20:36:54.810751 140409124681472 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.1000213623046875, loss=0.7301573753356934
I0229 20:37:28.523702 140407379851008 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.8202502727508545, loss=0.7433532476425171
I0229 20:38:02.255703 140409124681472 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.307705402374268, loss=0.7502470016479492
I0229 20:38:35.976228 140407379851008 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.437621593475342, loss=0.8672166466712952
I0229 20:39:09.678624 140409124681472 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.748680591583252, loss=0.8273396492004395
I0229 20:39:43.396236 140407379851008 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.369785785675049, loss=0.7633422613143921
I0229 20:40:17.107013 140409124681472 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.073071002960205, loss=0.7697398662567139
I0229 20:40:50.839380 140407379851008 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.370456695556641, loss=0.7274323105812073
I0229 20:41:24.622863 140409124681472 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.01968240737915, loss=0.7480794191360474
I0229 20:41:29.157094 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:41:35.247007 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:41:43.901205 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:41:46.195835 140573303715648 submission_runner.py:411] Time since start: 55630.19s, 	Step: 158915, 	{'train/accuracy': 0.9239875674247742, 'train/loss': 0.26684898138046265, 'validation/accuracy': 0.7450999617576599, 'validation/loss': 1.073038935661316, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.8164268732070923, 'test/num_examples': 10000, 'score': 53606.83699655533, 'total_duration': 55630.18832850456, 'accumulated_submission_time': 53606.83699655533, 'accumulated_eval_time': 2011.4396004676819, 'accumulated_logging_time': 5.665586709976196}
I0229 20:41:46.254537 140408319375104 logging_writer.py:48] [158915] accumulated_eval_time=2011.439600, accumulated_logging_time=5.665587, accumulated_submission_time=53606.836997, global_step=158915, preemption_count=0, score=53606.836997, test/accuracy=0.617800, test/loss=1.816427, test/num_examples=10000, total_duration=55630.188329, train/accuracy=0.923988, train/loss=0.266849, validation/accuracy=0.745100, validation/loss=1.073039, validation/num_examples=50000
I0229 20:42:15.237384 140409837700864 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.304913520812988, loss=0.7312149405479431
I0229 20:42:48.895621 140408319375104 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.104808330535889, loss=0.7901872396469116
I0229 20:43:22.559114 140409837700864 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.530282497406006, loss=0.8743308186531067
I0229 20:43:56.276431 140408319375104 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.31403112411499, loss=0.7130072116851807
I0229 20:44:29.933409 140409837700864 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.663578987121582, loss=0.7001609206199646
I0229 20:45:03.608503 140408319375104 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.1808319091796875, loss=0.8228668570518494
I0229 20:45:37.348332 140409837700864 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.307610511779785, loss=0.733418345451355
I0229 20:46:11.004900 140408319375104 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.996704578399658, loss=0.7137163877487183
I0229 20:46:44.736938 140409837700864 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.2759246826171875, loss=0.7718170881271362
I0229 20:47:18.520314 140408319375104 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.411358833312988, loss=0.8279504776000977
I0229 20:47:52.202175 140409837700864 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.364762306213379, loss=0.8026212453842163
I0229 20:48:25.864164 140408319375104 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.015843391418457, loss=0.7600762844085693
I0229 20:48:59.578267 140409837700864 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.077153205871582, loss=0.7410297989845276
I0229 20:49:33.287084 140408319375104 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.764094352722168, loss=0.7215316295623779
I0229 20:50:07.050321 140409837700864 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.551350116729736, loss=0.8169166445732117
I0229 20:50:16.305384 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:50:22.431463 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:50:31.320009 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:50:33.631478 140573303715648 submission_runner.py:411] Time since start: 56157.62s, 	Step: 160429, 	{'train/accuracy': 0.9426020383834839, 'train/loss': 0.2054823786020279, 'validation/accuracy': 0.7473999857902527, 'validation/loss': 1.0706602334976196, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.8280524015426636, 'test/num_examples': 10000, 'score': 54116.81638598442, 'total_duration': 56157.62402033806, 'accumulated_submission_time': 54116.81638598442, 'accumulated_eval_time': 2028.7656478881836, 'accumulated_logging_time': 5.735450029373169}
I0229 20:50:33.677661 140409846093568 logging_writer.py:48] [160429] accumulated_eval_time=2028.765648, accumulated_logging_time=5.735450, accumulated_submission_time=54116.816386, global_step=160429, preemption_count=0, score=54116.816386, test/accuracy=0.620100, test/loss=1.828052, test/num_examples=10000, total_duration=56157.624020, train/accuracy=0.942602, train/loss=0.205482, validation/accuracy=0.747400, validation/loss=1.070660, validation/num_examples=50000
I0229 20:50:57.894304 140409854486272 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.485733985900879, loss=0.7913157939910889
I0229 20:51:31.557781 140409846093568 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.3072896003723145, loss=0.7863081693649292
I0229 20:52:05.266880 140409854486272 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.218606948852539, loss=0.7820711731910706
I0229 20:52:38.924805 140409846093568 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.237118721008301, loss=0.7646900415420532
I0229 20:53:12.637414 140409854486272 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.148610591888428, loss=0.7150433659553528
I0229 20:53:46.413953 140409846093568 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.083906173706055, loss=0.7466338276863098
I0229 20:54:20.088801 140409854486272 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.336824893951416, loss=0.8051998019218445
I0229 20:54:53.808397 140409846093568 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.168858528137207, loss=0.724123477935791
I0229 20:55:27.465909 140409854486272 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.045767784118652, loss=0.6953518390655518
I0229 20:56:01.117993 140409846093568 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.9315130710601807, loss=0.6611630916595459
I0229 20:56:34.823559 140409854486272 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.3958258628845215, loss=0.6641558408737183
I0229 20:57:08.571545 140409846093568 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.730092525482178, loss=0.7609505653381348
I0229 20:57:42.239103 140409854486272 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.3492751121521, loss=0.798779308795929
I0229 20:58:16.002213 140409846093568 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.3130574226379395, loss=0.7905911803245544
I0229 20:58:49.728275 140409854486272 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.37968111038208, loss=0.7298622131347656
I0229 20:59:03.685414 140573303715648 spec.py:321] Evaluating on the training split.
I0229 20:59:09.726033 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 20:59:18.737179 140573303715648 spec.py:349] Evaluating on the test split.
I0229 20:59:21.041043 140573303715648 submission_runner.py:411] Time since start: 56685.03s, 	Step: 161943, 	{'train/accuracy': 0.9362045526504517, 'train/loss': 0.22412645816802979, 'validation/accuracy': 0.7438799738883972, 'validation/loss': 1.0907021760940552, 'validation/num_examples': 50000, 'test/accuracy': 0.6215000152587891, 'test/loss': 1.8463952541351318, 'test/num_examples': 10000, 'score': 54626.75292801857, 'total_duration': 56685.03358387947, 'accumulated_submission_time': 54626.75292801857, 'accumulated_eval_time': 2046.1212282180786, 'accumulated_logging_time': 5.791545629501343}
I0229 20:59:21.090018 140409829308160 logging_writer.py:48] [161943] accumulated_eval_time=2046.121228, accumulated_logging_time=5.791546, accumulated_submission_time=54626.752928, global_step=161943, preemption_count=0, score=54626.752928, test/accuracy=0.621500, test/loss=1.846395, test/num_examples=10000, total_duration=56685.033584, train/accuracy=0.936205, train/loss=0.224126, validation/accuracy=0.743880, validation/loss=1.090702, validation/num_examples=50000
I0229 20:59:40.672322 140409837700864 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.624565601348877, loss=0.7830373644828796
I0229 21:00:14.305626 140409829308160 logging_writer.py:48] [162100] global_step=162100, grad_norm=5.195326328277588, loss=0.8324192762374878
I0229 21:00:47.985336 140409837700864 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.905843496322632, loss=0.638511061668396
I0229 21:01:21.713001 140409829308160 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.509397983551025, loss=0.7381172180175781
I0229 21:01:55.424194 140409837700864 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.456460475921631, loss=0.7466409802436829
I0229 21:02:29.181401 140409829308160 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.184626579284668, loss=0.695173978805542
I0229 21:03:02.861003 140409837700864 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.525406837463379, loss=0.6863429546356201
I0229 21:03:36.529671 140409829308160 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.119606971740723, loss=0.7524564266204834
I0229 21:04:10.244878 140409837700864 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.324369430541992, loss=0.7007359266281128
I0229 21:04:43.908170 140409829308160 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.535617828369141, loss=0.6982769966125488
I0229 21:05:17.588061 140409837700864 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.156810760498047, loss=0.696871817111969
I0229 21:05:51.356882 140409829308160 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.662883758544922, loss=0.7287216186523438
I0229 21:06:25.040759 140409837700864 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.25438928604126, loss=0.6917697191238403
I0229 21:06:58.722389 140409829308160 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.297997951507568, loss=0.7254513502120972
I0229 21:07:32.402071 140409837700864 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.357316017150879, loss=0.7144801020622253
I0229 21:07:51.064235 140573303715648 spec.py:321] Evaluating on the training split.
I0229 21:07:57.121672 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 21:08:06.151156 140573303715648 spec.py:349] Evaluating on the test split.
I0229 21:08:08.493355 140573303715648 submission_runner.py:411] Time since start: 57212.49s, 	Step: 163457, 	{'train/accuracy': 0.9362045526504517, 'train/loss': 0.22075380384922028, 'validation/accuracy': 0.746999979019165, 'validation/loss': 1.0740346908569336, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.84682035446167, 'test/num_examples': 10000, 'score': 55136.65639901161, 'total_duration': 57212.485882759094, 'accumulated_submission_time': 55136.65639901161, 'accumulated_eval_time': 2063.5502858161926, 'accumulated_logging_time': 5.850171327590942}
I0229 21:08:08.541546 140409124681472 logging_writer.py:48] [163457] accumulated_eval_time=2063.550286, accumulated_logging_time=5.850171, accumulated_submission_time=55136.656399, global_step=163457, preemption_count=0, score=55136.656399, test/accuracy=0.620500, test/loss=1.846820, test/num_examples=10000, total_duration=57212.485883, train/accuracy=0.936205, train/loss=0.220754, validation/accuracy=0.747000, validation/loss=1.074035, validation/num_examples=50000
I0229 21:08:23.393254 140409846093568 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.2066264152526855, loss=0.7631027698516846
I0229 21:08:57.041989 140409124681472 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.512836456298828, loss=0.735270082950592
I0229 21:09:30.708296 140409846093568 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.3501667976379395, loss=0.7396042943000793
I0229 21:10:04.440162 140409124681472 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.3159942626953125, loss=0.6950229406356812
I0229 21:10:38.127149 140409846093568 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.356904029846191, loss=0.6746991276741028
I0229 21:11:11.842305 140409124681472 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.464396953582764, loss=0.7257172465324402
I0229 21:11:45.533436 140409846093568 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.194095134735107, loss=0.679352343082428
I0229 21:12:19.313531 140409124681472 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.506374835968018, loss=0.7486457228660583
I0229 21:12:53.031470 140409846093568 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.383275985717773, loss=0.7220182418823242
I0229 21:13:26.709851 140409124681472 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.358249664306641, loss=0.6940145492553711
I0229 21:14:00.445230 140409846093568 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.592247009277344, loss=0.7409365177154541
I0229 21:14:34.091200 140409124681472 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.288156032562256, loss=0.756029486656189
I0229 21:15:07.753871 140409846093568 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.735389709472656, loss=0.7845642566680908
I0229 21:15:41.487097 140409124681472 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.425176620483398, loss=0.7448612451553345
I0229 21:16:15.149364 140409846093568 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.2920684814453125, loss=0.6897532939910889
I0229 21:16:38.533598 140573303715648 spec.py:321] Evaluating on the training split.
I0229 21:16:44.602758 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 21:16:53.275175 140573303715648 spec.py:349] Evaluating on the test split.
I0229 21:16:55.594458 140573303715648 submission_runner.py:411] Time since start: 57739.59s, 	Step: 164971, 	{'train/accuracy': 0.9383171200752258, 'train/loss': 0.21140213310718536, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.0842297077178955, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8529764413833618, 'test/num_examples': 10000, 'score': 55646.5770778656, 'total_duration': 57739.58699655533, 'accumulated_submission_time': 55646.5770778656, 'accumulated_eval_time': 2080.6110968589783, 'accumulated_logging_time': 5.909976959228516}
I0229 21:16:55.638570 140407371458304 logging_writer.py:48] [164971] accumulated_eval_time=2080.611097, accumulated_logging_time=5.909977, accumulated_submission_time=55646.577078, global_step=164971, preemption_count=0, score=55646.577078, test/accuracy=0.627300, test/loss=1.852976, test/num_examples=10000, total_duration=57739.586997, train/accuracy=0.938317, train/loss=0.211402, validation/accuracy=0.747620, validation/loss=1.084230, validation/num_examples=50000
I0229 21:17:05.731596 140407379851008 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.372650146484375, loss=0.6546991467475891
I0229 21:17:39.412423 140407371458304 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.55203104019165, loss=0.6825026273727417
I0229 21:18:13.209292 140407379851008 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.260955810546875, loss=0.66496741771698
I0229 21:18:46.912416 140407371458304 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.517247676849365, loss=0.7699040174484253
I0229 21:19:20.609647 140407379851008 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.288952827453613, loss=0.6436551213264465
I0229 21:19:54.334255 140407371458304 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.844470500946045, loss=0.7740662097930908
I0229 21:20:28.046375 140407379851008 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.521694660186768, loss=0.7804222702980042
I0229 21:21:01.771699 140407371458304 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.351505279541016, loss=0.700211763381958
I0229 21:21:35.416213 140407379851008 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.435604572296143, loss=0.7228617072105408
I0229 21:22:09.080817 140407371458304 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.854372978210449, loss=0.7579372525215149
I0229 21:22:42.850113 140407379851008 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.759955883026123, loss=0.7419430017471313
I0229 21:23:16.522516 140407371458304 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.790796756744385, loss=0.7959381341934204
I0229 21:23:50.280604 140407379851008 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.691206932067871, loss=0.6753496527671814
I0229 21:24:24.161845 140407371458304 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.444895267486572, loss=0.6721407771110535
I0229 21:24:57.882877 140407379851008 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.098090648651123, loss=0.6367631554603577
I0229 21:25:25.614486 140573303715648 spec.py:321] Evaluating on the training split.
I0229 21:25:31.762390 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 21:25:40.604354 140573303715648 spec.py:349] Evaluating on the test split.
I0229 21:25:42.940652 140573303715648 submission_runner.py:411] Time since start: 58266.93s, 	Step: 166484, 	{'train/accuracy': 0.9404296875, 'train/loss': 0.20761986076831818, 'validation/accuracy': 0.7492600083351135, 'validation/loss': 1.065981149673462, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.8334190845489502, 'test/num_examples': 10000, 'score': 56156.48152041435, 'total_duration': 58266.93318080902, 'accumulated_submission_time': 56156.48152041435, 'accumulated_eval_time': 2097.937203168869, 'accumulated_logging_time': 5.965330123901367}
I0229 21:25:42.984430 140407379851008 logging_writer.py:48] [166484] accumulated_eval_time=2097.937203, accumulated_logging_time=5.965330, accumulated_submission_time=56156.481520, global_step=166484, preemption_count=0, score=56156.481520, test/accuracy=0.623400, test/loss=1.833419, test/num_examples=10000, total_duration=58266.933181, train/accuracy=0.940430, train/loss=0.207620, validation/accuracy=0.749260, validation/loss=1.065981, validation/num_examples=50000
I0229 21:25:48.726424 140409846093568 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.575015544891357, loss=0.7159438133239746
I0229 21:26:22.406371 140407379851008 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.894761085510254, loss=0.7399752140045166
I0229 21:26:56.133970 140409846093568 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.494077682495117, loss=0.7556664943695068
I0229 21:27:29.830813 140407379851008 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.595414161682129, loss=0.7513769865036011
I0229 21:28:03.562789 140409846093568 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.991505146026611, loss=0.666893720626831
I0229 21:28:37.244704 140407379851008 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.362522602081299, loss=0.6987262964248657
I0229 21:29:10.969146 140409846093568 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.405895233154297, loss=0.704815685749054
I0229 21:29:44.676517 140407379851008 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.320405006408691, loss=0.7193645238876343
I0229 21:30:18.549796 140409846093568 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.481470108032227, loss=0.7564699649810791
I0229 21:30:52.271348 140407379851008 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.440904140472412, loss=0.698323130607605
I0229 21:31:25.995229 140409846093568 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.84708833694458, loss=0.7073119878768921
I0229 21:31:59.687927 140407379851008 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.559648036956787, loss=0.723401665687561
I0229 21:32:33.430039 140409846093568 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.518581390380859, loss=0.693305492401123
I0229 21:33:07.112731 140407379851008 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.791609764099121, loss=0.7519590854644775
I0229 21:33:40.848814 140409846093568 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.178424835205078, loss=0.6895425319671631
I0229 21:34:13.032476 140573303715648 spec.py:321] Evaluating on the training split.
I0229 21:34:19.188065 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 21:34:27.827664 140573303715648 spec.py:349] Evaluating on the test split.
I0229 21:34:30.174262 140573303715648 submission_runner.py:411] Time since start: 58794.17s, 	Step: 167997, 	{'train/accuracy': 0.9448142051696777, 'train/loss': 0.19656126201152802, 'validation/accuracy': 0.7514399886131287, 'validation/loss': 1.0683664083480835, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8146344423294067, 'test/num_examples': 10000, 'score': 56666.461283922195, 'total_duration': 58794.1668009758, 'accumulated_submission_time': 56666.461283922195, 'accumulated_eval_time': 2115.0789551734924, 'accumulated_logging_time': 6.01855206489563}
I0229 21:34:30.218682 140407379851008 logging_writer.py:48] [167997] accumulated_eval_time=2115.078955, accumulated_logging_time=6.018552, accumulated_submission_time=56666.461284, global_step=167997, preemption_count=0, score=56666.461284, test/accuracy=0.629100, test/loss=1.814634, test/num_examples=10000, total_duration=58794.166801, train/accuracy=0.944814, train/loss=0.196561, validation/accuracy=0.751440, validation/loss=1.068366, validation/num_examples=50000
I0229 21:34:31.576036 140408319375104 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.579951286315918, loss=0.7459592819213867
I0229 21:35:05.237369 140407379851008 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.291722297668457, loss=0.6340128183364868
I0229 21:35:38.928036 140408319375104 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.881663799285889, loss=0.7892257571220398
I0229 21:36:12.652483 140407379851008 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.322737693786621, loss=0.6477555632591248
I0229 21:36:46.351067 140408319375104 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.6317009925842285, loss=0.7404094934463501
I0229 21:37:20.051168 140407379851008 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.705894947052002, loss=0.7258910536766052
I0229 21:37:53.757601 140408319375104 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.2541890144348145, loss=0.6304495930671692
I0229 21:38:27.467873 140407379851008 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.6351728439331055, loss=0.6723916530609131
I0229 21:39:01.183318 140408319375104 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.266829490661621, loss=0.657367467880249
I0229 21:39:34.925678 140407379851008 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.410299777984619, loss=0.6607012748718262
I0229 21:40:08.594277 140408319375104 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.600968837738037, loss=0.7515376210212708
I0229 21:40:42.377963 140407379851008 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.344184875488281, loss=0.6310655474662781
I0229 21:41:16.069246 140408319375104 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.6512770652771, loss=0.7113869190216064
I0229 21:41:49.851728 140407379851008 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.737875938415527, loss=0.6945987939834595
I0229 21:42:23.790397 140408319375104 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.717723369598389, loss=0.6636735796928406
I0229 21:42:57.463748 140407379851008 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.296106338500977, loss=0.7855038046836853
I0229 21:43:00.309713 140573303715648 spec.py:321] Evaluating on the training split.
I0229 21:43:06.559188 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 21:43:15.384284 140573303715648 spec.py:349] Evaluating on the test split.
I0229 21:43:17.676241 140573303715648 submission_runner.py:411] Time since start: 59321.67s, 	Step: 169510, 	{'train/accuracy': 0.9545400142669678, 'train/loss': 0.16818206012248993, 'validation/accuracy': 0.7520399689674377, 'validation/loss': 1.063841462135315, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.82466459274292, 'test/num_examples': 10000, 'score': 57176.4802467823, 'total_duration': 59321.66878390312, 'accumulated_submission_time': 57176.4802467823, 'accumulated_eval_time': 2132.4454357624054, 'accumulated_logging_time': 6.0728843212127686}
I0229 21:43:17.721962 140409854486272 logging_writer.py:48] [169510] accumulated_eval_time=2132.445436, accumulated_logging_time=6.072884, accumulated_submission_time=57176.480247, global_step=169510, preemption_count=0, score=57176.480247, test/accuracy=0.624500, test/loss=1.824665, test/num_examples=10000, total_duration=59321.668784, train/accuracy=0.954540, train/loss=0.168182, validation/accuracy=0.752040, validation/loss=1.063841, validation/num_examples=50000
I0229 21:43:48.393328 140409862878976 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.123795509338379, loss=0.7393208742141724
I0229 21:44:22.060823 140409854486272 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.405586242675781, loss=0.6536577343940735
I0229 21:44:55.794284 140409862878976 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.217337608337402, loss=0.6843889951705933
I0229 21:45:29.511724 140409854486272 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.364335060119629, loss=0.6895215511322021
I0229 21:46:03.254109 140409862878976 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.594806671142578, loss=0.6581169962882996
I0229 21:46:36.936140 140409854486272 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.770371437072754, loss=0.6802173852920532
I0229 21:47:10.659269 140409862878976 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.51233434677124, loss=0.7237356901168823
I0229 21:47:44.350740 140409854486272 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.679849147796631, loss=0.6969323754310608
I0229 21:48:18.062315 140409862878976 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.498918056488037, loss=0.7130129933357239
I0229 21:48:51.875140 140409854486272 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.173532485961914, loss=0.6261330246925354
I0229 21:49:25.581527 140409862878976 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.870852470397949, loss=0.6732762455940247
I0229 21:49:59.295892 140409854486272 logging_writer.py:48] [170700] global_step=170700, grad_norm=5.038247108459473, loss=0.6823282241821289
I0229 21:50:33.039624 140409862878976 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.1721696853637695, loss=0.6397092342376709
I0229 21:51:06.759443 140409854486272 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.6571879386901855, loss=0.6800495386123657
I0229 21:51:40.499628 140409862878976 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.447983741760254, loss=0.6834900975227356
I0229 21:51:47.732781 140573303715648 spec.py:321] Evaluating on the training split.
I0229 21:51:53.761038 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 21:52:02.929610 140573303715648 spec.py:349] Evaluating on the test split.
I0229 21:52:05.220950 140573303715648 submission_runner.py:411] Time since start: 59849.21s, 	Step: 171023, 	{'train/accuracy': 0.9538225531578064, 'train/loss': 0.16630078852176666, 'validation/accuracy': 0.7504199743270874, 'validation/loss': 1.0649524927139282, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8314189910888672, 'test/num_examples': 10000, 'score': 57686.42001056671, 'total_duration': 59849.21348261833, 'accumulated_submission_time': 57686.42001056671, 'accumulated_eval_time': 2149.9335446357727, 'accumulated_logging_time': 6.129309177398682}
I0229 21:52:05.265454 140407379851008 logging_writer.py:48] [171023] accumulated_eval_time=2149.933545, accumulated_logging_time=6.129309, accumulated_submission_time=57686.420011, global_step=171023, preemption_count=0, score=57686.420011, test/accuracy=0.624700, test/loss=1.831419, test/num_examples=10000, total_duration=59849.213483, train/accuracy=0.953823, train/loss=0.166301, validation/accuracy=0.750420, validation/loss=1.064952, validation/num_examples=50000
I0229 21:52:31.488770 140408319375104 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.374112129211426, loss=0.6702075004577637
I0229 21:53:05.201132 140407379851008 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.377665996551514, loss=0.6657760143280029
I0229 21:53:38.840550 140408319375104 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.28227424621582, loss=0.5546036958694458
I0229 21:54:12.513010 140407379851008 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.585333824157715, loss=0.6736247539520264
I0229 21:54:46.315950 140408319375104 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.163045406341553, loss=0.6382174491882324
I0229 21:55:20.054169 140407379851008 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.425429344177246, loss=0.6840580701828003
I0229 21:55:53.693969 140408319375104 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.174723148345947, loss=0.6341227293014526
I0229 21:56:27.364503 140407379851008 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.498382091522217, loss=0.707539439201355
I0229 21:57:01.054084 140408319375104 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.289647579193115, loss=0.6461693048477173
I0229 21:57:34.802826 140407379851008 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.752459526062012, loss=0.7117039561271667
I0229 21:58:08.445964 140408319375104 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.5983405113220215, loss=0.665429413318634
I0229 21:58:42.095131 140407379851008 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.247790336608887, loss=0.6185039281845093
I0229 21:59:15.801659 140408319375104 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.530234336853027, loss=0.6767215728759766
I0229 21:59:49.495866 140407379851008 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.306974411010742, loss=0.5917958617210388
I0229 22:00:23.144154 140408319375104 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.40654182434082, loss=0.6664046049118042
I0229 22:00:35.401471 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:00:41.750688 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:00:50.626812 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:00:52.913812 140573303715648 submission_runner.py:411] Time since start: 60376.91s, 	Step: 172538, 	{'train/accuracy': 0.9530851244926453, 'train/loss': 0.16867898404598236, 'validation/accuracy': 0.7537999749183655, 'validation/loss': 1.0632176399230957, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8282188177108765, 'test/num_examples': 10000, 'score': 58196.485368967056, 'total_duration': 60376.90635275841, 'accumulated_submission_time': 58196.485368967056, 'accumulated_eval_time': 2167.4458363056183, 'accumulated_logging_time': 6.185018539428711}
I0229 22:00:52.958410 140407379851008 logging_writer.py:48] [172538] accumulated_eval_time=2167.445836, accumulated_logging_time=6.185019, accumulated_submission_time=58196.485369, global_step=172538, preemption_count=0, score=58196.485369, test/accuracy=0.627800, test/loss=1.828219, test/num_examples=10000, total_duration=60376.906353, train/accuracy=0.953085, train/loss=0.168679, validation/accuracy=0.753800, validation/loss=1.063218, validation/num_examples=50000
I0229 22:01:14.219979 140409846093568 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.344375133514404, loss=0.6699893474578857
I0229 22:01:47.906800 140407379851008 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.305820465087891, loss=0.5967113971710205
I0229 22:02:21.570038 140409846093568 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.877532958984375, loss=0.6928888559341431
I0229 22:02:55.293144 140407379851008 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.768073558807373, loss=0.708518385887146
I0229 22:03:28.983717 140409846093568 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.1533355712890625, loss=0.6124504804611206
I0229 22:04:02.717105 140407379851008 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.437891960144043, loss=0.6104888916015625
I0229 22:04:36.398792 140409846093568 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.244715690612793, loss=0.6294148564338684
I0229 22:05:10.135846 140407379851008 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.289844989776611, loss=0.6232690811157227
I0229 22:05:43.812043 140409846093568 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.676283359527588, loss=0.635577917098999
I0229 22:06:17.564592 140407379851008 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.969082355499268, loss=0.6921690702438354
I0229 22:06:51.311154 140409846093568 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.037134647369385, loss=0.5923622846603394
I0229 22:07:24.995455 140407379851008 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.680202007293701, loss=0.6965807676315308
I0229 22:07:58.695814 140409846093568 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.4081501960754395, loss=0.647183358669281
I0229 22:08:32.389833 140407379851008 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.480990409851074, loss=0.5855569839477539
I0229 22:09:06.091339 140409846093568 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.3937883377075195, loss=0.680812418460846
I0229 22:09:23.104451 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:09:29.212847 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:09:38.171013 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:09:40.490660 140573303715648 submission_runner.py:411] Time since start: 60904.48s, 	Step: 174052, 	{'train/accuracy': 0.9546994566917419, 'train/loss': 0.16579779982566833, 'validation/accuracy': 0.7526999711990356, 'validation/loss': 1.0615818500518799, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.8208074569702148, 'test/num_examples': 10000, 'score': 58706.56109046936, 'total_duration': 60904.483194589615, 'accumulated_submission_time': 58706.56109046936, 'accumulated_eval_time': 2184.832007408142, 'accumulated_logging_time': 6.240122318267822}
I0229 22:09:40.548488 140407379851008 logging_writer.py:48] [174052] accumulated_eval_time=2184.832007, accumulated_logging_time=6.240122, accumulated_submission_time=58706.561090, global_step=174052, preemption_count=0, score=58706.561090, test/accuracy=0.628500, test/loss=1.820807, test/num_examples=10000, total_duration=60904.483195, train/accuracy=0.954699, train/loss=0.165798, validation/accuracy=0.752700, validation/loss=1.061582, validation/num_examples=50000
I0229 22:09:57.084981 140409124681472 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.766123294830322, loss=0.6417543888092041
I0229 22:10:30.733533 140407379851008 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.439335346221924, loss=0.6326390504837036
I0229 22:11:04.397617 140409124681472 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.639010906219482, loss=0.6724465489387512
I0229 22:11:38.102410 140407379851008 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.7555832862854, loss=0.7115967273712158
I0229 22:12:11.842508 140409124681472 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.636348247528076, loss=0.6975031495094299
I0229 22:12:45.560980 140407379851008 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.280756950378418, loss=0.6804920434951782
I0229 22:13:19.317988 140409124681472 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.633370876312256, loss=0.6794019341468811
I0229 22:13:53.050805 140407379851008 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.265478134155273, loss=0.6423628330230713
I0229 22:14:26.739934 140409124681472 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.467276573181152, loss=0.708411693572998
I0229 22:15:00.461947 140407379851008 logging_writer.py:48] [175000] global_step=175000, grad_norm=3.9609484672546387, loss=0.6028076410293579
I0229 22:15:34.132153 140409124681472 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.442503452301025, loss=0.6944447159767151
I0229 22:16:07.863007 140407379851008 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.192115783691406, loss=0.6115682125091553
I0229 22:16:41.564629 140409124681472 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.464000225067139, loss=0.6783279776573181
I0229 22:17:15.206086 140407379851008 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.346491813659668, loss=0.6378706693649292
I0229 22:17:48.858550 140409124681472 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.4576029777526855, loss=0.5303670167922974
I0229 22:18:10.607916 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:18:16.716402 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:18:25.345801 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:18:27.609257 140573303715648 submission_runner.py:411] Time since start: 61431.60s, 	Step: 175566, 	{'train/accuracy': 0.9553571343421936, 'train/loss': 0.16501082479953766, 'validation/accuracy': 0.7545799612998962, 'validation/loss': 1.0558218955993652, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8308131694793701, 'test/num_examples': 10000, 'score': 59216.54894709587, 'total_duration': 61431.60178184509, 'accumulated_submission_time': 59216.54894709587, 'accumulated_eval_time': 2201.833286046982, 'accumulated_logging_time': 6.310078859329224}
I0229 22:18:27.658181 140407379851008 logging_writer.py:48] [175566] accumulated_eval_time=2201.833286, accumulated_logging_time=6.310079, accumulated_submission_time=59216.548947, global_step=175566, preemption_count=0, score=59216.548947, test/accuracy=0.626800, test/loss=1.830813, test/num_examples=10000, total_duration=61431.601782, train/accuracy=0.955357, train/loss=0.165011, validation/accuracy=0.754580, validation/loss=1.055822, validation/num_examples=50000
I0229 22:18:39.441340 140409846093568 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.252495765686035, loss=0.6302827000617981
I0229 22:19:13.206109 140407379851008 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.278588771820068, loss=0.6899149417877197
I0229 22:19:46.898662 140409846093568 logging_writer.py:48] [175800] global_step=175800, grad_norm=5.262889862060547, loss=0.7212170958518982
I0229 22:20:20.617330 140407379851008 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.22416877746582, loss=0.614998996257782
I0229 22:20:54.352067 140409846093568 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.561905860900879, loss=0.6133278012275696
I0229 22:21:28.074606 140407379851008 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.564703464508057, loss=0.635732889175415
I0229 22:22:01.730024 140409846093568 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.936198711395264, loss=0.7150863409042358
I0229 22:22:35.454547 140407379851008 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.3760881423950195, loss=0.6372026205062866
I0229 22:23:09.136110 140409846093568 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.471149444580078, loss=0.6827974915504456
I0229 22:23:42.841767 140407379851008 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.342162609100342, loss=0.6090759038925171
I0229 22:24:16.623282 140409846093568 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.102893352508545, loss=0.5425580143928528
I0229 22:24:50.298973 140407379851008 logging_writer.py:48] [176700] global_step=176700, grad_norm=5.0305657386779785, loss=0.7125821113586426
I0229 22:25:24.088701 140409846093568 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.703818321228027, loss=0.6541560292243958
I0229 22:25:57.809189 140407379851008 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.801460266113281, loss=0.6857618689537048
I0229 22:26:31.530049 140409846093568 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.896033763885498, loss=0.6649699807167053
I0229 22:26:57.651325 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:27:03.926657 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:27:12.893498 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:27:15.168780 140573303715648 submission_runner.py:411] Time since start: 61959.16s, 	Step: 177079, 	{'train/accuracy': 0.9563536047935486, 'train/loss': 0.15891145169734955, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0573482513427734, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.8204129934310913, 'test/num_examples': 10000, 'score': 59726.470982313156, 'total_duration': 61959.16131854057, 'accumulated_submission_time': 59726.470982313156, 'accumulated_eval_time': 2219.350727558136, 'accumulated_logging_time': 6.369282007217407}
I0229 22:27:15.216723 140407379851008 logging_writer.py:48] [177079] accumulated_eval_time=2219.350728, accumulated_logging_time=6.369282, accumulated_submission_time=59726.470982, global_step=177079, preemption_count=0, score=59726.470982, test/accuracy=0.628500, test/loss=1.820413, test/num_examples=10000, total_duration=61959.161319, train/accuracy=0.956354, train/loss=0.158911, validation/accuracy=0.754060, validation/loss=1.057348, validation/num_examples=50000
I0229 22:27:22.667721 140408319375104 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.362602233886719, loss=0.6599722504615784
I0229 22:27:56.362230 140407379851008 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.463179111480713, loss=0.5653122067451477
I0229 22:28:30.265922 140408319375104 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.347657680511475, loss=0.6099896430969238
I0229 22:29:04.005002 140407379851008 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.7283477783203125, loss=0.6966401934623718
I0229 22:29:37.712712 140408319375104 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.51842737197876, loss=0.6663141250610352
I0229 22:30:11.437437 140407379851008 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.412904739379883, loss=0.5966448783874512
I0229 22:30:45.190363 140408319375104 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.387814998626709, loss=0.6105966567993164
I0229 22:31:19.051648 140407379851008 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.578927516937256, loss=0.6307532787322998
I0229 22:31:52.736138 140408319375104 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.249662399291992, loss=0.651013195514679
I0229 22:32:26.444097 140407379851008 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.129283428192139, loss=0.583273708820343
I0229 22:33:00.125347 140408319375104 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.6495208740234375, loss=0.5836836099624634
I0229 22:33:33.818199 140407379851008 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.4151506423950195, loss=0.667723536491394
I0229 22:34:07.492304 140408319375104 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.340932846069336, loss=0.63155198097229
I0229 22:34:41.168386 140407379851008 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.98386287689209, loss=0.6549074649810791
I0229 22:35:14.889813 140408319375104 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.498885631561279, loss=0.6783788204193115
I0229 22:35:45.380021 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:35:51.461973 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:36:00.473423 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:36:02.788197 140573303715648 submission_runner.py:411] Time since start: 62486.78s, 	Step: 178592, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14903414249420166, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.055389404296875, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8235666751861572, 'test/num_examples': 10000, 'score': 60236.56385469437, 'total_duration': 62486.78073191643, 'accumulated_submission_time': 60236.56385469437, 'accumulated_eval_time': 2236.7588534355164, 'accumulated_logging_time': 6.427386999130249}
I0229 22:36:02.837327 140407379851008 logging_writer.py:48] [178592] accumulated_eval_time=2236.758853, accumulated_logging_time=6.427387, accumulated_submission_time=60236.563855, global_step=178592, preemption_count=0, score=60236.563855, test/accuracy=0.629700, test/loss=1.823567, test/num_examples=10000, total_duration=62486.780732, train/accuracy=0.960379, train/loss=0.149034, validation/accuracy=0.755180, validation/loss=1.055389, validation/num_examples=50000
I0229 22:36:05.860527 140409846093568 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.359742641448975, loss=0.5732933878898621
I0229 22:36:39.478198 140407379851008 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.347932815551758, loss=0.6437631845474243
I0229 22:37:13.178504 140409846093568 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.9083051681518555, loss=0.6322279572486877
I0229 22:37:47.019613 140407379851008 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.6594696044921875, loss=0.6637997031211853
I0229 22:38:20.666125 140409846093568 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.636110782623291, loss=0.6726920008659363
I0229 22:38:54.383979 140407379851008 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.256508827209473, loss=0.6294331550598145
I0229 22:39:28.071800 140409846093568 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.66861629486084, loss=0.6518232822418213
I0229 22:40:01.752672 140407379851008 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.367882251739502, loss=0.6312701106071472
I0229 22:40:35.496037 140409846093568 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.83953857421875, loss=0.5874337553977966
I0229 22:41:09.144342 140407379851008 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.716472148895264, loss=0.6474246382713318
I0229 22:41:42.867903 140409846093568 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.315433502197266, loss=0.6760509610176086
I0229 22:42:16.562281 140407379851008 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.522610187530518, loss=0.6868833899497986
I0229 22:42:50.264570 140409846093568 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.25501012802124, loss=0.6689684391021729
I0229 22:43:23.947982 140407379851008 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.322626113891602, loss=0.5770305395126343
I0229 22:43:57.870052 140409846093568 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.691547393798828, loss=0.633618175983429
I0229 22:44:31.580172 140407379851008 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.563260078430176, loss=0.6273314356803894
I0229 22:44:33.077338 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:44:39.123103 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:44:48.123472 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:44:50.399737 140573303715648 submission_runner.py:411] Time since start: 63014.39s, 	Step: 180106, 	{'train/accuracy': 0.9593032598495483, 'train/loss': 0.14969423413276672, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0523293018341064, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8204855918884277, 'test/num_examples': 10000, 'score': 60746.7347638607, 'total_duration': 63014.39211153984, 'accumulated_submission_time': 60746.7347638607, 'accumulated_eval_time': 2254.0810379981995, 'accumulated_logging_time': 6.485968351364136}
I0229 22:44:50.446983 140409124681472 logging_writer.py:48] [180106] accumulated_eval_time=2254.081038, accumulated_logging_time=6.485968, accumulated_submission_time=60746.734764, global_step=180106, preemption_count=0, score=60746.734764, test/accuracy=0.630600, test/loss=1.820486, test/num_examples=10000, total_duration=63014.392112, train/accuracy=0.959303, train/loss=0.149694, validation/accuracy=0.755920, validation/loss=1.052329, validation/num_examples=50000
I0229 22:45:22.484125 140409829308160 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.661778450012207, loss=0.5802651643753052
I0229 22:45:56.152787 140409124681472 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.946722030639648, loss=0.5980510711669922
I0229 22:46:29.799757 140409829308160 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.956777572631836, loss=0.6889933347702026
I0229 22:47:03.462651 140409124681472 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.312994956970215, loss=0.6326426267623901
I0229 22:47:37.125407 140409829308160 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.268169403076172, loss=0.5877323150634766
I0229 22:48:10.842969 140409124681472 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.300386905670166, loss=0.5912533402442932
I0229 22:48:44.502301 140409829308160 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.822180271148682, loss=0.6284863948822021
I0229 22:49:18.177208 140409124681472 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.198215007781982, loss=0.6084575653076172
I0229 22:49:51.993800 140409829308160 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.8661041259765625, loss=0.7718918323516846
I0229 22:50:25.644885 140409124681472 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.677946090698242, loss=0.6276928186416626
I0229 22:50:59.317283 140409829308160 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.667403221130371, loss=0.7102981209754944
I0229 22:51:33.005197 140409124681472 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.287338733673096, loss=0.5962132215499878
I0229 22:52:06.688196 140409829308160 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.499325275421143, loss=0.6171587705612183
I0229 22:52:40.326550 140409124681472 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.9209675788879395, loss=0.6702150106430054
I0229 22:53:14.075776 140409829308160 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.140159606933594, loss=0.5418716073036194
I0229 22:53:20.641614 140573303715648 spec.py:321] Evaluating on the training split.
I0229 22:53:26.738684 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 22:53:35.624678 140573303715648 spec.py:349] Evaluating on the test split.
I0229 22:53:37.945534 140573303715648 submission_runner.py:411] Time since start: 63541.94s, 	Step: 181621, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.1473979651927948, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0532605648040771, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.819870948791504, 'test/num_examples': 10000, 'score': 61256.86021757126, 'total_duration': 63541.93805527687, 'accumulated_submission_time': 61256.86021757126, 'accumulated_eval_time': 2271.384888648987, 'accumulated_logging_time': 6.543102502822876}
I0229 22:53:37.995228 140407379851008 logging_writer.py:48] [181621] accumulated_eval_time=2271.384889, accumulated_logging_time=6.543103, accumulated_submission_time=61256.860218, global_step=181621, preemption_count=0, score=61256.860218, test/accuracy=0.628600, test/loss=1.819871, test/num_examples=10000, total_duration=63541.938055, train/accuracy=0.960340, train/loss=0.147398, validation/accuracy=0.755460, validation/loss=1.053261, validation/num_examples=50000
I0229 22:54:04.940166 140408319375104 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.349223613739014, loss=0.5688578486442566
I0229 22:54:38.685348 140407379851008 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.710809230804443, loss=0.6514019966125488
I0229 22:55:12.391282 140408319375104 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.726299285888672, loss=0.5678812861442566
I0229 22:55:46.234068 140407379851008 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.581997871398926, loss=0.6494295597076416
I0229 22:56:19.942876 140408319375104 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.429559707641602, loss=0.6055256724357605
I0229 22:56:53.664299 140407379851008 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.3929948806762695, loss=0.6075485944747925
I0229 22:57:27.372467 140408319375104 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.927228927612305, loss=0.6512906551361084
I0229 22:58:01.144320 140407379851008 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.325403690338135, loss=0.5866271257400513
I0229 22:58:34.855252 140408319375104 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.281679153442383, loss=0.559217631816864
I0229 22:59:08.568811 140407379851008 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.582236289978027, loss=0.6480464339256287
I0229 22:59:42.293567 140408319375104 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.415492057800293, loss=0.6479161977767944
I0229 23:00:15.995866 140407379851008 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.253137111663818, loss=0.5877994894981384
I0229 23:00:49.737107 140408319375104 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.672300815582275, loss=0.6530882716178894
I0229 23:01:23.426717 140407379851008 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.343533992767334, loss=0.5616595149040222
I0229 23:01:57.182001 140408319375104 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.017226696014404, loss=0.5224998593330383
I0229 23:02:08.102645 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:02:14.144630 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:02:22.809319 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:02:25.107672 140573303715648 submission_runner.py:411] Time since start: 64069.10s, 	Step: 183134, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14953239262104034, 'validation/accuracy': 0.7563799619674683, 'validation/loss': 1.051788330078125, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8186100721359253, 'test/num_examples': 10000, 'score': 61766.89761161804, 'total_duration': 64069.10019540787, 'accumulated_submission_time': 61766.89761161804, 'accumulated_eval_time': 2288.3898487091064, 'accumulated_logging_time': 6.603374719619751}
I0229 23:02:25.157853 140407379851008 logging_writer.py:48] [183134] accumulated_eval_time=2288.389849, accumulated_logging_time=6.603375, accumulated_submission_time=61766.897612, global_step=183134, preemption_count=0, score=61766.897612, test/accuracy=0.631400, test/loss=1.818610, test/num_examples=10000, total_duration=64069.100195, train/accuracy=0.960200, train/loss=0.149532, validation/accuracy=0.756380, validation/loss=1.051788, validation/num_examples=50000
I0229 23:02:47.752902 140408319375104 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.521841526031494, loss=0.6503204107284546
I0229 23:03:21.418133 140407379851008 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.108165740966797, loss=0.6195874214172363
I0229 23:03:55.080108 140408319375104 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.2474284172058105, loss=0.577447235584259
I0229 23:04:28.802159 140407379851008 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.39986515045166, loss=0.6049399971961975
I0229 23:05:02.483375 140408319375104 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.746179103851318, loss=0.6303631067276001
I0229 23:05:36.224600 140407379851008 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.601669788360596, loss=0.6556889414787292
I0229 23:06:09.890429 140408319375104 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.224035739898682, loss=0.6228826642036438
I0229 23:06:43.660274 140407379851008 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.494505405426025, loss=0.6153231859207153
I0229 23:07:17.376200 140408319375104 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.63811731338501, loss=0.6701478958129883
I0229 23:07:51.272963 140407379851008 logging_writer.py:48] [184100] global_step=184100, grad_norm=5.308721542358398, loss=0.6435524225234985
I0229 23:08:24.937863 140408319375104 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.388788223266602, loss=0.6356582641601562
I0229 23:08:58.678657 140407379851008 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.29247522354126, loss=0.6325019001960754
I0229 23:09:32.344261 140408319375104 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.125612258911133, loss=0.6545058488845825
I0229 23:10:05.990379 140407379851008 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.385014057159424, loss=0.6445760130882263
I0229 23:10:39.656914 140408319375104 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.85518741607666, loss=0.653519332408905
I0229 23:10:55.301963 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:11:01.498568 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:11:10.494551 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:11:12.855087 140573303715648 submission_runner.py:411] Time since start: 64596.85s, 	Step: 184648, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14842946827411652, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0527596473693848, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.817617654800415, 'test/num_examples': 10000, 'score': 62276.97330927849, 'total_duration': 64596.847626686096, 'accumulated_submission_time': 62276.97330927849, 'accumulated_eval_time': 2305.9429218769073, 'accumulated_logging_time': 6.662899971008301}
I0229 23:11:12.904542 140408319375104 logging_writer.py:48] [184648] accumulated_eval_time=2305.942922, accumulated_logging_time=6.662900, accumulated_submission_time=62276.973309, global_step=184648, preemption_count=0, score=62276.973309, test/accuracy=0.630900, test/loss=1.817618, test/num_examples=10000, total_duration=64596.847627, train/accuracy=0.960778, train/loss=0.148429, validation/accuracy=0.755820, validation/loss=1.052760, validation/num_examples=50000
I0229 23:11:30.736406 140409124681472 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.450357913970947, loss=0.680289089679718
I0229 23:12:04.443373 140408319375104 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.154827117919922, loss=0.5742329955101013
I0229 23:12:38.129696 140409124681472 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.397021770477295, loss=0.6383790373802185
I0229 23:13:11.802628 140408319375104 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.276033878326416, loss=0.5743958950042725
I0229 23:13:45.557611 140409124681472 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.769324779510498, loss=0.6920908093452454
I0229 23:14:19.465349 140408319375104 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.470202445983887, loss=0.615745484828949
I0229 23:14:53.166900 140409124681472 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.810576438903809, loss=0.7001707553863525
I0229 23:15:26.878411 140408319375104 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.359822750091553, loss=0.5912057757377625
I0229 23:16:00.549747 140409124681472 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.538710594177246, loss=0.6674343943595886
I0229 23:16:34.256131 140408319375104 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.530825614929199, loss=0.6021671891212463
I0229 23:17:07.955618 140409124681472 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.905938625335693, loss=0.6927175521850586
I0229 23:17:41.683421 140408319375104 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.4069437980651855, loss=0.5646788477897644
I0229 23:18:15.400405 140409124681472 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.890499591827393, loss=0.6836195588111877
I0229 23:18:49.133215 140408319375104 logging_writer.py:48] [186000] global_step=186000, grad_norm=4.567065715789795, loss=0.65024334192276
I0229 23:19:22.870506 140409124681472 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.033237457275391, loss=0.6105484366416931
I0229 23:19:42.884692 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:19:48.971422 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:19:57.607160 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:19:59.893050 140573303715648 submission_runner.py:411] Time since start: 65123.89s, 	Step: 186161, 	{'train/accuracy': 0.9616150856018066, 'train/loss': 0.14672476053237915, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.051039218902588, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8158655166625977, 'test/num_examples': 10000, 'score': 62786.884974718094, 'total_duration': 65123.88558316231, 'accumulated_submission_time': 62786.884974718094, 'accumulated_eval_time': 2322.951239347458, 'accumulated_logging_time': 6.7222740650177}
I0229 23:19:59.941966 140409837700864 logging_writer.py:48] [186161] accumulated_eval_time=2322.951239, accumulated_logging_time=6.722274, accumulated_submission_time=62786.884975, global_step=186161, preemption_count=0, score=62786.884975, test/accuracy=0.630500, test/loss=1.815866, test/num_examples=10000, total_duration=65123.885583, train/accuracy=0.961615, train/loss=0.146725, validation/accuracy=0.755960, validation/loss=1.051039, validation/num_examples=50000
I0229 23:20:13.512420 140409854486272 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.718157768249512, loss=0.6347930431365967
I0229 23:20:47.200371 140409837700864 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.337063789367676, loss=0.5949993133544922
I0229 23:21:20.865911 140409854486272 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.80096960067749, loss=0.6029550433158875
I0229 23:21:54.515927 140409837700864 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.413506507873535, loss=0.5512732267379761
I0229 23:22:28.176141 140409854486272 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.715968132019043, loss=0.6876896619796753
I0229 23:23:01.908056 140409837700864 logging_writer.py:48] [186700] global_step=186700, grad_norm=4.2558913230896, loss=0.5839196443557739
I0229 23:23:35.582513 140409854486272 logging_writer.py:48] [186800] global_step=186800, grad_norm=4.410171985626221, loss=0.5860854983329773
I0229 23:24:09.293269 140409837700864 logging_writer.py:48] [186900] global_step=186900, grad_norm=4.510361671447754, loss=0.65366530418396
I0229 23:24:42.962074 140409854486272 logging_writer.py:48] [187000] global_step=187000, grad_norm=4.4525227546691895, loss=0.651980996131897
I0229 23:25:16.638108 140409837700864 logging_writer.py:48] [187100] global_step=187100, grad_norm=4.370107173919678, loss=0.5663674473762512
I0229 23:25:50.334967 140409854486272 logging_writer.py:48] [187200] global_step=187200, grad_norm=4.146069049835205, loss=0.5487865209579468
I0229 23:26:24.100018 140409837700864 logging_writer.py:48] [187300] global_step=187300, grad_norm=4.287411689758301, loss=0.5873717069625854
I0229 23:26:57.793485 140409854486272 logging_writer.py:48] [187400] global_step=187400, grad_norm=4.618066310882568, loss=0.6299485564231873
I0229 23:27:31.462985 140409837700864 logging_writer.py:48] [187500] global_step=187500, grad_norm=4.355523109436035, loss=0.666402280330658
I0229 23:28:05.223402 140409854486272 logging_writer.py:48] [187600] global_step=187600, grad_norm=4.226435661315918, loss=0.5867807865142822
I0229 23:28:29.942105 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:28:36.037145 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:28:44.983343 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:28:47.347364 140573303715648 submission_runner.py:411] Time since start: 65651.34s, 	Step: 187675, 	{'train/accuracy': 0.9621930718421936, 'train/loss': 0.14225736260414124, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0514984130859375, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8190748691558838, 'test/num_examples': 10000, 'score': 63296.81457424164, 'total_duration': 65651.33988261223, 'accumulated_submission_time': 63296.81457424164, 'accumulated_eval_time': 2340.3564281463623, 'accumulated_logging_time': 6.781188726425171}
I0229 23:28:47.399828 140407379851008 logging_writer.py:48] [187675] accumulated_eval_time=2340.356428, accumulated_logging_time=6.781189, accumulated_submission_time=63296.814574, global_step=187675, preemption_count=0, score=63296.814574, test/accuracy=0.631400, test/loss=1.819075, test/num_examples=10000, total_duration=65651.339883, train/accuracy=0.962193, train/loss=0.142257, validation/accuracy=0.756000, validation/loss=1.051498, validation/num_examples=50000
I0229 23:28:56.168329 140409124681472 logging_writer.py:48] [187700] global_step=187700, grad_norm=4.323901176452637, loss=0.6542245149612427
I0229 23:29:29.818421 140407379851008 logging_writer.py:48] [187800] global_step=187800, grad_norm=4.1840972900390625, loss=0.6118128299713135
I0229 23:30:03.542694 140409124681472 logging_writer.py:48] [187900] global_step=187900, grad_norm=4.848255157470703, loss=0.6614630818367004
I0229 23:30:37.282932 140407379851008 logging_writer.py:48] [188000] global_step=188000, grad_norm=4.376595973968506, loss=0.6082891225814819
I0229 23:31:11.006512 140409124681472 logging_writer.py:48] [188100] global_step=188100, grad_norm=4.507007598876953, loss=0.6646695733070374
I0229 23:31:44.735859 140407379851008 logging_writer.py:48] [188200] global_step=188200, grad_norm=5.3486409187316895, loss=0.6929568648338318
I0229 23:32:18.471778 140409124681472 logging_writer.py:48] [188300] global_step=188300, grad_norm=4.026933670043945, loss=0.5998751521110535
I0229 23:32:52.182979 140407379851008 logging_writer.py:48] [188400] global_step=188400, grad_norm=4.591797351837158, loss=0.6751545667648315
I0229 23:33:25.914908 140409124681472 logging_writer.py:48] [188500] global_step=188500, grad_norm=4.753396987915039, loss=0.6608656048774719
I0229 23:33:59.610200 140407379851008 logging_writer.py:48] [188600] global_step=188600, grad_norm=4.834062099456787, loss=0.6855823993682861
I0229 23:34:33.341184 140409124681472 logging_writer.py:48] [188700] global_step=188700, grad_norm=4.5043745040893555, loss=0.6209559440612793
I0229 23:35:07.034274 140407379851008 logging_writer.py:48] [188800] global_step=188800, grad_norm=3.977112054824829, loss=0.5846444368362427
I0229 23:35:40.763265 140409124681472 logging_writer.py:48] [188900] global_step=188900, grad_norm=4.470324993133545, loss=0.6579124927520752
I0229 23:36:14.464888 140407379851008 logging_writer.py:48] [189000] global_step=189000, grad_norm=4.459738254547119, loss=0.6796129941940308
I0229 23:36:48.184496 140409124681472 logging_writer.py:48] [189100] global_step=189100, grad_norm=4.698011875152588, loss=0.6315497159957886
I0229 23:37:17.632083 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:37:23.705532 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:37:32.505416 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:37:34.787332 140573303715648 submission_runner.py:411] Time since start: 66178.78s, 	Step: 189189, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.14548906683921814, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.0513103008270264, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8167539834976196, 'test/num_examples': 10000, 'score': 63806.97748732567, 'total_duration': 66178.77985548973, 'accumulated_submission_time': 63806.97748732567, 'accumulated_eval_time': 2357.5116155147552, 'accumulated_logging_time': 6.8434693813323975}
I0229 23:37:34.839591 140408319375104 logging_writer.py:48] [189189] accumulated_eval_time=2357.511616, accumulated_logging_time=6.843469, accumulated_submission_time=63806.977487, global_step=189189, preemption_count=0, score=63806.977487, test/accuracy=0.631200, test/loss=1.816754, test/num_examples=10000, total_duration=66178.779855, train/accuracy=0.960499, train/loss=0.145489, validation/accuracy=0.756260, validation/loss=1.051310, validation/num_examples=50000
I0229 23:37:38.888486 140409124681472 logging_writer.py:48] [189200] global_step=189200, grad_norm=4.188870429992676, loss=0.617085337638855
I0229 23:38:12.595141 140408319375104 logging_writer.py:48] [189300] global_step=189300, grad_norm=4.927593231201172, loss=0.6872196793556213
I0229 23:38:46.376031 140409124681472 logging_writer.py:48] [189400] global_step=189400, grad_norm=4.532571792602539, loss=0.7153184413909912
I0229 23:39:20.072739 140408319375104 logging_writer.py:48] [189500] global_step=189500, grad_norm=4.618448734283447, loss=0.6485892534255981
I0229 23:39:53.708628 140409124681472 logging_writer.py:48] [189600] global_step=189600, grad_norm=4.509309768676758, loss=0.6883538365364075
I0229 23:40:27.397996 140408319375104 logging_writer.py:48] [189700] global_step=189700, grad_norm=3.9714622497558594, loss=0.5418390035629272
I0229 23:41:01.109762 140409124681472 logging_writer.py:48] [189800] global_step=189800, grad_norm=4.691594123840332, loss=0.6572001576423645
I0229 23:41:34.775957 140408319375104 logging_writer.py:48] [189900] global_step=189900, grad_norm=4.255383491516113, loss=0.5994777679443359
I0229 23:42:08.466368 140409124681472 logging_writer.py:48] [190000] global_step=190000, grad_norm=4.610507488250732, loss=0.6991527080535889
I0229 23:42:42.211910 140408319375104 logging_writer.py:48] [190100] global_step=190100, grad_norm=4.381168365478516, loss=0.6187641620635986
I0229 23:43:15.873347 140409124681472 logging_writer.py:48] [190200] global_step=190200, grad_norm=4.247535228729248, loss=0.5843257904052734
I0229 23:43:49.552438 140408319375104 logging_writer.py:48] [190300] global_step=190300, grad_norm=5.26821756362915, loss=0.7150372862815857
I0229 23:44:23.238715 140409124681472 logging_writer.py:48] [190400] global_step=190400, grad_norm=4.607168674468994, loss=0.6485596299171448
I0229 23:44:57.049438 140408319375104 logging_writer.py:48] [190500] global_step=190500, grad_norm=4.511441707611084, loss=0.6177423000335693
I0229 23:45:30.722710 140409124681472 logging_writer.py:48] [190600] global_step=190600, grad_norm=4.391709804534912, loss=0.568151593208313
I0229 23:46:04.408031 140408319375104 logging_writer.py:48] [190700] global_step=190700, grad_norm=4.1447272300720215, loss=0.6205673217773438
I0229 23:46:04.893311 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:46:10.913334 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:46:19.597823 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:46:21.886268 140573303715648 submission_runner.py:411] Time since start: 66705.88s, 	Step: 190703, 	{'train/accuracy': 0.9595822691917419, 'train/loss': 0.15023556351661682, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0518090724945068, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8174747228622437, 'test/num_examples': 10000, 'score': 64316.961555957794, 'total_duration': 66705.87880730629, 'accumulated_submission_time': 64316.961555957794, 'accumulated_eval_time': 2374.5045187473297, 'accumulated_logging_time': 6.90625524520874}
I0229 23:46:21.935143 140409846093568 logging_writer.py:48] [190703] accumulated_eval_time=2374.504519, accumulated_logging_time=6.906255, accumulated_submission_time=64316.961556, global_step=190703, preemption_count=0, score=64316.961556, test/accuracy=0.630900, test/loss=1.817475, test/num_examples=10000, total_duration=66705.878807, train/accuracy=0.959582, train/loss=0.150236, validation/accuracy=0.755740, validation/loss=1.051809, validation/num_examples=50000
I0229 23:46:54.889329 140409854486272 logging_writer.py:48] [190800] global_step=190800, grad_norm=4.407320976257324, loss=0.6757575273513794
I0229 23:47:28.586170 140409846093568 logging_writer.py:48] [190900] global_step=190900, grad_norm=4.314751148223877, loss=0.6161192655563354
I0229 23:48:02.260205 140409854486272 logging_writer.py:48] [191000] global_step=191000, grad_norm=4.637888431549072, loss=0.705582320690155
I0229 23:48:35.976176 140409846093568 logging_writer.py:48] [191100] global_step=191100, grad_norm=4.51835823059082, loss=0.6268792152404785
I0229 23:49:09.655337 140409854486272 logging_writer.py:48] [191200] global_step=191200, grad_norm=4.340986728668213, loss=0.5891912579536438
I0229 23:49:43.328809 140409846093568 logging_writer.py:48] [191300] global_step=191300, grad_norm=4.94715690612793, loss=0.6794629693031311
I0229 23:50:17.069158 140409854486272 logging_writer.py:48] [191400] global_step=191400, grad_norm=4.187372207641602, loss=0.5373529195785522
I0229 23:50:50.850468 140409846093568 logging_writer.py:48] [191500] global_step=191500, grad_norm=4.828629493713379, loss=0.6572821140289307
I0229 23:51:24.542317 140409854486272 logging_writer.py:48] [191600] global_step=191600, grad_norm=4.32340145111084, loss=0.6006296873092651
I0229 23:51:58.277479 140409846093568 logging_writer.py:48] [191700] global_step=191700, grad_norm=4.776279449462891, loss=0.6662030220031738
I0229 23:52:31.969575 140409854486272 logging_writer.py:48] [191800] global_step=191800, grad_norm=4.610983371734619, loss=0.6618475317955017
I0229 23:53:05.690641 140409846093568 logging_writer.py:48] [191900] global_step=191900, grad_norm=4.341846466064453, loss=0.611840009689331
I0229 23:53:39.352221 140409854486272 logging_writer.py:48] [192000] global_step=192000, grad_norm=4.12087345123291, loss=0.6659955382347107
I0229 23:54:13.061133 140409846093568 logging_writer.py:48] [192100] global_step=192100, grad_norm=4.280242919921875, loss=0.6616929769515991
I0229 23:54:46.736342 140409854486272 logging_writer.py:48] [192200] global_step=192200, grad_norm=4.4053955078125, loss=0.6458788514137268
I0229 23:54:51.934890 140573303715648 spec.py:321] Evaluating on the training split.
I0229 23:54:58.754942 140573303715648 spec.py:333] Evaluating on the validation split.
I0229 23:55:07.732641 140573303715648 spec.py:349] Evaluating on the test split.
I0229 23:55:10.261366 140573303715648 submission_runner.py:411] Time since start: 67234.25s, 	Step: 192217, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14702478051185608, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0514979362487793, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8174049854278564, 'test/num_examples': 10000, 'score': 64826.8895072937, 'total_duration': 67234.2537624836, 'accumulated_submission_time': 64826.8895072937, 'accumulated_eval_time': 2392.830799102783, 'accumulated_logging_time': 6.966517210006714}
I0229 23:55:10.307557 140409124681472 logging_writer.py:48] [192217] accumulated_eval_time=2392.830799, accumulated_logging_time=6.966517, accumulated_submission_time=64826.889507, global_step=192217, preemption_count=0, score=64826.889507, test/accuracy=0.631400, test/loss=1.817405, test/num_examples=10000, total_duration=67234.253762, train/accuracy=0.960539, train/loss=0.147025, validation/accuracy=0.755860, validation/loss=1.051498, validation/num_examples=50000
I0229 23:55:38.601351 140409829308160 logging_writer.py:48] [192300] global_step=192300, grad_norm=4.6023688316345215, loss=0.6593804955482483
I0229 23:56:12.286063 140409124681472 logging_writer.py:48] [192400] global_step=192400, grad_norm=4.370792388916016, loss=0.6864532232284546
I0229 23:56:45.993480 140409829308160 logging_writer.py:48] [192500] global_step=192500, grad_norm=4.921908855438232, loss=0.7007406949996948
I0229 23:57:19.729835 140409124681472 logging_writer.py:48] [192600] global_step=192600, grad_norm=4.594620704650879, loss=0.6098064184188843
I0229 23:57:53.458897 140409829308160 logging_writer.py:48] [192700] global_step=192700, grad_norm=4.593227863311768, loss=0.591251015663147
I0229 23:58:27.138480 140409124681472 logging_writer.py:48] [192800] global_step=192800, grad_norm=4.450821399688721, loss=0.6006152033805847
I0229 23:59:00.785538 140409829308160 logging_writer.py:48] [192900] global_step=192900, grad_norm=4.54933500289917, loss=0.6389731168746948
I0229 23:59:34.556730 140409124681472 logging_writer.py:48] [193000] global_step=193000, grad_norm=4.581481456756592, loss=0.6385118961334229
I0301 00:00:08.270441 140409829308160 logging_writer.py:48] [193100] global_step=193100, grad_norm=4.344990253448486, loss=0.5809486508369446
I0301 00:00:41.971944 140409124681472 logging_writer.py:48] [193200] global_step=193200, grad_norm=4.643707275390625, loss=0.661923885345459
I0301 00:01:15.666894 140409829308160 logging_writer.py:48] [193300] global_step=193300, grad_norm=4.8487348556518555, loss=0.6368054151535034
I0301 00:01:49.438056 140409124681472 logging_writer.py:48] [193400] global_step=193400, grad_norm=4.097428321838379, loss=0.6321592926979065
I0301 00:02:23.129061 140409829308160 logging_writer.py:48] [193500] global_step=193500, grad_norm=4.5891242027282715, loss=0.6506699919700623
I0301 00:02:56.929613 140409124681472 logging_writer.py:48] [193600] global_step=193600, grad_norm=4.3422369956970215, loss=0.6109285950660706
I0301 00:03:30.647652 140409829308160 logging_writer.py:48] [193700] global_step=193700, grad_norm=4.526450157165527, loss=0.6520221829414368
I0301 00:03:40.562280 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:03:46.613851 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:03:55.533294 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:03:57.831984 140573303715648 submission_runner.py:411] Time since start: 67761.82s, 	Step: 193731, 	{'train/accuracy': 0.9588249325752258, 'train/loss': 0.14946289360523224, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0511360168457031, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8178002834320068, 'test/num_examples': 10000, 'score': 65337.07450246811, 'total_duration': 67761.82452487946, 'accumulated_submission_time': 65337.07450246811, 'accumulated_eval_time': 2410.1004548072815, 'accumulated_logging_time': 7.022372007369995}
I0301 00:03:57.885917 140408319375104 logging_writer.py:48] [193731] accumulated_eval_time=2410.100455, accumulated_logging_time=7.022372, accumulated_submission_time=65337.074502, global_step=193731, preemption_count=0, score=65337.074502, test/accuracy=0.631000, test/loss=1.817800, test/num_examples=10000, total_duration=67761.824525, train/accuracy=0.958825, train/loss=0.149463, validation/accuracy=0.755640, validation/loss=1.051136, validation/num_examples=50000
I0301 00:04:21.474568 140409124681472 logging_writer.py:48] [193800] global_step=193800, grad_norm=4.304569721221924, loss=0.6342366337776184
I0301 00:04:55.184577 140408319375104 logging_writer.py:48] [193900] global_step=193900, grad_norm=4.586500644683838, loss=0.6117646098136902
I0301 00:05:28.854132 140409124681472 logging_writer.py:48] [194000] global_step=194000, grad_norm=4.732912540435791, loss=0.6353514194488525
I0301 00:06:02.582835 140408319375104 logging_writer.py:48] [194100] global_step=194100, grad_norm=4.511962890625, loss=0.6065662503242493
I0301 00:06:36.253311 140409124681472 logging_writer.py:48] [194200] global_step=194200, grad_norm=4.551942348480225, loss=0.6314184665679932
I0301 00:07:09.987616 140408319375104 logging_writer.py:48] [194300] global_step=194300, grad_norm=4.657321929931641, loss=0.5740511417388916
I0301 00:07:43.690626 140409124681472 logging_writer.py:48] [194400] global_step=194400, grad_norm=5.00723934173584, loss=0.6851346492767334
I0301 00:08:17.438995 140408319375104 logging_writer.py:48] [194500] global_step=194500, grad_norm=4.510213375091553, loss=0.6426147222518921
I0301 00:08:51.129356 140409124681472 logging_writer.py:48] [194600] global_step=194600, grad_norm=4.331327438354492, loss=0.5621316432952881
I0301 00:09:24.901046 140408319375104 logging_writer.py:48] [194700] global_step=194700, grad_norm=4.735104560852051, loss=0.6454389095306396
I0301 00:09:58.634427 140409124681472 logging_writer.py:48] [194800] global_step=194800, grad_norm=4.258030414581299, loss=0.5848702192306519
I0301 00:10:32.393634 140408319375104 logging_writer.py:48] [194900] global_step=194900, grad_norm=4.7624640464782715, loss=0.6877674460411072
I0301 00:11:06.084084 140409124681472 logging_writer.py:48] [195000] global_step=195000, grad_norm=4.1932196617126465, loss=0.5680728554725647
I0301 00:11:39.844990 140408319375104 logging_writer.py:48] [195100] global_step=195100, grad_norm=4.006110191345215, loss=0.5668280124664307
I0301 00:12:13.567154 140409124681472 logging_writer.py:48] [195200] global_step=195200, grad_norm=4.47852087020874, loss=0.6409389972686768
I0301 00:12:27.873384 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:12:33.967366 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:12:42.824608 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:12:45.122233 140573303715648 submission_runner.py:411] Time since start: 68289.11s, 	Step: 195244, 	{'train/accuracy': 0.9619937539100647, 'train/loss': 0.14562161266803741, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.052322268486023, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.81663978099823, 'test/num_examples': 10000, 'score': 65846.99104523659, 'total_duration': 68289.11477446556, 'accumulated_submission_time': 65846.99104523659, 'accumulated_eval_time': 2427.3492562770844, 'accumulated_logging_time': 7.086922645568848}
I0301 00:12:45.172284 140409854486272 logging_writer.py:48] [195244] accumulated_eval_time=2427.349256, accumulated_logging_time=7.086923, accumulated_submission_time=65846.991045, global_step=195244, preemption_count=0, score=65846.991045, test/accuracy=0.631600, test/loss=1.816640, test/num_examples=10000, total_duration=68289.114774, train/accuracy=0.961994, train/loss=0.145622, validation/accuracy=0.755840, validation/loss=1.052322, validation/num_examples=50000
I0301 00:13:04.384583 140409862878976 logging_writer.py:48] [195300] global_step=195300, grad_norm=4.268990516662598, loss=0.600274384021759
I0301 00:13:38.050057 140409854486272 logging_writer.py:48] [195400] global_step=195400, grad_norm=4.524359703063965, loss=0.6276371479034424
I0301 00:14:11.784378 140409862878976 logging_writer.py:48] [195500] global_step=195500, grad_norm=4.765913486480713, loss=0.6182593703269958
I0301 00:14:45.420503 140409854486272 logging_writer.py:48] [195600] global_step=195600, grad_norm=4.550567626953125, loss=0.6445868611335754
I0301 00:15:19.178599 140409862878976 logging_writer.py:48] [195700] global_step=195700, grad_norm=4.423287868499756, loss=0.6220914125442505
I0301 00:15:52.887774 140409854486272 logging_writer.py:48] [195800] global_step=195800, grad_norm=5.1242265701293945, loss=0.6797035336494446
I0301 00:16:26.581779 140409862878976 logging_writer.py:48] [195900] global_step=195900, grad_norm=4.745988845825195, loss=0.6502192616462708
I0301 00:17:00.265657 140409854486272 logging_writer.py:48] [196000] global_step=196000, grad_norm=4.603808403015137, loss=0.6000841856002808
I0301 00:17:34.003788 140409862878976 logging_writer.py:48] [196100] global_step=196100, grad_norm=4.727138042449951, loss=0.6811861395835876
I0301 00:18:07.648772 140409854486272 logging_writer.py:48] [196200] global_step=196200, grad_norm=4.499798774719238, loss=0.5732752084732056
I0301 00:18:41.361215 140409862878976 logging_writer.py:48] [196300] global_step=196300, grad_norm=4.394832134246826, loss=0.5747693181037903
I0301 00:19:15.052483 140409854486272 logging_writer.py:48] [196400] global_step=196400, grad_norm=4.44785737991333, loss=0.6577486991882324
I0301 00:19:48.710925 140409862878976 logging_writer.py:48] [196500] global_step=196500, grad_norm=4.576354503631592, loss=0.6253248453140259
I0301 00:20:22.455558 140409854486272 logging_writer.py:48] [196600] global_step=196600, grad_norm=4.530418872833252, loss=0.585358738899231
I0301 00:20:56.150625 140409862878976 logging_writer.py:48] [196700] global_step=196700, grad_norm=4.372025489807129, loss=0.5883762240409851
I0301 00:21:15.243374 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:21:21.371716 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:21:30.104232 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:21:32.409964 140573303715648 submission_runner.py:411] Time since start: 68816.40s, 	Step: 196758, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14474235475063324, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.052221417427063, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8178486824035645, 'test/num_examples': 10000, 'score': 66356.99051713943, 'total_duration': 68816.40245962143, 'accumulated_submission_time': 66356.99051713943, 'accumulated_eval_time': 2444.515809059143, 'accumulated_logging_time': 7.1465301513671875}
I0301 00:21:32.460464 140409124681472 logging_writer.py:48] [196758] accumulated_eval_time=2444.515809, accumulated_logging_time=7.146530, accumulated_submission_time=66356.990517, global_step=196758, preemption_count=0, score=66356.990517, test/accuracy=0.632100, test/loss=1.817849, test/num_examples=10000, total_duration=68816.402460, train/accuracy=0.961276, train/loss=0.144742, validation/accuracy=0.755940, validation/loss=1.052221, validation/num_examples=50000
I0301 00:21:46.991268 140409829308160 logging_writer.py:48] [196800] global_step=196800, grad_norm=4.537612438201904, loss=0.655269205570221
I0301 00:22:20.658082 140409124681472 logging_writer.py:48] [196900] global_step=196900, grad_norm=4.986550331115723, loss=0.6598400473594666
I0301 00:22:54.421130 140409829308160 logging_writer.py:48] [197000] global_step=197000, grad_norm=4.583602428436279, loss=0.6083884239196777
I0301 00:23:28.090845 140409124681472 logging_writer.py:48] [197100] global_step=197100, grad_norm=4.260181427001953, loss=0.6190889477729797
I0301 00:24:01.839535 140409829308160 logging_writer.py:48] [197200] global_step=197200, grad_norm=4.477245807647705, loss=0.6127873659133911
I0301 00:24:35.571504 140409124681472 logging_writer.py:48] [197300] global_step=197300, grad_norm=4.488320827484131, loss=0.6045104265213013
I0301 00:25:09.271074 140409829308160 logging_writer.py:48] [197400] global_step=197400, grad_norm=4.638896942138672, loss=0.6254383325576782
I0301 00:25:43.011776 140409124681472 logging_writer.py:48] [197500] global_step=197500, grad_norm=4.255362510681152, loss=0.5505591630935669
I0301 00:26:16.701183 140409829308160 logging_writer.py:48] [197600] global_step=197600, grad_norm=4.870100975036621, loss=0.6233713626861572
I0301 00:26:50.423813 140409124681472 logging_writer.py:48] [197700] global_step=197700, grad_norm=4.701943397521973, loss=0.7027035355567932
I0301 00:27:24.174521 140409829308160 logging_writer.py:48] [197800] global_step=197800, grad_norm=4.510547637939453, loss=0.6246735453605652
I0301 00:27:57.900477 140409124681472 logging_writer.py:48] [197900] global_step=197900, grad_norm=4.058775424957275, loss=0.5393084287643433
I0301 00:28:31.675217 140409829308160 logging_writer.py:48] [198000] global_step=198000, grad_norm=4.4996209144592285, loss=0.5933205485343933
I0301 00:29:05.370634 140409124681472 logging_writer.py:48] [198100] global_step=198100, grad_norm=4.5058465003967285, loss=0.6307616233825684
I0301 00:29:39.146286 140409829308160 logging_writer.py:48] [198200] global_step=198200, grad_norm=4.621138572692871, loss=0.5702613592147827
I0301 00:30:02.522208 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:30:08.638610 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:30:17.400027 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:30:19.708838 140573303715648 submission_runner.py:411] Time since start: 69343.70s, 	Step: 198271, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.1464032083749771, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.0522739887237549, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8155436515808105, 'test/num_examples': 10000, 'score': 66866.98143553734, 'total_duration': 69343.70125079155, 'accumulated_submission_time': 66866.98143553734, 'accumulated_eval_time': 2461.702263355255, 'accumulated_logging_time': 7.207445859909058}
I0301 00:30:19.759351 140408319375104 logging_writer.py:48] [198271] accumulated_eval_time=2461.702263, accumulated_logging_time=7.207446, accumulated_submission_time=66866.981436, global_step=198271, preemption_count=0, score=66866.981436, test/accuracy=0.630900, test/loss=1.815544, test/num_examples=10000, total_duration=69343.701251, train/accuracy=0.961296, train/loss=0.146403, validation/accuracy=0.755500, validation/loss=1.052274, validation/num_examples=50000
I0301 00:30:29.876068 140409124681472 logging_writer.py:48] [198300] global_step=198300, grad_norm=4.250319004058838, loss=0.6446476578712463
I0301 00:31:03.566425 140408319375104 logging_writer.py:48] [198400] global_step=198400, grad_norm=4.394944667816162, loss=0.6365795135498047
I0301 00:31:37.199182 140409124681472 logging_writer.py:48] [198500] global_step=198500, grad_norm=5.100826263427734, loss=0.5982586145401001
I0301 00:32:10.874955 140408319375104 logging_writer.py:48] [198600] global_step=198600, grad_norm=4.733415126800537, loss=0.6509130001068115
I0301 00:32:44.523082 140409124681472 logging_writer.py:48] [198700] global_step=198700, grad_norm=4.460152626037598, loss=0.6370788812637329
I0301 00:33:18.239996 140408319375104 logging_writer.py:48] [198800] global_step=198800, grad_norm=4.471211910247803, loss=0.628068208694458
I0301 00:33:52.058327 140409124681472 logging_writer.py:48] [198900] global_step=198900, grad_norm=4.2801594734191895, loss=0.6192285418510437
I0301 00:34:25.722200 140408319375104 logging_writer.py:48] [199000] global_step=199000, grad_norm=5.084434509277344, loss=0.583389937877655
I0301 00:34:59.392881 140409124681472 logging_writer.py:48] [199100] global_step=199100, grad_norm=4.56575345993042, loss=0.6357683539390564
I0301 00:35:33.089113 140408319375104 logging_writer.py:48] [199200] global_step=199200, grad_norm=4.914201736450195, loss=0.6830342411994934
I0301 00:36:06.802643 140409124681472 logging_writer.py:48] [199300] global_step=199300, grad_norm=4.344317436218262, loss=0.6039412021636963
I0301 00:36:40.478493 140408319375104 logging_writer.py:48] [199400] global_step=199400, grad_norm=4.813410758972168, loss=0.6513913869857788
I0301 00:37:14.207034 140409124681472 logging_writer.py:48] [199500] global_step=199500, grad_norm=5.096917152404785, loss=0.7039549350738525
I0301 00:37:47.916731 140408319375104 logging_writer.py:48] [199600] global_step=199600, grad_norm=4.603658199310303, loss=0.6121317744255066
I0301 00:38:21.633348 140409124681472 logging_writer.py:48] [199700] global_step=199700, grad_norm=4.49213981628418, loss=0.6557798981666565
I0301 00:38:49.765415 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:38:55.807370 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:39:04.794173 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:39:07.088466 140573303715648 submission_runner.py:411] Time since start: 69871.08s, 	Step: 199785, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.1467967927455902, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0520784854888916, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8178911209106445, 'test/num_examples': 10000, 'score': 67376.91571760178, 'total_duration': 69871.08100628853, 'accumulated_submission_time': 67376.91571760178, 'accumulated_eval_time': 2479.025264263153, 'accumulated_logging_time': 7.268761396408081}
I0301 00:39:07.146906 140409854486272 logging_writer.py:48] [199785] accumulated_eval_time=2479.025264, accumulated_logging_time=7.268761, accumulated_submission_time=67376.915718, global_step=199785, preemption_count=0, score=67376.915718, test/accuracy=0.630800, test/loss=1.817891, test/num_examples=10000, total_duration=69871.081006, train/accuracy=0.960300, train/loss=0.146797, validation/accuracy=0.755820, validation/loss=1.052078, validation/num_examples=50000
I0301 00:39:12.547593 140409862878976 logging_writer.py:48] [199800] global_step=199800, grad_norm=4.263613700866699, loss=0.6192640662193298
I0301 00:39:46.365919 140409854486272 logging_writer.py:48] [199900] global_step=199900, grad_norm=4.546828269958496, loss=0.6283367276191711
I0301 00:40:20.073155 140409862878976 logging_writer.py:48] [200000] global_step=200000, grad_norm=4.293816089630127, loss=0.609311044216156
I0301 00:40:53.768320 140409854486272 logging_writer.py:48] [200100] global_step=200100, grad_norm=4.53016471862793, loss=0.6342331171035767
I0301 00:41:27.478288 140409862878976 logging_writer.py:48] [200200] global_step=200200, grad_norm=4.33699893951416, loss=0.589168131351471
I0301 00:42:01.202479 140409854486272 logging_writer.py:48] [200300] global_step=200300, grad_norm=4.919884204864502, loss=0.7525588274002075
I0301 00:42:34.884005 140409862878976 logging_writer.py:48] [200400] global_step=200400, grad_norm=4.42619514465332, loss=0.6030089855194092
I0301 00:43:08.645564 140409854486272 logging_writer.py:48] [200500] global_step=200500, grad_norm=4.7774128913879395, loss=0.6021726727485657
I0301 00:43:42.303384 140409862878976 logging_writer.py:48] [200600] global_step=200600, grad_norm=4.469173431396484, loss=0.6808676719665527
I0301 00:44:15.987418 140409854486272 logging_writer.py:48] [200700] global_step=200700, grad_norm=5.003758907318115, loss=0.669095516204834
I0301 00:44:49.700041 140409862878976 logging_writer.py:48] [200800] global_step=200800, grad_norm=5.113779067993164, loss=0.6598093509674072
I0301 00:45:23.356244 140409854486272 logging_writer.py:48] [200900] global_step=200900, grad_norm=4.360623359680176, loss=0.5560156106948853
I0301 00:45:57.095790 140409862878976 logging_writer.py:48] [201000] global_step=201000, grad_norm=4.674529075622559, loss=0.6244674921035767
I0301 00:46:30.774937 140409854486272 logging_writer.py:48] [201100] global_step=201100, grad_norm=4.927038669586182, loss=0.6291952729225159
I0301 00:47:04.441709 140409862878976 logging_writer.py:48] [201200] global_step=201200, grad_norm=4.475525379180908, loss=0.6452975273132324
I0301 00:47:37.347206 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:47:43.390912 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:47:52.367731 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:47:54.668527 140573303715648 submission_runner.py:411] Time since start: 70398.66s, 	Step: 201299, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.14999347925186157, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.052374243736267, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.818956971168518, 'test/num_examples': 10000, 'score': 67887.04694342613, 'total_duration': 70398.6608979702, 'accumulated_submission_time': 67887.04694342613, 'accumulated_eval_time': 2496.34637260437, 'accumulated_logging_time': 7.337639093399048}
I0301 00:47:54.722941 140407379851008 logging_writer.py:48] [201299] accumulated_eval_time=2496.346373, accumulated_logging_time=7.337639, accumulated_submission_time=67887.046943, global_step=201299, preemption_count=0, score=67887.046943, test/accuracy=0.630600, test/loss=1.818957, test/num_examples=10000, total_duration=70398.660898, train/accuracy=0.959901, train/loss=0.149993, validation/accuracy=0.756040, validation/loss=1.052374, validation/num_examples=50000
I0301 00:47:55.404455 140408319375104 logging_writer.py:48] [201300] global_step=201300, grad_norm=4.532520294189453, loss=0.6173309683799744
I0301 00:48:29.117528 140407379851008 logging_writer.py:48] [201400] global_step=201400, grad_norm=4.298094272613525, loss=0.5867257714271545
I0301 00:49:02.772669 140408319375104 logging_writer.py:48] [201500] global_step=201500, grad_norm=4.371047496795654, loss=0.4821697473526001
I0301 00:49:36.426196 140407379851008 logging_writer.py:48] [201600] global_step=201600, grad_norm=4.502434253692627, loss=0.650003969669342
I0301 00:50:10.108261 140408319375104 logging_writer.py:48] [201700] global_step=201700, grad_norm=4.6398444175720215, loss=0.6258794665336609
I0301 00:50:43.784954 140407379851008 logging_writer.py:48] [201800] global_step=201800, grad_norm=4.339041233062744, loss=0.6157037615776062
I0301 00:51:17.462487 140408319375104 logging_writer.py:48] [201900] global_step=201900, grad_norm=4.535589218139648, loss=0.619143009185791
I0301 00:51:51.184074 140407379851008 logging_writer.py:48] [202000] global_step=202000, grad_norm=4.771363735198975, loss=0.6259201169013977
I0301 00:52:24.939754 140408319375104 logging_writer.py:48] [202100] global_step=202100, grad_norm=4.2605204582214355, loss=0.6360048055648804
I0301 00:52:58.646105 140407379851008 logging_writer.py:48] [202200] global_step=202200, grad_norm=4.7636284828186035, loss=0.6119004487991333
I0301 00:53:32.287721 140408319375104 logging_writer.py:48] [202300] global_step=202300, grad_norm=4.534456729888916, loss=0.592444121837616
I0301 00:54:06.021453 140407379851008 logging_writer.py:48] [202400] global_step=202400, grad_norm=4.488908767700195, loss=0.637654721736908
I0301 00:54:39.722647 140408319375104 logging_writer.py:48] [202500] global_step=202500, grad_norm=4.462215423583984, loss=0.6806608438491821
I0301 00:55:13.475086 140407379851008 logging_writer.py:48] [202600] global_step=202600, grad_norm=4.239595413208008, loss=0.6030870676040649
I0301 00:55:47.177888 140408319375104 logging_writer.py:48] [202700] global_step=202700, grad_norm=4.438564300537109, loss=0.673895537853241
I0301 00:56:20.925180 140407379851008 logging_writer.py:48] [202800] global_step=202800, grad_norm=4.372342109680176, loss=0.5769042372703552
I0301 00:56:24.784920 140573303715648 spec.py:321] Evaluating on the training split.
I0301 00:56:30.950870 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 00:56:39.916786 140573303715648 spec.py:349] Evaluating on the test split.
I0301 00:56:42.185205 140573303715648 submission_runner.py:411] Time since start: 70926.18s, 	Step: 202813, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14336808025836945, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0518240928649902, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.817797303199768, 'test/num_examples': 10000, 'score': 68397.03761839867, 'total_duration': 70926.17774367332, 'accumulated_submission_time': 68397.03761839867, 'accumulated_eval_time': 2513.746617078781, 'accumulated_logging_time': 7.402098655700684}
I0301 00:56:42.235115 140409846093568 logging_writer.py:48] [202813] accumulated_eval_time=2513.746617, accumulated_logging_time=7.402099, accumulated_submission_time=68397.037618, global_step=202813, preemption_count=0, score=68397.037618, test/accuracy=0.631700, test/loss=1.817797, test/num_examples=10000, total_duration=70926.177744, train/accuracy=0.961735, train/loss=0.143368, validation/accuracy=0.755840, validation/loss=1.051824, validation/num_examples=50000
I0301 00:57:11.900296 140409854486272 logging_writer.py:48] [202900] global_step=202900, grad_norm=4.27290153503418, loss=0.6196812987327576
I0301 00:57:45.578597 140409846093568 logging_writer.py:48] [203000] global_step=203000, grad_norm=4.938678741455078, loss=0.6250765323638916
I0301 00:58:19.327624 140409854486272 logging_writer.py:48] [203100] global_step=203100, grad_norm=4.649634838104248, loss=0.6047845482826233
I0301 00:58:53.034606 140409846093568 logging_writer.py:48] [203200] global_step=203200, grad_norm=4.475520610809326, loss=0.5740283131599426
I0301 00:59:26.672040 140409854486272 logging_writer.py:48] [203300] global_step=203300, grad_norm=4.675662994384766, loss=0.6562342643737793
I0301 01:00:00.337464 140409846093568 logging_writer.py:48] [203400] global_step=203400, grad_norm=4.232132911682129, loss=0.6112689971923828
I0301 01:00:34.052255 140409854486272 logging_writer.py:48] [203500] global_step=203500, grad_norm=4.640519142150879, loss=0.6267739534378052
I0301 01:01:07.737557 140409846093568 logging_writer.py:48] [203600] global_step=203600, grad_norm=4.6432390213012695, loss=0.7159072160720825
I0301 01:01:41.446992 140409854486272 logging_writer.py:48] [203700] global_step=203700, grad_norm=4.729671478271484, loss=0.6210504174232483
I0301 01:02:15.159005 140409846093568 logging_writer.py:48] [203800] global_step=203800, grad_norm=4.302073001861572, loss=0.6942172646522522
I0301 01:02:48.830354 140409854486272 logging_writer.py:48] [203900] global_step=203900, grad_norm=4.716772079467773, loss=0.6634286642074585
I0301 01:03:22.602484 140409846093568 logging_writer.py:48] [204000] global_step=204000, grad_norm=4.39207649230957, loss=0.5915300846099854
I0301 01:03:56.288832 140409854486272 logging_writer.py:48] [204100] global_step=204100, grad_norm=4.527872562408447, loss=0.5882810354232788
I0301 01:04:30.065318 140409846093568 logging_writer.py:48] [204200] global_step=204200, grad_norm=4.2545390129089355, loss=0.6203290224075317
I0301 01:05:03.768644 140409854486272 logging_writer.py:48] [204300] global_step=204300, grad_norm=4.2182722091674805, loss=0.5937482118606567
I0301 01:05:12.321192 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:05:18.346928 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:05:27.102649 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:05:29.386728 140573303715648 submission_runner.py:411] Time since start: 71453.38s, 	Step: 204327, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14639762043952942, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0515127182006836, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8176378011703491, 'test/num_examples': 10000, 'score': 68907.05328130722, 'total_duration': 71453.3792629242, 'accumulated_submission_time': 68907.05328130722, 'accumulated_eval_time': 2530.812096595764, 'accumulated_logging_time': 7.462094306945801}
I0301 01:05:29.438469 140408319375104 logging_writer.py:48] [204327] accumulated_eval_time=2530.812097, accumulated_logging_time=7.462094, accumulated_submission_time=68907.053281, global_step=204327, preemption_count=0, score=68907.053281, test/accuracy=0.630500, test/loss=1.817638, test/num_examples=10000, total_duration=71453.379263, train/accuracy=0.960678, train/loss=0.146398, validation/accuracy=0.756060, validation/loss=1.051513, validation/num_examples=50000
I0301 01:05:54.336034 140409829308160 logging_writer.py:48] [204400] global_step=204400, grad_norm=4.420405864715576, loss=0.6027301549911499
I0301 01:06:27.963927 140408319375104 logging_writer.py:48] [204500] global_step=204500, grad_norm=4.173832893371582, loss=0.5557921528816223
I0301 01:07:01.627692 140409829308160 logging_writer.py:48] [204600] global_step=204600, grad_norm=4.666873455047607, loss=0.7379703521728516
I0301 01:07:35.320482 140408319375104 logging_writer.py:48] [204700] global_step=204700, grad_norm=4.813252925872803, loss=0.5938429236412048
I0301 01:08:09.044088 140409829308160 logging_writer.py:48] [204800] global_step=204800, grad_norm=4.161334037780762, loss=0.5674130320549011
I0301 01:08:42.731664 140408319375104 logging_writer.py:48] [204900] global_step=204900, grad_norm=4.365873336791992, loss=0.5734280943870544
I0301 01:09:16.456348 140409829308160 logging_writer.py:48] [205000] global_step=205000, grad_norm=4.425082206726074, loss=0.6679134368896484
I0301 01:09:50.127934 140408319375104 logging_writer.py:48] [205100] global_step=205100, grad_norm=5.188933849334717, loss=0.6643438339233398
I0301 01:10:23.868034 140409829308160 logging_writer.py:48] [205200] global_step=205200, grad_norm=4.70773458480835, loss=0.6351892948150635
I0301 01:10:57.536724 140408319375104 logging_writer.py:48] [205300] global_step=205300, grad_norm=4.616610050201416, loss=0.5799887776374817
I0301 01:11:31.198460 140409829308160 logging_writer.py:48] [205400] global_step=205400, grad_norm=4.404163837432861, loss=0.5453653335571289
I0301 01:12:04.945928 140408319375104 logging_writer.py:48] [205500] global_step=205500, grad_norm=4.402075290679932, loss=0.5418994426727295
I0301 01:12:38.633763 140409829308160 logging_writer.py:48] [205600] global_step=205600, grad_norm=4.355020523071289, loss=0.6121892333030701
I0301 01:13:12.337813 140408319375104 logging_writer.py:48] [205700] global_step=205700, grad_norm=5.079146862030029, loss=0.6813421249389648
I0301 01:13:46.054675 140409829308160 logging_writer.py:48] [205800] global_step=205800, grad_norm=4.4298930168151855, loss=0.6306224465370178
I0301 01:13:59.666778 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:14:05.880974 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:14:14.853374 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:14:17.141405 140573303715648 submission_runner.py:411] Time since start: 71981.13s, 	Step: 205842, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.1457848995923996, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0517550706863403, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8176188468933105, 'test/num_examples': 10000, 'score': 69417.21089410782, 'total_duration': 71981.13394188881, 'accumulated_submission_time': 69417.21089410782, 'accumulated_eval_time': 2548.2866756916046, 'accumulated_logging_time': 7.5248517990112305}
I0301 01:14:17.193169 140409846093568 logging_writer.py:48] [205842] accumulated_eval_time=2548.286676, accumulated_logging_time=7.524852, accumulated_submission_time=69417.210894, global_step=205842, preemption_count=0, score=69417.210894, test/accuracy=0.631000, test/loss=1.817619, test/num_examples=10000, total_duration=71981.133942, train/accuracy=0.960738, train/loss=0.145785, validation/accuracy=0.755980, validation/loss=1.051755, validation/num_examples=50000
I0301 01:14:37.016625 140409854486272 logging_writer.py:48] [205900] global_step=205900, grad_norm=4.5171732902526855, loss=0.5921151638031006
I0301 01:15:10.663953 140409846093568 logging_writer.py:48] [206000] global_step=206000, grad_norm=4.25951623916626, loss=0.5530775785446167
I0301 01:15:44.431816 140409854486272 logging_writer.py:48] [206100] global_step=206100, grad_norm=4.388105392456055, loss=0.6170775890350342
I0301 01:16:18.132915 140409846093568 logging_writer.py:48] [206200] global_step=206200, grad_norm=4.648654937744141, loss=0.6608310341835022
I0301 01:16:51.890468 140409854486272 logging_writer.py:48] [206300] global_step=206300, grad_norm=4.766915798187256, loss=0.619646430015564
I0301 01:17:25.601583 140409846093568 logging_writer.py:48] [206400] global_step=206400, grad_norm=4.762321472167969, loss=0.6603707671165466
I0301 01:17:59.279848 140409854486272 logging_writer.py:48] [206500] global_step=206500, grad_norm=4.2131266593933105, loss=0.5898005962371826
I0301 01:18:32.983088 140409846093568 logging_writer.py:48] [206600] global_step=206600, grad_norm=4.126815319061279, loss=0.6230211853981018
I0301 01:19:06.652494 140409854486272 logging_writer.py:48] [206700] global_step=206700, grad_norm=3.9800782203674316, loss=0.5573180317878723
I0301 01:19:40.387228 140409846093568 logging_writer.py:48] [206800] global_step=206800, grad_norm=4.564338207244873, loss=0.6056853532791138
I0301 01:20:14.077439 140409854486272 logging_writer.py:48] [206900] global_step=206900, grad_norm=4.905819416046143, loss=0.6631433367729187
I0301 01:20:47.744072 140409846093568 logging_writer.py:48] [207000] global_step=207000, grad_norm=4.424293518066406, loss=0.604659378528595
I0301 01:21:21.405921 140409854486272 logging_writer.py:48] [207100] global_step=207100, grad_norm=4.330721855163574, loss=0.5812298059463501
I0301 01:21:55.114689 140409846093568 logging_writer.py:48] [207200] global_step=207200, grad_norm=4.529636859893799, loss=0.6130817532539368
I0301 01:22:28.867898 140409854486272 logging_writer.py:48] [207300] global_step=207300, grad_norm=4.356988906860352, loss=0.6547321677207947
I0301 01:22:47.218254 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:22:53.279649 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:23:02.167922 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:23:04.612027 140573303715648 submission_runner.py:411] Time since start: 72508.60s, 	Step: 207356, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14816705882549286, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.0524675846099854, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8187187910079956, 'test/num_examples': 10000, 'score': 69927.16663312912, 'total_duration': 72508.60455965996, 'accumulated_submission_time': 69927.16663312912, 'accumulated_eval_time': 2565.68039393425, 'accumulated_logging_time': 7.586683750152588}
I0301 01:23:04.666580 140408319375104 logging_writer.py:48] [207356] accumulated_eval_time=2565.680394, accumulated_logging_time=7.586684, accumulated_submission_time=69927.166633, global_step=207356, preemption_count=0, score=69927.166633, test/accuracy=0.631600, test/loss=1.818719, test/num_examples=10000, total_duration=72508.604560, train/accuracy=0.959961, train/loss=0.148167, validation/accuracy=0.755540, validation/loss=1.052468, validation/num_examples=50000
I0301 01:23:19.827167 140409124681472 logging_writer.py:48] [207400] global_step=207400, grad_norm=4.64051628112793, loss=0.6315711140632629
I0301 01:23:53.473570 140408319375104 logging_writer.py:48] [207500] global_step=207500, grad_norm=4.715243339538574, loss=0.7028555870056152
I0301 01:24:27.137567 140409124681472 logging_writer.py:48] [207600] global_step=207600, grad_norm=4.532349586486816, loss=0.6361525654792786
I0301 01:25:00.865920 140408319375104 logging_writer.py:48] [207700] global_step=207700, grad_norm=4.479100704193115, loss=0.6106384992599487
I0301 01:25:34.558145 140409124681472 logging_writer.py:48] [207800] global_step=207800, grad_norm=4.894395351409912, loss=0.705623984336853
I0301 01:26:08.270545 140408319375104 logging_writer.py:48] [207900] global_step=207900, grad_norm=4.377776145935059, loss=0.6400786638259888
I0301 01:26:41.922250 140409124681472 logging_writer.py:48] [208000] global_step=208000, grad_norm=4.823434829711914, loss=0.7131837010383606
I0301 01:27:15.598239 140408319375104 logging_writer.py:48] [208100] global_step=208100, grad_norm=4.394243240356445, loss=0.6268132925033569
I0301 01:27:49.333391 140409124681472 logging_writer.py:48] [208200] global_step=208200, grad_norm=4.253256320953369, loss=0.6142528057098389
I0301 01:28:23.006990 140408319375104 logging_writer.py:48] [208300] global_step=208300, grad_norm=4.600981712341309, loss=0.5807005167007446
I0301 01:28:56.807014 140409124681472 logging_writer.py:48] [208400] global_step=208400, grad_norm=4.240387916564941, loss=0.5584449768066406
I0301 01:29:30.512767 140408319375104 logging_writer.py:48] [208500] global_step=208500, grad_norm=4.528720855712891, loss=0.6387322545051575
I0301 01:30:04.207649 140409124681472 logging_writer.py:48] [208600] global_step=208600, grad_norm=4.493133544921875, loss=0.6423951983451843
I0301 01:30:37.886933 140408319375104 logging_writer.py:48] [208700] global_step=208700, grad_norm=4.1295318603515625, loss=0.5991241335868835
I0301 01:31:11.635197 140409124681472 logging_writer.py:48] [208800] global_step=208800, grad_norm=4.572293758392334, loss=0.6474679112434387
I0301 01:31:34.668349 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:31:40.746305 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:31:49.640032 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:31:51.976064 140573303715648 submission_runner.py:411] Time since start: 73035.97s, 	Step: 208870, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14664709568023682, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0518457889556885, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.816641926765442, 'test/num_examples': 10000, 'score': 70437.09859132767, 'total_duration': 73035.96860289574, 'accumulated_submission_time': 70437.09859132767, 'accumulated_eval_time': 2582.988062143326, 'accumulated_logging_time': 7.651637077331543}
I0301 01:31:52.032453 140408319375104 logging_writer.py:48] [208870] accumulated_eval_time=2582.988062, accumulated_logging_time=7.651637, accumulated_submission_time=70437.098591, global_step=208870, preemption_count=0, score=70437.098591, test/accuracy=0.631200, test/loss=1.816642, test/num_examples=10000, total_duration=73035.968603, train/accuracy=0.960778, train/loss=0.146647, validation/accuracy=0.755400, validation/loss=1.051846, validation/num_examples=50000
I0301 01:32:02.520737 140409854486272 logging_writer.py:48] [208900] global_step=208900, grad_norm=4.209197044372559, loss=0.5544080138206482
I0301 01:32:36.245386 140408319375104 logging_writer.py:48] [209000] global_step=209000, grad_norm=4.574766635894775, loss=0.6665090322494507
I0301 01:33:09.968427 140409854486272 logging_writer.py:48] [209100] global_step=209100, grad_norm=4.388420581817627, loss=0.5991551876068115
I0301 01:33:43.672630 140408319375104 logging_writer.py:48] [209200] global_step=209200, grad_norm=4.656005859375, loss=0.7002962827682495
I0301 01:34:17.395987 140409854486272 logging_writer.py:48] [209300] global_step=209300, grad_norm=4.232172966003418, loss=0.5888763070106506
I0301 01:34:51.206719 140408319375104 logging_writer.py:48] [209400] global_step=209400, grad_norm=4.4095611572265625, loss=0.6232247352600098
I0301 01:35:24.884856 140409854486272 logging_writer.py:48] [209500] global_step=209500, grad_norm=4.705661773681641, loss=0.6192752718925476
I0301 01:35:58.588152 140408319375104 logging_writer.py:48] [209600] global_step=209600, grad_norm=4.298346996307373, loss=0.6138879060745239
I0301 01:36:32.289855 140409854486272 logging_writer.py:48] [209700] global_step=209700, grad_norm=4.617476463317871, loss=0.6213149428367615
I0301 01:37:06.012871 140408319375104 logging_writer.py:48] [209800] global_step=209800, grad_norm=4.542839527130127, loss=0.655781090259552
I0301 01:37:39.678389 140409854486272 logging_writer.py:48] [209900] global_step=209900, grad_norm=3.838836669921875, loss=0.544000506401062
I0301 01:38:13.367509 140408319375104 logging_writer.py:48] [210000] global_step=210000, grad_norm=5.239747524261475, loss=0.6735418438911438
I0301 01:38:47.086186 140409854486272 logging_writer.py:48] [210100] global_step=210100, grad_norm=3.9999382495880127, loss=0.5626698136329651
I0301 01:39:20.783965 140408319375104 logging_writer.py:48] [210200] global_step=210200, grad_norm=4.097153186798096, loss=0.5627865791320801
I0301 01:39:54.490968 140409854486272 logging_writer.py:48] [210300] global_step=210300, grad_norm=4.275064468383789, loss=0.6149476766586304
I0301 01:40:22.243326 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:40:28.377768 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:40:37.227911 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:40:39.551927 140573303715648 submission_runner.py:411] Time since start: 73563.54s, 	Step: 210384, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14701330661773682, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0524232387542725, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.817249059677124, 'test/num_examples': 10000, 'score': 70947.238874197, 'total_duration': 73563.5444612503, 'accumulated_submission_time': 70947.238874197, 'accumulated_eval_time': 2600.2966318130493, 'accumulated_logging_time': 7.717857122421265}
I0301 01:40:39.603897 140409829308160 logging_writer.py:48] [210384] accumulated_eval_time=2600.296632, accumulated_logging_time=7.717857, accumulated_submission_time=70947.238874, global_step=210384, preemption_count=0, score=70947.238874, test/accuracy=0.630400, test/loss=1.817249, test/num_examples=10000, total_duration=73563.544461, train/accuracy=0.960061, train/loss=0.147013, validation/accuracy=0.755860, validation/loss=1.052423, validation/num_examples=50000
I0301 01:40:45.344413 140409837700864 logging_writer.py:48] [210400] global_step=210400, grad_norm=4.64612340927124, loss=0.6287628412246704
I0301 01:41:19.199718 140409829308160 logging_writer.py:48] [210500] global_step=210500, grad_norm=4.568210601806641, loss=0.5770450830459595
I0301 01:41:52.858666 140409837700864 logging_writer.py:48] [210600] global_step=210600, grad_norm=4.72776985168457, loss=0.6405009031295776
I0301 01:42:26.530842 140409829308160 logging_writer.py:48] [210700] global_step=210700, grad_norm=4.488373279571533, loss=0.6680352091789246
I0301 01:43:00.196457 140409837700864 logging_writer.py:48] [210800] global_step=210800, grad_norm=4.195433616638184, loss=0.5933491587638855
I0301 01:43:33.934860 140409829308160 logging_writer.py:48] [210900] global_step=210900, grad_norm=4.332212924957275, loss=0.5926064252853394
I0301 01:44:07.624282 140409837700864 logging_writer.py:48] [211000] global_step=211000, grad_norm=4.652887344360352, loss=0.6464172601699829
I0301 01:44:41.402616 140409829308160 logging_writer.py:48] [211100] global_step=211100, grad_norm=4.136421203613281, loss=0.599254846572876
I0301 01:45:15.069900 140409837700864 logging_writer.py:48] [211200] global_step=211200, grad_norm=4.157799243927002, loss=0.5756014585494995
I0301 01:45:48.835278 140409829308160 logging_writer.py:48] [211300] global_step=211300, grad_norm=4.473275661468506, loss=0.6782193183898926
I0301 01:46:22.506973 140409837700864 logging_writer.py:48] [211400] global_step=211400, grad_norm=4.670717716217041, loss=0.6188963651657104
I0301 01:46:56.235871 140409829308160 logging_writer.py:48] [211500] global_step=211500, grad_norm=4.2963690757751465, loss=0.6163637638092041
I0301 01:47:30.047053 140409837700864 logging_writer.py:48] [211600] global_step=211600, grad_norm=4.476629734039307, loss=0.6033015251159668
I0301 01:48:03.740382 140409829308160 logging_writer.py:48] [211700] global_step=211700, grad_norm=4.352643013000488, loss=0.6566079258918762
I0301 01:48:37.467104 140409837700864 logging_writer.py:48] [211800] global_step=211800, grad_norm=4.484066963195801, loss=0.5617228150367737
I0301 01:49:09.683947 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:49:15.728700 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:49:24.457535 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:49:26.760972 140573303715648 submission_runner.py:411] Time since start: 74090.75s, 	Step: 211897, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.15002302825450897, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0529084205627441, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8178008794784546, 'test/num_examples': 10000, 'score': 71457.24890565872, 'total_duration': 74090.75350880623, 'accumulated_submission_time': 71457.24890565872, 'accumulated_eval_time': 2617.373607635498, 'accumulated_logging_time': 7.779970407485962}
I0301 01:49:26.818988 140408319375104 logging_writer.py:48] [211897] accumulated_eval_time=2617.373608, accumulated_logging_time=7.779970, accumulated_submission_time=71457.248906, global_step=211897, preemption_count=0, score=71457.248906, test/accuracy=0.631800, test/loss=1.817801, test/num_examples=10000, total_duration=74090.753509, train/accuracy=0.960320, train/loss=0.150023, validation/accuracy=0.755860, validation/loss=1.052908, validation/num_examples=50000
I0301 01:49:28.173613 140409124681472 logging_writer.py:48] [211900] global_step=211900, grad_norm=4.158144474029541, loss=0.5276438593864441
I0301 01:50:01.800353 140408319375104 logging_writer.py:48] [212000] global_step=212000, grad_norm=4.52558708190918, loss=0.6415601968765259
I0301 01:50:35.461644 140409124681472 logging_writer.py:48] [212100] global_step=212100, grad_norm=4.246509552001953, loss=0.5909393429756165
I0301 01:51:09.156910 140408319375104 logging_writer.py:48] [212200] global_step=212200, grad_norm=4.824549674987793, loss=0.6336615085601807
I0301 01:51:42.793021 140409124681472 logging_writer.py:48] [212300] global_step=212300, grad_norm=4.431711196899414, loss=0.640974223613739
I0301 01:52:16.533326 140408319375104 logging_writer.py:48] [212400] global_step=212400, grad_norm=4.535802364349365, loss=0.5782361626625061
I0301 01:52:50.251370 140409124681472 logging_writer.py:48] [212500] global_step=212500, grad_norm=4.716167449951172, loss=0.6479794383049011
I0301 01:53:24.059276 140408319375104 logging_writer.py:48] [212600] global_step=212600, grad_norm=4.5736236572265625, loss=0.6492990255355835
I0301 01:53:57.756125 140409124681472 logging_writer.py:48] [212700] global_step=212700, grad_norm=4.8026227951049805, loss=0.7459063529968262
I0301 01:54:31.393814 140408319375104 logging_writer.py:48] [212800] global_step=212800, grad_norm=4.240853309631348, loss=0.6046580076217651
I0301 01:55:05.067226 140409124681472 logging_writer.py:48] [212900] global_step=212900, grad_norm=4.510796070098877, loss=0.5544693470001221
I0301 01:55:38.796883 140408319375104 logging_writer.py:48] [213000] global_step=213000, grad_norm=4.4036946296691895, loss=0.5679910778999329
I0301 01:56:12.517180 140409124681472 logging_writer.py:48] [213100] global_step=213100, grad_norm=5.066084384918213, loss=0.6370226740837097
I0301 01:56:46.247056 140408319375104 logging_writer.py:48] [213200] global_step=213200, grad_norm=4.361241340637207, loss=0.6406693458557129
I0301 01:57:19.919007 140409124681472 logging_writer.py:48] [213300] global_step=213300, grad_norm=4.322378158569336, loss=0.5493364334106445
I0301 01:57:53.640642 140408319375104 logging_writer.py:48] [213400] global_step=213400, grad_norm=4.627473831176758, loss=0.6435018181800842
I0301 01:57:56.826261 140573303715648 spec.py:321] Evaluating on the training split.
I0301 01:58:03.077010 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 01:58:11.676089 140573303715648 spec.py:349] Evaluating on the test split.
I0301 01:58:13.982651 140573303715648 submission_runner.py:411] Time since start: 74617.98s, 	Step: 213411, 	{'train/accuracy': 0.9616748690605164, 'train/loss': 0.14489880204200745, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0528243780136108, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.818710446357727, 'test/num_examples': 10000, 'score': 71967.18639993668, 'total_duration': 74617.97519087791, 'accumulated_submission_time': 71967.18639993668, 'accumulated_eval_time': 2634.5299496650696, 'accumulated_logging_time': 7.84785270690918}
I0301 01:58:14.039800 140409846093568 logging_writer.py:48] [213411] accumulated_eval_time=2634.529950, accumulated_logging_time=7.847853, accumulated_submission_time=71967.186400, global_step=213411, preemption_count=0, score=71967.186400, test/accuracy=0.631700, test/loss=1.818710, test/num_examples=10000, total_duration=74617.975191, train/accuracy=0.961675, train/loss=0.144899, validation/accuracy=0.755740, validation/loss=1.052824, validation/num_examples=50000
I0301 01:58:44.295514 140409862878976 logging_writer.py:48] [213500] global_step=213500, grad_norm=4.529735565185547, loss=0.5986685752868652
I0301 01:59:18.060012 140409846093568 logging_writer.py:48] [213600] global_step=213600, grad_norm=4.691133975982666, loss=0.5775405764579773
I0301 01:59:51.849589 140409862878976 logging_writer.py:48] [213700] global_step=213700, grad_norm=4.6111273765563965, loss=0.6687617301940918
I0301 02:00:25.525002 140409846093568 logging_writer.py:48] [213800] global_step=213800, grad_norm=4.536543846130371, loss=0.6586810350418091
I0301 02:00:59.214141 140409862878976 logging_writer.py:48] [213900] global_step=213900, grad_norm=4.372378349304199, loss=0.6331666707992554
I0301 02:01:32.958889 140409846093568 logging_writer.py:48] [214000] global_step=214000, grad_norm=4.398961544036865, loss=0.5496698617935181
I0301 02:02:06.669742 140409862878976 logging_writer.py:48] [214100] global_step=214100, grad_norm=4.66539192199707, loss=0.5583569407463074
I0301 02:02:40.383179 140409846093568 logging_writer.py:48] [214200] global_step=214200, grad_norm=4.456552028656006, loss=0.7010034918785095
I0301 02:03:14.017282 140409862878976 logging_writer.py:48] [214300] global_step=214300, grad_norm=4.500699043273926, loss=0.5842523574829102
I0301 02:03:47.690885 140409846093568 logging_writer.py:48] [214400] global_step=214400, grad_norm=4.810140132904053, loss=0.6046831607818604
I0301 02:04:21.393391 140409862878976 logging_writer.py:48] [214500] global_step=214500, grad_norm=4.538748741149902, loss=0.6253695487976074
I0301 02:04:55.100705 140409846093568 logging_writer.py:48] [214600] global_step=214600, grad_norm=4.537598609924316, loss=0.582048237323761
I0301 02:05:28.840865 140409862878976 logging_writer.py:48] [214700] global_step=214700, grad_norm=4.440545558929443, loss=0.6483580470085144
I0301 02:06:02.592736 140409846093568 logging_writer.py:48] [214800] global_step=214800, grad_norm=4.37869119644165, loss=0.696266770362854
I0301 02:06:36.255420 140409862878976 logging_writer.py:48] [214900] global_step=214900, grad_norm=4.850969314575195, loss=0.6595149636268616
I0301 02:06:44.156315 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:06:50.280509 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:06:59.153860 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:07:01.455517 140573303715648 submission_runner.py:411] Time since start: 75145.45s, 	Step: 214925, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14911943674087524, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0532875061035156, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8189986944198608, 'test/num_examples': 10000, 'score': 72477.23326277733, 'total_duration': 75145.44805765152, 'accumulated_submission_time': 72477.23326277733, 'accumulated_eval_time': 2651.8291053771973, 'accumulated_logging_time': 7.914536952972412}
I0301 02:07:01.512184 140409829308160 logging_writer.py:48] [214925] accumulated_eval_time=2651.829105, accumulated_logging_time=7.914537, accumulated_submission_time=72477.233263, global_step=214925, preemption_count=0, score=72477.233263, test/accuracy=0.632100, test/loss=1.818999, test/num_examples=10000, total_duration=75145.448058, train/accuracy=0.959881, train/loss=0.149119, validation/accuracy=0.755580, validation/loss=1.053288, validation/num_examples=50000
I0301 02:07:27.093962 140409837700864 logging_writer.py:48] [215000] global_step=215000, grad_norm=4.096466064453125, loss=0.572186291217804
I0301 02:08:00.808621 140409829308160 logging_writer.py:48] [215100] global_step=215100, grad_norm=4.994826793670654, loss=0.6138191223144531
I0301 02:08:34.500966 140409837700864 logging_writer.py:48] [215200] global_step=215200, grad_norm=4.5777435302734375, loss=0.6784332990646362
I0301 02:09:08.248440 140409829308160 logging_writer.py:48] [215300] global_step=215300, grad_norm=4.431122303009033, loss=0.5950624942779541
I0301 02:09:41.954519 140409837700864 logging_writer.py:48] [215400] global_step=215400, grad_norm=4.3293280601501465, loss=0.6696845293045044
I0301 02:10:15.696841 140409829308160 logging_writer.py:48] [215500] global_step=215500, grad_norm=4.4404706954956055, loss=0.6510732769966125
I0301 02:10:49.387924 140409837700864 logging_writer.py:48] [215600] global_step=215600, grad_norm=4.429065227508545, loss=0.5665915608406067
I0301 02:11:23.142502 140409829308160 logging_writer.py:48] [215700] global_step=215700, grad_norm=4.518409729003906, loss=0.6890814304351807
I0301 02:11:56.961134 140409837700864 logging_writer.py:48] [215800] global_step=215800, grad_norm=4.468437194824219, loss=0.6005967855453491
I0301 02:12:30.624228 140409829308160 logging_writer.py:48] [215900] global_step=215900, grad_norm=5.066460132598877, loss=0.5805351734161377
I0301 02:13:04.341300 140409837700864 logging_writer.py:48] [216000] global_step=216000, grad_norm=4.337151527404785, loss=0.5834577679634094
I0301 02:13:38.023395 140409829308160 logging_writer.py:48] [216100] global_step=216100, grad_norm=4.292994976043701, loss=0.6464626789093018
I0301 02:14:11.738804 140409837700864 logging_writer.py:48] [216200] global_step=216200, grad_norm=4.453646183013916, loss=0.6375545859336853
I0301 02:14:45.428935 140409829308160 logging_writer.py:48] [216300] global_step=216300, grad_norm=4.198044300079346, loss=0.6861306428909302
I0301 02:15:19.149214 140409837700864 logging_writer.py:48] [216400] global_step=216400, grad_norm=4.694453716278076, loss=0.6306625008583069
I0301 02:15:31.739646 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:15:37.826930 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:15:46.440359 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:15:48.824844 140573303715648 submission_runner.py:411] Time since start: 75672.82s, 	Step: 216439, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14745484292507172, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0531450510025024, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8198258876800537, 'test/num_examples': 10000, 'score': 72987.39051795006, 'total_duration': 75672.81736660004, 'accumulated_submission_time': 72987.39051795006, 'accumulated_eval_time': 2668.91423535347, 'accumulated_logging_time': 7.982542037963867}
I0301 02:15:48.878913 140407379851008 logging_writer.py:48] [216439] accumulated_eval_time=2668.914235, accumulated_logging_time=7.982542, accumulated_submission_time=72987.390518, global_step=216439, preemption_count=0, score=72987.390518, test/accuracy=0.632200, test/loss=1.819826, test/num_examples=10000, total_duration=75672.817367, train/accuracy=0.961655, train/loss=0.147455, validation/accuracy=0.756020, validation/loss=1.053145, validation/num_examples=50000
I0301 02:16:09.754409 140408319375104 logging_writer.py:48] [216500] global_step=216500, grad_norm=4.366593360900879, loss=0.565546452999115
I0301 02:16:43.387531 140407379851008 logging_writer.py:48] [216600] global_step=216600, grad_norm=4.427557468414307, loss=0.6298848390579224
I0301 02:17:17.063214 140408319375104 logging_writer.py:48] [216700] global_step=216700, grad_norm=4.67183256149292, loss=0.6460084319114685
I0301 02:17:50.913822 140407379851008 logging_writer.py:48] [216800] global_step=216800, grad_norm=4.491737365722656, loss=0.6583088636398315
I0301 02:18:24.631844 140408319375104 logging_writer.py:48] [216900] global_step=216900, grad_norm=4.426863193511963, loss=0.5740624070167542
I0301 02:18:58.338854 140407379851008 logging_writer.py:48] [217000] global_step=217000, grad_norm=4.855568885803223, loss=0.6249179244041443
I0301 02:19:32.051628 140408319375104 logging_writer.py:48] [217100] global_step=217100, grad_norm=4.8768463134765625, loss=0.6649707555770874
I0301 02:20:05.725376 140407379851008 logging_writer.py:48] [217200] global_step=217200, grad_norm=4.578203201293945, loss=0.6381679177284241
I0301 02:20:39.472247 140408319375104 logging_writer.py:48] [217300] global_step=217300, grad_norm=4.637583255767822, loss=0.5707833766937256
I0301 02:21:13.160389 140407379851008 logging_writer.py:48] [217400] global_step=217400, grad_norm=4.505441665649414, loss=0.5649035573005676
I0301 02:21:46.869548 140408319375104 logging_writer.py:48] [217500] global_step=217500, grad_norm=4.128068447113037, loss=0.5884760022163391
I0301 02:22:20.556102 140407379851008 logging_writer.py:48] [217600] global_step=217600, grad_norm=4.621048450469971, loss=0.6850149035453796
I0301 02:22:54.280781 140408319375104 logging_writer.py:48] [217700] global_step=217700, grad_norm=4.4042863845825195, loss=0.5845644474029541
I0301 02:23:27.947695 140407379851008 logging_writer.py:48] [217800] global_step=217800, grad_norm=4.181438446044922, loss=0.6077337265014648
I0301 02:24:01.747479 140408319375104 logging_writer.py:48] [217900] global_step=217900, grad_norm=4.371236801147461, loss=0.6048716306686401
I0301 02:24:19.107579 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:24:25.196298 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:24:34.064297 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:24:36.383261 140573303715648 submission_runner.py:411] Time since start: 76200.38s, 	Step: 217953, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14770390093326569, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0527094602584839, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8178809881210327, 'test/num_examples': 10000, 'score': 73497.54736804962, 'total_duration': 76200.37576818466, 'accumulated_submission_time': 73497.54736804962, 'accumulated_eval_time': 2686.1898329257965, 'accumulated_logging_time': 8.048727750778198}
I0301 02:24:36.436842 140407379851008 logging_writer.py:48] [217953] accumulated_eval_time=2686.189833, accumulated_logging_time=8.048728, accumulated_submission_time=73497.547368, global_step=217953, preemption_count=0, score=73497.547368, test/accuracy=0.630400, test/loss=1.817881, test/num_examples=10000, total_duration=76200.375768, train/accuracy=0.960001, train/loss=0.147704, validation/accuracy=0.755780, validation/loss=1.052709, validation/num_examples=50000
I0301 02:24:52.621056 140409837700864 logging_writer.py:48] [218000] global_step=218000, grad_norm=4.182722091674805, loss=0.6296880841255188
I0301 02:25:26.296509 140407379851008 logging_writer.py:48] [218100] global_step=218100, grad_norm=4.378834247589111, loss=0.6550610065460205
I0301 02:26:00.059813 140409837700864 logging_writer.py:48] [218200] global_step=218200, grad_norm=4.9786248207092285, loss=0.5951762795448303
I0301 02:26:33.740076 140407379851008 logging_writer.py:48] [218300] global_step=218300, grad_norm=4.762042999267578, loss=0.6705634593963623
I0301 02:27:07.521668 140409837700864 logging_writer.py:48] [218400] global_step=218400, grad_norm=4.099930763244629, loss=0.5882078409194946
I0301 02:27:41.198407 140407379851008 logging_writer.py:48] [218500] global_step=218500, grad_norm=4.171928405761719, loss=0.5780140161514282
I0301 02:28:14.939348 140409837700864 logging_writer.py:48] [218600] global_step=218600, grad_norm=4.619797229766846, loss=0.7241165637969971
I0301 02:28:48.585781 140407379851008 logging_writer.py:48] [218700] global_step=218700, grad_norm=4.721585273742676, loss=0.6480624079704285
I0301 02:29:22.383620 140409837700864 logging_writer.py:48] [218800] global_step=218800, grad_norm=4.235709190368652, loss=0.6404236555099487
I0301 02:29:56.091823 140407379851008 logging_writer.py:48] [218900] global_step=218900, grad_norm=4.336836814880371, loss=0.6031594276428223
I0301 02:30:29.824518 140409837700864 logging_writer.py:48] [219000] global_step=219000, grad_norm=4.403349876403809, loss=0.6852060556411743
I0301 02:31:03.541934 140407379851008 logging_writer.py:48] [219100] global_step=219100, grad_norm=4.220399856567383, loss=0.6240055561065674
I0301 02:31:37.230134 140409837700864 logging_writer.py:48] [219200] global_step=219200, grad_norm=4.618383884429932, loss=0.5832099318504333
I0301 02:32:10.946867 140407379851008 logging_writer.py:48] [219300] global_step=219300, grad_norm=4.352349281311035, loss=0.6176754236221313
I0301 02:32:44.667866 140409837700864 logging_writer.py:48] [219400] global_step=219400, grad_norm=4.656472682952881, loss=0.611606776714325
I0301 02:33:06.697713 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:33:12.808447 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:33:21.521177 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:33:23.824087 140573303715648 submission_runner.py:411] Time since start: 76727.82s, 	Step: 219467, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14789921045303345, 'validation/accuracy': 0.7565199732780457, 'validation/loss': 1.0514085292816162, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8171485662460327, 'test/num_examples': 10000, 'score': 74007.73700547218, 'total_duration': 76727.81662344933, 'accumulated_submission_time': 74007.73700547218, 'accumulated_eval_time': 2703.3161799907684, 'accumulated_logging_time': 8.112717628479004}
I0301 02:33:23.878134 140409846093568 logging_writer.py:48] [219467] accumulated_eval_time=2703.316180, accumulated_logging_time=8.112718, accumulated_submission_time=74007.737005, global_step=219467, preemption_count=0, score=74007.737005, test/accuracy=0.631700, test/loss=1.817149, test/num_examples=10000, total_duration=76727.816623, train/accuracy=0.960120, train/loss=0.147899, validation/accuracy=0.756520, validation/loss=1.051409, validation/num_examples=50000
I0301 02:33:35.352037 140409862878976 logging_writer.py:48] [219500] global_step=219500, grad_norm=4.324874401092529, loss=0.5713478922843933
I0301 02:34:09.073272 140409846093568 logging_writer.py:48] [219600] global_step=219600, grad_norm=5.1221466064453125, loss=0.6375085115432739
I0301 02:34:42.694381 140409862878976 logging_writer.py:48] [219700] global_step=219700, grad_norm=5.004306316375732, loss=0.6997048854827881
I0301 02:35:16.368458 140409846093568 logging_writer.py:48] [219800] global_step=219800, grad_norm=4.4824066162109375, loss=0.7053563594818115
I0301 02:35:50.046192 140409862878976 logging_writer.py:48] [219900] global_step=219900, grad_norm=4.397780418395996, loss=0.5787157416343689
I0301 02:36:23.799808 140409846093568 logging_writer.py:48] [220000] global_step=220000, grad_norm=4.271552562713623, loss=0.6093314290046692
I0301 02:36:57.532284 140409862878976 logging_writer.py:48] [220100] global_step=220100, grad_norm=5.055051326751709, loss=0.6474732160568237
I0301 02:37:31.261443 140409846093568 logging_writer.py:48] [220200] global_step=220200, grad_norm=4.821837425231934, loss=0.6575660109519958
I0301 02:38:04.973151 140409862878976 logging_writer.py:48] [220300] global_step=220300, grad_norm=3.815559148788452, loss=0.5658129453659058
I0301 02:38:38.710132 140409846093568 logging_writer.py:48] [220400] global_step=220400, grad_norm=4.226726055145264, loss=0.6033233404159546
I0301 02:39:12.458388 140409862878976 logging_writer.py:48] [220500] global_step=220500, grad_norm=4.456855297088623, loss=0.6370126605033875
I0301 02:39:46.176965 140409846093568 logging_writer.py:48] [220600] global_step=220600, grad_norm=4.689470291137695, loss=0.6303346157073975
I0301 02:40:19.898279 140409862878976 logging_writer.py:48] [220700] global_step=220700, grad_norm=5.168103218078613, loss=0.6576870083808899
I0301 02:40:53.648098 140409846093568 logging_writer.py:48] [220800] global_step=220800, grad_norm=4.170992851257324, loss=0.644270122051239
I0301 02:41:27.368948 140409862878976 logging_writer.py:48] [220900] global_step=220900, grad_norm=4.558488845825195, loss=0.6147605776786804
I0301 02:41:54.147042 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:42:00.298472 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:42:09.177730 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:42:11.470787 140573303715648 submission_runner.py:411] Time since start: 77255.46s, 	Step: 220981, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.1483343243598938, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0523303747177124, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8170801401138306, 'test/num_examples': 10000, 'score': 74517.93527269363, 'total_duration': 77255.46332716942, 'accumulated_submission_time': 74517.93527269363, 'accumulated_eval_time': 2720.639880657196, 'accumulated_logging_time': 8.177234411239624}
I0301 02:42:11.525739 140407379851008 logging_writer.py:48] [220981] accumulated_eval_time=2720.639881, accumulated_logging_time=8.177234, accumulated_submission_time=74517.935273, global_step=220981, preemption_count=0, score=74517.935273, test/accuracy=0.632000, test/loss=1.817080, test/num_examples=10000, total_duration=77255.463327, train/accuracy=0.960340, train/loss=0.148334, validation/accuracy=0.755980, validation/loss=1.052330, validation/num_examples=50000
I0301 02:42:18.345843 140408319375104 logging_writer.py:48] [221000] global_step=221000, grad_norm=4.412785053253174, loss=0.544982373714447
I0301 02:42:51.984061 140407379851008 logging_writer.py:48] [221100] global_step=221100, grad_norm=4.635926246643066, loss=0.6175783276557922
I0301 02:43:25.662650 140408319375104 logging_writer.py:48] [221200] global_step=221200, grad_norm=4.774028301239014, loss=0.6324929594993591
I0301 02:43:59.398040 140407379851008 logging_writer.py:48] [221300] global_step=221300, grad_norm=4.4104766845703125, loss=0.5490294098854065
I0301 02:44:33.093415 140408319375104 logging_writer.py:48] [221400] global_step=221400, grad_norm=4.540133476257324, loss=0.570354700088501
I0301 02:45:06.821931 140407379851008 logging_writer.py:48] [221500] global_step=221500, grad_norm=4.797807216644287, loss=0.5948425531387329
I0301 02:45:40.526658 140408319375104 logging_writer.py:48] [221600] global_step=221600, grad_norm=4.4182233810424805, loss=0.641404926776886
I0301 02:46:14.252477 140407379851008 logging_writer.py:48] [221700] global_step=221700, grad_norm=4.530397415161133, loss=0.7083402872085571
I0301 02:46:47.932340 140408319375104 logging_writer.py:48] [221800] global_step=221800, grad_norm=4.780006408691406, loss=0.6170400381088257
I0301 02:47:21.694424 140407379851008 logging_writer.py:48] [221900] global_step=221900, grad_norm=4.285188674926758, loss=0.618841290473938
I0301 02:47:55.383983 140408319375104 logging_writer.py:48] [222000] global_step=222000, grad_norm=4.346700668334961, loss=0.6035347580909729
I0301 02:48:29.202175 140407379851008 logging_writer.py:48] [222100] global_step=222100, grad_norm=4.2589030265808105, loss=0.5590569376945496
I0301 02:49:02.919927 140408319375104 logging_writer.py:48] [222200] global_step=222200, grad_norm=4.374767780303955, loss=0.6487492322921753
I0301 02:49:36.574106 140407379851008 logging_writer.py:48] [222300] global_step=222300, grad_norm=4.486817836761475, loss=0.6837946176528931
I0301 02:50:10.291777 140408319375104 logging_writer.py:48] [222400] global_step=222400, grad_norm=4.865921497344971, loss=0.6463153958320618
I0301 02:50:41.735834 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:50:48.080239 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:50:56.862783 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:50:59.178466 140573303715648 submission_runner.py:411] Time since start: 77783.17s, 	Step: 222495, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14724181592464447, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0518176555633545, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8161752223968506, 'test/num_examples': 10000, 'score': 75028.07389211655, 'total_duration': 77783.17099523544, 'accumulated_submission_time': 75028.07389211655, 'accumulated_eval_time': 2738.0824706554413, 'accumulated_logging_time': 8.243817806243896}
I0301 02:50:59.230567 140407379851008 logging_writer.py:48] [222495] accumulated_eval_time=2738.082471, accumulated_logging_time=8.243818, accumulated_submission_time=75028.073892, global_step=222495, preemption_count=0, score=75028.073892, test/accuracy=0.631300, test/loss=1.816175, test/num_examples=10000, total_duration=77783.170995, train/accuracy=0.960758, train/loss=0.147242, validation/accuracy=0.755560, validation/loss=1.051818, validation/num_examples=50000
I0301 02:51:01.254203 140409837700864 logging_writer.py:48] [222500] global_step=222500, grad_norm=4.646688461303711, loss=0.7024695873260498
I0301 02:51:34.919956 140407379851008 logging_writer.py:48] [222600] global_step=222600, grad_norm=4.5917253494262695, loss=0.5643279552459717
I0301 02:52:08.624915 140409837700864 logging_writer.py:48] [222700] global_step=222700, grad_norm=4.726114273071289, loss=0.6288989782333374
I0301 02:52:42.299601 140407379851008 logging_writer.py:48] [222800] global_step=222800, grad_norm=4.0399861335754395, loss=0.5923404693603516
I0301 02:53:16.017895 140409837700864 logging_writer.py:48] [222900] global_step=222900, grad_norm=4.380472183227539, loss=0.6019429564476013
I0301 02:53:49.681569 140407379851008 logging_writer.py:48] [223000] global_step=223000, grad_norm=4.141355037689209, loss=0.6077609062194824
I0301 02:54:23.347104 140409837700864 logging_writer.py:48] [223100] global_step=223100, grad_norm=4.4463958740234375, loss=0.6323623061180115
I0301 02:54:57.124295 140407379851008 logging_writer.py:48] [223200] global_step=223200, grad_norm=4.244761943817139, loss=0.6442625522613525
I0301 02:55:30.853489 140409837700864 logging_writer.py:48] [223300] global_step=223300, grad_norm=4.705624103546143, loss=0.6455795764923096
I0301 02:56:04.500086 140407379851008 logging_writer.py:48] [223400] global_step=223400, grad_norm=4.47687292098999, loss=0.5804763436317444
I0301 02:56:38.185376 140409837700864 logging_writer.py:48] [223500] global_step=223500, grad_norm=4.362639427185059, loss=0.6300572752952576
I0301 02:57:11.924520 140407379851008 logging_writer.py:48] [223600] global_step=223600, grad_norm=4.559659957885742, loss=0.6324422955513
I0301 02:57:45.653869 140409837700864 logging_writer.py:48] [223700] global_step=223700, grad_norm=5.030714511871338, loss=0.630313515663147
I0301 02:58:19.331809 140407379851008 logging_writer.py:48] [223800] global_step=223800, grad_norm=4.237191200256348, loss=0.5123776793479919
I0301 02:58:53.050404 140409837700864 logging_writer.py:48] [223900] global_step=223900, grad_norm=4.319052696228027, loss=0.6031728386878967
I0301 02:59:26.734810 140407379851008 logging_writer.py:48] [224000] global_step=224000, grad_norm=4.431867599487305, loss=0.6502392292022705
I0301 02:59:29.246344 140573303715648 spec.py:321] Evaluating on the training split.
I0301 02:59:35.310368 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 02:59:44.130417 140573303715648 spec.py:349] Evaluating on the test split.
I0301 02:59:46.439537 140573303715648 submission_runner.py:411] Time since start: 78310.43s, 	Step: 224009, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.1484321802854538, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0512542724609375, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8154196739196777, 'test/num_examples': 10000, 'score': 75538.01856184006, 'total_duration': 78310.43208026886, 'accumulated_submission_time': 75538.01856184006, 'accumulated_eval_time': 2755.275614976883, 'accumulated_logging_time': 8.306404113769531}
I0301 02:59:46.499624 140408319375104 logging_writer.py:48] [224009] accumulated_eval_time=2755.275615, accumulated_logging_time=8.306404, accumulated_submission_time=75538.018562, global_step=224009, preemption_count=0, score=75538.018562, test/accuracy=0.630100, test/loss=1.815420, test/num_examples=10000, total_duration=78310.432080, train/accuracy=0.961396, train/loss=0.148432, validation/accuracy=0.756040, validation/loss=1.051254, validation/num_examples=50000
I0301 03:00:17.460551 140409829308160 logging_writer.py:48] [224100] global_step=224100, grad_norm=4.496914863586426, loss=0.5939847826957703
I0301 03:00:51.238202 140408319375104 logging_writer.py:48] [224200] global_step=224200, grad_norm=4.511371612548828, loss=0.6354025602340698
I0301 03:01:24.919349 140409829308160 logging_writer.py:48] [224300] global_step=224300, grad_norm=4.6701884269714355, loss=0.6298571228981018
I0301 03:01:58.695486 140408319375104 logging_writer.py:48] [224400] global_step=224400, grad_norm=5.372513771057129, loss=0.7098892331123352
I0301 03:02:32.419186 140409829308160 logging_writer.py:48] [224500] global_step=224500, grad_norm=4.621480464935303, loss=0.589157223701477
I0301 03:03:06.126987 140408319375104 logging_writer.py:48] [224600] global_step=224600, grad_norm=4.514378547668457, loss=0.7452706694602966
I0301 03:03:39.797225 140409829308160 logging_writer.py:48] [224700] global_step=224700, grad_norm=4.491212844848633, loss=0.6605176329612732
I0301 03:04:13.535723 140408319375104 logging_writer.py:48] [224800] global_step=224800, grad_norm=4.476955890655518, loss=0.6033428907394409
I0301 03:04:47.278309 140409829308160 logging_writer.py:48] [224900] global_step=224900, grad_norm=4.319932460784912, loss=0.5916711091995239
I0301 03:05:20.980598 140408319375104 logging_writer.py:48] [225000] global_step=225000, grad_norm=4.267608642578125, loss=0.6040918827056885
I0301 03:05:54.687949 140409829308160 logging_writer.py:48] [225100] global_step=225100, grad_norm=4.779843330383301, loss=0.5987977385520935
I0301 03:06:28.402104 140408319375104 logging_writer.py:48] [225200] global_step=225200, grad_norm=4.62844181060791, loss=0.6202582120895386
I0301 03:07:02.167559 140409829308160 logging_writer.py:48] [225300] global_step=225300, grad_norm=4.677204132080078, loss=0.6830494403839111
I0301 03:07:35.880208 140408319375104 logging_writer.py:48] [225400] global_step=225400, grad_norm=4.399037837982178, loss=0.7006648778915405
I0301 03:08:09.607425 140409829308160 logging_writer.py:48] [225500] global_step=225500, grad_norm=4.943692684173584, loss=0.6705644726753235
I0301 03:08:16.507500 140573303715648 spec.py:321] Evaluating on the training split.
I0301 03:08:22.625428 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 03:08:31.562796 140573303715648 spec.py:349] Evaluating on the test split.
I0301 03:08:33.837529 140573303715648 submission_runner.py:411] Time since start: 78837.83s, 	Step: 225522, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.14586791396141052, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0527108907699585, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8189946413040161, 'test/num_examples': 10000, 'score': 76047.95550060272, 'total_duration': 78837.83005452156, 'accumulated_submission_time': 76047.95550060272, 'accumulated_eval_time': 2772.6055793762207, 'accumulated_logging_time': 8.376575708389282}
I0301 03:08:33.893001 140407371458304 logging_writer.py:48] [225522] accumulated_eval_time=2772.605579, accumulated_logging_time=8.376576, accumulated_submission_time=76047.955501, global_step=225522, preemption_count=0, score=76047.955501, test/accuracy=0.631700, test/loss=1.818995, test/num_examples=10000, total_duration=78837.830055, train/accuracy=0.961336, train/loss=0.145868, validation/accuracy=0.755960, validation/loss=1.052711, validation/num_examples=50000
I0301 03:09:00.479523 140407379851008 logging_writer.py:48] [225600] global_step=225600, grad_norm=4.548552989959717, loss=0.6033049821853638
I0301 03:09:34.113549 140407371458304 logging_writer.py:48] [225700] global_step=225700, grad_norm=4.408092498779297, loss=0.5799145698547363
I0301 03:10:07.790488 140407379851008 logging_writer.py:48] [225800] global_step=225800, grad_norm=4.548743724822998, loss=0.6840518712997437
I0301 03:10:41.515606 140407371458304 logging_writer.py:48] [225900] global_step=225900, grad_norm=4.652569770812988, loss=0.6699264049530029
I0301 03:11:15.197412 140407379851008 logging_writer.py:48] [226000] global_step=226000, grad_norm=4.387965202331543, loss=0.6486883759498596
I0301 03:11:48.849125 140407371458304 logging_writer.py:48] [226100] global_step=226100, grad_norm=5.059095859527588, loss=0.6435872316360474
I0301 03:12:22.527271 140407379851008 logging_writer.py:48] [226200] global_step=226200, grad_norm=4.493879318237305, loss=0.727075457572937
I0301 03:12:56.276462 140407371458304 logging_writer.py:48] [226300] global_step=226300, grad_norm=4.680147171020508, loss=0.5732868313789368
I0301 03:13:29.983115 140407379851008 logging_writer.py:48] [226400] global_step=226400, grad_norm=4.421431541442871, loss=0.5982083082199097
I0301 03:14:03.635021 140407371458304 logging_writer.py:48] [226500] global_step=226500, grad_norm=4.342463493347168, loss=0.5753594040870667
I0301 03:14:37.274429 140407379851008 logging_writer.py:48] [226600] global_step=226600, grad_norm=4.745942115783691, loss=0.6558969020843506
I0301 03:15:11.005403 140407371458304 logging_writer.py:48] [226700] global_step=226700, grad_norm=4.883459568023682, loss=0.6814860701560974
I0301 03:15:44.662053 140407379851008 logging_writer.py:48] [226800] global_step=226800, grad_norm=4.490040302276611, loss=0.6634315252304077
I0301 03:16:18.324261 140407371458304 logging_writer.py:48] [226900] global_step=226900, grad_norm=4.577593803405762, loss=0.5731369256973267
I0301 03:16:52.028074 140407379851008 logging_writer.py:48] [227000] global_step=227000, grad_norm=4.689995765686035, loss=0.6422144174575806
I0301 03:17:03.949622 140573303715648 spec.py:321] Evaluating on the training split.
I0301 03:17:10.016328 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 03:17:18.567722 140573303715648 spec.py:349] Evaluating on the test split.
I0301 03:17:20.907033 140573303715648 submission_runner.py:411] Time since start: 79364.90s, 	Step: 227037, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14218345284461975, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0518858432769775, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8159196376800537, 'test/num_examples': 10000, 'score': 76557.94317746162, 'total_duration': 79364.89956021309, 'accumulated_submission_time': 76557.94317746162, 'accumulated_eval_time': 2789.5629415512085, 'accumulated_logging_time': 8.442211389541626}
I0301 03:17:20.968463 140409846093568 logging_writer.py:48] [227037] accumulated_eval_time=2789.562942, accumulated_logging_time=8.442211, accumulated_submission_time=76557.943177, global_step=227037, preemption_count=0, score=76557.943177, test/accuracy=0.631100, test/loss=1.815920, test/num_examples=10000, total_duration=79364.899560, train/accuracy=0.961934, train/loss=0.142183, validation/accuracy=0.755760, validation/loss=1.051886, validation/num_examples=50000
I0301 03:17:42.487451 140409854486272 logging_writer.py:48] [227100] global_step=227100, grad_norm=4.288466453552246, loss=0.5537494421005249
I0301 03:18:16.189921 140409846093568 logging_writer.py:48] [227200] global_step=227200, grad_norm=4.448681831359863, loss=0.5963979959487915
I0301 03:18:49.870622 140409854486272 logging_writer.py:48] [227300] global_step=227300, grad_norm=4.397252559661865, loss=0.659632682800293
I0301 03:19:23.631329 140409846093568 logging_writer.py:48] [227400] global_step=227400, grad_norm=4.688656330108643, loss=0.6589818596839905
I0301 03:19:57.663129 140409854486272 logging_writer.py:48] [227500] global_step=227500, grad_norm=4.892514705657959, loss=0.7290793061256409
I0301 03:20:31.399576 140409846093568 logging_writer.py:48] [227600] global_step=227600, grad_norm=4.675534725189209, loss=0.6742954850196838
I0301 03:21:05.096420 140409854486272 logging_writer.py:48] [227700] global_step=227700, grad_norm=4.186038494110107, loss=0.6173757910728455
I0301 03:21:38.828452 140409846093568 logging_writer.py:48] [227800] global_step=227800, grad_norm=4.464150428771973, loss=0.6375762820243835
I0301 03:22:12.524655 140409854486272 logging_writer.py:48] [227900] global_step=227900, grad_norm=4.344953536987305, loss=0.6116266250610352
I0301 03:22:46.260169 140409846093568 logging_writer.py:48] [228000] global_step=228000, grad_norm=4.952014446258545, loss=0.6062240600585938
I0301 03:23:19.957614 140409854486272 logging_writer.py:48] [228100] global_step=228100, grad_norm=4.306178092956543, loss=0.6169624328613281
I0301 03:23:53.696852 140409846093568 logging_writer.py:48] [228200] global_step=228200, grad_norm=4.3769731521606445, loss=0.7018740177154541
I0301 03:24:27.384742 140409854486272 logging_writer.py:48] [228300] global_step=228300, grad_norm=4.342329025268555, loss=0.6718147397041321
I0301 03:25:01.071050 140409846093568 logging_writer.py:48] [228400] global_step=228400, grad_norm=4.59786319732666, loss=0.6985710859298706
I0301 03:25:34.876687 140409854486272 logging_writer.py:48] [228500] global_step=228500, grad_norm=4.768243789672852, loss=0.6398019194602966
I0301 03:25:51.201421 140573303715648 spec.py:321] Evaluating on the training split.
I0301 03:25:57.532656 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 03:26:06.470429 140573303715648 spec.py:349] Evaluating on the test split.
I0301 03:26:08.740785 140573303715648 submission_runner.py:411] Time since start: 79892.73s, 	Step: 228550, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14717376232147217, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.052388072013855, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8176794052124023, 'test/num_examples': 10000, 'score': 77068.1039595604, 'total_duration': 79892.73308634758, 'accumulated_submission_time': 77068.1039595604, 'accumulated_eval_time': 2807.102013349533, 'accumulated_logging_time': 8.51409387588501}
I0301 03:26:08.800011 140408319375104 logging_writer.py:48] [228550] accumulated_eval_time=2807.102013, accumulated_logging_time=8.514094, accumulated_submission_time=77068.103960, global_step=228550, preemption_count=0, score=77068.103960, test/accuracy=0.630100, test/loss=1.817679, test/num_examples=10000, total_duration=79892.733086, train/accuracy=0.960858, train/loss=0.147174, validation/accuracy=0.755560, validation/loss=1.052388, validation/num_examples=50000
I0301 03:26:25.994360 140409124681472 logging_writer.py:48] [228600] global_step=228600, grad_norm=3.970852851867676, loss=0.525518000125885
I0301 03:26:59.658160 140408319375104 logging_writer.py:48] [228700] global_step=228700, grad_norm=4.385726451873779, loss=0.6602886915206909
I0301 03:27:33.350961 140409124681472 logging_writer.py:48] [228800] global_step=228800, grad_norm=4.454267501831055, loss=0.6269078254699707
I0301 03:28:07.020645 140408319375104 logging_writer.py:48] [228900] global_step=228900, grad_norm=4.699431896209717, loss=0.6622615456581116
I0301 03:28:40.775235 140409124681472 logging_writer.py:48] [229000] global_step=229000, grad_norm=4.062703609466553, loss=0.6096084117889404
I0301 03:29:14.483414 140408319375104 logging_writer.py:48] [229100] global_step=229100, grad_norm=3.837799549102783, loss=0.5888537168502808
I0301 03:29:48.166826 140409124681472 logging_writer.py:48] [229200] global_step=229200, grad_norm=4.671726226806641, loss=0.7223175764083862
I0301 03:30:21.883804 140408319375104 logging_writer.py:48] [229300] global_step=229300, grad_norm=4.580225467681885, loss=0.6739083528518677
I0301 03:30:55.656741 140409124681472 logging_writer.py:48] [229400] global_step=229400, grad_norm=4.6249470710754395, loss=0.6638345718383789
I0301 03:31:29.394373 140408319375104 logging_writer.py:48] [229500] global_step=229500, grad_norm=4.349277496337891, loss=0.6275054812431335
I0301 03:32:03.127069 140409124681472 logging_writer.py:48] [229600] global_step=229600, grad_norm=4.227046489715576, loss=0.6391687393188477
I0301 03:32:36.779120 140408319375104 logging_writer.py:48] [229700] global_step=229700, grad_norm=4.098665714263916, loss=0.5831204652786255
I0301 03:33:10.443971 140409124681472 logging_writer.py:48] [229800] global_step=229800, grad_norm=4.609235763549805, loss=0.6644203066825867
I0301 03:33:44.102690 140408319375104 logging_writer.py:48] [229900] global_step=229900, grad_norm=4.397244930267334, loss=0.639646053314209
I0301 03:34:17.776849 140409124681472 logging_writer.py:48] [230000] global_step=230000, grad_norm=5.053317546844482, loss=0.6420319080352783
I0301 03:34:38.832360 140573303715648 spec.py:321] Evaluating on the training split.
I0301 03:34:45.612681 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 03:34:54.396246 140573303715648 spec.py:349] Evaluating on the test split.
I0301 03:34:56.763920 140573303715648 submission_runner.py:411] Time since start: 80420.76s, 	Step: 230064, 	{'train/accuracy': 0.9589245915412903, 'train/loss': 0.14936187863349915, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.051660180091858, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8184075355529785, 'test/num_examples': 10000, 'score': 77578.0581843853, 'total_duration': 80420.75643348694, 'accumulated_submission_time': 77578.0581843853, 'accumulated_eval_time': 2825.0334997177124, 'accumulated_logging_time': 8.582934141159058}
I0301 03:34:56.818592 140409846093568 logging_writer.py:48] [230064] accumulated_eval_time=2825.033500, accumulated_logging_time=8.582934, accumulated_submission_time=77578.058184, global_step=230064, preemption_count=0, score=77578.058184, test/accuracy=0.631900, test/loss=1.818408, test/num_examples=10000, total_duration=80420.756433, train/accuracy=0.958925, train/loss=0.149362, validation/accuracy=0.755700, validation/loss=1.051660, validation/num_examples=50000
I0301 03:35:09.282305 140409854486272 logging_writer.py:48] [230100] global_step=230100, grad_norm=5.001926422119141, loss=0.6530223488807678
I0301 03:35:42.933847 140409846093568 logging_writer.py:48] [230200] global_step=230200, grad_norm=4.550692558288574, loss=0.6336163282394409
I0301 03:36:16.663679 140409854486272 logging_writer.py:48] [230300] global_step=230300, grad_norm=4.237388610839844, loss=0.6200164556503296
I0301 03:36:50.371742 140409846093568 logging_writer.py:48] [230400] global_step=230400, grad_norm=4.412971496582031, loss=0.704007089138031
I0301 03:37:24.179542 140409854486272 logging_writer.py:48] [230500] global_step=230500, grad_norm=4.490219593048096, loss=0.6216259598731995
I0301 03:37:57.860371 140409846093568 logging_writer.py:48] [230600] global_step=230600, grad_norm=4.4359660148620605, loss=0.7263909578323364
I0301 03:38:31.581312 140409854486272 logging_writer.py:48] [230700] global_step=230700, grad_norm=4.10782527923584, loss=0.5950151681900024
I0301 03:39:05.265103 140409846093568 logging_writer.py:48] [230800] global_step=230800, grad_norm=4.07774543762207, loss=0.5740602016448975
I0301 03:39:38.977371 140409854486272 logging_writer.py:48] [230900] global_step=230900, grad_norm=4.782671928405762, loss=0.5803318023681641
I0301 03:40:12.668503 140409846093568 logging_writer.py:48] [231000] global_step=231000, grad_norm=4.710946083068848, loss=0.6040043234825134
I0301 03:40:46.402945 140409854486272 logging_writer.py:48] [231100] global_step=231100, grad_norm=4.6737141609191895, loss=0.6703500151634216
I0301 03:41:20.105299 140409846093568 logging_writer.py:48] [231200] global_step=231200, grad_norm=4.7554168701171875, loss=0.6550000905990601
I0301 03:41:53.848084 140409854486272 logging_writer.py:48] [231300] global_step=231300, grad_norm=4.568131923675537, loss=0.6483275890350342
I0301 03:42:27.541443 140409846093568 logging_writer.py:48] [231400] global_step=231400, grad_norm=4.273611545562744, loss=0.6257988810539246
I0301 03:43:01.254026 140409854486272 logging_writer.py:48] [231500] global_step=231500, grad_norm=4.376997947692871, loss=0.6399790644645691
I0301 03:43:26.939255 140573303715648 spec.py:321] Evaluating on the training split.
I0301 03:43:33.034691 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 03:43:41.915258 140573303715648 spec.py:349] Evaluating on the test split.
I0301 03:43:44.305515 140573303715648 submission_runner.py:411] Time since start: 80948.30s, 	Step: 231577, 	{'train/accuracy': 0.9607182741165161, 'train/loss': 0.14643998444080353, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0522643327713013, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8174564838409424, 'test/num_examples': 10000, 'score': 78088.10909414291, 'total_duration': 80948.29805445671, 'accumulated_submission_time': 78088.10909414291, 'accumulated_eval_time': 2842.399694442749, 'accumulated_logging_time': 8.647887229919434}
I0301 03:43:44.361160 140409124681472 logging_writer.py:48] [231577] accumulated_eval_time=2842.399694, accumulated_logging_time=8.647887, accumulated_submission_time=78088.109094, global_step=231577, preemption_count=0, score=78088.109094, test/accuracy=0.631300, test/loss=1.817456, test/num_examples=10000, total_duration=80948.298054, train/accuracy=0.960718, train/loss=0.146440, validation/accuracy=0.755840, validation/loss=1.052264, validation/num_examples=50000
I0301 03:43:52.489699 140409829308160 logging_writer.py:48] [231600] global_step=231600, grad_norm=4.523736476898193, loss=0.626045823097229
I0301 03:44:26.206600 140409124681472 logging_writer.py:48] [231700] global_step=231700, grad_norm=4.2845635414123535, loss=0.5903933048248291
I0301 03:44:59.874182 140409829308160 logging_writer.py:48] [231800] global_step=231800, grad_norm=4.27409029006958, loss=0.5894364714622498
I0301 03:45:33.525601 140409124681472 logging_writer.py:48] [231900] global_step=231900, grad_norm=4.231903553009033, loss=0.5527754426002502
I0301 03:46:07.205863 140409829308160 logging_writer.py:48] [232000] global_step=232000, grad_norm=4.389956474304199, loss=0.693798303604126
I0301 03:46:40.959510 140409124681472 logging_writer.py:48] [232100] global_step=232100, grad_norm=4.392467021942139, loss=0.607352077960968
I0301 03:47:14.638609 140409829308160 logging_writer.py:48] [232200] global_step=232200, grad_norm=4.399694442749023, loss=0.5948368906974792
I0301 03:47:48.307795 140409124681472 logging_writer.py:48] [232300] global_step=232300, grad_norm=4.631749153137207, loss=0.621126651763916
I0301 03:48:22.014737 140409829308160 logging_writer.py:48] [232400] global_step=232400, grad_norm=4.557538986206055, loss=0.5743024945259094
I0301 03:48:55.715863 140409124681472 logging_writer.py:48] [232500] global_step=232500, grad_norm=4.874803066253662, loss=0.5963057279586792
I0301 03:49:29.375808 140409829308160 logging_writer.py:48] [232600] global_step=232600, grad_norm=4.404401779174805, loss=0.6611619591712952
I0301 03:50:03.176796 140409124681472 logging_writer.py:48] [232700] global_step=232700, grad_norm=4.120140552520752, loss=0.5897112488746643
I0301 03:50:36.838466 140409829308160 logging_writer.py:48] [232800] global_step=232800, grad_norm=4.804462432861328, loss=0.6535482406616211
I0301 03:51:10.541361 140409124681472 logging_writer.py:48] [232900] global_step=232900, grad_norm=4.677943229675293, loss=0.6185188889503479
I0301 03:51:44.263038 140409829308160 logging_writer.py:48] [233000] global_step=233000, grad_norm=4.420475006103516, loss=0.6512098908424377
I0301 03:52:14.381350 140573303715648 spec.py:321] Evaluating on the training split.
I0301 03:52:20.531330 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 03:52:29.420208 140573303715648 spec.py:349] Evaluating on the test split.
I0301 03:52:31.717163 140573303715648 submission_runner.py:411] Time since start: 81475.71s, 	Step: 233091, 	{'train/accuracy': 0.9599409699440002, 'train/loss': 0.1503974199295044, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0517245531082153, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.818180799484253, 'test/num_examples': 10000, 'score': 78598.0595126152, 'total_duration': 81475.70970320702, 'accumulated_submission_time': 78598.0595126152, 'accumulated_eval_time': 2859.735461950302, 'accumulated_logging_time': 8.714589595794678}
I0301 03:52:31.773191 140407379851008 logging_writer.py:48] [233091] accumulated_eval_time=2859.735462, accumulated_logging_time=8.714590, accumulated_submission_time=78598.059513, global_step=233091, preemption_count=0, score=78598.059513, test/accuracy=0.631300, test/loss=1.818181, test/num_examples=10000, total_duration=81475.709703, train/accuracy=0.959941, train/loss=0.150397, validation/accuracy=0.755980, validation/loss=1.051725, validation/num_examples=50000
I0301 03:52:35.131528 140408319375104 logging_writer.py:48] [233100] global_step=233100, grad_norm=4.388548374176025, loss=0.6650468111038208
I0301 03:53:08.743643 140407379851008 logging_writer.py:48] [233200] global_step=233200, grad_norm=4.453507900238037, loss=0.6168506145477295
I0301 03:53:42.409985 140408319375104 logging_writer.py:48] [233300] global_step=233300, grad_norm=4.367232799530029, loss=0.6402683258056641
I0301 03:54:16.156883 140407379851008 logging_writer.py:48] [233400] global_step=233400, grad_norm=4.863742351531982, loss=0.6414890885353088
I0301 03:54:49.857188 140408319375104 logging_writer.py:48] [233500] global_step=233500, grad_norm=4.413956165313721, loss=0.5960999131202698
I0301 03:55:23.565441 140407379851008 logging_writer.py:48] [233600] global_step=233600, grad_norm=4.209098815917969, loss=0.6169500350952148
I0301 03:55:57.356178 140408319375104 logging_writer.py:48] [233700] global_step=233700, grad_norm=4.349682331085205, loss=0.6807949542999268
I0301 03:56:31.082819 140407379851008 logging_writer.py:48] [233800] global_step=233800, grad_norm=4.1091532707214355, loss=0.5119518637657166
I0301 03:57:04.819513 140408319375104 logging_writer.py:48] [233900] global_step=233900, grad_norm=4.741777420043945, loss=0.7177099585533142
I0301 03:57:38.530120 140407379851008 logging_writer.py:48] [234000] global_step=234000, grad_norm=4.903453826904297, loss=0.7004145383834839
I0301 03:58:12.254357 140408319375104 logging_writer.py:48] [234100] global_step=234100, grad_norm=4.526152610778809, loss=0.5863001942634583
I0301 03:58:45.985890 140407379851008 logging_writer.py:48] [234200] global_step=234200, grad_norm=4.3120598793029785, loss=0.5930063724517822
I0301 03:59:19.723475 140408319375104 logging_writer.py:48] [234300] global_step=234300, grad_norm=4.368430137634277, loss=0.6574366092681885
I0301 03:59:53.429704 140407379851008 logging_writer.py:48] [234400] global_step=234400, grad_norm=4.835495948791504, loss=0.6189482808113098
I0301 04:00:27.149925 140408319375104 logging_writer.py:48] [234500] global_step=234500, grad_norm=4.6258063316345215, loss=0.6695011854171753
I0301 04:01:00.891230 140407379851008 logging_writer.py:48] [234600] global_step=234600, grad_norm=5.417562007904053, loss=0.6121031641960144
I0301 04:01:01.731568 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:01:07.865333 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:01:16.532939 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:01:18.842168 140573303715648 submission_runner.py:411] Time since start: 82002.83s, 	Step: 234604, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14579135179519653, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0515480041503906, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8177772760391235, 'test/num_examples': 10000, 'score': 79107.94815778732, 'total_duration': 82002.83469963074, 'accumulated_submission_time': 79107.94815778732, 'accumulated_eval_time': 2876.846007347107, 'accumulated_logging_time': 8.78090786933899}
I0301 04:01:18.895705 140409829308160 logging_writer.py:48] [234604] accumulated_eval_time=2876.846007, accumulated_logging_time=8.780908, accumulated_submission_time=79107.948158, global_step=234604, preemption_count=0, score=79107.948158, test/accuracy=0.630400, test/loss=1.817777, test/num_examples=10000, total_duration=82002.834700, train/accuracy=0.961496, train/loss=0.145791, validation/accuracy=0.756200, validation/loss=1.051548, validation/num_examples=50000
I0301 04:01:51.520380 140409837700864 logging_writer.py:48] [234700] global_step=234700, grad_norm=3.9393510818481445, loss=0.5642673969268799
I0301 04:02:25.278646 140409829308160 logging_writer.py:48] [234800] global_step=234800, grad_norm=4.209898948669434, loss=0.6443812847137451
I0301 04:02:58.979164 140409837700864 logging_writer.py:48] [234900] global_step=234900, grad_norm=4.186866283416748, loss=0.5476545691490173
I0301 04:03:32.766750 140409829308160 logging_writer.py:48] [235000] global_step=235000, grad_norm=4.511429309844971, loss=0.6026137471199036
I0301 04:04:06.434949 140409837700864 logging_writer.py:48] [235100] global_step=235100, grad_norm=4.428344249725342, loss=0.6548375487327576
I0301 04:04:40.098802 140409829308160 logging_writer.py:48] [235200] global_step=235200, grad_norm=4.341695785522461, loss=0.573182225227356
I0301 04:05:13.748307 140409837700864 logging_writer.py:48] [235300] global_step=235300, grad_norm=4.333562850952148, loss=0.5801131725311279
I0301 04:05:47.443593 140409829308160 logging_writer.py:48] [235400] global_step=235400, grad_norm=4.3630475997924805, loss=0.5856381058692932
I0301 04:06:21.149165 140409837700864 logging_writer.py:48] [235500] global_step=235500, grad_norm=4.854823112487793, loss=0.7066660523414612
I0301 04:06:54.873862 140409829308160 logging_writer.py:48] [235600] global_step=235600, grad_norm=4.355669975280762, loss=0.5584576725959778
I0301 04:07:28.570317 140409837700864 logging_writer.py:48] [235700] global_step=235700, grad_norm=4.480193138122559, loss=0.6503172516822815
I0301 04:08:02.415889 140409829308160 logging_writer.py:48] [235800] global_step=235800, grad_norm=4.135018825531006, loss=0.6008843779563904
I0301 04:08:36.132911 140409837700864 logging_writer.py:48] [235900] global_step=235900, grad_norm=4.889225006103516, loss=0.5768914222717285
I0301 04:09:09.786462 140409829308160 logging_writer.py:48] [236000] global_step=236000, grad_norm=4.409350872039795, loss=0.5942122340202332
I0301 04:09:43.467478 140409837700864 logging_writer.py:48] [236100] global_step=236100, grad_norm=4.8442511558532715, loss=0.6363435983657837
I0301 04:09:49.015159 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:09:55.058871 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:10:04.016870 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:10:06.332110 140573303715648 submission_runner.py:411] Time since start: 82530.32s, 	Step: 236118, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.14415684342384338, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.052870512008667, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.817879557609558, 'test/num_examples': 10000, 'score': 79617.99672365189, 'total_duration': 82530.32462906837, 'accumulated_submission_time': 79617.99672365189, 'accumulated_eval_time': 2894.1628913879395, 'accumulated_logging_time': 8.845736265182495}
I0301 04:10:06.387551 140408319375104 logging_writer.py:48] [236118] accumulated_eval_time=2894.162891, accumulated_logging_time=8.845736, accumulated_submission_time=79617.996724, global_step=236118, preemption_count=0, score=79617.996724, test/accuracy=0.631000, test/loss=1.817880, test/num_examples=10000, total_duration=82530.324629, train/accuracy=0.961436, train/loss=0.144157, validation/accuracy=0.755220, validation/loss=1.052871, validation/num_examples=50000
I0301 04:10:34.351640 140409124681472 logging_writer.py:48] [236200] global_step=236200, grad_norm=4.776307106018066, loss=0.5989124774932861
I0301 04:11:07.999824 140408319375104 logging_writer.py:48] [236300] global_step=236300, grad_norm=4.257646083831787, loss=0.5114254951477051
I0301 04:11:41.718870 140409124681472 logging_writer.py:48] [236400] global_step=236400, grad_norm=4.298416614532471, loss=0.5719482898712158
I0301 04:12:15.396369 140408319375104 logging_writer.py:48] [236500] global_step=236500, grad_norm=4.417309284210205, loss=0.6293351054191589
I0301 04:12:49.077239 140409124681472 logging_writer.py:48] [236600] global_step=236600, grad_norm=4.615616321563721, loss=0.6345474123954773
I0301 04:13:22.817816 140408319375104 logging_writer.py:48] [236700] global_step=236700, grad_norm=4.700852870941162, loss=0.7123138308525085
I0301 04:13:56.483978 140409124681472 logging_writer.py:48] [236800] global_step=236800, grad_norm=4.667946815490723, loss=0.6676934957504272
I0301 04:14:30.240205 140408319375104 logging_writer.py:48] [236900] global_step=236900, grad_norm=4.425496578216553, loss=0.6471409797668457
I0301 04:15:03.926048 140409124681472 logging_writer.py:48] [237000] global_step=237000, grad_norm=4.938593864440918, loss=0.6400623321533203
I0301 04:15:37.643405 140408319375104 logging_writer.py:48] [237100] global_step=237100, grad_norm=4.405824184417725, loss=0.6295366287231445
I0301 04:16:11.364211 140409124681472 logging_writer.py:48] [237200] global_step=237200, grad_norm=4.355246543884277, loss=0.6091249585151672
I0301 04:16:45.055046 140408319375104 logging_writer.py:48] [237300] global_step=237300, grad_norm=4.465887546539307, loss=0.6644127368927002
I0301 04:17:18.757838 140409124681472 logging_writer.py:48] [237400] global_step=237400, grad_norm=4.3618998527526855, loss=0.6143357753753662
I0301 04:17:52.452383 140408319375104 logging_writer.py:48] [237500] global_step=237500, grad_norm=4.54732608795166, loss=0.5796033143997192
I0301 04:18:26.125670 140409124681472 logging_writer.py:48] [237600] global_step=237600, grad_norm=3.999185800552368, loss=0.5307859182357788
I0301 04:18:36.412958 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:18:42.453067 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:18:51.221099 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:18:53.556106 140573303715648 submission_runner.py:411] Time since start: 83057.55s, 	Step: 237632, 	{'train/accuracy': 0.9603993892669678, 'train/loss': 0.1467776894569397, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0517122745513916, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.817185401916504, 'test/num_examples': 10000, 'score': 80127.95319676399, 'total_duration': 83057.54863166809, 'accumulated_submission_time': 80127.95319676399, 'accumulated_eval_time': 2911.3059771060944, 'accumulated_logging_time': 8.911030530929565}
I0301 04:18:53.610885 140409124681472 logging_writer.py:48] [237632] accumulated_eval_time=2911.305977, accumulated_logging_time=8.911031, accumulated_submission_time=80127.953197, global_step=237632, preemption_count=0, score=80127.953197, test/accuracy=0.631100, test/loss=1.817185, test/num_examples=10000, total_duration=83057.548632, train/accuracy=0.960399, train/loss=0.146778, validation/accuracy=0.755560, validation/loss=1.051712, validation/num_examples=50000
I0301 04:19:16.821464 140409829308160 logging_writer.py:48] [237700] global_step=237700, grad_norm=5.264571189880371, loss=0.7323430180549622
I0301 04:19:50.445723 140409124681472 logging_writer.py:48] [237800] global_step=237800, grad_norm=4.680475234985352, loss=0.6470811367034912
I0301 04:20:24.200930 140409829308160 logging_writer.py:48] [237900] global_step=237900, grad_norm=4.827353477478027, loss=0.5747008323669434
I0301 04:20:57.880599 140409124681472 logging_writer.py:48] [238000] global_step=238000, grad_norm=4.157547473907471, loss=0.5810811519622803
I0301 04:21:31.555056 140409829308160 logging_writer.py:48] [238100] global_step=238100, grad_norm=4.4464263916015625, loss=0.6385874152183533
I0301 04:22:05.206077 140409124681472 logging_writer.py:48] [238200] global_step=238200, grad_norm=4.413480758666992, loss=0.6178110241889954
I0301 04:22:38.982632 140409829308160 logging_writer.py:48] [238300] global_step=238300, grad_norm=4.560627460479736, loss=0.6196193099021912
I0301 04:23:12.699364 140409124681472 logging_writer.py:48] [238400] global_step=238400, grad_norm=4.485069274902344, loss=0.6049910187721252
I0301 04:23:46.396008 140409829308160 logging_writer.py:48] [238500] global_step=238500, grad_norm=4.414705276489258, loss=0.5608240962028503
I0301 04:24:20.057859 140409124681472 logging_writer.py:48] [238600] global_step=238600, grad_norm=4.297405242919922, loss=0.5597989559173584
I0301 04:24:53.786573 140409829308160 logging_writer.py:48] [238700] global_step=238700, grad_norm=4.189028739929199, loss=0.5648398399353027
I0301 04:25:27.453848 140409124681472 logging_writer.py:48] [238800] global_step=238800, grad_norm=4.582233905792236, loss=0.6159493923187256
I0301 04:26:01.129284 140409829308160 logging_writer.py:48] [238900] global_step=238900, grad_norm=4.759510040283203, loss=0.6996810436248779
I0301 04:26:34.900271 140409124681472 logging_writer.py:48] [239000] global_step=239000, grad_norm=5.0200676918029785, loss=0.682097852230072
I0301 04:27:08.591546 140409829308160 logging_writer.py:48] [239100] global_step=239100, grad_norm=4.477352142333984, loss=0.6918118000030518
I0301 04:27:23.597971 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:27:29.716929 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:27:38.598686 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:27:40.871167 140573303715648 submission_runner.py:411] Time since start: 83584.86s, 	Step: 239146, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14665235579013824, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.0525615215301514, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.817415714263916, 'test/num_examples': 10000, 'score': 80637.87059664726, 'total_duration': 83584.8637034893, 'accumulated_submission_time': 80637.87059664726, 'accumulated_eval_time': 2928.5791189670563, 'accumulated_logging_time': 8.975881099700928}
I0301 04:27:40.929754 140409854486272 logging_writer.py:48] [239146] accumulated_eval_time=2928.579119, accumulated_logging_time=8.975881, accumulated_submission_time=80637.870597, global_step=239146, preemption_count=0, score=80637.870597, test/accuracy=0.632100, test/loss=1.817416, test/num_examples=10000, total_duration=83584.863703, train/accuracy=0.960818, train/loss=0.146652, validation/accuracy=0.755380, validation/loss=1.052562, validation/num_examples=50000
I0301 04:27:59.462786 140409862878976 logging_writer.py:48] [239200] global_step=239200, grad_norm=4.201587200164795, loss=0.6016035079956055
I0301 04:28:33.163424 140409854486272 logging_writer.py:48] [239300] global_step=239300, grad_norm=4.152881622314453, loss=0.5613946914672852
I0301 04:29:06.838949 140409862878976 logging_writer.py:48] [239400] global_step=239400, grad_norm=4.642088890075684, loss=0.6016332507133484
I0301 04:29:40.505681 140409854486272 logging_writer.py:48] [239500] global_step=239500, grad_norm=4.453561782836914, loss=0.6784098744392395
I0301 04:30:14.239407 140409862878976 logging_writer.py:48] [239600] global_step=239600, grad_norm=4.675019264221191, loss=0.6348405480384827
I0301 04:30:47.989952 140409854486272 logging_writer.py:48] [239700] global_step=239700, grad_norm=4.869320392608643, loss=0.5890930891036987
I0301 04:31:21.686513 140409862878976 logging_writer.py:48] [239800] global_step=239800, grad_norm=4.565374374389648, loss=0.6316490769386292
I0301 04:31:55.387847 140409854486272 logging_writer.py:48] [239900] global_step=239900, grad_norm=4.729978084564209, loss=0.6440874934196472
I0301 04:32:29.164264 140409862878976 logging_writer.py:48] [240000] global_step=240000, grad_norm=4.862991809844971, loss=0.7309644818305969
I0301 04:33:02.879634 140409854486272 logging_writer.py:48] [240100] global_step=240100, grad_norm=4.847427845001221, loss=0.6705846786499023
I0301 04:33:36.626600 140409862878976 logging_writer.py:48] [240200] global_step=240200, grad_norm=4.060720443725586, loss=0.5534277558326721
I0301 04:34:10.321670 140409854486272 logging_writer.py:48] [240300] global_step=240300, grad_norm=4.169261455535889, loss=0.572716474533081
I0301 04:34:44.029265 140409862878976 logging_writer.py:48] [240400] global_step=240400, grad_norm=4.594461917877197, loss=0.6772482991218567
I0301 04:35:17.742558 140409854486272 logging_writer.py:48] [240500] global_step=240500, grad_norm=4.102580547332764, loss=0.5916250944137573
I0301 04:35:51.413839 140409862878976 logging_writer.py:48] [240600] global_step=240600, grad_norm=4.4214959144592285, loss=0.677854597568512
I0301 04:36:11.131783 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:36:17.310240 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:36:26.162925 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:36:28.449583 140573303715648 submission_runner.py:411] Time since start: 84112.44s, 	Step: 240660, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14722880721092224, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.052385687828064, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8181267976760864, 'test/num_examples': 10000, 'score': 81148.00268936157, 'total_duration': 84112.44211959839, 'accumulated_submission_time': 81148.00268936157, 'accumulated_eval_time': 2945.8968670368195, 'accumulated_logging_time': 9.044187307357788}
I0301 04:36:28.514449 140409829308160 logging_writer.py:48] [240660] accumulated_eval_time=2945.896867, accumulated_logging_time=9.044187, accumulated_submission_time=81148.002689, global_step=240660, preemption_count=0, score=81148.002689, test/accuracy=0.631000, test/loss=1.818127, test/num_examples=10000, total_duration=84112.442120, train/accuracy=0.960559, train/loss=0.147229, validation/accuracy=0.755820, validation/loss=1.052386, validation/num_examples=50000
I0301 04:36:42.314150 140410416518912 logging_writer.py:48] [240700] global_step=240700, grad_norm=4.473498344421387, loss=0.5982465744018555
I0301 04:37:16.009756 140409829308160 logging_writer.py:48] [240800] global_step=240800, grad_norm=4.245214939117432, loss=0.5914579033851624
I0301 04:37:49.683044 140410416518912 logging_writer.py:48] [240900] global_step=240900, grad_norm=4.278778553009033, loss=0.6143332123756409
I0301 04:38:23.361937 140409829308160 logging_writer.py:48] [241000] global_step=241000, grad_norm=4.633514404296875, loss=0.6277841925621033
I0301 04:38:57.101411 140410416518912 logging_writer.py:48] [241100] global_step=241100, grad_norm=4.455350875854492, loss=0.5992202758789062
I0301 04:39:30.773943 140409829308160 logging_writer.py:48] [241200] global_step=241200, grad_norm=4.224985599517822, loss=0.5936458706855774
I0301 04:40:04.445148 140410416518912 logging_writer.py:48] [241300] global_step=241300, grad_norm=4.923595428466797, loss=0.6105508208274841
I0301 04:40:38.192471 140409829308160 logging_writer.py:48] [241400] global_step=241400, grad_norm=3.8783719539642334, loss=0.6031653881072998
I0301 04:41:11.855762 140410416518912 logging_writer.py:48] [241500] global_step=241500, grad_norm=4.644275188446045, loss=0.5544462203979492
I0301 04:41:45.512566 140409829308160 logging_writer.py:48] [241600] global_step=241600, grad_norm=4.367866516113281, loss=0.6407171487808228
I0301 04:42:19.241100 140410416518912 logging_writer.py:48] [241700] global_step=241700, grad_norm=4.392086982727051, loss=0.6250641942024231
I0301 04:42:52.926515 140409829308160 logging_writer.py:48] [241800] global_step=241800, grad_norm=4.560273170471191, loss=0.5933559536933899
I0301 04:43:26.582133 140410416518912 logging_writer.py:48] [241900] global_step=241900, grad_norm=4.5076446533203125, loss=0.6437473297119141
I0301 04:44:00.292199 140409829308160 logging_writer.py:48] [242000] global_step=242000, grad_norm=4.426787853240967, loss=0.652694582939148
I0301 04:44:33.967978 140410416518912 logging_writer.py:48] [242100] global_step=242100, grad_norm=4.17034387588501, loss=0.5546149015426636
I0301 04:44:58.528088 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:45:04.748717 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:45:13.558534 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:45:15.851249 140573303715648 submission_runner.py:411] Time since start: 84639.84s, 	Step: 242174, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14488078653812408, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0521867275238037, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8185575008392334, 'test/num_examples': 10000, 'score': 81657.94163036346, 'total_duration': 84639.84378743172, 'accumulated_submission_time': 81657.94163036346, 'accumulated_eval_time': 2963.219983100891, 'accumulated_logging_time': 9.121662378311157}
I0301 04:45:15.907774 140407371458304 logging_writer.py:48] [242174] accumulated_eval_time=2963.219983, accumulated_logging_time=9.121662, accumulated_submission_time=81657.941630, global_step=242174, preemption_count=0, score=81657.941630, test/accuracy=0.631700, test/loss=1.818558, test/num_examples=10000, total_duration=84639.843787, train/accuracy=0.961934, train/loss=0.144881, validation/accuracy=0.755480, validation/loss=1.052187, validation/num_examples=50000
I0301 04:45:25.015398 140407379851008 logging_writer.py:48] [242200] global_step=242200, grad_norm=4.453875541687012, loss=0.6791040897369385
I0301 04:45:58.702441 140407371458304 logging_writer.py:48] [242300] global_step=242300, grad_norm=4.632266521453857, loss=0.6360796093940735
I0301 04:46:32.352867 140407379851008 logging_writer.py:48] [242400] global_step=242400, grad_norm=4.1442975997924805, loss=0.5770890116691589
I0301 04:47:06.105619 140407371458304 logging_writer.py:48] [242500] global_step=242500, grad_norm=4.447990417480469, loss=0.5710577964782715
I0301 04:47:39.790752 140407379851008 logging_writer.py:48] [242600] global_step=242600, grad_norm=4.228147983551025, loss=0.5960521697998047
I0301 04:48:13.514744 140407371458304 logging_writer.py:48] [242700] global_step=242700, grad_norm=4.979773044586182, loss=0.671492874622345
I0301 04:48:47.218707 140407379851008 logging_writer.py:48] [242800] global_step=242800, grad_norm=4.59206485748291, loss=0.6369849443435669
I0301 04:49:20.968498 140407371458304 logging_writer.py:48] [242900] global_step=242900, grad_norm=4.639806270599365, loss=0.6559067964553833
I0301 04:49:54.666543 140407379851008 logging_writer.py:48] [243000] global_step=243000, grad_norm=4.438470363616943, loss=0.6457099318504333
I0301 04:50:28.445316 140407371458304 logging_writer.py:48] [243100] global_step=243100, grad_norm=4.811403751373291, loss=0.6494594812393188
I0301 04:51:02.230499 140407379851008 logging_writer.py:48] [243200] global_step=243200, grad_norm=3.9193263053894043, loss=0.5815184712409973
I0301 04:51:35.923680 140407371458304 logging_writer.py:48] [243300] global_step=243300, grad_norm=4.501536846160889, loss=0.6617353558540344
I0301 04:52:09.644886 140407379851008 logging_writer.py:48] [243400] global_step=243400, grad_norm=4.091409206390381, loss=0.5942126512527466
I0301 04:52:43.333873 140407371458304 logging_writer.py:48] [243500] global_step=243500, grad_norm=4.4260735511779785, loss=0.6595735549926758
I0301 04:53:17.073068 140407379851008 logging_writer.py:48] [243600] global_step=243600, grad_norm=4.314165115356445, loss=0.6119352579116821
I0301 04:53:45.863250 140573303715648 spec.py:321] Evaluating on the training split.
I0301 04:53:52.033568 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 04:54:00.894520 140573303715648 spec.py:349] Evaluating on the test split.
I0301 04:54:03.200505 140573303715648 submission_runner.py:411] Time since start: 85167.19s, 	Step: 243687, 	{'train/accuracy': 0.9601601958274841, 'train/loss': 0.14693771302700043, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0522353649139404, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8188753128051758, 'test/num_examples': 10000, 'score': 82167.82444095612, 'total_duration': 85167.19301080704, 'accumulated_submission_time': 82167.82444095612, 'accumulated_eval_time': 2980.5571575164795, 'accumulated_logging_time': 9.191231966018677}
I0301 04:54:03.261989 140409846093568 logging_writer.py:48] [243687] accumulated_eval_time=2980.557158, accumulated_logging_time=9.191232, accumulated_submission_time=82167.824441, global_step=243687, preemption_count=0, score=82167.824441, test/accuracy=0.631700, test/loss=1.818875, test/num_examples=10000, total_duration=85167.193011, train/accuracy=0.960160, train/loss=0.146938, validation/accuracy=0.756100, validation/loss=1.052235, validation/num_examples=50000
I0301 04:54:07.969535 140409854486272 logging_writer.py:48] [243700] global_step=243700, grad_norm=4.170955181121826, loss=0.5656493902206421
I0301 04:54:41.546168 140409846093568 logging_writer.py:48] [243800] global_step=243800, grad_norm=4.190901279449463, loss=0.5836267471313477
I0301 04:55:15.188899 140409854486272 logging_writer.py:48] [243900] global_step=243900, grad_norm=4.9682393074035645, loss=0.6130838394165039
I0301 04:55:48.851847 140409846093568 logging_writer.py:48] [244000] global_step=244000, grad_norm=4.862034797668457, loss=0.610140860080719
I0301 04:56:22.571351 140409854486272 logging_writer.py:48] [244100] global_step=244100, grad_norm=4.319451332092285, loss=0.575681209564209
I0301 04:56:56.305040 140409846093568 logging_writer.py:48] [244200] global_step=244200, grad_norm=4.521727085113525, loss=0.6237834692001343
I0301 04:57:30.010681 140409854486272 logging_writer.py:48] [244300] global_step=244300, grad_norm=4.847927570343018, loss=0.6891198754310608
I0301 04:58:03.666906 140409846093568 logging_writer.py:48] [244400] global_step=244400, grad_norm=4.204747200012207, loss=0.5596798658370972
I0301 04:58:37.351741 140409854486272 logging_writer.py:48] [244500] global_step=244500, grad_norm=4.112130641937256, loss=0.5098368525505066
I0301 04:59:11.050651 140409846093568 logging_writer.py:48] [244600] global_step=244600, grad_norm=4.205223083496094, loss=0.6064234375953674
I0301 04:59:44.768067 140409854486272 logging_writer.py:48] [244700] global_step=244700, grad_norm=4.321908950805664, loss=0.6041138768196106
I0301 05:00:18.426254 140409846093568 logging_writer.py:48] [244800] global_step=244800, grad_norm=4.727433681488037, loss=0.6511779427528381
I0301 05:00:52.153014 140409854486272 logging_writer.py:48] [244900] global_step=244900, grad_norm=4.928490161895752, loss=0.6563528776168823
I0301 05:01:25.817003 140409846093568 logging_writer.py:48] [245000] global_step=245000, grad_norm=4.345095157623291, loss=0.6490746736526489
I0301 05:01:59.487477 140409854486272 logging_writer.py:48] [245100] global_step=245100, grad_norm=4.1091742515563965, loss=0.6094374060630798
I0301 05:02:33.206190 140409846093568 logging_writer.py:48] [245200] global_step=245200, grad_norm=4.616433143615723, loss=0.6090084910392761
I0301 05:02:33.214112 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:02:39.353641 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:02:48.094397 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:02:50.392746 140573303715648 submission_runner.py:411] Time since start: 85694.39s, 	Step: 245201, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.14838221669197083, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0516401529312134, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.817969560623169, 'test/num_examples': 10000, 'score': 82677.70756721497, 'total_duration': 85694.38528704643, 'accumulated_submission_time': 82677.70756721497, 'accumulated_eval_time': 2997.735734939575, 'accumulated_logging_time': 9.263187170028687}
I0301 05:02:50.450582 140408319375104 logging_writer.py:48] [245201] accumulated_eval_time=2997.735735, accumulated_logging_time=9.263187, accumulated_submission_time=82677.707567, global_step=245201, preemption_count=0, score=82677.707567, test/accuracy=0.631600, test/loss=1.817970, test/num_examples=10000, total_duration=85694.385287, train/accuracy=0.959861, train/loss=0.148382, validation/accuracy=0.756080, validation/loss=1.051640, validation/num_examples=50000
I0301 05:03:24.250072 140409124681472 logging_writer.py:48] [245300] global_step=245300, grad_norm=4.408076286315918, loss=0.6147987842559814
I0301 05:03:58.017894 140408319375104 logging_writer.py:48] [245400] global_step=245400, grad_norm=4.659361839294434, loss=0.6222862005233765
I0301 05:04:31.713830 140409124681472 logging_writer.py:48] [245500] global_step=245500, grad_norm=4.839382171630859, loss=0.7286788821220398
I0301 05:05:05.470923 140408319375104 logging_writer.py:48] [245600] global_step=245600, grad_norm=4.812089443206787, loss=0.5975871086120605
I0301 05:05:39.160872 140409124681472 logging_writer.py:48] [245700] global_step=245700, grad_norm=4.067573070526123, loss=0.5604915618896484
I0301 05:06:12.907218 140408319375104 logging_writer.py:48] [245800] global_step=245800, grad_norm=4.267817497253418, loss=0.6131614446640015
I0301 05:06:46.621156 140409124681472 logging_writer.py:48] [245900] global_step=245900, grad_norm=4.706146240234375, loss=0.6491026282310486
I0301 05:07:20.309718 140408319375104 logging_writer.py:48] [246000] global_step=246000, grad_norm=4.245161056518555, loss=0.585213303565979
I0301 05:07:53.969177 140409124681472 logging_writer.py:48] [246100] global_step=246100, grad_norm=4.194822311401367, loss=0.6089885830879211
I0301 05:08:27.730249 140408319375104 logging_writer.py:48] [246200] global_step=246200, grad_norm=4.623499870300293, loss=0.6155099272727966
I0301 05:09:01.445782 140409124681472 logging_writer.py:48] [246300] global_step=246300, grad_norm=4.583142280578613, loss=0.6083254814147949
I0301 05:09:35.256855 140408319375104 logging_writer.py:48] [246400] global_step=246400, grad_norm=4.477288246154785, loss=0.6672804951667786
I0301 05:10:08.963802 140409124681472 logging_writer.py:48] [246500] global_step=246500, grad_norm=4.221909523010254, loss=0.5919102430343628
I0301 05:10:42.626659 140408319375104 logging_writer.py:48] [246600] global_step=246600, grad_norm=4.528985500335693, loss=0.5622400045394897
I0301 05:11:16.318551 140409124681472 logging_writer.py:48] [246700] global_step=246700, grad_norm=4.113775253295898, loss=0.6205979585647583
I0301 05:11:20.492670 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:11:26.514353 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:11:35.381642 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:11:37.699248 140573303715648 submission_runner.py:411] Time since start: 86221.69s, 	Step: 246714, 	{'train/accuracy': 0.9618144035339355, 'train/loss': 0.14514252543449402, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0535144805908203, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8190034627914429, 'test/num_examples': 10000, 'score': 83187.67843437195, 'total_duration': 86221.69176626205, 'accumulated_submission_time': 83187.67843437195, 'accumulated_eval_time': 3014.9422421455383, 'accumulated_logging_time': 9.330884456634521}
I0301 05:11:37.752873 140409846093568 logging_writer.py:48] [246714] accumulated_eval_time=3014.942242, accumulated_logging_time=9.330884, accumulated_submission_time=83187.678434, global_step=246714, preemption_count=0, score=83187.678434, test/accuracy=0.630600, test/loss=1.819003, test/num_examples=10000, total_duration=86221.691766, train/accuracy=0.961814, train/loss=0.145143, validation/accuracy=0.755820, validation/loss=1.053514, validation/num_examples=50000
I0301 05:12:07.071503 140409854486272 logging_writer.py:48] [246800] global_step=246800, grad_norm=4.283518314361572, loss=0.5887631177902222
I0301 05:12:40.733851 140409846093568 logging_writer.py:48] [246900] global_step=246900, grad_norm=4.265655994415283, loss=0.6290954351425171
I0301 05:13:14.465878 140409854486272 logging_writer.py:48] [247000] global_step=247000, grad_norm=4.730611324310303, loss=0.6295410394668579
I0301 05:13:48.104396 140409846093568 logging_writer.py:48] [247100] global_step=247100, grad_norm=4.042935371398926, loss=0.5991118550300598
I0301 05:14:21.776988 140409854486272 logging_writer.py:48] [247200] global_step=247200, grad_norm=4.7106194496154785, loss=0.6734288334846497
I0301 05:14:55.449540 140409846093568 logging_writer.py:48] [247300] global_step=247300, grad_norm=4.533003807067871, loss=0.5990418195724487
I0301 05:15:29.254791 140409854486272 logging_writer.py:48] [247400] global_step=247400, grad_norm=4.448287487030029, loss=0.62786865234375
I0301 05:16:02.980129 140409846093568 logging_writer.py:48] [247500] global_step=247500, grad_norm=4.323844909667969, loss=0.6344110369682312
I0301 05:16:36.650919 140409854486272 logging_writer.py:48] [247600] global_step=247600, grad_norm=4.810048580169678, loss=0.653418242931366
I0301 05:17:10.300118 140409846093568 logging_writer.py:48] [247700] global_step=247700, grad_norm=4.619324684143066, loss=0.6602632999420166
I0301 05:17:44.025895 140409854486272 logging_writer.py:48] [247800] global_step=247800, grad_norm=5.181419372558594, loss=0.607500433921814
I0301 05:18:17.679150 140409846093568 logging_writer.py:48] [247900] global_step=247900, grad_norm=4.974359035491943, loss=0.6642748117446899
I0301 05:18:51.341720 140409854486272 logging_writer.py:48] [248000] global_step=248000, grad_norm=4.68139123916626, loss=0.640738844871521
I0301 05:19:25.011703 140409846093568 logging_writer.py:48] [248100] global_step=248100, grad_norm=4.790185928344727, loss=0.6320153474807739
I0301 05:19:58.730410 140409854486272 logging_writer.py:48] [248200] global_step=248200, grad_norm=4.036255836486816, loss=0.5676140785217285
I0301 05:20:07.965716 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:20:14.127760 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:20:22.967509 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:20:25.250924 140573303715648 submission_runner.py:411] Time since start: 86749.24s, 	Step: 248229, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.14667236804962158, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0514862537384033, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8160229921340942, 'test/num_examples': 10000, 'score': 83697.82090711594, 'total_duration': 86749.24345612526, 'accumulated_submission_time': 83697.82090711594, 'accumulated_eval_time': 3032.2274081707, 'accumulated_logging_time': 9.39405369758606}
I0301 05:20:25.310853 140407371458304 logging_writer.py:48] [248229] accumulated_eval_time=3032.227408, accumulated_logging_time=9.394054, accumulated_submission_time=83697.820907, global_step=248229, preemption_count=0, score=83697.820907, test/accuracy=0.631400, test/loss=1.816023, test/num_examples=10000, total_duration=86749.243456, train/accuracy=0.960479, train/loss=0.146672, validation/accuracy=0.755920, validation/loss=1.051486, validation/num_examples=50000
I0301 05:20:49.569266 140408319375104 logging_writer.py:48] [248300] global_step=248300, grad_norm=4.815835475921631, loss=0.610991358757019
I0301 05:21:23.247838 140407371458304 logging_writer.py:48] [248400] global_step=248400, grad_norm=4.668315410614014, loss=0.6740192174911499
I0301 05:21:57.022293 140408319375104 logging_writer.py:48] [248500] global_step=248500, grad_norm=4.351943492889404, loss=0.5383860468864441
I0301 05:22:30.724188 140407371458304 logging_writer.py:48] [248600] global_step=248600, grad_norm=4.576706409454346, loss=0.5951803922653198
I0301 05:23:04.487862 140408319375104 logging_writer.py:48] [248700] global_step=248700, grad_norm=4.8929762840271, loss=0.694238007068634
I0301 05:23:38.203283 140407371458304 logging_writer.py:48] [248800] global_step=248800, grad_norm=4.52437686920166, loss=0.7065573930740356
I0301 05:24:11.888573 140408319375104 logging_writer.py:48] [248900] global_step=248900, grad_norm=4.629756927490234, loss=0.7334907650947571
I0301 05:24:45.623478 140407371458304 logging_writer.py:48] [249000] global_step=249000, grad_norm=4.402221202850342, loss=0.5746292471885681
I0301 05:25:19.309202 140408319375104 logging_writer.py:48] [249100] global_step=249100, grad_norm=4.3041791915893555, loss=0.6397921442985535
I0301 05:25:53.035542 140407371458304 logging_writer.py:48] [249200] global_step=249200, grad_norm=4.622239112854004, loss=0.6187411546707153
I0301 05:26:26.719246 140408319375104 logging_writer.py:48] [249300] global_step=249300, grad_norm=4.208995342254639, loss=0.5658717751502991
I0301 05:27:00.434043 140407371458304 logging_writer.py:48] [249400] global_step=249400, grad_norm=4.58466100692749, loss=0.6242942214012146
I0301 05:27:34.170035 140408319375104 logging_writer.py:48] [249500] global_step=249500, grad_norm=4.555782794952393, loss=0.5970280766487122
I0301 05:28:07.821482 140407371458304 logging_writer.py:48] [249600] global_step=249600, grad_norm=4.390619277954102, loss=0.610868513584137
I0301 05:28:41.549799 140408319375104 logging_writer.py:48] [249700] global_step=249700, grad_norm=4.124312877655029, loss=0.5712653398513794
I0301 05:28:55.524907 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:29:01.553805 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:29:10.400136 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:29:12.750486 140573303715648 submission_runner.py:411] Time since start: 87276.74s, 	Step: 249743, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14769884943962097, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0521618127822876, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8176978826522827, 'test/num_examples': 10000, 'score': 84207.96512365341, 'total_duration': 87276.74302601814, 'accumulated_submission_time': 84207.96512365341, 'accumulated_eval_time': 3049.4529371261597, 'accumulated_logging_time': 9.46442985534668}
I0301 05:29:12.812629 140409846093568 logging_writer.py:48] [249743] accumulated_eval_time=3049.452937, accumulated_logging_time=9.464430, accumulated_submission_time=84207.965124, global_step=249743, preemption_count=0, score=84207.965124, test/accuracy=0.631300, test/loss=1.817698, test/num_examples=10000, total_duration=87276.743026, train/accuracy=0.960439, train/loss=0.147699, validation/accuracy=0.755920, validation/loss=1.052162, validation/num_examples=50000
I0301 05:29:32.336298 140409854486272 logging_writer.py:48] [249800] global_step=249800, grad_norm=4.439881801605225, loss=0.675552487373352
I0301 05:30:05.969321 140409846093568 logging_writer.py:48] [249900] global_step=249900, grad_norm=4.307506084442139, loss=0.6810243129730225
I0301 05:30:39.618040 140409854486272 logging_writer.py:48] [250000] global_step=250000, grad_norm=4.101353168487549, loss=0.6167652606964111
I0301 05:31:13.282845 140409846093568 logging_writer.py:48] [250100] global_step=250100, grad_norm=5.146404266357422, loss=0.5858789682388306
I0301 05:31:46.933793 140409854486272 logging_writer.py:48] [250200] global_step=250200, grad_norm=4.507344722747803, loss=0.5835400819778442
I0301 05:32:20.591014 140409846093568 logging_writer.py:48] [250300] global_step=250300, grad_norm=4.795079231262207, loss=0.6099619269371033
I0301 05:32:54.254735 140409854486272 logging_writer.py:48] [250400] global_step=250400, grad_norm=4.422823905944824, loss=0.622004508972168
I0301 05:33:27.895991 140409846093568 logging_writer.py:48] [250500] global_step=250500, grad_norm=4.307160377502441, loss=0.5452012419700623
I0301 05:34:01.661074 140409854486272 logging_writer.py:48] [250600] global_step=250600, grad_norm=4.533334732055664, loss=0.6550650000572205
I0301 05:34:35.311948 140409846093568 logging_writer.py:48] [250700] global_step=250700, grad_norm=4.281493663787842, loss=0.5979283452033997
I0301 05:35:08.983989 140409854486272 logging_writer.py:48] [250800] global_step=250800, grad_norm=4.892938613891602, loss=0.5908666849136353
I0301 05:35:42.660458 140409846093568 logging_writer.py:48] [250900] global_step=250900, grad_norm=4.482766151428223, loss=0.5794981718063354
I0301 05:36:16.323191 140409854486272 logging_writer.py:48] [251000] global_step=251000, grad_norm=4.4841694831848145, loss=0.5713081955909729
I0301 05:36:50.029156 140409846093568 logging_writer.py:48] [251100] global_step=251100, grad_norm=4.505244731903076, loss=0.6163561344146729
I0301 05:37:23.691457 140409854486272 logging_writer.py:48] [251200] global_step=251200, grad_norm=4.936485290527344, loss=0.6665505766868591
I0301 05:37:43.020868 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:37:49.071223 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:37:57.916194 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:38:00.218664 140573303715648 submission_runner.py:411] Time since start: 87804.21s, 	Step: 251259, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14819787442684174, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0509867668151855, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.816618800163269, 'test/num_examples': 10000, 'score': 84718.10183787346, 'total_duration': 87804.21118330956, 'accumulated_submission_time': 84718.10183787346, 'accumulated_eval_time': 3066.6506621837616, 'accumulated_logging_time': 9.539054870605469}
I0301 05:38:00.278926 140409124681472 logging_writer.py:48] [251259] accumulated_eval_time=3066.650662, accumulated_logging_time=9.539055, accumulated_submission_time=84718.101838, global_step=251259, preemption_count=0, score=84718.101838, test/accuracy=0.630700, test/loss=1.816619, test/num_examples=10000, total_duration=87804.211183, train/accuracy=0.960758, train/loss=0.148198, validation/accuracy=0.755680, validation/loss=1.050987, validation/num_examples=50000
I0301 05:38:14.430491 140409829308160 logging_writer.py:48] [251300] global_step=251300, grad_norm=4.38810396194458, loss=0.6450896859169006
I0301 05:38:48.098513 140409124681472 logging_writer.py:48] [251400] global_step=251400, grad_norm=4.326216697692871, loss=0.5910470485687256
I0301 05:39:21.800523 140409829308160 logging_writer.py:48] [251500] global_step=251500, grad_norm=4.427992820739746, loss=0.5874360203742981
I0301 05:39:55.591161 140409124681472 logging_writer.py:48] [251600] global_step=251600, grad_norm=4.7366766929626465, loss=0.5848872065544128
I0301 05:40:29.263357 140409829308160 logging_writer.py:48] [251700] global_step=251700, grad_norm=4.393643856048584, loss=0.5878727436065674
I0301 05:41:02.987748 140409124681472 logging_writer.py:48] [251800] global_step=251800, grad_norm=4.2910380363464355, loss=0.6121726632118225
I0301 05:41:36.670765 140409829308160 logging_writer.py:48] [251900] global_step=251900, grad_norm=4.439318656921387, loss=0.6725864410400391
I0301 05:42:10.401516 140409124681472 logging_writer.py:48] [252000] global_step=252000, grad_norm=4.299647808074951, loss=0.5945718884468079
I0301 05:42:44.078197 140409829308160 logging_writer.py:48] [252100] global_step=252100, grad_norm=4.376883029937744, loss=0.5967351198196411
I0301 05:43:17.818443 140409124681472 logging_writer.py:48] [252200] global_step=252200, grad_norm=4.458272457122803, loss=0.6428298354148865
I0301 05:43:51.497123 140409829308160 logging_writer.py:48] [252300] global_step=252300, grad_norm=4.698057651519775, loss=0.5709336996078491
I0301 05:44:25.217022 140409124681472 logging_writer.py:48] [252400] global_step=252400, grad_norm=4.499727249145508, loss=0.569890022277832
I0301 05:44:58.896581 140409829308160 logging_writer.py:48] [252500] global_step=252500, grad_norm=4.3488287925720215, loss=0.6212189197540283
I0301 05:45:32.624172 140409124681472 logging_writer.py:48] [252600] global_step=252600, grad_norm=3.9487481117248535, loss=0.5831102728843689
I0301 05:46:06.412841 140409829308160 logging_writer.py:48] [252700] global_step=252700, grad_norm=4.585907936096191, loss=0.6510269641876221
I0301 05:46:30.508853 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:46:36.632451 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:46:45.326507 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:46:47.668760 140573303715648 submission_runner.py:411] Time since start: 88331.66s, 	Step: 252773, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14798325300216675, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0520581007003784, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8171991109848022, 'test/num_examples': 10000, 'score': 85228.26144003868, 'total_duration': 88331.66128325462, 'accumulated_submission_time': 85228.26144003868, 'accumulated_eval_time': 3083.81050491333, 'accumulated_logging_time': 9.60982871055603}
I0301 05:46:47.725515 140408319375104 logging_writer.py:48] [252773] accumulated_eval_time=3083.810505, accumulated_logging_time=9.609829, accumulated_submission_time=85228.261440, global_step=252773, preemption_count=0, score=85228.261440, test/accuracy=0.631200, test/loss=1.817199, test/num_examples=10000, total_duration=88331.661283, train/accuracy=0.960738, train/loss=0.147983, validation/accuracy=0.755700, validation/loss=1.052058, validation/num_examples=50000
I0301 05:46:57.200216 140409124681472 logging_writer.py:48] [252800] global_step=252800, grad_norm=4.389614105224609, loss=0.6078428030014038
I0301 05:47:30.874589 140408319375104 logging_writer.py:48] [252900] global_step=252900, grad_norm=4.570903778076172, loss=0.663063645362854
I0301 05:48:04.539212 140409124681472 logging_writer.py:48] [253000] global_step=253000, grad_norm=4.478178977966309, loss=0.5866987705230713
I0301 05:48:38.289309 140408319375104 logging_writer.py:48] [253100] global_step=253100, grad_norm=4.228508472442627, loss=0.557183563709259
I0301 05:49:11.962094 140409124681472 logging_writer.py:48] [253200] global_step=253200, grad_norm=4.423107624053955, loss=0.662443995475769
I0301 05:49:45.615138 140408319375104 logging_writer.py:48] [253300] global_step=253300, grad_norm=4.331668376922607, loss=0.6117966175079346
I0301 05:50:19.311827 140409124681472 logging_writer.py:48] [253400] global_step=253400, grad_norm=4.957369804382324, loss=0.6647156476974487
I0301 05:50:53.010369 140408319375104 logging_writer.py:48] [253500] global_step=253500, grad_norm=4.4009599685668945, loss=0.5494305491447449
I0301 05:51:26.673732 140409124681472 logging_writer.py:48] [253600] global_step=253600, grad_norm=4.35699987411499, loss=0.5486105680465698
I0301 05:52:00.441115 140408319375104 logging_writer.py:48] [253700] global_step=253700, grad_norm=4.9036407470703125, loss=0.628322958946228
I0301 05:52:34.274129 140409124681472 logging_writer.py:48] [253800] global_step=253800, grad_norm=4.4311442375183105, loss=0.674660861492157
I0301 05:53:07.956195 140408319375104 logging_writer.py:48] [253900] global_step=253900, grad_norm=4.099050998687744, loss=0.5652952194213867
I0301 05:53:41.668519 140409124681472 logging_writer.py:48] [254000] global_step=254000, grad_norm=4.451824188232422, loss=0.6871901154518127
I0301 05:54:15.316397 140408319375104 logging_writer.py:48] [254100] global_step=254100, grad_norm=4.479306221008301, loss=0.6450581550598145
I0301 05:54:48.984727 140409124681472 logging_writer.py:48] [254200] global_step=254200, grad_norm=4.265379905700684, loss=0.6582340598106384
I0301 05:55:17.839926 140573303715648 spec.py:321] Evaluating on the training split.
I0301 05:55:23.919135 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 05:55:32.521134 140573303715648 spec.py:349] Evaluating on the test split.
I0301 05:55:34.822042 140573303715648 submission_runner.py:411] Time since start: 88858.81s, 	Step: 254287, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.15019813179969788, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0526988506317139, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8190534114837646, 'test/num_examples': 10000, 'score': 85738.3051803112, 'total_duration': 88858.81435346603, 'accumulated_submission_time': 85738.3051803112, 'accumulated_eval_time': 3100.79234957695, 'accumulated_logging_time': 9.677660465240479}
I0301 05:55:34.879973 140409829308160 logging_writer.py:48] [254287] accumulated_eval_time=3100.792350, accumulated_logging_time=9.677660, accumulated_submission_time=85738.305180, global_step=254287, preemption_count=0, score=85738.305180, test/accuracy=0.630700, test/loss=1.819053, test/num_examples=10000, total_duration=88858.814353, train/accuracy=0.959861, train/loss=0.150198, validation/accuracy=0.756040, validation/loss=1.052699, validation/num_examples=50000
I0301 05:55:39.599387 140409837700864 logging_writer.py:48] [254300] global_step=254300, grad_norm=4.765514850616455, loss=0.6228855848312378
I0301 05:56:13.304774 140409829308160 logging_writer.py:48] [254400] global_step=254400, grad_norm=4.166416168212891, loss=0.6255738735198975
I0301 05:56:46.992553 140409837700864 logging_writer.py:48] [254500] global_step=254500, grad_norm=5.032841682434082, loss=0.6326623558998108
I0301 05:57:20.678747 140409829308160 logging_writer.py:48] [254600] global_step=254600, grad_norm=4.7482709884643555, loss=0.6009024977684021
I0301 05:57:54.405240 140409837700864 logging_writer.py:48] [254700] global_step=254700, grad_norm=4.291492938995361, loss=0.6154969334602356
I0301 05:58:28.176686 140409829308160 logging_writer.py:48] [254800] global_step=254800, grad_norm=4.338462829589844, loss=0.6057154536247253
I0301 05:59:01.868251 140409837700864 logging_writer.py:48] [254900] global_step=254900, grad_norm=4.888948440551758, loss=0.6574011445045471
I0301 05:59:35.597873 140409829308160 logging_writer.py:48] [255000] global_step=255000, grad_norm=4.331772327423096, loss=0.5733076930046082
I0301 06:00:09.317816 140409837700864 logging_writer.py:48] [255100] global_step=255100, grad_norm=4.304017543792725, loss=0.6495145559310913
I0301 06:00:42.976529 140409829308160 logging_writer.py:48] [255200] global_step=255200, grad_norm=5.080079555511475, loss=0.7345800399780273
I0301 06:01:16.695622 140409837700864 logging_writer.py:48] [255300] global_step=255300, grad_norm=4.433375835418701, loss=0.5928918123245239
I0301 06:01:50.348589 140409829308160 logging_writer.py:48] [255400] global_step=255400, grad_norm=4.622326850891113, loss=0.6643761396408081
I0301 06:02:24.034287 140409837700864 logging_writer.py:48] [255500] global_step=255500, grad_norm=4.432269096374512, loss=0.560329258441925
I0301 06:02:57.750440 140409829308160 logging_writer.py:48] [255600] global_step=255600, grad_norm=4.235685348510742, loss=0.6441472172737122
I0301 06:03:31.413576 140409837700864 logging_writer.py:48] [255700] global_step=255700, grad_norm=4.315847396850586, loss=0.6437570452690125
I0301 06:04:05.151376 140409829308160 logging_writer.py:48] [255800] global_step=255800, grad_norm=4.827860355377197, loss=0.6174737215042114
I0301 06:04:05.158293 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:04:11.255873 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:04:19.913012 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:04:22.537894 140573303715648 submission_runner.py:411] Time since start: 89386.53s, 	Step: 255801, 	{'train/accuracy': 0.9608378410339355, 'train/loss': 0.1464589387178421, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0526121854782104, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8166823387145996, 'test/num_examples': 10000, 'score': 86248.5129327774, 'total_duration': 89386.53042554855, 'accumulated_submission_time': 86248.5129327774, 'accumulated_eval_time': 3118.1718657016754, 'accumulated_logging_time': 9.745689868927002}
I0301 06:04:22.602313 140409124681472 logging_writer.py:48] [255801] accumulated_eval_time=3118.171866, accumulated_logging_time=9.745690, accumulated_submission_time=86248.512933, global_step=255801, preemption_count=0, score=86248.512933, test/accuracy=0.630900, test/loss=1.816682, test/num_examples=10000, total_duration=89386.530426, train/accuracy=0.960838, train/loss=0.146459, validation/accuracy=0.755640, validation/loss=1.052612, validation/num_examples=50000
I0301 06:04:56.268481 140409846093568 logging_writer.py:48] [255900] global_step=255900, grad_norm=4.451626300811768, loss=0.6833066940307617
I0301 06:05:29.911912 140409124681472 logging_writer.py:48] [256000] global_step=256000, grad_norm=4.428534984588623, loss=0.6220476627349854
I0301 06:06:03.561788 140409846093568 logging_writer.py:48] [256100] global_step=256100, grad_norm=4.501429080963135, loss=0.603501558303833
I0301 06:06:37.329722 140409124681472 logging_writer.py:48] [256200] global_step=256200, grad_norm=4.247872352600098, loss=0.6490530967712402
I0301 06:07:10.980405 140409846093568 logging_writer.py:48] [256300] global_step=256300, grad_norm=4.276666164398193, loss=0.6314833164215088
I0301 06:07:44.670550 140409124681472 logging_writer.py:48] [256400] global_step=256400, grad_norm=4.820613861083984, loss=0.6455902457237244
I0301 06:08:18.378648 140409846093568 logging_writer.py:48] [256500] global_step=256500, grad_norm=3.9969992637634277, loss=0.5631545782089233
I0301 06:08:52.043634 140409124681472 logging_writer.py:48] [256600] global_step=256600, grad_norm=4.521630764007568, loss=0.6426321864128113
I0301 06:09:25.698821 140409846093568 logging_writer.py:48] [256700] global_step=256700, grad_norm=4.600632190704346, loss=0.6025617718696594
I0301 06:09:59.335944 140409124681472 logging_writer.py:48] [256800] global_step=256800, grad_norm=4.776947498321533, loss=0.610080897808075
I0301 06:10:33.113029 140409846093568 logging_writer.py:48] [256900] global_step=256900, grad_norm=4.779783248901367, loss=0.6603385210037231
I0301 06:11:06.818985 140409124681472 logging_writer.py:48] [257000] global_step=257000, grad_norm=4.357371807098389, loss=0.5904181599617004
I0301 06:11:40.525306 140409846093568 logging_writer.py:48] [257100] global_step=257100, grad_norm=4.534578323364258, loss=0.6647749543190002
I0301 06:12:14.184059 140409124681472 logging_writer.py:48] [257200] global_step=257200, grad_norm=4.423102855682373, loss=0.6672026515007019
I0301 06:12:47.887155 140409846093568 logging_writer.py:48] [257300] global_step=257300, grad_norm=4.465609073638916, loss=0.6343079209327698
I0301 06:12:52.741386 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:12:58.802962 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:13:07.613426 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:13:09.882254 140573303715648 submission_runner.py:411] Time since start: 89913.87s, 	Step: 257316, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.15020716190338135, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0523533821105957, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8178541660308838, 'test/num_examples': 10000, 'score': 86758.58062577248, 'total_duration': 89913.8745894432, 'accumulated_submission_time': 86758.58062577248, 'accumulated_eval_time': 3135.312481403351, 'accumulated_logging_time': 9.82122540473938}
I0301 06:13:09.980625 140408319375104 logging_writer.py:48] [257316] accumulated_eval_time=3135.312481, accumulated_logging_time=9.821225, accumulated_submission_time=86758.580626, global_step=257316, preemption_count=0, score=86758.580626, test/accuracy=0.632300, test/loss=1.817854, test/num_examples=10000, total_duration=89913.874589, train/accuracy=0.959762, train/loss=0.150207, validation/accuracy=0.756120, validation/loss=1.052353, validation/num_examples=50000
I0301 06:13:38.676826 140409829308160 logging_writer.py:48] [257400] global_step=257400, grad_norm=4.7894415855407715, loss=0.6183609962463379
I0301 06:14:12.348562 140408319375104 logging_writer.py:48] [257500] global_step=257500, grad_norm=4.625268936157227, loss=0.6396855115890503
I0301 06:14:46.090815 140409829308160 logging_writer.py:48] [257600] global_step=257600, grad_norm=4.233681678771973, loss=0.5944784283638
I0301 06:15:19.731833 140408319375104 logging_writer.py:48] [257700] global_step=257700, grad_norm=4.4779229164123535, loss=0.667795717716217
I0301 06:15:53.408228 140409829308160 logging_writer.py:48] [257800] global_step=257800, grad_norm=5.469631671905518, loss=0.6297363042831421
I0301 06:16:27.137463 140408319375104 logging_writer.py:48] [257900] global_step=257900, grad_norm=4.814140319824219, loss=0.6124957203865051
I0301 06:17:00.948448 140409829308160 logging_writer.py:48] [258000] global_step=258000, grad_norm=4.804600238800049, loss=0.6382520794868469
I0301 06:17:34.604731 140408319375104 logging_writer.py:48] [258100] global_step=258100, grad_norm=4.684857368469238, loss=0.7086924314498901
I0301 06:18:08.337963 140409829308160 logging_writer.py:48] [258200] global_step=258200, grad_norm=4.719837665557861, loss=0.6472868919372559
I0301 06:18:41.989652 140408319375104 logging_writer.py:48] [258300] global_step=258300, grad_norm=4.386380195617676, loss=0.6392267942428589
I0301 06:19:15.723477 140409829308160 logging_writer.py:48] [258400] global_step=258400, grad_norm=4.729742050170898, loss=0.6446248292922974
I0301 06:19:49.430404 140408319375104 logging_writer.py:48] [258500] global_step=258500, grad_norm=4.594121932983398, loss=0.626715898513794
I0301 06:20:23.144269 140409829308160 logging_writer.py:48] [258600] global_step=258600, grad_norm=4.456026554107666, loss=0.6352828145027161
I0301 06:20:56.893094 140408319375104 logging_writer.py:48] [258700] global_step=258700, grad_norm=4.200725078582764, loss=0.5981203317642212
I0301 06:21:30.617178 140409829308160 logging_writer.py:48] [258800] global_step=258800, grad_norm=4.360689640045166, loss=0.6429692506790161
I0301 06:21:40.188382 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:21:46.201170 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:21:55.023040 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:21:57.281279 140573303715648 submission_runner.py:411] Time since start: 90441.27s, 	Step: 258830, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14542406797409058, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.0518678426742554, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.817800521850586, 'test/num_examples': 10000, 'score': 87268.71463608742, 'total_duration': 90441.27367305756, 'accumulated_submission_time': 87268.71463608742, 'accumulated_eval_time': 3152.405182123184, 'accumulated_logging_time': 9.933086156845093}
I0301 06:21:57.346393 140407379851008 logging_writer.py:48] [258830] accumulated_eval_time=3152.405182, accumulated_logging_time=9.933086, accumulated_submission_time=87268.714636, global_step=258830, preemption_count=0, score=87268.714636, test/accuracy=0.631400, test/loss=1.817801, test/num_examples=10000, total_duration=90441.273673, train/accuracy=0.961037, train/loss=0.145424, validation/accuracy=0.756180, validation/loss=1.051868, validation/num_examples=50000
I0301 06:22:21.298995 140409124681472 logging_writer.py:48] [258900] global_step=258900, grad_norm=4.455626964569092, loss=0.6141208410263062
I0301 06:22:55.065734 140407379851008 logging_writer.py:48] [259000] global_step=259000, grad_norm=4.448674201965332, loss=0.6768398284912109
I0301 06:23:28.779268 140409124681472 logging_writer.py:48] [259100] global_step=259100, grad_norm=4.058785438537598, loss=0.5676928758621216
I0301 06:24:02.416787 140407379851008 logging_writer.py:48] [259200] global_step=259200, grad_norm=4.324997425079346, loss=0.5957306623458862
I0301 06:24:36.183705 140409124681472 logging_writer.py:48] [259300] global_step=259300, grad_norm=4.654960632324219, loss=0.6481829881668091
I0301 06:25:09.862893 140407379851008 logging_writer.py:48] [259400] global_step=259400, grad_norm=4.231481552124023, loss=0.5658606886863708
I0301 06:25:43.646483 140409124681472 logging_writer.py:48] [259500] global_step=259500, grad_norm=4.540353775024414, loss=0.567906379699707
I0301 06:26:17.365093 140407379851008 logging_writer.py:48] [259600] global_step=259600, grad_norm=4.506208419799805, loss=0.573767364025116
I0301 06:26:51.099632 140409124681472 logging_writer.py:48] [259700] global_step=259700, grad_norm=4.65798282623291, loss=0.6985163688659668
I0301 06:27:24.766623 140407379851008 logging_writer.py:48] [259800] global_step=259800, grad_norm=4.303276538848877, loss=0.6224812269210815
I0301 06:27:58.526071 140409124681472 logging_writer.py:48] [259900] global_step=259900, grad_norm=4.339612007141113, loss=0.634614884853363
I0301 06:28:32.204849 140407379851008 logging_writer.py:48] [260000] global_step=260000, grad_norm=4.535218238830566, loss=0.6760689616203308
I0301 06:29:05.963516 140409124681472 logging_writer.py:48] [260100] global_step=260100, grad_norm=4.677332878112793, loss=0.6823824048042297
I0301 06:29:39.661902 140407379851008 logging_writer.py:48] [260200] global_step=260200, grad_norm=4.596093654632568, loss=0.6179473400115967
I0301 06:30:13.346176 140409124681472 logging_writer.py:48] [260300] global_step=260300, grad_norm=4.630956172943115, loss=0.5761979222297668
I0301 06:30:27.347227 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:30:33.411397 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:30:42.289365 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:30:44.606196 140573303715648 submission_runner.py:411] Time since start: 90968.60s, 	Step: 260343, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14820601046085358, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0530979633331299, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8175643682479858, 'test/num_examples': 10000, 'score': 87778.6434044838, 'total_duration': 90968.59873461723, 'accumulated_submission_time': 87778.6434044838, 'accumulated_eval_time': 3169.6641025543213, 'accumulated_logging_time': 10.010907649993896}
I0301 06:30:44.670427 140409829308160 logging_writer.py:48] [260343] accumulated_eval_time=3169.664103, accumulated_logging_time=10.010908, accumulated_submission_time=87778.643404, global_step=260343, preemption_count=0, score=87778.643404, test/accuracy=0.631400, test/loss=1.817564, test/num_examples=10000, total_duration=90968.598735, train/accuracy=0.960120, train/loss=0.148206, validation/accuracy=0.755680, validation/loss=1.053098, validation/num_examples=50000
I0301 06:31:04.209518 140409837700864 logging_writer.py:48] [260400] global_step=260400, grad_norm=4.960458755493164, loss=0.6543123722076416
I0301 06:31:37.930485 140409829308160 logging_writer.py:48] [260500] global_step=260500, grad_norm=4.5243330001831055, loss=0.5770309567451477
I0301 06:32:11.576594 140409837700864 logging_writer.py:48] [260600] global_step=260600, grad_norm=4.539906978607178, loss=0.5904433131217957
I0301 06:32:45.304096 140409829308160 logging_writer.py:48] [260700] global_step=260700, grad_norm=4.765092849731445, loss=0.6335049867630005
I0301 06:33:18.974218 140409837700864 logging_writer.py:48] [260800] global_step=260800, grad_norm=4.55375862121582, loss=0.5920115113258362
I0301 06:33:52.672620 140409829308160 logging_writer.py:48] [260900] global_step=260900, grad_norm=4.0035529136657715, loss=0.5789247751235962
I0301 06:34:26.397639 140409837700864 logging_writer.py:48] [261000] global_step=261000, grad_norm=4.941783905029297, loss=0.7393884658813477
I0301 06:35:00.184692 140409829308160 logging_writer.py:48] [261100] global_step=261100, grad_norm=4.7291436195373535, loss=0.5844542980194092
I0301 06:35:33.878612 140409837700864 logging_writer.py:48] [261200] global_step=261200, grad_norm=4.614020824432373, loss=0.662865400314331
I0301 06:36:07.612787 140409829308160 logging_writer.py:48] [261300] global_step=261300, grad_norm=4.448276042938232, loss=0.6501662731170654
I0301 06:36:41.381216 140409837700864 logging_writer.py:48] [261400] global_step=261400, grad_norm=5.253316879272461, loss=0.6438939571380615
I0301 06:37:15.109001 140409829308160 logging_writer.py:48] [261500] global_step=261500, grad_norm=4.318119525909424, loss=0.5742499828338623
I0301 06:37:48.827584 140409837700864 logging_writer.py:48] [261600] global_step=261600, grad_norm=4.37793493270874, loss=0.5827391147613525
I0301 06:38:22.502583 140409829308160 logging_writer.py:48] [261700] global_step=261700, grad_norm=4.83726167678833, loss=0.6445892453193665
I0301 06:38:56.246932 140409837700864 logging_writer.py:48] [261800] global_step=261800, grad_norm=4.820938587188721, loss=0.6997823715209961
I0301 06:39:14.922619 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:39:20.981995 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:39:29.719957 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:39:32.157731 140573303715648 submission_runner.py:411] Time since start: 91496.15s, 	Step: 261857, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14960135519504547, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.051580786705017, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8180071115493774, 'test/num_examples': 10000, 'score': 88288.82423329353, 'total_duration': 91496.1502726078, 'accumulated_submission_time': 88288.82423329353, 'accumulated_eval_time': 3186.8991684913635, 'accumulated_logging_time': 10.086401224136353}
I0301 06:39:32.220331 140407379851008 logging_writer.py:48] [261857] accumulated_eval_time=3186.899168, accumulated_logging_time=10.086401, accumulated_submission_time=88288.824233, global_step=261857, preemption_count=0, score=88288.824233, test/accuracy=0.630600, test/loss=1.818007, test/num_examples=10000, total_duration=91496.150273, train/accuracy=0.960579, train/loss=0.149601, validation/accuracy=0.755500, validation/loss=1.051581, validation/num_examples=50000
I0301 06:39:47.027953 140408319375104 logging_writer.py:48] [261900] global_step=261900, grad_norm=4.483499050140381, loss=0.6691846251487732
I0301 06:40:20.673806 140407379851008 logging_writer.py:48] [262000] global_step=262000, grad_norm=4.302454948425293, loss=0.6604172587394714
I0301 06:40:54.391719 140408319375104 logging_writer.py:48] [262100] global_step=262100, grad_norm=4.700451374053955, loss=0.627837061882019
I0301 06:41:28.192489 140407379851008 logging_writer.py:48] [262200] global_step=262200, grad_norm=4.07773494720459, loss=0.5701838135719299
I0301 06:42:01.880977 140408319375104 logging_writer.py:48] [262300] global_step=262300, grad_norm=4.589241981506348, loss=0.5825909376144409
I0301 06:42:35.576943 140407379851008 logging_writer.py:48] [262400] global_step=262400, grad_norm=4.542462348937988, loss=0.6161606311798096
I0301 06:43:09.299979 140408319375104 logging_writer.py:48] [262500] global_step=262500, grad_norm=4.301100254058838, loss=0.6189295053482056
I0301 06:43:42.993098 140407379851008 logging_writer.py:48] [262600] global_step=262600, grad_norm=4.8749237060546875, loss=0.6093415021896362
I0301 06:44:16.712398 140408319375104 logging_writer.py:48] [262700] global_step=262700, grad_norm=5.051281452178955, loss=0.6531878709793091
I0301 06:44:50.416504 140407379851008 logging_writer.py:48] [262800] global_step=262800, grad_norm=4.300948619842529, loss=0.6198840141296387
I0301 06:45:24.103210 140408319375104 logging_writer.py:48] [262900] global_step=262900, grad_norm=4.30698299407959, loss=0.633022129535675
I0301 06:45:57.851018 140407379851008 logging_writer.py:48] [263000] global_step=263000, grad_norm=4.8924336433410645, loss=0.6049865484237671
I0301 06:46:31.536992 140408319375104 logging_writer.py:48] [263100] global_step=263100, grad_norm=4.235350608825684, loss=0.5397051572799683
I0301 06:47:05.287110 140407379851008 logging_writer.py:48] [263200] global_step=263200, grad_norm=4.548695087432861, loss=0.6188046932220459
I0301 06:47:39.113134 140408319375104 logging_writer.py:48] [263300] global_step=263300, grad_norm=4.2555389404296875, loss=0.5513611435890198
I0301 06:48:02.231806 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:48:08.359706 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:48:17.058665 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:48:19.425617 140573303715648 submission_runner.py:411] Time since start: 92023.42s, 	Step: 263370, 	{'train/accuracy': 0.9611965417861938, 'train/loss': 0.14855854213237762, 'validation/accuracy': 0.7565799951553345, 'validation/loss': 1.0513132810592651, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8187938928604126, 'test/num_examples': 10000, 'score': 88798.76346611977, 'total_duration': 92023.4181470871, 'accumulated_submission_time': 88798.76346611977, 'accumulated_eval_time': 3204.0929505825043, 'accumulated_logging_time': 10.161179780960083}
I0301 06:48:19.485790 140409854486272 logging_writer.py:48] [263370] accumulated_eval_time=3204.092951, accumulated_logging_time=10.161180, accumulated_submission_time=88798.763466, global_step=263370, preemption_count=0, score=88798.763466, test/accuracy=0.630900, test/loss=1.818794, test/num_examples=10000, total_duration=92023.418147, train/accuracy=0.961197, train/loss=0.148559, validation/accuracy=0.756580, validation/loss=1.051313, validation/num_examples=50000
I0301 06:48:29.964041 140409862878976 logging_writer.py:48] [263400] global_step=263400, grad_norm=4.156128406524658, loss=0.6902673840522766
I0301 06:49:03.624144 140409854486272 logging_writer.py:48] [263500] global_step=263500, grad_norm=4.243837356567383, loss=0.5867275595664978
I0301 06:49:37.337655 140409862878976 logging_writer.py:48] [263600] global_step=263600, grad_norm=4.824164867401123, loss=0.6214268207550049
I0301 06:50:11.023283 140409854486272 logging_writer.py:48] [263700] global_step=263700, grad_norm=4.257268905639648, loss=0.6096203923225403
I0301 06:50:44.768447 140409862878976 logging_writer.py:48] [263800] global_step=263800, grad_norm=4.584009170532227, loss=0.6529291868209839
I0301 06:51:18.456822 140409854486272 logging_writer.py:48] [263900] global_step=263900, grad_norm=4.382585048675537, loss=0.5560259222984314
I0301 06:51:52.193773 140409862878976 logging_writer.py:48] [264000] global_step=264000, grad_norm=4.315335273742676, loss=0.6390494704246521
I0301 06:52:25.981480 140409854486272 logging_writer.py:48] [264100] global_step=264100, grad_norm=4.542031764984131, loss=0.6076966524124146
I0301 06:52:59.720510 140409862878976 logging_writer.py:48] [264200] global_step=264200, grad_norm=4.427096843719482, loss=0.6287571787834167
I0301 06:53:33.542466 140409854486272 logging_writer.py:48] [264300] global_step=264300, grad_norm=4.672126770019531, loss=0.6127644777297974
I0301 06:54:07.271952 140409862878976 logging_writer.py:48] [264400] global_step=264400, grad_norm=4.841747283935547, loss=0.6138923764228821
I0301 06:54:41.023133 140409854486272 logging_writer.py:48] [264500] global_step=264500, grad_norm=4.477886199951172, loss=0.6110882759094238
I0301 06:55:14.756007 140409862878976 logging_writer.py:48] [264600] global_step=264600, grad_norm=4.478400230407715, loss=0.6699115037918091
I0301 06:55:48.451634 140409854486272 logging_writer.py:48] [264700] global_step=264700, grad_norm=4.415177822113037, loss=0.6063663959503174
I0301 06:56:22.162853 140409862878976 logging_writer.py:48] [264800] global_step=264800, grad_norm=4.233137607574463, loss=0.5912014842033386
I0301 06:56:49.608439 140573303715648 spec.py:321] Evaluating on the training split.
I0301 06:56:55.653953 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 06:57:04.614094 140573303715648 spec.py:349] Evaluating on the test split.
I0301 06:57:06.898565 140573303715648 submission_runner.py:411] Time since start: 92550.89s, 	Step: 264883, 	{'train/accuracy': 0.9622528553009033, 'train/loss': 0.1400790512561798, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0535967350006104, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.818920373916626, 'test/num_examples': 10000, 'score': 89308.81522655487, 'total_duration': 92550.89099025726, 'accumulated_submission_time': 89308.81522655487, 'accumulated_eval_time': 3221.382915019989, 'accumulated_logging_time': 10.232295989990234}
I0301 06:57:06.961146 140407379851008 logging_writer.py:48] [264883] accumulated_eval_time=3221.382915, accumulated_logging_time=10.232296, accumulated_submission_time=89308.815227, global_step=264883, preemption_count=0, score=89308.815227, test/accuracy=0.630900, test/loss=1.818920, test/num_examples=10000, total_duration=92550.890990, train/accuracy=0.962253, train/loss=0.140079, validation/accuracy=0.755620, validation/loss=1.053597, validation/num_examples=50000
I0301 06:57:13.021944 140408319375104 logging_writer.py:48] [264900] global_step=264900, grad_norm=4.233526229858398, loss=0.568274736404419
I0301 06:57:46.738481 140407379851008 logging_writer.py:48] [265000] global_step=265000, grad_norm=5.0005364418029785, loss=0.5959069728851318
I0301 06:58:20.435951 140408319375104 logging_writer.py:48] [265100] global_step=265100, grad_norm=4.546968460083008, loss=0.7075003385543823
I0301 06:58:54.131763 140407379851008 logging_writer.py:48] [265200] global_step=265200, grad_norm=4.6606316566467285, loss=0.6266583204269409
I0301 06:59:27.883404 140408319375104 logging_writer.py:48] [265300] global_step=265300, grad_norm=4.277297496795654, loss=0.6212607026100159
I0301 07:00:01.657798 140407379851008 logging_writer.py:48] [265400] global_step=265400, grad_norm=4.231668949127197, loss=0.5335702896118164
I0301 07:00:35.344589 140408319375104 logging_writer.py:48] [265500] global_step=265500, grad_norm=4.42425537109375, loss=0.6420219540596008
I0301 07:01:09.077672 140407379851008 logging_writer.py:48] [265600] global_step=265600, grad_norm=4.399816989898682, loss=0.5685644149780273
I0301 07:01:42.828968 140408319375104 logging_writer.py:48] [265700] global_step=265700, grad_norm=3.999795913696289, loss=0.5989716649055481
I0301 07:02:16.558149 140407379851008 logging_writer.py:48] [265800] global_step=265800, grad_norm=4.377542018890381, loss=0.6076570749282837
I0301 07:02:50.278902 140408319375104 logging_writer.py:48] [265900] global_step=265900, grad_norm=4.412303447723389, loss=0.5839565396308899
I0301 07:03:24.007224 140407379851008 logging_writer.py:48] [266000] global_step=266000, grad_norm=4.593283176422119, loss=0.6301323771476746
I0301 07:03:57.728070 140408319375104 logging_writer.py:48] [266100] global_step=266100, grad_norm=4.359956741333008, loss=0.6325268149375916
I0301 07:04:31.441704 140407379851008 logging_writer.py:48] [266200] global_step=266200, grad_norm=4.487639427185059, loss=0.5937555432319641
I0301 07:05:05.168281 140408319375104 logging_writer.py:48] [266300] global_step=266300, grad_norm=5.011476039886475, loss=0.7015950083732605
I0301 07:05:36.969160 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:05:43.438866 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:05:52.142393 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:05:54.536088 140573303715648 submission_runner.py:411] Time since start: 93078.53s, 	Step: 266396, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14752313494682312, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.050364375114441, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8159278631210327, 'test/num_examples': 10000, 'score': 89818.75279092789, 'total_duration': 93078.52862429619, 'accumulated_submission_time': 89818.75279092789, 'accumulated_eval_time': 3238.949809551239, 'accumulated_logging_time': 10.304664373397827}
I0301 07:05:54.599386 140409854486272 logging_writer.py:48] [266396] accumulated_eval_time=3238.949810, accumulated_logging_time=10.304664, accumulated_submission_time=89818.752791, global_step=266396, preemption_count=0, score=89818.752791, test/accuracy=0.630000, test/loss=1.815928, test/num_examples=10000, total_duration=93078.528624, train/accuracy=0.960758, train/loss=0.147523, validation/accuracy=0.755860, validation/loss=1.050364, validation/num_examples=50000
I0301 07:05:56.277929 140409862878976 logging_writer.py:48] [266400] global_step=266400, grad_norm=4.631500244140625, loss=0.6323192119598389
I0301 07:06:29.943824 140409854486272 logging_writer.py:48] [266500] global_step=266500, grad_norm=4.864608287811279, loss=0.6248874664306641
I0301 07:07:03.674962 140409862878976 logging_writer.py:48] [266600] global_step=266600, grad_norm=4.182182788848877, loss=0.5719651579856873
I0301 07:07:37.347874 140409854486272 logging_writer.py:48] [266700] global_step=266700, grad_norm=4.443445205688477, loss=0.611161470413208
I0301 07:08:11.044090 140409862878976 logging_writer.py:48] [266800] global_step=266800, grad_norm=4.162210941314697, loss=0.551260232925415
I0301 07:08:44.765772 140409854486272 logging_writer.py:48] [266900] global_step=266900, grad_norm=4.259777545928955, loss=0.5552743673324585
I0301 07:09:18.473710 140409862878976 logging_writer.py:48] [267000] global_step=267000, grad_norm=4.872194290161133, loss=0.6314706802368164
I0301 07:09:52.195648 140409854486272 logging_writer.py:48] [267100] global_step=267100, grad_norm=4.685102939605713, loss=0.6669847369194031
I0301 07:10:25.949297 140409862878976 logging_writer.py:48] [267200] global_step=267200, grad_norm=4.376583576202393, loss=0.5999102592468262
I0301 07:10:59.661763 140409854486272 logging_writer.py:48] [267300] global_step=267300, grad_norm=4.134295463562012, loss=0.5762876272201538
I0301 07:11:33.384023 140409862878976 logging_writer.py:48] [267400] global_step=267400, grad_norm=4.817193031311035, loss=0.7179003953933716
I0301 07:12:07.147008 140409854486272 logging_writer.py:48] [267500] global_step=267500, grad_norm=4.550351619720459, loss=0.6714363098144531
I0301 07:12:40.822336 140409862878976 logging_writer.py:48] [267600] global_step=267600, grad_norm=4.19057559967041, loss=0.6237722635269165
I0301 07:13:14.541557 140409854486272 logging_writer.py:48] [267700] global_step=267700, grad_norm=4.29071044921875, loss=0.536535382270813
I0301 07:13:48.235021 140409862878976 logging_writer.py:48] [267800] global_step=267800, grad_norm=4.757525444030762, loss=0.5788095593452454
I0301 07:14:21.968992 140409854486272 logging_writer.py:48] [267900] global_step=267900, grad_norm=4.115391254425049, loss=0.5625487565994263
I0301 07:14:24.815766 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:14:30.964799 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:14:39.631909 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:14:41.965332 140573303715648 submission_runner.py:411] Time since start: 93605.96s, 	Step: 267910, 	{'train/accuracy': 0.9594228267669678, 'train/loss': 0.14789187908172607, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0531243085861206, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.817447304725647, 'test/num_examples': 10000, 'score': 90328.8978767395, 'total_duration': 93605.95786976814, 'accumulated_submission_time': 90328.8978767395, 'accumulated_eval_time': 3256.0993373394012, 'accumulated_logging_time': 10.37904691696167}
I0301 07:14:42.030130 140407379851008 logging_writer.py:48] [267910] accumulated_eval_time=3256.099337, accumulated_logging_time=10.379047, accumulated_submission_time=90328.897877, global_step=267910, preemption_count=0, score=90328.897877, test/accuracy=0.630800, test/loss=1.817447, test/num_examples=10000, total_duration=93605.957870, train/accuracy=0.959423, train/loss=0.147892, validation/accuracy=0.755640, validation/loss=1.053124, validation/num_examples=50000
I0301 07:15:12.666453 140408319375104 logging_writer.py:48] [268000] global_step=268000, grad_norm=4.7389817237854, loss=0.634064793586731
I0301 07:15:46.303951 140407379851008 logging_writer.py:48] [268100] global_step=268100, grad_norm=4.048562049865723, loss=0.5523258447647095
I0301 07:16:19.978917 140408319375104 logging_writer.py:48] [268200] global_step=268200, grad_norm=4.17824125289917, loss=0.6344899535179138
I0301 07:16:53.691456 140407379851008 logging_writer.py:48] [268300] global_step=268300, grad_norm=4.5056867599487305, loss=0.6402126550674438
I0301 07:17:27.402686 140408319375104 logging_writer.py:48] [268400] global_step=268400, grad_norm=4.917242527008057, loss=0.7359261512756348
I0301 07:18:01.187088 140407379851008 logging_writer.py:48] [268500] global_step=268500, grad_norm=4.374122142791748, loss=0.6168075203895569
I0301 07:18:34.852499 140408319375104 logging_writer.py:48] [268600] global_step=268600, grad_norm=4.8092942237854, loss=0.604266881942749
I0301 07:19:08.576786 140407379851008 logging_writer.py:48] [268700] global_step=268700, grad_norm=4.867776870727539, loss=0.6269654035568237
I0301 07:19:42.291912 140408319375104 logging_writer.py:48] [268800] global_step=268800, grad_norm=4.216650485992432, loss=0.5572746396064758
I0301 07:20:15.993175 140407379851008 logging_writer.py:48] [268900] global_step=268900, grad_norm=4.3393096923828125, loss=0.5624184012413025
I0301 07:20:49.720225 140408319375104 logging_writer.py:48] [269000] global_step=269000, grad_norm=4.851720809936523, loss=0.6514602303504944
I0301 07:21:23.458874 140407379851008 logging_writer.py:48] [269100] global_step=269100, grad_norm=4.320865631103516, loss=0.5920125246047974
I0301 07:21:57.163896 140408319375104 logging_writer.py:48] [269200] global_step=269200, grad_norm=4.466886043548584, loss=0.658208966255188
I0301 07:22:30.804280 140407379851008 logging_writer.py:48] [269300] global_step=269300, grad_norm=5.22500467300415, loss=0.6506844162940979
I0301 07:23:04.488124 140408319375104 logging_writer.py:48] [269400] global_step=269400, grad_norm=4.772134780883789, loss=0.6532993316650391
I0301 07:23:12.043920 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:23:18.826631 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:23:27.737398 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:23:30.024781 140573303715648 submission_runner.py:411] Time since start: 94134.02s, 	Step: 269424, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.1480940580368042, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0508418083190918, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8160980939865112, 'test/num_examples': 10000, 'score': 90838.84168171883, 'total_duration': 94134.01732158661, 'accumulated_submission_time': 90838.84168171883, 'accumulated_eval_time': 3274.0801503658295, 'accumulated_logging_time': 10.45369815826416}
I0301 07:23:30.089904 140409837700864 logging_writer.py:48] [269424] accumulated_eval_time=3274.080150, accumulated_logging_time=10.453698, accumulated_submission_time=90838.841682, global_step=269424, preemption_count=0, score=90838.841682, test/accuracy=0.631700, test/loss=1.816098, test/num_examples=10000, total_duration=94134.017322, train/accuracy=0.960081, train/loss=0.148094, validation/accuracy=0.755880, validation/loss=1.050842, validation/num_examples=50000
I0301 07:23:56.025202 140409846093568 logging_writer.py:48] [269500] global_step=269500, grad_norm=4.328491687774658, loss=0.5859066247940063
I0301 07:24:29.788354 140409837700864 logging_writer.py:48] [269600] global_step=269600, grad_norm=4.747743606567383, loss=0.6232256293296814
I0301 07:25:03.460176 140409846093568 logging_writer.py:48] [269700] global_step=269700, grad_norm=4.808033466339111, loss=0.6956563591957092
I0301 07:25:37.183194 140409837700864 logging_writer.py:48] [269800] global_step=269800, grad_norm=4.183169841766357, loss=0.5773443579673767
I0301 07:26:10.856055 140409846093568 logging_writer.py:48] [269900] global_step=269900, grad_norm=4.195749282836914, loss=0.5674816370010376
I0301 07:26:44.558398 140409837700864 logging_writer.py:48] [270000] global_step=270000, grad_norm=4.851942539215088, loss=0.700193464756012
I0301 07:27:18.269651 140409846093568 logging_writer.py:48] [270100] global_step=270100, grad_norm=4.253378391265869, loss=0.6258116960525513
I0301 07:27:51.977923 140409837700864 logging_writer.py:48] [270200] global_step=270200, grad_norm=4.307322025299072, loss=0.5653448104858398
I0301 07:28:25.684275 140409846093568 logging_writer.py:48] [270300] global_step=270300, grad_norm=4.706187725067139, loss=0.6806828379631042
I0301 07:28:59.418796 140409837700864 logging_writer.py:48] [270400] global_step=270400, grad_norm=3.998537302017212, loss=0.5388706922531128
I0301 07:29:33.114672 140409846093568 logging_writer.py:48] [270500] global_step=270500, grad_norm=5.034801959991455, loss=0.7021748423576355
I0301 07:30:06.824442 140409837700864 logging_writer.py:48] [270600] global_step=270600, grad_norm=4.449362754821777, loss=0.6342113018035889
I0301 07:30:40.675523 140409846093568 logging_writer.py:48] [270700] global_step=270700, grad_norm=4.579176425933838, loss=0.6179941892623901
I0301 07:31:14.392626 140409837700864 logging_writer.py:48] [270800] global_step=270800, grad_norm=4.174044609069824, loss=0.5504797697067261
I0301 07:31:48.087212 140409846093568 logging_writer.py:48] [270900] global_step=270900, grad_norm=4.7023749351501465, loss=0.6850355267524719
I0301 07:32:00.061651 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:32:06.294459 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:32:15.156941 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:32:17.583911 140573303715648 submission_runner.py:411] Time since start: 94661.58s, 	Step: 270937, 	{'train/accuracy': 0.9594626426696777, 'train/loss': 0.14874282479286194, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0522432327270508, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.818058729171753, 'test/num_examples': 10000, 'score': 91348.74285554886, 'total_duration': 94661.57633805275, 'accumulated_submission_time': 91348.74285554886, 'accumulated_eval_time': 3291.6022622585297, 'accumulated_logging_time': 10.529751300811768}
I0301 07:32:17.660815 140407379851008 logging_writer.py:48] [270937] accumulated_eval_time=3291.602262, accumulated_logging_time=10.529751, accumulated_submission_time=91348.742856, global_step=270937, preemption_count=0, score=91348.742856, test/accuracy=0.631400, test/loss=1.818059, test/num_examples=10000, total_duration=94661.576338, train/accuracy=0.959463, train/loss=0.148743, validation/accuracy=0.755900, validation/loss=1.052243, validation/num_examples=50000
I0301 07:32:39.237200 140408319375104 logging_writer.py:48] [271000] global_step=271000, grad_norm=4.6958909034729, loss=0.59891676902771
I0301 07:33:12.933928 140407379851008 logging_writer.py:48] [271100] global_step=271100, grad_norm=4.663062572479248, loss=0.6470272541046143
I0301 07:33:46.636407 140408319375104 logging_writer.py:48] [271200] global_step=271200, grad_norm=4.376567840576172, loss=0.6118550300598145
I0301 07:34:20.353531 140407379851008 logging_writer.py:48] [271300] global_step=271300, grad_norm=4.5481462478637695, loss=0.5649476647377014
I0301 07:34:54.037502 140408319375104 logging_writer.py:48] [271400] global_step=271400, grad_norm=4.240304470062256, loss=0.551283597946167
I0301 07:35:27.768415 140407379851008 logging_writer.py:48] [271500] global_step=271500, grad_norm=4.212903022766113, loss=0.6117814779281616
I0301 07:36:01.419523 140408319375104 logging_writer.py:48] [271600] global_step=271600, grad_norm=4.222811222076416, loss=0.586275577545166
I0301 07:36:35.239572 140407379851008 logging_writer.py:48] [271700] global_step=271700, grad_norm=4.520397186279297, loss=0.6553303003311157
I0301 07:37:08.949910 140408319375104 logging_writer.py:48] [271800] global_step=271800, grad_norm=4.679006576538086, loss=0.6414211392402649
I0301 07:37:42.605754 140407379851008 logging_writer.py:48] [271900] global_step=271900, grad_norm=4.651037693023682, loss=0.6597998738288879
I0301 07:38:16.275223 140408319375104 logging_writer.py:48] [272000] global_step=272000, grad_norm=4.905656814575195, loss=0.6896443367004395
I0301 07:38:49.999350 140407379851008 logging_writer.py:48] [272100] global_step=272100, grad_norm=4.289152145385742, loss=0.5987073183059692
I0301 07:39:23.675212 140408319375104 logging_writer.py:48] [272200] global_step=272200, grad_norm=4.103984832763672, loss=0.595914900302887
I0301 07:39:57.327768 140407379851008 logging_writer.py:48] [272300] global_step=272300, grad_norm=4.7842278480529785, loss=0.6416312456130981
I0301 07:40:31.031198 140408319375104 logging_writer.py:48] [272400] global_step=272400, grad_norm=4.278007984161377, loss=0.6390435695648193
I0301 07:40:47.733198 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:40:53.787963 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:41:02.761787 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:41:05.126843 140573303715648 submission_runner.py:411] Time since start: 95189.12s, 	Step: 272451, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.1474275290966034, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.051373839378357, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.81630539894104, 'test/num_examples': 10000, 'score': 91858.74562954903, 'total_duration': 95189.11938214302, 'accumulated_submission_time': 91858.74562954903, 'accumulated_eval_time': 3308.995859146118, 'accumulated_logging_time': 10.616759300231934}
I0301 07:41:05.189930 140408319375104 logging_writer.py:48] [272451] accumulated_eval_time=3308.995859, accumulated_logging_time=10.616759, accumulated_submission_time=91858.745630, global_step=272451, preemption_count=0, score=91858.745630, test/accuracy=0.631200, test/loss=1.816305, test/num_examples=10000, total_duration=95189.119382, train/accuracy=0.961296, train/loss=0.147428, validation/accuracy=0.755480, validation/loss=1.051374, validation/num_examples=50000
I0301 07:41:22.017996 140409124681472 logging_writer.py:48] [272500] global_step=272500, grad_norm=4.497756481170654, loss=0.6077560782432556
I0301 07:41:55.685762 140408319375104 logging_writer.py:48] [272600] global_step=272600, grad_norm=4.5151166915893555, loss=0.5884512066841125
I0301 07:42:29.455390 140409124681472 logging_writer.py:48] [272700] global_step=272700, grad_norm=4.548336982727051, loss=0.564553439617157
I0301 07:43:03.136537 140408319375104 logging_writer.py:48] [272800] global_step=272800, grad_norm=4.522831916809082, loss=0.5865002274513245
I0301 07:43:36.861940 140409124681472 logging_writer.py:48] [272900] global_step=272900, grad_norm=4.455160617828369, loss=0.6480148434638977
I0301 07:44:10.661569 140408319375104 logging_writer.py:48] [273000] global_step=273000, grad_norm=4.474117279052734, loss=0.5709028840065002
I0301 07:44:44.373089 140409124681472 logging_writer.py:48] [273100] global_step=273100, grad_norm=4.614358901977539, loss=0.6462584733963013
I0301 07:45:18.095819 140408319375104 logging_writer.py:48] [273200] global_step=273200, grad_norm=3.9734995365142822, loss=0.6738502383232117
I0301 07:45:51.833093 140409124681472 logging_writer.py:48] [273300] global_step=273300, grad_norm=4.638182163238525, loss=0.5946134328842163
I0301 07:46:25.510814 140408319375104 logging_writer.py:48] [273400] global_step=273400, grad_norm=5.020691871643066, loss=0.6026459336280823
I0301 07:46:59.266658 140409124681472 logging_writer.py:48] [273500] global_step=273500, grad_norm=4.935054779052734, loss=0.6477285027503967
I0301 07:47:32.967970 140408319375104 logging_writer.py:48] [273600] global_step=273600, grad_norm=4.968386650085449, loss=0.6303623914718628
I0301 07:48:06.673721 140409124681472 logging_writer.py:48] [273700] global_step=273700, grad_norm=4.301890850067139, loss=0.5927773118019104
I0301 07:48:40.472789 140408319375104 logging_writer.py:48] [273800] global_step=273800, grad_norm=5.243903636932373, loss=0.628421425819397
I0301 07:49:14.157324 140409124681472 logging_writer.py:48] [273900] global_step=273900, grad_norm=3.966837167739868, loss=0.5347769856452942
I0301 07:49:35.203810 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:49:41.266558 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:49:50.174046 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:49:52.420576 140573303715648 submission_runner.py:411] Time since start: 95716.41s, 	Step: 273964, 	{'train/accuracy': 0.9615553021430969, 'train/loss': 0.14429840445518494, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0525203943252563, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8180094957351685, 'test/num_examples': 10000, 'score': 92368.68969726562, 'total_duration': 95716.41310477257, 'accumulated_submission_time': 92368.68969726562, 'accumulated_eval_time': 3326.2125630378723, 'accumulated_logging_time': 10.689738750457764}
I0301 07:49:52.483034 140409846093568 logging_writer.py:48] [273964] accumulated_eval_time=3326.212563, accumulated_logging_time=10.689739, accumulated_submission_time=92368.689697, global_step=273964, preemption_count=0, score=92368.689697, test/accuracy=0.630300, test/loss=1.818009, test/num_examples=10000, total_duration=95716.413105, train/accuracy=0.961555, train/loss=0.144298, validation/accuracy=0.755240, validation/loss=1.052520, validation/num_examples=50000
I0301 07:50:04.933846 140409854486272 logging_writer.py:48] [274000] global_step=274000, grad_norm=4.369854927062988, loss=0.6119978427886963
I0301 07:50:38.678437 140409846093568 logging_writer.py:48] [274100] global_step=274100, grad_norm=4.732358932495117, loss=0.6246441602706909
I0301 07:51:12.362019 140409854486272 logging_writer.py:48] [274200] global_step=274200, grad_norm=4.626216411590576, loss=0.6708645224571228
I0301 07:51:46.052230 140409846093568 logging_writer.py:48] [274300] global_step=274300, grad_norm=4.701886177062988, loss=0.6028579473495483
I0301 07:52:19.774528 140409854486272 logging_writer.py:48] [274400] global_step=274400, grad_norm=4.970097064971924, loss=0.6124546527862549
I0301 07:52:53.441270 140409846093568 logging_writer.py:48] [274500] global_step=274500, grad_norm=4.404878616333008, loss=0.5870741605758667
I0301 07:53:27.090291 140409854486272 logging_writer.py:48] [274600] global_step=274600, grad_norm=4.375025272369385, loss=0.5797035694122314
I0301 07:54:00.846791 140409846093568 logging_writer.py:48] [274700] global_step=274700, grad_norm=4.498321056365967, loss=0.6948906779289246
I0301 07:54:34.517065 140409854486272 logging_writer.py:48] [274800] global_step=274800, grad_norm=4.977087497711182, loss=0.694564938545227
I0301 07:55:08.297449 140409846093568 logging_writer.py:48] [274900] global_step=274900, grad_norm=4.4784016609191895, loss=0.6805949211120605
I0301 07:55:41.999109 140409854486272 logging_writer.py:48] [275000] global_step=275000, grad_norm=4.408657073974609, loss=0.6721349358558655
I0301 07:56:15.690061 140409846093568 logging_writer.py:48] [275100] global_step=275100, grad_norm=4.4516143798828125, loss=0.5996934175491333
I0301 07:56:49.372020 140409854486272 logging_writer.py:48] [275200] global_step=275200, grad_norm=4.759075164794922, loss=0.7213585376739502
I0301 07:57:23.142518 140409846093568 logging_writer.py:48] [275300] global_step=275300, grad_norm=4.807641983032227, loss=0.7034366130828857
I0301 07:57:56.790810 140409854486272 logging_writer.py:48] [275400] global_step=275400, grad_norm=4.212331295013428, loss=0.6288633346557617
I0301 07:58:22.528895 140573303715648 spec.py:321] Evaluating on the training split.
I0301 07:58:28.597401 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 07:58:37.211678 140573303715648 spec.py:349] Evaluating on the test split.
I0301 07:58:39.530234 140573303715648 submission_runner.py:411] Time since start: 96243.52s, 	Step: 275478, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14447790384292603, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0514625310897827, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8165348768234253, 'test/num_examples': 10000, 'score': 92878.66392302513, 'total_duration': 96243.52274870872, 'accumulated_submission_time': 92878.66392302513, 'accumulated_eval_time': 3343.2138271331787, 'accumulated_logging_time': 10.763943195343018}
I0301 07:58:39.593003 140407371458304 logging_writer.py:48] [275478] accumulated_eval_time=3343.213827, accumulated_logging_time=10.763943, accumulated_submission_time=92878.663923, global_step=275478, preemption_count=0, score=92878.663923, test/accuracy=0.630900, test/loss=1.816535, test/num_examples=10000, total_duration=96243.522749, train/accuracy=0.961595, train/loss=0.144478, validation/accuracy=0.755640, validation/loss=1.051463, validation/num_examples=50000
I0301 07:58:47.360487 140407379851008 logging_writer.py:48] [275500] global_step=275500, grad_norm=4.3931803703308105, loss=0.5526318550109863
I0301 07:59:21.013845 140407371458304 logging_writer.py:48] [275600] global_step=275600, grad_norm=4.423573017120361, loss=0.582488477230072
I0301 07:59:54.735213 140407379851008 logging_writer.py:48] [275700] global_step=275700, grad_norm=4.594388961791992, loss=0.6097069382667542
I0301 08:00:28.391558 140407371458304 logging_writer.py:48] [275800] global_step=275800, grad_norm=5.232285976409912, loss=0.6615051627159119
I0301 08:01:02.176062 140407379851008 logging_writer.py:48] [275900] global_step=275900, grad_norm=4.587055206298828, loss=0.6060237884521484
I0301 08:01:35.853042 140407371458304 logging_writer.py:48] [276000] global_step=276000, grad_norm=4.616246700286865, loss=0.6009942293167114
I0301 08:02:09.531904 140407379851008 logging_writer.py:48] [276100] global_step=276100, grad_norm=4.47868537902832, loss=0.6070852875709534
I0301 08:02:43.216912 140407371458304 logging_writer.py:48] [276200] global_step=276200, grad_norm=4.576891899108887, loss=0.6006825566291809
I0301 08:03:16.877799 140407379851008 logging_writer.py:48] [276300] global_step=276300, grad_norm=4.126153469085693, loss=0.57206130027771
I0301 08:03:50.593723 140407371458304 logging_writer.py:48] [276400] global_step=276400, grad_norm=4.3255414962768555, loss=0.5836387872695923
I0301 08:04:24.325434 140407379851008 logging_writer.py:48] [276500] global_step=276500, grad_norm=4.188165187835693, loss=0.5825951099395752
I0301 08:04:58.044115 140407371458304 logging_writer.py:48] [276600] global_step=276600, grad_norm=4.601266860961914, loss=0.6557536125183105
I0301 08:05:31.754307 140407379851008 logging_writer.py:48] [276700] global_step=276700, grad_norm=4.469019889831543, loss=0.6820192933082581
I0301 08:06:05.392949 140407371458304 logging_writer.py:48] [276800] global_step=276800, grad_norm=4.44657039642334, loss=0.6671439409255981
I0301 08:06:39.115007 140407379851008 logging_writer.py:48] [276900] global_step=276900, grad_norm=4.80165433883667, loss=0.6545862555503845
I0301 08:07:09.643789 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:07:15.731625 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 08:07:24.599917 140573303715648 spec.py:349] Evaluating on the test split.
I0301 08:07:26.867314 140573303715648 submission_runner.py:411] Time since start: 96770.86s, 	Step: 276992, 	{'train/accuracy': 0.9596420526504517, 'train/loss': 0.14797401428222656, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0522382259368896, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8188046216964722, 'test/num_examples': 10000, 'score': 93388.64336013794, 'total_duration': 96770.8598151207, 'accumulated_submission_time': 93388.64336013794, 'accumulated_eval_time': 3360.4372668266296, 'accumulated_logging_time': 10.837567806243896}
I0301 08:07:26.935741 140409854486272 logging_writer.py:48] [276992] accumulated_eval_time=3360.437267, accumulated_logging_time=10.837568, accumulated_submission_time=93388.643360, global_step=276992, preemption_count=0, score=93388.643360, test/accuracy=0.630800, test/loss=1.818805, test/num_examples=10000, total_duration=96770.859815, train/accuracy=0.959642, train/loss=0.147974, validation/accuracy=0.755940, validation/loss=1.052238, validation/num_examples=50000
I0301 08:07:29.963753 140409862878976 logging_writer.py:48] [277000] global_step=277000, grad_norm=4.506551742553711, loss=0.6044827103614807
I0301 08:08:03.631893 140409854486272 logging_writer.py:48] [277100] global_step=277100, grad_norm=4.706592082977295, loss=0.6107660531997681
I0301 08:08:37.375109 140409862878976 logging_writer.py:48] [277200] global_step=277200, grad_norm=4.728022575378418, loss=0.5818873047828674
I0301 08:09:11.039833 140409854486272 logging_writer.py:48] [277300] global_step=277300, grad_norm=4.643507480621338, loss=0.6671844720840454
I0301 08:09:44.739711 140409862878976 logging_writer.py:48] [277400] global_step=277400, grad_norm=4.664040565490723, loss=0.6489594578742981
I0301 08:10:18.461221 140409854486272 logging_writer.py:48] [277500] global_step=277500, grad_norm=4.517045974731445, loss=0.6124703288078308
I0301 08:10:52.123497 140409862878976 logging_writer.py:48] [277600] global_step=277600, grad_norm=4.486845970153809, loss=0.6097766160964966
I0301 08:11:26.091007 140409854486272 logging_writer.py:48] [277700] global_step=277700, grad_norm=4.843965530395508, loss=0.6784783601760864
I0301 08:11:59.749780 140409862878976 logging_writer.py:48] [277800] global_step=277800, grad_norm=4.475452899932861, loss=0.5692137479782104
I0301 08:12:33.484161 140409854486272 logging_writer.py:48] [277900] global_step=277900, grad_norm=4.636007308959961, loss=0.5913922190666199
I0301 08:13:07.233808 140409862878976 logging_writer.py:48] [278000] global_step=278000, grad_norm=4.818793773651123, loss=0.6530167460441589
I0301 08:13:40.966323 140409854486272 logging_writer.py:48] [278100] global_step=278100, grad_norm=4.225451469421387, loss=0.5879369378089905
I0301 08:14:14.615163 140409862878976 logging_writer.py:48] [278200] global_step=278200, grad_norm=4.27131986618042, loss=0.5705212354660034
I0301 08:14:48.313409 140409854486272 logging_writer.py:48] [278300] global_step=278300, grad_norm=4.658745765686035, loss=0.591935932636261
I0301 08:15:22.010336 140409862878976 logging_writer.py:48] [278400] global_step=278400, grad_norm=4.6486005783081055, loss=0.5973066091537476
I0301 08:15:55.680396 140409854486272 logging_writer.py:48] [278500] global_step=278500, grad_norm=5.023201942443848, loss=0.6611918210983276
I0301 08:15:57.176293 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:16:03.388641 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 08:16:12.047845 140573303715648 spec.py:349] Evaluating on the test split.
I0301 08:16:14.370564 140573303715648 submission_runner.py:411] Time since start: 97298.36s, 	Step: 278506, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14901840686798096, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0525351762771606, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.817908525466919, 'test/num_examples': 10000, 'score': 93898.81333494186, 'total_duration': 97298.36285185814, 'accumulated_submission_time': 93898.81333494186, 'accumulated_eval_time': 3377.6312334537506, 'accumulated_logging_time': 10.91645884513855}
I0301 08:16:14.435880 140409124681472 logging_writer.py:48] [278506] accumulated_eval_time=3377.631233, accumulated_logging_time=10.916459, accumulated_submission_time=93898.813335, global_step=278506, preemption_count=0, score=93898.813335, test/accuracy=0.631100, test/loss=1.817909, test/num_examples=10000, total_duration=97298.362852, train/accuracy=0.960658, train/loss=0.149018, validation/accuracy=0.755780, validation/loss=1.052535, validation/num_examples=50000
I0301 08:16:46.386772 140409829308160 logging_writer.py:48] [278600] global_step=278600, grad_norm=4.5062150955200195, loss=0.6034525632858276
I0301 08:17:20.039832 140409124681472 logging_writer.py:48] [278700] global_step=278700, grad_norm=4.604464530944824, loss=0.6932470798492432
I0301 08:17:53.713963 140409829308160 logging_writer.py:48] [278800] global_step=278800, grad_norm=4.879284381866455, loss=0.6387313604354858
I0301 08:18:27.436501 140409124681472 logging_writer.py:48] [278900] global_step=278900, grad_norm=4.506317615509033, loss=0.6648609638214111
I0301 08:19:01.116194 140409829308160 logging_writer.py:48] [279000] global_step=279000, grad_norm=4.3607001304626465, loss=0.6202538013458252
I0301 08:19:35.016108 140409124681472 logging_writer.py:48] [279100] global_step=279100, grad_norm=4.414947032928467, loss=0.596474826335907
I0301 08:20:08.700497 140409829308160 logging_writer.py:48] [279200] global_step=279200, grad_norm=4.9534382820129395, loss=0.6288591623306274
I0301 08:20:42.433762 140409124681472 logging_writer.py:48] [279300] global_step=279300, grad_norm=4.495405197143555, loss=0.6001396179199219
I0301 08:21:16.133108 140409829308160 logging_writer.py:48] [279400] global_step=279400, grad_norm=4.535249710083008, loss=0.6861149072647095
I0301 08:21:49.855536 140409124681472 logging_writer.py:48] [279500] global_step=279500, grad_norm=5.40416955947876, loss=0.6162847280502319
I0301 08:22:23.542280 140409829308160 logging_writer.py:48] [279600] global_step=279600, grad_norm=4.157373428344727, loss=0.590944230556488
I0301 08:22:57.261550 140409124681472 logging_writer.py:48] [279700] global_step=279700, grad_norm=4.840663433074951, loss=0.5509104132652283
I0301 08:23:30.990284 140409829308160 logging_writer.py:48] [279800] global_step=279800, grad_norm=5.489494323730469, loss=0.7057294845581055
I0301 08:24:04.739488 140409124681472 logging_writer.py:48] [279900] global_step=279900, grad_norm=4.392446517944336, loss=0.6815595626831055
I0301 08:24:38.452462 140409829308160 logging_writer.py:48] [280000] global_step=280000, grad_norm=4.421240329742432, loss=0.6399016976356506
I0301 08:24:44.667141 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:24:50.764992 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 08:24:59.371630 140573303715648 spec.py:349] Evaluating on the test split.
I0301 08:25:01.685108 140573303715648 submission_runner.py:411] Time since start: 97825.68s, 	Step: 280020, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14303480088710785, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0523576736450195, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8187475204467773, 'test/num_examples': 10000, 'score': 94408.975086689, 'total_duration': 97825.67763566971, 'accumulated_submission_time': 94408.975086689, 'accumulated_eval_time': 3394.6491374969482, 'accumulated_logging_time': 10.992409467697144}
I0301 08:25:01.767915 140409854486272 logging_writer.py:48] [280020] accumulated_eval_time=3394.649137, accumulated_logging_time=10.992409, accumulated_submission_time=94408.975087, global_step=280020, preemption_count=0, score=94408.975087, test/accuracy=0.630500, test/loss=1.818748, test/num_examples=10000, total_duration=97825.677636, train/accuracy=0.961097, train/loss=0.143035, validation/accuracy=0.755740, validation/loss=1.052358, validation/num_examples=50000
I0301 08:25:29.118031 140409862878976 logging_writer.py:48] [280100] global_step=280100, grad_norm=4.45734977722168, loss=0.5591184496879578
I0301 08:26:02.767215 140409854486272 logging_writer.py:48] [280200] global_step=280200, grad_norm=4.511624813079834, loss=0.618517279624939
I0301 08:26:36.484415 140409862878976 logging_writer.py:48] [280300] global_step=280300, grad_norm=4.505353927612305, loss=0.569900631904602
I0301 08:27:10.124171 140409854486272 logging_writer.py:48] [280400] global_step=280400, grad_norm=4.7624335289001465, loss=0.6436769962310791
I0301 08:27:43.766975 140409862878976 logging_writer.py:48] [280500] global_step=280500, grad_norm=4.442142009735107, loss=0.6243568658828735
I0301 08:28:17.399125 140409854486272 logging_writer.py:48] [280600] global_step=280600, grad_norm=4.723763465881348, loss=0.6766785383224487
I0301 08:28:51.057172 140409862878976 logging_writer.py:48] [280700] global_step=280700, grad_norm=4.693181037902832, loss=0.6172205209732056
I0301 08:29:24.785435 140409854486272 logging_writer.py:48] [280800] global_step=280800, grad_norm=4.179600238800049, loss=0.6007859110832214
I0301 08:29:58.456369 140409862878976 logging_writer.py:48] [280900] global_step=280900, grad_norm=4.665186405181885, loss=0.7022032737731934
I0301 08:30:32.168210 140409854486272 logging_writer.py:48] [281000] global_step=281000, grad_norm=4.263334274291992, loss=0.5601082444190979
I0301 08:31:05.839824 140409862878976 logging_writer.py:48] [281100] global_step=281100, grad_norm=4.403836727142334, loss=0.6513004302978516
I0301 08:31:39.573966 140409854486272 logging_writer.py:48] [281200] global_step=281200, grad_norm=4.5738606452941895, loss=0.5843147039413452
I0301 08:32:13.222621 140409862878976 logging_writer.py:48] [281300] global_step=281300, grad_norm=4.604363918304443, loss=0.6531364321708679
I0301 08:32:46.871214 140409854486272 logging_writer.py:48] [281400] global_step=281400, grad_norm=4.621283054351807, loss=0.6211665272712708
I0301 08:33:20.625937 140409862878976 logging_writer.py:48] [281500] global_step=281500, grad_norm=4.656118392944336, loss=0.6558073163032532
I0301 08:33:31.878113 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:33:37.957818 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 08:33:46.697432 140573303715648 spec.py:349] Evaluating on the test split.
I0301 08:33:48.994202 140573303715648 submission_runner.py:411] Time since start: 98352.99s, 	Step: 281535, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14687871932983398, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0521724224090576, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8173832893371582, 'test/num_examples': 10000, 'score': 94919.01567602158, 'total_duration': 98352.9867374897, 'accumulated_submission_time': 94919.01567602158, 'accumulated_eval_time': 3411.7651705741882, 'accumulated_logging_time': 11.085949182510376}
I0301 08:33:49.058645 140407371458304 logging_writer.py:48] [281535] accumulated_eval_time=3411.765171, accumulated_logging_time=11.085949, accumulated_submission_time=94919.015676, global_step=281535, preemption_count=0, score=94919.015676, test/accuracy=0.631600, test/loss=1.817383, test/num_examples=10000, total_duration=98352.986737, train/accuracy=0.961037, train/loss=0.146879, validation/accuracy=0.755640, validation/loss=1.052172, validation/num_examples=50000
I0301 08:34:11.278275 140407379851008 logging_writer.py:48] [281600] global_step=281600, grad_norm=4.7935099601745605, loss=0.5794234275817871
I0301 08:34:44.971368 140407371458304 logging_writer.py:48] [281700] global_step=281700, grad_norm=4.349058628082275, loss=0.5563315153121948
I0301 08:35:18.616965 140407379851008 logging_writer.py:48] [281800] global_step=281800, grad_norm=4.361805438995361, loss=0.5613734126091003
I0301 08:35:52.278230 140407371458304 logging_writer.py:48] [281900] global_step=281900, grad_norm=4.3402581214904785, loss=0.6070259809494019
I0301 08:36:26.015958 140407379851008 logging_writer.py:48] [282000] global_step=282000, grad_norm=4.656798362731934, loss=0.6467134952545166
I0301 08:36:59.723038 140407371458304 logging_writer.py:48] [282100] global_step=282100, grad_norm=4.266601085662842, loss=0.6084313988685608
I0301 08:37:33.510226 140407379851008 logging_writer.py:48] [282200] global_step=282200, grad_norm=4.51332426071167, loss=0.6260618567466736
I0301 08:38:07.198132 140407371458304 logging_writer.py:48] [282300] global_step=282300, grad_norm=4.5826497077941895, loss=0.5940804481506348
I0301 08:38:40.902490 140407379851008 logging_writer.py:48] [282400] global_step=282400, grad_norm=4.452245712280273, loss=0.5795895457267761
I0301 08:39:14.534377 140407371458304 logging_writer.py:48] [282500] global_step=282500, grad_norm=4.549961090087891, loss=0.6806423664093018
I0301 08:39:48.231865 140407379851008 logging_writer.py:48] [282600] global_step=282600, grad_norm=4.590281963348389, loss=0.6272687911987305
I0301 08:40:21.950615 140407371458304 logging_writer.py:48] [282700] global_step=282700, grad_norm=4.487967014312744, loss=0.6404606699943542
I0301 08:40:55.619222 140407379851008 logging_writer.py:48] [282800] global_step=282800, grad_norm=4.548887729644775, loss=0.6266944408416748
I0301 08:41:29.336753 140407371458304 logging_writer.py:48] [282900] global_step=282900, grad_norm=4.817065238952637, loss=0.6006156206130981
I0301 08:42:03.009382 140407379851008 logging_writer.py:48] [283000] global_step=283000, grad_norm=4.343633651733398, loss=0.6063394546508789
I0301 08:42:19.014810 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:42:25.141362 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 08:42:33.677807 140573303715648 spec.py:349] Evaluating on the test split.
I0301 08:42:35.934747 140573303715648 submission_runner.py:411] Time since start: 98879.93s, 	Step: 283049, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14507494866847992, 'validation/accuracy': 0.7563599944114685, 'validation/loss': 1.0516657829284668, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.817343831062317, 'test/num_examples': 10000, 'score': 95428.90130352974, 'total_duration': 98879.92723703384, 'accumulated_submission_time': 95428.90130352974, 'accumulated_eval_time': 3428.6850056648254, 'accumulated_logging_time': 11.162020683288574}
I0301 08:42:35.998131 140409837700864 logging_writer.py:48] [283049] accumulated_eval_time=3428.685006, accumulated_logging_time=11.162021, accumulated_submission_time=95428.901304, global_step=283049, preemption_count=0, score=95428.901304, test/accuracy=0.631000, test/loss=1.817344, test/num_examples=10000, total_duration=98879.927237, train/accuracy=0.961217, train/loss=0.145075, validation/accuracy=0.756360, validation/loss=1.051666, validation/num_examples=50000
I0301 08:42:53.512380 140409846093568 logging_writer.py:48] [283100] global_step=283100, grad_norm=4.4076337814331055, loss=0.6521331667900085
I0301 08:43:27.190863 140409837700864 logging_writer.py:48] [283200] global_step=283200, grad_norm=4.203374862670898, loss=0.5666101574897766
I0301 08:44:00.973467 140409846093568 logging_writer.py:48] [283300] global_step=283300, grad_norm=4.504460334777832, loss=0.6355791091918945
I0301 08:44:34.647215 140409837700864 logging_writer.py:48] [283400] global_step=283400, grad_norm=5.179168224334717, loss=0.6587754487991333
I0301 08:45:08.332677 140409846093568 logging_writer.py:48] [283500] global_step=283500, grad_norm=4.228405952453613, loss=0.6149219870567322
I0301 08:45:42.077516 140409837700864 logging_writer.py:48] [283600] global_step=283600, grad_norm=4.455235481262207, loss=0.6812734007835388
I0301 08:46:15.802996 140409846093568 logging_writer.py:48] [283700] global_step=283700, grad_norm=4.082505226135254, loss=0.5753242373466492
I0301 08:46:49.537738 140409837700864 logging_writer.py:48] [283800] global_step=283800, grad_norm=4.438084125518799, loss=0.6218270063400269
I0301 08:47:23.248043 140409846093568 logging_writer.py:48] [283900] global_step=283900, grad_norm=4.190701961517334, loss=0.6026651263237
I0301 08:47:56.987142 140409837700864 logging_writer.py:48] [284000] global_step=284000, grad_norm=4.896412372589111, loss=0.7406705617904663
I0301 08:48:30.691396 140409846093568 logging_writer.py:48] [284100] global_step=284100, grad_norm=4.538941383361816, loss=0.6653537154197693
I0301 08:49:04.403619 140409837700864 logging_writer.py:48] [284200] global_step=284200, grad_norm=4.390622138977051, loss=0.5800235867500305
I0301 08:49:38.092634 140409846093568 logging_writer.py:48] [284300] global_step=284300, grad_norm=4.583289623260498, loss=0.631349503993988
I0301 08:50:11.886342 140409837700864 logging_writer.py:48] [284400] global_step=284400, grad_norm=4.751253128051758, loss=0.5929908752441406
I0301 08:50:45.617733 140409846093568 logging_writer.py:48] [284500] global_step=284500, grad_norm=4.731436252593994, loss=0.6636732220649719
I0301 08:51:05.996409 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:51:12.128888 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 08:51:20.661448 140573303715648 spec.py:349] Evaluating on the test split.
I0301 08:51:22.972820 140573303715648 submission_runner.py:411] Time since start: 99406.97s, 	Step: 284562, 	{'train/accuracy': 0.9595025181770325, 'train/loss': 0.14882567524909973, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0531953573226929, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.819740653038025, 'test/num_examples': 10000, 'score': 95938.8274178505, 'total_duration': 99406.96535897255, 'accumulated_submission_time': 95938.8274178505, 'accumulated_eval_time': 3445.661366701126, 'accumulated_logging_time': 11.237503051757812}
I0301 08:51:23.036691 140407379851008 logging_writer.py:48] [284562] accumulated_eval_time=3445.661367, accumulated_logging_time=11.237503, accumulated_submission_time=95938.827418, global_step=284562, preemption_count=0, score=95938.827418, test/accuracy=0.630900, test/loss=1.819741, test/num_examples=10000, total_duration=99406.965359, train/accuracy=0.959503, train/loss=0.148826, validation/accuracy=0.755740, validation/loss=1.053195, validation/num_examples=50000
I0301 08:51:36.160261 140408319375104 logging_writer.py:48] [284600] global_step=284600, grad_norm=4.56038236618042, loss=0.6441516876220703
I0301 08:52:09.871504 140407379851008 logging_writer.py:48] [284700] global_step=284700, grad_norm=4.651607513427734, loss=0.6394258141517639
I0301 08:52:43.530773 140408319375104 logging_writer.py:48] [284800] global_step=284800, grad_norm=4.535331726074219, loss=0.6625427603721619
I0301 08:53:17.315322 140407379851008 logging_writer.py:48] [284900] global_step=284900, grad_norm=4.111326694488525, loss=0.5901622772216797
I0301 08:53:51.023220 140408319375104 logging_writer.py:48] [285000] global_step=285000, grad_norm=4.07092809677124, loss=0.5506523847579956
I0301 08:54:24.782304 140407379851008 logging_writer.py:48] [285100] global_step=285100, grad_norm=4.367259979248047, loss=0.6003261208534241
I0301 08:54:58.480157 140408319375104 logging_writer.py:48] [285200] global_step=285200, grad_norm=5.082311153411865, loss=0.6913627982139587
I0301 08:55:32.216717 140407379851008 logging_writer.py:48] [285300] global_step=285300, grad_norm=4.964079856872559, loss=0.6341478824615479
I0301 08:56:05.986676 140408319375104 logging_writer.py:48] [285400] global_step=285400, grad_norm=4.414739608764648, loss=0.5614653825759888
I0301 08:56:39.644407 140407379851008 logging_writer.py:48] [285500] global_step=285500, grad_norm=4.267304420471191, loss=0.6300404071807861
I0301 08:57:13.371339 140408319375104 logging_writer.py:48] [285600] global_step=285600, grad_norm=4.471285820007324, loss=0.6599883437156677
I0301 08:57:47.060244 140407379851008 logging_writer.py:48] [285700] global_step=285700, grad_norm=4.346641540527344, loss=0.6436607837677002
I0301 08:58:20.762522 140408319375104 logging_writer.py:48] [285800] global_step=285800, grad_norm=5.0394368171691895, loss=0.6388803124427795
I0301 08:58:54.488718 140407379851008 logging_writer.py:48] [285900] global_step=285900, grad_norm=4.229244232177734, loss=0.6110137701034546
I0301 08:59:28.235069 140408319375104 logging_writer.py:48] [286000] global_step=286000, grad_norm=5.016897201538086, loss=0.6289548873901367
I0301 08:59:53.302544 140573303715648 spec.py:321] Evaluating on the training split.
I0301 08:59:59.399687 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:00:08.271358 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:00:10.660225 140573303715648 submission_runner.py:411] Time since start: 99934.65s, 	Step: 286076, 	{'train/accuracy': 0.9608378410339355, 'train/loss': 0.14710548520088196, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.052351713180542, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.818746566772461, 'test/num_examples': 10000, 'score': 96449.02365016937, 'total_duration': 99934.65275216103, 'accumulated_submission_time': 96449.02365016937, 'accumulated_eval_time': 3463.0189859867096, 'accumulated_logging_time': 11.311057567596436}
I0301 09:00:10.723190 140409846093568 logging_writer.py:48] [286076] accumulated_eval_time=3463.018986, accumulated_logging_time=11.311058, accumulated_submission_time=96449.023650, global_step=286076, preemption_count=0, score=96449.023650, test/accuracy=0.631500, test/loss=1.818747, test/num_examples=10000, total_duration=99934.652752, train/accuracy=0.960838, train/loss=0.147105, validation/accuracy=0.756060, validation/loss=1.052352, validation/num_examples=50000
I0301 09:00:19.170665 140409854486272 logging_writer.py:48] [286100] global_step=286100, grad_norm=4.313198566436768, loss=0.6629935503005981
I0301 09:00:52.824004 140409846093568 logging_writer.py:48] [286200] global_step=286200, grad_norm=4.604543685913086, loss=0.6532847285270691
I0301 09:01:26.497063 140409854486272 logging_writer.py:48] [286300] global_step=286300, grad_norm=4.448902606964111, loss=0.6408531069755554
I0301 09:02:00.227809 140409846093568 logging_writer.py:48] [286400] global_step=286400, grad_norm=4.499879360198975, loss=0.6096001267433167
I0301 09:02:34.044327 140409854486272 logging_writer.py:48] [286500] global_step=286500, grad_norm=4.329228401184082, loss=0.6309220790863037
I0301 09:03:07.709467 140409846093568 logging_writer.py:48] [286600] global_step=286600, grad_norm=4.382477760314941, loss=0.5709908604621887
I0301 09:03:41.435253 140409854486272 logging_writer.py:48] [286700] global_step=286700, grad_norm=4.547746181488037, loss=0.5865695476531982
I0301 09:04:15.121660 140409846093568 logging_writer.py:48] [286800] global_step=286800, grad_norm=4.284970760345459, loss=0.5761760473251343
I0301 09:04:48.886543 140409854486272 logging_writer.py:48] [286900] global_step=286900, grad_norm=4.124175548553467, loss=0.5739303827285767
I0301 09:05:22.613705 140409846093568 logging_writer.py:48] [287000] global_step=287000, grad_norm=4.07897424697876, loss=0.5898244976997375
I0301 09:05:56.303690 140409854486272 logging_writer.py:48] [287100] global_step=287100, grad_norm=4.543664932250977, loss=0.6845322847366333
I0301 09:06:30.038658 140409846093568 logging_writer.py:48] [287200] global_step=287200, grad_norm=4.575305461883545, loss=0.6740971803665161
I0301 09:07:03.745863 140409854486272 logging_writer.py:48] [287300] global_step=287300, grad_norm=4.533454895019531, loss=0.6151164770126343
I0301 09:07:37.468932 140409846093568 logging_writer.py:48] [287400] global_step=287400, grad_norm=4.356314659118652, loss=0.5439515113830566
I0301 09:08:11.224430 140409854486272 logging_writer.py:48] [287500] global_step=287500, grad_norm=4.773191452026367, loss=0.6543165445327759
I0301 09:08:40.709634 140573303715648 spec.py:321] Evaluating on the training split.
I0301 09:08:46.754297 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:08:55.443929 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:08:57.768275 140573303715648 submission_runner.py:411] Time since start: 100461.76s, 	Step: 287589, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14632828533649445, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0514193773269653, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8166067600250244, 'test/num_examples': 10000, 'score': 96958.94147777557, 'total_duration': 100461.76079773903, 'accumulated_submission_time': 96958.94147777557, 'accumulated_eval_time': 3480.0775611400604, 'accumulated_logging_time': 11.383795976638794}
I0301 09:08:57.832363 140409124681472 logging_writer.py:48] [287589] accumulated_eval_time=3480.077561, accumulated_logging_time=11.383796, accumulated_submission_time=96958.941478, global_step=287589, preemption_count=0, score=96958.941478, test/accuracy=0.632100, test/loss=1.816607, test/num_examples=10000, total_duration=100461.760798, train/accuracy=0.960120, train/loss=0.146328, validation/accuracy=0.755740, validation/loss=1.051419, validation/num_examples=50000
I0301 09:09:01.888206 140409829308160 logging_writer.py:48] [287600] global_step=287600, grad_norm=4.2222113609313965, loss=0.6266721487045288
I0301 09:09:35.579078 140409124681472 logging_writer.py:48] [287700] global_step=287700, grad_norm=4.286606311798096, loss=0.6263184547424316
I0301 09:10:09.345748 140409829308160 logging_writer.py:48] [287800] global_step=287800, grad_norm=3.8520357608795166, loss=0.4914003014564514
I0301 09:10:43.027046 140409124681472 logging_writer.py:48] [287900] global_step=287900, grad_norm=4.199285507202148, loss=0.6177613735198975
I0301 09:11:16.720804 140409829308160 logging_writer.py:48] [288000] global_step=288000, grad_norm=4.563070774078369, loss=0.6745218634605408
I0301 09:11:50.447471 140409124681472 logging_writer.py:48] [288100] global_step=288100, grad_norm=4.895811080932617, loss=0.6552789807319641
I0301 09:12:24.226151 140409829308160 logging_writer.py:48] [288200] global_step=288200, grad_norm=4.502150058746338, loss=0.6078627109527588
I0301 09:12:57.952863 140409124681472 logging_writer.py:48] [288300] global_step=288300, grad_norm=4.280571460723877, loss=0.6155616044998169
I0301 09:13:31.670063 140409829308160 logging_writer.py:48] [288400] global_step=288400, grad_norm=4.6029253005981445, loss=0.6328474283218384
I0301 09:14:05.384039 140409124681472 logging_writer.py:48] [288500] global_step=288500, grad_norm=5.090810298919678, loss=0.6231304407119751
I0301 09:14:39.208860 140409829308160 logging_writer.py:48] [288600] global_step=288600, grad_norm=4.634936332702637, loss=0.6367752552032471
I0301 09:15:12.854420 140409124681472 logging_writer.py:48] [288700] global_step=288700, grad_norm=4.175612449645996, loss=0.5345587730407715
I0301 09:15:46.558397 140409829308160 logging_writer.py:48] [288800] global_step=288800, grad_norm=4.2410478591918945, loss=0.6391407251358032
I0301 09:16:20.280904 140409124681472 logging_writer.py:48] [288900] global_step=288900, grad_norm=4.534543514251709, loss=0.659970760345459
I0301 09:16:53.970249 140409829308160 logging_writer.py:48] [289000] global_step=289000, grad_norm=4.44863748550415, loss=0.6144253611564636
I0301 09:17:27.670153 140409124681472 logging_writer.py:48] [289100] global_step=289100, grad_norm=5.484093189239502, loss=0.6279679536819458
I0301 09:17:27.823240 140573303715648 spec.py:321] Evaluating on the training split.
I0301 09:17:33.912508 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:17:42.536527 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:17:44.846292 140573303715648 submission_runner.py:411] Time since start: 100988.84s, 	Step: 289102, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.1500391662120819, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0513322353363037, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.817582368850708, 'test/num_examples': 10000, 'score': 97468.86407732964, 'total_duration': 100988.83882284164, 'accumulated_submission_time': 97468.86407732964, 'accumulated_eval_time': 3497.1005821228027, 'accumulated_logging_time': 11.457651138305664}
I0301 09:17:44.913907 140409837700864 logging_writer.py:48] [289102] accumulated_eval_time=3497.100582, accumulated_logging_time=11.457651, accumulated_submission_time=97468.864077, global_step=289102, preemption_count=0, score=97468.864077, test/accuracy=0.630800, test/loss=1.817582, test/num_examples=10000, total_duration=100988.838823, train/accuracy=0.960300, train/loss=0.150039, validation/accuracy=0.756120, validation/loss=1.051332, validation/num_examples=50000
I0301 09:18:18.297031 140409846093568 logging_writer.py:48] [289200] global_step=289200, grad_norm=4.294222354888916, loss=0.6276808977127075
I0301 09:18:51.950061 140409837700864 logging_writer.py:48] [289300] global_step=289300, grad_norm=4.553036689758301, loss=0.5744671821594238
I0301 09:19:25.632158 140409846093568 logging_writer.py:48] [289400] global_step=289400, grad_norm=4.2410454750061035, loss=0.5879262089729309
I0301 09:19:59.345794 140409837700864 logging_writer.py:48] [289500] global_step=289500, grad_norm=4.603376388549805, loss=0.6216248273849487
I0301 09:20:33.137117 140409846093568 logging_writer.py:48] [289600] global_step=289600, grad_norm=4.696575164794922, loss=0.6924179792404175
I0301 09:21:06.849672 140409837700864 logging_writer.py:48] [289700] global_step=289700, grad_norm=4.36178731918335, loss=0.5734035968780518
I0301 09:21:40.531951 140409846093568 logging_writer.py:48] [289800] global_step=289800, grad_norm=4.370366096496582, loss=0.639079749584198
I0301 09:22:14.262722 140409837700864 logging_writer.py:48] [289900] global_step=289900, grad_norm=4.519296169281006, loss=0.5829043984413147
I0301 09:22:47.941781 140409846093568 logging_writer.py:48] [290000] global_step=290000, grad_norm=4.294797420501709, loss=0.6445352435112
I0301 09:23:21.673386 140409837700864 logging_writer.py:48] [290100] global_step=290100, grad_norm=4.390600204467773, loss=0.6536301374435425
I0301 09:23:55.359822 140409846093568 logging_writer.py:48] [290200] global_step=290200, grad_norm=5.291476726531982, loss=0.6968212723731995
I0301 09:24:29.030567 140409837700864 logging_writer.py:48] [290300] global_step=290300, grad_norm=4.483720302581787, loss=0.6483697891235352
I0301 09:25:02.778613 140409846093568 logging_writer.py:48] [290400] global_step=290400, grad_norm=4.3712568283081055, loss=0.5782188177108765
I0301 09:25:36.505618 140409837700864 logging_writer.py:48] [290500] global_step=290500, grad_norm=4.163514137268066, loss=0.608166515827179
I0301 09:26:10.220747 140409846093568 logging_writer.py:48] [290600] global_step=290600, grad_norm=4.274718284606934, loss=0.5900270938873291
I0301 09:26:15.082063 140573303715648 spec.py:321] Evaluating on the training split.
I0301 09:26:21.219237 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:26:29.818641 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:26:32.127530 140573303715648 submission_runner.py:411] Time since start: 101516.12s, 	Step: 290616, 	{'train/accuracy': 0.9607182741165161, 'train/loss': 0.14630237221717834, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0526319742202759, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8178383111953735, 'test/num_examples': 10000, 'score': 97978.9630844593, 'total_duration': 101516.11999130249, 'accumulated_submission_time': 97978.9630844593, 'accumulated_eval_time': 3514.145917892456, 'accumulated_logging_time': 11.534794092178345}
I0301 09:26:32.193019 140408319375104 logging_writer.py:48] [290616] accumulated_eval_time=3514.145918, accumulated_logging_time=11.534794, accumulated_submission_time=97978.963084, global_step=290616, preemption_count=0, score=97978.963084, test/accuracy=0.631500, test/loss=1.817838, test/num_examples=10000, total_duration=101516.119991, train/accuracy=0.960718, train/loss=0.146302, validation/accuracy=0.755980, validation/loss=1.052632, validation/num_examples=50000
I0301 09:27:00.961561 140409124681472 logging_writer.py:48] [290700] global_step=290700, grad_norm=4.6093878746032715, loss=0.6352959871292114
I0301 09:27:34.615856 140408319375104 logging_writer.py:48] [290800] global_step=290800, grad_norm=3.978282928466797, loss=0.570519745349884
I0301 09:28:08.268674 140409124681472 logging_writer.py:48] [290900] global_step=290900, grad_norm=5.013254642486572, loss=0.6668862104415894
I0301 09:28:41.933883 140408319375104 logging_writer.py:48] [291000] global_step=291000, grad_norm=4.65189790725708, loss=0.6461979150772095
I0301 09:29:15.702557 140409124681472 logging_writer.py:48] [291100] global_step=291100, grad_norm=4.608520984649658, loss=0.5991781949996948
I0301 09:29:49.396047 140408319375104 logging_writer.py:48] [291200] global_step=291200, grad_norm=4.632224082946777, loss=0.6719490885734558
I0301 09:30:23.149868 140409124681472 logging_writer.py:48] [291300] global_step=291300, grad_norm=4.5896100997924805, loss=0.6478530168533325
I0301 09:30:56.822439 140408319375104 logging_writer.py:48] [291400] global_step=291400, grad_norm=5.168609619140625, loss=0.5778533220291138
I0301 09:31:30.512278 140409124681472 logging_writer.py:48] [291500] global_step=291500, grad_norm=4.406562328338623, loss=0.7344679236412048
I0301 09:32:04.236740 140408319375104 logging_writer.py:48] [291600] global_step=291600, grad_norm=4.540752410888672, loss=0.5974035263061523
I0301 09:32:37.961941 140409124681472 logging_writer.py:48] [291700] global_step=291700, grad_norm=4.785947322845459, loss=0.6754509210586548
I0301 09:33:11.715960 140408319375104 logging_writer.py:48] [291800] global_step=291800, grad_norm=5.063935279846191, loss=0.6223247051239014
I0301 09:33:45.433132 140409124681472 logging_writer.py:48] [291900] global_step=291900, grad_norm=4.423947334289551, loss=0.5748991966247559
I0301 09:34:19.152332 140408319375104 logging_writer.py:48] [292000] global_step=292000, grad_norm=4.446665287017822, loss=0.6356468200683594
I0301 09:34:52.884306 140409124681472 logging_writer.py:48] [292100] global_step=292100, grad_norm=4.254428386688232, loss=0.5433162450790405
I0301 09:35:02.455381 140573303715648 spec.py:321] Evaluating on the training split.
I0301 09:35:08.596583 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:35:17.325056 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:35:19.624735 140573303715648 submission_runner.py:411] Time since start: 102043.62s, 	Step: 292130, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.1470670998096466, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.0529109239578247, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8188873529434204, 'test/num_examples': 10000, 'score': 98489.15401434898, 'total_duration': 102043.6172683239, 'accumulated_submission_time': 98489.15401434898, 'accumulated_eval_time': 3531.3152136802673, 'accumulated_logging_time': 11.611164569854736}
I0301 09:35:19.693189 140407379851008 logging_writer.py:48] [292130] accumulated_eval_time=3531.315214, accumulated_logging_time=11.611165, accumulated_submission_time=98489.154014, global_step=292130, preemption_count=0, score=98489.154014, test/accuracy=0.631000, test/loss=1.818887, test/num_examples=10000, total_duration=102043.617268, train/accuracy=0.960619, train/loss=0.147067, validation/accuracy=0.755380, validation/loss=1.052911, validation/num_examples=50000
I0301 09:35:43.599996 140409846093568 logging_writer.py:48] [292200] global_step=292200, grad_norm=4.6870036125183105, loss=0.6739123463630676
I0301 09:36:17.256902 140407379851008 logging_writer.py:48] [292300] global_step=292300, grad_norm=4.2049150466918945, loss=0.5954583883285522
I0301 09:36:51.000076 140409846093568 logging_writer.py:48] [292400] global_step=292400, grad_norm=4.742337703704834, loss=0.6332361698150635
I0301 09:37:24.716184 140407379851008 logging_writer.py:48] [292500] global_step=292500, grad_norm=4.341732025146484, loss=0.5891173481941223
I0301 09:37:58.451462 140409846093568 logging_writer.py:48] [292600] global_step=292600, grad_norm=4.7841620445251465, loss=0.6119482517242432
I0301 09:38:32.156753 140407379851008 logging_writer.py:48] [292700] global_step=292700, grad_norm=4.208704948425293, loss=0.6320030093193054
I0301 09:39:06.066706 140409846093568 logging_writer.py:48] [292800] global_step=292800, grad_norm=4.489476680755615, loss=0.6667662858963013
I0301 09:39:39.768665 140407379851008 logging_writer.py:48] [292900] global_step=292900, grad_norm=4.747235298156738, loss=0.6796477437019348
I0301 09:40:13.518815 140409846093568 logging_writer.py:48] [293000] global_step=293000, grad_norm=4.828376770019531, loss=0.678234338760376
I0301 09:40:47.218697 140407379851008 logging_writer.py:48] [293100] global_step=293100, grad_norm=4.562286853790283, loss=0.6749710440635681
I0301 09:41:20.928609 140409846093568 logging_writer.py:48] [293200] global_step=293200, grad_norm=4.480071067810059, loss=0.619195282459259
I0301 09:41:54.645604 140407379851008 logging_writer.py:48] [293300] global_step=293300, grad_norm=4.143637657165527, loss=0.6023885011672974
I0301 09:42:28.305126 140409846093568 logging_writer.py:48] [293400] global_step=293400, grad_norm=4.414614677429199, loss=0.5779557228088379
I0301 09:43:02.040074 140407379851008 logging_writer.py:48] [293500] global_step=293500, grad_norm=4.408539772033691, loss=0.5885792374610901
I0301 09:43:35.718229 140409846093568 logging_writer.py:48] [293600] global_step=293600, grad_norm=5.058602333068848, loss=0.6988670229911804
I0301 09:43:49.689703 140573303715648 spec.py:321] Evaluating on the training split.
I0301 09:43:55.741792 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:44:04.537977 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:44:06.829671 140573303715648 submission_runner.py:411] Time since start: 102570.82s, 	Step: 293643, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.1502828449010849, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0518311262130737, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.81728196144104, 'test/num_examples': 10000, 'score': 98999.07892155647, 'total_duration': 102570.82220602036, 'accumulated_submission_time': 98999.07892155647, 'accumulated_eval_time': 3548.4551446437836, 'accumulated_logging_time': 11.689434289932251}
I0301 09:44:06.895753 140408319375104 logging_writer.py:48] [293643] accumulated_eval_time=3548.455145, accumulated_logging_time=11.689434, accumulated_submission_time=98999.078922, global_step=293643, preemption_count=0, score=98999.078922, test/accuracy=0.630700, test/loss=1.817282, test/num_examples=10000, total_duration=102570.822206, train/accuracy=0.960738, train/loss=0.150283, validation/accuracy=0.755620, validation/loss=1.051831, validation/num_examples=50000
I0301 09:44:26.421991 140409124681472 logging_writer.py:48] [293700] global_step=293700, grad_norm=4.528419017791748, loss=0.6049270629882812
I0301 09:45:00.208940 140408319375104 logging_writer.py:48] [293800] global_step=293800, grad_norm=4.1957688331604, loss=0.5438260436058044
I0301 09:45:33.892133 140409124681472 logging_writer.py:48] [293900] global_step=293900, grad_norm=4.828251838684082, loss=0.6644051671028137
I0301 09:46:07.552671 140408319375104 logging_writer.py:48] [294000] global_step=294000, grad_norm=4.55169153213501, loss=0.6243311762809753
I0301 09:46:41.278656 140409124681472 logging_writer.py:48] [294100] global_step=294100, grad_norm=4.346363544464111, loss=0.582476019859314
I0301 09:47:14.963725 140408319375104 logging_writer.py:48] [294200] global_step=294200, grad_norm=4.289494037628174, loss=0.6364978551864624
I0301 09:47:48.692534 140409124681472 logging_writer.py:48] [294300] global_step=294300, grad_norm=4.153334140777588, loss=0.6378417015075684
I0301 09:48:22.366145 140408319375104 logging_writer.py:48] [294400] global_step=294400, grad_norm=4.234820365905762, loss=0.5701740980148315
I0301 09:48:56.086381 140409124681472 logging_writer.py:48] [294500] global_step=294500, grad_norm=5.020290851593018, loss=0.6774998307228088
I0301 09:49:29.765583 140408319375104 logging_writer.py:48] [294600] global_step=294600, grad_norm=4.277163505554199, loss=0.5783313512802124
I0301 09:50:03.493393 140409124681472 logging_writer.py:48] [294700] global_step=294700, grad_norm=4.595702648162842, loss=0.6338392496109009
I0301 09:50:37.178116 140408319375104 logging_writer.py:48] [294800] global_step=294800, grad_norm=4.609371662139893, loss=0.6133177280426025
I0301 09:51:10.934723 140409124681472 logging_writer.py:48] [294900] global_step=294900, grad_norm=4.7498250007629395, loss=0.5873504877090454
I0301 09:51:44.654166 140408319375104 logging_writer.py:48] [295000] global_step=295000, grad_norm=4.2360453605651855, loss=0.5623530149459839
I0301 09:52:18.313338 140409124681472 logging_writer.py:48] [295100] global_step=295100, grad_norm=4.556743144989014, loss=0.6338839530944824
I0301 09:52:36.968425 140573303715648 spec.py:321] Evaluating on the training split.
I0301 09:52:43.096461 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 09:52:51.656532 140573303715648 spec.py:349] Evaluating on the test split.
I0301 09:52:53.932711 140573303715648 submission_runner.py:411] Time since start: 103097.93s, 	Step: 295157, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14673267304897308, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0521495342254639, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.817894458770752, 'test/num_examples': 10000, 'score': 99509.08157086372, 'total_duration': 103097.92510271072, 'accumulated_submission_time': 99509.08157086372, 'accumulated_eval_time': 3565.4192507267, 'accumulated_logging_time': 11.765220880508423}
I0301 09:52:53.999358 140407379851008 logging_writer.py:48] [295157] accumulated_eval_time=3565.419251, accumulated_logging_time=11.765221, accumulated_submission_time=99509.081571, global_step=295157, preemption_count=0, score=99509.081571, test/accuracy=0.630700, test/loss=1.817894, test/num_examples=10000, total_duration=103097.925103, train/accuracy=0.960459, train/loss=0.146733, validation/accuracy=0.755560, validation/loss=1.052150, validation/num_examples=50000
I0301 09:53:08.845338 140408319375104 logging_writer.py:48] [295200] global_step=295200, grad_norm=4.294862270355225, loss=0.6662048101425171
I0301 09:53:42.499731 140407379851008 logging_writer.py:48] [295300] global_step=295300, grad_norm=4.193994522094727, loss=0.6301935315132141
I0301 09:54:16.171766 140408319375104 logging_writer.py:48] [295400] global_step=295400, grad_norm=4.967957496643066, loss=0.6365915536880493
I0301 09:54:49.918196 140407379851008 logging_writer.py:48] [295500] global_step=295500, grad_norm=4.417779445648193, loss=0.6389822959899902
I0301 09:55:23.628967 140408319375104 logging_writer.py:48] [295600] global_step=295600, grad_norm=4.462453842163086, loss=0.6551267504692078
I0301 09:55:57.366058 140407379851008 logging_writer.py:48] [295700] global_step=295700, grad_norm=5.3507080078125, loss=0.6514796018600464
I0301 09:56:31.061155 140408319375104 logging_writer.py:48] [295800] global_step=295800, grad_norm=4.257880687713623, loss=0.6844565272331238
I0301 09:57:04.773723 140407379851008 logging_writer.py:48] [295900] global_step=295900, grad_norm=4.6040143966674805, loss=0.6661440134048462
I0301 09:57:38.530344 140408319375104 logging_writer.py:48] [296000] global_step=296000, grad_norm=4.3279032707214355, loss=0.6797851324081421
I0301 09:58:12.284451 140407379851008 logging_writer.py:48] [296100] global_step=296100, grad_norm=4.220955848693848, loss=0.6112452745437622
I0301 09:58:45.965057 140408319375104 logging_writer.py:48] [296200] global_step=296200, grad_norm=4.197399139404297, loss=0.6454787254333496
I0301 09:59:19.700318 140407379851008 logging_writer.py:48] [296300] global_step=296300, grad_norm=4.229036808013916, loss=0.5636695027351379
I0301 09:59:53.385542 140408319375104 logging_writer.py:48] [296400] global_step=296400, grad_norm=4.577200889587402, loss=0.6397794485092163
I0301 10:00:27.141458 140407379851008 logging_writer.py:48] [296500] global_step=296500, grad_norm=4.281932353973389, loss=0.6177661418914795
I0301 10:01:00.825341 140408319375104 logging_writer.py:48] [296600] global_step=296600, grad_norm=5.124946117401123, loss=0.6378597617149353
I0301 10:01:24.212443 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:01:30.429684 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:01:39.268261 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:01:41.526497 140573303715648 submission_runner.py:411] Time since start: 103625.52s, 	Step: 296671, 	{'train/accuracy': 0.9592434167861938, 'train/loss': 0.14997491240501404, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.052897572517395, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.817719578742981, 'test/num_examples': 10000, 'score': 100019.2240486145, 'total_duration': 103625.51902842522, 'accumulated_submission_time': 100019.2240486145, 'accumulated_eval_time': 3582.7332623004913, 'accumulated_logging_time': 11.842433452606201}
I0301 10:01:41.598610 140409837700864 logging_writer.py:48] [296671] accumulated_eval_time=3582.733262, accumulated_logging_time=11.842433, accumulated_submission_time=100019.224049, global_step=296671, preemption_count=0, score=100019.224049, test/accuracy=0.630700, test/loss=1.817720, test/num_examples=10000, total_duration=103625.519028, train/accuracy=0.959243, train/loss=0.149975, validation/accuracy=0.755780, validation/loss=1.052898, validation/num_examples=50000
I0301 10:01:51.741489 140409854486272 logging_writer.py:48] [296700] global_step=296700, grad_norm=4.446977615356445, loss=0.5647266507148743
I0301 10:02:25.403514 140409837700864 logging_writer.py:48] [296800] global_step=296800, grad_norm=4.167867183685303, loss=0.5911672711372375
I0301 10:02:59.117310 140409854486272 logging_writer.py:48] [296900] global_step=296900, grad_norm=4.634879112243652, loss=0.6254159808158875
I0301 10:03:32.909712 140409837700864 logging_writer.py:48] [297000] global_step=297000, grad_norm=4.126211166381836, loss=0.5378486514091492
I0301 10:04:06.647600 140409854486272 logging_writer.py:48] [297100] global_step=297100, grad_norm=4.303449630737305, loss=0.568577766418457
I0301 10:04:40.375833 140409837700864 logging_writer.py:48] [297200] global_step=297200, grad_norm=4.814384937286377, loss=0.7118640542030334
I0301 10:05:14.069978 140409854486272 logging_writer.py:48] [297300] global_step=297300, grad_norm=4.261887550354004, loss=0.6556970477104187
I0301 10:05:47.801926 140409837700864 logging_writer.py:48] [297400] global_step=297400, grad_norm=4.274509429931641, loss=0.6091280579566956
I0301 10:06:21.489356 140409854486272 logging_writer.py:48] [297500] global_step=297500, grad_norm=4.148582935333252, loss=0.6411181688308716
I0301 10:06:55.175979 140409837700864 logging_writer.py:48] [297600] global_step=297600, grad_norm=4.871825695037842, loss=0.6966550350189209
I0301 10:07:28.934315 140409854486272 logging_writer.py:48] [297700] global_step=297700, grad_norm=4.59006929397583, loss=0.6625325083732605
I0301 10:08:02.634335 140409837700864 logging_writer.py:48] [297800] global_step=297800, grad_norm=4.616565704345703, loss=0.6243411898612976
I0301 10:08:36.393885 140409854486272 logging_writer.py:48] [297900] global_step=297900, grad_norm=4.5334272384643555, loss=0.7013593316078186
I0301 10:09:10.060642 140409837700864 logging_writer.py:48] [298000] global_step=298000, grad_norm=4.32847261428833, loss=0.5857985019683838
I0301 10:09:43.829772 140409854486272 logging_writer.py:48] [298100] global_step=298100, grad_norm=5.287766933441162, loss=0.7149425745010376
I0301 10:10:11.544645 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:10:17.610806 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:10:26.373353 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:10:28.758822 140573303715648 submission_runner.py:411] Time since start: 104152.75s, 	Step: 298184, 	{'train/accuracy': 0.9615553021430969, 'train/loss': 0.14568035304546356, 'validation/accuracy': 0.7562999725341797, 'validation/loss': 1.0515629053115845, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.816598892211914, 'test/num_examples': 10000, 'score': 100529.0994002819, 'total_duration': 104152.75135087967, 'accumulated_submission_time': 100529.0994002819, 'accumulated_eval_time': 3599.947383403778, 'accumulated_logging_time': 11.925029277801514}
I0301 10:10:28.825257 140408319375104 logging_writer.py:48] [298184] accumulated_eval_time=3599.947383, accumulated_logging_time=11.925029, accumulated_submission_time=100529.099400, global_step=298184, preemption_count=0, score=100529.099400, test/accuracy=0.631700, test/loss=1.816599, test/num_examples=10000, total_duration=104152.751351, train/accuracy=0.961555, train/loss=0.145680, validation/accuracy=0.756300, validation/loss=1.051563, validation/num_examples=50000
I0301 10:10:34.548338 140409124681472 logging_writer.py:48] [298200] global_step=298200, grad_norm=4.16667366027832, loss=0.5966411828994751
I0301 10:11:08.184938 140408319375104 logging_writer.py:48] [298300] global_step=298300, grad_norm=4.455921173095703, loss=0.6323898434638977
I0301 10:11:41.864973 140409124681472 logging_writer.py:48] [298400] global_step=298400, grad_norm=3.9834611415863037, loss=0.49863240122795105
I0301 10:12:15.525095 140408319375104 logging_writer.py:48] [298500] global_step=298500, grad_norm=4.069755554199219, loss=0.5624083280563354
I0301 10:12:49.190711 140409124681472 logging_writer.py:48] [298600] global_step=298600, grad_norm=4.205379486083984, loss=0.6237296462059021
I0301 10:13:22.908703 140408319375104 logging_writer.py:48] [298700] global_step=298700, grad_norm=4.493321418762207, loss=0.6157187223434448
I0301 10:13:56.553622 140409124681472 logging_writer.py:48] [298800] global_step=298800, grad_norm=4.073390007019043, loss=0.5183230638504028
I0301 10:14:30.302162 140408319375104 logging_writer.py:48] [298900] global_step=298900, grad_norm=4.4627156257629395, loss=0.5807616114616394
I0301 10:15:04.001929 140409124681472 logging_writer.py:48] [299000] global_step=299000, grad_norm=4.96148681640625, loss=0.6357963681221008
I0301 10:15:37.815049 140408319375104 logging_writer.py:48] [299100] global_step=299100, grad_norm=4.620666027069092, loss=0.6712439060211182
I0301 10:16:11.544610 140409124681472 logging_writer.py:48] [299200] global_step=299200, grad_norm=5.019362926483154, loss=0.6267390847206116
I0301 10:16:45.260686 140408319375104 logging_writer.py:48] [299300] global_step=299300, grad_norm=4.605121612548828, loss=0.6132400631904602
I0301 10:17:18.983017 140409124681472 logging_writer.py:48] [299400] global_step=299400, grad_norm=4.911959171295166, loss=0.6539602279663086
I0301 10:17:52.639743 140408319375104 logging_writer.py:48] [299500] global_step=299500, grad_norm=4.190372943878174, loss=0.632899284362793
I0301 10:18:26.412842 140409124681472 logging_writer.py:48] [299600] global_step=299600, grad_norm=4.544782638549805, loss=0.7232301235198975
I0301 10:18:58.866587 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:19:05.074476 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:19:13.994286 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:19:16.273839 140573303715648 submission_runner.py:411] Time since start: 104680.27s, 	Step: 299698, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14568652212619781, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0521283149719238, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8183579444885254, 'test/num_examples': 10000, 'score': 101039.07216191292, 'total_duration': 104680.2663693428, 'accumulated_submission_time': 101039.07216191292, 'accumulated_eval_time': 3617.3545808792114, 'accumulated_logging_time': 12.00115966796875}
I0301 10:19:16.340829 140409846093568 logging_writer.py:48] [299698] accumulated_eval_time=3617.354581, accumulated_logging_time=12.001160, accumulated_submission_time=101039.072162, global_step=299698, preemption_count=0, score=101039.072162, test/accuracy=0.631500, test/loss=1.818358, test/num_examples=10000, total_duration=104680.266369, train/accuracy=0.961356, train/loss=0.145687, validation/accuracy=0.755740, validation/loss=1.052128, validation/num_examples=50000
I0301 10:19:17.368074 140409854486272 logging_writer.py:48] [299700] global_step=299700, grad_norm=4.583619117736816, loss=0.6661192774772644
I0301 10:19:51.028697 140409846093568 logging_writer.py:48] [299800] global_step=299800, grad_norm=4.242490768432617, loss=0.548810601234436
I0301 10:20:24.753822 140409854486272 logging_writer.py:48] [299900] global_step=299900, grad_norm=4.222062110900879, loss=0.5423269867897034
I0301 10:20:58.438790 140409846093568 logging_writer.py:48] [300000] global_step=300000, grad_norm=5.300466537475586, loss=0.6364748477935791
I0301 10:21:32.173225 140409854486272 logging_writer.py:48] [300100] global_step=300100, grad_norm=4.352529048919678, loss=0.6840137243270874
I0301 10:22:06.035881 140409846093568 logging_writer.py:48] [300200] global_step=300200, grad_norm=4.260459899902344, loss=0.6161140203475952
I0301 10:22:39.766541 140409854486272 logging_writer.py:48] [300300] global_step=300300, grad_norm=4.293936729431152, loss=0.6125494837760925
I0301 10:23:13.459774 140409846093568 logging_writer.py:48] [300400] global_step=300400, grad_norm=4.342800140380859, loss=0.5834199786186218
I0301 10:23:47.186421 140409854486272 logging_writer.py:48] [300500] global_step=300500, grad_norm=4.414355754852295, loss=0.646709144115448
I0301 10:24:20.875442 140409846093568 logging_writer.py:48] [300600] global_step=300600, grad_norm=4.411999702453613, loss=0.5953215956687927
I0301 10:24:54.600520 140409854486272 logging_writer.py:48] [300700] global_step=300700, grad_norm=4.315948486328125, loss=0.6227847337722778
I0301 10:25:28.325472 140409846093568 logging_writer.py:48] [300800] global_step=300800, grad_norm=4.555266857147217, loss=0.6949471235275269
I0301 10:26:02.048804 140409854486272 logging_writer.py:48] [300900] global_step=300900, grad_norm=4.173025608062744, loss=0.6500132083892822
I0301 10:26:35.749656 140409846093568 logging_writer.py:48] [301000] global_step=301000, grad_norm=4.5730671882629395, loss=0.5959004163742065
I0301 10:27:09.492665 140409854486272 logging_writer.py:48] [301100] global_step=301100, grad_norm=4.252856254577637, loss=0.5915420055389404
I0301 10:27:43.171373 140409846093568 logging_writer.py:48] [301200] global_step=301200, grad_norm=4.929337501525879, loss=0.631205677986145
I0301 10:27:46.345707 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:27:52.747093 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:28:01.549095 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:28:04.003229 140573303715648 submission_runner.py:411] Time since start: 105208.00s, 	Step: 301211, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.15000775456428528, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0508867502212524, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.81587815284729, 'test/num_examples': 10000, 'score': 101549.00704622269, 'total_duration': 105207.99576115608, 'accumulated_submission_time': 101549.00704622269, 'accumulated_eval_time': 3635.0120573043823, 'accumulated_logging_time': 12.07826280593872}
I0301 10:28:04.071216 140408319375104 logging_writer.py:48] [301211] accumulated_eval_time=3635.012057, accumulated_logging_time=12.078263, accumulated_submission_time=101549.007046, global_step=301211, preemption_count=0, score=101549.007046, test/accuracy=0.631800, test/loss=1.815878, test/num_examples=10000, total_duration=105207.995761, train/accuracy=0.960599, train/loss=0.150008, validation/accuracy=0.755660, validation/loss=1.050887, validation/num_examples=50000
I0301 10:28:34.456641 140409124681472 logging_writer.py:48] [301300] global_step=301300, grad_norm=4.460690021514893, loss=0.6237950325012207
I0301 10:29:08.136426 140408319375104 logging_writer.py:48] [301400] global_step=301400, grad_norm=4.93217134475708, loss=0.6445368528366089
I0301 10:29:41.834039 140409124681472 logging_writer.py:48] [301500] global_step=301500, grad_norm=4.746837139129639, loss=0.7007831931114197
I0301 10:30:15.567713 140408319375104 logging_writer.py:48] [301600] global_step=301600, grad_norm=4.55479097366333, loss=0.5741920471191406
I0301 10:30:49.333526 140409124681472 logging_writer.py:48] [301700] global_step=301700, grad_norm=4.4070820808410645, loss=0.5549280643463135
I0301 10:31:23.007057 140408319375104 logging_writer.py:48] [301800] global_step=301800, grad_norm=4.162526607513428, loss=0.6039329767227173
I0301 10:31:56.791229 140409124681472 logging_writer.py:48] [301900] global_step=301900, grad_norm=4.537947654724121, loss=0.6700654029846191
I0301 10:32:30.468257 140408319375104 logging_writer.py:48] [302000] global_step=302000, grad_norm=4.605446815490723, loss=0.6229751706123352
I0301 10:33:04.149937 140409124681472 logging_writer.py:48] [302100] global_step=302100, grad_norm=4.23399543762207, loss=0.5699052214622498
I0301 10:33:37.865363 140408319375104 logging_writer.py:48] [302200] global_step=302200, grad_norm=4.313960075378418, loss=0.5772625207901001
I0301 10:34:11.656759 140409124681472 logging_writer.py:48] [302300] global_step=302300, grad_norm=4.112637996673584, loss=0.583167552947998
I0301 10:34:45.333824 140408319375104 logging_writer.py:48] [302400] global_step=302400, grad_norm=4.383938312530518, loss=0.5743818283081055
I0301 10:35:19.102287 140409124681472 logging_writer.py:48] [302500] global_step=302500, grad_norm=4.954595565795898, loss=0.676283597946167
I0301 10:35:52.792001 140408319375104 logging_writer.py:48] [302600] global_step=302600, grad_norm=4.360376834869385, loss=0.6010076999664307
I0301 10:36:26.552369 140409124681472 logging_writer.py:48] [302700] global_step=302700, grad_norm=4.047338485717773, loss=0.5749762058258057
I0301 10:36:34.112134 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:36:40.173931 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:36:48.986896 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:36:51.233553 140573303715648 submission_runner.py:411] Time since start: 105735.23s, 	Step: 302724, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14836092293262482, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0511231422424316, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8171393871307373, 'test/num_examples': 10000, 'score': 102058.9766690731, 'total_duration': 105735.22607922554, 'accumulated_submission_time': 102058.9766690731, 'accumulated_eval_time': 3652.1334154605865, 'accumulated_logging_time': 12.158013582229614}
I0301 10:36:51.298924 140409846093568 logging_writer.py:48] [302724] accumulated_eval_time=3652.133415, accumulated_logging_time=12.158014, accumulated_submission_time=102058.976669, global_step=302724, preemption_count=0, score=102058.976669, test/accuracy=0.631300, test/loss=1.817139, test/num_examples=10000, total_duration=105735.226079, train/accuracy=0.960798, train/loss=0.148361, validation/accuracy=0.755400, validation/loss=1.051123, validation/num_examples=50000
I0301 10:37:17.268102 140409854486272 logging_writer.py:48] [302800] global_step=302800, grad_norm=4.582944393157959, loss=0.5632967948913574
I0301 10:37:50.986770 140409846093568 logging_writer.py:48] [302900] global_step=302900, grad_norm=4.293055534362793, loss=0.621026337146759
I0301 10:38:24.716465 140409854486272 logging_writer.py:48] [303000] global_step=303000, grad_norm=4.017846584320068, loss=0.6065843105316162
I0301 10:38:58.421795 140409846093568 logging_writer.py:48] [303100] global_step=303100, grad_norm=4.433018207550049, loss=0.5600756406784058
I0301 10:39:32.161728 140409854486272 logging_writer.py:48] [303200] global_step=303200, grad_norm=4.796546459197998, loss=0.6562652587890625
I0301 10:40:05.879831 140409846093568 logging_writer.py:48] [303300] global_step=303300, grad_norm=4.26281213760376, loss=0.6064810752868652
I0301 10:40:39.793980 140409854486272 logging_writer.py:48] [303400] global_step=303400, grad_norm=4.423054218292236, loss=0.6933537125587463
I0301 10:41:13.515656 140409846093568 logging_writer.py:48] [303500] global_step=303500, grad_norm=4.3195929527282715, loss=0.5884265899658203
I0301 10:41:47.210965 140409854486272 logging_writer.py:48] [303600] global_step=303600, grad_norm=4.126790523529053, loss=0.5590170621871948
I0301 10:42:20.929544 140409846093568 logging_writer.py:48] [303700] global_step=303700, grad_norm=4.835318565368652, loss=0.6170177459716797
I0301 10:42:54.673917 140409854486272 logging_writer.py:48] [303800] global_step=303800, grad_norm=4.567075252532959, loss=0.6227820515632629
I0301 10:43:28.403129 140409846093568 logging_writer.py:48] [303900] global_step=303900, grad_norm=4.675076007843018, loss=0.5889431238174438
I0301 10:44:02.138443 140409854486272 logging_writer.py:48] [304000] global_step=304000, grad_norm=4.186972618103027, loss=0.5825080871582031
I0301 10:44:35.855167 140409846093568 logging_writer.py:48] [304100] global_step=304100, grad_norm=4.588363170623779, loss=0.6365528702735901
I0301 10:45:09.584674 140409854486272 logging_writer.py:48] [304200] global_step=304200, grad_norm=4.588233470916748, loss=0.5989614129066467
I0301 10:45:21.535869 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:45:27.580465 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:45:36.214085 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:45:38.521897 140573303715648 submission_runner.py:411] Time since start: 106262.51s, 	Step: 304237, 	{'train/accuracy': 0.9625318646430969, 'train/loss': 0.14060242474079132, 'validation/accuracy': 0.7562400102615356, 'validation/loss': 1.0528939962387085, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.817326307296753, 'test/num_examples': 10000, 'score': 102569.14319133759, 'total_duration': 106262.51443743706, 'accumulated_submission_time': 102569.14319133759, 'accumulated_eval_time': 3669.119397878647, 'accumulated_logging_time': 12.233417749404907}
I0301 10:45:38.590334 140407379851008 logging_writer.py:48] [304237] accumulated_eval_time=3669.119398, accumulated_logging_time=12.233418, accumulated_submission_time=102569.143191, global_step=304237, preemption_count=0, score=102569.143191, test/accuracy=0.630700, test/loss=1.817326, test/num_examples=10000, total_duration=106262.514437, train/accuracy=0.962532, train/loss=0.140602, validation/accuracy=0.756240, validation/loss=1.052894, validation/num_examples=50000
I0301 10:46:00.161832 140408319375104 logging_writer.py:48] [304300] global_step=304300, grad_norm=4.15522575378418, loss=0.6260383129119873
I0301 10:46:33.954877 140407379851008 logging_writer.py:48] [304400] global_step=304400, grad_norm=4.330509662628174, loss=0.5923027992248535
I0301 10:47:07.662791 140408319375104 logging_writer.py:48] [304500] global_step=304500, grad_norm=4.431814670562744, loss=0.6298851370811462
I0301 10:47:41.322054 140407379851008 logging_writer.py:48] [304600] global_step=304600, grad_norm=4.762619495391846, loss=0.6545469760894775
I0301 10:48:15.024841 140408319375104 logging_writer.py:48] [304700] global_step=304700, grad_norm=4.4512858390808105, loss=0.6091508269309998
I0301 10:48:48.750006 140407379851008 logging_writer.py:48] [304800] global_step=304800, grad_norm=4.375970363616943, loss=0.5765044689178467
I0301 10:49:22.418171 140408319375104 logging_writer.py:48] [304900] global_step=304900, grad_norm=4.223848819732666, loss=0.6105811595916748
I0301 10:49:56.133044 140407379851008 logging_writer.py:48] [305000] global_step=305000, grad_norm=4.557668209075928, loss=0.6378100514411926
I0301 10:50:29.820773 140408319375104 logging_writer.py:48] [305100] global_step=305100, grad_norm=4.570991039276123, loss=0.6501453518867493
I0301 10:51:03.491022 140407379851008 logging_writer.py:48] [305200] global_step=305200, grad_norm=4.318885803222656, loss=0.6400623321533203
I0301 10:51:37.174522 140408319375104 logging_writer.py:48] [305300] global_step=305300, grad_norm=4.573567867279053, loss=0.5859474539756775
I0301 10:52:10.929300 140407379851008 logging_writer.py:48] [305400] global_step=305400, grad_norm=4.108893394470215, loss=0.624944806098938
I0301 10:52:44.832065 140408319375104 logging_writer.py:48] [305500] global_step=305500, grad_norm=4.149323463439941, loss=0.6230289340019226
I0301 10:53:18.549733 140407379851008 logging_writer.py:48] [305600] global_step=305600, grad_norm=4.705124378204346, loss=0.6135673522949219
I0301 10:53:52.247279 140408319375104 logging_writer.py:48] [305700] global_step=305700, grad_norm=4.3160295486450195, loss=0.6622624397277832
I0301 10:54:08.591514 140573303715648 spec.py:321] Evaluating on the training split.
I0301 10:54:14.716691 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 10:54:23.464419 140573303715648 spec.py:349] Evaluating on the test split.
I0301 10:54:25.764731 140573303715648 submission_runner.py:411] Time since start: 106789.76s, 	Step: 305750, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14791885018348694, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0525130033493042, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8191286325454712, 'test/num_examples': 10000, 'score': 103079.07321691513, 'total_duration': 106789.75725913048, 'accumulated_submission_time': 103079.07321691513, 'accumulated_eval_time': 3686.2925584316254, 'accumulated_logging_time': 12.312384128570557}
I0301 10:54:25.832082 140409846093568 logging_writer.py:48] [305750] accumulated_eval_time=3686.292558, accumulated_logging_time=12.312384, accumulated_submission_time=103079.073217, global_step=305750, preemption_count=0, score=103079.073217, test/accuracy=0.630700, test/loss=1.819129, test/num_examples=10000, total_duration=106789.757259, train/accuracy=0.960340, train/loss=0.147919, validation/accuracy=0.755700, validation/loss=1.052513, validation/num_examples=50000
I0301 10:54:42.963315 140409854486272 logging_writer.py:48] [305800] global_step=305800, grad_norm=4.338796615600586, loss=0.5664243102073669
I0301 10:55:16.615773 140409846093568 logging_writer.py:48] [305900] global_step=305900, grad_norm=4.527281284332275, loss=0.6319863796234131
I0301 10:55:50.340845 140409854486272 logging_writer.py:48] [306000] global_step=306000, grad_norm=4.5141921043396, loss=0.5568419694900513
I0301 10:56:23.973818 140409846093568 logging_writer.py:48] [306100] global_step=306100, grad_norm=4.406784534454346, loss=0.6671968698501587
I0301 10:56:57.615905 140409854486272 logging_writer.py:48] [306200] global_step=306200, grad_norm=4.593914985656738, loss=0.5276980996131897
I0301 10:57:31.376668 140409846093568 logging_writer.py:48] [306300] global_step=306300, grad_norm=4.450911998748779, loss=0.6092166304588318
I0301 10:58:05.073443 140409854486272 logging_writer.py:48] [306400] global_step=306400, grad_norm=4.554907321929932, loss=0.6621711850166321
I0301 10:58:38.957986 140409846093568 logging_writer.py:48] [306500] global_step=306500, grad_norm=4.699504375457764, loss=0.638374924659729
I0301 10:59:12.675867 140409854486272 logging_writer.py:48] [306600] global_step=306600, grad_norm=4.239096641540527, loss=0.5603076219558716
I0301 10:59:46.442589 140409846093568 logging_writer.py:48] [306700] global_step=306700, grad_norm=5.103028297424316, loss=0.7189934253692627
I0301 11:00:20.143190 140409854486272 logging_writer.py:48] [306800] global_step=306800, grad_norm=4.322729587554932, loss=0.6672556400299072
I0301 11:00:53.873003 140409846093568 logging_writer.py:48] [306900] global_step=306900, grad_norm=4.47312068939209, loss=0.6400338411331177
I0301 11:01:27.579440 140409854486272 logging_writer.py:48] [307000] global_step=307000, grad_norm=4.520141124725342, loss=0.6201782822608948
I0301 11:02:01.312206 140409846093568 logging_writer.py:48] [307100] global_step=307100, grad_norm=4.184089660644531, loss=0.5851419568061829
I0301 11:02:35.019745 140409854486272 logging_writer.py:48] [307200] global_step=307200, grad_norm=4.565040111541748, loss=0.6094185709953308
I0301 11:02:56.061558 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:03:03.094288 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:03:11.943173 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:03:14.256640 140573303715648 submission_runner.py:411] Time since start: 107318.25s, 	Step: 307264, 	{'train/accuracy': 0.9594826102256775, 'train/loss': 0.14922106266021729, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0515639781951904, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8189918994903564, 'test/num_examples': 10000, 'score': 103589.23261809349, 'total_duration': 107318.24916386604, 'accumulated_submission_time': 103589.23261809349, 'accumulated_eval_time': 3704.4875917434692, 'accumulated_logging_time': 12.390041828155518}
I0301 11:03:14.323931 140407379851008 logging_writer.py:48] [307264] accumulated_eval_time=3704.487592, accumulated_logging_time=12.390042, accumulated_submission_time=103589.232618, global_step=307264, preemption_count=0, score=103589.232618, test/accuracy=0.630900, test/loss=1.818992, test/num_examples=10000, total_duration=107318.249164, train/accuracy=0.959483, train/loss=0.149221, validation/accuracy=0.755760, validation/loss=1.051564, validation/num_examples=50000
I0301 11:03:26.776960 140408319375104 logging_writer.py:48] [307300] global_step=307300, grad_norm=4.601661205291748, loss=0.6057511568069458
I0301 11:04:00.481430 140407379851008 logging_writer.py:48] [307400] global_step=307400, grad_norm=4.142817974090576, loss=0.5944714546203613
I0301 11:04:34.168756 140408319375104 logging_writer.py:48] [307500] global_step=307500, grad_norm=4.206263542175293, loss=0.582818329334259
I0301 11:05:07.912530 140407379851008 logging_writer.py:48] [307600] global_step=307600, grad_norm=4.512593746185303, loss=0.6457930207252502
I0301 11:05:41.678313 140408319375104 logging_writer.py:48] [307700] global_step=307700, grad_norm=4.7312140464782715, loss=0.6396636366844177
I0301 11:06:15.395036 140407379851008 logging_writer.py:48] [307800] global_step=307800, grad_norm=4.518626689910889, loss=0.6923852562904358
I0301 11:06:49.111150 140408319375104 logging_writer.py:48] [307900] global_step=307900, grad_norm=4.604222774505615, loss=0.6184611320495605
I0301 11:07:22.807707 140407379851008 logging_writer.py:48] [308000] global_step=308000, grad_norm=4.276139736175537, loss=0.598047137260437
I0301 11:07:56.559062 140408319375104 logging_writer.py:48] [308100] global_step=308100, grad_norm=4.532309055328369, loss=0.6765108108520508
I0301 11:08:30.277818 140407379851008 logging_writer.py:48] [308200] global_step=308200, grad_norm=4.518794536590576, loss=0.6211296319961548
I0301 11:09:03.990030 140408319375104 logging_writer.py:48] [308300] global_step=308300, grad_norm=4.317414283752441, loss=0.6224960684776306
I0301 11:09:37.645174 140407379851008 logging_writer.py:48] [308400] global_step=308400, grad_norm=4.55128812789917, loss=0.64800626039505
I0301 11:10:11.325555 140408319375104 logging_writer.py:48] [308500] global_step=308500, grad_norm=4.420083522796631, loss=0.5004749298095703
I0301 11:10:45.125051 140407379851008 logging_writer.py:48] [308600] global_step=308600, grad_norm=5.01870059967041, loss=0.6901479363441467
I0301 11:11:18.879620 140408319375104 logging_writer.py:48] [308700] global_step=308700, grad_norm=4.075599193572998, loss=0.6273253560066223
I0301 11:11:44.278489 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:11:50.389472 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:11:59.222943 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:12:01.536837 140573303715648 submission_runner.py:411] Time since start: 107845.53s, 	Step: 308777, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14702920615673065, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.051123023033142, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8163002729415894, 'test/num_examples': 10000, 'score': 104099.11788463593, 'total_duration': 107845.5293803215, 'accumulated_submission_time': 104099.11788463593, 'accumulated_eval_time': 3721.7458941936493, 'accumulated_logging_time': 12.466761350631714}
I0301 11:12:01.604556 140408319375104 logging_writer.py:48] [308777] accumulated_eval_time=3721.745894, accumulated_logging_time=12.466761, accumulated_submission_time=104099.117885, global_step=308777, preemption_count=0, score=104099.117885, test/accuracy=0.631500, test/loss=1.816300, test/num_examples=10000, total_duration=107845.529380, train/accuracy=0.960379, train/loss=0.147029, validation/accuracy=0.755880, validation/loss=1.051123, validation/num_examples=50000
I0301 11:12:09.689186 140409124681472 logging_writer.py:48] [308800] global_step=308800, grad_norm=4.705901145935059, loss=0.635804295539856
I0301 11:12:43.340672 140408319375104 logging_writer.py:48] [308900] global_step=308900, grad_norm=4.451828479766846, loss=0.6260918974876404
I0301 11:13:17.112596 140409124681472 logging_writer.py:48] [309000] global_step=309000, grad_norm=4.23740816116333, loss=0.5924838781356812
I0301 11:13:50.827452 140408319375104 logging_writer.py:48] [309100] global_step=309100, grad_norm=4.57481575012207, loss=0.7820548415184021
I0301 11:14:24.551555 140409124681472 logging_writer.py:48] [309200] global_step=309200, grad_norm=4.336435317993164, loss=0.6115450263023376
I0301 11:14:58.229791 140408319375104 logging_writer.py:48] [309300] global_step=309300, grad_norm=4.482122898101807, loss=0.603583812713623
I0301 11:15:31.958318 140409124681472 logging_writer.py:48] [309400] global_step=309400, grad_norm=4.646601676940918, loss=0.6453002691268921
I0301 11:16:05.665796 140408319375104 logging_writer.py:48] [309500] global_step=309500, grad_norm=5.063292026519775, loss=0.648618757724762
I0301 11:16:39.347743 140409124681472 logging_writer.py:48] [309600] global_step=309600, grad_norm=4.94950532913208, loss=0.6279212832450867
I0301 11:17:13.113632 140408319375104 logging_writer.py:48] [309700] global_step=309700, grad_norm=4.44577169418335, loss=0.5646356344223022
I0301 11:17:46.804604 140409124681472 logging_writer.py:48] [309800] global_step=309800, grad_norm=4.767199516296387, loss=0.5964000821113586
I0301 11:18:20.536781 140408319375104 logging_writer.py:48] [309900] global_step=309900, grad_norm=4.372183799743652, loss=0.6211191415786743
I0301 11:18:54.241856 140409124681472 logging_writer.py:48] [310000] global_step=310000, grad_norm=3.949209451675415, loss=0.5593084692955017
I0301 11:19:27.959417 140408319375104 logging_writer.py:48] [310100] global_step=310100, grad_norm=4.957501411437988, loss=0.6657295227050781
I0301 11:20:01.666460 140409124681472 logging_writer.py:48] [310200] global_step=310200, grad_norm=4.183927059173584, loss=0.5942714214324951
I0301 11:20:31.818650 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:20:37.852452 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:20:46.432524 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:20:48.784531 140573303715648 submission_runner.py:411] Time since start: 108372.78s, 	Step: 310291, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.148784339427948, 'validation/accuracy': 0.7563199996948242, 'validation/loss': 1.0511804819107056, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.81773841381073, 'test/num_examples': 10000, 'score': 104609.26225018501, 'total_duration': 108372.77680826187, 'accumulated_submission_time': 104609.26225018501, 'accumulated_eval_time': 3738.7114639282227, 'accumulated_logging_time': 12.54440450668335}
I0301 11:20:48.855064 140407379851008 logging_writer.py:48] [310291] accumulated_eval_time=3738.711464, accumulated_logging_time=12.544405, accumulated_submission_time=104609.262250, global_step=310291, preemption_count=0, score=104609.262250, test/accuracy=0.630400, test/loss=1.817738, test/num_examples=10000, total_duration=108372.776808, train/accuracy=0.959742, train/loss=0.148784, validation/accuracy=0.756320, validation/loss=1.051180, validation/num_examples=50000
I0301 11:20:52.224988 140408319375104 logging_writer.py:48] [310300] global_step=310300, grad_norm=4.680166721343994, loss=0.6606847047805786
I0301 11:21:25.925380 140407379851008 logging_writer.py:48] [310400] global_step=310400, grad_norm=4.279805660247803, loss=0.6167359948158264
I0301 11:21:59.557992 140408319375104 logging_writer.py:48] [310500] global_step=310500, grad_norm=4.579370498657227, loss=0.6341633796691895
I0301 11:22:33.231784 140407379851008 logging_writer.py:48] [310600] global_step=310600, grad_norm=4.387625694274902, loss=0.6233274936676025
I0301 11:23:06.972799 140408319375104 logging_writer.py:48] [310700] global_step=310700, grad_norm=5.058076858520508, loss=0.6405460238456726
I0301 11:23:40.637907 140407379851008 logging_writer.py:48] [310800] global_step=310800, grad_norm=4.397342205047607, loss=0.6084365844726562
I0301 11:24:14.360106 140408319375104 logging_writer.py:48] [310900] global_step=310900, grad_norm=4.570168495178223, loss=0.6503984928131104
I0301 11:24:48.108180 140407379851008 logging_writer.py:48] [311000] global_step=311000, grad_norm=4.449625492095947, loss=0.6228328943252563
I0301 11:25:21.823101 140408319375104 logging_writer.py:48] [311100] global_step=311100, grad_norm=5.072695255279541, loss=0.6105307936668396
I0301 11:25:55.535586 140407379851008 logging_writer.py:48] [311200] global_step=311200, grad_norm=4.485715389251709, loss=0.5766894221305847
I0301 11:26:29.178597 140408319375104 logging_writer.py:48] [311300] global_step=311300, grad_norm=4.922657489776611, loss=0.6939507722854614
I0301 11:27:02.841150 140407379851008 logging_writer.py:48] [311400] global_step=311400, grad_norm=4.6659369468688965, loss=0.6451369524002075
I0301 11:27:36.474979 140408319375104 logging_writer.py:48] [311500] global_step=311500, grad_norm=5.024024486541748, loss=0.5747562646865845
I0301 11:28:10.152846 140407379851008 logging_writer.py:48] [311600] global_step=311600, grad_norm=4.237807750701904, loss=0.6614217162132263
I0301 11:28:43.855016 140408319375104 logging_writer.py:48] [311700] global_step=311700, grad_norm=4.49301290512085, loss=0.6186882853507996
I0301 11:29:17.668056 140407379851008 logging_writer.py:48] [311800] global_step=311800, grad_norm=4.232205867767334, loss=0.5953108072280884
I0301 11:29:18.823914 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:29:24.954504 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:29:33.726623 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:29:36.008877 140573303715648 submission_runner.py:411] Time since start: 108900.00s, 	Step: 311805, 	{'train/accuracy': 0.9624720811843872, 'train/loss': 0.14410333335399628, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0523790121078491, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8187018632888794, 'test/num_examples': 10000, 'score': 105119.16359496117, 'total_duration': 108900.00142145157, 'accumulated_submission_time': 105119.16359496117, 'accumulated_eval_time': 3755.8963787555695, 'accumulated_logging_time': 12.624755382537842}
I0301 11:29:36.079393 140407371458304 logging_writer.py:48] [311805] accumulated_eval_time=3755.896379, accumulated_logging_time=12.624755, accumulated_submission_time=105119.163595, global_step=311805, preemption_count=0, score=105119.163595, test/accuracy=0.631100, test/loss=1.818702, test/num_examples=10000, total_duration=108900.001421, train/accuracy=0.962472, train/loss=0.144103, validation/accuracy=0.756120, validation/loss=1.052379, validation/num_examples=50000
I0301 11:30:08.349118 140407379851008 logging_writer.py:48] [311900] global_step=311900, grad_norm=5.197519302368164, loss=0.615534245967865
I0301 11:30:42.003799 140407371458304 logging_writer.py:48] [312000] global_step=312000, grad_norm=4.099677085876465, loss=0.5522500276565552
I0301 11:31:15.770128 140407379851008 logging_writer.py:48] [312100] global_step=312100, grad_norm=4.933655261993408, loss=0.6885773539543152
I0301 11:31:49.441423 140407371458304 logging_writer.py:48] [312200] global_step=312200, grad_norm=4.7449140548706055, loss=0.6809157133102417
I0301 11:32:23.219971 140407379851008 logging_writer.py:48] [312300] global_step=312300, grad_norm=4.863711357116699, loss=0.6931895017623901
I0301 11:32:56.906135 140407371458304 logging_writer.py:48] [312400] global_step=312400, grad_norm=4.401168346405029, loss=0.6464273929595947
I0301 11:33:30.681455 140407379851008 logging_writer.py:48] [312500] global_step=312500, grad_norm=4.5725603103637695, loss=0.6171123385429382
I0301 11:34:04.409813 140407371458304 logging_writer.py:48] [312600] global_step=312600, grad_norm=4.523133277893066, loss=0.6684735417366028
I0301 11:34:38.117549 140407379851008 logging_writer.py:48] [312700] global_step=312700, grad_norm=4.010626316070557, loss=0.5532283186912537
I0301 11:35:11.866008 140407371458304 logging_writer.py:48] [312800] global_step=312800, grad_norm=4.554524898529053, loss=0.6722708344459534
I0301 11:35:45.543723 140407379851008 logging_writer.py:48] [312900] global_step=312900, grad_norm=4.326934337615967, loss=0.5808820724487305
I0301 11:36:19.205117 140407371458304 logging_writer.py:48] [313000] global_step=313000, grad_norm=4.691734313964844, loss=0.6624195575714111
I0301 11:36:52.978309 140407379851008 logging_writer.py:48] [313100] global_step=313100, grad_norm=4.492672920227051, loss=0.6299576163291931
I0301 11:37:26.689462 140407371458304 logging_writer.py:48] [313200] global_step=313200, grad_norm=4.781494617462158, loss=0.7251899242401123
I0301 11:38:00.404499 140407379851008 logging_writer.py:48] [313300] global_step=313300, grad_norm=4.610500812530518, loss=0.5877482891082764
I0301 11:38:06.283056 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:38:12.380902 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:38:20.892165 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:38:23.216235 140573303715648 submission_runner.py:411] Time since start: 109427.21s, 	Step: 313319, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14517399668693542, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0512357950210571, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8169692754745483, 'test/num_examples': 10000, 'score': 105629.2938606739, 'total_duration': 109427.20877575874, 'accumulated_submission_time': 105629.2938606739, 'accumulated_eval_time': 3772.8295102119446, 'accumulated_logging_time': 12.706658840179443}
I0301 11:38:23.297901 140407371458304 logging_writer.py:48] [313319] accumulated_eval_time=3772.829510, accumulated_logging_time=12.706659, accumulated_submission_time=105629.293861, global_step=313319, preemption_count=0, score=105629.293861, test/accuracy=0.631400, test/loss=1.816969, test/num_examples=10000, total_duration=109427.208776, train/accuracy=0.960938, train/loss=0.145174, validation/accuracy=0.756040, validation/loss=1.051236, validation/num_examples=50000
I0301 11:38:50.974904 140409124681472 logging_writer.py:48] [313400] global_step=313400, grad_norm=4.372317790985107, loss=0.6475496888160706
I0301 11:39:24.661793 140407371458304 logging_writer.py:48] [313500] global_step=313500, grad_norm=4.485997676849365, loss=0.6047345995903015
I0301 11:39:58.325241 140409124681472 logging_writer.py:48] [313600] global_step=313600, grad_norm=4.447457313537598, loss=0.6457401514053345
I0301 11:40:32.005320 140407371458304 logging_writer.py:48] [313700] global_step=313700, grad_norm=4.564481735229492, loss=0.628541111946106
I0301 11:41:05.675696 140409124681472 logging_writer.py:48] [313800] global_step=313800, grad_norm=4.606351375579834, loss=0.568973958492279
I0301 11:41:39.475745 140407371458304 logging_writer.py:48] [313900] global_step=313900, grad_norm=4.54116678237915, loss=0.6411460638046265
I0301 11:42:13.134323 140409124681472 logging_writer.py:48] [314000] global_step=314000, grad_norm=4.243268966674805, loss=0.6000863909721375
I0301 11:42:46.830397 140407371458304 logging_writer.py:48] [314100] global_step=314100, grad_norm=4.357979774475098, loss=0.5671190023422241
I0301 11:43:20.495108 140409124681472 logging_writer.py:48] [314200] global_step=314200, grad_norm=4.478145599365234, loss=0.6636210680007935
I0301 11:43:54.190410 140407371458304 logging_writer.py:48] [314300] global_step=314300, grad_norm=4.548386096954346, loss=0.6185519099235535
I0301 11:44:27.865109 140409124681472 logging_writer.py:48] [314400] global_step=314400, grad_norm=4.185141086578369, loss=0.5763845443725586
I0301 11:45:01.581009 140407371458304 logging_writer.py:48] [314500] global_step=314500, grad_norm=4.706252098083496, loss=0.7041019797325134
I0301 11:45:35.221455 140409124681472 logging_writer.py:48] [314600] global_step=314600, grad_norm=4.264660835266113, loss=0.5921306014060974
I0301 11:46:08.901744 140407371458304 logging_writer.py:48] [314700] global_step=314700, grad_norm=4.509090423583984, loss=0.6647793650627136
I0301 11:46:42.630649 140409124681472 logging_writer.py:48] [314800] global_step=314800, grad_norm=4.951050758361816, loss=0.6419574618339539
I0301 11:46:53.248962 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:46:59.623322 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:47:08.421528 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:47:10.770588 140573303715648 submission_runner.py:411] Time since start: 109954.76s, 	Step: 314833, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.1482522189617157, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0523267984390259, 'validation/num_examples': 50000, 'test/accuracy': 0.632900059223175, 'test/loss': 1.8191454410552979, 'test/num_examples': 10000, 'score': 106139.17324519157, 'total_duration': 109954.76312589645, 'accumulated_submission_time': 106139.17324519157, 'accumulated_eval_time': 3790.351084470749, 'accumulated_logging_time': 12.800882577896118}
I0301 11:47:10.829033 140407371458304 logging_writer.py:48] [314833] accumulated_eval_time=3790.351084, accumulated_logging_time=12.800883, accumulated_submission_time=106139.173245, global_step=314833, preemption_count=0, score=106139.173245, test/accuracy=0.632900, test/loss=1.819145, test/num_examples=10000, total_duration=109954.763126, train/accuracy=0.960061, train/loss=0.148252, validation/accuracy=0.755620, validation/loss=1.052327, validation/num_examples=50000
I0301 11:47:33.932508 140407379851008 logging_writer.py:48] [314900] global_step=314900, grad_norm=4.216503143310547, loss=0.5641762018203735
I0301 11:48:07.616210 140407371458304 logging_writer.py:48] [315000] global_step=315000, grad_norm=4.311788082122803, loss=0.5923046469688416
I0301 11:48:41.319046 140407379851008 logging_writer.py:48] [315100] global_step=315100, grad_norm=4.733556270599365, loss=0.6004219055175781
I0301 11:49:15.019816 140407371458304 logging_writer.py:48] [315200] global_step=315200, grad_norm=4.399158000946045, loss=0.5503321886062622
I0301 11:49:48.762884 140407379851008 logging_writer.py:48] [315300] global_step=315300, grad_norm=4.180598735809326, loss=0.5944894552230835
I0301 11:50:22.445430 140407371458304 logging_writer.py:48] [315400] global_step=315400, grad_norm=4.472717761993408, loss=0.6376616954803467
I0301 11:50:56.173818 140407379851008 logging_writer.py:48] [315500] global_step=315500, grad_norm=4.2614336013793945, loss=0.6406235098838806
I0301 11:51:29.892028 140407371458304 logging_writer.py:48] [315600] global_step=315600, grad_norm=4.53570032119751, loss=0.6630723476409912
I0301 11:52:03.583417 140407379851008 logging_writer.py:48] [315700] global_step=315700, grad_norm=4.782728672027588, loss=0.672119140625
I0301 11:52:37.317351 140407371458304 logging_writer.py:48] [315800] global_step=315800, grad_norm=4.454040050506592, loss=0.6456373929977417
I0301 11:53:11.009405 140407379851008 logging_writer.py:48] [315900] global_step=315900, grad_norm=4.507747173309326, loss=0.6045219302177429
I0301 11:53:44.899983 140407371458304 logging_writer.py:48] [316000] global_step=316000, grad_norm=5.386213302612305, loss=0.6631975173950195
I0301 11:54:18.639060 140407379851008 logging_writer.py:48] [316100] global_step=316100, grad_norm=4.454433441162109, loss=0.6511495113372803
I0301 11:54:52.402513 140407371458304 logging_writer.py:48] [316200] global_step=316200, grad_norm=4.726233959197998, loss=0.6446409225463867
I0301 11:55:26.143910 140407379851008 logging_writer.py:48] [316300] global_step=316300, grad_norm=4.277480125427246, loss=0.6605688333511353
I0301 11:55:40.783437 140573303715648 spec.py:321] Evaluating on the training split.
I0301 11:55:46.854951 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 11:55:55.579110 140573303715648 spec.py:349] Evaluating on the test split.
I0301 11:55:57.906729 140573303715648 submission_runner.py:411] Time since start: 110481.90s, 	Step: 316345, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14861001074314117, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0517127513885498, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8172281980514526, 'test/num_examples': 10000, 'score': 106649.0578083992, 'total_duration': 110481.89925575256, 'accumulated_submission_time': 106649.0578083992, 'accumulated_eval_time': 3807.474318265915, 'accumulated_logging_time': 12.868227481842041}
I0301 11:55:57.977839 140409829308160 logging_writer.py:48] [316345] accumulated_eval_time=3807.474318, accumulated_logging_time=12.868227, accumulated_submission_time=106649.057808, global_step=316345, preemption_count=0, score=106649.057808, test/accuracy=0.631600, test/loss=1.817228, test/num_examples=10000, total_duration=110481.899256, train/accuracy=0.960379, train/loss=0.148610, validation/accuracy=0.755980, validation/loss=1.051713, validation/num_examples=50000
I0301 11:56:16.804026 140409837700864 logging_writer.py:48] [316400] global_step=316400, grad_norm=4.600551128387451, loss=0.5867972373962402
I0301 11:56:50.456543 140409829308160 logging_writer.py:48] [316500] global_step=316500, grad_norm=4.321667671203613, loss=0.7016206979751587
I0301 11:57:24.166886 140409837700864 logging_writer.py:48] [316600] global_step=316600, grad_norm=4.386678218841553, loss=0.6150253415107727
I0301 11:57:57.829341 140409829308160 logging_writer.py:48] [316700] global_step=316700, grad_norm=4.570437908172607, loss=0.6311833262443542
I0301 11:58:31.514728 140409837700864 logging_writer.py:48] [316800] global_step=316800, grad_norm=4.73765754699707, loss=0.6387337446212769
I0301 11:59:05.244692 140409829308160 logging_writer.py:48] [316900] global_step=316900, grad_norm=4.134093284606934, loss=0.5398290157318115
I0301 11:59:39.004329 140409837700864 logging_writer.py:48] [317000] global_step=317000, grad_norm=4.229572296142578, loss=0.6215294599533081
I0301 12:00:12.742918 140409829308160 logging_writer.py:48] [317100] global_step=317100, grad_norm=4.431115627288818, loss=0.6281679272651672
I0301 12:00:46.423856 140409837700864 logging_writer.py:48] [317200] global_step=317200, grad_norm=4.1911163330078125, loss=0.5347407460212708
I0301 12:01:20.087463 140409829308160 logging_writer.py:48] [317300] global_step=317300, grad_norm=4.531113624572754, loss=0.5840812921524048
I0301 12:01:53.799413 140409837700864 logging_writer.py:48] [317400] global_step=317400, grad_norm=4.564125061035156, loss=0.6604152321815491
I0301 12:02:27.465211 140409829308160 logging_writer.py:48] [317500] global_step=317500, grad_norm=4.19849967956543, loss=0.5513346195220947
I0301 12:03:01.124215 140409837700864 logging_writer.py:48] [317600] global_step=317600, grad_norm=4.7603607177734375, loss=0.6302649974822998
I0301 12:03:34.850626 140409829308160 logging_writer.py:48] [317700] global_step=317700, grad_norm=4.4675421714782715, loss=0.636882483959198
I0301 12:04:08.509070 140409837700864 logging_writer.py:48] [317800] global_step=317800, grad_norm=4.8363423347473145, loss=0.6493018269538879
I0301 12:04:28.174386 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:04:34.282239 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:04:42.895627 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:04:45.176508 140573303715648 submission_runner.py:411] Time since start: 111009.17s, 	Step: 317860, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14700134098529816, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0530093908309937, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.817755937576294, 'test/num_examples': 10000, 'score': 107159.1848680973, 'total_duration': 111009.16904592514, 'accumulated_submission_time': 107159.1848680973, 'accumulated_eval_time': 3824.4763877391815, 'accumulated_logging_time': 12.949540615081787}
I0301 12:04:45.250191 140408319375104 logging_writer.py:48] [317860] accumulated_eval_time=3824.476388, accumulated_logging_time=12.949541, accumulated_submission_time=107159.184868, global_step=317860, preemption_count=0, score=107159.184868, test/accuracy=0.630400, test/loss=1.817756, test/num_examples=10000, total_duration=111009.169046, train/accuracy=0.960100, train/loss=0.147001, validation/accuracy=0.755420, validation/loss=1.053009, validation/num_examples=50000
I0301 12:04:59.082249 140409124681472 logging_writer.py:48] [317900] global_step=317900, grad_norm=4.200447082519531, loss=0.5713022947311401
I0301 12:05:32.771600 140408319375104 logging_writer.py:48] [318000] global_step=318000, grad_norm=4.1862897872924805, loss=0.6056775450706482
I0301 12:06:06.607727 140409124681472 logging_writer.py:48] [318100] global_step=318100, grad_norm=4.321907997131348, loss=0.6651321053504944
I0301 12:06:40.305140 140408319375104 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.627617835998535, loss=0.6548977494239807
I0301 12:07:13.995349 140409124681472 logging_writer.py:48] [318300] global_step=318300, grad_norm=4.486873626708984, loss=0.5956242084503174
I0301 12:07:47.724936 140408319375104 logging_writer.py:48] [318400] global_step=318400, grad_norm=4.509459495544434, loss=0.6386293172836304
I0301 12:08:21.433346 140409124681472 logging_writer.py:48] [318500] global_step=318500, grad_norm=4.2058210372924805, loss=0.6625080704689026
I0301 12:08:55.158372 140408319375104 logging_writer.py:48] [318600] global_step=318600, grad_norm=4.440230846405029, loss=0.5685091018676758
I0301 12:09:28.842308 140409124681472 logging_writer.py:48] [318700] global_step=318700, grad_norm=4.24633264541626, loss=0.5844380259513855
I0301 12:10:02.530935 140408319375104 logging_writer.py:48] [318800] global_step=318800, grad_norm=4.259385108947754, loss=0.5605326890945435
I0301 12:10:36.288484 140409124681472 logging_writer.py:48] [318900] global_step=318900, grad_norm=4.379683971405029, loss=0.5946797132492065
I0301 12:11:10.025399 140408319375104 logging_writer.py:48] [319000] global_step=319000, grad_norm=4.271580219268799, loss=0.6358858942985535
I0301 12:11:43.741071 140409124681472 logging_writer.py:48] [319100] global_step=319100, grad_norm=4.821819305419922, loss=0.6435803174972534
I0301 12:12:17.646173 140408319375104 logging_writer.py:48] [319200] global_step=319200, grad_norm=4.444039821624756, loss=0.5875933170318604
I0301 12:12:51.309471 140409124681472 logging_writer.py:48] [319300] global_step=319300, grad_norm=4.581669330596924, loss=0.6564454436302185
I0301 12:13:15.356775 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:13:21.426512 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:13:30.184008 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:13:32.480472 140573303715648 submission_runner.py:411] Time since start: 111536.47s, 	Step: 319373, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14438830316066742, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0518078804016113, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8153104782104492, 'test/num_examples': 10000, 'score': 107669.21872401237, 'total_duration': 111536.4730143547, 'accumulated_submission_time': 107669.21872401237, 'accumulated_eval_time': 3841.6000397205353, 'accumulated_logging_time': 13.036304712295532}
I0301 12:13:32.557552 140407379851008 logging_writer.py:48] [319373] accumulated_eval_time=3841.600040, accumulated_logging_time=13.036305, accumulated_submission_time=107669.218724, global_step=319373, preemption_count=0, score=107669.218724, test/accuracy=0.631900, test/loss=1.815310, test/num_examples=10000, total_duration=111536.473014, train/accuracy=0.961256, train/loss=0.144388, validation/accuracy=0.755980, validation/loss=1.051808, validation/num_examples=50000
I0301 12:13:42.000689 140408319375104 logging_writer.py:48] [319400] global_step=319400, grad_norm=3.989929676055908, loss=0.5894286632537842
I0301 12:14:15.682843 140407379851008 logging_writer.py:48] [319500] global_step=319500, grad_norm=4.409209251403809, loss=0.6088622808456421
I0301 12:14:49.343802 140408319375104 logging_writer.py:48] [319600] global_step=319600, grad_norm=4.468053817749023, loss=0.5637665390968323
I0301 12:15:23.044054 140407379851008 logging_writer.py:48] [319700] global_step=319700, grad_norm=4.393306732177734, loss=0.6362696886062622
I0301 12:15:56.731463 140408319375104 logging_writer.py:48] [319800] global_step=319800, grad_norm=4.531147003173828, loss=0.6009596586227417
I0301 12:16:30.436609 140407379851008 logging_writer.py:48] [319900] global_step=319900, grad_norm=4.2073235511779785, loss=0.5490882396697998
I0301 12:17:04.123802 140408319375104 logging_writer.py:48] [320000] global_step=320000, grad_norm=5.126795768737793, loss=0.6512261033058167
I0301 12:17:37.782568 140407379851008 logging_writer.py:48] [320100] global_step=320100, grad_norm=4.422733306884766, loss=0.6149793267250061
I0301 12:18:11.560787 140408319375104 logging_writer.py:48] [320200] global_step=320200, grad_norm=4.290072441101074, loss=0.6301801800727844
I0301 12:18:45.219166 140407379851008 logging_writer.py:48] [320300] global_step=320300, grad_norm=4.322738170623779, loss=0.6457753777503967
I0301 12:19:18.927827 140408319375104 logging_writer.py:48] [320400] global_step=320400, grad_norm=4.462427139282227, loss=0.5840330123901367
I0301 12:19:52.605152 140407379851008 logging_writer.py:48] [320500] global_step=320500, grad_norm=4.600754737854004, loss=0.6689231991767883
I0301 12:20:26.357524 140408319375104 logging_writer.py:48] [320600] global_step=320600, grad_norm=4.7505316734313965, loss=0.5441194772720337
I0301 12:21:00.046422 140407379851008 logging_writer.py:48] [320700] global_step=320700, grad_norm=4.856022834777832, loss=0.6820939183235168
I0301 12:21:33.819432 140408319375104 logging_writer.py:48] [320800] global_step=320800, grad_norm=4.401937484741211, loss=0.6173890829086304
I0301 12:22:02.599438 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:22:08.650511 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:22:17.467018 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:22:19.778900 140573303715648 submission_runner.py:411] Time since start: 112063.77s, 	Step: 320887, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14686663448810577, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.0516546964645386, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8159147500991821, 'test/num_examples': 10000, 'score': 108179.18863844872, 'total_duration': 112063.77143478394, 'accumulated_submission_time': 108179.18863844872, 'accumulated_eval_time': 3858.7794704437256, 'accumulated_logging_time': 13.124491453170776}
I0301 12:22:19.851241 140409837700864 logging_writer.py:48] [320887] accumulated_eval_time=3858.779470, accumulated_logging_time=13.124491, accumulated_submission_time=108179.188638, global_step=320887, preemption_count=0, score=108179.188638, test/accuracy=0.631300, test/loss=1.815915, test/num_examples=10000, total_duration=112063.771435, train/accuracy=0.961137, train/loss=0.146867, validation/accuracy=0.756180, validation/loss=1.051655, validation/num_examples=50000
I0301 12:22:24.573836 140409846093568 logging_writer.py:48] [320900] global_step=320900, grad_norm=4.675238132476807, loss=0.5518258810043335
I0301 12:22:58.229084 140409837700864 logging_writer.py:48] [321000] global_step=321000, grad_norm=4.701530456542969, loss=0.6872608661651611
I0301 12:23:31.954779 140409846093568 logging_writer.py:48] [321100] global_step=321100, grad_norm=4.888469696044922, loss=0.6481306552886963
I0301 12:24:05.643563 140409837700864 logging_writer.py:48] [321200] global_step=321200, grad_norm=3.9281020164489746, loss=0.6028736233711243
I0301 12:24:39.417018 140409846093568 logging_writer.py:48] [321300] global_step=321300, grad_norm=5.256760597229004, loss=0.6493271589279175
I0301 12:25:13.111307 140409837700864 logging_writer.py:48] [321400] global_step=321400, grad_norm=4.099712371826172, loss=0.5899238586425781
I0301 12:25:46.844955 140409846093568 logging_writer.py:48] [321500] global_step=321500, grad_norm=4.144301891326904, loss=0.5955325365066528
I0301 12:26:20.550713 140409837700864 logging_writer.py:48] [321600] global_step=321600, grad_norm=4.494765758514404, loss=0.5845869779586792
I0301 12:26:54.282817 140409846093568 logging_writer.py:48] [321700] global_step=321700, grad_norm=4.452639102935791, loss=0.5906026363372803
I0301 12:27:27.967975 140409837700864 logging_writer.py:48] [321800] global_step=321800, grad_norm=4.402175426483154, loss=0.6028543710708618
I0301 12:28:01.709401 140409846093568 logging_writer.py:48] [321900] global_step=321900, grad_norm=4.638537406921387, loss=0.6121687293052673
I0301 12:28:35.390952 140409837700864 logging_writer.py:48] [322000] global_step=322000, grad_norm=5.23652458190918, loss=0.6169034242630005
I0301 12:29:09.117499 140409846093568 logging_writer.py:48] [322100] global_step=322100, grad_norm=4.285943508148193, loss=0.592130184173584
I0301 12:29:42.781616 140409837700864 logging_writer.py:48] [322200] global_step=322200, grad_norm=4.619875907897949, loss=0.6456553936004639
I0301 12:30:16.501025 140409846093568 logging_writer.py:48] [322300] global_step=322300, grad_norm=4.639319896697998, loss=0.6789065003395081
I0301 12:30:49.781994 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:30:56.044179 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:31:04.738966 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:31:07.079195 140573303715648 submission_runner.py:411] Time since start: 112591.07s, 	Step: 322400, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.1446283608675003, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.0537073612213135, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8188451528549194, 'test/num_examples': 10000, 'score': 108689.04994726181, 'total_duration': 112591.07172703743, 'accumulated_submission_time': 108689.04994726181, 'accumulated_eval_time': 3876.076628923416, 'accumulated_logging_time': 13.206662654876709}
I0301 12:31:07.153526 140408319375104 logging_writer.py:48] [322400] accumulated_eval_time=3876.076629, accumulated_logging_time=13.206663, accumulated_submission_time=108689.049947, global_step=322400, preemption_count=0, score=108689.049947, test/accuracy=0.630400, test/loss=1.818845, test/num_examples=10000, total_duration=112591.071727, train/accuracy=0.961396, train/loss=0.144628, validation/accuracy=0.755380, validation/loss=1.053707, validation/num_examples=50000
I0301 12:31:07.494609 140409124681472 logging_writer.py:48] [322400] global_step=322400, grad_norm=5.902036666870117, loss=0.6418986916542053
I0301 12:31:41.147713 140408319375104 logging_writer.py:48] [322500] global_step=322500, grad_norm=4.480085372924805, loss=0.6161020994186401
I0301 12:32:14.869960 140409124681472 logging_writer.py:48] [322600] global_step=322600, grad_norm=4.388128280639648, loss=0.5831353664398193
I0301 12:32:48.556559 140408319375104 logging_writer.py:48] [322700] global_step=322700, grad_norm=4.6090545654296875, loss=0.6178008913993835
I0301 12:33:22.264547 140409124681472 logging_writer.py:48] [322800] global_step=322800, grad_norm=4.3138933181762695, loss=0.6309727430343628
I0301 12:33:55.937117 140408319375104 logging_writer.py:48] [322900] global_step=322900, grad_norm=4.625308990478516, loss=0.5523478984832764
I0301 12:34:29.644884 140409124681472 logging_writer.py:48] [323000] global_step=323000, grad_norm=4.680907726287842, loss=0.6603594422340393
I0301 12:35:03.343538 140408319375104 logging_writer.py:48] [323100] global_step=323100, grad_norm=4.284290790557861, loss=0.6159489154815674
I0301 12:35:37.018125 140409124681472 logging_writer.py:48] [323200] global_step=323200, grad_norm=4.456334590911865, loss=0.6076459288597107
I0301 12:36:10.653789 140408319375104 logging_writer.py:48] [323300] global_step=323300, grad_norm=4.540272235870361, loss=0.6950226426124573
I0301 12:36:44.428523 140409124681472 logging_writer.py:48] [323400] global_step=323400, grad_norm=4.661126613616943, loss=0.6161412000656128
I0301 12:37:18.211012 140408319375104 logging_writer.py:48] [323500] global_step=323500, grad_norm=4.465493679046631, loss=0.6515271067619324
I0301 12:37:51.920728 140409124681472 logging_writer.py:48] [323600] global_step=323600, grad_norm=4.892448425292969, loss=0.6797767281532288
I0301 12:38:25.631936 140408319375104 logging_writer.py:48] [323700] global_step=323700, grad_norm=4.4123921394348145, loss=0.652816891670227
I0301 12:38:59.352922 140409124681472 logging_writer.py:48] [323800] global_step=323800, grad_norm=4.915592670440674, loss=0.6100298762321472
I0301 12:39:33.053208 140408319375104 logging_writer.py:48] [323900] global_step=323900, grad_norm=4.7873029708862305, loss=0.5944653153419495
I0301 12:39:37.241896 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:39:43.366556 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:39:52.251949 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:39:54.502105 140573303715648 submission_runner.py:411] Time since start: 113118.49s, 	Step: 323914, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14845605194568634, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0515974760055542, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8185746669769287, 'test/num_examples': 10000, 'score': 109199.06824493408, 'total_duration': 113118.49463415146, 'accumulated_submission_time': 109199.06824493408, 'accumulated_eval_time': 3893.3367924690247, 'accumulated_logging_time': 13.290921926498413}
I0301 12:39:54.576721 140407379851008 logging_writer.py:48] [323914] accumulated_eval_time=3893.336792, accumulated_logging_time=13.290922, accumulated_submission_time=109199.068245, global_step=323914, preemption_count=0, score=109199.068245, test/accuracy=0.631900, test/loss=1.818575, test/num_examples=10000, total_duration=113118.494634, train/accuracy=0.959881, train/loss=0.148456, validation/accuracy=0.755820, validation/loss=1.051597, validation/num_examples=50000
I0301 12:40:23.821169 140408319375104 logging_writer.py:48] [324000] global_step=324000, grad_norm=4.115952014923096, loss=0.6203456521034241
I0301 12:40:57.513670 140407379851008 logging_writer.py:48] [324100] global_step=324100, grad_norm=5.4249114990234375, loss=0.6541402339935303
I0301 12:41:31.186170 140408319375104 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.433254241943359, loss=0.6583185195922852
I0301 12:42:04.877982 140407379851008 logging_writer.py:48] [324300] global_step=324300, grad_norm=4.974931240081787, loss=0.556935727596283
I0301 12:42:38.669935 140408319375104 logging_writer.py:48] [324400] global_step=324400, grad_norm=4.170233726501465, loss=0.6313865184783936
I0301 12:43:12.389174 140407379851008 logging_writer.py:48] [324500] global_step=324500, grad_norm=4.867670059204102, loss=0.6900326609611511
I0301 12:43:46.092561 140408319375104 logging_writer.py:48] [324600] global_step=324600, grad_norm=4.773454189300537, loss=0.5457979440689087
I0301 12:44:19.830266 140407379851008 logging_writer.py:48] [324700] global_step=324700, grad_norm=4.4611334800720215, loss=0.6198374032974243
I0301 12:44:53.598138 140408319375104 logging_writer.py:48] [324800] global_step=324800, grad_norm=5.194809913635254, loss=0.657325804233551
I0301 12:45:27.326735 140407379851008 logging_writer.py:48] [324900] global_step=324900, grad_norm=4.999459743499756, loss=0.6780327558517456
I0301 12:46:01.050281 140408319375104 logging_writer.py:48] [325000] global_step=325000, grad_norm=4.4525980949401855, loss=0.6218575239181519
I0301 12:46:34.768698 140407379851008 logging_writer.py:48] [325100] global_step=325100, grad_norm=4.383100509643555, loss=0.6530242562294006
I0301 12:47:08.495918 140408319375104 logging_writer.py:48] [325200] global_step=325200, grad_norm=4.8352532386779785, loss=0.5871758460998535
I0301 12:47:42.220257 140407379851008 logging_writer.py:48] [325300] global_step=325300, grad_norm=4.678266525268555, loss=0.6691572666168213
I0301 12:48:15.973423 140408319375104 logging_writer.py:48] [325400] global_step=325400, grad_norm=4.299222946166992, loss=0.6227924823760986
I0301 12:48:24.550809 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:48:30.700032 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:48:39.444007 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:48:41.797129 140573303715648 submission_runner.py:411] Time since start: 113645.79s, 	Step: 325427, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.14786991477012634, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0521526336669922, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8173801898956299, 'test/num_examples': 10000, 'score': 109708.97253608704, 'total_duration': 113645.7896540165, 'accumulated_submission_time': 109708.97253608704, 'accumulated_eval_time': 3910.5830614566803, 'accumulated_logging_time': 13.375164985656738}
I0301 12:48:41.876695 140409846093568 logging_writer.py:48] [325427] accumulated_eval_time=3910.583061, accumulated_logging_time=13.375165, accumulated_submission_time=109708.972536, global_step=325427, preemption_count=0, score=109708.972536, test/accuracy=0.630500, test/loss=1.817380, test/num_examples=10000, total_duration=113645.789654, train/accuracy=0.960140, train/loss=0.147870, validation/accuracy=0.755980, validation/loss=1.052153, validation/num_examples=50000
I0301 12:49:06.847475 140409854486272 logging_writer.py:48] [325500] global_step=325500, grad_norm=4.7809953689575195, loss=0.6113138794898987
I0301 12:49:40.602444 140409846093568 logging_writer.py:48] [325600] global_step=325600, grad_norm=4.685520172119141, loss=0.6836819052696228
I0301 12:50:14.269072 140409854486272 logging_writer.py:48] [325700] global_step=325700, grad_norm=4.874601364135742, loss=0.6217812299728394
I0301 12:50:47.945368 140409846093568 logging_writer.py:48] [325800] global_step=325800, grad_norm=4.626133918762207, loss=0.6620663404464722
I0301 12:51:21.618453 140409854486272 logging_writer.py:48] [325900] global_step=325900, grad_norm=4.8352885246276855, loss=0.5610195994377136
I0301 12:51:55.377242 140409846093568 logging_writer.py:48] [326000] global_step=326000, grad_norm=4.411679744720459, loss=0.6413044929504395
I0301 12:52:29.034150 140409854486272 logging_writer.py:48] [326100] global_step=326100, grad_norm=4.315384387969971, loss=0.6627370715141296
I0301 12:53:02.715731 140409846093568 logging_writer.py:48] [326200] global_step=326200, grad_norm=4.252589702606201, loss=0.6232699751853943
I0301 12:53:36.372253 140409854486272 logging_writer.py:48] [326300] global_step=326300, grad_norm=4.5027875900268555, loss=0.6199812889099121
I0301 12:54:10.133949 140409846093568 logging_writer.py:48] [326400] global_step=326400, grad_norm=4.769238471984863, loss=0.6516816020011902
I0301 12:54:43.800698 140409854486272 logging_writer.py:48] [326500] global_step=326500, grad_norm=4.469581604003906, loss=0.6404842734336853
I0301 12:55:17.603713 140409846093568 logging_writer.py:48] [326600] global_step=326600, grad_norm=4.336636066436768, loss=0.5761662125587463
I0301 12:55:51.263550 140409854486272 logging_writer.py:48] [326700] global_step=326700, grad_norm=4.925945281982422, loss=0.638572096824646
I0301 12:56:24.969996 140409846093568 logging_writer.py:48] [326800] global_step=326800, grad_norm=5.197035789489746, loss=0.6768540143966675
I0301 12:56:58.634542 140409854486272 logging_writer.py:48] [326900] global_step=326900, grad_norm=5.128005504608154, loss=0.6673665642738342
I0301 12:57:11.889209 140573303715648 spec.py:321] Evaluating on the training split.
I0301 12:57:17.970488 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 12:57:26.731307 140573303715648 spec.py:349] Evaluating on the test split.
I0301 12:57:29.047080 140573303715648 submission_runner.py:411] Time since start: 114173.04s, 	Step: 326941, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.1470252126455307, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0519651174545288, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8180609941482544, 'test/num_examples': 10000, 'score': 110218.91332650185, 'total_duration': 114173.03961229324, 'accumulated_submission_time': 110218.91332650185, 'accumulated_eval_time': 3927.7408850193024, 'accumulated_logging_time': 13.466815948486328}
I0301 12:57:29.116233 140408319375104 logging_writer.py:48] [326941] accumulated_eval_time=3927.740885, accumulated_logging_time=13.466816, accumulated_submission_time=110218.913327, global_step=326941, preemption_count=0, score=110218.913327, test/accuracy=0.631600, test/loss=1.818061, test/num_examples=10000, total_duration=114173.039612, train/accuracy=0.960878, train/loss=0.147025, validation/accuracy=0.755760, validation/loss=1.051965, validation/num_examples=50000
I0301 12:57:49.355576 140409124681472 logging_writer.py:48] [327000] global_step=327000, grad_norm=5.0779595375061035, loss=0.6365713477134705
I0301 12:58:23.013492 140408319375104 logging_writer.py:48] [327100] global_step=327100, grad_norm=4.4966816902160645, loss=0.6048405170440674
I0301 12:58:56.707317 140409124681472 logging_writer.py:48] [327200] global_step=327200, grad_norm=4.349212169647217, loss=0.6479875445365906
I0301 12:59:30.435090 140408319375104 logging_writer.py:48] [327300] global_step=327300, grad_norm=4.109109401702881, loss=0.6409573554992676
I0301 13:00:04.122910 140409124681472 logging_writer.py:48] [327400] global_step=327400, grad_norm=4.591473579406738, loss=0.64454585313797
I0301 13:00:37.849521 140408319375104 logging_writer.py:48] [327500] global_step=327500, grad_norm=4.349932670593262, loss=0.6704939007759094
I0301 13:01:11.628472 140409124681472 logging_writer.py:48] [327600] global_step=327600, grad_norm=4.589875221252441, loss=0.6543856859207153
I0301 13:01:45.355463 140408319375104 logging_writer.py:48] [327700] global_step=327700, grad_norm=4.685257434844971, loss=0.6784909963607788
I0301 13:02:19.326341 140409124681472 logging_writer.py:48] [327800] global_step=327800, grad_norm=4.625117778778076, loss=0.6663200259208679
I0301 13:02:53.015971 140408319375104 logging_writer.py:48] [327900] global_step=327900, grad_norm=4.5644354820251465, loss=0.5947317481040955
I0301 13:03:26.689924 140409124681472 logging_writer.py:48] [328000] global_step=328000, grad_norm=4.387823104858398, loss=0.642271876335144
I0301 13:04:00.427356 140408319375104 logging_writer.py:48] [328100] global_step=328100, grad_norm=4.225172519683838, loss=0.6214510798454285
I0301 13:04:34.134963 140409124681472 logging_writer.py:48] [328200] global_step=328200, grad_norm=4.427811622619629, loss=0.6357043981552124
I0301 13:05:07.867903 140408319375104 logging_writer.py:48] [328300] global_step=328300, grad_norm=4.522629737854004, loss=0.675359845161438
I0301 13:05:41.544090 140409124681472 logging_writer.py:48] [328400] global_step=328400, grad_norm=4.566713809967041, loss=0.6397175192832947
I0301 13:05:59.242446 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:06:05.401517 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:06:14.092834 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:06:16.427187 140573303715648 submission_runner.py:411] Time since start: 114700.42s, 	Step: 328454, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14584684371948242, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.051677942276001, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8172355890274048, 'test/num_examples': 10000, 'score': 110728.96795773506, 'total_duration': 114700.41973114014, 'accumulated_submission_time': 110728.96795773506, 'accumulated_eval_time': 3944.9255831241608, 'accumulated_logging_time': 13.548635721206665}
I0301 13:06:16.499514 140409846093568 logging_writer.py:48] [328454] accumulated_eval_time=3944.925583, accumulated_logging_time=13.548636, accumulated_submission_time=110728.967958, global_step=328454, preemption_count=0, score=110728.967958, test/accuracy=0.632300, test/loss=1.817236, test/num_examples=10000, total_duration=114700.419731, train/accuracy=0.961476, train/loss=0.145847, validation/accuracy=0.755760, validation/loss=1.051678, validation/num_examples=50000
I0301 13:06:32.357990 140409854486272 logging_writer.py:48] [328500] global_step=328500, grad_norm=4.504069805145264, loss=0.597743034362793
I0301 13:07:05.995290 140409846093568 logging_writer.py:48] [328600] global_step=328600, grad_norm=4.559367656707764, loss=0.6098994612693787
I0301 13:07:39.767578 140409854486272 logging_writer.py:48] [328700] global_step=328700, grad_norm=4.556239128112793, loss=0.572981059551239
I0301 13:08:13.435542 140409846093568 logging_writer.py:48] [328800] global_step=328800, grad_norm=5.046917915344238, loss=0.6060866117477417
I0301 13:08:47.077793 140409854486272 logging_writer.py:48] [328900] global_step=328900, grad_norm=4.072428226470947, loss=0.5603622198104858
I0301 13:09:20.791735 140409846093568 logging_writer.py:48] [329000] global_step=329000, grad_norm=4.572961330413818, loss=0.6796905398368835
I0301 13:09:54.476987 140409854486272 logging_writer.py:48] [329100] global_step=329100, grad_norm=5.786568641662598, loss=0.6908707022666931
I0301 13:10:28.196485 140409846093568 logging_writer.py:48] [329200] global_step=329200, grad_norm=4.183035373687744, loss=0.5869712829589844
I0301 13:11:01.875852 140409854486272 logging_writer.py:48] [329300] global_step=329300, grad_norm=4.805110454559326, loss=0.7178527116775513
I0301 13:11:35.536351 140409846093568 logging_writer.py:48] [329400] global_step=329400, grad_norm=4.378979682922363, loss=0.647391140460968
I0301 13:12:09.263061 140409854486272 logging_writer.py:48] [329500] global_step=329500, grad_norm=4.5449137687683105, loss=0.6438356041908264
I0301 13:12:42.962419 140409846093568 logging_writer.py:48] [329600] global_step=329600, grad_norm=4.6489691734313965, loss=0.6955426931381226
I0301 13:13:16.727015 140409854486272 logging_writer.py:48] [329700] global_step=329700, grad_norm=3.983659029006958, loss=0.5797929167747498
I0301 13:13:50.525109 140409846093568 logging_writer.py:48] [329800] global_step=329800, grad_norm=4.9018402099609375, loss=0.6533625721931458
I0301 13:14:24.184886 140409854486272 logging_writer.py:48] [329900] global_step=329900, grad_norm=4.325839042663574, loss=0.6675971746444702
I0301 13:14:46.588048 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:14:52.857824 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:15:01.251167 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:15:03.724299 140573303715648 submission_runner.py:411] Time since start: 115227.72s, 	Step: 329968, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14799167215824127, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0517678260803223, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.816851019859314, 'test/num_examples': 10000, 'score': 111238.9862473011, 'total_duration': 115227.71682953835, 'accumulated_submission_time': 111238.9862473011, 'accumulated_eval_time': 3962.0617773532867, 'accumulated_logging_time': 13.631106853485107}
I0301 13:15:03.796207 140407371458304 logging_writer.py:48] [329968] accumulated_eval_time=3962.061777, accumulated_logging_time=13.631107, accumulated_submission_time=111238.986247, global_step=329968, preemption_count=0, score=111238.986247, test/accuracy=0.631700, test/loss=1.816851, test/num_examples=10000, total_duration=115227.716830, train/accuracy=0.960419, train/loss=0.147992, validation/accuracy=0.755820, validation/loss=1.051768, validation/num_examples=50000
I0301 13:15:14.922381 140407379851008 logging_writer.py:48] [330000] global_step=330000, grad_norm=4.097105979919434, loss=0.5945237278938293
I0301 13:15:48.594925 140407371458304 logging_writer.py:48] [330100] global_step=330100, grad_norm=4.3697614669799805, loss=0.6083986759185791
I0301 13:16:22.336582 140407379851008 logging_writer.py:48] [330200] global_step=330200, grad_norm=4.88352632522583, loss=0.6999707818031311
I0301 13:16:56.034449 140407371458304 logging_writer.py:48] [330300] global_step=330300, grad_norm=4.293478012084961, loss=0.6120288968086243
I0301 13:17:29.797516 140407379851008 logging_writer.py:48] [330400] global_step=330400, grad_norm=4.435540199279785, loss=0.6478795409202576
I0301 13:18:03.495977 140407371458304 logging_writer.py:48] [330500] global_step=330500, grad_norm=4.608046054840088, loss=0.6316840052604675
I0301 13:18:37.237246 140407379851008 logging_writer.py:48] [330600] global_step=330600, grad_norm=4.374659061431885, loss=0.6218886375427246
I0301 13:19:10.889837 140407371458304 logging_writer.py:48] [330700] global_step=330700, grad_norm=4.844204425811768, loss=0.6387215256690979
I0301 13:19:44.774409 140407379851008 logging_writer.py:48] [330800] global_step=330800, grad_norm=3.962984561920166, loss=0.5445787906646729
I0301 13:20:18.492340 140407371458304 logging_writer.py:48] [330900] global_step=330900, grad_norm=4.138248920440674, loss=0.5658604502677917
I0301 13:20:52.266092 140407379851008 logging_writer.py:48] [331000] global_step=331000, grad_norm=4.377921104431152, loss=0.6446361541748047
I0301 13:21:25.960216 140407371458304 logging_writer.py:48] [331100] global_step=331100, grad_norm=4.5161919593811035, loss=0.6730546951293945
I0301 13:21:59.702745 140407379851008 logging_writer.py:48] [331200] global_step=331200, grad_norm=4.329708576202393, loss=0.6244092583656311
I0301 13:22:33.392726 140407371458304 logging_writer.py:48] [331300] global_step=331300, grad_norm=4.915101051330566, loss=0.6378514170646667
I0301 13:23:07.151961 140407379851008 logging_writer.py:48] [331400] global_step=331400, grad_norm=4.319201469421387, loss=0.6230678558349609
I0301 13:23:33.922071 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:23:39.951501 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:23:48.543511 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:23:50.853699 140573303715648 submission_runner.py:411] Time since start: 115754.85s, 	Step: 331481, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.15248756110668182, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0509434938430786, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8166096210479736, 'test/num_examples': 10000, 'score': 111749.04176855087, 'total_duration': 115754.84624505043, 'accumulated_submission_time': 111749.04176855087, 'accumulated_eval_time': 3978.993365049362, 'accumulated_logging_time': 13.714733600616455}
I0301 13:23:50.929645 140407379851008 logging_writer.py:48] [331481] accumulated_eval_time=3978.993365, accumulated_logging_time=13.714734, accumulated_submission_time=111749.041769, global_step=331481, preemption_count=0, score=111749.041769, test/accuracy=0.631100, test/loss=1.816610, test/num_examples=10000, total_duration=115754.846245, train/accuracy=0.959881, train/loss=0.152488, validation/accuracy=0.756100, validation/loss=1.050943, validation/num_examples=50000
I0301 13:23:57.664965 140408319375104 logging_writer.py:48] [331500] global_step=331500, grad_norm=4.365179538726807, loss=0.5456846952438354
I0301 13:24:31.337238 140407379851008 logging_writer.py:48] [331600] global_step=331600, grad_norm=4.159219264984131, loss=0.6300911903381348
I0301 13:25:05.026660 140408319375104 logging_writer.py:48] [331700] global_step=331700, grad_norm=4.53999662399292, loss=0.5600271224975586
I0301 13:25:38.790586 140407379851008 logging_writer.py:48] [331800] global_step=331800, grad_norm=5.206848621368408, loss=0.6164318919181824
I0301 13:26:12.472530 140408319375104 logging_writer.py:48] [331900] global_step=331900, grad_norm=4.3763651847839355, loss=0.6249789595603943
I0301 13:26:46.149982 140407379851008 logging_writer.py:48] [332000] global_step=332000, grad_norm=4.840384483337402, loss=0.6608111262321472
I0301 13:27:19.767370 140408319375104 logging_writer.py:48] [332100] global_step=332100, grad_norm=3.822904109954834, loss=0.5694904327392578
I0301 13:27:53.477879 140407379851008 logging_writer.py:48] [332200] global_step=332200, grad_norm=4.878243923187256, loss=0.5934245586395264
I0301 13:28:27.099609 140408319375104 logging_writer.py:48] [332300] global_step=332300, grad_norm=4.631087779998779, loss=0.6594221591949463
I0301 13:29:00.752611 140407379851008 logging_writer.py:48] [332400] global_step=332400, grad_norm=4.4857282638549805, loss=0.5945221781730652
I0301 13:29:34.463710 140408319375104 logging_writer.py:48] [332500] global_step=332500, grad_norm=4.401370048522949, loss=0.6492908000946045
I0301 13:30:08.156769 140407379851008 logging_writer.py:48] [332600] global_step=332600, grad_norm=4.466861248016357, loss=0.6445618867874146
I0301 13:30:41.801672 140408319375104 logging_writer.py:48] [332700] global_step=332700, grad_norm=4.029829978942871, loss=0.5972869396209717
I0301 13:31:15.487545 140407379851008 logging_writer.py:48] [332800] global_step=332800, grad_norm=4.64784574508667, loss=0.5981658697128296
I0301 13:31:49.364583 140408319375104 logging_writer.py:48] [332900] global_step=332900, grad_norm=4.2233734130859375, loss=0.5782392024993896
I0301 13:32:20.870402 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:32:26.951068 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:32:35.759079 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:32:37.999703 140573303715648 submission_runner.py:411] Time since start: 116281.99s, 	Step: 332995, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14434514939785004, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0530338287353516, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.818378210067749, 'test/num_examples': 10000, 'score': 112258.91333699226, 'total_duration': 116281.9922375679, 'accumulated_submission_time': 112258.91333699226, 'accumulated_eval_time': 3996.1226150989532, 'accumulated_logging_time': 13.801108837127686}
I0301 13:32:38.073716 140409837700864 logging_writer.py:48] [332995] accumulated_eval_time=3996.122615, accumulated_logging_time=13.801109, accumulated_submission_time=112258.913337, global_step=332995, preemption_count=0, score=112258.913337, test/accuracy=0.631300, test/loss=1.818378, test/num_examples=10000, total_duration=116281.992238, train/accuracy=0.961735, train/loss=0.144345, validation/accuracy=0.755800, validation/loss=1.053034, validation/num_examples=50000
I0301 13:32:40.105457 140409846093568 logging_writer.py:48] [333000] global_step=333000, grad_norm=4.608273983001709, loss=0.6013680100440979
I0301 13:33:13.855083 140409837700864 logging_writer.py:48] [333100] global_step=333100, grad_norm=4.510056495666504, loss=0.6502780914306641
I0301 13:33:47.512335 140409846093568 logging_writer.py:48] [333200] global_step=333200, grad_norm=4.989474773406982, loss=0.6251359581947327
I0301 13:34:21.199932 140409837700864 logging_writer.py:48] [333300] global_step=333300, grad_norm=4.744852066040039, loss=0.6149423122406006
I0301 13:34:54.933478 140409846093568 logging_writer.py:48] [333400] global_step=333400, grad_norm=4.588253498077393, loss=0.636925995349884
I0301 13:35:28.698193 140409837700864 logging_writer.py:48] [333500] global_step=333500, grad_norm=4.462632656097412, loss=0.5942676663398743
I0301 13:36:02.374932 140409846093568 logging_writer.py:48] [333600] global_step=333600, grad_norm=5.027530670166016, loss=0.6943463087081909
I0301 13:36:36.160321 140409837700864 logging_writer.py:48] [333700] global_step=333700, grad_norm=4.628931522369385, loss=0.6698309183120728
I0301 13:37:09.845897 140409846093568 logging_writer.py:48] [333800] global_step=333800, grad_norm=4.3457159996032715, loss=0.7038080096244812
I0301 13:37:43.583019 140409837700864 logging_writer.py:48] [333900] global_step=333900, grad_norm=4.704835891723633, loss=0.5653526782989502
I0301 13:38:17.374920 140409846093568 logging_writer.py:48] [334000] global_step=334000, grad_norm=4.215250492095947, loss=0.5725740790367126
I0301 13:38:51.034964 140409837700864 logging_writer.py:48] [334100] global_step=334100, grad_norm=4.409623622894287, loss=0.6035427451133728
I0301 13:39:24.765444 140409846093568 logging_writer.py:48] [334200] global_step=334200, grad_norm=4.248566627502441, loss=0.6394340991973877
I0301 13:39:58.459795 140409837700864 logging_writer.py:48] [334300] global_step=334300, grad_norm=3.944453001022339, loss=0.5532340407371521
I0301 13:40:32.184349 140409846093568 logging_writer.py:48] [334400] global_step=334400, grad_norm=4.54633903503418, loss=0.5856961607933044
I0301 13:41:05.861051 140409837700864 logging_writer.py:48] [334500] global_step=334500, grad_norm=4.3269734382629395, loss=0.6206793189048767
I0301 13:41:08.029974 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:41:14.060349 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:41:22.770981 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:41:25.081676 140573303715648 submission_runner.py:411] Time since start: 116809.07s, 	Step: 334508, 	{'train/accuracy': 0.9585060477256775, 'train/loss': 0.15001627802848816, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.052400827407837, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.819270372390747, 'test/num_examples': 10000, 'score': 112768.79986357689, 'total_duration': 116809.0741918087, 'accumulated_submission_time': 112768.79986357689, 'accumulated_eval_time': 4013.174238204956, 'accumulated_logging_time': 13.886026620864868}
I0301 13:41:25.158292 140409846093568 logging_writer.py:48] [334508] accumulated_eval_time=4013.174238, accumulated_logging_time=13.886027, accumulated_submission_time=112768.799864, global_step=334508, preemption_count=0, score=112768.799864, test/accuracy=0.631100, test/loss=1.819270, test/num_examples=10000, total_duration=116809.074192, train/accuracy=0.958506, train/loss=0.150016, validation/accuracy=0.756000, validation/loss=1.052401, validation/num_examples=50000
I0301 13:41:56.441402 140409854486272 logging_writer.py:48] [334600] global_step=334600, grad_norm=4.264974594116211, loss=0.5564585328102112
I0301 13:42:30.134731 140409846093568 logging_writer.py:48] [334700] global_step=334700, grad_norm=4.388862609863281, loss=0.5967485904693604
I0301 13:43:03.795881 140409854486272 logging_writer.py:48] [334800] global_step=334800, grad_norm=4.521028518676758, loss=0.6327731609344482
I0301 13:43:37.472245 140409846093568 logging_writer.py:48] [334900] global_step=334900, grad_norm=4.420467853546143, loss=0.6174125671386719
I0301 13:44:11.227776 140409854486272 logging_writer.py:48] [335000] global_step=335000, grad_norm=4.802305221557617, loss=0.6349117755889893
I0301 13:44:44.970644 140409846093568 logging_writer.py:48] [335100] global_step=335100, grad_norm=4.62108039855957, loss=0.5760618448257446
I0301 13:45:18.692378 140409854486272 logging_writer.py:48] [335200] global_step=335200, grad_norm=4.422915458679199, loss=0.6657717227935791
I0301 13:45:52.416595 140409846093568 logging_writer.py:48] [335300] global_step=335300, grad_norm=4.526054859161377, loss=0.6537303924560547
I0301 13:46:26.142376 140409854486272 logging_writer.py:48] [335400] global_step=335400, grad_norm=4.21439266204834, loss=0.5412573218345642
I0301 13:46:59.856740 140409846093568 logging_writer.py:48] [335500] global_step=335500, grad_norm=4.695138454437256, loss=0.6806418299674988
I0301 13:47:33.588569 140409854486272 logging_writer.py:48] [335600] global_step=335600, grad_norm=4.2669997215271, loss=0.6152655482292175
I0301 13:48:07.316398 140409846093568 logging_writer.py:48] [335700] global_step=335700, grad_norm=4.806358814239502, loss=0.6906777024269104
I0301 13:48:41.039360 140409854486272 logging_writer.py:48] [335800] global_step=335800, grad_norm=4.5034871101379395, loss=0.5436252355575562
I0301 13:49:14.731033 140409846093568 logging_writer.py:48] [335900] global_step=335900, grad_norm=4.384567737579346, loss=0.6445674300193787
I0301 13:49:48.447721 140409854486272 logging_writer.py:48] [336000] global_step=336000, grad_norm=4.729875087738037, loss=0.6036000847816467
I0301 13:49:55.341635 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:50:01.446491 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:50:10.219146 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:50:12.883983 140573303715648 submission_runner.py:411] Time since start: 117336.88s, 	Step: 336022, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.14542129635810852, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0526785850524902, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8194116353988647, 'test/num_examples': 10000, 'score': 113278.91282248497, 'total_duration': 117336.87652873993, 'accumulated_submission_time': 113278.91282248497, 'accumulated_eval_time': 4030.7165541648865, 'accumulated_logging_time': 13.973962306976318}
I0301 13:50:12.960517 140407371458304 logging_writer.py:48] [336022] accumulated_eval_time=4030.716554, accumulated_logging_time=13.973962, accumulated_submission_time=113278.912822, global_step=336022, preemption_count=0, score=113278.912822, test/accuracy=0.631600, test/loss=1.819412, test/num_examples=10000, total_duration=117336.876529, train/accuracy=0.961336, train/loss=0.145421, validation/accuracy=0.756200, validation/loss=1.052679, validation/num_examples=50000
I0301 13:50:39.589232 140407379851008 logging_writer.py:48] [336100] global_step=336100, grad_norm=4.503303050994873, loss=0.6210336089134216
I0301 13:51:13.213485 140407371458304 logging_writer.py:48] [336200] global_step=336200, grad_norm=4.852864742279053, loss=0.6207073926925659
I0301 13:51:46.893285 140407379851008 logging_writer.py:48] [336300] global_step=336300, grad_norm=4.695908069610596, loss=0.6466450691223145
I0301 13:52:20.657390 140407371458304 logging_writer.py:48] [336400] global_step=336400, grad_norm=4.613738059997559, loss=0.5745919942855835
I0301 13:52:54.352898 140407379851008 logging_writer.py:48] [336500] global_step=336500, grad_norm=4.681292533874512, loss=0.6491972208023071
I0301 13:53:28.117610 140407371458304 logging_writer.py:48] [336600] global_step=336600, grad_norm=4.301652908325195, loss=0.647415041923523
I0301 13:54:01.828401 140407379851008 logging_writer.py:48] [336700] global_step=336700, grad_norm=4.862433910369873, loss=0.5613105893135071
I0301 13:54:35.524873 140407371458304 logging_writer.py:48] [336800] global_step=336800, grad_norm=4.68496036529541, loss=0.6483847498893738
I0301 13:55:09.248514 140407379851008 logging_writer.py:48] [336900] global_step=336900, grad_norm=4.553691864013672, loss=0.6092862486839294
I0301 13:55:42.957185 140407371458304 logging_writer.py:48] [337000] global_step=337000, grad_norm=4.32527494430542, loss=0.582704484462738
I0301 13:56:16.733493 140407379851008 logging_writer.py:48] [337100] global_step=337100, grad_norm=4.500874042510986, loss=0.6288723349571228
I0301 13:56:50.461226 140407371458304 logging_writer.py:48] [337200] global_step=337200, grad_norm=4.839661121368408, loss=0.6105008721351624
I0301 13:57:24.149626 140407379851008 logging_writer.py:48] [337300] global_step=337300, grad_norm=4.174750328063965, loss=0.5673369765281677
I0301 13:57:57.922247 140407371458304 logging_writer.py:48] [337400] global_step=337400, grad_norm=4.237851142883301, loss=0.5918931365013123
I0301 13:58:31.601236 140407379851008 logging_writer.py:48] [337500] global_step=337500, grad_norm=4.251530647277832, loss=0.6269715428352356
I0301 13:58:43.207525 140573303715648 spec.py:321] Evaluating on the training split.
I0301 13:58:49.353333 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 13:58:58.060081 140573303715648 spec.py:349] Evaluating on the test split.
I0301 13:59:00.377817 140573303715648 submission_runner.py:411] Time since start: 117864.37s, 	Step: 337536, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.1482814997434616, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0519975423812866, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8193473815917969, 'test/num_examples': 10000, 'score': 113789.0877354145, 'total_duration': 117864.37035274506, 'accumulated_submission_time': 113789.0877354145, 'accumulated_eval_time': 4047.8868165016174, 'accumulated_logging_time': 14.062806129455566}
I0301 13:59:00.452373 140407371458304 logging_writer.py:48] [337536] accumulated_eval_time=4047.886817, accumulated_logging_time=14.062806, accumulated_submission_time=113789.087735, global_step=337536, preemption_count=0, score=113789.087735, test/accuracy=0.631700, test/loss=1.819347, test/num_examples=10000, total_duration=117864.370353, train/accuracy=0.960798, train/loss=0.148281, validation/accuracy=0.756080, validation/loss=1.051998, validation/num_examples=50000
I0301 13:59:22.316856 140409837700864 logging_writer.py:48] [337600] global_step=337600, grad_norm=4.923886299133301, loss=0.7171486020088196
I0301 13:59:55.958437 140407371458304 logging_writer.py:48] [337700] global_step=337700, grad_norm=4.7360453605651855, loss=0.6569844484329224
I0301 14:00:29.630719 140409837700864 logging_writer.py:48] [337800] global_step=337800, grad_norm=4.740508556365967, loss=0.6281147003173828
I0301 14:01:03.292323 140407371458304 logging_writer.py:48] [337900] global_step=337900, grad_norm=4.1544904708862305, loss=0.7107104063034058
I0301 14:01:36.989501 140409837700864 logging_writer.py:48] [338000] global_step=338000, grad_norm=4.534074783325195, loss=0.6347406506538391
I0301 14:02:10.659018 140407371458304 logging_writer.py:48] [338100] global_step=338100, grad_norm=4.564809322357178, loss=0.6337859630584717
I0301 14:02:44.473263 140409837700864 logging_writer.py:48] [338200] global_step=338200, grad_norm=4.833313941955566, loss=0.6475403904914856
I0301 14:03:18.102120 140407371458304 logging_writer.py:48] [338300] global_step=338300, grad_norm=4.293127536773682, loss=0.5845272541046143
I0301 14:03:51.780502 140409837700864 logging_writer.py:48] [338400] global_step=338400, grad_norm=4.407612323760986, loss=0.6534422636032104
I0301 14:04:25.509704 140407371458304 logging_writer.py:48] [338500] global_step=338500, grad_norm=4.355589866638184, loss=0.6265434622764587
I0301 14:04:59.181662 140409837700864 logging_writer.py:48] [338600] global_step=338600, grad_norm=4.3609514236450195, loss=0.662752628326416
I0301 14:05:32.847355 140407371458304 logging_writer.py:48] [338700] global_step=338700, grad_norm=5.121255874633789, loss=0.6713495850563049
I0301 14:06:06.535145 140409837700864 logging_writer.py:48] [338800] global_step=338800, grad_norm=4.613969326019287, loss=0.5536265969276428
I0301 14:06:40.217963 140407371458304 logging_writer.py:48] [338900] global_step=338900, grad_norm=4.247252941131592, loss=0.5799903273582458
I0301 14:07:13.891683 140409837700864 logging_writer.py:48] [339000] global_step=339000, grad_norm=4.63149356842041, loss=0.6424623131752014
I0301 14:07:30.563378 140573303715648 spec.py:321] Evaluating on the training split.
I0301 14:07:36.598532 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 14:07:45.265928 140573303715648 spec.py:349] Evaluating on the test split.
I0301 14:07:47.577614 140573303715648 submission_runner.py:411] Time since start: 118391.57s, 	Step: 339051, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.15035422146320343, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.052941083908081, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8172272443771362, 'test/num_examples': 10000, 'score': 114299.12990474701, 'total_duration': 118391.57015228271, 'accumulated_submission_time': 114299.12990474701, 'accumulated_eval_time': 4064.900999069214, 'accumulated_logging_time': 14.147011280059814}
I0301 14:07:47.650748 140409829308160 logging_writer.py:48] [339051] accumulated_eval_time=4064.900999, accumulated_logging_time=14.147011, accumulated_submission_time=114299.129905, global_step=339051, preemption_count=0, score=114299.129905, test/accuracy=0.631200, test/loss=1.817227, test/num_examples=10000, total_duration=118391.570152, train/accuracy=0.959742, train/loss=0.150354, validation/accuracy=0.755240, validation/loss=1.052941, validation/num_examples=50000
I0301 14:08:04.453926 140409854486272 logging_writer.py:48] [339100] global_step=339100, grad_norm=4.495675086975098, loss=0.5818884968757629
I0301 14:08:38.232928 140409829308160 logging_writer.py:48] [339200] global_step=339200, grad_norm=4.425458908081055, loss=0.6521021127700806
I0301 14:09:11.873196 140409854486272 logging_writer.py:48] [339300] global_step=339300, grad_norm=4.619644641876221, loss=0.6852432489395142
I0301 14:09:45.585136 140409829308160 logging_writer.py:48] [339400] global_step=339400, grad_norm=4.816972255706787, loss=0.6585742235183716
I0301 14:10:19.247214 140409854486272 logging_writer.py:48] [339500] global_step=339500, grad_norm=4.799056529998779, loss=0.6198142170906067
I0301 14:10:52.960137 140409829308160 logging_writer.py:48] [339600] global_step=339600, grad_norm=4.2857489585876465, loss=0.6288584470748901
I0301 14:11:26.675117 140409854486272 logging_writer.py:48] [339700] global_step=339700, grad_norm=4.778103828430176, loss=0.7190698385238647
I0301 14:12:00.379683 140409829308160 logging_writer.py:48] [339800] global_step=339800, grad_norm=4.190309047698975, loss=0.6210763454437256
I0301 14:12:34.126036 140409854486272 logging_writer.py:48] [339900] global_step=339900, grad_norm=4.441476345062256, loss=0.6020829081535339
I0301 14:13:07.791611 140409829308160 logging_writer.py:48] [340000] global_step=340000, grad_norm=4.837119102478027, loss=0.6515636444091797
I0301 14:13:41.460075 140409854486272 logging_writer.py:48] [340100] global_step=340100, grad_norm=4.438194274902344, loss=0.6182059645652771
I0301 14:14:15.167882 140409829308160 logging_writer.py:48] [340200] global_step=340200, grad_norm=4.531871795654297, loss=0.6852403879165649
I0301 14:14:48.953047 140409854486272 logging_writer.py:48] [340300] global_step=340300, grad_norm=3.9273276329040527, loss=0.5477781295776367
I0301 14:15:22.608673 140409829308160 logging_writer.py:48] [340400] global_step=340400, grad_norm=4.59368371963501, loss=0.6663437485694885
I0301 14:15:56.294095 140409854486272 logging_writer.py:48] [340500] global_step=340500, grad_norm=4.061491966247559, loss=0.5704486966133118
I0301 14:16:17.681995 140573303715648 spec.py:321] Evaluating on the training split.
I0301 14:16:23.841832 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 14:16:32.418732 140573303715648 spec.py:349] Evaluating on the test split.
I0301 14:16:34.738821 140573303715648 submission_runner.py:411] Time since start: 118918.73s, 	Step: 340565, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.1452462524175644, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.053504467010498, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8191752433776855, 'test/num_examples': 10000, 'score': 114809.09057998657, 'total_duration': 118918.73136019707, 'accumulated_submission_time': 114809.09057998657, 'accumulated_eval_time': 4081.9577882289886, 'accumulated_logging_time': 14.231061697006226}
I0301 14:16:34.812298 140409124681472 logging_writer.py:48] [340565] accumulated_eval_time=4081.957788, accumulated_logging_time=14.231062, accumulated_submission_time=114809.090580, global_step=340565, preemption_count=0, score=114809.090580, test/accuracy=0.631700, test/loss=1.819175, test/num_examples=10000, total_duration=118918.731360, train/accuracy=0.961396, train/loss=0.145246, validation/accuracy=0.756000, validation/loss=1.053504, validation/num_examples=50000
I0301 14:16:46.940425 140409829308160 logging_writer.py:48] [340600] global_step=340600, grad_norm=4.0683770179748535, loss=0.530890703201294
I0301 14:17:20.579426 140409124681472 logging_writer.py:48] [340700] global_step=340700, grad_norm=4.467339038848877, loss=0.6338573098182678
I0301 14:17:54.248496 140409829308160 logging_writer.py:48] [340800] global_step=340800, grad_norm=4.598914623260498, loss=0.6164487600326538
I0301 14:18:27.940719 140409124681472 logging_writer.py:48] [340900] global_step=340900, grad_norm=5.498919486999512, loss=0.6509631276130676
I0301 14:19:01.652497 140409829308160 logging_writer.py:48] [341000] global_step=341000, grad_norm=4.152306079864502, loss=0.5852531790733337
I0301 14:19:35.313818 140409124681472 logging_writer.py:48] [341100] global_step=341100, grad_norm=4.6413655281066895, loss=0.604555606842041
I0301 14:20:08.996216 140409829308160 logging_writer.py:48] [341200] global_step=341200, grad_norm=4.678744792938232, loss=0.6404851675033569
I0301 14:20:42.698031 140409124681472 logging_writer.py:48] [341300] global_step=341300, grad_norm=4.4163289070129395, loss=0.6495353579521179
I0301 14:21:16.509596 140409829308160 logging_writer.py:48] [341400] global_step=341400, grad_norm=4.406301021575928, loss=0.6491039991378784
I0301 14:21:50.184289 140409124681472 logging_writer.py:48] [341500] global_step=341500, grad_norm=4.549914360046387, loss=0.6052008867263794
I0301 14:22:23.850981 140409829308160 logging_writer.py:48] [341600] global_step=341600, grad_norm=4.641517639160156, loss=0.686166524887085
I0301 14:22:57.561458 140409124681472 logging_writer.py:48] [341700] global_step=341700, grad_norm=4.794929504394531, loss=0.623283326625824
I0301 14:23:31.213594 140409829308160 logging_writer.py:48] [341800] global_step=341800, grad_norm=4.203075885772705, loss=0.5922803282737732
I0301 14:24:04.877840 140409124681472 logging_writer.py:48] [341900] global_step=341900, grad_norm=4.3330488204956055, loss=0.6024476289749146
I0301 14:24:38.541606 140409829308160 logging_writer.py:48] [342000] global_step=342000, grad_norm=4.5891642570495605, loss=0.6590691208839417
I0301 14:25:04.993063 140573303715648 spec.py:321] Evaluating on the training split.
I0301 14:25:11.095823 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 14:25:19.778648 140573303715648 spec.py:349] Evaluating on the test split.
I0301 14:25:22.087345 140573303715648 submission_runner.py:411] Time since start: 119446.08s, 	Step: 342080, 	{'train/accuracy': 0.9623724222183228, 'train/loss': 0.14301443099975586, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0525835752487183, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8193016052246094, 'test/num_examples': 10000, 'score': 115319.19979858398, 'total_duration': 119446.07986211777, 'accumulated_submission_time': 115319.19979858398, 'accumulated_eval_time': 4099.052008152008, 'accumulated_logging_time': 14.317641258239746}
I0301 14:25:22.162289 140409124681472 logging_writer.py:48] [342080] accumulated_eval_time=4099.052008, accumulated_logging_time=14.317641, accumulated_submission_time=115319.199799, global_step=342080, preemption_count=0, score=115319.199799, test/accuracy=0.630900, test/loss=1.819302, test/num_examples=10000, total_duration=119446.079862, train/accuracy=0.962372, train/loss=0.143014, validation/accuracy=0.755720, validation/loss=1.052584, validation/num_examples=50000
I0301 14:25:29.238137 140409846093568 logging_writer.py:48] [342100] global_step=342100, grad_norm=4.4634222984313965, loss=0.5916314125061035
I0301 14:26:02.899546 140409124681472 logging_writer.py:48] [342200] global_step=342200, grad_norm=4.281632423400879, loss=0.6005419492721558
I0301 14:26:36.539221 140409846093568 logging_writer.py:48] [342300] global_step=342300, grad_norm=4.048175811767578, loss=0.6022323966026306
I0301 14:27:10.315438 140409124681472 logging_writer.py:48] [342400] global_step=342400, grad_norm=4.547728538513184, loss=0.6602590084075928
I0301 14:27:44.016830 140409846093568 logging_writer.py:48] [342500] global_step=342500, grad_norm=4.69562292098999, loss=0.6070961952209473
I0301 14:28:17.756045 140409124681472 logging_writer.py:48] [342600] global_step=342600, grad_norm=4.5094828605651855, loss=0.5525511503219604
I0301 14:28:51.484749 140409846093568 logging_writer.py:48] [342700] global_step=342700, grad_norm=4.2066521644592285, loss=0.5947213768959045
I0301 14:29:25.170655 140409124681472 logging_writer.py:48] [342800] global_step=342800, grad_norm=4.378573417663574, loss=0.6306911110877991
I0301 14:29:58.861911 140409846093568 logging_writer.py:48] [342900] global_step=342900, grad_norm=4.19992208480835, loss=0.5722888708114624
I0301 14:30:32.634056 140409124681472 logging_writer.py:48] [343000] global_step=343000, grad_norm=4.24572229385376, loss=0.6168661117553711
I0301 14:31:06.316811 140409846093568 logging_writer.py:48] [343100] global_step=343100, grad_norm=4.6197428703308105, loss=0.5820798277854919
I0301 14:31:40.085127 140409124681472 logging_writer.py:48] [343200] global_step=343200, grad_norm=4.369152545928955, loss=0.6269767880439758
I0301 14:32:13.793251 140409846093568 logging_writer.py:48] [343300] global_step=343300, grad_norm=4.652148246765137, loss=0.6471871733665466
I0301 14:32:47.513253 140409124681472 logging_writer.py:48] [343400] global_step=343400, grad_norm=4.262645721435547, loss=0.5788396000862122
I0301 14:33:21.276201 140409846093568 logging_writer.py:48] [343500] global_step=343500, grad_norm=4.12001895904541, loss=0.5674710273742676
I0301 14:33:52.395989 140573303715648 spec.py:321] Evaluating on the training split.
I0301 14:33:58.465065 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 14:34:07.206808 140573303715648 spec.py:349] Evaluating on the test split.
I0301 14:34:09.492502 140573303715648 submission_runner.py:411] Time since start: 119973.49s, 	Step: 343594, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.1461528092622757, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0523217916488647, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8172719478607178, 'test/num_examples': 10000, 'score': 115829.36331558228, 'total_duration': 119973.48504543304, 'accumulated_submission_time': 115829.36331558228, 'accumulated_eval_time': 4116.148473501205, 'accumulated_logging_time': 14.404023885726929}
I0301 14:34:09.569203 140407379851008 logging_writer.py:48] [343594] accumulated_eval_time=4116.148474, accumulated_logging_time=14.404024, accumulated_submission_time=115829.363316, global_step=343594, preemption_count=0, score=115829.363316, test/accuracy=0.631600, test/loss=1.817272, test/num_examples=10000, total_duration=119973.485045, train/accuracy=0.960599, train/loss=0.146153, validation/accuracy=0.756080, validation/loss=1.052322, validation/num_examples=50000
I0301 14:34:12.003924 140408319375104 logging_writer.py:48] [343600] global_step=343600, grad_norm=4.209030628204346, loss=0.5993399024009705
I0301 14:34:45.672728 140407379851008 logging_writer.py:48] [343700] global_step=343700, grad_norm=4.155104160308838, loss=0.6153203845024109
I0301 14:35:19.333427 140408319375104 logging_writer.py:48] [343800] global_step=343800, grad_norm=3.9972777366638184, loss=0.5796492695808411
I0301 14:35:52.981923 140407379851008 logging_writer.py:48] [343900] global_step=343900, grad_norm=5.081767559051514, loss=0.6813366413116455
I0301 14:36:26.655212 140408319375104 logging_writer.py:48] [344000] global_step=344000, grad_norm=4.748190402984619, loss=0.6241258382797241
I0301 14:37:00.375481 140407379851008 logging_writer.py:48] [344100] global_step=344100, grad_norm=4.267460346221924, loss=0.5747606754302979
I0301 14:37:34.029258 140408319375104 logging_writer.py:48] [344200] global_step=344200, grad_norm=4.71510124206543, loss=0.7238763570785522
I0301 14:38:07.695754 140407379851008 logging_writer.py:48] [344300] global_step=344300, grad_norm=4.342386722564697, loss=0.6511878967285156
I0301 14:38:41.367389 140408319375104 logging_writer.py:48] [344400] global_step=344400, grad_norm=4.59547758102417, loss=0.5881167054176331
I0301 14:39:15.252408 140407379851008 logging_writer.py:48] [344500] global_step=344500, grad_norm=4.381102085113525, loss=0.6108332872390747
I0301 14:39:48.922072 140408319375104 logging_writer.py:48] [344600] global_step=344600, grad_norm=5.2133259773254395, loss=0.6628891825675964
I0301 14:40:22.662954 140407379851008 logging_writer.py:48] [344700] global_step=344700, grad_norm=4.326615810394287, loss=0.568466067314148
I0301 14:40:56.350922 140408319375104 logging_writer.py:48] [344800] global_step=344800, grad_norm=4.274626731872559, loss=0.6244608759880066
I0301 14:41:30.031605 140407379851008 logging_writer.py:48] [344900] global_step=344900, grad_norm=4.188465595245361, loss=0.6089575290679932
I0301 14:42:03.749778 140408319375104 logging_writer.py:48] [345000] global_step=345000, grad_norm=4.8301005363464355, loss=0.6973219513893127
I0301 14:42:37.393785 140407379851008 logging_writer.py:48] [345100] global_step=345100, grad_norm=4.129139423370361, loss=0.6568706035614014
I0301 14:42:39.559646 140573303715648 spec.py:321] Evaluating on the training split.
I0301 14:42:45.723444 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 14:42:54.117265 140573303715648 spec.py:349] Evaluating on the test split.
I0301 14:42:56.424413 140573303715648 submission_runner.py:411] Time since start: 120500.42s, 	Step: 345108, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14669817686080933, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0524567365646362, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8181381225585938, 'test/num_examples': 10000, 'score': 116339.2823779583, 'total_duration': 120500.41694951057, 'accumulated_submission_time': 116339.2823779583, 'accumulated_eval_time': 4133.013184070587, 'accumulated_logging_time': 14.49116039276123}
I0301 14:42:56.498287 140408319375104 logging_writer.py:48] [345108] accumulated_eval_time=4133.013184, accumulated_logging_time=14.491160, accumulated_submission_time=116339.282378, global_step=345108, preemption_count=0, score=116339.282378, test/accuracy=0.630400, test/loss=1.818138, test/num_examples=10000, total_duration=120500.416950, train/accuracy=0.960340, train/loss=0.146698, validation/accuracy=0.755900, validation/loss=1.052457, validation/num_examples=50000
I0301 14:43:27.793405 140409846093568 logging_writer.py:48] [345200] global_step=345200, grad_norm=4.063638687133789, loss=0.5801675915718079
I0301 14:44:01.477774 140408319375104 logging_writer.py:48] [345300] global_step=345300, grad_norm=5.00992488861084, loss=0.6105383038520813
I0301 14:44:35.151565 140409846093568 logging_writer.py:48] [345400] global_step=345400, grad_norm=4.644731521606445, loss=0.6487771272659302
I0301 14:45:08.880180 140408319375104 logging_writer.py:48] [345500] global_step=345500, grad_norm=5.159507751464844, loss=0.7002397775650024
I0301 14:45:42.674793 140409846093568 logging_writer.py:48] [345600] global_step=345600, grad_norm=4.525700092315674, loss=0.6548112630844116
I0301 14:46:16.341852 140408319375104 logging_writer.py:48] [345700] global_step=345700, grad_norm=4.300452709197998, loss=0.6536833047866821
I0301 14:46:50.052193 140409846093568 logging_writer.py:48] [345800] global_step=345800, grad_norm=4.333613872528076, loss=0.5845978260040283
I0301 14:47:23.732751 140408319375104 logging_writer.py:48] [345900] global_step=345900, grad_norm=4.70796537399292, loss=0.6449787020683289
I0301 14:47:57.451548 140409846093568 logging_writer.py:48] [346000] global_step=346000, grad_norm=4.424689292907715, loss=0.6463121175765991
I0301 14:48:31.146451 140408319375104 logging_writer.py:48] [346100] global_step=346100, grad_norm=4.519418716430664, loss=0.6407333612442017
I0301 14:49:04.853355 140409846093568 logging_writer.py:48] [346200] global_step=346200, grad_norm=4.478489398956299, loss=0.6188873648643494
I0301 14:49:38.537965 140408319375104 logging_writer.py:48] [346300] global_step=346300, grad_norm=4.290362358093262, loss=0.570466160774231
I0301 14:50:12.279976 140409846093568 logging_writer.py:48] [346400] global_step=346400, grad_norm=4.360658645629883, loss=0.6409374475479126
I0301 14:50:45.948904 140408319375104 logging_writer.py:48] [346500] global_step=346500, grad_norm=5.035429954528809, loss=0.685610294342041
I0301 14:51:19.667051 140409846093568 logging_writer.py:48] [346600] global_step=346600, grad_norm=4.474608898162842, loss=0.6554173827171326
I0301 14:51:26.565844 140573303715648 spec.py:321] Evaluating on the training split.
I0301 14:51:33.666175 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 14:51:42.313104 140573303715648 spec.py:349] Evaluating on the test split.
I0301 14:51:44.639220 140573303715648 submission_runner.py:411] Time since start: 121028.63s, 	Step: 346622, 	{'train/accuracy': 0.9592235088348389, 'train/loss': 0.148650661110878, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.051628589630127, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8176651000976562, 'test/num_examples': 10000, 'score': 116849.27911138535, 'total_duration': 121028.6317665577, 'accumulated_submission_time': 116849.27911138535, 'accumulated_eval_time': 4151.086514234543, 'accumulated_logging_time': 14.57667088508606}
I0301 14:51:44.714479 140407379851008 logging_writer.py:48] [346622] accumulated_eval_time=4151.086514, accumulated_logging_time=14.576671, accumulated_submission_time=116849.279111, global_step=346622, preemption_count=0, score=116849.279111, test/accuracy=0.631200, test/loss=1.817665, test/num_examples=10000, total_duration=121028.631767, train/accuracy=0.959224, train/loss=0.148651, validation/accuracy=0.755980, validation/loss=1.051629, validation/num_examples=50000
I0301 14:52:11.320223 140409124681472 logging_writer.py:48] [346700] global_step=346700, grad_norm=4.3016157150268555, loss=0.6363816261291504
I0301 14:52:44.987105 140407379851008 logging_writer.py:48] [346800] global_step=346800, grad_norm=4.485558986663818, loss=0.7400447726249695
I0301 14:53:18.629514 140409124681472 logging_writer.py:48] [346900] global_step=346900, grad_norm=4.712850093841553, loss=0.6257773041725159
I0301 14:53:52.390333 140407379851008 logging_writer.py:48] [347000] global_step=347000, grad_norm=5.080281734466553, loss=0.6722127199172974
I0301 14:54:26.069056 140409124681472 logging_writer.py:48] [347100] global_step=347100, grad_norm=4.744091033935547, loss=0.6383399367332458
I0301 14:54:59.832378 140407379851008 logging_writer.py:48] [347200] global_step=347200, grad_norm=4.4228386878967285, loss=0.6251157522201538
I0301 14:55:33.509119 140409124681472 logging_writer.py:48] [347300] global_step=347300, grad_norm=5.572301864624023, loss=0.7203269600868225
I0301 14:56:07.252863 140407379851008 logging_writer.py:48] [347400] global_step=347400, grad_norm=4.207958221435547, loss=0.5534323453903198
I0301 14:56:40.934617 140409124681472 logging_writer.py:48] [347500] global_step=347500, grad_norm=4.410228252410889, loss=0.6247706413269043
I0301 14:57:14.658150 140407379851008 logging_writer.py:48] [347600] global_step=347600, grad_norm=4.7326979637146, loss=0.6363331079483032
I0301 14:57:48.470306 140409124681472 logging_writer.py:48] [347700] global_step=347700, grad_norm=4.6939239501953125, loss=0.652219295501709
I0301 14:58:22.200107 140407379851008 logging_writer.py:48] [347800] global_step=347800, grad_norm=4.5447893142700195, loss=0.6381388902664185
I0301 14:58:55.893930 140409124681472 logging_writer.py:48] [347900] global_step=347900, grad_norm=4.471024513244629, loss=0.6408597230911255
I0301 14:59:29.588793 140407379851008 logging_writer.py:48] [348000] global_step=348000, grad_norm=4.752020835876465, loss=0.6142697334289551
I0301 15:00:03.252608 140409124681472 logging_writer.py:48] [348100] global_step=348100, grad_norm=4.567298889160156, loss=0.6222316026687622
I0301 15:00:14.874211 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:00:20.918180 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:00:29.638963 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:00:31.948596 140573303715648 submission_runner.py:411] Time since start: 121555.94s, 	Step: 348136, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14957855641841888, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0520284175872803, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8168818950653076, 'test/num_examples': 10000, 'score': 117359.37034726143, 'total_duration': 121555.94112229347, 'accumulated_submission_time': 117359.37034726143, 'accumulated_eval_time': 4168.160834312439, 'accumulated_logging_time': 14.661510467529297}
I0301 15:00:32.023440 140409846093568 logging_writer.py:48] [348136] accumulated_eval_time=4168.160834, accumulated_logging_time=14.661510, accumulated_submission_time=117359.370347, global_step=348136, preemption_count=0, score=117359.370347, test/accuracy=0.632400, test/loss=1.816882, test/num_examples=10000, total_duration=121555.941122, train/accuracy=0.960539, train/loss=0.149579, validation/accuracy=0.756160, validation/loss=1.052028, validation/num_examples=50000
I0301 15:00:53.941699 140409854486272 logging_writer.py:48] [348200] global_step=348200, grad_norm=4.574088096618652, loss=0.6504054665565491
I0301 15:01:27.572907 140409846093568 logging_writer.py:48] [348300] global_step=348300, grad_norm=4.743966102600098, loss=0.6549862027168274
I0301 15:02:01.288342 140409854486272 logging_writer.py:48] [348400] global_step=348400, grad_norm=5.181365013122559, loss=0.6126242280006409
I0301 15:02:34.980746 140409846093568 logging_writer.py:48] [348500] global_step=348500, grad_norm=4.5198235511779785, loss=0.5652524828910828
I0301 15:03:08.688323 140409854486272 logging_writer.py:48] [348600] global_step=348600, grad_norm=4.363583087921143, loss=0.6100288033485413
I0301 15:03:42.339437 140409846093568 logging_writer.py:48] [348700] global_step=348700, grad_norm=4.380483627319336, loss=0.6637572646141052
I0301 15:04:16.145662 140409854486272 logging_writer.py:48] [348800] global_step=348800, grad_norm=4.52902889251709, loss=0.5626559257507324
I0301 15:04:49.810822 140409846093568 logging_writer.py:48] [348900] global_step=348900, grad_norm=4.462979316711426, loss=0.5911262035369873
I0301 15:05:23.542890 140409854486272 logging_writer.py:48] [349000] global_step=349000, grad_norm=4.604465484619141, loss=0.6957942247390747
I0301 15:05:57.191021 140409846093568 logging_writer.py:48] [349100] global_step=349100, grad_norm=4.578485012054443, loss=0.616053581237793
I0301 15:06:30.873872 140409854486272 logging_writer.py:48] [349200] global_step=349200, grad_norm=4.990349769592285, loss=0.6123216152191162
I0301 15:07:04.597671 140409846093568 logging_writer.py:48] [349300] global_step=349300, grad_norm=4.194856643676758, loss=0.5846878290176392
I0301 15:07:38.282702 140409854486272 logging_writer.py:48] [349400] global_step=349400, grad_norm=4.702183723449707, loss=0.6945149898529053
I0301 15:08:12.015022 140409846093568 logging_writer.py:48] [349500] global_step=349500, grad_norm=4.2550883293151855, loss=0.6184231042861938
I0301 15:08:45.689908 140409854486272 logging_writer.py:48] [349600] global_step=349600, grad_norm=4.793116092681885, loss=0.6254562139511108
I0301 15:09:02.000040 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:09:08.181877 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:09:16.847743 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:09:19.132861 140573303715648 submission_runner.py:411] Time since start: 122083.13s, 	Step: 349650, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.1472361534833908, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0515118837356567, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8176124095916748, 'test/num_examples': 10000, 'score': 117869.27389907837, 'total_duration': 122083.12540221214, 'accumulated_submission_time': 117869.27389907837, 'accumulated_eval_time': 4185.293612003326, 'accumulated_logging_time': 14.747785091400146}
I0301 15:09:19.208467 140409829308160 logging_writer.py:48] [349650] accumulated_eval_time=4185.293612, accumulated_logging_time=14.747785, accumulated_submission_time=117869.273899, global_step=349650, preemption_count=0, score=117869.273899, test/accuracy=0.630600, test/loss=1.817612, test/num_examples=10000, total_duration=122083.125402, train/accuracy=0.960758, train/loss=0.147236, validation/accuracy=0.756080, validation/loss=1.051512, validation/num_examples=50000
I0301 15:09:36.366903 140409837700864 logging_writer.py:48] [349700] global_step=349700, grad_norm=4.912499904632568, loss=0.6470174789428711
I0301 15:10:10.152631 140409829308160 logging_writer.py:48] [349800] global_step=349800, grad_norm=4.278380870819092, loss=0.6546925902366638
I0301 15:10:43.935809 140409837700864 logging_writer.py:48] [349900] global_step=349900, grad_norm=4.841678619384766, loss=0.6288228034973145
I0301 15:11:17.600692 140409829308160 logging_writer.py:48] [350000] global_step=350000, grad_norm=4.130274295806885, loss=0.6458724737167358
I0301 15:11:51.365165 140409837700864 logging_writer.py:48] [350100] global_step=350100, grad_norm=4.641995906829834, loss=0.6203050017356873
I0301 15:12:25.016738 140409829308160 logging_writer.py:48] [350200] global_step=350200, grad_norm=4.520258903503418, loss=0.6192479729652405
I0301 15:12:58.666937 140409837700864 logging_writer.py:48] [350300] global_step=350300, grad_norm=4.3120880126953125, loss=0.6006336212158203
I0301 15:13:32.311051 140409829308160 logging_writer.py:48] [350400] global_step=350400, grad_norm=4.6807451248168945, loss=0.6377726197242737
I0301 15:14:05.950950 140409837700864 logging_writer.py:48] [350500] global_step=350500, grad_norm=4.525307655334473, loss=0.6857057213783264
I0301 15:14:39.639446 140409829308160 logging_writer.py:48] [350600] global_step=350600, grad_norm=4.395279407501221, loss=0.5853786468505859
I0301 15:15:13.378878 140409837700864 logging_writer.py:48] [350700] global_step=350700, grad_norm=4.669772624969482, loss=0.6981542110443115
I0301 15:15:47.038986 140409829308160 logging_writer.py:48] [350800] global_step=350800, grad_norm=4.847315311431885, loss=0.6026775240898132
I0301 15:16:20.824322 140409837700864 logging_writer.py:48] [350900] global_step=350900, grad_norm=4.432761192321777, loss=0.5281684398651123
I0301 15:16:54.510491 140409829308160 logging_writer.py:48] [351000] global_step=351000, grad_norm=5.0002875328063965, loss=0.590307891368866
I0301 15:17:28.287738 140409837700864 logging_writer.py:48] [351100] global_step=351100, grad_norm=4.7685546875, loss=0.6919505000114441
I0301 15:17:49.294575 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:17:55.382709 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:18:04.122738 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:18:06.454885 140573303715648 submission_runner.py:411] Time since start: 122610.45s, 	Step: 351164, 	{'train/accuracy': 0.9616150856018066, 'train/loss': 0.145010843873024, 'validation/accuracy': 0.756339967250824, 'validation/loss': 1.0514135360717773, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8179231882095337, 'test/num_examples': 10000, 'score': 118379.28754210472, 'total_duration': 122610.44742536545, 'accumulated_submission_time': 118379.28754210472, 'accumulated_eval_time': 4202.453873872757, 'accumulated_logging_time': 14.835796117782593}
I0301 15:18:06.547339 140409124681472 logging_writer.py:48] [351164] accumulated_eval_time=4202.453874, accumulated_logging_time=14.835796, accumulated_submission_time=118379.287542, global_step=351164, preemption_count=0, score=118379.287542, test/accuracy=0.631200, test/loss=1.817923, test/num_examples=10000, total_duration=122610.447425, train/accuracy=0.961615, train/loss=0.145011, validation/accuracy=0.756340, validation/loss=1.051414, validation/num_examples=50000
I0301 15:18:18.997755 140409829308160 logging_writer.py:48] [351200] global_step=351200, grad_norm=4.41301965713501, loss=0.6375850439071655
I0301 15:18:52.683433 140409124681472 logging_writer.py:48] [351300] global_step=351300, grad_norm=4.203190326690674, loss=0.6216107606887817
I0301 15:19:26.406813 140409829308160 logging_writer.py:48] [351400] global_step=351400, grad_norm=3.9222118854522705, loss=0.5627991557121277
I0301 15:20:00.089898 140409124681472 logging_writer.py:48] [351500] global_step=351500, grad_norm=4.505645751953125, loss=0.5983548760414124
I0301 15:20:33.830560 140409829308160 logging_writer.py:48] [351600] global_step=351600, grad_norm=4.602093696594238, loss=0.649759829044342
I0301 15:21:07.536956 140409124681472 logging_writer.py:48] [351700] global_step=351700, grad_norm=4.588604927062988, loss=0.5880201458930969
I0301 15:21:41.258646 140409829308160 logging_writer.py:48] [351800] global_step=351800, grad_norm=5.391940593719482, loss=0.642245888710022
I0301 15:22:15.023506 140409124681472 logging_writer.py:48] [351900] global_step=351900, grad_norm=4.54085636138916, loss=0.6144574880599976
I0301 15:22:48.710458 140409829308160 logging_writer.py:48] [352000] global_step=352000, grad_norm=4.563180446624756, loss=0.6352493166923523
I0301 15:23:22.439583 140409124681472 logging_writer.py:48] [352100] global_step=352100, grad_norm=4.693131446838379, loss=0.6009042263031006
I0301 15:23:56.195086 140409829308160 logging_writer.py:48] [352200] global_step=352200, grad_norm=4.414574146270752, loss=0.6128885746002197
I0301 15:24:29.913920 140409124681472 logging_writer.py:48] [352300] global_step=352300, grad_norm=4.340235710144043, loss=0.6127801537513733
I0301 15:25:03.659490 140409829308160 logging_writer.py:48] [352400] global_step=352400, grad_norm=4.032361030578613, loss=0.6224229335784912
I0301 15:25:37.357918 140409124681472 logging_writer.py:48] [352500] global_step=352500, grad_norm=4.450098991394043, loss=0.640358567237854
I0301 15:26:11.085937 140409829308160 logging_writer.py:48] [352600] global_step=352600, grad_norm=4.624826431274414, loss=0.676885724067688
I0301 15:26:36.456428 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:26:42.485450 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:26:51.172881 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:26:53.446625 140573303715648 submission_runner.py:411] Time since start: 123137.44s, 	Step: 352677, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14401593804359436, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.051655650138855, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8171354532241821, 'test/num_examples': 10000, 'score': 118889.12491345406, 'total_duration': 123137.43913602829, 'accumulated_submission_time': 118889.12491345406, 'accumulated_eval_time': 4219.443992614746, 'accumulated_logging_time': 14.937951803207397}
I0301 15:26:53.523407 140407379851008 logging_writer.py:48] [352677] accumulated_eval_time=4219.443993, accumulated_logging_time=14.937952, accumulated_submission_time=118889.124913, global_step=352677, preemption_count=0, score=118889.124913, test/accuracy=0.631300, test/loss=1.817135, test/num_examples=10000, total_duration=123137.439136, train/accuracy=0.961356, train/loss=0.144016, validation/accuracy=0.755580, validation/loss=1.051656, validation/num_examples=50000
I0301 15:27:01.635244 140408319375104 logging_writer.py:48] [352700] global_step=352700, grad_norm=4.3870110511779785, loss=0.6171270608901978
I0301 15:27:35.340310 140407379851008 logging_writer.py:48] [352800] global_step=352800, grad_norm=5.052059650421143, loss=0.6387629508972168
I0301 15:28:09.002746 140408319375104 logging_writer.py:48] [352900] global_step=352900, grad_norm=4.697354793548584, loss=0.6449377536773682
I0301 15:28:42.777084 140407379851008 logging_writer.py:48] [353000] global_step=353000, grad_norm=4.3702545166015625, loss=0.5848667025566101
I0301 15:29:16.429498 140408319375104 logging_writer.py:48] [353100] global_step=353100, grad_norm=5.230747699737549, loss=0.666767954826355
I0301 15:29:50.130161 140407379851008 logging_writer.py:48] [353200] global_step=353200, grad_norm=4.2932024002075195, loss=0.6538487672805786
I0301 15:30:23.852381 140408319375104 logging_writer.py:48] [353300] global_step=353300, grad_norm=4.439136981964111, loss=0.6498139500617981
I0301 15:30:57.564930 140407379851008 logging_writer.py:48] [353400] global_step=353400, grad_norm=4.389226913452148, loss=0.6757413148880005
I0301 15:31:31.257289 140408319375104 logging_writer.py:48] [353500] global_step=353500, grad_norm=4.731517314910889, loss=0.6164107322692871
I0301 15:32:04.928360 140407379851008 logging_writer.py:48] [353600] global_step=353600, grad_norm=4.747995853424072, loss=0.6503726840019226
I0301 15:32:38.657335 140408319375104 logging_writer.py:48] [353700] global_step=353700, grad_norm=4.724851131439209, loss=0.6285730004310608
I0301 15:33:12.366266 140407379851008 logging_writer.py:48] [353800] global_step=353800, grad_norm=4.366166114807129, loss=0.5864589214324951
I0301 15:33:46.026339 140408319375104 logging_writer.py:48] [353900] global_step=353900, grad_norm=4.383129596710205, loss=0.627621591091156
I0301 15:34:19.809182 140407379851008 logging_writer.py:48] [354000] global_step=354000, grad_norm=4.397693157196045, loss=0.6174764633178711
I0301 15:34:53.514454 140408319375104 logging_writer.py:48] [354100] global_step=354100, grad_norm=4.360208988189697, loss=0.5343299508094788
I0301 15:35:23.657466 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:35:29.729996 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:35:38.303786 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:35:40.577878 140573303715648 submission_runner.py:411] Time since start: 123664.57s, 	Step: 354191, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14840063452720642, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0514042377471924, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8169622421264648, 'test/num_examples': 10000, 'score': 119399.18874073029, 'total_duration': 123664.57041716576, 'accumulated_submission_time': 119399.18874073029, 'accumulated_eval_time': 4236.364371299744, 'accumulated_logging_time': 15.025042533874512}
I0301 15:35:40.666324 140407379851008 logging_writer.py:48] [354191] accumulated_eval_time=4236.364371, accumulated_logging_time=15.025043, accumulated_submission_time=119399.188741, global_step=354191, preemption_count=0, score=119399.188741, test/accuracy=0.631300, test/loss=1.816962, test/num_examples=10000, total_duration=123664.570417, train/accuracy=0.960120, train/loss=0.148401, validation/accuracy=0.755760, validation/loss=1.051404, validation/num_examples=50000
I0301 15:35:44.043507 140408319375104 logging_writer.py:48] [354200] global_step=354200, grad_norm=5.1380085945129395, loss=0.6669148206710815
I0301 15:36:17.776632 140407379851008 logging_writer.py:48] [354300] global_step=354300, grad_norm=4.698314666748047, loss=0.6160045266151428
I0301 15:36:51.478102 140408319375104 logging_writer.py:48] [354400] global_step=354400, grad_norm=4.340209484100342, loss=0.6534253358840942
I0301 15:37:25.233433 140407379851008 logging_writer.py:48] [354500] global_step=354500, grad_norm=4.799655437469482, loss=0.613820493221283
I0301 15:37:58.907709 140408319375104 logging_writer.py:48] [354600] global_step=354600, grad_norm=4.1966423988342285, loss=0.6177456378936768
I0301 15:38:32.666878 140407379851008 logging_writer.py:48] [354700] global_step=354700, grad_norm=4.648499488830566, loss=0.6426804065704346
I0301 15:39:06.372630 140408319375104 logging_writer.py:48] [354800] global_step=354800, grad_norm=4.293424129486084, loss=0.6256481409072876
I0301 15:39:40.058963 140407379851008 logging_writer.py:48] [354900] global_step=354900, grad_norm=4.924145698547363, loss=0.5889009237289429
I0301 15:40:13.774726 140408319375104 logging_writer.py:48] [355000] global_step=355000, grad_norm=4.694577217102051, loss=0.59251469373703
I0301 15:40:47.662063 140407379851008 logging_writer.py:48] [355100] global_step=355100, grad_norm=4.478397846221924, loss=0.5987237095832825
I0301 15:41:21.367934 140408319375104 logging_writer.py:48] [355200] global_step=355200, grad_norm=4.670332908630371, loss=0.7177112698554993
I0301 15:41:55.045458 140407379851008 logging_writer.py:48] [355300] global_step=355300, grad_norm=4.565176486968994, loss=0.6139531135559082
I0301 15:42:28.721431 140408319375104 logging_writer.py:48] [355400] global_step=355400, grad_norm=4.162048816680908, loss=0.5210642218589783
I0301 15:43:02.486814 140407379851008 logging_writer.py:48] [355500] global_step=355500, grad_norm=4.276988506317139, loss=0.6014159321784973
I0301 15:43:36.166898 140408319375104 logging_writer.py:48] [355600] global_step=355600, grad_norm=5.107186794281006, loss=0.630435585975647
I0301 15:44:09.930044 140407379851008 logging_writer.py:48] [355700] global_step=355700, grad_norm=4.961459159851074, loss=0.6333279609680176
I0301 15:44:10.753423 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:44:16.842282 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:44:25.574101 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:44:27.966944 140573303715648 submission_runner.py:411] Time since start: 124191.96s, 	Step: 355704, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.14975248277187347, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0527878999710083, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.818961262702942, 'test/num_examples': 10000, 'score': 119909.20295524597, 'total_duration': 124191.95948266983, 'accumulated_submission_time': 119909.20295524597, 'accumulated_eval_time': 4253.577853441238, 'accumulated_logging_time': 15.125241994857788}
I0301 15:44:28.041629 140409846093568 logging_writer.py:48] [355704] accumulated_eval_time=4253.577853, accumulated_logging_time=15.125242, accumulated_submission_time=119909.202955, global_step=355704, preemption_count=0, score=119909.202955, test/accuracy=0.630800, test/loss=1.818961, test/num_examples=10000, total_duration=124191.959483, train/accuracy=0.960300, train/loss=0.149752, validation/accuracy=0.755680, validation/loss=1.052788, validation/num_examples=50000
I0301 15:45:00.700167 140409862878976 logging_writer.py:48] [355800] global_step=355800, grad_norm=4.066808700561523, loss=0.5498690605163574
I0301 15:45:34.373030 140409846093568 logging_writer.py:48] [355900] global_step=355900, grad_norm=4.256416320800781, loss=0.6400212049484253
I0301 15:46:08.107349 140409862878976 logging_writer.py:48] [356000] global_step=356000, grad_norm=4.3825459480285645, loss=0.6149628758430481
I0301 15:46:41.901494 140409846093568 logging_writer.py:48] [356100] global_step=356100, grad_norm=4.468225479125977, loss=0.604840099811554
I0301 15:47:15.579911 140409862878976 logging_writer.py:48] [356200] global_step=356200, grad_norm=4.874779224395752, loss=0.652605414390564
I0301 15:47:49.272996 140409846093568 logging_writer.py:48] [356300] global_step=356300, grad_norm=4.903908729553223, loss=0.7398530840873718
I0301 15:48:22.968503 140409862878976 logging_writer.py:48] [356400] global_step=356400, grad_norm=5.659603595733643, loss=0.7001127600669861
I0301 15:48:56.635759 140409846093568 logging_writer.py:48] [356500] global_step=356500, grad_norm=4.633612155914307, loss=0.603583812713623
I0301 15:49:30.361093 140409862878976 logging_writer.py:48] [356600] global_step=356600, grad_norm=4.301537990570068, loss=0.6196423172950745
I0301 15:50:04.054759 140409846093568 logging_writer.py:48] [356700] global_step=356700, grad_norm=4.762570381164551, loss=0.5553296804428101
I0301 15:50:37.769925 140409862878976 logging_writer.py:48] [356800] global_step=356800, grad_norm=4.3487114906311035, loss=0.6408736109733582
I0301 15:51:11.471612 140409846093568 logging_writer.py:48] [356900] global_step=356900, grad_norm=4.109541893005371, loss=0.6421410441398621
I0301 15:51:45.145820 140409862878976 logging_writer.py:48] [357000] global_step=357000, grad_norm=4.4731645584106445, loss=0.6242323517799377
I0301 15:52:18.901492 140409846093568 logging_writer.py:48] [357100] global_step=357100, grad_norm=4.2539801597595215, loss=0.6078417897224426
I0301 15:52:52.675806 140409862878976 logging_writer.py:48] [357200] global_step=357200, grad_norm=5.049681186676025, loss=0.6536448001861572
I0301 15:52:58.221917 140573303715648 spec.py:321] Evaluating on the training split.
I0301 15:53:04.462876 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 15:53:13.072249 140573303715648 spec.py:349] Evaluating on the test split.
I0301 15:53:15.382271 140573303715648 submission_runner.py:411] Time since start: 124719.37s, 	Step: 357218, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.1427268236875534, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0533047914505005, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8208389282226562, 'test/num_examples': 10000, 'score': 120419.31431365013, 'total_duration': 124719.37481164932, 'accumulated_submission_time': 120419.31431365013, 'accumulated_eval_time': 4270.738157272339, 'accumulated_logging_time': 15.209940195083618}
I0301 15:53:15.458604 140407371458304 logging_writer.py:48] [357218] accumulated_eval_time=4270.738157, accumulated_logging_time=15.209940, accumulated_submission_time=120419.314314, global_step=357218, preemption_count=0, score=120419.314314, test/accuracy=0.631200, test/loss=1.820839, test/num_examples=10000, total_duration=124719.374812, train/accuracy=0.961416, train/loss=0.142727, validation/accuracy=0.755700, validation/loss=1.053305, validation/num_examples=50000
I0301 15:53:43.406058 140407379851008 logging_writer.py:48] [357300] global_step=357300, grad_norm=4.603365898132324, loss=0.6570854187011719
I0301 15:54:17.058444 140407371458304 logging_writer.py:48] [357400] global_step=357400, grad_norm=4.2994890213012695, loss=0.5785403847694397
I0301 15:54:50.761804 140407379851008 logging_writer.py:48] [357500] global_step=357500, grad_norm=4.420279026031494, loss=0.5786621570587158
I0301 15:55:24.450207 140407371458304 logging_writer.py:48] [357600] global_step=357600, grad_norm=4.694610118865967, loss=0.6146317720413208
I0301 15:55:58.146499 140407379851008 logging_writer.py:48] [357700] global_step=357700, grad_norm=4.222198963165283, loss=0.604404866695404
I0301 15:56:31.834879 140407371458304 logging_writer.py:48] [357800] global_step=357800, grad_norm=4.324527263641357, loss=0.5880522131919861
I0301 15:57:05.475962 140407379851008 logging_writer.py:48] [357900] global_step=357900, grad_norm=4.190910339355469, loss=0.6749266982078552
I0301 15:57:39.247873 140407371458304 logging_writer.py:48] [358000] global_step=358000, grad_norm=4.982807159423828, loss=0.6843831539154053
I0301 15:58:12.927032 140407379851008 logging_writer.py:48] [358100] global_step=358100, grad_norm=4.686878204345703, loss=0.6493144035339355
I0301 15:58:46.697060 140407371458304 logging_writer.py:48] [358200] global_step=358200, grad_norm=4.6506476402282715, loss=0.6034636497497559
I0301 15:59:20.520929 140407379851008 logging_writer.py:48] [358300] global_step=358300, grad_norm=4.415181636810303, loss=0.6329632997512817
I0301 15:59:54.202749 140407371458304 logging_writer.py:48] [358400] global_step=358400, grad_norm=4.071156024932861, loss=0.5721752047538757
I0301 16:00:27.897399 140407379851008 logging_writer.py:48] [358500] global_step=358500, grad_norm=4.776544094085693, loss=0.6756001710891724
I0301 16:01:01.604738 140407371458304 logging_writer.py:48] [358600] global_step=358600, grad_norm=4.602328777313232, loss=0.668175458908081
I0301 16:01:35.283793 140407379851008 logging_writer.py:48] [358700] global_step=358700, grad_norm=4.841376781463623, loss=0.6091344356536865
I0301 16:01:45.561226 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:01:51.636346 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:02:00.335155 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:02:02.642596 140573303715648 submission_runner.py:411] Time since start: 125246.64s, 	Step: 358732, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14587090909481049, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.052885890007019, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8178112506866455, 'test/num_examples': 10000, 'score': 120929.3461754322, 'total_duration': 125246.63512897491, 'accumulated_submission_time': 120929.3461754322, 'accumulated_eval_time': 4287.819470405579, 'accumulated_logging_time': 15.297495603561401}
I0301 16:02:02.724613 140409846093568 logging_writer.py:48] [358732] accumulated_eval_time=4287.819470, accumulated_logging_time=15.297496, accumulated_submission_time=120929.346175, global_step=358732, preemption_count=0, score=120929.346175, test/accuracy=0.631800, test/loss=1.817811, test/num_examples=10000, total_duration=125246.635129, train/accuracy=0.960977, train/loss=0.145871, validation/accuracy=0.755360, validation/loss=1.052886, validation/num_examples=50000
I0301 16:02:25.975428 140409854486272 logging_writer.py:48] [358800] global_step=358800, grad_norm=4.990253448486328, loss=0.7534279227256775
I0301 16:02:59.612706 140409846093568 logging_writer.py:48] [358900] global_step=358900, grad_norm=4.984439373016357, loss=0.7302675843238831
I0301 16:03:33.312532 140409854486272 logging_writer.py:48] [359000] global_step=359000, grad_norm=4.461507320404053, loss=0.5596358180046082
I0301 16:04:06.971244 140409846093568 logging_writer.py:48] [359100] global_step=359100, grad_norm=4.059849739074707, loss=0.5836620926856995
I0301 16:04:40.657014 140409854486272 logging_writer.py:48] [359200] global_step=359200, grad_norm=4.590666770935059, loss=0.6139101386070251
I0301 16:05:14.448228 140409846093568 logging_writer.py:48] [359300] global_step=359300, grad_norm=4.668152809143066, loss=0.6885524392127991
I0301 16:05:48.168972 140409854486272 logging_writer.py:48] [359400] global_step=359400, grad_norm=4.1805338859558105, loss=0.5825091004371643
I0301 16:06:21.811207 140409846093568 logging_writer.py:48] [359500] global_step=359500, grad_norm=4.407433986663818, loss=0.6533745527267456
I0301 16:06:55.494985 140409854486272 logging_writer.py:48] [359600] global_step=359600, grad_norm=4.206875801086426, loss=0.6128408312797546
I0301 16:07:29.122253 140409846093568 logging_writer.py:48] [359700] global_step=359700, grad_norm=4.497953414916992, loss=0.5834128856658936
I0301 16:08:02.787748 140409854486272 logging_writer.py:48] [359800] global_step=359800, grad_norm=4.388857364654541, loss=0.6351589560508728
I0301 16:08:36.536166 140409846093568 logging_writer.py:48] [359900] global_step=359900, grad_norm=4.646316051483154, loss=0.6111742258071899
I0301 16:09:10.237346 140409854486272 logging_writer.py:48] [360000] global_step=360000, grad_norm=4.8234758377075195, loss=0.6263282895088196
I0301 16:09:43.946894 140409846093568 logging_writer.py:48] [360100] global_step=360100, grad_norm=4.461792469024658, loss=0.5913867354393005
I0301 16:10:17.612549 140409854486272 logging_writer.py:48] [360200] global_step=360200, grad_norm=5.2288665771484375, loss=0.7896460890769958
I0301 16:10:32.892957 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:10:38.988767 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:10:47.625266 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:10:49.921171 140573303715648 submission_runner.py:411] Time since start: 125773.91s, 	Step: 360247, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.14724838733673096, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0523381233215332, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.817411184310913, 'test/num_examples': 10000, 'score': 121439.44315695763, 'total_duration': 125773.91371035576, 'accumulated_submission_time': 121439.44315695763, 'accumulated_eval_time': 4304.847638845444, 'accumulated_logging_time': 15.390103816986084}
I0301 16:10:50.001147 140409124681472 logging_writer.py:48] [360247] accumulated_eval_time=4304.847639, accumulated_logging_time=15.390104, accumulated_submission_time=121439.443157, global_step=360247, preemption_count=0, score=121439.443157, test/accuracy=0.631800, test/loss=1.817411, test/num_examples=10000, total_duration=125773.913710, train/accuracy=0.960300, train/loss=0.147248, validation/accuracy=0.755960, validation/loss=1.052338, validation/num_examples=50000
I0301 16:11:08.250843 140409829308160 logging_writer.py:48] [360300] global_step=360300, grad_norm=4.269696235656738, loss=0.5740687251091003
I0301 16:11:41.975966 140409124681472 logging_writer.py:48] [360400] global_step=360400, grad_norm=4.239506244659424, loss=0.5718123316764832
I0301 16:12:15.671357 140409829308160 logging_writer.py:48] [360500] global_step=360500, grad_norm=4.582273960113525, loss=0.6132977604866028
I0301 16:12:49.366373 140409124681472 logging_writer.py:48] [360600] global_step=360600, grad_norm=4.363186836242676, loss=0.5695619583129883
I0301 16:13:23.087416 140409829308160 logging_writer.py:48] [360700] global_step=360700, grad_norm=4.2516560554504395, loss=0.6045358180999756
I0301 16:13:56.782508 140409124681472 logging_writer.py:48] [360800] global_step=360800, grad_norm=4.461680889129639, loss=0.6844054460525513
I0301 16:14:30.486273 140409829308160 logging_writer.py:48] [360900] global_step=360900, grad_norm=3.8667211532592773, loss=0.5321358442306519
I0301 16:15:04.201952 140409124681472 logging_writer.py:48] [361000] global_step=361000, grad_norm=4.381277084350586, loss=0.6175068616867065
I0301 16:15:37.903463 140409829308160 logging_writer.py:48] [361100] global_step=361100, grad_norm=4.406924724578857, loss=0.6645669937133789
I0301 16:16:11.626125 140409124681472 logging_writer.py:48] [361200] global_step=361200, grad_norm=4.632650375366211, loss=0.6459954977035522
I0301 16:16:45.339779 140409829308160 logging_writer.py:48] [361300] global_step=361300, grad_norm=4.499477386474609, loss=0.653304398059845
I0301 16:17:19.087750 140409124681472 logging_writer.py:48] [361400] global_step=361400, grad_norm=4.0763397216796875, loss=0.5830457210540771
I0301 16:17:52.769396 140409829308160 logging_writer.py:48] [361500] global_step=361500, grad_norm=4.485312461853027, loss=0.6250172853469849
I0301 16:18:26.490890 140409124681472 logging_writer.py:48] [361600] global_step=361600, grad_norm=5.117594242095947, loss=0.7208280563354492
I0301 16:19:00.210851 140409829308160 logging_writer.py:48] [361700] global_step=361700, grad_norm=4.551730155944824, loss=0.6562269926071167
I0301 16:19:20.233623 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:19:26.388709 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:19:35.084325 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:19:37.389397 140573303715648 submission_runner.py:411] Time since start: 126301.38s, 	Step: 361761, 	{'train/accuracy': 0.9618144035339355, 'train/loss': 0.1445937156677246, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0517603158950806, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.817147135734558, 'test/num_examples': 10000, 'score': 121949.60582256317, 'total_duration': 126301.38193583488, 'accumulated_submission_time': 121949.60582256317, 'accumulated_eval_time': 4322.003366947174, 'accumulated_logging_time': 15.480299949645996}
I0301 16:19:37.468329 140409846093568 logging_writer.py:48] [361761] accumulated_eval_time=4322.003367, accumulated_logging_time=15.480300, accumulated_submission_time=121949.605823, global_step=361761, preemption_count=0, score=121949.605823, test/accuracy=0.631500, test/loss=1.817147, test/num_examples=10000, total_duration=126301.381936, train/accuracy=0.961814, train/loss=0.144594, validation/accuracy=0.755460, validation/loss=1.051760, validation/num_examples=50000
I0301 16:19:50.979389 140409854486272 logging_writer.py:48] [361800] global_step=361800, grad_norm=4.8467817306518555, loss=0.6305413842201233
I0301 16:20:24.686646 140409846093568 logging_writer.py:48] [361900] global_step=361900, grad_norm=4.539757251739502, loss=0.6632789969444275
I0301 16:20:58.313023 140409854486272 logging_writer.py:48] [362000] global_step=362000, grad_norm=4.582213401794434, loss=0.6097942590713501
I0301 16:21:31.981096 140409846093568 logging_writer.py:48] [362100] global_step=362100, grad_norm=4.316871643066406, loss=0.5850268602371216
I0301 16:22:05.705942 140409854486272 logging_writer.py:48] [362200] global_step=362200, grad_norm=4.740303993225098, loss=0.6028710603713989
I0301 16:22:39.371587 140409846093568 logging_writer.py:48] [362300] global_step=362300, grad_norm=4.037151336669922, loss=0.6214940547943115
I0301 16:23:13.005662 140409854486272 logging_writer.py:48] [362400] global_step=362400, grad_norm=4.563975811004639, loss=0.5734763741493225
I0301 16:23:46.799661 140409846093568 logging_writer.py:48] [362500] global_step=362500, grad_norm=4.310306549072266, loss=0.6189777851104736
I0301 16:24:20.444468 140409854486272 logging_writer.py:48] [362600] global_step=362600, grad_norm=4.619734287261963, loss=0.6054006814956665
I0301 16:24:54.123656 140409846093568 logging_writer.py:48] [362700] global_step=362700, grad_norm=4.25712251663208, loss=0.5113306045532227
I0301 16:25:27.863681 140409854486272 logging_writer.py:48] [362800] global_step=362800, grad_norm=5.547206401824951, loss=0.6416208744049072
I0301 16:26:01.566410 140409846093568 logging_writer.py:48] [362900] global_step=362900, grad_norm=4.6786065101623535, loss=0.6662840843200684
I0301 16:26:35.283846 140409854486272 logging_writer.py:48] [363000] global_step=363000, grad_norm=4.394500732421875, loss=0.594369113445282
I0301 16:27:09.002135 140409846093568 logging_writer.py:48] [363100] global_step=363100, grad_norm=4.313716411590576, loss=0.6044897437095642
I0301 16:27:42.721415 140409854486272 logging_writer.py:48] [363200] global_step=363200, grad_norm=4.558991432189941, loss=0.5601856112480164
I0301 16:28:07.450759 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:28:13.550343 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:28:22.257753 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:28:24.565644 140573303715648 submission_runner.py:411] Time since start: 126828.56s, 	Step: 363275, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.1499248892068863, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0522288084030151, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8169174194335938, 'test/num_examples': 10000, 'score': 122459.51914787292, 'total_duration': 126828.55815386772, 'accumulated_submission_time': 122459.51914787292, 'accumulated_eval_time': 4339.118178367615, 'accumulated_logging_time': 15.568543672561646}
I0301 16:28:24.642417 140407379851008 logging_writer.py:48] [363275] accumulated_eval_time=4339.118178, accumulated_logging_time=15.568544, accumulated_submission_time=122459.519148, global_step=363275, preemption_count=0, score=122459.519148, test/accuracy=0.630800, test/loss=1.816917, test/num_examples=10000, total_duration=126828.558154, train/accuracy=0.959762, train/loss=0.149925, validation/accuracy=0.755640, validation/loss=1.052229, validation/num_examples=50000
I0301 16:28:33.422140 140408319375104 logging_writer.py:48] [363300] global_step=363300, grad_norm=4.500148296356201, loss=0.5975903868675232
I0301 16:29:07.178159 140407379851008 logging_writer.py:48] [363400] global_step=363400, grad_norm=4.647363185882568, loss=0.6388413310050964
I0301 16:29:40.964361 140408319375104 logging_writer.py:48] [363500] global_step=363500, grad_norm=4.115897178649902, loss=0.585777759552002
I0301 16:30:14.648447 140407379851008 logging_writer.py:48] [363600] global_step=363600, grad_norm=4.584161758422852, loss=0.5861712098121643
I0301 16:30:48.377744 140408319375104 logging_writer.py:48] [363700] global_step=363700, grad_norm=4.192188262939453, loss=0.5171922445297241
I0301 16:31:22.060023 140407379851008 logging_writer.py:48] [363800] global_step=363800, grad_norm=4.574685573577881, loss=0.5879194736480713
I0301 16:31:55.785241 140408319375104 logging_writer.py:48] [363900] global_step=363900, grad_norm=4.3717193603515625, loss=0.5967898368835449
I0301 16:32:29.480785 140407379851008 logging_writer.py:48] [364000] global_step=364000, grad_norm=4.197953701019287, loss=0.5844399929046631
I0301 16:33:03.197423 140408319375104 logging_writer.py:48] [364100] global_step=364100, grad_norm=4.321238994598389, loss=0.5911688804626465
I0301 16:33:36.906614 140407379851008 logging_writer.py:48] [364200] global_step=364200, grad_norm=4.506852626800537, loss=0.5816389322280884
I0301 16:34:10.625454 140408319375104 logging_writer.py:48] [364300] global_step=364300, grad_norm=4.411428451538086, loss=0.6281591653823853
I0301 16:34:44.331596 140407379851008 logging_writer.py:48] [364400] global_step=364400, grad_norm=4.385880947113037, loss=0.6258209347724915
I0301 16:35:18.040953 140408319375104 logging_writer.py:48] [364500] global_step=364500, grad_norm=5.126035690307617, loss=0.7028506994247437
I0301 16:35:51.860760 140407379851008 logging_writer.py:48] [364600] global_step=364600, grad_norm=4.811208248138428, loss=0.6104536056518555
I0301 16:36:25.538873 140408319375104 logging_writer.py:48] [364700] global_step=364700, grad_norm=4.7754082679748535, loss=0.6498987078666687
I0301 16:36:54.663933 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:37:00.734643 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:37:09.398976 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:37:11.938897 140573303715648 submission_runner.py:411] Time since start: 127355.93s, 	Step: 364788, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14528235793113708, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.051038384437561, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.817543625831604, 'test/num_examples': 10000, 'score': 122969.4705851078, 'total_duration': 127355.93138194084, 'accumulated_submission_time': 122969.4705851078, 'accumulated_eval_time': 4356.393066167831, 'accumulated_logging_time': 15.656342029571533}
I0301 16:37:12.022620 140408319375104 logging_writer.py:48] [364788] accumulated_eval_time=4356.393066, accumulated_logging_time=15.656342, accumulated_submission_time=122969.470585, global_step=364788, preemption_count=0, score=122969.470585, test/accuracy=0.632500, test/loss=1.817544, test/num_examples=10000, total_duration=127355.931382, train/accuracy=0.961137, train/loss=0.145282, validation/accuracy=0.756100, validation/loss=1.051038, validation/num_examples=50000
I0301 16:37:16.403438 140409846093568 logging_writer.py:48] [364800] global_step=364800, grad_norm=4.175827980041504, loss=0.6141582727432251
I0301 16:37:50.039328 140408319375104 logging_writer.py:48] [364900] global_step=364900, grad_norm=4.287337303161621, loss=0.5949079990386963
I0301 16:38:23.718260 140409846093568 logging_writer.py:48] [365000] global_step=365000, grad_norm=4.742359161376953, loss=0.6501781344413757
I0301 16:38:57.371979 140408319375104 logging_writer.py:48] [365100] global_step=365100, grad_norm=4.389524936676025, loss=0.5984640121459961
I0301 16:39:31.093003 140409846093568 logging_writer.py:48] [365200] global_step=365200, grad_norm=4.206289768218994, loss=0.634792685508728
I0301 16:40:04.768444 140408319375104 logging_writer.py:48] [365300] global_step=365300, grad_norm=4.4266438484191895, loss=0.648250937461853
I0301 16:40:38.503235 140409846093568 logging_writer.py:48] [365400] global_step=365400, grad_norm=4.872704982757568, loss=0.6621293425559998
I0301 16:41:12.208076 140408319375104 logging_writer.py:48] [365500] global_step=365500, grad_norm=5.051167964935303, loss=0.6107668876647949
I0301 16:41:46.037755 140409846093568 logging_writer.py:48] [365600] global_step=365600, grad_norm=4.915196418762207, loss=0.6139441132545471
I0301 16:42:19.738870 140408319375104 logging_writer.py:48] [365700] global_step=365700, grad_norm=4.833305835723877, loss=0.7361661195755005
I0301 16:42:53.454599 140409846093568 logging_writer.py:48] [365800] global_step=365800, grad_norm=4.787035942077637, loss=0.664995551109314
I0301 16:43:27.132154 140408319375104 logging_writer.py:48] [365900] global_step=365900, grad_norm=4.3461103439331055, loss=0.5608481764793396
I0301 16:44:00.806797 140409846093568 logging_writer.py:48] [366000] global_step=366000, grad_norm=4.292316436767578, loss=0.6191390752792358
I0301 16:44:34.540556 140408319375104 logging_writer.py:48] [366100] global_step=366100, grad_norm=4.896265029907227, loss=0.6421723961830139
I0301 16:45:08.220071 140409846093568 logging_writer.py:48] [366200] global_step=366200, grad_norm=4.281407356262207, loss=0.667037844657898
I0301 16:45:41.956510 140408319375104 logging_writer.py:48] [366300] global_step=366300, grad_norm=4.35345458984375, loss=0.6247623562812805
I0301 16:45:41.964109 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:45:48.046278 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:45:56.608641 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:45:58.905884 140573303715648 submission_runner.py:411] Time since start: 127882.90s, 	Step: 366301, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14806565642356873, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0525013208389282, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.818870186805725, 'test/num_examples': 10000, 'score': 123479.34378123283, 'total_duration': 127882.89841532707, 'accumulated_submission_time': 123479.34378123283, 'accumulated_eval_time': 4373.334760904312, 'accumulated_logging_time': 15.750553607940674}
I0301 16:45:58.981539 140409124681472 logging_writer.py:48] [366301] accumulated_eval_time=4373.334761, accumulated_logging_time=15.750554, accumulated_submission_time=123479.343781, global_step=366301, preemption_count=0, score=123479.343781, test/accuracy=0.631700, test/loss=1.818870, test/num_examples=10000, total_duration=127882.898415, train/accuracy=0.960658, train/loss=0.148066, validation/accuracy=0.755820, validation/loss=1.052501, validation/num_examples=50000
I0301 16:46:32.654859 140409829308160 logging_writer.py:48] [366400] global_step=366400, grad_norm=4.637321472167969, loss=0.6471917629241943
I0301 16:47:06.364224 140409124681472 logging_writer.py:48] [366500] global_step=366500, grad_norm=4.7787580490112305, loss=0.6952645182609558
I0301 16:47:40.100897 140409829308160 logging_writer.py:48] [366600] global_step=366600, grad_norm=4.530472755432129, loss=0.6516846418380737
I0301 16:48:13.900439 140409124681472 logging_writer.py:48] [366700] global_step=366700, grad_norm=4.706754207611084, loss=0.5917104482650757
I0301 16:48:47.615813 140409829308160 logging_writer.py:48] [366800] global_step=366800, grad_norm=4.290656089782715, loss=0.6403934955596924
I0301 16:49:21.356750 140409124681472 logging_writer.py:48] [366900] global_step=366900, grad_norm=4.24708366394043, loss=0.6468747854232788
I0301 16:49:55.050445 140409829308160 logging_writer.py:48] [367000] global_step=367000, grad_norm=4.235690116882324, loss=0.5645298957824707
I0301 16:50:28.820836 140409124681472 logging_writer.py:48] [367100] global_step=367100, grad_norm=4.6780571937561035, loss=0.7084996700286865
I0301 16:51:02.542316 140409829308160 logging_writer.py:48] [367200] global_step=367200, grad_norm=4.919632434844971, loss=0.5909079909324646
I0301 16:51:36.243422 140409124681472 logging_writer.py:48] [367300] global_step=367300, grad_norm=4.207513809204102, loss=0.5747590661048889
I0301 16:52:09.977976 140409829308160 logging_writer.py:48] [367400] global_step=367400, grad_norm=4.557965278625488, loss=0.592278003692627
I0301 16:52:43.682741 140409124681472 logging_writer.py:48] [367500] global_step=367500, grad_norm=4.164173126220703, loss=0.5970778465270996
I0301 16:53:17.400683 140409829308160 logging_writer.py:48] [367600] global_step=367600, grad_norm=4.789641857147217, loss=0.5909263491630554
I0301 16:53:51.090906 140409124681472 logging_writer.py:48] [367700] global_step=367700, grad_norm=4.886181354522705, loss=0.6761022210121155
I0301 16:54:24.892718 140409829308160 logging_writer.py:48] [367800] global_step=367800, grad_norm=4.544517517089844, loss=0.6066071391105652
I0301 16:54:29.079193 140573303715648 spec.py:321] Evaluating on the training split.
I0301 16:54:35.142334 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 16:54:43.909636 140573303715648 spec.py:349] Evaluating on the test split.
I0301 16:54:46.241391 140573303715648 submission_runner.py:411] Time since start: 128410.23s, 	Step: 367814, 	{'train/accuracy': 0.9611567258834839, 'train/loss': 0.1470474749803543, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0533629655838013, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8194811344146729, 'test/num_examples': 10000, 'score': 123989.37150287628, 'total_duration': 128410.23392796516, 'accumulated_submission_time': 123989.37150287628, 'accumulated_eval_time': 4390.496916055679, 'accumulated_logging_time': 15.836224555969238}
I0301 16:54:46.321474 140408319375104 logging_writer.py:48] [367814] accumulated_eval_time=4390.496916, accumulated_logging_time=15.836225, accumulated_submission_time=123989.371503, global_step=367814, preemption_count=0, score=123989.371503, test/accuracy=0.631500, test/loss=1.819481, test/num_examples=10000, total_duration=128410.233928, train/accuracy=0.961157, train/loss=0.147047, validation/accuracy=0.755660, validation/loss=1.053363, validation/num_examples=50000
I0301 16:55:15.652061 140409124681472 logging_writer.py:48] [367900] global_step=367900, grad_norm=4.783140182495117, loss=0.5845348238945007
I0301 16:55:49.331565 140408319375104 logging_writer.py:48] [368000] global_step=368000, grad_norm=4.609858512878418, loss=0.6695611476898193
I0301 16:56:22.975373 140409124681472 logging_writer.py:48] [368100] global_step=368100, grad_norm=4.758347511291504, loss=0.6777728199958801
I0301 16:56:56.651494 140408319375104 logging_writer.py:48] [368200] global_step=368200, grad_norm=4.383428573608398, loss=0.6077397465705872
I0301 16:57:30.363508 140409124681472 logging_writer.py:48] [368300] global_step=368300, grad_norm=4.632511138916016, loss=0.7332162261009216
I0301 16:58:04.026499 140408319375104 logging_writer.py:48] [368400] global_step=368400, grad_norm=4.517886161804199, loss=0.6180725693702698
I0301 16:58:37.753365 140409124681472 logging_writer.py:48] [368500] global_step=368500, grad_norm=4.411027908325195, loss=0.6231297850608826
I0301 16:59:11.445396 140408319375104 logging_writer.py:48] [368600] global_step=368600, grad_norm=4.235428810119629, loss=0.5718005299568176
I0301 16:59:45.120122 140409124681472 logging_writer.py:48] [368700] global_step=368700, grad_norm=4.66977596282959, loss=0.6229739189147949
I0301 17:00:18.909306 140408319375104 logging_writer.py:48] [368800] global_step=368800, grad_norm=4.482003688812256, loss=0.6845179796218872
I0301 17:00:52.632170 140409124681472 logging_writer.py:48] [368900] global_step=368900, grad_norm=4.435003280639648, loss=0.565098226070404
I0301 17:01:26.309214 140408319375104 logging_writer.py:48] [369000] global_step=369000, grad_norm=4.208431720733643, loss=0.6513623595237732
I0301 17:01:59.984953 140409124681472 logging_writer.py:48] [369100] global_step=369100, grad_norm=4.268615245819092, loss=0.6498075127601624
I0301 17:02:33.660194 140408319375104 logging_writer.py:48] [369200] global_step=369200, grad_norm=5.046116352081299, loss=0.6827356815338135
I0301 17:03:07.385696 140409124681472 logging_writer.py:48] [369300] global_step=369300, grad_norm=5.148359298706055, loss=0.6499361991882324
I0301 17:03:16.285866 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:03:22.399965 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:03:31.045659 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:03:33.372584 140573303715648 submission_runner.py:411] Time since start: 128937.37s, 	Step: 369328, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14822474122047424, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0521869659423828, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8174362182617188, 'test/num_examples': 10000, 'score': 124499.26441526413, 'total_duration': 128937.36512541771, 'accumulated_submission_time': 124499.26441526413, 'accumulated_eval_time': 4407.583589076996, 'accumulated_logging_time': 15.926433086395264}
I0301 17:03:33.450602 140407379851008 logging_writer.py:48] [369328] accumulated_eval_time=4407.583589, accumulated_logging_time=15.926433, accumulated_submission_time=124499.264415, global_step=369328, preemption_count=0, score=124499.264415, test/accuracy=0.631200, test/loss=1.817436, test/num_examples=10000, total_duration=128937.365125, train/accuracy=0.960698, train/loss=0.148225, validation/accuracy=0.755860, validation/loss=1.052187, validation/num_examples=50000
I0301 17:03:58.037817 140408319375104 logging_writer.py:48] [369400] global_step=369400, grad_norm=4.09110689163208, loss=0.5936095714569092
I0301 17:04:31.743708 140407379851008 logging_writer.py:48] [369500] global_step=369500, grad_norm=4.716769218444824, loss=0.6513841152191162
I0301 17:05:05.413889 140408319375104 logging_writer.py:48] [369600] global_step=369600, grad_norm=4.4986114501953125, loss=0.6376868486404419
I0301 17:05:39.150217 140407379851008 logging_writer.py:48] [369700] global_step=369700, grad_norm=4.250036716461182, loss=0.5655186176300049
I0301 17:06:12.853933 140408319375104 logging_writer.py:48] [369800] global_step=369800, grad_norm=4.803267955780029, loss=0.6591139435768127
I0301 17:06:46.752774 140407379851008 logging_writer.py:48] [369900] global_step=369900, grad_norm=4.9525580406188965, loss=0.7046741247177124
I0301 17:07:20.445372 140408319375104 logging_writer.py:48] [370000] global_step=370000, grad_norm=4.061024188995361, loss=0.547663152217865
I0301 17:07:54.195534 140407379851008 logging_writer.py:48] [370100] global_step=370100, grad_norm=4.682063102722168, loss=0.6579546928405762
I0301 17:08:27.882094 140408319375104 logging_writer.py:48] [370200] global_step=370200, grad_norm=4.615840911865234, loss=0.6271769404411316
I0301 17:09:01.557851 140407379851008 logging_writer.py:48] [370300] global_step=370300, grad_norm=4.203744411468506, loss=0.6132362484931946
I0301 17:09:35.324012 140408319375104 logging_writer.py:48] [370400] global_step=370400, grad_norm=4.793957710266113, loss=0.6677950620651245
I0301 17:10:09.062651 140407379851008 logging_writer.py:48] [370500] global_step=370500, grad_norm=4.567541122436523, loss=0.5776358246803284
I0301 17:10:42.785518 140408319375104 logging_writer.py:48] [370600] global_step=370600, grad_norm=4.622304439544678, loss=0.5998662710189819
I0301 17:11:16.518274 140407379851008 logging_writer.py:48] [370700] global_step=370700, grad_norm=4.713202476501465, loss=0.608489990234375
I0301 17:11:50.192789 140408319375104 logging_writer.py:48] [370800] global_step=370800, grad_norm=4.599248886108398, loss=0.6173185110092163
I0301 17:12:03.468170 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:12:09.514655 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:12:18.069614 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:12:20.385973 140573303715648 submission_runner.py:411] Time since start: 129464.38s, 	Step: 370841, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14935502409934998, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0532011985778809, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8188133239746094, 'test/num_examples': 10000, 'score': 125009.21130037308, 'total_duration': 129464.3785161972, 'accumulated_submission_time': 125009.21130037308, 'accumulated_eval_time': 4424.501345396042, 'accumulated_logging_time': 16.016053199768066}
I0301 17:12:20.465336 140409854486272 logging_writer.py:48] [370841] accumulated_eval_time=4424.501345, accumulated_logging_time=16.016053, accumulated_submission_time=125009.211300, global_step=370841, preemption_count=0, score=125009.211300, test/accuracy=0.631600, test/loss=1.818813, test/num_examples=10000, total_duration=129464.378516, train/accuracy=0.960599, train/loss=0.149355, validation/accuracy=0.755440, validation/loss=1.053201, validation/num_examples=50000
I0301 17:12:40.750709 140409862878976 logging_writer.py:48] [370900] global_step=370900, grad_norm=4.303918361663818, loss=0.6387354731559753
I0301 17:13:14.415364 140409854486272 logging_writer.py:48] [371000] global_step=371000, grad_norm=4.309173583984375, loss=0.5932120084762573
I0301 17:13:48.165446 140409862878976 logging_writer.py:48] [371100] global_step=371100, grad_norm=4.676470756530762, loss=0.6474094390869141
I0301 17:14:21.819855 140409854486272 logging_writer.py:48] [371200] global_step=371200, grad_norm=4.603329658508301, loss=0.6704190373420715
I0301 17:14:55.584876 140409862878976 logging_writer.py:48] [371300] global_step=371300, grad_norm=4.530157089233398, loss=0.6509588956832886
I0301 17:15:29.263168 140409854486272 logging_writer.py:48] [371400] global_step=371400, grad_norm=4.1088080406188965, loss=0.6375317573547363
I0301 17:16:02.887829 140409862878976 logging_writer.py:48] [371500] global_step=371500, grad_norm=4.6990885734558105, loss=0.6722474694252014
I0301 17:16:36.560860 140409854486272 logging_writer.py:48] [371600] global_step=371600, grad_norm=4.365899562835693, loss=0.5921976566314697
I0301 17:17:10.313566 140409862878976 logging_writer.py:48] [371700] global_step=371700, grad_norm=4.5386962890625, loss=0.6058599948883057
I0301 17:17:43.999961 140409854486272 logging_writer.py:48] [371800] global_step=371800, grad_norm=5.077948093414307, loss=0.7494654059410095
I0301 17:18:17.688988 140409862878976 logging_writer.py:48] [371900] global_step=371900, grad_norm=4.189697742462158, loss=0.6739584803581238
I0301 17:18:51.517828 140409854486272 logging_writer.py:48] [372000] global_step=372000, grad_norm=4.331260681152344, loss=0.5369289517402649
I0301 17:19:25.193866 140409862878976 logging_writer.py:48] [372100] global_step=372100, grad_norm=5.062403678894043, loss=0.6370418667793274
I0301 17:19:58.855827 140409854486272 logging_writer.py:48] [372200] global_step=372200, grad_norm=4.424717426300049, loss=0.6208646297454834
I0301 17:20:32.568099 140409862878976 logging_writer.py:48] [372300] global_step=372300, grad_norm=4.5798659324646, loss=0.7042196989059448
I0301 17:20:50.574069 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:20:56.699220 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:21:05.436707 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:21:07.715336 140573303715648 submission_runner.py:411] Time since start: 129991.71s, 	Step: 372355, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14774680137634277, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0522220134735107, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8178412914276123, 'test/num_examples': 10000, 'score': 125519.24978709221, 'total_duration': 129991.7078435421, 'accumulated_submission_time': 125519.24978709221, 'accumulated_eval_time': 4441.642533063889, 'accumulated_logging_time': 16.10594081878662}
I0301 17:21:07.794423 140407379851008 logging_writer.py:48] [372355] accumulated_eval_time=4441.642533, accumulated_logging_time=16.105941, accumulated_submission_time=125519.249787, global_step=372355, preemption_count=0, score=125519.249787, test/accuracy=0.631500, test/loss=1.817841, test/num_examples=10000, total_duration=129991.707844, train/accuracy=0.960439, train/loss=0.147747, validation/accuracy=0.755760, validation/loss=1.052222, validation/num_examples=50000
I0301 17:21:23.323709 140408319375104 logging_writer.py:48] [372400] global_step=372400, grad_norm=4.282008171081543, loss=0.6160936951637268
I0301 17:21:57.024661 140407379851008 logging_writer.py:48] [372500] global_step=372500, grad_norm=5.247985363006592, loss=0.6400600671768188
I0301 17:22:30.687955 140408319375104 logging_writer.py:48] [372600] global_step=372600, grad_norm=4.385348320007324, loss=0.6252964735031128
I0301 17:23:04.381692 140407379851008 logging_writer.py:48] [372700] global_step=372700, grad_norm=4.26633882522583, loss=0.6096041798591614
I0301 17:23:38.068717 140408319375104 logging_writer.py:48] [372800] global_step=372800, grad_norm=4.2818498611450195, loss=0.6661549806594849
I0301 17:24:11.752892 140407379851008 logging_writer.py:48] [372900] global_step=372900, grad_norm=4.19359827041626, loss=0.5630376935005188
I0301 17:24:45.572230 140408319375104 logging_writer.py:48] [373000] global_step=373000, grad_norm=4.561751842498779, loss=0.5573548674583435
I0301 17:25:19.307040 140407379851008 logging_writer.py:48] [373100] global_step=373100, grad_norm=4.649432182312012, loss=0.6353209018707275
I0301 17:25:52.948900 140408319375104 logging_writer.py:48] [373200] global_step=373200, grad_norm=4.562900066375732, loss=0.602068305015564
I0301 17:26:26.638150 140407379851008 logging_writer.py:48] [373300] global_step=373300, grad_norm=4.328766345977783, loss=0.6032418012619019
I0301 17:27:00.389189 140408319375104 logging_writer.py:48] [373400] global_step=373400, grad_norm=4.079719543457031, loss=0.5985384583473206
I0301 17:27:34.107659 140407379851008 logging_writer.py:48] [373500] global_step=373500, grad_norm=4.578958988189697, loss=0.667055606842041
I0301 17:28:07.840199 140408319375104 logging_writer.py:48] [373600] global_step=373600, grad_norm=4.369322776794434, loss=0.6415607333183289
I0301 17:28:41.552495 140407379851008 logging_writer.py:48] [373700] global_step=373700, grad_norm=4.488905429840088, loss=0.6533166766166687
I0301 17:29:15.293155 140408319375104 logging_writer.py:48] [373800] global_step=373800, grad_norm=4.5875163078308105, loss=0.6308908462524414
I0301 17:29:38.024545 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:29:44.118179 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:29:52.810653 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:29:55.169282 140573303715648 submission_runner.py:411] Time since start: 130519.16s, 	Step: 373869, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14779599010944366, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0529639720916748, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8184047937393188, 'test/num_examples': 10000, 'score': 126029.41074037552, 'total_duration': 130519.1618218422, 'accumulated_submission_time': 126029.41074037552, 'accumulated_eval_time': 4458.787220478058, 'accumulated_logging_time': 16.19475793838501}
I0301 17:29:55.249407 140408319375104 logging_writer.py:48] [373869] accumulated_eval_time=4458.787220, accumulated_logging_time=16.194758, accumulated_submission_time=126029.410740, global_step=373869, preemption_count=0, score=126029.410740, test/accuracy=0.631900, test/loss=1.818405, test/num_examples=10000, total_duration=130519.161822, train/accuracy=0.960419, train/loss=0.147796, validation/accuracy=0.755740, validation/loss=1.052964, validation/num_examples=50000
I0301 17:30:05.997987 140409837700864 logging_writer.py:48] [373900] global_step=373900, grad_norm=4.129863262176514, loss=0.6301831007003784
I0301 17:30:39.591995 140408319375104 logging_writer.py:48] [374000] global_step=374000, grad_norm=4.399355411529541, loss=0.6125218868255615
I0301 17:31:13.474781 140409837700864 logging_writer.py:48] [374100] global_step=374100, grad_norm=4.574572563171387, loss=0.6584852337837219
I0301 17:31:47.161153 140408319375104 logging_writer.py:48] [374200] global_step=374200, grad_norm=4.8633198738098145, loss=0.6915729641914368
I0301 17:32:20.829557 140409837700864 logging_writer.py:48] [374300] global_step=374300, grad_norm=4.580568790435791, loss=0.6211217641830444
I0301 17:32:54.481462 140408319375104 logging_writer.py:48] [374400] global_step=374400, grad_norm=4.279178142547607, loss=0.6213022470474243
I0301 17:33:28.156522 140409837700864 logging_writer.py:48] [374500] global_step=374500, grad_norm=4.750144958496094, loss=0.6105464696884155
I0301 17:34:01.892912 140408319375104 logging_writer.py:48] [374600] global_step=374600, grad_norm=4.3533854484558105, loss=0.5647437572479248
I0301 17:34:35.574663 140409837700864 logging_writer.py:48] [374700] global_step=374700, grad_norm=4.624574184417725, loss=0.6073507070541382
I0301 17:35:09.341738 140408319375104 logging_writer.py:48] [374800] global_step=374800, grad_norm=5.360253810882568, loss=0.6326947212219238
I0301 17:35:43.040758 140409837700864 logging_writer.py:48] [374900] global_step=374900, grad_norm=4.385491847991943, loss=0.5832604169845581
I0301 17:36:16.763615 140408319375104 logging_writer.py:48] [375000] global_step=375000, grad_norm=4.5667877197265625, loss=0.5677008032798767
I0301 17:36:50.431764 140409837700864 logging_writer.py:48] [375100] global_step=375100, grad_norm=4.049530982971191, loss=0.5697413682937622
I0301 17:37:24.224460 140408319375104 logging_writer.py:48] [375200] global_step=375200, grad_norm=4.597047328948975, loss=0.6584455966949463
I0301 17:37:57.883931 140409837700864 logging_writer.py:48] [375300] global_step=375300, grad_norm=4.225977420806885, loss=0.5045556426048279
I0301 17:38:25.372960 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:38:31.414330 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:38:40.110373 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:38:42.409355 140573303715648 submission_runner.py:411] Time since start: 131046.40s, 	Step: 375383, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14621107280254364, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0519813299179077, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8168600797653198, 'test/num_examples': 10000, 'score': 126539.46411824226, 'total_duration': 131046.4018945694, 'accumulated_submission_time': 126539.46411824226, 'accumulated_eval_time': 4475.823565721512, 'accumulated_logging_time': 16.284391403198242}
I0301 17:38:42.493912 140407379851008 logging_writer.py:48] [375383] accumulated_eval_time=4475.823566, accumulated_logging_time=16.284391, accumulated_submission_time=126539.464118, global_step=375383, preemption_count=0, score=126539.464118, test/accuracy=0.630800, test/loss=1.816860, test/num_examples=10000, total_duration=131046.401895, train/accuracy=0.961117, train/loss=0.146211, validation/accuracy=0.755660, validation/loss=1.051981, validation/num_examples=50000
I0301 17:38:48.570305 140409124681472 logging_writer.py:48] [375400] global_step=375400, grad_norm=4.806235313415527, loss=0.623847484588623
I0301 17:39:22.214195 140407379851008 logging_writer.py:48] [375500] global_step=375500, grad_norm=4.251686096191406, loss=0.5440837144851685
I0301 17:39:55.876055 140409124681472 logging_writer.py:48] [375600] global_step=375600, grad_norm=4.389241695404053, loss=0.6054584980010986
I0301 17:40:29.609800 140407379851008 logging_writer.py:48] [375700] global_step=375700, grad_norm=4.783969879150391, loss=0.7195083498954773
I0301 17:41:03.290758 140409124681472 logging_writer.py:48] [375800] global_step=375800, grad_norm=5.107931613922119, loss=0.6488587856292725
I0301 17:41:37.033664 140407379851008 logging_writer.py:48] [375900] global_step=375900, grad_norm=4.548933506011963, loss=0.585708498954773
I0301 17:42:10.732588 140409124681472 logging_writer.py:48] [376000] global_step=376000, grad_norm=4.376382827758789, loss=0.6018623113632202
I0301 17:42:44.393098 140407379851008 logging_writer.py:48] [376100] global_step=376100, grad_norm=4.499627113342285, loss=0.6246368885040283
I0301 17:43:18.123310 140409124681472 logging_writer.py:48] [376200] global_step=376200, grad_norm=4.34087610244751, loss=0.6503047943115234
I0301 17:43:51.787891 140407379851008 logging_writer.py:48] [376300] global_step=376300, grad_norm=3.9214322566986084, loss=0.5709747076034546
I0301 17:44:25.515367 140409124681472 logging_writer.py:48] [376400] global_step=376400, grad_norm=4.301501750946045, loss=0.6278455257415771
I0301 17:44:59.212815 140407379851008 logging_writer.py:48] [376500] global_step=376500, grad_norm=4.80183219909668, loss=0.6397072672843933
I0301 17:45:32.937924 140409124681472 logging_writer.py:48] [376600] global_step=376600, grad_norm=4.9354329109191895, loss=0.6427680253982544
I0301 17:46:06.606408 140407379851008 logging_writer.py:48] [376700] global_step=376700, grad_norm=4.274130821228027, loss=0.6230567693710327
I0301 17:46:40.283146 140409124681472 logging_writer.py:48] [376800] global_step=376800, grad_norm=4.367358684539795, loss=0.6258549690246582
I0301 17:47:12.493161 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:47:18.545176 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:47:27.096581 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:47:29.407443 140573303715648 submission_runner.py:411] Time since start: 131573.40s, 	Step: 376897, 	{'train/accuracy': 0.9594427347183228, 'train/loss': 0.15016399323940277, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0519487857818604, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8168940544128418, 'test/num_examples': 10000, 'score': 127049.39342689514, 'total_duration': 131573.39998173714, 'accumulated_submission_time': 127049.39342689514, 'accumulated_eval_time': 4492.737801551819, 'accumulated_logging_time': 16.379032611846924}
I0301 17:47:29.491095 140408319375104 logging_writer.py:48] [376897] accumulated_eval_time=4492.737802, accumulated_logging_time=16.379033, accumulated_submission_time=127049.393427, global_step=376897, preemption_count=0, score=127049.393427, test/accuracy=0.631500, test/loss=1.816894, test/num_examples=10000, total_duration=131573.399982, train/accuracy=0.959443, train/loss=0.150164, validation/accuracy=0.755620, validation/loss=1.051949, validation/num_examples=50000
I0301 17:47:30.839342 140409124681472 logging_writer.py:48] [376900] global_step=376900, grad_norm=4.890085697174072, loss=0.6590952277183533
I0301 17:48:04.448316 140408319375104 logging_writer.py:48] [377000] global_step=377000, grad_norm=5.234494686126709, loss=0.6416319012641907
I0301 17:48:38.115731 140409124681472 logging_writer.py:48] [377100] global_step=377100, grad_norm=4.266168594360352, loss=0.6469714641571045
I0301 17:49:11.873142 140408319375104 logging_writer.py:48] [377200] global_step=377200, grad_norm=4.529013633728027, loss=0.6387836933135986
I0301 17:49:45.545339 140409124681472 logging_writer.py:48] [377300] global_step=377300, grad_norm=4.872068881988525, loss=0.6475477814674377
I0301 17:50:19.234043 140408319375104 logging_writer.py:48] [377400] global_step=377400, grad_norm=4.337324619293213, loss=0.6649911403656006
I0301 17:50:52.949269 140409124681472 logging_writer.py:48] [377500] global_step=377500, grad_norm=4.219407081604004, loss=0.5980644226074219
I0301 17:51:26.657871 140408319375104 logging_writer.py:48] [377600] global_step=377600, grad_norm=4.421224117279053, loss=0.6246793270111084
I0301 17:52:00.360094 140409124681472 logging_writer.py:48] [377700] global_step=377700, grad_norm=4.390115261077881, loss=0.610754668712616
I0301 17:52:34.093160 140408319375104 logging_writer.py:48] [377800] global_step=377800, grad_norm=4.541928768157959, loss=0.5882744789123535
I0301 17:53:07.780596 140409124681472 logging_writer.py:48] [377900] global_step=377900, grad_norm=4.524954319000244, loss=0.6451759934425354
I0301 17:53:41.748478 140408319375104 logging_writer.py:48] [378000] global_step=378000, grad_norm=4.616299629211426, loss=0.5857424736022949
I0301 17:54:15.415529 140409124681472 logging_writer.py:48] [378100] global_step=378100, grad_norm=4.181923866271973, loss=0.5957430601119995
I0301 17:54:49.159236 140408319375104 logging_writer.py:48] [378200] global_step=378200, grad_norm=4.168529987335205, loss=0.5610045194625854
I0301 17:55:22.945153 140409124681472 logging_writer.py:48] [378300] global_step=378300, grad_norm=4.853369235992432, loss=0.6395770907402039
I0301 17:55:56.654403 140408319375104 logging_writer.py:48] [378400] global_step=378400, grad_norm=4.462676525115967, loss=0.6048749089241028
I0301 17:55:59.515421 140573303715648 spec.py:321] Evaluating on the training split.
I0301 17:56:05.728960 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 17:56:14.453620 140573303715648 spec.py:349] Evaluating on the test split.
I0301 17:56:16.761530 140573303715648 submission_runner.py:411] Time since start: 132100.75s, 	Step: 378410, 	{'train/accuracy': 0.9618940949440002, 'train/loss': 0.14760050177574158, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0524741411209106, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8175469636917114, 'test/num_examples': 10000, 'score': 127559.34850096703, 'total_duration': 132100.75407028198, 'accumulated_submission_time': 127559.34850096703, 'accumulated_eval_time': 4509.983863592148, 'accumulated_logging_time': 16.47271466255188}
I0301 17:56:16.842556 140409854486272 logging_writer.py:48] [378410] accumulated_eval_time=4509.983864, accumulated_logging_time=16.472715, accumulated_submission_time=127559.348501, global_step=378410, preemption_count=0, score=127559.348501, test/accuracy=0.630800, test/loss=1.817547, test/num_examples=10000, total_duration=132100.754070, train/accuracy=0.961894, train/loss=0.147601, validation/accuracy=0.755880, validation/loss=1.052474, validation/num_examples=50000
I0301 17:56:47.545591 140409862878976 logging_writer.py:48] [378500] global_step=378500, grad_norm=4.820911884307861, loss=0.6434445977210999
I0301 17:57:21.187156 140409854486272 logging_writer.py:48] [378600] global_step=378600, grad_norm=4.504298210144043, loss=0.6566185355186462
I0301 17:57:54.857501 140409862878976 logging_writer.py:48] [378700] global_step=378700, grad_norm=4.588249206542969, loss=0.6277627944946289
I0301 17:58:28.635733 140409854486272 logging_writer.py:48] [378800] global_step=378800, grad_norm=4.472194194793701, loss=0.5395322442054749
I0301 17:59:02.313262 140409862878976 logging_writer.py:48] [378900] global_step=378900, grad_norm=4.815608501434326, loss=0.6064979434013367
I0301 17:59:36.061596 140409854486272 logging_writer.py:48] [379000] global_step=379000, grad_norm=4.740291595458984, loss=0.6696460247039795
I0301 18:00:09.726315 140409862878976 logging_writer.py:48] [379100] global_step=379100, grad_norm=4.508123874664307, loss=0.6197599172592163
I0301 18:00:43.367998 140409854486272 logging_writer.py:48] [379200] global_step=379200, grad_norm=4.300978660583496, loss=0.6472498774528503
I0301 18:01:17.072641 140409862878976 logging_writer.py:48] [379300] global_step=379300, grad_norm=4.407843112945557, loss=0.6610526442527771
I0301 18:01:50.911356 140409854486272 logging_writer.py:48] [379400] global_step=379400, grad_norm=4.147933483123779, loss=0.5511518716812134
I0301 18:02:24.619406 140409862878976 logging_writer.py:48] [379500] global_step=379500, grad_norm=5.245934963226318, loss=0.694894552230835
I0301 18:02:58.362643 140409854486272 logging_writer.py:48] [379600] global_step=379600, grad_norm=4.316534519195557, loss=0.6183338165283203
I0301 18:03:32.037638 140409862878976 logging_writer.py:48] [379700] global_step=379700, grad_norm=4.260537624359131, loss=0.5639948844909668
I0301 18:04:05.727880 140409854486272 logging_writer.py:48] [379800] global_step=379800, grad_norm=4.296660423278809, loss=0.6354318857192993
I0301 18:04:39.455638 140409862878976 logging_writer.py:48] [379900] global_step=379900, grad_norm=4.78974723815918, loss=0.6027315258979797
I0301 18:04:47.017301 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:04:53.128986 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:05:01.858141 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:05:04.196674 140573303715648 submission_runner.py:411] Time since start: 132628.19s, 	Step: 379924, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.1453332155942917, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0524545907974243, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.817813754081726, 'test/num_examples': 10000, 'score': 128069.45318174362, 'total_duration': 132628.18920063972, 'accumulated_submission_time': 128069.45318174362, 'accumulated_eval_time': 4527.163170099258, 'accumulated_logging_time': 16.564187049865723}
I0301 18:05:04.276137 140408319375104 logging_writer.py:48] [379924] accumulated_eval_time=4527.163170, accumulated_logging_time=16.564187, accumulated_submission_time=128069.453182, global_step=379924, preemption_count=0, score=128069.453182, test/accuracy=0.631100, test/loss=1.817814, test/num_examples=10000, total_duration=132628.189201, train/accuracy=0.961476, train/loss=0.145333, validation/accuracy=0.755940, validation/loss=1.052455, validation/num_examples=50000
I0301 18:05:30.219159 140409124681472 logging_writer.py:48] [380000] global_step=380000, grad_norm=5.015054702758789, loss=0.5823377370834351
I0301 18:06:03.862317 140408319375104 logging_writer.py:48] [380100] global_step=380100, grad_norm=4.280371189117432, loss=0.5914391279220581
I0301 18:06:37.584057 140409124681472 logging_writer.py:48] [380200] global_step=380200, grad_norm=4.227439880371094, loss=0.5821126699447632
I0301 18:07:11.258009 140408319375104 logging_writer.py:48] [380300] global_step=380300, grad_norm=4.687815189361572, loss=0.5882517099380493
I0301 18:07:45.058680 140409124681472 logging_writer.py:48] [380400] global_step=380400, grad_norm=4.202066421508789, loss=0.6063223481178284
I0301 18:08:18.774778 140408319375104 logging_writer.py:48] [380500] global_step=380500, grad_norm=4.318092346191406, loss=0.6054633259773254
I0301 18:08:52.520423 140409124681472 logging_writer.py:48] [380600] global_step=380600, grad_norm=4.814006328582764, loss=0.6400688886642456
I0301 18:09:26.244947 140408319375104 logging_writer.py:48] [380700] global_step=380700, grad_norm=3.927030086517334, loss=0.5800799131393433
I0301 18:09:59.980226 140409124681472 logging_writer.py:48] [380800] global_step=380800, grad_norm=5.065974235534668, loss=0.651401937007904
I0301 18:10:33.685580 140408319375104 logging_writer.py:48] [380900] global_step=380900, grad_norm=4.365757942199707, loss=0.5990674495697021
I0301 18:11:07.428167 140409124681472 logging_writer.py:48] [381000] global_step=381000, grad_norm=4.926488399505615, loss=0.693220853805542
I0301 18:11:41.150294 140408319375104 logging_writer.py:48] [381100] global_step=381100, grad_norm=4.172826290130615, loss=0.6330456733703613
I0301 18:12:14.880397 140409124681472 logging_writer.py:48] [381200] global_step=381200, grad_norm=4.22658109664917, loss=0.6009708642959595
I0301 18:12:48.603571 140408319375104 logging_writer.py:48] [381300] global_step=381300, grad_norm=4.4418792724609375, loss=0.710849404335022
I0301 18:13:22.343055 140409124681472 logging_writer.py:48] [381400] global_step=381400, grad_norm=4.476566314697266, loss=0.6421092748641968
I0301 18:13:34.272114 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:13:40.624358 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:13:49.388358 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:13:51.747179 140573303715648 submission_runner.py:411] Time since start: 133155.74s, 	Step: 381437, 	{'train/accuracy': 0.9616150856018066, 'train/loss': 0.14412277936935425, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.053081750869751, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8182154893875122, 'test/num_examples': 10000, 'score': 128579.37960290909, 'total_duration': 133155.73970913887, 'accumulated_submission_time': 128579.37960290909, 'accumulated_eval_time': 4544.638174533844, 'accumulated_logging_time': 16.653773546218872}
I0301 18:13:51.829097 140409854486272 logging_writer.py:48] [381437] accumulated_eval_time=4544.638175, accumulated_logging_time=16.653774, accumulated_submission_time=128579.379603, global_step=381437, preemption_count=0, score=128579.379603, test/accuracy=0.631400, test/loss=1.818215, test/num_examples=10000, total_duration=133155.739709, train/accuracy=0.961615, train/loss=0.144123, validation/accuracy=0.755800, validation/loss=1.053082, validation/num_examples=50000
I0301 18:14:13.407699 140409862878976 logging_writer.py:48] [381500] global_step=381500, grad_norm=4.520810127258301, loss=0.6781526207923889
I0301 18:14:47.070296 140409854486272 logging_writer.py:48] [381600] global_step=381600, grad_norm=4.714407444000244, loss=0.6287549734115601
I0301 18:15:20.790310 140409862878976 logging_writer.py:48] [381700] global_step=381700, grad_norm=4.763636589050293, loss=0.5287150740623474
I0301 18:15:54.496305 140409854486272 logging_writer.py:48] [381800] global_step=381800, grad_norm=4.877449035644531, loss=0.6439815759658813
I0301 18:16:28.249878 140409862878976 logging_writer.py:48] [381900] global_step=381900, grad_norm=4.607531547546387, loss=0.6546761989593506
I0301 18:17:01.940153 140409854486272 logging_writer.py:48] [382000] global_step=382000, grad_norm=4.71115779876709, loss=0.6775568127632141
I0301 18:17:35.716146 140409862878976 logging_writer.py:48] [382100] global_step=382100, grad_norm=5.576517105102539, loss=0.570307731628418
I0301 18:18:09.395232 140409854486272 logging_writer.py:48] [382200] global_step=382200, grad_norm=4.679326057434082, loss=0.5976948738098145
I0301 18:18:43.159018 140409862878976 logging_writer.py:48] [382300] global_step=382300, grad_norm=4.402194976806641, loss=0.5624449253082275
I0301 18:19:16.812847 140409854486272 logging_writer.py:48] [382400] global_step=382400, grad_norm=4.348926067352295, loss=0.6077287793159485
I0301 18:19:50.634195 140409862878976 logging_writer.py:48] [382500] global_step=382500, grad_norm=4.980016231536865, loss=0.653567373752594
I0301 18:20:24.383123 140409854486272 logging_writer.py:48] [382600] global_step=382600, grad_norm=4.571418285369873, loss=0.6310114860534668
I0301 18:20:58.094825 140409862878976 logging_writer.py:48] [382700] global_step=382700, grad_norm=4.641952991485596, loss=0.7196847200393677
I0301 18:21:31.810207 140409854486272 logging_writer.py:48] [382800] global_step=382800, grad_norm=4.314560413360596, loss=0.6325467824935913
I0301 18:22:05.494717 140409862878976 logging_writer.py:48] [382900] global_step=382900, grad_norm=4.110817909240723, loss=0.5399780869483948
I0301 18:22:21.852452 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:22:27.947002 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:22:36.666034 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:22:38.961385 140573303715648 submission_runner.py:411] Time since start: 133682.95s, 	Step: 382950, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14682477712631226, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0521663427352905, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8172036409378052, 'test/num_examples': 10000, 'score': 129089.33254384995, 'total_duration': 133682.95392680168, 'accumulated_submission_time': 129089.33254384995, 'accumulated_eval_time': 4561.7470643520355, 'accumulated_logging_time': 16.74677801132202}
I0301 18:22:39.044325 140408319375104 logging_writer.py:48] [382950] accumulated_eval_time=4561.747064, accumulated_logging_time=16.746778, accumulated_submission_time=129089.332544, global_step=382950, preemption_count=0, score=129089.332544, test/accuracy=0.631400, test/loss=1.817204, test/num_examples=10000, total_duration=133682.953927, train/accuracy=0.960320, train/loss=0.146825, validation/accuracy=0.755920, validation/loss=1.052166, validation/num_examples=50000
I0301 18:22:56.203518 140409124681472 logging_writer.py:48] [383000] global_step=383000, grad_norm=4.4322028160095215, loss=0.5799320340156555
I0301 18:23:29.901101 140408319375104 logging_writer.py:48] [383100] global_step=383100, grad_norm=4.690448760986328, loss=0.6410208940505981
I0301 18:24:03.579050 140409124681472 logging_writer.py:48] [383200] global_step=383200, grad_norm=4.857813358306885, loss=0.6046178340911865
I0301 18:24:37.267377 140408319375104 logging_writer.py:48] [383300] global_step=383300, grad_norm=4.59337854385376, loss=0.5874122977256775
I0301 18:25:11.002279 140409124681472 logging_writer.py:48] [383400] global_step=383400, grad_norm=4.765089988708496, loss=0.5974414944648743
I0301 18:25:44.684820 140408319375104 logging_writer.py:48] [383500] global_step=383500, grad_norm=4.468945026397705, loss=0.6547449827194214
I0301 18:26:18.518488 140409124681472 logging_writer.py:48] [383600] global_step=383600, grad_norm=4.965344429016113, loss=0.6008175015449524
I0301 18:26:52.238156 140408319375104 logging_writer.py:48] [383700] global_step=383700, grad_norm=4.02958869934082, loss=0.5967185497283936
I0301 18:27:25.910830 140409124681472 logging_writer.py:48] [383800] global_step=383800, grad_norm=5.056214809417725, loss=0.6611554026603699
I0301 18:27:59.655381 140408319375104 logging_writer.py:48] [383900] global_step=383900, grad_norm=5.8715972900390625, loss=0.7279477119445801
I0301 18:28:33.316987 140409124681472 logging_writer.py:48] [384000] global_step=384000, grad_norm=4.303327560424805, loss=0.6214223504066467
I0301 18:29:07.044673 140408319375104 logging_writer.py:48] [384100] global_step=384100, grad_norm=4.253494739532471, loss=0.6077455878257751
I0301 18:29:40.693526 140409124681472 logging_writer.py:48] [384200] global_step=384200, grad_norm=4.191648006439209, loss=0.6207791566848755
I0301 18:30:14.339644 140408319375104 logging_writer.py:48] [384300] global_step=384300, grad_norm=4.5060200691223145, loss=0.6169507503509521
I0301 18:30:48.099429 140409124681472 logging_writer.py:48] [384400] global_step=384400, grad_norm=4.297484874725342, loss=0.6550001502037048
I0301 18:31:09.166403 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:31:16.042166 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:31:24.726397 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:31:26.992473 140573303715648 submission_runner.py:411] Time since start: 134210.99s, 	Step: 384464, 	{'train/accuracy': 0.9594826102256775, 'train/loss': 0.14789094030857086, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0529996156692505, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8189606666564941, 'test/num_examples': 10000, 'score': 129599.38512706757, 'total_duration': 134210.98501205444, 'accumulated_submission_time': 129599.38512706757, 'accumulated_eval_time': 4579.573089122772, 'accumulated_logging_time': 16.839839935302734}
I0301 18:31:27.074112 140409846093568 logging_writer.py:48] [384464] accumulated_eval_time=4579.573089, accumulated_logging_time=16.839840, accumulated_submission_time=129599.385127, global_step=384464, preemption_count=0, score=129599.385127, test/accuracy=0.631300, test/loss=1.818961, test/num_examples=10000, total_duration=134210.985012, train/accuracy=0.959483, train/loss=0.147891, validation/accuracy=0.755680, validation/loss=1.053000, validation/num_examples=50000
I0301 18:31:39.563940 140409854486272 logging_writer.py:48] [384500] global_step=384500, grad_norm=4.466036796569824, loss=0.5916380286216736
I0301 18:32:13.376400 140409846093568 logging_writer.py:48] [384600] global_step=384600, grad_norm=4.804349899291992, loss=0.573701024055481
I0301 18:32:47.033582 140409854486272 logging_writer.py:48] [384700] global_step=384700, grad_norm=4.45949649810791, loss=0.6011983752250671
I0301 18:33:20.833132 140409846093568 logging_writer.py:48] [384800] global_step=384800, grad_norm=4.416162967681885, loss=0.5944408178329468
I0301 18:33:54.497466 140409854486272 logging_writer.py:48] [384900] global_step=384900, grad_norm=4.636704921722412, loss=0.6700674295425415
I0301 18:34:28.284667 140409846093568 logging_writer.py:48] [385000] global_step=385000, grad_norm=4.456409931182861, loss=0.6891323924064636
I0301 18:35:01.985864 140409854486272 logging_writer.py:48] [385100] global_step=385100, grad_norm=4.7648210525512695, loss=0.6769934892654419
I0301 18:35:35.703351 140409846093568 logging_writer.py:48] [385200] global_step=385200, grad_norm=4.551940441131592, loss=0.6212136149406433
I0301 18:36:09.423426 140409854486272 logging_writer.py:48] [385300] global_step=385300, grad_norm=4.658679008483887, loss=0.6453381180763245
I0301 18:36:43.130140 140409846093568 logging_writer.py:48] [385400] global_step=385400, grad_norm=4.497555255889893, loss=0.5613406896591187
I0301 18:37:16.833721 140409854486272 logging_writer.py:48] [385500] global_step=385500, grad_norm=4.768493175506592, loss=0.6781907677650452
I0301 18:37:50.532040 140409846093568 logging_writer.py:48] [385600] global_step=385600, grad_norm=4.457168102264404, loss=0.6526328325271606
I0301 18:38:24.290375 140409854486272 logging_writer.py:48] [385700] global_step=385700, grad_norm=4.636101245880127, loss=0.5853583812713623
I0301 18:38:57.993252 140409846093568 logging_writer.py:48] [385800] global_step=385800, grad_norm=4.46531343460083, loss=0.6503067016601562
I0301 18:39:31.687229 140409854486272 logging_writer.py:48] [385900] global_step=385900, grad_norm=4.686278343200684, loss=0.6572207808494568
I0301 18:39:57.122061 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:40:03.382960 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:40:11.973332 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:40:14.275810 140573303715648 submission_runner.py:411] Time since start: 134738.27s, 	Step: 385977, 	{'train/accuracy': 0.9602199792861938, 'train/loss': 0.14644360542297363, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0509934425354004, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8169879913330078, 'test/num_examples': 10000, 'score': 130109.36382508278, 'total_duration': 134738.26835227013, 'accumulated_submission_time': 130109.36382508278, 'accumulated_eval_time': 4596.726794719696, 'accumulated_logging_time': 16.932087182998657}
I0301 18:40:14.360762 140407371458304 logging_writer.py:48] [385977] accumulated_eval_time=4596.726795, accumulated_logging_time=16.932087, accumulated_submission_time=130109.363825, global_step=385977, preemption_count=0, score=130109.363825, test/accuracy=0.631500, test/loss=1.816988, test/num_examples=10000, total_duration=134738.268352, train/accuracy=0.960220, train/loss=0.146444, validation/accuracy=0.756120, validation/loss=1.050993, validation/num_examples=50000
I0301 18:40:22.431022 140407379851008 logging_writer.py:48] [386000] global_step=386000, grad_norm=4.386393070220947, loss=0.5940563678741455
I0301 18:40:56.013895 140407371458304 logging_writer.py:48] [386100] global_step=386100, grad_norm=4.010620594024658, loss=0.56875079870224
I0301 18:41:29.689772 140407379851008 logging_writer.py:48] [386200] global_step=386200, grad_norm=4.4019060134887695, loss=0.6090529561042786
I0301 18:42:03.439349 140407371458304 logging_writer.py:48] [386300] global_step=386300, grad_norm=4.366798400878906, loss=0.6323567032814026
I0301 18:42:37.127389 140407379851008 logging_writer.py:48] [386400] global_step=386400, grad_norm=4.474173545837402, loss=0.6606576442718506
I0301 18:43:10.753914 140407371458304 logging_writer.py:48] [386500] global_step=386500, grad_norm=4.594943523406982, loss=0.6996157765388489
I0301 18:43:44.410979 140407379851008 logging_writer.py:48] [386600] global_step=386600, grad_norm=4.624103546142578, loss=0.6545143723487854
I0301 18:44:18.168765 140407371458304 logging_writer.py:48] [386700] global_step=386700, grad_norm=4.37296724319458, loss=0.6001648902893066
I0301 18:44:51.870301 140407379851008 logging_writer.py:48] [386800] global_step=386800, grad_norm=5.088050842285156, loss=0.685245156288147
I0301 18:45:25.602925 140407371458304 logging_writer.py:48] [386900] global_step=386900, grad_norm=5.043332099914551, loss=0.5678996443748474
I0301 18:45:59.280930 140407379851008 logging_writer.py:48] [387000] global_step=387000, grad_norm=4.075070858001709, loss=0.6032512187957764
I0301 18:46:33.012516 140407371458304 logging_writer.py:48] [387100] global_step=387100, grad_norm=4.663134574890137, loss=0.6749927997589111
I0301 18:47:06.697753 140407379851008 logging_writer.py:48] [387200] global_step=387200, grad_norm=4.603631496429443, loss=0.6850478649139404
I0301 18:47:40.443021 140407371458304 logging_writer.py:48] [387300] global_step=387300, grad_norm=4.56313419342041, loss=0.662842869758606
I0301 18:48:14.106076 140407379851008 logging_writer.py:48] [387400] global_step=387400, grad_norm=4.1420159339904785, loss=0.5834141373634338
I0301 18:48:44.284588 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:48:50.334716 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:48:58.935810 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:49:01.251988 140573303715648 submission_runner.py:411] Time since start: 135265.24s, 	Step: 387491, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14855530858039856, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.051853060722351, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8155715465545654, 'test/num_examples': 10000, 'score': 130619.21668457985, 'total_duration': 135265.24450850487, 'accumulated_submission_time': 130619.21668457985, 'accumulated_eval_time': 4613.694131135941, 'accumulated_logging_time': 17.029442310333252}
I0301 18:49:01.328390 140407371458304 logging_writer.py:48] [387491] accumulated_eval_time=4613.694131, accumulated_logging_time=17.029442, accumulated_submission_time=130619.216685, global_step=387491, preemption_count=0, score=130619.216685, test/accuracy=0.631400, test/loss=1.815572, test/num_examples=10000, total_duration=135265.244509, train/accuracy=0.959961, train/loss=0.148555, validation/accuracy=0.755300, validation/loss=1.051853, validation/num_examples=50000
I0301 18:49:04.695234 140409837700864 logging_writer.py:48] [387500] global_step=387500, grad_norm=4.539047718048096, loss=0.5629600882530212
I0301 18:49:38.328730 140407371458304 logging_writer.py:48] [387600] global_step=387600, grad_norm=4.844761371612549, loss=0.675977349281311
I0301 18:50:12.006247 140409837700864 logging_writer.py:48] [387700] global_step=387700, grad_norm=4.6287665367126465, loss=0.5927286744117737
I0301 18:50:45.935787 140407371458304 logging_writer.py:48] [387800] global_step=387800, grad_norm=4.127176761627197, loss=0.6148940324783325
I0301 18:51:19.605973 140409837700864 logging_writer.py:48] [387900] global_step=387900, grad_norm=4.507950305938721, loss=0.6870041489601135
I0301 18:51:53.324575 140407371458304 logging_writer.py:48] [388000] global_step=388000, grad_norm=5.407992839813232, loss=0.6150288581848145
I0301 18:52:27.053425 140409837700864 logging_writer.py:48] [388100] global_step=388100, grad_norm=4.637833118438721, loss=0.5980595946311951
I0301 18:53:00.766169 140407371458304 logging_writer.py:48] [388200] global_step=388200, grad_norm=4.133747100830078, loss=0.5512295365333557
I0301 18:53:34.494802 140409837700864 logging_writer.py:48] [388300] global_step=388300, grad_norm=4.226996898651123, loss=0.5397329330444336
I0301 18:54:08.196478 140407371458304 logging_writer.py:48] [388400] global_step=388400, grad_norm=4.206341743469238, loss=0.6012529134750366
I0301 18:54:41.891837 140409837700864 logging_writer.py:48] [388500] global_step=388500, grad_norm=4.36264181137085, loss=0.6533351540565491
I0301 18:55:15.615194 140407371458304 logging_writer.py:48] [388600] global_step=388600, grad_norm=4.620891094207764, loss=0.6504659652709961
I0301 18:55:49.303091 140409837700864 logging_writer.py:48] [388700] global_step=388700, grad_norm=4.594915390014648, loss=0.636679470539093
I0301 18:56:23.045432 140407371458304 logging_writer.py:48] [388800] global_step=388800, grad_norm=4.554953098297119, loss=0.6636933088302612
I0301 18:56:56.923689 140409837700864 logging_writer.py:48] [388900] global_step=388900, grad_norm=4.06683874130249, loss=0.5788456201553345
I0301 18:57:30.572469 140407371458304 logging_writer.py:48] [389000] global_step=389000, grad_norm=4.4883575439453125, loss=0.656854510307312
I0301 18:57:31.393937 140573303715648 spec.py:321] Evaluating on the training split.
I0301 18:57:37.458896 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 18:57:46.095908 140573303715648 spec.py:349] Evaluating on the test split.
I0301 18:57:48.427963 140573303715648 submission_runner.py:411] Time since start: 135792.42s, 	Step: 389004, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.14659127593040466, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0521849393844604, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8187190294265747, 'test/num_examples': 10000, 'score': 131129.21193361282, 'total_duration': 135792.42049121857, 'accumulated_submission_time': 131129.21193361282, 'accumulated_eval_time': 4630.728091716766, 'accumulated_logging_time': 17.116252183914185}
I0301 18:57:48.506039 140407379851008 logging_writer.py:48] [389004] accumulated_eval_time=4630.728092, accumulated_logging_time=17.116252, accumulated_submission_time=131129.211934, global_step=389004, preemption_count=0, score=131129.211934, test/accuracy=0.630900, test/loss=1.818719, test/num_examples=10000, total_duration=135792.420491, train/accuracy=0.961436, train/loss=0.146591, validation/accuracy=0.755940, validation/loss=1.052185, validation/num_examples=50000
I0301 18:58:21.136871 140409124681472 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.979730606079102, loss=0.6664836406707764
I0301 18:58:54.798305 140407379851008 logging_writer.py:48] [389200] global_step=389200, grad_norm=4.458507061004639, loss=0.6217052340507507
I0301 18:59:28.454761 140409124681472 logging_writer.py:48] [389300] global_step=389300, grad_norm=4.392675399780273, loss=0.6705747842788696
I0301 19:00:02.195417 140407379851008 logging_writer.py:48] [389400] global_step=389400, grad_norm=4.639334678649902, loss=0.6273901462554932
I0301 19:00:35.874666 140409124681472 logging_writer.py:48] [389500] global_step=389500, grad_norm=4.596137046813965, loss=0.6464462876319885
I0301 19:01:09.594997 140407379851008 logging_writer.py:48] [389600] global_step=389600, grad_norm=4.743596076965332, loss=0.6117230653762817
I0301 19:01:43.278172 140409124681472 logging_writer.py:48] [389700] global_step=389700, grad_norm=5.222583293914795, loss=0.6421762704849243
I0301 19:02:17.021626 140407379851008 logging_writer.py:48] [389800] global_step=389800, grad_norm=4.814621925354004, loss=0.6704044938087463
I0301 19:02:50.779259 140409124681472 logging_writer.py:48] [389900] global_step=389900, grad_norm=4.641403675079346, loss=0.6926690340042114
I0301 19:03:24.472677 140407379851008 logging_writer.py:48] [390000] global_step=390000, grad_norm=4.352884292602539, loss=0.6271985173225403
I0301 19:03:58.136049 140409124681472 logging_writer.py:48] [390100] global_step=390100, grad_norm=4.642292499542236, loss=0.6294189095497131
I0301 19:04:31.879464 140407379851008 logging_writer.py:48] [390200] global_step=390200, grad_norm=4.528565406799316, loss=0.586894154548645
I0301 19:05:05.538076 140409124681472 logging_writer.py:48] [390300] global_step=390300, grad_norm=4.9442853927612305, loss=0.7149900197982788
I0301 19:05:39.299456 140407379851008 logging_writer.py:48] [390400] global_step=390400, grad_norm=4.769968509674072, loss=0.6147478222846985
I0301 19:06:12.987190 140409124681472 logging_writer.py:48] [390500] global_step=390500, grad_norm=4.77091121673584, loss=0.6703012585639954
I0301 19:06:18.520549 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:06:24.542786 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:06:33.342656 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:06:35.596323 140573303715648 submission_runner.py:411] Time since start: 136319.59s, 	Step: 390518, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14676417410373688, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0530712604522705, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8176332712173462, 'test/num_examples': 10000, 'score': 131639.15394306183, 'total_duration': 136319.58885860443, 'accumulated_submission_time': 131639.15394306183, 'accumulated_eval_time': 4647.803809642792, 'accumulated_logging_time': 17.206831455230713}
I0301 19:06:35.680171 140408319375104 logging_writer.py:48] [390518] accumulated_eval_time=4647.803810, accumulated_logging_time=17.206831, accumulated_submission_time=131639.153943, global_step=390518, preemption_count=0, score=131639.153943, test/accuracy=0.630700, test/loss=1.817633, test/num_examples=10000, total_duration=136319.588859, train/accuracy=0.960340, train/loss=0.146764, validation/accuracy=0.755700, validation/loss=1.053071, validation/num_examples=50000
I0301 19:07:03.596015 140409124681472 logging_writer.py:48] [390600] global_step=390600, grad_norm=4.576572418212891, loss=0.6396362781524658
I0301 19:07:37.310810 140408319375104 logging_writer.py:48] [390700] global_step=390700, grad_norm=4.533061981201172, loss=0.6142453551292419
I0301 19:08:10.980339 140409124681472 logging_writer.py:48] [390800] global_step=390800, grad_norm=4.379554271697998, loss=0.5626220107078552
I0301 19:08:44.709806 140408319375104 logging_writer.py:48] [390900] global_step=390900, grad_norm=4.133683204650879, loss=0.60491943359375
I0301 19:09:18.510663 140409124681472 logging_writer.py:48] [391000] global_step=391000, grad_norm=4.332607746124268, loss=0.6165565848350525
I0301 19:09:52.220206 140408319375104 logging_writer.py:48] [391100] global_step=391100, grad_norm=4.191318035125732, loss=0.6000198125839233
I0301 19:10:25.986306 140409124681472 logging_writer.py:48] [391200] global_step=391200, grad_norm=4.658051490783691, loss=0.653891384601593
I0301 19:10:59.657476 140408319375104 logging_writer.py:48] [391300] global_step=391300, grad_norm=5.1229963302612305, loss=0.6895326375961304
I0301 19:11:33.401192 140409124681472 logging_writer.py:48] [391400] global_step=391400, grad_norm=4.753824234008789, loss=0.7131263613700867
I0301 19:12:07.123517 140408319375104 logging_writer.py:48] [391500] global_step=391500, grad_norm=4.443023681640625, loss=0.6036112308502197
I0301 19:12:40.845820 140409124681472 logging_writer.py:48] [391600] global_step=391600, grad_norm=4.419027805328369, loss=0.6865039467811584
I0301 19:13:14.565713 140408319375104 logging_writer.py:48] [391700] global_step=391700, grad_norm=4.02244758605957, loss=0.5517308115959167
I0301 19:13:48.268441 140409124681472 logging_writer.py:48] [391800] global_step=391800, grad_norm=4.377752780914307, loss=0.6319509148597717
I0301 19:14:21.997853 140408319375104 logging_writer.py:48] [391900] global_step=391900, grad_norm=4.420414924621582, loss=0.6093733310699463
I0301 19:14:55.899913 140409124681472 logging_writer.py:48] [392000] global_step=392000, grad_norm=4.4043402671813965, loss=0.6349789500236511
I0301 19:15:05.815598 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:15:11.954516 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:15:20.569366 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:15:22.925408 140573303715648 submission_runner.py:411] Time since start: 136846.92s, 	Step: 392031, 	{'train/accuracy': 0.9616150856018066, 'train/loss': 0.1448460966348648, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0519707202911377, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8175158500671387, 'test/num_examples': 10000, 'score': 132149.21634721756, 'total_duration': 136846.91794514656, 'accumulated_submission_time': 132149.21634721756, 'accumulated_eval_time': 4664.913568973541, 'accumulated_logging_time': 17.301775693893433}
I0301 19:15:23.011026 140407371458304 logging_writer.py:48] [392031] accumulated_eval_time=4664.913569, accumulated_logging_time=17.301776, accumulated_submission_time=132149.216347, global_step=392031, preemption_count=0, score=132149.216347, test/accuracy=0.630600, test/loss=1.817516, test/num_examples=10000, total_duration=136846.917945, train/accuracy=0.961615, train/loss=0.144846, validation/accuracy=0.755900, validation/loss=1.051971, validation/num_examples=50000
I0301 19:15:46.599973 140407379851008 logging_writer.py:48] [392100] global_step=392100, grad_norm=4.204763889312744, loss=0.6113795042037964
I0301 19:16:20.302873 140407371458304 logging_writer.py:48] [392200] global_step=392200, grad_norm=4.4012131690979, loss=0.543167769908905
I0301 19:16:53.935937 140407379851008 logging_writer.py:48] [392300] global_step=392300, grad_norm=4.335414409637451, loss=0.6431787014007568
I0301 19:17:27.601744 140407371458304 logging_writer.py:48] [392400] global_step=392400, grad_norm=4.311905860900879, loss=0.6141042709350586
I0301 19:18:01.369962 140407379851008 logging_writer.py:48] [392500] global_step=392500, grad_norm=4.319235324859619, loss=0.6074267625808716
I0301 19:18:35.037746 140407371458304 logging_writer.py:48] [392600] global_step=392600, grad_norm=5.40447998046875, loss=0.6006847620010376
I0301 19:19:08.727341 140407379851008 logging_writer.py:48] [392700] global_step=392700, grad_norm=4.5998077392578125, loss=0.5708783864974976
I0301 19:19:42.426284 140407371458304 logging_writer.py:48] [392800] global_step=392800, grad_norm=5.222715854644775, loss=0.6107636094093323
I0301 19:20:16.105598 140407379851008 logging_writer.py:48] [392900] global_step=392900, grad_norm=4.735721588134766, loss=0.6159070134162903
I0301 19:20:49.754403 140407371458304 logging_writer.py:48] [393000] global_step=393000, grad_norm=4.819174766540527, loss=0.6877527832984924
I0301 19:21:23.701562 140407379851008 logging_writer.py:48] [393100] global_step=393100, grad_norm=4.163511276245117, loss=0.6095553040504456
I0301 19:21:57.368492 140407371458304 logging_writer.py:48] [393200] global_step=393200, grad_norm=4.33651876449585, loss=0.6198520064353943
I0301 19:22:31.128900 140407379851008 logging_writer.py:48] [393300] global_step=393300, grad_norm=4.445350646972656, loss=0.6039026975631714
I0301 19:23:04.782854 140407371458304 logging_writer.py:48] [393400] global_step=393400, grad_norm=4.769906044006348, loss=0.6102650165557861
I0301 19:23:38.513123 140407379851008 logging_writer.py:48] [393500] global_step=393500, grad_norm=4.592567443847656, loss=0.6257535219192505
I0301 19:23:53.152635 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:23:59.211473 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:24:07.984164 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:24:10.289263 140573303715648 submission_runner.py:411] Time since start: 137374.28s, 	Step: 393545, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.1471053957939148, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0518912076950073, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8179852962493896, 'test/num_examples': 10000, 'score': 132659.2893781662, 'total_duration': 137374.28175091743, 'accumulated_submission_time': 132659.2893781662, 'accumulated_eval_time': 4682.0501046180725, 'accumulated_logging_time': 17.397064208984375}
I0301 19:24:10.425652 140407379851008 logging_writer.py:48] [393545] accumulated_eval_time=4682.050105, accumulated_logging_time=17.397064, accumulated_submission_time=132659.289378, global_step=393545, preemption_count=0, score=132659.289378, test/accuracy=0.630600, test/loss=1.817985, test/num_examples=10000, total_duration=137374.281751, train/accuracy=0.960419, train/loss=0.147105, validation/accuracy=0.755720, validation/loss=1.051891, validation/num_examples=50000
I0301 19:24:29.292802 140408319375104 logging_writer.py:48] [393600] global_step=393600, grad_norm=4.282581329345703, loss=0.6574095487594604
I0301 19:25:02.942388 140407379851008 logging_writer.py:48] [393700] global_step=393700, grad_norm=4.427703380584717, loss=0.5884571075439453
I0301 19:25:36.659138 140408319375104 logging_writer.py:48] [393800] global_step=393800, grad_norm=4.3520050048828125, loss=0.5898587703704834
I0301 19:26:10.397341 140407379851008 logging_writer.py:48] [393900] global_step=393900, grad_norm=4.325663089752197, loss=0.6350565552711487
I0301 19:26:44.105771 140408319375104 logging_writer.py:48] [394000] global_step=394000, grad_norm=4.469906330108643, loss=0.660872757434845
I0301 19:27:17.982276 140407379851008 logging_writer.py:48] [394100] global_step=394100, grad_norm=4.499446392059326, loss=0.5839499831199646
I0301 19:27:51.638418 140408319375104 logging_writer.py:48] [394200] global_step=394200, grad_norm=4.489335536956787, loss=0.6241230964660645
I0301 19:28:25.375340 140407379851008 logging_writer.py:48] [394300] global_step=394300, grad_norm=4.329477310180664, loss=0.5975828766822815
I0301 19:28:59.070589 140408319375104 logging_writer.py:48] [394400] global_step=394400, grad_norm=4.42230749130249, loss=0.6286726593971252
I0301 19:29:32.787096 140407379851008 logging_writer.py:48] [394500] global_step=394500, grad_norm=4.953274726867676, loss=0.6239056587219238
I0301 19:30:06.499385 140408319375104 logging_writer.py:48] [394600] global_step=394600, grad_norm=5.008731842041016, loss=0.6043279767036438
I0301 19:30:40.239760 140407379851008 logging_writer.py:48] [394700] global_step=394700, grad_norm=5.2550740242004395, loss=0.7055675983428955
I0301 19:31:13.895341 140408319375104 logging_writer.py:48] [394800] global_step=394800, grad_norm=4.509800434112549, loss=0.5854263305664062
I0301 19:31:47.646150 140407379851008 logging_writer.py:48] [394900] global_step=394900, grad_norm=4.765107154846191, loss=0.6828648447990417
I0301 19:32:21.318315 140408319375104 logging_writer.py:48] [395000] global_step=395000, grad_norm=4.379465103149414, loss=0.6530039310455322
I0301 19:32:40.360471 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:32:46.425691 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:32:55.018435 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:32:57.275544 140573303715648 submission_runner.py:411] Time since start: 137901.27s, 	Step: 395058, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14874359965324402, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0518076419830322, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8184858560562134, 'test/num_examples': 10000, 'score': 133169.1497850418, 'total_duration': 137901.2680785656, 'accumulated_submission_time': 133169.1497850418, 'accumulated_eval_time': 4698.965122699738, 'accumulated_logging_time': 17.549309730529785}
I0301 19:32:57.364363 140409854486272 logging_writer.py:48] [395058] accumulated_eval_time=4698.965123, accumulated_logging_time=17.549310, accumulated_submission_time=133169.149785, global_step=395058, preemption_count=0, score=133169.149785, test/accuracy=0.632100, test/loss=1.818486, test/num_examples=10000, total_duration=137901.268079, train/accuracy=0.959961, train/loss=0.148744, validation/accuracy=0.755860, validation/loss=1.051808, validation/num_examples=50000
I0301 19:33:11.826248 140409862878976 logging_writer.py:48] [395100] global_step=395100, grad_norm=4.644092082977295, loss=0.5868282318115234
I0301 19:33:45.516945 140409854486272 logging_writer.py:48] [395200] global_step=395200, grad_norm=4.418881416320801, loss=0.6108481287956238
I0301 19:34:19.214994 140409862878976 logging_writer.py:48] [395300] global_step=395300, grad_norm=4.649324417114258, loss=0.6109662055969238
I0301 19:34:52.921420 140409854486272 logging_writer.py:48] [395400] global_step=395400, grad_norm=4.536108493804932, loss=0.6649948358535767
I0301 19:35:26.577150 140409862878976 logging_writer.py:48] [395500] global_step=395500, grad_norm=4.626463413238525, loss=0.6497406363487244
I0301 19:36:00.222731 140409854486272 logging_writer.py:48] [395600] global_step=395600, grad_norm=4.341488361358643, loss=0.5994530320167542
I0301 19:36:33.863669 140409862878976 logging_writer.py:48] [395700] global_step=395700, grad_norm=4.400083541870117, loss=0.5703415870666504
I0301 19:37:07.626291 140409854486272 logging_writer.py:48] [395800] global_step=395800, grad_norm=4.461648464202881, loss=0.6217153072357178
I0301 19:37:41.295033 140409862878976 logging_writer.py:48] [395900] global_step=395900, grad_norm=4.474955081939697, loss=0.6654411554336548
I0301 19:38:14.968646 140409854486272 logging_writer.py:48] [396000] global_step=396000, grad_norm=4.817095756530762, loss=0.6286867260932922
I0301 19:38:48.693308 140409862878976 logging_writer.py:48] [396100] global_step=396100, grad_norm=4.126638412475586, loss=0.5363016724586487
I0301 19:39:22.368135 140409854486272 logging_writer.py:48] [396200] global_step=396200, grad_norm=4.393011093139648, loss=0.613642692565918
I0301 19:39:56.079091 140409862878976 logging_writer.py:48] [396300] global_step=396300, grad_norm=4.19296407699585, loss=0.5796394348144531
I0301 19:40:29.837813 140409854486272 logging_writer.py:48] [396400] global_step=396400, grad_norm=4.911019802093506, loss=0.6687236428260803
I0301 19:41:03.501714 140409862878976 logging_writer.py:48] [396500] global_step=396500, grad_norm=4.825643539428711, loss=0.6914376616477966
I0301 19:41:27.556507 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:41:33.572449 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:41:42.299771 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:41:44.618084 140573303715648 submission_runner.py:411] Time since start: 138428.61s, 	Step: 396573, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.14444853365421295, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0519706010818481, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8179006576538086, 'test/num_examples': 10000, 'score': 133679.27163481712, 'total_duration': 138428.61062049866, 'accumulated_submission_time': 133679.27163481712, 'accumulated_eval_time': 4716.026647567749, 'accumulated_logging_time': 17.64861249923706}
I0301 19:41:44.702157 140407371458304 logging_writer.py:48] [396573] accumulated_eval_time=4716.026648, accumulated_logging_time=17.648612, accumulated_submission_time=133679.271635, global_step=396573, preemption_count=0, score=133679.271635, test/accuracy=0.630600, test/loss=1.817901, test/num_examples=10000, total_duration=138428.610620, train/accuracy=0.961436, train/loss=0.144449, validation/accuracy=0.755940, validation/loss=1.051971, validation/num_examples=50000
I0301 19:41:54.178131 140407379851008 logging_writer.py:48] [396600] global_step=396600, grad_norm=4.64398717880249, loss=0.6338515877723694
I0301 19:42:27.822395 140407371458304 logging_writer.py:48] [396700] global_step=396700, grad_norm=4.866178035736084, loss=0.5998496413230896
I0301 19:43:01.503577 140407379851008 logging_writer.py:48] [396800] global_step=396800, grad_norm=4.361466407775879, loss=0.597376823425293
I0301 19:43:35.236758 140407371458304 logging_writer.py:48] [396900] global_step=396900, grad_norm=4.7188029289245605, loss=0.5927548408508301
I0301 19:44:08.914828 140407379851008 logging_writer.py:48] [397000] global_step=397000, grad_norm=4.602262496948242, loss=0.6107786297798157
I0301 19:44:42.621896 140407371458304 logging_writer.py:48] [397100] global_step=397100, grad_norm=4.779541492462158, loss=0.5788013935089111
I0301 19:45:16.306309 140407379851008 logging_writer.py:48] [397200] global_step=397200, grad_norm=4.510985374450684, loss=0.6494662761688232
I0301 19:45:50.184534 140407371458304 logging_writer.py:48] [397300] global_step=397300, grad_norm=4.57993221282959, loss=0.6654277443885803
I0301 19:46:23.890636 140407379851008 logging_writer.py:48] [397400] global_step=397400, grad_norm=4.388181686401367, loss=0.6497265696525574
I0301 19:46:57.553160 140407371458304 logging_writer.py:48] [397500] global_step=397500, grad_norm=4.526754856109619, loss=0.644752025604248
I0301 19:47:31.285869 140407379851008 logging_writer.py:48] [397600] global_step=397600, grad_norm=4.697291851043701, loss=0.6079575419425964
I0301 19:48:04.931669 140407371458304 logging_writer.py:48] [397700] global_step=397700, grad_norm=4.627471923828125, loss=0.6021817922592163
I0301 19:48:38.617236 140407379851008 logging_writer.py:48] [397800] global_step=397800, grad_norm=4.141774654388428, loss=0.5416669845581055
I0301 19:49:12.381297 140407371458304 logging_writer.py:48] [397900] global_step=397900, grad_norm=4.438681125640869, loss=0.6778085827827454
I0301 19:49:46.088996 140407379851008 logging_writer.py:48] [398000] global_step=398000, grad_norm=4.6212592124938965, loss=0.6420744061470032
I0301 19:50:14.908855 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:50:20.889496 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:50:29.654213 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:50:31.882974 140573303715648 submission_runner.py:411] Time since start: 138955.88s, 	Step: 398087, 	{'train/accuracy': 0.96195387840271, 'train/loss': 0.14406336843967438, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0524848699569702, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8180168867111206, 'test/num_examples': 10000, 'score': 134189.4073984623, 'total_duration': 138955.87549710274, 'accumulated_submission_time': 134189.4073984623, 'accumulated_eval_time': 4733.000705003738, 'accumulated_logging_time': 17.743713855743408}
I0301 19:50:31.972183 140409837700864 logging_writer.py:48] [398087] accumulated_eval_time=4733.000705, accumulated_logging_time=17.743714, accumulated_submission_time=134189.407398, global_step=398087, preemption_count=0, score=134189.407398, test/accuracy=0.631100, test/loss=1.818017, test/num_examples=10000, total_duration=138955.875497, train/accuracy=0.961954, train/loss=0.144063, validation/accuracy=0.755360, validation/loss=1.052485, validation/num_examples=50000
I0301 19:50:36.689408 140409846093568 logging_writer.py:48] [398100] global_step=398100, grad_norm=4.602023601531982, loss=0.6642352342605591
I0301 19:51:10.265105 140409837700864 logging_writer.py:48] [398200] global_step=398200, grad_norm=4.671847343444824, loss=0.6359018087387085
I0301 19:51:44.003561 140409846093568 logging_writer.py:48] [398300] global_step=398300, grad_norm=4.854189872741699, loss=0.6935127973556519
I0301 19:52:17.696465 140409837700864 logging_writer.py:48] [398400] global_step=398400, grad_norm=4.524845123291016, loss=0.7023443579673767
I0301 19:52:51.349013 140409846093568 logging_writer.py:48] [398500] global_step=398500, grad_norm=4.533863544464111, loss=0.6144493222236633
I0301 19:53:25.030580 140409837700864 logging_writer.py:48] [398600] global_step=398600, grad_norm=4.253780364990234, loss=0.5510576963424683
I0301 19:53:58.780905 140409846093568 logging_writer.py:48] [398700] global_step=398700, grad_norm=4.536936283111572, loss=0.653803288936615
I0301 19:54:32.429774 140409837700864 logging_writer.py:48] [398800] global_step=398800, grad_norm=5.03957986831665, loss=0.6375772953033447
I0301 19:55:06.111029 140409846093568 logging_writer.py:48] [398900] global_step=398900, grad_norm=4.70034646987915, loss=0.6266084909439087
I0301 19:55:39.802008 140409837700864 logging_writer.py:48] [399000] global_step=399000, grad_norm=4.187130928039551, loss=0.5203129053115845
I0301 19:56:13.458398 140409846093568 logging_writer.py:48] [399100] global_step=399100, grad_norm=4.455107688903809, loss=0.6502819061279297
I0301 19:56:47.174834 140409837700864 logging_writer.py:48] [399200] global_step=399200, grad_norm=4.7973480224609375, loss=0.6499946713447571
I0301 19:57:20.900910 140409846093568 logging_writer.py:48] [399300] global_step=399300, grad_norm=4.63368558883667, loss=0.694068193435669
I0301 19:57:54.668568 140409837700864 logging_writer.py:48] [399400] global_step=399400, grad_norm=4.259829044342041, loss=0.5861018896102905
I0301 19:58:28.355334 140409846093568 logging_writer.py:48] [399500] global_step=399500, grad_norm=4.591733932495117, loss=0.5923285484313965
I0301 19:59:02.026393 140409837700864 logging_writer.py:48] [399600] global_step=399600, grad_norm=4.78836727142334, loss=0.6055462956428528
I0301 19:59:02.034269 140573303715648 spec.py:321] Evaluating on the training split.
I0301 19:59:08.337620 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 19:59:16.958023 140573303715648 spec.py:349] Evaluating on the test split.
I0301 19:59:19.250227 140573303715648 submission_runner.py:411] Time since start: 139483.24s, 	Step: 399601, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14742715656757355, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0510292053222656, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8171703815460205, 'test/num_examples': 10000, 'score': 134699.39883041382, 'total_duration': 139483.24274683, 'accumulated_submission_time': 134699.39883041382, 'accumulated_eval_time': 4750.216578006744, 'accumulated_logging_time': 17.843615293502808}
I0301 19:59:19.335062 140407371458304 logging_writer.py:48] [399601] accumulated_eval_time=4750.216578, accumulated_logging_time=17.843615, accumulated_submission_time=134699.398830, global_step=399601, preemption_count=0, score=134699.398830, test/accuracy=0.631400, test/loss=1.817170, test/num_examples=10000, total_duration=139483.242747, train/accuracy=0.959801, train/loss=0.147427, validation/accuracy=0.756080, validation/loss=1.051029, validation/num_examples=50000
I0301 19:59:53.044252 140407379851008 logging_writer.py:48] [399700] global_step=399700, grad_norm=4.883697986602783, loss=0.6385790109634399
I0301 20:00:26.709151 140407371458304 logging_writer.py:48] [399800] global_step=399800, grad_norm=5.036426067352295, loss=0.7136115431785583
I0301 20:01:00.442782 140407379851008 logging_writer.py:48] [399900] global_step=399900, grad_norm=4.3128485679626465, loss=0.6203446984291077
I0301 20:01:34.099518 140407371458304 logging_writer.py:48] [400000] global_step=400000, grad_norm=4.465862274169922, loss=0.610431969165802
I0301 20:02:07.764957 140407379851008 logging_writer.py:48] [400100] global_step=400100, grad_norm=4.257697105407715, loss=0.6138078570365906
I0301 20:02:41.513518 140407371458304 logging_writer.py:48] [400200] global_step=400200, grad_norm=4.363799095153809, loss=0.6415283679962158
I0301 20:03:15.187590 140407379851008 logging_writer.py:48] [400300] global_step=400300, grad_norm=4.862093925476074, loss=0.6936526298522949
I0301 20:03:48.851051 140407371458304 logging_writer.py:48] [400400] global_step=400400, grad_norm=4.548155784606934, loss=0.6074817180633545
I0301 20:04:22.611548 140407379851008 logging_writer.py:48] [400500] global_step=400500, grad_norm=4.738152980804443, loss=0.6519859433174133
I0301 20:04:56.340109 140407371458304 logging_writer.py:48] [400600] global_step=400600, grad_norm=4.234768390655518, loss=0.6036456823348999
I0301 20:05:30.013385 140407379851008 logging_writer.py:48] [400700] global_step=400700, grad_norm=4.307309150695801, loss=0.6119847297668457
I0301 20:06:03.745692 140407371458304 logging_writer.py:48] [400800] global_step=400800, grad_norm=4.377451419830322, loss=0.6067607402801514
I0301 20:06:37.444683 140407379851008 logging_writer.py:48] [400900] global_step=400900, grad_norm=4.284883499145508, loss=0.6043762564659119
I0301 20:07:11.174827 140407371458304 logging_writer.py:48] [401000] global_step=401000, grad_norm=4.540687084197998, loss=0.6124546527862549
I0301 20:07:44.852861 140407379851008 logging_writer.py:48] [401100] global_step=401100, grad_norm=5.2368011474609375, loss=0.6704530119895935
I0301 20:07:49.390722 140573303715648 spec.py:321] Evaluating on the training split.
I0301 20:07:55.501368 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 20:08:04.268345 140573303715648 spec.py:349] Evaluating on the test split.
I0301 20:08:06.582335 140573303715648 submission_runner.py:411] Time since start: 140010.57s, 	Step: 401115, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14720028638839722, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0520803928375244, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8182260990142822, 'test/num_examples': 10000, 'score': 135209.38359832764, 'total_duration': 140010.5748746395, 'accumulated_submission_time': 135209.38359832764, 'accumulated_eval_time': 4767.408142089844, 'accumulated_logging_time': 17.938778400421143}
I0301 20:08:06.666118 140407371458304 logging_writer.py:48] [401115] accumulated_eval_time=4767.408142, accumulated_logging_time=17.938778, accumulated_submission_time=135209.383598, global_step=401115, preemption_count=0, score=135209.383598, test/accuracy=0.631600, test/loss=1.818226, test/num_examples=10000, total_duration=140010.574875, train/accuracy=0.960539, train/loss=0.147200, validation/accuracy=0.755920, validation/loss=1.052080, validation/num_examples=50000
I0301 20:08:35.592304 140407379851008 logging_writer.py:48] [401200] global_step=401200, grad_norm=4.8119635581970215, loss=0.6200555562973022
I0301 20:09:09.307831 140407371458304 logging_writer.py:48] [401300] global_step=401300, grad_norm=5.01497220993042, loss=0.6539987921714783
I0301 20:09:42.974939 140407379851008 logging_writer.py:48] [401400] global_step=401400, grad_norm=4.578540802001953, loss=0.5732729434967041
I0301 20:10:16.819057 140407371458304 logging_writer.py:48] [401500] global_step=401500, grad_norm=4.781126976013184, loss=0.6167994737625122
I0301 20:10:50.542704 140407379851008 logging_writer.py:48] [401600] global_step=401600, grad_norm=4.592334747314453, loss=0.5395839810371399
I0301 20:11:24.204488 140407371458304 logging_writer.py:48] [401700] global_step=401700, grad_norm=4.698409080505371, loss=0.5798380374908447
I0301 20:11:57.878134 140407379851008 logging_writer.py:48] [401800] global_step=401800, grad_norm=4.3050971031188965, loss=0.5524904131889343
I0301 20:12:31.575648 140407371458304 logging_writer.py:48] [401900] global_step=401900, grad_norm=4.572719573974609, loss=0.6719909906387329
I0301 20:13:05.273910 140407379851008 logging_writer.py:48] [402000] global_step=402000, grad_norm=4.837393760681152, loss=0.6355724930763245
I0301 20:13:38.978486 140407371458304 logging_writer.py:48] [402100] global_step=402100, grad_norm=4.6381402015686035, loss=0.637112557888031
I0301 20:14:12.707496 140407379851008 logging_writer.py:48] [402200] global_step=402200, grad_norm=4.364574909210205, loss=0.6472839117050171
I0301 20:14:46.403325 140407371458304 logging_writer.py:48] [402300] global_step=402300, grad_norm=4.765072822570801, loss=0.6223908066749573
I0301 20:15:20.154848 140407379851008 logging_writer.py:48] [402400] global_step=402400, grad_norm=4.948991775512695, loss=0.610697865486145
I0301 20:15:53.859476 140407371458304 logging_writer.py:48] [402500] global_step=402500, grad_norm=4.45180082321167, loss=0.5500123500823975
I0301 20:16:27.723428 140407379851008 logging_writer.py:48] [402600] global_step=402600, grad_norm=4.704301834106445, loss=0.6948743462562561
I0301 20:16:36.631203 140573303715648 spec.py:321] Evaluating on the training split.
I0301 20:16:42.666143 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 20:16:51.299889 140573303715648 spec.py:349] Evaluating on the test split.
I0301 20:16:53.520238 140573303715648 submission_runner.py:411] Time since start: 140537.51s, 	Step: 402628, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.1483696550130844, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0520527362823486, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8170477151870728, 'test/num_examples': 10000, 'score': 135719.27980732918, 'total_duration': 140537.51276612282, 'accumulated_submission_time': 135719.27980732918, 'accumulated_eval_time': 4784.297115325928, 'accumulated_logging_time': 18.03217339515686}
I0301 20:16:53.606180 140408319375104 logging_writer.py:48] [402628] accumulated_eval_time=4784.297115, accumulated_logging_time=18.032173, accumulated_submission_time=135719.279807, global_step=402628, preemption_count=0, score=135719.279807, test/accuracy=0.631400, test/loss=1.817048, test/num_examples=10000, total_duration=140537.512766, train/accuracy=0.959961, train/loss=0.148370, validation/accuracy=0.755480, validation/loss=1.052053, validation/num_examples=50000
I0301 20:17:18.196164 140409124681472 logging_writer.py:48] [402700] global_step=402700, grad_norm=5.136935710906982, loss=0.7212262153625488
I0301 20:17:51.877124 140408319375104 logging_writer.py:48] [402800] global_step=402800, grad_norm=4.474039077758789, loss=0.5746337175369263
I0301 20:18:25.567266 140409124681472 logging_writer.py:48] [402900] global_step=402900, grad_norm=4.417942047119141, loss=0.6165305972099304
I0301 20:18:59.287157 140408319375104 logging_writer.py:48] [403000] global_step=403000, grad_norm=4.387556552886963, loss=0.5803149938583374
I0301 20:19:32.995167 140409124681472 logging_writer.py:48] [403100] global_step=403100, grad_norm=4.061926364898682, loss=0.5350506901741028
I0301 20:20:06.709630 140408319375104 logging_writer.py:48] [403200] global_step=403200, grad_norm=4.13552188873291, loss=0.5545214414596558
I0301 20:20:40.409897 140409124681472 logging_writer.py:48] [403300] global_step=403300, grad_norm=4.630893230438232, loss=0.7128305435180664
I0301 20:21:14.090951 140408319375104 logging_writer.py:48] [403400] global_step=403400, grad_norm=4.398993015289307, loss=0.6465058326721191
I0301 20:21:47.865850 140409124681472 logging_writer.py:48] [403500] global_step=403500, grad_norm=4.158064842224121, loss=0.6122219562530518
I0301 20:22:21.637192 140408319375104 logging_writer.py:48] [403600] global_step=403600, grad_norm=4.492546081542969, loss=0.6362173557281494
I0301 20:22:55.317158 140409124681472 logging_writer.py:48] [403700] global_step=403700, grad_norm=4.787919998168945, loss=0.6709650158882141
I0301 20:23:29.010255 140408319375104 logging_writer.py:48] [403800] global_step=403800, grad_norm=4.439793586730957, loss=0.6602182984352112
I0301 20:24:02.713691 140409124681472 logging_writer.py:48] [403900] global_step=403900, grad_norm=4.390744209289551, loss=0.6331449747085571
I0301 20:24:36.450206 140408319375104 logging_writer.py:48] [404000] global_step=404000, grad_norm=4.423614501953125, loss=0.5974806547164917
I0301 20:25:10.229021 140409124681472 logging_writer.py:48] [404100] global_step=404100, grad_norm=4.4947614669799805, loss=0.7008999586105347
I0301 20:25:23.526285 140573303715648 spec.py:321] Evaluating on the training split.
I0301 20:25:29.590385 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 20:25:38.260459 140573303715648 spec.py:349] Evaluating on the test split.
I0301 20:25:40.569870 140573303715648 submission_runner.py:411] Time since start: 141064.56s, 	Step: 404141, 	{'train/accuracy': 0.9599409699440002, 'train/loss': 0.1486997753381729, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.05121648311615, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.81659734249115, 'test/num_examples': 10000, 'score': 136229.12922286987, 'total_duration': 141064.5624115467, 'accumulated_submission_time': 136229.12922286987, 'accumulated_eval_time': 4801.340654373169, 'accumulated_logging_time': 18.129441499710083}
I0301 20:25:40.659317 140409862878976 logging_writer.py:48] [404141] accumulated_eval_time=4801.340654, accumulated_logging_time=18.129441, accumulated_submission_time=136229.129223, global_step=404141, preemption_count=0, score=136229.129223, test/accuracy=0.630200, test/loss=1.816597, test/num_examples=10000, total_duration=141064.562412, train/accuracy=0.959941, train/loss=0.148700, validation/accuracy=0.755920, validation/loss=1.051216, validation/num_examples=50000
I0301 20:26:00.851077 140410416518912 logging_writer.py:48] [404200] global_step=404200, grad_norm=4.27322244644165, loss=0.567505955696106
I0301 20:26:34.585747 140409862878976 logging_writer.py:48] [404300] global_step=404300, grad_norm=4.689543724060059, loss=0.6241966485977173
I0301 20:27:08.241880 140410416518912 logging_writer.py:48] [404400] global_step=404400, grad_norm=4.440389633178711, loss=0.6310644745826721
I0301 20:27:41.961654 140409862878976 logging_writer.py:48] [404500] global_step=404500, grad_norm=4.645217418670654, loss=0.6477670669555664
I0301 20:28:15.636761 140410416518912 logging_writer.py:48] [404600] global_step=404600, grad_norm=4.521373748779297, loss=0.5996071696281433
I0301 20:28:49.418096 140409862878976 logging_writer.py:48] [404700] global_step=404700, grad_norm=4.632709503173828, loss=0.6176275014877319
I0301 20:29:23.081795 140410416518912 logging_writer.py:48] [404800] global_step=404800, grad_norm=4.467976093292236, loss=0.6046853065490723
I0301 20:29:56.798792 140409862878976 logging_writer.py:48] [404900] global_step=404900, grad_norm=4.632622718811035, loss=0.6876829862594604
I0301 20:30:30.464217 140410416518912 logging_writer.py:48] [405000] global_step=405000, grad_norm=4.214545249938965, loss=0.5706697702407837
I0301 20:31:04.148822 140409862878976 logging_writer.py:48] [405100] global_step=405100, grad_norm=4.420687675476074, loss=0.6189451217651367
I0301 20:31:37.792152 140410416518912 logging_writer.py:48] [405200] global_step=405200, grad_norm=4.495203018188477, loss=0.6076529026031494
I0301 20:32:11.436132 140409862878976 logging_writer.py:48] [405300] global_step=405300, grad_norm=4.703759670257568, loss=0.5957562327384949
I0301 20:32:45.118321 140410416518912 logging_writer.py:48] [405400] global_step=405400, grad_norm=4.944070816040039, loss=0.6143567562103271
I0301 20:33:18.865997 140409862878976 logging_writer.py:48] [405500] global_step=405500, grad_norm=4.569480895996094, loss=0.6049010753631592
I0301 20:33:52.568779 140410416518912 logging_writer.py:48] [405600] global_step=405600, grad_norm=5.005288600921631, loss=0.6301319599151611
I0301 20:34:10.614156 140573303715648 spec.py:321] Evaluating on the training split.
I0301 20:34:16.720244 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 20:34:25.368989 140573303715648 spec.py:349] Evaluating on the test split.
I0301 20:34:27.772952 140573303715648 submission_runner.py:411] Time since start: 141591.77s, 	Step: 405655, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14601023495197296, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.051106333732605, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8162307739257812, 'test/num_examples': 10000, 'score': 136739.0093715191, 'total_duration': 141591.76547813416, 'accumulated_submission_time': 136739.0093715191, 'accumulated_eval_time': 4818.499400377274, 'accumulated_logging_time': 18.232341051101685}
I0301 20:34:27.854591 140407379851008 logging_writer.py:48] [405655] accumulated_eval_time=4818.499400, accumulated_logging_time=18.232341, accumulated_submission_time=136739.009372, global_step=405655, preemption_count=0, score=136739.009372, test/accuracy=0.630700, test/loss=1.816231, test/num_examples=10000, total_duration=141591.765478, train/accuracy=0.961416, train/loss=0.146010, validation/accuracy=0.755640, validation/loss=1.051106, validation/num_examples=50000
I0301 20:34:43.432291 140408319375104 logging_writer.py:48] [405700] global_step=405700, grad_norm=5.00696325302124, loss=0.6398722529411316
I0301 20:35:17.094463 140407379851008 logging_writer.py:48] [405800] global_step=405800, grad_norm=4.488490581512451, loss=0.6361297369003296
I0301 20:35:50.819031 140408319375104 logging_writer.py:48] [405900] global_step=405900, grad_norm=5.1977081298828125, loss=0.6248557567596436
I0301 20:36:24.595552 140407379851008 logging_writer.py:48] [406000] global_step=406000, grad_norm=4.2708916664123535, loss=0.5734276175498962
I0301 20:36:58.300628 140408319375104 logging_writer.py:48] [406100] global_step=406100, grad_norm=4.231391429901123, loss=0.5565831661224365
I0301 20:37:32.006474 140407379851008 logging_writer.py:48] [406200] global_step=406200, grad_norm=4.499108791351318, loss=0.6458990573883057
I0301 20:38:05.696092 140408319375104 logging_writer.py:48] [406300] global_step=406300, grad_norm=4.745631694793701, loss=0.5762509107589722
I0301 20:38:39.435879 140407379851008 logging_writer.py:48] [406400] global_step=406400, grad_norm=4.571310043334961, loss=0.5479511618614197
I0301 20:39:13.138238 140408319375104 logging_writer.py:48] [406500] global_step=406500, grad_norm=4.953179359436035, loss=0.623153805732727
I0301 20:39:46.823950 140407379851008 logging_writer.py:48] [406600] global_step=406600, grad_norm=4.488376617431641, loss=0.6209778785705566
I0301 20:40:20.533707 140408319375104 logging_writer.py:48] [406700] global_step=406700, grad_norm=5.270411491394043, loss=0.6212185621261597
I0301 20:40:54.308744 140407379851008 logging_writer.py:48] [406800] global_step=406800, grad_norm=4.856384754180908, loss=0.7130444645881653
I0301 20:41:28.017495 140408319375104 logging_writer.py:48] [406900] global_step=406900, grad_norm=4.302392959594727, loss=0.6403971910476685
I0301 20:42:01.696821 140407379851008 logging_writer.py:48] [407000] global_step=407000, grad_norm=4.476726055145264, loss=0.6305301785469055
I0301 20:42:35.433315 140408319375104 logging_writer.py:48] [407100] global_step=407100, grad_norm=4.894837856292725, loss=0.6459763050079346
I0301 20:42:57.858378 140573303715648 spec.py:321] Evaluating on the training split.
I0301 20:43:04.099694 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 20:43:12.776199 140573303715648 spec.py:349] Evaluating on the test split.
I0301 20:43:15.084910 140573303715648 submission_runner.py:411] Time since start: 142119.08s, 	Step: 407168, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.1470111906528473, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0524506568908691, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8184983730316162, 'test/num_examples': 10000, 'score': 137248.94324493408, 'total_duration': 142119.07744646072, 'accumulated_submission_time': 137248.94324493408, 'accumulated_eval_time': 4835.725882053375, 'accumulated_logging_time': 18.32414746284485}
I0301 20:43:15.175423 140409846093568 logging_writer.py:48] [407168] accumulated_eval_time=4835.725882, accumulated_logging_time=18.324147, accumulated_submission_time=137248.943245, global_step=407168, preemption_count=0, score=137248.943245, test/accuracy=0.631200, test/loss=1.818498, test/num_examples=10000, total_duration=142119.077446, train/accuracy=0.960858, train/loss=0.147011, validation/accuracy=0.755920, validation/loss=1.052451, validation/num_examples=50000
I0301 20:43:26.304281 140409854486272 logging_writer.py:48] [407200] global_step=407200, grad_norm=4.657719135284424, loss=0.6802907586097717
I0301 20:43:59.958317 140409846093568 logging_writer.py:48] [407300] global_step=407300, grad_norm=4.643426418304443, loss=0.634387731552124
I0301 20:44:33.634400 140409854486272 logging_writer.py:48] [407400] global_step=407400, grad_norm=4.0731201171875, loss=0.5367527008056641
I0301 20:45:07.351883 140409846093568 logging_writer.py:48] [407500] global_step=407500, grad_norm=4.087269306182861, loss=0.5331440567970276
I0301 20:45:41.038307 140409854486272 logging_writer.py:48] [407600] global_step=407600, grad_norm=4.88946008682251, loss=0.6530886292457581
I0301 20:46:14.686387 140409846093568 logging_writer.py:48] [407700] global_step=407700, grad_norm=4.427149772644043, loss=0.6799749732017517
I0301 20:46:48.414922 140409854486272 logging_writer.py:48] [407800] global_step=407800, grad_norm=4.2650065422058105, loss=0.6560114622116089
I0301 20:47:22.221499 140409846093568 logging_writer.py:48] [407900] global_step=407900, grad_norm=4.746518611907959, loss=0.6642177104949951
I0301 20:47:55.931135 140409854486272 logging_writer.py:48] [408000] global_step=408000, grad_norm=4.151421546936035, loss=0.5878598690032959
I0301 20:48:29.647309 140409846093568 logging_writer.py:48] [408100] global_step=408100, grad_norm=4.726045608520508, loss=0.620586633682251
I0301 20:49:03.319521 140409854486272 logging_writer.py:48] [408200] global_step=408200, grad_norm=4.61285400390625, loss=0.5356369018554688
I0301 20:49:37.059861 140409846093568 logging_writer.py:48] [408300] global_step=408300, grad_norm=4.098773956298828, loss=0.58966463804245
I0301 20:50:10.784005 140409854486272 logging_writer.py:48] [408400] global_step=408400, grad_norm=4.577248573303223, loss=0.5672420263290405
I0301 20:50:44.515057 140409846093568 logging_writer.py:48] [408500] global_step=408500, grad_norm=4.522388935089111, loss=0.6447182297706604
I0301 20:51:18.235170 140409854486272 logging_writer.py:48] [408600] global_step=408600, grad_norm=4.599256992340088, loss=0.5453824996948242
I0301 20:51:45.309381 140573303715648 spec.py:321] Evaluating on the training split.
I0301 20:51:51.367492 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 20:52:00.042355 140573303715648 spec.py:349] Evaluating on the test split.
I0301 20:52:02.346879 140573303715648 submission_runner.py:411] Time since start: 142646.34s, 	Step: 408682, 	{'train/accuracy': 0.9594826102256775, 'train/loss': 0.15044201910495758, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.0528854131698608, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8195046186447144, 'test/num_examples': 10000, 'score': 137759.00602436066, 'total_duration': 142646.33941054344, 'accumulated_submission_time': 137759.00602436066, 'accumulated_eval_time': 4852.763338327408, 'accumulated_logging_time': 18.42547035217285}
I0301 20:52:02.438279 140409124681472 logging_writer.py:48] [408682] accumulated_eval_time=4852.763338, accumulated_logging_time=18.425470, accumulated_submission_time=137759.006024, global_step=408682, preemption_count=0, score=137759.006024, test/accuracy=0.631200, test/loss=1.819505, test/num_examples=10000, total_duration=142646.339411, train/accuracy=0.959483, train/loss=0.150442, validation/accuracy=0.755500, validation/loss=1.052885, validation/num_examples=50000
I0301 20:52:08.828989 140409829308160 logging_writer.py:48] [408700] global_step=408700, grad_norm=5.442540645599365, loss=0.7427335977554321
I0301 20:52:42.479713 140409124681472 logging_writer.py:48] [408800] global_step=408800, grad_norm=4.3879852294921875, loss=0.5807141065597534
I0301 20:53:16.249574 140409829308160 logging_writer.py:48] [408900] global_step=408900, grad_norm=4.515197277069092, loss=0.6583988070487976
I0301 20:53:49.977976 140409124681472 logging_writer.py:48] [409000] global_step=409000, grad_norm=4.773667335510254, loss=0.5794118642807007
I0301 20:54:23.632447 140409829308160 logging_writer.py:48] [409100] global_step=409100, grad_norm=4.420971393585205, loss=0.576299250125885
I0301 20:54:57.345868 140409124681472 logging_writer.py:48] [409200] global_step=409200, grad_norm=4.8853654861450195, loss=0.6186380386352539
I0301 20:55:31.048648 140409829308160 logging_writer.py:48] [409300] global_step=409300, grad_norm=4.381998538970947, loss=0.6419508457183838
I0301 20:56:04.743500 140409124681472 logging_writer.py:48] [409400] global_step=409400, grad_norm=4.1898603439331055, loss=0.5666192770004272
I0301 20:56:38.454476 140409829308160 logging_writer.py:48] [409500] global_step=409500, grad_norm=4.375107765197754, loss=0.6275364756584167
I0301 20:57:12.159432 140409124681472 logging_writer.py:48] [409600] global_step=409600, grad_norm=4.505657196044922, loss=0.6504664421081543
I0301 20:57:45.845696 140409829308160 logging_writer.py:48] [409700] global_step=409700, grad_norm=4.657544136047363, loss=0.6350001692771912
I0301 20:58:19.488489 140409124681472 logging_writer.py:48] [409800] global_step=409800, grad_norm=4.442680835723877, loss=0.6740222573280334
I0301 20:58:53.159564 140409829308160 logging_writer.py:48] [409900] global_step=409900, grad_norm=4.324113368988037, loss=0.6515575647354126
I0301 20:59:26.970002 140409124681472 logging_writer.py:48] [410000] global_step=410000, grad_norm=4.485006809234619, loss=0.6354435682296753
I0301 21:00:00.672060 140409829308160 logging_writer.py:48] [410100] global_step=410100, grad_norm=4.7436723709106445, loss=0.6668870449066162
I0301 21:00:32.437989 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:00:38.460826 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:00:47.015418 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:00:49.289620 140573303715648 submission_runner.py:411] Time since start: 143173.28s, 	Step: 410196, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14655140042304993, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.051986813545227, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8148283958435059, 'test/num_examples': 10000, 'score': 138268.93678069115, 'total_duration': 143173.28214502335, 'accumulated_submission_time': 138268.93678069115, 'accumulated_eval_time': 4869.61491060257, 'accumulated_logging_time': 18.52701497077942}
I0301 21:00:49.384318 140409862878976 logging_writer.py:48] [410196] accumulated_eval_time=4869.614911, accumulated_logging_time=18.527015, accumulated_submission_time=138268.936781, global_step=410196, preemption_count=0, score=138268.936781, test/accuracy=0.630800, test/loss=1.814828, test/num_examples=10000, total_duration=143173.282145, train/accuracy=0.961057, train/loss=0.146551, validation/accuracy=0.755420, validation/loss=1.051987, validation/num_examples=50000
I0301 21:00:51.074957 140410416518912 logging_writer.py:48] [410200] global_step=410200, grad_norm=4.259533405303955, loss=0.5803394913673401
I0301 21:01:24.701482 140409862878976 logging_writer.py:48] [410300] global_step=410300, grad_norm=4.629549980163574, loss=0.6167198419570923
I0301 21:01:58.344774 140410416518912 logging_writer.py:48] [410400] global_step=410400, grad_norm=4.388918876647949, loss=0.6416535973548889
I0301 21:02:31.994836 140409862878976 logging_writer.py:48] [410500] global_step=410500, grad_norm=4.469271183013916, loss=0.6433489322662354
I0301 21:03:05.648406 140410416518912 logging_writer.py:48] [410600] global_step=410600, grad_norm=4.591733932495117, loss=0.6181938052177429
I0301 21:03:39.372479 140409862878976 logging_writer.py:48] [410700] global_step=410700, grad_norm=4.771202564239502, loss=0.6278059482574463
I0301 21:04:13.009505 140410416518912 logging_writer.py:48] [410800] global_step=410800, grad_norm=4.664548873901367, loss=0.6941361427307129
I0301 21:04:46.670985 140409862878976 logging_writer.py:48] [410900] global_step=410900, grad_norm=4.458696365356445, loss=0.6025955677032471
I0301 21:05:20.420558 140410416518912 logging_writer.py:48] [411000] global_step=411000, grad_norm=4.418477535247803, loss=0.638387143611908
I0301 21:05:54.105532 140409862878976 logging_writer.py:48] [411100] global_step=411100, grad_norm=4.9877142906188965, loss=0.6394168138504028
I0301 21:06:27.790595 140410416518912 logging_writer.py:48] [411200] global_step=411200, grad_norm=4.857760906219482, loss=0.6913149356842041
I0301 21:07:01.517356 140409862878976 logging_writer.py:48] [411300] global_step=411300, grad_norm=4.654140949249268, loss=0.6256012916564941
I0301 21:07:35.169401 140410416518912 logging_writer.py:48] [411400] global_step=411400, grad_norm=4.759093284606934, loss=0.6375396847724915
I0301 21:08:08.842706 140409862878976 logging_writer.py:48] [411500] global_step=411500, grad_norm=4.314515113830566, loss=0.5854399800300598
I0301 21:08:42.468031 140410416518912 logging_writer.py:48] [411600] global_step=411600, grad_norm=4.500029563903809, loss=0.5823359489440918
I0301 21:09:16.162083 140409862878976 logging_writer.py:48] [411700] global_step=411700, grad_norm=4.421360492706299, loss=0.6008426547050476
I0301 21:09:19.348325 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:09:25.422574 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:09:34.055330 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:09:36.456035 140573303715648 submission_runner.py:411] Time since start: 143700.45s, 	Step: 411711, 	{'train/accuracy': 0.9595025181770325, 'train/loss': 0.14963343739509583, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0508095026016235, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.816946029663086, 'test/num_examples': 10000, 'score': 138778.83112430573, 'total_duration': 143700.44857931137, 'accumulated_submission_time': 138778.83112430573, 'accumulated_eval_time': 4886.722577571869, 'accumulated_logging_time': 18.632123947143555}
I0301 21:09:36.541696 140408319375104 logging_writer.py:48] [411711] accumulated_eval_time=4886.722578, accumulated_logging_time=18.632124, accumulated_submission_time=138778.831124, global_step=411711, preemption_count=0, score=138778.831124, test/accuracy=0.631100, test/loss=1.816946, test/num_examples=10000, total_duration=143700.448579, train/accuracy=0.959503, train/loss=0.149633, validation/accuracy=0.755900, validation/loss=1.050810, validation/num_examples=50000
I0301 21:10:06.795492 140409124681472 logging_writer.py:48] [411800] global_step=411800, grad_norm=4.1278886795043945, loss=0.6293696165084839
I0301 21:10:40.474742 140408319375104 logging_writer.py:48] [411900] global_step=411900, grad_norm=4.8871989250183105, loss=0.618456244468689
I0301 21:11:14.177755 140409124681472 logging_writer.py:48] [412000] global_step=412000, grad_norm=3.837942123413086, loss=0.5449489951133728
I0301 21:11:47.974642 140408319375104 logging_writer.py:48] [412100] global_step=412100, grad_norm=4.237799644470215, loss=0.5756950378417969
I0301 21:12:21.652983 140409124681472 logging_writer.py:48] [412200] global_step=412200, grad_norm=4.5627899169921875, loss=0.5756514072418213
I0301 21:12:55.322386 140408319375104 logging_writer.py:48] [412300] global_step=412300, grad_norm=4.591851711273193, loss=0.6721432209014893
I0301 21:13:29.067110 140409124681472 logging_writer.py:48] [412400] global_step=412400, grad_norm=4.745257377624512, loss=0.6135854721069336
I0301 21:14:02.773035 140408319375104 logging_writer.py:48] [412500] global_step=412500, grad_norm=4.344073295593262, loss=0.6123063564300537
I0301 21:14:36.484760 140409124681472 logging_writer.py:48] [412600] global_step=412600, grad_norm=4.187967777252197, loss=0.5959635376930237
I0301 21:15:10.137879 140408319375104 logging_writer.py:48] [412700] global_step=412700, grad_norm=4.229300498962402, loss=0.6160584092140198
I0301 21:15:43.883596 140409124681472 logging_writer.py:48] [412800] global_step=412800, grad_norm=4.598211765289307, loss=0.6471323370933533
I0301 21:16:17.535006 140408319375104 logging_writer.py:48] [412900] global_step=412900, grad_norm=5.110908508300781, loss=0.6350647807121277
I0301 21:16:51.205644 140409124681472 logging_writer.py:48] [413000] global_step=413000, grad_norm=4.305187225341797, loss=0.6141265034675598
I0301 21:17:24.990605 140408319375104 logging_writer.py:48] [413100] global_step=413100, grad_norm=4.948174953460693, loss=0.7093998193740845
I0301 21:17:58.683924 140409124681472 logging_writer.py:48] [413200] global_step=413200, grad_norm=4.900876522064209, loss=0.6982165575027466
I0301 21:18:06.596288 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:18:12.672623 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:18:21.328273 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:18:23.752885 140573303715648 submission_runner.py:411] Time since start: 144227.75s, 	Step: 413225, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.14735817909240723, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0527368783950806, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8175504207611084, 'test/num_examples': 10000, 'score': 139288.81606531143, 'total_duration': 144227.74542737007, 'accumulated_submission_time': 139288.81606531143, 'accumulated_eval_time': 4903.879125356674, 'accumulated_logging_time': 18.72833561897278}
I0301 21:18:23.841234 140407379851008 logging_writer.py:48] [413225] accumulated_eval_time=4903.879125, accumulated_logging_time=18.728336, accumulated_submission_time=139288.816065, global_step=413225, preemption_count=0, score=139288.816065, test/accuracy=0.631900, test/loss=1.817550, test/num_examples=10000, total_duration=144227.745427, train/accuracy=0.960479, train/loss=0.147358, validation/accuracy=0.755820, validation/loss=1.052737, validation/num_examples=50000
I0301 21:18:49.507459 140409837700864 logging_writer.py:48] [413300] global_step=413300, grad_norm=5.0179901123046875, loss=0.6514573097229004
I0301 21:19:23.192977 140407379851008 logging_writer.py:48] [413400] global_step=413400, grad_norm=4.399519443511963, loss=0.6510339379310608
I0301 21:19:56.851356 140409837700864 logging_writer.py:48] [413500] global_step=413500, grad_norm=4.221158504486084, loss=0.6227781772613525
I0301 21:20:30.551527 140407379851008 logging_writer.py:48] [413600] global_step=413600, grad_norm=4.276962757110596, loss=0.5938495993614197
I0301 21:21:04.227972 140409837700864 logging_writer.py:48] [413700] global_step=413700, grad_norm=5.142091751098633, loss=0.5691596865653992
I0301 21:21:37.892878 140407379851008 logging_writer.py:48] [413800] global_step=413800, grad_norm=4.443181991577148, loss=0.600940465927124
I0301 21:22:11.515553 140409837700864 logging_writer.py:48] [413900] global_step=413900, grad_norm=4.401647090911865, loss=0.6599166393280029
I0301 21:22:45.177643 140407379851008 logging_writer.py:48] [414000] global_step=414000, grad_norm=4.514688491821289, loss=0.6067493557929993
I0301 21:23:18.905662 140409837700864 logging_writer.py:48] [414100] global_step=414100, grad_norm=4.546733379364014, loss=0.5958800315856934
I0301 21:23:52.716949 140407379851008 logging_writer.py:48] [414200] global_step=414200, grad_norm=4.4632720947265625, loss=0.6558310985565186
I0301 21:24:26.382074 140409837700864 logging_writer.py:48] [414300] global_step=414300, grad_norm=5.0917158126831055, loss=0.706005334854126
I0301 21:25:00.027251 140407379851008 logging_writer.py:48] [414400] global_step=414400, grad_norm=4.307278156280518, loss=0.6055819988250732
I0301 21:25:33.726169 140409837700864 logging_writer.py:48] [414500] global_step=414500, grad_norm=4.252994537353516, loss=0.655491828918457
I0301 21:26:07.444174 140407379851008 logging_writer.py:48] [414600] global_step=414600, grad_norm=4.942530155181885, loss=0.6258934736251831
I0301 21:26:41.094837 140409837700864 logging_writer.py:48] [414700] global_step=414700, grad_norm=4.800019264221191, loss=0.646976113319397
I0301 21:26:54.052519 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:27:00.203653 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:27:08.878123 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:27:11.197252 140573303715648 submission_runner.py:411] Time since start: 144755.19s, 	Step: 414740, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14647266268730164, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0517487525939941, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8170024156570435, 'test/num_examples': 10000, 'score': 139798.95591545105, 'total_duration': 144755.18977284431, 'accumulated_submission_time': 139798.95591545105, 'accumulated_eval_time': 4921.023787975311, 'accumulated_logging_time': 18.827582120895386}
I0301 21:27:11.296843 140409829308160 logging_writer.py:48] [414740] accumulated_eval_time=4921.023788, accumulated_logging_time=18.827582, accumulated_submission_time=139798.955915, global_step=414740, preemption_count=0, score=139798.955915, test/accuracy=0.631300, test/loss=1.817002, test/num_examples=10000, total_duration=144755.189773, train/accuracy=0.961137, train/loss=0.146473, validation/accuracy=0.755720, validation/loss=1.051749, validation/num_examples=50000
I0301 21:27:31.845889 140409854486272 logging_writer.py:48] [414800] global_step=414800, grad_norm=4.460175514221191, loss=0.5511428713798523
I0301 21:28:05.557917 140409829308160 logging_writer.py:48] [414900] global_step=414900, grad_norm=4.493392467498779, loss=0.6400144100189209
I0301 21:28:39.245442 140409854486272 logging_writer.py:48] [415000] global_step=415000, grad_norm=4.538265228271484, loss=0.6831911206245422
I0301 21:29:12.988605 140409829308160 logging_writer.py:48] [415100] global_step=415100, grad_norm=4.327418327331543, loss=0.6654199957847595
I0301 21:29:46.870650 140409854486272 logging_writer.py:48] [415200] global_step=415200, grad_norm=4.371689319610596, loss=0.6382794976234436
I0301 21:30:20.642341 140409829308160 logging_writer.py:48] [415300] global_step=415300, grad_norm=4.67738151550293, loss=0.5792739391326904
I0301 21:30:54.351519 140409854486272 logging_writer.py:48] [415400] global_step=415400, grad_norm=4.173823833465576, loss=0.5830278396606445
I0301 21:31:28.057366 140409829308160 logging_writer.py:48] [415500] global_step=415500, grad_norm=4.198090553283691, loss=0.610388457775116
I0301 21:32:01.771072 140409854486272 logging_writer.py:48] [415600] global_step=415600, grad_norm=4.34686803817749, loss=0.6406917572021484
I0301 21:32:35.530990 140409829308160 logging_writer.py:48] [415700] global_step=415700, grad_norm=4.445451736450195, loss=0.6841696500778198
I0301 21:33:09.259406 140409854486272 logging_writer.py:48] [415800] global_step=415800, grad_norm=5.273282051086426, loss=0.6277068853378296
I0301 21:33:42.980120 140409829308160 logging_writer.py:48] [415900] global_step=415900, grad_norm=5.098278045654297, loss=0.6984654068946838
I0301 21:34:16.699580 140409854486272 logging_writer.py:48] [416000] global_step=416000, grad_norm=4.419936656951904, loss=0.6633055806159973
I0301 21:34:50.385508 140409829308160 logging_writer.py:48] [416100] global_step=416100, grad_norm=4.489500045776367, loss=0.6805220246315002
I0301 21:35:24.109634 140409854486272 logging_writer.py:48] [416200] global_step=416200, grad_norm=5.225049018859863, loss=0.6176095604896545
I0301 21:35:41.430592 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:35:47.620594 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:35:56.387743 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:35:58.724759 140573303715648 submission_runner.py:411] Time since start: 145282.72s, 	Step: 416253, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14978693425655365, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0523784160614014, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8180079460144043, 'test/num_examples': 10000, 'score': 140309.01599621773, 'total_duration': 145282.7172703743, 'accumulated_submission_time': 140309.01599621773, 'accumulated_eval_time': 4938.317877531052, 'accumulated_logging_time': 18.94173526763916}
I0301 21:35:58.812027 140409837700864 logging_writer.py:48] [416253] accumulated_eval_time=4938.317878, accumulated_logging_time=18.941735, accumulated_submission_time=140309.015996, global_step=416253, preemption_count=0, score=140309.015996, test/accuracy=0.632200, test/loss=1.818008, test/num_examples=10000, total_duration=145282.717270, train/accuracy=0.960001, train/loss=0.149787, validation/accuracy=0.755600, validation/loss=1.052378, validation/num_examples=50000
I0301 21:36:15.022843 140409846093568 logging_writer.py:48] [416300] global_step=416300, grad_norm=4.14827299118042, loss=0.6019845604896545
I0301 21:36:48.727677 140409837700864 logging_writer.py:48] [416400] global_step=416400, grad_norm=4.521859645843506, loss=0.576200544834137
I0301 21:37:22.369268 140409846093568 logging_writer.py:48] [416500] global_step=416500, grad_norm=4.277298927307129, loss=0.5498851537704468
I0301 21:37:56.040916 140409837700864 logging_writer.py:48] [416600] global_step=416600, grad_norm=5.210309028625488, loss=0.6276869773864746
I0301 21:38:29.710366 140409846093568 logging_writer.py:48] [416700] global_step=416700, grad_norm=4.5373077392578125, loss=0.6064324378967285
I0301 21:39:03.438971 140409837700864 logging_writer.py:48] [416800] global_step=416800, grad_norm=4.396894931793213, loss=0.6602095365524292
I0301 21:39:37.148937 140409846093568 logging_writer.py:48] [416900] global_step=416900, grad_norm=4.370792388916016, loss=0.6692715883255005
I0301 21:40:10.849799 140409837700864 logging_writer.py:48] [417000] global_step=417000, grad_norm=4.706757545471191, loss=0.6508722305297852
I0301 21:40:44.513063 140409846093568 logging_writer.py:48] [417100] global_step=417100, grad_norm=4.0527167320251465, loss=0.6263302564620972
I0301 21:41:18.164388 140409837700864 logging_writer.py:48] [417200] global_step=417200, grad_norm=4.788268089294434, loss=0.6717039942741394
I0301 21:41:51.872524 140409846093568 logging_writer.py:48] [417300] global_step=417300, grad_norm=4.666472434997559, loss=0.6396927833557129
I0301 21:42:25.720419 140409837700864 logging_writer.py:48] [417400] global_step=417400, grad_norm=4.404595851898193, loss=0.5945961475372314
I0301 21:42:59.420088 140409846093568 logging_writer.py:48] [417500] global_step=417500, grad_norm=4.628051280975342, loss=0.6625126600265503
I0301 21:43:33.078146 140409837700864 logging_writer.py:48] [417600] global_step=417600, grad_norm=4.448788166046143, loss=0.6181776523590088
I0301 21:44:06.753408 140409846093568 logging_writer.py:48] [417700] global_step=417700, grad_norm=5.177745342254639, loss=0.7070772647857666
I0301 21:44:28.834169 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:44:34.919430 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:44:43.404544 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:44:45.716362 140573303715648 submission_runner.py:411] Time since start: 145809.71s, 	Step: 417767, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14640426635742188, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0517691373825073, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8175561428070068, 'test/num_examples': 10000, 'score': 140818.96789312363, 'total_duration': 145809.70890045166, 'accumulated_submission_time': 140818.96789312363, 'accumulated_eval_time': 4955.2000188827515, 'accumulated_logging_time': 19.039517164230347}
I0301 21:44:45.804327 140409124681472 logging_writer.py:48] [417767] accumulated_eval_time=4955.200019, accumulated_logging_time=19.039517, accumulated_submission_time=140818.967893, global_step=417767, preemption_count=0, score=140818.967893, test/accuracy=0.631300, test/loss=1.817556, test/num_examples=10000, total_duration=145809.708900, train/accuracy=0.961515, train/loss=0.146404, validation/accuracy=0.755600, validation/loss=1.051769, validation/num_examples=50000
I0301 21:44:57.271682 140409829308160 logging_writer.py:48] [417800] global_step=417800, grad_norm=4.3742899894714355, loss=0.61178058385849
I0301 21:45:30.983241 140409124681472 logging_writer.py:48] [417900] global_step=417900, grad_norm=4.676992893218994, loss=0.6558674573898315
I0301 21:46:04.718063 140409829308160 logging_writer.py:48] [418000] global_step=418000, grad_norm=4.509297847747803, loss=0.6507877111434937
I0301 21:46:38.436087 140409124681472 logging_writer.py:48] [418100] global_step=418100, grad_norm=4.347350120544434, loss=0.6555858254432678
I0301 21:47:12.163103 140409829308160 logging_writer.py:48] [418200] global_step=418200, grad_norm=4.344484806060791, loss=0.6102892756462097
I0301 21:47:45.884512 140409124681472 logging_writer.py:48] [418300] global_step=418300, grad_norm=4.564212799072266, loss=0.6368718147277832
I0301 21:48:19.769636 140409829308160 logging_writer.py:48] [418400] global_step=418400, grad_norm=4.705393314361572, loss=0.649425745010376
I0301 21:48:53.459236 140409124681472 logging_writer.py:48] [418500] global_step=418500, grad_norm=4.398339748382568, loss=0.6819182634353638
I0301 21:49:27.217473 140409829308160 logging_writer.py:48] [418600] global_step=418600, grad_norm=4.377403736114502, loss=0.5610858201980591
I0301 21:50:00.884804 140409124681472 logging_writer.py:48] [418700] global_step=418700, grad_norm=4.4935479164123535, loss=0.5987280607223511
I0301 21:50:34.619013 140409829308160 logging_writer.py:48] [418800] global_step=418800, grad_norm=4.70632791519165, loss=0.681350588798523
I0301 21:51:08.293246 140409124681472 logging_writer.py:48] [418900] global_step=418900, grad_norm=4.376149654388428, loss=0.6521990895271301
I0301 21:51:41.988935 140409829308160 logging_writer.py:48] [419000] global_step=419000, grad_norm=4.645007133483887, loss=0.6347838044166565
I0301 21:52:15.692915 140409124681472 logging_writer.py:48] [419100] global_step=419100, grad_norm=4.308516025543213, loss=0.5818837881088257
I0301 21:52:49.346063 140409829308160 logging_writer.py:48] [419200] global_step=419200, grad_norm=4.724115371704102, loss=0.6667047142982483
I0301 21:53:15.793908 140573303715648 spec.py:321] Evaluating on the training split.
I0301 21:53:21.962909 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 21:53:30.569449 140573303715648 spec.py:349] Evaluating on the test split.
I0301 21:53:32.890618 140573303715648 submission_runner.py:411] Time since start: 146336.88s, 	Step: 419280, 	{'train/accuracy': 0.9621133208274841, 'train/loss': 0.14442582428455353, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0518677234649658, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.815688967704773, 'test/num_examples': 10000, 'score': 141328.88626480103, 'total_duration': 146336.88316321373, 'accumulated_submission_time': 141328.88626480103, 'accumulated_eval_time': 4972.296702861786, 'accumulated_logging_time': 19.138569831848145}
I0301 21:53:32.980515 140407371458304 logging_writer.py:48] [419280] accumulated_eval_time=4972.296703, accumulated_logging_time=19.138570, accumulated_submission_time=141328.886265, global_step=419280, preemption_count=0, score=141328.886265, test/accuracy=0.631300, test/loss=1.815689, test/num_examples=10000, total_duration=146336.883163, train/accuracy=0.962113, train/loss=0.144426, validation/accuracy=0.755480, validation/loss=1.051868, validation/num_examples=50000
I0301 21:53:40.056109 140407379851008 logging_writer.py:48] [419300] global_step=419300, grad_norm=4.170888423919678, loss=0.5754916667938232
I0301 21:54:13.803827 140407371458304 logging_writer.py:48] [419400] global_step=419400, grad_norm=4.6849541664123535, loss=0.6593751907348633
I0301 21:54:47.505139 140407379851008 logging_writer.py:48] [419500] global_step=419500, grad_norm=4.570733070373535, loss=0.609032928943634
I0301 21:55:21.157723 140407371458304 logging_writer.py:48] [419600] global_step=419600, grad_norm=4.380672454833984, loss=0.631138026714325
I0301 21:55:54.911125 140407379851008 logging_writer.py:48] [419700] global_step=419700, grad_norm=4.346863746643066, loss=0.6070889830589294
I0301 21:56:28.573996 140407371458304 logging_writer.py:48] [419800] global_step=419800, grad_norm=4.305147171020508, loss=0.6047842502593994
I0301 21:57:02.211073 140407379851008 logging_writer.py:48] [419900] global_step=419900, grad_norm=4.292126178741455, loss=0.6367278695106506
I0301 21:57:35.862287 140407371458304 logging_writer.py:48] [420000] global_step=420000, grad_norm=4.17554235458374, loss=0.594749391078949
I0301 21:58:09.632090 140407379851008 logging_writer.py:48] [420100] global_step=420100, grad_norm=4.37890100479126, loss=0.5911199450492859
I0301 21:58:43.289821 140407371458304 logging_writer.py:48] [420200] global_step=420200, grad_norm=4.225561141967773, loss=0.6294398903846741
I0301 21:59:16.973070 140407379851008 logging_writer.py:48] [420300] global_step=420300, grad_norm=3.98219633102417, loss=0.578410267829895
I0301 21:59:50.672250 140407371458304 logging_writer.py:48] [420400] global_step=420400, grad_norm=4.432325839996338, loss=0.6104125380516052
I0301 22:00:24.434851 140407379851008 logging_writer.py:48] [420500] global_step=420500, grad_norm=4.1435418128967285, loss=0.5947800278663635
I0301 22:00:58.136711 140407371458304 logging_writer.py:48] [420600] global_step=420600, grad_norm=4.535682678222656, loss=0.7021163702011108
I0301 22:01:31.813879 140407379851008 logging_writer.py:48] [420700] global_step=420700, grad_norm=4.874960422515869, loss=0.694144606590271
I0301 22:02:02.974515 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:02:08.993435 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:02:17.540202 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:02:19.872898 140573303715648 submission_runner.py:411] Time since start: 146863.87s, 	Step: 420794, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.146013543009758, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0519076585769653, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8182135820388794, 'test/num_examples': 10000, 'score': 141838.8102095127, 'total_duration': 146863.86543941498, 'accumulated_submission_time': 141838.8102095127, 'accumulated_eval_time': 4989.195043087006, 'accumulated_logging_time': 19.238277673721313}
I0301 22:02:19.965491 140409837700864 logging_writer.py:48] [420794] accumulated_eval_time=4989.195043, accumulated_logging_time=19.238278, accumulated_submission_time=141838.810210, global_step=420794, preemption_count=0, score=141838.810210, test/accuracy=0.631800, test/loss=1.818214, test/num_examples=10000, total_duration=146863.865439, train/accuracy=0.960240, train/loss=0.146014, validation/accuracy=0.756000, validation/loss=1.051908, validation/num_examples=50000
I0301 22:02:22.353067 140409854486272 logging_writer.py:48] [420800] global_step=420800, grad_norm=4.180798530578613, loss=0.5375634431838989
I0301 22:02:56.010275 140409837700864 logging_writer.py:48] [420900] global_step=420900, grad_norm=4.527604579925537, loss=0.6599834561347961
I0301 22:03:29.720250 140409854486272 logging_writer.py:48] [421000] global_step=421000, grad_norm=4.1690826416015625, loss=0.6708770394325256
I0301 22:04:03.405580 140409837700864 logging_writer.py:48] [421100] global_step=421100, grad_norm=4.576547622680664, loss=0.6139509677886963
I0301 22:04:37.140587 140409854486272 logging_writer.py:48] [421200] global_step=421200, grad_norm=4.840919494628906, loss=0.6793507933616638
I0301 22:05:10.832894 140409837700864 logging_writer.py:48] [421300] global_step=421300, grad_norm=4.6556477546691895, loss=0.6520910263061523
I0301 22:05:44.560322 140409854486272 logging_writer.py:48] [421400] global_step=421400, grad_norm=4.303548336029053, loss=0.623569130897522
I0301 22:06:18.277981 140409837700864 logging_writer.py:48] [421500] global_step=421500, grad_norm=4.125626087188721, loss=0.5689022541046143
I0301 22:06:52.048141 140409854486272 logging_writer.py:48] [421600] global_step=421600, grad_norm=4.771101474761963, loss=0.6284040212631226
I0301 22:07:25.731360 140409837700864 logging_writer.py:48] [421700] global_step=421700, grad_norm=4.141147613525391, loss=0.6051687002182007
I0301 22:07:59.491756 140409854486272 logging_writer.py:48] [421800] global_step=421800, grad_norm=4.56185245513916, loss=0.5954554677009583
I0301 22:08:33.150454 140409837700864 logging_writer.py:48] [421900] global_step=421900, grad_norm=5.0420660972595215, loss=0.6960572004318237
I0301 22:09:06.902395 140409854486272 logging_writer.py:48] [422000] global_step=422000, grad_norm=4.930181503295898, loss=0.7029787302017212
I0301 22:09:40.599673 140409837700864 logging_writer.py:48] [422100] global_step=422100, grad_norm=4.6739301681518555, loss=0.6062256097793579
I0301 22:10:14.326343 140409854486272 logging_writer.py:48] [422200] global_step=422200, grad_norm=4.59869384765625, loss=0.5604689717292786
I0301 22:10:48.014854 140409837700864 logging_writer.py:48] [422300] global_step=422300, grad_norm=4.453845500946045, loss=0.650670051574707
I0301 22:10:50.183303 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:10:56.330063 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:11:05.004865 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:11:07.315274 140573303715648 submission_runner.py:411] Time since start: 147391.31s, 	Step: 422308, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.1461603045463562, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0520353317260742, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8166799545288086, 'test/num_examples': 10000, 'score': 142348.95408391953, 'total_duration': 147391.3078136444, 'accumulated_submission_time': 142348.95408391953, 'accumulated_eval_time': 5006.32697892189, 'accumulated_logging_time': 19.343008279800415}
I0301 22:11:07.401022 140409829308160 logging_writer.py:48] [422308] accumulated_eval_time=5006.326979, accumulated_logging_time=19.343008, accumulated_submission_time=142348.954084, global_step=422308, preemption_count=0, score=142348.954084, test/accuracy=0.631400, test/loss=1.816680, test/num_examples=10000, total_duration=147391.307814, train/accuracy=0.961017, train/loss=0.146160, validation/accuracy=0.755740, validation/loss=1.052035, validation/num_examples=50000
I0301 22:11:38.716179 140409846093568 logging_writer.py:48] [422400] global_step=422400, grad_norm=5.082103252410889, loss=0.684623122215271
I0301 22:12:12.358158 140409829308160 logging_writer.py:48] [422500] global_step=422500, grad_norm=4.8463640213012695, loss=0.6774606108665466
I0301 22:12:46.164820 140409846093568 logging_writer.py:48] [422600] global_step=422600, grad_norm=4.259693145751953, loss=0.603602409362793
I0301 22:13:19.877625 140409829308160 logging_writer.py:48] [422700] global_step=422700, grad_norm=4.671298980712891, loss=0.6548060774803162
I0301 22:13:53.575139 140409846093568 logging_writer.py:48] [422800] global_step=422800, grad_norm=4.248082637786865, loss=0.6021692752838135
I0301 22:14:27.266653 140409829308160 logging_writer.py:48] [422900] global_step=422900, grad_norm=4.602573394775391, loss=0.5589109063148499
I0301 22:15:00.971065 140409846093568 logging_writer.py:48] [423000] global_step=423000, grad_norm=4.661126136779785, loss=0.6490722298622131
I0301 22:15:34.682586 140409829308160 logging_writer.py:48] [423100] global_step=423100, grad_norm=4.426711082458496, loss=0.6658943891525269
I0301 22:16:08.363064 140409846093568 logging_writer.py:48] [423200] global_step=423200, grad_norm=4.431097030639648, loss=0.6443629264831543
I0301 22:16:42.001003 140409829308160 logging_writer.py:48] [423300] global_step=423300, grad_norm=5.000253677368164, loss=0.6315189599990845
I0301 22:17:15.659923 140409846093568 logging_writer.py:48] [423400] global_step=423400, grad_norm=5.0788493156433105, loss=0.6663622856140137
I0301 22:17:49.374165 140409829308160 logging_writer.py:48] [423500] global_step=423500, grad_norm=4.443708896636963, loss=0.5936846733093262
I0301 22:18:23.089375 140409846093568 logging_writer.py:48] [423600] global_step=423600, grad_norm=4.253347873687744, loss=0.5546759963035583
I0301 22:18:56.967128 140409829308160 logging_writer.py:48] [423700] global_step=423700, grad_norm=4.38966703414917, loss=0.6091381311416626
I0301 22:19:30.686649 140409846093568 logging_writer.py:48] [423800] global_step=423800, grad_norm=4.421277046203613, loss=0.6475811004638672
I0301 22:19:37.564090 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:19:44.332384 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:19:53.024438 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:19:55.350226 140573303715648 submission_runner.py:411] Time since start: 147919.34s, 	Step: 423822, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14758779108524323, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0516517162322998, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8173928260803223, 'test/num_examples': 10000, 'score': 142859.0488152504, 'total_duration': 147919.34276366234, 'accumulated_submission_time': 142859.0488152504, 'accumulated_eval_time': 5024.113070011139, 'accumulated_logging_time': 19.438355922698975}
I0301 22:19:55.438781 140408319375104 logging_writer.py:48] [423822] accumulated_eval_time=5024.113070, accumulated_logging_time=19.438356, accumulated_submission_time=142859.048815, global_step=423822, preemption_count=0, score=142859.048815, test/accuracy=0.632300, test/loss=1.817393, test/num_examples=10000, total_duration=147919.342764, train/accuracy=0.960320, train/loss=0.147588, validation/accuracy=0.755560, validation/loss=1.051652, validation/num_examples=50000
I0301 22:20:22.122776 140409124681472 logging_writer.py:48] [423900] global_step=423900, grad_norm=4.955465316772461, loss=0.7181810736656189
I0301 22:20:55.850492 140408319375104 logging_writer.py:48] [424000] global_step=424000, grad_norm=4.796105861663818, loss=0.6091486811637878
I0301 22:21:29.604791 140409124681472 logging_writer.py:48] [424100] global_step=424100, grad_norm=4.027078628540039, loss=0.5951010584831238
I0301 22:22:03.288907 140408319375104 logging_writer.py:48] [424200] global_step=424200, grad_norm=4.032899856567383, loss=0.5591853857040405
I0301 22:22:37.013323 140409124681472 logging_writer.py:48] [424300] global_step=424300, grad_norm=4.694172382354736, loss=0.6254920959472656
I0301 22:23:10.714117 140408319375104 logging_writer.py:48] [424400] global_step=424400, grad_norm=4.118358612060547, loss=0.587367832660675
I0301 22:23:44.473144 140409124681472 logging_writer.py:48] [424500] global_step=424500, grad_norm=4.379152297973633, loss=0.597224235534668
I0301 22:24:18.133456 140408319375104 logging_writer.py:48] [424600] global_step=424600, grad_norm=4.175266742706299, loss=0.6317949891090393
I0301 22:24:51.868061 140409124681472 logging_writer.py:48] [424700] global_step=424700, grad_norm=4.992808818817139, loss=0.6409841775894165
I0301 22:25:25.764914 140408319375104 logging_writer.py:48] [424800] global_step=424800, grad_norm=5.02509880065918, loss=0.6367826461791992
I0301 22:25:59.447314 140409124681472 logging_writer.py:48] [424900] global_step=424900, grad_norm=4.718719959259033, loss=0.6423215866088867
I0301 22:26:33.116295 140408319375104 logging_writer.py:48] [425000] global_step=425000, grad_norm=4.446101188659668, loss=0.6203708648681641
I0301 22:27:06.866607 140409124681472 logging_writer.py:48] [425100] global_step=425100, grad_norm=4.139169692993164, loss=0.5484516620635986
I0301 22:27:40.592677 140408319375104 logging_writer.py:48] [425200] global_step=425200, grad_norm=4.4327006340026855, loss=0.6354727149009705
I0301 22:28:14.332445 140409124681472 logging_writer.py:48] [425300] global_step=425300, grad_norm=4.8225932121276855, loss=0.5887523889541626
I0301 22:28:25.595139 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:28:31.726854 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:28:40.366034 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:28:42.687757 140573303715648 submission_runner.py:411] Time since start: 148446.68s, 	Step: 425335, 	{'train/accuracy': 0.9591637253761292, 'train/loss': 0.150650754570961, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0516971349716187, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8176788091659546, 'test/num_examples': 10000, 'score': 143369.13305354118, 'total_duration': 148446.680259943, 'accumulated_submission_time': 143369.13305354118, 'accumulated_eval_time': 5041.205612659454, 'accumulated_logging_time': 19.53954839706421}
I0301 22:28:42.777226 140407379851008 logging_writer.py:48] [425335] accumulated_eval_time=5041.205613, accumulated_logging_time=19.539548, accumulated_submission_time=143369.133054, global_step=425335, preemption_count=0, score=143369.133054, test/accuracy=0.631100, test/loss=1.817679, test/num_examples=10000, total_duration=148446.680260, train/accuracy=0.959164, train/loss=0.150651, validation/accuracy=0.755900, validation/loss=1.051697, validation/num_examples=50000
I0301 22:29:05.045872 140409837700864 logging_writer.py:48] [425400] global_step=425400, grad_norm=4.529933452606201, loss=0.6395636796951294
I0301 22:29:38.785867 140407379851008 logging_writer.py:48] [425500] global_step=425500, grad_norm=4.638629913330078, loss=0.6191766262054443
I0301 22:30:12.499862 140409837700864 logging_writer.py:48] [425600] global_step=425600, grad_norm=5.049384117126465, loss=0.6408856511116028
I0301 22:30:46.250018 140407379851008 logging_writer.py:48] [425700] global_step=425700, grad_norm=3.997077226638794, loss=0.5269243717193604
I0301 22:31:20.066475 140409837700864 logging_writer.py:48] [425800] global_step=425800, grad_norm=4.378284454345703, loss=0.5303297638893127
I0301 22:31:53.819717 140407379851008 logging_writer.py:48] [425900] global_step=425900, grad_norm=4.737290382385254, loss=0.5670796036720276
I0301 22:32:27.541430 140409837700864 logging_writer.py:48] [426000] global_step=426000, grad_norm=4.282232761383057, loss=0.6648544073104858
I0301 22:33:01.257833 140407379851008 logging_writer.py:48] [426100] global_step=426100, grad_norm=4.303397178649902, loss=0.6137451529502869
I0301 22:33:34.985628 140409837700864 logging_writer.py:48] [426200] global_step=426200, grad_norm=4.329371929168701, loss=0.6159725189208984
I0301 22:34:08.732633 140407379851008 logging_writer.py:48] [426300] global_step=426300, grad_norm=4.444267749786377, loss=0.6337000131607056
I0301 22:34:42.406292 140409837700864 logging_writer.py:48] [426400] global_step=426400, grad_norm=4.421771049499512, loss=0.5859522223472595
I0301 22:35:16.145283 140407379851008 logging_writer.py:48] [426500] global_step=426500, grad_norm=5.159841060638428, loss=0.6849222779273987
I0301 22:35:49.840105 140409837700864 logging_writer.py:48] [426600] global_step=426600, grad_norm=4.675652027130127, loss=0.6178595423698425
I0301 22:36:23.580478 140407379851008 logging_writer.py:48] [426700] global_step=426700, grad_norm=4.6792497634887695, loss=0.674777626991272
I0301 22:36:57.285765 140409837700864 logging_writer.py:48] [426800] global_step=426800, grad_norm=4.532431602478027, loss=0.6211824417114258
I0301 22:37:13.014616 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:37:19.078794 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:37:27.601151 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:37:29.941000 140573303715648 submission_runner.py:411] Time since start: 148973.93s, 	Step: 426848, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.14581602811813354, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0528355836868286, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8194468021392822, 'test/num_examples': 10000, 'score': 143879.30010700226, 'total_duration': 148973.93353700638, 'accumulated_submission_time': 143879.30010700226, 'accumulated_eval_time': 5058.131942987442, 'accumulated_logging_time': 19.6390221118927}
I0301 22:37:30.035504 140407371458304 logging_writer.py:48] [426848] accumulated_eval_time=5058.131943, accumulated_logging_time=19.639022, accumulated_submission_time=143879.300107, global_step=426848, preemption_count=0, score=143879.300107, test/accuracy=0.631300, test/loss=1.819447, test/num_examples=10000, total_duration=148973.933537, train/accuracy=0.960898, train/loss=0.145816, validation/accuracy=0.755980, validation/loss=1.052836, validation/num_examples=50000
I0301 22:37:47.885722 140407379851008 logging_writer.py:48] [426900] global_step=426900, grad_norm=4.105890274047852, loss=0.6529900431632996
I0301 22:38:21.636942 140407371458304 logging_writer.py:48] [427000] global_step=427000, grad_norm=4.818317413330078, loss=0.6172088384628296
I0301 22:38:55.310580 140407379851008 logging_writer.py:48] [427100] global_step=427100, grad_norm=4.663815975189209, loss=0.653077244758606
I0301 22:39:29.081494 140407371458304 logging_writer.py:48] [427200] global_step=427200, grad_norm=4.147375106811523, loss=0.6152493357658386
I0301 22:40:02.779166 140407379851008 logging_writer.py:48] [427300] global_step=427300, grad_norm=4.409419536590576, loss=0.6005219221115112
I0301 22:40:36.524784 140407371458304 logging_writer.py:48] [427400] global_step=427400, grad_norm=4.554811000823975, loss=0.6488155722618103
I0301 22:41:10.220900 140407379851008 logging_writer.py:48] [427500] global_step=427500, grad_norm=4.459773540496826, loss=0.5739128589630127
I0301 22:41:43.952081 140407371458304 logging_writer.py:48] [427600] global_step=427600, grad_norm=4.3543596267700195, loss=0.5697243809700012
I0301 22:42:17.646407 140407379851008 logging_writer.py:48] [427700] global_step=427700, grad_norm=4.291163444519043, loss=0.6087589859962463
I0301 22:42:51.384540 140407371458304 logging_writer.py:48] [427800] global_step=427800, grad_norm=4.845448017120361, loss=0.6159359216690063
I0301 22:43:25.134313 140407379851008 logging_writer.py:48] [427900] global_step=427900, grad_norm=4.024133205413818, loss=0.5739781260490417
I0301 22:43:58.828125 140407371458304 logging_writer.py:48] [428000] global_step=428000, grad_norm=4.497734069824219, loss=0.6809458136558533
I0301 22:44:32.555759 140407379851008 logging_writer.py:48] [428100] global_step=428100, grad_norm=4.875921249389648, loss=0.6945028305053711
I0301 22:45:06.566947 140407371458304 logging_writer.py:48] [428200] global_step=428200, grad_norm=4.614322185516357, loss=0.657619833946228
I0301 22:45:40.240643 140407379851008 logging_writer.py:48] [428300] global_step=428300, grad_norm=4.485960960388184, loss=0.6576877236366272
I0301 22:45:59.955065 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:46:06.145360 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:46:14.803441 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:46:17.122255 140573303715648 submission_runner.py:411] Time since start: 149501.11s, 	Step: 428360, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.146694615483284, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.051979660987854, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8167650699615479, 'test/num_examples': 10000, 'score': 144389.14986610413, 'total_duration': 149501.11479592323, 'accumulated_submission_time': 144389.14986610413, 'accumulated_eval_time': 5075.299085140228, 'accumulated_logging_time': 19.744468927383423}
I0301 22:46:17.209710 140407379851008 logging_writer.py:48] [428360] accumulated_eval_time=5075.299085, accumulated_logging_time=19.744469, accumulated_submission_time=144389.149866, global_step=428360, preemption_count=0, score=144389.149866, test/accuracy=0.630700, test/loss=1.816765, test/num_examples=10000, total_duration=149501.114796, train/accuracy=0.961177, train/loss=0.146695, validation/accuracy=0.755720, validation/loss=1.051980, validation/num_examples=50000
I0301 22:46:31.018312 140409837700864 logging_writer.py:48] [428400] global_step=428400, grad_norm=4.482991695404053, loss=0.6426870822906494
I0301 22:47:04.705682 140407379851008 logging_writer.py:48] [428500] global_step=428500, grad_norm=4.166969299316406, loss=0.630910336971283
I0301 22:47:38.382511 140409837700864 logging_writer.py:48] [428600] global_step=428600, grad_norm=4.419736385345459, loss=0.626713216304779
I0301 22:48:12.112793 140407379851008 logging_writer.py:48] [428700] global_step=428700, grad_norm=4.233779430389404, loss=0.5573080778121948
I0301 22:48:45.853629 140409837700864 logging_writer.py:48] [428800] global_step=428800, grad_norm=4.79242467880249, loss=0.7054111957550049
I0301 22:49:19.571289 140407379851008 logging_writer.py:48] [428900] global_step=428900, grad_norm=4.862642288208008, loss=0.6199050545692444
I0301 22:49:53.332912 140409837700864 logging_writer.py:48] [429000] global_step=429000, grad_norm=4.525084495544434, loss=0.5906702876091003
I0301 22:50:27.047500 140407379851008 logging_writer.py:48] [429100] global_step=429100, grad_norm=4.820676803588867, loss=0.6326847076416016
I0301 22:51:00.727610 140409837700864 logging_writer.py:48] [429200] global_step=429200, grad_norm=3.920905351638794, loss=0.5606294870376587
I0301 22:51:34.444301 140407379851008 logging_writer.py:48] [429300] global_step=429300, grad_norm=4.478365898132324, loss=0.6547673344612122
I0301 22:52:08.176865 140409837700864 logging_writer.py:48] [429400] global_step=429400, grad_norm=4.362778663635254, loss=0.6490770578384399
I0301 22:52:41.858606 140407379851008 logging_writer.py:48] [429500] global_step=429500, grad_norm=4.643539905548096, loss=0.6166954636573792
I0301 22:53:15.566285 140409837700864 logging_writer.py:48] [429600] global_step=429600, grad_norm=4.554100513458252, loss=0.6170457601547241
I0301 22:53:49.257070 140407379851008 logging_writer.py:48] [429700] global_step=429700, grad_norm=4.062283515930176, loss=0.5578200817108154
I0301 22:54:22.941549 140409837700864 logging_writer.py:48] [429800] global_step=429800, grad_norm=4.874934196472168, loss=0.6711100339889526
I0301 22:54:47.358983 140573303715648 spec.py:321] Evaluating on the training split.
I0301 22:54:53.624145 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 22:55:02.177261 140573303715648 spec.py:349] Evaluating on the test split.
I0301 22:55:04.493361 140573303715648 submission_runner.py:411] Time since start: 150028.49s, 	Step: 429874, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14246562123298645, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.051815152168274, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8176268339157104, 'test/num_examples': 10000, 'score': 144899.22899580002, 'total_duration': 150028.48589587212, 'accumulated_submission_time': 144899.22899580002, 'accumulated_eval_time': 5092.433423757553, 'accumulated_logging_time': 19.842193365097046}
I0301 22:55:04.583900 140409124681472 logging_writer.py:48] [429874] accumulated_eval_time=5092.433424, accumulated_logging_time=19.842193, accumulated_submission_time=144899.228996, global_step=429874, preemption_count=0, score=144899.228996, test/accuracy=0.630900, test/loss=1.817627, test/num_examples=10000, total_duration=150028.485896, train/accuracy=0.961535, train/loss=0.142466, validation/accuracy=0.755800, validation/loss=1.051815, validation/num_examples=50000
I0301 22:55:13.681239 140409829308160 logging_writer.py:48] [429900] global_step=429900, grad_norm=4.584015846252441, loss=0.6326830387115479
I0301 22:55:47.499926 140409124681472 logging_writer.py:48] [430000] global_step=430000, grad_norm=4.715420246124268, loss=0.6477943658828735
I0301 22:56:21.250973 140409829308160 logging_writer.py:48] [430100] global_step=430100, grad_norm=4.596051216125488, loss=0.6363470554351807
I0301 22:56:54.924947 140409124681472 logging_writer.py:48] [430200] global_step=430200, grad_norm=4.450381278991699, loss=0.6469841599464417
I0301 22:57:28.680916 140409829308160 logging_writer.py:48] [430300] global_step=430300, grad_norm=4.948707103729248, loss=0.682468831539154
I0301 22:58:02.405504 140409124681472 logging_writer.py:48] [430400] global_step=430400, grad_norm=4.567476749420166, loss=0.6829902529716492
I0301 22:58:36.136659 140409829308160 logging_writer.py:48] [430500] global_step=430500, grad_norm=4.071753025054932, loss=0.6044113636016846
I0301 22:59:09.803712 140409124681472 logging_writer.py:48] [430600] global_step=430600, grad_norm=4.719698905944824, loss=0.5840988755226135
I0301 22:59:43.575643 140409829308160 logging_writer.py:48] [430700] global_step=430700, grad_norm=4.451362609863281, loss=0.6773054599761963
I0301 23:00:17.260869 140409124681472 logging_writer.py:48] [430800] global_step=430800, grad_norm=4.202773571014404, loss=0.5577418208122253
I0301 23:00:51.029284 140409829308160 logging_writer.py:48] [430900] global_step=430900, grad_norm=4.4745941162109375, loss=0.6746910214424133
I0301 23:01:24.727256 140409124681472 logging_writer.py:48] [431000] global_step=431000, grad_norm=4.4904680252075195, loss=0.5761507153511047
I0301 23:01:58.514294 140409829308160 logging_writer.py:48] [431100] global_step=431100, grad_norm=4.361076831817627, loss=0.6577381491661072
I0301 23:02:32.241898 140409124681472 logging_writer.py:48] [431200] global_step=431200, grad_norm=4.741372108459473, loss=0.5837367177009583
I0301 23:03:05.905030 140409829308160 logging_writer.py:48] [431300] global_step=431300, grad_norm=4.914823055267334, loss=0.6682081818580627
I0301 23:03:34.655603 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:03:40.752991 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:03:49.378407 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:03:51.702679 140573303715648 submission_runner.py:411] Time since start: 150555.70s, 	Step: 431387, 	{'train/accuracy': 0.9611567258834839, 'train/loss': 0.14664770662784576, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 1.0512664318084717, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.817805528640747, 'test/num_examples': 10000, 'score': 145409.23196220398, 'total_duration': 150555.69521546364, 'accumulated_submission_time': 145409.23196220398, 'accumulated_eval_time': 5109.480449914932, 'accumulated_logging_time': 19.94320559501648}
I0301 23:03:51.792619 140409837700864 logging_writer.py:48] [431387] accumulated_eval_time=5109.480450, accumulated_logging_time=19.943206, accumulated_submission_time=145409.231962, global_step=431387, preemption_count=0, score=145409.231962, test/accuracy=0.631900, test/loss=1.817806, test/num_examples=10000, total_duration=150555.695215, train/accuracy=0.961157, train/loss=0.146648, validation/accuracy=0.756280, validation/loss=1.051266, validation/num_examples=50000
I0301 23:03:56.519346 140409846093568 logging_writer.py:48] [431400] global_step=431400, grad_norm=4.449596405029297, loss=0.612817108631134
I0301 23:04:30.230364 140409837700864 logging_writer.py:48] [431500] global_step=431500, grad_norm=4.456758975982666, loss=0.5812264084815979
I0301 23:05:03.892045 140409846093568 logging_writer.py:48] [431600] global_step=431600, grad_norm=4.964184761047363, loss=0.641359269618988
I0301 23:05:37.550130 140409837700864 logging_writer.py:48] [431700] global_step=431700, grad_norm=4.7344183921813965, loss=0.6030691266059875
I0301 23:06:11.288120 140409846093568 logging_writer.py:48] [431800] global_step=431800, grad_norm=4.116085052490234, loss=0.5796720385551453
I0301 23:06:44.988506 140409837700864 logging_writer.py:48] [431900] global_step=431900, grad_norm=4.200058460235596, loss=0.6453730463981628
I0301 23:07:18.731995 140409846093568 logging_writer.py:48] [432000] global_step=432000, grad_norm=4.399829864501953, loss=0.6192631125450134
I0301 23:07:52.546045 140409837700864 logging_writer.py:48] [432100] global_step=432100, grad_norm=4.606457710266113, loss=0.6271840333938599
I0301 23:08:26.211749 140409846093568 logging_writer.py:48] [432200] global_step=432200, grad_norm=4.154611587524414, loss=0.625730037689209
I0301 23:08:59.876219 140409837700864 logging_writer.py:48] [432300] global_step=432300, grad_norm=4.286580562591553, loss=0.578815221786499
I0301 23:09:33.578039 140409846093568 logging_writer.py:48] [432400] global_step=432400, grad_norm=4.840374946594238, loss=0.6220659017562866
I0301 23:10:07.327697 140409837700864 logging_writer.py:48] [432500] global_step=432500, grad_norm=4.378469467163086, loss=0.6686172485351562
I0301 23:10:41.061076 140409846093568 logging_writer.py:48] [432600] global_step=432600, grad_norm=4.322325229644775, loss=0.5845823287963867
I0301 23:11:14.765809 140409837700864 logging_writer.py:48] [432700] global_step=432700, grad_norm=4.278146266937256, loss=0.5581898093223572
I0301 23:11:48.485216 140409846093568 logging_writer.py:48] [432800] global_step=432800, grad_norm=4.712483882904053, loss=0.6945602893829346
I0301 23:12:22.216073 140409837700864 logging_writer.py:48] [432900] global_step=432900, grad_norm=4.526337623596191, loss=0.6355836391448975
I0301 23:12:22.222872 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:12:28.462985 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:12:36.989764 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:12:39.313046 140573303715648 submission_runner.py:411] Time since start: 151083.31s, 	Step: 432901, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.1482953429222107, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0520941019058228, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.81902277469635, 'test/num_examples': 10000, 'score': 145919.59135246277, 'total_duration': 151083.30557370186, 'accumulated_submission_time': 145919.59135246277, 'accumulated_eval_time': 5126.57053732872, 'accumulated_logging_time': 20.042986392974854}
I0301 23:12:39.397847 140409854486272 logging_writer.py:48] [432901] accumulated_eval_time=5126.570537, accumulated_logging_time=20.042986, accumulated_submission_time=145919.591352, global_step=432901, preemption_count=0, score=145919.591352, test/accuracy=0.631900, test/loss=1.819023, test/num_examples=10000, total_duration=151083.305574, train/accuracy=0.959861, train/loss=0.148295, validation/accuracy=0.756200, validation/loss=1.052094, validation/num_examples=50000
I0301 23:13:13.033065 140409862878976 logging_writer.py:48] [433000] global_step=433000, grad_norm=4.163344860076904, loss=0.5725133419036865
I0301 23:13:46.752593 140409854486272 logging_writer.py:48] [433100] global_step=433100, grad_norm=4.799590587615967, loss=0.6709087491035461
I0301 23:14:20.562226 140409862878976 logging_writer.py:48] [433200] global_step=433200, grad_norm=4.735847473144531, loss=0.671757698059082
I0301 23:14:54.253023 140409854486272 logging_writer.py:48] [433300] global_step=433300, grad_norm=4.967317581176758, loss=0.6801068782806396
I0301 23:15:28.015866 140409862878976 logging_writer.py:48] [433400] global_step=433400, grad_norm=4.4629411697387695, loss=0.6466346383094788
I0301 23:16:01.688759 140409854486272 logging_writer.py:48] [433500] global_step=433500, grad_norm=4.201630115509033, loss=0.5534048676490784
I0301 23:16:35.460775 140409862878976 logging_writer.py:48] [433600] global_step=433600, grad_norm=4.3875956535339355, loss=0.6067282557487488
I0301 23:17:09.142361 140409854486272 logging_writer.py:48] [433700] global_step=433700, grad_norm=4.335555553436279, loss=0.6514257788658142
I0301 23:17:42.900364 140409862878976 logging_writer.py:48] [433800] global_step=433800, grad_norm=4.604629993438721, loss=0.6445911526679993
I0301 23:18:16.608147 140409854486272 logging_writer.py:48] [433900] global_step=433900, grad_norm=4.122826099395752, loss=0.5529050827026367
I0301 23:18:50.353281 140409862878976 logging_writer.py:48] [434000] global_step=434000, grad_norm=4.436727523803711, loss=0.6793407201766968
I0301 23:19:24.070778 140409854486272 logging_writer.py:48] [434100] global_step=434100, grad_norm=4.444815635681152, loss=0.5760399103164673
I0301 23:19:57.781006 140409862878976 logging_writer.py:48] [434200] global_step=434200, grad_norm=4.601142406463623, loss=0.678682804107666
I0301 23:20:31.550541 140409854486272 logging_writer.py:48] [434300] global_step=434300, grad_norm=4.208434104919434, loss=0.5055815577507019
I0301 23:21:05.202402 140409862878976 logging_writer.py:48] [434400] global_step=434400, grad_norm=4.3353424072265625, loss=0.6352707743644714
I0301 23:21:09.387734 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:21:15.589149 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:21:24.288930 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:21:26.547090 140573303715648 submission_runner.py:411] Time since start: 151610.54s, 	Step: 434414, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.14585164189338684, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0522480010986328, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8176249265670776, 'test/num_examples': 10000, 'score': 146429.5111811161, 'total_duration': 151610.5396182537, 'accumulated_submission_time': 146429.5111811161, 'accumulated_eval_time': 5143.729845046997, 'accumulated_logging_time': 20.137884616851807}
I0301 23:21:26.635098 140409124681472 logging_writer.py:48] [434414] accumulated_eval_time=5143.729845, accumulated_logging_time=20.137885, accumulated_submission_time=146429.511181, global_step=434414, preemption_count=0, score=146429.511181, test/accuracy=0.631800, test/loss=1.817625, test/num_examples=10000, total_duration=151610.539618, train/accuracy=0.961177, train/loss=0.145852, validation/accuracy=0.756140, validation/loss=1.052248, validation/num_examples=50000
I0301 23:21:55.903896 140409829308160 logging_writer.py:48] [434500] global_step=434500, grad_norm=4.7819294929504395, loss=0.6203437447547913
I0301 23:22:29.580404 140409124681472 logging_writer.py:48] [434600] global_step=434600, grad_norm=4.307033538818359, loss=0.5893739461898804
I0301 23:23:03.264377 140409829308160 logging_writer.py:48] [434700] global_step=434700, grad_norm=4.20751953125, loss=0.5763804316520691
I0301 23:23:36.995416 140409124681472 logging_writer.py:48] [434800] global_step=434800, grad_norm=4.526379585266113, loss=0.6438300013542175
I0301 23:24:10.717711 140409829308160 logging_writer.py:48] [434900] global_step=434900, grad_norm=4.448070526123047, loss=0.6150275468826294
I0301 23:24:44.442341 140409124681472 logging_writer.py:48] [435000] global_step=435000, grad_norm=4.76475191116333, loss=0.6790912747383118
I0301 23:25:18.217952 140409829308160 logging_writer.py:48] [435100] global_step=435100, grad_norm=4.504683971405029, loss=0.5962693691253662
I0301 23:25:51.920715 140409124681472 logging_writer.py:48] [435200] global_step=435200, grad_norm=4.330977439880371, loss=0.601962685585022
I0301 23:26:25.702490 140409829308160 logging_writer.py:48] [435300] global_step=435300, grad_norm=4.647674560546875, loss=0.5961132645606995
I0301 23:26:59.372149 140409124681472 logging_writer.py:48] [435400] global_step=435400, grad_norm=4.567071437835693, loss=0.7104579210281372
I0301 23:27:33.055068 140409829308160 logging_writer.py:48] [435500] global_step=435500, grad_norm=4.796299934387207, loss=0.6587981581687927
I0301 23:28:06.747795 140409124681472 logging_writer.py:48] [435600] global_step=435600, grad_norm=4.644412040710449, loss=0.653980016708374
I0301 23:28:40.459227 140409829308160 logging_writer.py:48] [435700] global_step=435700, grad_norm=4.699787616729736, loss=0.6213020086288452
I0301 23:29:14.176672 140409124681472 logging_writer.py:48] [435800] global_step=435800, grad_norm=4.538197994232178, loss=0.5487232804298401
I0301 23:29:47.915676 140409829308160 logging_writer.py:48] [435900] global_step=435900, grad_norm=4.18301248550415, loss=0.6011449694633484
I0301 23:29:56.837326 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:30:03.023462 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:30:11.522679 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:30:13.856777 140573303715648 submission_runner.py:411] Time since start: 152137.85s, 	Step: 435928, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.14708548784255981, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0511912107467651, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.816450834274292, 'test/num_examples': 10000, 'score': 146939.64353322983, 'total_duration': 152137.84930348396, 'accumulated_submission_time': 146939.64353322983, 'accumulated_eval_time': 5160.749233961105, 'accumulated_logging_time': 20.236326932907104}
I0301 23:30:13.943747 140409837700864 logging_writer.py:48] [435928] accumulated_eval_time=5160.749234, accumulated_logging_time=20.236327, accumulated_submission_time=146939.643533, global_step=435928, preemption_count=0, score=146939.643533, test/accuracy=0.631100, test/loss=1.816451, test/num_examples=10000, total_duration=152137.849303, train/accuracy=0.960918, train/loss=0.147085, validation/accuracy=0.756020, validation/loss=1.051191, validation/num_examples=50000
I0301 23:30:38.558213 140409846093568 logging_writer.py:48] [436000] global_step=436000, grad_norm=4.221586227416992, loss=0.5558395981788635
I0301 23:31:12.227713 140409837700864 logging_writer.py:48] [436100] global_step=436100, grad_norm=4.453090190887451, loss=0.5842499732971191
I0301 23:31:45.965965 140409846093568 logging_writer.py:48] [436200] global_step=436200, grad_norm=4.261134147644043, loss=0.6222467422485352
I0301 23:32:19.661386 140409837700864 logging_writer.py:48] [436300] global_step=436300, grad_norm=4.43251895904541, loss=0.6401569247245789
I0301 23:32:53.399302 140409846093568 logging_writer.py:48] [436400] global_step=436400, grad_norm=4.5667901039123535, loss=0.6479778289794922
I0301 23:33:27.072888 140409837700864 logging_writer.py:48] [436500] global_step=436500, grad_norm=4.593815326690674, loss=0.6686042547225952
I0301 23:34:00.816090 140409846093568 logging_writer.py:48] [436600] global_step=436600, grad_norm=4.988003730773926, loss=0.6694336533546448
I0301 23:34:34.498530 140409837700864 logging_writer.py:48] [436700] global_step=436700, grad_norm=4.687002182006836, loss=0.7058552503585815
I0301 23:35:08.239235 140409846093568 logging_writer.py:48] [436800] global_step=436800, grad_norm=5.239253044128418, loss=0.7518404126167297
I0301 23:35:41.938194 140409837700864 logging_writer.py:48] [436900] global_step=436900, grad_norm=4.4168381690979, loss=0.6202324628829956
I0301 23:36:15.673640 140409846093568 logging_writer.py:48] [437000] global_step=437000, grad_norm=4.406721115112305, loss=0.6306926608085632
I0301 23:36:49.371428 140409837700864 logging_writer.py:48] [437100] global_step=437100, grad_norm=4.202098369598389, loss=0.6310456991195679
I0301 23:37:23.102950 140409846093568 logging_writer.py:48] [437200] global_step=437200, grad_norm=4.672155380249023, loss=0.6367844939231873
I0301 23:37:56.788180 140409837700864 logging_writer.py:48] [437300] global_step=437300, grad_norm=4.602010726928711, loss=0.6539776921272278
I0301 23:38:30.516701 140409846093568 logging_writer.py:48] [437400] global_step=437400, grad_norm=4.45514440536499, loss=0.6704428791999817
I0301 23:38:43.916571 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:38:50.047343 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:38:58.383934 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:39:00.710275 140573303715648 submission_runner.py:411] Time since start: 152664.70s, 	Step: 437441, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14643460512161255, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0538361072540283, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8216536045074463, 'test/num_examples': 10000, 'score': 147449.54594254494, 'total_duration': 152664.7027964592, 'accumulated_submission_time': 147449.54594254494, 'accumulated_eval_time': 5177.542870759964, 'accumulated_logging_time': 20.333744525909424}
I0301 23:39:00.796139 140409829308160 logging_writer.py:48] [437441] accumulated_eval_time=5177.542871, accumulated_logging_time=20.333745, accumulated_submission_time=147449.545943, global_step=437441, preemption_count=0, score=147449.545943, test/accuracy=0.631100, test/loss=1.821654, test/num_examples=10000, total_duration=152664.702796, train/accuracy=0.960459, train/loss=0.146435, validation/accuracy=0.755860, validation/loss=1.053836, validation/num_examples=50000
I0301 23:39:20.967751 140409854486272 logging_writer.py:48] [437500] global_step=437500, grad_norm=4.343057155609131, loss=0.6354706287384033
I0301 23:39:54.592751 140409829308160 logging_writer.py:48] [437600] global_step=437600, grad_norm=4.19462776184082, loss=0.5730751752853394
I0301 23:40:28.303119 140409854486272 logging_writer.py:48] [437700] global_step=437700, grad_norm=4.188925266265869, loss=0.5924233198165894
I0301 23:41:01.959225 140409829308160 logging_writer.py:48] [437800] global_step=437800, grad_norm=5.083237648010254, loss=0.7057441473007202
I0301 23:41:35.645341 140409854486272 logging_writer.py:48] [437900] global_step=437900, grad_norm=4.215304851531982, loss=0.611093282699585
I0301 23:42:09.377759 140409829308160 logging_writer.py:48] [438000] global_step=438000, grad_norm=4.501729488372803, loss=0.5756956338882446
I0301 23:42:43.044960 140409854486272 logging_writer.py:48] [438100] global_step=438100, grad_norm=4.053352355957031, loss=0.5080804228782654
I0301 23:43:16.802556 140409829308160 logging_writer.py:48] [438200] global_step=438200, grad_norm=4.249540328979492, loss=0.5800005197525024
I0301 23:43:50.485295 140409854486272 logging_writer.py:48] [438300] global_step=438300, grad_norm=4.341391086578369, loss=0.6046226024627686
I0301 23:44:24.241120 140409829308160 logging_writer.py:48] [438400] global_step=438400, grad_norm=4.7691826820373535, loss=0.5973947048187256
I0301 23:44:58.064630 140409854486272 logging_writer.py:48] [438500] global_step=438500, grad_norm=4.624846458435059, loss=0.6266300082206726
I0301 23:45:31.717999 140409829308160 logging_writer.py:48] [438600] global_step=438600, grad_norm=4.593475818634033, loss=0.6086437106132507
I0301 23:46:05.435770 140409854486272 logging_writer.py:48] [438700] global_step=438700, grad_norm=4.389808654785156, loss=0.5808950066566467
I0301 23:46:39.130830 140409829308160 logging_writer.py:48] [438800] global_step=438800, grad_norm=4.850914478302002, loss=0.6848803162574768
I0301 23:47:12.813556 140409854486272 logging_writer.py:48] [438900] global_step=438900, grad_norm=4.353953838348389, loss=0.6600169539451599
I0301 23:47:30.854814 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:47:36.941596 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:47:45.386328 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:47:47.730178 140573303715648 submission_runner.py:411] Time since start: 153191.72s, 	Step: 438955, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14564605057239532, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0515245199203491, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.817052960395813, 'test/num_examples': 10000, 'score': 147959.53197169304, 'total_duration': 153191.72272014618, 'accumulated_submission_time': 147959.53197169304, 'accumulated_eval_time': 5194.418187379837, 'accumulated_logging_time': 20.43113422393799}
I0301 23:47:47.820146 140408319375104 logging_writer.py:48] [438955] accumulated_eval_time=5194.418187, accumulated_logging_time=20.431134, accumulated_submission_time=147959.531972, global_step=438955, preemption_count=0, score=147959.531972, test/accuracy=0.631000, test/loss=1.817053, test/num_examples=10000, total_duration=153191.722720, train/accuracy=0.960698, train/loss=0.145646, validation/accuracy=0.755700, validation/loss=1.051525, validation/num_examples=50000
I0301 23:48:03.342340 140409124681472 logging_writer.py:48] [439000] global_step=439000, grad_norm=4.476101875305176, loss=0.6086606979370117
I0301 23:48:36.996951 140408319375104 logging_writer.py:48] [439100] global_step=439100, grad_norm=4.87111759185791, loss=0.6986428499221802
I0301 23:49:10.660868 140409124681472 logging_writer.py:48] [439200] global_step=439200, grad_norm=4.413393974304199, loss=0.5785936117172241
I0301 23:49:44.380654 140408319375104 logging_writer.py:48] [439300] global_step=439300, grad_norm=4.096988677978516, loss=0.5748171210289001
I0301 23:50:18.104385 140409124681472 logging_writer.py:48] [439400] global_step=439400, grad_norm=4.658116340637207, loss=0.6488350629806519
I0301 23:50:51.894779 140408319375104 logging_writer.py:48] [439500] global_step=439500, grad_norm=4.638415336608887, loss=0.6392216682434082
I0301 23:51:25.614778 140409124681472 logging_writer.py:48] [439600] global_step=439600, grad_norm=4.622553825378418, loss=0.6736822724342346
I0301 23:51:59.346290 140408319375104 logging_writer.py:48] [439700] global_step=439700, grad_norm=4.72808313369751, loss=0.6578578948974609
I0301 23:52:33.108723 140409124681472 logging_writer.py:48] [439800] global_step=439800, grad_norm=5.054946422576904, loss=0.6281617879867554
I0301 23:53:06.806713 140408319375104 logging_writer.py:48] [439900] global_step=439900, grad_norm=4.612160682678223, loss=0.6407146453857422
I0301 23:53:40.546101 140409124681472 logging_writer.py:48] [440000] global_step=440000, grad_norm=4.599387168884277, loss=0.6642765402793884
I0301 23:54:14.281112 140408319375104 logging_writer.py:48] [440100] global_step=440100, grad_norm=4.6259894371032715, loss=0.6344842910766602
I0301 23:54:47.972837 140409124681472 logging_writer.py:48] [440200] global_step=440200, grad_norm=4.642736434936523, loss=0.6749823689460754
I0301 23:55:21.692042 140408319375104 logging_writer.py:48] [440300] global_step=440300, grad_norm=4.275525093078613, loss=0.5514360666275024
I0301 23:55:55.400301 140409124681472 logging_writer.py:48] [440400] global_step=440400, grad_norm=4.156613826751709, loss=0.6511433124542236
I0301 23:56:17.829906 140573303715648 spec.py:321] Evaluating on the training split.
I0301 23:56:23.880562 140573303715648 spec.py:333] Evaluating on the validation split.
I0301 23:56:32.613913 140573303715648 spec.py:349] Evaluating on the test split.
I0301 23:56:34.998223 140573303715648 submission_runner.py:411] Time since start: 153718.99s, 	Step: 440468, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14878422021865845, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0531373023986816, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8179895877838135, 'test/num_examples': 10000, 'score': 148469.47192668915, 'total_duration': 153718.99074602127, 'accumulated_submission_time': 148469.47192668915, 'accumulated_eval_time': 5211.586438894272, 'accumulated_logging_time': 20.53074598312378}
I0301 23:56:35.089208 140407379851008 logging_writer.py:48] [440468] accumulated_eval_time=5211.586439, accumulated_logging_time=20.530746, accumulated_submission_time=148469.471927, global_step=440468, preemption_count=0, score=148469.471927, test/accuracy=0.632000, test/loss=1.817990, test/num_examples=10000, total_duration=153718.990746, train/accuracy=0.960419, train/loss=0.148784, validation/accuracy=0.755460, validation/loss=1.053137, validation/num_examples=50000
I0301 23:56:46.236784 140408319375104 logging_writer.py:48] [440500] global_step=440500, grad_norm=4.604219436645508, loss=0.6786504983901978
I0301 23:57:20.008261 140407379851008 logging_writer.py:48] [440600] global_step=440600, grad_norm=4.468470573425293, loss=0.6588708758354187
I0301 23:57:53.751086 140408319375104 logging_writer.py:48] [440700] global_step=440700, grad_norm=4.648878574371338, loss=0.6437601447105408
I0301 23:58:27.451120 140407379851008 logging_writer.py:48] [440800] global_step=440800, grad_norm=4.665126800537109, loss=0.6328890323638916
I0301 23:59:01.182129 140408319375104 logging_writer.py:48] [440900] global_step=440900, grad_norm=4.640181064605713, loss=0.6697896718978882
I0301 23:59:34.859122 140407379851008 logging_writer.py:48] [441000] global_step=441000, grad_norm=4.107654571533203, loss=0.5924473404884338
I0302 00:00:08.582464 140408319375104 logging_writer.py:48] [441100] global_step=441100, grad_norm=4.796587944030762, loss=0.6289945840835571
I0302 00:00:42.298672 140407379851008 logging_writer.py:48] [441200] global_step=441200, grad_norm=4.380340576171875, loss=0.6481108665466309
I0302 00:01:16.004314 140408319375104 logging_writer.py:48] [441300] global_step=441300, grad_norm=4.524250507354736, loss=0.6573609709739685
I0302 00:01:49.655919 140407379851008 logging_writer.py:48] [441400] global_step=441400, grad_norm=4.807711124420166, loss=0.6645084619522095
I0302 00:02:23.294191 140408319375104 logging_writer.py:48] [441500] global_step=441500, grad_norm=4.279675006866455, loss=0.5813149809837341
I0302 00:02:56.965994 140407379851008 logging_writer.py:48] [441600] global_step=441600, grad_norm=4.473799228668213, loss=0.6658605933189392
I0302 00:03:30.758245 140408319375104 logging_writer.py:48] [441700] global_step=441700, grad_norm=4.458707809448242, loss=0.6542407274246216
I0302 00:04:04.477993 140407379851008 logging_writer.py:48] [441800] global_step=441800, grad_norm=4.392642498016357, loss=0.6035053730010986
I0302 00:04:38.205369 140408319375104 logging_writer.py:48] [441900] global_step=441900, grad_norm=4.557431221008301, loss=0.7529765367507935
I0302 00:05:05.334046 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:05:11.386659 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:05:20.013650 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:05:22.324553 140573303715648 submission_runner.py:411] Time since start: 154246.32s, 	Step: 441982, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14585812389850616, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0520803928375244, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8177663087844849, 'test/num_examples': 10000, 'score': 148979.646399498, 'total_duration': 154246.31707787514, 'accumulated_submission_time': 148979.646399498, 'accumulated_eval_time': 5228.576928853989, 'accumulated_logging_time': 20.633300065994263}
I0302 00:05:22.418088 140409829308160 logging_writer.py:48] [441982] accumulated_eval_time=5228.576929, accumulated_logging_time=20.633300, accumulated_submission_time=148979.646399, global_step=441982, preemption_count=0, score=148979.646399, test/accuracy=0.631400, test/loss=1.817766, test/num_examples=10000, total_duration=154246.317078, train/accuracy=0.961037, train/loss=0.145858, validation/accuracy=0.756100, validation/loss=1.052080, validation/num_examples=50000
I0302 00:05:28.825544 140409837700864 logging_writer.py:48] [442000] global_step=442000, grad_norm=4.25269889831543, loss=0.5845920443534851
I0302 00:06:02.549805 140409829308160 logging_writer.py:48] [442100] global_step=442100, grad_norm=4.453127861022949, loss=0.6480720639228821
I0302 00:06:36.194311 140409837700864 logging_writer.py:48] [442200] global_step=442200, grad_norm=4.35331392288208, loss=0.6644783020019531
I0302 00:07:09.878396 140409829308160 logging_writer.py:48] [442300] global_step=442300, grad_norm=4.457346439361572, loss=0.6742632985115051
I0302 00:07:43.572919 140409837700864 logging_writer.py:48] [442400] global_step=442400, grad_norm=4.447381496429443, loss=0.6508015394210815
I0302 00:08:17.266293 140409829308160 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.948105335235596, loss=0.6806265115737915
I0302 00:08:50.981534 140409837700864 logging_writer.py:48] [442600] global_step=442600, grad_norm=4.062891483306885, loss=0.6113576292991638
I0302 00:09:24.858837 140409829308160 logging_writer.py:48] [442700] global_step=442700, grad_norm=4.707243919372559, loss=0.6659178733825684
I0302 00:09:58.591552 140409837700864 logging_writer.py:48] [442800] global_step=442800, grad_norm=4.7507452964782715, loss=0.5931916236877441
I0302 00:10:32.291748 140409829308160 logging_writer.py:48] [442900] global_step=442900, grad_norm=4.0146613121032715, loss=0.6084602475166321
I0302 00:11:06.025470 140409837700864 logging_writer.py:48] [443000] global_step=443000, grad_norm=4.5518670082092285, loss=0.6340952515602112
I0302 00:11:39.713058 140409829308160 logging_writer.py:48] [443100] global_step=443100, grad_norm=4.287911891937256, loss=0.5917286276817322
I0302 00:12:13.449347 140409837700864 logging_writer.py:48] [443200] global_step=443200, grad_norm=4.727970600128174, loss=0.6122843623161316
I0302 00:12:47.126156 140409829308160 logging_writer.py:48] [443300] global_step=443300, grad_norm=4.7870893478393555, loss=0.6568463444709778
I0302 00:13:20.855899 140409837700864 logging_writer.py:48] [443400] global_step=443400, grad_norm=4.323835372924805, loss=0.648383378982544
I0302 00:13:52.638379 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:13:58.689195 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:14:07.386332 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:14:09.689405 140573303715648 submission_runner.py:411] Time since start: 154773.68s, 	Step: 443496, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.1478644162416458, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0514676570892334, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8164036273956299, 'test/num_examples': 10000, 'score': 149489.79416179657, 'total_duration': 154773.68194508553, 'accumulated_submission_time': 149489.79416179657, 'accumulated_eval_time': 5245.627908229828, 'accumulated_logging_time': 20.737919569015503}
I0302 00:14:09.779998 140409124681472 logging_writer.py:48] [443496] accumulated_eval_time=5245.627908, accumulated_logging_time=20.737920, accumulated_submission_time=149489.794162, global_step=443496, preemption_count=0, score=149489.794162, test/accuracy=0.631500, test/loss=1.816404, test/num_examples=10000, total_duration=154773.681945, train/accuracy=0.960180, train/loss=0.147864, validation/accuracy=0.755700, validation/loss=1.051468, validation/num_examples=50000
I0302 00:14:11.467525 140409829308160 logging_writer.py:48] [443500] global_step=443500, grad_norm=4.612070083618164, loss=0.6100313663482666
I0302 00:14:45.130950 140409124681472 logging_writer.py:48] [443600] global_step=443600, grad_norm=4.258574962615967, loss=0.6548349261283875
I0302 00:15:18.837883 140409829308160 logging_writer.py:48] [443700] global_step=443700, grad_norm=4.755549430847168, loss=0.6189258694648743
I0302 00:15:52.682991 140409124681472 logging_writer.py:48] [443800] global_step=443800, grad_norm=4.742627143859863, loss=0.6405812501907349
I0302 00:16:26.320915 140409829308160 logging_writer.py:48] [443900] global_step=443900, grad_norm=4.745220184326172, loss=0.6028439402580261
I0302 00:17:00.001375 140409124681472 logging_writer.py:48] [444000] global_step=444000, grad_norm=5.4107747077941895, loss=0.581939697265625
I0302 00:17:33.714132 140409829308160 logging_writer.py:48] [444100] global_step=444100, grad_norm=4.645539283752441, loss=0.6558959484100342
I0302 00:18:07.360935 140409124681472 logging_writer.py:48] [444200] global_step=444200, grad_norm=4.761912822723389, loss=0.7218385338783264
I0302 00:18:41.074366 140409829308160 logging_writer.py:48] [444300] global_step=444300, grad_norm=4.743218898773193, loss=0.6366033554077148
I0302 00:19:14.748289 140409124681472 logging_writer.py:48] [444400] global_step=444400, grad_norm=5.5113525390625, loss=0.6127218008041382
I0302 00:19:48.478062 140409829308160 logging_writer.py:48] [444500] global_step=444500, grad_norm=4.601369857788086, loss=0.6325571537017822
I0302 00:20:22.175039 140409124681472 logging_writer.py:48] [444600] global_step=444600, grad_norm=4.578803062438965, loss=0.6341979503631592
I0302 00:20:55.813183 140409829308160 logging_writer.py:48] [444700] global_step=444700, grad_norm=4.036924839019775, loss=0.5863845348358154
I0302 00:21:29.674344 140409124681472 logging_writer.py:48] [444800] global_step=444800, grad_norm=4.248476982116699, loss=0.5504466891288757
I0302 00:22:03.382729 140409829308160 logging_writer.py:48] [444900] global_step=444900, grad_norm=4.587033748626709, loss=0.6900714039802551
I0302 00:22:37.046555 140409124681472 logging_writer.py:48] [445000] global_step=445000, grad_norm=4.262876987457275, loss=0.616718053817749
I0302 00:22:39.893748 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:22:45.914989 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:22:54.520233 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:22:56.863095 140573303715648 submission_runner.py:411] Time since start: 155300.86s, 	Step: 445010, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14759701490402222, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0521252155303955, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.817966341972351, 'test/num_examples': 10000, 'score': 149999.8376750946, 'total_duration': 155300.85563635826, 'accumulated_submission_time': 149999.8376750946, 'accumulated_eval_time': 5262.597207069397, 'accumulated_logging_time': 20.838828086853027}
I0302 00:22:56.959835 140408319375104 logging_writer.py:48] [445010] accumulated_eval_time=5262.597207, accumulated_logging_time=20.838828, accumulated_submission_time=149999.837675, global_step=445010, preemption_count=0, score=149999.837675, test/accuracy=0.631200, test/loss=1.817966, test/num_examples=10000, total_duration=155300.855636, train/accuracy=0.960938, train/loss=0.147597, validation/accuracy=0.755700, validation/loss=1.052125, validation/num_examples=50000
I0302 00:23:27.630254 140409124681472 logging_writer.py:48] [445100] global_step=445100, grad_norm=4.954866409301758, loss=0.6430147290229797
I0302 00:24:01.314478 140408319375104 logging_writer.py:48] [445200] global_step=445200, grad_norm=4.07232141494751, loss=0.5594969391822815
I0302 00:24:35.039738 140409124681472 logging_writer.py:48] [445300] global_step=445300, grad_norm=4.281859397888184, loss=0.5456270575523376
I0302 00:25:08.718383 140408319375104 logging_writer.py:48] [445400] global_step=445400, grad_norm=4.490034103393555, loss=0.5867472887039185
I0302 00:25:42.463826 140409124681472 logging_writer.py:48] [445500] global_step=445500, grad_norm=4.648472785949707, loss=0.626114010810852
I0302 00:26:16.188742 140408319375104 logging_writer.py:48] [445600] global_step=445600, grad_norm=5.049036979675293, loss=0.6951749324798584
I0302 00:26:49.928461 140409124681472 logging_writer.py:48] [445700] global_step=445700, grad_norm=4.366036415100098, loss=0.6020451784133911
I0302 00:27:23.647161 140408319375104 logging_writer.py:48] [445800] global_step=445800, grad_norm=4.041252613067627, loss=0.5626555681228638
I0302 00:27:57.405803 140409124681472 logging_writer.py:48] [445900] global_step=445900, grad_norm=4.367079734802246, loss=0.5732203125953674
I0302 00:28:31.082788 140408319375104 logging_writer.py:48] [446000] global_step=446000, grad_norm=4.5402750968933105, loss=0.6539266109466553
I0302 00:29:04.813886 140409124681472 logging_writer.py:48] [446100] global_step=446100, grad_norm=4.223288059234619, loss=0.5684486031532288
I0302 00:29:38.528987 140408319375104 logging_writer.py:48] [446200] global_step=446200, grad_norm=4.283508777618408, loss=0.6542965173721313
I0302 00:30:12.290415 140409124681472 logging_writer.py:48] [446300] global_step=446300, grad_norm=4.777310371398926, loss=0.7009230256080627
I0302 00:30:45.988036 140408319375104 logging_writer.py:48] [446400] global_step=446400, grad_norm=4.770445346832275, loss=0.575407862663269
I0302 00:31:19.747565 140409124681472 logging_writer.py:48] [446500] global_step=446500, grad_norm=4.563192367553711, loss=0.6599072217941284
I0302 00:31:27.000168 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:31:33.090309 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:31:41.694910 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:31:44.052647 140573303715648 submission_runner.py:411] Time since start: 155828.05s, 	Step: 446523, 	{'train/accuracy': 0.9608378410339355, 'train/loss': 0.14829719066619873, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0520795583724976, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8180619478225708, 'test/num_examples': 10000, 'score': 150509.80757832527, 'total_duration': 155828.0451927185, 'accumulated_submission_time': 150509.80757832527, 'accumulated_eval_time': 5279.649640798569, 'accumulated_logging_time': 20.945537328720093}
I0302 00:31:44.153947 140407379851008 logging_writer.py:48] [446523] accumulated_eval_time=5279.649641, accumulated_logging_time=20.945537, accumulated_submission_time=150509.807578, global_step=446523, preemption_count=0, score=150509.807578, test/accuracy=0.631300, test/loss=1.818062, test/num_examples=10000, total_duration=155828.045193, train/accuracy=0.960838, train/loss=0.148297, validation/accuracy=0.755860, validation/loss=1.052080, validation/num_examples=50000
I0302 00:32:10.395904 140408319375104 logging_writer.py:48] [446600] global_step=446600, grad_norm=4.67503547668457, loss=0.6205519437789917
I0302 00:32:44.089481 140407379851008 logging_writer.py:48] [446700] global_step=446700, grad_norm=4.785571098327637, loss=0.5762394666671753
I0302 00:33:17.773441 140408319375104 logging_writer.py:48] [446800] global_step=446800, grad_norm=4.517182350158691, loss=0.6308093070983887
I0302 00:33:51.514733 140407379851008 logging_writer.py:48] [446900] global_step=446900, grad_norm=4.920769691467285, loss=0.6190388798713684
I0302 00:34:25.213783 140408319375104 logging_writer.py:48] [447000] global_step=447000, grad_norm=4.445838928222656, loss=0.5912633538246155
I0302 00:34:58.910006 140407379851008 logging_writer.py:48] [447100] global_step=447100, grad_norm=5.107122421264648, loss=0.6493303775787354
I0302 00:35:32.563570 140408319375104 logging_writer.py:48] [447200] global_step=447200, grad_norm=4.58098840713501, loss=0.5553692579269409
I0302 00:36:06.317311 140407379851008 logging_writer.py:48] [447300] global_step=447300, grad_norm=4.1039533615112305, loss=0.6325236558914185
I0302 00:36:39.976634 140408319375104 logging_writer.py:48] [447400] global_step=447400, grad_norm=4.650385856628418, loss=0.6394574046134949
I0302 00:37:13.657749 140407379851008 logging_writer.py:48] [447500] global_step=447500, grad_norm=4.538212299346924, loss=0.6089227199554443
I0302 00:37:47.384644 140408319375104 logging_writer.py:48] [447600] global_step=447600, grad_norm=4.036640644073486, loss=0.5188243985176086
I0302 00:38:21.069726 140407379851008 logging_writer.py:48] [447700] global_step=447700, grad_norm=5.2590556144714355, loss=0.7059097290039062
I0302 00:38:54.746729 140408319375104 logging_writer.py:48] [447800] global_step=447800, grad_norm=4.546157360076904, loss=0.5939969420433044
I0302 00:39:28.499912 140407379851008 logging_writer.py:48] [447900] global_step=447900, grad_norm=4.651285648345947, loss=0.6634429693222046
I0302 00:40:02.265732 140408319375104 logging_writer.py:48] [448000] global_step=448000, grad_norm=4.408100128173828, loss=0.593633234500885
I0302 00:40:14.216626 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:40:20.273202 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:40:28.631742 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:40:30.976489 140573303715648 submission_runner.py:411] Time since start: 156354.97s, 	Step: 448037, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.1471826732158661, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0523767471313477, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.818080186843872, 'test/num_examples': 10000, 'score': 151019.7969942093, 'total_duration': 156354.96886587143, 'accumulated_submission_time': 151019.7969942093, 'accumulated_eval_time': 5296.409289121628, 'accumulated_logging_time': 21.0596821308136}
I0302 00:40:31.063730 140409829308160 logging_writer.py:48] [448037] accumulated_eval_time=5296.409289, accumulated_logging_time=21.059682, accumulated_submission_time=151019.796994, global_step=448037, preemption_count=0, score=151019.796994, test/accuracy=0.630600, test/loss=1.818080, test/num_examples=10000, total_duration=156354.968866, train/accuracy=0.961316, train/loss=0.147183, validation/accuracy=0.755860, validation/loss=1.052377, validation/num_examples=50000
I0302 00:40:52.578422 140409837700864 logging_writer.py:48] [448100] global_step=448100, grad_norm=4.132062911987305, loss=0.6604165434837341
I0302 00:41:26.304090 140409829308160 logging_writer.py:48] [448200] global_step=448200, grad_norm=4.752771854400635, loss=0.7057377696037292
I0302 00:41:59.999805 140409837700864 logging_writer.py:48] [448300] global_step=448300, grad_norm=4.675832748413086, loss=0.6390694379806519
I0302 00:42:33.743543 140409829308160 logging_writer.py:48] [448400] global_step=448400, grad_norm=4.902249813079834, loss=0.6584147810935974
I0302 00:43:07.424886 140409837700864 logging_writer.py:48] [448500] global_step=448500, grad_norm=4.6847615242004395, loss=0.6471570730209351
I0302 00:43:41.166821 140409829308160 logging_writer.py:48] [448600] global_step=448600, grad_norm=4.6541032791137695, loss=0.5473980903625488
I0302 00:44:14.848563 140409837700864 logging_writer.py:48] [448700] global_step=448700, grad_norm=4.452518939971924, loss=0.620111882686615
I0302 00:44:48.617979 140409829308160 logging_writer.py:48] [448800] global_step=448800, grad_norm=4.75063419342041, loss=0.6689105033874512
I0302 00:45:22.301796 140409837700864 logging_writer.py:48] [448900] global_step=448900, grad_norm=4.617571830749512, loss=0.6169872879981995
I0302 00:45:56.074756 140409829308160 logging_writer.py:48] [449000] global_step=449000, grad_norm=4.491372108459473, loss=0.6500738263130188
I0302 00:46:29.949942 140409837700864 logging_writer.py:48] [449100] global_step=449100, grad_norm=4.1992268562316895, loss=0.6214261651039124
I0302 00:47:03.708495 140409829308160 logging_writer.py:48] [449200] global_step=449200, grad_norm=4.417893409729004, loss=0.6227229237556458
I0302 00:47:37.403874 140409837700864 logging_writer.py:48] [449300] global_step=449300, grad_norm=4.590556621551514, loss=0.647344708442688
I0302 00:48:11.159410 140409829308160 logging_writer.py:48] [449400] global_step=449400, grad_norm=4.139949321746826, loss=0.561856746673584
I0302 00:48:44.861216 140409837700864 logging_writer.py:48] [449500] global_step=449500, grad_norm=4.440640926361084, loss=0.6437692642211914
I0302 00:49:01.199758 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:49:07.363408 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:49:15.964587 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:49:18.276330 140573303715648 submission_runner.py:411] Time since start: 156882.27s, 	Step: 449550, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14854641258716583, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.050680160522461, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.816544771194458, 'test/num_examples': 10000, 'score': 151529.86161398888, 'total_duration': 156882.26883530617, 'accumulated_submission_time': 151529.86161398888, 'accumulated_eval_time': 5313.485778331757, 'accumulated_logging_time': 21.158769369125366}
I0302 00:49:18.363676 140408319375104 logging_writer.py:48] [449550] accumulated_eval_time=5313.485778, accumulated_logging_time=21.158769, accumulated_submission_time=151529.861614, global_step=449550, preemption_count=0, score=151529.861614, test/accuracy=0.630800, test/loss=1.816545, test/num_examples=10000, total_duration=156882.268835, train/accuracy=0.960340, train/loss=0.148546, validation/accuracy=0.755660, validation/loss=1.050680, validation/num_examples=50000
I0302 00:49:35.564035 140409124681472 logging_writer.py:48] [449600] global_step=449600, grad_norm=4.322066783905029, loss=0.6120938062667847
I0302 00:50:09.218607 140408319375104 logging_writer.py:48] [449700] global_step=449700, grad_norm=4.3098883628845215, loss=0.5895300507545471
I0302 00:50:42.901116 140409124681472 logging_writer.py:48] [449800] global_step=449800, grad_norm=4.541607856750488, loss=0.6515712738037109
I0302 00:51:16.600012 140408319375104 logging_writer.py:48] [449900] global_step=449900, grad_norm=4.638588905334473, loss=0.5992981791496277
I0302 00:51:50.275522 140409124681472 logging_writer.py:48] [450000] global_step=450000, grad_norm=4.7717766761779785, loss=0.6144394278526306
I0302 00:52:24.136703 140408319375104 logging_writer.py:48] [450100] global_step=450100, grad_norm=4.06989860534668, loss=0.6204850077629089
I0302 00:52:57.845600 140409124681472 logging_writer.py:48] [450200] global_step=450200, grad_norm=4.361574649810791, loss=0.625950038433075
I0302 00:53:31.562798 140408319375104 logging_writer.py:48] [450300] global_step=450300, grad_norm=4.859210014343262, loss=0.7024527192115784
I0302 00:54:05.276912 140409124681472 logging_writer.py:48] [450400] global_step=450400, grad_norm=4.304807662963867, loss=0.6456291079521179
I0302 00:54:39.006545 140408319375104 logging_writer.py:48] [450500] global_step=450500, grad_norm=4.805055618286133, loss=0.6488810777664185
I0302 00:55:12.725721 140409124681472 logging_writer.py:48] [450600] global_step=450600, grad_norm=4.569746494293213, loss=0.6260921955108643
I0302 00:55:46.438105 140408319375104 logging_writer.py:48] [450700] global_step=450700, grad_norm=4.920121192932129, loss=0.6567243933677673
I0302 00:56:20.110984 140409124681472 logging_writer.py:48] [450800] global_step=450800, grad_norm=4.984960556030273, loss=0.5589084029197693
I0302 00:56:53.836088 140408319375104 logging_writer.py:48] [450900] global_step=450900, grad_norm=4.649170398712158, loss=0.6369993686676025
I0302 00:57:27.555664 140409124681472 logging_writer.py:48] [451000] global_step=451000, grad_norm=5.033142566680908, loss=0.6490216255187988
I0302 00:57:48.295466 140573303715648 spec.py:321] Evaluating on the training split.
I0302 00:57:54.309133 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 00:58:02.910157 140573303715648 spec.py:349] Evaluating on the test split.
I0302 00:58:05.236379 140573303715648 submission_runner.py:411] Time since start: 157409.23s, 	Step: 451063, 	{'train/accuracy': 0.9596220850944519, 'train/loss': 0.14822249114513397, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0514514446258545, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8174322843551636, 'test/num_examples': 10000, 'score': 152039.7231528759, 'total_duration': 157409.22890758514, 'accumulated_submission_time': 152039.7231528759, 'accumulated_eval_time': 5330.426630020142, 'accumulated_logging_time': 21.256157636642456}
I0302 00:58:05.327809 140409837700864 logging_writer.py:48] [451063] accumulated_eval_time=5330.426630, accumulated_logging_time=21.256158, accumulated_submission_time=152039.723153, global_step=451063, preemption_count=0, score=152039.723153, test/accuracy=0.631600, test/loss=1.817432, test/num_examples=10000, total_duration=157409.228908, train/accuracy=0.959622, train/loss=0.148222, validation/accuracy=0.755940, validation/loss=1.051451, validation/num_examples=50000
I0302 00:58:18.148665 140409862878976 logging_writer.py:48] [451100] global_step=451100, grad_norm=4.394560813903809, loss=0.630290150642395
I0302 00:58:51.842918 140409837700864 logging_writer.py:48] [451200] global_step=451200, grad_norm=4.369093418121338, loss=0.6537207365036011
I0302 00:59:25.591769 140409862878976 logging_writer.py:48] [451300] global_step=451300, grad_norm=4.050917625427246, loss=0.6095784306526184
I0302 00:59:59.245779 140409837700864 logging_writer.py:48] [451400] global_step=451400, grad_norm=4.334831714630127, loss=0.6178921461105347
I0302 01:00:32.936114 140409862878976 logging_writer.py:48] [451500] global_step=451500, grad_norm=4.703579425811768, loss=0.6670234203338623
I0302 01:01:06.656861 140409837700864 logging_writer.py:48] [451600] global_step=451600, grad_norm=4.73378324508667, loss=0.5843513607978821
I0302 01:01:40.309977 140409862878976 logging_writer.py:48] [451700] global_step=451700, grad_norm=4.585277080535889, loss=0.6183750033378601
I0302 01:02:14.029876 140409837700864 logging_writer.py:48] [451800] global_step=451800, grad_norm=4.046337604522705, loss=0.5125153064727783
I0302 01:02:47.705503 140409862878976 logging_writer.py:48] [451900] global_step=451900, grad_norm=4.57461404800415, loss=0.6560912132263184
I0302 01:03:21.405281 140409837700864 logging_writer.py:48] [452000] global_step=452000, grad_norm=4.439471244812012, loss=0.6341732144355774
I0302 01:03:55.124324 140409862878976 logging_writer.py:48] [452100] global_step=452100, grad_norm=4.634022235870361, loss=0.595637857913971
I0302 01:04:28.876234 140409837700864 logging_writer.py:48] [452200] global_step=452200, grad_norm=4.495944499969482, loss=0.6936157941818237
I0302 01:05:02.562106 140409862878976 logging_writer.py:48] [452300] global_step=452300, grad_norm=4.248629570007324, loss=0.5915238857269287
I0302 01:05:36.265520 140409837700864 logging_writer.py:48] [452400] global_step=452400, grad_norm=4.682138442993164, loss=0.6370292901992798
I0302 01:06:09.973274 140409862878976 logging_writer.py:48] [452500] global_step=452500, grad_norm=5.074554443359375, loss=0.6211296319961548
I0302 01:06:35.424367 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:06:41.581894 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:06:50.254063 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:06:52.506127 140573303715648 submission_runner.py:411] Time since start: 157936.50s, 	Step: 452577, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.1482243835926056, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0523943901062012, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8172379732131958, 'test/num_examples': 10000, 'score': 152549.7496676445, 'total_duration': 157936.49866342545, 'accumulated_submission_time': 152549.7496676445, 'accumulated_eval_time': 5347.50834107399, 'accumulated_logging_time': 21.35846757888794}
I0302 01:06:52.597884 140409124681472 logging_writer.py:48] [452577] accumulated_eval_time=5347.508341, accumulated_logging_time=21.358468, accumulated_submission_time=152549.749668, global_step=452577, preemption_count=0, score=152549.749668, test/accuracy=0.631600, test/loss=1.817238, test/num_examples=10000, total_duration=157936.498663, train/accuracy=0.960260, train/loss=0.148224, validation/accuracy=0.755220, validation/loss=1.052394, validation/num_examples=50000
I0302 01:07:00.682384 140409829308160 logging_writer.py:48] [452600] global_step=452600, grad_norm=4.820448875427246, loss=0.6318161487579346
I0302 01:07:34.319614 140409124681472 logging_writer.py:48] [452700] global_step=452700, grad_norm=4.561576843261719, loss=0.6528908610343933
I0302 01:08:08.050524 140409829308160 logging_writer.py:48] [452800] global_step=452800, grad_norm=4.600182056427002, loss=0.5829257965087891
I0302 01:08:41.751075 140409124681472 logging_writer.py:48] [452900] global_step=452900, grad_norm=4.489963531494141, loss=0.6514707803726196
I0302 01:09:15.440152 140409829308160 logging_writer.py:48] [453000] global_step=453000, grad_norm=4.462800979614258, loss=0.6063249111175537
I0302 01:09:49.198628 140409124681472 logging_writer.py:48] [453100] global_step=453100, grad_norm=4.253273010253906, loss=0.6153120994567871
I0302 01:10:22.917151 140409829308160 logging_writer.py:48] [453200] global_step=453200, grad_norm=4.188564777374268, loss=0.5570725798606873
I0302 01:10:56.848374 140409124681472 logging_writer.py:48] [453300] global_step=453300, grad_norm=4.594119071960449, loss=0.6275826096534729
I0302 01:11:30.559714 140409829308160 logging_writer.py:48] [453400] global_step=453400, grad_norm=4.580609321594238, loss=0.5732870101928711
I0302 01:12:04.253721 140409124681472 logging_writer.py:48] [453500] global_step=453500, grad_norm=4.134565353393555, loss=0.5916216373443604
I0302 01:12:37.902048 140409829308160 logging_writer.py:48] [453600] global_step=453600, grad_norm=4.397861480712891, loss=0.6600649952888489
I0302 01:13:11.567186 140409124681472 logging_writer.py:48] [453700] global_step=453700, grad_norm=4.5009613037109375, loss=0.6287025809288025
I0302 01:13:45.292680 140409829308160 logging_writer.py:48] [453800] global_step=453800, grad_norm=4.818447589874268, loss=0.6426498293876648
I0302 01:14:18.987230 140409124681472 logging_writer.py:48] [453900] global_step=453900, grad_norm=4.900924205780029, loss=0.6316088438034058
I0302 01:14:52.714437 140409829308160 logging_writer.py:48] [454000] global_step=454000, grad_norm=4.31427001953125, loss=0.6114652752876282
I0302 01:15:22.555728 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:15:28.596338 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:15:37.249765 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:15:39.610104 140573303715648 submission_runner.py:411] Time since start: 158463.60s, 	Step: 454090, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14767880737781525, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.052739143371582, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8187165260314941, 'test/num_examples': 10000, 'score': 153059.6380867958, 'total_duration': 158463.60263371468, 'accumulated_submission_time': 153059.6380867958, 'accumulated_eval_time': 5364.562657833099, 'accumulated_logging_time': 21.45997190475464}
I0302 01:15:39.703113 140407379851008 logging_writer.py:48] [454090] accumulated_eval_time=5364.562658, accumulated_logging_time=21.459972, accumulated_submission_time=153059.638087, global_step=454090, preemption_count=0, score=153059.638087, test/accuracy=0.630500, test/loss=1.818717, test/num_examples=10000, total_duration=158463.602634, train/accuracy=0.961017, train/loss=0.147679, validation/accuracy=0.756040, validation/loss=1.052739, validation/num_examples=50000
I0302 01:15:43.421462 140408319375104 logging_writer.py:48] [454100] global_step=454100, grad_norm=5.068382740020752, loss=0.6947713494300842
I0302 01:16:17.058833 140407379851008 logging_writer.py:48] [454200] global_step=454200, grad_norm=4.83533239364624, loss=0.6601133346557617
I0302 01:16:50.901472 140408319375104 logging_writer.py:48] [454300] global_step=454300, grad_norm=4.92194128036499, loss=0.7141594886779785
I0302 01:17:24.616230 140407379851008 logging_writer.py:48] [454400] global_step=454400, grad_norm=4.952548027038574, loss=0.6683508157730103
I0302 01:17:58.267864 140408319375104 logging_writer.py:48] [454500] global_step=454500, grad_norm=4.925127983093262, loss=0.6842681169509888
I0302 01:18:32.019871 140407379851008 logging_writer.py:48] [454600] global_step=454600, grad_norm=4.278721809387207, loss=0.6724675893783569
I0302 01:19:05.684414 140408319375104 logging_writer.py:48] [454700] global_step=454700, grad_norm=4.6430888175964355, loss=0.621300995349884
I0302 01:19:39.447402 140407379851008 logging_writer.py:48] [454800] global_step=454800, grad_norm=4.628242015838623, loss=0.6727254986763
I0302 01:20:13.172055 140408319375104 logging_writer.py:48] [454900] global_step=454900, grad_norm=4.057985782623291, loss=0.5251936912536621
I0302 01:20:46.890254 140407379851008 logging_writer.py:48] [455000] global_step=455000, grad_norm=4.333069324493408, loss=0.6224329471588135
I0302 01:21:20.618670 140408319375104 logging_writer.py:48] [455100] global_step=455100, grad_norm=4.381782054901123, loss=0.6674083471298218
I0302 01:21:54.292744 140407379851008 logging_writer.py:48] [455200] global_step=455200, grad_norm=4.210834503173828, loss=0.6139364242553711
I0302 01:22:27.987381 140408319375104 logging_writer.py:48] [455300] global_step=455300, grad_norm=4.66470193862915, loss=0.6427263021469116
I0302 01:23:01.767865 140407379851008 logging_writer.py:48] [455400] global_step=455400, grad_norm=5.0806884765625, loss=0.6861580014228821
I0302 01:23:35.506822 140408319375104 logging_writer.py:48] [455500] global_step=455500, grad_norm=4.134464740753174, loss=0.651520848274231
I0302 01:24:09.156412 140407379851008 logging_writer.py:48] [455600] global_step=455600, grad_norm=4.518725395202637, loss=0.6727303862571716
I0302 01:24:09.638958 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:24:15.709057 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:24:24.152369 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:24:26.486360 140573303715648 submission_runner.py:411] Time since start: 158990.48s, 	Step: 455603, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14806123077869415, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0525990724563599, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8188402652740479, 'test/num_examples': 10000, 'score': 153569.5043578148, 'total_duration': 158990.47890138626, 'accumulated_submission_time': 153569.5043578148, 'accumulated_eval_time': 5381.410010099411, 'accumulated_logging_time': 21.563621997833252}
I0302 01:24:26.579611 140408319375104 logging_writer.py:48] [455603] accumulated_eval_time=5381.410010, accumulated_logging_time=21.563622, accumulated_submission_time=153569.504358, global_step=455603, preemption_count=0, score=153569.504358, test/accuracy=0.631000, test/loss=1.818840, test/num_examples=10000, total_duration=158990.478901, train/accuracy=0.960599, train/loss=0.148061, validation/accuracy=0.755760, validation/loss=1.052599, validation/num_examples=50000
I0302 01:24:59.516437 140409837700864 logging_writer.py:48] [455700] global_step=455700, grad_norm=4.625283718109131, loss=0.5775152444839478
I0302 01:25:33.181872 140408319375104 logging_writer.py:48] [455800] global_step=455800, grad_norm=4.709146976470947, loss=0.5767498016357422
I0302 01:26:06.897646 140409837700864 logging_writer.py:48] [455900] global_step=455900, grad_norm=4.262402057647705, loss=0.5657515525817871
I0302 01:26:40.612288 140408319375104 logging_writer.py:48] [456000] global_step=456000, grad_norm=4.036528587341309, loss=0.5907697677612305
I0302 01:27:14.277833 140409837700864 logging_writer.py:48] [456100] global_step=456100, grad_norm=4.362542629241943, loss=0.6317357420921326
I0302 01:27:48.003171 140408319375104 logging_writer.py:48] [456200] global_step=456200, grad_norm=4.182393550872803, loss=0.6005803346633911
I0302 01:28:21.669766 140409837700864 logging_writer.py:48] [456300] global_step=456300, grad_norm=4.154229640960693, loss=0.597602367401123
I0302 01:28:55.330578 140408319375104 logging_writer.py:48] [456400] global_step=456400, grad_norm=4.206451892852783, loss=0.5573634505271912
I0302 01:29:29.081329 140409837700864 logging_writer.py:48] [456500] global_step=456500, grad_norm=4.234293460845947, loss=0.6448125839233398
I0302 01:30:02.769871 140408319375104 logging_writer.py:48] [456600] global_step=456600, grad_norm=4.273260593414307, loss=0.6237894296646118
I0302 01:30:36.416075 140409837700864 logging_writer.py:48] [456700] global_step=456700, grad_norm=4.604228973388672, loss=0.645707905292511
I0302 01:31:10.086062 140408319375104 logging_writer.py:48] [456800] global_step=456800, grad_norm=4.115164756774902, loss=0.585113525390625
I0302 01:31:43.816141 140409837700864 logging_writer.py:48] [456900] global_step=456900, grad_norm=4.289604187011719, loss=0.673173725605011
I0302 01:32:17.488917 140408319375104 logging_writer.py:48] [457000] global_step=457000, grad_norm=4.770336627960205, loss=0.6083894371986389
I0302 01:32:51.237905 140409837700864 logging_writer.py:48] [457100] global_step=457100, grad_norm=4.384143352508545, loss=0.5913774967193604
I0302 01:32:56.781877 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:33:03.017611 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:33:11.614582 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:33:13.935781 140573303715648 submission_runner.py:411] Time since start: 159517.93s, 	Step: 457118, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14632202684879303, 'validation/accuracy': 0.7562999725341797, 'validation/loss': 1.0514073371887207, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8175268173217773, 'test/num_examples': 10000, 'score': 154079.6365056038, 'total_duration': 159517.92832422256, 'accumulated_submission_time': 154079.6365056038, 'accumulated_eval_time': 5398.56386756897, 'accumulated_logging_time': 21.66671848297119}
I0302 01:33:14.027908 140407379851008 logging_writer.py:48] [457118] accumulated_eval_time=5398.563868, accumulated_logging_time=21.666718, accumulated_submission_time=154079.636506, global_step=457118, preemption_count=0, score=154079.636506, test/accuracy=0.631800, test/loss=1.817527, test/num_examples=10000, total_duration=159517.928324, train/accuracy=0.961416, train/loss=0.146322, validation/accuracy=0.756300, validation/loss=1.051407, validation/num_examples=50000
I0302 01:33:42.008698 140408319375104 logging_writer.py:48] [457200] global_step=457200, grad_norm=4.533161640167236, loss=0.6283470988273621
I0302 01:34:15.728712 140407379851008 logging_writer.py:48] [457300] global_step=457300, grad_norm=4.4820237159729, loss=0.6201874613761902
I0302 01:34:49.407716 140408319375104 logging_writer.py:48] [457400] global_step=457400, grad_norm=4.378742694854736, loss=0.5874953269958496
I0302 01:35:23.237460 140407379851008 logging_writer.py:48] [457500] global_step=457500, grad_norm=4.540603160858154, loss=0.6273548007011414
I0302 01:35:56.913118 140408319375104 logging_writer.py:48] [457600] global_step=457600, grad_norm=4.543532371520996, loss=0.6578031778335571
I0302 01:36:30.588222 140407379851008 logging_writer.py:48] [457700] global_step=457700, grad_norm=4.165053844451904, loss=0.571863055229187
I0302 01:37:04.308532 140408319375104 logging_writer.py:48] [457800] global_step=457800, grad_norm=4.775295734405518, loss=0.6652376055717468
I0302 01:37:38.025298 140407379851008 logging_writer.py:48] [457900] global_step=457900, grad_norm=4.14752197265625, loss=0.5690885186195374
I0302 01:38:11.738389 140408319375104 logging_writer.py:48] [458000] global_step=458000, grad_norm=4.676809310913086, loss=0.6499919295310974
I0302 01:38:45.421741 140407379851008 logging_writer.py:48] [458100] global_step=458100, grad_norm=4.449573993682861, loss=0.6269499659538269
I0302 01:39:19.145860 140408319375104 logging_writer.py:48] [458200] global_step=458200, grad_norm=4.122130393981934, loss=0.5733053088188171
I0302 01:39:52.815969 140407379851008 logging_writer.py:48] [458300] global_step=458300, grad_norm=4.842579364776611, loss=0.6660286784172058
I0302 01:40:26.536263 140408319375104 logging_writer.py:48] [458400] global_step=458400, grad_norm=4.271919250488281, loss=0.5719270706176758
I0302 01:41:00.245531 140407379851008 logging_writer.py:48] [458500] global_step=458500, grad_norm=4.639472007751465, loss=0.6701716184616089
I0302 01:41:34.015744 140408319375104 logging_writer.py:48] [458600] global_step=458600, grad_norm=4.387600421905518, loss=0.6138161420822144
I0302 01:41:44.253016 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:41:50.354711 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:41:58.994767 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:42:01.336566 140573303715648 submission_runner.py:411] Time since start: 160045.33s, 	Step: 458632, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14432230591773987, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0519403219223022, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8187532424926758, 'test/num_examples': 10000, 'score': 154589.79145264626, 'total_duration': 160045.3290655613, 'accumulated_submission_time': 154589.79145264626, 'accumulated_eval_time': 5415.647324562073, 'accumulated_logging_time': 21.76831364631653}
I0302 01:42:01.453375 140409846093568 logging_writer.py:48] [458632] accumulated_eval_time=5415.647325, accumulated_logging_time=21.768314, accumulated_submission_time=154589.791453, global_step=458632, preemption_count=0, score=154589.791453, test/accuracy=0.630800, test/loss=1.818753, test/num_examples=10000, total_duration=160045.329066, train/accuracy=0.961496, train/loss=0.144322, validation/accuracy=0.756200, validation/loss=1.051940, validation/num_examples=50000
I0302 01:42:24.718264 140409854486272 logging_writer.py:48] [458700] global_step=458700, grad_norm=4.705533981323242, loss=0.6874005198478699
I0302 01:42:58.404909 140409846093568 logging_writer.py:48] [458800] global_step=458800, grad_norm=4.822549819946289, loss=0.7186388969421387
I0302 01:43:32.063520 140409854486272 logging_writer.py:48] [458900] global_step=458900, grad_norm=4.98352575302124, loss=0.6544926166534424
I0302 01:44:05.715615 140409846093568 logging_writer.py:48] [459000] global_step=459000, grad_norm=4.540142059326172, loss=0.6994031071662903
I0302 01:44:39.396918 140409854486272 logging_writer.py:48] [459100] global_step=459100, grad_norm=4.675516128540039, loss=0.6471191644668579
I0302 01:45:13.082838 140409846093568 logging_writer.py:48] [459200] global_step=459200, grad_norm=4.08665657043457, loss=0.5658810138702393
I0302 01:45:46.738070 140409854486272 logging_writer.py:48] [459300] global_step=459300, grad_norm=4.564357280731201, loss=0.6556909084320068
I0302 01:46:20.483806 140409846093568 logging_writer.py:48] [459400] global_step=459400, grad_norm=4.516382694244385, loss=0.66805499792099
I0302 01:46:54.192103 140409854486272 logging_writer.py:48] [459500] global_step=459500, grad_norm=4.5090436935424805, loss=0.5987140536308289
I0302 01:47:27.935240 140409846093568 logging_writer.py:48] [459600] global_step=459600, grad_norm=4.241409778594971, loss=0.628308892250061
I0302 01:48:01.718645 140409854486272 logging_writer.py:48] [459700] global_step=459700, grad_norm=4.7560224533081055, loss=0.6094018220901489
I0302 01:48:35.399574 140409846093568 logging_writer.py:48] [459800] global_step=459800, grad_norm=4.847243309020996, loss=0.5771663188934326
I0302 01:49:09.038914 140409854486272 logging_writer.py:48] [459900] global_step=459900, grad_norm=4.641816139221191, loss=0.6329033374786377
I0302 01:49:42.696308 140409846093568 logging_writer.py:48] [460000] global_step=460000, grad_norm=4.433576583862305, loss=0.6589610576629639
I0302 01:50:16.363428 140409854486272 logging_writer.py:48] [460100] global_step=460100, grad_norm=4.278243064880371, loss=0.6490253806114197
I0302 01:50:31.658456 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:50:37.936039 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:50:46.495215 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:50:48.807407 140573303715648 submission_runner.py:411] Time since start: 160572.80s, 	Step: 460147, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14610415697097778, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.051734209060669, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8186067342758179, 'test/num_examples': 10000, 'score': 155099.9243299961, 'total_duration': 160572.79992508888, 'accumulated_submission_time': 155099.9243299961, 'accumulated_eval_time': 5432.796221256256, 'accumulated_logging_time': 21.89662194252014}
I0302 01:50:48.903695 140408319375104 logging_writer.py:48] [460147] accumulated_eval_time=5432.796221, accumulated_logging_time=21.896622, accumulated_submission_time=155099.924330, global_step=460147, preemption_count=0, score=155099.924330, test/accuracy=0.631000, test/loss=1.818607, test/num_examples=10000, total_duration=160572.799925, train/accuracy=0.960678, train/loss=0.146104, validation/accuracy=0.756180, validation/loss=1.051734, validation/num_examples=50000
I0302 01:51:07.079547 140409124681472 logging_writer.py:48] [460200] global_step=460200, grad_norm=4.746844291687012, loss=0.6498228311538696
I0302 01:51:40.726439 140408319375104 logging_writer.py:48] [460300] global_step=460300, grad_norm=4.138333320617676, loss=0.558802604675293
I0302 01:52:14.481336 140409124681472 logging_writer.py:48] [460400] global_step=460400, grad_norm=4.108269214630127, loss=0.53545743227005
I0302 01:52:48.217833 140408319375104 logging_writer.py:48] [460500] global_step=460500, grad_norm=4.764919757843018, loss=0.6867757439613342
I0302 01:53:21.966107 140409124681472 logging_writer.py:48] [460600] global_step=460600, grad_norm=4.447399139404297, loss=0.6697534322738647
I0302 01:53:55.766559 140408319375104 logging_writer.py:48] [460700] global_step=460700, grad_norm=4.5460662841796875, loss=0.6388524770736694
I0302 01:54:29.459682 140409124681472 logging_writer.py:48] [460800] global_step=460800, grad_norm=4.669891357421875, loss=0.6217683553695679
I0302 01:55:03.151817 140408319375104 logging_writer.py:48] [460900] global_step=460900, grad_norm=4.448185920715332, loss=0.6505864858627319
I0302 01:55:36.897029 140409124681472 logging_writer.py:48] [461000] global_step=461000, grad_norm=4.687106132507324, loss=0.6751890182495117
I0302 01:56:10.628868 140408319375104 logging_writer.py:48] [461100] global_step=461100, grad_norm=4.485295295715332, loss=0.636234700679779
I0302 01:56:44.350978 140409124681472 logging_writer.py:48] [461200] global_step=461200, grad_norm=4.529227256774902, loss=0.6416647434234619
I0302 01:57:18.040152 140408319375104 logging_writer.py:48] [461300] global_step=461300, grad_norm=4.736011981964111, loss=0.6484476923942566
I0302 01:57:51.791688 140409124681472 logging_writer.py:48] [461400] global_step=461400, grad_norm=4.595649719238281, loss=0.6899190545082092
I0302 01:58:25.527349 140408319375104 logging_writer.py:48] [461500] global_step=461500, grad_norm=5.116833209991455, loss=0.710754930973053
I0302 01:58:59.222820 140409124681472 logging_writer.py:48] [461600] global_step=461600, grad_norm=4.241921901702881, loss=0.5895980596542358
I0302 01:59:18.935239 140573303715648 spec.py:321] Evaluating on the training split.
I0302 01:59:25.059158 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 01:59:34.017584 140573303715648 spec.py:349] Evaluating on the test split.
I0302 01:59:36.415804 140573303715648 submission_runner.py:411] Time since start: 161100.41s, 	Step: 461660, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14789460599422455, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.0528876781463623, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8191816806793213, 'test/num_examples': 10000, 'score': 155609.88244462013, 'total_duration': 161100.4083454609, 'accumulated_submission_time': 155609.88244462013, 'accumulated_eval_time': 5450.2767379283905, 'accumulated_logging_time': 22.00607419013977}
I0302 01:59:36.513014 140409837700864 logging_writer.py:48] [461660] accumulated_eval_time=5450.276738, accumulated_logging_time=22.006074, accumulated_submission_time=155609.882445, global_step=461660, preemption_count=0, score=155609.882445, test/accuracy=0.631400, test/loss=1.819182, test/num_examples=10000, total_duration=161100.408345, train/accuracy=0.959801, train/loss=0.147895, validation/accuracy=0.755540, validation/loss=1.052888, validation/num_examples=50000
I0302 01:59:50.391283 140409846093568 logging_writer.py:48] [461700] global_step=461700, grad_norm=4.181859493255615, loss=0.533464789390564
I0302 02:00:24.089364 140409837700864 logging_writer.py:48] [461800] global_step=461800, grad_norm=4.147411346435547, loss=0.5884420871734619
I0302 02:00:57.728216 140409846093568 logging_writer.py:48] [461900] global_step=461900, grad_norm=4.560338497161865, loss=0.5570297241210938
I0302 02:01:31.404412 140409837700864 logging_writer.py:48] [462000] global_step=462000, grad_norm=4.312496185302734, loss=0.6001915335655212
I0302 02:02:05.106637 140409846093568 logging_writer.py:48] [462100] global_step=462100, grad_norm=4.333494186401367, loss=0.634531557559967
I0302 02:02:38.796397 140409837700864 logging_writer.py:48] [462200] global_step=462200, grad_norm=4.243779182434082, loss=0.5709623098373413
I0302 02:03:12.430430 140409846093568 logging_writer.py:48] [462300] global_step=462300, grad_norm=4.552526473999023, loss=0.649793803691864
I0302 02:03:46.106979 140409837700864 logging_writer.py:48] [462400] global_step=462400, grad_norm=4.6496171951293945, loss=0.6931761503219604
I0302 02:04:19.849728 140409846093568 logging_writer.py:48] [462500] global_step=462500, grad_norm=4.593929767608643, loss=0.677706241607666
I0302 02:04:53.523514 140409837700864 logging_writer.py:48] [462600] global_step=462600, grad_norm=4.094998359680176, loss=0.6066953539848328
I0302 02:05:27.283849 140409846093568 logging_writer.py:48] [462700] global_step=462700, grad_norm=4.006206512451172, loss=0.5617215037345886
I0302 02:06:01.011085 140409837700864 logging_writer.py:48] [462800] global_step=462800, grad_norm=4.424013614654541, loss=0.5806103348731995
I0302 02:06:34.746768 140409846093568 logging_writer.py:48] [462900] global_step=462900, grad_norm=4.3865461349487305, loss=0.651289165019989
I0302 02:07:08.419417 140409837700864 logging_writer.py:48] [463000] global_step=463000, grad_norm=4.117223739624023, loss=0.5603423118591309
I0302 02:07:42.057420 140409846093568 logging_writer.py:48] [463100] global_step=463100, grad_norm=4.71800422668457, loss=0.6083955764770508
I0302 02:08:06.455559 140573303715648 spec.py:321] Evaluating on the training split.
I0302 02:08:12.520277 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 02:08:21.033120 140573303715648 spec.py:349] Evaluating on the test split.
I0302 02:08:23.339584 140573303715648 submission_runner.py:411] Time since start: 161627.33s, 	Step: 463174, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14644663035869598, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.052324891090393, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.818250298500061, 'test/num_examples': 10000, 'score': 156119.75332951546, 'total_duration': 161627.33212256432, 'accumulated_submission_time': 156119.75332951546, 'accumulated_eval_time': 5467.160728693008, 'accumulated_logging_time': 22.11421036720276}
I0302 02:08:23.433828 140409124681472 logging_writer.py:48] [463174] accumulated_eval_time=5467.160729, accumulated_logging_time=22.114210, accumulated_submission_time=156119.753330, global_step=463174, preemption_count=0, score=156119.753330, test/accuracy=0.630800, test/loss=1.818250, test/num_examples=10000, total_duration=161627.332123, train/accuracy=0.960100, train/loss=0.146447, validation/accuracy=0.755660, validation/loss=1.052325, validation/num_examples=50000
I0302 02:08:32.561137 140409829308160 logging_writer.py:48] [463200] global_step=463200, grad_norm=4.6954169273376465, loss=0.6645385026931763
I0302 02:09:06.273666 140409124681472 logging_writer.py:48] [463300] global_step=463300, grad_norm=4.512946128845215, loss=0.5933209657669067
I0302 02:09:39.991504 140409829308160 logging_writer.py:48] [463400] global_step=463400, grad_norm=4.167542934417725, loss=0.5491659045219421
I0302 02:10:13.723496 140409124681472 logging_writer.py:48] [463500] global_step=463500, grad_norm=4.4693403244018555, loss=0.614251971244812
I0302 02:10:47.440442 140409829308160 logging_writer.py:48] [463600] global_step=463600, grad_norm=4.581692218780518, loss=0.661479651927948
I0302 02:11:21.156725 140409124681472 logging_writer.py:48] [463700] global_step=463700, grad_norm=4.247251510620117, loss=0.5344550609588623
I0302 02:11:54.871212 140409829308160 logging_writer.py:48] [463800] global_step=463800, grad_norm=4.2404046058654785, loss=0.5539799928665161
I0302 02:12:28.779175 140409124681472 logging_writer.py:48] [463900] global_step=463900, grad_norm=5.223947048187256, loss=0.6801284551620483
I0302 02:13:02.472048 140409829308160 logging_writer.py:48] [464000] global_step=464000, grad_norm=4.371212959289551, loss=0.5548712611198425
I0302 02:13:36.217938 140409124681472 logging_writer.py:48] [464100] global_step=464100, grad_norm=4.556860446929932, loss=0.7013491988182068
I0302 02:14:09.938387 140409829308160 logging_writer.py:48] [464200] global_step=464200, grad_norm=4.83466100692749, loss=0.6636515259742737
I0302 02:14:43.646371 140409124681472 logging_writer.py:48] [464300] global_step=464300, grad_norm=4.595499038696289, loss=0.6654227375984192
I0302 02:15:17.378861 140409829308160 logging_writer.py:48] [464400] global_step=464400, grad_norm=4.572371482849121, loss=0.7169407606124878
I0302 02:15:51.070444 140409124681472 logging_writer.py:48] [464500] global_step=464500, grad_norm=4.130573272705078, loss=0.5643417835235596
I0302 02:16:24.804959 140409829308160 logging_writer.py:48] [464600] global_step=464600, grad_norm=4.275094032287598, loss=0.6177616715431213
I0302 02:16:53.621476 140573303715648 spec.py:321] Evaluating on the training split.
I0302 02:16:59.651999 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 02:17:08.194170 140573303715648 spec.py:349] Evaluating on the test split.
I0302 02:17:10.492130 140573303715648 submission_runner.py:411] Time since start: 162154.48s, 	Step: 464687, 	{'train/accuracy': 0.9594228267669678, 'train/loss': 0.15030647814273834, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0526986122131348, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8184653520584106, 'test/num_examples': 10000, 'score': 156629.86915493011, 'total_duration': 162154.48467326164, 'accumulated_submission_time': 156629.86915493011, 'accumulated_eval_time': 5484.03133893013, 'accumulated_logging_time': 22.21840262413025}
I0302 02:17:10.587220 140407379851008 logging_writer.py:48] [464687] accumulated_eval_time=5484.031339, accumulated_logging_time=22.218403, accumulated_submission_time=156629.869155, global_step=464687, preemption_count=0, score=156629.869155, test/accuracy=0.631600, test/loss=1.818465, test/num_examples=10000, total_duration=162154.484673, train/accuracy=0.959423, train/loss=0.150306, validation/accuracy=0.755580, validation/loss=1.052699, validation/num_examples=50000
I0302 02:17:15.296123 140408319375104 logging_writer.py:48] [464700] global_step=464700, grad_norm=4.314864158630371, loss=0.6254331469535828
I0302 02:17:48.902202 140407379851008 logging_writer.py:48] [464800] global_step=464800, grad_norm=4.3070502281188965, loss=0.6758051514625549
I0302 02:18:22.665575 140408319375104 logging_writer.py:48] [464900] global_step=464900, grad_norm=4.437058925628662, loss=0.6262196898460388
I0302 02:18:56.357896 140407379851008 logging_writer.py:48] [465000] global_step=465000, grad_norm=4.339929103851318, loss=0.6468807458877563
I0302 02:19:29.992125 140408319375104 logging_writer.py:48] [465100] global_step=465100, grad_norm=4.3569722175598145, loss=0.6282187700271606
I0302 02:20:03.663000 140407379851008 logging_writer.py:48] [465200] global_step=465200, grad_norm=4.3280110359191895, loss=0.5607099533081055
I0302 02:20:37.332155 140408319375104 logging_writer.py:48] [465300] global_step=465300, grad_norm=4.3054094314575195, loss=0.6136284470558167
I0302 02:21:10.972491 140407379851008 logging_writer.py:48] [465400] global_step=465400, grad_norm=4.443854331970215, loss=0.6278969049453735
I0302 02:21:44.621106 140408319375104 logging_writer.py:48] [465500] global_step=465500, grad_norm=4.713172912597656, loss=0.672057032585144
I0302 02:22:18.263416 140407379851008 logging_writer.py:48] [465600] global_step=465600, grad_norm=4.919757843017578, loss=0.7168753743171692
I0302 02:22:51.924602 140408319375104 logging_writer.py:48] [465700] global_step=465700, grad_norm=4.35821533203125, loss=0.5997288823127747
I0302 02:23:25.668158 140407379851008 logging_writer.py:48] [465800] global_step=465800, grad_norm=4.533949375152588, loss=0.6830930709838867
I0302 02:23:59.315134 140408319375104 logging_writer.py:48] [465900] global_step=465900, grad_norm=4.393385887145996, loss=0.6273209452629089
I0302 02:24:33.084320 140407379851008 logging_writer.py:48] [466000] global_step=466000, grad_norm=4.796074390411377, loss=0.6831802725791931
I0302 02:25:06.746542 140408319375104 logging_writer.py:48] [466100] global_step=466100, grad_norm=4.587848663330078, loss=0.661596417427063
I0302 02:25:40.411343 140407379851008 logging_writer.py:48] [466200] global_step=466200, grad_norm=4.49954080581665, loss=0.5667650699615479
I0302 02:25:40.560908 140573303715648 spec.py:321] Evaluating on the training split.
I0302 02:25:46.690461 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 02:25:55.318537 140573303715648 spec.py:349] Evaluating on the test split.
I0302 02:25:57.613979 140573303715648 submission_runner.py:411] Time since start: 162681.61s, 	Step: 466202, 	{'train/accuracy': 0.9622927308082581, 'train/loss': 0.14320872724056244, 'validation/accuracy': 0.7555199861526489, 'validation/loss': 1.0532876253128052, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8178514242172241, 'test/num_examples': 10000, 'score': 157139.771348238, 'total_duration': 162681.6063232422, 'accumulated_submission_time': 157139.771348238, 'accumulated_eval_time': 5501.084171056747, 'accumulated_logging_time': 22.32434344291687}
I0302 02:25:57.709954 140409124681472 logging_writer.py:48] [466202] accumulated_eval_time=5501.084171, accumulated_logging_time=22.324343, accumulated_submission_time=157139.771348, global_step=466202, preemption_count=0, score=157139.771348, test/accuracy=0.631500, test/loss=1.817851, test/num_examples=10000, total_duration=162681.606323, train/accuracy=0.962293, train/loss=0.143209, validation/accuracy=0.755520, validation/loss=1.053288, validation/num_examples=50000
I0302 02:26:31.080428 140409829308160 logging_writer.py:48] [466300] global_step=466300, grad_norm=5.306000232696533, loss=0.6942369937896729
I0302 02:27:04.810402 140409124681472 logging_writer.py:48] [466400] global_step=466400, grad_norm=4.668536186218262, loss=0.6561277508735657
I0302 02:27:38.510072 140409829308160 logging_writer.py:48] [466500] global_step=466500, grad_norm=4.5484938621521, loss=0.5860064625740051
I0302 02:28:12.243858 140409124681472 logging_writer.py:48] [466600] global_step=466600, grad_norm=4.267990589141846, loss=0.5579439401626587
I0302 02:28:45.947037 140409829308160 logging_writer.py:48] [466700] global_step=466700, grad_norm=4.5620503425598145, loss=0.6104130148887634
I0302 02:29:19.685425 140409124681472 logging_writer.py:48] [466800] global_step=466800, grad_norm=4.414959907531738, loss=0.7214927077293396
I0302 02:29:53.373784 140409829308160 logging_writer.py:48] [466900] global_step=466900, grad_norm=4.629764556884766, loss=0.6373838186264038
I0302 02:30:27.191375 140409124681472 logging_writer.py:48] [467000] global_step=467000, grad_norm=4.1761064529418945, loss=0.5464944243431091
I0302 02:31:00.900679 140409829308160 logging_writer.py:48] [467100] global_step=467100, grad_norm=4.867020606994629, loss=0.6346197128295898
I0302 02:31:34.573310 140409124681472 logging_writer.py:48] [467200] global_step=467200, grad_norm=4.246218204498291, loss=0.6063241958618164
I0302 02:32:08.273642 140409829308160 logging_writer.py:48] [467300] global_step=467300, grad_norm=4.582034111022949, loss=0.6279606819152832
I0302 02:32:41.925502 140409124681472 logging_writer.py:48] [467400] global_step=467400, grad_norm=4.589698314666748, loss=0.7410385608673096
I0302 02:33:15.578131 140409829308160 logging_writer.py:48] [467500] global_step=467500, grad_norm=4.348155975341797, loss=0.6331896781921387
I0302 02:33:49.312978 140409124681472 logging_writer.py:48] [467600] global_step=467600, grad_norm=4.4027533531188965, loss=0.6378239393234253
I0302 02:34:22.976841 140409829308160 logging_writer.py:48] [467700] global_step=467700, grad_norm=4.486198425292969, loss=0.6178168058395386
I0302 02:34:27.825455 140573303715648 spec.py:321] Evaluating on the training split.
I0302 02:34:33.878317 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 02:34:42.470794 140573303715648 spec.py:349] Evaluating on the test split.
I0302 02:34:44.814342 140573303715648 submission_runner.py:411] Time since start: 163208.81s, 	Step: 467716, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14546874165534973, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0522892475128174, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.816857933998108, 'test/num_examples': 10000, 'score': 157649.81596922874, 'total_duration': 163208.80686163902, 'accumulated_submission_time': 157649.81596922874, 'accumulated_eval_time': 5518.072999954224, 'accumulated_logging_time': 22.43128538131714}
I0302 02:34:44.908369 140407379851008 logging_writer.py:48] [467716] accumulated_eval_time=5518.073000, accumulated_logging_time=22.431285, accumulated_submission_time=157649.815969, global_step=467716, preemption_count=0, score=157649.815969, test/accuracy=0.631700, test/loss=1.816858, test/num_examples=10000, total_duration=163208.806862, train/accuracy=0.960957, train/loss=0.145469, validation/accuracy=0.755920, validation/loss=1.052289, validation/num_examples=50000
I0302 02:35:13.563516 140408319375104 logging_writer.py:48] [467800] global_step=467800, grad_norm=4.831365585327148, loss=0.6654123663902283
I0302 02:35:47.221029 140407379851008 logging_writer.py:48] [467900] global_step=467900, grad_norm=4.629206657409668, loss=0.5630680322647095
I0302 02:36:20.861126 140408319375104 logging_writer.py:48] [468000] global_step=468000, grad_norm=4.765774726867676, loss=0.6485099792480469
I0302 02:36:54.633185 140407379851008 logging_writer.py:48] [468100] global_step=468100, grad_norm=4.744932174682617, loss=0.6461567878723145
I0302 02:37:28.317869 140408319375104 logging_writer.py:48] [468200] global_step=468200, grad_norm=4.413794040679932, loss=0.6351797580718994
I0302 02:38:01.985808 140407379851008 logging_writer.py:48] [468300] global_step=468300, grad_norm=4.06439733505249, loss=0.6041314601898193
I0302 02:38:35.682676 140408319375104 logging_writer.py:48] [468400] global_step=468400, grad_norm=4.380521297454834, loss=0.6229626536369324
I0302 02:39:09.424520 140407379851008 logging_writer.py:48] [468500] global_step=468500, grad_norm=4.66702938079834, loss=0.6303309798240662
I0302 02:39:43.110384 140408319375104 logging_writer.py:48] [468600] global_step=468600, grad_norm=4.50546407699585, loss=0.5952327251434326
I0302 02:40:16.863909 140407379851008 logging_writer.py:48] [468700] global_step=468700, grad_norm=4.1786651611328125, loss=0.5926188826560974
I0302 02:40:50.543204 140408319375104 logging_writer.py:48] [468800] global_step=468800, grad_norm=4.661386966705322, loss=0.6647817492485046
I0302 02:41:24.301891 140407379851008 logging_writer.py:48] [468900] global_step=468900, grad_norm=4.263524532318115, loss=0.6332548260688782
I0302 02:41:57.974162 140408319375104 logging_writer.py:48] [469000] global_step=469000, grad_norm=4.351039886474609, loss=0.627408504486084
I0302 02:42:31.634093 140407379851008 logging_writer.py:48] [469100] global_step=469100, grad_norm=4.59692907333374, loss=0.7178441882133484
I0302 02:43:05.476624 140408319375104 logging_writer.py:48] [469200] global_step=469200, grad_norm=4.754067897796631, loss=0.6453773379325867
I0302 02:43:15.071952 140573303715648 spec.py:321] Evaluating on the training split.
I0302 02:43:21.160871 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 02:43:29.663547 140573303715648 spec.py:349] Evaluating on the test split.
I0302 02:43:32.044638 140573303715648 submission_runner.py:411] Time since start: 163736.04s, 	Step: 469230, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14637921750545502, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0511075258255005, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8159784078598022, 'test/num_examples': 10000, 'score': 158159.90993785858, 'total_duration': 163736.03716516495, 'accumulated_submission_time': 158159.90993785858, 'accumulated_eval_time': 5535.045620441437, 'accumulated_logging_time': 22.535234928131104}
I0302 02:43:32.146142 140409837700864 logging_writer.py:48] [469230] accumulated_eval_time=5535.045620, accumulated_logging_time=22.535235, accumulated_submission_time=158159.909938, global_step=469230, preemption_count=0, score=158159.909938, test/accuracy=0.631300, test/loss=1.815978, test/num_examples=10000, total_duration=163736.037165, train/accuracy=0.960858, train/loss=0.146379, validation/accuracy=0.755720, validation/loss=1.051108, validation/num_examples=50000
I0302 02:43:56.013388 140409854486272 logging_writer.py:48] [469300] global_step=469300, grad_norm=4.077666282653809, loss=0.5836491584777832
I0302 02:44:29.641533 140409837700864 logging_writer.py:48] [469400] global_step=469400, grad_norm=4.725078582763672, loss=0.6847246885299683
I0302 02:45:03.354110 140409854486272 logging_writer.py:48] [469500] global_step=469500, grad_norm=5.123489856719971, loss=0.665218710899353
I0302 02:45:37.017941 140409837700864 logging_writer.py:48] [469600] global_step=469600, grad_norm=4.5906982421875, loss=0.6376524567604065
I0302 02:46:10.709397 140409854486272 logging_writer.py:48] [469700] global_step=469700, grad_norm=4.858361721038818, loss=0.5937125086784363
I0302 02:46:44.417210 140409837700864 logging_writer.py:48] [469800] global_step=469800, grad_norm=4.452565670013428, loss=0.6790619492530823
I0302 02:47:18.091798 140409854486272 logging_writer.py:48] [469900] global_step=469900, grad_norm=4.27983283996582, loss=0.5421029329299927
I0302 02:47:51.833590 140409837700864 logging_writer.py:48] [470000] global_step=470000, grad_norm=4.42199182510376, loss=0.6260104179382324
I0302 02:48:25.560571 140409854486272 logging_writer.py:48] [470100] global_step=470100, grad_norm=4.284514904022217, loss=0.6175888776779175
I0302 02:48:59.399221 140409837700864 logging_writer.py:48] [470200] global_step=470200, grad_norm=4.282007694244385, loss=0.5652060508728027
I0302 02:49:33.122094 140409854486272 logging_writer.py:48] [470300] global_step=470300, grad_norm=4.822522163391113, loss=0.5940415859222412
I0302 02:50:06.843819 140409837700864 logging_writer.py:48] [470400] global_step=470400, grad_norm=4.762670516967773, loss=0.625110387802124
I0302 02:50:40.556931 140409854486272 logging_writer.py:48] [470500] global_step=470500, grad_norm=4.39691686630249, loss=0.6402465105056763
I0302 02:51:14.303775 140409837700864 logging_writer.py:48] [470600] global_step=470600, grad_norm=5.163388729095459, loss=0.7193533182144165
I0302 02:51:48.038535 140409854486272 logging_writer.py:48] [470700] global_step=470700, grad_norm=4.425193786621094, loss=0.7270343899726868
I0302 02:52:02.343855 140573303715648 spec.py:321] Evaluating on the training split.
I0302 02:52:08.612107 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 02:52:17.230668 140573303715648 spec.py:349] Evaluating on the test split.
I0302 02:52:19.504685 140573303715648 submission_runner.py:411] Time since start: 164263.50s, 	Step: 470744, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.14714637398719788, 'validation/accuracy': 0.7562199831008911, 'validation/loss': 1.051742672920227, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8177167177200317, 'test/num_examples': 10000, 'score': 158670.03724741936, 'total_duration': 164263.49721503258, 'accumulated_submission_time': 158670.03724741936, 'accumulated_eval_time': 5552.206401586533, 'accumulated_logging_time': 22.64659857749939}
I0302 02:52:19.595316 140408319375104 logging_writer.py:48] [470744] accumulated_eval_time=5552.206402, accumulated_logging_time=22.646599, accumulated_submission_time=158670.037247, global_step=470744, preemption_count=0, score=158670.037247, test/accuracy=0.631100, test/loss=1.817717, test/num_examples=10000, total_duration=164263.497215, train/accuracy=0.960260, train/loss=0.147146, validation/accuracy=0.756220, validation/loss=1.051743, validation/num_examples=50000
I0302 02:52:38.781036 140409124681472 logging_writer.py:48] [470800] global_step=470800, grad_norm=5.006216526031494, loss=0.6504672765731812
I0302 02:53:12.449738 140408319375104 logging_writer.py:48] [470900] global_step=470900, grad_norm=4.928892612457275, loss=0.6761924624443054
I0302 02:53:46.167906 140409124681472 logging_writer.py:48] [471000] global_step=471000, grad_norm=4.666561126708984, loss=0.6461752653121948
I0302 02:54:19.813157 140408319375104 logging_writer.py:48] [471100] global_step=471100, grad_norm=4.311086177825928, loss=0.7000111937522888
I0302 02:54:53.522381 140409124681472 logging_writer.py:48] [471200] global_step=471200, grad_norm=4.210422992706299, loss=0.5960203409194946
I0302 02:55:27.355774 140408319375104 logging_writer.py:48] [471300] global_step=471300, grad_norm=4.223664283752441, loss=0.6049385666847229
I0302 02:56:01.071954 140409124681472 logging_writer.py:48] [471400] global_step=471400, grad_norm=4.3492889404296875, loss=0.6161803603172302
I0302 02:56:34.732431 140408319375104 logging_writer.py:48] [471500] global_step=471500, grad_norm=4.083086013793945, loss=0.5869520306587219
I0302 02:57:08.491263 140409124681472 logging_writer.py:48] [471600] global_step=471600, grad_norm=4.694850444793701, loss=0.6720184087753296
I0302 02:57:42.180445 140408319375104 logging_writer.py:48] [471700] global_step=471700, grad_norm=4.373356819152832, loss=0.5917689800262451
I0302 02:58:15.931772 140409124681472 logging_writer.py:48] [471800] global_step=471800, grad_norm=4.295175552368164, loss=0.5760859847068787
I0302 02:58:49.582638 140408319375104 logging_writer.py:48] [471900] global_step=471900, grad_norm=4.517620086669922, loss=0.6819586753845215
I0302 02:59:23.243495 140409124681472 logging_writer.py:48] [472000] global_step=472000, grad_norm=4.560212135314941, loss=0.5908324718475342
I0302 02:59:56.939058 140408319375104 logging_writer.py:48] [472100] global_step=472100, grad_norm=4.701503753662109, loss=0.6915472149848938
I0302 03:00:30.629127 140409124681472 logging_writer.py:48] [472200] global_step=472200, grad_norm=4.298614025115967, loss=0.6052418351173401
I0302 03:00:49.625744 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:00:55.676991 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:01:04.370665 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:01:06.616877 140573303715648 submission_runner.py:411] Time since start: 164790.61s, 	Step: 472258, 	{'train/accuracy': 0.9595224857330322, 'train/loss': 0.1507411003112793, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0522160530090332, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8183821439743042, 'test/num_examples': 10000, 'score': 159179.998026371, 'total_duration': 164790.60931801796, 'accumulated_submission_time': 159179.998026371, 'accumulated_eval_time': 5569.197404384613, 'accumulated_logging_time': 22.748106718063354}
I0302 03:01:06.712977 140409846093568 logging_writer.py:48] [472258] accumulated_eval_time=5569.197404, accumulated_logging_time=22.748107, accumulated_submission_time=159179.998026, global_step=472258, preemption_count=0, score=159179.998026, test/accuracy=0.632400, test/loss=1.818382, test/num_examples=10000, total_duration=164790.609318, train/accuracy=0.959522, train/loss=0.150741, validation/accuracy=0.755720, validation/loss=1.052216, validation/num_examples=50000
I0302 03:01:21.279034 140409854486272 logging_writer.py:48] [472300] global_step=472300, grad_norm=5.221088409423828, loss=0.6490095257759094
I0302 03:01:54.921376 140409846093568 logging_writer.py:48] [472400] global_step=472400, grad_norm=4.758487224578857, loss=0.6965509653091431
I0302 03:02:28.611605 140409854486272 logging_writer.py:48] [472500] global_step=472500, grad_norm=4.728736400604248, loss=0.6808112263679504
I0302 03:03:02.287925 140409846093568 logging_writer.py:48] [472600] global_step=472600, grad_norm=4.823655128479004, loss=0.6180779933929443
I0302 03:03:36.018937 140409854486272 logging_writer.py:48] [472700] global_step=472700, grad_norm=4.230917453765869, loss=0.5829436182975769
I0302 03:04:09.688754 140409846093568 logging_writer.py:48] [472800] global_step=472800, grad_norm=4.367046356201172, loss=0.6356433033943176
I0302 03:04:43.424197 140409854486272 logging_writer.py:48] [472900] global_step=472900, grad_norm=4.818750381469727, loss=0.7042734026908875
I0302 03:05:17.116629 140409846093568 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.58638334274292, loss=0.6151052713394165
I0302 03:05:50.845813 140409854486272 logging_writer.py:48] [473100] global_step=473100, grad_norm=4.480713367462158, loss=0.5581898093223572
I0302 03:06:24.525161 140409846093568 logging_writer.py:48] [473200] global_step=473200, grad_norm=4.541871547698975, loss=0.5791153311729431
I0302 03:06:58.278959 140409854486272 logging_writer.py:48] [473300] global_step=473300, grad_norm=4.186320781707764, loss=0.5929471254348755
I0302 03:07:32.023090 140409846093568 logging_writer.py:48] [473400] global_step=473400, grad_norm=4.698960304260254, loss=0.615886390209198
I0302 03:08:05.706110 140409854486272 logging_writer.py:48] [473500] global_step=473500, grad_norm=4.5316338539123535, loss=0.6498815417289734
I0302 03:08:39.363121 140409846093568 logging_writer.py:48] [473600] global_step=473600, grad_norm=5.267882347106934, loss=0.5961142182350159
I0302 03:09:13.102716 140409854486272 logging_writer.py:48] [473700] global_step=473700, grad_norm=4.523433685302734, loss=0.6142261028289795
I0302 03:09:36.818405 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:09:42.898265 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:09:51.538676 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:09:53.840748 140573303715648 submission_runner.py:411] Time since start: 165317.83s, 	Step: 473772, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.1431065946817398, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0513827800750732, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8170549869537354, 'test/num_examples': 10000, 'score': 159690.03354144096, 'total_duration': 165317.8332746029, 'accumulated_submission_time': 159690.03354144096, 'accumulated_eval_time': 5586.219715356827, 'accumulated_logging_time': 22.854050636291504}
I0302 03:09:53.931822 140408319375104 logging_writer.py:48] [473772] accumulated_eval_time=5586.219715, accumulated_logging_time=22.854051, accumulated_submission_time=159690.033541, global_step=473772, preemption_count=0, score=159690.033541, test/accuracy=0.631100, test/loss=1.817055, test/num_examples=10000, total_duration=165317.833275, train/accuracy=0.961854, train/loss=0.143107, validation/accuracy=0.756020, validation/loss=1.051383, validation/num_examples=50000
I0302 03:10:03.683589 140409124681472 logging_writer.py:48] [473800] global_step=473800, grad_norm=4.470821857452393, loss=0.6139150857925415
I0302 03:10:37.323398 140408319375104 logging_writer.py:48] [473900] global_step=473900, grad_norm=4.411835193634033, loss=0.6516073346138
I0302 03:11:10.964535 140409124681472 logging_writer.py:48] [474000] global_step=474000, grad_norm=4.47580623626709, loss=0.6525205373764038
I0302 03:11:44.752539 140408319375104 logging_writer.py:48] [474100] global_step=474100, grad_norm=4.639913082122803, loss=0.5682832598686218
I0302 03:12:18.425766 140409124681472 logging_writer.py:48] [474200] global_step=474200, grad_norm=5.269049167633057, loss=0.6000123023986816
I0302 03:12:52.197586 140408319375104 logging_writer.py:48] [474300] global_step=474300, grad_norm=4.434475898742676, loss=0.6128323674201965
I0302 03:13:25.843186 140409124681472 logging_writer.py:48] [474400] global_step=474400, grad_norm=4.429776191711426, loss=0.5969903469085693
I0302 03:13:59.594624 140408319375104 logging_writer.py:48] [474500] global_step=474500, grad_norm=4.354176998138428, loss=0.5930615663528442
I0302 03:14:33.311365 140409124681472 logging_writer.py:48] [474600] global_step=474600, grad_norm=4.182295322418213, loss=0.5802150964736938
I0302 03:15:07.018001 140408319375104 logging_writer.py:48] [474700] global_step=474700, grad_norm=4.914246559143066, loss=0.7037332057952881
I0302 03:15:40.708471 140409124681472 logging_writer.py:48] [474800] global_step=474800, grad_norm=4.612372875213623, loss=0.6817131638526917
I0302 03:16:14.394701 140408319375104 logging_writer.py:48] [474900] global_step=474900, grad_norm=4.112433910369873, loss=0.5700149536132812
I0302 03:16:48.026626 140409124681472 logging_writer.py:48] [475000] global_step=475000, grad_norm=4.458740711212158, loss=0.6333603858947754
I0302 03:17:21.694092 140408319375104 logging_writer.py:48] [475100] global_step=475100, grad_norm=4.469658851623535, loss=0.6438235640525818
I0302 03:17:55.411897 140409124681472 logging_writer.py:48] [475200] global_step=475200, grad_norm=4.775360107421875, loss=0.619152843952179
I0302 03:18:23.872288 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:18:29.923203 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:18:38.542733 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:18:40.805269 140573303715648 submission_runner.py:411] Time since start: 165844.80s, 	Step: 475286, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.14568601548671722, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.051494836807251, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8169790506362915, 'test/num_examples': 10000, 'score': 160199.90467762947, 'total_duration': 165844.797778368, 'accumulated_submission_time': 160199.90467762947, 'accumulated_eval_time': 5603.152618169785, 'accumulated_logging_time': 22.95527720451355}
I0302 03:18:40.897052 140407371458304 logging_writer.py:48] [475286] accumulated_eval_time=5603.152618, accumulated_logging_time=22.955277, accumulated_submission_time=160199.904678, global_step=475286, preemption_count=0, score=160199.904678, test/accuracy=0.631300, test/loss=1.816979, test/num_examples=10000, total_duration=165844.797778, train/accuracy=0.960898, train/loss=0.145686, validation/accuracy=0.756160, validation/loss=1.051495, validation/num_examples=50000
I0302 03:18:45.944997 140407379851008 logging_writer.py:48] [475300] global_step=475300, grad_norm=4.693511009216309, loss=0.5908392071723938
I0302 03:19:19.677472 140407371458304 logging_writer.py:48] [475400] global_step=475400, grad_norm=4.79651403427124, loss=0.6717345118522644
I0302 03:19:53.475373 140407379851008 logging_writer.py:48] [475500] global_step=475500, grad_norm=4.586146831512451, loss=0.5898752808570862
I0302 03:20:27.161772 140407371458304 logging_writer.py:48] [475600] global_step=475600, grad_norm=4.773188591003418, loss=0.6305887699127197
I0302 03:21:00.895855 140407379851008 logging_writer.py:48] [475700] global_step=475700, grad_norm=4.259371280670166, loss=0.5874126553535461
I0302 03:21:34.554318 140407371458304 logging_writer.py:48] [475800] global_step=475800, grad_norm=4.330351829528809, loss=0.60007643699646
I0302 03:22:08.229312 140407379851008 logging_writer.py:48] [475900] global_step=475900, grad_norm=4.305759429931641, loss=0.612379252910614
I0302 03:22:41.938220 140407371458304 logging_writer.py:48] [476000] global_step=476000, grad_norm=4.4992499351501465, loss=0.6157341599464417
I0302 03:23:15.630962 140407379851008 logging_writer.py:48] [476100] global_step=476100, grad_norm=4.677428245544434, loss=0.6141375303268433
I0302 03:23:49.364086 140407371458304 logging_writer.py:48] [476200] global_step=476200, grad_norm=4.490505218505859, loss=0.6170592308044434
I0302 03:24:23.046310 140407379851008 logging_writer.py:48] [476300] global_step=476300, grad_norm=4.431398391723633, loss=0.5481816530227661
I0302 03:24:56.796524 140407371458304 logging_writer.py:48] [476400] global_step=476400, grad_norm=4.302511215209961, loss=0.6553136706352234
I0302 03:25:30.460139 140407379851008 logging_writer.py:48] [476500] global_step=476500, grad_norm=5.0246357917785645, loss=0.6744310259819031
I0302 03:26:04.226828 140407371458304 logging_writer.py:48] [476600] global_step=476600, grad_norm=4.424008846282959, loss=0.5835403203964233
I0302 03:26:37.932774 140407379851008 logging_writer.py:48] [476700] global_step=476700, grad_norm=4.5888261795043945, loss=0.6221323013305664
I0302 03:27:11.144405 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:27:17.309687 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:27:25.925368 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:27:28.200036 140573303715648 submission_runner.py:411] Time since start: 166372.19s, 	Step: 476800, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.1460122913122177, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0520609617233276, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8172602653503418, 'test/num_examples': 10000, 'score': 160710.0825135708, 'total_duration': 166372.19257593155, 'accumulated_submission_time': 160710.0825135708, 'accumulated_eval_time': 5620.2082052230835, 'accumulated_logging_time': 23.058412313461304}
I0302 03:27:28.294553 140409124681472 logging_writer.py:48] [476800] accumulated_eval_time=5620.208205, accumulated_logging_time=23.058412, accumulated_submission_time=160710.082514, global_step=476800, preemption_count=0, score=160710.082514, test/accuracy=0.630700, test/loss=1.817260, test/num_examples=10000, total_duration=166372.192576, train/accuracy=0.960858, train/loss=0.146012, validation/accuracy=0.755580, validation/loss=1.052061, validation/num_examples=50000
I0302 03:27:28.661605 140409829308160 logging_writer.py:48] [476800] global_step=476800, grad_norm=4.399396896362305, loss=0.5688158273696899
I0302 03:28:02.279229 140409124681472 logging_writer.py:48] [476900] global_step=476900, grad_norm=4.407439708709717, loss=0.5790672302246094
I0302 03:28:35.969204 140409829308160 logging_writer.py:48] [477000] global_step=477000, grad_norm=4.226021766662598, loss=0.6312249302864075
I0302 03:29:09.628694 140409124681472 logging_writer.py:48] [477100] global_step=477100, grad_norm=4.808840274810791, loss=0.6530324816703796
I0302 03:29:43.368161 140409829308160 logging_writer.py:48] [477200] global_step=477200, grad_norm=4.615044116973877, loss=0.6218172311782837
I0302 03:30:17.010985 140409124681472 logging_writer.py:48] [477300] global_step=477300, grad_norm=4.846244812011719, loss=0.6940262317657471
I0302 03:30:50.690424 140409829308160 logging_writer.py:48] [477400] global_step=477400, grad_norm=4.230870246887207, loss=0.55076664686203
I0302 03:31:24.376767 140409124681472 logging_writer.py:48] [477500] global_step=477500, grad_norm=4.308644771575928, loss=0.5803358554840088
I0302 03:31:58.192493 140409829308160 logging_writer.py:48] [477600] global_step=477600, grad_norm=4.631322860717773, loss=0.5888500809669495
I0302 03:32:31.885079 140409124681472 logging_writer.py:48] [477700] global_step=477700, grad_norm=4.303622722625732, loss=0.5647806525230408
I0302 03:33:05.574565 140409829308160 logging_writer.py:48] [477800] global_step=477800, grad_norm=4.6895575523376465, loss=0.7015382051467896
I0302 03:33:39.212485 140409124681472 logging_writer.py:48] [477900] global_step=477900, grad_norm=4.260207176208496, loss=0.6175537705421448
I0302 03:34:12.925064 140409829308160 logging_writer.py:48] [478000] global_step=478000, grad_norm=4.643256664276123, loss=0.6508823037147522
I0302 03:34:46.608721 140409124681472 logging_writer.py:48] [478100] global_step=478100, grad_norm=4.163599014282227, loss=0.6405584812164307
I0302 03:35:20.266184 140409829308160 logging_writer.py:48] [478200] global_step=478200, grad_norm=4.479396343231201, loss=0.6071214079856873
I0302 03:35:54.220929 140409124681472 logging_writer.py:48] [478300] global_step=478300, grad_norm=4.312437534332275, loss=0.6656047701835632
I0302 03:35:58.403557 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:36:04.739559 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:36:13.236107 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:36:15.539967 140573303715648 submission_runner.py:411] Time since start: 166899.53s, 	Step: 478314, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14928925037384033, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0533448457717896, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8197903633117676, 'test/num_examples': 10000, 'score': 161220.12222194672, 'total_duration': 166899.53250861168, 'accumulated_submission_time': 161220.12222194672, 'accumulated_eval_time': 5637.344567537308, 'accumulated_logging_time': 23.16335129737854}
I0302 03:36:15.636587 140407371458304 logging_writer.py:48] [478314] accumulated_eval_time=5637.344568, accumulated_logging_time=23.163351, accumulated_submission_time=161220.122222, global_step=478314, preemption_count=0, score=161220.122222, test/accuracy=0.631000, test/loss=1.819790, test/num_examples=10000, total_duration=166899.532509, train/accuracy=0.960041, train/loss=0.149289, validation/accuracy=0.755400, validation/loss=1.053345, validation/num_examples=50000
I0302 03:36:44.975844 140407379851008 logging_writer.py:48] [478400] global_step=478400, grad_norm=4.60669469833374, loss=0.6205364465713501
I0302 03:37:18.663513 140407371458304 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.34227991104126, loss=0.696718692779541
I0302 03:37:52.311619 140407379851008 logging_writer.py:48] [478600] global_step=478600, grad_norm=5.120398998260498, loss=0.6072108149528503
I0302 03:38:26.091019 140407371458304 logging_writer.py:48] [478700] global_step=478700, grad_norm=4.450302600860596, loss=0.622790515422821
I0302 03:38:59.762323 140407379851008 logging_writer.py:48] [478800] global_step=478800, grad_norm=4.551711559295654, loss=0.661808431148529
I0302 03:39:33.526892 140407371458304 logging_writer.py:48] [478900] global_step=478900, grad_norm=4.36475133895874, loss=0.6090312600135803
I0302 03:40:07.207485 140407379851008 logging_writer.py:48] [479000] global_step=479000, grad_norm=4.770602703094482, loss=0.6022898554801941
I0302 03:40:40.958109 140407371458304 logging_writer.py:48] [479100] global_step=479100, grad_norm=3.990549325942993, loss=0.5650460720062256
I0302 03:41:14.625100 140407379851008 logging_writer.py:48] [479200] global_step=479200, grad_norm=4.68367338180542, loss=0.6670289635658264
I0302 03:41:48.359803 140407371458304 logging_writer.py:48] [479300] global_step=479300, grad_norm=4.090514183044434, loss=0.6292098760604858
I0302 03:42:22.102229 140407379851008 logging_writer.py:48] [479400] global_step=479400, grad_norm=4.578747272491455, loss=0.7157300710678101
I0302 03:42:55.790631 140407371458304 logging_writer.py:48] [479500] global_step=479500, grad_norm=4.446251392364502, loss=0.5702610611915588
I0302 03:43:29.510250 140407379851008 logging_writer.py:48] [479600] global_step=479600, grad_norm=4.598074436187744, loss=0.6736486554145813
I0302 03:44:03.201908 140407371458304 logging_writer.py:48] [479700] global_step=479700, grad_norm=4.331854820251465, loss=0.5278775691986084
I0302 03:44:36.988290 140407379851008 logging_writer.py:48] [479800] global_step=479800, grad_norm=5.284753322601318, loss=0.6243651509284973
I0302 03:44:45.552747 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:44:51.689834 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:45:00.325113 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:45:02.655382 140573303715648 submission_runner.py:411] Time since start: 167426.65s, 	Step: 479827, 	{'train/accuracy': 0.9615553021430969, 'train/loss': 0.14341308176517487, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0522792339324951, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8189347982406616, 'test/num_examples': 10000, 'score': 161729.96871495247, 'total_duration': 167426.6479022503, 'accumulated_submission_time': 161729.96871495247, 'accumulated_eval_time': 5654.447158336639, 'accumulated_logging_time': 23.269815683364868}
I0302 03:45:02.751779 140407379851008 logging_writer.py:48] [479827] accumulated_eval_time=5654.447158, accumulated_logging_time=23.269816, accumulated_submission_time=161729.968715, global_step=479827, preemption_count=0, score=161729.968715, test/accuracy=0.630400, test/loss=1.818935, test/num_examples=10000, total_duration=167426.647902, train/accuracy=0.961555, train/loss=0.143413, validation/accuracy=0.755620, validation/loss=1.052279, validation/num_examples=50000
I0302 03:45:27.625472 140408319375104 logging_writer.py:48] [479900] global_step=479900, grad_norm=4.651594161987305, loss=0.6738501787185669
I0302 03:46:01.316590 140407379851008 logging_writer.py:48] [480000] global_step=480000, grad_norm=4.483852863311768, loss=0.6115861535072327
I0302 03:46:34.996645 140408319375104 logging_writer.py:48] [480100] global_step=480100, grad_norm=4.9373650550842285, loss=0.6631569266319275
I0302 03:47:08.622900 140407379851008 logging_writer.py:48] [480200] global_step=480200, grad_norm=4.635573863983154, loss=0.6720397472381592
I0302 03:47:42.272782 140408319375104 logging_writer.py:48] [480300] global_step=480300, grad_norm=4.548619747161865, loss=0.644526481628418
I0302 03:48:15.972099 140407379851008 logging_writer.py:48] [480400] global_step=480400, grad_norm=4.557753086090088, loss=0.6400570869445801
I0302 03:48:49.662038 140408319375104 logging_writer.py:48] [480500] global_step=480500, grad_norm=4.590963363647461, loss=0.6619243621826172
I0302 03:49:23.308745 140407379851008 logging_writer.py:48] [480600] global_step=480600, grad_norm=4.6846842765808105, loss=0.6117496490478516
I0302 03:49:56.998846 140408319375104 logging_writer.py:48] [480700] global_step=480700, grad_norm=4.025608062744141, loss=0.5150903463363647
I0302 03:50:30.779853 140407379851008 logging_writer.py:48] [480800] global_step=480800, grad_norm=5.206421375274658, loss=0.6670109033584595
I0302 03:51:04.478173 140408319375104 logging_writer.py:48] [480900] global_step=480900, grad_norm=4.602462291717529, loss=0.680347740650177
I0302 03:51:38.138801 140407379851008 logging_writer.py:48] [481000] global_step=481000, grad_norm=4.627362251281738, loss=0.5887719392776489
I0302 03:52:11.846727 140408319375104 logging_writer.py:48] [481100] global_step=481100, grad_norm=4.522290229797363, loss=0.5983543395996094
I0302 03:52:45.478324 140407379851008 logging_writer.py:48] [481200] global_step=481200, grad_norm=4.863770484924316, loss=0.6669262051582336
I0302 03:53:19.142475 140408319375104 logging_writer.py:48] [481300] global_step=481300, grad_norm=4.617269039154053, loss=0.6312505006790161
I0302 03:53:32.751977 140573303715648 spec.py:321] Evaluating on the training split.
I0302 03:53:38.815069 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 03:53:47.257106 140573303715648 spec.py:349] Evaluating on the test split.
I0302 03:53:49.651020 140573303715648 submission_runner.py:411] Time since start: 167953.64s, 	Step: 481342, 	{'train/accuracy': 0.9592434167861938, 'train/loss': 0.14939920604228973, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0525703430175781, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8172633647918701, 'test/num_examples': 10000, 'score': 162239.8997218609, 'total_duration': 167953.64355373383, 'accumulated_submission_time': 162239.8997218609, 'accumulated_eval_time': 5671.34614443779, 'accumulated_logging_time': 23.37634825706482}
I0302 03:53:49.745693 140408319375104 logging_writer.py:48] [481342] accumulated_eval_time=5671.346144, accumulated_logging_time=23.376348, accumulated_submission_time=162239.899722, global_step=481342, preemption_count=0, score=162239.899722, test/accuracy=0.631800, test/loss=1.817263, test/num_examples=10000, total_duration=167953.643554, train/accuracy=0.959243, train/loss=0.149399, validation/accuracy=0.755480, validation/loss=1.052570, validation/num_examples=50000
I0302 03:54:09.608782 140409124681472 logging_writer.py:48] [481400] global_step=481400, grad_norm=4.3290791511535645, loss=0.6081716418266296
I0302 03:54:43.308546 140408319375104 logging_writer.py:48] [481500] global_step=481500, grad_norm=4.182254314422607, loss=0.6776260733604431
I0302 03:55:17.022350 140409124681472 logging_writer.py:48] [481600] global_step=481600, grad_norm=5.137625694274902, loss=0.6691288352012634
I0302 03:55:50.727375 140408319375104 logging_writer.py:48] [481700] global_step=481700, grad_norm=4.3521342277526855, loss=0.5946742296218872
I0302 03:56:24.422732 140409124681472 logging_writer.py:48] [481800] global_step=481800, grad_norm=5.110356330871582, loss=0.5711211562156677
I0302 03:56:58.232258 140408319375104 logging_writer.py:48] [481900] global_step=481900, grad_norm=4.754983425140381, loss=0.6376562714576721
I0302 03:57:31.959947 140409124681472 logging_writer.py:48] [482000] global_step=482000, grad_norm=4.433590888977051, loss=0.6263578534126282
I0302 03:58:05.708794 140408319375104 logging_writer.py:48] [482100] global_step=482100, grad_norm=4.16234827041626, loss=0.5503097176551819
I0302 03:58:39.389891 140409124681472 logging_writer.py:48] [482200] global_step=482200, grad_norm=4.238544940948486, loss=0.6358872652053833
I0302 03:59:13.105805 140408319375104 logging_writer.py:48] [482300] global_step=482300, grad_norm=4.517074108123779, loss=0.5848715305328369
I0302 03:59:46.809064 140409124681472 logging_writer.py:48] [482400] global_step=482400, grad_norm=4.347211837768555, loss=0.6321425437927246
I0302 04:00:20.530732 140408319375104 logging_writer.py:48] [482500] global_step=482500, grad_norm=4.729020118713379, loss=0.6266161203384399
I0302 04:00:54.225918 140409124681472 logging_writer.py:48] [482600] global_step=482600, grad_norm=4.54971170425415, loss=0.60753333568573
I0302 04:01:27.954526 140408319375104 logging_writer.py:48] [482700] global_step=482700, grad_norm=4.400939464569092, loss=0.5701199769973755
I0302 04:02:01.627106 140409124681472 logging_writer.py:48] [482800] global_step=482800, grad_norm=4.243381977081299, loss=0.526187002658844
I0302 04:02:19.956295 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:02:26.002432 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:02:34.778923 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:02:37.431650 140573303715648 submission_runner.py:411] Time since start: 168481.42s, 	Step: 482856, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.1477699726819992, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0522754192352295, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.818324089050293, 'test/num_examples': 10000, 'score': 162750.039809227, 'total_duration': 168481.42416000366, 'accumulated_submission_time': 162750.039809227, 'accumulated_eval_time': 5688.821419477463, 'accumulated_logging_time': 23.48125386238098}
I0302 04:02:37.529461 140407379851008 logging_writer.py:48] [482856] accumulated_eval_time=5688.821419, accumulated_logging_time=23.481254, accumulated_submission_time=162750.039809, global_step=482856, preemption_count=0, score=162750.039809, test/accuracy=0.631600, test/loss=1.818324, test/num_examples=10000, total_duration=168481.424160, train/accuracy=0.961655, train/loss=0.147770, validation/accuracy=0.756000, validation/loss=1.052275, validation/num_examples=50000
I0302 04:02:52.657431 140408319375104 logging_writer.py:48] [482900] global_step=482900, grad_norm=4.933032989501953, loss=0.6242156028747559
I0302 04:03:26.293749 140407379851008 logging_writer.py:48] [483000] global_step=483000, grad_norm=4.3574347496032715, loss=0.6137174367904663
I0302 04:03:59.940995 140408319375104 logging_writer.py:48] [483100] global_step=483100, grad_norm=4.597100734710693, loss=0.6600576639175415
I0302 04:04:33.647550 140407379851008 logging_writer.py:48] [483200] global_step=483200, grad_norm=4.385398864746094, loss=0.6296441555023193
I0302 04:05:07.320357 140408319375104 logging_writer.py:48] [483300] global_step=483300, grad_norm=4.043710708618164, loss=0.5632586479187012
I0302 04:05:40.992820 140407379851008 logging_writer.py:48] [483400] global_step=483400, grad_norm=4.497297286987305, loss=0.5895983576774597
I0302 04:06:14.638003 140408319375104 logging_writer.py:48] [483500] global_step=483500, grad_norm=4.439268112182617, loss=0.6203312873840332
I0302 04:06:48.337775 140407379851008 logging_writer.py:48] [483600] global_step=483600, grad_norm=4.52371072769165, loss=0.6065707206726074
I0302 04:07:22.012281 140408319375104 logging_writer.py:48] [483700] global_step=483700, grad_norm=4.4014811515808105, loss=0.6459950804710388
I0302 04:07:55.669024 140407379851008 logging_writer.py:48] [483800] global_step=483800, grad_norm=4.670925617218018, loss=0.6398576498031616
I0302 04:08:29.285845 140408319375104 logging_writer.py:48] [483900] global_step=483900, grad_norm=4.363050937652588, loss=0.6206150054931641
I0302 04:09:03.074547 140407379851008 logging_writer.py:48] [484000] global_step=484000, grad_norm=4.260074615478516, loss=0.5767925381660461
I0302 04:09:36.704968 140408319375104 logging_writer.py:48] [484100] global_step=484100, grad_norm=4.572079181671143, loss=0.6737426519393921
I0302 04:10:10.365840 140407379851008 logging_writer.py:48] [484200] global_step=484200, grad_norm=4.495290279388428, loss=0.6026251316070557
I0302 04:10:44.099153 140408319375104 logging_writer.py:48] [484300] global_step=484300, grad_norm=4.440871715545654, loss=0.5300193428993225
I0302 04:11:07.522538 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:11:13.623613 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:11:22.244795 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:11:24.533089 140573303715648 submission_runner.py:411] Time since start: 169008.53s, 	Step: 484371, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14725343883037567, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0523960590362549, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8176476955413818, 'test/num_examples': 10000, 'score': 163259.96401071548, 'total_duration': 169008.52561068535, 'accumulated_submission_time': 163259.96401071548, 'accumulated_eval_time': 5705.831904172897, 'accumulated_logging_time': 23.58875036239624}
I0302 04:11:24.639583 140409837700864 logging_writer.py:48] [484371] accumulated_eval_time=5705.831904, accumulated_logging_time=23.588750, accumulated_submission_time=163259.964011, global_step=484371, preemption_count=0, score=163259.964011, test/accuracy=0.630700, test/loss=1.817648, test/num_examples=10000, total_duration=169008.525611, train/accuracy=0.960180, train/loss=0.147253, validation/accuracy=0.755480, validation/loss=1.052396, validation/num_examples=50000
I0302 04:11:34.756198 140409846093568 logging_writer.py:48] [484400] global_step=484400, grad_norm=4.760165214538574, loss=0.6556400060653687
I0302 04:12:08.412244 140409837700864 logging_writer.py:48] [484500] global_step=484500, grad_norm=4.783735275268555, loss=0.7258960604667664
I0302 04:12:42.053368 140409846093568 logging_writer.py:48] [484600] global_step=484600, grad_norm=4.363958835601807, loss=0.5694988965988159
I0302 04:13:15.846785 140409837700864 logging_writer.py:48] [484700] global_step=484700, grad_norm=4.535098075866699, loss=0.6303526759147644
I0302 04:13:49.545826 140409846093568 logging_writer.py:48] [484800] global_step=484800, grad_norm=4.013631343841553, loss=0.6123527884483337
I0302 04:14:23.318345 140409837700864 logging_writer.py:48] [484900] global_step=484900, grad_norm=4.118298053741455, loss=0.5934878587722778
I0302 04:14:57.074386 140409846093568 logging_writer.py:48] [485000] global_step=485000, grad_norm=5.38518762588501, loss=0.6695933938026428
I0302 04:15:30.766467 140409837700864 logging_writer.py:48] [485100] global_step=485100, grad_norm=4.775961399078369, loss=0.610948920249939
I0302 04:16:04.494573 140409846093568 logging_writer.py:48] [485200] global_step=485200, grad_norm=4.438261985778809, loss=0.643671452999115
I0302 04:16:38.191974 140409837700864 logging_writer.py:48] [485300] global_step=485300, grad_norm=4.265923500061035, loss=0.6260626912117004
I0302 04:17:11.826505 140409846093568 logging_writer.py:48] [485400] global_step=485400, grad_norm=4.9006571769714355, loss=0.6428524255752563
I0302 04:17:45.493036 140409837700864 logging_writer.py:48] [485500] global_step=485500, grad_norm=4.298608779907227, loss=0.594478964805603
I0302 04:18:19.220599 140409846093568 logging_writer.py:48] [485600] global_step=485600, grad_norm=4.7298359870910645, loss=0.6343228220939636
I0302 04:18:52.917598 140409837700864 logging_writer.py:48] [485700] global_step=485700, grad_norm=4.388664245605469, loss=0.5767303705215454
I0302 04:19:26.640393 140409846093568 logging_writer.py:48] [485800] global_step=485800, grad_norm=4.914173126220703, loss=0.6647336483001709
I0302 04:19:54.743930 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:20:00.890516 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:20:09.521950 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:20:11.874670 140573303715648 submission_runner.py:411] Time since start: 169535.87s, 	Step: 485885, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1493615359067917, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0511548519134521, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8172670602798462, 'test/num_examples': 10000, 'score': 163769.99699783325, 'total_duration': 169535.86717271805, 'accumulated_submission_time': 163769.99699783325, 'accumulated_eval_time': 5722.962559461594, 'accumulated_logging_time': 23.705347299575806}
I0302 04:20:12.029550 140407371458304 logging_writer.py:48] [485885] accumulated_eval_time=5722.962559, accumulated_logging_time=23.705347, accumulated_submission_time=163769.996998, global_step=485885, preemption_count=0, score=163769.996998, test/accuracy=0.632500, test/loss=1.817267, test/num_examples=10000, total_duration=169535.867173, train/accuracy=0.960479, train/loss=0.149362, validation/accuracy=0.756080, validation/loss=1.051155, validation/num_examples=50000
I0302 04:20:17.436264 140407379851008 logging_writer.py:48] [485900] global_step=485900, grad_norm=4.27421236038208, loss=0.5823473334312439
I0302 04:20:51.085508 140407371458304 logging_writer.py:48] [486000] global_step=486000, grad_norm=4.419478416442871, loss=0.6275467872619629
I0302 04:21:24.878057 140407379851008 logging_writer.py:48] [486100] global_step=486100, grad_norm=4.819372653961182, loss=0.6418907046318054
I0302 04:21:58.596092 140407371458304 logging_writer.py:48] [486200] global_step=486200, grad_norm=4.806870937347412, loss=0.6354436278343201
I0302 04:22:32.274183 140407379851008 logging_writer.py:48] [486300] global_step=486300, grad_norm=4.486638069152832, loss=0.5955150127410889
I0302 04:23:06.003181 140407371458304 logging_writer.py:48] [486400] global_step=486400, grad_norm=4.764166831970215, loss=0.6721338033676147
I0302 04:23:39.720299 140407379851008 logging_writer.py:48] [486500] global_step=486500, grad_norm=4.499562740325928, loss=0.6368328332901001
I0302 04:24:13.446903 140407371458304 logging_writer.py:48] [486600] global_step=486600, grad_norm=4.539505481719971, loss=0.6426549553871155
I0302 04:24:47.163731 140407379851008 logging_writer.py:48] [486700] global_step=486700, grad_norm=4.46583366394043, loss=0.607176661491394
I0302 04:25:20.906052 140407371458304 logging_writer.py:48] [486800] global_step=486800, grad_norm=4.538576126098633, loss=0.5693750381469727
I0302 04:25:54.621896 140407379851008 logging_writer.py:48] [486900] global_step=486900, grad_norm=4.501855373382568, loss=0.6252047419548035
I0302 04:26:28.353843 140407371458304 logging_writer.py:48] [487000] global_step=487000, grad_norm=4.92014217376709, loss=0.6119408011436462
I0302 04:27:02.079429 140407379851008 logging_writer.py:48] [487100] global_step=487100, grad_norm=4.798882484436035, loss=0.6408860683441162
I0302 04:27:35.864752 140407371458304 logging_writer.py:48] [487200] global_step=487200, grad_norm=4.692434787750244, loss=0.6887985467910767
I0302 04:28:09.532688 140407379851008 logging_writer.py:48] [487300] global_step=487300, grad_norm=4.442205905914307, loss=0.6665832996368408
I0302 04:28:42.018617 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:28:48.097152 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:28:56.669044 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:28:58.998696 140573303715648 submission_runner.py:411] Time since start: 170062.99s, 	Step: 487398, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14796902239322662, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0512391328811646, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.816259503364563, 'test/num_examples': 10000, 'score': 164279.91232395172, 'total_duration': 170062.991219759, 'accumulated_submission_time': 164279.91232395172, 'accumulated_eval_time': 5739.9425756931305, 'accumulated_logging_time': 23.87461256980896}
I0302 04:28:59.091964 140409846093568 logging_writer.py:48] [487398] accumulated_eval_time=5739.942576, accumulated_logging_time=23.874613, accumulated_submission_time=164279.912324, global_step=487398, preemption_count=0, score=164279.912324, test/accuracy=0.631100, test/loss=1.816260, test/num_examples=10000, total_duration=170062.991220, train/accuracy=0.961316, train/loss=0.147969, validation/accuracy=0.755880, validation/loss=1.051239, validation/num_examples=50000
I0302 04:29:00.112760 140409854486272 logging_writer.py:48] [487400] global_step=487400, grad_norm=4.324977397918701, loss=0.6025934815406799
I0302 04:29:33.767910 140409846093568 logging_writer.py:48] [487500] global_step=487500, grad_norm=4.304561614990234, loss=0.5906047224998474
I0302 04:30:07.530644 140409854486272 logging_writer.py:48] [487600] global_step=487600, grad_norm=4.316789627075195, loss=0.5973367691040039
I0302 04:30:41.192415 140409846093568 logging_writer.py:48] [487700] global_step=487700, grad_norm=4.70954704284668, loss=0.6184683442115784
I0302 04:31:14.848747 140409854486272 logging_writer.py:48] [487800] global_step=487800, grad_norm=4.499294757843018, loss=0.653225302696228
I0302 04:31:48.594883 140409846093568 logging_writer.py:48] [487900] global_step=487900, grad_norm=4.557075023651123, loss=0.6506170034408569
I0302 04:32:22.274423 140409854486272 logging_writer.py:48] [488000] global_step=488000, grad_norm=4.510908603668213, loss=0.6489061117172241
I0302 04:32:55.995178 140409846093568 logging_writer.py:48] [488100] global_step=488100, grad_norm=4.695789813995361, loss=0.6781322956085205
I0302 04:33:29.791130 140409854486272 logging_writer.py:48] [488200] global_step=488200, grad_norm=4.289350986480713, loss=0.6042472720146179
I0302 04:34:03.490192 140409846093568 logging_writer.py:48] [488300] global_step=488300, grad_norm=4.23598051071167, loss=0.6451002359390259
I0302 04:34:37.216070 140409854486272 logging_writer.py:48] [488400] global_step=488400, grad_norm=4.347886085510254, loss=0.5768141746520996
I0302 04:35:10.885170 140409846093568 logging_writer.py:48] [488500] global_step=488500, grad_norm=4.690395355224609, loss=0.725530743598938
I0302 04:35:44.570203 140409854486272 logging_writer.py:48] [488600] global_step=488600, grad_norm=4.069993019104004, loss=0.5812392830848694
I0302 04:36:18.247641 140409846093568 logging_writer.py:48] [488700] global_step=488700, grad_norm=4.741328716278076, loss=0.7150891423225403
I0302 04:36:52.013196 140409854486272 logging_writer.py:48] [488800] global_step=488800, grad_norm=4.566388130187988, loss=0.6020551919937134
I0302 04:37:25.652197 140409846093568 logging_writer.py:48] [488900] global_step=488900, grad_norm=4.12033748626709, loss=0.5931646823883057
I0302 04:37:29.185173 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:37:35.213130 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:37:43.776660 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:37:46.135003 140573303715648 submission_runner.py:411] Time since start: 170590.13s, 	Step: 488912, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14805105328559875, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0511592626571655, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.817075252532959, 'test/num_examples': 10000, 'score': 164789.9333343506, 'total_duration': 170590.12753081322, 'accumulated_submission_time': 164789.9333343506, 'accumulated_eval_time': 5756.892340660095, 'accumulated_logging_time': 23.98096513748169}
I0302 04:37:46.234951 140407371458304 logging_writer.py:48] [488912] accumulated_eval_time=5756.892341, accumulated_logging_time=23.980965, accumulated_submission_time=164789.933334, global_step=488912, preemption_count=0, score=164789.933334, test/accuracy=0.630600, test/loss=1.817075, test/num_examples=10000, total_duration=170590.127531, train/accuracy=0.959981, train/loss=0.148051, validation/accuracy=0.756000, validation/loss=1.051159, validation/num_examples=50000
I0302 04:38:16.188419 140407379851008 logging_writer.py:48] [489000] global_step=489000, grad_norm=4.825124263763428, loss=0.6452740430831909
I0302 04:38:49.822760 140407371458304 logging_writer.py:48] [489100] global_step=489100, grad_norm=4.508859157562256, loss=0.6786096096038818
I0302 04:39:23.543938 140407379851008 logging_writer.py:48] [489200] global_step=489200, grad_norm=4.398797512054443, loss=0.6847268342971802
I0302 04:39:57.293311 140407371458304 logging_writer.py:48] [489300] global_step=489300, grad_norm=4.468290328979492, loss=0.6265085935592651
I0302 04:40:31.023053 140407379851008 logging_writer.py:48] [489400] global_step=489400, grad_norm=3.986616849899292, loss=0.5447869300842285
I0302 04:41:04.681777 140407371458304 logging_writer.py:48] [489500] global_step=489500, grad_norm=4.558080196380615, loss=0.6195184588432312
I0302 04:41:38.410673 140407379851008 logging_writer.py:48] [489600] global_step=489600, grad_norm=4.299569606781006, loss=0.572945773601532
I0302 04:42:12.083326 140407371458304 logging_writer.py:48] [489700] global_step=489700, grad_norm=4.354938507080078, loss=0.5820848345756531
I0302 04:42:45.779833 140407379851008 logging_writer.py:48] [489800] global_step=489800, grad_norm=4.659635066986084, loss=0.6126815676689148
I0302 04:43:19.462319 140407371458304 logging_writer.py:48] [489900] global_step=489900, grad_norm=4.264013767242432, loss=0.623772382736206
I0302 04:43:53.119455 140407379851008 logging_writer.py:48] [490000] global_step=490000, grad_norm=4.149050235748291, loss=0.5611507296562195
I0302 04:44:26.860837 140407371458304 logging_writer.py:48] [490100] global_step=490100, grad_norm=4.662934303283691, loss=0.6662522554397583
I0302 04:45:00.547709 140407379851008 logging_writer.py:48] [490200] global_step=490200, grad_norm=4.565597057342529, loss=0.6004895567893982
I0302 04:45:34.257419 140407371458304 logging_writer.py:48] [490300] global_step=490300, grad_norm=4.740699768066406, loss=0.6076586842536926
I0302 04:46:08.056167 140407379851008 logging_writer.py:48] [490400] global_step=490400, grad_norm=4.92097806930542, loss=0.644542932510376
I0302 04:46:16.270400 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:46:22.286842 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:46:31.041803 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:46:33.333572 140573303715648 submission_runner.py:411] Time since start: 171117.33s, 	Step: 490426, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14710621535778046, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0521148443222046, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8199012279510498, 'test/num_examples': 10000, 'score': 165299.89897084236, 'total_duration': 171117.3261051178, 'accumulated_submission_time': 165299.89897084236, 'accumulated_eval_time': 5773.9554533958435, 'accumulated_logging_time': 24.091283559799194}
I0302 04:46:33.440663 140409854486272 logging_writer.py:48] [490426] accumulated_eval_time=5773.955453, accumulated_logging_time=24.091284, accumulated_submission_time=165299.898971, global_step=490426, preemption_count=0, score=165299.898971, test/accuracy=0.631800, test/loss=1.819901, test/num_examples=10000, total_duration=171117.326105, train/accuracy=0.960678, train/loss=0.147106, validation/accuracy=0.755940, validation/loss=1.052115, validation/num_examples=50000
I0302 04:46:58.677750 140409862878976 logging_writer.py:48] [490500] global_step=490500, grad_norm=4.250170707702637, loss=0.5518248677253723
I0302 04:47:32.322090 140409854486272 logging_writer.py:48] [490600] global_step=490600, grad_norm=4.2169013023376465, loss=0.6092789769172668
I0302 04:48:05.972670 140409862878976 logging_writer.py:48] [490700] global_step=490700, grad_norm=4.640498638153076, loss=0.6731604933738708
I0302 04:48:39.700489 140409854486272 logging_writer.py:48] [490800] global_step=490800, grad_norm=4.867478847503662, loss=0.6207461357116699
I0302 04:49:13.383684 140409862878976 logging_writer.py:48] [490900] global_step=490900, grad_norm=4.358428001403809, loss=0.5345838069915771
I0302 04:49:47.120711 140409854486272 logging_writer.py:48] [491000] global_step=491000, grad_norm=4.675979137420654, loss=0.7085462808609009
I0302 04:50:20.831539 140409862878976 logging_writer.py:48] [491100] global_step=491100, grad_norm=4.515913963317871, loss=0.6163828372955322
I0302 04:50:54.541498 140409854486272 logging_writer.py:48] [491200] global_step=491200, grad_norm=4.566675662994385, loss=0.6519047021865845
I0302 04:51:28.280845 140409862878976 logging_writer.py:48] [491300] global_step=491300, grad_norm=4.352964878082275, loss=0.6007608771324158
I0302 04:52:02.032378 140409854486272 logging_writer.py:48] [491400] global_step=491400, grad_norm=4.01846981048584, loss=0.6589802503585815
I0302 04:52:35.723444 140409862878976 logging_writer.py:48] [491500] global_step=491500, grad_norm=4.48140811920166, loss=0.6921838521957397
I0302 04:53:09.442107 140409854486272 logging_writer.py:48] [491600] global_step=491600, grad_norm=4.296606540679932, loss=0.559309184551239
I0302 04:53:43.124885 140409862878976 logging_writer.py:48] [491700] global_step=491700, grad_norm=4.7817487716674805, loss=0.6876183748245239
I0302 04:54:16.843644 140409854486272 logging_writer.py:48] [491800] global_step=491800, grad_norm=4.471102714538574, loss=0.5529500246047974
I0302 04:54:50.553396 140409862878976 logging_writer.py:48] [491900] global_step=491900, grad_norm=4.391226768493652, loss=0.6392690539360046
I0302 04:55:03.531558 140573303715648 spec.py:321] Evaluating on the training split.
I0302 04:55:09.586466 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 04:55:18.247622 140573303715648 spec.py:349] Evaluating on the test split.
I0302 04:55:20.583550 140573303715648 submission_runner.py:411] Time since start: 171644.58s, 	Step: 491940, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14875519275665283, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0525541305541992, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8181631565093994, 'test/num_examples': 10000, 'score': 165809.91813397408, 'total_duration': 171644.57608389854, 'accumulated_submission_time': 165809.91813397408, 'accumulated_eval_time': 5791.007390499115, 'accumulated_logging_time': 24.209453582763672}
I0302 04:55:20.683435 140409837700864 logging_writer.py:48] [491940] accumulated_eval_time=5791.007390, accumulated_logging_time=24.209454, accumulated_submission_time=165809.918134, global_step=491940, preemption_count=0, score=165809.918134, test/accuracy=0.630700, test/loss=1.818163, test/num_examples=10000, total_duration=171644.576084, train/accuracy=0.960100, train/loss=0.148755, validation/accuracy=0.755780, validation/loss=1.052554, validation/num_examples=50000
I0302 04:55:41.227848 140409846093568 logging_writer.py:48] [492000] global_step=492000, grad_norm=4.016887664794922, loss=0.5583426356315613
I0302 04:56:14.874651 140409837700864 logging_writer.py:48] [492100] global_step=492100, grad_norm=4.588027477264404, loss=0.66829913854599
I0302 04:56:48.550778 140409846093568 logging_writer.py:48] [492200] global_step=492200, grad_norm=4.045302867889404, loss=0.5502541065216064
I0302 04:57:22.261292 140409837700864 logging_writer.py:48] [492300] global_step=492300, grad_norm=4.246262073516846, loss=0.6184601783752441
I0302 04:57:55.935373 140409846093568 logging_writer.py:48] [492400] global_step=492400, grad_norm=4.387775897979736, loss=0.6372569799423218
I0302 04:58:29.752077 140409837700864 logging_writer.py:48] [492500] global_step=492500, grad_norm=4.935376167297363, loss=0.5684740543365479
I0302 04:59:03.400695 140409846093568 logging_writer.py:48] [492600] global_step=492600, grad_norm=4.45761775970459, loss=0.661480724811554
I0302 04:59:37.116456 140409837700864 logging_writer.py:48] [492700] global_step=492700, grad_norm=4.596197128295898, loss=0.6730777025222778
I0302 05:00:10.777317 140409846093568 logging_writer.py:48] [492800] global_step=492800, grad_norm=4.724610805511475, loss=0.6319591999053955
I0302 05:00:44.441771 140409837700864 logging_writer.py:48] [492900] global_step=492900, grad_norm=4.5866265296936035, loss=0.647056519985199
I0302 05:01:18.175880 140409846093568 logging_writer.py:48] [493000] global_step=493000, grad_norm=4.830044269561768, loss=0.6550998687744141
I0302 05:01:51.858728 140409837700864 logging_writer.py:48] [493100] global_step=493100, grad_norm=4.561225891113281, loss=0.6177728772163391
I0302 05:02:25.601897 140409846093568 logging_writer.py:48] [493200] global_step=493200, grad_norm=4.328481197357178, loss=0.5994024276733398
I0302 05:02:59.272491 140409837700864 logging_writer.py:48] [493300] global_step=493300, grad_norm=4.213383197784424, loss=0.6182000637054443
I0302 05:03:33.077116 140409846093568 logging_writer.py:48] [493400] global_step=493400, grad_norm=4.843820095062256, loss=0.6172181367874146
I0302 05:03:50.730761 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:03:56.820834 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:04:05.579006 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:04:07.920990 140573303715648 submission_runner.py:411] Time since start: 172171.91s, 	Step: 493454, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.1470497101545334, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0523558855056763, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8182610273361206, 'test/num_examples': 10000, 'score': 166319.89604902267, 'total_duration': 172171.9135248661, 'accumulated_submission_time': 166319.89604902267, 'accumulated_eval_time': 5808.1975655555725, 'accumulated_logging_time': 24.3194260597229}
I0302 05:04:08.020238 140409854486272 logging_writer.py:48] [493454] accumulated_eval_time=5808.197566, accumulated_logging_time=24.319426, accumulated_submission_time=166319.896049, global_step=493454, preemption_count=0, score=166319.896049, test/accuracy=0.631000, test/loss=1.818261, test/num_examples=10000, total_duration=172171.913525, train/accuracy=0.960738, train/loss=0.147050, validation/accuracy=0.755800, validation/loss=1.052356, validation/num_examples=50000
I0302 05:04:23.939795 140409862878976 logging_writer.py:48] [493500] global_step=493500, grad_norm=4.720780372619629, loss=0.6322009563446045
I0302 05:04:57.613982 140409854486272 logging_writer.py:48] [493600] global_step=493600, grad_norm=4.165126323699951, loss=0.6116394996643066
I0302 05:05:31.299932 140409862878976 logging_writer.py:48] [493700] global_step=493700, grad_norm=4.436431407928467, loss=0.6306897401809692
I0302 05:06:05.010304 140409854486272 logging_writer.py:48] [493800] global_step=493800, grad_norm=4.827645778656006, loss=0.5952309370040894
I0302 05:06:38.653454 140409862878976 logging_writer.py:48] [493900] global_step=493900, grad_norm=4.165938854217529, loss=0.5873873233795166
I0302 05:07:12.354323 140409854486272 logging_writer.py:48] [494000] global_step=494000, grad_norm=4.460763931274414, loss=0.5995008945465088
I0302 05:07:46.063600 140409862878976 logging_writer.py:48] [494100] global_step=494100, grad_norm=4.558570861816406, loss=0.5820967555046082
I0302 05:08:19.732231 140409854486272 logging_writer.py:48] [494200] global_step=494200, grad_norm=4.326722145080566, loss=0.7179225087165833
I0302 05:08:53.461434 140409862878976 logging_writer.py:48] [494300] global_step=494300, grad_norm=4.704471111297607, loss=0.6256545186042786
I0302 05:09:27.177675 140409854486272 logging_writer.py:48] [494400] global_step=494400, grad_norm=4.486811637878418, loss=0.6402370929718018
I0302 05:10:00.909579 140409862878976 logging_writer.py:48] [494500] global_step=494500, grad_norm=4.312722206115723, loss=0.5394179224967957
I0302 05:10:34.716170 140409854486272 logging_writer.py:48] [494600] global_step=494600, grad_norm=4.522491931915283, loss=0.6834860444068909
I0302 05:11:08.440834 140409862878976 logging_writer.py:48] [494700] global_step=494700, grad_norm=4.560067653656006, loss=0.7037508487701416
I0302 05:11:42.182664 140409854486272 logging_writer.py:48] [494800] global_step=494800, grad_norm=4.2327046394348145, loss=0.601140558719635
I0302 05:12:15.826432 140409862878976 logging_writer.py:48] [494900] global_step=494900, grad_norm=4.966727256774902, loss=0.6220013499259949
I0302 05:12:38.208828 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:12:44.237231 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:12:52.956962 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:12:55.213766 140573303715648 submission_runner.py:411] Time since start: 172699.21s, 	Step: 494968, 	{'train/accuracy': 0.9615752100944519, 'train/loss': 0.1471329629421234, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0521728992462158, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8184555768966675, 'test/num_examples': 10000, 'score': 166830.0151219368, 'total_duration': 172699.20630049706, 'accumulated_submission_time': 166830.0151219368, 'accumulated_eval_time': 5825.202464342117, 'accumulated_logging_time': 24.428896188735962}
I0302 05:12:55.314135 140407379851008 logging_writer.py:48] [494968] accumulated_eval_time=5825.202464, accumulated_logging_time=24.428896, accumulated_submission_time=166830.015122, global_step=494968, preemption_count=0, score=166830.015122, test/accuracy=0.631800, test/loss=1.818456, test/num_examples=10000, total_duration=172699.206300, train/accuracy=0.961575, train/loss=0.147133, validation/accuracy=0.756080, validation/loss=1.052173, validation/num_examples=50000
I0302 05:13:06.411072 140408319375104 logging_writer.py:48] [495000] global_step=495000, grad_norm=4.653444766998291, loss=0.6546691656112671
I0302 05:13:40.092662 140407379851008 logging_writer.py:48] [495100] global_step=495100, grad_norm=4.170271873474121, loss=0.5604168772697449
I0302 05:14:13.806380 140408319375104 logging_writer.py:48] [495200] global_step=495200, grad_norm=4.700253009796143, loss=0.6282649636268616
I0302 05:14:47.449145 140407379851008 logging_writer.py:48] [495300] global_step=495300, grad_norm=4.469728469848633, loss=0.6059868335723877
I0302 05:15:21.158833 140408319375104 logging_writer.py:48] [495400] global_step=495400, grad_norm=4.757603645324707, loss=0.6827654242515564
I0302 05:15:54.912019 140407379851008 logging_writer.py:48] [495500] global_step=495500, grad_norm=4.5753397941589355, loss=0.7364396452903748
I0302 05:16:28.639973 140408319375104 logging_writer.py:48] [495600] global_step=495600, grad_norm=4.810967445373535, loss=0.6682315468788147
I0302 05:17:02.318409 140407379851008 logging_writer.py:48] [495700] global_step=495700, grad_norm=4.541828632354736, loss=0.6272950172424316
I0302 05:17:35.984697 140408319375104 logging_writer.py:48] [495800] global_step=495800, grad_norm=4.046079158782959, loss=0.5817040205001831
I0302 05:18:09.720555 140407379851008 logging_writer.py:48] [495900] global_step=495900, grad_norm=4.294715881347656, loss=0.6384580135345459
I0302 05:18:43.406540 140408319375104 logging_writer.py:48] [496000] global_step=496000, grad_norm=4.165120601654053, loss=0.6532981395721436
I0302 05:19:17.049534 140407379851008 logging_writer.py:48] [496100] global_step=496100, grad_norm=4.6448445320129395, loss=0.6772459745407104
I0302 05:19:50.709670 140408319375104 logging_writer.py:48] [496200] global_step=496200, grad_norm=4.133565425872803, loss=0.5960810780525208
I0302 05:20:24.369393 140407379851008 logging_writer.py:48] [496300] global_step=496300, grad_norm=4.611379623413086, loss=0.6774125695228577
I0302 05:20:58.097567 140408319375104 logging_writer.py:48] [496400] global_step=496400, grad_norm=4.137370586395264, loss=0.6722068190574646
I0302 05:21:25.529741 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:21:31.560317 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:21:40.048843 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:21:42.349704 140573303715648 submission_runner.py:411] Time since start: 173226.34s, 	Step: 496483, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14514996111392975, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0518877506256104, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8172544240951538, 'test/num_examples': 10000, 'score': 167340.15970230103, 'total_duration': 173226.34223484993, 'accumulated_submission_time': 167340.15970230103, 'accumulated_eval_time': 5842.022374391556, 'accumulated_logging_time': 24.539676189422607}
I0302 05:21:42.445455 140409837700864 logging_writer.py:48] [496483] accumulated_eval_time=5842.022374, accumulated_logging_time=24.539676, accumulated_submission_time=167340.159702, global_step=496483, preemption_count=0, score=167340.159702, test/accuracy=0.631300, test/loss=1.817254, test/num_examples=10000, total_duration=173226.342235, train/accuracy=0.961316, train/loss=0.145150, validation/accuracy=0.755880, validation/loss=1.051888, validation/num_examples=50000
I0302 05:21:48.526653 140409846093568 logging_writer.py:48] [496500] global_step=496500, grad_norm=4.605525493621826, loss=0.6342445611953735
I0302 05:22:22.190898 140409837700864 logging_writer.py:48] [496600] global_step=496600, grad_norm=4.262400150299072, loss=0.5511685609817505
I0302 05:22:56.001281 140409846093568 logging_writer.py:48] [496700] global_step=496700, grad_norm=4.916049957275391, loss=0.6147854924201965
I0302 05:23:29.672968 140409837700864 logging_writer.py:48] [496800] global_step=496800, grad_norm=4.341699600219727, loss=0.5855003595352173
I0302 05:24:03.406008 140409846093568 logging_writer.py:48] [496900] global_step=496900, grad_norm=5.01157808303833, loss=0.60477215051651
I0302 05:24:37.067615 140409837700864 logging_writer.py:48] [497000] global_step=497000, grad_norm=4.307594299316406, loss=0.5812734365463257
I0302 05:25:10.751811 140409846093568 logging_writer.py:48] [497100] global_step=497100, grad_norm=4.76590633392334, loss=0.606356143951416
I0302 05:25:44.474702 140409837700864 logging_writer.py:48] [497200] global_step=497200, grad_norm=4.545895576477051, loss=0.6349180340766907
I0302 05:26:18.149376 140409846093568 logging_writer.py:48] [497300] global_step=497300, grad_norm=4.53262996673584, loss=0.6371661424636841
I0302 05:26:51.849774 140409837700864 logging_writer.py:48] [497400] global_step=497400, grad_norm=4.69793701171875, loss=0.6382452249526978
I0302 05:27:25.536667 140409846093568 logging_writer.py:48] [497500] global_step=497500, grad_norm=4.319911479949951, loss=0.5903197526931763
I0302 05:27:59.266542 140409837700864 logging_writer.py:48] [497600] global_step=497600, grad_norm=4.560577392578125, loss=0.6082180738449097
I0302 05:28:32.945379 140409846093568 logging_writer.py:48] [497700] global_step=497700, grad_norm=4.223453998565674, loss=0.6000854969024658
I0302 05:29:06.801548 140409837700864 logging_writer.py:48] [497800] global_step=497800, grad_norm=4.4499993324279785, loss=0.6164842247962952
I0302 05:29:40.548335 140409846093568 logging_writer.py:48] [497900] global_step=497900, grad_norm=4.747044563293457, loss=0.6224763989448547
I0302 05:30:12.405973 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:30:18.432048 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:30:27.063414 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:30:29.388294 140573303715648 submission_runner.py:411] Time since start: 173753.38s, 	Step: 497996, 	{'train/accuracy': 0.9597815275192261, 'train/loss': 0.14616695046424866, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0516399145126343, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8174744844436646, 'test/num_examples': 10000, 'score': 167850.05058646202, 'total_duration': 173753.38080906868, 'accumulated_submission_time': 167850.05058646202, 'accumulated_eval_time': 5859.004626750946, 'accumulated_logging_time': 24.64591932296753}
I0302 05:30:29.491538 140409829308160 logging_writer.py:48] [497996] accumulated_eval_time=5859.004627, accumulated_logging_time=24.645919, accumulated_submission_time=167850.050586, global_step=497996, preemption_count=0, score=167850.050586, test/accuracy=0.631400, test/loss=1.817474, test/num_examples=10000, total_duration=173753.380809, train/accuracy=0.959782, train/loss=0.146167, validation/accuracy=0.756000, validation/loss=1.051640, validation/num_examples=50000
I0302 05:30:31.185331 140409846093568 logging_writer.py:48] [498000] global_step=498000, grad_norm=4.122294902801514, loss=0.5589894652366638
I0302 05:31:04.879019 140409829308160 logging_writer.py:48] [498100] global_step=498100, grad_norm=4.115549564361572, loss=0.6010266542434692
I0302 05:31:38.554358 140409846093568 logging_writer.py:48] [498200] global_step=498200, grad_norm=4.420222282409668, loss=0.5800456404685974
I0302 05:32:12.212996 140409829308160 logging_writer.py:48] [498300] global_step=498300, grad_norm=4.975809574127197, loss=0.6998719573020935
I0302 05:32:45.880098 140409846093568 logging_writer.py:48] [498400] global_step=498400, grad_norm=4.610713005065918, loss=0.7052226662635803
I0302 05:33:19.599242 140409829308160 logging_writer.py:48] [498500] global_step=498500, grad_norm=4.4249749183654785, loss=0.5237582921981812
I0302 05:33:53.291368 140409846093568 logging_writer.py:48] [498600] global_step=498600, grad_norm=4.7228102684021, loss=0.6614841222763062
I0302 05:34:26.945851 140409829308160 logging_writer.py:48] [498700] global_step=498700, grad_norm=4.582587718963623, loss=0.6220018863677979
I0302 05:35:00.699743 140409846093568 logging_writer.py:48] [498800] global_step=498800, grad_norm=4.562042713165283, loss=0.6248534321784973
I0302 05:35:34.414203 140409829308160 logging_writer.py:48] [498900] global_step=498900, grad_norm=4.264547348022461, loss=0.6232401728630066
I0302 05:36:08.141451 140409846093568 logging_writer.py:48] [499000] global_step=499000, grad_norm=4.500779151916504, loss=0.61248379945755
I0302 05:36:41.813929 140409829308160 logging_writer.py:48] [499100] global_step=499100, grad_norm=4.60359525680542, loss=0.6547553539276123
I0302 05:37:15.576998 140409846093568 logging_writer.py:48] [499200] global_step=499200, grad_norm=4.2012553215026855, loss=0.5865035653114319
I0302 05:37:49.241940 140409829308160 logging_writer.py:48] [499300] global_step=499300, grad_norm=4.068448066711426, loss=0.6072314977645874
I0302 05:38:22.907608 140409846093568 logging_writer.py:48] [499400] global_step=499400, grad_norm=4.503180027008057, loss=0.6109582781791687
I0302 05:38:56.615823 140409829308160 logging_writer.py:48] [499500] global_step=499500, grad_norm=4.453186511993408, loss=0.6095778942108154
I0302 05:38:59.461167 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:39:05.676183 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:39:14.156759 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:39:16.461967 140573303715648 submission_runner.py:411] Time since start: 174280.45s, 	Step: 499510, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14555037021636963, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0524622201919556, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8173248767852783, 'test/num_examples': 10000, 'score': 168359.94944262505, 'total_duration': 174280.4545059204, 'accumulated_submission_time': 168359.94944262505, 'accumulated_eval_time': 5876.005375862122, 'accumulated_logging_time': 24.75995421409607}
I0302 05:39:16.563050 140407379851008 logging_writer.py:48] [499510] accumulated_eval_time=5876.005376, accumulated_logging_time=24.759954, accumulated_submission_time=168359.949443, global_step=499510, preemption_count=0, score=168359.949443, test/accuracy=0.631400, test/loss=1.817325, test/num_examples=10000, total_duration=174280.454506, train/accuracy=0.961276, train/loss=0.145550, validation/accuracy=0.755940, validation/loss=1.052462, validation/num_examples=50000
I0302 05:39:47.244732 140408319375104 logging_writer.py:48] [499600] global_step=499600, grad_norm=4.624259948730469, loss=0.7102593779563904
I0302 05:40:20.918946 140407379851008 logging_writer.py:48] [499700] global_step=499700, grad_norm=4.235593795776367, loss=0.5830602645874023
I0302 05:40:54.603193 140408319375104 logging_writer.py:48] [499800] global_step=499800, grad_norm=4.354458808898926, loss=0.5947183966636658
I0302 05:41:28.351183 140407379851008 logging_writer.py:48] [499900] global_step=499900, grad_norm=4.194704532623291, loss=0.643604040145874
I0302 05:42:02.071332 140408319375104 logging_writer.py:48] [500000] global_step=500000, grad_norm=4.201901435852051, loss=0.6618135571479797
I0302 05:42:35.752806 140407379851008 logging_writer.py:48] [500100] global_step=500100, grad_norm=4.169037818908691, loss=0.6110786199569702
I0302 05:43:09.490576 140408319375104 logging_writer.py:48] [500200] global_step=500200, grad_norm=4.049499988555908, loss=0.5534708499908447
I0302 05:43:43.259249 140407379851008 logging_writer.py:48] [500300] global_step=500300, grad_norm=4.285964488983154, loss=0.6224344968795776
I0302 05:44:16.951040 140408319375104 logging_writer.py:48] [500400] global_step=500400, grad_norm=4.145273208618164, loss=0.6253307461738586
I0302 05:44:50.674519 140407379851008 logging_writer.py:48] [500500] global_step=500500, grad_norm=5.152426719665527, loss=0.6297842264175415
I0302 05:45:24.360636 140408319375104 logging_writer.py:48] [500600] global_step=500600, grad_norm=4.465348720550537, loss=0.6998649835586548
I0302 05:45:58.071247 140407379851008 logging_writer.py:48] [500700] global_step=500700, grad_norm=4.591249942779541, loss=0.6355528235435486
I0302 05:46:31.767346 140408319375104 logging_writer.py:48] [500800] global_step=500800, grad_norm=4.62386417388916, loss=0.6171992421150208
I0302 05:47:05.543574 140407379851008 logging_writer.py:48] [500900] global_step=500900, grad_norm=4.644356727600098, loss=0.5784880518913269
I0302 05:47:39.262605 140408319375104 logging_writer.py:48] [501000] global_step=501000, grad_norm=4.412484645843506, loss=0.6709635853767395
I0302 05:47:46.490306 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:47:53.312200 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:48:01.753165 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:48:04.247109 140573303715648 submission_runner.py:411] Time since start: 174808.24s, 	Step: 501023, 	{'train/accuracy': 0.9595224857330322, 'train/loss': 0.14923210442066193, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.052715539932251, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8189656734466553, 'test/num_examples': 10000, 'score': 168869.80708909035, 'total_duration': 174808.23965120316, 'accumulated_submission_time': 168869.80708909035, 'accumulated_eval_time': 5893.762127637863, 'accumulated_logging_time': 24.870811223983765}
I0302 05:48:04.347023 140409837700864 logging_writer.py:48] [501023] accumulated_eval_time=5893.762128, accumulated_logging_time=24.870811, accumulated_submission_time=168869.807089, global_step=501023, preemption_count=0, score=168869.807089, test/accuracy=0.630400, test/loss=1.818966, test/num_examples=10000, total_duration=174808.239651, train/accuracy=0.959522, train/loss=0.149232, validation/accuracy=0.755740, validation/loss=1.052716, validation/num_examples=50000
I0302 05:48:30.645679 140409846093568 logging_writer.py:48] [501100] global_step=501100, grad_norm=4.509385585784912, loss=0.6440718173980713
I0302 05:49:04.266711 140409837700864 logging_writer.py:48] [501200] global_step=501200, grad_norm=4.929437160491943, loss=0.6391581892967224
I0302 05:49:37.943238 140409846093568 logging_writer.py:48] [501300] global_step=501300, grad_norm=4.4838433265686035, loss=0.624925971031189
I0302 05:50:11.653104 140409837700864 logging_writer.py:48] [501400] global_step=501400, grad_norm=4.528539180755615, loss=0.5934216380119324
I0302 05:50:45.361345 140409846093568 logging_writer.py:48] [501500] global_step=501500, grad_norm=4.396056652069092, loss=0.6118675470352173
I0302 05:51:19.034399 140409837700864 logging_writer.py:48] [501600] global_step=501600, grad_norm=4.499624252319336, loss=0.6025074124336243
I0302 05:51:52.795743 140409846093568 logging_writer.py:48] [501700] global_step=501700, grad_norm=4.638381004333496, loss=0.7356137633323669
I0302 05:52:26.479394 140409837700864 logging_writer.py:48] [501800] global_step=501800, grad_norm=4.344046592712402, loss=0.6168503165245056
I0302 05:53:00.234587 140409846093568 logging_writer.py:48] [501900] global_step=501900, grad_norm=4.317845344543457, loss=0.6495460867881775
I0302 05:53:33.966159 140409837700864 logging_writer.py:48] [502000] global_step=502000, grad_norm=4.328840732574463, loss=0.5615499019622803
I0302 05:54:07.642182 140409846093568 logging_writer.py:48] [502100] global_step=502100, grad_norm=4.152444362640381, loss=0.5461406707763672
I0302 05:54:41.387952 140409837700864 logging_writer.py:48] [502200] global_step=502200, grad_norm=4.47756290435791, loss=0.5555298924446106
I0302 05:55:15.071552 140409846093568 logging_writer.py:48] [502300] global_step=502300, grad_norm=4.658479690551758, loss=0.7022972702980042
I0302 05:55:48.737670 140409837700864 logging_writer.py:48] [502400] global_step=502400, grad_norm=4.316313743591309, loss=0.6116195321083069
I0302 05:56:22.466981 140409846093568 logging_writer.py:48] [502500] global_step=502500, grad_norm=4.594287872314453, loss=0.6333165168762207
I0302 05:56:34.395442 140573303715648 spec.py:321] Evaluating on the training split.
I0302 05:56:40.442688 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 05:56:48.969666 140573303715648 spec.py:349] Evaluating on the test split.
I0302 05:56:51.241212 140573303715648 submission_runner.py:411] Time since start: 175335.23s, 	Step: 502537, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14916537702083588, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0511752367019653, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8172820806503296, 'test/num_examples': 10000, 'score': 169379.7861609459, 'total_duration': 175335.23373770714, 'accumulated_submission_time': 169379.7861609459, 'accumulated_eval_time': 5910.6078407764435, 'accumulated_logging_time': 24.980355262756348}
I0302 05:56:51.338403 140407371458304 logging_writer.py:48] [502537] accumulated_eval_time=5910.607841, accumulated_logging_time=24.980355, accumulated_submission_time=169379.786161, global_step=502537, preemption_count=0, score=169379.786161, test/accuracy=0.631200, test/loss=1.817282, test/num_examples=10000, total_duration=175335.233738, train/accuracy=0.960100, train/loss=0.149165, validation/accuracy=0.755840, validation/loss=1.051175, validation/num_examples=50000
I0302 05:57:12.907796 140407379851008 logging_writer.py:48] [502600] global_step=502600, grad_norm=4.7622270584106445, loss=0.6548551917076111
I0302 05:57:46.546123 140407371458304 logging_writer.py:48] [502700] global_step=502700, grad_norm=4.526524543762207, loss=0.6077789664268494
I0302 05:58:20.194013 140407379851008 logging_writer.py:48] [502800] global_step=502800, grad_norm=4.8324713706970215, loss=0.6462878584861755
I0302 05:58:53.923204 140407371458304 logging_writer.py:48] [502900] global_step=502900, grad_norm=4.372360706329346, loss=0.60722815990448
I0302 05:59:27.685808 140407379851008 logging_writer.py:48] [503000] global_step=503000, grad_norm=4.463885307312012, loss=0.6272611618041992
I0302 06:00:01.414084 140407371458304 logging_writer.py:48] [503100] global_step=503100, grad_norm=5.0500874519348145, loss=0.7396092414855957
I0302 06:00:35.094868 140407379851008 logging_writer.py:48] [503200] global_step=503200, grad_norm=4.2967681884765625, loss=0.6099865436553955
I0302 06:01:08.814350 140407371458304 logging_writer.py:48] [503300] global_step=503300, grad_norm=4.604288578033447, loss=0.678679883480072
I0302 06:01:42.514556 140407379851008 logging_writer.py:48] [503400] global_step=503400, grad_norm=4.481007099151611, loss=0.5892382264137268
I0302 06:02:16.230383 140407371458304 logging_writer.py:48] [503500] global_step=503500, grad_norm=4.435634136199951, loss=0.5885345935821533
I0302 06:02:49.874386 140407379851008 logging_writer.py:48] [503600] global_step=503600, grad_norm=4.582929611206055, loss=0.6521652936935425
I0302 06:03:23.602344 140407371458304 logging_writer.py:48] [503700] global_step=503700, grad_norm=4.993561267852783, loss=0.6513947248458862
I0302 06:03:57.279488 140407379851008 logging_writer.py:48] [503800] global_step=503800, grad_norm=4.486771106719971, loss=0.6078027486801147
I0302 06:04:30.990541 140407371458304 logging_writer.py:48] [503900] global_step=503900, grad_norm=4.4951677322387695, loss=0.5455980896949768
I0302 06:05:04.694948 140407379851008 logging_writer.py:48] [504000] global_step=504000, grad_norm=4.441972255706787, loss=0.6243311166763306
I0302 06:05:21.331315 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:05:27.366328 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:05:35.921108 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:05:38.238050 140573303715648 submission_runner.py:411] Time since start: 175862.23s, 	Step: 504051, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14610904455184937, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0522167682647705, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8169641494750977, 'test/num_examples': 10000, 'score': 169889.70877027512, 'total_duration': 175862.2305700779, 'accumulated_submission_time': 169889.70877027512, 'accumulated_eval_time': 5927.514526128769, 'accumulated_logging_time': 25.08798623085022}
I0302 06:05:38.334751 140409837700864 logging_writer.py:48] [504051] accumulated_eval_time=5927.514526, accumulated_logging_time=25.087986, accumulated_submission_time=169889.708770, global_step=504051, preemption_count=0, score=169889.708770, test/accuracy=0.630800, test/loss=1.816964, test/num_examples=10000, total_duration=175862.230570, train/accuracy=0.960758, train/loss=0.146109, validation/accuracy=0.756040, validation/loss=1.052217, validation/num_examples=50000
I0302 06:05:55.169459 140409846093568 logging_writer.py:48] [504100] global_step=504100, grad_norm=4.827311992645264, loss=0.622411847114563
I0302 06:06:28.805909 140409837700864 logging_writer.py:48] [504200] global_step=504200, grad_norm=4.283544540405273, loss=0.6438798308372498
I0302 06:07:02.506533 140409846093568 logging_writer.py:48] [504300] global_step=504300, grad_norm=4.495384693145752, loss=0.6596454381942749
I0302 06:07:36.195023 140409837700864 logging_writer.py:48] [504400] global_step=504400, grad_norm=4.6897783279418945, loss=0.5912536978721619
I0302 06:08:09.841933 140409846093568 logging_writer.py:48] [504500] global_step=504500, grad_norm=4.265567779541016, loss=0.5527819991111755
I0302 06:08:43.539274 140409837700864 logging_writer.py:48] [504600] global_step=504600, grad_norm=4.698607444763184, loss=0.6666349768638611
I0302 06:09:17.215353 140409846093568 logging_writer.py:48] [504700] global_step=504700, grad_norm=4.8426032066345215, loss=0.5954585671424866
I0302 06:09:50.898557 140409837700864 logging_writer.py:48] [504800] global_step=504800, grad_norm=4.50920295715332, loss=0.5976815223693848
I0302 06:10:24.592291 140409846093568 logging_writer.py:48] [504900] global_step=504900, grad_norm=4.894883632659912, loss=0.6832689642906189
I0302 06:10:58.332293 140409837700864 logging_writer.py:48] [505000] global_step=505000, grad_norm=4.234485626220703, loss=0.560611367225647
I0302 06:11:31.998360 140409846093568 logging_writer.py:48] [505100] global_step=505100, grad_norm=4.470863342285156, loss=0.6236031651496887
I0302 06:12:05.769927 140409837700864 logging_writer.py:48] [505200] global_step=505200, grad_norm=4.193559646606445, loss=0.5776803493499756
I0302 06:12:39.488796 140409846093568 logging_writer.py:48] [505300] global_step=505300, grad_norm=4.320488452911377, loss=0.5516613125801086
I0302 06:13:13.209575 140409837700864 logging_writer.py:48] [505400] global_step=505400, grad_norm=4.42149543762207, loss=0.6326093673706055
I0302 06:13:46.855507 140409846093568 logging_writer.py:48] [505500] global_step=505500, grad_norm=4.8096208572387695, loss=0.6365220546722412
I0302 06:14:08.564395 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:14:14.652427 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:14:23.099019 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:14:25.408792 140573303715648 submission_runner.py:411] Time since start: 176389.40s, 	Step: 505566, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.14602042734622955, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.052260160446167, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8182252645492554, 'test/num_examples': 10000, 'score': 170399.8674080372, 'total_duration': 176389.4013133049, 'accumulated_submission_time': 170399.8674080372, 'accumulated_eval_time': 5944.358859062195, 'accumulated_logging_time': 25.19463801383972}
I0302 06:14:25.517856 140407379851008 logging_writer.py:48] [505566] accumulated_eval_time=5944.358859, accumulated_logging_time=25.194638, accumulated_submission_time=170399.867408, global_step=505566, preemption_count=0, score=170399.867408, test/accuracy=0.632000, test/loss=1.818225, test/num_examples=10000, total_duration=176389.401313, train/accuracy=0.961177, train/loss=0.146020, validation/accuracy=0.755820, validation/loss=1.052260, validation/num_examples=50000
I0302 06:14:37.295280 140408319375104 logging_writer.py:48] [505600] global_step=505600, grad_norm=4.375644683837891, loss=0.5686265826225281
I0302 06:15:10.944103 140407379851008 logging_writer.py:48] [505700] global_step=505700, grad_norm=4.304393291473389, loss=0.6551589369773865
I0302 06:15:44.661681 140408319375104 logging_writer.py:48] [505800] global_step=505800, grad_norm=4.375067234039307, loss=0.64866042137146
I0302 06:16:18.354960 140407379851008 logging_writer.py:48] [505900] global_step=505900, grad_norm=5.2392096519470215, loss=0.6340671181678772
I0302 06:16:52.059903 140408319375104 logging_writer.py:48] [506000] global_step=506000, grad_norm=5.067323684692383, loss=0.6060630083084106
I0302 06:17:25.773519 140407379851008 logging_writer.py:48] [506100] global_step=506100, grad_norm=3.7732694149017334, loss=0.5446971654891968
I0302 06:17:59.546894 140408319375104 logging_writer.py:48] [506200] global_step=506200, grad_norm=4.150326728820801, loss=0.5382144451141357
I0302 06:18:33.214671 140407379851008 logging_writer.py:48] [506300] global_step=506300, grad_norm=4.6083173751831055, loss=0.6059499979019165
I0302 06:19:06.917263 140408319375104 logging_writer.py:48] [506400] global_step=506400, grad_norm=4.706943511962891, loss=0.6371243596076965
I0302 06:19:40.633284 140407379851008 logging_writer.py:48] [506500] global_step=506500, grad_norm=4.258624076843262, loss=0.6111586689949036
I0302 06:20:14.289678 140408319375104 logging_writer.py:48] [506600] global_step=506600, grad_norm=4.719374656677246, loss=0.6046823263168335
I0302 06:20:48.070173 140407379851008 logging_writer.py:48] [506700] global_step=506700, grad_norm=4.584280014038086, loss=0.5575005412101746
I0302 06:21:21.742813 140408319375104 logging_writer.py:48] [506800] global_step=506800, grad_norm=4.675513744354248, loss=0.5918807983398438
I0302 06:21:55.446758 140407379851008 logging_writer.py:48] [506900] global_step=506900, grad_norm=4.435705661773682, loss=0.5886558890342712
I0302 06:22:29.163556 140408319375104 logging_writer.py:48] [507000] global_step=507000, grad_norm=4.028313636779785, loss=0.5355832576751709
I0302 06:22:55.682805 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:23:01.991337 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:23:10.607764 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:23:13.220788 140573303715648 submission_runner.py:411] Time since start: 176917.21s, 	Step: 507080, 	{'train/accuracy': 0.9620934128761292, 'train/loss': 0.14316003024578094, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.0530349016189575, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8179951906204224, 'test/num_examples': 10000, 'score': 170909.96117901802, 'total_duration': 176917.21332883835, 'accumulated_submission_time': 170909.96117901802, 'accumulated_eval_time': 5961.896793842316, 'accumulated_logging_time': 25.31332540512085}
I0302 06:23:13.302967 140409846093568 logging_writer.py:48] [507080] accumulated_eval_time=5961.896794, accumulated_logging_time=25.313325, accumulated_submission_time=170909.961179, global_step=507080, preemption_count=0, score=170909.961179, test/accuracy=0.630100, test/loss=1.817995, test/num_examples=10000, total_duration=176917.213329, train/accuracy=0.962093, train/loss=0.143160, validation/accuracy=0.755500, validation/loss=1.053035, validation/num_examples=50000
I0302 06:23:20.395606 140409854486272 logging_writer.py:48] [507100] global_step=507100, grad_norm=4.483187198638916, loss=0.595747709274292
I0302 06:23:54.031819 140409846093568 logging_writer.py:48] [507200] global_step=507200, grad_norm=4.62738561630249, loss=0.5896428823471069
I0302 06:24:27.776504 140409854486272 logging_writer.py:48] [507300] global_step=507300, grad_norm=4.5406317710876465, loss=0.5773488283157349
I0302 06:25:01.478528 140409846093568 logging_writer.py:48] [507400] global_step=507400, grad_norm=4.371722221374512, loss=0.6687295436859131
I0302 06:25:35.147440 140409854486272 logging_writer.py:48] [507500] global_step=507500, grad_norm=4.402944087982178, loss=0.6077098846435547
I0302 06:26:08.795166 140409846093568 logging_writer.py:48] [507600] global_step=507600, grad_norm=4.783841609954834, loss=0.6478161215782166
I0302 06:26:42.477330 140409854486272 logging_writer.py:48] [507700] global_step=507700, grad_norm=4.149390697479248, loss=0.5802260637283325
I0302 06:27:16.215265 140409846093568 logging_writer.py:48] [507800] global_step=507800, grad_norm=4.828892230987549, loss=0.6733382940292358
I0302 06:27:49.938492 140409854486272 logging_writer.py:48] [507900] global_step=507900, grad_norm=4.324519157409668, loss=0.6290426254272461
I0302 06:28:23.604222 140409846093568 logging_writer.py:48] [508000] global_step=508000, grad_norm=4.684487342834473, loss=0.7051950693130493
I0302 06:28:57.309246 140409854486272 logging_writer.py:48] [508100] global_step=508100, grad_norm=4.320737361907959, loss=0.5505259037017822
I0302 06:29:30.985328 140409846093568 logging_writer.py:48] [508200] global_step=508200, grad_norm=4.6802239418029785, loss=0.6693848967552185
I0302 06:30:04.827517 140409854486272 logging_writer.py:48] [508300] global_step=508300, grad_norm=4.266772270202637, loss=0.540779709815979
I0302 06:30:38.554753 140409846093568 logging_writer.py:48] [508400] global_step=508400, grad_norm=4.069901466369629, loss=0.5388972759246826
I0302 06:31:12.288391 140409854486272 logging_writer.py:48] [508500] global_step=508500, grad_norm=4.138808727264404, loss=0.5918644666671753
I0302 06:31:43.369829 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:31:49.475898 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:31:58.012032 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:32:00.349444 140573303715648 submission_runner.py:411] Time since start: 177444.34s, 	Step: 508594, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.1459125131368637, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0524475574493408, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8188080787658691, 'test/num_examples': 10000, 'score': 171419.9574327469, 'total_duration': 177444.34196448326, 'accumulated_submission_time': 171419.9574327469, 'accumulated_eval_time': 5978.876355171204, 'accumulated_logging_time': 25.40578317642212}
I0302 06:32:00.450703 140408319375104 logging_writer.py:48] [508594] accumulated_eval_time=5978.876355, accumulated_logging_time=25.405783, accumulated_submission_time=171419.957433, global_step=508594, preemption_count=0, score=171419.957433, test/accuracy=0.630500, test/loss=1.818808, test/num_examples=10000, total_duration=177444.341964, train/accuracy=0.961276, train/loss=0.145913, validation/accuracy=0.755940, validation/loss=1.052448, validation/num_examples=50000
I0302 06:32:02.824757 140409124681472 logging_writer.py:48] [508600] global_step=508600, grad_norm=4.670553207397461, loss=0.7282026410102844
I0302 06:32:36.535857 140408319375104 logging_writer.py:48] [508700] global_step=508700, grad_norm=4.554744243621826, loss=0.6034952402114868
I0302 06:33:10.214597 140409124681472 logging_writer.py:48] [508800] global_step=508800, grad_norm=4.631964683532715, loss=0.558030366897583
I0302 06:33:43.885667 140408319375104 logging_writer.py:48] [508900] global_step=508900, grad_norm=4.197390556335449, loss=0.5779127478599548
I0302 06:34:17.587570 140409124681472 logging_writer.py:48] [509000] global_step=509000, grad_norm=4.3677191734313965, loss=0.6835370659828186
I0302 06:34:51.289207 140408319375104 logging_writer.py:48] [509100] global_step=509100, grad_norm=4.354681015014648, loss=0.619998037815094
I0302 06:35:24.988678 140409124681472 logging_writer.py:48] [509200] global_step=509200, grad_norm=4.543273448944092, loss=0.6630508899688721
I0302 06:35:58.633538 140408319375104 logging_writer.py:48] [509300] global_step=509300, grad_norm=4.492743015289307, loss=0.683501124382019
I0302 06:36:32.512770 140409124681472 logging_writer.py:48] [509400] global_step=509400, grad_norm=4.233587265014648, loss=0.5294221043586731
I0302 06:37:06.202892 140408319375104 logging_writer.py:48] [509500] global_step=509500, grad_norm=4.422802925109863, loss=0.6070393323898315
I0302 06:37:39.922356 140409124681472 logging_writer.py:48] [509600] global_step=509600, grad_norm=4.659510135650635, loss=0.6786564588546753
I0302 06:38:13.575823 140408319375104 logging_writer.py:48] [509700] global_step=509700, grad_norm=4.381435394287109, loss=0.6375404596328735
I0302 06:38:47.248948 140409124681472 logging_writer.py:48] [509800] global_step=509800, grad_norm=4.512608528137207, loss=0.6258767247200012
I0302 06:39:20.961614 140408319375104 logging_writer.py:48] [509900] global_step=509900, grad_norm=4.414112091064453, loss=0.579289436340332
I0302 06:39:54.629244 140409124681472 logging_writer.py:48] [510000] global_step=510000, grad_norm=4.693562984466553, loss=0.609481692314148
I0302 06:40:28.331579 140408319375104 logging_writer.py:48] [510100] global_step=510100, grad_norm=4.74397611618042, loss=0.681704044342041
I0302 06:40:30.506165 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:40:36.721242 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:40:45.178056 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:40:47.438549 140573303715648 submission_runner.py:411] Time since start: 177971.43s, 	Step: 510108, 	{'train/accuracy': 0.9589046239852905, 'train/loss': 0.1507239043712616, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.0527056455612183, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8177225589752197, 'test/num_examples': 10000, 'score': 171929.94116711617, 'total_duration': 177971.4310863018, 'accumulated_submission_time': 171929.94116711617, 'accumulated_eval_time': 5995.808697462082, 'accumulated_logging_time': 25.51852774620056}
I0302 06:40:47.540927 140409854486272 logging_writer.py:48] [510108] accumulated_eval_time=5995.808697, accumulated_logging_time=25.518528, accumulated_submission_time=171929.941167, global_step=510108, preemption_count=0, score=171929.941167, test/accuracy=0.630900, test/loss=1.817723, test/num_examples=10000, total_duration=177971.431086, train/accuracy=0.958905, train/loss=0.150724, validation/accuracy=0.755500, validation/loss=1.052706, validation/num_examples=50000
I0302 06:41:18.811133 140409862878976 logging_writer.py:48] [510200] global_step=510200, grad_norm=5.061990261077881, loss=0.7171368598937988
I0302 06:41:52.459431 140409854486272 logging_writer.py:48] [510300] global_step=510300, grad_norm=4.334343433380127, loss=0.5136623382568359
I0302 06:42:26.141828 140409862878976 logging_writer.py:48] [510400] global_step=510400, grad_norm=4.485628604888916, loss=0.6423893570899963
I0302 06:42:59.952713 140409854486272 logging_writer.py:48] [510500] global_step=510500, grad_norm=4.40966272354126, loss=0.6527185440063477
I0302 06:43:33.612952 140409862878976 logging_writer.py:48] [510600] global_step=510600, grad_norm=4.604453086853027, loss=0.6318466067314148
I0302 06:44:07.297320 140409854486272 logging_writer.py:48] [510700] global_step=510700, grad_norm=4.434474945068359, loss=0.5265069603919983
I0302 06:44:41.011501 140409862878976 logging_writer.py:48] [510800] global_step=510800, grad_norm=4.372089862823486, loss=0.6063648462295532
I0302 06:45:14.681392 140409854486272 logging_writer.py:48] [510900] global_step=510900, grad_norm=4.365402698516846, loss=0.564108669757843
I0302 06:45:48.398072 140409862878976 logging_writer.py:48] [511000] global_step=511000, grad_norm=4.463191986083984, loss=0.5953696370124817
I0302 06:46:22.072822 140409854486272 logging_writer.py:48] [511100] global_step=511100, grad_norm=4.826702117919922, loss=0.6881204843521118
I0302 06:46:55.727671 140409862878976 logging_writer.py:48] [511200] global_step=511200, grad_norm=4.128010272979736, loss=0.6016800403594971
I0302 06:47:29.389133 140409854486272 logging_writer.py:48] [511300] global_step=511300, grad_norm=4.469057083129883, loss=0.6021687984466553
I0302 06:48:03.119986 140409862878976 logging_writer.py:48] [511400] global_step=511400, grad_norm=4.719821929931641, loss=0.6738677024841309
I0302 06:48:36.899648 140409854486272 logging_writer.py:48] [511500] global_step=511500, grad_norm=4.485255241394043, loss=0.6177895665168762
I0302 06:49:10.561951 140409862878976 logging_writer.py:48] [511600] global_step=511600, grad_norm=4.136076927185059, loss=0.559818685054779
I0302 06:49:17.439018 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:49:23.540315 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:49:32.135346 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:49:34.429598 140573303715648 submission_runner.py:411] Time since start: 178498.42s, 	Step: 511622, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.1448870450258255, 'validation/accuracy': 0.7563799619674683, 'validation/loss': 1.051182508468628, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8169662952423096, 'test/num_examples': 10000, 'score': 172439.7688140869, 'total_duration': 178498.42212200165, 'accumulated_submission_time': 172439.7688140869, 'accumulated_eval_time': 6012.799216747284, 'accumulated_logging_time': 25.631613731384277}
I0302 06:49:34.537271 140409124681472 logging_writer.py:48] [511622] accumulated_eval_time=6012.799217, accumulated_logging_time=25.631614, accumulated_submission_time=172439.768814, global_step=511622, preemption_count=0, score=172439.768814, test/accuracy=0.630800, test/loss=1.816966, test/num_examples=10000, total_duration=178498.422122, train/accuracy=0.961416, train/loss=0.144887, validation/accuracy=0.756380, validation/loss=1.051183, validation/num_examples=50000
I0302 06:50:01.181956 140409829308160 logging_writer.py:48] [511700] global_step=511700, grad_norm=4.702733039855957, loss=0.618811309337616
I0302 06:50:34.833513 140409124681472 logging_writer.py:48] [511800] global_step=511800, grad_norm=4.46765661239624, loss=0.6489570140838623
I0302 06:51:08.550683 140409829308160 logging_writer.py:48] [511900] global_step=511900, grad_norm=3.895094156265259, loss=0.60186767578125
I0302 06:51:42.226814 140409124681472 logging_writer.py:48] [512000] global_step=512000, grad_norm=4.472507476806641, loss=0.6794966459274292
I0302 06:52:15.899446 140409829308160 logging_writer.py:48] [512100] global_step=512100, grad_norm=4.305386066436768, loss=0.5941798686981201
I0302 06:52:49.605862 140409124681472 logging_writer.py:48] [512200] global_step=512200, grad_norm=4.28322172164917, loss=0.6272802948951721
I0302 06:53:23.287943 140409829308160 logging_writer.py:48] [512300] global_step=512300, grad_norm=4.543117523193359, loss=0.6492615938186646
I0302 06:53:57.028894 140409124681472 logging_writer.py:48] [512400] global_step=512400, grad_norm=4.077219486236572, loss=0.5781220197677612
I0302 06:54:30.676293 140409829308160 logging_writer.py:48] [512500] global_step=512500, grad_norm=4.376976013183594, loss=0.6509538888931274
I0302 06:55:04.580517 140409124681472 logging_writer.py:48] [512600] global_step=512600, grad_norm=4.529862403869629, loss=0.628204882144928
I0302 06:55:38.251252 140409829308160 logging_writer.py:48] [512700] global_step=512700, grad_norm=4.486134052276611, loss=0.669181764125824
I0302 06:56:11.984493 140409124681472 logging_writer.py:48] [512800] global_step=512800, grad_norm=4.613148212432861, loss=0.6263405084609985
I0302 06:56:45.680707 140409829308160 logging_writer.py:48] [512900] global_step=512900, grad_norm=4.298247814178467, loss=0.6105581521987915
I0302 06:57:19.338959 140409124681472 logging_writer.py:48] [513000] global_step=513000, grad_norm=4.930253028869629, loss=0.6898163557052612
I0302 06:57:53.108472 140409829308160 logging_writer.py:48] [513100] global_step=513100, grad_norm=4.378836154937744, loss=0.5668555498123169
I0302 06:58:04.709319 140573303715648 spec.py:321] Evaluating on the training split.
I0302 06:58:10.797468 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 06:58:19.384242 140573303715648 spec.py:349] Evaluating on the test split.
I0302 06:58:21.708860 140573303715648 submission_runner.py:411] Time since start: 179025.70s, 	Step: 513136, 	{'train/accuracy': 0.9617745280265808, 'train/loss': 0.14499138295650482, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0533603429794312, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8187692165374756, 'test/num_examples': 10000, 'score': 172949.87129354477, 'total_duration': 179025.70138788223, 'accumulated_submission_time': 172949.87129354477, 'accumulated_eval_time': 6029.798710823059, 'accumulated_logging_time': 25.749578952789307}
I0302 06:58:21.808753 140408319375104 logging_writer.py:48] [513136] accumulated_eval_time=6029.798711, accumulated_logging_time=25.749579, accumulated_submission_time=172949.871294, global_step=513136, preemption_count=0, score=172949.871294, test/accuracy=0.630700, test/loss=1.818769, test/num_examples=10000, total_duration=179025.701388, train/accuracy=0.961775, train/loss=0.144991, validation/accuracy=0.755720, validation/loss=1.053360, validation/num_examples=50000
I0302 06:58:43.678441 140409846093568 logging_writer.py:48] [513200] global_step=513200, grad_norm=4.36025857925415, loss=0.6215489506721497
I0302 06:59:17.323549 140408319375104 logging_writer.py:48] [513300] global_step=513300, grad_norm=4.574991226196289, loss=0.6283450126647949
I0302 06:59:51.010447 140409846093568 logging_writer.py:48] [513400] global_step=513400, grad_norm=3.957653522491455, loss=0.5502985715866089
I0302 07:00:24.748470 140408319375104 logging_writer.py:48] [513500] global_step=513500, grad_norm=4.340128421783447, loss=0.5794183015823364
I0302 07:00:58.566448 140409846093568 logging_writer.py:48] [513600] global_step=513600, grad_norm=4.431148529052734, loss=0.6224251985549927
I0302 07:01:32.241803 140408319375104 logging_writer.py:48] [513700] global_step=513700, grad_norm=4.259811878204346, loss=0.6004636287689209
I0302 07:02:05.982637 140409846093568 logging_writer.py:48] [513800] global_step=513800, grad_norm=4.575557231903076, loss=0.6855809688568115
I0302 07:02:39.650721 140408319375104 logging_writer.py:48] [513900] global_step=513900, grad_norm=5.223268032073975, loss=0.6593174934387207
I0302 07:03:13.281729 140409846093568 logging_writer.py:48] [514000] global_step=514000, grad_norm=4.271974086761475, loss=0.6103082299232483
I0302 07:03:46.996955 140408319375104 logging_writer.py:48] [514100] global_step=514100, grad_norm=4.4747395515441895, loss=0.5870552659034729
I0302 07:04:20.674154 140409846093568 logging_writer.py:48] [514200] global_step=514200, grad_norm=4.9921183586120605, loss=0.6408255100250244
I0302 07:04:54.353804 140408319375104 logging_writer.py:48] [514300] global_step=514300, grad_norm=4.754274368286133, loss=0.6195809841156006
I0302 07:05:28.113580 140409846093568 logging_writer.py:48] [514400] global_step=514400, grad_norm=5.081371784210205, loss=0.6355940699577332
I0302 07:06:01.815927 140408319375104 logging_writer.py:48] [514500] global_step=514500, grad_norm=4.342192649841309, loss=0.6770216226577759
I0302 07:06:35.550652 140409846093568 logging_writer.py:48] [514600] global_step=514600, grad_norm=4.355984687805176, loss=0.5941094160079956
I0302 07:06:51.845580 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:06:57.906490 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:07:06.741002 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:07:09.403378 140573303715648 submission_runner.py:411] Time since start: 179553.40s, 	Step: 514650, 	{'train/accuracy': 0.9599210619926453, 'train/loss': 0.14730440080165863, 'validation/accuracy': 0.7555199861526489, 'validation/loss': 1.0519886016845703, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.817663311958313, 'test/num_examples': 10000, 'score': 173459.8376841545, 'total_duration': 179553.39589834213, 'accumulated_submission_time': 173459.8376841545, 'accumulated_eval_time': 6047.356439828873, 'accumulated_logging_time': 25.85953950881958}
I0302 07:07:09.506614 140409837700864 logging_writer.py:48] [514650] accumulated_eval_time=6047.356440, accumulated_logging_time=25.859540, accumulated_submission_time=173459.837684, global_step=514650, preemption_count=0, score=173459.837684, test/accuracy=0.631200, test/loss=1.817663, test/num_examples=10000, total_duration=179553.395898, train/accuracy=0.959921, train/loss=0.147304, validation/accuracy=0.755520, validation/loss=1.051989, validation/num_examples=50000
I0302 07:07:26.690925 140409854486272 logging_writer.py:48] [514700] global_step=514700, grad_norm=4.339693546295166, loss=0.614815890789032
I0302 07:08:00.391146 140409837700864 logging_writer.py:48] [514800] global_step=514800, grad_norm=4.948581695556641, loss=0.6499742269515991
I0302 07:08:34.052041 140409854486272 logging_writer.py:48] [514900] global_step=514900, grad_norm=4.847719669342041, loss=0.5766772627830505
I0302 07:09:07.723841 140409837700864 logging_writer.py:48] [515000] global_step=515000, grad_norm=4.970109462738037, loss=0.6771309971809387
I0302 07:09:41.455195 140409854486272 logging_writer.py:48] [515100] global_step=515100, grad_norm=4.250672340393066, loss=0.5642486810684204
I0302 07:10:15.138442 140409837700864 logging_writer.py:48] [515200] global_step=515200, grad_norm=4.8686065673828125, loss=0.6154714226722717
I0302 07:10:48.871506 140409854486272 logging_writer.py:48] [515300] global_step=515300, grad_norm=4.709255695343018, loss=0.6629934310913086
I0302 07:11:22.538865 140409837700864 logging_writer.py:48] [515400] global_step=515400, grad_norm=4.095822334289551, loss=0.6051510572433472
I0302 07:11:56.278645 140409854486272 logging_writer.py:48] [515500] global_step=515500, grad_norm=4.908012866973877, loss=0.67292720079422
I0302 07:12:29.987096 140409837700864 logging_writer.py:48] [515600] global_step=515600, grad_norm=4.571401596069336, loss=0.6259649991989136
I0302 07:13:03.730405 140409854486272 logging_writer.py:48] [515700] global_step=515700, grad_norm=4.304989337921143, loss=0.54594886302948
I0302 07:13:37.557924 140409837700864 logging_writer.py:48] [515800] global_step=515800, grad_norm=4.172877311706543, loss=0.5622982978820801
I0302 07:14:11.272530 140409854486272 logging_writer.py:48] [515900] global_step=515900, grad_norm=4.575997829437256, loss=0.6363776922225952
I0302 07:14:45.043454 140409837700864 logging_writer.py:48] [516000] global_step=516000, grad_norm=4.403698921203613, loss=0.6548743844032288
I0302 07:15:18.741881 140409854486272 logging_writer.py:48] [516100] global_step=516100, grad_norm=4.354910850524902, loss=0.5754122734069824
I0302 07:15:39.455347 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:15:45.585347 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:15:54.060038 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:15:56.453086 140573303715648 submission_runner.py:411] Time since start: 180080.45s, 	Step: 516163, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.1467035561800003, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0525480508804321, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8186434507369995, 'test/num_examples': 10000, 'score': 173969.71770739555, 'total_duration': 180080.4456114769, 'accumulated_submission_time': 173969.71770739555, 'accumulated_eval_time': 6064.35414147377, 'accumulated_logging_time': 25.972986936569214}
I0302 07:15:56.556092 140408319375104 logging_writer.py:48] [516163] accumulated_eval_time=6064.354141, accumulated_logging_time=25.972987, accumulated_submission_time=173969.717707, global_step=516163, preemption_count=0, score=173969.717707, test/accuracy=0.630700, test/loss=1.818643, test/num_examples=10000, total_duration=180080.445611, train/accuracy=0.960898, train/loss=0.146704, validation/accuracy=0.755580, validation/loss=1.052548, validation/num_examples=50000
I0302 07:16:09.365437 140409124681472 logging_writer.py:48] [516200] global_step=516200, grad_norm=4.468445301055908, loss=0.5881272554397583
I0302 07:16:43.040575 140408319375104 logging_writer.py:48] [516300] global_step=516300, grad_norm=4.2744669914245605, loss=0.6446689367294312
I0302 07:17:16.687389 140409124681472 logging_writer.py:48] [516400] global_step=516400, grad_norm=4.27706241607666, loss=0.5818384289741516
I0302 07:17:50.356656 140408319375104 logging_writer.py:48] [516500] global_step=516500, grad_norm=4.5851287841796875, loss=0.613904595375061
I0302 07:18:24.075865 140409124681472 logging_writer.py:48] [516600] global_step=516600, grad_norm=4.034809589385986, loss=0.5959485173225403
I0302 07:18:57.744220 140408319375104 logging_writer.py:48] [516700] global_step=516700, grad_norm=4.532514572143555, loss=0.6670043468475342
I0302 07:19:31.500128 140409124681472 logging_writer.py:48] [516800] global_step=516800, grad_norm=5.016389846801758, loss=0.6409915089607239
I0302 07:20:05.174633 140408319375104 logging_writer.py:48] [516900] global_step=516900, grad_norm=4.433694839477539, loss=0.5500425696372986
I0302 07:20:38.858599 140409124681472 logging_writer.py:48] [517000] global_step=517000, grad_norm=4.415719985961914, loss=0.6786707043647766
I0302 07:21:12.598539 140408319375104 logging_writer.py:48] [517100] global_step=517100, grad_norm=4.314918041229248, loss=0.5979890823364258
I0302 07:21:46.309107 140409124681472 logging_writer.py:48] [517200] global_step=517200, grad_norm=4.381552696228027, loss=0.6132618188858032
I0302 07:22:20.046967 140408319375104 logging_writer.py:48] [517300] global_step=517300, grad_norm=4.78808069229126, loss=0.6160593628883362
I0302 07:22:53.736727 140409124681472 logging_writer.py:48] [517400] global_step=517400, grad_norm=4.967336177825928, loss=0.6457533240318298
I0302 07:23:27.487724 140408319375104 logging_writer.py:48] [517500] global_step=517500, grad_norm=4.212600231170654, loss=0.6635777354240417
I0302 07:24:01.168650 140409124681472 logging_writer.py:48] [517600] global_step=517600, grad_norm=5.122211933135986, loss=0.6866860389709473
I0302 07:24:26.631281 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:24:32.712175 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:24:41.230802 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:24:43.584551 140573303715648 submission_runner.py:411] Time since start: 180607.58s, 	Step: 517677, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14729200303554535, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0521091222763062, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.816991925239563, 'test/num_examples': 10000, 'score': 174479.7213087082, 'total_duration': 180607.57706212997, 'accumulated_submission_time': 174479.7213087082, 'accumulated_eval_time': 6081.307334423065, 'accumulated_logging_time': 26.08626389503479}
I0302 07:24:43.684758 140407371458304 logging_writer.py:48] [517677] accumulated_eval_time=6081.307334, accumulated_logging_time=26.086264, accumulated_submission_time=174479.721309, global_step=517677, preemption_count=0, score=174479.721309, test/accuracy=0.631900, test/loss=1.816992, test/num_examples=10000, total_duration=180607.577062, train/accuracy=0.960599, train/loss=0.147292, validation/accuracy=0.755640, validation/loss=1.052109, validation/num_examples=50000
I0302 07:24:51.775169 140407379851008 logging_writer.py:48] [517700] global_step=517700, grad_norm=4.244962215423584, loss=0.5539774894714355
I0302 07:25:25.464901 140407371458304 logging_writer.py:48] [517800] global_step=517800, grad_norm=4.210844993591309, loss=0.572587251663208
I0302 07:25:59.269397 140407379851008 logging_writer.py:48] [517900] global_step=517900, grad_norm=4.641289234161377, loss=0.6197799444198608
I0302 07:26:32.977916 140407371458304 logging_writer.py:48] [518000] global_step=518000, grad_norm=4.451864719390869, loss=0.6419937610626221
I0302 07:27:06.713665 140407379851008 logging_writer.py:48] [518100] global_step=518100, grad_norm=4.249905586242676, loss=0.593076229095459
I0302 07:27:40.379185 140407371458304 logging_writer.py:48] [518200] global_step=518200, grad_norm=4.222280025482178, loss=0.5918195843696594
I0302 07:28:14.024387 140407379851008 logging_writer.py:48] [518300] global_step=518300, grad_norm=4.408295154571533, loss=0.6175521612167358
I0302 07:28:47.729905 140407371458304 logging_writer.py:48] [518400] global_step=518400, grad_norm=4.694777488708496, loss=0.5929784774780273
I0302 07:29:21.401487 140407379851008 logging_writer.py:48] [518500] global_step=518500, grad_norm=4.529763698577881, loss=0.6490223407745361
I0302 07:29:55.119913 140407371458304 logging_writer.py:48] [518600] global_step=518600, grad_norm=4.102594375610352, loss=0.6801097393035889
I0302 07:30:28.798532 140407379851008 logging_writer.py:48] [518700] global_step=518700, grad_norm=4.014031887054443, loss=0.5326915383338928
I0302 07:31:02.530726 140407371458304 logging_writer.py:48] [518800] global_step=518800, grad_norm=4.65740966796875, loss=0.6664865016937256
I0302 07:31:36.278887 140407379851008 logging_writer.py:48] [518900] global_step=518900, grad_norm=4.515151023864746, loss=0.5911479592323303
I0302 07:32:10.037611 140407371458304 logging_writer.py:48] [519000] global_step=519000, grad_norm=4.3863701820373535, loss=0.634004533290863
I0302 07:32:43.737721 140407379851008 logging_writer.py:48] [519100] global_step=519100, grad_norm=4.768811225891113, loss=0.6769261360168457
I0302 07:33:13.824532 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:33:19.959473 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:33:28.408684 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:33:30.678351 140573303715648 submission_runner.py:411] Time since start: 181134.67s, 	Step: 519191, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1465890109539032, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0526906251907349, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8196641206741333, 'test/num_examples': 10000, 'score': 174989.78865027428, 'total_duration': 181134.6708905697, 'accumulated_submission_time': 174989.78865027428, 'accumulated_eval_time': 6098.161105394363, 'accumulated_logging_time': 26.196840047836304}
I0302 07:33:30.780232 140409124681472 logging_writer.py:48] [519191] accumulated_eval_time=6098.161105, accumulated_logging_time=26.196840, accumulated_submission_time=174989.788650, global_step=519191, preemption_count=0, score=174989.788650, test/accuracy=0.630600, test/loss=1.819664, test/num_examples=10000, total_duration=181134.670891, train/accuracy=0.960479, train/loss=0.146589, validation/accuracy=0.755680, validation/loss=1.052691, validation/num_examples=50000
I0302 07:33:34.156353 140409829308160 logging_writer.py:48] [519200] global_step=519200, grad_norm=4.369040012359619, loss=0.6242605447769165
I0302 07:34:07.794620 140409124681472 logging_writer.py:48] [519300] global_step=519300, grad_norm=4.314729690551758, loss=0.6568284034729004
I0302 07:34:41.545153 140409829308160 logging_writer.py:48] [519400] global_step=519400, grad_norm=4.203736305236816, loss=0.610315203666687
I0302 07:35:15.233911 140409124681472 logging_writer.py:48] [519500] global_step=519500, grad_norm=4.614819049835205, loss=0.5972329378128052
I0302 07:35:48.935133 140409829308160 logging_writer.py:48] [519600] global_step=519600, grad_norm=4.553845405578613, loss=0.660475492477417
I0302 07:36:22.632411 140409124681472 logging_writer.py:48] [519700] global_step=519700, grad_norm=4.997896671295166, loss=0.6278791427612305
I0302 07:36:56.399621 140409829308160 logging_writer.py:48] [519800] global_step=519800, grad_norm=4.586429595947266, loss=0.6360253691673279
I0302 07:37:30.078705 140409124681472 logging_writer.py:48] [519900] global_step=519900, grad_norm=4.150307655334473, loss=0.5608378648757935
I0302 07:38:04.008332 140409829308160 logging_writer.py:48] [520000] global_step=520000, grad_norm=4.3657965660095215, loss=0.596759557723999
I0302 07:38:37.687942 140409124681472 logging_writer.py:48] [520100] global_step=520100, grad_norm=5.869483470916748, loss=0.7120957970619202
I0302 07:39:11.461818 140409829308160 logging_writer.py:48] [520200] global_step=520200, grad_norm=4.565841197967529, loss=0.5976677536964417
I0302 07:39:45.147844 140409124681472 logging_writer.py:48] [520300] global_step=520300, grad_norm=4.380802154541016, loss=0.6508781909942627
I0302 07:40:18.923171 140409829308160 logging_writer.py:48] [520400] global_step=520400, grad_norm=4.456153869628906, loss=0.5830103158950806
I0302 07:40:52.593122 140409124681472 logging_writer.py:48] [520500] global_step=520500, grad_norm=4.817500591278076, loss=0.7296916246414185
I0302 07:41:26.370973 140409829308160 logging_writer.py:48] [520600] global_step=520600, grad_norm=4.5047712326049805, loss=0.5808374881744385
I0302 07:42:00.044958 140409124681472 logging_writer.py:48] [520700] global_step=520700, grad_norm=4.239752292633057, loss=0.6594426035881042
I0302 07:42:00.870033 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:42:07.050040 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:42:15.592667 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:42:17.862426 140573303715648 submission_runner.py:411] Time since start: 181661.85s, 	Step: 520704, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14867927134037018, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0523520708084106, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8196110725402832, 'test/num_examples': 10000, 'score': 175499.8068125248, 'total_duration': 181661.85493016243, 'accumulated_submission_time': 175499.8068125248, 'accumulated_eval_time': 6115.153409481049, 'accumulated_logging_time': 26.310802698135376}
I0302 07:42:17.968220 140407379851008 logging_writer.py:48] [520704] accumulated_eval_time=6115.153409, accumulated_logging_time=26.310803, accumulated_submission_time=175499.806813, global_step=520704, preemption_count=0, score=175499.806813, test/accuracy=0.630900, test/loss=1.819611, test/num_examples=10000, total_duration=181661.854930, train/accuracy=0.960001, train/loss=0.148679, validation/accuracy=0.756040, validation/loss=1.052352, validation/num_examples=50000
I0302 07:42:50.665501 140408319375104 logging_writer.py:48] [520800] global_step=520800, grad_norm=4.666401386260986, loss=0.678343653678894
I0302 07:43:24.321940 140407379851008 logging_writer.py:48] [520900] global_step=520900, grad_norm=4.5768256187438965, loss=0.5908803939819336
I0302 07:43:58.009462 140408319375104 logging_writer.py:48] [521000] global_step=521000, grad_norm=4.995455741882324, loss=0.6749489307403564
I0302 07:44:31.816523 140407379851008 logging_writer.py:48] [521100] global_step=521100, grad_norm=4.444714546203613, loss=0.6180854439735413
I0302 07:45:05.544420 140408319375104 logging_writer.py:48] [521200] global_step=521200, grad_norm=5.560726642608643, loss=0.6433762907981873
I0302 07:45:39.205261 140407379851008 logging_writer.py:48] [521300] global_step=521300, grad_norm=4.086005210876465, loss=0.5102551579475403
I0302 07:46:12.920166 140408319375104 logging_writer.py:48] [521400] global_step=521400, grad_norm=4.939049243927002, loss=0.7577435970306396
I0302 07:46:46.611111 140407379851008 logging_writer.py:48] [521500] global_step=521500, grad_norm=4.896027565002441, loss=0.6254701018333435
I0302 07:47:20.330892 140408319375104 logging_writer.py:48] [521600] global_step=521600, grad_norm=4.031525611877441, loss=0.59187912940979
I0302 07:47:54.060517 140407379851008 logging_writer.py:48] [521700] global_step=521700, grad_norm=4.764165878295898, loss=0.6634482741355896
I0302 07:48:27.786322 140408319375104 logging_writer.py:48] [521800] global_step=521800, grad_norm=4.459372520446777, loss=0.6923440098762512
I0302 07:49:01.535998 140407379851008 logging_writer.py:48] [521900] global_step=521900, grad_norm=4.677670478820801, loss=0.7117519378662109
I0302 07:49:35.247614 140408319375104 logging_writer.py:48] [522000] global_step=522000, grad_norm=4.842355251312256, loss=0.6246635317802429
I0302 07:50:08.989398 140407379851008 logging_writer.py:48] [522100] global_step=522100, grad_norm=4.339913845062256, loss=0.5844239592552185
I0302 07:50:42.883029 140408319375104 logging_writer.py:48] [522200] global_step=522200, grad_norm=4.658407211303711, loss=0.6066722273826599
I0302 07:50:48.098786 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:50:54.199722 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:51:02.817513 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:51:05.137911 140573303715648 submission_runner.py:411] Time since start: 182189.13s, 	Step: 522217, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.1452644020318985, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0519838333129883, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8193875551223755, 'test/num_examples': 10000, 'score': 176009.86840701103, 'total_duration': 182189.1304523945, 'accumulated_submission_time': 176009.86840701103, 'accumulated_eval_time': 6132.192498922348, 'accumulated_logging_time': 26.426344871520996}
I0302 07:51:05.242923 140407379851008 logging_writer.py:48] [522217] accumulated_eval_time=6132.192499, accumulated_logging_time=26.426345, accumulated_submission_time=176009.868407, global_step=522217, preemption_count=0, score=176009.868407, test/accuracy=0.631400, test/loss=1.819388, test/num_examples=10000, total_duration=182189.130452, train/accuracy=0.961735, train/loss=0.145264, validation/accuracy=0.756140, validation/loss=1.051984, validation/num_examples=50000
I0302 07:51:33.604517 140409829308160 logging_writer.py:48] [522300] global_step=522300, grad_norm=4.527307510375977, loss=0.5428221225738525
I0302 07:52:07.248844 140407379851008 logging_writer.py:48] [522400] global_step=522400, grad_norm=4.547388076782227, loss=0.6069961786270142
I0302 07:52:40.917725 140409829308160 logging_writer.py:48] [522500] global_step=522500, grad_norm=5.283181667327881, loss=0.6850541830062866
I0302 07:53:14.613399 140407379851008 logging_writer.py:48] [522600] global_step=522600, grad_norm=4.569841384887695, loss=0.6106759309768677
I0302 07:53:48.290068 140409829308160 logging_writer.py:48] [522700] global_step=522700, grad_norm=5.122369766235352, loss=0.6822288632392883
I0302 07:54:21.994257 140407379851008 logging_writer.py:48] [522800] global_step=522800, grad_norm=4.500458717346191, loss=0.6007042527198792
I0302 07:54:55.679222 140409829308160 logging_writer.py:48] [522900] global_step=522900, grad_norm=4.562406539916992, loss=0.6561409831047058
I0302 07:55:29.361013 140407379851008 logging_writer.py:48] [523000] global_step=523000, grad_norm=4.725458145141602, loss=0.6417714953422546
I0302 07:56:03.137173 140409829308160 logging_writer.py:48] [523100] global_step=523100, grad_norm=4.414742469787598, loss=0.5881215929985046
I0302 07:56:36.930909 140407379851008 logging_writer.py:48] [523200] global_step=523200, grad_norm=4.57517147064209, loss=0.634681224822998
I0302 07:57:10.700928 140409829308160 logging_writer.py:48] [523300] global_step=523300, grad_norm=4.816892623901367, loss=0.6697272062301636
I0302 07:57:44.385064 140407379851008 logging_writer.py:48] [523400] global_step=523400, grad_norm=4.486858367919922, loss=0.6448006629943848
I0302 07:58:18.145615 140409829308160 logging_writer.py:48] [523500] global_step=523500, grad_norm=4.286945343017578, loss=0.570597767829895
I0302 07:58:51.825384 140407379851008 logging_writer.py:48] [523600] global_step=523600, grad_norm=4.221003532409668, loss=0.5907436609268188
I0302 07:59:25.617588 140409829308160 logging_writer.py:48] [523700] global_step=523700, grad_norm=4.231590270996094, loss=0.5633784532546997
I0302 07:59:35.183133 140573303715648 spec.py:321] Evaluating on the training split.
I0302 07:59:41.283751 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 07:59:49.786837 140573303715648 spec.py:349] Evaluating on the test split.
I0302 07:59:52.094027 140573303715648 submission_runner.py:411] Time since start: 182716.09s, 	Step: 523730, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.1496659517288208, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0520333051681519, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.817821979522705, 'test/num_examples': 10000, 'score': 176519.7387931347, 'total_duration': 182716.08656191826, 'accumulated_submission_time': 176519.7387931347, 'accumulated_eval_time': 6149.103352069855, 'accumulated_logging_time': 26.54132080078125}
I0302 07:59:52.206830 140408319375104 logging_writer.py:48] [523730] accumulated_eval_time=6149.103352, accumulated_logging_time=26.541321, accumulated_submission_time=176519.738793, global_step=523730, preemption_count=0, score=176519.738793, test/accuracy=0.631700, test/loss=1.817822, test/num_examples=10000, total_duration=182716.086562, train/accuracy=0.960100, train/loss=0.149666, validation/accuracy=0.755880, validation/loss=1.052033, validation/num_examples=50000
I0302 08:00:16.108899 140409124681472 logging_writer.py:48] [523800] global_step=523800, grad_norm=4.106352806091309, loss=0.6328492164611816
I0302 08:00:49.809881 140408319375104 logging_writer.py:48] [523900] global_step=523900, grad_norm=4.147909164428711, loss=0.6088241338729858
I0302 08:01:23.483704 140409124681472 logging_writer.py:48] [524000] global_step=524000, grad_norm=4.83309268951416, loss=0.6103009581565857
I0302 08:01:57.178975 140408319375104 logging_writer.py:48] [524100] global_step=524100, grad_norm=4.412428379058838, loss=0.6163362264633179
I0302 08:02:30.870239 140409124681472 logging_writer.py:48] [524200] global_step=524200, grad_norm=4.444616794586182, loss=0.6329634785652161
I0302 08:03:04.667040 140408319375104 logging_writer.py:48] [524300] global_step=524300, grad_norm=5.226434707641602, loss=0.6110259294509888
I0302 08:03:38.419802 140409124681472 logging_writer.py:48] [524400] global_step=524400, grad_norm=4.313588619232178, loss=0.5973929762840271
I0302 08:04:12.086022 140408319375104 logging_writer.py:48] [524500] global_step=524500, grad_norm=4.642392158508301, loss=0.6918793320655823
I0302 08:04:45.826998 140409124681472 logging_writer.py:48] [524600] global_step=524600, grad_norm=5.216236114501953, loss=0.6159722805023193
I0302 08:05:19.515538 140408319375104 logging_writer.py:48] [524700] global_step=524700, grad_norm=4.505189418792725, loss=0.5913732647895813
I0302 08:05:53.280989 140409124681472 logging_writer.py:48] [524800] global_step=524800, grad_norm=4.396100044250488, loss=0.6127856373786926
I0302 08:06:26.975492 140408319375104 logging_writer.py:48] [524900] global_step=524900, grad_norm=4.524663925170898, loss=0.7184829115867615
I0302 08:07:00.640345 140409124681472 logging_writer.py:48] [525000] global_step=525000, grad_norm=4.776944637298584, loss=0.6469888687133789
I0302 08:07:34.300797 140408319375104 logging_writer.py:48] [525100] global_step=525100, grad_norm=4.7868332862854, loss=0.6808061599731445
I0302 08:08:08.062849 140409124681472 logging_writer.py:48] [525200] global_step=525200, grad_norm=4.401889801025391, loss=0.6549004912376404
I0302 08:08:22.358932 140573303715648 spec.py:321] Evaluating on the training split.
I0302 08:08:28.405969 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 08:08:36.952290 140573303715648 spec.py:349] Evaluating on the test split.
I0302 08:08:39.231468 140573303715648 submission_runner.py:411] Time since start: 183243.22s, 	Step: 525244, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14859789609909058, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0526304244995117, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8184934854507446, 'test/num_examples': 10000, 'score': 177029.82167744637, 'total_duration': 183243.22400975227, 'accumulated_submission_time': 177029.82167744637, 'accumulated_eval_time': 6165.975840806961, 'accumulated_logging_time': 26.663962841033936}
I0302 08:08:39.335711 140409846093568 logging_writer.py:48] [525244] accumulated_eval_time=6165.975841, accumulated_logging_time=26.663963, accumulated_submission_time=177029.821677, global_step=525244, preemption_count=0, score=177029.821677, test/accuracy=0.631700, test/loss=1.818493, test/num_examples=10000, total_duration=183243.224010, train/accuracy=0.960519, train/loss=0.148598, validation/accuracy=0.756040, validation/loss=1.052630, validation/num_examples=50000
I0302 08:08:58.678392 140409854486272 logging_writer.py:48] [525300] global_step=525300, grad_norm=4.687916278839111, loss=0.6379328966140747
I0302 08:09:32.328937 140409846093568 logging_writer.py:48] [525400] global_step=525400, grad_norm=4.936479091644287, loss=0.6493260264396667
I0302 08:10:05.982718 140409854486272 logging_writer.py:48] [525500] global_step=525500, grad_norm=4.364898681640625, loss=0.5884163975715637
I0302 08:10:39.670289 140409846093568 logging_writer.py:48] [525600] global_step=525600, grad_norm=4.268357276916504, loss=0.6059840321540833
I0302 08:11:13.343724 140409854486272 logging_writer.py:48] [525700] global_step=525700, grad_norm=5.098962783813477, loss=0.648038387298584
I0302 08:11:47.004595 140409846093568 logging_writer.py:48] [525800] global_step=525800, grad_norm=4.56838321685791, loss=0.6490920782089233
I0302 08:12:20.716137 140409854486272 logging_writer.py:48] [525900] global_step=525900, grad_norm=5.227877140045166, loss=0.7572343945503235
I0302 08:12:54.425788 140409846093568 logging_writer.py:48] [526000] global_step=526000, grad_norm=5.16348934173584, loss=0.6677766442298889
I0302 08:13:28.086128 140409854486272 logging_writer.py:48] [526100] global_step=526100, grad_norm=4.871039390563965, loss=0.6367071866989136
I0302 08:14:01.833537 140409846093568 logging_writer.py:48] [526200] global_step=526200, grad_norm=4.328972816467285, loss=0.6127973794937134
I0302 08:14:35.487107 140409854486272 logging_writer.py:48] [526300] global_step=526300, grad_norm=4.091131687164307, loss=0.6071801781654358
I0302 08:15:09.304094 140409846093568 logging_writer.py:48] [526400] global_step=526400, grad_norm=4.275859832763672, loss=0.6005213260650635
I0302 08:15:42.980881 140409854486272 logging_writer.py:48] [526500] global_step=526500, grad_norm=4.023512840270996, loss=0.5655173659324646
I0302 08:16:16.644075 140409846093568 logging_writer.py:48] [526600] global_step=526600, grad_norm=4.352029800415039, loss=0.6503943204879761
I0302 08:16:50.349204 140409854486272 logging_writer.py:48] [526700] global_step=526700, grad_norm=4.415680408477783, loss=0.6209934949874878
I0302 08:17:09.374364 140573303715648 spec.py:321] Evaluating on the training split.
I0302 08:17:15.468153 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 08:17:24.037341 140573303715648 spec.py:349] Evaluating on the test split.
I0302 08:17:26.345943 140573303715648 submission_runner.py:411] Time since start: 183770.34s, 	Step: 526758, 	{'train/accuracy': 0.9609375, 'train/loss': 0.1478055715560913, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.051428198814392, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8161977529525757, 'test/num_examples': 10000, 'score': 177539.7880001068, 'total_duration': 183770.3384861946, 'accumulated_submission_time': 177539.7880001068, 'accumulated_eval_time': 6182.947371721268, 'accumulated_logging_time': 26.780529737472534}
I0302 08:17:26.448436 140409124681472 logging_writer.py:48] [526758] accumulated_eval_time=6182.947372, accumulated_logging_time=26.780530, accumulated_submission_time=177539.788000, global_step=526758, preemption_count=0, score=177539.788000, test/accuracy=0.631500, test/loss=1.816198, test/num_examples=10000, total_duration=183770.338486, train/accuracy=0.960938, train/loss=0.147806, validation/accuracy=0.755660, validation/loss=1.051428, validation/num_examples=50000
I0302 08:17:40.930442 140409829308160 logging_writer.py:48] [526800] global_step=526800, grad_norm=4.682169437408447, loss=0.611892819404602
I0302 08:18:14.583578 140409124681472 logging_writer.py:48] [526900] global_step=526900, grad_norm=4.7986273765563965, loss=0.6740670800209045
I0302 08:18:48.250113 140409829308160 logging_writer.py:48] [527000] global_step=527000, grad_norm=4.577278137207031, loss=0.6548441648483276
I0302 08:19:22.005280 140409124681472 logging_writer.py:48] [527100] global_step=527100, grad_norm=3.9260547161102295, loss=0.5608978271484375
I0302 08:19:55.714628 140409829308160 logging_writer.py:48] [527200] global_step=527200, grad_norm=4.544393539428711, loss=0.5647624731063843
I0302 08:20:29.417056 140409124681472 logging_writer.py:48] [527300] global_step=527300, grad_norm=4.614749431610107, loss=0.6445336937904358
I0302 08:21:03.313499 140409829308160 logging_writer.py:48] [527400] global_step=527400, grad_norm=4.649725437164307, loss=0.6143830418586731
I0302 08:21:36.998440 140409124681472 logging_writer.py:48] [527500] global_step=527500, grad_norm=4.4679365158081055, loss=0.6389580965042114
I0302 08:22:10.677790 140409829308160 logging_writer.py:48] [527600] global_step=527600, grad_norm=5.315615653991699, loss=0.6802670359611511
I0302 08:22:44.424699 140409124681472 logging_writer.py:48] [527700] global_step=527700, grad_norm=4.234062194824219, loss=0.5993124842643738
I0302 08:23:18.113743 140409829308160 logging_writer.py:48] [527800] global_step=527800, grad_norm=4.682418346405029, loss=0.6074161529541016
I0302 08:23:51.871566 140409124681472 logging_writer.py:48] [527900] global_step=527900, grad_norm=4.767230987548828, loss=0.6293690800666809
I0302 08:24:25.565141 140409829308160 logging_writer.py:48] [528000] global_step=528000, grad_norm=4.485507011413574, loss=0.6017005443572998
I0302 08:24:59.294325 140409124681472 logging_writer.py:48] [528100] global_step=528100, grad_norm=4.475929260253906, loss=0.6283363103866577
I0302 08:25:32.949916 140409829308160 logging_writer.py:48] [528200] global_step=528200, grad_norm=4.099869251251221, loss=0.6581191420555115
I0302 08:25:56.383117 140573303715648 spec.py:321] Evaluating on the training split.
I0302 08:26:02.554915 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 08:26:11.316965 140573303715648 spec.py:349] Evaluating on the test split.
I0302 08:26:14.135048 140573303715648 submission_runner.py:411] Time since start: 184298.13s, 	Step: 528271, 	{'train/accuracy': 0.9596420526504517, 'train/loss': 0.14783921837806702, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.051953673362732, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8174481391906738, 'test/num_examples': 10000, 'score': 178049.65263915062, 'total_duration': 184298.12759184837, 'accumulated_submission_time': 178049.65263915062, 'accumulated_eval_time': 6200.699266672134, 'accumulated_logging_time': 26.892802953720093}
I0302 08:26:14.223150 140409846093568 logging_writer.py:48] [528271] accumulated_eval_time=6200.699267, accumulated_logging_time=26.892803, accumulated_submission_time=178049.652639, global_step=528271, preemption_count=0, score=178049.652639, test/accuracy=0.630600, test/loss=1.817448, test/num_examples=10000, total_duration=184298.127592, train/accuracy=0.959642, train/loss=0.147839, validation/accuracy=0.755800, validation/loss=1.051954, validation/num_examples=50000
I0302 08:26:24.611259 140409854486272 logging_writer.py:48] [528300] global_step=528300, grad_norm=4.715656280517578, loss=0.6397011280059814
I0302 08:26:58.259106 140409846093568 logging_writer.py:48] [528400] global_step=528400, grad_norm=3.973015546798706, loss=0.5243641138076782
I0302 08:27:32.093942 140409854486272 logging_writer.py:48] [528500] global_step=528500, grad_norm=4.493856906890869, loss=0.6484552621841431
I0302 08:28:05.733423 140409846093568 logging_writer.py:48] [528600] global_step=528600, grad_norm=4.604225158691406, loss=0.6498929262161255
I0302 08:28:39.416377 140409854486272 logging_writer.py:48] [528700] global_step=528700, grad_norm=4.804856300354004, loss=0.710111677646637
I0302 08:29:13.073777 140409846093568 logging_writer.py:48] [528800] global_step=528800, grad_norm=4.268247127532959, loss=0.6307037472724915
I0302 08:29:46.784000 140409854486272 logging_writer.py:48] [528900] global_step=528900, grad_norm=4.266412258148193, loss=0.5787327289581299
I0302 08:30:20.442353 140409846093568 logging_writer.py:48] [529000] global_step=529000, grad_norm=4.801300048828125, loss=0.660007894039154
I0302 08:30:54.131876 140409854486272 logging_writer.py:48] [529100] global_step=529100, grad_norm=4.525960922241211, loss=0.648286759853363
I0302 08:31:27.781638 140409846093568 logging_writer.py:48] [529200] global_step=529200, grad_norm=4.254288673400879, loss=0.6121082305908203
I0302 08:32:01.464967 140409854486272 logging_writer.py:48] [529300] global_step=529300, grad_norm=4.561629295349121, loss=0.5924071669578552
I0302 08:32:35.172466 140409846093568 logging_writer.py:48] [529400] global_step=529400, grad_norm=4.57867431640625, loss=0.6306606531143188
I0302 08:33:08.849339 140409854486272 logging_writer.py:48] [529500] global_step=529500, grad_norm=4.2235589027404785, loss=0.5369208455085754
I0302 08:33:42.646028 140409846093568 logging_writer.py:48] [529600] global_step=529600, grad_norm=4.482385158538818, loss=0.5942928194999695
I0302 08:34:16.316974 140409854486272 logging_writer.py:48] [529700] global_step=529700, grad_norm=4.553273677825928, loss=0.6206684708595276
I0302 08:34:44.143722 140573303715648 spec.py:321] Evaluating on the training split.
I0302 08:34:50.230363 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 08:34:58.809039 140573303715648 spec.py:349] Evaluating on the test split.
I0302 08:35:01.087678 140573303715648 submission_runner.py:411] Time since start: 184825.08s, 	Step: 529784, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14770235121250153, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.051684856414795, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8184610605239868, 'test/num_examples': 10000, 'score': 178559.2515347004, 'total_duration': 184825.0801999569, 'accumulated_submission_time': 178559.2515347004, 'accumulated_eval_time': 6217.64315867424, 'accumulated_logging_time': 27.24454164505005}
I0302 08:35:01.187634 140407379851008 logging_writer.py:48] [529784] accumulated_eval_time=6217.643159, accumulated_logging_time=27.244542, accumulated_submission_time=178559.251535, global_step=529784, preemption_count=0, score=178559.251535, test/accuracy=0.631300, test/loss=1.818461, test/num_examples=10000, total_duration=184825.080200, train/accuracy=0.960738, train/loss=0.147702, validation/accuracy=0.756000, validation/loss=1.051685, validation/num_examples=50000
I0302 08:35:06.910086 140408319375104 logging_writer.py:48] [529800] global_step=529800, grad_norm=4.4702911376953125, loss=0.6224242448806763
I0302 08:35:40.539233 140407379851008 logging_writer.py:48] [529900] global_step=529900, grad_norm=4.404223918914795, loss=0.6431319713592529
I0302 08:36:14.261017 140408319375104 logging_writer.py:48] [530000] global_step=530000, grad_norm=4.541840076446533, loss=0.6466783881187439
I0302 08:36:47.956661 140407379851008 logging_writer.py:48] [530100] global_step=530100, grad_norm=4.905922889709473, loss=0.6234052181243896
I0302 08:37:21.648885 140408319375104 logging_writer.py:48] [530200] global_step=530200, grad_norm=4.5393195152282715, loss=0.6270963549613953
I0302 08:37:55.361568 140407379851008 logging_writer.py:48] [530300] global_step=530300, grad_norm=4.458739280700684, loss=0.5626627206802368
I0302 08:38:29.021911 140408319375104 logging_writer.py:48] [530400] global_step=530400, grad_norm=4.052768230438232, loss=0.5105214715003967
I0302 08:39:02.705216 140407379851008 logging_writer.py:48] [530500] global_step=530500, grad_norm=4.223814487457275, loss=0.577914834022522
I0302 08:39:36.459860 140408319375104 logging_writer.py:48] [530600] global_step=530600, grad_norm=4.238039493560791, loss=0.5515841245651245
I0302 08:40:10.154632 140407379851008 logging_writer.py:48] [530700] global_step=530700, grad_norm=4.503594875335693, loss=0.6951538324356079
I0302 08:40:43.819489 140408319375104 logging_writer.py:48] [530800] global_step=530800, grad_norm=4.577719688415527, loss=0.5857430100440979
I0302 08:41:17.494862 140407379851008 logging_writer.py:48] [530900] global_step=530900, grad_norm=4.395679473876953, loss=0.660111665725708
I0302 08:41:51.222169 140408319375104 logging_writer.py:48] [531000] global_step=531000, grad_norm=4.696570873260498, loss=0.6607217788696289
I0302 08:42:24.889852 140407379851008 logging_writer.py:48] [531100] global_step=531100, grad_norm=4.686050891876221, loss=0.7377991676330566
I0302 08:42:58.623547 140408319375104 logging_writer.py:48] [531200] global_step=531200, grad_norm=4.725380897521973, loss=0.6441457867622375
I0302 08:43:31.099457 140573303715648 spec.py:321] Evaluating on the training split.
I0302 08:43:37.170141 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 08:43:45.720209 140573303715648 spec.py:349] Evaluating on the test split.
I0302 08:43:48.053787 140573303715648 submission_runner.py:411] Time since start: 185352.05s, 	Step: 531298, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14695332944393158, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0518393516540527, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8184059858322144, 'test/num_examples': 10000, 'score': 179069.09046268463, 'total_duration': 185352.04632163048, 'accumulated_submission_time': 179069.09046268463, 'accumulated_eval_time': 6234.597435712814, 'accumulated_logging_time': 27.356879711151123}
I0302 08:43:48.159396 140409829308160 logging_writer.py:48] [531298] accumulated_eval_time=6234.597436, accumulated_logging_time=27.356880, accumulated_submission_time=179069.090463, global_step=531298, preemption_count=0, score=179069.090463, test/accuracy=0.631300, test/loss=1.818406, test/num_examples=10000, total_duration=185352.046322, train/accuracy=0.960180, train/loss=0.146953, validation/accuracy=0.755700, validation/loss=1.051839, validation/num_examples=50000
I0302 08:43:49.172361 140409837700864 logging_writer.py:48] [531300] global_step=531300, grad_norm=4.73002815246582, loss=0.6155608892440796
I0302 08:44:22.851087 140409829308160 logging_writer.py:48] [531400] global_step=531400, grad_norm=4.479085922241211, loss=0.5921734571456909
I0302 08:44:56.516206 140409837700864 logging_writer.py:48] [531500] global_step=531500, grad_norm=4.213727951049805, loss=0.5736834406852722
I0302 08:45:30.184376 140409829308160 logging_writer.py:48] [531600] global_step=531600, grad_norm=4.3194379806518555, loss=0.5436171889305115
I0302 08:46:03.916243 140409837700864 logging_writer.py:48] [531700] global_step=531700, grad_norm=4.5033674240112305, loss=0.6011393070220947
I0302 08:46:37.639280 140409829308160 logging_writer.py:48] [531800] global_step=531800, grad_norm=4.761997699737549, loss=0.6487296223640442
I0302 08:47:11.323473 140409837700864 logging_writer.py:48] [531900] global_step=531900, grad_norm=4.508004188537598, loss=0.6056831479072571
I0302 08:47:45.026113 140409829308160 logging_writer.py:48] [532000] global_step=532000, grad_norm=4.373270511627197, loss=0.6602972745895386
I0302 08:48:18.712090 140409837700864 logging_writer.py:48] [532100] global_step=532100, grad_norm=4.2403740882873535, loss=0.6234909296035767
I0302 08:48:52.380115 140409829308160 logging_writer.py:48] [532200] global_step=532200, grad_norm=4.556755065917969, loss=0.6295223832130432
I0302 08:49:26.110725 140409837700864 logging_writer.py:48] [532300] global_step=532300, grad_norm=4.172703266143799, loss=0.5787566900253296
I0302 08:49:59.834343 140409829308160 logging_writer.py:48] [532400] global_step=532400, grad_norm=5.686766624450684, loss=0.6425578594207764
I0302 08:50:33.567708 140409837700864 logging_writer.py:48] [532500] global_step=532500, grad_norm=4.7377705574035645, loss=0.700390636920929
I0302 08:51:07.250502 140409829308160 logging_writer.py:48] [532600] global_step=532600, grad_norm=4.799960613250732, loss=0.5969504714012146
I0302 08:51:41.058216 140409837700864 logging_writer.py:48] [532700] global_step=532700, grad_norm=4.6190409660339355, loss=0.5765523314476013
I0302 08:52:14.771258 140409829308160 logging_writer.py:48] [532800] global_step=532800, grad_norm=4.317469120025635, loss=0.6214349865913391
I0302 08:52:18.281359 140573303715648 spec.py:321] Evaluating on the training split.
I0302 08:52:24.402423 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 08:52:32.928238 140573303715648 spec.py:349] Evaluating on the test split.
I0302 08:52:35.247838 140573303715648 submission_runner.py:411] Time since start: 185879.24s, 	Step: 532812, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14978213608264923, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0504176616668701, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8165922164916992, 'test/num_examples': 10000, 'score': 179579.1422381401, 'total_duration': 185879.24037575722, 'accumulated_submission_time': 179579.1422381401, 'accumulated_eval_time': 6251.563860416412, 'accumulated_logging_time': 27.472768306732178}
I0302 08:52:35.352467 140408319375104 logging_writer.py:48] [532812] accumulated_eval_time=6251.563860, accumulated_logging_time=27.472768, accumulated_submission_time=179579.142238, global_step=532812, preemption_count=0, score=179579.142238, test/accuracy=0.630600, test/loss=1.816592, test/num_examples=10000, total_duration=185879.240376, train/accuracy=0.960559, train/loss=0.149782, validation/accuracy=0.755760, validation/loss=1.050418, validation/num_examples=50000
I0302 08:53:05.307545 140409124681472 logging_writer.py:48] [532900] global_step=532900, grad_norm=4.478759288787842, loss=0.5801593065261841
I0302 08:53:39.020953 140408319375104 logging_writer.py:48] [533000] global_step=533000, grad_norm=4.239666938781738, loss=0.6223199963569641
I0302 08:54:12.668148 140409124681472 logging_writer.py:48] [533100] global_step=533100, grad_norm=4.484074592590332, loss=0.5801223516464233
I0302 08:54:46.388547 140408319375104 logging_writer.py:48] [533200] global_step=533200, grad_norm=4.455080032348633, loss=0.6093519926071167
I0302 08:55:20.067512 140409124681472 logging_writer.py:48] [533300] global_step=533300, grad_norm=4.283257961273193, loss=0.6057839393615723
I0302 08:55:53.764955 140408319375104 logging_writer.py:48] [533400] global_step=533400, grad_norm=4.661910057067871, loss=0.6874291300773621
I0302 08:56:27.475738 140409124681472 logging_writer.py:48] [533500] global_step=533500, grad_norm=4.4549689292907715, loss=0.5829086899757385
I0302 08:57:01.124092 140408319375104 logging_writer.py:48] [533600] global_step=533600, grad_norm=3.874044895172119, loss=0.5512211918830872
I0302 08:57:34.897659 140409124681472 logging_writer.py:48] [533700] global_step=533700, grad_norm=4.251547813415527, loss=0.585105299949646
I0302 08:58:08.644418 140408319375104 logging_writer.py:48] [533800] global_step=533800, grad_norm=4.195601463317871, loss=0.583145797252655
I0302 08:58:42.321835 140409124681472 logging_writer.py:48] [533900] global_step=533900, grad_norm=4.6164021492004395, loss=0.585991382598877
I0302 08:59:15.994229 140408319375104 logging_writer.py:48] [534000] global_step=534000, grad_norm=4.105276584625244, loss=0.5750401020050049
I0302 08:59:49.771056 140409124681472 logging_writer.py:48] [534100] global_step=534100, grad_norm=4.265934467315674, loss=0.6520830392837524
I0302 09:00:23.480382 140408319375104 logging_writer.py:48] [534200] global_step=534200, grad_norm=4.769167423248291, loss=0.6549731492996216
I0302 09:00:57.184569 140409124681472 logging_writer.py:48] [534300] global_step=534300, grad_norm=4.435644626617432, loss=0.6240324378013611
I0302 09:01:05.407908 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:01:11.461775 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:01:19.977006 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:01:22.291191 140573303715648 submission_runner.py:411] Time since start: 186406.28s, 	Step: 534326, 	{'train/accuracy': 0.9616748690605164, 'train/loss': 0.1470065712928772, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0519754886627197, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8166451454162598, 'test/num_examples': 10000, 'score': 180089.1278910637, 'total_duration': 186406.28373670578, 'accumulated_submission_time': 180089.1278910637, 'accumulated_eval_time': 6268.447098493576, 'accumulated_logging_time': 27.58768367767334}
I0302 09:01:22.402150 140407379851008 logging_writer.py:48] [534326] accumulated_eval_time=6268.447098, accumulated_logging_time=27.587684, accumulated_submission_time=180089.127891, global_step=534326, preemption_count=0, score=180089.127891, test/accuracy=0.631900, test/loss=1.816645, test/num_examples=10000, total_duration=186406.283737, train/accuracy=0.961675, train/loss=0.147007, validation/accuracy=0.756120, validation/loss=1.051975, validation/num_examples=50000
I0302 09:01:47.606181 140409829308160 logging_writer.py:48] [534400] global_step=534400, grad_norm=4.623845100402832, loss=0.6331809759140015
I0302 09:02:21.246677 140407379851008 logging_writer.py:48] [534500] global_step=534500, grad_norm=4.843953609466553, loss=0.6893458962440491
I0302 09:02:54.925772 140409829308160 logging_writer.py:48] [534600] global_step=534600, grad_norm=4.244871139526367, loss=0.6316709518432617
I0302 09:03:28.655501 140407379851008 logging_writer.py:48] [534700] global_step=534700, grad_norm=4.373150825500488, loss=0.6722025871276855
I0302 09:04:02.389662 140409829308160 logging_writer.py:48] [534800] global_step=534800, grad_norm=4.501255989074707, loss=0.5942836403846741
I0302 09:04:36.108886 140407379851008 logging_writer.py:48] [534900] global_step=534900, grad_norm=4.4112324714660645, loss=0.6289125680923462
I0302 09:05:09.772726 140409829308160 logging_writer.py:48] [535000] global_step=535000, grad_norm=4.24344539642334, loss=0.6076737642288208
I0302 09:05:43.509133 140407379851008 logging_writer.py:48] [535100] global_step=535100, grad_norm=4.59842586517334, loss=0.6919689774513245
I0302 09:06:17.161844 140409829308160 logging_writer.py:48] [535200] global_step=535200, grad_norm=4.289548397064209, loss=0.5909345746040344
I0302 09:06:50.858291 140407379851008 logging_writer.py:48] [535300] global_step=535300, grad_norm=4.23805046081543, loss=0.5682706236839294
I0302 09:07:24.554898 140409829308160 logging_writer.py:48] [535400] global_step=535400, grad_norm=3.8796586990356445, loss=0.5356384515762329
I0302 09:07:58.235455 140407379851008 logging_writer.py:48] [535500] global_step=535500, grad_norm=4.147005081176758, loss=0.5878303647041321
I0302 09:08:31.963772 140409829308160 logging_writer.py:48] [535600] global_step=535600, grad_norm=4.88735294342041, loss=0.7317844033241272
I0302 09:09:05.634233 140407379851008 logging_writer.py:48] [535700] global_step=535700, grad_norm=4.814694881439209, loss=0.6075106263160706
I0302 09:09:39.379987 140409829308160 logging_writer.py:48] [535800] global_step=535800, grad_norm=4.51153564453125, loss=0.6755236983299255
I0302 09:09:52.363837 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:09:58.401669 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:10:06.914786 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:10:09.213566 140573303715648 submission_runner.py:411] Time since start: 186933.21s, 	Step: 535840, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14293372631072998, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0528799295425415, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8189215660095215, 'test/num_examples': 10000, 'score': 180599.02153396606, 'total_duration': 186933.2061035633, 'accumulated_submission_time': 180599.02153396606, 'accumulated_eval_time': 6285.296785116196, 'accumulated_logging_time': 27.708674669265747}
I0302 09:10:09.322878 140407379851008 logging_writer.py:48] [535840] accumulated_eval_time=6285.296785, accumulated_logging_time=27.708675, accumulated_submission_time=180599.021534, global_step=535840, preemption_count=0, score=180599.021534, test/accuracy=0.630800, test/loss=1.818922, test/num_examples=10000, total_duration=186933.206104, train/accuracy=0.961416, train/loss=0.142934, validation/accuracy=0.755360, validation/loss=1.052880, validation/num_examples=50000
I0302 09:10:29.915092 140408319375104 logging_writer.py:48] [535900] global_step=535900, grad_norm=4.714478015899658, loss=0.6410714387893677
I0302 09:11:03.595258 140407379851008 logging_writer.py:48] [536000] global_step=536000, grad_norm=4.122284889221191, loss=0.5738139748573303
I0302 09:11:37.302843 140408319375104 logging_writer.py:48] [536100] global_step=536100, grad_norm=4.5808868408203125, loss=0.6586146354675293
I0302 09:12:10.959095 140407379851008 logging_writer.py:48] [536200] global_step=536200, grad_norm=4.97305965423584, loss=0.6667882204055786
I0302 09:12:44.685430 140408319375104 logging_writer.py:48] [536300] global_step=536300, grad_norm=4.657296657562256, loss=0.6131470203399658
I0302 09:13:18.391348 140407379851008 logging_writer.py:48] [536400] global_step=536400, grad_norm=5.3216094970703125, loss=0.6228340864181519
I0302 09:13:52.109159 140408319375104 logging_writer.py:48] [536500] global_step=536500, grad_norm=4.908776760101318, loss=0.6705985069274902
I0302 09:14:25.832727 140407379851008 logging_writer.py:48] [536600] global_step=536600, grad_norm=4.119871139526367, loss=0.6063104271888733
I0302 09:14:59.540043 140408319375104 logging_writer.py:48] [536700] global_step=536700, grad_norm=4.258863925933838, loss=0.6151881814002991
I0302 09:15:33.286464 140407379851008 logging_writer.py:48] [536800] global_step=536800, grad_norm=4.557236194610596, loss=0.6162634491920471
I0302 09:16:07.006081 140408319375104 logging_writer.py:48] [536900] global_step=536900, grad_norm=4.452710151672363, loss=0.6062142252922058
I0302 09:16:40.760753 140407379851008 logging_writer.py:48] [537000] global_step=537000, grad_norm=4.407194137573242, loss=0.6732118129730225
I0302 09:17:14.489549 140408319375104 logging_writer.py:48] [537100] global_step=537100, grad_norm=5.193929672241211, loss=0.6379599571228027
I0302 09:17:48.153831 140407379851008 logging_writer.py:48] [537200] global_step=537200, grad_norm=4.540515899658203, loss=0.5976023077964783
I0302 09:18:21.887793 140408319375104 logging_writer.py:48] [537300] global_step=537300, grad_norm=4.284997463226318, loss=0.6359797716140747
I0302 09:18:39.221032 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:18:45.390414 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:18:53.829367 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:18:56.103180 140573303715648 submission_runner.py:411] Time since start: 187460.10s, 	Step: 537353, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.1454615592956543, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0528595447540283, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8187016248703003, 'test/num_examples': 10000, 'score': 181108.84668302536, 'total_duration': 187460.0957119465, 'accumulated_submission_time': 181108.84668302536, 'accumulated_eval_time': 6302.178875923157, 'accumulated_logging_time': 27.830414295196533}
I0302 09:18:56.209540 140408319375104 logging_writer.py:48] [537353] accumulated_eval_time=6302.178876, accumulated_logging_time=27.830414, accumulated_submission_time=181108.846683, global_step=537353, preemption_count=0, score=181108.846683, test/accuracy=0.631500, test/loss=1.818702, test/num_examples=10000, total_duration=187460.095712, train/accuracy=0.960798, train/loss=0.145462, validation/accuracy=0.755760, validation/loss=1.052860, validation/num_examples=50000
I0302 09:19:12.359821 140409829308160 logging_writer.py:48] [537400] global_step=537400, grad_norm=3.9546875953674316, loss=0.6099177002906799
I0302 09:19:46.066695 140408319375104 logging_writer.py:48] [537500] global_step=537500, grad_norm=4.7302680015563965, loss=0.6132099628448486
I0302 09:20:19.788244 140409829308160 logging_writer.py:48] [537600] global_step=537600, grad_norm=4.726304531097412, loss=0.6473739743232727
I0302 09:20:53.550171 140408319375104 logging_writer.py:48] [537700] global_step=537700, grad_norm=4.309032440185547, loss=0.6435797810554504
I0302 09:21:27.238672 140409829308160 logging_writer.py:48] [537800] global_step=537800, grad_norm=4.409879207611084, loss=0.5903468728065491
I0302 09:22:01.001016 140408319375104 logging_writer.py:48] [537900] global_step=537900, grad_norm=4.6919732093811035, loss=0.6146969795227051
I0302 09:22:34.774575 140409829308160 logging_writer.py:48] [538000] global_step=538000, grad_norm=4.796442985534668, loss=0.5936497449874878
I0302 09:23:08.478921 140408319375104 logging_writer.py:48] [538100] global_step=538100, grad_norm=4.586904048919678, loss=0.5846306681632996
I0302 09:23:42.131454 140409829308160 logging_writer.py:48] [538200] global_step=538200, grad_norm=4.504967212677002, loss=0.5810670852661133
I0302 09:24:15.871442 140408319375104 logging_writer.py:48] [538300] global_step=538300, grad_norm=4.459157466888428, loss=0.5686607956886292
I0302 09:24:49.577620 140409829308160 logging_writer.py:48] [538400] global_step=538400, grad_norm=4.558093070983887, loss=0.5921264886856079
I0302 09:25:23.245457 140408319375104 logging_writer.py:48] [538500] global_step=538500, grad_norm=4.507352828979492, loss=0.632798433303833
I0302 09:25:56.906610 140409829308160 logging_writer.py:48] [538600] global_step=538600, grad_norm=4.753943920135498, loss=0.655302882194519
I0302 09:26:30.636394 140408319375104 logging_writer.py:48] [538700] global_step=538700, grad_norm=4.360946178436279, loss=0.6789449453353882
I0302 09:27:04.303730 140409829308160 logging_writer.py:48] [538800] global_step=538800, grad_norm=4.400953769683838, loss=0.5726896524429321
I0302 09:27:26.386693 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:27:32.451090 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:27:41.167046 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:27:43.477553 140573303715648 submission_runner.py:411] Time since start: 187987.47s, 	Step: 538867, 	{'train/accuracy': 0.9594626426696777, 'train/loss': 0.14877939224243164, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0516453981399536, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8169374465942383, 'test/num_examples': 10000, 'score': 181618.95030093193, 'total_duration': 187987.47008562088, 'accumulated_submission_time': 181618.95030093193, 'accumulated_eval_time': 6319.269691467285, 'accumulated_logging_time': 27.948353052139282}
I0302 09:27:43.584425 140408319375104 logging_writer.py:48] [538867] accumulated_eval_time=6319.269691, accumulated_logging_time=27.948353, accumulated_submission_time=181618.950301, global_step=538867, preemption_count=0, score=181618.950301, test/accuracy=0.630700, test/loss=1.816937, test/num_examples=10000, total_duration=187987.470086, train/accuracy=0.959463, train/loss=0.148779, validation/accuracy=0.756080, validation/loss=1.051645, validation/num_examples=50000
I0302 09:27:55.053106 140409124681472 logging_writer.py:48] [538900] global_step=538900, grad_norm=4.61754035949707, loss=0.6469687819480896
I0302 09:28:28.726422 140408319375104 logging_writer.py:48] [539000] global_step=539000, grad_norm=4.2196221351623535, loss=0.6187360882759094
I0302 09:29:02.488587 140409124681472 logging_writer.py:48] [539100] global_step=539100, grad_norm=4.474717140197754, loss=0.5533558130264282
I0302 09:29:36.162459 140408319375104 logging_writer.py:48] [539200] global_step=539200, grad_norm=4.167549133300781, loss=0.5680749416351318
I0302 09:30:09.849819 140409124681472 logging_writer.py:48] [539300] global_step=539300, grad_norm=4.101795673370361, loss=0.5269492268562317
I0302 09:30:43.562173 140408319375104 logging_writer.py:48] [539400] global_step=539400, grad_norm=4.37406587600708, loss=0.6079220771789551
I0302 09:31:17.236610 140409124681472 logging_writer.py:48] [539500] global_step=539500, grad_norm=4.39101505279541, loss=0.5446901917457581
I0302 09:31:50.962961 140408319375104 logging_writer.py:48] [539600] global_step=539600, grad_norm=4.376454830169678, loss=0.6886597871780396
I0302 09:32:24.665321 140409124681472 logging_writer.py:48] [539700] global_step=539700, grad_norm=4.818604469299316, loss=0.7260405421257019
I0302 09:32:58.374295 140408319375104 logging_writer.py:48] [539800] global_step=539800, grad_norm=4.420583724975586, loss=0.6005908250808716
I0302 09:33:32.108240 140409124681472 logging_writer.py:48] [539900] global_step=539900, grad_norm=4.377768039703369, loss=0.5653143525123596
I0302 09:34:05.815850 140408319375104 logging_writer.py:48] [540000] global_step=540000, grad_norm=4.457474708557129, loss=0.6269590854644775
I0302 09:34:39.561648 140409124681472 logging_writer.py:48] [540100] global_step=540100, grad_norm=4.470504283905029, loss=0.6400991678237915
I0302 09:35:13.360189 140408319375104 logging_writer.py:48] [540200] global_step=540200, grad_norm=4.37175989151001, loss=0.5687812566757202
I0302 09:35:47.033339 140409124681472 logging_writer.py:48] [540300] global_step=540300, grad_norm=4.574990749359131, loss=0.6698719263076782
I0302 09:36:13.478507 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:36:19.570076 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:36:28.206298 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:36:30.501550 140573303715648 submission_runner.py:411] Time since start: 188514.49s, 	Step: 540380, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.1483667641878128, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.051335334777832, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.815916895866394, 'test/num_examples': 10000, 'score': 182128.77370357513, 'total_duration': 188514.49407696724, 'accumulated_submission_time': 182128.77370357513, 'accumulated_eval_time': 6336.292678594589, 'accumulated_logging_time': 28.065372705459595}
I0302 09:36:30.605421 140409846093568 logging_writer.py:48] [540380] accumulated_eval_time=6336.292679, accumulated_logging_time=28.065373, accumulated_submission_time=182128.773704, global_step=540380, preemption_count=0, score=182128.773704, test/accuracy=0.630900, test/loss=1.815917, test/num_examples=10000, total_duration=188514.494077, train/accuracy=0.959762, train/loss=0.148367, validation/accuracy=0.755360, validation/loss=1.051335, validation/num_examples=50000
I0302 09:36:37.675467 140409862878976 logging_writer.py:48] [540400] global_step=540400, grad_norm=5.0159831047058105, loss=0.622156023979187
I0302 09:37:11.385616 140409846093568 logging_writer.py:48] [540500] global_step=540500, grad_norm=4.638397693634033, loss=0.6033361554145813
I0302 09:37:45.037622 140409862878976 logging_writer.py:48] [540600] global_step=540600, grad_norm=4.379765033721924, loss=0.582870602607727
I0302 09:38:18.694122 140409846093568 logging_writer.py:48] [540700] global_step=540700, grad_norm=5.02528190612793, loss=0.6306095719337463
I0302 09:38:52.379621 140409862878976 logging_writer.py:48] [540800] global_step=540800, grad_norm=4.592761516571045, loss=0.6486488580703735
I0302 09:39:26.085720 140409846093568 logging_writer.py:48] [540900] global_step=540900, grad_norm=4.779688835144043, loss=0.7225380539894104
I0302 09:39:59.781040 140409862878976 logging_writer.py:48] [541000] global_step=541000, grad_norm=4.783397674560547, loss=0.6468284726142883
I0302 09:40:33.507021 140409846093568 logging_writer.py:48] [541100] global_step=541100, grad_norm=4.689289093017578, loss=0.6681613326072693
I0302 09:41:07.241204 140409862878976 logging_writer.py:48] [541200] global_step=541200, grad_norm=4.656066417694092, loss=0.625647783279419
I0302 09:41:40.957561 140409846093568 logging_writer.py:48] [541300] global_step=541300, grad_norm=5.057980537414551, loss=0.7297251224517822
I0302 09:42:14.708369 140409862878976 logging_writer.py:48] [541400] global_step=541400, grad_norm=4.183066368103027, loss=0.5913382768630981
I0302 09:42:48.389325 140409846093568 logging_writer.py:48] [541500] global_step=541500, grad_norm=4.407608509063721, loss=0.5694500207901001
I0302 09:43:22.066174 140409862878976 logging_writer.py:48] [541600] global_step=541600, grad_norm=4.267220973968506, loss=0.6011956334114075
I0302 09:43:55.765743 140409846093568 logging_writer.py:48] [541700] global_step=541700, grad_norm=4.297889709472656, loss=0.6503749489784241
I0302 09:44:29.464768 140409862878976 logging_writer.py:48] [541800] global_step=541800, grad_norm=4.406526565551758, loss=0.5491904616355896
I0302 09:45:00.577252 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:45:06.780859 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:45:15.251576 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:45:17.615169 140573303715648 submission_runner.py:411] Time since start: 189041.61s, 	Step: 541894, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14823448657989502, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0529454946517944, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.817833662033081, 'test/num_examples': 10000, 'score': 182638.67547369003, 'total_duration': 189041.6077091694, 'accumulated_submission_time': 182638.67547369003, 'accumulated_eval_time': 6353.330547809601, 'accumulated_logging_time': 28.181137323379517}
I0302 09:45:17.728833 140409124681472 logging_writer.py:48] [541894] accumulated_eval_time=6353.330548, accumulated_logging_time=28.181137, accumulated_submission_time=182638.675474, global_step=541894, preemption_count=0, score=182638.675474, test/accuracy=0.630400, test/loss=1.817834, test/num_examples=10000, total_duration=189041.607709, train/accuracy=0.960599, train/loss=0.148234, validation/accuracy=0.755780, validation/loss=1.052945, validation/num_examples=50000
I0302 09:45:20.106431 140409829308160 logging_writer.py:48] [541900] global_step=541900, grad_norm=4.971881866455078, loss=0.6750028133392334
I0302 09:45:53.849463 140409124681472 logging_writer.py:48] [542000] global_step=542000, grad_norm=4.468946933746338, loss=0.5983906984329224
I0302 09:46:27.505111 140409829308160 logging_writer.py:48] [542100] global_step=542100, grad_norm=4.305966854095459, loss=0.6551008224487305
I0302 09:47:01.186508 140409124681472 logging_writer.py:48] [542200] global_step=542200, grad_norm=4.337466239929199, loss=0.6513113975524902
I0302 09:47:35.107183 140409829308160 logging_writer.py:48] [542300] global_step=542300, grad_norm=4.479075908660889, loss=0.6170375347137451
I0302 09:48:08.765650 140409124681472 logging_writer.py:48] [542400] global_step=542400, grad_norm=4.432282447814941, loss=0.5700703859329224
I0302 09:48:42.498575 140409829308160 logging_writer.py:48] [542500] global_step=542500, grad_norm=5.323773384094238, loss=0.6704778671264648
I0302 09:49:16.191962 140409124681472 logging_writer.py:48] [542600] global_step=542600, grad_norm=4.762484073638916, loss=0.6808258295059204
I0302 09:49:49.932685 140409829308160 logging_writer.py:48] [542700] global_step=542700, grad_norm=4.495019435882568, loss=0.6823814511299133
I0302 09:50:23.622845 140409124681472 logging_writer.py:48] [542800] global_step=542800, grad_norm=4.672701835632324, loss=0.6207764148712158
I0302 09:50:57.350661 140409829308160 logging_writer.py:48] [542900] global_step=542900, grad_norm=4.713923454284668, loss=0.7259042859077454
I0302 09:51:31.042972 140409124681472 logging_writer.py:48] [543000] global_step=543000, grad_norm=4.145371437072754, loss=0.5903088450431824
I0302 09:52:04.772188 140409829308160 logging_writer.py:48] [543100] global_step=543100, grad_norm=4.4611921310424805, loss=0.6603474617004395
I0302 09:52:38.445657 140409124681472 logging_writer.py:48] [543200] global_step=543200, grad_norm=5.1347575187683105, loss=0.6725313067436218
I0302 09:53:12.279602 140409829308160 logging_writer.py:48] [543300] global_step=543300, grad_norm=4.537944316864014, loss=0.6167084574699402
I0302 09:53:46.030199 140409124681472 logging_writer.py:48] [543400] global_step=543400, grad_norm=4.662022113800049, loss=0.671741247177124
I0302 09:53:47.858857 140573303715648 spec.py:321] Evaluating on the training split.
I0302 09:53:53.881268 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 09:54:02.343081 140573303715648 spec.py:349] Evaluating on the test split.
I0302 09:54:04.641191 140573303715648 submission_runner.py:411] Time since start: 189568.63s, 	Step: 543407, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14724302291870117, 'validation/accuracy': 0.7562199831008911, 'validation/loss': 1.0523183345794678, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8182512521743774, 'test/num_examples': 10000, 'score': 183148.73386335373, 'total_duration': 189568.63373208046, 'accumulated_submission_time': 183148.73386335373, 'accumulated_eval_time': 6370.11282992363, 'accumulated_logging_time': 28.306031942367554}
I0302 09:54:04.749459 140407379851008 logging_writer.py:48] [543407] accumulated_eval_time=6370.112830, accumulated_logging_time=28.306032, accumulated_submission_time=183148.733863, global_step=543407, preemption_count=0, score=183148.733863, test/accuracy=0.630500, test/loss=1.818251, test/num_examples=10000, total_duration=189568.633732, train/accuracy=0.961496, train/loss=0.147243, validation/accuracy=0.756220, validation/loss=1.052318, validation/num_examples=50000
I0302 09:54:36.473138 140408319375104 logging_writer.py:48] [543500] global_step=543500, grad_norm=4.651330471038818, loss=0.5993028283119202
I0302 09:55:10.122545 140407379851008 logging_writer.py:48] [543600] global_step=543600, grad_norm=4.184974670410156, loss=0.6708003282546997
I0302 09:55:43.791507 140408319375104 logging_writer.py:48] [543700] global_step=543700, grad_norm=4.347840785980225, loss=0.5760606527328491
I0302 09:56:17.458410 140407379851008 logging_writer.py:48] [543800] global_step=543800, grad_norm=4.742247581481934, loss=0.6167281270027161
I0302 09:56:51.213464 140408319375104 logging_writer.py:48] [543900] global_step=543900, grad_norm=4.469025611877441, loss=0.7135778665542603
I0302 09:57:24.950475 140407379851008 logging_writer.py:48] [544000] global_step=544000, grad_norm=4.483891487121582, loss=0.588867723941803
I0302 09:57:58.657006 140408319375104 logging_writer.py:48] [544100] global_step=544100, grad_norm=4.508688926696777, loss=0.5878702998161316
I0302 09:58:32.327195 140407379851008 logging_writer.py:48] [544200] global_step=544200, grad_norm=4.583954334259033, loss=0.6587876081466675
I0302 09:59:06.072173 140408319375104 logging_writer.py:48] [544300] global_step=544300, grad_norm=4.734419822692871, loss=0.6645017862319946
I0302 09:59:39.869471 140407379851008 logging_writer.py:48] [544400] global_step=544400, grad_norm=4.718369483947754, loss=0.649167001247406
I0302 10:00:13.538280 140408319375104 logging_writer.py:48] [544500] global_step=544500, grad_norm=4.556190490722656, loss=0.6269328594207764
I0302 10:00:47.254884 140407379851008 logging_writer.py:48] [544600] global_step=544600, grad_norm=4.563377857208252, loss=0.5213737487792969
I0302 10:01:20.927510 140408319375104 logging_writer.py:48] [544700] global_step=544700, grad_norm=4.8012375831604, loss=0.6935152411460876
I0302 10:01:54.582058 140407379851008 logging_writer.py:48] [544800] global_step=544800, grad_norm=4.5477118492126465, loss=0.6069285869598389
I0302 10:02:28.252203 140408319375104 logging_writer.py:48] [544900] global_step=544900, grad_norm=4.952982425689697, loss=0.5946369171142578
I0302 10:02:34.818182 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:02:40.835554 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:02:49.311050 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:02:51.700730 140573303715648 submission_runner.py:411] Time since start: 190095.69s, 	Step: 544921, 	{'train/accuracy': 0.9620336294174194, 'train/loss': 0.14223508536815643, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.051290512084961, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8176825046539307, 'test/num_examples': 10000, 'score': 183658.73257374763, 'total_duration': 190095.69326162338, 'accumulated_submission_time': 183658.73257374763, 'accumulated_eval_time': 6386.995321273804, 'accumulated_logging_time': 28.42442011833191}
I0302 10:02:51.809615 140409837700864 logging_writer.py:48] [544921] accumulated_eval_time=6386.995321, accumulated_logging_time=28.424420, accumulated_submission_time=183658.732574, global_step=544921, preemption_count=0, score=183658.732574, test/accuracy=0.631700, test/loss=1.817683, test/num_examples=10000, total_duration=190095.693262, train/accuracy=0.962034, train/loss=0.142235, validation/accuracy=0.755820, validation/loss=1.051291, validation/num_examples=50000
I0302 10:03:18.739776 140409862878976 logging_writer.py:48] [545000] global_step=545000, grad_norm=4.625051021575928, loss=0.6020254492759705
I0302 10:03:52.404953 140409837700864 logging_writer.py:48] [545100] global_step=545100, grad_norm=4.01179313659668, loss=0.5665091276168823
I0302 10:04:26.135433 140409862878976 logging_writer.py:48] [545200] global_step=545200, grad_norm=4.957487106323242, loss=0.6884338855743408
I0302 10:04:59.790526 140409837700864 logging_writer.py:48] [545300] global_step=545300, grad_norm=4.716644287109375, loss=0.6361523866653442
I0302 10:05:33.450333 140409862878976 logging_writer.py:48] [545400] global_step=545400, grad_norm=4.237127304077148, loss=0.6575892567634583
I0302 10:06:07.231232 140409837700864 logging_writer.py:48] [545500] global_step=545500, grad_norm=4.466610908508301, loss=0.5974860787391663
I0302 10:06:40.947451 140409862878976 logging_writer.py:48] [545600] global_step=545600, grad_norm=4.171910762786865, loss=0.6110800504684448
I0302 10:07:14.664203 140409837700864 logging_writer.py:48] [545700] global_step=545700, grad_norm=4.952672481536865, loss=0.6515980958938599
I0302 10:07:48.394894 140409862878976 logging_writer.py:48] [545800] global_step=545800, grad_norm=4.58707857131958, loss=0.6600335836410522
I0302 10:08:22.048748 140409837700864 logging_writer.py:48] [545900] global_step=545900, grad_norm=4.186086177825928, loss=0.5740233659744263
I0302 10:08:55.763477 140409862878976 logging_writer.py:48] [546000] global_step=546000, grad_norm=4.81436824798584, loss=0.636070728302002
I0302 10:09:29.444102 140409837700864 logging_writer.py:48] [546100] global_step=546100, grad_norm=5.145755767822266, loss=0.653651237487793
I0302 10:10:03.174709 140409862878976 logging_writer.py:48] [546200] global_step=546200, grad_norm=4.462493419647217, loss=0.6626451015472412
I0302 10:10:36.862810 140409837700864 logging_writer.py:48] [546300] global_step=546300, grad_norm=5.242727756500244, loss=0.6266189813613892
I0302 10:11:10.591704 140409862878976 logging_writer.py:48] [546400] global_step=546400, grad_norm=4.826271057128906, loss=0.6378415822982788
I0302 10:11:21.900853 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:11:28.049492 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:11:36.736191 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:11:39.103768 140573303715648 submission_runner.py:411] Time since start: 190623.10s, 	Step: 546435, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14639462530612946, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.0523061752319336, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8177745342254639, 'test/num_examples': 10000, 'score': 184168.75291776657, 'total_duration': 190623.0963089466, 'accumulated_submission_time': 184168.75291776657, 'accumulated_eval_time': 6404.198199510574, 'accumulated_logging_time': 28.544098138809204}
I0302 10:11:39.211647 140407379851008 logging_writer.py:48] [546435] accumulated_eval_time=6404.198200, accumulated_logging_time=28.544098, accumulated_submission_time=184168.752918, global_step=546435, preemption_count=0, score=184168.752918, test/accuracy=0.631300, test/loss=1.817775, test/num_examples=10000, total_duration=190623.096309, train/accuracy=0.960957, train/loss=0.146395, validation/accuracy=0.755540, validation/loss=1.052306, validation/num_examples=50000
I0302 10:12:01.525698 140408319375104 logging_writer.py:48] [546500] global_step=546500, grad_norm=4.596749305725098, loss=0.6146383285522461
I0302 10:12:35.248010 140407379851008 logging_writer.py:48] [546600] global_step=546600, grad_norm=4.235962390899658, loss=0.5881685018539429
I0302 10:13:08.939449 140408319375104 logging_writer.py:48] [546700] global_step=546700, grad_norm=4.2472968101501465, loss=0.6490616202354431
I0302 10:13:42.641408 140407379851008 logging_writer.py:48] [546800] global_step=546800, grad_norm=4.768857479095459, loss=0.6187220215797424
I0302 10:14:16.303575 140408319375104 logging_writer.py:48] [546900] global_step=546900, grad_norm=4.411064147949219, loss=0.5795528888702393
I0302 10:14:50.007578 140407379851008 logging_writer.py:48] [547000] global_step=547000, grad_norm=4.67892599105835, loss=0.6907485127449036
I0302 10:15:23.714592 140408319375104 logging_writer.py:48] [547100] global_step=547100, grad_norm=4.913159370422363, loss=0.6520918607711792
I0302 10:15:57.417942 140407379851008 logging_writer.py:48] [547200] global_step=547200, grad_norm=4.458796977996826, loss=0.6001773476600647
I0302 10:16:31.102964 140408319375104 logging_writer.py:48] [547300] global_step=547300, grad_norm=4.631019115447998, loss=0.6742095351219177
I0302 10:17:04.765151 140407379851008 logging_writer.py:48] [547400] global_step=547400, grad_norm=4.021676063537598, loss=0.6069568395614624
I0302 10:17:38.489149 140408319375104 logging_writer.py:48] [547500] global_step=547500, grad_norm=4.356549263000488, loss=0.6300936341285706
I0302 10:18:12.360459 140407379851008 logging_writer.py:48] [547600] global_step=547600, grad_norm=4.6157121658325195, loss=0.6094930171966553
I0302 10:18:46.069006 140408319375104 logging_writer.py:48] [547700] global_step=547700, grad_norm=4.2610015869140625, loss=0.6146711111068726
I0302 10:19:19.770244 140407379851008 logging_writer.py:48] [547800] global_step=547800, grad_norm=4.790930271148682, loss=0.6605963706970215
I0302 10:19:53.505857 140408319375104 logging_writer.py:48] [547900] global_step=547900, grad_norm=4.464382171630859, loss=0.5517022013664246
I0302 10:20:09.138790 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:20:15.217298 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:20:23.759368 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:20:26.122439 140573303715648 submission_runner.py:411] Time since start: 191150.11s, 	Step: 547948, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.14879298210144043, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0521609783172607, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.817501425743103, 'test/num_examples': 10000, 'score': 184678.612347126, 'total_duration': 191150.11498069763, 'accumulated_submission_time': 184678.612347126, 'accumulated_eval_time': 6421.181804418564, 'accumulated_logging_time': 28.66173243522644}
I0302 10:20:26.230221 140408319375104 logging_writer.py:48] [547948] accumulated_eval_time=6421.181804, accumulated_logging_time=28.661732, accumulated_submission_time=184678.612347, global_step=547948, preemption_count=0, score=184678.612347, test/accuracy=0.631400, test/loss=1.817501, test/num_examples=10000, total_duration=191150.114981, train/accuracy=0.959742, train/loss=0.148793, validation/accuracy=0.755640, validation/loss=1.052161, validation/num_examples=50000
I0302 10:20:44.105019 140409854486272 logging_writer.py:48] [548000] global_step=548000, grad_norm=4.95414400100708, loss=0.623532772064209
I0302 10:21:17.768945 140408319375104 logging_writer.py:48] [548100] global_step=548100, grad_norm=4.337599277496338, loss=0.5646397471427917
I0302 10:21:51.441249 140409854486272 logging_writer.py:48] [548200] global_step=548200, grad_norm=5.107764720916748, loss=0.6390812397003174
I0302 10:22:25.157298 140408319375104 logging_writer.py:48] [548300] global_step=548300, grad_norm=4.702724933624268, loss=0.6395549774169922
I0302 10:22:58.855242 140409854486272 logging_writer.py:48] [548400] global_step=548400, grad_norm=4.360395431518555, loss=0.6039019227027893
I0302 10:23:32.593929 140408319375104 logging_writer.py:48] [548500] global_step=548500, grad_norm=4.4740471839904785, loss=0.6628686189651489
I0302 10:24:06.473367 140409854486272 logging_writer.py:48] [548600] global_step=548600, grad_norm=4.759738445281982, loss=0.610866367816925
I0302 10:24:40.194472 140408319375104 logging_writer.py:48] [548700] global_step=548700, grad_norm=4.323146343231201, loss=0.570696234703064
I0302 10:25:13.873761 140409854486272 logging_writer.py:48] [548800] global_step=548800, grad_norm=4.8315749168396, loss=0.6254288554191589
I0302 10:25:47.612744 140408319375104 logging_writer.py:48] [548900] global_step=548900, grad_norm=4.376572608947754, loss=0.6232650876045227
I0302 10:26:21.312490 140409854486272 logging_writer.py:48] [549000] global_step=549000, grad_norm=5.223120212554932, loss=0.6959563493728638
I0302 10:26:55.035274 140408319375104 logging_writer.py:48] [549100] global_step=549100, grad_norm=4.830346584320068, loss=0.7193523049354553
I0302 10:27:28.686600 140409854486272 logging_writer.py:48] [549200] global_step=549200, grad_norm=4.430027484893799, loss=0.6858921051025391
I0302 10:28:02.413968 140408319375104 logging_writer.py:48] [549300] global_step=549300, grad_norm=4.0522894859313965, loss=0.5756396055221558
I0302 10:28:36.118097 140409854486272 logging_writer.py:48] [549400] global_step=549400, grad_norm=4.389463901519775, loss=0.6593259572982788
I0302 10:28:56.171166 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:29:02.445775 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:29:10.967598 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:29:13.657834 140573303715648 submission_runner.py:411] Time since start: 191677.65s, 	Step: 549461, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14835363626480103, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0511138439178467, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8153493404388428, 'test/num_examples': 10000, 'score': 185188.48404669762, 'total_duration': 191677.65038204193, 'accumulated_submission_time': 185188.48404669762, 'accumulated_eval_time': 6438.668433189392, 'accumulated_logging_time': 28.779770612716675}
I0302 10:29:13.752031 140409846093568 logging_writer.py:48] [549461] accumulated_eval_time=6438.668433, accumulated_logging_time=28.779771, accumulated_submission_time=185188.484047, global_step=549461, preemption_count=0, score=185188.484047, test/accuracy=0.630900, test/loss=1.815349, test/num_examples=10000, total_duration=191677.650382, train/accuracy=0.960180, train/loss=0.148354, validation/accuracy=0.756080, validation/loss=1.051114, validation/num_examples=50000
I0302 10:29:27.224948 140409862878976 logging_writer.py:48] [549500] global_step=549500, grad_norm=4.349989414215088, loss=0.5591595768928528
I0302 10:30:00.897981 140409846093568 logging_writer.py:48] [549600] global_step=549600, grad_norm=4.889806747436523, loss=0.6497592926025391
I0302 10:30:34.750461 140409862878976 logging_writer.py:48] [549700] global_step=549700, grad_norm=4.6648268699646, loss=0.59514319896698
I0302 10:31:08.387850 140409846093568 logging_writer.py:48] [549800] global_step=549800, grad_norm=4.6293206214904785, loss=0.6797183156013489
I0302 10:31:42.083553 140409862878976 logging_writer.py:48] [549900] global_step=549900, grad_norm=4.319425106048584, loss=0.6065126657485962
I0302 10:32:15.734477 140409846093568 logging_writer.py:48] [550000] global_step=550000, grad_norm=4.138729572296143, loss=0.5965993404388428
I0302 10:32:49.444254 140409862878976 logging_writer.py:48] [550100] global_step=550100, grad_norm=4.740164279937744, loss=0.5959442853927612
I0302 10:33:23.112464 140409846093568 logging_writer.py:48] [550200] global_step=550200, grad_norm=4.384994029998779, loss=0.6360024809837341
I0302 10:33:56.787441 140409862878976 logging_writer.py:48] [550300] global_step=550300, grad_norm=3.902297019958496, loss=0.5760207176208496
I0302 10:34:30.507056 140409846093568 logging_writer.py:48] [550400] global_step=550400, grad_norm=4.344611644744873, loss=0.6020588874816895
I0302 10:35:04.224009 140409862878976 logging_writer.py:48] [550500] global_step=550500, grad_norm=4.593606472015381, loss=0.5831234455108643
I0302 10:35:37.944278 140409846093568 logging_writer.py:48] [550600] global_step=550600, grad_norm=4.299427032470703, loss=0.6084347367286682
I0302 10:36:11.653296 140409862878976 logging_writer.py:48] [550700] global_step=550700, grad_norm=4.408937931060791, loss=0.6428003311157227
I0302 10:36:45.422875 140409846093568 logging_writer.py:48] [550800] global_step=550800, grad_norm=4.412240028381348, loss=0.59034663438797
I0302 10:37:19.084945 140409862878976 logging_writer.py:48] [550900] global_step=550900, grad_norm=4.614099502563477, loss=0.6848499774932861
I0302 10:37:43.848086 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:37:49.898240 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:37:58.400601 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:38:00.659190 140573303715648 submission_runner.py:411] Time since start: 192204.65s, 	Step: 550975, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.143163800239563, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.0519856214523315, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8175721168518066, 'test/num_examples': 10000, 'score': 185698.51256275177, 'total_duration': 192204.65172839165, 'accumulated_submission_time': 185698.51256275177, 'accumulated_eval_time': 6455.479493379593, 'accumulated_logging_time': 28.882810354232788}
I0302 10:38:00.768931 140409854486272 logging_writer.py:48] [550975] accumulated_eval_time=6455.479493, accumulated_logging_time=28.882810, accumulated_submission_time=185698.512563, global_step=550975, preemption_count=0, score=185698.512563, test/accuracy=0.631700, test/loss=1.817572, test/num_examples=10000, total_duration=192204.651728, train/accuracy=0.961336, train/loss=0.143164, validation/accuracy=0.755540, validation/loss=1.051986, validation/num_examples=50000
I0302 10:38:09.517687 140409862878976 logging_writer.py:48] [551000] global_step=551000, grad_norm=4.6916704177856445, loss=0.6348179578781128
I0302 10:38:43.220216 140409854486272 logging_writer.py:48] [551100] global_step=551100, grad_norm=4.175675868988037, loss=0.5314218401908875
I0302 10:39:16.864373 140409862878976 logging_writer.py:48] [551200] global_step=551200, grad_norm=4.402019500732422, loss=0.5615980625152588
I0302 10:39:50.561285 140409854486272 logging_writer.py:48] [551300] global_step=551300, grad_norm=4.795046806335449, loss=0.6101202368736267
I0302 10:40:24.287257 140409862878976 logging_writer.py:48] [551400] global_step=551400, grad_norm=4.569803714752197, loss=0.7131629586219788
I0302 10:40:58.003399 140409854486272 logging_writer.py:48] [551500] global_step=551500, grad_norm=4.456190586090088, loss=0.6941698789596558
I0302 10:41:31.688783 140409862878976 logging_writer.py:48] [551600] global_step=551600, grad_norm=4.29315185546875, loss=0.5371009707450867
I0302 10:42:05.336084 140409854486272 logging_writer.py:48] [551700] global_step=551700, grad_norm=4.432285785675049, loss=0.6755493879318237
I0302 10:42:39.118393 140409862878976 logging_writer.py:48] [551800] global_step=551800, grad_norm=4.555624961853027, loss=0.6671140193939209
I0302 10:43:12.792846 140409854486272 logging_writer.py:48] [551900] global_step=551900, grad_norm=4.606584072113037, loss=0.6183467507362366
I0302 10:43:46.507468 140409862878976 logging_writer.py:48] [552000] global_step=552000, grad_norm=4.651065826416016, loss=0.6004182696342468
I0302 10:44:20.214628 140409854486272 logging_writer.py:48] [552100] global_step=552100, grad_norm=4.495033264160156, loss=0.601208508014679
I0302 10:44:53.894594 140409862878976 logging_writer.py:48] [552200] global_step=552200, grad_norm=4.722423553466797, loss=0.660520076751709
I0302 10:45:27.603901 140409854486272 logging_writer.py:48] [552300] global_step=552300, grad_norm=4.376346588134766, loss=0.6227607727050781
I0302 10:46:01.278765 140409862878976 logging_writer.py:48] [552400] global_step=552400, grad_norm=4.1777262687683105, loss=0.5610183477401733
I0302 10:46:30.784094 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:46:36.989010 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:46:45.466998 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:46:47.757073 140573303715648 submission_runner.py:411] Time since start: 192731.75s, 	Step: 552489, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14542822539806366, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.051978588104248, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8172557353973389, 'test/num_examples': 10000, 'score': 186208.4559469223, 'total_duration': 192731.74961352348, 'accumulated_submission_time': 186208.4559469223, 'accumulated_eval_time': 6472.452427625656, 'accumulated_logging_time': 29.003294467926025}
I0302 10:46:47.867212 140407371458304 logging_writer.py:48] [552489] accumulated_eval_time=6472.452428, accumulated_logging_time=29.003294, accumulated_submission_time=186208.455947, global_step=552489, preemption_count=0, score=186208.455947, test/accuracy=0.631000, test/loss=1.817256, test/num_examples=10000, total_duration=192731.749614, train/accuracy=0.961057, train/loss=0.145428, validation/accuracy=0.755840, validation/loss=1.051979, validation/num_examples=50000
I0302 10:46:51.918641 140408319375104 logging_writer.py:48] [552500] global_step=552500, grad_norm=5.104808330535889, loss=0.6863192319869995
I0302 10:47:25.571101 140407371458304 logging_writer.py:48] [552600] global_step=552600, grad_norm=4.796825408935547, loss=0.6330513954162598
I0302 10:47:59.199433 140408319375104 logging_writer.py:48] [552700] global_step=552700, grad_norm=5.059511184692383, loss=0.5857850313186646
I0302 10:48:32.884270 140407371458304 logging_writer.py:48] [552800] global_step=552800, grad_norm=5.142670631408691, loss=0.6681607365608215
I0302 10:49:06.705813 140408319375104 logging_writer.py:48] [552900] global_step=552900, grad_norm=4.421649932861328, loss=0.5945735573768616
I0302 10:49:40.386384 140407371458304 logging_writer.py:48] [553000] global_step=553000, grad_norm=4.985879421234131, loss=0.6601765155792236
I0302 10:50:14.073753 140408319375104 logging_writer.py:48] [553100] global_step=553100, grad_norm=4.590516567230225, loss=0.6552170515060425
I0302 10:50:47.771507 140407371458304 logging_writer.py:48] [553200] global_step=553200, grad_norm=4.633366107940674, loss=0.6329625248908997
I0302 10:51:21.456504 140408319375104 logging_writer.py:48] [553300] global_step=553300, grad_norm=4.371769428253174, loss=0.5761159062385559
I0302 10:51:55.190644 140407371458304 logging_writer.py:48] [553400] global_step=553400, grad_norm=4.1925530433654785, loss=0.5916271209716797
I0302 10:52:28.824670 140408319375104 logging_writer.py:48] [553500] global_step=553500, grad_norm=4.12130880355835, loss=0.6032021045684814
I0302 10:53:02.493134 140407371458304 logging_writer.py:48] [553600] global_step=553600, grad_norm=4.329172611236572, loss=0.5914093852043152
I0302 10:53:36.188973 140408319375104 logging_writer.py:48] [553700] global_step=553700, grad_norm=4.9191508293151855, loss=0.6982376575469971
I0302 10:54:09.938972 140407371458304 logging_writer.py:48] [553800] global_step=553800, grad_norm=4.412801742553711, loss=0.6993767023086548
I0302 10:54:43.581153 140408319375104 logging_writer.py:48] [553900] global_step=553900, grad_norm=4.14548397064209, loss=0.5927860736846924
I0302 10:55:17.409987 140407371458304 logging_writer.py:48] [554000] global_step=554000, grad_norm=4.586772441864014, loss=0.6386677026748657
I0302 10:55:17.910341 140573303715648 spec.py:321] Evaluating on the training split.
I0302 10:55:23.949847 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 10:55:32.495637 140573303715648 spec.py:349] Evaluating on the test split.
I0302 10:55:34.828127 140573303715648 submission_runner.py:411] Time since start: 193258.82s, 	Step: 554003, 	{'train/accuracy': 0.9595423936843872, 'train/loss': 0.149004265666008, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.052505373954773, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8183035850524902, 'test/num_examples': 10000, 'score': 186718.42628335953, 'total_duration': 193258.82066512108, 'accumulated_submission_time': 186718.42628335953, 'accumulated_eval_time': 6489.370160579681, 'accumulated_logging_time': 29.125250101089478}
I0302 10:55:34.940329 140407379851008 logging_writer.py:48] [554003] accumulated_eval_time=6489.370161, accumulated_logging_time=29.125250, accumulated_submission_time=186718.426283, global_step=554003, preemption_count=0, score=186718.426283, test/accuracy=0.632300, test/loss=1.818304, test/num_examples=10000, total_duration=193258.820665, train/accuracy=0.959542, train/loss=0.149004, validation/accuracy=0.755360, validation/loss=1.052505, validation/num_examples=50000
I0302 10:56:08.026398 140408319375104 logging_writer.py:48] [554100] global_step=554100, grad_norm=4.279290199279785, loss=0.553503692150116
I0302 10:56:41.676995 140407379851008 logging_writer.py:48] [554200] global_step=554200, grad_norm=4.518703460693359, loss=0.6477628946304321
I0302 10:57:15.355647 140408319375104 logging_writer.py:48] [554300] global_step=554300, grad_norm=4.939280033111572, loss=0.6302993893623352
I0302 10:57:49.078834 140407379851008 logging_writer.py:48] [554400] global_step=554400, grad_norm=4.617255210876465, loss=0.6485308408737183
I0302 10:58:22.760786 140408319375104 logging_writer.py:48] [554500] global_step=554500, grad_norm=4.347465515136719, loss=0.5767970085144043
I0302 10:58:56.454105 140407379851008 logging_writer.py:48] [554600] global_step=554600, grad_norm=4.633889198303223, loss=0.6243162751197815
I0302 10:59:30.140257 140408319375104 logging_writer.py:48] [554700] global_step=554700, grad_norm=4.351146221160889, loss=0.6349430680274963
I0302 11:00:03.813454 140407379851008 logging_writer.py:48] [554800] global_step=554800, grad_norm=4.568512439727783, loss=0.6489832401275635
I0302 11:00:37.578542 140408319375104 logging_writer.py:48] [554900] global_step=554900, grad_norm=4.7937164306640625, loss=0.6551690101623535
I0302 11:01:11.372122 140407379851008 logging_writer.py:48] [555000] global_step=555000, grad_norm=4.279113292694092, loss=0.5735466480255127
I0302 11:01:45.065260 140408319375104 logging_writer.py:48] [555100] global_step=555100, grad_norm=4.516511917114258, loss=0.5792130827903748
I0302 11:02:18.717209 140407379851008 logging_writer.py:48] [555200] global_step=555200, grad_norm=4.229927062988281, loss=0.5916330814361572
I0302 11:02:52.430442 140408319375104 logging_writer.py:48] [555300] global_step=555300, grad_norm=4.757917404174805, loss=0.6583617925643921
I0302 11:03:26.135726 140407379851008 logging_writer.py:48] [555400] global_step=555400, grad_norm=4.107095241546631, loss=0.5547508001327515
I0302 11:03:59.822670 140408319375104 logging_writer.py:48] [555500] global_step=555500, grad_norm=4.263575553894043, loss=0.6040127873420715
I0302 11:04:05.018207 140573303715648 spec.py:321] Evaluating on the training split.
I0302 11:04:11.102214 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 11:04:19.484194 140573303715648 spec.py:349] Evaluating on the test split.
I0302 11:04:21.786418 140573303715648 submission_runner.py:411] Time since start: 193785.78s, 	Step: 555517, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.1476895958185196, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0525918006896973, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8194420337677002, 'test/num_examples': 10000, 'score': 187228.433716774, 'total_duration': 193785.77895975113, 'accumulated_submission_time': 187228.433716774, 'accumulated_eval_time': 6506.138321399689, 'accumulated_logging_time': 29.2472882270813}
I0302 11:04:21.896990 140407371458304 logging_writer.py:48] [555517] accumulated_eval_time=6506.138321, accumulated_logging_time=29.247288, accumulated_submission_time=187228.433717, global_step=555517, preemption_count=0, score=187228.433717, test/accuracy=0.631500, test/loss=1.819442, test/num_examples=10000, total_duration=193785.778960, train/accuracy=0.960658, train/loss=0.147690, validation/accuracy=0.755940, validation/loss=1.052592, validation/num_examples=50000
I0302 11:04:50.201389 140407379851008 logging_writer.py:48] [555600] global_step=555600, grad_norm=4.765164375305176, loss=0.7195708155632019
I0302 11:05:23.880594 140407371458304 logging_writer.py:48] [555700] global_step=555700, grad_norm=4.5631184577941895, loss=0.6408084630966187
I0302 11:05:57.513280 140407379851008 logging_writer.py:48] [555800] global_step=555800, grad_norm=4.315597057342529, loss=0.6306850910186768
I0302 11:06:31.186694 140407371458304 logging_writer.py:48] [555900] global_step=555900, grad_norm=3.949629068374634, loss=0.5603839159011841
I0302 11:07:04.905408 140407379851008 logging_writer.py:48] [556000] global_step=556000, grad_norm=5.3437042236328125, loss=0.6419585943222046
I0302 11:07:38.813347 140407371458304 logging_writer.py:48] [556100] global_step=556100, grad_norm=4.316189765930176, loss=0.6154568195343018
I0302 11:08:12.470607 140407379851008 logging_writer.py:48] [556200] global_step=556200, grad_norm=4.685598850250244, loss=0.7019529938697815
I0302 11:08:46.180272 140407371458304 logging_writer.py:48] [556300] global_step=556300, grad_norm=4.390452861785889, loss=0.5876147747039795
I0302 11:09:19.832290 140407379851008 logging_writer.py:48] [556400] global_step=556400, grad_norm=4.473074913024902, loss=0.6775774955749512
I0302 11:09:53.512557 140407371458304 logging_writer.py:48] [556500] global_step=556500, grad_norm=4.907181739807129, loss=0.737739086151123
I0302 11:10:27.152785 140407379851008 logging_writer.py:48] [556600] global_step=556600, grad_norm=4.409766674041748, loss=0.5765924453735352
I0302 11:11:00.871432 140407371458304 logging_writer.py:48] [556700] global_step=556700, grad_norm=5.050223350524902, loss=0.5823112726211548
I0302 11:11:34.508091 140407379851008 logging_writer.py:48] [556800] global_step=556800, grad_norm=4.647564888000488, loss=0.6567766070365906
I0302 11:12:08.176497 140407371458304 logging_writer.py:48] [556900] global_step=556900, grad_norm=4.012143135070801, loss=0.5706055760383606
I0302 11:12:41.831606 140407379851008 logging_writer.py:48] [557000] global_step=557000, grad_norm=4.544386386871338, loss=0.5881164073944092
I0302 11:12:52.076552 140573303715648 spec.py:321] Evaluating on the training split.
I0302 11:12:58.226957 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 11:13:06.905312 140573303715648 spec.py:349] Evaluating on the test split.
I0302 11:13:09.198988 140573303715648 submission_runner.py:411] Time since start: 194313.19s, 	Step: 557032, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14595405757427216, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.05264151096344, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.818196177482605, 'test/num_examples': 10000, 'score': 187738.54244542122, 'total_duration': 194313.19152712822, 'accumulated_submission_time': 187738.54244542122, 'accumulated_eval_time': 6523.260720252991, 'accumulated_logging_time': 29.367608070373535}
I0302 11:13:09.311076 140409846093568 logging_writer.py:48] [557032] accumulated_eval_time=6523.260720, accumulated_logging_time=29.367608, accumulated_submission_time=187738.542445, global_step=557032, preemption_count=0, score=187738.542445, test/accuracy=0.631100, test/loss=1.818196, test/num_examples=10000, total_duration=194313.191527, train/accuracy=0.960738, train/loss=0.145954, validation/accuracy=0.755840, validation/loss=1.052642, validation/num_examples=50000
I0302 11:13:32.670633 140409854486272 logging_writer.py:48] [557100] global_step=557100, grad_norm=4.581367492675781, loss=0.6020825505256653
I0302 11:14:06.320099 140409846093568 logging_writer.py:48] [557200] global_step=557200, grad_norm=4.6426591873168945, loss=0.6659177541732788
I0302 11:14:39.966539 140409854486272 logging_writer.py:48] [557300] global_step=557300, grad_norm=4.581770420074463, loss=0.611109733581543
I0302 11:15:13.660878 140409846093568 logging_writer.py:48] [557400] global_step=557400, grad_norm=4.74700927734375, loss=0.624863862991333
I0302 11:15:47.368216 140409854486272 logging_writer.py:48] [557500] global_step=557500, grad_norm=4.190113544464111, loss=0.6247509717941284
I0302 11:16:21.026421 140409846093568 logging_writer.py:48] [557600] global_step=557600, grad_norm=4.089890480041504, loss=0.6074358820915222
I0302 11:16:54.740535 140409854486272 logging_writer.py:48] [557700] global_step=557700, grad_norm=4.1450276374816895, loss=0.61224365234375
I0302 11:17:28.437227 140409846093568 logging_writer.py:48] [557800] global_step=557800, grad_norm=4.453846454620361, loss=0.6581524610519409
I0302 11:18:02.176344 140409854486272 logging_writer.py:48] [557900] global_step=557900, grad_norm=4.684438705444336, loss=0.6923399567604065
I0302 11:18:35.863152 140409846093568 logging_writer.py:48] [558000] global_step=558000, grad_norm=4.413052082061768, loss=0.6616185903549194
I0302 11:19:09.559751 140409854486272 logging_writer.py:48] [558100] global_step=558100, grad_norm=4.380405426025391, loss=0.5475926995277405
I0302 11:19:43.347812 140409846093568 logging_writer.py:48] [558200] global_step=558200, grad_norm=4.440515041351318, loss=0.597338855266571
I0302 11:20:17.055530 140409854486272 logging_writer.py:48] [558300] global_step=558300, grad_norm=4.09262752532959, loss=0.5597035884857178
I0302 11:20:50.720585 140409846093568 logging_writer.py:48] [558400] global_step=558400, grad_norm=4.922242641448975, loss=0.60283362865448
I0302 11:21:24.434134 140409854486272 logging_writer.py:48] [558500] global_step=558500, grad_norm=4.619376182556152, loss=0.5965262055397034
I0302 11:21:39.415956 140573303715648 spec.py:321] Evaluating on the training split.
I0302 11:21:45.470362 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 11:21:53.866366 140573303715648 spec.py:349] Evaluating on the test split.
I0302 11:21:56.188828 140573303715648 submission_runner.py:411] Time since start: 194840.18s, 	Step: 558546, 	{'train/accuracy': 0.9609972834587097, 'train/loss': 0.14530199766159058, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0516624450683594, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8173389434814453, 'test/num_examples': 10000, 'score': 188248.5768508911, 'total_duration': 194840.18134617805, 'accumulated_submission_time': 188248.5768508911, 'accumulated_eval_time': 6540.03351855278, 'accumulated_logging_time': 29.49054718017578}
I0302 11:21:56.327163 140407379851008 logging_writer.py:48] [558546] accumulated_eval_time=6540.033519, accumulated_logging_time=29.490547, accumulated_submission_time=188248.576851, global_step=558546, preemption_count=0, score=188248.576851, test/accuracy=0.631200, test/loss=1.817339, test/num_examples=10000, total_duration=194840.181346, train/accuracy=0.960997, train/loss=0.145302, validation/accuracy=0.755920, validation/loss=1.051662, validation/num_examples=50000
I0302 11:22:14.901571 140408319375104 logging_writer.py:48] [558600] global_step=558600, grad_norm=4.429727077484131, loss=0.6326101422309875
I0302 11:22:48.534232 140407379851008 logging_writer.py:48] [558700] global_step=558700, grad_norm=4.360327243804932, loss=0.5837445259094238
I0302 11:23:22.208712 140408319375104 logging_writer.py:48] [558800] global_step=558800, grad_norm=4.287967205047607, loss=0.6021924018859863
I0302 11:23:55.851178 140407379851008 logging_writer.py:48] [558900] global_step=558900, grad_norm=4.427668571472168, loss=0.5782780051231384
I0302 11:24:29.499715 140408319375104 logging_writer.py:48] [559000] global_step=559000, grad_norm=4.424278259277344, loss=0.629692792892456
I0302 11:25:03.234448 140407379851008 logging_writer.py:48] [559100] global_step=559100, grad_norm=4.462774276733398, loss=0.6345006227493286
I0302 11:25:37.039323 140408319375104 logging_writer.py:48] [559200] global_step=559200, grad_norm=4.685812950134277, loss=0.6115649938583374
I0302 11:26:10.751898 140407379851008 logging_writer.py:48] [559300] global_step=559300, grad_norm=4.6996870040893555, loss=0.6602609157562256
I0302 11:26:44.407015 140408319375104 logging_writer.py:48] [559400] global_step=559400, grad_norm=4.163329124450684, loss=0.611085832118988
I0302 11:27:18.044878 140407379851008 logging_writer.py:48] [559500] global_step=559500, grad_norm=5.377302169799805, loss=0.6877669095993042
I0302 11:27:51.723543 140408319375104 logging_writer.py:48] [559600] global_step=559600, grad_norm=4.334029674530029, loss=0.6794306039810181
I0302 11:28:25.442884 140407379851008 logging_writer.py:48] [559700] global_step=559700, grad_norm=4.559398651123047, loss=0.5832784175872803
I0302 11:28:59.166009 140408319375104 logging_writer.py:48] [559800] global_step=559800, grad_norm=4.513134956359863, loss=0.5859891772270203
I0302 11:29:32.910188 140407379851008 logging_writer.py:48] [559900] global_step=559900, grad_norm=4.49072265625, loss=0.6625763177871704
I0302 11:30:05.422789 140573303715648 spec.py:321] Evaluating on the training split.
I0302 11:30:11.481878 140573303715648 spec.py:333] Evaluating on the validation split.
I0302 11:30:19.933776 140573303715648 spec.py:349] Evaluating on the test split.
I0302 11:30:22.235018 140573303715648 submission_runner.py:411] Time since start: 195346.23s, 	Step: 559998, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14729043841362, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0520939826965332, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8173184394836426, 'test/num_examples': 10000, 'score': 188737.60403227806, 'total_duration': 195346.22756004333, 'accumulated_submission_time': 188737.60403227806, 'accumulated_eval_time': 6556.845715045929, 'accumulated_logging_time': 29.64098310470581}
I0302 11:30:22.345977 140409854486272 logging_writer.py:48] [559998] accumulated_eval_time=6556.845715, accumulated_logging_time=29.640983, accumulated_submission_time=188737.604032, global_step=559998, preemption_count=0, score=188737.604032, test/accuracy=0.631400, test/loss=1.817318, test/num_examples=10000, total_duration=195346.227560, train/accuracy=0.960858, train/loss=0.147290, validation/accuracy=0.756080, validation/loss=1.052094, validation/num_examples=50000
I0302 11:30:22.449794 140409862878976 logging_writer.py:48] [559998] global_step=559998, preemption_count=0, score=188737.604032
I0302 11:30:22.864439 140573303715648 checkpoints.py:490] Saving checkpoint at step: 559998
I0302 11:30:24.078363 140573303715648 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax/trial_1/checkpoint_559998
I0302 11:30:24.106858 140573303715648 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_3/imagenet_resnet_jax/trial_1/checkpoint_559998.
I0302 11:30:24.950352 140573303715648 submission_runner.py:676] Final imagenet_resnet score: 188737.60403227806
