python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_3 --overwrite=true --save_checkpoints=false --rng_seed=3502690548 --max_global_steps=399999 --tuning_ruleset=self 2>&1 | tee -a /logs/wmt_jax_03-12-2024-04-09-55.log
I0312 04:10:16.847820 139834281293632 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax because --overwrite was set.
I0312 04:10:16.850811 139834281293632 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax.
I0312 04:10:17.962567 139834281293632 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0312 04:10:17.963797 139834281293632 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0312 04:10:17.963958 139834281293632 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0312 04:10:18.908125 139834281293632 submission_runner.py:612] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax/trial_1.
I0312 04:10:19.120765 139834281293632 submission_runner.py:209] Initializing dataset.
I0312 04:10:19.132839 139834281293632 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:10:19.140302 139834281293632 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:10:19.288731 139834281293632 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:10:21.208614 139834281293632 submission_runner.py:220] Initializing model.
I0312 04:10:29.891123 139834281293632 submission_runner.py:262] Initializing optimizer.
I0312 04:10:30.880248 139834281293632 submission_runner.py:269] Initializing metrics bundle.
I0312 04:10:30.880446 139834281293632 submission_runner.py:287] Initializing checkpoint and logger.
I0312 04:10:30.881281 139834281293632 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax/trial_1 with prefix checkpoint_
I0312 04:10:30.881414 139834281293632 submission_runner.py:307] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax/trial_1/meta_data_0.json.
I0312 04:10:30.881648 139834281293632 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0312 04:10:30.881729 139834281293632 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0312 04:10:31.171361 139834281293632 logger_utils.py:220] Unable to record git information. Continuing without it.
I0312 04:10:31.437539 139834281293632 submission_runner.py:311] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax/trial_1/flags_0.json.
I0312 04:10:31.447452 139834281293632 submission_runner.py:321] Starting training loop.
I0312 04:11:08.994675 139669710812928 logging_writer.py:48] [0] global_step=0, grad_norm=5.213356018066406, loss=11.182037353515625
I0312 04:11:09.010558 139834281293632 spec.py:321] Evaluating on the training split.
I0312 04:11:09.014274 139834281293632 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:11:09.017465 139834281293632 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:11:09.056106 139834281293632 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:11:16.822746 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 04:16:09.163528 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 04:16:09.166806 139834281293632 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:16:09.170662 139834281293632 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:16:09.204276 139834281293632 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:16:15.851951 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 04:20:58.585028 139834281293632 spec.py:349] Evaluating on the test split.
I0312 04:20:58.587608 139834281293632 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:20:58.590433 139834281293632 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:20:58.625985 139834281293632 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:21:01.415945 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 04:25:44.241845 139834281293632 submission_runner.py:420] Time since start: 912.79s, 	Step: 1, 	{'train/accuracy': 0.00043934190762229264, 'train/loss': 11.199871063232422, 'train/bleu': 4.8358820565900907e-11, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.212837219238281, 'validation/bleu': 5.650264238095942e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.209013938903809, 'test/bleu': 2.4277196510573813e-10, 'test/num_examples': 3003, 'score': 37.56307125091553, 'total_duration': 912.794314622879, 'accumulated_submission_time': 37.56307125091553, 'accumulated_eval_time': 875.231207370758, 'accumulated_logging_time': 0}
I0312 04:25:44.261380 139664575932160 logging_writer.py:48] [1] accumulated_eval_time=875.231207, accumulated_logging_time=0, accumulated_submission_time=37.563071, global_step=1, preemption_count=0, score=37.563071, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.209014, test/num_examples=3003, total_duration=912.794315, train/accuracy=0.000439, train/bleu=0.000000, train/loss=11.199871, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.212837, validation/num_examples=3000
I0312 04:26:19.773184 139664567539456 logging_writer.py:48] [100] global_step=100, grad_norm=0.1469373106956482, loss=8.213343620300293
I0312 04:26:55.301365 139664575932160 logging_writer.py:48] [200] global_step=200, grad_norm=0.33080440759658813, loss=7.509772777557373
I0312 04:27:30.897491 139664567539456 logging_writer.py:48] [300] global_step=300, grad_norm=0.523612916469574, loss=6.851390838623047
I0312 04:28:06.481304 139664575932160 logging_writer.py:48] [400] global_step=400, grad_norm=0.44578367471694946, loss=6.305877208709717
I0312 04:28:42.060650 139664567539456 logging_writer.py:48] [500] global_step=500, grad_norm=0.524145245552063, loss=5.917375564575195
I0312 04:29:17.668058 139664575932160 logging_writer.py:48] [600] global_step=600, grad_norm=0.38458338379859924, loss=5.577060699462891
I0312 04:29:53.279542 139664567539456 logging_writer.py:48] [700] global_step=700, grad_norm=0.6974735856056213, loss=5.36085844039917
I0312 04:30:28.907992 139664575932160 logging_writer.py:48] [800] global_step=800, grad_norm=0.6120167970657349, loss=5.0599751472473145
I0312 04:31:04.514890 139664567539456 logging_writer.py:48] [900] global_step=900, grad_norm=0.5850392580032349, loss=4.827595233917236
I0312 04:31:40.135883 139664575932160 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5808860063552856, loss=4.590226650238037
I0312 04:32:15.786834 139664567539456 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5106044411659241, loss=4.363474369049072
I0312 04:32:51.396224 139664575932160 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4731607735157013, loss=3.973982572555542
I0312 04:33:26.996743 139664567539456 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5378528833389282, loss=3.9052977561950684
I0312 04:34:02.593099 139664575932160 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5965867042541504, loss=3.8373806476593018
I0312 04:34:38.163902 139664567539456 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.45145413279533386, loss=3.5729150772094727
I0312 04:35:13.774447 139664575932160 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4924553334712982, loss=3.4437010288238525
I0312 04:35:49.354144 139664567539456 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.45678314566612244, loss=3.3697798252105713
I0312 04:36:24.967883 139664575932160 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.3982168436050415, loss=3.261728048324585
I0312 04:37:00.557714 139664567539456 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.42257916927337646, loss=3.245424509048462
I0312 04:37:36.168375 139664575932160 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5098631978034973, loss=3.2186076641082764
I0312 04:38:11.790281 139664567539456 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.39229416847229004, loss=3.0240797996520996
I0312 04:38:47.431937 139664575932160 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.41104838252067566, loss=3.0622472763061523
I0312 04:39:23.060244 139664567539456 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.36739906668663025, loss=3.040445566177368
I0312 04:39:44.486039 139834281293632 spec.py:321] Evaluating on the training split.
I0312 04:39:47.460283 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 04:42:26.551625 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 04:42:29.225124 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 04:45:09.436163 139834281293632 spec.py:349] Evaluating on the test split.
I0312 04:45:12.106194 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 04:47:30.667675 139834281293632 submission_runner.py:420] Time since start: 2219.22s, 	Step: 2362, 	{'train/accuracy': 0.5115175843238831, 'train/loss': 2.8756186962127686, 'train/bleu': 22.22694865293249, 'validation/accuracy': 0.5132980346679688, 'validation/loss': 2.8582029342651367, 'validation/bleu': 18.48454646661626, 'validation/num_examples': 3000, 'test/accuracy': 0.5118703246116638, 'test/loss': 2.9111058712005615, 'test/bleu': 17.134560690987676, 'test/num_examples': 3003, 'score': 877.7042384147644, 'total_duration': 2219.220136165619, 'accumulated_submission_time': 877.7042384147644, 'accumulated_eval_time': 1341.4127733707428, 'accumulated_logging_time': 0.030313491821289062}
I0312 04:47:30.681572 139664575932160 logging_writer.py:48] [2362] accumulated_eval_time=1341.412773, accumulated_logging_time=0.030313, accumulated_submission_time=877.704238, global_step=2362, preemption_count=0, score=877.704238, test/accuracy=0.511870, test/bleu=17.134561, test/loss=2.911106, test/num_examples=3003, total_duration=2219.220136, train/accuracy=0.511518, train/bleu=22.226949, train/loss=2.875619, validation/accuracy=0.513298, validation/bleu=18.484546, validation/loss=2.858203, validation/num_examples=3000
I0312 04:47:44.554387 139664567539456 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.40150439739227295, loss=2.927546739578247
I0312 04:48:20.120624 139664575932160 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.32818666100502014, loss=2.9162471294403076
I0312 04:48:55.692998 139664567539456 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.27551764249801636, loss=2.9021835327148438
I0312 04:49:31.260334 139664575932160 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3913949728012085, loss=2.833430767059326
I0312 04:50:06.827542 139664567539456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.34121230244636536, loss=2.820610523223877
I0312 04:50:42.389468 139664575932160 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.38241228461265564, loss=2.8377082347869873
I0312 04:51:17.961437 139664567539456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2499876171350479, loss=2.7209110260009766
I0312 04:51:53.554202 139664575932160 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2828657329082489, loss=2.6616883277893066
I0312 04:52:29.165305 139664567539456 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3817179203033447, loss=2.603696346282959
I0312 04:53:04.716962 139664575932160 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.21966008841991425, loss=2.581425666809082
I0312 04:53:40.273513 139664567539456 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2422853410243988, loss=2.6005117893218994
I0312 04:54:15.867193 139664575932160 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.19342835247516632, loss=2.4536471366882324
I0312 04:54:51.484012 139664567539456 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.22635146975517273, loss=2.515794515609741
I0312 04:55:27.115038 139664575932160 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.25250697135925293, loss=2.5121281147003174
I0312 04:56:02.715414 139664567539456 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.21814610064029694, loss=2.496314287185669
I0312 04:56:38.264005 139664575932160 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.19793866574764252, loss=2.523616075515747
I0312 04:57:13.849052 139664567539456 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.17151878774166107, loss=2.572460412979126
I0312 04:57:49.431275 139664575932160 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.19049057364463806, loss=2.4010329246520996
I0312 04:58:25.060925 139664567539456 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.19186750054359436, loss=2.380791187286377
I0312 04:59:00.703102 139664575932160 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.17703589797019958, loss=2.3606863021850586
I0312 04:59:36.285159 139664567539456 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.17523235082626343, loss=2.394775867462158
I0312 05:00:11.880941 139664575932160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.1720692664384842, loss=2.263523578643799
I0312 05:00:47.457975 139664567539456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.1505773663520813, loss=2.3321402072906494
I0312 05:01:23.023539 139664575932160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1755443662405014, loss=2.3017306327819824
I0312 05:01:30.911988 139834281293632 spec.py:321] Evaluating on the training split.
I0312 05:01:33.875647 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:05:43.385886 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 05:05:46.049005 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:08:23.368143 139834281293632 spec.py:349] Evaluating on the test split.
I0312 05:08:26.031094 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:10:50.586455 139834281293632 submission_runner.py:420] Time since start: 3619.14s, 	Step: 4724, 	{'train/accuracy': 0.5785503387451172, 'train/loss': 2.2474136352539062, 'train/bleu': 27.207589751580535, 'validation/accuracy': 0.5907180309295654, 'validation/loss': 2.1485509872436523, 'validation/bleu': 23.473832158007244, 'validation/num_examples': 3000, 'test/accuracy': 0.5931090712547302, 'test/loss': 2.126178741455078, 'test/bleu': 21.86833526566419, 'test/num_examples': 3003, 'score': 1717.85214304924, 'total_duration': 3619.1389334201813, 'accumulated_submission_time': 1717.85214304924, 'accumulated_eval_time': 1901.0871896743774, 'accumulated_logging_time': 0.05343770980834961}
I0312 05:10:50.600613 139664567539456 logging_writer.py:48] [4724] accumulated_eval_time=1901.087190, accumulated_logging_time=0.053438, accumulated_submission_time=1717.852143, global_step=4724, preemption_count=0, score=1717.852143, test/accuracy=0.593109, test/bleu=21.868335, test/loss=2.126179, test/num_examples=3003, total_duration=3619.138933, train/accuracy=0.578550, train/bleu=27.207590, train/loss=2.247414, validation/accuracy=0.590718, validation/bleu=23.473832, validation/loss=2.148551, validation/num_examples=3000
I0312 05:11:17.968444 139664575932160 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.17055821418762207, loss=2.332028388977051
I0312 05:11:53.497878 139664567539456 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.16951783001422882, loss=2.32092022895813
I0312 05:12:29.068302 139664575932160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.150112584233284, loss=2.248832941055298
I0312 05:13:04.615135 139664567539456 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.24967873096466064, loss=2.2999839782714844
I0312 05:13:40.194172 139664575932160 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.15872856974601746, loss=2.2466492652893066
I0312 05:14:15.777853 139664567539456 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.15323399007320404, loss=2.26790714263916
I0312 05:14:51.333878 139664575932160 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.16055706143379211, loss=2.2178032398223877
I0312 05:15:26.931103 139664567539456 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.20222929120063782, loss=2.367727279663086
I0312 05:16:02.573296 139664575932160 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.17138265073299408, loss=2.243917226791382
I0312 05:16:38.132443 139664567539456 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.20206452906131744, loss=2.337672472000122
I0312 05:17:13.680291 139664575932160 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.16177406907081604, loss=2.139031410217285
I0312 05:17:49.269495 139664567539456 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.173325315117836, loss=2.192301034927368
I0312 05:18:24.848197 139664575932160 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.17457327246665955, loss=2.3280134201049805
I0312 05:19:00.427855 139664567539456 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.17616508901119232, loss=2.206125259399414
I0312 05:19:36.025180 139664575932160 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.16329455375671387, loss=2.2053890228271484
I0312 05:20:11.622051 139664567539456 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.13918545842170715, loss=2.1672441959381104
I0312 05:20:47.185316 139664575932160 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.1616085022687912, loss=2.15578031539917
I0312 05:21:22.784298 139664567539456 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.2036423236131668, loss=2.180103063583374
I0312 05:21:58.360966 139664575932160 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.17310185730457306, loss=2.223889112472534
I0312 05:22:33.933994 139664567539456 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.216115802526474, loss=2.120269775390625
I0312 05:23:09.491905 139664575932160 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.14696478843688965, loss=2.1739635467529297
I0312 05:23:45.062864 139664567539456 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.166451558470726, loss=2.1262388229370117
I0312 05:24:20.623279 139664575932160 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.2159290462732315, loss=2.1593101024627686
I0312 05:24:50.917537 139834281293632 spec.py:321] Evaluating on the training split.
I0312 05:24:53.887964 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:27:22.544130 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 05:27:25.211593 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:29:52.943516 139834281293632 spec.py:349] Evaluating on the test split.
I0312 05:29:55.616975 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:32:08.338971 139834281293632 submission_runner.py:420] Time since start: 4896.89s, 	Step: 7087, 	{'train/accuracy': 0.6039042472839355, 'train/loss': 2.021613597869873, 'train/bleu': 29.456385494006174, 'validation/accuracy': 0.6163345575332642, 'validation/loss': 1.9296290874481201, 'validation/bleu': 25.38209760851271, 'validation/num_examples': 3000, 'test/accuracy': 0.6229620575904846, 'test/loss': 1.8805855512619019, 'test/bleu': 24.412837919066433, 'test/num_examples': 3003, 'score': 2558.0870866775513, 'total_duration': 4896.891449689865, 'accumulated_submission_time': 2558.0870866775513, 'accumulated_eval_time': 2338.5085804462433, 'accumulated_logging_time': 0.07808709144592285}
I0312 05:32:08.353190 139664567539456 logging_writer.py:48] [7087] accumulated_eval_time=2338.508580, accumulated_logging_time=0.078087, accumulated_submission_time=2558.087087, global_step=7087, preemption_count=0, score=2558.087087, test/accuracy=0.622962, test/bleu=24.412838, test/loss=1.880586, test/num_examples=3003, total_duration=4896.891450, train/accuracy=0.603904, train/bleu=29.456385, train/loss=2.021614, validation/accuracy=0.616335, validation/bleu=25.382098, validation/loss=1.929629, validation/num_examples=3000
I0312 05:32:13.343432 139664575932160 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.14433535933494568, loss=2.1321494579315186
I0312 05:32:48.840615 139664567539456 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.16732867062091827, loss=2.1866085529327393
I0312 05:33:24.401907 139664575932160 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.15301860868930817, loss=2.048112392425537
I0312 05:33:59.976260 139664567539456 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.1519838273525238, loss=2.114180326461792
I0312 05:34:35.507809 139664575932160 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.1638435572385788, loss=2.0375921726226807
I0312 05:35:11.067656 139664567539456 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.16112938523292542, loss=2.166440725326538
I0312 05:35:46.637454 139664575932160 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.19167737662792206, loss=2.1162633895874023
I0312 05:36:22.237159 139664567539456 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1782091110944748, loss=2.007756233215332
I0312 05:36:57.810338 139664575932160 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.17402951419353485, loss=2.0809054374694824
I0312 05:37:33.398149 139664567539456 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.23954074084758759, loss=2.138286828994751
I0312 05:38:09.007723 139664575932160 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.164444699883461, loss=2.074286937713623
I0312 05:38:44.547467 139664567539456 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17707879841327667, loss=2.1647322177886963
I0312 05:39:20.119626 139664575932160 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.17437101900577545, loss=2.092160224914551
I0312 05:39:55.669850 139664567539456 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.17391572892665863, loss=2.08437442779541
I0312 05:40:31.245143 139664575932160 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.15053686499595642, loss=2.0653703212738037
I0312 05:41:06.791240 139664567539456 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.17241324484348297, loss=1.933880090713501
I0312 05:41:42.341263 139664575932160 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.20749907195568085, loss=2.0957071781158447
I0312 05:42:17.892549 139664567539456 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.18786750733852386, loss=1.9957605600357056
I0312 05:42:53.479642 139664575932160 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.19806161522865295, loss=2.08732271194458
I0312 05:43:29.053912 139664567539456 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.16026772558689117, loss=2.0440967082977295
I0312 05:44:04.671826 139664575932160 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.14621470868587494, loss=2.0887107849121094
I0312 05:44:40.303787 139664567539456 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.14937390387058258, loss=2.1283974647521973
I0312 05:45:15.925933 139664575932160 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.15271778404712677, loss=1.9844043254852295
I0312 05:45:51.520362 139664567539456 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.15412582457065582, loss=2.007439613342285
I0312 05:46:08.667635 139834281293632 spec.py:321] Evaluating on the training split.
I0312 05:46:11.643850 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:48:52.117262 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 05:48:54.815632 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:51:19.867141 139834281293632 spec.py:349] Evaluating on the test split.
I0312 05:51:22.534195 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 05:53:40.938783 139834281293632 submission_runner.py:420] Time since start: 6189.49s, 	Step: 9450, 	{'train/accuracy': 0.6103935837745667, 'train/loss': 1.9617842435836792, 'train/bleu': 29.466753468752557, 'validation/accuracy': 0.6299116015434265, 'validation/loss': 1.8166937828063965, 'validation/bleu': 26.3658578673267, 'validation/num_examples': 3000, 'test/accuracy': 0.6379989981651306, 'test/loss': 1.7546569108963013, 'test/bleu': 25.231059478606504, 'test/num_examples': 3003, 'score': 3398.317975282669, 'total_duration': 6189.491263628006, 'accumulated_submission_time': 3398.317975282669, 'accumulated_eval_time': 2790.7796771526337, 'accumulated_logging_time': 0.10334300994873047}
I0312 05:53:40.954681 139664575932160 logging_writer.py:48] [9450] accumulated_eval_time=2790.779677, accumulated_logging_time=0.103343, accumulated_submission_time=3398.317975, global_step=9450, preemption_count=0, score=3398.317975, test/accuracy=0.637999, test/bleu=25.231059, test/loss=1.754657, test/num_examples=3003, total_duration=6189.491264, train/accuracy=0.610394, train/bleu=29.466753, train/loss=1.961784, validation/accuracy=0.629912, validation/bleu=26.365858, validation/loss=1.816694, validation/num_examples=3000
I0312 05:53:59.060309 139664567539456 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.16812239587306976, loss=2.0586605072021484
I0312 05:54:34.624342 139664575932160 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.16703921556472778, loss=2.0091700553894043
I0312 05:55:10.170630 139664567539456 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.22244903445243835, loss=1.9465091228485107
I0312 05:55:45.702166 139664575932160 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1518022119998932, loss=1.9943324327468872
I0312 05:56:21.257043 139664567539456 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.18706625699996948, loss=2.0265119075775146
I0312 05:56:56.866890 139664575932160 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.15038014948368073, loss=2.0003082752227783
I0312 05:57:32.428996 139664567539456 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.146348774433136, loss=2.050978899002075
I0312 05:58:07.951018 139664575932160 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.193440243601799, loss=1.9942618608474731
I0312 05:58:43.518170 139664567539456 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.15391437709331512, loss=1.926851749420166
I0312 05:59:19.078109 139664575932160 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.15159766376018524, loss=2.0032846927642822
I0312 05:59:54.644439 139664567539456 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2144605815410614, loss=2.020069122314453
I0312 06:00:30.210007 139664575932160 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.20506539940834045, loss=1.9656118154525757
I0312 06:01:05.768986 139664567539456 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.16286684572696686, loss=2.0240797996520996
I0312 06:01:41.296621 139664575932160 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.16168677806854248, loss=2.0707082748413086
I0312 06:02:16.830353 139664567539456 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.170492485165596, loss=2.0429980754852295
I0312 06:02:52.358281 139664575932160 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.16083912551403046, loss=2.059154987335205
I0312 06:03:27.882606 139664567539456 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.18689408898353577, loss=1.9794634580612183
I0312 06:04:03.416893 139664575932160 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.31008148193359375, loss=1.9307447671890259
I0312 06:04:38.950108 139664567539456 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.16759763658046722, loss=2.039139986038208
I0312 06:05:14.535786 139664575932160 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.18001127243041992, loss=1.941686749458313
I0312 06:05:50.091904 139664567539456 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17704807221889496, loss=1.9631843566894531
I0312 06:06:25.618582 139664575932160 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.18839424848556519, loss=1.996101975440979
I0312 06:07:01.163708 139664567539456 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.18095709383487701, loss=1.930668592453003
I0312 06:07:36.711604 139664575932160 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1549254208803177, loss=1.906927227973938
I0312 06:07:41.042603 139834281293632 spec.py:321] Evaluating on the training split.
I0312 06:07:44.004998 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:10:43.219392 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 06:10:45.886157 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:13:11.097333 139834281293632 spec.py:349] Evaluating on the test split.
I0312 06:13:13.770254 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:15:22.728754 139834281293632 submission_runner.py:420] Time since start: 7491.28s, 	Step: 11814, 	{'train/accuracy': 0.6192044615745544, 'train/loss': 1.901554822921753, 'train/bleu': 29.886880074281077, 'validation/accuracy': 0.6387893557548523, 'validation/loss': 1.749039649963379, 'validation/bleu': 26.820170193034105, 'validation/num_examples': 3000, 'test/accuracy': 0.6472139954566956, 'test/loss': 1.68716299533844, 'test/bleu': 25.661115609964735, 'test/num_examples': 3003, 'score': 4238.3237154483795, 'total_duration': 7491.281229496002, 'accumulated_submission_time': 4238.3237154483795, 'accumulated_eval_time': 3252.4657728672028, 'accumulated_logging_time': 0.13004493713378906}
I0312 06:15:22.744389 139664567539456 logging_writer.py:48] [11814] accumulated_eval_time=3252.465773, accumulated_logging_time=0.130045, accumulated_submission_time=4238.323715, global_step=11814, preemption_count=0, score=4238.323715, test/accuracy=0.647214, test/bleu=25.661116, test/loss=1.687163, test/num_examples=3003, total_duration=7491.281229, train/accuracy=0.619204, train/bleu=29.886880, train/loss=1.901555, validation/accuracy=0.638789, validation/bleu=26.820170, validation/loss=1.749040, validation/num_examples=3000
I0312 06:15:53.624868 139664575932160 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.30526748299598694, loss=2.0075523853302
I0312 06:16:29.162365 139664567539456 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.20215201377868652, loss=1.9700801372528076
I0312 06:17:04.717244 139664575932160 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.25665226578712463, loss=1.9728769063949585
I0312 06:17:40.348169 139664567539456 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.18230798840522766, loss=1.9587095975875854
I0312 06:18:15.901929 139664575932160 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.22569209337234497, loss=2.0240895748138428
I0312 06:18:51.453797 139664567539456 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.1663721650838852, loss=1.862370491027832
I0312 06:19:27.058739 139664575932160 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2056906372308731, loss=2.0674149990081787
I0312 06:20:02.690067 139664567539456 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2204573005437851, loss=1.9472719430923462
I0312 06:20:38.261013 139664575932160 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.17333552241325378, loss=1.9605399370193481
I0312 06:21:13.852955 139664567539456 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.21828708052635193, loss=1.939483404159546
I0312 06:21:49.482325 139664575932160 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.26568594574928284, loss=1.9247276782989502
I0312 06:22:25.066591 139664567539456 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.259049654006958, loss=1.995390772819519
I0312 06:23:00.628447 139664575932160 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.17643597722053528, loss=1.9057902097702026
I0312 06:23:36.215307 139664567539456 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.1892959624528885, loss=1.9816093444824219
I0312 06:24:11.791253 139664575932160 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.18781276047229767, loss=1.9065386056900024
I0312 06:24:47.365090 139664567539456 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.19039222598075867, loss=1.8965623378753662
I0312 06:25:22.961728 139664575932160 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.15729010105133057, loss=1.9757812023162842
I0312 06:25:58.543073 139664567539456 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.16981902718544006, loss=1.9115246534347534
I0312 06:26:34.139748 139664575932160 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.1865505874156952, loss=1.9722238779067993
I0312 06:27:09.718264 139664567539456 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.19182302057743073, loss=1.852563500404358
I0312 06:27:45.322226 139664575932160 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.19975221157073975, loss=1.958174467086792
I0312 06:28:20.896137 139664567539456 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.19370822608470917, loss=1.9812300205230713
I0312 06:28:56.464339 139664575932160 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.18766604363918304, loss=1.9078395366668701
I0312 06:29:22.882427 139834281293632 spec.py:321] Evaluating on the training split.
I0312 06:29:25.850543 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:33:17.632538 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 06:33:20.304498 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:35:55.890007 139834281293632 spec.py:349] Evaluating on the test split.
I0312 06:35:58.554716 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:38:18.153552 139834281293632 submission_runner.py:420] Time since start: 8866.71s, 	Step: 14176, 	{'train/accuracy': 0.6295682191848755, 'train/loss': 1.819696068763733, 'train/bleu': 30.651995885245505, 'validation/accuracy': 0.6438357830047607, 'validation/loss': 1.7108651399612427, 'validation/bleu': 27.24148470012765, 'validation/num_examples': 3000, 'test/accuracy': 0.6528964042663574, 'test/loss': 1.6480978727340698, 'test/bleu': 26.293182704377568, 'test/num_examples': 3003, 'score': 5078.380287885666, 'total_duration': 8866.706029176712, 'accumulated_submission_time': 5078.380287885666, 'accumulated_eval_time': 3787.7368624210358, 'accumulated_logging_time': 0.15613007545471191}
I0312 06:38:18.169630 139664567539456 logging_writer.py:48] [14176] accumulated_eval_time=3787.736862, accumulated_logging_time=0.156130, accumulated_submission_time=5078.380288, global_step=14176, preemption_count=0, score=5078.380288, test/accuracy=0.652896, test/bleu=26.293183, test/loss=1.648098, test/num_examples=3003, total_duration=8866.706029, train/accuracy=0.629568, train/bleu=30.651996, train/loss=1.819696, validation/accuracy=0.643836, validation/bleu=27.241485, validation/loss=1.710865, validation/num_examples=3000
I0312 06:38:27.058648 139664575932160 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.21544836461544037, loss=1.923512578010559
I0312 06:39:02.523148 139664567539456 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.23084042966365814, loss=1.9033621549606323
I0312 06:39:38.051374 139664575932160 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.19857607781887054, loss=1.9858908653259277
I0312 06:40:13.610582 139664567539456 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.19787608087062836, loss=1.998092770576477
I0312 06:40:49.287824 139664575932160 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.18810760974884033, loss=1.990350604057312
I0312 06:41:24.889706 139664567539456 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.15759508311748505, loss=1.9240509271621704
I0312 06:42:00.494575 139664575932160 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.1818317174911499, loss=1.9188053607940674
I0312 06:42:36.075719 139664567539456 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.2584574520587921, loss=1.9747562408447266
I0312 06:43:11.645338 139664575932160 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18162068724632263, loss=1.896955132484436
I0312 06:43:47.215658 139664567539456 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1898755431175232, loss=1.9105936288833618
I0312 06:44:22.790498 139664575932160 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2230934351682663, loss=1.9144713878631592
I0312 06:44:58.380488 139664567539456 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.1795346587896347, loss=1.9824001789093018
I0312 06:45:33.970678 139664575932160 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.18481199443340302, loss=1.891471266746521
I0312 06:46:09.562061 139664567539456 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18364129960536957, loss=1.81959068775177
I0312 06:46:45.190965 139664575932160 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.180719256401062, loss=1.9809167385101318
I0312 06:47:20.757516 139664567539456 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.21625064313411713, loss=1.8733890056610107
I0312 06:47:56.371799 139664575932160 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2463865429162979, loss=1.912570595741272
I0312 06:48:31.991086 139664567539456 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.16383418440818787, loss=1.9377256631851196
I0312 06:49:07.650332 139664575932160 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1562010496854782, loss=1.9520093202590942
I0312 06:49:43.224290 139664567539456 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1869788020849228, loss=1.9283726215362549
I0312 06:50:18.813660 139664575932160 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2532442510128021, loss=1.9120023250579834
I0312 06:50:54.393343 139664567539456 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.24903613328933716, loss=1.971688151359558
I0312 06:51:29.983066 139664575932160 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2196626216173172, loss=1.91254460811615
I0312 06:52:05.569030 139664567539456 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.22532998025417328, loss=1.9087907075881958
I0312 06:52:18.442901 139834281293632 spec.py:321] Evaluating on the training split.
I0312 06:52:21.412497 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:55:18.751786 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 06:55:21.423678 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 06:57:50.123907 139834281293632 spec.py:349] Evaluating on the test split.
I0312 06:57:52.806237 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:00:05.428116 139834281293632 submission_runner.py:420] Time since start: 10173.98s, 	Step: 16538, 	{'train/accuracy': 0.6271182894706726, 'train/loss': 1.8289114236831665, 'train/bleu': 30.539107747251062, 'validation/accuracy': 0.6476299166679382, 'validation/loss': 1.6841576099395752, 'validation/bleu': 27.438749015513427, 'validation/num_examples': 3000, 'test/accuracy': 0.6548022031784058, 'test/loss': 1.6183347702026367, 'test/bleu': 26.148766488248864, 'test/num_examples': 3003, 'score': 5918.573413133621, 'total_duration': 10173.98059129715, 'accumulated_submission_time': 5918.573413133621, 'accumulated_eval_time': 4254.722022771835, 'accumulated_logging_time': 0.18175792694091797}
I0312 07:00:05.444124 139664575932160 logging_writer.py:48] [16538] accumulated_eval_time=4254.722023, accumulated_logging_time=0.181758, accumulated_submission_time=5918.573413, global_step=16538, preemption_count=0, score=5918.573413, test/accuracy=0.654802, test/bleu=26.148766, test/loss=1.618335, test/num_examples=3003, total_duration=10173.980591, train/accuracy=0.627118, train/bleu=30.539108, train/loss=1.828911, validation/accuracy=0.647630, validation/bleu=27.438749, validation/loss=1.684158, validation/num_examples=3000
I0312 07:00:27.822505 139664567539456 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.21413327753543854, loss=1.864747166633606
I0312 07:01:03.314697 139664575932160 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.19346274435520172, loss=1.8804987668991089
I0312 07:01:38.863172 139664567539456 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.17886286973953247, loss=1.8128255605697632
I0312 07:02:14.439648 139664575932160 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.17716044187545776, loss=1.8690698146820068
I0312 07:02:50.011639 139664567539456 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2304636389017105, loss=1.9321218729019165
I0312 07:03:25.570808 139664575932160 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.20217843353748322, loss=1.9770832061767578
I0312 07:04:01.139732 139664567539456 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.1955406367778778, loss=1.8784037828445435
I0312 07:04:36.710228 139664575932160 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.18451839685440063, loss=1.9207172393798828
I0312 07:05:12.348214 139664567539456 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.18460936844348907, loss=1.9026989936828613
I0312 07:05:47.924438 139664575932160 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.17517633736133575, loss=1.9014817476272583
I0312 07:06:23.511244 139664567539456 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.18588785827159882, loss=1.9350459575653076
I0312 07:06:59.102872 139664575932160 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.18976260721683502, loss=1.8800188302993774
I0312 07:07:34.674199 139664567539456 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.18667452037334442, loss=1.8795738220214844
I0312 07:08:10.256610 139664575932160 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.2372644990682602, loss=1.9325885772705078
I0312 07:08:45.919685 139664567539456 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.19429001212120056, loss=1.7970836162567139
I0312 07:09:21.529628 139664575932160 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.20613569021224976, loss=1.8667058944702148
I0312 07:09:57.058198 139664567539456 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.19756294786930084, loss=1.9221121072769165
I0312 07:10:32.640642 139664575932160 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.24390293657779694, loss=1.8722296953201294
I0312 07:11:08.191186 139664567539456 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.1949610412120819, loss=1.9087382555007935
I0312 07:11:43.802800 139664575932160 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.21120722591876984, loss=1.8618526458740234
I0312 07:12:19.378152 139664567539456 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.20070262253284454, loss=1.8672738075256348
I0312 07:12:54.949230 139664575932160 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.18743382394313812, loss=1.7707384824752808
I0312 07:13:30.500863 139664567539456 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.17927740514278412, loss=1.8199931383132935
I0312 07:14:06.044724 139664575932160 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.20843945443630219, loss=1.8666770458221436
I0312 07:14:06.050696 139834281293632 spec.py:321] Evaluating on the training split.
I0312 07:14:08.723675 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:17:32.511432 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 07:17:35.183494 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:19:53.840744 139834281293632 spec.py:349] Evaluating on the test split.
I0312 07:19:56.511147 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:22:07.619438 139834281293632 submission_runner.py:420] Time since start: 11496.17s, 	Step: 18901, 	{'train/accuracy': 0.6572504639625549, 'train/loss': 1.6087989807128906, 'train/bleu': 32.22472034445667, 'validation/accuracy': 0.651436448097229, 'validation/loss': 1.6518349647521973, 'validation/bleu': 27.6112143291353, 'validation/num_examples': 3000, 'test/accuracy': 0.6612864136695862, 'test/loss': 1.5909876823425293, 'test/bleu': 26.802334863576778, 'test/num_examples': 3003, 'score': 6759.098012447357, 'total_duration': 11496.171914815903, 'accumulated_submission_time': 6759.098012447357, 'accumulated_eval_time': 4736.290683746338, 'accumulated_logging_time': 0.20765352249145508}
I0312 07:22:07.636364 139664567539456 logging_writer.py:48] [18901] accumulated_eval_time=4736.290684, accumulated_logging_time=0.207654, accumulated_submission_time=6759.098012, global_step=18901, preemption_count=0, score=6759.098012, test/accuracy=0.661286, test/bleu=26.802335, test/loss=1.590988, test/num_examples=3003, total_duration=11496.171915, train/accuracy=0.657250, train/bleu=32.224720, train/loss=1.608799, validation/accuracy=0.651436, validation/bleu=27.611214, validation/loss=1.651835, validation/num_examples=3000
I0312 07:22:43.139083 139664575932160 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.19638608396053314, loss=1.8240572214126587
I0312 07:23:18.651484 139664567539456 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.16740961372852325, loss=1.792198657989502
I0312 07:23:54.204167 139664575932160 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.24412943422794342, loss=1.903802752494812
I0312 07:24:29.747964 139664567539456 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.18421436846256256, loss=1.8327869176864624
I0312 07:25:05.330391 139664575932160 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1789606660604477, loss=1.8454554080963135
I0312 07:25:40.915323 139664567539456 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.18607603013515472, loss=1.882969856262207
I0312 07:26:16.474486 139664575932160 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2316163331270218, loss=1.8870677947998047
I0312 07:26:52.095767 139664567539456 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.21330301463603973, loss=1.8527013063430786
I0312 07:27:27.675034 139664575932160 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.16851232945919037, loss=1.8884197473526
I0312 07:28:03.257904 139664567539456 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16705290973186493, loss=1.908724308013916
I0312 07:28:38.827755 139664575932160 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.18779930472373962, loss=1.926527500152588
I0312 07:29:14.426695 139664567539456 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.17922884225845337, loss=1.763485074043274
I0312 07:29:50.030417 139664575932160 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.18942546844482422, loss=1.8566330671310425
I0312 07:30:25.601714 139664567539456 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.21072688698768616, loss=1.8759877681732178
I0312 07:31:01.244296 139664575932160 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.17689423263072968, loss=1.891263484954834
I0312 07:31:36.893116 139664567539456 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.22031749784946442, loss=1.8563032150268555
I0312 07:32:12.447566 139664575932160 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.2591536045074463, loss=1.7606267929077148
I0312 07:32:48.049698 139664567539456 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.20333755016326904, loss=1.8625962734222412
I0312 07:33:23.625429 139664575932160 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.24110709130764008, loss=1.8522543907165527
I0312 07:33:59.183721 139664567539456 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.24649833142757416, loss=1.7564386129379272
I0312 07:34:34.762802 139664575932160 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.24872954189777374, loss=1.8591680526733398
I0312 07:35:10.345304 139664567539456 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.20110252499580383, loss=1.8300899267196655
I0312 07:35:45.930068 139664575932160 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.17301173508167267, loss=1.9324800968170166
I0312 07:36:07.694035 139834281293632 spec.py:321] Evaluating on the training split.
I0312 07:36:10.660778 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:39:54.235984 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 07:39:56.905816 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:42:36.268876 139834281293632 spec.py:349] Evaluating on the test split.
I0312 07:42:38.957153 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 07:45:42.843605 139834281293632 submission_runner.py:420] Time since start: 12911.40s, 	Step: 21263, 	{'train/accuracy': 0.6349077224731445, 'train/loss': 1.7626363039016724, 'train/bleu': 30.705749056075486, 'validation/accuracy': 0.6522423624992371, 'validation/loss': 1.6453105211257935, 'validation/bleu': 27.61326392444762, 'validation/num_examples': 3000, 'test/accuracy': 0.6627622246742249, 'test/loss': 1.5679388046264648, 'test/bleu': 26.717003601278886, 'test/num_examples': 3003, 'score': 7599.073549985886, 'total_duration': 12911.396076202393, 'accumulated_submission_time': 7599.073549985886, 'accumulated_eval_time': 5311.4401948452, 'accumulated_logging_time': 0.23539137840270996}
I0312 07:45:42.861173 139664567539456 logging_writer.py:48] [21263] accumulated_eval_time=5311.440195, accumulated_logging_time=0.235391, accumulated_submission_time=7599.073550, global_step=21263, preemption_count=0, score=7599.073550, test/accuracy=0.662762, test/bleu=26.717004, test/loss=1.567939, test/num_examples=3003, total_duration=12911.396076, train/accuracy=0.634908, train/bleu=30.705749, train/loss=1.762636, validation/accuracy=0.652242, validation/bleu=27.613264, validation/loss=1.645311, validation/num_examples=3000
I0312 07:45:56.331578 139664575932160 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.20865103602409363, loss=1.9244695901870728
I0312 07:46:31.838275 139664567539456 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.18097369372844696, loss=1.8760908842086792
I0312 07:47:07.398507 139664575932160 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.1802005022764206, loss=1.9184110164642334
I0312 07:47:42.981029 139664567539456 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.22481414675712585, loss=1.8953815698623657
I0312 07:48:18.551816 139664575932160 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.1954532116651535, loss=1.7588175535202026
I0312 07:48:54.126531 139664567539456 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.1925380975008011, loss=1.7799636125564575
I0312 07:49:29.745489 139664575932160 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.18750548362731934, loss=1.6998714208602905
I0312 07:50:05.318812 139664567539456 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.1687842756509781, loss=1.8135640621185303
I0312 07:50:40.924040 139664575932160 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.29350441694259644, loss=1.8506606817245483
I0312 07:51:16.508202 139664567539456 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.2363775223493576, loss=1.8655927181243896
I0312 07:51:52.087245 139664575932160 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.20366215705871582, loss=1.748180866241455
I0312 07:52:27.643308 139664567539456 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.20747435092926025, loss=1.9581847190856934
I0312 07:53:03.189650 139664575932160 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.19751010835170746, loss=1.832167148590088
I0312 07:53:38.786639 139664567539456 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.18899567425251007, loss=1.7833397388458252
I0312 07:54:14.313009 139664575932160 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.19714787602424622, loss=1.9799137115478516
I0312 07:54:49.877318 139664567539456 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2417629212141037, loss=1.815399408340454
I0312 07:55:25.466876 139664575932160 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.21341656148433685, loss=1.8989423513412476
I0312 07:56:01.028576 139664567539456 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.2695586681365967, loss=1.886603593826294
I0312 07:56:36.619149 139664575932160 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.18145132064819336, loss=1.7739063501358032
I0312 07:57:12.195532 139664567539456 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2010146677494049, loss=1.8239390850067139
I0312 07:57:47.796183 139664575932160 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.19274461269378662, loss=1.8760020732879639
I0312 07:58:23.375435 139664567539456 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.20009402930736542, loss=1.8852415084838867
I0312 07:58:58.966068 139664575932160 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.17446468770503998, loss=1.7297028303146362
I0312 07:59:34.511995 139664567539456 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2663972079753876, loss=1.9107266664505005
I0312 07:59:43.121307 139834281293632 spec.py:321] Evaluating on the training split.
I0312 07:59:46.083536 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:03:53.780771 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 08:03:56.452250 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:06:36.800213 139834281293632 spec.py:349] Evaluating on the test split.
I0312 08:06:39.472249 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:09:00.881848 139834281293632 submission_runner.py:420] Time since start: 14309.43s, 	Step: 23626, 	{'train/accuracy': 0.6350238919258118, 'train/loss': 1.7771228551864624, 'train/bleu': 30.852382591325615, 'validation/accuracy': 0.6539162397384644, 'validation/loss': 1.631062388420105, 'validation/bleu': 27.745339331702276, 'validation/num_examples': 3000, 'test/accuracy': 0.6658416390419006, 'test/loss': 1.549932837486267, 'test/bleu': 27.201885738538692, 'test/num_examples': 3003, 'score': 8439.251418828964, 'total_duration': 14309.434327602386, 'accumulated_submission_time': 8439.251418828964, 'accumulated_eval_time': 5869.200685739517, 'accumulated_logging_time': 0.26202821731567383}
I0312 08:09:00.899510 139664575932160 logging_writer.py:48] [23626] accumulated_eval_time=5869.200686, accumulated_logging_time=0.262028, accumulated_submission_time=8439.251419, global_step=23626, preemption_count=0, score=8439.251419, test/accuracy=0.665842, test/bleu=27.201886, test/loss=1.549933, test/num_examples=3003, total_duration=14309.434328, train/accuracy=0.635024, train/bleu=30.852383, train/loss=1.777123, validation/accuracy=0.653916, validation/bleu=27.745339, validation/loss=1.631062, validation/num_examples=3000
I0312 08:09:27.504568 139664567539456 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.20949633419513702, loss=1.8733325004577637
I0312 08:10:03.001857 139664575932160 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.1691669076681137, loss=1.8274919986724854
I0312 08:10:38.567829 139664567539456 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.1760018765926361, loss=1.8264408111572266
I0312 08:11:14.160409 139664575932160 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2829011082649231, loss=1.8590680360794067
I0312 08:11:49.739133 139664567539456 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.19764350354671478, loss=1.8499583005905151
I0312 08:12:25.300947 139664575932160 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2058420032262802, loss=1.8641057014465332
I0312 08:13:00.881221 139664567539456 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.20429255068302155, loss=1.8120431900024414
I0312 08:13:36.428650 139664575932160 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.1860421597957611, loss=1.7867687940597534
I0312 08:14:12.004185 139664567539456 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.25972914695739746, loss=1.8206583261489868
I0312 08:14:47.559790 139664575932160 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.19497555494308472, loss=1.869128942489624
I0312 08:15:23.111309 139664567539456 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.1947031170129776, loss=1.8040868043899536
I0312 08:15:58.697427 139664575932160 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.20205096900463104, loss=1.8860799074172974
I0312 08:16:34.262799 139664567539456 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.19528082013130188, loss=1.8644596338272095
I0312 08:17:09.819082 139664575932160 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3054329454898834, loss=1.8677151203155518
I0312 08:17:45.419262 139664567539456 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.24897629022598267, loss=1.822685956954956
I0312 08:18:20.984277 139664575932160 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.17853635549545288, loss=1.8685455322265625
I0312 08:18:56.597063 139664567539456 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.20309659838676453, loss=1.876656174659729
I0312 08:19:32.187993 139664575932160 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.18864792585372925, loss=1.8259632587432861
I0312 08:20:07.815761 139664567539456 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2611180245876312, loss=1.8969521522521973
I0312 08:20:43.441220 139664575932160 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.20809824764728546, loss=1.7363967895507812
I0312 08:21:19.046536 139664567539456 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.2676948308944702, loss=1.838233232498169
I0312 08:21:54.626289 139664575932160 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.20423391461372375, loss=1.8141156435012817
I0312 08:22:30.186890 139664567539456 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.1958266943693161, loss=1.8072983026504517
I0312 08:23:01.184834 139834281293632 spec.py:321] Evaluating on the training split.
I0312 08:23:04.151603 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:25:54.017257 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 08:25:56.681167 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:28:22.982656 139834281293632 spec.py:349] Evaluating on the test split.
I0312 08:28:25.659295 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:30:47.222476 139834281293632 submission_runner.py:420] Time since start: 15615.77s, 	Step: 25989, 	{'train/accuracy': 0.6467454433441162, 'train/loss': 1.6869769096374512, 'train/bleu': 31.536803516208945, 'validation/accuracy': 0.6557761430740356, 'validation/loss': 1.6167680025100708, 'validation/bleu': 27.694900356711823, 'validation/num_examples': 3000, 'test/accuracy': 0.6658997535705566, 'test/loss': 1.5409635305404663, 'test/bleu': 27.170063679613282, 'test/num_examples': 3003, 'score': 9279.454516649246, 'total_duration': 15615.774932384491, 'accumulated_submission_time': 9279.454516649246, 'accumulated_eval_time': 6335.238257408142, 'accumulated_logging_time': 0.29000329971313477}
I0312 08:30:47.239824 139664575932160 logging_writer.py:48] [25989] accumulated_eval_time=6335.238257, accumulated_logging_time=0.290003, accumulated_submission_time=9279.454517, global_step=25989, preemption_count=0, score=9279.454517, test/accuracy=0.665900, test/bleu=27.170064, test/loss=1.540964, test/num_examples=3003, total_duration=15615.774932, train/accuracy=0.646745, train/bleu=31.536804, train/loss=1.686977, validation/accuracy=0.655776, validation/bleu=27.694900, validation/loss=1.616768, validation/num_examples=3000
I0312 08:30:51.518172 139664567539456 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.19434402883052826, loss=1.7941062450408936
I0312 08:31:27.019824 139664575932160 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.21882691979408264, loss=1.7546625137329102
I0312 08:32:02.571250 139664567539456 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.41933685541152954, loss=1.8440275192260742
I0312 08:32:38.140291 139664575932160 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.1919356882572174, loss=1.7767902612686157
I0312 08:33:13.692761 139664567539456 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.17818745970726013, loss=1.7885096073150635
I0312 08:33:49.232190 139664575932160 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1845300793647766, loss=1.858916163444519
I0312 08:34:24.790723 139664567539456 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.25678908824920654, loss=1.7946622371673584
I0312 08:35:00.336008 139664575932160 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.24803070724010468, loss=1.8270227909088135
I0312 08:35:35.933194 139664567539456 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.19719915091991425, loss=1.8276458978652954
I0312 08:36:11.495822 139664575932160 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.17942661046981812, loss=1.7755420207977295
I0312 08:36:47.061207 139664567539456 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2556346356868744, loss=1.884164571762085
I0312 08:37:22.603402 139664575932160 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.19145521521568298, loss=1.7650302648544312
I0312 08:37:58.175007 139664567539456 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.18623673915863037, loss=1.8293852806091309
I0312 08:38:33.683413 139664575932160 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.23191656172275543, loss=1.7396117448806763
I0312 08:39:09.289690 139664567539456 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.22624430060386658, loss=1.8460663557052612
I0312 08:39:44.866923 139664575932160 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19981427490711212, loss=1.8131276369094849
I0312 08:40:20.418124 139664567539456 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2153448611497879, loss=1.8534060716629028
I0312 08:40:55.971858 139664575932160 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.19610537588596344, loss=1.8386625051498413
I0312 08:41:31.531269 139664567539456 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.27149698138237, loss=1.9087330102920532
I0312 08:42:07.114207 139664575932160 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.228753462433815, loss=1.831879734992981
I0312 08:42:42.665851 139664567539456 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.17305709421634674, loss=1.8448004722595215
I0312 08:43:18.249581 139664575932160 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.18688739836215973, loss=1.8408454656600952
I0312 08:43:53.822861 139664567539456 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.19338524341583252, loss=1.7847254276275635
I0312 08:44:29.375263 139664575932160 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2166527658700943, loss=1.7778476476669312
I0312 08:44:47.238615 139834281293632 spec.py:321] Evaluating on the training split.
I0312 08:44:50.203209 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:48:02.413985 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 08:48:05.081260 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:50:36.785569 139834281293632 spec.py:349] Evaluating on the test split.
I0312 08:50:39.459031 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 08:52:59.683244 139834281293632 submission_runner.py:420] Time since start: 16948.24s, 	Step: 28352, 	{'train/accuracy': 0.6393489837646484, 'train/loss': 1.732313871383667, 'train/bleu': 30.885567339140408, 'validation/accuracy': 0.6574996113777161, 'validation/loss': 1.6025561094284058, 'validation/bleu': 27.800652263048256, 'validation/num_examples': 3000, 'test/accuracy': 0.6687235236167908, 'test/loss': 1.5303418636322021, 'test/bleu': 27.420597130706373, 'test/num_examples': 3003, 'score': 10119.372649908066, 'total_duration': 16948.23571062088, 'accumulated_submission_time': 10119.372649908066, 'accumulated_eval_time': 6827.682823181152, 'accumulated_logging_time': 0.3175230026245117}
I0312 08:52:59.701867 139664567539456 logging_writer.py:48] [28352] accumulated_eval_time=6827.682823, accumulated_logging_time=0.317523, accumulated_submission_time=10119.372650, global_step=28352, preemption_count=0, score=10119.372650, test/accuracy=0.668724, test/bleu=27.420597, test/loss=1.530342, test/num_examples=3003, total_duration=16948.235711, train/accuracy=0.639349, train/bleu=30.885567, train/loss=1.732314, validation/accuracy=0.657500, validation/bleu=27.800652, validation/loss=1.602556, validation/num_examples=3000
I0312 08:53:17.128551 139664575932160 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.22344842553138733, loss=1.8493432998657227
I0312 08:53:52.766453 139664567539456 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.19887180626392365, loss=1.826128363609314
I0312 08:54:28.379116 139664575932160 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.2052099108695984, loss=1.8751661777496338
I0312 08:55:03.996827 139664567539456 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.2301996648311615, loss=1.8225127458572388
I0312 08:55:39.575665 139664575932160 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1966310739517212, loss=1.7913466691970825
I0312 08:56:15.186370 139664567539456 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.19765329360961914, loss=1.8633650541305542
I0312 08:56:50.712287 139664575932160 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.20214501023292542, loss=1.7736777067184448
I0312 08:57:26.278068 139664567539456 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.21605239808559418, loss=1.7915699481964111
I0312 08:58:01.842287 139664575932160 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.20460723340511322, loss=1.829307198524475
I0312 08:58:37.420576 139664567539456 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.21463331580162048, loss=1.854658842086792
I0312 08:59:12.977020 139664575932160 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.21019038558006287, loss=1.786678433418274
I0312 08:59:48.557948 139664567539456 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3114610016345978, loss=1.8032498359680176
I0312 09:00:24.135777 139664575932160 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.21724851429462433, loss=1.8590201139450073
I0312 09:00:59.678034 139664567539456 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.21431976556777954, loss=1.7453577518463135
I0312 09:01:35.236861 139664575932160 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.18167029321193695, loss=1.7329140901565552
I0312 09:02:10.844127 139664567539456 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.21449176967144012, loss=1.712446689605713
I0312 09:02:46.469832 139664575932160 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.19205045700073242, loss=1.8256192207336426
I0312 09:03:22.082027 139664567539456 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.218562513589859, loss=1.8234752416610718
I0312 09:03:57.674961 139664575932160 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.19389422237873077, loss=1.8365131616592407
I0312 09:04:33.266144 139664567539456 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.24220798909664154, loss=1.7865341901779175
I0312 09:05:08.829743 139664575932160 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.20950908958911896, loss=1.7501221895217896
I0312 09:05:44.379386 139664567539456 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.20833195745944977, loss=1.8683816194534302
I0312 09:06:19.924971 139664575932160 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.22210773825645447, loss=1.8570976257324219
I0312 09:06:55.479497 139664567539456 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.18737848103046417, loss=1.814880609512329
I0312 09:06:59.826100 139834281293632 spec.py:321] Evaluating on the training split.
I0312 09:07:02.800743 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:10:37.028243 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 09:10:39.697622 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:13:04.250141 139834281293632 spec.py:349] Evaluating on the test split.
I0312 09:13:06.924541 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:15:22.316207 139834281293632 submission_runner.py:420] Time since start: 18290.87s, 	Step: 30714, 	{'train/accuracy': 0.6419368982315063, 'train/loss': 1.733702540397644, 'train/bleu': 31.349138689209163, 'validation/accuracy': 0.6595702171325684, 'validation/loss': 1.598743200302124, 'validation/bleu': 28.190826470172745, 'validation/num_examples': 3000, 'test/accuracy': 0.6724769473075867, 'test/loss': 1.5186891555786133, 'test/bleu': 27.439883614878845, 'test/num_examples': 3003, 'score': 10959.414959907532, 'total_duration': 18290.868687152863, 'accumulated_submission_time': 10959.414959907532, 'accumulated_eval_time': 7330.172876596451, 'accumulated_logging_time': 0.345379114151001}
I0312 09:15:22.334051 139664575932160 logging_writer.py:48] [30714] accumulated_eval_time=7330.172877, accumulated_logging_time=0.345379, accumulated_submission_time=10959.414960, global_step=30714, preemption_count=0, score=10959.414960, test/accuracy=0.672477, test/bleu=27.439884, test/loss=1.518689, test/num_examples=3003, total_duration=18290.868687, train/accuracy=0.641937, train/bleu=31.349139, train/loss=1.733703, validation/accuracy=0.659570, validation/bleu=28.190826, validation/loss=1.598743, validation/num_examples=3000
I0312 09:15:53.196970 139664567539456 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.22069065272808075, loss=1.733890414237976
I0312 09:16:28.710461 139664575932160 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.19092987477779388, loss=1.735092043876648
I0312 09:17:04.265990 139664567539456 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.22239911556243896, loss=1.803795576095581
I0312 09:17:39.820016 139664575932160 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.1853248029947281, loss=1.7429747581481934
I0312 09:18:15.369458 139664567539456 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.22306588292121887, loss=1.749220848083496
I0312 09:18:50.936077 139664575932160 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.20859602093696594, loss=1.820505142211914
I0312 09:19:26.501247 139664567539456 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20198822021484375, loss=1.848474144935608
I0312 09:20:02.053460 139664575932160 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.19186708331108093, loss=1.804567813873291
I0312 09:20:37.620705 139664567539456 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.19832029938697815, loss=1.760549783706665
I0312 09:21:13.186319 139664575932160 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.19041286408901215, loss=1.8429193496704102
I0312 09:21:48.760457 139664567539456 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.23844106495380402, loss=1.7901479005813599
I0312 09:22:24.393295 139664575932160 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.19813543558120728, loss=1.845590353012085
I0312 09:22:59.996551 139664567539456 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.19584353268146515, loss=1.8224948644638062
I0312 09:23:35.568505 139664575932160 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2076892852783203, loss=1.8240890502929688
I0312 09:24:11.185895 139664567539456 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.19694708287715912, loss=1.786528468132019
I0312 09:24:46.769771 139664575932160 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.17236176133155823, loss=1.7656638622283936
I0312 09:25:22.359229 139664567539456 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.1932995766401291, loss=1.8197603225708008
I0312 09:25:57.933589 139664575932160 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.18645671010017395, loss=1.7649027109146118
I0312 09:26:33.527574 139664567539456 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.19972148537635803, loss=1.791413426399231
I0312 09:27:09.096708 139664575932160 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.20724381506443024, loss=1.7499984502792358
I0312 09:27:44.676191 139664567539456 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.24175061285495758, loss=1.8178194761276245
I0312 09:28:20.249532 139664575932160 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.28756341338157654, loss=1.8481988906860352
I0312 09:28:55.831694 139664567539456 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2155122011899948, loss=1.7685750722885132
I0312 09:29:22.584303 139834281293632 spec.py:321] Evaluating on the training split.
I0312 09:29:25.541821 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:32:28.039575 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 09:32:30.708655 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:35:01.308275 139834281293632 spec.py:349] Evaluating on the test split.
I0312 09:35:03.990808 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:37:27.780337 139834281293632 submission_runner.py:420] Time since start: 19616.33s, 	Step: 33077, 	{'train/accuracy': 0.6454497575759888, 'train/loss': 1.7001781463623047, 'train/bleu': 31.41565148825551, 'validation/accuracy': 0.6618516445159912, 'validation/loss': 1.5838629007339478, 'validation/bleu': 28.273850035586964, 'validation/num_examples': 3000, 'test/accuracy': 0.6725350022315979, 'test/loss': 1.5059255361557007, 'test/bleu': 27.469167647169623, 'test/num_examples': 3003, 'score': 11799.583815813065, 'total_duration': 19616.332770109177, 'accumulated_submission_time': 11799.583815813065, 'accumulated_eval_time': 7815.368814945221, 'accumulated_logging_time': 0.3722670078277588}
I0312 09:37:27.798452 139664575932160 logging_writer.py:48] [33077] accumulated_eval_time=7815.368815, accumulated_logging_time=0.372267, accumulated_submission_time=11799.583816, global_step=33077, preemption_count=0, score=11799.583816, test/accuracy=0.672535, test/bleu=27.469168, test/loss=1.505926, test/num_examples=3003, total_duration=19616.332770, train/accuracy=0.645450, train/bleu=31.415651, train/loss=1.700178, validation/accuracy=0.661852, validation/bleu=28.273850, validation/loss=1.583863, validation/num_examples=3000
I0312 09:37:36.326244 139664567539456 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.19070890545845032, loss=1.7868291139602661
I0312 09:38:11.804927 139664575932160 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.19084171950817108, loss=1.7532258033752441
I0312 09:38:47.361536 139664567539456 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.20644983649253845, loss=1.7425880432128906
I0312 09:39:22.912683 139664575932160 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.3029311001300812, loss=1.750465750694275
I0312 09:39:58.449253 139664567539456 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.18459287285804749, loss=1.7544490098953247
I0312 09:40:34.002085 139664575932160 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.22777609527111053, loss=1.7262766361236572
I0312 09:41:09.598855 139664567539456 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.23964175581932068, loss=1.7669687271118164
I0312 09:41:45.142085 139664575932160 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.20038484036922455, loss=1.7553205490112305
I0312 09:42:20.722287 139664567539456 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.1716635376214981, loss=1.779504418373108
I0312 09:42:56.262819 139664575932160 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.22510376572608948, loss=1.8066004514694214
I0312 09:43:31.873085 139664567539456 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.20101772248744965, loss=1.7589126825332642
I0312 09:44:07.467142 139664575932160 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.1833595186471939, loss=1.8795145750045776
I0312 09:44:43.075847 139664567539456 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.19453436136245728, loss=1.7470331192016602
I0312 09:45:18.667187 139664575932160 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.18791647255420685, loss=1.7984635829925537
I0312 09:45:54.235082 139664567539456 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1977030485868454, loss=1.78762686252594
I0312 09:46:29.810423 139664575932160 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.28804928064346313, loss=1.8141436576843262
I0312 09:47:05.390446 139664567539456 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.19583414494991302, loss=1.7778687477111816
I0312 09:47:40.949261 139664575932160 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.24424931406974792, loss=1.808772087097168
I0312 09:48:16.530817 139664567539456 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1860283762216568, loss=1.7566571235656738
I0312 09:48:52.116512 139664575932160 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.17987893521785736, loss=1.7133913040161133
I0312 09:49:27.719559 139664567539456 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.326591432094574, loss=1.7687746286392212
I0312 09:50:03.300409 139664575932160 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.20946893095970154, loss=1.8504438400268555
I0312 09:50:38.903504 139664567539456 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.23544952273368835, loss=1.830308198928833
I0312 09:51:14.456124 139664575932160 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.20046094059944153, loss=1.795833945274353
I0312 09:51:28.046269 139834281293632 spec.py:321] Evaluating on the training split.
I0312 09:51:31.003171 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:55:09.481235 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 09:55:12.147605 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 09:59:08.011851 139834281293632 spec.py:349] Evaluating on the test split.
I0312 09:59:10.669395 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:02:47.020379 139834281293632 submission_runner.py:420] Time since start: 21135.57s, 	Step: 35440, 	{'train/accuracy': 0.6440091729164124, 'train/loss': 1.7076191902160645, 'train/bleu': 31.415912395473594, 'validation/accuracy': 0.6626452207565308, 'validation/loss': 1.580370545387268, 'validation/bleu': 27.312578619277826, 'validation/num_examples': 3000, 'test/accuracy': 0.6744408011436462, 'test/loss': 1.50215744972229, 'test/bleu': 27.711011086782264, 'test/num_examples': 3003, 'score': 12639.751176595688, 'total_duration': 21135.57282590866, 'accumulated_submission_time': 12639.751176595688, 'accumulated_eval_time': 8494.342839956284, 'accumulated_logging_time': 0.39959216117858887}
I0312 10:02:47.038493 139664567539456 logging_writer.py:48] [35440] accumulated_eval_time=8494.342840, accumulated_logging_time=0.399592, accumulated_submission_time=12639.751177, global_step=35440, preemption_count=0, score=12639.751177, test/accuracy=0.674441, test/bleu=27.711011, test/loss=1.502157, test/num_examples=3003, total_duration=21135.572826, train/accuracy=0.644009, train/bleu=31.415912, train/loss=1.707619, validation/accuracy=0.662645, validation/bleu=27.312579, validation/loss=1.580371, validation/num_examples=3000
I0312 10:03:08.674862 139664575932160 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2009773850440979, loss=1.8768218755722046
I0312 10:03:44.231946 139664567539456 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.2183515578508377, loss=1.7675679922103882
I0312 10:04:19.825930 139664575932160 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.1850319802761078, loss=1.8207790851593018
I0312 10:04:55.372788 139664567539456 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.19951963424682617, loss=1.720928430557251
I0312 10:05:30.975353 139664575932160 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.17904765903949738, loss=1.7659246921539307
I0312 10:06:06.601859 139664567539456 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.18362097442150116, loss=1.778552770614624
I0312 10:06:42.156990 139664575932160 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.19569770991802216, loss=1.760002851486206
I0312 10:07:17.764546 139664567539456 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.19672498106956482, loss=1.8172039985656738
I0312 10:07:53.372266 139664575932160 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2087981253862381, loss=1.7471674680709839
I0312 10:08:28.931710 139664567539456 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.20353570580482483, loss=1.8003500699996948
I0312 10:09:04.518291 139664575932160 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.20149031281471252, loss=1.8632913827896118
I0312 10:09:40.100580 139664567539456 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.2155357450246811, loss=1.7687907218933105
I0312 10:10:15.685420 139664575932160 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.20768123865127563, loss=1.7981332540512085
I0312 10:10:51.234490 139664567539456 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.4150574207305908, loss=1.8682371377944946
I0312 10:11:26.793892 139664575932160 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.1993740051984787, loss=1.740371823310852
I0312 10:12:02.359019 139664567539456 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.19183599948883057, loss=1.7673170566558838
I0312 10:12:37.959649 139664575932160 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.2028840184211731, loss=1.730196475982666
I0312 10:13:13.528062 139664567539456 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.20201490819454193, loss=1.817586064338684
I0312 10:13:49.089654 139664575932160 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2182278037071228, loss=1.770537257194519
I0312 10:14:24.661556 139664567539456 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.19013085961341858, loss=1.7462964057922363
I0312 10:15:00.227123 139664575932160 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.2074771374464035, loss=1.8125743865966797
I0312 10:15:35.775680 139664567539456 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.21318846940994263, loss=1.8376014232635498
I0312 10:16:11.326420 139664575932160 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.21703793108463287, loss=1.8352224826812744
I0312 10:16:46.876705 139664567539456 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.19477704167366028, loss=1.6893261671066284
I0312 10:16:47.307143 139834281293632 spec.py:321] Evaluating on the training split.
I0312 10:16:50.280692 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:20:06.021929 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 10:20:08.680653 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:23:01.657341 139834281293632 spec.py:349] Evaluating on the test split.
I0312 10:23:04.324206 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:25:22.463310 139834281293632 submission_runner.py:420] Time since start: 22491.02s, 	Step: 37803, 	{'train/accuracy': 0.662269115447998, 'train/loss': 1.5625323057174683, 'train/bleu': 32.457272192649356, 'validation/accuracy': 0.6636123657226562, 'validation/loss': 1.5751655101776123, 'validation/bleu': 28.33616940026225, 'validation/num_examples': 3000, 'test/accuracy': 0.6750218272209167, 'test/loss': 1.493459939956665, 'test/bleu': 27.95044166549037, 'test/num_examples': 3003, 'score': 13479.938393354416, 'total_duration': 22491.015778541565, 'accumulated_submission_time': 13479.938393354416, 'accumulated_eval_time': 9009.498941421509, 'accumulated_logging_time': 0.4266171455383301}
I0312 10:25:22.484768 139664575932160 logging_writer.py:48] [37803] accumulated_eval_time=9009.498941, accumulated_logging_time=0.426617, accumulated_submission_time=13479.938393, global_step=37803, preemption_count=0, score=13479.938393, test/accuracy=0.675022, test/bleu=27.950442, test/loss=1.493460, test/num_examples=3003, total_duration=22491.015779, train/accuracy=0.662269, train/bleu=32.457272, train/loss=1.562532, validation/accuracy=0.663612, validation/bleu=28.336169, validation/loss=1.575166, validation/num_examples=3000
I0312 10:25:57.260469 139664567539456 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.18856853246688843, loss=1.8041677474975586
I0312 10:26:32.778873 139664575932160 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.19060638546943665, loss=1.7707165479660034
I0312 10:27:08.319248 139664567539456 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.19638898968696594, loss=1.7482149600982666
I0312 10:27:43.892082 139664575932160 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1913357377052307, loss=1.8108028173446655
I0312 10:28:19.486349 139664567539456 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.1931357979774475, loss=1.6895262002944946
I0312 10:28:55.057477 139664575932160 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.1896786093711853, loss=1.6982206106185913
I0312 10:29:30.646589 139664567539456 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22098711133003235, loss=1.7819502353668213
I0312 10:30:06.225708 139664575932160 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.20038597285747528, loss=1.7455991506576538
I0312 10:30:41.821160 139664567539456 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2063940465450287, loss=1.7647485733032227
I0312 10:31:17.397574 139664575932160 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.38937610387802124, loss=1.8359071016311646
I0312 10:31:52.963079 139664567539456 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.21647655963897705, loss=1.8215173482894897
I0312 10:32:28.528511 139664575932160 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.19171321392059326, loss=1.8133851289749146
I0312 10:33:04.077734 139664567539456 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.200051948428154, loss=1.7164762020111084
I0312 10:33:39.613898 139664575932160 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.20797409117221832, loss=1.8022267818450928
I0312 10:34:15.181185 139664567539456 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.19827710092067719, loss=1.8283846378326416
I0312 10:34:50.718560 139664575932160 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.19189730286598206, loss=1.763575553894043
I0312 10:35:26.369213 139664567539456 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.19476789236068726, loss=1.8282054662704468
I0312 10:36:01.964035 139664575932160 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.2039974331855774, loss=1.7083162069320679
I0312 10:36:37.619753 139664567539456 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.1766928732395172, loss=1.737516164779663
I0312 10:37:13.220943 139664575932160 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.20899264514446259, loss=1.7088557481765747
I0312 10:37:48.791225 139664567539456 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.22248575091362, loss=1.763824224472046
I0312 10:38:24.394546 139664575932160 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.20016121864318848, loss=1.7317181825637817
I0312 10:38:59.989058 139664567539456 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.7485991716384888, loss=1.7709200382232666
I0312 10:39:22.484913 139834281293632 spec.py:321] Evaluating on the training split.
I0312 10:39:25.451235 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:43:26.937140 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 10:43:29.604611 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:46:31.538639 139834281293632 spec.py:349] Evaluating on the test split.
I0312 10:46:34.199935 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 10:49:42.723243 139834281293632 submission_runner.py:420] Time since start: 23951.28s, 	Step: 40165, 	{'train/accuracy': 0.6452246308326721, 'train/loss': 1.6897084712982178, 'train/bleu': 31.456404929888553, 'validation/accuracy': 0.6639595031738281, 'validation/loss': 1.5691026449203491, 'validation/bleu': 28.455962457363174, 'validation/num_examples': 3000, 'test/accuracy': 0.6757073998451233, 'test/loss': 1.4875746965408325, 'test/bleu': 27.787590158145814, 'test/num_examples': 3003, 'score': 14319.857029676437, 'total_duration': 23951.275722503662, 'accumulated_submission_time': 14319.857029676437, 'accumulated_eval_time': 9629.737220525742, 'accumulated_logging_time': 0.4572136402130127}
I0312 10:49:42.742530 139664575932160 logging_writer.py:48] [40165] accumulated_eval_time=9629.737221, accumulated_logging_time=0.457214, accumulated_submission_time=14319.857030, global_step=40165, preemption_count=0, score=14319.857030, test/accuracy=0.675707, test/bleu=27.787590, test/loss=1.487575, test/num_examples=3003, total_duration=23951.275723, train/accuracy=0.645225, train/bleu=31.456405, train/loss=1.689708, validation/accuracy=0.663960, validation/bleu=28.455962, validation/loss=1.569103, validation/num_examples=3000
I0312 10:49:55.554952 139664567539456 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19345782697200775, loss=1.7636277675628662
I0312 10:50:31.116216 139664575932160 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.21379120647907257, loss=1.747960090637207
I0312 10:51:06.675295 139664567539456 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.324599951505661, loss=1.8191763162612915
I0312 10:51:42.225030 139664575932160 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.19333235919475555, loss=1.7644579410552979
I0312 10:52:17.829011 139664567539456 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20172534883022308, loss=1.7144713401794434
I0312 10:52:53.395650 139664575932160 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.19200696051120758, loss=1.7285162210464478
I0312 10:53:28.967932 139664567539456 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.19425155222415924, loss=1.8156746625900269
I0312 10:54:04.575129 139664575932160 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.2008286714553833, loss=1.8377723693847656
I0312 10:54:40.164812 139664567539456 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.22255612909793854, loss=1.7277840375900269
I0312 10:55:15.753469 139664575932160 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1827760636806488, loss=1.7932602167129517
I0312 10:55:51.345975 139664567539456 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.22568485140800476, loss=1.6998800039291382
I0312 10:56:26.919281 139664575932160 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.22975876927375793, loss=1.858322024345398
I0312 10:57:02.487613 139664567539456 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.22038522362709045, loss=1.8520814180374146
I0312 10:57:38.095865 139664575932160 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.18778561055660248, loss=1.7803436517715454
I0312 10:58:13.681032 139664567539456 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.23337247967720032, loss=1.7940884828567505
I0312 10:58:49.298364 139664575932160 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.19765515625476837, loss=1.7560245990753174
I0312 10:59:24.956406 139664567539456 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.19164282083511353, loss=1.7814087867736816
I0312 11:00:00.531917 139664575932160 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.2197313755750656, loss=1.7711374759674072
I0312 11:00:36.134662 139664567539456 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2195211797952652, loss=1.7562965154647827
I0312 11:01:11.764174 139664575932160 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.19198261201381683, loss=1.733489990234375
I0312 11:01:47.364697 139664567539456 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.18821501731872559, loss=1.7597694396972656
I0312 11:02:22.969234 139664575932160 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.21335478127002716, loss=1.7550699710845947
I0312 11:02:58.561883 139664567539456 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.20265063643455505, loss=1.8574684858322144
I0312 11:03:34.144883 139664575932160 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.19358395040035248, loss=1.738821029663086
I0312 11:03:42.769982 139834281293632 spec.py:321] Evaluating on the training split.
I0312 11:03:45.737059 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:06:49.706846 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 11:06:52.373236 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:09:28.941377 139834281293632 spec.py:349] Evaluating on the test split.
I0312 11:09:31.604946 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:11:53.347103 139834281293632 submission_runner.py:420] Time since start: 25281.90s, 	Step: 42526, 	{'train/accuracy': 0.642837405204773, 'train/loss': 1.7198340892791748, 'train/bleu': 31.784501626762236, 'validation/accuracy': 0.6651126146316528, 'validation/loss': 1.5587149858474731, 'validation/bleu': 28.53575327431008, 'validation/num_examples': 3000, 'test/accuracy': 0.6788914203643799, 'test/loss': 1.47401762008667, 'test/bleu': 28.226920449596552, 'test/num_examples': 3003, 'score': 15159.80151104927, 'total_duration': 25281.899538993835, 'accumulated_submission_time': 15159.80151104927, 'accumulated_eval_time': 10120.314247369766, 'accumulated_logging_time': 0.48659610748291016}
I0312 11:11:53.370345 139664567539456 logging_writer.py:48] [42526] accumulated_eval_time=10120.314247, accumulated_logging_time=0.486596, accumulated_submission_time=15159.801511, global_step=42526, preemption_count=0, score=15159.801511, test/accuracy=0.678891, test/bleu=28.226920, test/loss=1.474018, test/num_examples=3003, total_duration=25281.899539, train/accuracy=0.642837, train/bleu=31.784502, train/loss=1.719834, validation/accuracy=0.665113, validation/bleu=28.535753, validation/loss=1.558715, validation/num_examples=3000
I0312 11:12:19.986908 139664575932160 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.20439407229423523, loss=1.7250490188598633
I0312 11:12:55.550192 139664567539456 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.18459098041057587, loss=1.7067596912384033
I0312 11:13:31.130805 139664575932160 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.22121423482894897, loss=1.7329555749893188
I0312 11:14:06.675989 139664567539456 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.20766903460025787, loss=1.8210264444351196
I0312 11:14:42.239858 139664575932160 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.19116586446762085, loss=1.7515692710876465
I0312 11:15:17.799131 139664567539456 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.19242557883262634, loss=1.7917053699493408
I0312 11:15:53.351665 139664575932160 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.19048233330249786, loss=1.7243660688400269
I0312 11:16:28.908520 139664567539456 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.19635750353336334, loss=1.7286498546600342
I0312 11:17:04.486276 139664575932160 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.20928356051445007, loss=1.8219947814941406
I0312 11:17:40.027502 139664567539456 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.23926080763339996, loss=1.8306771516799927
I0312 11:18:15.597056 139664575932160 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.23921921849250793, loss=1.7507621049880981
I0312 11:18:51.180393 139664567539456 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.199100062251091, loss=1.7599642276763916
I0312 11:19:26.773994 139664575932160 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.19686052203178406, loss=1.807289481163025
I0312 11:20:02.366632 139664567539456 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2059771865606308, loss=1.7614365816116333
I0312 11:20:37.934633 139664575932160 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.19337929785251617, loss=1.7800686359405518
I0312 11:21:13.534052 139664567539456 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.21110153198242188, loss=1.756256103515625
I0312 11:21:49.142282 139664575932160 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.23984269797801971, loss=1.7889745235443115
I0312 11:22:24.772290 139664567539456 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.19984328746795654, loss=1.7849156856536865
I0312 11:23:00.391493 139664575932160 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.23420585691928864, loss=1.6992554664611816
I0312 11:23:35.947597 139664567539456 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19883789122104645, loss=1.8034892082214355
I0312 11:24:11.563897 139664575932160 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.18198874592781067, loss=1.662278652191162
I0312 11:24:47.132520 139664567539456 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2119758576154709, loss=1.7794077396392822
I0312 11:25:22.729970 139664575932160 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.18446752429008484, loss=1.646965742111206
I0312 11:25:53.423178 139834281293632 spec.py:321] Evaluating on the training split.
I0312 11:25:56.397160 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:29:55.353304 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 11:29:58.034228 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:32:20.697503 139834281293632 spec.py:349] Evaluating on the test split.
I0312 11:32:23.367323 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:34:49.619276 139834281293632 submission_runner.py:420] Time since start: 26658.17s, 	Step: 44888, 	{'train/accuracy': 0.6535823345184326, 'train/loss': 1.6201032400131226, 'train/bleu': 32.10639596896463, 'validation/accuracy': 0.6656457781791687, 'validation/loss': 1.5512747764587402, 'validation/bleu': 28.288459980167566, 'validation/num_examples': 3000, 'test/accuracy': 0.6773343086242676, 'test/loss': 1.4703198671340942, 'test/bleu': 28.39207283070988, 'test/num_examples': 3003, 'score': 15999.772643089294, 'total_duration': 26658.1717505455, 'accumulated_submission_time': 15999.772643089294, 'accumulated_eval_time': 10656.51029253006, 'accumulated_logging_time': 0.519707202911377}
I0312 11:34:49.639840 139664567539456 logging_writer.py:48] [44888] accumulated_eval_time=10656.510293, accumulated_logging_time=0.519707, accumulated_submission_time=15999.772643, global_step=44888, preemption_count=0, score=15999.772643, test/accuracy=0.677334, test/bleu=28.392073, test/loss=1.470320, test/num_examples=3003, total_duration=26658.171751, train/accuracy=0.653582, train/bleu=32.106396, train/loss=1.620103, validation/accuracy=0.665646, validation/bleu=28.288460, validation/loss=1.551275, validation/num_examples=3000
I0312 11:34:54.275616 139664575932160 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2046663761138916, loss=1.6947523355484009
I0312 11:35:29.793992 139664567539456 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.19742579758167267, loss=1.728230357170105
I0312 11:36:05.346609 139664575932160 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.21575315296649933, loss=1.7356274127960205
I0312 11:36:40.932150 139664567539456 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.20558899641036987, loss=1.7654428482055664
I0312 11:37:16.545000 139664575932160 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.1939980834722519, loss=1.7687488794326782
I0312 11:37:52.171765 139664567539456 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2068917453289032, loss=1.6570063829421997
I0312 11:38:27.746787 139664575932160 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.19112780690193176, loss=1.7840478420257568
I0312 11:39:03.378391 139664567539456 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.18330489099025726, loss=1.8219705820083618
I0312 11:39:38.933640 139664575932160 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.1883697211742401, loss=1.769389033317566
I0312 11:40:14.534822 139664567539456 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.21065278351306915, loss=1.7051104307174683
I0312 11:40:50.114378 139664575932160 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.18895013630390167, loss=1.717505931854248
I0312 11:41:25.699353 139664567539456 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.33038949966430664, loss=1.776676893234253
I0312 11:42:01.253713 139664575932160 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.18359722197055817, loss=1.6927411556243896
I0312 11:42:36.912949 139664567539456 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2617117166519165, loss=1.7159422636032104
I0312 11:43:12.526198 139664575932160 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2139538824558258, loss=1.8162068128585815
I0312 11:43:48.102470 139664567539456 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.18628233671188354, loss=1.7115272283554077
I0312 11:44:23.665795 139664575932160 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.21990451216697693, loss=1.7930387258529663
I0312 11:44:59.250321 139664567539456 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.20933453738689423, loss=1.665838360786438
I0312 11:45:34.852606 139664575932160 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.19452203810214996, loss=1.831451416015625
I0312 11:46:10.505416 139664567539456 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2257828712463379, loss=1.7336279153823853
I0312 11:46:46.131644 139664575932160 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.21462544798851013, loss=1.7162458896636963
I0312 11:47:21.723528 139664567539456 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.18003590404987335, loss=1.6969826221466064
I0312 11:47:57.301543 139664575932160 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.18704228103160858, loss=1.7542486190795898
I0312 11:48:32.870378 139664567539456 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.24368999898433685, loss=1.7511955499649048
I0312 11:48:49.672497 139834281293632 spec.py:321] Evaluating on the training split.
I0312 11:48:52.641725 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:52:53.129822 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 11:52:55.799000 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:56:06.956694 139834281293632 spec.py:349] Evaluating on the test split.
I0312 11:56:09.645036 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 11:59:08.410189 139834281293632 submission_runner.py:420] Time since start: 28116.96s, 	Step: 47249, 	{'train/accuracy': 0.6474329829216003, 'train/loss': 1.6818699836730957, 'train/bleu': 31.870938741943913, 'validation/accuracy': 0.6667864918708801, 'validation/loss': 1.5460501909255981, 'validation/bleu': 28.37202242681093, 'validation/num_examples': 3000, 'test/accuracy': 0.6796002984046936, 'test/loss': 1.4612714052200317, 'test/bleu': 28.36357916679935, 'test/num_examples': 3003, 'score': 16839.723083019257, 'total_duration': 28116.962640285492, 'accumulated_submission_time': 16839.723083019257, 'accumulated_eval_time': 11275.247904539108, 'accumulated_logging_time': 0.5494298934936523}
I0312 11:59:08.435091 139664575932160 logging_writer.py:48] [47249] accumulated_eval_time=11275.247905, accumulated_logging_time=0.549430, accumulated_submission_time=16839.723083, global_step=47249, preemption_count=0, score=16839.723083, test/accuracy=0.679600, test/bleu=28.363579, test/loss=1.461271, test/num_examples=3003, total_duration=28116.962640, train/accuracy=0.647433, train/bleu=31.870939, train/loss=1.681870, validation/accuracy=0.666786, validation/bleu=28.372022, validation/loss=1.546050, validation/num_examples=3000
I0312 11:59:26.924258 139664567539456 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.20478148758411407, loss=1.7327899932861328
I0312 12:00:02.434797 139664575932160 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20074035227298737, loss=1.7772786617279053
I0312 12:00:37.989964 139664567539456 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.188365638256073, loss=1.6847463846206665
I0312 12:01:13.554357 139664575932160 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.18274828791618347, loss=1.7659516334533691
I0312 12:01:49.140255 139664567539456 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.20913581550121307, loss=1.7834488153457642
I0312 12:02:24.704589 139664575932160 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.4831692576408386, loss=1.6973270177841187
I0312 12:03:00.274799 139664567539456 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.22054436802864075, loss=1.751922845840454
I0312 12:03:35.869198 139664575932160 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.24831458926200867, loss=1.803772211074829
I0312 12:04:11.443945 139664567539456 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.22984173893928528, loss=1.8305093050003052
I0312 12:04:47.014265 139664575932160 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18945468962192535, loss=1.8123540878295898
I0312 12:05:22.561013 139664567539456 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.191941499710083, loss=1.6720653772354126
I0312 12:05:58.160405 139664575932160 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.20026756823062897, loss=1.65998375415802
I0312 12:06:33.718443 139664567539456 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.18613849580287933, loss=1.8028886318206787
I0312 12:07:09.322365 139664575932160 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.20150378346443176, loss=1.6982227563858032
I0312 12:07:44.859677 139664567539456 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.1881759762763977, loss=1.683529019355774
I0312 12:08:20.436072 139664575932160 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.21129567921161652, loss=1.7145307064056396
I0312 12:08:56.013244 139664567539456 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.20757487416267395, loss=1.847732424736023
I0312 12:09:31.611770 139664575932160 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.19226884841918945, loss=1.7049164772033691
I0312 12:10:07.216373 139664567539456 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.24307405948638916, loss=1.7444998025894165
I0312 12:10:42.797578 139664575932160 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.20011016726493835, loss=1.681384801864624
I0312 12:11:18.390181 139664567539456 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.21436014771461487, loss=1.7408415079116821
I0312 12:11:53.983494 139664575932160 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1855463683605194, loss=1.6829651594161987
I0312 12:12:29.560262 139664567539456 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.230570450425148, loss=1.7731600999832153
I0312 12:13:05.190567 139664575932160 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.20504315197467804, loss=1.7688376903533936
I0312 12:13:08.466362 139834281293632 spec.py:321] Evaluating on the training split.
I0312 12:13:11.427605 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 12:16:16.792841 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 12:16:19.471808 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 12:18:49.457431 139834281293632 spec.py:349] Evaluating on the test split.
I0312 12:18:52.125782 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 12:21:23.702891 139834281293632 submission_runner.py:420] Time since start: 29452.26s, 	Step: 49611, 	{'train/accuracy': 0.6456196904182434, 'train/loss': 1.6879490613937378, 'train/bleu': 31.93500844978868, 'validation/accuracy': 0.6677908301353455, 'validation/loss': 1.5375560522079468, 'validation/bleu': 28.72653963720832, 'validation/num_examples': 3000, 'test/accuracy': 0.6794027090072632, 'test/loss': 1.4521712064743042, 'test/bleu': 28.18185372945468, 'test/num_examples': 3003, 'score': 17679.671919107437, 'total_duration': 29452.255368232727, 'accumulated_submission_time': 17679.671919107437, 'accumulated_eval_time': 11770.484377622604, 'accumulated_logging_time': 0.5856420993804932}
I0312 12:21:23.722332 139664567539456 logging_writer.py:48] [49611] accumulated_eval_time=11770.484378, accumulated_logging_time=0.585642, accumulated_submission_time=17679.671919, global_step=49611, preemption_count=0, score=17679.671919, test/accuracy=0.679403, test/bleu=28.181854, test/loss=1.452171, test/num_examples=3003, total_duration=29452.255368, train/accuracy=0.645620, train/bleu=31.935008, train/loss=1.687949, validation/accuracy=0.667791, validation/bleu=28.726540, validation/loss=1.537556, validation/num_examples=3000
I0312 12:21:55.686177 139664575932160 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1927410066127777, loss=1.6691865921020508
I0312 12:22:31.275360 139664567539456 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.20332461595535278, loss=1.6655701398849487
I0312 12:23:06.877686 139664575932160 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.20645840466022491, loss=1.740767478942871
I0312 12:23:42.479185 139664567539456 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.20464089512825012, loss=1.7042773962020874
I0312 12:24:18.061317 139664575932160 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1924140304327011, loss=1.713587760925293
I0312 12:24:53.630399 139664567539456 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.20136398077011108, loss=1.7568435668945312
I0312 12:25:29.215206 139664575932160 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.20241552591323853, loss=1.754423975944519
I0312 12:26:04.818241 139664567539456 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.20175401866436005, loss=1.7909784317016602
I0312 12:26:40.388049 139664575932160 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21774165332317352, loss=1.7431845664978027
I0312 12:27:15.952812 139664567539456 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.20224185287952423, loss=1.7847578525543213
I0312 12:27:51.554972 139664575932160 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.2106325626373291, loss=1.7659412622451782
I0312 12:28:27.178708 139664567539456 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.21760645508766174, loss=1.7234208583831787
I0312 12:29:02.764732 139664575932160 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.19622689485549927, loss=1.7528215646743774
I0312 12:29:38.351675 139664567539456 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.20603354275226593, loss=1.6469602584838867
I0312 12:30:13.916649 139664575932160 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.22460509836673737, loss=1.7516223192214966
I0312 12:30:49.497182 139664567539456 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.24165838956832886, loss=1.7678033113479614
I0312 12:31:25.112247 139664575932160 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.21671898663043976, loss=1.7278729677200317
I0312 12:32:00.732505 139664567539456 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.21499581634998322, loss=1.7276962995529175
I0312 12:32:36.358320 139664575932160 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.21950331330299377, loss=1.661126971244812
I0312 12:33:11.978590 139664567539456 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.2032741755247116, loss=1.7609587907791138
I0312 12:33:47.579381 139664575932160 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.1928253322839737, loss=1.6679999828338623
I0312 12:34:23.144514 139664567539456 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.18908266723155975, loss=1.6819329261779785
I0312 12:34:58.732765 139664575932160 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.5408912301063538, loss=1.6656439304351807
I0312 12:35:23.705596 139834281293632 spec.py:321] Evaluating on the training split.
I0312 12:35:26.668601 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 12:39:36.321399 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 12:39:38.985144 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 12:44:03.720572 139834281293632 spec.py:349] Evaluating on the test split.
I0312 12:44:06.394496 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 12:48:24.752724 139834281293632 submission_runner.py:420] Time since start: 31073.31s, 	Step: 51972, 	{'train/accuracy': 0.6551151871681213, 'train/loss': 1.6218953132629395, 'train/bleu': 32.32626977076003, 'validation/accuracy': 0.6678900122642517, 'validation/loss': 1.5312690734863281, 'validation/bleu': 28.88246091612095, 'validation/num_examples': 3000, 'test/accuracy': 0.682877242565155, 'test/loss': 1.4454905986785889, 'test/bleu': 28.490701505346525, 'test/num_examples': 3003, 'score': 18519.571103811264, 'total_duration': 31073.305172920227, 'accumulated_submission_time': 18519.571103811264, 'accumulated_eval_time': 12551.531423330307, 'accumulated_logging_time': 0.6143500804901123}
I0312 12:48:24.773077 139664567539456 logging_writer.py:48] [51972] accumulated_eval_time=12551.531423, accumulated_logging_time=0.614350, accumulated_submission_time=18519.571104, global_step=51972, preemption_count=0, score=18519.571104, test/accuracy=0.682877, test/bleu=28.490702, test/loss=1.445491, test/num_examples=3003, total_duration=31073.305173, train/accuracy=0.655115, train/bleu=32.326270, train/loss=1.621895, validation/accuracy=0.667890, validation/bleu=28.882461, validation/loss=1.531269, validation/num_examples=3000
I0312 12:48:35.060062 139664575932160 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.19274349510669708, loss=1.8004353046417236
I0312 12:49:10.569075 139664567539456 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.21590949594974518, loss=1.8278495073318481
I0312 12:49:46.140191 139664575932160 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.19045886397361755, loss=1.6923846006393433
I0312 12:50:21.705094 139664567539456 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.20031674206256866, loss=1.7403149604797363
I0312 12:50:57.291437 139664575932160 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.2135302871465683, loss=1.7324066162109375
I0312 12:51:32.850812 139664567539456 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19986693561077118, loss=1.6657295227050781
I0312 12:52:08.425594 139664575932160 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.22676047682762146, loss=1.7307143211364746
I0312 12:52:43.981349 139664567539456 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2319788783788681, loss=1.7590326070785522
I0312 12:53:19.559634 139664575932160 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.20091640949249268, loss=1.683748722076416
I0312 12:53:55.120508 139664567539456 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.23222066462039948, loss=1.7300530672073364
I0312 12:54:30.725801 139664575932160 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.19876211881637573, loss=1.7672264575958252
I0312 12:55:06.348242 139664567539456 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.20083634555339813, loss=1.7691694498062134
I0312 12:55:41.899158 139664575932160 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.28227224946022034, loss=1.7651985883712769
I0312 12:56:17.499797 139664567539456 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2037164717912674, loss=1.6251798868179321
I0312 12:56:53.087362 139664575932160 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19772890210151672, loss=1.7407821416854858
I0312 12:57:28.656268 139664567539456 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.1831139624118805, loss=1.7033650875091553
I0312 12:58:04.245206 139664575932160 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.21769213676452637, loss=1.7119886875152588
I0312 12:58:39.820950 139664567539456 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.18148015439510345, loss=1.6375995874404907
I0312 12:59:15.415019 139664575932160 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2099573165178299, loss=1.7780828475952148
I0312 12:59:51.004749 139664567539456 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.18713918328285217, loss=1.6738470792770386
I0312 13:00:26.634555 139664575932160 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.19526338577270508, loss=1.7510359287261963
I0312 13:01:02.292375 139664567539456 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.20597392320632935, loss=1.7567522525787354
I0312 13:01:37.880691 139664575932160 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.3072433471679688, loss=1.7473230361938477
I0312 13:02:13.449224 139664567539456 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2149331122636795, loss=1.722378134727478
I0312 13:02:24.921759 139834281293632 spec.py:321] Evaluating on the training split.
I0312 13:02:27.902186 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:06:21.724067 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 13:06:24.393169 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:09:34.075944 139834281293632 spec.py:349] Evaluating on the test split.
I0312 13:09:36.741836 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:13:13.206162 139834281293632 submission_runner.py:420] Time since start: 32561.76s, 	Step: 54334, 	{'train/accuracy': 0.6510263681411743, 'train/loss': 1.655605435371399, 'train/bleu': 32.088404891666734, 'validation/accuracy': 0.6696755290031433, 'validation/loss': 1.5259546041488647, 'validation/bleu': 28.627795726474464, 'validation/num_examples': 3000, 'test/accuracy': 0.682540237903595, 'test/loss': 1.4420441389083862, 'test/bleu': 28.32718456710534, 'test/num_examples': 3003, 'score': 19359.637244701385, 'total_duration': 32561.758611679077, 'accumulated_submission_time': 19359.637244701385, 'accumulated_eval_time': 13199.815757513046, 'accumulated_logging_time': 0.6437675952911377}
I0312 13:13:13.231263 139664575932160 logging_writer.py:48] [54334] accumulated_eval_time=13199.815758, accumulated_logging_time=0.643768, accumulated_submission_time=19359.637245, global_step=54334, preemption_count=0, score=19359.637245, test/accuracy=0.682540, test/bleu=28.327185, test/loss=1.442044, test/num_examples=3003, total_duration=32561.758612, train/accuracy=0.651026, train/bleu=32.088405, train/loss=1.655605, validation/accuracy=0.669676, validation/bleu=28.627796, validation/loss=1.525955, validation/num_examples=3000
I0312 13:13:37.071201 139664567539456 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19406265020370483, loss=1.6206547021865845
I0312 13:14:12.628503 139664575932160 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.18182650208473206, loss=1.7168817520141602
I0312 13:14:48.200023 139664567539456 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.20338861644268036, loss=1.7157089710235596
I0312 13:15:23.784142 139664575932160 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.19850273430347443, loss=1.7366901636123657
I0312 13:15:59.365256 139664567539456 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1896304190158844, loss=1.7738760709762573
I0312 13:16:34.981478 139664575932160 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.23816916346549988, loss=1.7178224325180054
I0312 13:17:10.568702 139664567539456 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.1849132627248764, loss=1.7806658744812012
I0312 13:17:46.118727 139664575932160 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.22375138103961945, loss=1.695446491241455
I0312 13:18:21.722769 139664567539456 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.21695806086063385, loss=1.6718213558197021
I0312 13:18:57.305838 139664575932160 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.20697592198848724, loss=1.7306225299835205
I0312 13:19:32.882808 139664567539456 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.21763548254966736, loss=1.7062132358551025
I0312 13:20:08.464696 139664575932160 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.1841334104537964, loss=1.672429084777832
I0312 13:20:44.078689 139664567539456 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.21515659987926483, loss=1.678997278213501
I0312 13:21:19.664165 139664575932160 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.20243017375469208, loss=1.6430984735488892
I0312 13:21:55.274613 139664567539456 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2132302075624466, loss=1.7704530954360962
I0312 13:22:30.876827 139664575932160 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.21361827850341797, loss=1.702066421508789
I0312 13:23:06.465783 139664567539456 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.22365204989910126, loss=1.7004164457321167
I0312 13:23:42.027802 139664575932160 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.22928808629512787, loss=1.8000197410583496
I0312 13:24:17.608839 139664567539456 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2015770524740219, loss=1.7929987907409668
I0312 13:24:53.210197 139664575932160 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.19500410556793213, loss=1.7521717548370361
I0312 13:25:28.787235 139664567539456 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.22785910964012146, loss=1.8124221563339233
I0312 13:26:04.378359 139664575932160 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.20531976222991943, loss=1.6621416807174683
I0312 13:26:39.953816 139664567539456 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.219169020652771, loss=1.7512911558151245
I0312 13:27:13.456695 139834281293632 spec.py:321] Evaluating on the training split.
I0312 13:27:16.422569 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:31:17.575728 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 13:31:20.259679 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:34:06.382062 139834281293632 spec.py:349] Evaluating on the test split.
I0312 13:34:09.053788 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:37:03.305739 139834281293632 submission_runner.py:420] Time since start: 33991.86s, 	Step: 56696, 	{'train/accuracy': 0.6673276424407959, 'train/loss': 1.5445960760116577, 'train/bleu': 33.360906251543774, 'validation/accuracy': 0.6722297072410583, 'validation/loss': 1.5170300006866455, 'validation/bleu': 29.03924901341143, 'validation/num_examples': 3000, 'test/accuracy': 0.6850502490997314, 'test/loss': 1.4306808710098267, 'test/bleu': 28.62387632662793, 'test/num_examples': 3003, 'score': 20199.781319618225, 'total_duration': 33991.85821199417, 'accumulated_submission_time': 20199.781319618225, 'accumulated_eval_time': 13789.664745807648, 'accumulated_logging_time': 0.678861141204834}
I0312 13:37:03.327937 139664575932160 logging_writer.py:48] [56696] accumulated_eval_time=13789.664746, accumulated_logging_time=0.678861, accumulated_submission_time=20199.781320, global_step=56696, preemption_count=0, score=20199.781320, test/accuracy=0.685050, test/bleu=28.623876, test/loss=1.430681, test/num_examples=3003, total_duration=33991.858212, train/accuracy=0.667328, train/bleu=33.360906, train/loss=1.544596, validation/accuracy=0.672230, validation/bleu=29.039249, validation/loss=1.517030, validation/num_examples=3000
I0312 13:37:05.111450 139664567539456 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.20138561725616455, loss=1.7189102172851562
I0312 13:37:40.592415 139664575932160 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.20877481997013092, loss=1.7399091720581055
I0312 13:38:16.121375 139664567539456 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1884656697511673, loss=1.6445800065994263
I0312 13:38:51.681321 139664575932160 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2021808624267578, loss=1.753603219985962
I0312 13:39:27.241148 139664567539456 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19016708433628082, loss=1.716512680053711
I0312 13:40:02.831205 139664575932160 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1958283632993698, loss=1.7408921718597412
I0312 13:40:38.413349 139664567539456 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.2162352353334427, loss=1.6650464534759521
I0312 13:41:14.000586 139664575932160 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.1939343810081482, loss=1.6683262586593628
I0312 13:41:49.605465 139664567539456 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1849905401468277, loss=1.7170823812484741
I0312 13:42:25.173171 139664575932160 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19581525027751923, loss=1.6967804431915283
I0312 13:43:00.765467 139664567539456 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.230636328458786, loss=1.6278821229934692
I0312 13:43:36.374089 139664575932160 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.20592018961906433, loss=1.7157409191131592
I0312 13:44:11.989790 139664567539456 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.1880110502243042, loss=1.6342846155166626
I0312 13:44:47.551031 139664575932160 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.1854707896709442, loss=1.650492787361145
I0312 13:45:23.112833 139664567539456 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.2075989693403244, loss=1.704371452331543
I0312 13:45:58.666371 139664575932160 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19271337985992432, loss=1.7157036066055298
I0312 13:46:34.262062 139664567539456 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.18886923789978027, loss=1.6479960680007935
I0312 13:47:09.861744 139664575932160 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.20001763105392456, loss=1.747792363166809
I0312 13:47:45.475487 139664567539456 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.18975026905536652, loss=1.665120244026184
I0312 13:48:21.072731 139664575932160 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.200633704662323, loss=1.757948398590088
I0312 13:48:56.666305 139664567539456 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.22353903949260712, loss=1.7043123245239258
I0312 13:49:32.277784 139664575932160 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.23056891560554504, loss=1.7075384855270386
I0312 13:50:07.853181 139664567539456 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.19446983933448792, loss=1.7052196264266968
I0312 13:50:43.419993 139664575932160 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.21579088270664215, loss=1.681602120399475
I0312 13:51:03.407764 139834281293632 spec.py:321] Evaluating on the training split.
I0312 13:51:06.371404 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:55:09.126000 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 13:55:11.773663 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 13:57:58.389713 139834281293632 spec.py:349] Evaluating on the test split.
I0312 13:58:01.056074 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:01:03.135663 139834281293632 submission_runner.py:420] Time since start: 35431.69s, 	Step: 59058, 	{'train/accuracy': 0.6547285318374634, 'train/loss': 1.627714991569519, 'train/bleu': 32.10986175858725, 'validation/accuracy': 0.6729241013526917, 'validation/loss': 1.5107427835464478, 'validation/bleu': 29.169344862532746, 'validation/num_examples': 3000, 'test/accuracy': 0.6850270628929138, 'test/loss': 1.4248459339141846, 'test/bleu': 28.680309752241467, 'test/num_examples': 3003, 'score': 21039.779182434082, 'total_duration': 35431.68813467026, 'accumulated_submission_time': 21039.779182434082, 'accumulated_eval_time': 14389.392583847046, 'accumulated_logging_time': 0.7105739116668701}
I0312 14:01:03.157419 139664567539456 logging_writer.py:48] [59058] accumulated_eval_time=14389.392584, accumulated_logging_time=0.710574, accumulated_submission_time=21039.779182, global_step=59058, preemption_count=0, score=21039.779182, test/accuracy=0.685027, test/bleu=28.680310, test/loss=1.424846, test/num_examples=3003, total_duration=35431.688135, train/accuracy=0.654729, train/bleu=32.109862, train/loss=1.627715, validation/accuracy=0.672924, validation/bleu=29.169345, validation/loss=1.510743, validation/num_examples=3000
I0312 14:01:18.441687 139664575932160 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19297446310520172, loss=1.6476742029190063
I0312 14:01:53.966273 139664567539456 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.21117860078811646, loss=1.766669511795044
I0312 14:02:29.496366 139664575932160 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.182709738612175, loss=1.7315821647644043
I0312 14:03:05.065005 139664567539456 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.1998002827167511, loss=1.7118794918060303
I0312 14:03:40.615367 139664575932160 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.19166818261146545, loss=1.6714061498641968
I0312 14:04:16.180326 139664567539456 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.24800117313861847, loss=1.7188678979873657
I0312 14:04:51.766973 139664575932160 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.19953659176826477, loss=1.6723954677581787
I0312 14:05:27.313472 139664567539456 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.1957443356513977, loss=1.637558102607727
I0312 14:06:02.858263 139664575932160 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19110599160194397, loss=1.6833441257476807
I0312 14:06:38.436842 139664567539456 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.19656506180763245, loss=1.695151925086975
I0312 14:07:14.003505 139664575932160 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.20742227137088776, loss=1.7109358310699463
I0312 14:07:49.555018 139664567539456 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.18849213421344757, loss=1.810937523841858
I0312 14:08:25.123263 139664575932160 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.19573581218719482, loss=1.6615734100341797
I0312 14:09:00.668681 139664567539456 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20064154267311096, loss=1.7880992889404297
I0312 14:09:36.260368 139664575932160 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.1993878185749054, loss=1.6936646699905396
I0312 14:10:11.847383 139664567539456 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.18159490823745728, loss=1.716643214225769
I0312 14:10:47.410771 139664575932160 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.18881940841674805, loss=1.7253130674362183
I0312 14:11:23.003045 139664567539456 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.18920336663722992, loss=1.7284481525421143
I0312 14:11:58.536103 139664575932160 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.21588613092899323, loss=1.7675507068634033
I0312 14:12:34.116338 139664567539456 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.19667887687683105, loss=1.695376992225647
I0312 14:13:09.685358 139664575932160 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.22845301032066345, loss=1.7632747888565063
I0312 14:13:45.264708 139664567539456 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.21583417057991028, loss=1.7018680572509766
I0312 14:14:20.849203 139664575932160 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.19135932624340057, loss=1.7314915657043457
I0312 14:14:56.463607 139664567539456 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.19309738278388977, loss=1.6889688968658447
I0312 14:15:03.314148 139834281293632 spec.py:321] Evaluating on the training split.
I0312 14:15:06.275554 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:18:52.938611 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 14:18:55.626733 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:22:29.509620 139834281293632 spec.py:349] Evaluating on the test split.
I0312 14:22:32.170888 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:24:55.121679 139834281293632 submission_runner.py:420] Time since start: 36863.67s, 	Step: 61421, 	{'train/accuracy': 0.6533963084220886, 'train/loss': 1.6542311906814575, 'train/bleu': 32.42845141366208, 'validation/accuracy': 0.6724777221679688, 'validation/loss': 1.5102542638778687, 'validation/bleu': 29.03658871071236, 'validation/num_examples': 3000, 'test/accuracy': 0.6873162388801575, 'test/loss': 1.4158756732940674, 'test/bleu': 28.770193378464274, 'test/num_examples': 3003, 'score': 21879.856380939484, 'total_duration': 36863.674137830734, 'accumulated_submission_time': 21879.856380939484, 'accumulated_eval_time': 14981.200040340424, 'accumulated_logging_time': 0.7415766716003418}
I0312 14:24:55.149510 139664575932160 logging_writer.py:48] [61421] accumulated_eval_time=14981.200040, accumulated_logging_time=0.741577, accumulated_submission_time=21879.856381, global_step=61421, preemption_count=0, score=21879.856381, test/accuracy=0.687316, test/bleu=28.770193, test/loss=1.415876, test/num_examples=3003, total_duration=36863.674138, train/accuracy=0.653396, train/bleu=32.428451, train/loss=1.654231, validation/accuracy=0.672478, validation/bleu=29.036589, validation/loss=1.510254, validation/num_examples=3000
I0312 14:25:23.559984 139664567539456 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.20596739649772644, loss=1.703971028327942
I0312 14:25:59.134484 139664575932160 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2109697014093399, loss=1.667838454246521
I0312 14:26:34.753902 139664567539456 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3415275514125824, loss=1.6975935697555542
I0312 14:27:10.362263 139664575932160 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.19807174801826477, loss=1.6466948986053467
I0312 14:27:45.961753 139664567539456 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.20869362354278564, loss=1.6633670330047607
I0312 14:28:21.559766 139664575932160 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.25377902388572693, loss=1.833655595779419
I0312 14:28:57.153136 139664567539456 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.19077345728874207, loss=1.6024845838546753
I0312 14:29:32.736041 139664575932160 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19746696949005127, loss=1.6733944416046143
I0312 14:30:08.307803 139664567539456 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.18292942643165588, loss=1.6619548797607422
I0312 14:30:43.897522 139664575932160 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.21991842985153198, loss=1.7024812698364258
I0312 14:31:19.488497 139664567539456 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.18773558735847473, loss=1.7984511852264404
I0312 14:31:55.088405 139664575932160 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.1955895721912384, loss=1.6384050846099854
I0312 14:32:30.690089 139664567539456 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.19742830097675323, loss=1.7614067792892456
I0312 14:33:06.278840 139664575932160 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2159225046634674, loss=1.7726702690124512
I0312 14:33:41.857692 139664567539456 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.20071758329868317, loss=1.7815512418746948
I0312 14:34:17.470683 139664575932160 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.25564366579055786, loss=1.7411549091339111
I0312 14:34:53.078284 139664567539456 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.1943124532699585, loss=1.6448731422424316
I0312 14:35:28.661775 139664575932160 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2053745687007904, loss=1.663485050201416
I0312 14:36:04.259299 139664567539456 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.19801132380962372, loss=1.6659126281738281
I0312 14:36:39.853878 139664575932160 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2075267881155014, loss=1.774182677268982
I0312 14:37:15.423767 139664567539456 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.19507203996181488, loss=1.7119247913360596
I0312 14:37:51.007499 139664575932160 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.19179430603981018, loss=1.6855127811431885
I0312 14:38:26.581843 139664567539456 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.19643789529800415, loss=1.6876254081726074
I0312 14:38:55.132073 139834281293632 spec.py:321] Evaluating on the training split.
I0312 14:38:58.104252 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:42:20.436878 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 14:42:23.107172 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:45:06.480209 139834281293632 spec.py:349] Evaluating on the test split.
I0312 14:45:09.138824 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 14:47:45.175801 139834281293632 submission_runner.py:420] Time since start: 38233.73s, 	Step: 63782, 	{'train/accuracy': 0.6634188890457153, 'train/loss': 1.5673255920410156, 'train/bleu': 32.64206969909681, 'validation/accuracy': 0.6739531755447388, 'validation/loss': 1.4961543083190918, 'validation/bleu': 29.020543803915928, 'validation/num_examples': 3000, 'test/accuracy': 0.6861774325370789, 'test/loss': 1.4073783159255981, 'test/bleu': 28.776615374007704, 'test/num_examples': 3003, 'score': 22719.75696492195, 'total_duration': 38233.72824931145, 'accumulated_submission_time': 22719.75696492195, 'accumulated_eval_time': 15511.243689775467, 'accumulated_logging_time': 0.7795286178588867}
I0312 14:47:45.202158 139664575932160 logging_writer.py:48] [63782] accumulated_eval_time=15511.243690, accumulated_logging_time=0.779529, accumulated_submission_time=22719.756965, global_step=63782, preemption_count=0, score=22719.756965, test/accuracy=0.686177, test/bleu=28.776615, test/loss=1.407378, test/num_examples=3003, total_duration=38233.728249, train/accuracy=0.663419, train/bleu=32.642070, train/loss=1.567326, validation/accuracy=0.673953, validation/bleu=29.020544, validation/loss=1.496154, validation/num_examples=3000
I0312 14:47:51.961001 139664567539456 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.2126391977071762, loss=1.7564973831176758
I0312 14:48:27.456876 139664575932160 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.20558486878871918, loss=1.7749751806259155
I0312 14:49:03.017031 139664567539456 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2088397592306137, loss=1.695978045463562
I0312 14:49:38.581252 139664575932160 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.244239941239357, loss=1.6641298532485962
I0312 14:50:14.140791 139664567539456 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2024628221988678, loss=1.703399658203125
I0312 14:50:49.730758 139664575932160 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19001910090446472, loss=1.7190498113632202
I0312 14:51:25.308748 139664567539456 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.20082136988639832, loss=1.6761410236358643
I0312 14:52:00.865924 139664575932160 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.211829274892807, loss=1.7138973474502563
I0312 14:52:36.445272 139664567539456 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2100481241941452, loss=1.716891884803772
I0312 14:53:12.025967 139664575932160 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19884541630744934, loss=1.7143751382827759
I0312 14:53:47.578693 139664567539456 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.22821162641048431, loss=1.6068007946014404
I0312 14:54:23.191249 139664575932160 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2183266580104828, loss=1.634158730506897
I0312 14:54:58.774843 139664567539456 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.18216918408870697, loss=1.5889230966567993
I0312 14:55:34.395025 139664575932160 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.21422681212425232, loss=1.7516140937805176
I0312 14:56:10.010112 139664567539456 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.21223290264606476, loss=1.7463388442993164
I0312 14:56:45.590768 139664575932160 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.21899272501468658, loss=1.7144396305084229
I0312 14:57:21.200455 139664567539456 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.18838956952095032, loss=1.6764838695526123
I0312 14:57:56.842192 139664575932160 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.20016294717788696, loss=1.7022912502288818
I0312 14:58:32.441493 139664567539456 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2088870406150818, loss=1.7073919773101807
I0312 14:59:08.033151 139664575932160 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.19471168518066406, loss=1.6026149988174438
I0312 14:59:43.614918 139664567539456 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.20974230766296387, loss=1.6211923360824585
I0312 15:00:19.264339 139664575932160 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2025274783372879, loss=1.6474754810333252
I0312 15:00:54.841845 139664567539456 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19624094665050507, loss=1.7716484069824219
I0312 15:01:30.426455 139664575932160 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.18323037028312683, loss=1.632931113243103
I0312 15:01:45.439543 139834281293632 spec.py:321] Evaluating on the training split.
I0312 15:01:48.401767 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:05:19.252120 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 15:05:21.921208 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:08:04.274282 139834281293632 spec.py:349] Evaluating on the test split.
I0312 15:08:06.946150 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:10:41.056591 139834281293632 submission_runner.py:420] Time since start: 39609.61s, 	Step: 66144, 	{'train/accuracy': 0.6572242975234985, 'train/loss': 1.6180578470230103, 'train/bleu': 32.916716181700515, 'validation/accuracy': 0.6756022572517395, 'validation/loss': 1.4923619031906128, 'validation/bleu': 29.525804694230857, 'validation/num_examples': 3000, 'test/accuracy': 0.6896519660949707, 'test/loss': 1.402874231338501, 'test/bleu': 28.885937702504354, 'test/num_examples': 3003, 'score': 23559.911987304688, 'total_duration': 39609.60906815529, 'accumulated_submission_time': 23559.911987304688, 'accumulated_eval_time': 16046.860683441162, 'accumulated_logging_time': 0.8157198429107666}
I0312 15:10:41.079004 139664567539456 logging_writer.py:48] [66144] accumulated_eval_time=16046.860683, accumulated_logging_time=0.815720, accumulated_submission_time=23559.911987, global_step=66144, preemption_count=0, score=23559.911987, test/accuracy=0.689652, test/bleu=28.885938, test/loss=1.402874, test/num_examples=3003, total_duration=39609.609068, train/accuracy=0.657224, train/bleu=32.916716, train/loss=1.618058, validation/accuracy=0.675602, validation/bleu=29.525805, validation/loss=1.492362, validation/num_examples=3000
I0312 15:11:01.315134 139664575932160 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.21067707240581512, loss=1.5893828868865967
I0312 15:11:36.866845 139664567539456 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2376977950334549, loss=1.7977653741836548
I0312 15:12:12.465322 139664575932160 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.20887257158756256, loss=1.6526188850402832
I0312 15:12:48.045306 139664567539456 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2245187908411026, loss=1.656346082687378
I0312 15:13:23.638552 139664575932160 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.19064801931381226, loss=1.7113635540008545
I0312 15:13:59.207912 139664567539456 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.226019486784935, loss=1.7005048990249634
I0312 15:14:34.780591 139664575932160 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.20758944749832153, loss=1.7011651992797852
I0312 15:15:10.373791 139664567539456 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.19396762549877167, loss=1.6280181407928467
I0312 15:15:46.016206 139664575932160 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.195921391248703, loss=1.7091305255889893
I0312 15:16:21.627357 139664567539456 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.21242518723011017, loss=1.7284741401672363
I0312 15:16:57.182973 139664575932160 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.19724465906620026, loss=1.6389310359954834
I0312 15:17:32.776646 139664567539456 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.21817028522491455, loss=1.692050814628601
I0312 15:18:08.347116 139664575932160 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.2121318131685257, loss=1.6976051330566406
I0312 15:18:43.953614 139664567539456 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.1914576292037964, loss=1.69212007522583
I0312 15:19:19.546034 139664575932160 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.6970638632774353, loss=1.6660847663879395
I0312 15:19:55.111895 139664567539456 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.19657135009765625, loss=1.6687992811203003
I0312 15:20:30.662636 139664575932160 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.24103090167045593, loss=1.7558841705322266
I0312 15:21:06.258425 139664567539456 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2208738625049591, loss=1.6827316284179688
I0312 15:21:41.834942 139664575932160 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.19539298117160797, loss=1.6000324487686157
I0312 15:22:17.474455 139664567539456 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.20800544321537018, loss=1.7498725652694702
I0312 15:22:53.096909 139664575932160 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.20116619765758514, loss=1.6985901594161987
I0312 15:23:28.738675 139664567539456 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.22105301916599274, loss=1.7202068567276
I0312 15:24:04.396525 139664575932160 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.19827812910079956, loss=1.6066831350326538
I0312 15:24:39.990709 139664567539456 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.20674046874046326, loss=1.6676274538040161
I0312 15:24:41.136772 139834281293632 spec.py:321] Evaluating on the training split.
I0312 15:24:44.120000 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:28:05.809028 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 15:28:08.490941 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:31:14.461257 139834281293632 spec.py:349] Evaluating on the test split.
I0312 15:31:17.135046 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:33:51.596578 139834281293632 submission_runner.py:420] Time since start: 41000.15s, 	Step: 68505, 	{'train/accuracy': 0.6578312516212463, 'train/loss': 1.619662880897522, 'train/bleu': 32.58494738936097, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.4812695980072021, 'validation/bleu': 29.290810532146438, 'validation/num_examples': 3000, 'test/accuracy': 0.691499650478363, 'test/loss': 1.3894480466842651, 'test/bleu': 28.92139389859646, 'test/num_examples': 3003, 'score': 24399.88455271721, 'total_duration': 41000.14904499054, 'accumulated_submission_time': 24399.88455271721, 'accumulated_eval_time': 16597.32043647766, 'accumulated_logging_time': 0.848196268081665}
I0312 15:33:51.619999 139664575932160 logging_writer.py:48] [68505] accumulated_eval_time=16597.320436, accumulated_logging_time=0.848196, accumulated_submission_time=24399.884553, global_step=68505, preemption_count=0, score=24399.884553, test/accuracy=0.691500, test/bleu=28.921394, test/loss=1.389448, test/num_examples=3003, total_duration=41000.149045, train/accuracy=0.657831, train/bleu=32.584947, train/loss=1.619663, validation/accuracy=0.677289, validation/bleu=29.290811, validation/loss=1.481270, validation/num_examples=3000
I0312 15:34:25.686399 139664567539456 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.22848549485206604, loss=1.7312650680541992
I0312 15:35:01.229602 139664575932160 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.19122450053691864, loss=1.6208584308624268
I0312 15:35:36.794415 139664567539456 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.19500155746936798, loss=1.6672947406768799
I0312 15:36:12.410072 139664575932160 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.1980256587266922, loss=1.695299506187439
I0312 15:36:47.958525 139664567539456 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.20579083263874054, loss=1.6973087787628174
I0312 15:37:23.553302 139664575932160 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.1987256407737732, loss=1.6369467973709106
I0312 15:37:59.132070 139664567539456 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.21406367421150208, loss=1.6373050212860107
I0312 15:38:34.723499 139664575932160 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.21180325746536255, loss=1.6519311666488647
I0312 15:39:10.311219 139664567539456 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.18992988765239716, loss=1.7009329795837402
I0312 15:39:45.858062 139664575932160 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.21311180293560028, loss=1.717978835105896
I0312 15:40:21.444223 139664567539456 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.21163775026798248, loss=1.6228469610214233
I0312 15:40:57.023607 139664575932160 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.19609561562538147, loss=1.600793719291687
I0312 15:41:32.604835 139664567539456 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.20001457631587982, loss=1.6480562686920166
I0312 15:42:08.227727 139664575932160 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2143077403306961, loss=1.6999253034591675
I0312 15:42:43.793049 139664567539456 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.18794265389442444, loss=1.6130472421646118
I0312 15:43:19.382149 139664575932160 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.24114225804805756, loss=1.7227051258087158
I0312 15:43:54.982331 139664567539456 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.6485551595687866, loss=1.7049524784088135
I0312 15:44:30.531241 139664575932160 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2043083757162094, loss=1.651604413986206
I0312 15:45:06.122323 139664567539456 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.1956569403409958, loss=1.7017449140548706
I0312 15:45:41.727588 139664575932160 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.20082341134548187, loss=1.7057324647903442
I0312 15:46:17.303952 139664567539456 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.22263474762439728, loss=1.7167925834655762
I0312 15:46:52.882331 139664575932160 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2028590887784958, loss=1.690185546875
I0312 15:47:28.454644 139664567539456 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.19909246265888214, loss=1.6155146360397339
I0312 15:47:51.646904 139834281293632 spec.py:321] Evaluating on the training split.
I0312 15:47:54.614277 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:52:12.526287 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 15:52:15.189009 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:55:42.273528 139834281293632 spec.py:349] Evaluating on the test split.
I0312 15:55:44.930088 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 15:59:07.689227 139834281293632 submission_runner.py:420] Time since start: 42516.24s, 	Step: 70867, 	{'train/accuracy': 0.6645039319992065, 'train/loss': 1.5631052255630493, 'train/bleu': 32.747214248427234, 'validation/accuracy': 0.677238941192627, 'validation/loss': 1.4762758016586304, 'validation/bleu': 29.26196255330381, 'validation/num_examples': 3000, 'test/accuracy': 0.6918947100639343, 'test/loss': 1.386744737625122, 'test/bleu': 29.064057246440694, 'test/num_examples': 3003, 'score': 25239.83142542839, 'total_duration': 42516.241681814194, 'accumulated_submission_time': 25239.83142542839, 'accumulated_eval_time': 17273.362685203552, 'accumulated_logging_time': 0.881028413772583}
I0312 15:59:07.712442 139664575932160 logging_writer.py:48] [70867] accumulated_eval_time=17273.362685, accumulated_logging_time=0.881028, accumulated_submission_time=25239.831425, global_step=70867, preemption_count=0, score=25239.831425, test/accuracy=0.691895, test/bleu=29.064057, test/loss=1.386745, test/num_examples=3003, total_duration=42516.241682, train/accuracy=0.664504, train/bleu=32.747214, train/loss=1.563105, validation/accuracy=0.677239, validation/bleu=29.261963, validation/loss=1.476276, validation/num_examples=3000
I0312 15:59:19.790698 139664567539456 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.1979535073041916, loss=1.6882404088974
I0312 15:59:55.294626 139664575932160 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20519901812076569, loss=1.6975460052490234
I0312 16:00:30.846714 139664567539456 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.1959972083568573, loss=1.6228652000427246
I0312 16:01:06.462083 139664575932160 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.19764532148838043, loss=1.686532735824585
I0312 16:01:42.035759 139664567539456 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.18688581883907318, loss=1.6826001405715942
I0312 16:02:17.600564 139664575932160 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.204639732837677, loss=1.657646894454956
I0312 16:02:53.214287 139664567539456 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.19532433152198792, loss=1.6474934816360474
I0312 16:03:28.786346 139664575932160 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.1981903612613678, loss=1.6148555278778076
I0312 16:04:04.359640 139664567539456 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.19681328535079956, loss=1.6537206172943115
I0312 16:04:39.960458 139664575932160 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.20994195342063904, loss=1.7329789400100708
I0312 16:05:15.546479 139664567539456 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.19422127306461334, loss=1.6945631504058838
I0312 16:05:51.144104 139664575932160 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.19685234129428864, loss=1.6145274639129639
I0312 16:06:26.707710 139664567539456 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.19887353479862213, loss=1.6111700534820557
I0312 16:07:02.297152 139664575932160 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.21932637691497803, loss=1.7253607511520386
I0312 16:07:37.903634 139664567539456 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2160084992647171, loss=1.5721731185913086
I0312 16:08:13.509041 139664575932160 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.21201761066913605, loss=1.6627874374389648
I0312 16:08:49.049926 139664567539456 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2032296508550644, loss=1.698224663734436
I0312 16:09:24.658633 139664575932160 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2196969985961914, loss=1.657758355140686
I0312 16:10:00.278360 139664567539456 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.24586106836795807, loss=1.7066642045974731
I0312 16:10:35.921359 139664575932160 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.20420531928539276, loss=1.7759432792663574
I0312 16:11:11.510247 139664567539456 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.20453813672065735, loss=1.5743542909622192
I0312 16:11:47.083067 139664575932160 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.19963355362415314, loss=1.6264299154281616
I0312 16:12:22.674315 139664567539456 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.19641940295696259, loss=1.5983244180679321
I0312 16:12:58.275233 139664575932160 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.21255724132061005, loss=1.6495565176010132
I0312 16:13:07.968638 139834281293632 spec.py:321] Evaluating on the training split.
I0312 16:13:10.950619 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 16:17:29.211931 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 16:17:31.899813 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 16:20:41.310139 139834281293632 spec.py:349] Evaluating on the test split.
I0312 16:20:43.970111 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 16:24:00.228499 139834281293632 submission_runner.py:420] Time since start: 44008.78s, 	Step: 73229, 	{'train/accuracy': 0.6643652319908142, 'train/loss': 1.574614405632019, 'train/bleu': 32.75438032641624, 'validation/accuracy': 0.6768794059753418, 'validation/loss': 1.4778293371200562, 'validation/bleu': 29.360067346815917, 'validation/num_examples': 3000, 'test/accuracy': 0.6896170973777771, 'test/loss': 1.3845664262771606, 'test/bleu': 29.036617066834708, 'test/num_examples': 3003, 'score': 26080.004014968872, 'total_duration': 44008.78098034859, 'accumulated_submission_time': 26080.004014968872, 'accumulated_eval_time': 17925.62251186371, 'accumulated_logging_time': 0.9149608612060547}
I0312 16:24:00.252481 139664567539456 logging_writer.py:48] [73229] accumulated_eval_time=17925.622512, accumulated_logging_time=0.914961, accumulated_submission_time=26080.004015, global_step=73229, preemption_count=0, score=26080.004015, test/accuracy=0.689617, test/bleu=29.036617, test/loss=1.384566, test/num_examples=3003, total_duration=44008.780980, train/accuracy=0.664365, train/bleu=32.754380, train/loss=1.574614, validation/accuracy=0.676879, validation/bleu=29.360067, validation/loss=1.477829, validation/num_examples=3000
I0312 16:24:25.802360 139664575932160 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.20786282420158386, loss=1.589539885520935
I0312 16:25:01.315854 139664567539456 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.23954620957374573, loss=1.634629726409912
I0312 16:25:36.888334 139664575932160 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.18726511299610138, loss=1.6368190050125122
I0312 16:26:12.448665 139664567539456 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.21468551456928253, loss=1.660145878791809
I0312 16:26:48.047778 139664575932160 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.19306091964244843, loss=1.6501339673995972
I0312 16:27:23.620588 139664567539456 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.20744755864143372, loss=1.629081130027771
I0312 16:27:59.208866 139664575932160 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.20971356332302094, loss=1.7046363353729248
I0312 16:28:34.784248 139664567539456 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.21058407425880432, loss=1.6089814901351929
I0312 16:29:10.379679 139664575932160 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.2170141637325287, loss=1.6396064758300781
I0312 16:29:45.972543 139664567539456 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.19897662103176117, loss=1.6441885232925415
I0312 16:30:21.560018 139664575932160 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.19546014070510864, loss=1.7684788703918457
I0312 16:30:57.145130 139664567539456 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2090398669242859, loss=1.6509690284729004
I0312 16:31:32.789038 139664575932160 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.22421319782733917, loss=1.725966453552246
I0312 16:32:08.388868 139664567539456 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.19976939260959625, loss=1.6118879318237305
I0312 16:32:43.967139 139664575932160 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.20774894952774048, loss=1.6275906562805176
I0312 16:33:19.573931 139664567539456 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.22842927277088165, loss=1.6923601627349854
I0312 16:33:55.182121 139664575932160 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.20439068973064423, loss=1.6319434642791748
I0312 16:34:30.759923 139664567539456 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2072342336177826, loss=1.6288411617279053
I0312 16:35:06.315598 139664575932160 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.21122980117797852, loss=1.720194697380066
I0312 16:35:41.879245 139664567539456 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20476998388767242, loss=1.6452670097351074
I0312 16:36:17.465631 139664575932160 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.18998950719833374, loss=1.6349458694458008
I0312 16:36:53.069031 139664567539456 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20897197723388672, loss=1.6456319093704224
I0312 16:37:28.681407 139664575932160 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.18782542645931244, loss=1.5757755041122437
I0312 16:38:00.430398 139834281293632 spec.py:321] Evaluating on the training split.
I0312 16:38:03.412611 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 16:42:25.735862 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 16:42:28.403192 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 16:45:58.516349 139834281293632 spec.py:349] Evaluating on the test split.
I0312 16:46:01.185856 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 16:50:15.802249 139834281293632 submission_runner.py:420] Time since start: 45584.35s, 	Step: 75591, 	{'train/accuracy': 0.6765106320381165, 'train/loss': 1.4892783164978027, 'train/bleu': 33.98489511197334, 'validation/accuracy': 0.6804627180099487, 'validation/loss': 1.4630177021026611, 'validation/bleu': 29.06213421059305, 'validation/num_examples': 3000, 'test/accuracy': 0.6935797333717346, 'test/loss': 1.3710495233535767, 'test/bleu': 29.248124891316294, 'test/num_examples': 3003, 'score': 26920.101365327835, 'total_duration': 45584.35468482971, 'accumulated_submission_time': 26920.101365327835, 'accumulated_eval_time': 18660.99428129196, 'accumulated_logging_time': 0.9479324817657471}
I0312 16:50:15.830815 139664567539456 logging_writer.py:48] [75591] accumulated_eval_time=18660.994281, accumulated_logging_time=0.947932, accumulated_submission_time=26920.101365, global_step=75591, preemption_count=0, score=26920.101365, test/accuracy=0.693580, test/bleu=29.248125, test/loss=1.371050, test/num_examples=3003, total_duration=45584.354685, train/accuracy=0.676511, train/bleu=33.984895, train/loss=1.489278, validation/accuracy=0.680463, validation/bleu=29.062134, validation/loss=1.463018, validation/num_examples=3000
I0312 16:50:19.399894 139664575932160 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.1963469386100769, loss=1.6841936111450195
I0312 16:50:54.895200 139664567539456 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.25627774000167847, loss=1.6546422243118286
I0312 16:51:30.492621 139664575932160 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.20663811266422272, loss=1.7012757062911987
I0312 16:52:06.142518 139664567539456 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2056034505367279, loss=1.6772100925445557
I0312 16:52:41.727038 139664575932160 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20041270554065704, loss=1.7131599187850952
I0312 16:53:17.339522 139664567539456 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.20944462716579437, loss=1.5690456628799438
I0312 16:53:52.952329 139664575932160 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21146903932094574, loss=1.64390230178833
I0312 16:54:28.584801 139664567539456 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.25045478343963623, loss=1.6739901304244995
I0312 16:55:04.209964 139664575932160 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.20705948770046234, loss=1.6611156463623047
I0312 16:55:39.840996 139664567539456 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.192719966173172, loss=1.7003519535064697
I0312 16:56:15.423326 139664575932160 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2640499472618103, loss=1.6957511901855469
I0312 16:56:51.029950 139664567539456 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.20156989991664886, loss=1.663442850112915
I0312 16:57:26.623450 139664575932160 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.20671774446964264, loss=1.7482991218566895
I0312 16:58:02.233314 139664567539456 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.19422177970409393, loss=1.6632800102233887
I0312 16:58:37.824367 139664575932160 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.22032742202281952, loss=1.5890758037567139
I0312 16:59:13.385147 139664567539456 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.20468074083328247, loss=1.6193095445632935
I0312 16:59:48.991089 139664575932160 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.19801408052444458, loss=1.6062029600143433
I0312 17:00:24.609792 139664567539456 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.1975889950990677, loss=1.6549737453460693
I0312 17:01:00.180542 139664575932160 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.20971344411373138, loss=1.633009910583496
I0312 17:01:35.760873 139664567539456 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.20013487339019775, loss=1.6515448093414307
I0312 17:02:11.337452 139664575932160 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.20028091967105865, loss=1.6407939195632935
I0312 17:02:46.963142 139664567539456 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.20886948704719543, loss=1.664612054824829
I0312 17:03:22.515924 139664575932160 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.20921428501605988, loss=1.6126794815063477
I0312 17:03:58.104518 139664567539456 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.19855643808841705, loss=1.6416480541229248
I0312 17:04:15.971884 139834281293632 spec.py:321] Evaluating on the training split.
I0312 17:04:18.952872 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:08:57.702879 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 17:09:00.375136 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:13:28.387893 139834281293632 spec.py:349] Evaluating on the test split.
I0312 17:13:31.075754 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:18:07.965372 139834281293632 submission_runner.py:420] Time since start: 47256.52s, 	Step: 77952, 	{'train/accuracy': 0.6673470735549927, 'train/loss': 1.5419315099716187, 'train/bleu': 32.9429847730829, 'validation/accuracy': 0.680648684501648, 'validation/loss': 1.4586224555969238, 'validation/bleu': 28.36214799696568, 'validation/num_examples': 3000, 'test/accuracy': 0.6959154009819031, 'test/loss': 1.3664885759353638, 'test/bleu': 29.535897402171667, 'test/num_examples': 3003, 'score': 27760.15807056427, 'total_duration': 47256.517852544785, 'accumulated_submission_time': 27760.15807056427, 'accumulated_eval_time': 19492.98771739006, 'accumulated_logging_time': 0.9874765872955322}
I0312 17:18:07.990529 139664575932160 logging_writer.py:48] [77952] accumulated_eval_time=19492.987717, accumulated_logging_time=0.987477, accumulated_submission_time=27760.158071, global_step=77952, preemption_count=0, score=27760.158071, test/accuracy=0.695915, test/bleu=29.535897, test/loss=1.366489, test/num_examples=3003, total_duration=47256.517853, train/accuracy=0.667347, train/bleu=32.942985, train/loss=1.541932, validation/accuracy=0.680649, validation/bleu=28.362148, validation/loss=1.458622, validation/num_examples=3000
I0312 17:18:25.373678 139664567539456 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.21165205538272858, loss=1.7406893968582153
I0312 17:19:00.872757 139664575932160 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.21183578670024872, loss=1.637247085571289
I0312 17:19:36.441215 139664567539456 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.2220112830400467, loss=1.707922339439392
I0312 17:20:12.017492 139664575932160 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.19804783165454865, loss=1.6113009452819824
I0312 17:20:47.579467 139664567539456 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2302182912826538, loss=1.5925267934799194
I0312 17:21:23.163883 139664575932160 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.1951422244310379, loss=1.6213865280151367
I0312 17:21:58.753366 139664567539456 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2221495509147644, loss=1.718743085861206
I0312 17:22:34.327092 139664575932160 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.18904925882816315, loss=1.596883773803711
I0312 17:23:09.904562 139664567539456 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.21468262374401093, loss=1.6448397636413574
I0312 17:23:45.475248 139664575932160 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.20438577234745026, loss=1.6068183183670044
I0312 17:24:21.065383 139664567539456 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2406667321920395, loss=1.6083601713180542
I0312 17:24:56.651349 139664575932160 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.203624427318573, loss=1.6557724475860596
I0312 17:25:32.253365 139664567539456 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.19962500035762787, loss=1.6028870344161987
I0312 17:26:07.881728 139664575932160 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.22890760004520416, loss=1.6025090217590332
I0312 17:26:43.483966 139664567539456 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.20068109035491943, loss=1.6182042360305786
I0312 17:27:19.077015 139664575932160 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2161472886800766, loss=1.622340440750122
I0312 17:27:54.650280 139664567539456 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.20838063955307007, loss=1.6360795497894287
I0312 17:28:30.229859 139664575932160 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2049330323934555, loss=1.768865942955017
I0312 17:29:05.810899 139664567539456 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.21869894862174988, loss=1.6207884550094604
I0312 17:29:41.385550 139664575932160 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.20537206530570984, loss=1.5432136058807373
I0312 17:30:16.986468 139664567539456 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.19670435786247253, loss=1.5569970607757568
I0312 17:30:52.564896 139664575932160 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.36177030205726624, loss=1.658768653869629
I0312 17:31:28.145727 139664567539456 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.1922684907913208, loss=1.6119152307510376
I0312 17:32:03.729377 139664575932160 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.20274056494235992, loss=1.7236467599868774
I0312 17:32:08.081497 139834281293632 spec.py:321] Evaluating on the training split.
I0312 17:32:11.047099 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:35:57.859227 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 17:36:00.526803 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:38:35.511834 139834281293632 spec.py:349] Evaluating on the test split.
I0312 17:38:38.188378 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:41:03.667541 139834281293632 submission_runner.py:420] Time since start: 48632.22s, 	Step: 80314, 	{'train/accuracy': 0.6680836081504822, 'train/loss': 1.5501035451889038, 'train/bleu': 32.917216136696155, 'validation/accuracy': 0.6810950636863708, 'validation/loss': 1.452638030052185, 'validation/bleu': 29.69538787912534, 'validation/num_examples': 3000, 'test/accuracy': 0.6945790648460388, 'test/loss': 1.360152244567871, 'test/bleu': 29.357325711266018, 'test/num_examples': 3003, 'score': 28600.16921401024, 'total_duration': 48632.22001385689, 'accumulated_submission_time': 28600.16921401024, 'accumulated_eval_time': 20028.57369875908, 'accumulated_logging_time': 1.0211431980133057}
I0312 17:41:03.692376 139664567539456 logging_writer.py:48] [80314] accumulated_eval_time=20028.573699, accumulated_logging_time=1.021143, accumulated_submission_time=28600.169214, global_step=80314, preemption_count=0, score=28600.169214, test/accuracy=0.694579, test/bleu=29.357326, test/loss=1.360152, test/num_examples=3003, total_duration=48632.220014, train/accuracy=0.668084, train/bleu=32.917216, train/loss=1.550104, validation/accuracy=0.681095, validation/bleu=29.695388, validation/loss=1.452638, validation/num_examples=3000
I0312 17:41:34.557969 139664575932160 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20557688176631927, loss=1.6286845207214355
I0312 17:42:10.067583 139664567539456 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.20901507139205933, loss=1.6462253332138062
I0312 17:42:45.663876 139664575932160 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.1889522224664688, loss=1.5850021839141846
I0312 17:43:21.227645 139664567539456 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20853939652442932, loss=1.6120024919509888
I0312 17:43:56.791105 139664575932160 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.219163179397583, loss=1.7220603227615356
I0312 17:44:32.368993 139664567539456 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2135394811630249, loss=1.6370232105255127
I0312 17:45:07.945708 139664575932160 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.20099909603595734, loss=1.661463975906372
I0312 17:45:43.491347 139664567539456 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.2047240436077118, loss=1.6854033470153809
I0312 17:46:19.079972 139664575932160 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.18975085020065308, loss=1.5641402006149292
I0312 17:46:54.644469 139664567539456 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.23222990334033966, loss=1.5898771286010742
I0312 17:47:30.245370 139664575932160 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.20912553369998932, loss=1.6180906295776367
I0312 17:48:05.848413 139664567539456 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.19616401195526123, loss=1.6323683261871338
I0312 17:48:41.433871 139664575932160 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.21921628713607788, loss=1.6363190412521362
I0312 17:49:17.034172 139664567539456 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.21353064477443695, loss=1.6420739889144897
I0312 17:49:52.660681 139664575932160 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.19711358845233917, loss=1.5911859273910522
I0312 17:50:28.246094 139664567539456 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.2136315554380417, loss=1.6920101642608643
I0312 17:51:03.795783 139664575932160 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2153138667345047, loss=1.6055858135223389
I0312 17:51:39.354682 139664567539456 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.1982080191373825, loss=1.617215633392334
I0312 17:52:14.961084 139664575932160 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.19759061932563782, loss=1.6528762578964233
I0312 17:52:50.588216 139664567539456 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.20029893517494202, loss=1.7067874670028687
I0312 17:53:26.147901 139664575932160 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20891475677490234, loss=1.595323920249939
I0312 17:54:01.723191 139664567539456 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20697495341300964, loss=1.7111915349960327
I0312 17:54:37.311466 139664575932160 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.2070995420217514, loss=1.6062649488449097
I0312 17:55:03.715988 139834281293632 spec.py:321] Evaluating on the training split.
I0312 17:55:06.683458 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 17:59:09.244295 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 17:59:11.905812 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:02:25.390301 139834281293632 spec.py:349] Evaluating on the test split.
I0312 18:02:28.051348 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:05:40.621661 139834281293632 submission_runner.py:420] Time since start: 50109.17s, 	Step: 82676, 	{'train/accuracy': 0.6728290319442749, 'train/loss': 1.5096951723098755, 'train/bleu': 33.626086977802274, 'validation/accuracy': 0.6835501194000244, 'validation/loss': 1.4427237510681152, 'validation/bleu': 29.914303811257952, 'validation/num_examples': 3000, 'test/accuracy': 0.6989832520484924, 'test/loss': 1.3479254245758057, 'test/bleu': 29.86139070986005, 'test/num_examples': 3003, 'score': 29440.110209703445, 'total_duration': 50109.174132585526, 'accumulated_submission_time': 29440.110209703445, 'accumulated_eval_time': 20665.479345798492, 'accumulated_logging_time': 1.05684494972229}
I0312 18:05:40.647174 139664567539456 logging_writer.py:48] [82676] accumulated_eval_time=20665.479346, accumulated_logging_time=1.056845, accumulated_submission_time=29440.110210, global_step=82676, preemption_count=0, score=29440.110210, test/accuracy=0.698983, test/bleu=29.861391, test/loss=1.347925, test/num_examples=3003, total_duration=50109.174133, train/accuracy=0.672829, train/bleu=33.626087, train/loss=1.509695, validation/accuracy=0.683550, validation/bleu=29.914304, validation/loss=1.442724, validation/num_examples=3000
I0312 18:05:49.522090 139664575932160 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.21371014416217804, loss=1.6566834449768066
I0312 18:06:25.058615 139664567539456 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.258002907037735, loss=1.6264135837554932
I0312 18:07:00.631736 139664575932160 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2071669101715088, loss=1.571549892425537
I0312 18:07:36.187774 139664567539456 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.19978201389312744, loss=1.5968981981277466
I0312 18:08:11.779992 139664575932160 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.21758222579956055, loss=1.656558632850647
I0312 18:08:47.336369 139664567539456 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.19079285860061646, loss=1.5907307863235474
I0312 18:09:22.898234 139664575932160 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.25941240787506104, loss=1.6648540496826172
I0312 18:09:58.408294 139664567539456 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.21189972758293152, loss=1.6475416421890259
I0312 18:10:33.997323 139664575932160 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.1968083530664444, loss=1.6423423290252686
I0312 18:11:09.576578 139664567539456 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.25887706875801086, loss=1.6601248979568481
I0312 18:11:45.131730 139664575932160 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.20664043724536896, loss=1.6461044549942017
I0312 18:12:20.690521 139664567539456 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.19903963804244995, loss=1.678702473640442
I0312 18:12:56.287926 139664575932160 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.20923763513565063, loss=1.6680617332458496
I0312 18:13:31.890584 139664567539456 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.2108442485332489, loss=1.5901652574539185
I0312 18:14:07.458414 139664575932160 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.2057528793811798, loss=1.5958503484725952
I0312 18:14:43.019728 139664567539456 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.21553726494312286, loss=1.6257375478744507
I0312 18:15:18.585372 139664575932160 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.21578393876552582, loss=1.6026755571365356
I0312 18:15:54.188683 139664567539456 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.2170063704252243, loss=1.6403567790985107
I0312 18:16:29.767111 139664575932160 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.21104229986667633, loss=1.6072337627410889
I0312 18:17:05.361282 139664567539456 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.21396194398403168, loss=1.572993516921997
I0312 18:17:40.974646 139664575932160 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.19432315230369568, loss=1.6060138940811157
I0312 18:18:16.559076 139664567539456 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.21213985979557037, loss=1.617722988128662
I0312 18:18:52.154592 139664575932160 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2055252492427826, loss=1.6234612464904785
I0312 18:19:27.727418 139664567539456 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.1997014582157135, loss=1.6037180423736572
I0312 18:19:40.954526 139834281293632 spec.py:321] Evaluating on the training split.
I0312 18:19:43.917456 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:22:36.349454 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 18:22:39.016765 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:25:17.411329 139834281293632 spec.py:349] Evaluating on the test split.
I0312 18:25:20.075359 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:28:01.467795 139834281293632 submission_runner.py:420] Time since start: 51450.02s, 	Step: 85039, 	{'train/accuracy': 0.6680791974067688, 'train/loss': 1.537145972251892, 'train/bleu': 33.650963677786145, 'validation/accuracy': 0.6841328740119934, 'validation/loss': 1.439337968826294, 'validation/bleu': 29.67939265029403, 'validation/num_examples': 3000, 'test/accuracy': 0.6989135146141052, 'test/loss': 1.3375647068023682, 'test/bleu': 29.94726672862276, 'test/num_examples': 3003, 'score': 30280.33723807335, 'total_duration': 51450.02022433281, 'accumulated_submission_time': 30280.33723807335, 'accumulated_eval_time': 21165.99251294136, 'accumulated_logging_time': 1.0917017459869385}
I0312 18:28:01.500359 139664575932160 logging_writer.py:48] [85039] accumulated_eval_time=21165.992513, accumulated_logging_time=1.091702, accumulated_submission_time=30280.337238, global_step=85039, preemption_count=0, score=30280.337238, test/accuracy=0.698914, test/bleu=29.947267, test/loss=1.337565, test/num_examples=3003, total_duration=51450.020224, train/accuracy=0.668079, train/bleu=33.650964, train/loss=1.537146, validation/accuracy=0.684133, validation/bleu=29.679393, validation/loss=1.439338, validation/num_examples=3000
I0312 18:28:23.512881 139664567539456 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.20699720084667206, loss=1.503735899925232
I0312 18:28:59.046177 139664575932160 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.21238373219966888, loss=1.6576387882232666
I0312 18:29:34.632034 139664567539456 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.20735448598861694, loss=1.6779258251190186
I0312 18:30:10.255843 139664575932160 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.2148372083902359, loss=1.590456247329712
I0312 18:30:45.838139 139664567539456 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.20922960340976715, loss=1.5408583879470825
I0312 18:31:21.509055 139664575932160 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2158193290233612, loss=1.6453531980514526
I0312 18:31:57.089041 139664567539456 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.20536993443965912, loss=1.5784646272659302
I0312 18:32:32.676632 139664575932160 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.20583555102348328, loss=1.6191157102584839
I0312 18:33:08.234972 139664567539456 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.2655808627605438, loss=1.5965576171875
I0312 18:33:43.847144 139664575932160 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.21569696068763733, loss=1.621495008468628
I0312 18:34:19.423851 139664567539456 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.21728503704071045, loss=1.6366490125656128
I0312 18:34:55.024127 139664575932160 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.20271115005016327, loss=1.574367642402649
I0312 18:35:30.605251 139664567539456 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.21020552515983582, loss=1.6138783693313599
I0312 18:36:06.206804 139664575932160 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.22203171253204346, loss=1.5929347276687622
I0312 18:36:41.777375 139664567539456 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2124415785074234, loss=1.5977646112442017
I0312 18:37:17.345790 139664575932160 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.20073316991329193, loss=1.5836281776428223
I0312 18:37:52.957049 139664567539456 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.19891494512557983, loss=1.6214529275894165
I0312 18:38:28.574795 139664575932160 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2203208953142166, loss=1.5994735956192017
I0312 18:39:04.176514 139664567539456 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.23732751607894897, loss=1.6891508102416992
I0312 18:39:39.779337 139664575932160 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.21110771596431732, loss=1.6209484338760376
I0312 18:40:15.355654 139664567539456 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.21026301383972168, loss=1.6203194856643677
I0312 18:40:50.919288 139664575932160 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2262028157711029, loss=1.5686956644058228
I0312 18:41:26.476876 139664567539456 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.20367585122585297, loss=1.5610617399215698
I0312 18:42:02.056979 139664575932160 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.21561266481876373, loss=1.6010624170303345
I0312 18:42:02.063880 139834281293632 spec.py:321] Evaluating on the training split.
I0312 18:42:04.753607 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:45:44.859176 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 18:45:47.523524 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:48:57.777376 139834281293632 spec.py:349] Evaluating on the test split.
I0312 18:49:00.439306 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 18:52:11.956480 139834281293632 submission_runner.py:420] Time since start: 52900.51s, 	Step: 87401, 	{'train/accuracy': 0.6687669157981873, 'train/loss': 1.5372868776321411, 'train/bleu': 33.083875804779154, 'validation/accuracy': 0.6854843497276306, 'validation/loss': 1.4303168058395386, 'validation/bleu': 30.13844842941912, 'validation/num_examples': 3000, 'test/accuracy': 0.7013421654701233, 'test/loss': 1.3306312561035156, 'test/bleu': 29.809605628979703, 'test/num_examples': 3003, 'score': 31120.81729412079, 'total_duration': 52900.50894832611, 'accumulated_submission_time': 31120.81729412079, 'accumulated_eval_time': 21775.885035037994, 'accumulated_logging_time': 1.1347846984863281}
I0312 18:52:11.982603 139664567539456 logging_writer.py:48] [87401] accumulated_eval_time=21775.885035, accumulated_logging_time=1.134785, accumulated_submission_time=31120.817294, global_step=87401, preemption_count=0, score=31120.817294, test/accuracy=0.701342, test/bleu=29.809606, test/loss=1.330631, test/num_examples=3003, total_duration=52900.508948, train/accuracy=0.668767, train/bleu=33.083876, train/loss=1.537287, validation/accuracy=0.685484, validation/bleu=30.138448, validation/loss=1.430317, validation/num_examples=3000
I0312 18:52:47.456089 139664575932160 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.21346433460712433, loss=1.5753082036972046
I0312 18:53:23.015928 139664567539456 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.22027450799942017, loss=1.683841347694397
I0312 18:53:58.598859 139664575932160 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.2094441056251526, loss=1.6173725128173828
I0312 18:54:34.162579 139664567539456 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.2082289457321167, loss=1.526585578918457
I0312 18:55:09.776417 139664575932160 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.2083536684513092, loss=1.5794981718063354
I0312 18:55:45.382275 139664567539456 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.20594249665737152, loss=1.5652090311050415
I0312 18:56:20.963009 139664575932160 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.21532651782035828, loss=1.6322829723358154
I0312 18:56:56.537277 139664567539456 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.20527881383895874, loss=1.5989614725112915
I0312 18:57:32.108541 139664575932160 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.213729128241539, loss=1.5450568199157715
I0312 18:58:07.685206 139664567539456 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.23791062831878662, loss=1.60456383228302
I0312 18:58:43.257989 139664575932160 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.19426123797893524, loss=1.546442985534668
I0312 18:59:18.849828 139664567539456 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.20466794073581696, loss=1.5893666744232178
I0312 18:59:54.417584 139664575932160 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21666228771209717, loss=1.6083019971847534
I0312 19:00:29.991706 139664567539456 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.2080165147781372, loss=1.6108229160308838
I0312 19:01:05.592121 139664575932160 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.20966736972332, loss=1.4983553886413574
I0312 19:01:41.128604 139664567539456 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.19910475611686707, loss=1.5269025564193726
I0312 19:02:16.705106 139664575932160 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.20764511823654175, loss=1.550582766532898
I0312 19:02:52.278885 139664567539456 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2109614908695221, loss=1.5529961585998535
I0312 19:03:27.849993 139664575932160 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.20427797734737396, loss=1.5397119522094727
I0312 19:04:03.432939 139664567539456 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.2190551906824112, loss=1.530299425125122
I0312 19:04:39.015618 139664575932160 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.22801387310028076, loss=1.6324080228805542
I0312 19:05:14.621069 139664567539456 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.20767630636692047, loss=1.6107813119888306
I0312 19:05:50.207402 139664575932160 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.2255120724439621, loss=1.6327602863311768
I0312 19:06:11.983117 139834281293632 spec.py:321] Evaluating on the training split.
I0312 19:06:14.949753 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:09:47.729677 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 19:09:50.401041 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:12:29.754374 139834281293632 spec.py:349] Evaluating on the test split.
I0312 19:12:32.421856 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:15:29.816254 139834281293632 submission_runner.py:420] Time since start: 54298.37s, 	Step: 89763, 	{'train/accuracy': 0.6764796376228333, 'train/loss': 1.4874475002288818, 'train/bleu': 33.59193421065701, 'validation/accuracy': 0.686129093170166, 'validation/loss': 1.4221100807189941, 'validation/bleu': 30.10132998243684, 'validation/num_examples': 3000, 'test/accuracy': 0.7024112939834595, 'test/loss': 1.3254374265670776, 'test/bleu': 30.162695170328295, 'test/num_examples': 3003, 'score': 31960.73653626442, 'total_duration': 54298.36872267723, 'accumulated_submission_time': 31960.73653626442, 'accumulated_eval_time': 22333.71812582016, 'accumulated_logging_time': 1.1713433265686035}
I0312 19:15:29.842651 139664567539456 logging_writer.py:48] [89763] accumulated_eval_time=22333.718126, accumulated_logging_time=1.171343, accumulated_submission_time=31960.736536, global_step=89763, preemption_count=0, score=31960.736536, test/accuracy=0.702411, test/bleu=30.162695, test/loss=1.325437, test/num_examples=3003, total_duration=54298.368723, train/accuracy=0.676480, train/bleu=33.591934, train/loss=1.487448, validation/accuracy=0.686129, validation/bleu=30.101330, validation/loss=1.422110, validation/num_examples=3000
I0312 19:15:43.369833 139664575932160 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.21156184375286102, loss=1.5382903814315796
I0312 19:16:18.881709 139664567539456 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2072659283876419, loss=1.5853452682495117
I0312 19:16:54.431516 139664575932160 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.21516315639019012, loss=1.5953364372253418
I0312 19:17:30.009583 139664567539456 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.22062887251377106, loss=1.5855276584625244
I0312 19:18:05.620988 139664575932160 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.20976267755031586, loss=1.5925415754318237
I0312 19:18:41.204293 139664567539456 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2073560208082199, loss=1.673403024673462
I0312 19:19:16.789067 139664575932160 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.21992754936218262, loss=1.562701940536499
I0312 19:19:52.358982 139664567539456 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.2367645502090454, loss=1.5495455265045166
I0312 19:20:27.930980 139664575932160 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.21227850019931793, loss=1.5939897298812866
I0312 19:21:03.519801 139664567539456 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.2135312259197235, loss=1.5444473028182983
I0312 19:21:39.130037 139664575932160 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.20356467366218567, loss=1.591478705406189
I0312 19:22:14.732682 139664567539456 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.2163519263267517, loss=1.5455763339996338
I0312 19:22:50.302824 139664575932160 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21139077842235565, loss=1.5731687545776367
I0312 19:23:25.888019 139664567539456 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.21013440191745758, loss=1.5409077405929565
I0312 19:24:01.484455 139664575932160 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.22701945900917053, loss=1.680757999420166
I0312 19:24:37.053714 139664567539456 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.21126656234264374, loss=1.5611217021942139
I0312 19:25:12.660511 139664575932160 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2099827527999878, loss=1.5640218257904053
I0312 19:25:48.314868 139664567539456 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.21193619072437286, loss=1.5751278400421143
I0312 19:26:23.947581 139664575932160 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.19491265714168549, loss=1.5779032707214355
I0312 19:26:59.577633 139664567539456 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.20925472676753998, loss=1.5819677114486694
I0312 19:27:35.169661 139664575932160 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3344961106777191, loss=1.684098482131958
I0312 19:28:10.758903 139664567539456 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2105451375246048, loss=1.6662843227386475
I0312 19:28:46.326980 139664575932160 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2152831107378006, loss=1.5791906118392944
I0312 19:29:21.917288 139664567539456 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.21458688378334045, loss=1.5819510221481323
I0312 19:29:29.834874 139834281293632 spec.py:321] Evaluating on the training split.
I0312 19:29:32.812561 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:33:48.324121 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 19:33:50.996586 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:37:25.029280 139834281293632 spec.py:349] Evaluating on the test split.
I0312 19:37:27.704833 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:41:18.595927 139834281293632 submission_runner.py:420] Time since start: 55847.15s, 	Step: 92124, 	{'train/accuracy': 0.6730408668518066, 'train/loss': 1.5171024799346924, 'train/bleu': 33.473575376051215, 'validation/accuracy': 0.6871086359024048, 'validation/loss': 1.4207348823547363, 'validation/bleu': 29.507340418635277, 'validation/num_examples': 3000, 'test/accuracy': 0.7033408880233765, 'test/loss': 1.3194371461868286, 'test/bleu': 30.099988416489992, 'test/num_examples': 3003, 'score': 32800.644548892975, 'total_duration': 55847.14839839935, 'accumulated_submission_time': 32800.644548892975, 'accumulated_eval_time': 23042.47913169861, 'accumulated_logging_time': 1.2089464664459229}
I0312 19:41:18.622787 139664575932160 logging_writer.py:48] [92124] accumulated_eval_time=23042.479132, accumulated_logging_time=1.208946, accumulated_submission_time=32800.644549, global_step=92124, preemption_count=0, score=32800.644549, test/accuracy=0.703341, test/bleu=30.099988, test/loss=1.319437, test/num_examples=3003, total_duration=55847.148398, train/accuracy=0.673041, train/bleu=33.473575, train/loss=1.517102, validation/accuracy=0.687109, validation/bleu=29.507340, validation/loss=1.420735, validation/num_examples=3000
I0312 19:41:45.968322 139664567539456 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.20124050974845886, loss=1.5795602798461914
I0312 19:42:21.546577 139664575932160 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21123485267162323, loss=1.5798708200454712
I0312 19:42:57.140622 139664567539456 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.24349352717399597, loss=1.6224530935287476
I0312 19:43:32.703818 139664575932160 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.2185230702161789, loss=1.5964512825012207
I0312 19:44:08.301945 139664567539456 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.21059532463550568, loss=1.5718061923980713
I0312 19:44:43.866989 139664575932160 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.20860083401203156, loss=1.5830378532409668
I0312 19:45:19.481473 139664567539456 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.20792736113071442, loss=1.534989595413208
I0312 19:45:55.082297 139664575932160 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2321779876947403, loss=1.6800943613052368
I0312 19:46:30.685830 139664567539456 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21385444700717926, loss=1.6233521699905396
I0312 19:47:06.253544 139664575932160 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.22661346197128296, loss=1.56890070438385
I0312 19:47:41.831900 139664567539456 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.19838310778141022, loss=1.515601396560669
I0312 19:48:17.405135 139664575932160 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.2281380444765091, loss=1.6165192127227783
I0312 19:48:52.976304 139664567539456 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21723572909832, loss=1.5344752073287964
I0312 19:49:28.566069 139664575932160 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.20953704416751862, loss=1.576124906539917
I0312 19:50:04.146594 139664567539456 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.22119444608688354, loss=1.5616267919540405
I0312 19:50:39.720762 139664575932160 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.21823740005493164, loss=1.5293176174163818
I0312 19:51:15.294404 139664567539456 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.20414142310619354, loss=1.5064778327941895
I0312 19:51:50.905219 139664575932160 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.22842925786972046, loss=1.6228234767913818
I0312 19:52:26.467469 139664567539456 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.20498597621917725, loss=1.5410650968551636
I0312 19:53:02.057015 139664575932160 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.20225638151168823, loss=1.5220128297805786
I0312 19:53:37.644402 139664567539456 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.21577654778957367, loss=1.5878849029541016
I0312 19:54:13.227726 139664575932160 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.21476224064826965, loss=1.601717472076416
I0312 19:54:48.824611 139664567539456 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.22845104336738586, loss=1.6529617309570312
I0312 19:55:18.803311 139834281293632 spec.py:321] Evaluating on the training split.
I0312 19:55:21.771813 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 19:58:36.530355 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 19:58:39.188677 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:01:29.062259 139834281293632 spec.py:349] Evaluating on the test split.
I0312 20:01:31.753571 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:04:09.066598 139834281293632 submission_runner.py:420] Time since start: 57217.62s, 	Step: 94486, 	{'train/accuracy': 0.6843194961547852, 'train/loss': 1.4386706352233887, 'train/bleu': 34.57505063779952, 'validation/accuracy': 0.6879270076751709, 'validation/loss': 1.4136199951171875, 'validation/bleu': 30.146262777719443, 'validation/num_examples': 3000, 'test/accuracy': 0.7031317353248596, 'test/loss': 1.3148167133331299, 'test/bleu': 30.017533821881695, 'test/num_examples': 3003, 'score': 33640.74346327782, 'total_duration': 57217.61907982826, 'accumulated_submission_time': 33640.74346327782, 'accumulated_eval_time': 23572.74237060547, 'accumulated_logging_time': 1.2449169158935547}
I0312 20:04:09.093047 139664575932160 logging_writer.py:48] [94486] accumulated_eval_time=23572.742371, accumulated_logging_time=1.244917, accumulated_submission_time=33640.743463, global_step=94486, preemption_count=0, score=33640.743463, test/accuracy=0.703132, test/bleu=30.017534, test/loss=1.314817, test/num_examples=3003, total_duration=57217.619080, train/accuracy=0.684319, train/bleu=34.575051, train/loss=1.438671, validation/accuracy=0.687927, validation/bleu=30.146263, validation/loss=1.413620, validation/num_examples=3000
I0312 20:04:14.441395 139664567539456 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.7215669751167297, loss=1.6152607202529907
I0312 20:04:49.971846 139664575932160 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.2103992998600006, loss=1.5759756565093994
I0312 20:05:25.559437 139664567539456 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.22857892513275146, loss=1.6763198375701904
I0312 20:06:01.123036 139664575932160 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.20272378623485565, loss=1.5579463243484497
I0312 20:06:36.712962 139664567539456 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.20556099712848663, loss=1.5141456127166748
I0312 20:07:12.295641 139664575932160 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.21406827867031097, loss=1.5178881883621216
I0312 20:07:47.902772 139664567539456 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2105320692062378, loss=1.571387767791748
I0312 20:08:23.473194 139664575932160 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.22409264743328094, loss=1.6541669368743896
I0312 20:08:59.069699 139664567539456 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.20878195762634277, loss=1.501338005065918
I0312 20:09:34.643930 139664575932160 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2020796537399292, loss=1.4965499639511108
I0312 20:10:10.252538 139664567539456 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.21593181788921356, loss=1.5854610204696655
I0312 20:10:45.826178 139664575932160 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.2126866728067398, loss=1.4942172765731812
I0312 20:11:21.401168 139664567539456 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.233274444937706, loss=1.5861413478851318
I0312 20:11:56.989331 139664575932160 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.22549597918987274, loss=1.5637707710266113
I0312 20:12:32.539487 139664567539456 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.2335502803325653, loss=1.5914684534072876
I0312 20:13:08.113413 139664575932160 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.20600439608097076, loss=1.5099067687988281
I0312 20:13:43.698193 139664567539456 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.21361930668354034, loss=1.5733592510223389
I0312 20:14:19.272734 139664575932160 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.21351271867752075, loss=1.6278356313705444
I0312 20:14:54.852197 139664567539456 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.23070809245109558, loss=1.6125870943069458
I0312 20:15:30.457355 139664575932160 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.21224768459796906, loss=1.547032356262207
I0312 20:16:06.076492 139664567539456 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.3002382516860962, loss=1.5109318494796753
I0312 20:16:41.721009 139664575932160 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.21474671363830566, loss=1.4908894300460815
I0312 20:17:17.326976 139664567539456 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.21858933568000793, loss=1.572538137435913
I0312 20:17:52.943962 139664575932160 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.2065548151731491, loss=1.5553234815597534
I0312 20:18:09.404323 139834281293632 spec.py:321] Evaluating on the training split.
I0312 20:18:12.383903 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:22:27.429434 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 20:22:30.118142 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:26:08.229414 139834281293632 spec.py:349] Evaluating on the test split.
I0312 20:26:10.890610 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:29:57.318576 139834281293632 submission_runner.py:420] Time since start: 58765.87s, 	Step: 96848, 	{'train/accuracy': 0.678167462348938, 'train/loss': 1.4757825136184692, 'train/bleu': 33.812887360216756, 'validation/accuracy': 0.6871830224990845, 'validation/loss': 1.4069184064865112, 'validation/bleu': 30.265721225486356, 'validation/num_examples': 3000, 'test/accuracy': 0.7035849094390869, 'test/loss': 1.3081387281417847, 'test/bleu': 30.46707014258092, 'test/num_examples': 3003, 'score': 34480.9733145237, 'total_duration': 58765.871055841446, 'accumulated_submission_time': 34480.9733145237, 'accumulated_eval_time': 24280.656585216522, 'accumulated_logging_time': 1.280604362487793}
I0312 20:29:57.345517 139664567539456 logging_writer.py:48] [96848] accumulated_eval_time=24280.656585, accumulated_logging_time=1.280604, accumulated_submission_time=34480.973315, global_step=96848, preemption_count=0, score=34480.973315, test/accuracy=0.703585, test/bleu=30.467070, test/loss=1.308139, test/num_examples=3003, total_duration=58765.871056, train/accuracy=0.678167, train/bleu=33.812887, train/loss=1.475783, validation/accuracy=0.687183, validation/bleu=30.265721, validation/loss=1.406918, validation/num_examples=3000
I0312 20:30:16.197323 139664575932160 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.2081354558467865, loss=1.570194959640503
I0312 20:30:51.746275 139664567539456 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.2211381196975708, loss=1.6142985820770264
I0312 20:31:27.299474 139664575932160 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.20885862410068512, loss=1.5075647830963135
I0312 20:32:02.868250 139664567539456 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.21740759909152985, loss=1.5346416234970093
I0312 20:32:38.445697 139664575932160 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.21564944088459015, loss=1.5469658374786377
I0312 20:33:14.021413 139664567539456 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.2225341498851776, loss=1.5775501728057861
I0312 20:33:49.604772 139664575932160 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.22207899391651154, loss=1.58500075340271
I0312 20:34:25.159716 139664567539456 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.22100548446178436, loss=1.5505561828613281
I0312 20:35:00.745515 139664575932160 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21101979911327362, loss=1.5103139877319336
I0312 20:35:36.361519 139664567539456 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21808744966983795, loss=1.5346622467041016
I0312 20:36:11.981072 139664575932160 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.20436564087867737, loss=1.5523509979248047
I0312 20:36:47.601719 139664567539456 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21663440763950348, loss=1.5147628784179688
I0312 20:37:23.198630 139664575932160 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.206114262342453, loss=1.5439165830612183
I0312 20:37:58.779836 139664567539456 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.22178897261619568, loss=1.636457920074463
I0312 20:38:34.357681 139664575932160 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.2084333449602127, loss=1.5026353597640991
I0312 20:39:09.934941 139664567539456 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21571610867977142, loss=1.4572445154190063
I0312 20:39:45.511645 139664575932160 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.213494211435318, loss=1.4768376350402832
I0312 20:40:21.112161 139664567539456 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21902914345264435, loss=1.5346579551696777
I0312 20:40:56.711689 139664575932160 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.23246781527996063, loss=1.6101738214492798
I0312 20:41:32.301928 139664567539456 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.220834419131279, loss=1.6150076389312744
I0312 20:42:07.892715 139664575932160 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.22189189493656158, loss=1.5118955373764038
I0312 20:42:43.462051 139664567539456 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.20098283886909485, loss=1.548745036125183
I0312 20:43:19.064195 139664575932160 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.2127152532339096, loss=1.5911651849746704
I0312 20:43:54.676396 139664567539456 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.24412086606025696, loss=1.5448096990585327
I0312 20:43:57.596349 139834281293632 spec.py:321] Evaluating on the training split.
I0312 20:44:00.572938 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:47:28.812651 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 20:47:31.495196 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:50:26.893937 139834281293632 spec.py:349] Evaluating on the test split.
I0312 20:50:29.563024 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 20:53:20.512112 139834281293632 submission_runner.py:420] Time since start: 60169.06s, 	Step: 99210, 	{'train/accuracy': 0.6783736944198608, 'train/loss': 1.4781726598739624, 'train/bleu': 34.20753458293841, 'validation/accuracy': 0.6882989406585693, 'validation/loss': 1.4027429819107056, 'validation/bleu': 30.174071090892355, 'validation/num_examples': 3000, 'test/accuracy': 0.7043286561965942, 'test/loss': 1.3000746965408325, 'test/bleu': 30.263596635691624, 'test/num_examples': 3003, 'score': 35321.14249563217, 'total_duration': 60169.06457781792, 'accumulated_submission_time': 35321.14249563217, 'accumulated_eval_time': 24843.572281837463, 'accumulated_logging_time': 1.3168067932128906}
I0312 20:53:20.539529 139664575932160 logging_writer.py:48] [99210] accumulated_eval_time=24843.572282, accumulated_logging_time=1.316807, accumulated_submission_time=35321.142496, global_step=99210, preemption_count=0, score=35321.142496, test/accuracy=0.704329, test/bleu=30.263597, test/loss=1.300075, test/num_examples=3003, total_duration=60169.064578, train/accuracy=0.678374, train/bleu=34.207535, train/loss=1.478173, validation/accuracy=0.688299, validation/bleu=30.174071, validation/loss=1.402743, validation/num_examples=3000
I0312 20:53:52.861129 139664567539456 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21492622792720795, loss=1.5431146621704102
I0312 20:54:28.404396 139664575932160 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.21975864470005035, loss=1.5743510723114014
I0312 20:55:03.980733 139664567539456 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.22099758684635162, loss=1.5244957208633423
I0312 20:55:39.556823 139664575932160 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.2219870239496231, loss=1.5530304908752441
I0312 20:56:15.152579 139664567539456 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.24478358030319214, loss=1.5867130756378174
I0312 20:56:50.769704 139664575932160 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.22573262453079224, loss=1.538028359413147
I0312 20:57:26.376868 139664567539456 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.21203315258026123, loss=1.4964900016784668
I0312 20:58:01.941195 139664575932160 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22153586149215698, loss=1.5682425498962402
I0312 20:58:37.530756 139664567539456 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.22315093874931335, loss=1.554355502128601
I0312 20:59:13.160694 139664575932160 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.2128252387046814, loss=1.50803542137146
I0312 20:59:48.752072 139664567539456 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.20558299124240875, loss=1.5656635761260986
I0312 21:00:24.325943 139664575932160 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.22326204180717468, loss=1.496614933013916
I0312 21:00:59.882604 139664567539456 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.21570423245429993, loss=1.5581499338150024
I0312 21:01:35.453124 139664575932160 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.2134954333305359, loss=1.5286420583724976
I0312 21:02:11.047545 139664567539456 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.2194814831018448, loss=1.5456253290176392
I0312 21:02:46.632039 139664575932160 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.23157477378845215, loss=1.5963752269744873
I0312 21:03:22.221982 139664567539456 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.21971052885055542, loss=1.6233259439468384
I0312 21:03:57.792038 139664575932160 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.2265440821647644, loss=1.4992685317993164
I0312 21:04:33.370069 139664567539456 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.21816593408584595, loss=1.5263019800186157
I0312 21:05:08.950376 139664575932160 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.21251322329044342, loss=1.490516185760498
I0312 21:05:44.548889 139664567539456 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.22118622064590454, loss=1.4962422847747803
I0312 21:06:20.117528 139664575932160 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.22083109617233276, loss=1.5532156229019165
I0312 21:06:55.719647 139664567539456 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3436959981918335, loss=1.553130865097046
I0312 21:07:20.697873 139834281293632 spec.py:321] Evaluating on the training split.
I0312 21:07:23.658435 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 21:11:11.334136 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 21:11:14.000382 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 21:15:03.229834 139834281293632 spec.py:349] Evaluating on the test split.
I0312 21:15:05.896087 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 21:19:11.682233 139834281293632 submission_runner.py:420] Time since start: 61720.23s, 	Step: 101572, 	{'train/accuracy': 0.6858225464820862, 'train/loss': 1.430709719657898, 'train/bleu': 34.802840963750796, 'validation/accuracy': 0.689452052116394, 'validation/loss': 1.4000771045684814, 'validation/bleu': 29.648971887837902, 'validation/num_examples': 3000, 'test/accuracy': 0.7056533694267273, 'test/loss': 1.2988643646240234, 'test/bleu': 30.105600273315, 'test/num_examples': 3003, 'score': 36161.22002506256, 'total_duration': 61720.23471450806, 'accumulated_submission_time': 36161.22002506256, 'accumulated_eval_time': 25554.55659222603, 'accumulated_logging_time': 1.3547017574310303}
I0312 21:19:11.709126 139664575932160 logging_writer.py:48] [101572] accumulated_eval_time=25554.556592, accumulated_logging_time=1.354702, accumulated_submission_time=36161.220025, global_step=101572, preemption_count=0, score=36161.220025, test/accuracy=0.705653, test/bleu=30.105600, test/loss=1.298864, test/num_examples=3003, total_duration=61720.234715, train/accuracy=0.685823, train/bleu=34.802841, train/loss=1.430710, validation/accuracy=0.689452, validation/bleu=29.648972, validation/loss=1.400077, validation/num_examples=3000
I0312 21:19:22.011231 139664567539456 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22315353155136108, loss=1.557829737663269
I0312 21:19:57.478313 139664575932160 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.22690890729427338, loss=1.5513359308242798
I0312 21:20:33.047826 139664567539456 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.231386199593544, loss=1.5166728496551514
I0312 21:21:08.609386 139664575932160 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.2249303162097931, loss=1.6190742254257202
I0312 21:21:44.169491 139664567539456 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.21291324496269226, loss=1.5254926681518555
I0312 21:22:19.746981 139664575932160 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.22476699948310852, loss=1.543225646018982
I0312 21:22:55.309000 139664567539456 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2377944439649582, loss=1.5674651861190796
I0312 21:23:30.873154 139664575932160 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.236427441239357, loss=1.5342239141464233
I0312 21:24:06.434902 139664567539456 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21811184287071228, loss=1.5266042947769165
I0312 21:24:42.007715 139664575932160 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.2132292240858078, loss=1.5626660585403442
I0312 21:25:17.590980 139664567539456 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.21598897874355316, loss=1.4619888067245483
I0312 21:25:53.190654 139664575932160 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23916269838809967, loss=1.5808340311050415
I0312 21:26:28.751019 139664567539456 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.22295571863651276, loss=1.4984437227249146
I0312 21:27:04.313905 139664575932160 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.21098947525024414, loss=1.4889276027679443
I0312 21:27:39.870209 139664567539456 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.21198312938213348, loss=1.4944794178009033
I0312 21:28:15.446017 139664575932160 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.22275836765766144, loss=1.5234124660491943
I0312 21:28:51.011876 139664567539456 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.22238542139530182, loss=1.5994738340377808
I0312 21:29:26.577197 139664575932160 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.21007278561592102, loss=1.5558810234069824
I0312 21:30:02.149788 139664567539456 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.2313283085823059, loss=1.5387877225875854
I0312 21:30:37.765651 139664575932160 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.21649152040481567, loss=1.4898806810379028
I0312 21:31:13.348264 139664567539456 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21832185983657837, loss=1.5947251319885254
I0312 21:31:48.918316 139664575932160 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.2540839612483978, loss=1.4567499160766602
I0312 21:32:24.491468 139664567539456 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22620819509029388, loss=1.5584079027175903
I0312 21:33:00.060715 139664575932160 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.22416232526302338, loss=1.5149513483047485
I0312 21:33:11.888942 139834281293632 spec.py:321] Evaluating on the training split.
I0312 21:33:14.873833 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 21:37:02.850573 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 21:37:05.546444 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 21:40:23.303164 139834281293632 spec.py:349] Evaluating on the test split.
I0312 21:40:25.985235 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 21:44:03.433750 139834281293632 submission_runner.py:420] Time since start: 63211.99s, 	Step: 103935, 	{'train/accuracy': 0.6802985072135925, 'train/loss': 1.4587417840957642, 'train/bleu': 34.38016920874783, 'validation/accuracy': 0.6917335391044617, 'validation/loss': 1.3879741430282593, 'validation/bleu': 30.13557535263701, 'validation/num_examples': 3000, 'test/accuracy': 0.7094184160232544, 'test/loss': 1.2878787517547607, 'test/bleu': 30.68149962599856, 'test/num_examples': 3003, 'score': 37001.32061576843, 'total_duration': 63211.986221790314, 'accumulated_submission_time': 37001.32061576843, 'accumulated_eval_time': 26206.101365804672, 'accumulated_logging_time': 1.3905627727508545}
I0312 21:44:03.461968 139664567539456 logging_writer.py:48] [103935] accumulated_eval_time=26206.101366, accumulated_logging_time=1.390563, accumulated_submission_time=37001.320616, global_step=103935, preemption_count=0, score=37001.320616, test/accuracy=0.709418, test/bleu=30.681500, test/loss=1.287879, test/num_examples=3003, total_duration=63211.986222, train/accuracy=0.680299, train/bleu=34.380169, train/loss=1.458742, validation/accuracy=0.691734, validation/bleu=30.135575, validation/loss=1.387974, validation/num_examples=3000
I0312 21:44:26.872206 139664575932160 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.23492850363254547, loss=1.5758711099624634
I0312 21:45:02.391101 139664567539456 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.22180232405662537, loss=1.5201809406280518
I0312 21:45:37.980703 139664575932160 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.22762873768806458, loss=1.5799661874771118
I0312 21:46:13.562388 139664567539456 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.21559180319309235, loss=1.468833327293396
I0312 21:46:49.129339 139664575932160 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.2345200777053833, loss=1.6289253234863281
I0312 21:47:24.737662 139664567539456 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.22571952641010284, loss=1.5186285972595215
I0312 21:48:00.337053 139664575932160 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22807706892490387, loss=1.5283665657043457
I0312 21:48:35.936988 139664567539456 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.23121149837970734, loss=1.4864364862442017
I0312 21:49:11.526994 139664575932160 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.24471722543239594, loss=1.5041491985321045
I0312 21:49:47.124876 139664567539456 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.2177453339099884, loss=1.5610244274139404
I0312 21:50:22.725908 139664575932160 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.30585771799087524, loss=1.5281541347503662
I0312 21:50:58.333846 139664567539456 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.24398966133594513, loss=1.540792465209961
I0312 21:51:33.924316 139664575932160 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.2238583266735077, loss=1.5920554399490356
I0312 21:52:09.493042 139664567539456 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.22246968746185303, loss=1.5521665811538696
I0312 21:52:45.066056 139664575932160 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.2235203981399536, loss=1.485242486000061
I0312 21:53:20.646245 139664567539456 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.22446298599243164, loss=1.4702483415603638
I0312 21:53:56.204546 139664575932160 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.24166829884052277, loss=1.5133250951766968
I0312 21:54:31.815936 139664567539456 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.21854470670223236, loss=1.4911071062088013
I0312 21:55:07.418884 139664575932160 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.24637874960899353, loss=1.5810925960540771
I0312 21:55:43.003018 139664567539456 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.23325122892856598, loss=1.5348503589630127
I0312 21:56:18.599207 139664575932160 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.21648536622524261, loss=1.5914459228515625
I0312 21:56:54.184411 139664567539456 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.23344719409942627, loss=1.4901511669158936
I0312 21:57:29.733739 139664575932160 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3608405292034149, loss=1.4858465194702148
I0312 21:58:03.604681 139834281293632 spec.py:321] Evaluating on the training split.
I0312 21:58:06.570525 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:02:00.236057 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 22:02:02.909670 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:04:58.671019 139834281293632 spec.py:349] Evaluating on the test split.
I0312 22:05:01.354740 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:08:14.625480 139834281293632 submission_runner.py:420] Time since start: 64663.18s, 	Step: 106297, 	{'train/accuracy': 0.6838071942329407, 'train/loss': 1.4393730163574219, 'train/bleu': 34.89865767951385, 'validation/accuracy': 0.6919938921928406, 'validation/loss': 1.3836085796356201, 'validation/bleu': 30.601454222110416, 'validation/num_examples': 3000, 'test/accuracy': 0.7088606357574463, 'test/loss': 1.2818655967712402, 'test/bleu': 30.694700225262544, 'test/num_examples': 3003, 'score': 37841.38295006752, 'total_duration': 64663.177958250046, 'accumulated_submission_time': 37841.38295006752, 'accumulated_eval_time': 26817.122111082077, 'accumulated_logging_time': 1.4279468059539795}
I0312 22:08:14.652931 139664567539456 logging_writer.py:48] [106297] accumulated_eval_time=26817.122111, accumulated_logging_time=1.427947, accumulated_submission_time=37841.382950, global_step=106297, preemption_count=0, score=37841.382950, test/accuracy=0.708861, test/bleu=30.694700, test/loss=1.281866, test/num_examples=3003, total_duration=64663.177958, train/accuracy=0.683807, train/bleu=34.898658, train/loss=1.439373, validation/accuracy=0.691994, validation/bleu=30.601454, validation/loss=1.383609, validation/num_examples=3000
I0312 22:08:16.097888 139664575932160 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.22113537788391113, loss=1.5941637754440308
I0312 22:08:51.521474 139664567539456 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.21906927227973938, loss=1.5601145029067993
I0312 22:09:27.024697 139664575932160 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.22157086431980133, loss=1.504418969154358
I0312 22:10:02.583543 139664567539456 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.21980063617229462, loss=1.4667079448699951
I0312 22:10:38.130254 139664575932160 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.24577057361602783, loss=1.529549479484558
I0312 22:11:13.696716 139664567539456 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22597624361515045, loss=1.4904872179031372
I0312 22:11:49.262022 139664575932160 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.22674155235290527, loss=1.4885722398757935
I0312 22:12:24.829756 139664567539456 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.21022747457027435, loss=1.4440370798110962
I0312 22:13:00.428880 139664575932160 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.23854827880859375, loss=1.5409574508666992
I0312 22:13:36.013661 139664567539456 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.22625303268432617, loss=1.5564396381378174
I0312 22:14:11.595027 139664575932160 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.21584881842136383, loss=1.4586172103881836
I0312 22:14:47.212036 139664567539456 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.24590295553207397, loss=1.5293049812316895
I0312 22:15:22.829714 139664575932160 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.23575446009635925, loss=1.5024535655975342
I0312 22:15:58.388818 139664567539456 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.22873283922672272, loss=1.5460811853408813
I0312 22:16:33.946926 139664575932160 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.2253454178571701, loss=1.445639729499817
I0312 22:17:09.506702 139664567539456 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.2360873520374298, loss=1.4900155067443848
I0312 22:17:45.075607 139664575932160 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.23098911345005035, loss=1.538417100906372
I0312 22:18:20.650492 139664567539456 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.22537311911582947, loss=1.500928521156311
I0312 22:18:56.240074 139664575932160 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.24229706823825836, loss=1.5982338190078735
I0312 22:19:31.790408 139664567539456 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.2347029149532318, loss=1.4780809879302979
I0312 22:20:07.421630 139664575932160 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.22493058443069458, loss=1.5152207612991333
I0312 22:20:43.027153 139664567539456 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.2345554530620575, loss=1.4766470193862915
I0312 22:21:18.614647 139664575932160 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.2303919792175293, loss=1.5070630311965942
I0312 22:21:54.220314 139664567539456 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.22715523838996887, loss=1.4621925354003906
I0312 22:22:14.951578 139834281293632 spec.py:321] Evaluating on the training split.
I0312 22:22:17.934799 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:26:06.514775 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 22:26:09.197152 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:29:54.725806 139834281293632 spec.py:349] Evaluating on the test split.
I0312 22:29:57.413385 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:33:51.865737 139834281293632 submission_runner.py:420] Time since start: 66200.42s, 	Step: 108660, 	{'train/accuracy': 0.6914603114128113, 'train/loss': 1.3917185068130493, 'train/bleu': 34.794085077857396, 'validation/accuracy': 0.6927502155303955, 'validation/loss': 1.3769105672836304, 'validation/bleu': 30.702235298531477, 'validation/num_examples': 3000, 'test/accuracy': 0.7099994421005249, 'test/loss': 1.2747658491134644, 'test/bleu': 30.653324968205062, 'test/num_examples': 3003, 'score': 38681.60069704056, 'total_duration': 66200.41820144653, 'accumulated_submission_time': 38681.60069704056, 'accumulated_eval_time': 27514.036210536957, 'accumulated_logging_time': 1.465364694595337}
I0312 22:33:51.893965 139664575932160 logging_writer.py:48] [108660] accumulated_eval_time=27514.036211, accumulated_logging_time=1.465365, accumulated_submission_time=38681.600697, global_step=108660, preemption_count=0, score=38681.600697, test/accuracy=0.709999, test/bleu=30.653325, test/loss=1.274766, test/num_examples=3003, total_duration=66200.418201, train/accuracy=0.691460, train/bleu=34.794085, train/loss=1.391719, validation/accuracy=0.692750, validation/bleu=30.702235, validation/loss=1.376911, validation/num_examples=3000
I0312 22:34:06.458345 139664567539456 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.23282216489315033, loss=1.4837058782577515
I0312 22:34:41.990421 139664575932160 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22180259227752686, loss=1.4867476224899292
I0312 22:35:17.516282 139664567539456 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.22634077072143555, loss=1.5012216567993164
I0312 22:35:53.112950 139664575932160 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.2509844899177551, loss=1.5220820903778076
I0312 22:36:28.699430 139664567539456 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.22820554673671722, loss=1.5156182050704956
I0312 22:37:04.276500 139664575932160 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.24269460141658783, loss=1.4990407228469849
I0312 22:37:39.855911 139664567539456 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.24886468052864075, loss=1.4802076816558838
I0312 22:38:15.456923 139664575932160 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.23415158689022064, loss=1.5302598476409912
I0312 22:38:51.058110 139664567539456 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22355234622955322, loss=1.4535207748413086
I0312 22:39:26.662674 139664575932160 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.22858472168445587, loss=1.5122830867767334
I0312 22:40:02.230297 139664567539456 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.240853413939476, loss=1.4676296710968018
I0312 22:40:37.802969 139664575932160 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.22623474895954132, loss=1.4785231351852417
I0312 22:41:13.345494 139664567539456 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.23859097063541412, loss=1.5386803150177002
I0312 22:41:48.889109 139664575932160 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.2379666268825531, loss=1.4932351112365723
I0312 22:42:24.522425 139664567539456 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.2328202724456787, loss=1.4557297229766846
I0312 22:43:00.175929 139664575932160 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.2312667816877365, loss=1.4802297353744507
I0312 22:43:35.775288 139664567539456 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.23873916268348694, loss=1.483182668685913
I0312 22:44:11.356967 139664575932160 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.23753733932971954, loss=1.521235466003418
I0312 22:44:46.927989 139664567539456 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.26623377203941345, loss=1.5809377431869507
I0312 22:45:22.506489 139664575932160 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.22509637475013733, loss=1.5149540901184082
I0312 22:45:58.075221 139664567539456 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.23359982669353485, loss=1.5207509994506836
I0312 22:46:33.655461 139664575932160 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.23914040625095367, loss=1.515032172203064
I0312 22:47:09.227453 139664567539456 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.23012809455394745, loss=1.5007386207580566
I0312 22:47:44.812511 139664575932160 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.23040679097175598, loss=1.4974522590637207
I0312 22:47:51.999191 139834281293632 spec.py:321] Evaluating on the training split.
I0312 22:47:54.968997 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:51:27.743089 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 22:51:30.419577 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:54:30.401324 139834281293632 spec.py:349] Evaluating on the test split.
I0312 22:54:33.072499 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 22:57:28.715306 139834281293632 submission_runner.py:420] Time since start: 67617.27s, 	Step: 111022, 	{'train/accuracy': 0.6853946447372437, 'train/loss': 1.4290424585342407, 'train/bleu': 34.54282435835048, 'validation/accuracy': 0.6935685873031616, 'validation/loss': 1.3749322891235352, 'validation/bleu': 30.51653350310114, 'validation/num_examples': 3000, 'test/accuracy': 0.7101040482521057, 'test/loss': 1.273871898651123, 'test/bleu': 30.655470179904714, 'test/num_examples': 3003, 'score': 39521.62386965752, 'total_duration': 67617.26778793335, 'accumulated_submission_time': 39521.62386965752, 'accumulated_eval_time': 28090.752275705338, 'accumulated_logging_time': 1.504340410232544}
I0312 22:57:28.744395 139664567539456 logging_writer.py:48] [111022] accumulated_eval_time=28090.752276, accumulated_logging_time=1.504340, accumulated_submission_time=39521.623870, global_step=111022, preemption_count=0, score=39521.623870, test/accuracy=0.710104, test/bleu=30.655470, test/loss=1.273872, test/num_examples=3003, total_duration=67617.267788, train/accuracy=0.685395, train/bleu=34.542824, train/loss=1.429042, validation/accuracy=0.693569, validation/bleu=30.516534, validation/loss=1.374932, validation/num_examples=3000
I0312 22:57:56.778887 139664575932160 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.2234286069869995, loss=1.4591667652130127
I0312 22:58:32.321006 139664567539456 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.2340245544910431, loss=1.487058162689209
I0312 22:59:07.940638 139664575932160 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.2284303456544876, loss=1.5298566818237305
I0312 22:59:43.562046 139664567539456 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.2384389042854309, loss=1.5515695810317993
I0312 23:00:19.161973 139664575932160 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.22495394945144653, loss=1.4738221168518066
I0312 23:00:54.728002 139664567539456 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.2228856086730957, loss=1.4941563606262207
I0312 23:01:30.294732 139664575932160 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.24906688928604126, loss=1.4790725708007812
I0312 23:02:05.839388 139664567539456 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.23419640958309174, loss=1.516953706741333
I0312 23:02:41.390217 139664575932160 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.2542366683483124, loss=1.5699031352996826
I0312 23:03:16.965867 139664567539456 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.2209932804107666, loss=1.4687703847885132
I0312 23:03:52.555695 139664575932160 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.2237938940525055, loss=1.366250991821289
I0312 23:04:28.151216 139664567539456 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.23575161397457123, loss=1.4789903163909912
I0312 23:05:03.738888 139664575932160 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.2508663833141327, loss=1.4320435523986816
I0312 23:05:39.313232 139664567539456 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.22438262403011322, loss=1.491621732711792
I0312 23:06:14.879307 139664575932160 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.24650853872299194, loss=1.4991446733474731
I0312 23:06:50.470318 139664567539456 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.22829696536064148, loss=1.4794615507125854
I0312 23:07:26.052264 139664575932160 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.2243831753730774, loss=1.4373193979263306
I0312 23:08:01.652508 139664567539456 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.22561143338680267, loss=1.4889130592346191
I0312 23:08:37.237329 139664575932160 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.23460860550403595, loss=1.5189484357833862
I0312 23:09:12.804240 139664567539456 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.2285425364971161, loss=1.4825350046157837
I0312 23:09:48.406284 139664575932160 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.2312176525592804, loss=1.4638692140579224
I0312 23:10:23.998003 139664567539456 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.23222053050994873, loss=1.474442958831787
I0312 23:10:59.561519 139664575932160 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.23024491965770721, loss=1.4940721988677979
I0312 23:11:28.796483 139834281293632 spec.py:321] Evaluating on the training split.
I0312 23:11:31.768970 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 23:15:24.997354 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 23:15:27.676521 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 23:18:54.080178 139834281293632 spec.py:349] Evaluating on the test split.
I0312 23:18:56.764026 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 23:22:52.476406 139834281293632 submission_runner.py:420] Time since start: 69141.03s, 	Step: 113384, 	{'train/accuracy': 0.6964467763900757, 'train/loss': 1.3651326894760132, 'train/bleu': 35.61580584857655, 'validation/accuracy': 0.6939033269882202, 'validation/loss': 1.3725615739822388, 'validation/bleu': 30.671236136957305, 'validation/num_examples': 3000, 'test/accuracy': 0.7107896208763123, 'test/loss': 1.2676246166229248, 'test/bleu': 30.63459049836028, 'test/num_examples': 3003, 'score': 40361.59369277954, 'total_duration': 69141.02885961533, 'accumulated_submission_time': 40361.59369277954, 'accumulated_eval_time': 28774.43212223053, 'accumulated_logging_time': 1.543736219406128}
I0312 23:22:52.505307 139664567539456 logging_writer.py:48] [113384] accumulated_eval_time=28774.432122, accumulated_logging_time=1.543736, accumulated_submission_time=40361.593693, global_step=113384, preemption_count=0, score=40361.593693, test/accuracy=0.710790, test/bleu=30.634590, test/loss=1.267625, test/num_examples=3003, total_duration=69141.028860, train/accuracy=0.696447, train/bleu=35.615806, train/loss=1.365133, validation/accuracy=0.693903, validation/bleu=30.671236, validation/loss=1.372562, validation/num_examples=3000
I0312 23:22:58.559662 139664575932160 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.2268114537000656, loss=1.489140510559082
I0312 23:23:34.061335 139664567539456 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.23002491891384125, loss=1.4186185598373413
I0312 23:24:09.589838 139664575932160 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.23200325667858124, loss=1.508996605873108
I0312 23:24:45.161004 139664567539456 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.23021167516708374, loss=1.5052210092544556
I0312 23:25:20.758279 139664575932160 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.2351265549659729, loss=1.438236951828003
I0312 23:25:56.361639 139664567539456 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.24218520522117615, loss=1.4977378845214844
I0312 23:26:31.959261 139664575932160 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.23119691014289856, loss=1.5240321159362793
I0312 23:27:07.532511 139664567539456 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3290679454803467, loss=1.4728033542633057
I0312 23:27:43.113036 139664575932160 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.22779417037963867, loss=1.4769307374954224
I0312 23:28:18.720930 139664567539456 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.23207098245620728, loss=1.485595941543579
I0312 23:28:54.274885 139664575932160 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.24313171207904816, loss=1.5175578594207764
I0312 23:29:29.891093 139664567539456 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.23547177016735077, loss=1.5270962715148926
I0312 23:30:05.470032 139664575932160 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.232364684343338, loss=1.5099493265151978
I0312 23:30:41.040933 139664567539456 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.23044852912425995, loss=1.5099122524261475
I0312 23:31:16.609298 139664575932160 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.23150676488876343, loss=1.4816334247589111
I0312 23:31:52.201233 139664567539456 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.23356013000011444, loss=1.4864414930343628
I0312 23:32:27.772775 139664575932160 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.2280527949333191, loss=1.4453877210617065
I0312 23:33:03.334958 139664567539456 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.2322806715965271, loss=1.499937653541565
I0312 23:33:38.908591 139664575932160 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.2347661852836609, loss=1.453845500946045
I0312 23:34:14.476101 139664567539456 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.23969857394695282, loss=1.468923568725586
I0312 23:34:50.056184 139664575932160 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.23300425708293915, loss=1.4702272415161133
I0312 23:35:25.611867 139664567539456 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.22205646336078644, loss=1.4820555448532104
I0312 23:36:01.178538 139664575932160 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.2500305771827698, loss=1.4972864389419556
I0312 23:36:36.741940 139664567539456 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.22739604115486145, loss=1.513028621673584
I0312 23:36:52.817566 139834281293632 spec.py:321] Evaluating on the training split.
I0312 23:36:55.789475 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 23:40:27.218216 139834281293632 spec.py:333] Evaluating on the validation split.
I0312 23:40:29.889277 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 23:43:32.049732 139834281293632 spec.py:349] Evaluating on the test split.
I0312 23:43:34.708389 139834281293632 workload.py:181] Translating evaluation dataset.
I0312 23:46:30.187805 139834281293632 submission_runner.py:420] Time since start: 70558.74s, 	Step: 115747, 	{'train/accuracy': 0.6904262900352478, 'train/loss': 1.3907667398452759, 'train/bleu': 35.42563523204688, 'validation/accuracy': 0.693990170955658, 'validation/loss': 1.3671605587005615, 'validation/bleu': 30.734451759132792, 'validation/num_examples': 3000, 'test/accuracy': 0.7119516730308533, 'test/loss': 1.2626426219940186, 'test/bleu': 30.893957355730375, 'test/num_examples': 3003, 'score': 41201.82719826698, 'total_duration': 70558.74024915695, 'accumulated_submission_time': 41201.82719826698, 'accumulated_eval_time': 29351.802276611328, 'accumulated_logging_time': 1.5817480087280273}
I0312 23:46:30.221032 139664575932160 logging_writer.py:48] [115747] accumulated_eval_time=29351.802277, accumulated_logging_time=1.581748, accumulated_submission_time=41201.827198, global_step=115747, preemption_count=0, score=41201.827198, test/accuracy=0.711952, test/bleu=30.893957, test/loss=1.262643, test/num_examples=3003, total_duration=70558.740249, train/accuracy=0.690426, train/bleu=35.425635, train/loss=1.390767, validation/accuracy=0.693990, validation/bleu=30.734452, validation/loss=1.367161, validation/num_examples=3000
I0312 23:46:49.380390 139664567539456 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.22606505453586578, loss=1.4464665651321411
I0312 23:47:24.913927 139664575932160 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.24239014089107513, loss=1.5002048015594482
I0312 23:48:00.507416 139664567539456 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.24353399872779846, loss=1.5244868993759155
I0312 23:48:36.105243 139664575932160 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.2335076779127121, loss=1.4254448413848877
I0312 23:49:11.697906 139664567539456 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.2301328182220459, loss=1.4332926273345947
I0312 23:49:47.257742 139664575932160 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.2275731861591339, loss=1.4848428964614868
I0312 23:50:22.835360 139664567539456 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.23348186910152435, loss=1.4358484745025635
I0312 23:50:58.427750 139664575932160 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.22734792530536652, loss=1.4868916273117065
I0312 23:51:34.005842 139664567539456 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.23740525543689728, loss=1.4299123287200928
I0312 23:52:09.557456 139664575932160 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.23313270509243011, loss=1.4429779052734375
I0312 23:52:45.132595 139664567539456 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.2367255538702011, loss=1.522379994392395
I0312 23:53:20.706512 139664575932160 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.24350573122501373, loss=1.457611322402954
I0312 23:53:56.284585 139664567539456 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.23777909576892853, loss=1.5238693952560425
I0312 23:54:31.878171 139664575932160 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.254047155380249, loss=1.484897255897522
I0312 23:55:07.455941 139664567539456 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.2373793125152588, loss=1.4658349752426147
I0312 23:55:43.021419 139664575932160 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.2257944643497467, loss=1.4346449375152588
I0312 23:56:18.565979 139664567539456 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.23559749126434326, loss=1.4728033542633057
I0312 23:56:54.157880 139664575932160 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.2386886179447174, loss=1.5317399501800537
I0312 23:57:29.754865 139664567539456 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.22060933709144592, loss=1.4554897546768188
I0312 23:58:05.341928 139664575932160 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.23358531296253204, loss=1.497665524482727
I0312 23:58:40.923201 139664567539456 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.2564356327056885, loss=1.584214687347412
I0312 23:59:16.520297 139664575932160 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.22655846178531647, loss=1.4406116008758545
I0312 23:59:52.114745 139664567539456 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.25008708238601685, loss=1.507480502128601
I0313 00:00:27.712831 139664575932160 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.22209805250167847, loss=1.4129451513290405
I0313 00:00:30.278644 139834281293632 spec.py:321] Evaluating on the training split.
I0313 00:00:33.245452 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:04:14.071283 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 00:04:16.744009 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:07:10.687745 139834281293632 spec.py:349] Evaluating on the test split.
I0313 00:07:13.363695 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:09:58.702072 139834281293632 submission_runner.py:420] Time since start: 71967.25s, 	Step: 118109, 	{'train/accuracy': 0.6913244128227234, 'train/loss': 1.3941733837127686, 'train/bleu': 34.82535315227721, 'validation/accuracy': 0.6944116950035095, 'validation/loss': 1.3669185638427734, 'validation/bleu': 30.558124798140806, 'validation/num_examples': 3000, 'test/accuracy': 0.7115681767463684, 'test/loss': 1.2619701623916626, 'test/bleu': 30.788337111092257, 'test/num_examples': 3003, 'score': 42041.80287861824, 'total_duration': 71967.25450706482, 'accumulated_submission_time': 42041.80287861824, 'accumulated_eval_time': 29920.225604772568, 'accumulated_logging_time': 1.624889850616455}
I0313 00:09:58.737949 139664567539456 logging_writer.py:48] [118109] accumulated_eval_time=29920.225605, accumulated_logging_time=1.624890, accumulated_submission_time=42041.802879, global_step=118109, preemption_count=0, score=42041.802879, test/accuracy=0.711568, test/bleu=30.788337, test/loss=1.261970, test/num_examples=3003, total_duration=71967.254507, train/accuracy=0.691324, train/bleu=34.825353, train/loss=1.394173, validation/accuracy=0.694412, validation/bleu=30.558125, validation/loss=1.366919, validation/num_examples=3000
I0313 00:10:31.432497 139664575932160 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.2313683182001114, loss=1.4678902626037598
I0313 00:11:07.004466 139664567539456 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.2282584309577942, loss=1.406282663345337
I0313 00:11:42.566086 139664575932160 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.24111494421958923, loss=1.5461024045944214
I0313 00:12:18.133619 139664567539456 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.23307034373283386, loss=1.4911028146743774
I0313 00:12:53.715706 139664575932160 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.24104847013950348, loss=1.503418207168579
I0313 00:13:29.293004 139664567539456 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.22718746960163116, loss=1.548508644104004
I0313 00:14:04.872932 139664575932160 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.2467031180858612, loss=1.449739933013916
I0313 00:14:40.471524 139664567539456 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.22876326739788055, loss=1.40141761302948
I0313 00:15:16.055701 139664575932160 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.2328721582889557, loss=1.5040459632873535
I0313 00:15:51.687123 139664567539456 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.2282412201166153, loss=1.4212862253189087
I0313 00:16:27.280552 139664575932160 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.2342906892299652, loss=1.502450704574585
I0313 00:17:02.863919 139664567539456 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.24445022642612457, loss=1.4779431819915771
I0313 00:17:38.409639 139664575932160 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.22460539638996124, loss=1.437374472618103
I0313 00:18:13.998170 139664567539456 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23202520608901978, loss=1.5070644617080688
I0313 00:18:49.565991 139664575932160 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.2386641651391983, loss=1.4806913137435913
I0313 00:19:25.157249 139664567539456 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.22913497686386108, loss=1.4760863780975342
I0313 00:20:00.706454 139664575932160 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.2367301881313324, loss=1.490067958831787
I0313 00:20:36.280859 139664567539456 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.22795382142066956, loss=1.4290881156921387
I0313 00:21:11.872670 139664575932160 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.23017501831054688, loss=1.4274171590805054
I0313 00:21:47.415476 139664567539456 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.23131299018859863, loss=1.4985755681991577
I0313 00:22:22.986653 139664575932160 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.23586119711399078, loss=1.467341661453247
I0313 00:22:58.560134 139664567539456 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.23704040050506592, loss=1.4468852281570435
I0313 00:23:34.151632 139664575932160 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.23750019073486328, loss=1.4692786931991577
I0313 00:23:58.736260 139834281293632 spec.py:321] Evaluating on the training split.
I0313 00:24:01.702627 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:27:23.635609 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 00:27:26.316419 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:30:38.986962 139834281293632 spec.py:349] Evaluating on the test split.
I0313 00:30:41.668298 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:33:58.013240 139834281293632 submission_runner.py:420] Time since start: 73406.57s, 	Step: 120471, 	{'train/accuracy': 0.6960253119468689, 'train/loss': 1.363867998123169, 'train/bleu': 35.065851404224986, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.363057017326355, 'validation/bleu': 30.73799429326848, 'validation/num_examples': 3000, 'test/accuracy': 0.7127418518066406, 'test/loss': 1.2584396600723267, 'test/bleu': 31.113236211947505, 'test/num_examples': 3003, 'score': 42881.7198240757, 'total_duration': 73406.56572580338, 'accumulated_submission_time': 42881.7198240757, 'accumulated_eval_time': 30519.502540826797, 'accumulated_logging_time': 1.671435832977295}
I0313 00:33:58.043339 139664567539456 logging_writer.py:48] [120471] accumulated_eval_time=30519.502541, accumulated_logging_time=1.671436, accumulated_submission_time=42881.719824, global_step=120471, preemption_count=0, score=42881.719824, test/accuracy=0.712742, test/bleu=31.113236, test/loss=1.258440, test/num_examples=3003, total_duration=73406.565726, train/accuracy=0.696025, train/bleu=35.065851, train/loss=1.363868, validation/accuracy=0.695540, validation/bleu=30.737994, validation/loss=1.363057, validation/num_examples=3000
I0313 00:34:08.709105 139664575932160 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.23746153712272644, loss=1.460038423538208
I0313 00:34:44.275066 139664567539456 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.24565522372722626, loss=1.470916509628296
I0313 00:35:19.844280 139664575932160 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.23494286835193634, loss=1.4091027975082397
I0313 00:35:55.374468 139664567539456 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.23732423782348633, loss=1.4176223278045654
I0313 00:36:31.031373 139664575932160 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.23657368123531342, loss=1.4362026453018188
I0313 00:37:06.656974 139664567539456 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.24717594683170319, loss=1.4526264667510986
I0313 00:37:42.310637 139664575932160 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.24549204111099243, loss=1.5353941917419434
I0313 00:38:17.915207 139664567539456 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.2339581400156021, loss=1.4233492612838745
I0313 00:38:53.506702 139664575932160 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.23404644429683685, loss=1.549527645111084
I0313 00:39:29.093329 139664567539456 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.23619043827056885, loss=1.3706110715866089
I0313 00:40:04.689509 139664575932160 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.24084270000457764, loss=1.487350583076477
I0313 00:40:40.275343 139664567539456 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.24730779230594635, loss=1.4949171543121338
I0313 00:41:15.855491 139664575932160 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.2431921511888504, loss=1.4857990741729736
I0313 00:41:51.483958 139664567539456 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23659995198249817, loss=1.4696033000946045
I0313 00:42:27.101837 139664575932160 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.2310400754213333, loss=1.5004082918167114
I0313 00:43:02.733988 139664567539456 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.24038425087928772, loss=1.4898016452789307
I0313 00:43:38.424989 139664575932160 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.236891508102417, loss=1.4681150913238525
I0313 00:44:14.045061 139664567539456 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.2370813488960266, loss=1.4229366779327393
I0313 00:44:49.687813 139664575932160 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.24617542326450348, loss=1.4783570766448975
I0313 00:45:25.311100 139664567539456 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.23180292546749115, loss=1.4564193487167358
I0313 00:46:00.892177 139664575932160 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.24088120460510254, loss=1.494654893875122
I0313 00:46:36.517600 139664567539456 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.24141009151935577, loss=1.5072118043899536
I0313 00:47:12.101178 139664575932160 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.24649591743946075, loss=1.4883599281311035
I0313 00:47:47.695466 139664567539456 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.23029492795467377, loss=1.4663763046264648
I0313 00:47:58.087961 139834281293632 spec.py:321] Evaluating on the training split.
I0313 00:48:01.069033 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:52:08.078399 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 00:52:10.750869 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:55:24.491779 139834281293632 spec.py:349] Evaluating on the test split.
I0313 00:55:27.165262 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 00:58:57.287414 139834281293632 submission_runner.py:420] Time since start: 74905.84s, 	Step: 122831, 	{'train/accuracy': 0.6932611465454102, 'train/loss': 1.3812857866287231, 'train/bleu': 35.56988353162563, 'validation/accuracy': 0.6957756280899048, 'validation/loss': 1.3611180782318115, 'validation/bleu': 30.736138291930217, 'validation/num_examples': 3000, 'test/accuracy': 0.7138341665267944, 'test/loss': 1.256298542022705, 'test/bleu': 31.07882520707422, 'test/num_examples': 3003, 'score': 43721.67870378494, 'total_duration': 74905.83988952637, 'accumulated_submission_time': 43721.67870378494, 'accumulated_eval_time': 31178.701949357986, 'accumulated_logging_time': 1.7121977806091309}
I0313 00:58:57.318766 139664575932160 logging_writer.py:48] [122831] accumulated_eval_time=31178.701949, accumulated_logging_time=1.712198, accumulated_submission_time=43721.678704, global_step=122831, preemption_count=0, score=43721.678704, test/accuracy=0.713834, test/bleu=31.078825, test/loss=1.256299, test/num_examples=3003, total_duration=74905.839890, train/accuracy=0.693261, train/bleu=35.569884, train/loss=1.381286, validation/accuracy=0.695776, validation/bleu=30.736138, validation/loss=1.361118, validation/num_examples=3000
I0313 00:59:22.168870 139664567539456 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.23119719326496124, loss=1.4150618314743042
I0313 00:59:57.695894 139664575932160 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.2470521628856659, loss=1.5076549053192139
I0313 01:00:33.258439 139664567539456 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.2457243651151657, loss=1.597051978111267
I0313 01:01:08.851402 139664575932160 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.24398469924926758, loss=1.4691907167434692
I0313 01:01:44.454786 139664567539456 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.24724777042865753, loss=1.510911226272583
I0313 01:02:20.064156 139664575932160 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.2316906899213791, loss=1.4359997510910034
I0313 01:02:55.647457 139664567539456 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.23904795944690704, loss=1.45500910282135
I0313 01:03:31.222694 139664575932160 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.22445036470890045, loss=1.403227686882019
I0313 01:04:06.799100 139664567539456 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.2368127405643463, loss=1.583769679069519
I0313 01:04:42.370304 139664575932160 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.22432434558868408, loss=1.40510094165802
I0313 01:05:17.961894 139664567539456 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.23371917009353638, loss=1.4262079000473022
I0313 01:05:53.504293 139664575932160 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.23051689565181732, loss=1.4975776672363281
I0313 01:06:29.100462 139664567539456 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.2439976930618286, loss=1.4703240394592285
I0313 01:07:04.678531 139664575932160 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.23344525694847107, loss=1.4432508945465088
I0313 01:07:40.302732 139664567539456 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.2363453060388565, loss=1.4195455312728882
I0313 01:08:15.870455 139664575932160 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.22426651418209076, loss=1.466130256652832
I0313 01:08:51.434239 139664567539456 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.2411929965019226, loss=1.413710355758667
I0313 01:09:27.047873 139664575932160 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.23210787773132324, loss=1.4943410158157349
I0313 01:10:02.610538 139664567539456 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.2304968386888504, loss=1.5014835596084595
I0313 01:10:38.177309 139664575932160 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.23356060683727264, loss=1.4424314498901367
I0313 01:11:13.794463 139664567539456 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.24137841165065765, loss=1.4394514560699463
I0313 01:11:49.448652 139664575932160 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.24092985689640045, loss=1.5043495893478394
I0313 01:12:25.046341 139664567539456 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.2313092201948166, loss=1.5319948196411133
I0313 01:12:57.496256 139834281293632 spec.py:321] Evaluating on the training split.
I0313 01:13:00.461156 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 01:16:35.128582 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 01:16:37.800784 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 01:19:46.798254 139834281293632 spec.py:349] Evaluating on the test split.
I0313 01:19:49.474956 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 01:22:49.223362 139834281293632 submission_runner.py:420] Time since start: 76337.78s, 	Step: 125193, 	{'train/accuracy': 0.6958445310592651, 'train/loss': 1.3700186014175415, 'train/bleu': 35.29065202228168, 'validation/accuracy': 0.6958004236221313, 'validation/loss': 1.3598471879959106, 'validation/bleu': 30.81504363371959, 'validation/num_examples': 3000, 'test/accuracy': 0.7141479253768921, 'test/loss': 1.2538695335388184, 'test/bleu': 31.1441452580602, 'test/num_examples': 3003, 'score': 44561.77640080452, 'total_duration': 76337.77584266663, 'accumulated_submission_time': 44561.77640080452, 'accumulated_eval_time': 31770.429005146027, 'accumulated_logging_time': 1.752969741821289}
I0313 01:22:49.253838 139664575932160 logging_writer.py:48] [125193] accumulated_eval_time=31770.429005, accumulated_logging_time=1.752970, accumulated_submission_time=44561.776401, global_step=125193, preemption_count=0, score=44561.776401, test/accuracy=0.714148, test/bleu=31.144145, test/loss=1.253870, test/num_examples=3003, total_duration=76337.775843, train/accuracy=0.695845, train/bleu=35.290652, train/loss=1.370019, validation/accuracy=0.695800, validation/bleu=30.815044, validation/loss=1.359847, validation/num_examples=3000
I0313 01:22:52.114485 139664567539456 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.22849047183990479, loss=1.401072382926941
I0313 01:23:27.603237 139664575932160 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.2364315539598465, loss=1.4288638830184937
I0313 01:24:03.151856 139664567539456 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.24031606316566467, loss=1.5155370235443115
I0313 01:24:38.708530 139664575932160 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.2366212159395218, loss=1.468917965888977
I0313 01:25:14.308366 139664567539456 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.24032312631607056, loss=1.4692004919052124
I0313 01:25:49.874269 139664575932160 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.2549726963043213, loss=1.4929360151290894
I0313 01:26:25.443909 139664567539456 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.22647514939308167, loss=1.398642897605896
I0313 01:27:01.001125 139664575932160 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.2395016849040985, loss=1.4679299592971802
I0313 01:27:36.673944 139664567539456 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.22916626930236816, loss=1.4307130575180054
I0313 01:28:12.272661 139664575932160 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.2400684356689453, loss=1.488153338432312
I0313 01:28:47.869474 139664567539456 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.2287173569202423, loss=1.340996265411377
I0313 01:29:23.451110 139664575932160 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.23706099390983582, loss=1.4969213008880615
I0313 01:29:59.010608 139664567539456 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.2391933798789978, loss=1.5173988342285156
I0313 01:30:34.592246 139664575932160 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.23704387247562408, loss=1.522835612297058
I0313 01:31:10.170882 139664567539456 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.23546265065670013, loss=1.4827494621276855
I0313 01:31:45.773177 139664575932160 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.2438105344772339, loss=1.4728416204452515
I0313 01:32:21.418558 139664567539456 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.23957538604736328, loss=1.4838980436325073
I0313 01:32:57.030858 139664575932160 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.24009160697460175, loss=1.540034294128418
I0313 01:33:32.638390 139664567539456 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.2313361018896103, loss=1.4736603498458862
I0313 01:34:08.216001 139664575932160 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.23736171424388885, loss=1.5203790664672852
I0313 01:34:43.837356 139664567539456 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.240543395280838, loss=1.4713753461837769
I0313 01:35:19.439833 139664575932160 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.2350555807352066, loss=1.47117018699646
I0313 01:35:55.026298 139664567539456 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.24463385343551636, loss=1.5150731801986694
I0313 01:36:30.634470 139664575932160 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.22921764850616455, loss=1.3915588855743408
I0313 01:36:49.566665 139834281293632 spec.py:321] Evaluating on the training split.
I0313 01:36:52.536371 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 01:40:07.656942 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 01:40:10.334072 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 01:43:25.737725 139834281293632 spec.py:349] Evaluating on the test split.
I0313 01:43:28.419919 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 01:46:47.287900 139834281293632 submission_runner.py:420] Time since start: 77775.84s, 	Step: 127555, 	{'train/accuracy': 0.6929237842559814, 'train/loss': 1.3827592134475708, 'train/bleu': 35.22217671734824, 'validation/accuracy': 0.6960979700088501, 'validation/loss': 1.3584469556808472, 'validation/bleu': 30.751015518353736, 'validation/num_examples': 3000, 'test/accuracy': 0.7144616842269897, 'test/loss': 1.2528407573699951, 'test/bleu': 31.0004361617364, 'test/num_examples': 3003, 'score': 45402.009125709534, 'total_duration': 77775.84038686752, 'accumulated_submission_time': 45402.009125709534, 'accumulated_eval_time': 32368.1502096653, 'accumulated_logging_time': 1.7922303676605225}
I0313 01:46:47.319383 139664567539456 logging_writer.py:48] [127555] accumulated_eval_time=32368.150210, accumulated_logging_time=1.792230, accumulated_submission_time=45402.009126, global_step=127555, preemption_count=0, score=45402.009126, test/accuracy=0.714462, test/bleu=31.000436, test/loss=1.252841, test/num_examples=3003, total_duration=77775.840387, train/accuracy=0.692924, train/bleu=35.222177, train/loss=1.382759, validation/accuracy=0.696098, validation/bleu=30.751016, validation/loss=1.358447, validation/num_examples=3000
I0313 01:47:03.646789 139664575932160 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.23310549557209015, loss=1.4338867664337158
I0313 01:47:39.154241 139664567539456 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.2419428825378418, loss=1.5323396921157837
I0313 01:48:14.734661 139664575932160 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.23691420257091522, loss=1.464497447013855
I0313 01:48:50.318126 139664567539456 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.25084638595581055, loss=1.5544569492340088
I0313 01:49:25.914814 139664575932160 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.2425188273191452, loss=1.509577751159668
I0313 01:50:01.502155 139664567539456 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.2419809252023697, loss=1.4900044202804565
I0313 01:50:37.094523 139664575932160 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.23944488167762756, loss=1.467101812362671
I0313 01:51:12.647977 139664567539456 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.23417989909648895, loss=1.3802826404571533
I0313 01:51:48.232222 139664575932160 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.2425064891576767, loss=1.3947428464889526
I0313 01:52:23.792682 139664567539456 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.22144441306591034, loss=1.4091556072235107
I0313 01:52:59.403175 139664575932160 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.2322755753993988, loss=1.4196443557739258
I0313 01:53:34.983672 139664567539456 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.24094721674919128, loss=1.4926127195358276
I0313 01:54:10.573146 139664575932160 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.23945176601409912, loss=1.4441933631896973
I0313 01:54:46.129235 139664567539456 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.22924934327602386, loss=1.3897294998168945
I0313 01:55:21.733541 139664575932160 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.23135006427764893, loss=1.318190097808838
I0313 01:55:57.295026 139664567539456 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.24972157180309296, loss=1.500425934791565
I0313 01:56:32.911946 139664575932160 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.2343894988298416, loss=1.3991727828979492
I0313 01:57:08.542091 139664567539456 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.23762506246566772, loss=1.5171868801116943
I0313 01:57:44.116723 139664575932160 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.23657463490962982, loss=1.4993267059326172
I0313 01:58:19.733747 139664567539456 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.22408561408519745, loss=1.4136395454406738
I0313 01:58:55.331761 139664575932160 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.2404882162809372, loss=1.398855447769165
I0313 01:59:30.913378 139664567539456 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.23384490609169006, loss=1.4353258609771729
I0313 02:00:06.510544 139664575932160 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.22516325116157532, loss=1.454517126083374
I0313 02:00:42.102858 139664567539456 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.2338101863861084, loss=1.4767154455184937
I0313 02:00:47.509432 139834281293632 spec.py:321] Evaluating on the training split.
I0313 02:00:50.473340 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:04:17.989723 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 02:04:20.668447 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:07:42.851204 139834281293632 spec.py:349] Evaluating on the test split.
I0313 02:07:45.519771 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:10:57.958635 139834281293632 submission_runner.py:420] Time since start: 79226.51s, 	Step: 129917, 	{'train/accuracy': 0.6957194805145264, 'train/loss': 1.3678160905838013, 'train/bleu': 35.71389968451469, 'validation/accuracy': 0.6961227655410767, 'validation/loss': 1.3585906028747559, 'validation/bleu': 30.797391608123867, 'validation/num_examples': 3000, 'test/accuracy': 0.7143106460571289, 'test/loss': 1.2522014379501343, 'test/bleu': 30.975489200283697, 'test/num_examples': 3003, 'score': 46242.11846613884, 'total_duration': 79226.51109528542, 'accumulated_submission_time': 46242.11846613884, 'accumulated_eval_time': 32978.599338293076, 'accumulated_logging_time': 1.833895206451416}
I0313 02:10:57.989616 139664575932160 logging_writer.py:48] [129917] accumulated_eval_time=32978.599338, accumulated_logging_time=1.833895, accumulated_submission_time=46242.118466, global_step=129917, preemption_count=0, score=46242.118466, test/accuracy=0.714311, test/bleu=30.975489, test/loss=1.252201, test/num_examples=3003, total_duration=79226.511095, train/accuracy=0.695719, train/bleu=35.713900, train/loss=1.367816, validation/accuracy=0.696123, validation/bleu=30.797392, validation/loss=1.358591, validation/num_examples=3000
I0313 02:11:27.825045 139664567539456 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.23028512299060822, loss=1.4673835039138794
I0313 02:12:03.370750 139664575932160 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.23769956827163696, loss=1.4278788566589355
I0313 02:12:38.971637 139664567539456 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.23186562955379486, loss=1.4193607568740845
I0313 02:13:14.555167 139664575932160 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.23356983065605164, loss=1.463725209236145
I0313 02:13:50.122001 139664567539456 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.2275124192237854, loss=1.4869903326034546
I0313 02:14:25.714560 139664575932160 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.23958858847618103, loss=1.4815951585769653
I0313 02:15:01.295989 139664567539456 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.2475578337907791, loss=1.4789807796478271
I0313 02:15:36.873156 139664575932160 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.22744087874889374, loss=1.4562108516693115
I0313 02:16:12.473556 139664567539456 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.23551440238952637, loss=1.4913444519042969
I0313 02:16:48.042985 139664575932160 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.2442147135734558, loss=1.4414498805999756
I0313 02:17:23.628942 139664567539456 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.23296552896499634, loss=1.4602254629135132
I0313 02:17:59.177159 139664575932160 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.22424902021884918, loss=1.4432984590530396
I0313 02:18:34.774917 139664567539456 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.24047480523586273, loss=1.48649001121521
I0313 02:19:10.375713 139664575932160 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.22615692019462585, loss=1.3563311100006104
I0313 02:19:46.006165 139664567539456 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.22750304639339447, loss=1.428787112236023
I0313 02:20:21.648813 139664575932160 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.23021553456783295, loss=1.4750784635543823
I0313 02:20:57.244949 139664567539456 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.22670917212963104, loss=1.4104268550872803
I0313 02:21:32.824483 139664575932160 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.23346306383609772, loss=1.4525848627090454
I0313 02:22:08.413706 139664567539456 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.22554655373096466, loss=1.4243628978729248
I0313 02:22:43.979552 139664575932160 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.23115579783916473, loss=1.4052339792251587
I0313 02:23:19.572651 139664567539456 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.24032242596149445, loss=1.4208216667175293
I0313 02:23:55.178236 139664575932160 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.23867401480674744, loss=1.4121936559677124
I0313 02:24:30.763726 139664567539456 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.23088033497333527, loss=1.4460724592208862
I0313 02:24:58.214232 139834281293632 spec.py:321] Evaluating on the training split.
I0313 02:25:01.187699 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:28:36.776464 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 02:28:39.451834 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:31:54.081649 139834281293632 spec.py:349] Evaluating on the test split.
I0313 02:31:56.742213 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:35:05.093899 139834281293632 submission_runner.py:420] Time since start: 80673.65s, 	Step: 132279, 	{'train/accuracy': 0.6954941153526306, 'train/loss': 1.3714572191238403, 'train/bleu': 35.85878042865687, 'validation/accuracy': 0.6958624124526978, 'validation/loss': 1.3585927486419678, 'validation/bleu': 30.77876783788234, 'validation/num_examples': 3000, 'test/accuracy': 0.7146244049072266, 'test/loss': 1.2520089149475098, 'test/bleu': 31.02888880102673, 'test/num_examples': 3003, 'score': 47082.262501716614, 'total_duration': 80673.64637875557, 'accumulated_submission_time': 47082.262501716614, 'accumulated_eval_time': 33585.4789557457, 'accumulated_logging_time': 1.8751137256622314}
I0313 02:35:05.126035 139664575932160 logging_writer.py:48] [132279] accumulated_eval_time=33585.478956, accumulated_logging_time=1.875114, accumulated_submission_time=47082.262502, global_step=132279, preemption_count=0, score=47082.262502, test/accuracy=0.714624, test/bleu=31.028889, test/loss=1.252009, test/num_examples=3003, total_duration=80673.646379, train/accuracy=0.695494, train/bleu=35.858780, train/loss=1.371457, validation/accuracy=0.695862, validation/bleu=30.778768, validation/loss=1.358593, validation/num_examples=3000
I0313 02:35:12.947462 139664567539456 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.2278994619846344, loss=1.4823355674743652
I0313 02:35:48.433272 139664575932160 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.23025426268577576, loss=1.425801396369934
I0313 02:36:24.005661 139664567539456 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.2304346263408661, loss=1.4151182174682617
I0313 02:36:59.566405 139664575932160 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.23419812321662903, loss=1.4760935306549072
I0313 02:37:35.150949 139664567539456 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.23496952652931213, loss=1.4070056676864624
I0313 02:38:10.725326 139664575932160 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.22765396535396576, loss=1.4853849411010742
I0313 02:38:46.389907 139664567539456 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.23146338760852814, loss=1.4643279314041138
I0313 02:39:22.002959 139664575932160 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.22674775123596191, loss=1.44295334815979
I0313 02:39:57.598639 139664567539456 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.2294313609600067, loss=1.4506781101226807
I0313 02:40:33.180024 139664575932160 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.23506975173950195, loss=1.4183539152145386
I0313 02:41:08.776067 139664567539456 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.23490552604198456, loss=1.4895782470703125
I0313 02:41:44.371583 139664575932160 logging_writer.py:48] [133400] global_step=133400, grad_norm=0.22157053649425507, loss=1.4058797359466553
I0313 02:42:19.981926 139664567539456 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.23197975754737854, loss=1.4240858554840088
I0313 02:42:55.545220 139664575932160 logging_writer.py:48] [133600] global_step=133600, grad_norm=0.23521766066551208, loss=1.461812138557434
I0313 02:43:31.116200 139664567539456 logging_writer.py:48] [133700] global_step=133700, grad_norm=0.2383020520210266, loss=1.479191541671753
I0313 02:44:06.711992 139664575932160 logging_writer.py:48] [133800] global_step=133800, grad_norm=0.24157805740833282, loss=1.4702980518341064
I0313 02:44:42.309415 139664567539456 logging_writer.py:48] [133900] global_step=133900, grad_norm=0.23959173262119293, loss=1.4350758790969849
I0313 02:45:17.955898 139664575932160 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.23885485529899597, loss=1.4705301523208618
I0313 02:45:53.600192 139664567539456 logging_writer.py:48] [134100] global_step=134100, grad_norm=0.24393121898174286, loss=1.484209418296814
I0313 02:46:29.213036 139664575932160 logging_writer.py:48] [134200] global_step=134200, grad_norm=0.22819867730140686, loss=1.4256196022033691
I0313 02:47:04.828602 139664567539456 logging_writer.py:48] [134300] global_step=134300, grad_norm=0.2462804764509201, loss=1.470410943031311
I0313 02:47:40.429304 139664575932160 logging_writer.py:48] [134400] global_step=134400, grad_norm=0.22339273989200592, loss=1.421651840209961
I0313 02:48:16.034426 139664567539456 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.24255390465259552, loss=1.505900502204895
I0313 02:48:51.633968 139664575932160 logging_writer.py:48] [134600] global_step=134600, grad_norm=0.22604013979434967, loss=1.3959556818008423
I0313 02:49:05.247441 139834281293632 spec.py:321] Evaluating on the training split.
I0313 02:49:08.217376 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:52:36.497969 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 02:52:39.166029 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:55:53.287727 139834281293632 spec.py:349] Evaluating on the test split.
I0313 02:55:55.959472 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 02:59:05.400660 139834281293632 submission_runner.py:420] Time since start: 82113.95s, 	Step: 134640, 	{'train/accuracy': 0.6974707841873169, 'train/loss': 1.3609025478363037, 'train/bleu': 35.34378432455704, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 47922.3007247448, 'total_duration': 82113.95312094688, 'accumulated_submission_time': 47922.3007247448, 'accumulated_eval_time': 34185.63210296631, 'accumulated_logging_time': 1.917802095413208}
I0313 02:59:05.432791 139664567539456 logging_writer.py:48] [134640] accumulated_eval_time=34185.632103, accumulated_logging_time=1.917802, accumulated_submission_time=47922.300725, global_step=134640, preemption_count=0, score=47922.300725, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=82113.953121, train/accuracy=0.697471, train/bleu=35.343784, train/loss=1.360903, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 02:59:27.088567 139664575932160 logging_writer.py:48] [134700] global_step=134700, grad_norm=0.23610153794288635, loss=1.4155068397521973
I0313 03:00:02.628776 139664567539456 logging_writer.py:48] [134800] global_step=134800, grad_norm=0.23604387044906616, loss=1.4750643968582153
I0313 03:00:38.186631 139664575932160 logging_writer.py:48] [134900] global_step=134900, grad_norm=0.23371051251888275, loss=1.3504077196121216
I0313 03:01:13.763188 139664567539456 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.2364853471517563, loss=1.4728130102157593
I0313 03:01:49.327039 139664575932160 logging_writer.py:48] [135100] global_step=135100, grad_norm=0.2341269999742508, loss=1.3967708349227905
I0313 03:02:24.929189 139664567539456 logging_writer.py:48] [135200] global_step=135200, grad_norm=0.2327333390712738, loss=1.4550070762634277
I0313 03:03:00.538195 139664575932160 logging_writer.py:48] [135300] global_step=135300, grad_norm=0.23935911059379578, loss=1.4588574171066284
I0313 03:03:36.164799 139664567539456 logging_writer.py:48] [135400] global_step=135400, grad_norm=0.2341800332069397, loss=1.3990647792816162
I0313 03:04:11.784397 139664575932160 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.22554223239421844, loss=1.4558875560760498
I0313 03:04:47.357496 139664567539456 logging_writer.py:48] [135600] global_step=135600, grad_norm=0.24370476603507996, loss=1.5539640188217163
I0313 03:05:22.933441 139664575932160 logging_writer.py:48] [135700] global_step=135700, grad_norm=0.22763440012931824, loss=1.413055658340454
I0313 03:05:58.516448 139664567539456 logging_writer.py:48] [135800] global_step=135800, grad_norm=0.2383633702993393, loss=1.4591951370239258
I0313 03:06:34.079797 139664575932160 logging_writer.py:48] [135900] global_step=135900, grad_norm=0.23035895824432373, loss=1.3869280815124512
I0313 03:07:09.661306 139664567539456 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.23111267387866974, loss=1.4648816585540771
I0313 03:07:45.258492 139664575932160 logging_writer.py:48] [136100] global_step=136100, grad_norm=0.2395458072423935, loss=1.4825396537780762
I0313 03:08:20.876906 139664567539456 logging_writer.py:48] [136200] global_step=136200, grad_norm=0.2285759299993515, loss=1.4443559646606445
I0313 03:08:56.448485 139664575932160 logging_writer.py:48] [136300] global_step=136300, grad_norm=0.23558305203914642, loss=1.4220972061157227
I0313 03:09:32.043622 139664567539456 logging_writer.py:48] [136400] global_step=136400, grad_norm=0.2399633824825287, loss=1.3971787691116333
I0313 03:10:07.615787 139664575932160 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.2354559302330017, loss=1.4541095495224
I0313 03:10:43.201148 139664567539456 logging_writer.py:48] [136600] global_step=136600, grad_norm=0.2344631850719452, loss=1.5154139995574951
I0313 03:11:18.841706 139664575932160 logging_writer.py:48] [136700] global_step=136700, grad_norm=0.23562751710414886, loss=1.514968752861023
I0313 03:11:54.460506 139664567539456 logging_writer.py:48] [136800] global_step=136800, grad_norm=0.23784133791923523, loss=1.4312443733215332
I0313 03:12:30.058145 139664575932160 logging_writer.py:48] [136900] global_step=136900, grad_norm=0.2390488237142563, loss=1.4621928930282593
I0313 03:13:05.648106 139664567539456 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.23837827146053314, loss=1.398363471031189
I0313 03:13:05.654064 139834281293632 spec.py:321] Evaluating on the training split.
I0313 03:13:08.338458 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 03:16:38.325451 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 03:16:40.999857 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 03:19:54.999980 139834281293632 spec.py:349] Evaluating on the test split.
I0313 03:19:57.679441 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 03:23:07.043499 139834281293632 submission_runner.py:420] Time since start: 83555.60s, 	Step: 137001, 	{'train/accuracy': 0.699103832244873, 'train/loss': 1.3539986610412598, 'train/bleu': 35.55390017405729, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 48762.441814661026, 'total_duration': 83555.59596848488, 'accumulated_submission_time': 48762.441814661026, 'accumulated_eval_time': 34787.02144932747, 'accumulated_logging_time': 1.959381103515625}
I0313 03:23:07.075801 139664575932160 logging_writer.py:48] [137001] accumulated_eval_time=34787.021449, accumulated_logging_time=1.959381, accumulated_submission_time=48762.441815, global_step=137001, preemption_count=0, score=48762.441815, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=83555.595968, train/accuracy=0.699104, train/bleu=35.553900, train/loss=1.353999, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 03:23:42.553067 139664567539456 logging_writer.py:48] [137100] global_step=137100, grad_norm=0.2423120141029358, loss=1.463750958442688
I0313 03:24:18.078647 139664575932160 logging_writer.py:48] [137200] global_step=137200, grad_norm=0.23971408605575562, loss=1.5051859617233276
I0313 03:24:53.648577 139664567539456 logging_writer.py:48] [137300] global_step=137300, grad_norm=0.23809024691581726, loss=1.4875651597976685
I0313 03:25:29.194188 139664575932160 logging_writer.py:48] [137400] global_step=137400, grad_norm=0.2341955602169037, loss=1.4606122970581055
I0313 03:26:04.741103 139664567539456 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.2277991771697998, loss=1.3869562149047852
I0313 03:26:40.329222 139664575932160 logging_writer.py:48] [137600] global_step=137600, grad_norm=0.2397036850452423, loss=1.5119510889053345
I0313 03:27:15.954425 139664567539456 logging_writer.py:48] [137700] global_step=137700, grad_norm=0.22320149838924408, loss=1.446780800819397
I0313 03:27:51.589295 139664575932160 logging_writer.py:48] [137800] global_step=137800, grad_norm=0.2385961264371872, loss=1.3696515560150146
I0313 03:28:27.245544 139664567539456 logging_writer.py:48] [137900] global_step=137900, grad_norm=0.2361411601305008, loss=1.5095051527023315
I0313 03:29:02.867430 139664575932160 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.22110334038734436, loss=1.3575663566589355
I0313 03:29:38.473070 139664567539456 logging_writer.py:48] [138100] global_step=138100, grad_norm=0.22833724319934845, loss=1.4144288301467896
I0313 03:30:14.062702 139664575932160 logging_writer.py:48] [138200] global_step=138200, grad_norm=0.23980477452278137, loss=1.4196853637695312
I0313 03:30:49.638469 139664567539456 logging_writer.py:48] [138300] global_step=138300, grad_norm=0.2302829623222351, loss=1.4832956790924072
I0313 03:31:25.261509 139664575932160 logging_writer.py:48] [138400] global_step=138400, grad_norm=0.2312806397676468, loss=1.409720778465271
I0313 03:32:00.893495 139664567539456 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.2436920702457428, loss=1.422567367553711
I0313 03:32:36.464544 139664575932160 logging_writer.py:48] [138600] global_step=138600, grad_norm=0.2260599136352539, loss=1.4500209093093872
I0313 03:33:12.045746 139664567539456 logging_writer.py:48] [138700] global_step=138700, grad_norm=0.23625710606575012, loss=1.4668304920196533
I0313 03:33:47.617873 139664575932160 logging_writer.py:48] [138800] global_step=138800, grad_norm=0.23377513885498047, loss=1.379302978515625
I0313 03:34:23.203571 139664567539456 logging_writer.py:48] [138900] global_step=138900, grad_norm=0.23060207068920135, loss=1.472401738166809
I0313 03:34:58.785711 139664575932160 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.22902144491672516, loss=1.3728913068771362
I0313 03:35:34.401583 139664567539456 logging_writer.py:48] [139100] global_step=139100, grad_norm=0.24373359978199005, loss=1.4553678035736084
I0313 03:36:09.981138 139664575932160 logging_writer.py:48] [139200] global_step=139200, grad_norm=0.24123720824718475, loss=1.4548020362854004
I0313 03:36:45.557374 139664567539456 logging_writer.py:48] [139300] global_step=139300, grad_norm=0.22556272149085999, loss=1.4357606172561646
I0313 03:37:07.363813 139834281293632 spec.py:321] Evaluating on the training split.
I0313 03:37:10.346684 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 03:40:47.433136 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 03:40:50.108029 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 03:44:03.708774 139834281293632 spec.py:349] Evaluating on the test split.
I0313 03:44:06.373179 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 03:47:16.024559 139834281293632 submission_runner.py:420] Time since start: 85004.58s, 	Step: 139363, 	{'train/accuracy': 0.6987770199775696, 'train/loss': 1.35377836227417, 'train/bleu': 35.64343763817623, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 49602.64884185791, 'total_duration': 85004.57704162598, 'accumulated_submission_time': 49602.64884185791, 'accumulated_eval_time': 35395.68215799332, 'accumulated_logging_time': 2.0005979537963867}
I0313 03:47:16.057149 139664575932160 logging_writer.py:48] [139363] accumulated_eval_time=35395.682158, accumulated_logging_time=2.000598, accumulated_submission_time=49602.648842, global_step=139363, preemption_count=0, score=49602.648842, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=85004.577042, train/accuracy=0.698777, train/bleu=35.643438, train/loss=1.353778, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 03:47:29.585742 139664567539456 logging_writer.py:48] [139400] global_step=139400, grad_norm=0.22550275921821594, loss=1.3854382038116455
I0313 03:48:05.129244 139664575932160 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.23177655041217804, loss=1.4726104736328125
I0313 03:48:40.723525 139664567539456 logging_writer.py:48] [139600] global_step=139600, grad_norm=0.2376149594783783, loss=1.4251943826675415
I0313 03:49:16.309196 139664575932160 logging_writer.py:48] [139700] global_step=139700, grad_norm=0.23363302648067474, loss=1.4758034944534302
I0313 03:49:51.883620 139664567539456 logging_writer.py:48] [139800] global_step=139800, grad_norm=0.24110712110996246, loss=1.4726601839065552
I0313 03:50:27.477174 139664575932160 logging_writer.py:48] [139900] global_step=139900, grad_norm=0.2419426292181015, loss=1.442309856414795
I0313 03:51:03.086311 139664567539456 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.24333369731903076, loss=1.4281141757965088
I0313 03:51:38.683913 139664575932160 logging_writer.py:48] [140100] global_step=140100, grad_norm=0.22632722556591034, loss=1.364973783493042
I0313 03:52:14.300162 139664567539456 logging_writer.py:48] [140200] global_step=140200, grad_norm=0.22828838229179382, loss=1.4376879930496216
I0313 03:52:49.900644 139664575932160 logging_writer.py:48] [140300] global_step=140300, grad_norm=0.24646639823913574, loss=1.4357500076293945
I0313 03:53:25.475734 139664567539456 logging_writer.py:48] [140400] global_step=140400, grad_norm=0.2271505445241928, loss=1.3970955610275269
I0313 03:54:01.044009 139664575932160 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.23071148991584778, loss=1.4611918926239014
I0313 03:54:36.624697 139664567539456 logging_writer.py:48] [140600] global_step=140600, grad_norm=0.23048922419548035, loss=1.4851382970809937
I0313 03:55:12.226866 139664575932160 logging_writer.py:48] [140700] global_step=140700, grad_norm=0.23912477493286133, loss=1.446451187133789
I0313 03:55:47.789502 139664567539456 logging_writer.py:48] [140800] global_step=140800, grad_norm=0.2358795404434204, loss=1.45841646194458
I0313 03:56:23.388833 139664575932160 logging_writer.py:48] [140900] global_step=140900, grad_norm=0.22580672800540924, loss=1.4638891220092773
I0313 03:56:58.969088 139664567539456 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.2285289615392685, loss=1.4171561002731323
I0313 03:57:34.552537 139664575932160 logging_writer.py:48] [141100] global_step=141100, grad_norm=0.23467294871807098, loss=1.4591577053070068
I0313 03:58:10.158657 139664567539456 logging_writer.py:48] [141200] global_step=141200, grad_norm=0.24261491000652313, loss=1.4283134937286377
I0313 03:58:45.686809 139664575932160 logging_writer.py:48] [141300] global_step=141300, grad_norm=0.23151224851608276, loss=1.384105920791626
I0313 03:59:21.278653 139664567539456 logging_writer.py:48] [141400] global_step=141400, grad_norm=0.22838352620601654, loss=1.3587725162506104
I0313 03:59:56.865260 139664575932160 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.23944009840488434, loss=1.472752332687378
I0313 04:00:32.461529 139664567539456 logging_writer.py:48] [141600] global_step=141600, grad_norm=0.2429259568452835, loss=1.4747238159179688
I0313 04:01:08.029266 139664575932160 logging_writer.py:48] [141700] global_step=141700, grad_norm=0.23929308354854584, loss=1.4295638799667358
I0313 04:01:16.281489 139834281293632 spec.py:321] Evaluating on the training split.
I0313 04:01:19.244443 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:04:52.757818 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 04:04:55.432668 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:08:09.094499 139834281293632 spec.py:349] Evaluating on the test split.
I0313 04:08:11.785524 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:11:22.801550 139834281293632 submission_runner.py:420] Time since start: 86451.35s, 	Step: 141725, 	{'train/accuracy': 0.6979433298110962, 'train/loss': 1.3597429990768433, 'train/bleu': 35.16172339456949, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 50442.791553497314, 'total_duration': 86451.35402941704, 'accumulated_submission_time': 50442.791553497314, 'accumulated_eval_time': 36002.202165842056, 'accumulated_logging_time': 2.0436899662017822}
I0313 04:11:22.836357 139664567539456 logging_writer.py:48] [141725] accumulated_eval_time=36002.202166, accumulated_logging_time=2.043690, accumulated_submission_time=50442.791553, global_step=141725, preemption_count=0, score=50442.791553, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=86451.354029, train/accuracy=0.697943, train/bleu=35.161723, train/loss=1.359743, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 04:11:49.828153 139664575932160 logging_writer.py:48] [141800] global_step=141800, grad_norm=0.2399320900440216, loss=1.4820055961608887
I0313 04:12:25.398812 139664567539456 logging_writer.py:48] [141900] global_step=141900, grad_norm=0.2371099889278412, loss=1.5053397417068481
I0313 04:13:01.015250 139664575932160 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.23613466322422028, loss=1.4557286500930786
I0313 04:13:36.586873 139664567539456 logging_writer.py:48] [142100] global_step=142100, grad_norm=0.24031761288642883, loss=1.4061511754989624
I0313 04:14:12.195978 139664575932160 logging_writer.py:48] [142200] global_step=142200, grad_norm=0.23406238853931427, loss=1.4403151273727417
I0313 04:14:47.776259 139664567539456 logging_writer.py:48] [142300] global_step=142300, grad_norm=0.24129699170589447, loss=1.4476146697998047
I0313 04:15:23.360813 139664575932160 logging_writer.py:48] [142400] global_step=142400, grad_norm=0.24371437728405, loss=1.4917737245559692
I0313 04:15:58.945412 139664567539456 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.23845332860946655, loss=1.4831496477127075
I0313 04:16:34.526542 139664575932160 logging_writer.py:48] [142600] global_step=142600, grad_norm=0.23949354887008667, loss=1.4691473245620728
I0313 04:17:10.127353 139664567539456 logging_writer.py:48] [142700] global_step=142700, grad_norm=0.22450801730155945, loss=1.3441784381866455
I0313 04:17:45.707876 139664575932160 logging_writer.py:48] [142800] global_step=142800, grad_norm=0.23312437534332275, loss=1.472515344619751
I0313 04:18:21.301922 139664567539456 logging_writer.py:48] [142900] global_step=142900, grad_norm=0.22665944695472717, loss=1.432465672492981
I0313 04:18:56.937656 139664575932160 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.233657106757164, loss=1.487916350364685
I0313 04:19:32.541514 139664567539456 logging_writer.py:48] [143100] global_step=143100, grad_norm=0.2282106727361679, loss=1.429235816001892
I0313 04:20:08.110972 139664575932160 logging_writer.py:48] [143200] global_step=143200, grad_norm=0.23665982484817505, loss=1.460017442703247
I0313 04:20:43.719105 139664567539456 logging_writer.py:48] [143300] global_step=143300, grad_norm=0.2453370988368988, loss=1.53627347946167
I0313 04:21:19.302244 139664575932160 logging_writer.py:48] [143400] global_step=143400, grad_norm=0.22918061912059784, loss=1.4267871379852295
I0313 04:21:54.892096 139664567539456 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.24205924570560455, loss=1.478661298751831
I0313 04:22:30.478586 139664575932160 logging_writer.py:48] [143600] global_step=143600, grad_norm=0.23670709133148193, loss=1.4601062536239624
I0313 04:23:06.043116 139664567539456 logging_writer.py:48] [143700] global_step=143700, grad_norm=0.2336391657590866, loss=1.4035255908966064
I0313 04:23:41.611222 139664575932160 logging_writer.py:48] [143800] global_step=143800, grad_norm=0.22888897359371185, loss=1.4012387990951538
I0313 04:24:17.211323 139664567539456 logging_writer.py:48] [143900] global_step=143900, grad_norm=0.2278624325990677, loss=1.4804327487945557
I0313 04:24:52.803924 139664575932160 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.24357841908931732, loss=1.4812630414962769
I0313 04:25:23.133905 139834281293632 spec.py:321] Evaluating on the training split.
I0313 04:25:26.114120 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:29:00.309189 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 04:29:02.998762 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:32:16.932075 139834281293632 spec.py:349] Evaluating on the test split.
I0313 04:32:19.597969 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:35:29.447792 139834281293632 submission_runner.py:420] Time since start: 87898.00s, 	Step: 144087, 	{'train/accuracy': 0.6965329051017761, 'train/loss': 1.362047791481018, 'train/bleu': 35.24575844551815, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 51283.00962305069, 'total_duration': 87898.00025892258, 'accumulated_submission_time': 51283.00962305069, 'accumulated_eval_time': 36608.516001701355, 'accumulated_logging_time': 2.087331771850586}
I0313 04:35:29.481767 139664567539456 logging_writer.py:48] [144087] accumulated_eval_time=36608.516002, accumulated_logging_time=2.087332, accumulated_submission_time=51283.009623, global_step=144087, preemption_count=0, score=51283.009623, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=87898.000259, train/accuracy=0.696533, train/bleu=35.245758, train/loss=1.362048, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 04:35:34.477683 139664575932160 logging_writer.py:48] [144100] global_step=144100, grad_norm=0.2238554209470749, loss=1.3424296379089355
I0313 04:36:09.984622 139664567539456 logging_writer.py:48] [144200] global_step=144200, grad_norm=0.2257061004638672, loss=1.4238080978393555
I0313 04:36:45.581229 139664575932160 logging_writer.py:48] [144300] global_step=144300, grad_norm=0.23167532682418823, loss=1.4319310188293457
I0313 04:37:21.197400 139664567539456 logging_writer.py:48] [144400] global_step=144400, grad_norm=0.23544718325138092, loss=1.4831997156143188
I0313 04:37:56.754114 139664575932160 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.23797719180583954, loss=1.383638620376587
I0313 04:38:32.364081 139664567539456 logging_writer.py:48] [144600] global_step=144600, grad_norm=0.21931259334087372, loss=1.4160946607589722
I0313 04:39:07.956508 139664575932160 logging_writer.py:48] [144700] global_step=144700, grad_norm=0.22684136033058167, loss=1.3772664070129395
I0313 04:39:43.526741 139664567539456 logging_writer.py:48] [144800] global_step=144800, grad_norm=0.23386862874031067, loss=1.4611238241195679
I0313 04:40:19.097691 139664575932160 logging_writer.py:48] [144900] global_step=144900, grad_norm=0.24067138135433197, loss=1.4702939987182617
I0313 04:40:54.661180 139664567539456 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.23375388979911804, loss=1.4243738651275635
I0313 04:41:30.240671 139664575932160 logging_writer.py:48] [145100] global_step=145100, grad_norm=0.22675688564777374, loss=1.4702647924423218
I0313 04:42:05.793196 139664567539456 logging_writer.py:48] [145200] global_step=145200, grad_norm=0.2347838133573532, loss=1.4057890176773071
I0313 04:42:41.360971 139664575932160 logging_writer.py:48] [145300] global_step=145300, grad_norm=0.23868653178215027, loss=1.4710073471069336
I0313 04:43:16.947467 139664567539456 logging_writer.py:48] [145400] global_step=145400, grad_norm=0.2564149796962738, loss=1.4698954820632935
I0313 04:43:52.521070 139664575932160 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.22520750761032104, loss=1.475175142288208
I0313 04:44:28.107835 139664567539456 logging_writer.py:48] [145600] global_step=145600, grad_norm=0.23439151048660278, loss=1.3439167737960815
I0313 04:45:03.717750 139664575932160 logging_writer.py:48] [145700] global_step=145700, grad_norm=0.22730928659439087, loss=1.410740852355957
I0313 04:45:39.282659 139664567539456 logging_writer.py:48] [145800] global_step=145800, grad_norm=0.24267727136611938, loss=1.4621919393539429
I0313 04:46:14.865116 139664575932160 logging_writer.py:48] [145900] global_step=145900, grad_norm=0.23473332822322845, loss=1.4659864902496338
I0313 04:46:50.413737 139664567539456 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.22932127118110657, loss=1.44502592086792
I0313 04:47:25.983178 139664575932160 logging_writer.py:48] [146100] global_step=146100, grad_norm=0.24213889241218567, loss=1.432356595993042
I0313 04:48:01.570439 139664567539456 logging_writer.py:48] [146200] global_step=146200, grad_norm=0.23465685546398163, loss=1.542525053024292
I0313 04:48:37.201721 139664575932160 logging_writer.py:48] [146300] global_step=146300, grad_norm=0.23464983701705933, loss=1.4297163486480713
I0313 04:49:12.817179 139664567539456 logging_writer.py:48] [146400] global_step=146400, grad_norm=0.23817217350006104, loss=1.4787460565567017
I0313 04:49:29.594186 139834281293632 spec.py:321] Evaluating on the training split.
I0313 04:49:32.564130 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:53:06.610565 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 04:53:09.283885 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:56:23.158648 139834281293632 spec.py:349] Evaluating on the test split.
I0313 04:56:25.831517 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 04:59:37.645428 139834281293632 submission_runner.py:420] Time since start: 89346.20s, 	Step: 146449, 	{'train/accuracy': 0.6960597634315491, 'train/loss': 1.3612914085388184, 'train/bleu': 35.78783044286703, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 52123.04042816162, 'total_duration': 89346.19786715508, 'accumulated_submission_time': 52123.04042816162, 'accumulated_eval_time': 37216.56715321541, 'accumulated_logging_time': 2.130098581314087}
I0313 04:59:37.686977 139664575932160 logging_writer.py:48] [146449] accumulated_eval_time=37216.567153, accumulated_logging_time=2.130099, accumulated_submission_time=52123.040428, global_step=146449, preemption_count=0, score=52123.040428, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=89346.197867, train/accuracy=0.696060, train/bleu=35.787830, train/loss=1.361291, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 04:59:56.176442 139664567539456 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.2341018170118332, loss=1.4910963773727417
I0313 05:00:31.706957 139664575932160 logging_writer.py:48] [146600] global_step=146600, grad_norm=0.232659250497818, loss=1.4681087732315063
I0313 05:01:07.275789 139664567539456 logging_writer.py:48] [146700] global_step=146700, grad_norm=0.2353483885526657, loss=1.459840178489685
I0313 05:01:42.835845 139664575932160 logging_writer.py:48] [146800] global_step=146800, grad_norm=0.23800094425678253, loss=1.4870625734329224
I0313 05:02:18.416453 139664567539456 logging_writer.py:48] [146900] global_step=146900, grad_norm=0.23577846586704254, loss=1.4359978437423706
I0313 05:02:54.000674 139664575932160 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.2203926146030426, loss=1.4405262470245361
I0313 05:03:29.571230 139664567539456 logging_writer.py:48] [147100] global_step=147100, grad_norm=0.22955761849880219, loss=1.3964824676513672
I0313 05:04:05.178112 139664575932160 logging_writer.py:48] [147200] global_step=147200, grad_norm=0.23522993922233582, loss=1.503777265548706
I0313 05:04:40.774779 139664567539456 logging_writer.py:48] [147300] global_step=147300, grad_norm=0.23244889080524445, loss=1.474483847618103
I0313 05:05:16.341930 139664575932160 logging_writer.py:48] [147400] global_step=147400, grad_norm=0.22325174510478973, loss=1.4276115894317627
I0313 05:05:51.884513 139664567539456 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.23594598472118378, loss=1.4452065229415894
I0313 05:06:27.448953 139664575932160 logging_writer.py:48] [147600] global_step=147600, grad_norm=0.23642496764659882, loss=1.4162362813949585
I0313 05:07:03.037319 139664567539456 logging_writer.py:48] [147700] global_step=147700, grad_norm=0.2436540424823761, loss=1.3949545621871948
I0313 05:07:38.597395 139664575932160 logging_writer.py:48] [147800] global_step=147800, grad_norm=0.22901155054569244, loss=1.4637531042099
I0313 05:08:14.202671 139664567539456 logging_writer.py:48] [147900] global_step=147900, grad_norm=0.23694196343421936, loss=1.3957018852233887
I0313 05:08:49.775606 139664575932160 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.2441054880619049, loss=1.4693478345870972
I0313 05:09:25.372982 139664567539456 logging_writer.py:48] [148100] global_step=148100, grad_norm=0.23685011267662048, loss=1.4122201204299927
I0313 05:10:00.955542 139664575932160 logging_writer.py:48] [148200] global_step=148200, grad_norm=0.24133425951004028, loss=1.4805915355682373
I0313 05:10:36.558866 139664567539456 logging_writer.py:48] [148300] global_step=148300, grad_norm=0.2496069073677063, loss=1.3760243654251099
I0313 05:11:12.137674 139664575932160 logging_writer.py:48] [148400] global_step=148400, grad_norm=0.24350769817829132, loss=1.4806843996047974
I0313 05:11:47.773969 139664567539456 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.22956934571266174, loss=1.3548774719238281
I0313 05:12:23.375529 139664575932160 logging_writer.py:48] [148600] global_step=148600, grad_norm=0.24467960000038147, loss=1.517225980758667
I0313 05:12:58.975092 139664567539456 logging_writer.py:48] [148700] global_step=148700, grad_norm=0.22770127654075623, loss=1.4445133209228516
I0313 05:13:34.586289 139664575932160 logging_writer.py:48] [148800] global_step=148800, grad_norm=0.2423778921365738, loss=1.47576904296875
I0313 05:13:37.871920 139834281293632 spec.py:321] Evaluating on the training split.
I0313 05:13:40.846630 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 05:17:05.272967 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 05:17:07.970011 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 05:20:22.601269 139834281293632 spec.py:349] Evaluating on the test split.
I0313 05:20:25.265338 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 05:23:35.270108 139834281293632 submission_runner.py:420] Time since start: 90783.82s, 	Step: 148811, 	{'train/accuracy': 0.6943530440330505, 'train/loss': 1.3751060962677002, 'train/bleu': 35.51247185279916, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 52963.1450676918, 'total_duration': 90783.82259011269, 'accumulated_submission_time': 52963.1450676918, 'accumulated_eval_time': 37813.96529150009, 'accumulated_logging_time': 2.1822049617767334}
I0313 05:23:35.302985 139664567539456 logging_writer.py:48] [148811] accumulated_eval_time=37813.965292, accumulated_logging_time=2.182205, accumulated_submission_time=52963.145068, global_step=148811, preemption_count=0, score=52963.145068, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=90783.822590, train/accuracy=0.694353, train/bleu=35.512472, train/loss=1.375106, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 05:24:07.251122 139664575932160 logging_writer.py:48] [148900] global_step=148900, grad_norm=0.24033880233764648, loss=1.4672377109527588
I0313 05:24:42.748857 139664567539456 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.24046294391155243, loss=1.448286771774292
I0313 05:25:18.308204 139664575932160 logging_writer.py:48] [149100] global_step=149100, grad_norm=0.23736967146396637, loss=1.4554928541183472
I0313 05:25:53.864887 139664567539456 logging_writer.py:48] [149200] global_step=149200, grad_norm=0.23499692976474762, loss=1.4159824848175049
I0313 05:26:29.428252 139664575932160 logging_writer.py:48] [149300] global_step=149300, grad_norm=0.24192267656326294, loss=1.4224549531936646
I0313 05:27:04.989356 139664567539456 logging_writer.py:48] [149400] global_step=149400, grad_norm=0.22752811014652252, loss=1.429959774017334
I0313 05:27:40.552178 139664575932160 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.2338196486234665, loss=1.4479151964187622
I0313 05:28:16.092999 139664567539456 logging_writer.py:48] [149600] global_step=149600, grad_norm=0.23628956079483032, loss=1.4686082601547241
I0313 05:28:51.657140 139664575932160 logging_writer.py:48] [149700] global_step=149700, grad_norm=0.22932744026184082, loss=1.4574826955795288
I0313 05:29:27.251793 139664567539456 logging_writer.py:48] [149800] global_step=149800, grad_norm=0.2226087599992752, loss=1.4167486429214478
I0313 05:30:02.837774 139664575932160 logging_writer.py:48] [149900] global_step=149900, grad_norm=0.232980877161026, loss=1.4606748819351196
I0313 05:30:38.419955 139664567539456 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.240220844745636, loss=1.479880452156067
I0313 05:31:14.007008 139664575932160 logging_writer.py:48] [150100] global_step=150100, grad_norm=0.2316371649503708, loss=1.4005615711212158
I0313 05:31:49.602813 139664567539456 logging_writer.py:48] [150200] global_step=150200, grad_norm=0.2356247901916504, loss=1.434898853302002
I0313 05:32:25.173061 139664575932160 logging_writer.py:48] [150300] global_step=150300, grad_norm=0.2297382801771164, loss=1.3862136602401733
I0313 05:33:00.763506 139664567539456 logging_writer.py:48] [150400] global_step=150400, grad_norm=0.22746069729328156, loss=1.3946292400360107
I0313 05:33:36.325747 139664575932160 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.23682454228401184, loss=1.4498823881149292
I0313 05:34:11.885188 139664567539456 logging_writer.py:48] [150600] global_step=150600, grad_norm=0.2438391149044037, loss=1.4518775939941406
I0313 05:34:47.438219 139664575932160 logging_writer.py:48] [150700] global_step=150700, grad_norm=1.2616506814956665, loss=1.4680033922195435
I0313 05:35:22.999180 139664567539456 logging_writer.py:48] [150800] global_step=150800, grad_norm=0.24035067856311798, loss=1.4052650928497314
I0313 05:35:58.585832 139664575932160 logging_writer.py:48] [150900] global_step=150900, grad_norm=0.2381482571363449, loss=1.487246036529541
I0313 05:36:34.190217 139664567539456 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.23716986179351807, loss=1.4593130350112915
I0313 05:37:09.801161 139664575932160 logging_writer.py:48] [151100] global_step=151100, grad_norm=0.23844395577907562, loss=1.4995086193084717
I0313 05:37:35.491593 139834281293632 spec.py:321] Evaluating on the training split.
I0313 05:37:38.458582 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 05:41:10.320871 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 05:41:13.002710 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 05:44:26.978128 139834281293632 spec.py:349] Evaluating on the test split.
I0313 05:44:29.641187 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 05:47:39.346204 139834281293632 submission_runner.py:420] Time since start: 92227.90s, 	Step: 151174, 	{'train/accuracy': 0.6944592595100403, 'train/loss': 1.378005027770996, 'train/bleu': 35.85757723474082, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 53803.25436472893, 'total_duration': 92227.89867305756, 'accumulated_submission_time': 53803.25436472893, 'accumulated_eval_time': 38417.81984090805, 'accumulated_logging_time': 2.2239112854003906}
I0313 05:47:39.379855 139664567539456 logging_writer.py:48] [151174] accumulated_eval_time=38417.819841, accumulated_logging_time=2.223911, accumulated_submission_time=53803.254365, global_step=151174, preemption_count=0, score=53803.254365, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=92227.898673, train/accuracy=0.694459, train/bleu=35.857577, train/loss=1.378005, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 05:47:48.989090 139664575932160 logging_writer.py:48] [151200] global_step=151200, grad_norm=0.2326265573501587, loss=1.4532557725906372
I0313 05:48:24.487334 139664567539456 logging_writer.py:48] [151300] global_step=151300, grad_norm=0.23153452575206757, loss=1.3664216995239258
I0313 05:49:00.073476 139664575932160 logging_writer.py:48] [151400] global_step=151400, grad_norm=0.231330007314682, loss=1.4797347784042358
I0313 05:49:35.701647 139664567539456 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.2397959679365158, loss=1.4587963819503784
I0313 05:50:11.372447 139664575932160 logging_writer.py:48] [151600] global_step=151600, grad_norm=0.23826785385608673, loss=1.3933926820755005
I0313 05:50:47.047288 139664567539456 logging_writer.py:48] [151700] global_step=151700, grad_norm=0.23608706891536713, loss=1.4127469062805176
I0313 05:51:22.704246 139664575932160 logging_writer.py:48] [151800] global_step=151800, grad_norm=0.24690091609954834, loss=1.4980731010437012
I0313 05:51:58.299753 139664567539456 logging_writer.py:48] [151900] global_step=151900, grad_norm=0.23785163462162018, loss=1.5031890869140625
I0313 05:52:33.893434 139664575932160 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.25654590129852295, loss=1.4563075304031372
I0313 05:53:09.515225 139664567539456 logging_writer.py:48] [152100] global_step=152100, grad_norm=0.23799602687358856, loss=1.4770605564117432
I0313 05:53:45.114728 139664575932160 logging_writer.py:48] [152200] global_step=152200, grad_norm=0.23250317573547363, loss=1.464402675628662
I0313 05:54:20.774029 139664567539456 logging_writer.py:48] [152300] global_step=152300, grad_norm=0.2364877611398697, loss=1.3805543184280396
I0313 05:54:56.449765 139664575932160 logging_writer.py:48] [152400] global_step=152400, grad_norm=0.23243771493434906, loss=1.426693081855774
I0313 05:55:32.054340 139664567539456 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.24261395633220673, loss=1.5039499998092651
I0313 05:56:07.632821 139664575932160 logging_writer.py:48] [152600] global_step=152600, grad_norm=0.23618343472480774, loss=1.4051437377929688
I0313 05:56:43.213972 139664567539456 logging_writer.py:48] [152700] global_step=152700, grad_norm=0.22773593664169312, loss=1.3550065755844116
I0313 05:57:18.805501 139664575932160 logging_writer.py:48] [152800] global_step=152800, grad_norm=0.228300079703331, loss=1.473989486694336
I0313 05:57:54.421835 139664567539456 logging_writer.py:48] [152900] global_step=152900, grad_norm=0.24404777586460114, loss=1.4448245763778687
I0313 05:58:30.083728 139664575932160 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.2323433756828308, loss=1.458285927772522
I0313 05:59:05.715796 139664567539456 logging_writer.py:48] [153100] global_step=153100, grad_norm=0.23216940462589264, loss=1.4333997964859009
I0313 05:59:41.310244 139664575932160 logging_writer.py:48] [153200] global_step=153200, grad_norm=0.2508782148361206, loss=1.3641977310180664
I0313 06:00:16.927724 139664567539456 logging_writer.py:48] [153300] global_step=153300, grad_norm=0.23434904217720032, loss=1.5048781633377075
I0313 06:00:52.495064 139664575932160 logging_writer.py:48] [153400] global_step=153400, grad_norm=0.23050309717655182, loss=1.4393246173858643
I0313 06:01:28.064575 139664567539456 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.23537838459014893, loss=1.4366264343261719
I0313 06:01:39.538736 139834281293632 spec.py:321] Evaluating on the training split.
I0313 06:01:42.505025 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:05:13.806422 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 06:05:16.481482 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:08:30.374534 139834281293632 spec.py:349] Evaluating on the test split.
I0313 06:08:33.050583 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:11:42.922781 139834281293632 submission_runner.py:420] Time since start: 93671.48s, 	Step: 153534, 	{'train/accuracy': 0.6963521242141724, 'train/loss': 1.3660151958465576, 'train/bleu': 35.2620739329758, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 54643.327904224396, 'total_duration': 93671.47524333, 'accumulated_submission_time': 54643.327904224396, 'accumulated_eval_time': 39021.20381522179, 'accumulated_logging_time': 2.266636848449707}
I0313 06:11:42.959160 139664575932160 logging_writer.py:48] [153534] accumulated_eval_time=39021.203815, accumulated_logging_time=2.266637, accumulated_submission_time=54643.327904, global_step=153534, preemption_count=0, score=54643.327904, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=93671.475243, train/accuracy=0.696352, train/bleu=35.262074, train/loss=1.366015, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 06:12:06.752816 139664567539456 logging_writer.py:48] [153600] global_step=153600, grad_norm=0.22536532580852509, loss=1.4293649196624756
I0313 06:12:42.275387 139664575932160 logging_writer.py:48] [153700] global_step=153700, grad_norm=0.2344028502702713, loss=1.3947747945785522
I0313 06:13:17.838603 139664567539456 logging_writer.py:48] [153800] global_step=153800, grad_norm=0.23156563937664032, loss=1.4003090858459473
I0313 06:13:53.407991 139664575932160 logging_writer.py:48] [153900] global_step=153900, grad_norm=0.3399032652378082, loss=1.431992769241333
I0313 06:14:28.976625 139664567539456 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.23196753859519958, loss=1.4559075832366943
I0313 06:15:04.537007 139664575932160 logging_writer.py:48] [154100] global_step=154100, grad_norm=0.23192700743675232, loss=1.4202231168746948
I0313 06:15:40.128697 139664567539456 logging_writer.py:48] [154200] global_step=154200, grad_norm=0.24290452897548676, loss=1.5454152822494507
I0313 06:16:15.720213 139664575932160 logging_writer.py:48] [154300] global_step=154300, grad_norm=0.22709259390830994, loss=1.424221396446228
I0313 06:16:51.287538 139664567539456 logging_writer.py:48] [154400] global_step=154400, grad_norm=0.234614297747612, loss=1.4511654376983643
I0313 06:17:26.917466 139664575932160 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.23714426159858704, loss=1.400259017944336
I0313 06:18:02.498049 139664567539456 logging_writer.py:48] [154600] global_step=154600, grad_norm=0.2383377104997635, loss=1.4520493745803833
I0313 06:18:38.059737 139664575932160 logging_writer.py:48] [154700] global_step=154700, grad_norm=0.23571951687335968, loss=1.3378705978393555
I0313 06:19:13.660494 139664567539456 logging_writer.py:48] [154800] global_step=154800, grad_norm=0.22844688594341278, loss=1.4758062362670898
I0313 06:19:49.284632 139664575932160 logging_writer.py:48] [154900] global_step=154900, grad_norm=0.22468756139278412, loss=1.4330193996429443
I0313 06:20:24.889313 139664567539456 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.22949200868606567, loss=1.413148045539856
I0313 06:21:00.466612 139664575932160 logging_writer.py:48] [155100] global_step=155100, grad_norm=0.24230264127254486, loss=1.4859708547592163
I0313 06:21:36.086936 139664567539456 logging_writer.py:48] [155200] global_step=155200, grad_norm=0.24681884050369263, loss=1.4519611597061157
I0313 06:22:11.730675 139664575932160 logging_writer.py:48] [155300] global_step=155300, grad_norm=0.23962607979774475, loss=1.4717891216278076
I0313 06:22:47.325083 139664567539456 logging_writer.py:48] [155400] global_step=155400, grad_norm=0.24629276990890503, loss=1.5132277011871338
I0313 06:23:22.936293 139664575932160 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.22977960109710693, loss=1.4343072175979614
I0313 06:23:58.516511 139664567539456 logging_writer.py:48] [155600] global_step=155600, grad_norm=0.23149509727954865, loss=1.4677501916885376
I0313 06:24:34.117548 139664575932160 logging_writer.py:48] [155700] global_step=155700, grad_norm=0.22868776321411133, loss=1.4400382041931152
I0313 06:25:09.699459 139664567539456 logging_writer.py:48] [155800] global_step=155800, grad_norm=0.23089627921581268, loss=1.432182788848877
I0313 06:25:43.197434 139834281293632 spec.py:321] Evaluating on the training split.
I0313 06:25:46.164272 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:29:11.362946 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 06:29:14.028221 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:32:27.886079 139834281293632 spec.py:349] Evaluating on the test split.
I0313 06:32:30.557577 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:35:41.742811 139834281293632 submission_runner.py:420] Time since start: 95110.30s, 	Step: 155896, 	{'train/accuracy': 0.6948073506355286, 'train/loss': 1.376955270767212, 'train/bleu': 35.64392910309191, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 55483.48570561409, 'total_duration': 95110.29526019096, 'accumulated_submission_time': 55483.48570561409, 'accumulated_eval_time': 39619.749111652374, 'accumulated_logging_time': 2.3116683959960938}
I0313 06:35:41.786780 139664575932160 logging_writer.py:48] [155896] accumulated_eval_time=39619.749112, accumulated_logging_time=2.311668, accumulated_submission_time=55483.485706, global_step=155896, preemption_count=0, score=55483.485706, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=95110.295260, train/accuracy=0.694807, train/bleu=35.643929, train/loss=1.376955, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 06:35:43.586379 139664567539456 logging_writer.py:48] [155900] global_step=155900, grad_norm=0.23265500366687775, loss=1.5033631324768066
I0313 06:36:19.113273 139664575932160 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.23800590634346008, loss=1.4262489080429077
I0313 06:36:54.700459 139664567539456 logging_writer.py:48] [156100] global_step=156100, grad_norm=0.24054789543151855, loss=1.5524705648422241
I0313 06:37:30.288197 139664575932160 logging_writer.py:48] [156200] global_step=156200, grad_norm=0.23840810358524323, loss=1.5175575017929077
I0313 06:38:05.871683 139664567539456 logging_writer.py:48] [156300] global_step=156300, grad_norm=0.2191067934036255, loss=1.3145500421524048
I0313 06:38:41.459934 139664575932160 logging_writer.py:48] [156400] global_step=156400, grad_norm=0.22288455069065094, loss=1.3577990531921387
I0313 06:39:17.027158 139664567539456 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.22899460792541504, loss=1.4410932064056396
I0313 06:39:52.680140 139664575932160 logging_writer.py:48] [156600] global_step=156600, grad_norm=0.23532499372959137, loss=1.3671553134918213
I0313 06:40:28.293289 139664567539456 logging_writer.py:48] [156700] global_step=156700, grad_norm=0.23683394491672516, loss=1.4059762954711914
I0313 06:41:03.864063 139664575932160 logging_writer.py:48] [156800] global_step=156800, grad_norm=0.26783764362335205, loss=1.4332135915756226
I0313 06:41:39.446195 139664567539456 logging_writer.py:48] [156900] global_step=156900, grad_norm=0.2292022407054901, loss=1.460031270980835
I0313 06:42:15.035274 139664575932160 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.23658360540866852, loss=1.4855890274047852
I0313 06:42:50.651296 139664567539456 logging_writer.py:48] [157100] global_step=157100, grad_norm=0.2332119643688202, loss=1.4442342519760132
I0313 06:43:26.239872 139664575932160 logging_writer.py:48] [157200] global_step=157200, grad_norm=0.23575003445148468, loss=1.4621336460113525
I0313 06:44:01.863840 139664567539456 logging_writer.py:48] [157300] global_step=157300, grad_norm=0.23370733857154846, loss=1.372390627861023
I0313 06:44:37.454930 139664575932160 logging_writer.py:48] [157400] global_step=157400, grad_norm=0.23979689180850983, loss=1.4687548875808716
I0313 06:45:13.073603 139664567539456 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.2223135530948639, loss=1.4552987813949585
I0313 06:45:48.735825 139664575932160 logging_writer.py:48] [157600] global_step=157600, grad_norm=0.24037571251392365, loss=1.4749985933303833
I0313 06:46:24.348800 139664567539456 logging_writer.py:48] [157700] global_step=157700, grad_norm=0.23632094264030457, loss=1.4410780668258667
I0313 06:46:59.905921 139664575932160 logging_writer.py:48] [157800] global_step=157800, grad_norm=0.22813616693019867, loss=1.419699788093567
I0313 06:47:35.484742 139664567539456 logging_writer.py:48] [157900] global_step=157900, grad_norm=0.2320096641778946, loss=1.4747841358184814
I0313 06:48:11.096277 139664575932160 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.23144851624965668, loss=1.4302512407302856
I0313 06:48:46.688763 139664567539456 logging_writer.py:48] [158100] global_step=158100, grad_norm=0.23328717052936554, loss=1.4337878227233887
I0313 06:49:22.261333 139664575932160 logging_writer.py:48] [158200] global_step=158200, grad_norm=0.23545505106449127, loss=1.4532266855239868
I0313 06:49:41.911827 139834281293632 spec.py:321] Evaluating on the training split.
I0313 06:49:44.885890 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:53:16.643216 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 06:53:19.318579 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:56:33.095269 139834281293632 spec.py:349] Evaluating on the test split.
I0313 06:56:35.785620 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 06:59:46.535694 139834281293632 submission_runner.py:420] Time since start: 96555.09s, 	Step: 158257, 	{'train/accuracy': 0.6981350183486938, 'train/loss': 1.3524571657180786, 'train/bleu': 35.56427327468007, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 56323.527406692505, 'total_duration': 96555.0881664753, 'accumulated_submission_time': 56323.527406692505, 'accumulated_eval_time': 40224.372921705246, 'accumulated_logging_time': 2.366755485534668}
I0313 06:59:46.570486 139664567539456 logging_writer.py:48] [158257] accumulated_eval_time=40224.372922, accumulated_logging_time=2.366755, accumulated_submission_time=56323.527407, global_step=158257, preemption_count=0, score=56323.527407, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=96555.088166, train/accuracy=0.698135, train/bleu=35.564273, train/loss=1.352457, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 07:00:02.192399 139664575932160 logging_writer.py:48] [158300] global_step=158300, grad_norm=0.22938716411590576, loss=1.4887067079544067
I0313 07:00:37.695102 139664567539456 logging_writer.py:48] [158400] global_step=158400, grad_norm=0.2840462327003479, loss=1.436671257019043
I0313 07:01:13.237856 139664575932160 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.22774164378643036, loss=1.3553500175476074
I0313 07:01:48.801582 139664567539456 logging_writer.py:48] [158600] global_step=158600, grad_norm=0.23381300270557404, loss=1.458851933479309
I0313 07:02:24.406542 139664575932160 logging_writer.py:48] [158700] global_step=158700, grad_norm=0.2340782731771469, loss=1.4687777757644653
I0313 07:02:59.996740 139664567539456 logging_writer.py:48] [158800] global_step=158800, grad_norm=0.24683503806591034, loss=1.526778221130371
I0313 07:03:35.562240 139664575932160 logging_writer.py:48] [158900] global_step=158900, grad_norm=0.23295597732067108, loss=1.4068748950958252
I0313 07:04:11.117691 139664567539456 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.23724262416362762, loss=1.4978575706481934
I0313 07:04:46.695976 139664575932160 logging_writer.py:48] [159100] global_step=159100, grad_norm=0.23267747461795807, loss=1.4147945642471313
I0313 07:05:22.267616 139664567539456 logging_writer.py:48] [159200] global_step=159200, grad_norm=0.24313689768314362, loss=1.4377299547195435
I0313 07:05:57.847519 139664575932160 logging_writer.py:48] [159300] global_step=159300, grad_norm=0.2509136497974396, loss=1.5202882289886475
I0313 07:06:33.438831 139664567539456 logging_writer.py:48] [159400] global_step=159400, grad_norm=0.23404759168624878, loss=1.4206178188323975
I0313 07:07:09.010952 139664575932160 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.22491654753684998, loss=1.3700690269470215
I0313 07:07:44.615237 139664567539456 logging_writer.py:48] [159600] global_step=159600, grad_norm=0.22700433433055878, loss=1.515627384185791
I0313 07:08:20.212466 139664575932160 logging_writer.py:48] [159700] global_step=159700, grad_norm=0.2245330810546875, loss=1.4117339849472046
I0313 07:08:55.812827 139664567539456 logging_writer.py:48] [159800] global_step=159800, grad_norm=0.22544856369495392, loss=1.3980759382247925
I0313 07:09:31.395555 139664575932160 logging_writer.py:48] [159900] global_step=159900, grad_norm=0.2290497124195099, loss=1.4351133108139038
I0313 07:10:07.019383 139664567539456 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.22927017509937286, loss=1.428917646408081
I0313 07:10:42.646984 139664575932160 logging_writer.py:48] [160100] global_step=160100, grad_norm=0.2232530415058136, loss=1.410434603691101
I0313 07:11:18.271514 139664567539456 logging_writer.py:48] [160200] global_step=160200, grad_norm=0.24419184029102325, loss=1.530064344406128
I0313 07:11:53.866538 139664575932160 logging_writer.py:48] [160300] global_step=160300, grad_norm=0.23552513122558594, loss=1.4325881004333496
I0313 07:12:29.447375 139664567539456 logging_writer.py:48] [160400] global_step=160400, grad_norm=0.2345006763935089, loss=1.42610764503479
I0313 07:13:05.048563 139664575932160 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.22585062682628632, loss=1.4431414604187012
I0313 07:13:40.614472 139664567539456 logging_writer.py:48] [160600] global_step=160600, grad_norm=0.23967131972312927, loss=1.511852741241455
I0313 07:13:46.742037 139834281293632 spec.py:321] Evaluating on the training split.
I0313 07:13:49.705998 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 07:17:24.359788 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 07:17:27.055777 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 07:20:42.805868 139834281293632 spec.py:349] Evaluating on the test split.
I0313 07:20:45.483404 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 07:23:55.128558 139834281293632 submission_runner.py:420] Time since start: 98003.68s, 	Step: 160619, 	{'train/accuracy': 0.6961396336555481, 'train/loss': 1.3666020631790161, 'train/bleu': 35.31053510832538, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 57163.6197514534, 'total_duration': 98003.68104243279, 'accumulated_submission_time': 57163.6197514534, 'accumulated_eval_time': 40832.7593934536, 'accumulated_logging_time': 2.4104344844818115}
I0313 07:23:55.164674 139664575932160 logging_writer.py:48] [160619] accumulated_eval_time=40832.759393, accumulated_logging_time=2.410434, accumulated_submission_time=57163.619751, global_step=160619, preemption_count=0, score=57163.619751, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=98003.681042, train/accuracy=0.696140, train/bleu=35.310535, train/loss=1.366602, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 07:24:24.264859 139664567539456 logging_writer.py:48] [160700] global_step=160700, grad_norm=0.22994865477085114, loss=1.3845038414001465
I0313 07:24:59.822567 139664575932160 logging_writer.py:48] [160800] global_step=160800, grad_norm=0.24391600489616394, loss=1.4996148347854614
I0313 07:25:35.389850 139664567539456 logging_writer.py:48] [160900] global_step=160900, grad_norm=0.23420274257659912, loss=1.5578733682632446
I0313 07:26:10.973918 139664575932160 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.22868801653385162, loss=1.4097057580947876
I0313 07:26:46.565554 139664567539456 logging_writer.py:48] [161100] global_step=161100, grad_norm=0.22459536790847778, loss=1.4230642318725586
I0313 07:27:22.123109 139664575932160 logging_writer.py:48] [161200] global_step=161200, grad_norm=0.24006865918636322, loss=1.380772590637207
I0313 07:27:57.732007 139664567539456 logging_writer.py:48] [161300] global_step=161300, grad_norm=0.23495855927467346, loss=1.465395450592041
I0313 07:28:33.323972 139664575932160 logging_writer.py:48] [161400] global_step=161400, grad_norm=0.23324136435985565, loss=1.503368854522705
I0313 07:29:08.915356 139664567539456 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.22999992966651917, loss=1.4280674457550049
I0313 07:29:44.469957 139664575932160 logging_writer.py:48] [161600] global_step=161600, grad_norm=0.23711444437503815, loss=1.473841905593872
I0313 07:30:20.083137 139664567539456 logging_writer.py:48] [161700] global_step=161700, grad_norm=0.22998769581317902, loss=1.3814704418182373
I0313 07:30:55.686805 139664575932160 logging_writer.py:48] [161800] global_step=161800, grad_norm=0.3010229170322418, loss=1.4903472661972046
I0313 07:31:31.281831 139664567539456 logging_writer.py:48] [161900] global_step=161900, grad_norm=0.2654910683631897, loss=1.5724709033966064
I0313 07:32:06.913085 139664575932160 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.23528918623924255, loss=1.4077028036117554
I0313 07:32:42.475650 139664567539456 logging_writer.py:48] [162100] global_step=162100, grad_norm=0.2457524836063385, loss=1.4494184255599976
I0313 07:33:18.045768 139664575932160 logging_writer.py:48] [162200] global_step=162200, grad_norm=0.23210914433002472, loss=1.4329856634140015
I0313 07:33:53.663688 139664567539456 logging_writer.py:48] [162300] global_step=162300, grad_norm=0.22732774913311005, loss=1.4397153854370117
I0313 07:34:29.224519 139664575932160 logging_writer.py:48] [162400] global_step=162400, grad_norm=0.2335495948791504, loss=1.4628709554672241
I0313 07:35:04.811637 139664567539456 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.24796296656131744, loss=1.5141311883926392
I0313 07:35:40.416746 139664575932160 logging_writer.py:48] [162600] global_step=162600, grad_norm=0.2411639541387558, loss=1.5098387002944946
I0313 07:36:16.049757 139664567539456 logging_writer.py:48] [162700] global_step=162700, grad_norm=0.2603561580181122, loss=1.467511773109436
I0313 07:36:51.618160 139664575932160 logging_writer.py:48] [162800] global_step=162800, grad_norm=0.23487743735313416, loss=1.4655959606170654
I0313 07:37:27.209096 139664567539456 logging_writer.py:48] [162900] global_step=162900, grad_norm=0.22562441229820251, loss=1.4620152711868286
I0313 07:37:55.377264 139834281293632 spec.py:321] Evaluating on the training split.
I0313 07:37:58.349280 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 07:41:29.520075 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 07:41:32.195591 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 07:44:47.262693 139834281293632 spec.py:349] Evaluating on the test split.
I0313 07:44:49.948233 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 07:47:59.808897 139834281293632 submission_runner.py:420] Time since start: 99448.36s, 	Step: 162981, 	{'train/accuracy': 0.6966844201087952, 'train/loss': 1.3696157932281494, 'train/bleu': 35.52774077610771, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 58003.75202512741, 'total_duration': 99448.36136484146, 'accumulated_submission_time': 58003.75202512741, 'accumulated_eval_time': 41437.19096302986, 'accumulated_logging_time': 2.4548990726470947}
I0313 07:47:59.844827 139664575932160 logging_writer.py:48] [162981] accumulated_eval_time=41437.190963, accumulated_logging_time=2.454899, accumulated_submission_time=58003.752025, global_step=162981, preemption_count=0, score=58003.752025, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=99448.361365, train/accuracy=0.696684, train/bleu=35.527741, train/loss=1.369616, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 07:48:06.962577 139664567539456 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.24158290028572083, loss=1.4587905406951904
I0313 07:48:42.476139 139664575932160 logging_writer.py:48] [163100] global_step=163100, grad_norm=0.23220790922641754, loss=1.511659026145935
I0313 07:49:18.058499 139664567539456 logging_writer.py:48] [163200] global_step=163200, grad_norm=0.2392241656780243, loss=1.515307903289795
I0313 07:49:53.661378 139664575932160 logging_writer.py:48] [163300] global_step=163300, grad_norm=0.22719106078147888, loss=1.4900169372558594
I0313 07:50:29.273772 139664567539456 logging_writer.py:48] [163400] global_step=163400, grad_norm=0.2332575023174286, loss=1.4919788837432861
I0313 07:51:04.881346 139664575932160 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.23083528876304626, loss=1.498137354850769
I0313 07:51:40.466989 139664567539456 logging_writer.py:48] [163600] global_step=163600, grad_norm=0.2382408082485199, loss=1.4925484657287598
I0313 07:52:16.047526 139664575932160 logging_writer.py:48] [163700] global_step=163700, grad_norm=0.2270336002111435, loss=1.4359545707702637
I0313 07:52:51.645010 139664567539456 logging_writer.py:48] [163800] global_step=163800, grad_norm=0.22574372589588165, loss=1.4014766216278076
I0313 07:53:27.267969 139664575932160 logging_writer.py:48] [163900] global_step=163900, grad_norm=0.23744267225265503, loss=1.4775199890136719
I0313 07:54:02.882817 139664567539456 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.24046435952186584, loss=1.4514505863189697
I0313 07:54:38.479670 139664575932160 logging_writer.py:48] [164100] global_step=164100, grad_norm=0.22535845637321472, loss=1.422658920288086
I0313 07:55:14.101209 139664567539456 logging_writer.py:48] [164200] global_step=164200, grad_norm=0.23207934200763702, loss=1.421359658241272
I0313 07:55:49.727544 139664575932160 logging_writer.py:48] [164300] global_step=164300, grad_norm=0.2457222044467926, loss=1.4014085531234741
I0313 07:56:25.319200 139664567539456 logging_writer.py:48] [164400] global_step=164400, grad_norm=0.2293960601091385, loss=1.4379994869232178
I0313 07:57:00.919974 139664575932160 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.2316112369298935, loss=1.4804238080978394
I0313 07:57:36.491885 139664567539456 logging_writer.py:48] [164600] global_step=164600, grad_norm=0.24935898184776306, loss=1.4673131704330444
I0313 07:58:12.057237 139664575932160 logging_writer.py:48] [164700] global_step=164700, grad_norm=0.23161309957504272, loss=1.4522091150283813
I0313 07:58:47.675938 139664567539456 logging_writer.py:48] [164800] global_step=164800, grad_norm=0.2261185348033905, loss=1.453833818435669
I0313 07:59:23.293405 139664575932160 logging_writer.py:48] [164900] global_step=164900, grad_norm=0.22933782637119293, loss=1.4782794713974
I0313 07:59:58.901914 139664567539456 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.2276773750782013, loss=1.371943712234497
I0313 08:00:34.469412 139664575932160 logging_writer.py:48] [165100] global_step=165100, grad_norm=0.25056731700897217, loss=1.5040374994277954
I0313 08:01:10.072384 139664567539456 logging_writer.py:48] [165200] global_step=165200, grad_norm=0.23413875699043274, loss=1.450292706489563
I0313 08:01:45.666591 139664575932160 logging_writer.py:48] [165300] global_step=165300, grad_norm=0.21803158521652222, loss=1.4098762273788452
I0313 08:01:59.981682 139834281293632 spec.py:321] Evaluating on the training split.
I0313 08:02:02.966977 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:05:39.163837 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 08:05:41.835607 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:08:56.019845 139834281293632 spec.py:349] Evaluating on the test split.
I0313 08:08:58.695938 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:12:08.252162 139834281293632 submission_runner.py:420] Time since start: 100896.80s, 	Step: 165342, 	{'train/accuracy': 0.6953707337379456, 'train/loss': 1.377099871635437, 'train/bleu': 35.801689686697244, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 58843.80774831772, 'total_duration': 100896.804625988, 'accumulated_submission_time': 58843.80774831772, 'accumulated_eval_time': 42045.46137666702, 'accumulated_logging_time': 2.499814748764038}
I0313 08:12:08.289399 139664567539456 logging_writer.py:48] [165342] accumulated_eval_time=42045.461377, accumulated_logging_time=2.499815, accumulated_submission_time=58843.807748, global_step=165342, preemption_count=0, score=58843.807748, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=100896.804626, train/accuracy=0.695371, train/bleu=35.801690, train/loss=1.377100, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 08:12:29.232293 139664575932160 logging_writer.py:48] [165400] global_step=165400, grad_norm=0.2356133610010147, loss=1.506662368774414
I0313 08:13:04.752121 139664567539456 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.24601811170578003, loss=1.4186487197875977
I0313 08:13:40.348772 139664575932160 logging_writer.py:48] [165600] global_step=165600, grad_norm=0.23644575476646423, loss=1.4787765741348267
I0313 08:14:15.947867 139664567539456 logging_writer.py:48] [165700] global_step=165700, grad_norm=0.23326823115348816, loss=1.4736599922180176
I0313 08:14:51.608144 139664575932160 logging_writer.py:48] [165800] global_step=165800, grad_norm=0.2357562631368637, loss=1.487243413925171
I0313 08:15:27.233555 139664567539456 logging_writer.py:48] [165900] global_step=165900, grad_norm=0.23690102994441986, loss=1.388182520866394
I0313 08:16:02.903288 139664575932160 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.24042783677577972, loss=1.3971089124679565
I0313 08:16:38.548109 139664567539456 logging_writer.py:48] [166100] global_step=166100, grad_norm=0.2352782040834427, loss=1.4571589231491089
I0313 08:17:14.208288 139664575932160 logging_writer.py:48] [166200] global_step=166200, grad_norm=0.22806736826896667, loss=1.3762143850326538
I0313 08:17:49.799784 139664567539456 logging_writer.py:48] [166300] global_step=166300, grad_norm=0.22672288119792938, loss=1.3324358463287354
I0313 08:18:25.388795 139664575932160 logging_writer.py:48] [166400] global_step=166400, grad_norm=0.24083270132541656, loss=1.4370381832122803
I0313 08:19:00.988633 139664567539456 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.23056645691394806, loss=1.451307773590088
I0313 08:19:36.604089 139664575932160 logging_writer.py:48] [166600] global_step=166600, grad_norm=0.23159492015838623, loss=1.4290803670883179
I0313 08:20:12.206499 139664567539456 logging_writer.py:48] [166700] global_step=166700, grad_norm=0.23906761407852173, loss=1.4272745847702026
I0313 08:20:47.786321 139664575932160 logging_writer.py:48] [166800] global_step=166800, grad_norm=0.23834174871444702, loss=1.4667108058929443
I0313 08:21:23.373853 139664567539456 logging_writer.py:48] [166900] global_step=166900, grad_norm=0.22826972603797913, loss=1.4617575407028198
I0313 08:21:58.990497 139664575932160 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.23084448277950287, loss=1.42073392868042
I0313 08:22:34.560653 139664567539456 logging_writer.py:48] [167100] global_step=167100, grad_norm=0.2435101866722107, loss=1.5435069799423218
I0313 08:23:10.185137 139664575932160 logging_writer.py:48] [167200] global_step=167200, grad_norm=0.24111175537109375, loss=1.449229121208191
I0313 08:23:45.792827 139664567539456 logging_writer.py:48] [167300] global_step=167300, grad_norm=0.22458884119987488, loss=1.3706198930740356
I0313 08:24:21.371423 139664575932160 logging_writer.py:48] [167400] global_step=167400, grad_norm=0.2252013385295868, loss=1.437817931175232
I0313 08:24:56.979851 139664567539456 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.25162968039512634, loss=1.502415418624878
I0313 08:25:32.633209 139664575932160 logging_writer.py:48] [167600] global_step=167600, grad_norm=0.22842787206172943, loss=1.4347333908081055
I0313 08:26:08.287197 139664567539456 logging_writer.py:48] [167700] global_step=167700, grad_norm=0.22669601440429688, loss=1.4289186000823975
I0313 08:26:08.294556 139834281293632 spec.py:321] Evaluating on the training split.
I0313 08:26:10.991688 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:29:47.899082 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 08:29:50.573464 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:33:04.611950 139834281293632 spec.py:349] Evaluating on the test split.
I0313 08:33:07.284616 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:36:17.172945 139834281293632 submission_runner.py:420] Time since start: 102345.73s, 	Step: 167701, 	{'train/accuracy': 0.6936147212982178, 'train/loss': 1.382083773612976, 'train/bleu': 35.36641362085959, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 59683.72746658325, 'total_duration': 102345.72543001175, 'accumulated_submission_time': 59683.72746658325, 'accumulated_eval_time': 42654.33971261978, 'accumulated_logging_time': 2.5470104217529297}
I0313 08:36:17.209334 139664575932160 logging_writer.py:48] [167701] accumulated_eval_time=42654.339713, accumulated_logging_time=2.547010, accumulated_submission_time=59683.727467, global_step=167701, preemption_count=0, score=59683.727467, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=102345.725430, train/accuracy=0.693615, train/bleu=35.366414, train/loss=1.382084, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 08:36:52.723160 139664567539456 logging_writer.py:48] [167800] global_step=167800, grad_norm=0.23909802734851837, loss=1.3978030681610107
I0313 08:37:28.262124 139664575932160 logging_writer.py:48] [167900] global_step=167900, grad_norm=0.23152123391628265, loss=1.4698737859725952
I0313 08:38:03.832638 139664567539456 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.22426769137382507, loss=1.393983006477356
I0313 08:38:39.409537 139664575932160 logging_writer.py:48] [168100] global_step=168100, grad_norm=0.23126566410064697, loss=1.3809095621109009
I0313 08:39:15.032516 139664567539456 logging_writer.py:48] [168200] global_step=168200, grad_norm=0.2255333811044693, loss=1.4261656999588013
I0313 08:39:50.647945 139664575932160 logging_writer.py:48] [168300] global_step=168300, grad_norm=0.22421805560588837, loss=1.4654269218444824
I0313 08:40:26.264432 139664567539456 logging_writer.py:48] [168400] global_step=168400, grad_norm=0.23743502795696259, loss=1.4932845830917358
I0313 08:41:01.821016 139664575932160 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.2395179718732834, loss=1.4461981058120728
I0313 08:41:37.395035 139664567539456 logging_writer.py:48] [168600] global_step=168600, grad_norm=0.2259208709001541, loss=1.4159621000289917
I0313 08:42:12.974030 139664575932160 logging_writer.py:48] [168700] global_step=168700, grad_norm=0.24133706092834473, loss=1.4650225639343262
I0313 08:42:48.538253 139664567539456 logging_writer.py:48] [168800] global_step=168800, grad_norm=0.24121224880218506, loss=1.4391731023788452
I0313 08:43:24.131978 139664575932160 logging_writer.py:48] [168900] global_step=168900, grad_norm=0.24223627150058746, loss=1.412772536277771
I0313 08:43:59.702224 139664567539456 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.22991995513439178, loss=1.4771426916122437
I0313 08:44:35.248795 139664575932160 logging_writer.py:48] [169100] global_step=169100, grad_norm=0.23056377470493317, loss=1.414604663848877
I0313 08:45:10.835959 139664567539456 logging_writer.py:48] [169200] global_step=169200, grad_norm=0.25573307275772095, loss=1.4710972309112549
I0313 08:45:46.400159 139664575932160 logging_writer.py:48] [169300] global_step=169300, grad_norm=0.22252877056598663, loss=1.454667091369629
I0313 08:46:22.010357 139664567539456 logging_writer.py:48] [169400] global_step=169400, grad_norm=0.22718416154384613, loss=1.4458855390548706
I0313 08:46:57.598512 139664575932160 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.23785147070884705, loss=1.4287248849868774
I0313 08:47:33.200390 139664567539456 logging_writer.py:48] [169600] global_step=169600, grad_norm=0.23360885679721832, loss=1.353528380393982
I0313 08:48:08.783470 139664575932160 logging_writer.py:48] [169700] global_step=169700, grad_norm=0.23958832025527954, loss=1.4386529922485352
I0313 08:48:44.351379 139664567539456 logging_writer.py:48] [169800] global_step=169800, grad_norm=0.2411261349916458, loss=1.4784902334213257
I0313 08:49:19.924481 139664575932160 logging_writer.py:48] [169900] global_step=169900, grad_norm=0.23316578567028046, loss=1.4021880626678467
I0313 08:49:55.507001 139664567539456 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.2380296289920807, loss=1.4097542762756348
I0313 08:50:17.296851 139834281293632 spec.py:321] Evaluating on the training split.
I0313 08:50:20.268404 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:53:35.492841 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 08:53:38.161961 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 08:56:51.962674 139834281293632 spec.py:349] Evaluating on the test split.
I0313 08:56:54.634556 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:00:04.419650 139834281293632 submission_runner.py:420] Time since start: 103772.97s, 	Step: 170063, 	{'train/accuracy': 0.6958931684494019, 'train/loss': 1.3654128313064575, 'train/bleu': 35.463748492037126, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 60523.73269152641, 'total_duration': 103772.97212171555, 'accumulated_submission_time': 60523.73269152641, 'accumulated_eval_time': 43241.46246051788, 'accumulated_logging_time': 2.5924875736236572}
I0313 09:00:04.455792 139664575932160 logging_writer.py:48] [170063] accumulated_eval_time=43241.462461, accumulated_logging_time=2.592488, accumulated_submission_time=60523.732692, global_step=170063, preemption_count=0, score=60523.732692, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=103772.972122, train/accuracy=0.695893, train/bleu=35.463748, train/loss=1.365413, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 09:00:17.969049 139664567539456 logging_writer.py:48] [170100] global_step=170100, grad_norm=0.2346259206533432, loss=1.348446011543274
I0313 09:00:53.491678 139664575932160 logging_writer.py:48] [170200] global_step=170200, grad_norm=0.23838452994823456, loss=1.436248779296875
I0313 09:01:29.088194 139664567539456 logging_writer.py:48] [170300] global_step=170300, grad_norm=0.23046962916851044, loss=1.490358591079712
I0313 09:02:04.662028 139664575932160 logging_writer.py:48] [170400] global_step=170400, grad_norm=0.23803365230560303, loss=1.5674278736114502
I0313 09:02:40.305655 139664567539456 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.23301957547664642, loss=1.3831160068511963
I0313 09:03:15.931089 139664575932160 logging_writer.py:48] [170600] global_step=170600, grad_norm=0.23753400146961212, loss=1.5209041833877563
I0313 09:03:51.509944 139664567539456 logging_writer.py:48] [170700] global_step=170700, grad_norm=0.22675596177577972, loss=1.413491129875183
I0313 09:04:27.110485 139664575932160 logging_writer.py:48] [170800] global_step=170800, grad_norm=0.24302881956100464, loss=1.4353392124176025
I0313 09:05:02.692478 139664567539456 logging_writer.py:48] [170900] global_step=170900, grad_norm=0.23330897092819214, loss=1.4103717803955078
I0313 09:05:38.275685 139664575932160 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.23829713463783264, loss=1.482591986656189
I0313 09:06:13.854655 139664567539456 logging_writer.py:48] [171100] global_step=171100, grad_norm=0.22667890787124634, loss=1.4268311262130737
I0313 09:06:49.441740 139664575932160 logging_writer.py:48] [171200] global_step=171200, grad_norm=0.23880887031555176, loss=1.466554880142212
I0313 09:07:25.022763 139664567539456 logging_writer.py:48] [171300] global_step=171300, grad_norm=0.23213276267051697, loss=1.424971342086792
I0313 09:08:00.629080 139664575932160 logging_writer.py:48] [171400] global_step=171400, grad_norm=0.23987281322479248, loss=1.4022730588912964
I0313 09:08:36.220302 139664567539456 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.24029114842414856, loss=1.5137451887130737
I0313 09:09:11.796344 139664575932160 logging_writer.py:48] [171600] global_step=171600, grad_norm=0.23673860728740692, loss=1.393966794013977
I0313 09:09:47.378115 139664567539456 logging_writer.py:48] [171700] global_step=171700, grad_norm=0.23710694909095764, loss=1.4017386436462402
I0313 09:10:22.994319 139664575932160 logging_writer.py:48] [171800] global_step=171800, grad_norm=0.23441879451274872, loss=1.440301537513733
I0313 09:10:58.602518 139664567539456 logging_writer.py:48] [171900] global_step=171900, grad_norm=0.23672257363796234, loss=1.4314380884170532
I0313 09:11:34.220131 139664575932160 logging_writer.py:48] [172000] global_step=172000, grad_norm=0.2255745679140091, loss=1.425925850868225
I0313 09:12:09.807608 139664567539456 logging_writer.py:48] [172100] global_step=172100, grad_norm=0.23454003036022186, loss=1.4229968786239624
I0313 09:12:45.420250 139664575932160 logging_writer.py:48] [172200] global_step=172200, grad_norm=0.24613326787948608, loss=1.5239207744598389
I0313 09:13:21.030327 139664567539456 logging_writer.py:48] [172300] global_step=172300, grad_norm=0.23645702004432678, loss=1.4137507677078247
I0313 09:13:56.668791 139664575932160 logging_writer.py:48] [172400] global_step=172400, grad_norm=0.2281094193458557, loss=1.436208724975586
I0313 09:14:04.578244 139834281293632 spec.py:321] Evaluating on the training split.
I0313 09:14:07.578814 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:17:39.061854 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 09:17:41.743653 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:20:55.762880 139834281293632 spec.py:349] Evaluating on the test split.
I0313 09:20:58.438940 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:24:08.104556 139834281293632 submission_runner.py:420] Time since start: 105216.66s, 	Step: 172424, 	{'train/accuracy': 0.6973316669464111, 'train/loss': 1.3603744506835938, 'train/bleu': 35.595653273466674, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 61363.77395796776, 'total_duration': 105216.65703773499, 'accumulated_submission_time': 61363.77395796776, 'accumulated_eval_time': 43844.98872923851, 'accumulated_logging_time': 2.6387388706207275}
I0313 09:24:08.142498 139664567539456 logging_writer.py:48] [172424] accumulated_eval_time=43844.988729, accumulated_logging_time=2.638739, accumulated_submission_time=61363.773958, global_step=172424, preemption_count=0, score=61363.773958, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=105216.657038, train/accuracy=0.697332, train/bleu=35.595653, train/loss=1.360374, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 09:24:35.497026 139664575932160 logging_writer.py:48] [172500] global_step=172500, grad_norm=0.22687280178070068, loss=1.4213247299194336
I0313 09:25:11.025542 139664567539456 logging_writer.py:48] [172600] global_step=172600, grad_norm=0.2288689911365509, loss=1.4304137229919434
I0313 09:25:46.633700 139664575932160 logging_writer.py:48] [172700] global_step=172700, grad_norm=0.24329598248004913, loss=1.4013915061950684
I0313 09:26:22.247726 139664567539456 logging_writer.py:48] [172800] global_step=172800, grad_norm=0.2341753989458084, loss=1.4773060083389282
I0313 09:26:57.867107 139664575932160 logging_writer.py:48] [172900] global_step=172900, grad_norm=0.2287185788154602, loss=1.3295862674713135
I0313 09:27:33.446258 139664567539456 logging_writer.py:48] [173000] global_step=173000, grad_norm=0.2463599145412445, loss=1.4668245315551758
I0313 09:28:09.031552 139664575932160 logging_writer.py:48] [173100] global_step=173100, grad_norm=0.23809008300304413, loss=1.468637466430664
I0313 09:28:44.618443 139664567539456 logging_writer.py:48] [173200] global_step=173200, grad_norm=0.23774059116840363, loss=1.4111509323120117
I0313 09:29:20.219017 139664575932160 logging_writer.py:48] [173300] global_step=173300, grad_norm=0.22803561389446259, loss=1.419012188911438
I0313 09:29:55.797917 139664567539456 logging_writer.py:48] [173400] global_step=173400, grad_norm=0.23310184478759766, loss=1.4645408391952515
I0313 09:30:31.361350 139664575932160 logging_writer.py:48] [173500] global_step=173500, grad_norm=0.23648656904697418, loss=1.3782997131347656
I0313 09:31:06.981878 139664567539456 logging_writer.py:48] [173600] global_step=173600, grad_norm=0.24011710286140442, loss=1.418250322341919
I0313 09:31:42.586316 139664575932160 logging_writer.py:48] [173700] global_step=173700, grad_norm=0.23228633403778076, loss=1.4409767389297485
I0313 09:32:18.225163 139664567539456 logging_writer.py:48] [173800] global_step=173800, grad_norm=0.235322967171669, loss=1.4928619861602783
I0313 09:32:53.837515 139664575932160 logging_writer.py:48] [173900] global_step=173900, grad_norm=0.23708663880825043, loss=1.48777437210083
I0313 09:33:29.481194 139664567539456 logging_writer.py:48] [174000] global_step=174000, grad_norm=0.23619836568832397, loss=1.4255378246307373
I0313 09:34:05.056395 139664575932160 logging_writer.py:48] [174100] global_step=174100, grad_norm=0.23754340410232544, loss=1.4324383735656738
I0313 09:34:40.625559 139664567539456 logging_writer.py:48] [174200] global_step=174200, grad_norm=0.2356623113155365, loss=1.461086392402649
I0313 09:35:16.205346 139664575932160 logging_writer.py:48] [174300] global_step=174300, grad_norm=0.23036225140094757, loss=1.428431749343872
I0313 09:35:51.837288 139664567539456 logging_writer.py:48] [174400] global_step=174400, grad_norm=0.23294207453727722, loss=1.4003597497940063
I0313 09:36:27.432560 139664575932160 logging_writer.py:48] [174500] global_step=174500, grad_norm=0.2311239242553711, loss=1.4281429052352905
I0313 09:37:03.027013 139664567539456 logging_writer.py:48] [174600] global_step=174600, grad_norm=0.2260591983795166, loss=1.3642340898513794
I0313 09:37:38.628169 139664575932160 logging_writer.py:48] [174700] global_step=174700, grad_norm=0.23265032470226288, loss=1.4337263107299805
I0313 09:38:08.242538 139834281293632 spec.py:321] Evaluating on the training split.
I0313 09:38:11.214417 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:41:41.155508 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 09:41:43.825283 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:44:57.701498 139834281293632 spec.py:349] Evaluating on the test split.
I0313 09:45:00.367136 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 09:48:10.884765 139834281293632 submission_runner.py:420] Time since start: 106659.44s, 	Step: 174785, 	{'train/accuracy': 0.6968789100646973, 'train/loss': 1.3602083921432495, 'train/bleu': 35.25066549945658, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 62203.792006254196, 'total_duration': 106659.43720054626, 'accumulated_submission_time': 62203.792006254196, 'accumulated_eval_time': 44447.63086080551, 'accumulated_logging_time': 2.6856791973114014}
I0313 09:48:10.932199 139664567539456 logging_writer.py:48] [174785] accumulated_eval_time=44447.630861, accumulated_logging_time=2.685679, accumulated_submission_time=62203.792006, global_step=174785, preemption_count=0, score=62203.792006, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=106659.437201, train/accuracy=0.696879, train/bleu=35.250665, train/loss=1.360208, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 09:48:16.649134 139664575932160 logging_writer.py:48] [174800] global_step=174800, grad_norm=0.2305549830198288, loss=1.416439175605774
I0313 09:48:52.175346 139664567539456 logging_writer.py:48] [174900] global_step=174900, grad_norm=0.24350574612617493, loss=1.5020300149917603
I0313 09:49:27.765123 139664575932160 logging_writer.py:48] [175000] global_step=175000, grad_norm=0.22588133811950684, loss=1.4322444200515747
I0313 09:50:03.374344 139664567539456 logging_writer.py:48] [175100] global_step=175100, grad_norm=0.24291259050369263, loss=1.4480901956558228
I0313 09:50:38.947687 139664575932160 logging_writer.py:48] [175200] global_step=175200, grad_norm=0.2258071005344391, loss=1.4119207859039307
I0313 09:51:14.562952 139664567539456 logging_writer.py:48] [175300] global_step=175300, grad_norm=0.2482011765241623, loss=1.458277940750122
I0313 09:51:50.153814 139664575932160 logging_writer.py:48] [175400] global_step=175400, grad_norm=0.23418182134628296, loss=1.4189544916152954
I0313 09:52:25.765122 139664567539456 logging_writer.py:48] [175500] global_step=175500, grad_norm=0.2351221740245819, loss=1.4444630146026611
I0313 09:53:01.383915 139664575932160 logging_writer.py:48] [175600] global_step=175600, grad_norm=0.22889147698879242, loss=1.3595585823059082
I0313 09:53:36.975142 139664567539456 logging_writer.py:48] [175700] global_step=175700, grad_norm=0.25024887919425964, loss=1.4876457452774048
I0313 09:54:12.544491 139664575932160 logging_writer.py:48] [175800] global_step=175800, grad_norm=0.2267671674489975, loss=1.3924037218093872
I0313 09:54:48.103320 139664567539456 logging_writer.py:48] [175900] global_step=175900, grad_norm=0.2322533279657364, loss=1.4468480348587036
I0313 09:55:23.740256 139664575932160 logging_writer.py:48] [176000] global_step=176000, grad_norm=0.2326803058385849, loss=1.4783605337142944
I0313 09:55:59.360575 139664567539456 logging_writer.py:48] [176100] global_step=176100, grad_norm=0.24983422458171844, loss=1.4648305177688599
I0313 09:56:34.982335 139664575932160 logging_writer.py:48] [176200] global_step=176200, grad_norm=0.23885837197303772, loss=1.4723421335220337
I0313 09:57:10.564199 139664567539456 logging_writer.py:48] [176300] global_step=176300, grad_norm=0.2382243275642395, loss=1.429717779159546
I0313 09:57:46.128970 139664575932160 logging_writer.py:48] [176400] global_step=176400, grad_norm=0.2234625667333603, loss=1.3647922277450562
I0313 09:58:21.730143 139664567539456 logging_writer.py:48] [176500] global_step=176500, grad_norm=0.23131981492042542, loss=1.4702001810073853
I0313 09:58:57.326647 139664575932160 logging_writer.py:48] [176600] global_step=176600, grad_norm=0.23836424946784973, loss=1.4363499879837036
I0313 09:59:32.924829 139664567539456 logging_writer.py:48] [176700] global_step=176700, grad_norm=0.23478005826473236, loss=1.3998936414718628
I0313 10:00:08.506821 139664575932160 logging_writer.py:48] [176800] global_step=176800, grad_norm=0.2254747599363327, loss=1.3534315824508667
I0313 10:00:44.106094 139664567539456 logging_writer.py:48] [176900] global_step=176900, grad_norm=0.24464213848114014, loss=1.3958187103271484
I0313 10:01:19.696337 139664575932160 logging_writer.py:48] [177000] global_step=177000, grad_norm=0.24030712246894836, loss=1.4560973644256592
I0313 10:01:55.306393 139664567539456 logging_writer.py:48] [177100] global_step=177100, grad_norm=0.2339843511581421, loss=1.4881452322006226
I0313 10:02:11.047196 139834281293632 spec.py:321] Evaluating on the training split.
I0313 10:02:14.023089 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:05:42.715591 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 10:05:45.374849 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:08:59.283663 139834281293632 spec.py:349] Evaluating on the test split.
I0313 10:09:01.969097 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:12:11.808186 139834281293632 submission_runner.py:420] Time since start: 108100.36s, 	Step: 177146, 	{'train/accuracy': 0.6965775489807129, 'train/loss': 1.3650387525558472, 'train/bleu': 35.620679245105535, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 63043.82459115982, 'total_duration': 108100.36066675186, 'accumulated_submission_time': 63043.82459115982, 'accumulated_eval_time': 45048.39179825783, 'accumulated_logging_time': 2.7433371543884277}
I0313 10:12:11.847378 139664575932160 logging_writer.py:48] [177146] accumulated_eval_time=45048.391798, accumulated_logging_time=2.743337, accumulated_submission_time=63043.824591, global_step=177146, preemption_count=0, score=63043.824591, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=108100.360667, train/accuracy=0.696578, train/bleu=35.620679, train/loss=1.365039, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 10:12:31.379924 139664567539456 logging_writer.py:48] [177200] global_step=177200, grad_norm=0.2351405918598175, loss=1.3946677446365356
I0313 10:13:06.889233 139664575932160 logging_writer.py:48] [177300] global_step=177300, grad_norm=0.23197925090789795, loss=1.4596598148345947
I0313 10:13:42.456470 139664567539456 logging_writer.py:48] [177400] global_step=177400, grad_norm=0.2416977435350418, loss=1.475117802619934
I0313 10:14:18.038109 139664575932160 logging_writer.py:48] [177500] global_step=177500, grad_norm=0.23659205436706543, loss=1.3998024463653564
I0313 10:14:53.641046 139664567539456 logging_writer.py:48] [177600] global_step=177600, grad_norm=0.24137596786022186, loss=1.4631028175354004
I0313 10:15:29.231131 139664575932160 logging_writer.py:48] [177700] global_step=177700, grad_norm=0.22984305024147034, loss=1.4447517395019531
I0313 10:16:04.811423 139664567539456 logging_writer.py:48] [177800] global_step=177800, grad_norm=0.22886516153812408, loss=1.4631036520004272
I0313 10:16:40.405157 139664575932160 logging_writer.py:48] [177900] global_step=177900, grad_norm=0.23872990906238556, loss=1.4863919019699097
I0313 10:17:15.985604 139664567539456 logging_writer.py:48] [178000] global_step=178000, grad_norm=0.24179737269878387, loss=1.499693512916565
I0313 10:17:51.627561 139664575932160 logging_writer.py:48] [178100] global_step=178100, grad_norm=0.24073342978954315, loss=1.419345736503601
I0313 10:18:27.260224 139664567539456 logging_writer.py:48] [178200] global_step=178200, grad_norm=0.2401827722787857, loss=1.5335185527801514
I0313 10:19:02.883462 139664575932160 logging_writer.py:48] [178300] global_step=178300, grad_norm=0.23503559827804565, loss=1.442784070968628
I0313 10:19:38.472280 139664567539456 logging_writer.py:48] [178400] global_step=178400, grad_norm=0.22089675068855286, loss=1.451501488685608
I0313 10:20:14.062719 139664575932160 logging_writer.py:48] [178500] global_step=178500, grad_norm=0.24177950620651245, loss=1.473565697669983
I0313 10:20:49.636777 139664567539456 logging_writer.py:48] [178600] global_step=178600, grad_norm=0.22113052010536194, loss=1.3533878326416016
I0313 10:21:25.256532 139664575932160 logging_writer.py:48] [178700] global_step=178700, grad_norm=0.23297984898090363, loss=1.4339224100112915
I0313 10:22:00.854110 139664567539456 logging_writer.py:48] [178800] global_step=178800, grad_norm=0.23722079396247864, loss=1.4305732250213623
I0313 10:22:36.442086 139664575932160 logging_writer.py:48] [178900] global_step=178900, grad_norm=0.24039487540721893, loss=1.5056912899017334
I0313 10:23:11.997440 139664567539456 logging_writer.py:48] [179000] global_step=179000, grad_norm=0.23308512568473816, loss=1.4856780767440796
I0313 10:23:47.578693 139664575932160 logging_writer.py:48] [179100] global_step=179100, grad_norm=0.23239584267139435, loss=1.4479997158050537
I0313 10:24:23.165238 139664567539456 logging_writer.py:48] [179200] global_step=179200, grad_norm=0.24088306725025177, loss=1.4015114307403564
I0313 10:24:58.744127 139664575932160 logging_writer.py:48] [179300] global_step=179300, grad_norm=0.23983503878116608, loss=1.4606974124908447
I0313 10:25:34.336768 139664567539456 logging_writer.py:48] [179400] global_step=179400, grad_norm=0.2407349944114685, loss=1.4295704364776611
I0313 10:26:09.951516 139664575932160 logging_writer.py:48] [179500] global_step=179500, grad_norm=0.24557983875274658, loss=1.501569151878357
I0313 10:26:12.159569 139834281293632 spec.py:321] Evaluating on the training split.
I0313 10:26:15.138082 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:29:48.726016 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 10:29:51.402885 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:33:06.495498 139834281293632 spec.py:349] Evaluating on the test split.
I0313 10:33:09.176456 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:36:19.073976 139834281293632 submission_runner.py:420] Time since start: 109547.63s, 	Step: 179508, 	{'train/accuracy': 0.6962001919746399, 'train/loss': 1.3616358041763306, 'train/bleu': 35.35056694507423, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 63884.055609703064, 'total_duration': 109547.62645864487, 'accumulated_submission_time': 63884.055609703064, 'accumulated_eval_time': 45655.30615091324, 'accumulated_logging_time': 2.792304277420044}
I0313 10:36:19.112574 139664567539456 logging_writer.py:48] [179508] accumulated_eval_time=45655.306151, accumulated_logging_time=2.792304, accumulated_submission_time=63884.055610, global_step=179508, preemption_count=0, score=63884.055610, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=109547.626459, train/accuracy=0.696200, train/bleu=35.350567, train/loss=1.361636, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 10:36:52.154687 139664575932160 logging_writer.py:48] [179600] global_step=179600, grad_norm=0.22959569096565247, loss=1.4218387603759766
I0313 10:37:27.728312 139664567539456 logging_writer.py:48] [179700] global_step=179700, grad_norm=0.22748856246471405, loss=1.4392485618591309
I0313 10:38:03.312756 139664575932160 logging_writer.py:48] [179800] global_step=179800, grad_norm=0.23955778777599335, loss=1.4298415184020996
I0313 10:38:38.900448 139664567539456 logging_writer.py:48] [179900] global_step=179900, grad_norm=0.2335716336965561, loss=1.47412109375
I0313 10:39:14.491796 139664575932160 logging_writer.py:48] [180000] global_step=180000, grad_norm=0.23029650747776031, loss=1.3870199918746948
I0313 10:39:50.058331 139664567539456 logging_writer.py:48] [180100] global_step=180100, grad_norm=0.2275037318468094, loss=1.433794617652893
I0313 10:40:25.727621 139664575932160 logging_writer.py:48] [180200] global_step=180200, grad_norm=0.23738017678260803, loss=1.451056718826294
I0313 10:41:01.318523 139664567539456 logging_writer.py:48] [180300] global_step=180300, grad_norm=0.23342157900333405, loss=1.471782922744751
I0313 10:41:36.912127 139664575932160 logging_writer.py:48] [180400] global_step=180400, grad_norm=0.233705535531044, loss=1.485261082649231
I0313 10:42:12.509728 139664567539456 logging_writer.py:48] [180500] global_step=180500, grad_norm=0.23434194922447205, loss=1.3710834980010986
I0313 10:42:48.062023 139664575932160 logging_writer.py:48] [180600] global_step=180600, grad_norm=0.2685467302799225, loss=1.4352375268936157
I0313 10:43:23.646545 139664567539456 logging_writer.py:48] [180700] global_step=180700, grad_norm=0.23477764427661896, loss=1.4617770910263062
I0313 10:43:59.264773 139664575932160 logging_writer.py:48] [180800] global_step=180800, grad_norm=0.2447228729724884, loss=1.4384486675262451
I0313 10:44:34.875704 139664567539456 logging_writer.py:48] [180900] global_step=180900, grad_norm=0.24530085921287537, loss=1.4789526462554932
I0313 10:45:10.471499 139664575932160 logging_writer.py:48] [181000] global_step=181000, grad_norm=0.23774586617946625, loss=1.444512963294983
I0313 10:45:46.102912 139664567539456 logging_writer.py:48] [181100] global_step=181100, grad_norm=0.24079015851020813, loss=1.4268757104873657
I0313 10:46:21.711654 139664575932160 logging_writer.py:48] [181200] global_step=181200, grad_norm=0.23534846305847168, loss=1.373982310295105
I0313 10:46:57.338287 139664567539456 logging_writer.py:48] [181300] global_step=181300, grad_norm=0.23139739036560059, loss=1.3731358051300049
I0313 10:47:32.962887 139664575932160 logging_writer.py:48] [181400] global_step=181400, grad_norm=0.23232649266719818, loss=1.4468296766281128
I0313 10:48:08.610809 139664567539456 logging_writer.py:48] [181500] global_step=181500, grad_norm=0.23385806381702423, loss=1.4521825313568115
I0313 10:48:44.256032 139664575932160 logging_writer.py:48] [181600] global_step=181600, grad_norm=0.2287541925907135, loss=1.4604554176330566
I0313 10:49:19.860349 139664567539456 logging_writer.py:48] [181700] global_step=181700, grad_norm=0.23375128209590912, loss=1.4170345067977905
I0313 10:49:55.454157 139664575932160 logging_writer.py:48] [181800] global_step=181800, grad_norm=0.24009259045124054, loss=1.4751042127609253
I0313 10:50:19.360306 139834281293632 spec.py:321] Evaluating on the training split.
I0313 10:50:22.329147 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:53:57.207307 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 10:53:59.876528 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 10:57:13.982704 139834281293632 spec.py:349] Evaluating on the test split.
I0313 10:57:16.650001 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:00:26.330284 139834281293632 submission_runner.py:420] Time since start: 110994.88s, 	Step: 181869, 	{'train/accuracy': 0.6943336129188538, 'train/loss': 1.380504846572876, 'train/bleu': 35.76284631446216, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 64724.220274209976, 'total_duration': 110994.8827548027, 'accumulated_submission_time': 64724.220274209976, 'accumulated_eval_time': 46262.27606844902, 'accumulated_logging_time': 2.8404438495635986}
I0313 11:00:26.369776 139664567539456 logging_writer.py:48] [181869] accumulated_eval_time=46262.276068, accumulated_logging_time=2.840444, accumulated_submission_time=64724.220274, global_step=181869, preemption_count=0, score=64724.220274, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=110994.882755, train/accuracy=0.694334, train/bleu=35.762846, train/loss=1.380505, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 11:00:37.741656 139664575932160 logging_writer.py:48] [181900] global_step=181900, grad_norm=0.2400389164686203, loss=1.47659432888031
I0313 11:01:13.240303 139664567539456 logging_writer.py:48] [182000] global_step=182000, grad_norm=0.23424361646175385, loss=1.4182816743850708
I0313 11:01:48.828180 139664575932160 logging_writer.py:48] [182100] global_step=182100, grad_norm=0.235227569937706, loss=1.4229539632797241
I0313 11:02:24.504857 139664567539456 logging_writer.py:48] [182200] global_step=182200, grad_norm=0.24652718007564545, loss=1.4708685874938965
I0313 11:03:00.062519 139664575932160 logging_writer.py:48] [182300] global_step=182300, grad_norm=0.22850635647773743, loss=1.408521294593811
I0313 11:03:35.644282 139664567539456 logging_writer.py:48] [182400] global_step=182400, grad_norm=0.23525913059711456, loss=1.4799168109893799
I0313 11:04:11.223413 139664575932160 logging_writer.py:48] [182500] global_step=182500, grad_norm=0.23118628561496735, loss=1.4578310251235962
I0313 11:04:46.814552 139664567539456 logging_writer.py:48] [182600] global_step=182600, grad_norm=0.24410343170166016, loss=1.475201964378357
I0313 11:05:22.408353 139664575932160 logging_writer.py:48] [182700] global_step=182700, grad_norm=0.2312290072441101, loss=1.3934791088104248
I0313 11:05:58.030365 139664567539456 logging_writer.py:48] [182800] global_step=182800, grad_norm=0.2391844391822815, loss=1.467933177947998
I0313 11:06:33.671106 139664575932160 logging_writer.py:48] [182900] global_step=182900, grad_norm=0.2357996106147766, loss=1.4245009422302246
I0313 11:07:09.304941 139664567539456 logging_writer.py:48] [183000] global_step=183000, grad_norm=0.2343551069498062, loss=1.439815640449524
I0313 11:07:44.874675 139664575932160 logging_writer.py:48] [183100] global_step=183100, grad_norm=0.23886620998382568, loss=1.4103198051452637
I0313 11:08:20.471085 139664567539456 logging_writer.py:48] [183200] global_step=183200, grad_norm=0.23769377171993256, loss=1.4144684076309204
I0313 11:08:56.053801 139664575932160 logging_writer.py:48] [183300] global_step=183300, grad_norm=0.22225737571716309, loss=1.4322336912155151
I0313 11:09:31.663594 139664567539456 logging_writer.py:48] [183400] global_step=183400, grad_norm=0.22811660170555115, loss=1.4327481985092163
I0313 11:10:07.290299 139664575932160 logging_writer.py:48] [183500] global_step=183500, grad_norm=0.23854860663414001, loss=1.4612112045288086
I0313 11:10:42.917448 139664567539456 logging_writer.py:48] [183600] global_step=183600, grad_norm=0.23204128444194794, loss=1.4235990047454834
I0313 11:11:18.530817 139664575932160 logging_writer.py:48] [183700] global_step=183700, grad_norm=0.22732102870941162, loss=1.4745761156082153
I0313 11:11:54.113378 139664567539456 logging_writer.py:48] [183800] global_step=183800, grad_norm=0.23407112061977386, loss=1.4078716039657593
I0313 11:12:29.688874 139664575932160 logging_writer.py:48] [183900] global_step=183900, grad_norm=0.23433789610862732, loss=1.392543077468872
I0313 11:13:05.252094 139664567539456 logging_writer.py:48] [184000] global_step=184000, grad_norm=0.2256992757320404, loss=1.4482816457748413
I0313 11:13:40.824398 139664575932160 logging_writer.py:48] [184100] global_step=184100, grad_norm=0.23463743925094604, loss=1.4237035512924194
I0313 11:14:16.404749 139664567539456 logging_writer.py:48] [184200] global_step=184200, grad_norm=0.24127209186553955, loss=1.4410945177078247
I0313 11:14:26.432998 139834281293632 spec.py:321] Evaluating on the training split.
I0313 11:14:29.399779 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:17:59.546428 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 11:18:02.245185 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:21:16.197296 139834281293632 spec.py:349] Evaluating on the test split.
I0313 11:21:18.864750 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:24:28.656117 139834281293632 submission_runner.py:420] Time since start: 112437.21s, 	Step: 184230, 	{'train/accuracy': 0.6938932538032532, 'train/loss': 1.3816889524459839, 'train/bleu': 35.454537768174426, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 65564.20058608055, 'total_duration': 112437.20860123634, 'accumulated_submission_time': 65564.20058608055, 'accumulated_eval_time': 46864.499136924744, 'accumulated_logging_time': 2.888885021209717}
I0313 11:24:28.696359 139664575932160 logging_writer.py:48] [184230] accumulated_eval_time=46864.499137, accumulated_logging_time=2.888885, accumulated_submission_time=65564.200586, global_step=184230, preemption_count=0, score=65564.200586, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=112437.208601, train/accuracy=0.693893, train/bleu=35.454538, train/loss=1.381689, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 11:24:53.911668 139664567539456 logging_writer.py:48] [184300] global_step=184300, grad_norm=0.2255944013595581, loss=1.4027527570724487
I0313 11:25:29.474418 139664575932160 logging_writer.py:48] [184400] global_step=184400, grad_norm=0.23003773391246796, loss=1.4104019403457642
I0313 11:26:05.086455 139664567539456 logging_writer.py:48] [184500] global_step=184500, grad_norm=0.22363455593585968, loss=1.3917890787124634
I0313 11:26:40.682029 139664575932160 logging_writer.py:48] [184600] global_step=184600, grad_norm=0.23220130801200867, loss=1.46979558467865
I0313 11:27:16.286355 139664567539456 logging_writer.py:48] [184700] global_step=184700, grad_norm=0.241771399974823, loss=1.5403226613998413
I0313 11:27:51.868716 139664575932160 logging_writer.py:48] [184800] global_step=184800, grad_norm=0.2311127483844757, loss=1.4269565343856812
I0313 11:28:27.453956 139664567539456 logging_writer.py:48] [184900] global_step=184900, grad_norm=0.23773078620433807, loss=1.3995925188064575
I0313 11:29:03.075149 139664575932160 logging_writer.py:48] [185000] global_step=185000, grad_norm=0.23171330988407135, loss=1.4203993082046509
I0313 11:29:38.677927 139664567539456 logging_writer.py:48] [185100] global_step=185100, grad_norm=0.2300843894481659, loss=1.3754616975784302
I0313 11:30:14.302527 139664575932160 logging_writer.py:48] [185200] global_step=185200, grad_norm=0.23845691978931427, loss=1.4918885231018066
I0313 11:30:49.891273 139664567539456 logging_writer.py:48] [185300] global_step=185300, grad_norm=0.22104378044605255, loss=1.4430944919586182
I0313 11:31:25.492095 139664575932160 logging_writer.py:48] [185400] global_step=185400, grad_norm=0.23751676082611084, loss=1.4410845041275024
I0313 11:32:01.068004 139664567539456 logging_writer.py:48] [185500] global_step=185500, grad_norm=0.24149420857429504, loss=1.4505513906478882
I0313 11:32:36.650769 139664575932160 logging_writer.py:48] [185600] global_step=185600, grad_norm=0.2498822659254074, loss=1.5047897100448608
I0313 11:33:12.254137 139664567539456 logging_writer.py:48] [185700] global_step=185700, grad_norm=0.24788151681423187, loss=1.4389958381652832
I0313 11:33:47.831294 139664575932160 logging_writer.py:48] [185800] global_step=185800, grad_norm=0.23132939636707306, loss=1.401957631111145
I0313 11:34:23.422809 139664567539456 logging_writer.py:48] [185900] global_step=185900, grad_norm=0.22733135521411896, loss=1.4220551252365112
I0313 11:34:59.024998 139664575932160 logging_writer.py:48] [186000] global_step=186000, grad_norm=0.22206856310367584, loss=1.4109002351760864
I0313 11:35:34.622015 139664567539456 logging_writer.py:48] [186100] global_step=186100, grad_norm=0.24457192420959473, loss=1.4537433385849
I0313 11:36:10.240592 139664575932160 logging_writer.py:48] [186200] global_step=186200, grad_norm=0.23023255169391632, loss=1.4178160429000854
I0313 11:36:45.812090 139664567539456 logging_writer.py:48] [186300] global_step=186300, grad_norm=0.23688848316669464, loss=1.453769564628601
I0313 11:37:21.447136 139664575932160 logging_writer.py:48] [186400] global_step=186400, grad_norm=0.2345522940158844, loss=1.4272781610488892
I0313 11:37:57.078971 139664567539456 logging_writer.py:48] [186500] global_step=186500, grad_norm=0.23493218421936035, loss=1.4187822341918945
I0313 11:38:28.843657 139834281293632 spec.py:321] Evaluating on the training split.
I0313 11:38:31.812601 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:41:57.166665 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 11:41:59.863965 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:45:15.477846 139834281293632 spec.py:349] Evaluating on the test split.
I0313 11:45:18.152796 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 11:48:28.346522 139834281293632 submission_runner.py:420] Time since start: 113876.90s, 	Step: 186591, 	{'train/accuracy': 0.6969566345214844, 'train/loss': 1.3639436960220337, 'train/bleu': 35.72708821302105, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 66404.26647567749, 'total_duration': 113876.89898991585, 'accumulated_submission_time': 66404.26647567749, 'accumulated_eval_time': 47464.00195264816, 'accumulated_logging_time': 2.9393835067749023}
I0313 11:48:28.385682 139664575932160 logging_writer.py:48] [186591] accumulated_eval_time=47464.001953, accumulated_logging_time=2.939384, accumulated_submission_time=66404.266476, global_step=186591, preemption_count=0, score=66404.266476, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=113876.898990, train/accuracy=0.696957, train/bleu=35.727088, train/loss=1.363944, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 11:48:31.967909 139664567539456 logging_writer.py:48] [186600] global_step=186600, grad_norm=0.24218876659870148, loss=1.4304264783859253
I0313 11:49:07.487434 139664575932160 logging_writer.py:48] [186700] global_step=186700, grad_norm=0.23616810142993927, loss=1.421552300453186
I0313 11:49:43.037560 139664567539456 logging_writer.py:48] [186800] global_step=186800, grad_norm=0.22595210373401642, loss=1.3750994205474854
I0313 11:50:18.613362 139664575932160 logging_writer.py:48] [186900] global_step=186900, grad_norm=0.22778740525245667, loss=1.4224979877471924
I0313 11:50:54.220806 139664567539456 logging_writer.py:48] [187000] global_step=187000, grad_norm=0.22684775292873383, loss=1.451914668083191
I0313 11:51:29.777223 139664575932160 logging_writer.py:48] [187100] global_step=187100, grad_norm=0.23364926874637604, loss=1.4447077512741089
I0313 11:52:05.371937 139664567539456 logging_writer.py:48] [187200] global_step=187200, grad_norm=0.23096199333667755, loss=1.437565565109253
I0313 11:52:40.972685 139664575932160 logging_writer.py:48] [187300] global_step=187300, grad_norm=0.22622378170490265, loss=1.4725722074508667
I0313 11:53:16.556980 139664567539456 logging_writer.py:48] [187400] global_step=187400, grad_norm=0.25114884972572327, loss=1.4642680883407593
I0313 11:53:52.172762 139664575932160 logging_writer.py:48] [187500] global_step=187500, grad_norm=0.23994913697242737, loss=1.4780398607254028
I0313 11:54:27.775382 139664567539456 logging_writer.py:48] [187600] global_step=187600, grad_norm=0.23130416870117188, loss=1.4018731117248535
I0313 11:55:03.392222 139664575932160 logging_writer.py:48] [187700] global_step=187700, grad_norm=0.2485402375459671, loss=1.4222787618637085
I0313 11:55:38.977235 139664567539456 logging_writer.py:48] [187800] global_step=187800, grad_norm=0.27555927634239197, loss=1.387709617614746
I0313 11:56:14.581619 139664575932160 logging_writer.py:48] [187900] global_step=187900, grad_norm=0.22441723942756653, loss=1.3944655656814575
I0313 11:56:50.152052 139664567539456 logging_writer.py:48] [188000] global_step=188000, grad_norm=0.24041996896266937, loss=1.4803439378738403
I0313 11:57:25.741195 139664575932160 logging_writer.py:48] [188100] global_step=188100, grad_norm=0.23089800775051117, loss=1.4343228340148926
I0313 11:58:01.311391 139664567539456 logging_writer.py:48] [188200] global_step=188200, grad_norm=0.2299284040927887, loss=1.5267454385757446
I0313 11:58:36.924461 139664575932160 logging_writer.py:48] [188300] global_step=188300, grad_norm=0.2356438785791397, loss=1.393272876739502
I0313 11:59:12.522025 139664567539456 logging_writer.py:48] [188400] global_step=188400, grad_norm=0.23272474110126495, loss=1.4139533042907715
I0313 11:59:48.109415 139664575932160 logging_writer.py:48] [188500] global_step=188500, grad_norm=0.22993506491184235, loss=1.5053050518035889
I0313 12:00:23.705175 139664567539456 logging_writer.py:48] [188600] global_step=188600, grad_norm=0.24220052361488342, loss=1.4526938199996948
I0313 12:00:59.316045 139664575932160 logging_writer.py:48] [188700] global_step=188700, grad_norm=0.2331504076719284, loss=1.3762940168380737
I0313 12:01:34.927010 139664567539456 logging_writer.py:48] [188800] global_step=188800, grad_norm=0.2295946329832077, loss=1.410884141921997
I0313 12:02:10.522549 139664575932160 logging_writer.py:48] [188900] global_step=188900, grad_norm=0.22661679983139038, loss=1.426737666130066
I0313 12:02:28.377397 139834281293632 spec.py:321] Evaluating on the training split.
I0313 12:02:31.349197 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:06:11.771869 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 12:06:14.441341 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:09:28.355742 139834281293632 spec.py:349] Evaluating on the test split.
I0313 12:09:31.040144 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:12:41.019969 139834281293632 submission_runner.py:420] Time since start: 115329.57s, 	Step: 188952, 	{'train/accuracy': 0.6942804455757141, 'train/loss': 1.3843278884887695, 'train/bleu': 35.36090369464056, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 67244.17678833008, 'total_duration': 115329.57243132591, 'accumulated_submission_time': 67244.17678833008, 'accumulated_eval_time': 48076.644453287125, 'accumulated_logging_time': 2.9872334003448486}
I0313 12:12:41.059924 139664567539456 logging_writer.py:48] [188952] accumulated_eval_time=48076.644453, accumulated_logging_time=2.987233, accumulated_submission_time=67244.176788, global_step=188952, preemption_count=0, score=67244.176788, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=115329.572431, train/accuracy=0.694280, train/bleu=35.360904, train/loss=1.384328, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 12:12:58.459095 139664575932160 logging_writer.py:48] [189000] global_step=189000, grad_norm=0.2306293100118637, loss=1.4496314525604248
I0313 12:13:33.993774 139664567539456 logging_writer.py:48] [189100] global_step=189100, grad_norm=0.22720573842525482, loss=1.4397119283676147
I0313 12:14:09.568503 139664575932160 logging_writer.py:48] [189200] global_step=189200, grad_norm=0.2415565848350525, loss=1.4256137609481812
I0313 12:14:45.147043 139664567539456 logging_writer.py:48] [189300] global_step=189300, grad_norm=0.23304498195648193, loss=1.4577397108078003
I0313 12:15:20.713659 139664575932160 logging_writer.py:48] [189400] global_step=189400, grad_norm=0.24540451169013977, loss=1.410382628440857
I0313 12:15:56.309267 139664567539456 logging_writer.py:48] [189500] global_step=189500, grad_norm=0.2361481636762619, loss=1.4649510383605957
I0313 12:16:31.934642 139664575932160 logging_writer.py:48] [189600] global_step=189600, grad_norm=0.2340579777956009, loss=1.504051685333252
I0313 12:17:07.583387 139664567539456 logging_writer.py:48] [189700] global_step=189700, grad_norm=0.2336272895336151, loss=1.4035732746124268
I0313 12:17:43.229133 139664575932160 logging_writer.py:48] [189800] global_step=189800, grad_norm=0.2352321594953537, loss=1.4100066423416138
I0313 12:18:18.865264 139664567539456 logging_writer.py:48] [189900] global_step=189900, grad_norm=0.25100383162498474, loss=1.4899519681930542
I0313 12:18:54.464307 139664575932160 logging_writer.py:48] [190000] global_step=190000, grad_norm=0.23658809065818787, loss=1.3842333555221558
I0313 12:19:30.054174 139664567539456 logging_writer.py:48] [190100] global_step=190100, grad_norm=0.22741542756557465, loss=1.4484038352966309
I0313 12:20:05.644908 139664575932160 logging_writer.py:48] [190200] global_step=190200, grad_norm=0.22247262299060822, loss=1.4277652502059937
I0313 12:20:41.256161 139664567539456 logging_writer.py:48] [190300] global_step=190300, grad_norm=0.23026932775974274, loss=1.4807226657867432
I0313 12:21:16.858373 139664575932160 logging_writer.py:48] [190400] global_step=190400, grad_norm=0.22703246772289276, loss=1.4772181510925293
I0313 12:21:52.473811 139664567539456 logging_writer.py:48] [190500] global_step=190500, grad_norm=0.2490159571170807, loss=1.5012010335922241
I0313 12:22:28.070732 139664575932160 logging_writer.py:48] [190600] global_step=190600, grad_norm=0.23574964702129364, loss=1.483030080795288
I0313 12:23:03.711724 139664567539456 logging_writer.py:48] [190700] global_step=190700, grad_norm=0.22880248725414276, loss=1.3481645584106445
I0313 12:23:39.301316 139664575932160 logging_writer.py:48] [190800] global_step=190800, grad_norm=0.24111291766166687, loss=1.4719399213790894
I0313 12:24:14.912492 139664567539456 logging_writer.py:48] [190900] global_step=190900, grad_norm=0.22589920461177826, loss=1.4111257791519165
I0313 12:24:50.499188 139664575932160 logging_writer.py:48] [191000] global_step=191000, grad_norm=0.23823681473731995, loss=1.4639947414398193
I0313 12:25:26.089093 139664567539456 logging_writer.py:48] [191100] global_step=191100, grad_norm=0.23311041295528412, loss=1.473598837852478
I0313 12:26:01.690905 139664575932160 logging_writer.py:48] [191200] global_step=191200, grad_norm=0.23354904353618622, loss=1.4422935247421265
I0313 12:26:37.249050 139664567539456 logging_writer.py:48] [191300] global_step=191300, grad_norm=0.2357131391763687, loss=1.4131731986999512
I0313 12:26:41.236473 139834281293632 spec.py:321] Evaluating on the training split.
I0313 12:26:44.214122 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:30:07.097743 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 12:30:09.767601 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:33:24.053589 139834281293632 spec.py:349] Evaluating on the test split.
I0313 12:33:26.720108 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:36:38.014156 139834281293632 submission_runner.py:420] Time since start: 116766.57s, 	Step: 191313, 	{'train/accuracy': 0.6944150328636169, 'train/loss': 1.3720909357070923, 'train/bleu': 35.722617039369105, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 68084.2696146965, 'total_duration': 116766.5665898323, 'accumulated_submission_time': 68084.2696146965, 'accumulated_eval_time': 48673.42203640938, 'accumulated_logging_time': 3.0373005867004395}
I0313 12:36:38.062165 139664575932160 logging_writer.py:48] [191313] accumulated_eval_time=48673.422036, accumulated_logging_time=3.037301, accumulated_submission_time=68084.269615, global_step=191313, preemption_count=0, score=68084.269615, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=116766.566590, train/accuracy=0.694415, train/bleu=35.722617, train/loss=1.372091, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 12:37:09.311578 139664567539456 logging_writer.py:48] [191400] global_step=191400, grad_norm=0.23741796612739563, loss=1.395995020866394
I0313 12:37:44.861733 139664575932160 logging_writer.py:48] [191500] global_step=191500, grad_norm=0.2339678406715393, loss=1.4929189682006836
I0313 12:38:20.424899 139664567539456 logging_writer.py:48] [191600] global_step=191600, grad_norm=0.23807521164417267, loss=1.3958425521850586
I0313 12:38:56.006171 139664575932160 logging_writer.py:48] [191700] global_step=191700, grad_norm=0.23384013772010803, loss=1.438140869140625
I0313 12:39:31.585769 139664567539456 logging_writer.py:48] [191800] global_step=191800, grad_norm=0.22692760825157166, loss=1.4244827032089233
I0313 12:40:07.155841 139664575932160 logging_writer.py:48] [191900] global_step=191900, grad_norm=0.23079729080200195, loss=1.4317251443862915
I0313 12:40:42.775232 139664567539456 logging_writer.py:48] [192000] global_step=192000, grad_norm=0.23343060910701752, loss=1.4092738628387451
I0313 12:41:18.334962 139664575932160 logging_writer.py:48] [192100] global_step=192100, grad_norm=0.23483166098594666, loss=1.4510726928710938
I0313 12:41:53.957833 139664567539456 logging_writer.py:48] [192200] global_step=192200, grad_norm=0.23056331276893616, loss=1.4145303964614868
I0313 12:42:29.563583 139664575932160 logging_writer.py:48] [192300] global_step=192300, grad_norm=0.22748889029026031, loss=1.44272780418396
I0313 12:43:05.191091 139664567539456 logging_writer.py:48] [192400] global_step=192400, grad_norm=0.23339906334877014, loss=1.4076642990112305
I0313 12:43:40.806910 139664575932160 logging_writer.py:48] [192500] global_step=192500, grad_norm=0.2306051403284073, loss=1.364570140838623
I0313 12:44:16.407660 139664567539456 logging_writer.py:48] [192600] global_step=192600, grad_norm=0.22296984493732452, loss=1.4199156761169434
I0313 12:44:51.987157 139664575932160 logging_writer.py:48] [192700] global_step=192700, grad_norm=0.23295828700065613, loss=1.48604154586792
I0313 12:45:27.572664 139664567539456 logging_writer.py:48] [192800] global_step=192800, grad_norm=0.22872807085514069, loss=1.445634126663208
I0313 12:46:03.192095 139664575932160 logging_writer.py:48] [192900] global_step=192900, grad_norm=0.23296697437763214, loss=1.4118419885635376
I0313 12:46:38.803294 139664567539456 logging_writer.py:48] [193000] global_step=193000, grad_norm=0.24201463162899017, loss=1.4438046216964722
I0313 12:47:14.403805 139664575932160 logging_writer.py:48] [193100] global_step=193100, grad_norm=0.24867656826972961, loss=1.524319052696228
I0313 12:47:50.017671 139664567539456 logging_writer.py:48] [193200] global_step=193200, grad_norm=0.24192623794078827, loss=1.4654576778411865
I0313 12:48:25.624136 139664575932160 logging_writer.py:48] [193300] global_step=193300, grad_norm=0.23408223688602448, loss=1.4568612575531006
I0313 12:49:01.246031 139664567539456 logging_writer.py:48] [193400] global_step=193400, grad_norm=0.23746249079704285, loss=1.4386063814163208
I0313 12:49:36.826805 139664575932160 logging_writer.py:48] [193500] global_step=193500, grad_norm=0.23824632167816162, loss=1.4779565334320068
I0313 12:50:12.427450 139664567539456 logging_writer.py:48] [193600] global_step=193600, grad_norm=0.22439029812812805, loss=1.345813274383545
I0313 12:50:38.096368 139834281293632 spec.py:321] Evaluating on the training split.
I0313 12:50:41.065192 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:54:16.974269 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 12:54:19.650245 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 12:57:33.733884 139834281293632 spec.py:349] Evaluating on the test split.
I0313 12:57:36.416263 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:00:46.294393 139834281293632 submission_runner.py:420] Time since start: 118214.85s, 	Step: 193674, 	{'train/accuracy': 0.6968308687210083, 'train/loss': 1.3575750589370728, 'train/bleu': 35.36122057156541, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 68924.22136425972, 'total_duration': 118214.84686422348, 'accumulated_submission_time': 68924.22136425972, 'accumulated_eval_time': 49281.62001180649, 'accumulated_logging_time': 3.0965020656585693}
I0313 13:00:46.334427 139664575932160 logging_writer.py:48] [193674] accumulated_eval_time=49281.620012, accumulated_logging_time=3.096502, accumulated_submission_time=68924.221364, global_step=193674, preemption_count=0, score=68924.221364, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=118214.846864, train/accuracy=0.696831, train/bleu=35.361221, train/loss=1.357575, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 13:00:55.938279 139664567539456 logging_writer.py:48] [193700] global_step=193700, grad_norm=0.2278069108724594, loss=1.4283885955810547
I0313 13:01:31.462367 139664575932160 logging_writer.py:48] [193800] global_step=193800, grad_norm=0.2285488098859787, loss=1.461031436920166
I0313 13:02:07.042720 139664567539456 logging_writer.py:48] [193900] global_step=193900, grad_norm=0.23817536234855652, loss=1.4062228202819824
I0313 13:02:42.694876 139664575932160 logging_writer.py:48] [194000] global_step=194000, grad_norm=0.2338409721851349, loss=1.46642005443573
I0313 13:03:18.323387 139664567539456 logging_writer.py:48] [194100] global_step=194100, grad_norm=0.2390720695257187, loss=1.5027202367782593
I0313 13:03:53.921195 139664575932160 logging_writer.py:48] [194200] global_step=194200, grad_norm=0.2338351160287857, loss=1.4673963785171509
I0313 13:04:29.482591 139664567539456 logging_writer.py:48] [194300] global_step=194300, grad_norm=0.23549838364124298, loss=1.3892309665679932
I0313 13:05:05.081469 139664575932160 logging_writer.py:48] [194400] global_step=194400, grad_norm=0.23932060599327087, loss=1.4767954349517822
I0313 13:05:40.649846 139664567539456 logging_writer.py:48] [194500] global_step=194500, grad_norm=0.2406243085861206, loss=1.3861193656921387
I0313 13:06:16.242439 139664575932160 logging_writer.py:48] [194600] global_step=194600, grad_norm=0.2357063889503479, loss=1.420100212097168
I0313 13:06:51.826153 139664567539456 logging_writer.py:48] [194700] global_step=194700, grad_norm=0.23514945805072784, loss=1.359737515449524
I0313 13:07:27.429933 139664575932160 logging_writer.py:48] [194800] global_step=194800, grad_norm=0.24090829491615295, loss=1.4697654247283936
I0313 13:08:03.049286 139664567539456 logging_writer.py:48] [194900] global_step=194900, grad_norm=0.2373005747795105, loss=1.4887232780456543
I0313 13:08:38.686113 139664575932160 logging_writer.py:48] [195000] global_step=195000, grad_norm=0.23056215047836304, loss=1.48867666721344
I0313 13:09:14.301069 139664567539456 logging_writer.py:48] [195100] global_step=195100, grad_norm=0.23009327054023743, loss=1.44179105758667
I0313 13:09:49.895745 139664575932160 logging_writer.py:48] [195200] global_step=195200, grad_norm=0.23048660159111023, loss=1.4165568351745605
I0313 13:10:25.492556 139664567539456 logging_writer.py:48] [195300] global_step=195300, grad_norm=0.2370745688676834, loss=1.4761911630630493
I0313 13:11:01.078181 139664575932160 logging_writer.py:48] [195400] global_step=195400, grad_norm=0.24758249521255493, loss=1.450971007347107
I0313 13:11:36.728383 139664567539456 logging_writer.py:48] [195500] global_step=195500, grad_norm=0.234837606549263, loss=1.4193267822265625
I0313 13:12:12.312763 139664575932160 logging_writer.py:48] [195600] global_step=195600, grad_norm=0.2338975965976715, loss=1.477308750152588
I0313 13:12:47.942833 139664567539456 logging_writer.py:48] [195700] global_step=195700, grad_norm=0.2393282651901245, loss=1.3828694820404053
I0313 13:13:23.526614 139664575932160 logging_writer.py:48] [195800] global_step=195800, grad_norm=0.22806142270565033, loss=1.41655433177948
I0313 13:13:59.098790 139664567539456 logging_writer.py:48] [195900] global_step=195900, grad_norm=0.2463889867067337, loss=1.381164312362671
I0313 13:14:34.697711 139664575932160 logging_writer.py:48] [196000] global_step=196000, grad_norm=0.2303065061569214, loss=1.3672031164169312
I0313 13:14:46.509647 139834281293632 spec.py:321] Evaluating on the training split.
I0313 13:14:49.485247 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:18:31.899346 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 13:18:34.577942 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:21:48.875582 139834281293632 spec.py:349] Evaluating on the test split.
I0313 13:21:51.567950 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:25:03.143113 139834281293632 submission_runner.py:420] Time since start: 119671.70s, 	Step: 196035, 	{'train/accuracy': 0.6957708597183228, 'train/loss': 1.3637819290161133, 'train/bleu': 35.55747837773363, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 69764.31381583214, 'total_duration': 119671.69554686546, 'accumulated_submission_time': 69764.31381583214, 'accumulated_eval_time': 49898.25337815285, 'accumulated_logging_time': 3.1466569900512695}
I0313 13:25:03.189288 139664567539456 logging_writer.py:48] [196035] accumulated_eval_time=49898.253378, accumulated_logging_time=3.146657, accumulated_submission_time=69764.313816, global_step=196035, preemption_count=0, score=69764.313816, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=119671.695547, train/accuracy=0.695771, train/bleu=35.557478, train/loss=1.363782, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 13:25:26.670433 139664575932160 logging_writer.py:48] [196100] global_step=196100, grad_norm=0.2314876765012741, loss=1.509602665901184
I0313 13:26:02.273748 139664567539456 logging_writer.py:48] [196200] global_step=196200, grad_norm=0.2278580516576767, loss=1.4619176387786865
I0313 13:26:37.861579 139664575932160 logging_writer.py:48] [196300] global_step=196300, grad_norm=0.23456281423568726, loss=1.410642385482788
I0313 13:27:13.458005 139664567539456 logging_writer.py:48] [196400] global_step=196400, grad_norm=0.2340819090604782, loss=1.4472973346710205
I0313 13:27:49.056975 139664575932160 logging_writer.py:48] [196500] global_step=196500, grad_norm=0.23645243048667908, loss=1.4905402660369873
I0313 13:28:24.662661 139664567539456 logging_writer.py:48] [196600] global_step=196600, grad_norm=0.24043907225131989, loss=1.4481922388076782
I0313 13:29:00.265769 139664575932160 logging_writer.py:48] [196700] global_step=196700, grad_norm=0.23910008370876312, loss=1.418540358543396
I0313 13:29:35.857644 139664567539456 logging_writer.py:48] [196800] global_step=196800, grad_norm=0.2500026822090149, loss=1.4478251934051514
I0313 13:30:11.485261 139664575932160 logging_writer.py:48] [196900] global_step=196900, grad_norm=0.2412959188222885, loss=1.4768270254135132
I0313 13:30:47.070354 139664567539456 logging_writer.py:48] [197000] global_step=197000, grad_norm=0.2299048751592636, loss=1.4410649538040161
I0313 13:31:22.702780 139664575932160 logging_writer.py:48] [197100] global_step=197100, grad_norm=0.23479443788528442, loss=1.4031431674957275
I0313 13:31:58.270852 139664567539456 logging_writer.py:48] [197200] global_step=197200, grad_norm=0.23470862209796906, loss=1.483067274093628
I0313 13:32:33.858998 139664575932160 logging_writer.py:48] [197300] global_step=197300, grad_norm=0.2387373447418213, loss=1.4245898723602295
I0313 13:33:09.446367 139664567539456 logging_writer.py:48] [197400] global_step=197400, grad_norm=0.2324153035879135, loss=1.4330179691314697
I0313 13:33:45.037327 139664575932160 logging_writer.py:48] [197500] global_step=197500, grad_norm=0.24426822364330292, loss=1.4522337913513184
I0313 13:34:20.614056 139664567539456 logging_writer.py:48] [197600] global_step=197600, grad_norm=0.2302125096321106, loss=1.4118229150772095
I0313 13:34:56.204354 139664575932160 logging_writer.py:48] [197700] global_step=197700, grad_norm=0.22964781522750854, loss=1.400621771812439
I0313 13:35:31.789387 139664567539456 logging_writer.py:48] [197800] global_step=197800, grad_norm=0.22963854670524597, loss=1.4135472774505615
I0313 13:36:07.390052 139664575932160 logging_writer.py:48] [197900] global_step=197900, grad_norm=0.22529453039169312, loss=1.4381484985351562
I0313 13:36:42.980660 139664567539456 logging_writer.py:48] [198000] global_step=198000, grad_norm=0.231983944773674, loss=1.4596524238586426
I0313 13:37:18.584657 139664575932160 logging_writer.py:48] [198100] global_step=198100, grad_norm=0.23651854693889618, loss=1.4638781547546387
I0313 13:37:54.187190 139664567539456 logging_writer.py:48] [198200] global_step=198200, grad_norm=0.23813432455062866, loss=1.4885685443878174
I0313 13:38:29.782026 139664575932160 logging_writer.py:48] [198300] global_step=198300, grad_norm=0.23915158212184906, loss=1.4072846174240112
I0313 13:39:03.298902 139834281293632 spec.py:321] Evaluating on the training split.
I0313 13:39:06.268480 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:42:37.150415 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 13:42:39.820475 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:45:54.018417 139834281293632 spec.py:349] Evaluating on the test split.
I0313 13:45:56.692086 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 13:49:06.745395 139834281293632 submission_runner.py:420] Time since start: 121115.30s, 	Step: 198396, 	{'train/accuracy': 0.6959868669509888, 'train/loss': 1.3676646947860718, 'train/bleu': 35.303194536835115, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 70604.34129023552, 'total_duration': 121115.29785180092, 'accumulated_submission_time': 70604.34129023552, 'accumulated_eval_time': 50501.69980740547, 'accumulated_logging_time': 3.2037465572357178}
I0313 13:49:06.786592 139664567539456 logging_writer.py:48] [198396] accumulated_eval_time=50501.699807, accumulated_logging_time=3.203747, accumulated_submission_time=70604.341290, global_step=198396, preemption_count=0, score=70604.341290, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=121115.297852, train/accuracy=0.695987, train/bleu=35.303195, train/loss=1.367665, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 13:49:08.585919 139664575932160 logging_writer.py:48] [198400] global_step=198400, grad_norm=0.22829902172088623, loss=1.3921349048614502
I0313 13:49:44.087245 139664567539456 logging_writer.py:48] [198500] global_step=198500, grad_norm=0.2491241693496704, loss=1.4920175075531006
I0313 13:50:19.636635 139664575932160 logging_writer.py:48] [198600] global_step=198600, grad_norm=0.233181893825531, loss=1.4670478105545044
I0313 13:50:55.217055 139664567539456 logging_writer.py:48] [198700] global_step=198700, grad_norm=0.23035842180252075, loss=1.4266427755355835
I0313 13:51:30.810595 139664575932160 logging_writer.py:48] [198800] global_step=198800, grad_norm=0.2375686764717102, loss=1.4572073221206665
I0313 13:52:06.420365 139664567539456 logging_writer.py:48] [198900] global_step=198900, grad_norm=0.25388771295547485, loss=1.4672937393188477
I0313 13:52:41.978632 139664575932160 logging_writer.py:48] [199000] global_step=199000, grad_norm=0.23426665365695953, loss=1.450514793395996
I0313 13:53:17.541946 139664567539456 logging_writer.py:48] [199100] global_step=199100, grad_norm=0.238885760307312, loss=1.5342381000518799
I0313 13:53:53.111427 139664575932160 logging_writer.py:48] [199200] global_step=199200, grad_norm=0.22990533709526062, loss=1.4108607769012451
I0313 13:54:28.676599 139664567539456 logging_writer.py:48] [199300] global_step=199300, grad_norm=0.24860131740570068, loss=1.4100459814071655
I0313 13:55:04.262115 139664575932160 logging_writer.py:48] [199400] global_step=199400, grad_norm=0.23107415437698364, loss=1.4439013004302979
I0313 13:55:39.840206 139664567539456 logging_writer.py:48] [199500] global_step=199500, grad_norm=0.23172001540660858, loss=1.442891001701355
I0313 13:56:15.397333 139664575932160 logging_writer.py:48] [199600] global_step=199600, grad_norm=0.23322871327400208, loss=1.4444503784179688
I0313 13:56:50.973042 139664567539456 logging_writer.py:48] [199700] global_step=199700, grad_norm=0.24139347672462463, loss=1.4805957078933716
I0313 13:57:26.550919 139664575932160 logging_writer.py:48] [199800] global_step=199800, grad_norm=0.22809875011444092, loss=1.402421474456787
I0313 13:58:02.139683 139664567539456 logging_writer.py:48] [199900] global_step=199900, grad_norm=0.2296876758337021, loss=1.4583094120025635
I0313 13:58:37.716781 139664575932160 logging_writer.py:48] [200000] global_step=200000, grad_norm=0.2320479452610016, loss=1.3872207403182983
I0313 13:59:13.302807 139664567539456 logging_writer.py:48] [200100] global_step=200100, grad_norm=0.24167755246162415, loss=1.4890505075454712
I0313 13:59:48.893547 139664575932160 logging_writer.py:48] [200200] global_step=200200, grad_norm=0.23989351093769073, loss=1.4042484760284424
I0313 14:00:24.481053 139664567539456 logging_writer.py:48] [200300] global_step=200300, grad_norm=0.2367590069770813, loss=1.5002110004425049
I0313 14:01:00.086238 139664575932160 logging_writer.py:48] [200400] global_step=200400, grad_norm=0.22527658939361572, loss=1.4136877059936523
I0313 14:01:35.666095 139664567539456 logging_writer.py:48] [200500] global_step=200500, grad_norm=0.23522910475730896, loss=1.4787297248840332
I0313 14:02:11.246630 139664575932160 logging_writer.py:48] [200600] global_step=200600, grad_norm=0.23340915143489838, loss=1.4643981456756592
I0313 14:02:46.818067 139664567539456 logging_writer.py:48] [200700] global_step=200700, grad_norm=0.2358441948890686, loss=1.4644838571548462
I0313 14:03:06.816246 139834281293632 spec.py:321] Evaluating on the training split.
I0313 14:03:09.788324 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:06:40.072729 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 14:06:42.745168 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:09:56.735934 139834281293632 spec.py:349] Evaluating on the test split.
I0313 14:09:59.416168 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:13:09.223072 139834281293632 submission_runner.py:420] Time since start: 122557.78s, 	Step: 200758, 	{'train/accuracy': 0.6991065740585327, 'train/loss': 1.352362036705017, 'train/bleu': 35.527593314348735, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 71444.29108786583, 'total_duration': 122557.77553725243, 'accumulated_submission_time': 71444.29108786583, 'accumulated_eval_time': 51104.10656738281, 'accumulated_logging_time': 3.2538113594055176}
I0313 14:13:09.264845 139664575932160 logging_writer.py:48] [200758] accumulated_eval_time=51104.106567, accumulated_logging_time=3.253811, accumulated_submission_time=71444.291088, global_step=200758, preemption_count=0, score=71444.291088, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=122557.775537, train/accuracy=0.699107, train/bleu=35.527593, train/loss=1.352362, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 14:13:24.548982 139664567539456 logging_writer.py:48] [200800] global_step=200800, grad_norm=0.23810982704162598, loss=1.4438687562942505
I0313 14:14:00.082777 139664575932160 logging_writer.py:48] [200900] global_step=200900, grad_norm=0.22922633588314056, loss=1.3958090543746948
I0313 14:14:35.662566 139664567539456 logging_writer.py:48] [201000] global_step=201000, grad_norm=0.2351967841386795, loss=1.48306143283844
I0313 14:15:11.253975 139664575932160 logging_writer.py:48] [201100] global_step=201100, grad_norm=0.23544937372207642, loss=1.45614492893219
I0313 14:15:46.795253 139664567539456 logging_writer.py:48] [201200] global_step=201200, grad_norm=0.2284517139196396, loss=1.4404330253601074
I0313 14:16:22.384515 139664575932160 logging_writer.py:48] [201300] global_step=201300, grad_norm=0.2398185282945633, loss=1.5254734754562378
I0313 14:16:57.976907 139664567539456 logging_writer.py:48] [201400] global_step=201400, grad_norm=0.23640844225883484, loss=1.499464511871338
I0313 14:17:33.586056 139664575932160 logging_writer.py:48] [201500] global_step=201500, grad_norm=0.24309596419334412, loss=1.4492042064666748
I0313 14:18:09.216386 139664567539456 logging_writer.py:48] [201600] global_step=201600, grad_norm=0.22925463318824768, loss=1.3736354112625122
I0313 14:18:44.809562 139664575932160 logging_writer.py:48] [201700] global_step=201700, grad_norm=0.22951611876487732, loss=1.3925725221633911
I0313 14:19:20.399351 139664567539456 logging_writer.py:48] [201800] global_step=201800, grad_norm=0.23433896899223328, loss=1.4640120267868042
I0313 14:19:55.993737 139664575932160 logging_writer.py:48] [201900] global_step=201900, grad_norm=0.22589363157749176, loss=1.460584044456482
I0313 14:20:31.580730 139664567539456 logging_writer.py:48] [202000] global_step=202000, grad_norm=0.23078051209449768, loss=1.430610179901123
I0313 14:21:07.190293 139664575932160 logging_writer.py:48] [202100] global_step=202100, grad_norm=0.23539412021636963, loss=1.4991967678070068
I0313 14:21:42.783702 139664567539456 logging_writer.py:48] [202200] global_step=202200, grad_norm=0.2270774245262146, loss=1.4836987257003784
I0313 14:22:18.375720 139664575932160 logging_writer.py:48] [202300] global_step=202300, grad_norm=0.23091727495193481, loss=1.3723915815353394
I0313 14:22:53.963556 139664567539456 logging_writer.py:48] [202400] global_step=202400, grad_norm=0.23185589909553528, loss=1.4536350965499878
I0313 14:23:29.565371 139664575932160 logging_writer.py:48] [202500] global_step=202500, grad_norm=0.24338975548744202, loss=1.4934008121490479
I0313 14:24:05.153565 139664567539456 logging_writer.py:48] [202600] global_step=202600, grad_norm=0.24173252284526825, loss=1.4333996772766113
I0313 14:24:40.757246 139664575932160 logging_writer.py:48] [202700] global_step=202700, grad_norm=0.23569664359092712, loss=1.4656833410263062
I0313 14:25:16.369357 139664567539456 logging_writer.py:48] [202800] global_step=202800, grad_norm=0.22613458335399628, loss=1.4518449306488037
I0313 14:25:51.979763 139664575932160 logging_writer.py:48] [202900] global_step=202900, grad_norm=0.23630203306674957, loss=1.498220443725586
I0313 14:26:27.673683 139664567539456 logging_writer.py:48] [203000] global_step=203000, grad_norm=0.2344895303249359, loss=1.484559178352356
I0313 14:27:03.278197 139664575932160 logging_writer.py:48] [203100] global_step=203100, grad_norm=0.2366465926170349, loss=1.381321907043457
I0313 14:27:09.403610 139834281293632 spec.py:321] Evaluating on the training split.
I0313 14:27:12.373965 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:30:33.490132 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 14:30:36.159801 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:33:50.255461 139834281293632 spec.py:349] Evaluating on the test split.
I0313 14:33:52.924196 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:37:02.920911 139834281293632 submission_runner.py:420] Time since start: 123991.47s, 	Step: 203119, 	{'train/accuracy': 0.6929601430892944, 'train/loss': 1.3850677013397217, 'train/bleu': 35.516672633384594, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 72284.34855413437, 'total_duration': 123991.47338604927, 'accumulated_submission_time': 72284.34855413437, 'accumulated_eval_time': 51697.62382078171, 'accumulated_logging_time': 3.3048605918884277}
I0313 14:37:02.961935 139664567539456 logging_writer.py:48] [203119] accumulated_eval_time=51697.623821, accumulated_logging_time=3.304861, accumulated_submission_time=72284.348554, global_step=203119, preemption_count=0, score=72284.348554, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=123991.473386, train/accuracy=0.692960, train/bleu=35.516673, train/loss=1.385068, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 14:37:32.110118 139664575932160 logging_writer.py:48] [203200] global_step=203200, grad_norm=0.22999104857444763, loss=1.406917929649353
I0313 14:38:07.663469 139664567539456 logging_writer.py:48] [203300] global_step=203300, grad_norm=0.24062038958072662, loss=1.5146758556365967
I0313 14:38:43.265683 139664575932160 logging_writer.py:48] [203400] global_step=203400, grad_norm=0.22959968447685242, loss=1.4140340089797974
I0313 14:39:18.839463 139664567539456 logging_writer.py:48] [203500] global_step=203500, grad_norm=0.24191173911094666, loss=1.5122121572494507
I0313 14:39:54.470526 139664575932160 logging_writer.py:48] [203600] global_step=203600, grad_norm=0.23506738245487213, loss=1.4272737503051758
I0313 14:40:30.072441 139664567539456 logging_writer.py:48] [203700] global_step=203700, grad_norm=0.2279156595468521, loss=1.388951063156128
I0313 14:41:05.672225 139664575932160 logging_writer.py:48] [203800] global_step=203800, grad_norm=0.22741137444972992, loss=1.404021978378296
I0313 14:41:41.288756 139664567539456 logging_writer.py:48] [203900] global_step=203900, grad_norm=0.23078709840774536, loss=1.4333505630493164
I0313 14:42:16.884125 139664575932160 logging_writer.py:48] [204000] global_step=204000, grad_norm=0.23401899635791779, loss=1.422047734260559
I0313 14:42:52.485829 139664567539456 logging_writer.py:48] [204100] global_step=204100, grad_norm=0.23432904481887817, loss=1.5150984525680542
I0313 14:43:28.079921 139664575932160 logging_writer.py:48] [204200] global_step=204200, grad_norm=0.2308647334575653, loss=1.4603495597839355
I0313 14:44:03.693153 139664567539456 logging_writer.py:48] [204300] global_step=204300, grad_norm=0.22603508830070496, loss=1.418364405632019
I0313 14:44:39.303106 139664575932160 logging_writer.py:48] [204400] global_step=204400, grad_norm=0.22825920581817627, loss=1.4996607303619385
I0313 14:45:14.907809 139664567539456 logging_writer.py:48] [204500] global_step=204500, grad_norm=0.24166113138198853, loss=1.4064065217971802
I0313 14:45:50.509890 139664575932160 logging_writer.py:48] [204600] global_step=204600, grad_norm=0.22570057213306427, loss=1.3941411972045898
I0313 14:46:26.112063 139664567539456 logging_writer.py:48] [204700] global_step=204700, grad_norm=0.2394898384809494, loss=1.4533978700637817
I0313 14:47:01.712831 139664575932160 logging_writer.py:48] [204800] global_step=204800, grad_norm=0.23084232211112976, loss=1.4230419397354126
I0313 14:47:37.308407 139664567539456 logging_writer.py:48] [204900] global_step=204900, grad_norm=0.234696164727211, loss=1.4792938232421875
I0313 14:48:12.898787 139664575932160 logging_writer.py:48] [205000] global_step=205000, grad_norm=0.2416927069425583, loss=1.4379663467407227
I0313 14:48:48.467397 139664567539456 logging_writer.py:48] [205100] global_step=205100, grad_norm=0.2306785136461258, loss=1.4826442003250122
I0313 14:49:24.054439 139664575932160 logging_writer.py:48] [205200] global_step=205200, grad_norm=0.23269033432006836, loss=1.3794571161270142
I0313 14:49:59.649152 139664567539456 logging_writer.py:48] [205300] global_step=205300, grad_norm=0.23731817305088043, loss=1.422806978225708
I0313 14:50:35.258941 139664575932160 logging_writer.py:48] [205400] global_step=205400, grad_norm=0.22137577831745148, loss=1.4035452604293823
I0313 14:51:03.096742 139834281293632 spec.py:321] Evaluating on the training split.
I0313 14:51:06.072019 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:54:38.967142 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 14:54:41.637462 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 14:57:55.594055 139834281293632 spec.py:349] Evaluating on the test split.
I0313 14:57:58.270898 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:01:08.226828 139834281293632 submission_runner.py:420] Time since start: 125436.78s, 	Step: 205480, 	{'train/accuracy': 0.6939802765846252, 'train/loss': 1.3751130104064941, 'train/bleu': 35.385792217309884, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 73124.40326523781, 'total_duration': 125436.77930998802, 'accumulated_submission_time': 73124.40326523781, 'accumulated_eval_time': 52302.753858566284, 'accumulated_logging_time': 3.355266809463501}
I0313 15:01:08.270008 139664567539456 logging_writer.py:48] [205480] accumulated_eval_time=52302.753859, accumulated_logging_time=3.355267, accumulated_submission_time=73124.403265, global_step=205480, preemption_count=0, score=73124.403265, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=125436.779310, train/accuracy=0.693980, train/bleu=35.385792, train/loss=1.375113, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 15:01:15.739284 139664575932160 logging_writer.py:48] [205500] global_step=205500, grad_norm=0.2295849472284317, loss=1.3654359579086304
I0313 15:01:51.297833 139664567539456 logging_writer.py:48] [205600] global_step=205600, grad_norm=0.22862987220287323, loss=1.4590378999710083
I0313 15:02:26.858927 139664575932160 logging_writer.py:48] [205700] global_step=205700, grad_norm=0.2249802052974701, loss=1.4061318635940552
I0313 15:03:02.424517 139664567539456 logging_writer.py:48] [205800] global_step=205800, grad_norm=0.23493222892284393, loss=1.4238309860229492
I0313 15:03:38.098025 139664575932160 logging_writer.py:48] [205900] global_step=205900, grad_norm=0.23392078280448914, loss=1.458128571510315
I0313 15:04:13.702101 139664567539456 logging_writer.py:48] [206000] global_step=206000, grad_norm=0.2348172813653946, loss=1.4426214694976807
I0313 15:04:49.296136 139664575932160 logging_writer.py:48] [206100] global_step=206100, grad_norm=0.23585321009159088, loss=1.4597326517105103
I0313 15:05:24.895739 139664567539456 logging_writer.py:48] [206200] global_step=206200, grad_norm=0.24014684557914734, loss=1.4395045042037964
I0313 15:06:00.470676 139664575932160 logging_writer.py:48] [206300] global_step=206300, grad_norm=0.24580515921115875, loss=1.505385398864746
I0313 15:06:36.054433 139664567539456 logging_writer.py:48] [206400] global_step=206400, grad_norm=0.22316646575927734, loss=1.4401168823242188
I0313 15:07:11.663520 139664575932160 logging_writer.py:48] [206500] global_step=206500, grad_norm=0.2389303743839264, loss=1.4576705694198608
I0313 15:07:47.257884 139664567539456 logging_writer.py:48] [206600] global_step=206600, grad_norm=0.22507140040397644, loss=1.3344321250915527
I0313 15:08:22.830092 139664575932160 logging_writer.py:48] [206700] global_step=206700, grad_norm=0.242293119430542, loss=1.6117199659347534
I0313 15:08:58.457613 139664567539456 logging_writer.py:48] [206800] global_step=206800, grad_norm=0.23124408721923828, loss=1.499144196510315
I0313 15:09:34.043416 139664575932160 logging_writer.py:48] [206900] global_step=206900, grad_norm=0.22724886238574982, loss=1.4408026933670044
I0313 15:10:09.643458 139664567539456 logging_writer.py:48] [207000] global_step=207000, grad_norm=0.22958409786224365, loss=1.3718338012695312
I0313 15:10:45.265025 139664575932160 logging_writer.py:48] [207100] global_step=207100, grad_norm=0.2307780385017395, loss=1.423117756843567
I0313 15:11:20.913096 139664567539456 logging_writer.py:48] [207200] global_step=207200, grad_norm=0.23451057076454163, loss=1.4188870191574097
I0313 15:11:56.488960 139664575932160 logging_writer.py:48] [207300] global_step=207300, grad_norm=0.23139682412147522, loss=1.406026005744934
I0313 15:12:32.078884 139664567539456 logging_writer.py:48] [207400] global_step=207400, grad_norm=0.23367619514465332, loss=1.4761567115783691
I0313 15:13:07.634796 139664575932160 logging_writer.py:48] [207500] global_step=207500, grad_norm=0.23756174743175507, loss=1.4569344520568848
I0313 15:13:43.228463 139664567539456 logging_writer.py:48] [207600] global_step=207600, grad_norm=0.2405678629875183, loss=1.4843416213989258
I0313 15:14:18.782370 139664575932160 logging_writer.py:48] [207700] global_step=207700, grad_norm=0.2293502241373062, loss=1.480215311050415
I0313 15:14:54.384473 139664567539456 logging_writer.py:48] [207800] global_step=207800, grad_norm=0.23698295652866364, loss=1.4237936735153198
I0313 15:15:08.317871 139834281293632 spec.py:321] Evaluating on the training split.
I0313 15:15:11.293537 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:18:55.145105 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 15:18:57.834152 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:22:12.008557 139834281293632 spec.py:349] Evaluating on the test split.
I0313 15:22:14.683874 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:25:26.309937 139834281293632 submission_runner.py:420] Time since start: 126894.86s, 	Step: 207841, 	{'train/accuracy': 0.697274386882782, 'train/loss': 1.3620195388793945, 'train/bleu': 35.38380765178742, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 73964.36746478081, 'total_duration': 126894.86238741875, 'accumulated_submission_time': 73964.36746478081, 'accumulated_eval_time': 52920.74584150314, 'accumulated_logging_time': 3.409360647201538}
I0313 15:25:26.360889 139664575932160 logging_writer.py:48] [207841] accumulated_eval_time=52920.745842, accumulated_logging_time=3.409361, accumulated_submission_time=73964.367465, global_step=207841, preemption_count=0, score=73964.367465, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=126894.862387, train/accuracy=0.697274, train/bleu=35.383808, train/loss=1.362020, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 15:25:47.746253 139664567539456 logging_writer.py:48] [207900] global_step=207900, grad_norm=0.24865490198135376, loss=1.5117838382720947
I0313 15:26:23.324384 139664575932160 logging_writer.py:48] [208000] global_step=208000, grad_norm=0.23461587727069855, loss=1.4297336339950562
I0313 15:26:58.888971 139664567539456 logging_writer.py:48] [208100] global_step=208100, grad_norm=0.22642333805561066, loss=1.385392665863037
I0313 15:27:34.447525 139664575932160 logging_writer.py:48] [208200] global_step=208200, grad_norm=0.2300509661436081, loss=1.409561276435852
I0313 15:28:10.040910 139664567539456 logging_writer.py:48] [208300] global_step=208300, grad_norm=0.23167909681797028, loss=1.4542204141616821
I0313 15:28:45.617606 139664575932160 logging_writer.py:48] [208400] global_step=208400, grad_norm=0.2377994805574417, loss=1.4420983791351318
I0313 15:29:21.220989 139664567539456 logging_writer.py:48] [208500] global_step=208500, grad_norm=0.23504091799259186, loss=1.4901151657104492
I0313 15:29:56.823258 139664575932160 logging_writer.py:48] [208600] global_step=208600, grad_norm=0.23196826875209808, loss=1.4539767503738403
I0313 15:30:32.425904 139664567539456 logging_writer.py:48] [208700] global_step=208700, grad_norm=0.23551125824451447, loss=1.4626401662826538
I0313 15:31:07.995691 139664575932160 logging_writer.py:48] [208800] global_step=208800, grad_norm=0.2217191904783249, loss=1.4161479473114014
I0313 15:31:43.586217 139664567539456 logging_writer.py:48] [208900] global_step=208900, grad_norm=0.23024769127368927, loss=1.3817505836486816
I0313 15:32:19.197063 139664575932160 logging_writer.py:48] [209000] global_step=209000, grad_norm=0.2367401123046875, loss=1.529468059539795
I0313 15:32:54.806716 139664567539456 logging_writer.py:48] [209100] global_step=209100, grad_norm=0.23016731441020966, loss=1.4093910455703735
I0313 15:33:30.419769 139664575932160 logging_writer.py:48] [209200] global_step=209200, grad_norm=0.23242802917957306, loss=1.4151724576950073
I0313 15:34:06.020411 139664567539456 logging_writer.py:48] [209300] global_step=209300, grad_norm=0.24298042058944702, loss=1.5025216341018677
I0313 15:34:41.643598 139664575932160 logging_writer.py:48] [209400] global_step=209400, grad_norm=0.22986045479774475, loss=1.4911143779754639
I0313 15:35:17.221266 139664567539456 logging_writer.py:48] [209500] global_step=209500, grad_norm=0.22858281433582306, loss=1.4202241897583008
I0313 15:35:52.788059 139664575932160 logging_writer.py:48] [209600] global_step=209600, grad_norm=0.23276269435882568, loss=1.4669584035873413
I0313 15:36:28.385293 139664567539456 logging_writer.py:48] [209700] global_step=209700, grad_norm=0.2431381493806839, loss=1.5382461547851562
I0313 15:37:04.002530 139664575932160 logging_writer.py:48] [209800] global_step=209800, grad_norm=0.22513000667095184, loss=1.397768497467041
I0313 15:37:39.605532 139664567539456 logging_writer.py:48] [209900] global_step=209900, grad_norm=0.226886585354805, loss=1.411922812461853
I0313 15:38:15.237143 139664575932160 logging_writer.py:48] [210000] global_step=210000, grad_norm=0.22820046544075012, loss=1.4578088521957397
I0313 15:38:50.846064 139664567539456 logging_writer.py:48] [210100] global_step=210100, grad_norm=0.2286537140607834, loss=1.4656625986099243
I0313 15:39:26.481263 139664575932160 logging_writer.py:48] [210200] global_step=210200, grad_norm=0.22233347594738007, loss=1.3917789459228516
I0313 15:39:26.487311 139834281293632 spec.py:321] Evaluating on the training split.
I0313 15:39:29.166937 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:43:04.505010 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 15:43:07.175973 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:46:21.275011 139834281293632 spec.py:349] Evaluating on the test split.
I0313 15:46:23.947991 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 15:49:33.713923 139834281293632 submission_runner.py:420] Time since start: 128342.27s, 	Step: 210201, 	{'train/accuracy': 0.6986714601516724, 'train/loss': 1.350701928138733, 'train/bleu': 35.170028935962286, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 74804.41276717186, 'total_duration': 128342.26640844345, 'accumulated_submission_time': 74804.41276717186, 'accumulated_eval_time': 53527.97238135338, 'accumulated_logging_time': 3.470571994781494}
I0313 15:49:33.755495 139664567539456 logging_writer.py:48] [210201] accumulated_eval_time=53527.972381, accumulated_logging_time=3.470572, accumulated_submission_time=74804.412767, global_step=210201, preemption_count=0, score=74804.412767, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=128342.266408, train/accuracy=0.698671, train/bleu=35.170029, train/loss=1.350702, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 15:50:09.303856 139664575932160 logging_writer.py:48] [210300] global_step=210300, grad_norm=0.2333461344242096, loss=1.4571346044540405
I0313 15:50:44.836881 139664567539456 logging_writer.py:48] [210400] global_step=210400, grad_norm=0.23162035644054413, loss=1.411009669303894
I0313 15:51:20.449570 139664575932160 logging_writer.py:48] [210500] global_step=210500, grad_norm=0.24305349588394165, loss=1.4766913652420044
I0313 15:51:56.022099 139664567539456 logging_writer.py:48] [210600] global_step=210600, grad_norm=0.23480291664600372, loss=1.4311014413833618
I0313 15:52:31.623184 139664575932160 logging_writer.py:48] [210700] global_step=210700, grad_norm=0.22639033198356628, loss=1.3819018602371216
I0313 15:53:07.207736 139664567539456 logging_writer.py:48] [210800] global_step=210800, grad_norm=0.2287643700838089, loss=1.4309639930725098
I0313 15:53:42.758265 139664575932160 logging_writer.py:48] [210900] global_step=210900, grad_norm=0.23969796299934387, loss=1.4015520811080933
I0313 15:54:18.345919 139664567539456 logging_writer.py:48] [211000] global_step=211000, grad_norm=0.22973456978797913, loss=1.4268126487731934
I0313 15:54:53.926896 139664575932160 logging_writer.py:48] [211100] global_step=211100, grad_norm=0.23644502460956573, loss=1.4092210531234741
I0313 15:55:29.509470 139664567539456 logging_writer.py:48] [211200] global_step=211200, grad_norm=0.2313900738954544, loss=1.4621189832687378
I0313 15:56:05.113007 139664575932160 logging_writer.py:48] [211300] global_step=211300, grad_norm=0.22809243202209473, loss=1.4227077960968018
I0313 15:56:40.717911 139664567539456 logging_writer.py:48] [211400] global_step=211400, grad_norm=0.22548337280750275, loss=1.435469150543213
I0313 15:57:16.349411 139664575932160 logging_writer.py:48] [211500] global_step=211500, grad_norm=0.24372120201587677, loss=1.4980438947677612
I0313 15:57:51.947349 139664567539456 logging_writer.py:48] [211600] global_step=211600, grad_norm=0.23628924787044525, loss=1.3935496807098389
I0313 15:58:27.515669 139664575932160 logging_writer.py:48] [211700] global_step=211700, grad_norm=0.23551003634929657, loss=1.498334288597107
I0313 15:59:03.126693 139664567539456 logging_writer.py:48] [211800] global_step=211800, grad_norm=0.23917633295059204, loss=1.452390432357788
I0313 15:59:38.795100 139664575932160 logging_writer.py:48] [211900] global_step=211900, grad_norm=0.2393718659877777, loss=1.4424121379852295
I0313 16:00:14.395449 139664567539456 logging_writer.py:48] [212000] global_step=212000, grad_norm=0.24487513303756714, loss=1.4473838806152344
I0313 16:00:49.954952 139664575932160 logging_writer.py:48] [212100] global_step=212100, grad_norm=0.2284335196018219, loss=1.4842947721481323
I0313 16:01:25.568017 139664567539456 logging_writer.py:48] [212200] global_step=212200, grad_norm=0.22822751104831696, loss=1.4577889442443848
I0313 16:02:01.164048 139664575932160 logging_writer.py:48] [212300] global_step=212300, grad_norm=0.2338716685771942, loss=1.5133936405181885
I0313 16:02:36.784595 139664567539456 logging_writer.py:48] [212400] global_step=212400, grad_norm=0.23256532847881317, loss=1.4686594009399414
I0313 16:03:12.371896 139664575932160 logging_writer.py:48] [212500] global_step=212500, grad_norm=0.22442321479320526, loss=1.4191333055496216
I0313 16:03:33.809536 139834281293632 spec.py:321] Evaluating on the training split.
I0313 16:03:36.800429 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:07:06.073535 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 16:07:08.769437 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:10:23.632854 139834281293632 spec.py:349] Evaluating on the test split.
I0313 16:10:26.307088 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:13:35.902290 139834281293632 submission_runner.py:420] Time since start: 129784.45s, 	Step: 212562, 	{'train/accuracy': 0.699073314666748, 'train/loss': 1.3556746244430542, 'train/bleu': 35.37286067844723, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 75644.38429164886, 'total_duration': 129784.45477199554, 'accumulated_submission_time': 75644.38429164886, 'accumulated_eval_time': 54130.06509780884, 'accumulated_logging_time': 3.5222976207733154}
I0313 16:13:35.945370 139664567539456 logging_writer.py:48] [212562] accumulated_eval_time=54130.065098, accumulated_logging_time=3.522298, accumulated_submission_time=75644.384292, global_step=212562, preemption_count=0, score=75644.384292, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=129784.454772, train/accuracy=0.699073, train/bleu=35.372861, train/loss=1.355675, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 16:13:49.804983 139664575932160 logging_writer.py:48] [212600] global_step=212600, grad_norm=0.22957485914230347, loss=1.4356236457824707
I0313 16:14:25.363218 139664567539456 logging_writer.py:48] [212700] global_step=212700, grad_norm=0.23540975153446198, loss=1.4180344343185425
I0313 16:15:00.928501 139664575932160 logging_writer.py:48] [212800] global_step=212800, grad_norm=0.23219794034957886, loss=1.4452488422393799
I0313 16:15:36.509086 139664567539456 logging_writer.py:48] [212900] global_step=212900, grad_norm=0.23118679225444794, loss=1.3875054121017456
I0313 16:16:12.145681 139664575932160 logging_writer.py:48] [213000] global_step=213000, grad_norm=0.22772076725959778, loss=1.4006389379501343
I0313 16:16:47.732309 139664567539456 logging_writer.py:48] [213100] global_step=213100, grad_norm=0.2320168912410736, loss=1.4615131616592407
I0313 16:17:23.342430 139664575932160 logging_writer.py:48] [213200] global_step=213200, grad_norm=0.2354493886232376, loss=1.4528826475143433
I0313 16:17:58.928119 139664567539456 logging_writer.py:48] [213300] global_step=213300, grad_norm=0.22949831187725067, loss=1.4702612161636353
I0313 16:18:34.515120 139664575932160 logging_writer.py:48] [213400] global_step=213400, grad_norm=0.2315814346075058, loss=1.4104350805282593
I0313 16:19:10.118557 139664567539456 logging_writer.py:48] [213500] global_step=213500, grad_norm=0.23633532226085663, loss=1.4827426671981812
I0313 16:19:45.700645 139664575932160 logging_writer.py:48] [213600] global_step=213600, grad_norm=0.245835080742836, loss=1.4405919313430786
I0313 16:20:21.276052 139664567539456 logging_writer.py:48] [213700] global_step=213700, grad_norm=0.24114565551280975, loss=1.404160976409912
I0313 16:20:56.899351 139664575932160 logging_writer.py:48] [213800] global_step=213800, grad_norm=0.2343709021806717, loss=1.5815762281417847
I0313 16:21:32.501691 139664567539456 logging_writer.py:48] [213900] global_step=213900, grad_norm=0.2452416718006134, loss=1.4583781957626343
I0313 16:22:08.088979 139664575932160 logging_writer.py:48] [214000] global_step=214000, grad_norm=0.2325585037469864, loss=1.4268004894256592
I0313 16:22:43.708872 139664567539456 logging_writer.py:48] [214100] global_step=214100, grad_norm=0.23889106512069702, loss=1.4408910274505615
I0313 16:23:19.305533 139664575932160 logging_writer.py:48] [214200] global_step=214200, grad_norm=0.2319936901330948, loss=1.4542700052261353
I0313 16:23:54.959257 139664567539456 logging_writer.py:48] [214300] global_step=214300, grad_norm=0.22391974925994873, loss=1.4298572540283203
I0313 16:24:30.582592 139664575932160 logging_writer.py:48] [214400] global_step=214400, grad_norm=0.242586150765419, loss=1.442429780960083
I0313 16:25:06.197969 139664567539456 logging_writer.py:48] [214500] global_step=214500, grad_norm=0.22866307199001312, loss=1.468837022781372
I0313 16:25:41.775616 139664575932160 logging_writer.py:48] [214600] global_step=214600, grad_norm=0.2270989567041397, loss=1.495012640953064
I0313 16:26:17.405418 139664567539456 logging_writer.py:48] [214700] global_step=214700, grad_norm=0.2377089112997055, loss=1.41957688331604
I0313 16:26:53.005671 139664575932160 logging_writer.py:48] [214800] global_step=214800, grad_norm=0.23276633024215698, loss=1.4368512630462646
I0313 16:27:28.584677 139664567539456 logging_writer.py:48] [214900] global_step=214900, grad_norm=0.22752171754837036, loss=1.407952070236206
I0313 16:27:36.138460 139834281293632 spec.py:321] Evaluating on the training split.
I0313 16:27:39.111368 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:31:18.868984 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 16:31:21.564327 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:34:37.584385 139834281293632 spec.py:349] Evaluating on the test split.
I0313 16:34:40.284378 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:37:52.062163 139834281293632 submission_runner.py:420] Time since start: 131240.61s, 	Step: 214923, 	{'train/accuracy': 0.6939623951911926, 'train/loss': 1.3788866996765137, 'train/bleu': 35.34666481423096, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 76484.49631166458, 'total_duration': 131240.6146156788, 'accumulated_submission_time': 76484.49631166458, 'accumulated_eval_time': 54745.98873233795, 'accumulated_logging_time': 3.5742805004119873}
I0313 16:37:52.112570 139664575932160 logging_writer.py:48] [214923] accumulated_eval_time=54745.988732, accumulated_logging_time=3.574281, accumulated_submission_time=76484.496312, global_step=214923, preemption_count=0, score=76484.496312, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=131240.614616, train/accuracy=0.693962, train/bleu=35.346665, train/loss=1.378887, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 16:38:19.850671 139664567539456 logging_writer.py:48] [215000] global_step=215000, grad_norm=0.23334452509880066, loss=1.430606484413147
I0313 16:38:55.424601 139664575932160 logging_writer.py:48] [215100] global_step=215100, grad_norm=0.24140872061252594, loss=1.453358769416809
I0313 16:39:30.998187 139664567539456 logging_writer.py:48] [215200] global_step=215200, grad_norm=0.23593439161777496, loss=1.4245059490203857
I0313 16:40:06.564492 139664575932160 logging_writer.py:48] [215300] global_step=215300, grad_norm=0.23512759804725647, loss=1.4307149648666382
I0313 16:40:42.130763 139664567539456 logging_writer.py:48] [215400] global_step=215400, grad_norm=0.23778879642486572, loss=1.4230278730392456
I0313 16:41:17.706086 139664575932160 logging_writer.py:48] [215500] global_step=215500, grad_norm=0.21935850381851196, loss=1.3951599597930908
I0313 16:41:53.306964 139664567539456 logging_writer.py:48] [215600] global_step=215600, grad_norm=0.23121008276939392, loss=1.3986886739730835
I0313 16:42:28.909044 139664575932160 logging_writer.py:48] [215700] global_step=215700, grad_norm=0.2930488586425781, loss=1.4220097064971924
I0313 16:43:04.487568 139664567539456 logging_writer.py:48] [215800] global_step=215800, grad_norm=0.24090418219566345, loss=1.477779746055603
I0313 16:43:40.088996 139664575932160 logging_writer.py:48] [215900] global_step=215900, grad_norm=0.23168183863162994, loss=1.4272234439849854
I0313 16:44:15.675525 139664567539456 logging_writer.py:48] [216000] global_step=216000, grad_norm=0.2331637442111969, loss=1.4916352033615112
I0313 16:44:51.324754 139664575932160 logging_writer.py:48] [216100] global_step=216100, grad_norm=0.23356002569198608, loss=1.4127280712127686
I0313 16:45:26.887794 139664567539456 logging_writer.py:48] [216200] global_step=216200, grad_norm=0.24219708144664764, loss=1.4807363748550415
I0313 16:46:02.450011 139664575932160 logging_writer.py:48] [216300] global_step=216300, grad_norm=0.23090313374996185, loss=1.463800072669983
I0313 16:46:38.020790 139664567539456 logging_writer.py:48] [216400] global_step=216400, grad_norm=0.23591536283493042, loss=1.4412553310394287
I0313 16:47:13.619542 139664575932160 logging_writer.py:48] [216500] global_step=216500, grad_norm=0.2297699898481369, loss=1.4133399724960327
I0313 16:47:49.184046 139664567539456 logging_writer.py:48] [216600] global_step=216600, grad_norm=0.2278618961572647, loss=1.4734580516815186
I0313 16:48:24.771862 139664575932160 logging_writer.py:48] [216700] global_step=216700, grad_norm=0.2320808619260788, loss=1.4340413808822632
I0313 16:49:00.415364 139664567539456 logging_writer.py:48] [216800] global_step=216800, grad_norm=0.24598319828510284, loss=1.4802350997924805
I0313 16:49:36.085174 139664575932160 logging_writer.py:48] [216900] global_step=216900, grad_norm=0.250569224357605, loss=1.4127269983291626
I0313 16:50:11.677693 139664567539456 logging_writer.py:48] [217000] global_step=217000, grad_norm=0.23491834104061127, loss=1.3909406661987305
I0313 16:50:47.273566 139664575932160 logging_writer.py:48] [217100] global_step=217100, grad_norm=0.2385922521352768, loss=1.4458906650543213
I0313 16:51:22.881416 139664567539456 logging_writer.py:48] [217200] global_step=217200, grad_norm=0.2416568249464035, loss=1.4616620540618896
I0313 16:51:52.137462 139834281293632 spec.py:321] Evaluating on the training split.
I0313 16:51:55.115605 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:55:37.006822 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 16:55:39.688349 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 16:58:53.764058 139834281293632 spec.py:349] Evaluating on the test split.
I0313 16:58:56.466233 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:02:06.202265 139834281293632 submission_runner.py:420] Time since start: 132694.75s, 	Step: 217284, 	{'train/accuracy': 0.6991239786148071, 'train/loss': 1.3457363843917847, 'train/bleu': 35.3880923790759, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 77324.43882918358, 'total_duration': 132694.75473570824, 'accumulated_submission_time': 77324.43882918358, 'accumulated_eval_time': 55360.05347490311, 'accumulated_logging_time': 3.6353468894958496}
I0313 17:02:06.245981 139664575932160 logging_writer.py:48] [217284] accumulated_eval_time=55360.053475, accumulated_logging_time=3.635347, accumulated_submission_time=77324.438829, global_step=217284, preemption_count=0, score=77324.438829, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=132694.754736, train/accuracy=0.699124, train/bleu=35.388092, train/loss=1.345736, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 17:02:12.301881 139664567539456 logging_writer.py:48] [217300] global_step=217300, grad_norm=0.23152679204940796, loss=1.4196215867996216
I0313 17:02:47.791324 139664575932160 logging_writer.py:48] [217400] global_step=217400, grad_norm=0.23524639010429382, loss=1.4852772951126099
I0313 17:03:23.360937 139664567539456 logging_writer.py:48] [217500] global_step=217500, grad_norm=0.24496468901634216, loss=1.5107637643814087
I0313 17:03:58.941673 139664575932160 logging_writer.py:48] [217600] global_step=217600, grad_norm=0.2403348982334137, loss=1.4805903434753418
I0313 17:04:34.511775 139664567539456 logging_writer.py:48] [217700] global_step=217700, grad_norm=0.2271086573600769, loss=1.324898362159729
I0313 17:05:10.098076 139664575932160 logging_writer.py:48] [217800] global_step=217800, grad_norm=0.24903114140033722, loss=1.511877417564392
I0313 17:05:45.701084 139664567539456 logging_writer.py:48] [217900] global_step=217900, grad_norm=0.2267017662525177, loss=1.4847724437713623
I0313 17:06:21.292160 139664575932160 logging_writer.py:48] [218000] global_step=218000, grad_norm=0.23777757585048676, loss=1.4690223932266235
I0313 17:06:56.853463 139664567539456 logging_writer.py:48] [218100] global_step=218100, grad_norm=0.23056234419345856, loss=1.4241116046905518
I0313 17:07:32.455393 139664575932160 logging_writer.py:48] [218200] global_step=218200, grad_norm=0.22865954041481018, loss=1.4403184652328491
I0313 17:08:08.039480 139664567539456 logging_writer.py:48] [218300] global_step=218300, grad_norm=0.22846733033657074, loss=1.4504661560058594
I0313 17:08:43.600217 139664575932160 logging_writer.py:48] [218400] global_step=218400, grad_norm=0.23842750489711761, loss=1.4234271049499512
I0313 17:09:19.224363 139664567539456 logging_writer.py:48] [218500] global_step=218500, grad_norm=0.25210216641426086, loss=1.4488004446029663
I0313 17:09:54.848039 139664575932160 logging_writer.py:48] [218600] global_step=218600, grad_norm=0.23570364713668823, loss=1.466849446296692
I0313 17:10:30.432540 139664567539456 logging_writer.py:48] [218700] global_step=218700, grad_norm=0.2348056584596634, loss=1.4249316453933716
I0313 17:11:06.023097 139664575932160 logging_writer.py:48] [218800] global_step=218800, grad_norm=0.2373911440372467, loss=1.4614917039871216
I0313 17:11:41.612483 139664567539456 logging_writer.py:48] [218900] global_step=218900, grad_norm=0.23261447250843048, loss=1.379351258277893
I0313 17:12:17.271087 139664575932160 logging_writer.py:48] [219000] global_step=219000, grad_norm=0.23732048273086548, loss=1.4156543016433716
I0313 17:12:52.869185 139664567539456 logging_writer.py:48] [219100] global_step=219100, grad_norm=0.2308991253376007, loss=1.3920491933822632
I0313 17:13:28.471291 139664575932160 logging_writer.py:48] [219200] global_step=219200, grad_norm=0.22917287051677704, loss=1.4108713865280151
I0313 17:14:04.094078 139664567539456 logging_writer.py:48] [219300] global_step=219300, grad_norm=0.23968996107578278, loss=1.374808430671692
I0313 17:14:39.737293 139664575932160 logging_writer.py:48] [219400] global_step=219400, grad_norm=0.22772543132305145, loss=1.3944066762924194
I0313 17:15:15.366002 139664567539456 logging_writer.py:48] [219500] global_step=219500, grad_norm=0.23254834115505219, loss=1.4710752964019775
I0313 17:15:50.972348 139664575932160 logging_writer.py:48] [219600] global_step=219600, grad_norm=0.24243439733982086, loss=1.4128942489624023
I0313 17:16:06.349292 139834281293632 spec.py:321] Evaluating on the training split.
I0313 17:16:09.325285 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:19:42.014038 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 17:19:44.690719 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:22:58.570340 139834281293632 spec.py:349] Evaluating on the test split.
I0313 17:23:01.230195 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:26:11.140130 139834281293632 submission_runner.py:420] Time since start: 134139.69s, 	Step: 219645, 	{'train/accuracy': 0.6964263319969177, 'train/loss': 1.3674417734146118, 'train/bleu': 35.6649330863915, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 78164.45926618576, 'total_duration': 134139.69259738922, 'accumulated_submission_time': 78164.45926618576, 'accumulated_eval_time': 55964.844245672226, 'accumulated_logging_time': 3.688246011734009}
I0313 17:26:11.182479 139664567539456 logging_writer.py:48] [219645] accumulated_eval_time=55964.844246, accumulated_logging_time=3.688246, accumulated_submission_time=78164.459266, global_step=219645, preemption_count=0, score=78164.459266, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=134139.692597, train/accuracy=0.696426, train/bleu=35.664933, train/loss=1.367442, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 17:26:31.080287 139664575932160 logging_writer.py:48] [219700] global_step=219700, grad_norm=0.2366439253091812, loss=1.4063634872436523
I0313 17:27:06.641497 139664567539456 logging_writer.py:48] [219800] global_step=219800, grad_norm=0.22920817136764526, loss=1.4476730823516846
I0313 17:27:42.226486 139664575932160 logging_writer.py:48] [219900] global_step=219900, grad_norm=0.24169833958148956, loss=1.4788817167282104
I0313 17:28:17.798442 139664567539456 logging_writer.py:48] [220000] global_step=220000, grad_norm=0.23917090892791748, loss=1.4587053060531616
I0313 17:28:53.382630 139664575932160 logging_writer.py:48] [220100] global_step=220100, grad_norm=0.23493027687072754, loss=1.434876799583435
I0313 17:29:28.960824 139664567539456 logging_writer.py:48] [220200] global_step=220200, grad_norm=0.22788836061954498, loss=1.4814468622207642
I0313 17:30:04.584205 139664575932160 logging_writer.py:48] [220300] global_step=220300, grad_norm=0.2608721852302551, loss=1.5635930299758911
I0313 17:30:40.195243 139664567539456 logging_writer.py:48] [220400] global_step=220400, grad_norm=0.2240256518125534, loss=1.3920975923538208
I0313 17:31:15.783519 139664575932160 logging_writer.py:48] [220500] global_step=220500, grad_norm=0.2323494404554367, loss=1.4723718166351318
I0313 17:31:51.351032 139664567539456 logging_writer.py:48] [220600] global_step=220600, grad_norm=0.22780536115169525, loss=1.4207285642623901
I0313 17:32:26.918458 139664575932160 logging_writer.py:48] [220700] global_step=220700, grad_norm=0.24332073330879211, loss=1.402858018875122
I0313 17:33:02.510544 139664567539456 logging_writer.py:48] [220800] global_step=220800, grad_norm=0.22929394245147705, loss=1.4080135822296143
I0313 17:33:38.135679 139664575932160 logging_writer.py:48] [220900] global_step=220900, grad_norm=0.23499703407287598, loss=1.467366337776184
I0313 17:34:13.733400 139664567539456 logging_writer.py:48] [221000] global_step=221000, grad_norm=0.2208741307258606, loss=1.4372940063476562
I0313 17:34:49.358228 139664575932160 logging_writer.py:48] [221100] global_step=221100, grad_norm=0.23001447319984436, loss=1.4649654626846313
I0313 17:35:24.951126 139664567539456 logging_writer.py:48] [221200] global_step=221200, grad_norm=0.3897489011287689, loss=1.4790664911270142
I0313 17:36:00.562311 139664575932160 logging_writer.py:48] [221300] global_step=221300, grad_norm=0.23226776719093323, loss=1.4422903060913086
I0313 17:36:36.133994 139664567539456 logging_writer.py:48] [221400] global_step=221400, grad_norm=0.23936761915683746, loss=1.4792476892471313
I0313 17:37:11.763765 139664575932160 logging_writer.py:48] [221500] global_step=221500, grad_norm=0.23402555286884308, loss=1.484323501586914
I0313 17:37:47.340871 139664567539456 logging_writer.py:48] [221600] global_step=221600, grad_norm=0.22867673635482788, loss=1.3418225049972534
I0313 17:38:22.946737 139664575932160 logging_writer.py:48] [221700] global_step=221700, grad_norm=0.22663922607898712, loss=1.3496911525726318
I0313 17:38:58.525591 139664567539456 logging_writer.py:48] [221800] global_step=221800, grad_norm=0.23990026116371155, loss=1.5060185194015503
I0313 17:39:34.113452 139664575932160 logging_writer.py:48] [221900] global_step=221900, grad_norm=0.2361549586057663, loss=1.4533315896987915
I0313 17:40:09.703632 139664567539456 logging_writer.py:48] [222000] global_step=222000, grad_norm=0.22953708469867706, loss=1.4290472269058228
I0313 17:40:11.205319 139834281293632 spec.py:321] Evaluating on the training split.
I0313 17:40:14.176153 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:43:58.516446 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 17:44:01.191305 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:47:15.115091 139834281293632 spec.py:349] Evaluating on the test split.
I0313 17:47:17.785212 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 17:50:27.453178 139834281293632 submission_runner.py:420] Time since start: 135596.01s, 	Step: 222006, 	{'train/accuracy': 0.6972445249557495, 'train/loss': 1.3616267442703247, 'train/bleu': 35.77868590202388, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 79004.4024219513, 'total_duration': 135596.00564146042, 'accumulated_submission_time': 79004.4024219513, 'accumulated_eval_time': 56581.09203457832, 'accumulated_logging_time': 3.739297866821289}
I0313 17:50:27.496869 139664575932160 logging_writer.py:48] [222006] accumulated_eval_time=56581.092035, accumulated_logging_time=3.739298, accumulated_submission_time=79004.402422, global_step=222006, preemption_count=0, score=79004.402422, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=135596.005641, train/accuracy=0.697245, train/bleu=35.778686, train/loss=1.361627, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 17:51:01.214498 139664567539456 logging_writer.py:48] [222100] global_step=222100, grad_norm=0.24186137318611145, loss=1.4708856344223022
I0313 17:51:36.782843 139664575932160 logging_writer.py:48] [222200] global_step=222200, grad_norm=0.2378310263156891, loss=1.3831926584243774
I0313 17:52:12.377251 139664567539456 logging_writer.py:48] [222300] global_step=222300, grad_norm=0.23105865716934204, loss=1.4189256429672241
I0313 17:52:47.961092 139664575932160 logging_writer.py:48] [222400] global_step=222400, grad_norm=0.24655331671237946, loss=1.4217720031738281
I0313 17:53:23.593054 139664567539456 logging_writer.py:48] [222500] global_step=222500, grad_norm=0.22403006255626678, loss=1.3739982843399048
I0313 17:53:59.230547 139664575932160 logging_writer.py:48] [222600] global_step=222600, grad_norm=0.23821859061717987, loss=1.4571110010147095
I0313 17:54:34.821491 139664567539456 logging_writer.py:48] [222700] global_step=222700, grad_norm=0.24287228286266327, loss=1.4129754304885864
I0313 17:55:10.417304 139664575932160 logging_writer.py:48] [222800] global_step=222800, grad_norm=0.22985674440860748, loss=1.414552092552185
I0313 17:55:45.987909 139664567539456 logging_writer.py:48] [222900] global_step=222900, grad_norm=0.2439955174922943, loss=1.5113791227340698
I0313 17:56:21.585965 139664575932160 logging_writer.py:48] [223000] global_step=223000, grad_norm=0.23256134986877441, loss=1.397377371788025
I0313 17:56:57.198675 139664567539456 logging_writer.py:48] [223100] global_step=223100, grad_norm=0.23368823528289795, loss=1.436647653579712
I0313 17:57:32.802473 139664575932160 logging_writer.py:48] [223200] global_step=223200, grad_norm=0.2498725950717926, loss=1.3745802640914917
I0313 17:58:08.421060 139664567539456 logging_writer.py:48] [223300] global_step=223300, grad_norm=0.23282845318317413, loss=1.4575371742248535
I0313 17:58:44.018238 139664575932160 logging_writer.py:48] [223400] global_step=223400, grad_norm=0.23050394654273987, loss=1.4248883724212646
I0313 17:59:19.624744 139664567539456 logging_writer.py:48] [223500] global_step=223500, grad_norm=0.23096276819705963, loss=1.4692784547805786
I0313 17:59:55.226213 139664575932160 logging_writer.py:48] [223600] global_step=223600, grad_norm=0.23361822962760925, loss=1.3911699056625366
I0313 18:00:30.823822 139664567539456 logging_writer.py:48] [223700] global_step=223700, grad_norm=0.24275536835193634, loss=1.4782062768936157
I0313 18:01:06.433769 139664575932160 logging_writer.py:48] [223800] global_step=223800, grad_norm=0.2338564544916153, loss=1.4244834184646606
I0313 18:01:42.048803 139664567539456 logging_writer.py:48] [223900] global_step=223900, grad_norm=0.240876242518425, loss=1.419110655784607
I0313 18:02:17.685180 139664575932160 logging_writer.py:48] [224000] global_step=224000, grad_norm=0.23592574894428253, loss=1.3874386548995972
I0313 18:02:53.281275 139664567539456 logging_writer.py:48] [224100] global_step=224100, grad_norm=0.23126034438610077, loss=1.3759379386901855
I0313 18:03:28.868015 139664575932160 logging_writer.py:48] [224200] global_step=224200, grad_norm=0.2349092811346054, loss=1.4831353425979614
I0313 18:04:04.444926 139664567539456 logging_writer.py:48] [224300] global_step=224300, grad_norm=0.23930129408836365, loss=1.4801278114318848
I0313 18:04:27.657994 139834281293632 spec.py:321] Evaluating on the training split.
I0313 18:04:30.630961 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:08:10.558179 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 18:08:13.236903 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:11:29.158643 139834281293632 spec.py:349] Evaluating on the test split.
I0313 18:11:31.839311 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:14:41.445039 139834281293632 submission_runner.py:420] Time since start: 137050.00s, 	Step: 224367, 	{'train/accuracy': 0.6952289342880249, 'train/loss': 1.3711453676223755, 'train/bleu': 35.30621007605715, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 79844.48212599754, 'total_duration': 137049.9975218773, 'accumulated_submission_time': 79844.48212599754, 'accumulated_eval_time': 57194.87903165817, 'accumulated_logging_time': 3.7929728031158447}
I0313 18:14:41.489003 139664575932160 logging_writer.py:48] [224367] accumulated_eval_time=57194.879032, accumulated_logging_time=3.792973, accumulated_submission_time=79844.482126, global_step=224367, preemption_count=0, score=79844.482126, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=137049.997522, train/accuracy=0.695229, train/bleu=35.306210, train/loss=1.371145, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 18:14:53.580599 139664567539456 logging_writer.py:48] [224400] global_step=224400, grad_norm=0.23992829024791718, loss=1.4556807279586792
I0313 18:15:29.137516 139664575932160 logging_writer.py:48] [224500] global_step=224500, grad_norm=0.2318396419286728, loss=1.385578989982605
I0313 18:16:04.742609 139664567539456 logging_writer.py:48] [224600] global_step=224600, grad_norm=0.2373901605606079, loss=1.4912105798721313
I0313 18:16:40.348799 139664575932160 logging_writer.py:48] [224700] global_step=224700, grad_norm=0.2278483659029007, loss=1.387839674949646
I0313 18:17:15.987208 139664567539456 logging_writer.py:48] [224800] global_step=224800, grad_norm=0.23451419174671173, loss=1.479880928993225
I0313 18:17:51.551941 139664575932160 logging_writer.py:48] [224900] global_step=224900, grad_norm=0.23165324330329895, loss=1.3847662210464478
I0313 18:18:27.127126 139664567539456 logging_writer.py:48] [225000] global_step=225000, grad_norm=0.2297065556049347, loss=1.4479303359985352
I0313 18:19:02.725681 139664575932160 logging_writer.py:48] [225100] global_step=225100, grad_norm=0.2360132932662964, loss=1.458400845527649
I0313 18:19:38.296653 139664567539456 logging_writer.py:48] [225200] global_step=225200, grad_norm=0.23260584473609924, loss=1.480899453163147
I0313 18:20:13.880434 139664575932160 logging_writer.py:48] [225300] global_step=225300, grad_norm=0.23206235468387604, loss=1.5016554594039917
I0313 18:20:49.459458 139664567539456 logging_writer.py:48] [225400] global_step=225400, grad_norm=0.2264547497034073, loss=1.490803599357605
I0313 18:21:25.029887 139664575932160 logging_writer.py:48] [225500] global_step=225500, grad_norm=0.23000988364219666, loss=1.4830931425094604
I0313 18:22:00.636883 139664567539456 logging_writer.py:48] [225600] global_step=225600, grad_norm=0.23125778138637543, loss=1.4642677307128906
I0313 18:22:36.245998 139664575932160 logging_writer.py:48] [225700] global_step=225700, grad_norm=0.23021100461483002, loss=1.452006459236145
I0313 18:23:11.879062 139664567539456 logging_writer.py:48] [225800] global_step=225800, grad_norm=0.23086899518966675, loss=1.4806855916976929
I0313 18:23:47.502324 139664575932160 logging_writer.py:48] [225900] global_step=225900, grad_norm=0.23728077113628387, loss=1.4592009782791138
I0313 18:24:23.152969 139664567539456 logging_writer.py:48] [226000] global_step=226000, grad_norm=0.23556946218013763, loss=1.459059715270996
I0313 18:24:58.783860 139664575932160 logging_writer.py:48] [226100] global_step=226100, grad_norm=0.24537304043769836, loss=1.5456058979034424
I0313 18:25:34.382523 139664567539456 logging_writer.py:48] [226200] global_step=226200, grad_norm=0.23554962873458862, loss=1.3764469623565674
I0313 18:26:09.971477 139664575932160 logging_writer.py:48] [226300] global_step=226300, grad_norm=0.2282359004020691, loss=1.4542100429534912
I0313 18:26:45.576004 139664567539456 logging_writer.py:48] [226400] global_step=226400, grad_norm=0.23946228623390198, loss=1.3958063125610352
I0313 18:27:21.218453 139664575932160 logging_writer.py:48] [226500] global_step=226500, grad_norm=0.23605364561080933, loss=1.5244133472442627
I0313 18:27:56.871164 139664567539456 logging_writer.py:48] [226600] global_step=226600, grad_norm=0.23114179074764252, loss=1.4419806003570557
I0313 18:28:32.528105 139664575932160 logging_writer.py:48] [226700] global_step=226700, grad_norm=0.22862808406352997, loss=1.368886113166809
I0313 18:28:41.525713 139834281293632 spec.py:321] Evaluating on the training split.
I0313 18:28:44.518586 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:32:15.995487 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 18:32:18.680305 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:35:32.660462 139834281293632 spec.py:349] Evaluating on the test split.
I0313 18:35:35.337850 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:38:45.312282 139834281293632 submission_runner.py:420] Time since start: 138493.86s, 	Step: 226727, 	{'train/accuracy': 0.6972992420196533, 'train/loss': 1.3629869222640991, 'train/bleu': 35.395366730208174, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 80684.43466353416, 'total_duration': 138493.86475038528, 'accumulated_submission_time': 80684.43466353416, 'accumulated_eval_time': 57798.66555047035, 'accumulated_logging_time': 3.8469254970550537}
I0313 18:38:45.357210 139664567539456 logging_writer.py:48] [226727] accumulated_eval_time=57798.665550, accumulated_logging_time=3.846925, accumulated_submission_time=80684.434664, global_step=226727, preemption_count=0, score=80684.434664, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=138493.864750, train/accuracy=0.697299, train/bleu=35.395367, train/loss=1.362987, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 18:39:11.642700 139664575932160 logging_writer.py:48] [226800] global_step=226800, grad_norm=0.23916569352149963, loss=1.4154647588729858
I0313 18:39:47.194225 139664567539456 logging_writer.py:48] [226900] global_step=226900, grad_norm=0.2478865534067154, loss=1.5401618480682373
I0313 18:40:22.780439 139664575932160 logging_writer.py:48] [227000] global_step=227000, grad_norm=0.2383752018213272, loss=1.418083906173706
I0313 18:40:58.370258 139664567539456 logging_writer.py:48] [227100] global_step=227100, grad_norm=0.24072180688381195, loss=1.4215604066848755
I0313 18:41:33.930803 139664575932160 logging_writer.py:48] [227200] global_step=227200, grad_norm=0.2375265657901764, loss=1.4139020442962646
I0313 18:42:09.618164 139664567539456 logging_writer.py:48] [227300] global_step=227300, grad_norm=0.22046782076358795, loss=1.373665690422058
I0313 18:42:45.184848 139664575932160 logging_writer.py:48] [227400] global_step=227400, grad_norm=0.23767416179180145, loss=1.4746384620666504
I0313 18:43:20.793680 139664567539456 logging_writer.py:48] [227500] global_step=227500, grad_norm=0.23800568282604218, loss=1.4457390308380127
I0313 18:43:56.425739 139664575932160 logging_writer.py:48] [227600] global_step=227600, grad_norm=0.22920958697795868, loss=1.4730751514434814
I0313 18:44:32.028381 139664567539456 logging_writer.py:48] [227700] global_step=227700, grad_norm=0.228293314576149, loss=1.443825364112854
I0313 18:45:07.633695 139664575932160 logging_writer.py:48] [227800] global_step=227800, grad_norm=0.23372212052345276, loss=1.4204347133636475
I0313 18:45:43.212886 139664567539456 logging_writer.py:48] [227900] global_step=227900, grad_norm=0.23951253294944763, loss=1.4347206354141235
I0313 18:46:18.823718 139664575932160 logging_writer.py:48] [228000] global_step=228000, grad_norm=0.23688849806785583, loss=1.3893067836761475
I0313 18:46:54.428835 139664567539456 logging_writer.py:48] [228100] global_step=228100, grad_norm=0.6091803312301636, loss=1.5477700233459473
I0313 18:47:30.033260 139664575932160 logging_writer.py:48] [228200] global_step=228200, grad_norm=0.2475508600473404, loss=1.4591089487075806
I0313 18:48:05.630654 139664567539456 logging_writer.py:48] [228300] global_step=228300, grad_norm=0.23502512276172638, loss=1.4158021211624146
I0313 18:48:41.254220 139664575932160 logging_writer.py:48] [228400] global_step=228400, grad_norm=0.23359517753124237, loss=1.4120793342590332
I0313 18:49:16.889142 139664567539456 logging_writer.py:48] [228500] global_step=228500, grad_norm=0.22730393707752228, loss=1.388183832168579
I0313 18:49:52.545455 139664575932160 logging_writer.py:48] [228600] global_step=228600, grad_norm=0.2385542094707489, loss=1.3941880464553833
I0313 18:50:28.154809 139664567539456 logging_writer.py:48] [228700] global_step=228700, grad_norm=0.23397742211818695, loss=1.4540523290634155
I0313 18:51:03.742496 139664575932160 logging_writer.py:48] [228800] global_step=228800, grad_norm=0.2338886857032776, loss=1.3903597593307495
I0313 18:51:39.336419 139664567539456 logging_writer.py:48] [228900] global_step=228900, grad_norm=0.22776563465595245, loss=1.3991010189056396
I0313 18:52:14.950506 139664575932160 logging_writer.py:48] [229000] global_step=229000, grad_norm=0.23855039477348328, loss=1.47386634349823
I0313 18:52:45.638287 139834281293632 spec.py:321] Evaluating on the training split.
I0313 18:52:48.612016 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:56:08.868921 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 18:56:11.554569 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 18:59:25.332381 139834281293632 spec.py:349] Evaluating on the test split.
I0313 18:59:28.017958 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:02:37.762816 139834281293632 submission_runner.py:420] Time since start: 139926.32s, 	Step: 229088, 	{'train/accuracy': 0.6930255889892578, 'train/loss': 1.3843538761138916, 'train/bleu': 35.532078761547865, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 81524.6353867054, 'total_duration': 139926.31528425217, 'accumulated_submission_time': 81524.6353867054, 'accumulated_eval_time': 58390.79001760483, 'accumulated_logging_time': 3.900733709335327}
I0313 19:02:37.808849 139664567539456 logging_writer.py:48] [229088] accumulated_eval_time=58390.790018, accumulated_logging_time=3.900734, accumulated_submission_time=81524.635387, global_step=229088, preemption_count=0, score=81524.635387, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=139926.315284, train/accuracy=0.693026, train/bleu=35.532079, train/loss=1.384354, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 19:02:42.447190 139664575932160 logging_writer.py:48] [229100] global_step=229100, grad_norm=0.23331338167190552, loss=1.4217597246170044
I0313 19:03:17.981863 139664567539456 logging_writer.py:48] [229200] global_step=229200, grad_norm=0.2268855720758438, loss=1.4258586168289185
I0313 19:03:53.550778 139664575932160 logging_writer.py:48] [229300] global_step=229300, grad_norm=0.23189526796340942, loss=1.3859708309173584
I0313 19:04:29.150561 139664567539456 logging_writer.py:48] [229400] global_step=229400, grad_norm=0.24259300529956818, loss=1.4931674003601074
I0313 19:05:04.755754 139664575932160 logging_writer.py:48] [229500] global_step=229500, grad_norm=0.23908817768096924, loss=1.4058789014816284
I0313 19:05:40.354955 139664567539456 logging_writer.py:48] [229600] global_step=229600, grad_norm=0.23350977897644043, loss=1.4891526699066162
I0313 19:06:15.945945 139664575932160 logging_writer.py:48] [229700] global_step=229700, grad_norm=0.23133929073810577, loss=1.4637370109558105
I0313 19:06:51.578501 139664567539456 logging_writer.py:48] [229800] global_step=229800, grad_norm=0.23630383610725403, loss=1.4893287420272827
I0313 19:07:27.279411 139664575932160 logging_writer.py:48] [229900] global_step=229900, grad_norm=0.2347332090139389, loss=1.5027332305908203
I0313 19:08:02.947143 139664567539456 logging_writer.py:48] [230000] global_step=230000, grad_norm=0.22679321467876434, loss=1.4378312826156616
I0313 19:08:38.544000 139664575932160 logging_writer.py:48] [230100] global_step=230100, grad_norm=0.2409244030714035, loss=1.4894840717315674
I0313 19:09:14.180346 139664567539456 logging_writer.py:48] [230200] global_step=230200, grad_norm=0.23733662068843842, loss=1.4881463050842285
I0313 19:09:49.771759 139664575932160 logging_writer.py:48] [230300] global_step=230300, grad_norm=0.22887149453163147, loss=1.457078456878662
I0313 19:10:25.381373 139664567539456 logging_writer.py:48] [230400] global_step=230400, grad_norm=0.23895977437496185, loss=1.4422271251678467
I0313 19:11:01.012670 139664575932160 logging_writer.py:48] [230500] global_step=230500, grad_norm=0.23159880936145782, loss=1.4451488256454468
I0313 19:11:36.637633 139664567539456 logging_writer.py:48] [230600] global_step=230600, grad_norm=0.24202968180179596, loss=1.4530298709869385
I0313 19:12:12.246283 139664575932160 logging_writer.py:48] [230700] global_step=230700, grad_norm=0.2442227154970169, loss=1.4360448122024536
I0313 19:12:47.873280 139664567539456 logging_writer.py:48] [230800] global_step=230800, grad_norm=0.23981328308582306, loss=1.4822807312011719
I0313 19:13:23.472866 139664575932160 logging_writer.py:48] [230900] global_step=230900, grad_norm=0.23518407344818115, loss=1.5265253782272339
I0313 19:13:59.109445 139664567539456 logging_writer.py:48] [231000] global_step=231000, grad_norm=0.2268221229314804, loss=1.4116787910461426
I0313 19:14:34.757351 139664575932160 logging_writer.py:48] [231100] global_step=231100, grad_norm=0.23378005623817444, loss=1.5208826065063477
I0313 19:15:10.340430 139664567539456 logging_writer.py:48] [231200] global_step=231200, grad_norm=0.24443762004375458, loss=1.4256905317306519
I0313 19:15:45.957104 139664575932160 logging_writer.py:48] [231300] global_step=231300, grad_norm=0.22795867919921875, loss=1.3932528495788574
I0313 19:16:21.544100 139664567539456 logging_writer.py:48] [231400] global_step=231400, grad_norm=0.2446821928024292, loss=1.4963568449020386
I0313 19:16:38.002930 139834281293632 spec.py:321] Evaluating on the training split.
I0313 19:16:40.971334 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:20:21.752080 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 19:20:24.441873 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:23:40.114499 139834281293632 spec.py:349] Evaluating on the test split.
I0313 19:23:42.807900 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:26:52.878969 139834281293632 submission_runner.py:420] Time since start: 141381.43s, 	Step: 231448, 	{'train/accuracy': 0.697636067867279, 'train/loss': 1.3580584526062012, 'train/bleu': 35.64368150155568, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 82364.74400091171, 'total_duration': 141381.4314494133, 'accumulated_submission_time': 82364.74400091171, 'accumulated_eval_time': 59005.66600751877, 'accumulated_logging_time': 3.956840753555298}
I0313 19:26:52.923664 139664575932160 logging_writer.py:48] [231448] accumulated_eval_time=59005.666008, accumulated_logging_time=3.956841, accumulated_submission_time=82364.744001, global_step=231448, preemption_count=0, score=82364.744001, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=141381.431449, train/accuracy=0.697636, train/bleu=35.643682, train/loss=1.358058, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 19:27:11.764932 139664567539456 logging_writer.py:48] [231500] global_step=231500, grad_norm=0.2232237011194229, loss=1.3681164979934692
I0313 19:27:47.279802 139664575932160 logging_writer.py:48] [231600] global_step=231600, grad_norm=0.2291068583726883, loss=1.4186426401138306
I0313 19:28:22.863634 139664567539456 logging_writer.py:48] [231700] global_step=231700, grad_norm=0.23810335993766785, loss=1.4665541648864746
I0313 19:28:58.517417 139664575932160 logging_writer.py:48] [231800] global_step=231800, grad_norm=0.2337070107460022, loss=1.3387770652770996
I0313 19:29:34.216415 139664567539456 logging_writer.py:48] [231900] global_step=231900, grad_norm=0.23238779604434967, loss=1.413419246673584
I0313 19:30:09.821306 139664575932160 logging_writer.py:48] [232000] global_step=232000, grad_norm=0.24288317561149597, loss=1.5185284614562988
I0313 19:30:45.430263 139664567539456 logging_writer.py:48] [232100] global_step=232100, grad_norm=0.2424057275056839, loss=1.4199557304382324
I0313 19:31:21.027196 139664575932160 logging_writer.py:48] [232200] global_step=232200, grad_norm=0.22577573359012604, loss=1.3910166025161743
I0313 19:31:56.630378 139664567539456 logging_writer.py:48] [232300] global_step=232300, grad_norm=0.23199303448200226, loss=1.438867211341858
I0313 19:32:32.228564 139664575932160 logging_writer.py:48] [232400] global_step=232400, grad_norm=0.23187844455242157, loss=1.4235426187515259
I0313 19:33:07.841479 139664567539456 logging_writer.py:48] [232500] global_step=232500, grad_norm=0.2441229522228241, loss=1.486206293106079
I0313 19:33:43.441282 139664575932160 logging_writer.py:48] [232600] global_step=232600, grad_norm=0.22441473603248596, loss=1.4063383340835571
I0313 19:34:19.102294 139664567539456 logging_writer.py:48] [232700] global_step=232700, grad_norm=0.23449863493442535, loss=1.4257259368896484
I0313 19:34:54.711535 139664575932160 logging_writer.py:48] [232800] global_step=232800, grad_norm=0.2440715730190277, loss=1.4936703443527222
I0313 19:35:30.321799 139664567539456 logging_writer.py:48] [232900] global_step=232900, grad_norm=0.23918059468269348, loss=1.4283400774002075
I0313 19:36:05.916479 139664575932160 logging_writer.py:48] [233000] global_step=233000, grad_norm=0.22768785059452057, loss=1.408738374710083
I0313 19:36:41.536136 139664567539456 logging_writer.py:48] [233100] global_step=233100, grad_norm=0.2291705459356308, loss=1.4197553396224976
I0313 19:37:17.164982 139664575932160 logging_writer.py:48] [233200] global_step=233200, grad_norm=0.2293993979692459, loss=1.4478257894515991
I0313 19:37:52.756692 139664567539456 logging_writer.py:48] [233300] global_step=233300, grad_norm=0.24361351132392883, loss=1.485671877861023
I0313 19:38:28.379553 139664575932160 logging_writer.py:48] [233400] global_step=233400, grad_norm=0.6242250800132751, loss=1.4764584302902222
I0313 19:39:04.015344 139664567539456 logging_writer.py:48] [233500] global_step=233500, grad_norm=0.24078136682510376, loss=1.4346164464950562
I0313 19:39:39.595806 139664575932160 logging_writer.py:48] [233600] global_step=233600, grad_norm=0.24529631435871124, loss=1.4703607559204102
I0313 19:40:15.193017 139664567539456 logging_writer.py:48] [233700] global_step=233700, grad_norm=0.23766489326953888, loss=1.4883438348770142
I0313 19:40:50.757742 139664575932160 logging_writer.py:48] [233800] global_step=233800, grad_norm=0.2402748465538025, loss=1.4626330137252808
I0313 19:40:52.973528 139834281293632 spec.py:321] Evaluating on the training split.
I0313 19:40:55.941330 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:44:20.784826 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 19:44:23.465522 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:47:37.587650 139834281293632 spec.py:349] Evaluating on the test split.
I0313 19:47:40.257060 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 19:50:51.833920 139834281293632 submission_runner.py:420] Time since start: 142820.39s, 	Step: 233808, 	{'train/accuracy': 0.6989806294441223, 'train/loss': 1.349443793296814, 'train/bleu': 35.61818829777432, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 83204.71192860603, 'total_duration': 142820.386374712, 'accumulated_submission_time': 83204.71192860603, 'accumulated_eval_time': 59604.526320934296, 'accumulated_logging_time': 4.011416673660278}
I0313 19:50:51.889766 139664567539456 logging_writer.py:48] [233808] accumulated_eval_time=59604.526321, accumulated_logging_time=4.011417, accumulated_submission_time=83204.711929, global_step=233808, preemption_count=0, score=83204.711929, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=142820.386375, train/accuracy=0.698981, train/bleu=35.618188, train/loss=1.349444, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 19:51:24.969613 139664575932160 logging_writer.py:48] [233900] global_step=233900, grad_norm=0.24493853747844696, loss=1.398168921470642
I0313 19:52:00.557170 139664567539456 logging_writer.py:48] [234000] global_step=234000, grad_norm=0.23373086750507355, loss=1.4587970972061157
I0313 19:52:36.161000 139664575932160 logging_writer.py:48] [234100] global_step=234100, grad_norm=0.23430202901363373, loss=1.474448561668396
I0313 19:53:11.768954 139664567539456 logging_writer.py:48] [234200] global_step=234200, grad_norm=0.24013029038906097, loss=1.4848262071609497
I0313 19:53:47.364292 139664575932160 logging_writer.py:48] [234300] global_step=234300, grad_norm=0.2328212857246399, loss=1.4567339420318604
I0313 19:54:22.955566 139664567539456 logging_writer.py:48] [234400] global_step=234400, grad_norm=0.23876889050006866, loss=1.447566032409668
I0313 19:54:58.554988 139664575932160 logging_writer.py:48] [234500] global_step=234500, grad_norm=0.22668732702732086, loss=1.4269516468048096
I0313 19:55:34.136980 139664567539456 logging_writer.py:48] [234600] global_step=234600, grad_norm=0.2283811867237091, loss=1.4175589084625244
I0313 19:56:09.720102 139664575932160 logging_writer.py:48] [234700] global_step=234700, grad_norm=0.22131897509098053, loss=1.4389759302139282
I0313 19:56:45.319709 139664567539456 logging_writer.py:48] [234800] global_step=234800, grad_norm=0.23507097363471985, loss=1.4391652345657349
I0313 19:57:20.968471 139664575932160 logging_writer.py:48] [234900] global_step=234900, grad_norm=0.22325536608695984, loss=1.4019323587417603
I0313 19:57:56.582484 139664567539456 logging_writer.py:48] [235000] global_step=235000, grad_norm=0.23821747303009033, loss=1.4675538539886475
I0313 19:58:32.193756 139664575932160 logging_writer.py:48] [235100] global_step=235100, grad_norm=0.23467257618904114, loss=1.4354166984558105
I0313 19:59:07.782430 139664567539456 logging_writer.py:48] [235200] global_step=235200, grad_norm=0.24225261807441711, loss=1.452469825744629
I0313 19:59:43.365552 139664575932160 logging_writer.py:48] [235300] global_step=235300, grad_norm=0.2380724847316742, loss=1.4774953126907349
I0313 20:00:19.009425 139664567539456 logging_writer.py:48] [235400] global_step=235400, grad_norm=0.23142023384571075, loss=1.4404782056808472
I0313 20:00:54.661718 139664575932160 logging_writer.py:48] [235500] global_step=235500, grad_norm=0.23012283444404602, loss=1.449333906173706
I0313 20:01:30.279805 139664567539456 logging_writer.py:48] [235600] global_step=235600, grad_norm=0.2255030870437622, loss=1.3846793174743652
I0313 20:02:05.861393 139664575932160 logging_writer.py:48] [235700] global_step=235700, grad_norm=0.2748858630657196, loss=1.4951303005218506
I0313 20:02:41.447823 139664567539456 logging_writer.py:48] [235800] global_step=235800, grad_norm=0.23979075253009796, loss=1.465097188949585
I0313 20:03:17.065535 139664575932160 logging_writer.py:48] [235900] global_step=235900, grad_norm=0.2434483766555786, loss=1.5246496200561523
I0313 20:03:52.658641 139664567539456 logging_writer.py:48] [236000] global_step=236000, grad_norm=0.24131643772125244, loss=1.5506647825241089
I0313 20:04:28.278035 139664575932160 logging_writer.py:48] [236100] global_step=236100, grad_norm=0.23362505435943604, loss=1.3917112350463867
I0313 20:04:51.863882 139834281293632 spec.py:321] Evaluating on the training split.
I0313 20:04:54.840681 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:08:14.704261 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 20:08:17.374083 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:11:31.439938 139834281293632 spec.py:349] Evaluating on the test split.
I0313 20:11:34.110121 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:14:43.991018 139834281293632 submission_runner.py:420] Time since start: 144252.54s, 	Step: 236168, 	{'train/accuracy': 0.6970456838607788, 'train/loss': 1.3616652488708496, 'train/bleu': 35.49304734732505, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 84044.6024339199, 'total_duration': 144252.54349827766, 'accumulated_submission_time': 84044.6024339199, 'accumulated_eval_time': 60196.6534178257, 'accumulated_logging_time': 4.07750678062439}
I0313 20:14:44.038003 139664567539456 logging_writer.py:48] [236168] accumulated_eval_time=60196.653418, accumulated_logging_time=4.077507, accumulated_submission_time=84044.602434, global_step=236168, preemption_count=0, score=84044.602434, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=144252.543498, train/accuracy=0.697046, train/bleu=35.493047, train/loss=1.361665, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 20:14:55.783386 139664575932160 logging_writer.py:48] [236200] global_step=236200, grad_norm=0.23080798983573914, loss=1.412802815437317
I0313 20:15:31.298618 139664567539456 logging_writer.py:48] [236300] global_step=236300, grad_norm=0.24624723196029663, loss=1.4643014669418335
I0313 20:16:06.865241 139664575932160 logging_writer.py:48] [236400] global_step=236400, grad_norm=0.2345554679632187, loss=1.425356388092041
I0313 20:16:42.486732 139664567539456 logging_writer.py:48] [236500] global_step=236500, grad_norm=0.24874813854694366, loss=1.459730625152588
I0313 20:17:18.153138 139664575932160 logging_writer.py:48] [236600] global_step=236600, grad_norm=0.23348195850849152, loss=1.4354075193405151
I0313 20:17:53.789590 139664567539456 logging_writer.py:48] [236700] global_step=236700, grad_norm=0.23124204576015472, loss=1.5028407573699951
I0313 20:18:29.398122 139664575932160 logging_writer.py:48] [236800] global_step=236800, grad_norm=0.22077396512031555, loss=1.3992111682891846
I0313 20:19:04.998652 139664567539456 logging_writer.py:48] [236900] global_step=236900, grad_norm=0.22442179918289185, loss=1.4389013051986694
I0313 20:19:40.576822 139664575932160 logging_writer.py:48] [237000] global_step=237000, grad_norm=0.23371165990829468, loss=1.4388998746871948
I0313 20:20:16.185583 139664567539456 logging_writer.py:48] [237100] global_step=237100, grad_norm=0.2366400957107544, loss=1.5222382545471191
I0313 20:20:51.803776 139664575932160 logging_writer.py:48] [237200] global_step=237200, grad_norm=0.23153620958328247, loss=1.4521484375
I0313 20:21:27.371389 139664567539456 logging_writer.py:48] [237300] global_step=237300, grad_norm=0.23316608369350433, loss=1.4579521417617798
I0313 20:22:03.004341 139664575932160 logging_writer.py:48] [237400] global_step=237400, grad_norm=0.23130935430526733, loss=1.4897011518478394
I0313 20:22:38.627800 139664567539456 logging_writer.py:48] [237500] global_step=237500, grad_norm=0.2406136691570282, loss=1.4692025184631348
I0313 20:23:14.220392 139664575932160 logging_writer.py:48] [237600] global_step=237600, grad_norm=0.23773159086704254, loss=1.4189441204071045
I0313 20:23:49.808151 139664567539456 logging_writer.py:48] [237700] global_step=237700, grad_norm=0.23529313504695892, loss=1.4019529819488525
I0313 20:24:25.404229 139664575932160 logging_writer.py:48] [237800] global_step=237800, grad_norm=0.23876671493053436, loss=1.4211140871047974
I0313 20:25:00.996757 139664567539456 logging_writer.py:48] [237900] global_step=237900, grad_norm=0.2314785122871399, loss=1.3931005001068115
I0313 20:25:36.594005 139664575932160 logging_writer.py:48] [238000] global_step=238000, grad_norm=0.22884732484817505, loss=1.4431580305099487
I0313 20:26:12.222343 139664567539456 logging_writer.py:48] [238100] global_step=238100, grad_norm=0.23521968722343445, loss=1.4579095840454102
I0313 20:26:47.785123 139664575932160 logging_writer.py:48] [238200] global_step=238200, grad_norm=0.4020259380340576, loss=1.4823763370513916
I0313 20:27:23.391275 139664567539456 logging_writer.py:48] [238300] global_step=238300, grad_norm=0.23199202120304108, loss=1.4620963335037231
I0313 20:27:58.964668 139664575932160 logging_writer.py:48] [238400] global_step=238400, grad_norm=0.2381264716386795, loss=1.4505003690719604
I0313 20:28:34.554254 139664567539456 logging_writer.py:48] [238500] global_step=238500, grad_norm=0.22948892414569855, loss=1.4644392728805542
I0313 20:28:44.222624 139834281293632 spec.py:321] Evaluating on the training split.
I0313 20:28:47.195393 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:32:11.241474 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 20:32:13.917813 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:35:29.279354 139834281293632 spec.py:349] Evaluating on the test split.
I0313 20:35:31.969883 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:38:41.925411 139834281293632 submission_runner.py:420] Time since start: 145690.48s, 	Step: 238529, 	{'train/accuracy': 0.6947537064552307, 'train/loss': 1.3757495880126953, 'train/bleu': 35.53998794997194, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 84884.70688819885, 'total_duration': 145690.4778895378, 'accumulated_submission_time': 84884.70688819885, 'accumulated_eval_time': 60794.356152296066, 'accumulated_logging_time': 4.133155822753906}
I0313 20:38:41.972603 139664575932160 logging_writer.py:48] [238529] accumulated_eval_time=60794.356152, accumulated_logging_time=4.133156, accumulated_submission_time=84884.706888, global_step=238529, preemption_count=0, score=84884.706888, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=145690.477890, train/accuracy=0.694754, train/bleu=35.539988, train/loss=1.375750, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 20:39:07.600818 139664567539456 logging_writer.py:48] [238600] global_step=238600, grad_norm=0.23581981658935547, loss=1.4859662055969238
I0313 20:39:43.154425 139664575932160 logging_writer.py:48] [238700] global_step=238700, grad_norm=0.22826090455055237, loss=1.4222698211669922
I0313 20:40:18.814198 139664567539456 logging_writer.py:48] [238800] global_step=238800, grad_norm=0.22947323322296143, loss=1.4093471765518188
I0313 20:40:54.401290 139664575932160 logging_writer.py:48] [238900] global_step=238900, grad_norm=0.24641789495944977, loss=1.473459005355835
I0313 20:41:29.998005 139664567539456 logging_writer.py:48] [239000] global_step=239000, grad_norm=0.24316729605197906, loss=1.476142406463623
I0313 20:42:05.587740 139664575932160 logging_writer.py:48] [239100] global_step=239100, grad_norm=0.23563560843467712, loss=1.4287742376327515
I0313 20:42:41.234834 139664567539456 logging_writer.py:48] [239200] global_step=239200, grad_norm=0.22438408434391022, loss=1.437234878540039
I0313 20:43:16.816690 139664575932160 logging_writer.py:48] [239300] global_step=239300, grad_norm=0.23865853250026703, loss=1.4767757654190063
I0313 20:43:52.401915 139664567539456 logging_writer.py:48] [239400] global_step=239400, grad_norm=0.24218885600566864, loss=1.452420711517334
I0313 20:44:27.993466 139664575932160 logging_writer.py:48] [239500] global_step=239500, grad_norm=0.238875150680542, loss=1.5336805582046509
I0313 20:45:03.597439 139664567539456 logging_writer.py:48] [239600] global_step=239600, grad_norm=0.23527461290359497, loss=1.4786185026168823
I0313 20:45:39.185752 139664575932160 logging_writer.py:48] [239700] global_step=239700, grad_norm=0.24346858263015747, loss=1.443422555923462
I0313 20:46:14.793389 139664567539456 logging_writer.py:48] [239800] global_step=239800, grad_norm=0.2223709523677826, loss=1.3915201425552368
I0313 20:46:50.386346 139664575932160 logging_writer.py:48] [239900] global_step=239900, grad_norm=0.23278671503067017, loss=1.4339443445205688
I0313 20:47:25.977518 139664567539456 logging_writer.py:48] [240000] global_step=240000, grad_norm=0.23312585055828094, loss=1.398682951927185
I0313 20:48:01.578492 139664575932160 logging_writer.py:48] [240100] global_step=240100, grad_norm=0.23245450854301453, loss=1.5093109607696533
I0313 20:48:37.185005 139664567539456 logging_writer.py:48] [240200] global_step=240200, grad_norm=0.2456890344619751, loss=1.4459725618362427
I0313 20:49:12.811081 139664575932160 logging_writer.py:48] [240300] global_step=240300, grad_norm=0.23379573225975037, loss=1.444960117340088
I0313 20:49:48.403518 139664567539456 logging_writer.py:48] [240400] global_step=240400, grad_norm=0.24510495364665985, loss=1.4771342277526855
I0313 20:50:24.011140 139664575932160 logging_writer.py:48] [240500] global_step=240500, grad_norm=0.23220300674438477, loss=1.3848402500152588
I0313 20:50:59.608579 139664567539456 logging_writer.py:48] [240600] global_step=240600, grad_norm=0.2358981817960739, loss=1.4785007238388062
I0313 20:51:35.187473 139664575932160 logging_writer.py:48] [240700] global_step=240700, grad_norm=0.22617849707603455, loss=1.4792675971984863
I0313 20:52:10.778846 139664567539456 logging_writer.py:48] [240800] global_step=240800, grad_norm=0.23724104464054108, loss=1.4023430347442627
I0313 20:52:42.141569 139834281293632 spec.py:321] Evaluating on the training split.
I0313 20:52:45.109721 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:56:18.508423 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 20:56:21.188419 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 20:59:36.417131 139834281293632 spec.py:349] Evaluating on the test split.
I0313 20:59:39.092890 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:02:49.088406 139834281293632 submission_runner.py:420] Time since start: 147137.64s, 	Step: 240890, 	{'train/accuracy': 0.6979270577430725, 'train/loss': 1.3634157180786133, 'train/bleu': 35.702276531386154, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 85724.79488253593, 'total_duration': 147137.6408853531, 'accumulated_submission_time': 85724.79488253593, 'accumulated_eval_time': 61401.30293893814, 'accumulated_logging_time': 4.189852237701416}
I0313 21:02:49.134716 139664575932160 logging_writer.py:48] [240890] accumulated_eval_time=61401.302939, accumulated_logging_time=4.189852, accumulated_submission_time=85724.794883, global_step=240890, preemption_count=0, score=85724.794883, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=147137.640885, train/accuracy=0.697927, train/bleu=35.702277, train/loss=1.363416, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 21:02:53.061468 139664567539456 logging_writer.py:48] [240900] global_step=240900, grad_norm=0.2364659458398819, loss=1.5246472358703613
I0313 21:03:28.585274 139664575932160 logging_writer.py:48] [241000] global_step=241000, grad_norm=0.2415187954902649, loss=1.501053810119629
I0313 21:04:04.153974 139664567539456 logging_writer.py:48] [241100] global_step=241100, grad_norm=0.2280842810869217, loss=1.419381856918335
I0313 21:04:39.732908 139664575932160 logging_writer.py:48] [241200] global_step=241200, grad_norm=0.2262682169675827, loss=1.339076280593872
I0313 21:05:15.312509 139664567539456 logging_writer.py:48] [241300] global_step=241300, grad_norm=0.24247848987579346, loss=1.5234469175338745
I0313 21:05:50.940124 139664575932160 logging_writer.py:48] [241400] global_step=241400, grad_norm=0.23648256063461304, loss=1.423761010169983
I0313 21:06:26.533767 139664567539456 logging_writer.py:48] [241500] global_step=241500, grad_norm=0.23178476095199585, loss=1.380171775817871
I0313 21:07:02.111341 139664575932160 logging_writer.py:48] [241600] global_step=241600, grad_norm=0.2403176724910736, loss=1.4691240787506104
I0313 21:07:37.704778 139664567539456 logging_writer.py:48] [241700] global_step=241700, grad_norm=0.23259548842906952, loss=1.4031060934066772
I0313 21:08:13.329174 139664575932160 logging_writer.py:48] [241800] global_step=241800, grad_norm=0.24011757969856262, loss=1.4698044061660767
I0313 21:08:48.954009 139664567539456 logging_writer.py:48] [241900] global_step=241900, grad_norm=0.2316024899482727, loss=1.5029146671295166
I0313 21:09:24.594750 139664575932160 logging_writer.py:48] [242000] global_step=242000, grad_norm=0.23308084905147552, loss=1.4628368616104126
I0313 21:10:00.204200 139664567539456 logging_writer.py:48] [242100] global_step=242100, grad_norm=0.2341288924217224, loss=1.479060411453247
I0313 21:10:35.829936 139664575932160 logging_writer.py:48] [242200] global_step=242200, grad_norm=0.23492461442947388, loss=1.4298402070999146
I0313 21:11:11.425060 139664567539456 logging_writer.py:48] [242300] global_step=242300, grad_norm=0.24687325954437256, loss=1.502135992050171
I0313 21:11:47.063892 139664575932160 logging_writer.py:48] [242400] global_step=242400, grad_norm=0.22601810097694397, loss=1.413540005683899
I0313 21:12:22.691395 139664567539456 logging_writer.py:48] [242500] global_step=242500, grad_norm=0.23244386911392212, loss=1.4715460538864136
I0313 21:12:58.333091 139664575932160 logging_writer.py:48] [242600] global_step=242600, grad_norm=0.2356838434934616, loss=1.4967283010482788
I0313 21:13:33.959495 139664567539456 logging_writer.py:48] [242700] global_step=242700, grad_norm=0.2294601947069168, loss=1.4744890928268433
I0313 21:14:09.545248 139664575932160 logging_writer.py:48] [242800] global_step=242800, grad_norm=0.23371250927448273, loss=1.4282690286636353
I0313 21:14:45.200453 139664567539456 logging_writer.py:48] [242900] global_step=242900, grad_norm=0.2255515605211258, loss=1.4604074954986572
I0313 21:15:20.795648 139664575932160 logging_writer.py:48] [243000] global_step=243000, grad_norm=0.23496656119823456, loss=1.4491075277328491
I0313 21:15:56.437371 139664567539456 logging_writer.py:48] [243100] global_step=243100, grad_norm=0.23342265188694, loss=1.4164905548095703
I0313 21:16:32.053290 139664575932160 logging_writer.py:48] [243200] global_step=243200, grad_norm=0.2349853813648224, loss=1.4361947774887085
I0313 21:16:49.216197 139834281293632 spec.py:321] Evaluating on the training split.
I0313 21:16:52.191395 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:20:33.489828 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 21:20:36.167542 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:23:50.347380 139834281293632 spec.py:349] Evaluating on the test split.
I0313 21:23:53.007789 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:27:03.140862 139834281293632 submission_runner.py:420] Time since start: 148591.69s, 	Step: 243250, 	{'train/accuracy': 0.6946918368339539, 'train/loss': 1.3715825080871582, 'train/bleu': 35.802417390103905, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 86564.79521155357, 'total_duration': 148591.69333863258, 'accumulated_submission_time': 86564.79521155357, 'accumulated_eval_time': 62015.227550029755, 'accumulated_logging_time': 4.245082378387451}
I0313 21:27:03.187494 139664567539456 logging_writer.py:48] [243250] accumulated_eval_time=62015.227550, accumulated_logging_time=4.245082, accumulated_submission_time=86564.795212, global_step=243250, preemption_count=0, score=86564.795212, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=148591.693339, train/accuracy=0.694692, train/bleu=35.802417, train/loss=1.371583, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 21:27:21.302394 139664575932160 logging_writer.py:48] [243300] global_step=243300, grad_norm=0.2351996898651123, loss=1.3754503726959229
I0313 21:27:56.850473 139664567539456 logging_writer.py:48] [243400] global_step=243400, grad_norm=0.246297687292099, loss=1.4577391147613525
I0313 21:28:32.496955 139664575932160 logging_writer.py:48] [243500] global_step=243500, grad_norm=0.2390516698360443, loss=1.4745136499404907
I0313 21:29:08.149355 139664567539456 logging_writer.py:48] [243600] global_step=243600, grad_norm=0.23605671525001526, loss=1.4744200706481934
I0313 21:29:43.747362 139664575932160 logging_writer.py:48] [243700] global_step=243700, grad_norm=0.23198045790195465, loss=1.4155042171478271
I0313 21:30:19.343974 139664567539456 logging_writer.py:48] [243800] global_step=243800, grad_norm=0.23188404738903046, loss=1.485280990600586
I0313 21:30:54.949362 139664575932160 logging_writer.py:48] [243900] global_step=243900, grad_norm=0.2257453352212906, loss=1.391442060470581
I0313 21:31:30.557206 139664567539456 logging_writer.py:48] [244000] global_step=244000, grad_norm=0.22280071675777435, loss=1.4663212299346924
I0313 21:32:06.165411 139664575932160 logging_writer.py:48] [244100] global_step=244100, grad_norm=0.24105772376060486, loss=1.4614801406860352
I0313 21:32:41.790535 139664567539456 logging_writer.py:48] [244200] global_step=244200, grad_norm=0.23675017058849335, loss=1.5181810855865479
I0313 21:33:17.393269 139664575932160 logging_writer.py:48] [244300] global_step=244300, grad_norm=0.23381370306015015, loss=1.4411190748214722
I0313 21:33:52.994705 139664567539456 logging_writer.py:48] [244400] global_step=244400, grad_norm=0.2430477738380432, loss=1.388888955116272
I0313 21:34:28.629198 139664575932160 logging_writer.py:48] [244500] global_step=244500, grad_norm=0.23084232211112976, loss=1.430119514465332
I0313 21:35:04.244014 139664567539456 logging_writer.py:48] [244600] global_step=244600, grad_norm=0.23142282664775848, loss=1.4593472480773926
I0313 21:35:39.842054 139664575932160 logging_writer.py:48] [244700] global_step=244700, grad_norm=0.2415018081665039, loss=1.4657368659973145
I0313 21:36:15.424869 139664567539456 logging_writer.py:48] [244800] global_step=244800, grad_norm=0.23425176739692688, loss=1.3849270343780518
I0313 21:36:51.031403 139664575932160 logging_writer.py:48] [244900] global_step=244900, grad_norm=0.23228232562541962, loss=1.4309656620025635
I0313 21:37:26.651348 139664567539456 logging_writer.py:48] [245000] global_step=245000, grad_norm=0.2360222488641739, loss=1.4450905323028564
I0313 21:38:02.235172 139664575932160 logging_writer.py:48] [245100] global_step=245100, grad_norm=0.23448489606380463, loss=1.4523850679397583
I0313 21:38:37.842505 139664567539456 logging_writer.py:48] [245200] global_step=245200, grad_norm=0.2390226125717163, loss=1.418624758720398
I0313 21:39:13.459658 139664575932160 logging_writer.py:48] [245300] global_step=245300, grad_norm=0.2317938208580017, loss=1.4523324966430664
I0313 21:39:49.077191 139664567539456 logging_writer.py:48] [245400] global_step=245400, grad_norm=0.21692952513694763, loss=1.4129455089569092
I0313 21:40:24.717764 139664575932160 logging_writer.py:48] [245500] global_step=245500, grad_norm=0.23366709053516388, loss=1.359826683998108
I0313 21:41:00.340884 139664567539456 logging_writer.py:48] [245600] global_step=245600, grad_norm=0.2389756739139557, loss=1.4442096948623657
I0313 21:41:03.264211 139834281293632 spec.py:321] Evaluating on the training split.
I0313 21:41:06.230679 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:44:34.804113 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 21:44:37.470620 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:47:51.352815 139834281293632 spec.py:349] Evaluating on the test split.
I0313 21:47:54.022712 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 21:51:03.928543 139834281293632 submission_runner.py:420] Time since start: 150032.48s, 	Step: 245610, 	{'train/accuracy': 0.6926801204681396, 'train/loss': 1.3849468231201172, 'train/bleu': 35.56962577221613, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 87404.78988575935, 'total_duration': 150032.48102235794, 'accumulated_submission_time': 87404.78988575935, 'accumulated_eval_time': 62615.89183759689, 'accumulated_logging_time': 4.302200794219971}
I0313 21:51:03.975460 139664575932160 logging_writer.py:48] [245610] accumulated_eval_time=62615.891838, accumulated_logging_time=4.302201, accumulated_submission_time=87404.789886, global_step=245610, preemption_count=0, score=87404.789886, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=150032.481022, train/accuracy=0.692680, train/bleu=35.569626, train/loss=1.384947, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 21:51:36.302720 139664567539456 logging_writer.py:48] [245700] global_step=245700, grad_norm=0.23105867207050323, loss=1.422871470451355
I0313 21:52:11.857757 139664575932160 logging_writer.py:48] [245800] global_step=245800, grad_norm=0.23757514357566833, loss=1.4468694925308228
I0313 21:52:47.455987 139664567539456 logging_writer.py:48] [245900] global_step=245900, grad_norm=0.23607929050922394, loss=1.3854564428329468
I0313 21:53:23.058707 139664575932160 logging_writer.py:48] [246000] global_step=246000, grad_norm=0.23542967438697815, loss=1.3975414037704468
I0313 21:53:58.663427 139664567539456 logging_writer.py:48] [246100] global_step=246100, grad_norm=0.23097123205661774, loss=1.4301389455795288
I0313 21:54:34.268797 139664575932160 logging_writer.py:48] [246200] global_step=246200, grad_norm=0.2341751903295517, loss=1.4457612037658691
I0313 21:55:09.869922 139664567539456 logging_writer.py:48] [246300] global_step=246300, grad_norm=0.23456953465938568, loss=1.4515975713729858
I0313 21:55:45.485927 139664575932160 logging_writer.py:48] [246400] global_step=246400, grad_norm=0.24199804663658142, loss=1.5008701086044312
I0313 21:56:21.078714 139664567539456 logging_writer.py:48] [246500] global_step=246500, grad_norm=0.24815596640110016, loss=1.4808741807937622
I0313 21:56:56.685619 139664575932160 logging_writer.py:48] [246600] global_step=246600, grad_norm=0.23766696453094482, loss=1.4563795328140259
I0313 21:57:32.280012 139664567539456 logging_writer.py:48] [246700] global_step=246700, grad_norm=0.23557056486606598, loss=1.505639672279358
I0313 21:58:07.955987 139664575932160 logging_writer.py:48] [246800] global_step=246800, grad_norm=0.24226777255535126, loss=1.3992465734481812
I0313 21:58:43.572201 139664567539456 logging_writer.py:48] [246900] global_step=246900, grad_norm=0.24158643186092377, loss=1.4924837350845337
I0313 21:59:19.184249 139664575932160 logging_writer.py:48] [247000] global_step=247000, grad_norm=0.2271752506494522, loss=1.4347798824310303
I0313 21:59:54.787456 139664567539456 logging_writer.py:48] [247100] global_step=247100, grad_norm=0.23548276722431183, loss=1.4573440551757812
I0313 22:00:30.390571 139664575932160 logging_writer.py:48] [247200] global_step=247200, grad_norm=0.2284088432788849, loss=1.354218602180481
I0313 22:01:06.024685 139664567539456 logging_writer.py:48] [247300] global_step=247300, grad_norm=0.23952540755271912, loss=1.4634913206100464
I0313 22:01:41.613399 139664575932160 logging_writer.py:48] [247400] global_step=247400, grad_norm=0.23959863185882568, loss=1.4594188928604126
I0313 22:02:17.196798 139664567539456 logging_writer.py:48] [247500] global_step=247500, grad_norm=0.2339424192905426, loss=1.4574027061462402
I0313 22:02:52.848302 139664575932160 logging_writer.py:48] [247600] global_step=247600, grad_norm=0.25273317098617554, loss=1.4763514995574951
I0313 22:03:28.546264 139664567539456 logging_writer.py:48] [247700] global_step=247700, grad_norm=0.23889455199241638, loss=1.5040644407272339
I0313 22:04:04.265130 139664575932160 logging_writer.py:48] [247800] global_step=247800, grad_norm=0.22528661787509918, loss=1.3816174268722534
I0313 22:04:39.929874 139664567539456 logging_writer.py:48] [247900] global_step=247900, grad_norm=0.23047395050525665, loss=1.4654419422149658
I0313 22:05:04.228265 139834281293632 spec.py:321] Evaluating on the training split.
I0313 22:05:07.194151 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:08:38.881437 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 22:08:41.554486 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:11:55.341525 139834281293632 spec.py:349] Evaluating on the test split.
I0313 22:11:58.014368 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:15:07.722651 139834281293632 submission_runner.py:420] Time since start: 151476.28s, 	Step: 247970, 	{'train/accuracy': 0.6970303654670715, 'train/loss': 1.3621553182601929, 'train/bleu': 35.03329818639915, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 88244.95991444588, 'total_duration': 151476.27511763573, 'accumulated_submission_time': 88244.95991444588, 'accumulated_eval_time': 63219.3861579895, 'accumulated_logging_time': 4.359274387359619}
I0313 22:15:07.771148 139664575932160 logging_writer.py:48] [247970] accumulated_eval_time=63219.386158, accumulated_logging_time=4.359274, accumulated_submission_time=88244.959914, global_step=247970, preemption_count=0, score=88244.959914, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=151476.275118, train/accuracy=0.697030, train/bleu=35.033298, train/loss=1.362155, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 22:15:18.801456 139664567539456 logging_writer.py:48] [248000] global_step=248000, grad_norm=0.2316601574420929, loss=1.4375057220458984
I0313 22:15:54.308676 139664575932160 logging_writer.py:48] [248100] global_step=248100, grad_norm=0.2519139349460602, loss=1.4858219623565674
I0313 22:16:29.888791 139664567539456 logging_writer.py:48] [248200] global_step=248200, grad_norm=0.24430912733078003, loss=1.4738216400146484
I0313 22:17:05.510118 139664575932160 logging_writer.py:48] [248300] global_step=248300, grad_norm=0.3973877429962158, loss=1.4257447719573975
I0313 22:17:41.112028 139664567539456 logging_writer.py:48] [248400] global_step=248400, grad_norm=0.23541657626628876, loss=1.4384161233901978
I0313 22:18:16.698221 139664575932160 logging_writer.py:48] [248500] global_step=248500, grad_norm=0.23286639153957367, loss=1.3835735321044922
I0313 22:18:52.311473 139664567539456 logging_writer.py:48] [248600] global_step=248600, grad_norm=0.23442590236663818, loss=1.4812464714050293
I0313 22:19:27.921072 139664575932160 logging_writer.py:48] [248700] global_step=248700, grad_norm=0.23652541637420654, loss=1.4503984451293945
I0313 22:20:03.516916 139664567539456 logging_writer.py:48] [248800] global_step=248800, grad_norm=0.23147936165332794, loss=1.4848190546035767
I0313 22:20:39.129400 139664575932160 logging_writer.py:48] [248900] global_step=248900, grad_norm=0.23252975940704346, loss=1.397744059562683
I0313 22:21:14.711560 139664567539456 logging_writer.py:48] [249000] global_step=249000, grad_norm=0.23007641732692719, loss=1.4425188302993774
I0313 22:21:50.323208 139664575932160 logging_writer.py:48] [249100] global_step=249100, grad_norm=0.2344515025615692, loss=1.417336106300354
I0313 22:22:25.892186 139664567539456 logging_writer.py:48] [249200] global_step=249200, grad_norm=0.2291417270898819, loss=1.451653003692627
I0313 22:23:01.465751 139664575932160 logging_writer.py:48] [249300] global_step=249300, grad_norm=0.2367624193429947, loss=1.4026464223861694
I0313 22:23:37.093379 139664567539456 logging_writer.py:48] [249400] global_step=249400, grad_norm=0.23069559037685394, loss=1.427310824394226
I0313 22:24:12.696351 139664575932160 logging_writer.py:48] [249500] global_step=249500, grad_norm=0.23504307866096497, loss=1.3759475946426392
I0313 22:24:48.276065 139664567539456 logging_writer.py:48] [249600] global_step=249600, grad_norm=0.2272961586713791, loss=1.4392712116241455
I0313 22:25:23.861344 139664575932160 logging_writer.py:48] [249700] global_step=249700, grad_norm=0.2356836050748825, loss=1.422576904296875
I0313 22:25:59.426499 139664567539456 logging_writer.py:48] [249800] global_step=249800, grad_norm=0.24088819324970245, loss=1.5118225812911987
I0313 22:26:35.005341 139664575932160 logging_writer.py:48] [249900] global_step=249900, grad_norm=0.230439692735672, loss=1.4234955310821533
I0313 22:27:10.592103 139664567539456 logging_writer.py:48] [250000] global_step=250000, grad_norm=0.23385637998580933, loss=1.5107090473175049
I0313 22:27:46.186557 139664575932160 logging_writer.py:48] [250100] global_step=250100, grad_norm=0.23024415969848633, loss=1.4345173835754395
I0313 22:28:21.784739 139664567539456 logging_writer.py:48] [250200] global_step=250200, grad_norm=0.23653262853622437, loss=1.5719163417816162
I0313 22:28:57.368493 139664575932160 logging_writer.py:48] [250300] global_step=250300, grad_norm=0.22790440917015076, loss=1.4547600746154785
I0313 22:29:07.748057 139834281293632 spec.py:321] Evaluating on the training split.
I0313 22:29:10.721631 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:32:43.128076 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 22:32:45.790174 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:35:59.553525 139834281293632 spec.py:349] Evaluating on the test split.
I0313 22:36:02.243690 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:39:11.959371 139834281293632 submission_runner.py:420] Time since start: 152920.51s, 	Step: 250331, 	{'train/accuracy': 0.6966925263404846, 'train/loss': 1.3629604578018188, 'train/bleu': 35.4255910535756, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 89084.85697770119, 'total_duration': 152920.51184105873, 'accumulated_submission_time': 89084.85697770119, 'accumulated_eval_time': 63823.59740900993, 'accumulated_logging_time': 4.417787790298462}
I0313 22:39:12.007842 139664567539456 logging_writer.py:48] [250331] accumulated_eval_time=63823.597409, accumulated_logging_time=4.417788, accumulated_submission_time=89084.856978, global_step=250331, preemption_count=0, score=89084.856978, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=152920.511841, train/accuracy=0.696693, train/bleu=35.425591, train/loss=1.362960, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 22:39:36.911464 139664575932160 logging_writer.py:48] [250400] global_step=250400, grad_norm=0.23433013260364532, loss=1.3558423519134521
I0313 22:40:12.524575 139664567539456 logging_writer.py:48] [250500] global_step=250500, grad_norm=0.23823189735412598, loss=1.4244632720947266
I0313 22:40:48.127784 139664575932160 logging_writer.py:48] [250600] global_step=250600, grad_norm=0.23747038841247559, loss=1.5210140943527222
I0313 22:41:23.766788 139664567539456 logging_writer.py:48] [250700] global_step=250700, grad_norm=0.22086100280284882, loss=1.399286150932312
I0313 22:41:59.354769 139664575932160 logging_writer.py:48] [250800] global_step=250800, grad_norm=0.23576286435127258, loss=1.4072139263153076
I0313 22:42:34.961358 139664567539456 logging_writer.py:48] [250900] global_step=250900, grad_norm=0.24410218000411987, loss=1.475007176399231
I0313 22:43:10.567940 139664575932160 logging_writer.py:48] [251000] global_step=251000, grad_norm=0.2276727855205536, loss=1.4285264015197754
I0313 22:43:46.212996 139664567539456 logging_writer.py:48] [251100] global_step=251100, grad_norm=0.23543651401996613, loss=1.4763716459274292
I0313 22:44:21.855213 139664575932160 logging_writer.py:48] [251200] global_step=251200, grad_norm=0.2307998090982437, loss=1.3473434448242188
I0313 22:44:57.495513 139664567539456 logging_writer.py:48] [251300] global_step=251300, grad_norm=0.2319195717573166, loss=1.399781584739685
I0313 22:45:33.117841 139664575932160 logging_writer.py:48] [251400] global_step=251400, grad_norm=0.2308434545993805, loss=1.4004919528961182
I0313 22:46:08.708343 139664567539456 logging_writer.py:48] [251500] global_step=251500, grad_norm=0.24016377329826355, loss=1.504711627960205
I0313 22:46:44.325178 139664575932160 logging_writer.py:48] [251600] global_step=251600, grad_norm=0.24474073946475983, loss=1.4146665334701538
I0313 22:47:19.906792 139664567539456 logging_writer.py:48] [251700] global_step=251700, grad_norm=0.24070902168750763, loss=1.4623913764953613
I0313 22:47:55.500607 139664575932160 logging_writer.py:48] [251800] global_step=251800, grad_norm=0.2263944149017334, loss=1.4475204944610596
I0313 22:48:31.090425 139664567539456 logging_writer.py:48] [251900] global_step=251900, grad_norm=0.24414467811584473, loss=1.4384173154830933
I0313 22:49:06.729088 139664575932160 logging_writer.py:48] [252000] global_step=252000, grad_norm=0.23241537809371948, loss=1.4674879312515259
I0313 22:49:42.325615 139664567539456 logging_writer.py:48] [252100] global_step=252100, grad_norm=0.23890681564807892, loss=1.5076181888580322
I0313 22:50:17.946089 139664575932160 logging_writer.py:48] [252200] global_step=252200, grad_norm=0.22931896150112152, loss=1.3990426063537598
I0313 22:50:53.559700 139664567539456 logging_writer.py:48] [252300] global_step=252300, grad_norm=0.2630262076854706, loss=1.457776427268982
I0313 22:51:29.230905 139664575932160 logging_writer.py:48] [252400] global_step=252400, grad_norm=0.24682244658470154, loss=1.4767704010009766
I0313 22:52:04.872750 139664567539456 logging_writer.py:48] [252500] global_step=252500, grad_norm=0.23670460283756256, loss=1.4252421855926514
I0313 22:52:40.485590 139664575932160 logging_writer.py:48] [252600] global_step=252600, grad_norm=0.23679986596107483, loss=1.4305051565170288
I0313 22:53:12.230976 139834281293632 spec.py:321] Evaluating on the training split.
I0313 22:53:15.203498 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 22:56:49.744979 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 22:56:52.432049 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:00:06.703637 139834281293632 spec.py:349] Evaluating on the test split.
I0313 23:00:09.382863 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:03:19.360887 139834281293632 submission_runner.py:420] Time since start: 154367.91s, 	Step: 252691, 	{'train/accuracy': 0.693827211856842, 'train/loss': 1.379385232925415, 'train/bleu': 35.42538705946164, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 89924.99598526955, 'total_duration': 154367.91337037086, 'accumulated_submission_time': 89924.99598526955, 'accumulated_eval_time': 64430.72727441788, 'accumulated_logging_time': 4.476908206939697}
I0313 23:03:19.407546 139664567539456 logging_writer.py:48] [252691] accumulated_eval_time=64430.727274, accumulated_logging_time=4.476908, accumulated_submission_time=89924.995985, global_step=252691, preemption_count=0, score=89924.995985, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=154367.913370, train/accuracy=0.693827, train/bleu=35.425387, train/loss=1.379385, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 23:03:22.981736 139664575932160 logging_writer.py:48] [252700] global_step=252700, grad_norm=0.23164990544319153, loss=1.4569529294967651
I0313 23:03:58.578189 139664567539456 logging_writer.py:48] [252800] global_step=252800, grad_norm=0.24153855443000793, loss=1.431749939918518
I0313 23:04:34.178558 139664575932160 logging_writer.py:48] [252900] global_step=252900, grad_norm=0.23868650197982788, loss=1.442632794380188
I0313 23:05:09.804152 139664567539456 logging_writer.py:48] [253000] global_step=253000, grad_norm=0.23822066187858582, loss=1.4161906242370605
I0313 23:05:45.373371 139664575932160 logging_writer.py:48] [253100] global_step=253100, grad_norm=0.23507681488990784, loss=1.478365182876587
I0313 23:06:21.017817 139664567539456 logging_writer.py:48] [253200] global_step=253200, grad_norm=0.24168017506599426, loss=1.4763200283050537
I0313 23:06:56.597346 139664575932160 logging_writer.py:48] [253300] global_step=253300, grad_norm=0.2363821417093277, loss=1.3870431184768677
I0313 23:07:32.194359 139664567539456 logging_writer.py:48] [253400] global_step=253400, grad_norm=0.23716114461421967, loss=1.3915683031082153
I0313 23:08:07.769403 139664575932160 logging_writer.py:48] [253500] global_step=253500, grad_norm=0.23780977725982666, loss=1.4342035055160522
I0313 23:08:43.372555 139664567539456 logging_writer.py:48] [253600] global_step=253600, grad_norm=0.2358446568250656, loss=1.4625394344329834
I0313 23:09:18.998104 139664575932160 logging_writer.py:48] [253700] global_step=253700, grad_norm=0.23462608456611633, loss=1.4199568033218384
I0313 23:09:54.592893 139664567539456 logging_writer.py:48] [253800] global_step=253800, grad_norm=0.2298659235239029, loss=1.4089568853378296
I0313 23:10:30.242337 139664575932160 logging_writer.py:48] [253900] global_step=253900, grad_norm=0.2306852489709854, loss=1.3745752573013306
I0313 23:11:05.893678 139664567539456 logging_writer.py:48] [254000] global_step=254000, grad_norm=0.2379896342754364, loss=1.4378106594085693
I0313 23:11:41.495590 139664575932160 logging_writer.py:48] [254100] global_step=254100, grad_norm=0.2335682213306427, loss=1.4483667612075806
I0313 23:12:17.081871 139664567539456 logging_writer.py:48] [254200] global_step=254200, grad_norm=0.2363480031490326, loss=1.4819836616516113
I0313 23:12:52.698318 139664575932160 logging_writer.py:48] [254300] global_step=254300, grad_norm=0.2338027060031891, loss=1.4968154430389404
I0313 23:13:28.338002 139664567539456 logging_writer.py:48] [254400] global_step=254400, grad_norm=0.23672433197498322, loss=1.481379508972168
I0313 23:14:03.972029 139664575932160 logging_writer.py:48] [254500] global_step=254500, grad_norm=0.2329845279455185, loss=1.5065120458602905
I0313 23:14:39.566583 139664567539456 logging_writer.py:48] [254600] global_step=254600, grad_norm=0.2287975698709488, loss=1.4063377380371094
I0313 23:15:15.187558 139664575932160 logging_writer.py:48] [254700] global_step=254700, grad_norm=0.23837511241436005, loss=1.438455581665039
I0313 23:15:50.854439 139664567539456 logging_writer.py:48] [254800] global_step=254800, grad_norm=0.23180152475833893, loss=1.4521806240081787
I0313 23:16:26.443005 139664575932160 logging_writer.py:48] [254900] global_step=254900, grad_norm=0.23288507759571075, loss=1.4369380474090576
I0313 23:17:02.052963 139664567539456 logging_writer.py:48] [255000] global_step=255000, grad_norm=0.2366606593132019, loss=1.3983761072158813
I0313 23:17:19.578400 139834281293632 spec.py:321] Evaluating on the training split.
I0313 23:17:22.551092 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:20:42.996001 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 23:20:45.671217 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:23:59.491088 139834281293632 spec.py:349] Evaluating on the test split.
I0313 23:24:02.171461 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:27:11.948962 139834281293632 submission_runner.py:420] Time since start: 155800.50s, 	Step: 255051, 	{'train/accuracy': 0.6940208673477173, 'train/loss': 1.374923586845398, 'train/bleu': 35.44952550740223, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 90765.08199381828, 'total_duration': 155800.5014474392, 'accumulated_submission_time': 90765.08199381828, 'accumulated_eval_time': 65023.09778881073, 'accumulated_logging_time': 4.534471273422241}
I0313 23:27:11.997393 139664575932160 logging_writer.py:48] [255051] accumulated_eval_time=65023.097789, accumulated_logging_time=4.534471, accumulated_submission_time=90765.081994, global_step=255051, preemption_count=0, score=90765.081994, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=155800.501447, train/accuracy=0.694021, train/bleu=35.449526, train/loss=1.374924, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 23:27:29.745460 139664567539456 logging_writer.py:48] [255100] global_step=255100, grad_norm=0.23297251760959625, loss=1.446101188659668
I0313 23:28:05.291096 139664575932160 logging_writer.py:48] [255200] global_step=255200, grad_norm=0.2319556325674057, loss=1.4163480997085571
I0313 23:28:40.871505 139664567539456 logging_writer.py:48] [255300] global_step=255300, grad_norm=0.24182407557964325, loss=1.4949564933776855
I0313 23:29:16.486546 139664575932160 logging_writer.py:48] [255400] global_step=255400, grad_norm=0.23061871528625488, loss=1.4571468830108643
I0313 23:29:52.049240 139664567539456 logging_writer.py:48] [255500] global_step=255500, grad_norm=0.2255074679851532, loss=1.335973858833313
I0313 23:30:27.621706 139664575932160 logging_writer.py:48] [255600] global_step=255600, grad_norm=0.23737001419067383, loss=1.471338152885437
I0313 23:31:03.216030 139664567539456 logging_writer.py:48] [255700] global_step=255700, grad_norm=0.23472411930561066, loss=1.4335960149765015
I0313 23:31:38.814887 139664575932160 logging_writer.py:48] [255800] global_step=255800, grad_norm=0.23106268048286438, loss=1.4677966833114624
I0313 23:32:14.402958 139664567539456 logging_writer.py:48] [255900] global_step=255900, grad_norm=0.24309004843235016, loss=1.431103229522705
I0313 23:32:50.020656 139664575932160 logging_writer.py:48] [256000] global_step=256000, grad_norm=0.24162332713603973, loss=1.4651683568954468
I0313 23:33:25.590323 139664567539456 logging_writer.py:48] [256100] global_step=256100, grad_norm=0.25041866302490234, loss=1.4684382677078247
I0313 23:34:01.199472 139664575932160 logging_writer.py:48] [256200] global_step=256200, grad_norm=0.23380716145038605, loss=1.4423960447311401
I0313 23:34:36.805944 139664567539456 logging_writer.py:48] [256300] global_step=256300, grad_norm=0.22434590756893158, loss=1.4603097438812256
I0313 23:35:12.407538 139664575932160 logging_writer.py:48] [256400] global_step=256400, grad_norm=0.23372937738895416, loss=1.489322304725647
I0313 23:35:48.000335 139664567539456 logging_writer.py:48] [256500] global_step=256500, grad_norm=0.24131885170936584, loss=1.4617326259613037
I0313 23:36:23.552928 139664575932160 logging_writer.py:48] [256600] global_step=256600, grad_norm=0.23170144855976105, loss=1.441532850265503
I0313 23:36:59.157155 139664567539456 logging_writer.py:48] [256700] global_step=256700, grad_norm=0.2308019995689392, loss=1.4735264778137207
I0313 23:37:34.753280 139664575932160 logging_writer.py:48] [256800] global_step=256800, grad_norm=0.23294702172279358, loss=1.3929756879806519
I0313 23:38:10.349502 139664567539456 logging_writer.py:48] [256900] global_step=256900, grad_norm=0.23059958219528198, loss=1.3940863609313965
I0313 23:38:45.981466 139664575932160 logging_writer.py:48] [257000] global_step=257000, grad_norm=0.22783207893371582, loss=1.3963768482208252
I0313 23:39:21.579409 139664567539456 logging_writer.py:48] [257100] global_step=257100, grad_norm=0.24092264473438263, loss=1.5047955513000488
I0313 23:39:57.200068 139664575932160 logging_writer.py:48] [257200] global_step=257200, grad_norm=0.22806592285633087, loss=1.470104694366455
I0313 23:40:32.790597 139664567539456 logging_writer.py:48] [257300] global_step=257300, grad_norm=0.22212031483650208, loss=1.4299535751342773
I0313 23:41:08.387703 139664575932160 logging_writer.py:48] [257400] global_step=257400, grad_norm=0.23767270147800446, loss=1.4156140089035034
I0313 23:41:12.019118 139834281293632 spec.py:321] Evaluating on the training split.
I0313 23:41:14.990290 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:44:52.628921 139834281293632 spec.py:333] Evaluating on the validation split.
I0313 23:44:55.313390 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:48:08.969112 139834281293632 spec.py:349] Evaluating on the test split.
I0313 23:48:11.646006 139834281293632 workload.py:181] Translating evaluation dataset.
I0313 23:51:21.294612 139834281293632 submission_runner.py:420] Time since start: 157249.85s, 	Step: 257412, 	{'train/accuracy': 0.6943745017051697, 'train/loss': 1.3740166425704956, 'train/bleu': 35.384300906911996, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 91605.0241189003, 'total_duration': 157249.84708356857, 'accumulated_submission_time': 91605.0241189003, 'accumulated_eval_time': 65632.37321901321, 'accumulated_logging_time': 4.591864347457886}
I0313 23:51:21.342070 139664567539456 logging_writer.py:48] [257412] accumulated_eval_time=65632.373219, accumulated_logging_time=4.591864, accumulated_submission_time=91605.024119, global_step=257412, preemption_count=0, score=91605.024119, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=157249.847084, train/accuracy=0.694375, train/bleu=35.384301, train/loss=1.374017, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0313 23:51:52.953913 139664575932160 logging_writer.py:48] [257500] global_step=257500, grad_norm=0.2285986691713333, loss=1.4172145128250122
I0313 23:52:28.502437 139664567539456 logging_writer.py:48] [257600] global_step=257600, grad_norm=0.2288265824317932, loss=1.430138349533081
I0313 23:53:04.082590 139664575932160 logging_writer.py:48] [257700] global_step=257700, grad_norm=0.24431639909744263, loss=1.5138881206512451
I0313 23:53:39.688254 139664567539456 logging_writer.py:48] [257800] global_step=257800, grad_norm=0.22174400091171265, loss=1.3582333326339722
I0313 23:54:15.323272 139664575932160 logging_writer.py:48] [257900] global_step=257900, grad_norm=0.2298845648765564, loss=1.4468249082565308
I0313 23:54:50.901764 139664567539456 logging_writer.py:48] [258000] global_step=258000, grad_norm=0.22656437754631042, loss=1.3920601606369019
I0313 23:55:26.466602 139664575932160 logging_writer.py:48] [258100] global_step=258100, grad_norm=0.24466900527477264, loss=1.49434232711792
I0313 23:56:02.080404 139664567539456 logging_writer.py:48] [258200] global_step=258200, grad_norm=0.23619161546230316, loss=1.5521677732467651
I0313 23:56:37.698703 139664575932160 logging_writer.py:48] [258300] global_step=258300, grad_norm=0.23074080049991608, loss=1.40300452709198
I0313 23:57:13.323157 139664567539456 logging_writer.py:48] [258400] global_step=258400, grad_norm=0.23431088030338287, loss=1.4659178256988525
I0313 23:57:48.918477 139664575932160 logging_writer.py:48] [258500] global_step=258500, grad_norm=0.23169445991516113, loss=1.4082788228988647
I0313 23:58:24.510305 139664567539456 logging_writer.py:48] [258600] global_step=258600, grad_norm=0.23455633223056793, loss=1.4701721668243408
I0313 23:59:00.108324 139664575932160 logging_writer.py:48] [258700] global_step=258700, grad_norm=0.2452135533094406, loss=1.3971043825149536
I0313 23:59:35.720190 139664567539456 logging_writer.py:48] [258800] global_step=258800, grad_norm=0.22692380845546722, loss=1.4119977951049805
I0314 00:00:11.300912 139664575932160 logging_writer.py:48] [258900] global_step=258900, grad_norm=0.2316867560148239, loss=1.3855003118515015
I0314 00:00:46.900521 139664567539456 logging_writer.py:48] [259000] global_step=259000, grad_norm=0.22542579472064972, loss=1.4346096515655518
I0314 00:01:22.495104 139664575932160 logging_writer.py:48] [259100] global_step=259100, grad_norm=0.22656065225601196, loss=1.4487254619598389
I0314 00:01:58.081276 139664567539456 logging_writer.py:48] [259200] global_step=259200, grad_norm=0.23816166818141937, loss=1.4214880466461182
I0314 00:02:33.705773 139664575932160 logging_writer.py:48] [259300] global_step=259300, grad_norm=0.23750609159469604, loss=1.4915862083435059
I0314 00:03:09.363714 139664567539456 logging_writer.py:48] [259400] global_step=259400, grad_norm=0.23244662582874298, loss=1.4730806350708008
I0314 00:03:45.019633 139664575932160 logging_writer.py:48] [259500] global_step=259500, grad_norm=0.24152374267578125, loss=1.481968879699707
I0314 00:04:20.660237 139664567539456 logging_writer.py:48] [259600] global_step=259600, grad_norm=0.22470687329769135, loss=1.4016553163528442
I0314 00:04:56.243489 139664575932160 logging_writer.py:48] [259700] global_step=259700, grad_norm=0.22722363471984863, loss=1.4813591241836548
I0314 00:05:21.618442 139834281293632 spec.py:321] Evaluating on the training split.
I0314 00:05:24.588739 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:08:52.446028 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 00:08:55.148319 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:12:09.177665 139834281293632 spec.py:349] Evaluating on the test split.
I0314 00:12:11.845370 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:15:21.592501 139834281293632 submission_runner.py:420] Time since start: 158690.14s, 	Step: 259773, 	{'train/accuracy': 0.6928779482841492, 'train/loss': 1.3875993490219116, 'train/bleu': 35.38147552282423, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 92445.21916270256, 'total_duration': 158690.14498519897, 'accumulated_submission_time': 92445.21916270256, 'accumulated_eval_time': 66232.347230196, 'accumulated_logging_time': 4.648017883300781}
I0314 00:15:21.641063 139664567539456 logging_writer.py:48] [259773] accumulated_eval_time=66232.347230, accumulated_logging_time=4.648018, accumulated_submission_time=92445.219163, global_step=259773, preemption_count=0, score=92445.219163, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=158690.144985, train/accuracy=0.692878, train/bleu=35.381476, train/loss=1.387599, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 00:15:31.601205 139664575932160 logging_writer.py:48] [259800] global_step=259800, grad_norm=0.23128476738929749, loss=1.4371095895767212
I0314 00:16:07.118276 139664567539456 logging_writer.py:48] [259900] global_step=259900, grad_norm=0.23594853281974792, loss=1.4068944454193115
I0314 00:16:42.729451 139664575932160 logging_writer.py:48] [260000] global_step=260000, grad_norm=0.23462721705436707, loss=1.4552764892578125
I0314 00:17:18.330624 139664567539456 logging_writer.py:48] [260100] global_step=260100, grad_norm=0.23026154935359955, loss=1.3832699060440063
I0314 00:17:53.924305 139664575932160 logging_writer.py:48] [260200] global_step=260200, grad_norm=0.22764146327972412, loss=1.4260374307632446
I0314 00:18:29.528359 139664567539456 logging_writer.py:48] [260300] global_step=260300, grad_norm=0.24421705305576324, loss=1.5008500814437866
I0314 00:19:05.163303 139664575932160 logging_writer.py:48] [260400] global_step=260400, grad_norm=0.23169833421707153, loss=1.3752802610397339
I0314 00:19:40.778597 139664567539456 logging_writer.py:48] [260500] global_step=260500, grad_norm=0.23455150425434113, loss=1.4496228694915771
I0314 00:20:16.387891 139664575932160 logging_writer.py:48] [260600] global_step=260600, grad_norm=0.24604150652885437, loss=1.4422128200531006
I0314 00:20:51.992826 139664567539456 logging_writer.py:48] [260700] global_step=260700, grad_norm=0.23715820908546448, loss=1.4363800287246704
I0314 00:21:27.584082 139664575932160 logging_writer.py:48] [260800] global_step=260800, grad_norm=0.24031572043895721, loss=1.4653429985046387
I0314 00:22:03.200281 139664567539456 logging_writer.py:48] [260900] global_step=260900, grad_norm=0.23496712744235992, loss=1.439155101776123
I0314 00:22:38.790457 139664575932160 logging_writer.py:48] [261000] global_step=261000, grad_norm=0.23511064052581787, loss=1.4478179216384888
I0314 00:23:14.382989 139664567539456 logging_writer.py:48] [261100] global_step=261100, grad_norm=0.2260109931230545, loss=1.4309126138687134
I0314 00:23:49.981486 139664575932160 logging_writer.py:48] [261200] global_step=261200, grad_norm=0.23552395403385162, loss=1.530473232269287
I0314 00:24:25.614076 139664567539456 logging_writer.py:48] [261300] global_step=261300, grad_norm=0.24598786234855652, loss=1.540717363357544
I0314 00:25:01.235864 139664575932160 logging_writer.py:48] [261400] global_step=261400, grad_norm=0.22724951803684235, loss=1.4780107736587524
I0314 00:25:36.800585 139664567539456 logging_writer.py:48] [261500] global_step=261500, grad_norm=0.23870949447155, loss=1.4890594482421875
I0314 00:26:12.389760 139664575932160 logging_writer.py:48] [261600] global_step=261600, grad_norm=0.2310008555650711, loss=1.5253524780273438
I0314 00:26:47.975275 139664567539456 logging_writer.py:48] [261700] global_step=261700, grad_norm=0.23585185408592224, loss=1.3961166143417358
I0314 00:27:23.573879 139664575932160 logging_writer.py:48] [261800] global_step=261800, grad_norm=0.23351840674877167, loss=1.4596953392028809
I0314 00:27:59.197112 139664567539456 logging_writer.py:48] [261900] global_step=261900, grad_norm=0.23494531214237213, loss=1.4386682510375977
I0314 00:28:34.792176 139664575932160 logging_writer.py:48] [262000] global_step=262000, grad_norm=0.2558898329734802, loss=1.4799760580062866
I0314 00:29:10.394793 139664567539456 logging_writer.py:48] [262100] global_step=262100, grad_norm=0.2361350655555725, loss=1.3849414587020874
I0314 00:29:21.856829 139834281293632 spec.py:321] Evaluating on the training split.
I0314 00:29:24.832156 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:32:54.857109 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 00:32:57.528494 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:36:11.334837 139834281293632 spec.py:349] Evaluating on the test split.
I0314 00:36:14.001503 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:39:23.975785 139834281293632 submission_runner.py:420] Time since start: 160132.53s, 	Step: 262134, 	{'train/accuracy': 0.6953383088111877, 'train/loss': 1.3716511726379395, 'train/bleu': 35.612914454909664, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 93285.35460090637, 'total_duration': 160132.52825331688, 'accumulated_submission_time': 93285.35460090637, 'accumulated_eval_time': 66834.46612238884, 'accumulated_logging_time': 4.7053093910217285}
I0314 00:39:24.038994 139664575932160 logging_writer.py:48] [262134] accumulated_eval_time=66834.466122, accumulated_logging_time=4.705309, accumulated_submission_time=93285.354601, global_step=262134, preemption_count=0, score=93285.354601, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=160132.528253, train/accuracy=0.695338, train/bleu=35.612914, train/loss=1.371651, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 00:39:47.834568 139664567539456 logging_writer.py:48] [262200] global_step=262200, grad_norm=0.2331923097372055, loss=1.4655983448028564
I0314 00:40:23.380874 139664575932160 logging_writer.py:48] [262300] global_step=262300, grad_norm=0.236973837018013, loss=1.4743324518203735
I0314 00:40:58.975311 139664567539456 logging_writer.py:48] [262400] global_step=262400, grad_norm=0.22731240093708038, loss=1.4241434335708618
I0314 00:41:34.615761 139664575932160 logging_writer.py:48] [262500] global_step=262500, grad_norm=0.23977941274642944, loss=1.4415425062179565
I0314 00:42:10.266600 139664567539456 logging_writer.py:48] [262600] global_step=262600, grad_norm=0.23171445727348328, loss=1.4060670137405396
I0314 00:42:45.875983 139664575932160 logging_writer.py:48] [262700] global_step=262700, grad_norm=0.2386622428894043, loss=1.5122182369232178
I0314 00:43:21.484821 139664567539456 logging_writer.py:48] [262800] global_step=262800, grad_norm=0.2347206026315689, loss=1.4445091485977173
I0314 00:43:57.075299 139664575932160 logging_writer.py:48] [262900] global_step=262900, grad_norm=0.24011829495429993, loss=1.3930600881576538
I0314 00:44:32.667508 139664567539456 logging_writer.py:48] [263000] global_step=263000, grad_norm=0.23672139644622803, loss=1.430199146270752
I0314 00:45:08.275468 139664575932160 logging_writer.py:48] [263100] global_step=263100, grad_norm=0.2279464155435562, loss=1.4600906372070312
I0314 00:45:43.881331 139664567539456 logging_writer.py:48] [263200] global_step=263200, grad_norm=0.23456206917762756, loss=1.4474880695343018
I0314 00:46:19.504685 139664575932160 logging_writer.py:48] [263300] global_step=263300, grad_norm=0.2454293817281723, loss=1.4205995798110962
I0314 00:46:55.137613 139664567539456 logging_writer.py:48] [263400] global_step=263400, grad_norm=0.22499503195285797, loss=1.4328949451446533
I0314 00:47:30.725540 139664575932160 logging_writer.py:48] [263500] global_step=263500, grad_norm=0.2323547899723053, loss=1.4123297929763794
I0314 00:48:06.337538 139664567539456 logging_writer.py:48] [263600] global_step=263600, grad_norm=0.23765143752098083, loss=1.4859257936477661
I0314 00:48:41.935375 139664575932160 logging_writer.py:48] [263700] global_step=263700, grad_norm=0.23724690079689026, loss=1.4397192001342773
I0314 00:49:17.553875 139664567539456 logging_writer.py:48] [263800] global_step=263800, grad_norm=0.23626430332660675, loss=1.450088381767273
I0314 00:49:53.188727 139664575932160 logging_writer.py:48] [263900] global_step=263900, grad_norm=0.24840374290943146, loss=1.448670506477356
I0314 00:50:28.809413 139664567539456 logging_writer.py:48] [264000] global_step=264000, grad_norm=0.23545849323272705, loss=1.4111789464950562
I0314 00:51:04.471334 139664575932160 logging_writer.py:48] [264100] global_step=264100, grad_norm=0.23606334626674652, loss=1.4099323749542236
I0314 00:51:40.130801 139664567539456 logging_writer.py:48] [264200] global_step=264200, grad_norm=0.23366163671016693, loss=1.49573814868927
I0314 00:52:15.735114 139664575932160 logging_writer.py:48] [264300] global_step=264300, grad_norm=0.2359960824251175, loss=1.45341956615448
I0314 00:52:51.332058 139664567539456 logging_writer.py:48] [264400] global_step=264400, grad_norm=0.22972337901592255, loss=1.490724802017212
I0314 00:53:24.133141 139834281293632 spec.py:321] Evaluating on the training split.
I0314 00:53:27.102083 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 00:56:59.676512 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 00:57:02.382989 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:00:16.293270 139834281293632 spec.py:349] Evaluating on the test split.
I0314 01:00:18.962116 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:03:28.599098 139834281293632 submission_runner.py:420] Time since start: 161577.15s, 	Step: 264494, 	{'train/accuracy': 0.6960185170173645, 'train/loss': 1.37083899974823, 'train/bleu': 35.742916565992346, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 94125.36615252495, 'total_duration': 161577.15155768394, 'accumulated_submission_time': 94125.36615252495, 'accumulated_eval_time': 67438.93201184273, 'accumulated_logging_time': 4.778682708740234}
I0314 01:03:28.647886 139664575932160 logging_writer.py:48] [264494] accumulated_eval_time=67438.932012, accumulated_logging_time=4.778683, accumulated_submission_time=94125.366153, global_step=264494, preemption_count=0, score=94125.366153, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=161577.151558, train/accuracy=0.696019, train/bleu=35.742917, train/loss=1.370839, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 01:03:31.164495 139664567539456 logging_writer.py:48] [264500] global_step=264500, grad_norm=0.22896862030029297, loss=1.4960721731185913
I0314 01:04:06.686812 139664575932160 logging_writer.py:48] [264600] global_step=264600, grad_norm=0.23028840124607086, loss=1.4488232135772705
I0314 01:04:42.285334 139664567539456 logging_writer.py:48] [264700] global_step=264700, grad_norm=0.2321489006280899, loss=1.384493112564087
I0314 01:05:17.917053 139664575932160 logging_writer.py:48] [264800] global_step=264800, grad_norm=0.24179209768772125, loss=1.5617510080337524
I0314 01:05:53.538791 139664567539456 logging_writer.py:48] [264900] global_step=264900, grad_norm=0.2314853072166443, loss=1.484567642211914
I0314 01:06:29.159614 139664575932160 logging_writer.py:48] [265000] global_step=265000, grad_norm=0.2363516241312027, loss=1.5008597373962402
I0314 01:07:04.776299 139664567539456 logging_writer.py:48] [265100] global_step=265100, grad_norm=0.22980816662311554, loss=1.4073305130004883
I0314 01:07:40.341703 139664575932160 logging_writer.py:48] [265200] global_step=265200, grad_norm=0.23574508726596832, loss=1.4469982385635376
I0314 01:08:15.940630 139664567539456 logging_writer.py:48] [265300] global_step=265300, grad_norm=0.23069439828395844, loss=1.4529601335525513
I0314 01:08:51.537002 139664575932160 logging_writer.py:48] [265400] global_step=265400, grad_norm=0.23871222138404846, loss=1.4278309345245361
I0314 01:09:27.138925 139664567539456 logging_writer.py:48] [265500] global_step=265500, grad_norm=0.24006523191928864, loss=1.4028679132461548
I0314 01:10:02.751810 139664575932160 logging_writer.py:48] [265600] global_step=265600, grad_norm=0.2344975620508194, loss=1.4399425983428955
I0314 01:10:38.333890 139664567539456 logging_writer.py:48] [265700] global_step=265700, grad_norm=0.2403307557106018, loss=1.5091665983200073
I0314 01:11:13.921588 139664575932160 logging_writer.py:48] [265800] global_step=265800, grad_norm=0.22568905353546143, loss=1.433830976486206
I0314 01:11:49.504565 139664567539456 logging_writer.py:48] [265900] global_step=265900, grad_norm=0.24471652507781982, loss=1.4381862878799438
I0314 01:12:25.101931 139664575932160 logging_writer.py:48] [266000] global_step=266000, grad_norm=0.2283768355846405, loss=1.4182552099227905
I0314 01:13:00.685697 139664567539456 logging_writer.py:48] [266100] global_step=266100, grad_norm=0.23399360477924347, loss=1.497507095336914
I0314 01:13:36.291271 139664575932160 logging_writer.py:48] [266200] global_step=266200, grad_norm=0.23460175096988678, loss=1.4343597888946533
I0314 01:14:11.907019 139664567539456 logging_writer.py:48] [266300] global_step=266300, grad_norm=0.23270905017852783, loss=1.3997291326522827
I0314 01:14:47.536884 139664575932160 logging_writer.py:48] [266400] global_step=266400, grad_norm=0.23048172891139984, loss=1.4266756772994995
I0314 01:15:23.148806 139664567539456 logging_writer.py:48] [266500] global_step=266500, grad_norm=0.22464683651924133, loss=1.4180340766906738
I0314 01:15:58.735153 139664575932160 logging_writer.py:48] [266600] global_step=266600, grad_norm=0.22940851747989655, loss=1.4362350702285767
I0314 01:16:34.319433 139664567539456 logging_writer.py:48] [266700] global_step=266700, grad_norm=0.22396090626716614, loss=1.430260181427002
I0314 01:17:09.905577 139664575932160 logging_writer.py:48] [266800] global_step=266800, grad_norm=0.22837454080581665, loss=1.4190717935562134
I0314 01:17:28.857357 139834281293632 spec.py:321] Evaluating on the training split.
I0314 01:17:31.829431 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:21:07.548863 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 01:21:10.224966 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:24:25.667829 139834281293632 spec.py:349] Evaluating on the test split.
I0314 01:24:28.352177 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:27:39.931729 139834281293632 submission_runner.py:420] Time since start: 163028.48s, 	Step: 266855, 	{'train/accuracy': 0.6939523220062256, 'train/loss': 1.3873682022094727, 'train/bleu': 35.54363592145518, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 94965.49518156052, 'total_duration': 163028.48417282104, 'accumulated_submission_time': 94965.49518156052, 'accumulated_eval_time': 68050.00629425049, 'accumulated_logging_time': 4.836698532104492}
I0314 01:27:39.992537 139664567539456 logging_writer.py:48] [266855] accumulated_eval_time=68050.006294, accumulated_logging_time=4.836699, accumulated_submission_time=94965.495182, global_step=266855, preemption_count=0, score=94965.495182, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=163028.484173, train/accuracy=0.693952, train/bleu=35.543636, train/loss=1.387368, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 01:27:56.349098 139664575932160 logging_writer.py:48] [266900] global_step=266900, grad_norm=0.2404140681028366, loss=1.3796424865722656
I0314 01:28:31.956956 139664567539456 logging_writer.py:48] [267000] global_step=267000, grad_norm=0.2510971128940582, loss=1.4821559190750122
I0314 01:29:07.619527 139664575932160 logging_writer.py:48] [267100] global_step=267100, grad_norm=0.23469427227973938, loss=1.5073274374008179
I0314 01:29:43.222295 139664567539456 logging_writer.py:48] [267200] global_step=267200, grad_norm=0.2356012910604477, loss=1.4480094909667969
I0314 01:30:18.800545 139664575932160 logging_writer.py:48] [267300] global_step=267300, grad_norm=0.23634451627731323, loss=1.4297816753387451
I0314 01:30:54.391303 139664567539456 logging_writer.py:48] [267400] global_step=267400, grad_norm=0.23287396132946014, loss=1.4934815168380737
I0314 01:31:29.990074 139664575932160 logging_writer.py:48] [267500] global_step=267500, grad_norm=0.23349244892597198, loss=1.4378224611282349
I0314 01:32:05.608032 139664567539456 logging_writer.py:48] [267600] global_step=267600, grad_norm=0.23109827935695648, loss=1.444783091545105
I0314 01:32:41.196087 139664575932160 logging_writer.py:48] [267700] global_step=267700, grad_norm=0.2378641664981842, loss=1.4568438529968262
I0314 01:33:16.821695 139664567539456 logging_writer.py:48] [267800] global_step=267800, grad_norm=0.24115586280822754, loss=1.4846893548965454
I0314 01:33:52.405303 139664575932160 logging_writer.py:48] [267900] global_step=267900, grad_norm=0.23825402557849884, loss=1.3900103569030762
I0314 01:34:28.029394 139664567539456 logging_writer.py:48] [268000] global_step=268000, grad_norm=0.24065814912319183, loss=1.4887971878051758
I0314 01:35:03.638585 139664575932160 logging_writer.py:48] [268100] global_step=268100, grad_norm=0.23544040322303772, loss=1.4417879581451416
I0314 01:35:39.241899 139664567539456 logging_writer.py:48] [268200] global_step=268200, grad_norm=0.2438189834356308, loss=1.497369647026062
I0314 01:36:14.834177 139664575932160 logging_writer.py:48] [268300] global_step=268300, grad_norm=0.242270365357399, loss=1.3964999914169312
I0314 01:36:50.428814 139664567539456 logging_writer.py:48] [268400] global_step=268400, grad_norm=0.23404061794281006, loss=1.4782415628433228
I0314 01:37:26.053403 139664575932160 logging_writer.py:48] [268500] global_step=268500, grad_norm=0.2337048351764679, loss=1.456265926361084
I0314 01:38:01.674565 139664567539456 logging_writer.py:48] [268600] global_step=268600, grad_norm=0.22635336220264435, loss=1.4299474954605103
I0314 01:38:37.253791 139664575932160 logging_writer.py:48] [268700] global_step=268700, grad_norm=0.24251846969127655, loss=1.4944658279418945
I0314 01:39:12.884356 139664567539456 logging_writer.py:48] [268800] global_step=268800, grad_norm=0.24527095258235931, loss=1.4795706272125244
I0314 01:39:48.461205 139664575932160 logging_writer.py:48] [268900] global_step=268900, grad_norm=0.23838433623313904, loss=1.4965680837631226
I0314 01:40:24.065051 139664567539456 logging_writer.py:48] [269000] global_step=269000, grad_norm=0.2253342717885971, loss=1.4061741828918457
I0314 01:40:59.680987 139664575932160 logging_writer.py:48] [269100] global_step=269100, grad_norm=0.23846864700317383, loss=1.494692087173462
I0314 01:41:35.332357 139664567539456 logging_writer.py:48] [269200] global_step=269200, grad_norm=0.24032923579216003, loss=1.486997365951538
I0314 01:41:40.034450 139834281293632 spec.py:321] Evaluating on the training split.
I0314 01:41:43.002046 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:45:10.510002 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 01:45:13.181382 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:48:29.011134 139834281293632 spec.py:349] Evaluating on the test split.
I0314 01:48:31.711983 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 01:51:42.874747 139834281293632 submission_runner.py:420] Time since start: 164471.43s, 	Step: 269215, 	{'train/accuracy': 0.6959852576255798, 'train/loss': 1.371500015258789, 'train/bleu': 35.770785621182796, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 95805.45388770103, 'total_duration': 164471.42721128464, 'accumulated_submission_time': 95805.45388770103, 'accumulated_eval_time': 68652.84652853012, 'accumulated_logging_time': 4.907514810562134}
I0314 01:51:42.926741 139664575932160 logging_writer.py:48] [269215] accumulated_eval_time=68652.846529, accumulated_logging_time=4.907515, accumulated_submission_time=95805.453888, global_step=269215, preemption_count=0, score=95805.453888, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=164471.427211, train/accuracy=0.695985, train/bleu=35.770786, train/loss=1.371500, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 01:52:13.491613 139664567539456 logging_writer.py:48] [269300] global_step=269300, grad_norm=0.23404277861118317, loss=1.4609339237213135
I0314 01:52:49.054273 139664575932160 logging_writer.py:48] [269400] global_step=269400, grad_norm=0.23201632499694824, loss=1.4667555093765259
I0314 01:53:24.610424 139664567539456 logging_writer.py:48] [269500] global_step=269500, grad_norm=0.24675732851028442, loss=1.52829909324646
I0314 01:54:00.193895 139664575932160 logging_writer.py:48] [269600] global_step=269600, grad_norm=0.24059166014194489, loss=1.4905000925064087
I0314 01:54:35.788991 139664567539456 logging_writer.py:48] [269700] global_step=269700, grad_norm=0.23349067568778992, loss=1.4444612264633179
I0314 01:55:11.405086 139664575932160 logging_writer.py:48] [269800] global_step=269800, grad_norm=0.22951148450374603, loss=1.4320718050003052
I0314 01:55:46.980822 139664567539456 logging_writer.py:48] [269900] global_step=269900, grad_norm=0.2296605408191681, loss=1.370477557182312
I0314 01:56:22.580414 139664575932160 logging_writer.py:48] [270000] global_step=270000, grad_norm=0.23988164961338043, loss=1.4624290466308594
I0314 01:56:58.141617 139664567539456 logging_writer.py:48] [270100] global_step=270100, grad_norm=0.23003657162189484, loss=1.4695703983306885
I0314 01:57:33.736708 139664575932160 logging_writer.py:48] [270200] global_step=270200, grad_norm=0.24568858742713928, loss=1.4447340965270996
I0314 01:58:09.348601 139664567539456 logging_writer.py:48] [270300] global_step=270300, grad_norm=0.24467752873897552, loss=1.4948292970657349
I0314 01:58:44.993865 139664575932160 logging_writer.py:48] [270400] global_step=270400, grad_norm=0.24340113997459412, loss=1.4208130836486816
I0314 01:59:20.624485 139664567539456 logging_writer.py:48] [270500] global_step=270500, grad_norm=0.23246873915195465, loss=1.4171292781829834
I0314 01:59:56.207858 139664575932160 logging_writer.py:48] [270600] global_step=270600, grad_norm=0.23812225461006165, loss=1.4700331687927246
I0314 02:00:31.793426 139664567539456 logging_writer.py:48] [270700] global_step=270700, grad_norm=0.2390999048948288, loss=1.5891176462173462
I0314 02:01:07.375484 139664575932160 logging_writer.py:48] [270800] global_step=270800, grad_norm=0.23473496735095978, loss=1.3935236930847168
I0314 02:01:43.027678 139664567539456 logging_writer.py:48] [270900] global_step=270900, grad_norm=0.23427212238311768, loss=1.4015284776687622
I0314 02:02:18.622187 139664575932160 logging_writer.py:48] [271000] global_step=271000, grad_norm=0.23164837062358856, loss=1.4524604082107544
I0314 02:02:54.224782 139664567539456 logging_writer.py:48] [271100] global_step=271100, grad_norm=0.23479457199573517, loss=1.4447404146194458
I0314 02:03:29.812215 139664575932160 logging_writer.py:48] [271200] global_step=271200, grad_norm=0.23743845522403717, loss=1.4577879905700684
I0314 02:04:05.456210 139664567539456 logging_writer.py:48] [271300] global_step=271300, grad_norm=0.2350776493549347, loss=1.5175493955612183
I0314 02:04:41.035018 139664575932160 logging_writer.py:48] [271400] global_step=271400, grad_norm=0.2319253832101822, loss=1.4661309719085693
I0314 02:05:16.640742 139664567539456 logging_writer.py:48] [271500] global_step=271500, grad_norm=0.24058452248573303, loss=1.4500267505645752
I0314 02:05:43.054521 139834281293632 spec.py:321] Evaluating on the training split.
I0314 02:05:46.026817 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:09:20.246312 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 02:09:22.917775 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:12:36.697091 139834281293632 spec.py:349] Evaluating on the test split.
I0314 02:12:39.375252 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:15:48.950052 139834281293632 submission_runner.py:420] Time since start: 165917.50s, 	Step: 271576, 	{'train/accuracy': 0.694656491279602, 'train/loss': 1.3786284923553467, 'train/bleu': 35.713971528505866, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 96645.50051903725, 'total_duration': 165917.50253725052, 'accumulated_submission_time': 96645.50051903725, 'accumulated_eval_time': 69258.74201393127, 'accumulated_logging_time': 4.968581914901733}
I0314 02:15:49.045957 139664575932160 logging_writer.py:48] [271576] accumulated_eval_time=69258.742014, accumulated_logging_time=4.968582, accumulated_submission_time=96645.500519, global_step=271576, preemption_count=0, score=96645.500519, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=165917.502537, train/accuracy=0.694656, train/bleu=35.713972, train/loss=1.378628, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 02:15:57.927326 139664567539456 logging_writer.py:48] [271600] global_step=271600, grad_norm=0.240489199757576, loss=1.5465528964996338
I0314 02:16:33.455996 139664575932160 logging_writer.py:48] [271700] global_step=271700, grad_norm=0.2336476743221283, loss=1.4185839891433716
I0314 02:17:09.012077 139664567539456 logging_writer.py:48] [271800] global_step=271800, grad_norm=0.23567183315753937, loss=1.4793134927749634
I0314 02:17:44.626576 139664575932160 logging_writer.py:48] [271900] global_step=271900, grad_norm=0.22920282185077667, loss=1.504075050354004
I0314 02:18:20.261177 139664567539456 logging_writer.py:48] [272000] global_step=272000, grad_norm=0.23410248756408691, loss=1.4599461555480957
I0314 02:18:55.872471 139664575932160 logging_writer.py:48] [272100] global_step=272100, grad_norm=0.23568877577781677, loss=1.4722858667373657
I0314 02:19:31.457899 139664567539456 logging_writer.py:48] [272200] global_step=272200, grad_norm=0.23960080742835999, loss=1.4578876495361328
I0314 02:20:07.046661 139664575932160 logging_writer.py:48] [272300] global_step=272300, grad_norm=0.242639422416687, loss=1.5182580947875977
I0314 02:20:42.626534 139664567539456 logging_writer.py:48] [272400] global_step=272400, grad_norm=0.2576999068260193, loss=1.3974653482437134
I0314 02:21:18.239262 139664575932160 logging_writer.py:48] [272500] global_step=272500, grad_norm=0.22936832904815674, loss=1.4504019021987915
I0314 02:21:53.848928 139664567539456 logging_writer.py:48] [272600] global_step=272600, grad_norm=0.23960085213184357, loss=1.4353491067886353
I0314 02:22:29.444134 139664575932160 logging_writer.py:48] [272700] global_step=272700, grad_norm=0.2463863492012024, loss=1.4884873628616333
I0314 02:23:05.061577 139664567539456 logging_writer.py:48] [272800] global_step=272800, grad_norm=0.2352321743965149, loss=1.4001530408859253
I0314 02:23:40.629220 139664575932160 logging_writer.py:48] [272900] global_step=272900, grad_norm=0.22970005869865417, loss=1.3765746355056763
I0314 02:24:16.245297 139664567539456 logging_writer.py:48] [273000] global_step=273000, grad_norm=0.23622390627861023, loss=1.4031678438186646
I0314 02:24:51.818815 139664575932160 logging_writer.py:48] [273100] global_step=273100, grad_norm=0.2424655705690384, loss=1.512871265411377
I0314 02:25:27.415414 139664567539456 logging_writer.py:48] [273200] global_step=273200, grad_norm=0.23401175439357758, loss=1.390057921409607
I0314 02:26:03.036862 139664575932160 logging_writer.py:48] [273300] global_step=273300, grad_norm=0.23202019929885864, loss=1.4115368127822876
I0314 02:26:38.663346 139664567539456 logging_writer.py:48] [273400] global_step=273400, grad_norm=0.23193955421447754, loss=1.3748435974121094
I0314 02:27:14.305983 139664575932160 logging_writer.py:48] [273500] global_step=273500, grad_norm=0.23527508974075317, loss=1.4630945920944214
I0314 02:27:49.943122 139664567539456 logging_writer.py:48] [273600] global_step=273600, grad_norm=0.24509313702583313, loss=1.432783603668213
I0314 02:28:25.566009 139664575932160 logging_writer.py:48] [273700] global_step=273700, grad_norm=0.2282153218984604, loss=1.4223237037658691
I0314 02:29:01.161687 139664567539456 logging_writer.py:48] [273800] global_step=273800, grad_norm=0.24145852029323578, loss=1.4822512865066528
I0314 02:29:36.753990 139664575932160 logging_writer.py:48] [273900] global_step=273900, grad_norm=0.23212376236915588, loss=1.441023826599121
I0314 02:29:49.269643 139834281293632 spec.py:321] Evaluating on the training split.
I0314 02:29:52.241280 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:33:17.181824 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 02:33:19.861237 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:36:34.724287 139834281293632 spec.py:349] Evaluating on the test split.
I0314 02:36:37.401721 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:39:47.307370 139834281293632 submission_runner.py:420] Time since start: 167355.86s, 	Step: 273937, 	{'train/accuracy': 0.6959747076034546, 'train/loss': 1.3692622184753418, 'train/bleu': 35.55752862460547, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 97485.64288377762, 'total_duration': 167355.8598549366, 'accumulated_submission_time': 97485.64288377762, 'accumulated_eval_time': 69856.77969145775, 'accumulated_logging_time': 5.073653221130371}
I0314 02:39:47.357702 139664567539456 logging_writer.py:48] [273937] accumulated_eval_time=69856.779691, accumulated_logging_time=5.073653, accumulated_submission_time=97485.642884, global_step=273937, preemption_count=0, score=97485.642884, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=167355.859855, train/accuracy=0.695975, train/bleu=35.557529, train/loss=1.369262, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 02:40:10.101193 139664575932160 logging_writer.py:48] [274000] global_step=274000, grad_norm=0.23404476046562195, loss=1.500710129737854
I0314 02:40:45.675581 139664567539456 logging_writer.py:48] [274100] global_step=274100, grad_norm=0.2246931493282318, loss=1.4331644773483276
I0314 02:41:21.327170 139664575932160 logging_writer.py:48] [274200] global_step=274200, grad_norm=0.24305912852287292, loss=1.4674099683761597
I0314 02:41:56.964760 139664567539456 logging_writer.py:48] [274300] global_step=274300, grad_norm=0.2504899799823761, loss=1.5100624561309814
I0314 02:42:32.584859 139664575932160 logging_writer.py:48] [274400] global_step=274400, grad_norm=0.2341700941324234, loss=1.3887759447097778
I0314 02:43:08.181963 139664567539456 logging_writer.py:48] [274500] global_step=274500, grad_norm=0.24495792388916016, loss=1.482354998588562
I0314 02:43:43.806843 139664575932160 logging_writer.py:48] [274600] global_step=274600, grad_norm=0.2355784922838211, loss=1.403949499130249
I0314 02:44:19.446765 139664567539456 logging_writer.py:48] [274700] global_step=274700, grad_norm=0.23195458948612213, loss=1.399060845375061
I0314 02:44:55.087743 139664575932160 logging_writer.py:48] [274800] global_step=274800, grad_norm=0.23006071150302887, loss=1.3771584033966064
I0314 02:45:30.763711 139664567539456 logging_writer.py:48] [274900] global_step=274900, grad_norm=0.2343156635761261, loss=1.4274393320083618
I0314 02:46:06.393071 139664575932160 logging_writer.py:48] [275000] global_step=275000, grad_norm=0.24101699888706207, loss=1.4233226776123047
I0314 02:46:41.997318 139664567539456 logging_writer.py:48] [275100] global_step=275100, grad_norm=0.22946800291538239, loss=1.4526269435882568
I0314 02:47:17.619377 139664575932160 logging_writer.py:48] [275200] global_step=275200, grad_norm=0.23945476114749908, loss=1.5208323001861572
I0314 02:47:53.235035 139664567539456 logging_writer.py:48] [275300] global_step=275300, grad_norm=0.23459942638874054, loss=1.4486476182937622
I0314 02:48:28.863007 139664575932160 logging_writer.py:48] [275400] global_step=275400, grad_norm=0.23938900232315063, loss=1.4561355113983154
I0314 02:49:04.463750 139664567539456 logging_writer.py:48] [275500] global_step=275500, grad_norm=0.23257024586200714, loss=1.519272804260254
I0314 02:49:40.066757 139664575932160 logging_writer.py:48] [275600] global_step=275600, grad_norm=0.25091731548309326, loss=1.4618722200393677
I0314 02:50:15.700557 139664567539456 logging_writer.py:48] [275700] global_step=275700, grad_norm=0.23435235023498535, loss=1.4491982460021973
I0314 02:50:51.324155 139664575932160 logging_writer.py:48] [275800] global_step=275800, grad_norm=0.23007072508335114, loss=1.4433341026306152
I0314 02:51:26.936346 139664567539456 logging_writer.py:48] [275900] global_step=275900, grad_norm=0.23750609159469604, loss=1.467943787574768
I0314 02:52:02.542105 139664575932160 logging_writer.py:48] [276000] global_step=276000, grad_norm=0.24128903448581696, loss=1.4857107400894165
I0314 02:52:38.182898 139664567539456 logging_writer.py:48] [276100] global_step=276100, grad_norm=0.23476779460906982, loss=1.4297449588775635
I0314 02:53:13.795271 139664575932160 logging_writer.py:48] [276200] global_step=276200, grad_norm=0.23332107067108154, loss=1.476640224456787
I0314 02:53:47.311632 139834281293632 spec.py:321] Evaluating on the training split.
I0314 02:53:50.287516 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 02:57:17.498140 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 02:57:20.181141 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:00:34.823662 139834281293632 spec.py:349] Evaluating on the test split.
I0314 03:00:37.494843 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:03:47.268841 139834281293632 submission_runner.py:420] Time since start: 168795.82s, 	Step: 276296, 	{'train/accuracy': 0.6965546607971191, 'train/loss': 1.3616505861282349, 'train/bleu': 35.67617262040431, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 98325.51252913475, 'total_duration': 168795.82131123543, 'accumulated_submission_time': 98325.51252913475, 'accumulated_eval_time': 70456.73684310913, 'accumulated_logging_time': 5.134132146835327}
I0314 03:03:47.319680 139664567539456 logging_writer.py:48] [276296] accumulated_eval_time=70456.736843, accumulated_logging_time=5.134132, accumulated_submission_time=98325.512529, global_step=276296, preemption_count=0, score=98325.512529, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=168795.821311, train/accuracy=0.696555, train/bleu=35.676173, train/loss=1.361651, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 03:03:49.120065 139664575932160 logging_writer.py:48] [276300] global_step=276300, grad_norm=0.23568308353424072, loss=1.41897714138031
I0314 03:04:24.647926 139664567539456 logging_writer.py:48] [276400] global_step=276400, grad_norm=0.2389998883008957, loss=1.505013346672058
I0314 03:05:00.216294 139664575932160 logging_writer.py:48] [276500] global_step=276500, grad_norm=0.22392423450946808, loss=1.3692630529403687
I0314 03:05:35.850656 139664567539456 logging_writer.py:48] [276600] global_step=276600, grad_norm=0.2318887561559677, loss=1.3925141096115112
I0314 03:06:11.469101 139664575932160 logging_writer.py:48] [276700] global_step=276700, grad_norm=0.23439382016658783, loss=1.4929834604263306
I0314 03:06:47.116235 139664567539456 logging_writer.py:48] [276800] global_step=276800, grad_norm=0.23531638085842133, loss=1.4179850816726685
I0314 03:07:22.720222 139664575932160 logging_writer.py:48] [276900] global_step=276900, grad_norm=0.2285531610250473, loss=1.316360354423523
I0314 03:07:58.330414 139664567539456 logging_writer.py:48] [277000] global_step=277000, grad_norm=0.24164560437202454, loss=1.5163319110870361
I0314 03:08:33.948217 139664575932160 logging_writer.py:48] [277100] global_step=277100, grad_norm=0.22188925743103027, loss=1.3804463148117065
I0314 03:09:09.572514 139664567539456 logging_writer.py:48] [277200] global_step=277200, grad_norm=0.23532481491565704, loss=1.4430540800094604
I0314 03:09:45.203451 139664575932160 logging_writer.py:48] [277300] global_step=277300, grad_norm=0.23378245532512665, loss=1.4465539455413818
I0314 03:10:20.879756 139664567539456 logging_writer.py:48] [277400] global_step=277400, grad_norm=0.2407573163509369, loss=1.4752508401870728
I0314 03:10:56.556513 139664575932160 logging_writer.py:48] [277500] global_step=277500, grad_norm=0.22412826120853424, loss=1.4086052179336548
I0314 03:11:32.206480 139664567539456 logging_writer.py:48] [277600] global_step=277600, grad_norm=0.24204814434051514, loss=1.4923219680786133
I0314 03:12:07.885844 139664575932160 logging_writer.py:48] [277700] global_step=277700, grad_norm=0.23307067155838013, loss=1.441319465637207
I0314 03:12:43.477060 139664567539456 logging_writer.py:48] [277800] global_step=277800, grad_norm=0.2354070097208023, loss=1.4257946014404297
I0314 03:13:19.089455 139664575932160 logging_writer.py:48] [277900] global_step=277900, grad_norm=0.2277170866727829, loss=1.372089147567749
I0314 03:13:54.717409 139664567539456 logging_writer.py:48] [278000] global_step=278000, grad_norm=0.2373274862766266, loss=1.4384032487869263
I0314 03:14:30.341811 139664575932160 logging_writer.py:48] [278100] global_step=278100, grad_norm=0.23258362710475922, loss=1.4295296669006348
I0314 03:15:05.959179 139664567539456 logging_writer.py:48] [278200] global_step=278200, grad_norm=0.2249620407819748, loss=1.3862709999084473
I0314 03:15:41.569378 139664575932160 logging_writer.py:48] [278300] global_step=278300, grad_norm=0.23177184164524078, loss=1.481385350227356
I0314 03:16:17.153977 139664567539456 logging_writer.py:48] [278400] global_step=278400, grad_norm=0.2376832365989685, loss=1.4680250883102417
I0314 03:16:52.729120 139664575932160 logging_writer.py:48] [278500] global_step=278500, grad_norm=0.231613889336586, loss=1.4444643259048462
I0314 03:17:28.332025 139664567539456 logging_writer.py:48] [278600] global_step=278600, grad_norm=0.22501780092716217, loss=1.3836416006088257
I0314 03:17:47.621260 139834281293632 spec.py:321] Evaluating on the training split.
I0314 03:17:50.593003 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:21:18.987303 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 03:21:21.651228 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:24:35.640185 139834281293632 spec.py:349] Evaluating on the test split.
I0314 03:24:38.316343 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:27:48.842789 139834281293632 submission_runner.py:420] Time since start: 170237.40s, 	Step: 278656, 	{'train/accuracy': 0.6967638731002808, 'train/loss': 1.3642253875732422, 'train/bleu': 35.39564349576458, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 99165.73145341873, 'total_duration': 170237.39524626732, 'accumulated_submission_time': 99165.73145341873, 'accumulated_eval_time': 71057.9582953453, 'accumulated_logging_time': 5.194442510604858}
I0314 03:27:48.903619 139664575932160 logging_writer.py:48] [278656] accumulated_eval_time=71057.958295, accumulated_logging_time=5.194443, accumulated_submission_time=99165.731453, global_step=278656, preemption_count=0, score=99165.731453, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=170237.395246, train/accuracy=0.696764, train/bleu=35.395643, train/loss=1.364225, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 03:28:04.900056 139664567539456 logging_writer.py:48] [278700] global_step=278700, grad_norm=0.23692703247070312, loss=1.441070318222046
I0314 03:28:40.445557 139664575932160 logging_writer.py:48] [278800] global_step=278800, grad_norm=0.24219371378421783, loss=1.4457824230194092
I0314 03:29:16.053211 139664567539456 logging_writer.py:48] [278900] global_step=278900, grad_norm=0.24193838238716125, loss=1.4940906763076782
I0314 03:29:51.639941 139664575932160 logging_writer.py:48] [279000] global_step=279000, grad_norm=0.2827121913433075, loss=1.5082473754882812
I0314 03:30:27.273772 139664567539456 logging_writer.py:48] [279100] global_step=279100, grad_norm=0.234847292304039, loss=1.4079575538635254
I0314 03:31:02.846371 139664575932160 logging_writer.py:48] [279200] global_step=279200, grad_norm=0.2262929081916809, loss=1.4563956260681152
I0314 03:31:38.457884 139664567539456 logging_writer.py:48] [279300] global_step=279300, grad_norm=0.23962751030921936, loss=1.4226045608520508
I0314 03:32:14.075844 139664575932160 logging_writer.py:48] [279400] global_step=279400, grad_norm=0.23552405834197998, loss=1.433289885520935
I0314 03:32:49.705180 139664567539456 logging_writer.py:48] [279500] global_step=279500, grad_norm=0.23759101331233978, loss=1.4599008560180664
I0314 03:33:25.361720 139664575932160 logging_writer.py:48] [279600] global_step=279600, grad_norm=0.2273620367050171, loss=1.438671588897705
I0314 03:34:00.978810 139664567539456 logging_writer.py:48] [279700] global_step=279700, grad_norm=0.2317049503326416, loss=1.4580049514770508
I0314 03:34:36.587680 139664575932160 logging_writer.py:48] [279800] global_step=279800, grad_norm=0.23904839158058167, loss=1.5559165477752686
I0314 03:35:12.181318 139664567539456 logging_writer.py:48] [279900] global_step=279900, grad_norm=0.23940420150756836, loss=1.410014033317566
I0314 03:35:47.802392 139664575932160 logging_writer.py:48] [280000] global_step=280000, grad_norm=0.2275826632976532, loss=1.4357261657714844
I0314 03:36:23.422903 139664567539456 logging_writer.py:48] [280100] global_step=280100, grad_norm=0.2416456788778305, loss=1.4628602266311646
I0314 03:36:59.035298 139664575932160 logging_writer.py:48] [280200] global_step=280200, grad_norm=0.2352987825870514, loss=1.4533655643463135
I0314 03:37:34.664895 139664567539456 logging_writer.py:48] [280300] global_step=280300, grad_norm=0.24208591878414154, loss=1.4070371389389038
I0314 03:38:10.281212 139664575932160 logging_writer.py:48] [280400] global_step=280400, grad_norm=0.23830977082252502, loss=1.4068089723587036
I0314 03:38:45.903650 139664567539456 logging_writer.py:48] [280500] global_step=280500, grad_norm=0.2314651757478714, loss=1.4231441020965576
I0314 03:39:21.535487 139664575932160 logging_writer.py:48] [280600] global_step=280600, grad_norm=0.2375396192073822, loss=1.4699654579162598
I0314 03:39:57.229960 139664567539456 logging_writer.py:48] [280700] global_step=280700, grad_norm=0.2343403548002243, loss=1.4330039024353027
I0314 03:40:32.851869 139664575932160 logging_writer.py:48] [280800] global_step=280800, grad_norm=0.22694683074951172, loss=1.435404896736145
I0314 03:41:08.471151 139664567539456 logging_writer.py:48] [280900] global_step=280900, grad_norm=0.2329811453819275, loss=1.5149413347244263
I0314 03:41:44.073435 139664575932160 logging_writer.py:48] [281000] global_step=281000, grad_norm=0.2406766414642334, loss=1.420223355293274
I0314 03:41:49.148037 139834281293632 spec.py:321] Evaluating on the training split.
I0314 03:41:52.138183 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:45:12.259746 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 03:45:14.947115 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:48:29.080036 139834281293632 spec.py:349] Evaluating on the test split.
I0314 03:48:31.763600 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 03:51:43.264477 139834281293632 submission_runner.py:420] Time since start: 171671.82s, 	Step: 281016, 	{'train/accuracy': 0.6934937834739685, 'train/loss': 1.3861702680587769, 'train/bleu': 35.466408250843514, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 100005.89224982262, 'total_duration': 171671.8169312477, 'accumulated_submission_time': 100005.89224982262, 'accumulated_eval_time': 71652.07466721535, 'accumulated_logging_time': 5.26610803604126}
I0314 03:51:43.325089 139664567539456 logging_writer.py:48] [281016] accumulated_eval_time=71652.074667, accumulated_logging_time=5.266108, accumulated_submission_time=100005.892250, global_step=281016, preemption_count=0, score=100005.892250, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=171671.816931, train/accuracy=0.693494, train/bleu=35.466408, train/loss=1.386170, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 03:52:13.539987 139664575932160 logging_writer.py:48] [281100] global_step=281100, grad_norm=0.2370060384273529, loss=1.4404544830322266
I0314 03:52:49.052660 139664567539456 logging_writer.py:48] [281200] global_step=281200, grad_norm=0.2177899330854416, loss=1.3617758750915527
I0314 03:53:24.667820 139664575932160 logging_writer.py:48] [281300] global_step=281300, grad_norm=0.2339571863412857, loss=1.4328960180282593
I0314 03:54:00.295299 139664567539456 logging_writer.py:48] [281400] global_step=281400, grad_norm=0.23140276968479156, loss=1.442064642906189
I0314 03:54:35.893764 139664575932160 logging_writer.py:48] [281500] global_step=281500, grad_norm=0.23073284327983856, loss=1.4301997423171997
I0314 03:55:11.506370 139664567539456 logging_writer.py:48] [281600] global_step=281600, grad_norm=0.23024442791938782, loss=1.4151793718338013
I0314 03:55:47.151800 139664575932160 logging_writer.py:48] [281700] global_step=281700, grad_norm=0.2333584874868393, loss=1.4614406824111938
I0314 03:56:22.795736 139664567539456 logging_writer.py:48] [281800] global_step=281800, grad_norm=0.22457937896251678, loss=1.4279152154922485
I0314 03:56:58.426402 139664575932160 logging_writer.py:48] [281900] global_step=281900, grad_norm=0.2362869828939438, loss=1.434133529663086
I0314 03:57:34.034272 139664567539456 logging_writer.py:48] [282000] global_step=282000, grad_norm=0.2394239902496338, loss=1.431219220161438
I0314 03:58:09.672633 139664575932160 logging_writer.py:48] [282100] global_step=282100, grad_norm=0.23608548939228058, loss=1.4240739345550537
I0314 03:58:45.264817 139664567539456 logging_writer.py:48] [282200] global_step=282200, grad_norm=0.24007850885391235, loss=1.4996260404586792
I0314 03:59:20.897849 139664575932160 logging_writer.py:48] [282300] global_step=282300, grad_norm=0.23009908199310303, loss=1.4526108503341675
I0314 03:59:56.476567 139664567539456 logging_writer.py:48] [282400] global_step=282400, grad_norm=0.2404782623052597, loss=1.4830069541931152
I0314 04:00:32.102776 139664575932160 logging_writer.py:48] [282500] global_step=282500, grad_norm=0.24363411962985992, loss=1.4354794025421143
I0314 04:01:07.721662 139664567539456 logging_writer.py:48] [282600] global_step=282600, grad_norm=0.24666795134544373, loss=1.3982930183410645
I0314 04:01:43.324580 139664575932160 logging_writer.py:48] [282700] global_step=282700, grad_norm=0.2409476786851883, loss=1.4964244365692139
I0314 04:02:18.929836 139664567539456 logging_writer.py:48] [282800] global_step=282800, grad_norm=0.2355382740497589, loss=1.4263956546783447
I0314 04:02:54.565939 139664575932160 logging_writer.py:48] [282900] global_step=282900, grad_norm=0.2295272797346115, loss=1.3666738271713257
I0314 04:03:30.181996 139664567539456 logging_writer.py:48] [283000] global_step=283000, grad_norm=0.23041988909244537, loss=1.4575854539871216
I0314 04:04:05.773885 139664575932160 logging_writer.py:48] [283100] global_step=283100, grad_norm=0.22778771817684174, loss=1.4313735961914062
I0314 04:04:41.397477 139664567539456 logging_writer.py:48] [283200] global_step=283200, grad_norm=0.23534442484378815, loss=1.4114046096801758
I0314 04:05:17.010649 139664575932160 logging_writer.py:48] [283300] global_step=283300, grad_norm=0.23294539749622345, loss=1.4435248374938965
I0314 04:05:43.428221 139834281293632 spec.py:321] Evaluating on the training split.
I0314 04:05:46.413101 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:09:05.270373 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 04:09:07.942001 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:12:21.795157 139834281293632 spec.py:349] Evaluating on the test split.
I0314 04:12:24.457932 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:15:33.884268 139834281293632 submission_runner.py:420] Time since start: 173102.44s, 	Step: 283376, 	{'train/accuracy': 0.6972944736480713, 'train/loss': 1.3638888597488403, 'train/bleu': 35.66149794669415, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 100845.91227436066, 'total_duration': 173102.43675160408, 'accumulated_submission_time': 100845.91227436066, 'accumulated_eval_time': 72242.53067946434, 'accumulated_logging_time': 5.33665919303894}
I0314 04:15:33.936244 139664567539456 logging_writer.py:48] [283376] accumulated_eval_time=72242.530679, accumulated_logging_time=5.336659, accumulated_submission_time=100845.912274, global_step=283376, preemption_count=0, score=100845.912274, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=173102.436752, train/accuracy=0.697294, train/bleu=35.661498, train/loss=1.363889, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 04:15:42.828366 139664575932160 logging_writer.py:48] [283400] global_step=283400, grad_norm=0.23608578741550446, loss=1.511220097541809
I0314 04:16:18.353003 139664567539456 logging_writer.py:48] [283500] global_step=283500, grad_norm=0.23124612867832184, loss=1.452294111251831
I0314 04:16:53.931538 139664575932160 logging_writer.py:48] [283600] global_step=283600, grad_norm=0.23423990607261658, loss=1.4309693574905396
I0314 04:17:29.507152 139664567539456 logging_writer.py:48] [283700] global_step=283700, grad_norm=0.24380122125148773, loss=1.4503679275512695
I0314 04:18:05.081267 139664575932160 logging_writer.py:48] [283800] global_step=283800, grad_norm=0.2324773073196411, loss=1.446838140487671
I0314 04:18:40.660974 139664567539456 logging_writer.py:48] [283900] global_step=283900, grad_norm=0.23419173061847687, loss=1.438051462173462
I0314 04:19:16.254437 139664575932160 logging_writer.py:48] [284000] global_step=284000, grad_norm=0.23245327174663544, loss=1.489389419555664
I0314 04:19:51.840308 139664567539456 logging_writer.py:48] [284100] global_step=284100, grad_norm=0.24029095470905304, loss=1.4650977849960327
I0314 04:20:27.428172 139664575932160 logging_writer.py:48] [284200] global_step=284200, grad_norm=0.2425871193408966, loss=1.435835838317871
I0314 04:21:03.020800 139664567539456 logging_writer.py:48] [284300] global_step=284300, grad_norm=0.23076732456684113, loss=1.417131781578064
I0314 04:21:38.645724 139664575932160 logging_writer.py:48] [284400] global_step=284400, grad_norm=0.22742891311645508, loss=1.447413682937622
I0314 04:22:14.229577 139664567539456 logging_writer.py:48] [284500] global_step=284500, grad_norm=0.2301674634218216, loss=1.4049584865570068
I0314 04:22:49.819595 139664575932160 logging_writer.py:48] [284600] global_step=284600, grad_norm=0.23942068219184875, loss=1.4919041395187378
I0314 04:23:25.423493 139664567539456 logging_writer.py:48] [284700] global_step=284700, grad_norm=0.24247804284095764, loss=1.4239009618759155
I0314 04:24:01.017467 139664575932160 logging_writer.py:48] [284800] global_step=284800, grad_norm=0.24017615616321564, loss=1.4689112901687622
I0314 04:24:36.582702 139664567539456 logging_writer.py:48] [284900] global_step=284900, grad_norm=0.2262944132089615, loss=1.4081470966339111
I0314 04:25:12.193998 139664575932160 logging_writer.py:48] [285000] global_step=285000, grad_norm=0.24101842939853668, loss=1.40155029296875
I0314 04:25:47.804314 139664567539456 logging_writer.py:48] [285100] global_step=285100, grad_norm=0.23804885149002075, loss=1.4821497201919556
I0314 04:26:23.382060 139664575932160 logging_writer.py:48] [285200] global_step=285200, grad_norm=0.2413504272699356, loss=1.3992642164230347
I0314 04:26:59.012465 139664567539456 logging_writer.py:48] [285300] global_step=285300, grad_norm=0.22915057837963104, loss=1.3681720495224
I0314 04:27:34.642481 139664575932160 logging_writer.py:48] [285400] global_step=285400, grad_norm=0.23848536610603333, loss=1.3548866510391235
I0314 04:28:10.226489 139664567539456 logging_writer.py:48] [285500] global_step=285500, grad_norm=0.22815945744514465, loss=1.4056265354156494
I0314 04:28:45.820940 139664575932160 logging_writer.py:48] [285600] global_step=285600, grad_norm=0.23892514407634735, loss=1.5134634971618652
I0314 04:29:21.448800 139664567539456 logging_writer.py:48] [285700] global_step=285700, grad_norm=0.22728674113750458, loss=1.3780893087387085
I0314 04:29:33.982197 139834281293632 spec.py:321] Evaluating on the training split.
I0314 04:29:36.949656 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:33:04.060016 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 04:33:06.720592 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:36:20.773552 139834281293632 spec.py:349] Evaluating on the test split.
I0314 04:36:23.446645 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:39:33.116675 139834281293632 submission_runner.py:420] Time since start: 174541.67s, 	Step: 285737, 	{'train/accuracy': 0.6911959052085876, 'train/loss': 1.3953195810317993, 'train/bleu': 36.23396595613439, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 101685.87846279144, 'total_duration': 174541.66915917397, 'accumulated_submission_time': 101685.87846279144, 'accumulated_eval_time': 72841.66510772705, 'accumulated_logging_time': 5.3976500034332275}
I0314 04:39:33.170056 139664575932160 logging_writer.py:48] [285737] accumulated_eval_time=72841.665108, accumulated_logging_time=5.397650, accumulated_submission_time=101685.878463, global_step=285737, preemption_count=0, score=101685.878463, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=174541.669159, train/accuracy=0.691196, train/bleu=36.233966, train/loss=1.395320, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 04:39:55.890000 139664567539456 logging_writer.py:48] [285800] global_step=285800, grad_norm=0.2400968223810196, loss=1.4417434930801392
I0314 04:40:31.413383 139664575932160 logging_writer.py:48] [285900] global_step=285900, grad_norm=0.23789526522159576, loss=1.4523648023605347
I0314 04:41:06.997897 139664567539456 logging_writer.py:48] [286000] global_step=286000, grad_norm=0.23375637829303741, loss=1.3728725910186768
I0314 04:41:42.564973 139664575932160 logging_writer.py:48] [286100] global_step=286100, grad_norm=0.22570697963237762, loss=1.449288249015808
I0314 04:42:18.143167 139664567539456 logging_writer.py:48] [286200] global_step=286200, grad_norm=0.2367328405380249, loss=1.4469943046569824
I0314 04:42:53.703573 139664575932160 logging_writer.py:48] [286300] global_step=286300, grad_norm=0.23483702540397644, loss=1.4661370515823364
I0314 04:43:29.275206 139664567539456 logging_writer.py:48] [286400] global_step=286400, grad_norm=0.2348480224609375, loss=1.3796055316925049
I0314 04:44:04.877383 139664575932160 logging_writer.py:48] [286500] global_step=286500, grad_norm=0.2330862283706665, loss=1.3940695524215698
I0314 04:44:40.477780 139664567539456 logging_writer.py:48] [286600] global_step=286600, grad_norm=0.2420772761106491, loss=1.4558395147323608
I0314 04:45:16.044412 139664575932160 logging_writer.py:48] [286700] global_step=286700, grad_norm=0.2476806789636612, loss=1.4997717142105103
I0314 04:45:51.626143 139664567539456 logging_writer.py:48] [286800] global_step=286800, grad_norm=0.23761385679244995, loss=1.473500370979309
I0314 04:46:27.230890 139664575932160 logging_writer.py:48] [286900] global_step=286900, grad_norm=0.2356061488389969, loss=1.4033563137054443
I0314 04:47:02.809334 139664567539456 logging_writer.py:48] [287000] global_step=287000, grad_norm=0.22767169773578644, loss=1.5042345523834229
I0314 04:47:38.421008 139664575932160 logging_writer.py:48] [287100] global_step=287100, grad_norm=0.24171216785907745, loss=1.4783254861831665
I0314 04:48:13.983688 139664567539456 logging_writer.py:48] [287200] global_step=287200, grad_norm=0.2369379699230194, loss=1.3950611352920532
I0314 04:48:49.547464 139664575932160 logging_writer.py:48] [287300] global_step=287300, grad_norm=0.23823468387126923, loss=1.4973299503326416
I0314 04:49:25.169566 139664567539456 logging_writer.py:48] [287400] global_step=287400, grad_norm=0.23352612555027008, loss=1.4757856130599976
I0314 04:50:00.794631 139664575932160 logging_writer.py:48] [287500] global_step=287500, grad_norm=0.23717689514160156, loss=1.486489176750183
I0314 04:50:36.409685 139664567539456 logging_writer.py:48] [287600] global_step=287600, grad_norm=0.24001744389533997, loss=1.520198106765747
I0314 04:51:12.013983 139664575932160 logging_writer.py:48] [287700] global_step=287700, grad_norm=0.2425285279750824, loss=1.4708701372146606
I0314 04:51:47.580811 139664567539456 logging_writer.py:48] [287800] global_step=287800, grad_norm=0.22441616654396057, loss=1.4357824325561523
I0314 04:52:23.171676 139664575932160 logging_writer.py:48] [287900] global_step=287900, grad_norm=0.2309778779745102, loss=1.351788878440857
I0314 04:52:58.766165 139664567539456 logging_writer.py:48] [288000] global_step=288000, grad_norm=0.23051874339580536, loss=1.4439483880996704
I0314 04:53:33.360018 139834281293632 spec.py:321] Evaluating on the training split.
I0314 04:53:36.331118 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 04:57:05.504193 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 04:57:08.186620 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:00:23.963449 139834281293632 spec.py:349] Evaluating on the test split.
I0314 05:00:26.659414 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:03:36.225478 139834281293632 submission_runner.py:420] Time since start: 175984.78s, 	Step: 288099, 	{'train/accuracy': 0.6965842843055725, 'train/loss': 1.3646329641342163, 'train/bleu': 35.65843571065414, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 102525.98865199089, 'total_duration': 175984.77796077728, 'accumulated_submission_time': 102525.98865199089, 'accumulated_eval_time': 73444.53052544594, 'accumulated_logging_time': 5.460918426513672}
I0314 05:03:36.276751 139664575932160 logging_writer.py:48] [288099] accumulated_eval_time=73444.530525, accumulated_logging_time=5.460918, accumulated_submission_time=102525.988652, global_step=288099, preemption_count=0, score=102525.988652, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=175984.777961, train/accuracy=0.696584, train/bleu=35.658436, train/loss=1.364633, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 05:03:37.010879 139664567539456 logging_writer.py:48] [288100] global_step=288100, grad_norm=0.2285197526216507, loss=1.365407109260559
I0314 05:04:12.508442 139664575932160 logging_writer.py:48] [288200] global_step=288200, grad_norm=0.234583780169487, loss=1.4629490375518799
I0314 05:04:48.099571 139664567539456 logging_writer.py:48] [288300] global_step=288300, grad_norm=0.3282937705516815, loss=1.4805877208709717
I0314 05:05:23.724972 139664575932160 logging_writer.py:48] [288400] global_step=288400, grad_norm=0.23483042418956757, loss=1.4735794067382812
I0314 05:05:59.336790 139664567539456 logging_writer.py:48] [288500] global_step=288500, grad_norm=0.23744183778762817, loss=1.4242897033691406
I0314 05:06:34.973401 139664575932160 logging_writer.py:48] [288600] global_step=288600, grad_norm=0.2344408482313156, loss=1.449896216392517
I0314 05:07:10.592450 139664567539456 logging_writer.py:48] [288700] global_step=288700, grad_norm=0.24253049492835999, loss=1.4248000383377075
I0314 05:07:46.162580 139664575932160 logging_writer.py:48] [288800] global_step=288800, grad_norm=0.2398367077112198, loss=1.4846737384796143
I0314 05:08:21.806692 139664567539456 logging_writer.py:48] [288900] global_step=288900, grad_norm=0.23964910209178925, loss=1.4262272119522095
I0314 05:08:57.438373 139664575932160 logging_writer.py:48] [289000] global_step=289000, grad_norm=0.23942925035953522, loss=1.4187599420547485
I0314 05:09:33.045210 139664567539456 logging_writer.py:48] [289100] global_step=289100, grad_norm=0.23947902023792267, loss=1.3983572721481323
I0314 05:10:08.686545 139664575932160 logging_writer.py:48] [289200] global_step=289200, grad_norm=0.22973555326461792, loss=1.4727678298950195
I0314 05:10:44.268563 139664567539456 logging_writer.py:48] [289300] global_step=289300, grad_norm=0.2332904189825058, loss=1.4118586778640747
I0314 05:11:19.868743 139664575932160 logging_writer.py:48] [289400] global_step=289400, grad_norm=0.2392585277557373, loss=1.4563665390014648
I0314 05:11:55.463865 139664567539456 logging_writer.py:48] [289500] global_step=289500, grad_norm=0.2499997615814209, loss=1.4660532474517822
I0314 05:12:31.048789 139664575932160 logging_writer.py:48] [289600] global_step=289600, grad_norm=0.2319362759590149, loss=1.4637421369552612
I0314 05:13:06.659282 139664567539456 logging_writer.py:48] [289700] global_step=289700, grad_norm=0.23763450980186462, loss=1.4616855382919312
I0314 05:13:42.268519 139664575932160 logging_writer.py:48] [289800] global_step=289800, grad_norm=0.23675622045993805, loss=1.461169719696045
I0314 05:14:17.865107 139664567539456 logging_writer.py:48] [289900] global_step=289900, grad_norm=0.23512186110019684, loss=1.4574670791625977
I0314 05:14:53.475297 139664575932160 logging_writer.py:48] [290000] global_step=290000, grad_norm=0.2365940362215042, loss=1.3612561225891113
I0314 05:15:29.074348 139664567539456 logging_writer.py:48] [290100] global_step=290100, grad_norm=0.244608536362648, loss=1.428220510482788
I0314 05:16:04.694946 139664575932160 logging_writer.py:48] [290200] global_step=290200, grad_norm=0.23170709609985352, loss=1.4838677644729614
I0314 05:16:40.295531 139664567539456 logging_writer.py:48] [290300] global_step=290300, grad_norm=0.23613183200359344, loss=1.3482449054718018
I0314 05:17:15.888391 139664575932160 logging_writer.py:48] [290400] global_step=290400, grad_norm=0.2534872889518738, loss=1.4418749809265137
I0314 05:17:36.228693 139834281293632 spec.py:321] Evaluating on the training split.
I0314 05:17:39.190863 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:21:22.706355 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 05:21:25.383809 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:24:39.151122 139834281293632 spec.py:349] Evaluating on the test split.
I0314 05:24:41.818916 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:27:51.484985 139834281293632 submission_runner.py:420] Time since start: 177440.04s, 	Step: 290459, 	{'train/accuracy': 0.696201503276825, 'train/loss': 1.3672454357147217, 'train/bleu': 35.46740520559859, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 103365.85811543465, 'total_duration': 177440.03746771812, 'accumulated_submission_time': 103365.85811543465, 'accumulated_eval_time': 74059.78676986694, 'accumulated_logging_time': 5.522222518920898}
I0314 05:27:51.537100 139664567539456 logging_writer.py:48] [290459] accumulated_eval_time=74059.786770, accumulated_logging_time=5.522223, accumulated_submission_time=103365.858115, global_step=290459, preemption_count=0, score=103365.858115, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=177440.037468, train/accuracy=0.696202, train/bleu=35.467405, train/loss=1.367245, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 05:28:06.474534 139664575932160 logging_writer.py:48] [290500] global_step=290500, grad_norm=0.2266634851694107, loss=1.4138317108154297
I0314 05:28:41.987203 139664567539456 logging_writer.py:48] [290600] global_step=290600, grad_norm=0.24260897934436798, loss=1.414433240890503
I0314 05:29:17.577475 139664575932160 logging_writer.py:48] [290700] global_step=290700, grad_norm=0.23665539920330048, loss=1.5352816581726074
I0314 05:29:53.162539 139664567539456 logging_writer.py:48] [290800] global_step=290800, grad_norm=0.23892533779144287, loss=1.4127466678619385
I0314 05:30:28.767670 139664575932160 logging_writer.py:48] [290900] global_step=290900, grad_norm=0.23590584099292755, loss=1.399423599243164
I0314 05:31:04.318819 139664567539456 logging_writer.py:48] [291000] global_step=291000, grad_norm=0.2252725213766098, loss=1.433816909790039
I0314 05:31:39.936640 139664575932160 logging_writer.py:48] [291100] global_step=291100, grad_norm=0.2325609028339386, loss=1.4109125137329102
I0314 05:32:15.529590 139664567539456 logging_writer.py:48] [291200] global_step=291200, grad_norm=0.2280634045600891, loss=1.4663060903549194
I0314 05:32:51.183480 139664575932160 logging_writer.py:48] [291300] global_step=291300, grad_norm=0.23806136846542358, loss=1.4867368936538696
I0314 05:33:26.844734 139664567539456 logging_writer.py:48] [291400] global_step=291400, grad_norm=0.22489552199840546, loss=1.3799704313278198
I0314 05:34:02.453313 139664575932160 logging_writer.py:48] [291500] global_step=291500, grad_norm=0.22714798152446747, loss=1.4448833465576172
I0314 05:34:38.039932 139664567539456 logging_writer.py:48] [291600] global_step=291600, grad_norm=0.23733069002628326, loss=1.4533687829971313
I0314 05:35:13.660718 139664575932160 logging_writer.py:48] [291700] global_step=291700, grad_norm=0.23691444098949432, loss=1.4971191883087158
I0314 05:35:49.237597 139664567539456 logging_writer.py:48] [291800] global_step=291800, grad_norm=0.2331162393093109, loss=1.3957414627075195
I0314 05:36:24.826522 139664575932160 logging_writer.py:48] [291900] global_step=291900, grad_norm=0.23301269114017487, loss=1.376627802848816
I0314 05:37:00.441126 139664567539456 logging_writer.py:48] [292000] global_step=292000, grad_norm=0.23609980940818787, loss=1.4500343799591064
I0314 05:37:36.015461 139664575932160 logging_writer.py:48] [292100] global_step=292100, grad_norm=0.2310391515493393, loss=1.463148832321167
I0314 05:38:11.601463 139664567539456 logging_writer.py:48] [292200] global_step=292200, grad_norm=0.23922796547412872, loss=1.407867193222046
I0314 05:38:47.187912 139664575932160 logging_writer.py:48] [292300] global_step=292300, grad_norm=0.22508499026298523, loss=1.4703693389892578
I0314 05:39:22.784591 139664567539456 logging_writer.py:48] [292400] global_step=292400, grad_norm=0.23423361778259277, loss=1.408139705657959
I0314 05:39:58.383193 139664575932160 logging_writer.py:48] [292500] global_step=292500, grad_norm=0.24188785254955292, loss=1.4651267528533936
I0314 05:40:33.994231 139664567539456 logging_writer.py:48] [292600] global_step=292600, grad_norm=0.23349863290786743, loss=1.4615578651428223
I0314 05:41:09.594092 139664575932160 logging_writer.py:48] [292700] global_step=292700, grad_norm=0.2470705658197403, loss=1.4578502178192139
I0314 05:41:45.192980 139664567539456 logging_writer.py:48] [292800] global_step=292800, grad_norm=0.2286214977502823, loss=1.4433765411376953
I0314 05:41:51.680615 139834281293632 spec.py:321] Evaluating on the training split.
I0314 05:41:54.657700 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:45:19.909029 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 05:45:22.586424 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:48:36.235928 139834281293632 spec.py:349] Evaluating on the test split.
I0314 05:48:38.921036 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 05:51:48.426175 139834281293632 submission_runner.py:420] Time since start: 178876.98s, 	Step: 292820, 	{'train/accuracy': 0.695793867111206, 'train/loss': 1.3687043190002441, 'train/bleu': 35.59809594260861, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 104205.91893053055, 'total_duration': 178876.97866034508, 'accumulated_submission_time': 104205.91893053055, 'accumulated_eval_time': 74656.53228163719, 'accumulated_logging_time': 5.5849597454071045}
I0314 05:51:48.478059 139664575932160 logging_writer.py:48] [292820] accumulated_eval_time=74656.532282, accumulated_logging_time=5.584960, accumulated_submission_time=104205.918931, global_step=292820, preemption_count=0, score=104205.918931, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=178876.978660, train/accuracy=0.695794, train/bleu=35.598096, train/loss=1.368704, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 05:52:17.282768 139664567539456 logging_writer.py:48] [292900] global_step=292900, grad_norm=0.2349116951227188, loss=1.4443190097808838
I0314 05:52:52.833340 139664575932160 logging_writer.py:48] [293000] global_step=293000, grad_norm=0.23225824534893036, loss=1.4349992275238037
I0314 05:53:28.451490 139664567539456 logging_writer.py:48] [293100] global_step=293100, grad_norm=0.22849008440971375, loss=1.4405949115753174
I0314 05:54:04.039002 139664575932160 logging_writer.py:48] [293200] global_step=293200, grad_norm=0.23674646019935608, loss=1.459038496017456
I0314 05:54:39.657934 139664567539456 logging_writer.py:48] [293300] global_step=293300, grad_norm=0.2397310435771942, loss=1.499698519706726
I0314 05:55:15.332319 139664575932160 logging_writer.py:48] [293400] global_step=293400, grad_norm=0.2292720377445221, loss=1.4725178480148315
I0314 05:55:50.934426 139664567539456 logging_writer.py:48] [293500] global_step=293500, grad_norm=0.2295386642217636, loss=1.4671317338943481
I0314 05:56:26.533984 139664575932160 logging_writer.py:48] [293600] global_step=293600, grad_norm=0.22485825419425964, loss=1.4347134828567505
I0314 05:57:02.112521 139664567539456 logging_writer.py:48] [293700] global_step=293700, grad_norm=0.2277720421552658, loss=1.4146806001663208
I0314 05:57:37.704855 139664575932160 logging_writer.py:48] [293800] global_step=293800, grad_norm=0.223916694521904, loss=1.329370141029358
I0314 05:58:13.327820 139664567539456 logging_writer.py:48] [293900] global_step=293900, grad_norm=0.24278207123279572, loss=1.409192442893982
I0314 05:58:48.944633 139664575932160 logging_writer.py:48] [294000] global_step=294000, grad_norm=0.22692514955997467, loss=1.4730005264282227
I0314 05:59:24.567736 139664567539456 logging_writer.py:48] [294100] global_step=294100, grad_norm=0.22611360251903534, loss=1.483376145362854
I0314 06:00:00.184636 139664575932160 logging_writer.py:48] [294200] global_step=294200, grad_norm=0.2315893918275833, loss=1.423650860786438
I0314 06:00:35.864849 139664567539456 logging_writer.py:48] [294300] global_step=294300, grad_norm=0.22852954268455505, loss=1.379393458366394
I0314 06:01:11.467627 139664575932160 logging_writer.py:48] [294400] global_step=294400, grad_norm=0.2325328290462494, loss=1.4327672719955444
I0314 06:01:47.071385 139664567539456 logging_writer.py:48] [294500] global_step=294500, grad_norm=0.23972468078136444, loss=1.4500895738601685
I0314 06:02:22.692665 139664575932160 logging_writer.py:48] [294600] global_step=294600, grad_norm=0.2384171038866043, loss=1.4117475748062134
I0314 06:02:58.305200 139664567539456 logging_writer.py:48] [294700] global_step=294700, grad_norm=0.23622271418571472, loss=1.477571964263916
I0314 06:03:33.924868 139664575932160 logging_writer.py:48] [294800] global_step=294800, grad_norm=0.23625776171684265, loss=1.434747338294983
I0314 06:04:09.549907 139664567539456 logging_writer.py:48] [294900] global_step=294900, grad_norm=0.2312750667333603, loss=1.4403352737426758
I0314 06:04:45.141194 139664575932160 logging_writer.py:48] [295000] global_step=295000, grad_norm=0.23072496056556702, loss=1.420369029045105
I0314 06:05:20.764464 139664567539456 logging_writer.py:48] [295100] global_step=295100, grad_norm=0.23259283602237701, loss=1.501680850982666
I0314 06:05:48.633118 139834281293632 spec.py:321] Evaluating on the training split.
I0314 06:05:51.612663 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:09:14.600993 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 06:09:17.282401 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:12:31.433723 139834281293632 spec.py:349] Evaluating on the test split.
I0314 06:12:34.098060 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:15:43.798547 139834281293632 submission_runner.py:420] Time since start: 180312.35s, 	Step: 295180, 	{'train/accuracy': 0.6961166858673096, 'train/loss': 1.3674596548080444, 'train/bleu': 35.220003376167796, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 105045.99169325829, 'total_duration': 180312.35103321075, 'accumulated_submission_time': 105045.99169325829, 'accumulated_eval_time': 75251.69767832756, 'accumulated_logging_time': 5.647981643676758}
I0314 06:15:43.850725 139664575932160 logging_writer.py:48] [295180] accumulated_eval_time=75251.697678, accumulated_logging_time=5.647982, accumulated_submission_time=105045.991693, global_step=295180, preemption_count=0, score=105045.991693, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=180312.351033, train/accuracy=0.696117, train/bleu=35.220003, train/loss=1.367460, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 06:15:51.323140 139664567539456 logging_writer.py:48] [295200] global_step=295200, grad_norm=0.2337689995765686, loss=1.4703867435455322
I0314 06:16:26.877108 139664575932160 logging_writer.py:48] [295300] global_step=295300, grad_norm=0.24359406530857086, loss=1.5229594707489014
I0314 06:17:02.452592 139664567539456 logging_writer.py:48] [295400] global_step=295400, grad_norm=0.24659277498722076, loss=1.428663730621338
I0314 06:17:38.019374 139664575932160 logging_writer.py:48] [295500] global_step=295500, grad_norm=0.22958028316497803, loss=1.4441282749176025
I0314 06:18:13.617199 139664567539456 logging_writer.py:48] [295600] global_step=295600, grad_norm=0.2353452742099762, loss=1.4296905994415283
I0314 06:18:49.215195 139664575932160 logging_writer.py:48] [295700] global_step=295700, grad_norm=0.23749585449695587, loss=1.4452660083770752
I0314 06:19:24.843927 139664567539456 logging_writer.py:48] [295800] global_step=295800, grad_norm=0.24081170558929443, loss=1.462399959564209
I0314 06:20:00.409499 139664575932160 logging_writer.py:48] [295900] global_step=295900, grad_norm=0.24260155856609344, loss=1.400702714920044
I0314 06:20:36.015421 139664567539456 logging_writer.py:48] [296000] global_step=296000, grad_norm=0.22773629426956177, loss=1.4758015871047974
I0314 06:21:11.624173 139664575932160 logging_writer.py:48] [296100] global_step=296100, grad_norm=0.2433275729417801, loss=1.5234041213989258
I0314 06:21:47.203336 139664567539456 logging_writer.py:48] [296200] global_step=296200, grad_norm=0.2385508418083191, loss=1.519287109375
I0314 06:22:22.839701 139664575932160 logging_writer.py:48] [296300] global_step=296300, grad_norm=0.22266358137130737, loss=1.4153614044189453
I0314 06:22:58.435417 139664567539456 logging_writer.py:48] [296400] global_step=296400, grad_norm=0.22556379437446594, loss=1.4305943250656128
I0314 06:23:34.047162 139664575932160 logging_writer.py:48] [296500] global_step=296500, grad_norm=0.23807170987129211, loss=1.39290452003479
I0314 06:24:09.667913 139664567539456 logging_writer.py:48] [296600] global_step=296600, grad_norm=0.22869837284088135, loss=1.3865330219268799
I0314 06:24:45.305004 139664575932160 logging_writer.py:48] [296700] global_step=296700, grad_norm=0.22461915016174316, loss=1.4456729888916016
I0314 06:25:20.935994 139664567539456 logging_writer.py:48] [296800] global_step=296800, grad_norm=0.23765438795089722, loss=1.5103486776351929
I0314 06:25:56.521412 139664575932160 logging_writer.py:48] [296900] global_step=296900, grad_norm=0.23265907168388367, loss=1.4546667337417603
I0314 06:26:32.104195 139664567539456 logging_writer.py:48] [297000] global_step=297000, grad_norm=0.23681622743606567, loss=1.449096918106079
I0314 06:27:07.706518 139664575932160 logging_writer.py:48] [297100] global_step=297100, grad_norm=0.23324134945869446, loss=1.4079769849777222
I0314 06:27:43.305644 139664567539456 logging_writer.py:48] [297200] global_step=297200, grad_norm=0.23831798136234283, loss=1.436525821685791
I0314 06:28:18.928217 139664575932160 logging_writer.py:48] [297300] global_step=297300, grad_norm=0.23379072546958923, loss=1.403541088104248
I0314 06:28:54.550098 139664567539456 logging_writer.py:48] [297400] global_step=297400, grad_norm=0.23589003086090088, loss=1.4640848636627197
I0314 06:29:30.198126 139664575932160 logging_writer.py:48] [297500] global_step=297500, grad_norm=0.234860360622406, loss=1.537394642829895
I0314 06:29:43.803832 139834281293632 spec.py:321] Evaluating on the training split.
I0314 06:29:46.773112 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:33:15.217617 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 06:33:17.894663 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:36:31.820003 139834281293632 spec.py:349] Evaluating on the test split.
I0314 06:36:34.492362 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:39:46.091892 139834281293632 submission_runner.py:420] Time since start: 181754.64s, 	Step: 297540, 	{'train/accuracy': 0.6968651413917542, 'train/loss': 1.370174765586853, 'train/bleu': 35.314651608415545, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 105885.86472034454, 'total_duration': 181754.64432263374, 'accumulated_submission_time': 105885.86472034454, 'accumulated_eval_time': 75853.98563599586, 'accumulated_logging_time': 5.709059000015259}
I0314 06:39:46.158285 139664567539456 logging_writer.py:48] [297540] accumulated_eval_time=75853.985636, accumulated_logging_time=5.709059, accumulated_submission_time=105885.864720, global_step=297540, preemption_count=0, score=105885.864720, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=181754.644323, train/accuracy=0.696865, train/bleu=35.314652, train/loss=1.370175, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 06:40:07.828949 139664575932160 logging_writer.py:48] [297600] global_step=297600, grad_norm=0.24314020574092865, loss=1.462459683418274
I0314 06:40:43.404081 139664567539456 logging_writer.py:48] [297700] global_step=297700, grad_norm=0.2355513870716095, loss=1.4792691469192505
I0314 06:41:18.998379 139664575932160 logging_writer.py:48] [297800] global_step=297800, grad_norm=0.2326785922050476, loss=1.4102905988693237
I0314 06:41:54.607982 139664567539456 logging_writer.py:48] [297900] global_step=297900, grad_norm=0.2311428040266037, loss=1.4382598400115967
I0314 06:42:30.210896 139664575932160 logging_writer.py:48] [298000] global_step=298000, grad_norm=0.23870523273944855, loss=1.4061065912246704
I0314 06:43:05.816110 139664567539456 logging_writer.py:48] [298100] global_step=298100, grad_norm=0.22567252814769745, loss=1.410988450050354
I0314 06:43:41.402088 139664575932160 logging_writer.py:48] [298200] global_step=298200, grad_norm=0.23722410202026367, loss=1.458718180656433
I0314 06:44:16.994201 139664567539456 logging_writer.py:48] [298300] global_step=298300, grad_norm=0.229880228638649, loss=1.4202983379364014
I0314 06:44:52.589072 139664575932160 logging_writer.py:48] [298400] global_step=298400, grad_norm=0.23556634783744812, loss=1.3846662044525146
I0314 06:45:28.206231 139664567539456 logging_writer.py:48] [298500] global_step=298500, grad_norm=0.23217713832855225, loss=1.4197063446044922
I0314 06:46:03.861461 139664575932160 logging_writer.py:48] [298600] global_step=298600, grad_norm=0.23340409994125366, loss=1.4743480682373047
I0314 06:46:39.487944 139664567539456 logging_writer.py:48] [298700] global_step=298700, grad_norm=0.22500142455101013, loss=1.474508285522461
I0314 06:47:15.076885 139664575932160 logging_writer.py:48] [298800] global_step=298800, grad_norm=0.2350182682275772, loss=1.4355525970458984
I0314 06:47:50.646440 139664567539456 logging_writer.py:48] [298900] global_step=298900, grad_norm=0.24474160373210907, loss=1.3760839700698853
I0314 06:48:26.237559 139664575932160 logging_writer.py:48] [299000] global_step=299000, grad_norm=0.22928115725517273, loss=1.4528088569641113
I0314 06:49:01.855829 139664567539456 logging_writer.py:48] [299100] global_step=299100, grad_norm=0.23261484503746033, loss=1.4481875896453857
I0314 06:49:37.495971 139664575932160 logging_writer.py:48] [299200] global_step=299200, grad_norm=0.24057410657405853, loss=1.5073347091674805
I0314 06:50:13.084395 139664567539456 logging_writer.py:48] [299300] global_step=299300, grad_norm=0.23893268406391144, loss=1.484727382659912
I0314 06:50:48.734323 139664575932160 logging_writer.py:48] [299400] global_step=299400, grad_norm=0.2304052859544754, loss=1.4606162309646606
I0314 06:51:24.338576 139664567539456 logging_writer.py:48] [299500] global_step=299500, grad_norm=0.24108411371707916, loss=1.4698400497436523
I0314 06:51:59.944929 139664575932160 logging_writer.py:48] [299600] global_step=299600, grad_norm=0.23529615998268127, loss=1.5244420766830444
I0314 06:52:35.545694 139664567539456 logging_writer.py:48] [299700] global_step=299700, grad_norm=0.2356712520122528, loss=1.484490156173706
I0314 06:53:11.152754 139664575932160 logging_writer.py:48] [299800] global_step=299800, grad_norm=0.23945246636867523, loss=1.422803282737732
I0314 06:53:46.093580 139834281293632 spec.py:321] Evaluating on the training split.
I0314 06:53:49.060602 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 06:57:19.780638 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 06:57:22.451029 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:00:36.295451 139834281293632 spec.py:349] Evaluating on the test split.
I0314 07:00:38.971754 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:03:48.674420 139834281293632 submission_runner.py:420] Time since start: 183197.23s, 	Step: 299900, 	{'train/accuracy': 0.6927401423454285, 'train/loss': 1.3830639123916626, 'train/bleu': 35.545994958747805, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 106725.71911907196, 'total_duration': 183197.22690081596, 'accumulated_submission_time': 106725.71911907196, 'accumulated_eval_time': 76456.5664255619, 'accumulated_logging_time': 5.785603046417236}
I0314 07:03:48.729532 139664567539456 logging_writer.py:48] [299900] accumulated_eval_time=76456.566426, accumulated_logging_time=5.785603, accumulated_submission_time=106725.719119, global_step=299900, preemption_count=0, score=106725.719119, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=183197.226901, train/accuracy=0.692740, train/bleu=35.545995, train/loss=1.383064, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 07:03:49.107160 139664575932160 logging_writer.py:48] [299900] global_step=299900, grad_norm=0.22417446970939636, loss=1.391697883605957
I0314 07:04:24.625608 139664567539456 logging_writer.py:48] [300000] global_step=300000, grad_norm=0.23036609590053558, loss=1.4427096843719482
I0314 07:05:00.200663 139664575932160 logging_writer.py:48] [300100] global_step=300100, grad_norm=0.24060291051864624, loss=1.3995178937911987
I0314 07:05:35.808909 139664567539456 logging_writer.py:48] [300200] global_step=300200, grad_norm=0.22690680623054504, loss=1.4032706022262573
I0314 07:06:11.389235 139664575932160 logging_writer.py:48] [300300] global_step=300300, grad_norm=0.2475866973400116, loss=1.5177866220474243
I0314 07:06:46.976690 139664567539456 logging_writer.py:48] [300400] global_step=300400, grad_norm=0.21881210803985596, loss=1.407657265663147
I0314 07:07:22.565160 139664575932160 logging_writer.py:48] [300500] global_step=300500, grad_norm=0.23532472550868988, loss=1.4293774366378784
I0314 07:07:58.171168 139664567539456 logging_writer.py:48] [300600] global_step=300600, grad_norm=0.22704681754112244, loss=1.363768458366394
I0314 07:08:33.792367 139664575932160 logging_writer.py:48] [300700] global_step=300700, grad_norm=0.22974608838558197, loss=1.4768790006637573
I0314 07:09:09.387430 139664567539456 logging_writer.py:48] [300800] global_step=300800, grad_norm=0.22744379937648773, loss=1.4231775999069214
I0314 07:09:45.005437 139664575932160 logging_writer.py:48] [300900] global_step=300900, grad_norm=0.22980129718780518, loss=1.4688323736190796
I0314 07:10:20.600033 139664567539456 logging_writer.py:48] [301000] global_step=301000, grad_norm=0.24829745292663574, loss=1.5211608409881592
I0314 07:10:56.205114 139664575932160 logging_writer.py:48] [301100] global_step=301100, grad_norm=0.2291582226753235, loss=1.4546475410461426
I0314 07:11:31.823984 139664567539456 logging_writer.py:48] [301200] global_step=301200, grad_norm=0.2324414849281311, loss=1.4712508916854858
I0314 07:12:07.427303 139664575932160 logging_writer.py:48] [301300] global_step=301300, grad_norm=0.22978629171848297, loss=1.479230523109436
I0314 07:12:43.075410 139664567539456 logging_writer.py:48] [301400] global_step=301400, grad_norm=0.23389460146427155, loss=1.444786787033081
I0314 07:13:18.708104 139664575932160 logging_writer.py:48] [301500] global_step=301500, grad_norm=0.23974022269248962, loss=1.4599274396896362
I0314 07:13:54.369677 139664567539456 logging_writer.py:48] [301600] global_step=301600, grad_norm=0.2347019910812378, loss=1.43453848361969
I0314 07:14:29.982506 139664575932160 logging_writer.py:48] [301700] global_step=301700, grad_norm=0.2311094105243683, loss=1.462679386138916
I0314 07:15:05.571666 139664567539456 logging_writer.py:48] [301800] global_step=301800, grad_norm=0.2445545792579651, loss=1.507690668106079
I0314 07:15:41.178927 139664575932160 logging_writer.py:48] [301900] global_step=301900, grad_norm=0.24571017920970917, loss=1.427064061164856
I0314 07:16:16.765415 139664567539456 logging_writer.py:48] [302000] global_step=302000, grad_norm=0.22647202014923096, loss=1.350279450416565
I0314 07:16:52.333993 139664575932160 logging_writer.py:48] [302100] global_step=302100, grad_norm=0.23571842908859253, loss=1.4805908203125
I0314 07:17:27.961048 139664567539456 logging_writer.py:48] [302200] global_step=302200, grad_norm=0.23329059779644012, loss=1.4825756549835205
I0314 07:17:48.712579 139834281293632 spec.py:321] Evaluating on the training split.
I0314 07:17:51.699663 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:21:16.969781 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 07:21:19.643150 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:24:33.897144 139834281293632 spec.py:349] Evaluating on the test split.
I0314 07:24:36.563137 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:27:46.345445 139834281293632 submission_runner.py:420] Time since start: 184634.90s, 	Step: 302260, 	{'train/accuracy': 0.6952672600746155, 'train/loss': 1.3717122077941895, 'train/bleu': 35.55978230786836, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 107565.61857748032, 'total_duration': 184634.89792728424, 'accumulated_submission_time': 107565.61857748032, 'accumulated_eval_time': 77054.19925522804, 'accumulated_logging_time': 5.850619554519653}
I0314 07:27:46.399669 139664575932160 logging_writer.py:48] [302260] accumulated_eval_time=77054.199255, accumulated_logging_time=5.850620, accumulated_submission_time=107565.618577, global_step=302260, preemption_count=0, score=107565.618577, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=184634.897927, train/accuracy=0.695267, train/bleu=35.559782, train/loss=1.371712, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 07:28:00.984287 139664567539456 logging_writer.py:48] [302300] global_step=302300, grad_norm=0.23017114400863647, loss=1.4847567081451416
I0314 07:28:36.527344 139664575932160 logging_writer.py:48] [302400] global_step=302400, grad_norm=0.2456071525812149, loss=1.4099926948547363
I0314 07:29:12.129086 139664567539456 logging_writer.py:48] [302500] global_step=302500, grad_norm=0.23853640258312225, loss=1.44468355178833
I0314 07:29:47.713342 139664575932160 logging_writer.py:48] [302600] global_step=302600, grad_norm=0.2408057153224945, loss=1.4856685400009155
I0314 07:30:23.303612 139664567539456 logging_writer.py:48] [302700] global_step=302700, grad_norm=0.23663842678070068, loss=1.4641212224960327
I0314 07:30:58.906816 139664575932160 logging_writer.py:48] [302800] global_step=302800, grad_norm=0.235658198595047, loss=1.5079182386398315
I0314 07:31:34.537674 139664567539456 logging_writer.py:48] [302900] global_step=302900, grad_norm=0.23733970522880554, loss=1.5181770324707031
I0314 07:32:10.154156 139664575932160 logging_writer.py:48] [303000] global_step=303000, grad_norm=0.2306055873632431, loss=1.3468236923217773
I0314 07:32:45.767228 139664567539456 logging_writer.py:48] [303100] global_step=303100, grad_norm=0.233541801571846, loss=1.5034430027008057
I0314 07:33:21.390396 139664575932160 logging_writer.py:48] [303200] global_step=303200, grad_norm=0.6184576153755188, loss=1.4202189445495605
I0314 07:33:57.011978 139664567539456 logging_writer.py:48] [303300] global_step=303300, grad_norm=0.23938986659049988, loss=1.5270867347717285
I0314 07:34:32.622045 139664575932160 logging_writer.py:48] [303400] global_step=303400, grad_norm=0.240061417222023, loss=1.458265781402588
I0314 07:35:08.209161 139664567539456 logging_writer.py:48] [303500] global_step=303500, grad_norm=0.2321348637342453, loss=1.4786572456359863
I0314 07:35:43.838575 139664575932160 logging_writer.py:48] [303600] global_step=303600, grad_norm=0.22402667999267578, loss=1.3963346481323242
I0314 07:36:19.496517 139664567539456 logging_writer.py:48] [303700] global_step=303700, grad_norm=0.24034415185451508, loss=1.4242631196975708
I0314 07:36:55.119411 139664575932160 logging_writer.py:48] [303800] global_step=303800, grad_norm=0.23614493012428284, loss=1.3914475440979004
I0314 07:37:30.702488 139664567539456 logging_writer.py:48] [303900] global_step=303900, grad_norm=0.2447355091571808, loss=1.3783371448516846
I0314 07:38:06.317307 139664575932160 logging_writer.py:48] [304000] global_step=304000, grad_norm=0.2356014847755432, loss=1.4434677362442017
I0314 07:38:41.925569 139664567539456 logging_writer.py:48] [304100] global_step=304100, grad_norm=0.22703222930431366, loss=1.453791856765747
I0314 07:39:17.517282 139664575932160 logging_writer.py:48] [304200] global_step=304200, grad_norm=0.23664408922195435, loss=1.39689040184021
I0314 07:39:53.104980 139664567539456 logging_writer.py:48] [304300] global_step=304300, grad_norm=0.2265666425228119, loss=1.4308061599731445
I0314 07:40:28.721855 139664575932160 logging_writer.py:48] [304400] global_step=304400, grad_norm=0.23096103966236115, loss=1.4242292642593384
I0314 07:41:04.327199 139664567539456 logging_writer.py:48] [304500] global_step=304500, grad_norm=0.2363196760416031, loss=1.4510374069213867
I0314 07:41:39.993132 139664575932160 logging_writer.py:48] [304600] global_step=304600, grad_norm=0.24363483488559723, loss=1.4701087474822998
I0314 07:41:46.499421 139834281293632 spec.py:321] Evaluating on the training split.
I0314 07:41:49.480762 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:45:15.301367 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 07:45:17.966523 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:48:31.734138 139834281293632 spec.py:349] Evaluating on the test split.
I0314 07:48:34.404734 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 07:51:44.296492 139834281293632 submission_runner.py:420] Time since start: 186072.85s, 	Step: 304620, 	{'train/accuracy': 0.696880578994751, 'train/loss': 1.3628426790237427, 'train/bleu': 34.97261335988815, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 108405.63617539406, 'total_duration': 186072.84896922112, 'accumulated_submission_time': 108405.63617539406, 'accumulated_eval_time': 77651.99628734589, 'accumulated_logging_time': 5.915649890899658}
I0314 07:51:44.352286 139664567539456 logging_writer.py:48] [304620] accumulated_eval_time=77651.996287, accumulated_logging_time=5.915650, accumulated_submission_time=108405.636175, global_step=304620, preemption_count=0, score=108405.636175, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=186072.848969, train/accuracy=0.696881, train/bleu=34.972613, train/loss=1.362843, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 07:52:13.170918 139664575932160 logging_writer.py:48] [304700] global_step=304700, grad_norm=0.23232385516166687, loss=1.4668115377426147
I0314 07:52:48.727093 139664567539456 logging_writer.py:48] [304800] global_step=304800, grad_norm=0.23146310448646545, loss=1.4346038103103638
I0314 07:53:24.327134 139664575932160 logging_writer.py:48] [304900] global_step=304900, grad_norm=0.22480516135692596, loss=1.4506206512451172
I0314 07:53:59.946900 139664567539456 logging_writer.py:48] [305000] global_step=305000, grad_norm=0.23492945730686188, loss=1.4766857624053955
I0314 07:54:35.595978 139664575932160 logging_writer.py:48] [305100] global_step=305100, grad_norm=0.24234148859977722, loss=1.4544382095336914
I0314 07:55:11.272330 139664567539456 logging_writer.py:48] [305200] global_step=305200, grad_norm=0.23529605567455292, loss=1.4570671319961548
I0314 07:55:46.890401 139664575932160 logging_writer.py:48] [305300] global_step=305300, grad_norm=0.22883330285549164, loss=1.44709312915802
I0314 07:56:22.496380 139664567539456 logging_writer.py:48] [305400] global_step=305400, grad_norm=0.2424166351556778, loss=1.4838041067123413
I0314 07:56:58.092962 139664575932160 logging_writer.py:48] [305500] global_step=305500, grad_norm=0.23620468378067017, loss=1.4516898393630981
I0314 07:57:33.692009 139664567539456 logging_writer.py:48] [305600] global_step=305600, grad_norm=0.2396213412284851, loss=1.4204645156860352
I0314 07:58:09.275588 139664575932160 logging_writer.py:48] [305700] global_step=305700, grad_norm=0.2324802428483963, loss=1.4104280471801758
I0314 07:58:44.889247 139664567539456 logging_writer.py:48] [305800] global_step=305800, grad_norm=0.2256610244512558, loss=1.4056729078292847
I0314 07:59:20.535637 139664575932160 logging_writer.py:48] [305900] global_step=305900, grad_norm=0.23802019655704498, loss=1.4311989545822144
I0314 07:59:56.164733 139664567539456 logging_writer.py:48] [306000] global_step=306000, grad_norm=0.23027729988098145, loss=1.4998087882995605
I0314 08:00:31.827790 139664575932160 logging_writer.py:48] [306100] global_step=306100, grad_norm=0.22700746357440948, loss=1.4151926040649414
I0314 08:01:07.472931 139664567539456 logging_writer.py:48] [306200] global_step=306200, grad_norm=0.2225935310125351, loss=1.4292480945587158
I0314 08:01:43.093409 139664575932160 logging_writer.py:48] [306300] global_step=306300, grad_norm=0.22509396076202393, loss=1.4419453144073486
I0314 08:02:18.702145 139664567539456 logging_writer.py:48] [306400] global_step=306400, grad_norm=0.23002637922763824, loss=1.352669596672058
I0314 08:02:54.341452 139664575932160 logging_writer.py:48] [306500] global_step=306500, grad_norm=0.23770983517169952, loss=1.5144580602645874
I0314 08:03:29.938441 139664567539456 logging_writer.py:48] [306600] global_step=306600, grad_norm=0.2392110377550125, loss=1.5167021751403809
I0314 08:04:05.552578 139664575932160 logging_writer.py:48] [306700] global_step=306700, grad_norm=0.2384919971227646, loss=1.3661086559295654
I0314 08:04:41.162968 139664567539456 logging_writer.py:48] [306800] global_step=306800, grad_norm=0.23672202229499817, loss=1.4913235902786255
I0314 08:05:16.794320 139664575932160 logging_writer.py:48] [306900] global_step=306900, grad_norm=0.22265295684337616, loss=1.3964875936508179
I0314 08:05:44.307338 139834281293632 spec.py:321] Evaluating on the training split.
I0314 08:05:47.284750 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:09:14.149787 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 08:09:16.833956 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:12:31.335686 139834281293632 spec.py:349] Evaluating on the test split.
I0314 08:12:34.013329 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:15:44.188924 139834281293632 submission_runner.py:420] Time since start: 187512.74s, 	Step: 306979, 	{'train/accuracy': 0.6959228515625, 'train/loss': 1.370876431465149, 'train/bleu': 35.62716315697208, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 109245.50846171379, 'total_duration': 187512.7414045334, 'accumulated_submission_time': 109245.50846171379, 'accumulated_eval_time': 78251.87782359123, 'accumulated_logging_time': 5.980376243591309}
I0314 08:15:44.245928 139664567539456 logging_writer.py:48] [306979] accumulated_eval_time=78251.877824, accumulated_logging_time=5.980376, accumulated_submission_time=109245.508462, global_step=306979, preemption_count=0, score=109245.508462, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=187512.741405, train/accuracy=0.695923, train/bleu=35.627163, train/loss=1.370876, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 08:15:52.077970 139664575932160 logging_writer.py:48] [307000] global_step=307000, grad_norm=0.2301526665687561, loss=1.4118194580078125
I0314 08:16:27.599349 139664567539456 logging_writer.py:48] [307100] global_step=307100, grad_norm=0.23459364473819733, loss=1.3989744186401367
I0314 08:17:03.184935 139664575932160 logging_writer.py:48] [307200] global_step=307200, grad_norm=0.23420272767543793, loss=1.4571177959442139
I0314 08:17:38.774924 139664567539456 logging_writer.py:48] [307300] global_step=307300, grad_norm=0.2334543764591217, loss=1.476515769958496
I0314 08:18:14.360295 139664575932160 logging_writer.py:48] [307400] global_step=307400, grad_norm=0.23282188177108765, loss=1.4224493503570557
I0314 08:18:49.942958 139664567539456 logging_writer.py:48] [307500] global_step=307500, grad_norm=0.24017296731472015, loss=1.4396770000457764
I0314 08:19:25.574410 139664575932160 logging_writer.py:48] [307600] global_step=307600, grad_norm=0.23656876385211945, loss=1.452789545059204
I0314 08:20:01.165888 139664567539456 logging_writer.py:48] [307700] global_step=307700, grad_norm=0.23912332952022552, loss=1.4721150398254395
I0314 08:20:36.784478 139664575932160 logging_writer.py:48] [307800] global_step=307800, grad_norm=0.23291799426078796, loss=1.4638103246688843
I0314 08:21:12.387474 139664567539456 logging_writer.py:48] [307900] global_step=307900, grad_norm=0.23974987864494324, loss=1.399078130722046
I0314 08:21:47.994675 139664575932160 logging_writer.py:48] [308000] global_step=308000, grad_norm=0.2433227002620697, loss=1.4727106094360352
I0314 08:22:23.662338 139664567539456 logging_writer.py:48] [308100] global_step=308100, grad_norm=0.2350432574748993, loss=1.4775575399398804
I0314 08:22:59.292494 139664575932160 logging_writer.py:48] [308200] global_step=308200, grad_norm=0.22514817118644714, loss=1.3637659549713135
I0314 08:23:34.879221 139664567539456 logging_writer.py:48] [308300] global_step=308300, grad_norm=0.23101806640625, loss=1.405138611793518
I0314 08:24:10.468046 139664575932160 logging_writer.py:48] [308400] global_step=308400, grad_norm=0.2269541323184967, loss=1.4178521633148193
I0314 08:24:46.053525 139664567539456 logging_writer.py:48] [308500] global_step=308500, grad_norm=0.24462786316871643, loss=1.4997740983963013
I0314 08:25:21.677210 139664575932160 logging_writer.py:48] [308600] global_step=308600, grad_norm=0.2338077872991562, loss=1.4222122430801392
I0314 08:25:57.295829 139664567539456 logging_writer.py:48] [308700] global_step=308700, grad_norm=0.232550248503685, loss=1.3982971906661987
I0314 08:26:32.904835 139664575932160 logging_writer.py:48] [308800] global_step=308800, grad_norm=0.23435619473457336, loss=1.4537310600280762
I0314 08:27:08.513236 139664567539456 logging_writer.py:48] [308900] global_step=308900, grad_norm=0.23342421650886536, loss=1.530288815498352
I0314 08:27:44.111103 139664575932160 logging_writer.py:48] [309000] global_step=309000, grad_norm=0.24020560085773468, loss=1.4767948389053345
I0314 08:28:19.719455 139664567539456 logging_writer.py:48] [309100] global_step=309100, grad_norm=0.234343484044075, loss=1.4646730422973633
I0314 08:28:55.314096 139664575932160 logging_writer.py:48] [309200] global_step=309200, grad_norm=0.2372259944677353, loss=1.4685800075531006
I0314 08:29:30.973248 139664567539456 logging_writer.py:48] [309300] global_step=309300, grad_norm=0.243002787232399, loss=1.4433727264404297
I0314 08:29:44.214647 139834281293632 spec.py:321] Evaluating on the training split.
I0314 08:29:47.198904 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:33:28.230641 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 08:33:30.937146 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:36:45.137892 139834281293632 spec.py:349] Evaluating on the test split.
I0314 08:36:47.809109 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:39:57.882852 139834281293632 submission_runner.py:420] Time since start: 188966.44s, 	Step: 309339, 	{'train/accuracy': 0.6993377804756165, 'train/loss': 1.3491086959838867, 'train/bleu': 35.87252412013725, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 110085.39775514603, 'total_duration': 188966.43532323837, 'accumulated_submission_time': 110085.39775514603, 'accumulated_eval_time': 78865.54597783089, 'accumulated_logging_time': 6.046472072601318}
I0314 08:39:57.937589 139664575932160 logging_writer.py:48] [309339] accumulated_eval_time=78865.545978, accumulated_logging_time=6.046472, accumulated_submission_time=110085.397755, global_step=309339, preemption_count=0, score=110085.397755, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=188966.435323, train/accuracy=0.699338, train/bleu=35.872524, train/loss=1.349109, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 08:40:19.965663 139664567539456 logging_writer.py:48] [309400] global_step=309400, grad_norm=0.23249386250972748, loss=1.4609971046447754
I0314 08:40:55.503098 139664575932160 logging_writer.py:48] [309500] global_step=309500, grad_norm=0.2396482527256012, loss=1.4899656772613525
I0314 08:41:31.104080 139664567539456 logging_writer.py:48] [309600] global_step=309600, grad_norm=0.22319342195987701, loss=1.3597779273986816
I0314 08:42:06.667293 139664575932160 logging_writer.py:48] [309700] global_step=309700, grad_norm=0.23363663256168365, loss=1.523493766784668
I0314 08:42:42.364295 139664567539456 logging_writer.py:48] [309800] global_step=309800, grad_norm=0.22847692668437958, loss=1.3641672134399414
I0314 08:43:18.008943 139664575932160 logging_writer.py:48] [309900] global_step=309900, grad_norm=0.2280886173248291, loss=1.5155384540557861
I0314 08:43:53.623028 139664567539456 logging_writer.py:48] [310000] global_step=310000, grad_norm=0.22685667872428894, loss=1.3795745372772217
I0314 08:44:29.251527 139664575932160 logging_writer.py:48] [310100] global_step=310100, grad_norm=0.2336384356021881, loss=1.4606183767318726
I0314 08:45:04.892286 139664567539456 logging_writer.py:48] [310200] global_step=310200, grad_norm=0.23993773758411407, loss=1.4215902090072632
I0314 08:45:40.482005 139664575932160 logging_writer.py:48] [310300] global_step=310300, grad_norm=0.23827748000621796, loss=1.3862327337265015
I0314 08:46:16.092578 139664567539456 logging_writer.py:48] [310400] global_step=310400, grad_norm=0.24313388764858246, loss=1.3978420495986938
I0314 08:46:51.677133 139664575932160 logging_writer.py:48] [310500] global_step=310500, grad_norm=0.2355334311723709, loss=1.419213891029358
I0314 08:47:27.306549 139664567539456 logging_writer.py:48] [310600] global_step=310600, grad_norm=0.24039843678474426, loss=1.4496382474899292
I0314 08:48:02.904336 139664575932160 logging_writer.py:48] [310700] global_step=310700, grad_norm=0.22366377711296082, loss=1.4551388025283813
I0314 08:48:38.516081 139664567539456 logging_writer.py:48] [310800] global_step=310800, grad_norm=0.2246803194284439, loss=1.4097808599472046
I0314 08:49:14.127726 139664575932160 logging_writer.py:48] [310900] global_step=310900, grad_norm=0.2385322004556656, loss=1.494219422340393
I0314 08:49:49.724091 139664567539456 logging_writer.py:48] [311000] global_step=311000, grad_norm=0.2442030906677246, loss=1.4636412858963013
I0314 08:50:25.328958 139664575932160 logging_writer.py:48] [311100] global_step=311100, grad_norm=0.23951677978038788, loss=1.4041754007339478
I0314 08:51:00.965621 139664567539456 logging_writer.py:48] [311200] global_step=311200, grad_norm=0.2400328665971756, loss=1.5138534307479858
I0314 08:51:36.559380 139664575932160 logging_writer.py:48] [311300] global_step=311300, grad_norm=0.240525022149086, loss=1.4394537210464478
I0314 08:52:12.150652 139664567539456 logging_writer.py:48] [311400] global_step=311400, grad_norm=0.23267091810703278, loss=1.4408323764801025
I0314 08:52:47.754914 139664575932160 logging_writer.py:48] [311500] global_step=311500, grad_norm=0.2388618439435959, loss=1.4923776388168335
I0314 08:53:23.350138 139664567539456 logging_writer.py:48] [311600] global_step=311600, grad_norm=0.2394777238368988, loss=1.5009722709655762
I0314 08:53:57.997100 139834281293632 spec.py:321] Evaluating on the training split.
I0314 08:54:00.981411 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 08:57:25.045936 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 08:57:27.721072 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:00:41.598162 139834281293632 spec.py:349] Evaluating on the test split.
I0314 09:00:44.274724 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:03:54.036373 139834281293632 submission_runner.py:420] Time since start: 190402.59s, 	Step: 311699, 	{'train/accuracy': 0.6996234059333801, 'train/loss': 1.3519960641860962, 'train/bleu': 35.82919492644673, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 110925.37493872643, 'total_duration': 190402.58885407448, 'accumulated_submission_time': 110925.37493872643, 'accumulated_eval_time': 79461.58521485329, 'accumulated_logging_time': 6.110290288925171}
I0314 09:03:54.090926 139664575932160 logging_writer.py:48] [311699] accumulated_eval_time=79461.585215, accumulated_logging_time=6.110290, accumulated_submission_time=110925.374939, global_step=311699, preemption_count=0, score=110925.374939, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=190402.588854, train/accuracy=0.699623, train/bleu=35.829195, train/loss=1.351996, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 09:03:54.829991 139664567539456 logging_writer.py:48] [311700] global_step=311700, grad_norm=0.2253098487854004, loss=1.3811819553375244
I0314 09:04:30.382225 139664575932160 logging_writer.py:48] [311800] global_step=311800, grad_norm=0.23373004794120789, loss=1.5382145643234253
I0314 09:05:05.952141 139664567539456 logging_writer.py:48] [311900] global_step=311900, grad_norm=0.22837303578853607, loss=1.4965070486068726
I0314 09:05:41.499675 139664575932160 logging_writer.py:48] [312000] global_step=312000, grad_norm=0.24080947041511536, loss=1.4953432083129883
I0314 09:06:17.104161 139664567539456 logging_writer.py:48] [312100] global_step=312100, grad_norm=0.23223257064819336, loss=1.4560362100601196
I0314 09:06:52.720850 139664575932160 logging_writer.py:48] [312200] global_step=312200, grad_norm=0.23911260068416595, loss=1.487120509147644
I0314 09:07:28.295778 139664567539456 logging_writer.py:48] [312300] global_step=312300, grad_norm=0.23816514015197754, loss=1.4838900566101074
I0314 09:08:03.876257 139664575932160 logging_writer.py:48] [312400] global_step=312400, grad_norm=0.2444743514060974, loss=1.4270755052566528
I0314 09:08:39.463267 139664567539456 logging_writer.py:48] [312500] global_step=312500, grad_norm=0.2333330363035202, loss=1.5186880826950073
I0314 09:09:15.042409 139664575932160 logging_writer.py:48] [312600] global_step=312600, grad_norm=0.22865960001945496, loss=1.4867852926254272
I0314 09:09:50.650388 139664567539456 logging_writer.py:48] [312700] global_step=312700, grad_norm=0.23115339875221252, loss=1.45341157913208
I0314 09:10:26.253809 139664575932160 logging_writer.py:48] [312800] global_step=312800, grad_norm=0.2222977578639984, loss=1.4001845121383667
I0314 09:11:01.856058 139664567539456 logging_writer.py:48] [312900] global_step=312900, grad_norm=0.24655921757221222, loss=1.5131686925888062
I0314 09:11:37.489256 139664575932160 logging_writer.py:48] [313000] global_step=313000, grad_norm=0.23291334509849548, loss=1.395978569984436
I0314 09:12:13.125240 139664567539456 logging_writer.py:48] [313100] global_step=313100, grad_norm=0.24680687487125397, loss=1.4546781778335571
I0314 09:12:48.714532 139664575932160 logging_writer.py:48] [313200] global_step=313200, grad_norm=0.22744350135326385, loss=1.4645953178405762
I0314 09:13:24.306546 139664567539456 logging_writer.py:48] [313300] global_step=313300, grad_norm=0.2455083429813385, loss=1.5288145542144775
I0314 09:13:59.928313 139664575932160 logging_writer.py:48] [313400] global_step=313400, grad_norm=0.22801147401332855, loss=1.464436411857605
I0314 09:14:35.537348 139664567539456 logging_writer.py:48] [313500] global_step=313500, grad_norm=0.23952525854110718, loss=1.5426081418991089
I0314 09:15:11.197448 139664575932160 logging_writer.py:48] [313600] global_step=313600, grad_norm=0.23840805888175964, loss=1.43453049659729
I0314 09:15:46.787542 139664567539456 logging_writer.py:48] [313700] global_step=313700, grad_norm=0.2317935675382614, loss=1.4276602268218994
I0314 09:16:22.400853 139664575932160 logging_writer.py:48] [313800] global_step=313800, grad_norm=0.22552980482578278, loss=1.4251903295516968
I0314 09:16:57.993656 139664567539456 logging_writer.py:48] [313900] global_step=313900, grad_norm=0.2467699497938156, loss=1.5232679843902588
I0314 09:17:33.609868 139664575932160 logging_writer.py:48] [314000] global_step=314000, grad_norm=0.24064715206623077, loss=1.4490253925323486
I0314 09:17:54.344254 139834281293632 spec.py:321] Evaluating on the training split.
I0314 09:17:57.313806 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:21:26.033815 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 09:21:28.704150 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:24:44.153864 139834281293632 spec.py:349] Evaluating on the test split.
I0314 09:24:46.840977 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:27:58.153455 139834281293632 submission_runner.py:420] Time since start: 191846.71s, 	Step: 314060, 	{'train/accuracy': 0.6977344155311584, 'train/loss': 1.3559484481811523, 'train/bleu': 35.35065572173739, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 111765.5486676693, 'total_duration': 191846.70589494705, 'accumulated_submission_time': 111765.5486676693, 'accumulated_eval_time': 80065.39432311058, 'accumulated_logging_time': 6.174302101135254}
I0314 09:27:58.220534 139664567539456 logging_writer.py:48] [314060] accumulated_eval_time=80065.394323, accumulated_logging_time=6.174302, accumulated_submission_time=111765.548668, global_step=314060, preemption_count=0, score=111765.548668, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=191846.705895, train/accuracy=0.697734, train/bleu=35.350656, train/loss=1.355948, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 09:28:12.803926 139664575932160 logging_writer.py:48] [314100] global_step=314100, grad_norm=0.2372889518737793, loss=1.4348655939102173
I0314 09:28:48.370887 139664567539456 logging_writer.py:48] [314200] global_step=314200, grad_norm=0.2223007082939148, loss=1.3828493356704712
I0314 09:29:23.953718 139664575932160 logging_writer.py:48] [314300] global_step=314300, grad_norm=0.23582413792610168, loss=1.3922927379608154
I0314 09:29:59.533327 139664567539456 logging_writer.py:48] [314400] global_step=314400, grad_norm=0.24825574457645416, loss=1.4085055589675903
I0314 09:30:35.127985 139664575932160 logging_writer.py:48] [314500] global_step=314500, grad_norm=0.22272317111492157, loss=1.4052679538726807
I0314 09:31:10.733674 139664567539456 logging_writer.py:48] [314600] global_step=314600, grad_norm=0.24011336266994476, loss=1.536845088005066
I0314 09:31:46.303667 139664575932160 logging_writer.py:48] [314700] global_step=314700, grad_norm=0.2372235655784607, loss=1.4222010374069214
I0314 09:32:21.878390 139664567539456 logging_writer.py:48] [314800] global_step=314800, grad_norm=0.23416182398796082, loss=1.42203688621521
I0314 09:32:57.476505 139664575932160 logging_writer.py:48] [314900] global_step=314900, grad_norm=0.22364899516105652, loss=1.384587287902832
I0314 09:33:33.093666 139664567539456 logging_writer.py:48] [315000] global_step=315000, grad_norm=0.23435445129871368, loss=1.4869747161865234
I0314 09:34:08.693750 139664575932160 logging_writer.py:48] [315100] global_step=315100, grad_norm=0.23203489184379578, loss=1.4558531045913696
I0314 09:34:44.284255 139664567539456 logging_writer.py:48] [315200] global_step=315200, grad_norm=0.23192088305950165, loss=1.407518982887268
I0314 09:35:19.873556 139664575932160 logging_writer.py:48] [315300] global_step=315300, grad_norm=0.2316202074289322, loss=1.4609524011611938
I0314 09:35:55.443295 139664567539456 logging_writer.py:48] [315400] global_step=315400, grad_norm=0.2311263084411621, loss=1.4569754600524902
I0314 09:36:31.057415 139664575932160 logging_writer.py:48] [315500] global_step=315500, grad_norm=0.2363269329071045, loss=1.4212390184402466
I0314 09:37:06.668048 139664567539456 logging_writer.py:48] [315600] global_step=315600, grad_norm=0.23650585114955902, loss=1.488258719444275
I0314 09:37:42.272581 139664575932160 logging_writer.py:48] [315700] global_step=315700, grad_norm=0.23221486806869507, loss=1.4038162231445312
I0314 09:38:17.866605 139664567539456 logging_writer.py:48] [315800] global_step=315800, grad_norm=0.2301313728094101, loss=1.4453288316726685
I0314 09:38:53.468965 139664575932160 logging_writer.py:48] [315900] global_step=315900, grad_norm=0.2219889760017395, loss=1.4068098068237305
I0314 09:39:29.098694 139664567539456 logging_writer.py:48] [316000] global_step=316000, grad_norm=0.23316971957683563, loss=1.4191375970840454
I0314 09:40:04.733035 139664575932160 logging_writer.py:48] [316100] global_step=316100, grad_norm=0.23934967815876007, loss=1.4692864418029785
I0314 09:40:40.365814 139664567539456 logging_writer.py:48] [316200] global_step=316200, grad_norm=0.2469440996646881, loss=1.4651252031326294
I0314 09:41:16.031118 139664575932160 logging_writer.py:48] [316300] global_step=316300, grad_norm=0.23696018755435944, loss=1.4169985055923462
I0314 09:41:51.694510 139664567539456 logging_writer.py:48] [316400] global_step=316400, grad_norm=0.22354771196842194, loss=1.3973556756973267
I0314 09:41:58.168063 139834281293632 spec.py:321] Evaluating on the training split.
I0314 09:42:01.141367 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:45:39.718032 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 09:45:42.415380 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:48:56.728290 139834281293632 spec.py:349] Evaluating on the test split.
I0314 09:48:59.403880 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 09:52:09.330533 139834281293632 submission_runner.py:420] Time since start: 193297.88s, 	Step: 316420, 	{'train/accuracy': 0.6962667107582092, 'train/loss': 1.3646796941757202, 'train/bleu': 35.69826971741045, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 112605.41333723068, 'total_duration': 193297.88295459747, 'accumulated_submission_time': 112605.41333723068, 'accumulated_eval_time': 80676.55667924881, 'accumulated_logging_time': 6.251188278198242}
I0314 09:52:09.397370 139664575932160 logging_writer.py:48] [316420] accumulated_eval_time=80676.556679, accumulated_logging_time=6.251188, accumulated_submission_time=112605.413337, global_step=316420, preemption_count=0, score=112605.413337, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=193297.882955, train/accuracy=0.696267, train/bleu=35.698270, train/loss=1.364680, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 09:52:38.190575 139664567539456 logging_writer.py:48] [316500] global_step=316500, grad_norm=0.2283451110124588, loss=1.4580734968185425
I0314 09:53:13.780279 139664575932160 logging_writer.py:48] [316600] global_step=316600, grad_norm=0.23705044388771057, loss=1.435529112815857
I0314 09:53:49.366546 139664567539456 logging_writer.py:48] [316700] global_step=316700, grad_norm=0.23498955368995667, loss=1.4437685012817383
I0314 09:54:24.947996 139664575932160 logging_writer.py:48] [316800] global_step=316800, grad_norm=0.2291349172592163, loss=1.4731333255767822
I0314 09:55:00.548241 139664567539456 logging_writer.py:48] [316900] global_step=316900, grad_norm=0.22735942900180817, loss=1.4461551904678345
I0314 09:55:36.155847 139664575932160 logging_writer.py:48] [317000] global_step=317000, grad_norm=0.23010821640491486, loss=1.4405754804611206
I0314 09:56:11.744895 139664567539456 logging_writer.py:48] [317100] global_step=317100, grad_norm=0.2357814610004425, loss=1.5278788805007935
I0314 09:56:47.322269 139664575932160 logging_writer.py:48] [317200] global_step=317200, grad_norm=0.2398548722267151, loss=1.5349087715148926
I0314 09:57:22.910726 139664567539456 logging_writer.py:48] [317300] global_step=317300, grad_norm=0.23134738206863403, loss=1.352845549583435
I0314 09:57:58.533232 139664575932160 logging_writer.py:48] [317400] global_step=317400, grad_norm=0.23493808507919312, loss=1.4321283102035522
I0314 09:58:34.188809 139664567539456 logging_writer.py:48] [317500] global_step=317500, grad_norm=0.23370975255966187, loss=1.422971487045288
I0314 09:59:09.866927 139664575932160 logging_writer.py:48] [317600] global_step=317600, grad_norm=0.2356744259595871, loss=1.4411414861679077
I0314 09:59:45.441709 139664567539456 logging_writer.py:48] [317700] global_step=317700, grad_norm=0.23656770586967468, loss=1.4391015768051147
I0314 10:00:21.045448 139664575932160 logging_writer.py:48] [317800] global_step=317800, grad_norm=0.22953535616397858, loss=1.3731697797775269
I0314 10:00:56.636193 139664567539456 logging_writer.py:48] [317900] global_step=317900, grad_norm=0.23413589596748352, loss=1.3747808933258057
I0314 10:01:32.265574 139664575932160 logging_writer.py:48] [318000] global_step=318000, grad_norm=0.23031125962734222, loss=1.468709111213684
I0314 10:02:07.898650 139664567539456 logging_writer.py:48] [318100] global_step=318100, grad_norm=0.23175464570522308, loss=1.4172812700271606
I0314 10:02:43.499634 139664575932160 logging_writer.py:48] [318200] global_step=318200, grad_norm=0.23474162817001343, loss=1.519037127494812
I0314 10:03:19.090253 139664567539456 logging_writer.py:48] [318300] global_step=318300, grad_norm=0.22995387017726898, loss=1.4446206092834473
I0314 10:03:54.694034 139664575932160 logging_writer.py:48] [318400] global_step=318400, grad_norm=0.23838184773921967, loss=1.4268418550491333
I0314 10:04:30.280418 139664567539456 logging_writer.py:48] [318500] global_step=318500, grad_norm=0.23663876950740814, loss=1.5385551452636719
I0314 10:05:05.895903 139664575932160 logging_writer.py:48] [318600] global_step=318600, grad_norm=0.2372698187828064, loss=1.40566885471344
I0314 10:05:41.511701 139664567539456 logging_writer.py:48] [318700] global_step=318700, grad_norm=0.22880667448043823, loss=1.4661494493484497
I0314 10:06:09.373370 139834281293632 spec.py:321] Evaluating on the training split.
I0314 10:06:12.355639 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:09:41.702532 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 10:09:44.379543 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:12:58.121094 139834281293632 spec.py:349] Evaluating on the test split.
I0314 10:13:00.796703 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:16:10.437765 139834281293632 submission_runner.py:420] Time since start: 194738.99s, 	Step: 318780, 	{'train/accuracy': 0.6989819407463074, 'train/loss': 1.3501715660095215, 'train/bleu': 35.58819065628888, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 113445.30772137642, 'total_duration': 194738.9902229309, 'accumulated_submission_time': 113445.30772137642, 'accumulated_eval_time': 81277.62100315094, 'accumulated_logging_time': 6.328147649765015}
I0314 10:16:10.494865 139664575932160 logging_writer.py:48] [318780] accumulated_eval_time=81277.621003, accumulated_logging_time=6.328148, accumulated_submission_time=113445.307721, global_step=318780, preemption_count=0, score=113445.307721, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=194738.990223, train/accuracy=0.698982, train/bleu=35.588191, train/loss=1.350172, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 10:16:17.962905 139664567539456 logging_writer.py:48] [318800] global_step=318800, grad_norm=0.2396288812160492, loss=1.4062933921813965
I0314 10:16:53.494565 139664575932160 logging_writer.py:48] [318900] global_step=318900, grad_norm=0.23811477422714233, loss=1.4181331396102905
I0314 10:17:29.092225 139664567539456 logging_writer.py:48] [319000] global_step=319000, grad_norm=0.24664559960365295, loss=1.4786529541015625
I0314 10:18:04.728402 139664575932160 logging_writer.py:48] [319100] global_step=319100, grad_norm=0.24411022663116455, loss=1.4450870752334595
I0314 10:18:40.392338 139664567539456 logging_writer.py:48] [319200] global_step=319200, grad_norm=0.24257813394069672, loss=1.440589427947998
I0314 10:19:15.993523 139664575932160 logging_writer.py:48] [319300] global_step=319300, grad_norm=0.23324896395206451, loss=1.392931580543518
I0314 10:19:51.616975 139664567539456 logging_writer.py:48] [319400] global_step=319400, grad_norm=0.23143772780895233, loss=1.394972562789917
I0314 10:20:27.233958 139664575932160 logging_writer.py:48] [319500] global_step=319500, grad_norm=0.23600685596466064, loss=1.4101425409317017
I0314 10:21:02.821908 139664567539456 logging_writer.py:48] [319600] global_step=319600, grad_norm=0.24728959798812866, loss=1.5011577606201172
I0314 10:21:38.379736 139664575932160 logging_writer.py:48] [319700] global_step=319700, grad_norm=0.23465771973133087, loss=1.4536219835281372
I0314 10:22:13.970054 139664567539456 logging_writer.py:48] [319800] global_step=319800, grad_norm=0.23490221798419952, loss=1.4813454151153564
I0314 10:22:49.624996 139664575932160 logging_writer.py:48] [319900] global_step=319900, grad_norm=0.23040293157100677, loss=1.3999369144439697
I0314 10:23:25.252008 139664567539456 logging_writer.py:48] [320000] global_step=320000, grad_norm=0.24379326403141022, loss=1.4398945569992065
I0314 10:24:00.854478 139664575932160 logging_writer.py:48] [320100] global_step=320100, grad_norm=0.23072052001953125, loss=1.4239170551300049
I0314 10:24:36.479251 139664567539456 logging_writer.py:48] [320200] global_step=320200, grad_norm=0.23709511756896973, loss=1.441915512084961
I0314 10:25:12.088998 139664575932160 logging_writer.py:48] [320300] global_step=320300, grad_norm=0.22647158801555634, loss=1.4569628238677979
I0314 10:25:47.689113 139664567539456 logging_writer.py:48] [320400] global_step=320400, grad_norm=0.2373669296503067, loss=1.4569934606552124
I0314 10:26:23.265822 139664575932160 logging_writer.py:48] [320500] global_step=320500, grad_norm=0.23074942827224731, loss=1.436781883239746
I0314 10:26:58.886153 139664567539456 logging_writer.py:48] [320600] global_step=320600, grad_norm=0.22882230579853058, loss=1.3772008419036865
I0314 10:27:34.493443 139664575932160 logging_writer.py:48] [320700] global_step=320700, grad_norm=0.23026740550994873, loss=1.4533554315567017
I0314 10:28:10.076390 139664567539456 logging_writer.py:48] [320800] global_step=320800, grad_norm=0.2380020022392273, loss=1.414845585823059
I0314 10:28:45.665169 139664575932160 logging_writer.py:48] [320900] global_step=320900, grad_norm=0.2259429395198822, loss=1.3994730710983276
I0314 10:29:21.250646 139664567539456 logging_writer.py:48] [321000] global_step=321000, grad_norm=0.2415376901626587, loss=1.4172749519348145
I0314 10:29:56.849746 139664575932160 logging_writer.py:48] [321100] global_step=321100, grad_norm=0.23191571235656738, loss=1.4213873147964478
I0314 10:30:10.453366 139834281293632 spec.py:321] Evaluating on the training split.
I0314 10:30:13.428298 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:33:45.071444 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 10:33:47.751716 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:37:01.772456 139834281293632 spec.py:349] Evaluating on the test split.
I0314 10:37:04.444433 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:40:14.119221 139834281293632 submission_runner.py:420] Time since start: 196182.67s, 	Step: 321140, 	{'train/accuracy': 0.6966158151626587, 'train/loss': 1.3639127016067505, 'train/bleu': 35.62989032088873, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 114285.18418955803, 'total_duration': 196182.67170715332, 'accumulated_submission_time': 114285.18418955803, 'accumulated_eval_time': 81881.28680968285, 'accumulated_logging_time': 6.394540309906006}
I0314 10:40:14.175233 139664567539456 logging_writer.py:48] [321140] accumulated_eval_time=81881.286810, accumulated_logging_time=6.394540, accumulated_submission_time=114285.184190, global_step=321140, preemption_count=0, score=114285.184190, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=196182.671707, train/accuracy=0.696616, train/bleu=35.629890, train/loss=1.363913, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 10:40:35.823906 139664575932160 logging_writer.py:48] [321200] global_step=321200, grad_norm=0.23004336655139923, loss=1.4473748207092285
I0314 10:41:11.360873 139664567539456 logging_writer.py:48] [321300] global_step=321300, grad_norm=0.23826147615909576, loss=1.4539273977279663
I0314 10:41:46.918172 139664575932160 logging_writer.py:48] [321400] global_step=321400, grad_norm=0.22053001821041107, loss=1.3431613445281982
I0314 10:42:22.496301 139664567539456 logging_writer.py:48] [321500] global_step=321500, grad_norm=0.229751318693161, loss=1.4233794212341309
I0314 10:42:58.077677 139664575932160 logging_writer.py:48] [321600] global_step=321600, grad_norm=0.23708385229110718, loss=1.3892923593521118
I0314 10:43:33.688953 139664567539456 logging_writer.py:48] [321700] global_step=321700, grad_norm=0.24638016521930695, loss=1.4230893850326538
I0314 10:44:09.293926 139664575932160 logging_writer.py:48] [321800] global_step=321800, grad_norm=0.22939525544643402, loss=1.4586825370788574
I0314 10:44:44.857563 139664567539456 logging_writer.py:48] [321900] global_step=321900, grad_norm=0.2256266325712204, loss=1.3680535554885864
I0314 10:45:20.475807 139664575932160 logging_writer.py:48] [322000] global_step=322000, grad_norm=0.23209135234355927, loss=1.4102461338043213
I0314 10:45:56.073144 139664567539456 logging_writer.py:48] [322100] global_step=322100, grad_norm=0.2324848175048828, loss=1.4332408905029297
I0314 10:46:31.655696 139664575932160 logging_writer.py:48] [322200] global_step=322200, grad_norm=0.24386784434318542, loss=1.4931919574737549
I0314 10:47:07.252209 139664567539456 logging_writer.py:48] [322300] global_step=322300, grad_norm=0.23851068317890167, loss=1.3541218042373657
I0314 10:47:42.835513 139664575932160 logging_writer.py:48] [322400] global_step=322400, grad_norm=0.23141947388648987, loss=1.4560402631759644
I0314 10:48:18.480985 139664567539456 logging_writer.py:48] [322500] global_step=322500, grad_norm=0.2298952341079712, loss=1.456296682357788
I0314 10:48:54.109303 139664575932160 logging_writer.py:48] [322600] global_step=322600, grad_norm=0.23105768859386444, loss=1.3911036252975464
I0314 10:49:29.706645 139664567539456 logging_writer.py:48] [322700] global_step=322700, grad_norm=0.248109370470047, loss=1.5101146697998047
I0314 10:50:05.307681 139664575932160 logging_writer.py:48] [322800] global_step=322800, grad_norm=0.24569673836231232, loss=1.4488439559936523
I0314 10:50:40.911366 139664567539456 logging_writer.py:48] [322900] global_step=322900, grad_norm=0.23799075186252594, loss=1.4994635581970215
I0314 10:51:16.523838 139664575932160 logging_writer.py:48] [323000] global_step=323000, grad_norm=0.23891060054302216, loss=1.4669545888900757
I0314 10:51:52.077185 139664567539456 logging_writer.py:48] [323100] global_step=323100, grad_norm=0.23771291971206665, loss=1.4588549137115479
I0314 10:52:27.648102 139664575932160 logging_writer.py:48] [323200] global_step=323200, grad_norm=0.23348167538642883, loss=1.3953831195831299
I0314 10:53:03.287156 139664567539456 logging_writer.py:48] [323300] global_step=323300, grad_norm=0.2364136278629303, loss=1.4672226905822754
I0314 10:53:38.851581 139664575932160 logging_writer.py:48] [323400] global_step=323400, grad_norm=0.2293292135000229, loss=1.4254964590072632
I0314 10:54:14.462761 139664567539456 logging_writer.py:48] [323500] global_step=323500, grad_norm=0.2421274036169052, loss=1.4552942514419556
I0314 10:54:14.468283 139834281293632 spec.py:321] Evaluating on the training split.
I0314 10:54:17.151064 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 10:58:00.279563 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 10:58:02.957830 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:01:16.953283 139834281293632 spec.py:349] Evaluating on the test split.
I0314 11:01:19.629624 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:04:30.805129 139834281293632 submission_runner.py:420] Time since start: 197639.36s, 	Step: 323501, 	{'train/accuracy': 0.6984859704971313, 'train/loss': 1.3505821228027344, 'train/bleu': 35.67098677731126, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 115125.39649248123, 'total_duration': 197639.35759925842, 'accumulated_submission_time': 115125.39649248123, 'accumulated_eval_time': 82497.62356734276, 'accumulated_logging_time': 6.459453105926514}
I0314 11:04:30.863200 139664575932160 logging_writer.py:48] [323501] accumulated_eval_time=82497.623567, accumulated_logging_time=6.459453, accumulated_submission_time=115125.396492, global_step=323501, preemption_count=0, score=115125.396492, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=197639.357599, train/accuracy=0.698486, train/bleu=35.670987, train/loss=1.350582, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 11:05:06.433746 139664567539456 logging_writer.py:48] [323600] global_step=323600, grad_norm=0.23037762939929962, loss=1.4443464279174805
I0314 11:05:42.002381 139664575932160 logging_writer.py:48] [323700] global_step=323700, grad_norm=0.22953812777996063, loss=1.4122374057769775
I0314 11:06:17.610890 139664567539456 logging_writer.py:48] [323800] global_step=323800, grad_norm=0.24729721248149872, loss=1.4485830068588257
I0314 11:06:53.181247 139664575932160 logging_writer.py:48] [323900] global_step=323900, grad_norm=0.23772521317005157, loss=1.463560700416565
I0314 11:07:28.778179 139664567539456 logging_writer.py:48] [324000] global_step=324000, grad_norm=0.22964251041412354, loss=1.4710702896118164
I0314 11:08:04.387502 139664575932160 logging_writer.py:48] [324100] global_step=324100, grad_norm=0.2430940717458725, loss=1.510684609413147
I0314 11:08:39.987550 139664567539456 logging_writer.py:48] [324200] global_step=324200, grad_norm=0.23846974968910217, loss=1.4191772937774658
I0314 11:09:15.605953 139664575932160 logging_writer.py:48] [324300] global_step=324300, grad_norm=0.22723765671253204, loss=1.337223768234253
I0314 11:09:51.210479 139664567539456 logging_writer.py:48] [324400] global_step=324400, grad_norm=0.23368993401527405, loss=1.4401817321777344
I0314 11:10:26.879778 139664575932160 logging_writer.py:48] [324500] global_step=324500, grad_norm=0.2409960776567459, loss=1.4701932668685913
I0314 11:11:02.525858 139664567539456 logging_writer.py:48] [324600] global_step=324600, grad_norm=0.22332674264907837, loss=1.4204703569412231
I0314 11:11:38.137067 139664575932160 logging_writer.py:48] [324700] global_step=324700, grad_norm=0.2305392324924469, loss=1.4692535400390625
I0314 11:12:13.718098 139664567539456 logging_writer.py:48] [324800] global_step=324800, grad_norm=0.24016782641410828, loss=1.4527028799057007
I0314 11:12:49.297068 139664575932160 logging_writer.py:48] [324900] global_step=324900, grad_norm=0.23865345120429993, loss=1.4844666719436646
I0314 11:13:24.896412 139664567539456 logging_writer.py:48] [325000] global_step=325000, grad_norm=0.2275163233280182, loss=1.3871995210647583
I0314 11:14:00.489712 139664575932160 logging_writer.py:48] [325100] global_step=325100, grad_norm=0.24386607110500336, loss=1.4326117038726807
I0314 11:14:36.135219 139664567539456 logging_writer.py:48] [325200] global_step=325200, grad_norm=0.23246799409389496, loss=1.506425142288208
I0314 11:15:11.728496 139664575932160 logging_writer.py:48] [325300] global_step=325300, grad_norm=0.22790785133838654, loss=1.3485803604125977
I0314 11:15:47.280791 139664567539456 logging_writer.py:48] [325400] global_step=325400, grad_norm=0.22812972962856293, loss=1.4397516250610352
I0314 11:16:22.858435 139664575932160 logging_writer.py:48] [325500] global_step=325500, grad_norm=0.2312821000814438, loss=1.4427109956741333
I0314 11:16:58.441482 139664567539456 logging_writer.py:48] [325600] global_step=325600, grad_norm=0.23593665659427643, loss=1.4362537860870361
I0314 11:17:34.029076 139664575932160 logging_writer.py:48] [325700] global_step=325700, grad_norm=0.2331448346376419, loss=1.4105221033096313
I0314 11:18:09.598419 139664567539456 logging_writer.py:48] [325800] global_step=325800, grad_norm=0.2440272569656372, loss=1.501556158065796
I0314 11:18:31.007602 139834281293632 spec.py:321] Evaluating on the training split.
I0314 11:18:33.977515 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:22:09.467175 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 11:22:12.152225 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:25:27.990322 139834281293632 spec.py:349] Evaluating on the test split.
I0314 11:25:30.682543 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:28:40.814320 139834281293632 submission_runner.py:420] Time since start: 199089.37s, 	Step: 325862, 	{'train/accuracy': 0.6952317357063293, 'train/loss': 1.3720507621765137, 'train/bleu': 35.218722071193795, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 115965.46018266678, 'total_duration': 199089.36680173874, 'accumulated_submission_time': 115965.46018266678, 'accumulated_eval_time': 83107.43023467064, 'accumulated_logging_time': 6.526806592941284}
I0314 11:28:40.872875 139664575932160 logging_writer.py:48] [325862] accumulated_eval_time=83107.430235, accumulated_logging_time=6.526807, accumulated_submission_time=115965.460183, global_step=325862, preemption_count=0, score=115965.460183, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=199089.366802, train/accuracy=0.695232, train/bleu=35.218722, train/loss=1.372051, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 11:28:54.740459 139664567539456 logging_writer.py:48] [325900] global_step=325900, grad_norm=0.24619823694229126, loss=1.4492861032485962
I0314 11:29:30.297889 139664575932160 logging_writer.py:48] [326000] global_step=326000, grad_norm=0.23336319625377655, loss=1.4503378868103027
I0314 11:30:05.862137 139664567539456 logging_writer.py:48] [326100] global_step=326100, grad_norm=0.23202835023403168, loss=1.4740681648254395
I0314 11:30:41.413650 139664575932160 logging_writer.py:48] [326200] global_step=326200, grad_norm=0.22610914707183838, loss=1.4129968881607056
I0314 11:31:17.001778 139664567539456 logging_writer.py:48] [326300] global_step=326300, grad_norm=0.2375631183385849, loss=1.4264317750930786
I0314 11:31:52.604329 139664575932160 logging_writer.py:48] [326400] global_step=326400, grad_norm=0.2291553020477295, loss=1.412123441696167
I0314 11:32:28.217984 139664567539456 logging_writer.py:48] [326500] global_step=326500, grad_norm=0.23667514324188232, loss=1.4779871702194214
I0314 11:33:03.848740 139664575932160 logging_writer.py:48] [326600] global_step=326600, grad_norm=0.23470884561538696, loss=1.4365754127502441
I0314 11:33:39.447602 139664567539456 logging_writer.py:48] [326700] global_step=326700, grad_norm=0.23550130426883698, loss=1.4403295516967773
I0314 11:34:15.076919 139664575932160 logging_writer.py:48] [326800] global_step=326800, grad_norm=0.23362433910369873, loss=1.4461687803268433
I0314 11:34:50.652840 139664567539456 logging_writer.py:48] [326900] global_step=326900, grad_norm=0.23855337500572205, loss=1.4424928426742554
I0314 11:35:26.264428 139664575932160 logging_writer.py:48] [327000] global_step=327000, grad_norm=0.2347460389137268, loss=1.4664063453674316
I0314 11:36:01.839880 139664567539456 logging_writer.py:48] [327100] global_step=327100, grad_norm=0.23415537178516388, loss=1.4572919607162476
I0314 11:36:37.470592 139664575932160 logging_writer.py:48] [327200] global_step=327200, grad_norm=0.22575277090072632, loss=1.452763557434082
I0314 11:37:13.042158 139664567539456 logging_writer.py:48] [327300] global_step=327300, grad_norm=0.236402228474617, loss=1.3965152502059937
I0314 11:37:48.661009 139664575932160 logging_writer.py:48] [327400] global_step=327400, grad_norm=0.23836706578731537, loss=1.4348704814910889
I0314 11:38:24.304359 139664567539456 logging_writer.py:48] [327500] global_step=327500, grad_norm=0.2323409765958786, loss=1.4333878755569458
I0314 11:38:59.940177 139664575932160 logging_writer.py:48] [327600] global_step=327600, grad_norm=0.23316988348960876, loss=1.4438683986663818
I0314 11:39:35.554017 139664567539456 logging_writer.py:48] [327700] global_step=327700, grad_norm=0.2310566008090973, loss=1.4009028673171997
I0314 11:40:11.143793 139664575932160 logging_writer.py:48] [327800] global_step=327800, grad_norm=0.23544654250144958, loss=1.463811993598938
I0314 11:40:46.768744 139664567539456 logging_writer.py:48] [327900] global_step=327900, grad_norm=0.23744969069957733, loss=1.4449727535247803
I0314 11:41:22.370198 139664575932160 logging_writer.py:48] [328000] global_step=328000, grad_norm=0.23576408624649048, loss=1.4502143859863281
I0314 11:41:57.972256 139664567539456 logging_writer.py:48] [328100] global_step=328100, grad_norm=0.22596165537834167, loss=1.4453010559082031
I0314 11:42:33.592960 139664575932160 logging_writer.py:48] [328200] global_step=328200, grad_norm=0.2231893688440323, loss=1.3473153114318848
I0314 11:42:41.147460 139834281293632 spec.py:321] Evaluating on the training split.
I0314 11:42:44.115400 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:46:21.300775 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 11:46:23.983469 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:49:38.077385 139834281293632 spec.py:349] Evaluating on the test split.
I0314 11:49:40.746205 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 11:52:50.664985 139834281293632 submission_runner.py:420] Time since start: 200539.22s, 	Step: 328223, 	{'train/accuracy': 0.6982913613319397, 'train/loss': 1.3581576347351074, 'train/bleu': 35.58612960215996, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 116805.6553747654, 'total_duration': 200539.21746993065, 'accumulated_submission_time': 116805.6553747654, 'accumulated_eval_time': 83716.94771122932, 'accumulated_logging_time': 6.594550371170044}
I0314 11:52:50.721784 139664567539456 logging_writer.py:48] [328223] accumulated_eval_time=83716.947711, accumulated_logging_time=6.594550, accumulated_submission_time=116805.655375, global_step=328223, preemption_count=0, score=116805.655375, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=200539.217470, train/accuracy=0.698291, train/bleu=35.586130, train/loss=1.358158, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 11:53:18.461327 139664575932160 logging_writer.py:48] [328300] global_step=328300, grad_norm=0.23926390707492828, loss=1.5292768478393555
I0314 11:53:53.976850 139664567539456 logging_writer.py:48] [328400] global_step=328400, grad_norm=0.23719921708106995, loss=1.452774167060852
I0314 11:54:29.547897 139664575932160 logging_writer.py:48] [328500] global_step=328500, grad_norm=0.22868113219738007, loss=1.4157812595367432
I0314 11:55:05.116172 139664567539456 logging_writer.py:48] [328600] global_step=328600, grad_norm=0.2225162386894226, loss=1.3567373752593994
I0314 11:55:40.673184 139664575932160 logging_writer.py:48] [328700] global_step=328700, grad_norm=0.23597466945648193, loss=1.4412058591842651
I0314 11:56:16.269714 139664567539456 logging_writer.py:48] [328800] global_step=328800, grad_norm=0.2341124415397644, loss=1.4781464338302612
I0314 11:56:51.861755 139664575932160 logging_writer.py:48] [328900] global_step=328900, grad_norm=0.23165521025657654, loss=1.5133816003799438
I0314 11:57:27.476171 139664567539456 logging_writer.py:48] [329000] global_step=329000, grad_norm=0.24007299542427063, loss=1.4240024089813232
I0314 11:58:03.061658 139664575932160 logging_writer.py:48] [329100] global_step=329100, grad_norm=0.2278507947921753, loss=1.3974658250808716
I0314 11:58:38.674566 139664567539456 logging_writer.py:48] [329200] global_step=329200, grad_norm=0.222909078001976, loss=1.4070968627929688
I0314 11:59:14.290865 139664575932160 logging_writer.py:48] [329300] global_step=329300, grad_norm=0.24401231110095978, loss=1.5059894323349
I0314 11:59:49.917244 139664567539456 logging_writer.py:48] [329400] global_step=329400, grad_norm=0.2493322789669037, loss=1.4623334407806396
I0314 12:00:25.528851 139664575932160 logging_writer.py:48] [329500] global_step=329500, grad_norm=0.23546843230724335, loss=1.4679393768310547
I0314 12:01:01.087945 139664567539456 logging_writer.py:48] [329600] global_step=329600, grad_norm=0.23509739339351654, loss=1.4642586708068848
I0314 12:01:36.675189 139664575932160 logging_writer.py:48] [329700] global_step=329700, grad_norm=0.22617913782596588, loss=1.4552254676818848
I0314 12:02:12.279901 139664567539456 logging_writer.py:48] [329800] global_step=329800, grad_norm=0.2374492734670639, loss=1.4544572830200195
I0314 12:02:47.909868 139664575932160 logging_writer.py:48] [329900] global_step=329900, grad_norm=0.23513135313987732, loss=1.411832332611084
I0314 12:03:23.508756 139664567539456 logging_writer.py:48] [330000] global_step=330000, grad_norm=0.23951928317546844, loss=1.429948091506958
I0314 12:03:59.087345 139664575932160 logging_writer.py:48] [330100] global_step=330100, grad_norm=0.23470209538936615, loss=1.4384461641311646
I0314 12:04:34.702216 139664567539456 logging_writer.py:48] [330200] global_step=330200, grad_norm=0.2378406673669815, loss=1.429984211921692
I0314 12:05:10.295332 139664575932160 logging_writer.py:48] [330300] global_step=330300, grad_norm=0.2383558750152588, loss=1.4849660396575928
I0314 12:05:45.873950 139664567539456 logging_writer.py:48] [330400] global_step=330400, grad_norm=0.2289547473192215, loss=1.4375191926956177
I0314 12:06:21.449422 139664575932160 logging_writer.py:48] [330500] global_step=330500, grad_norm=0.23105575144290924, loss=1.4433428049087524
I0314 12:06:50.684669 139834281293632 spec.py:321] Evaluating on the training split.
I0314 12:06:53.653887 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:10:25.114428 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 12:10:27.783998 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:13:41.686887 139834281293632 spec.py:349] Evaluating on the test split.
I0314 12:13:44.364936 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:16:54.045447 139834281293632 submission_runner.py:420] Time since start: 201982.60s, 	Step: 330584, 	{'train/accuracy': 0.6947457790374756, 'train/loss': 1.3779659271240234, 'train/bleu': 35.72883824812817, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 117645.53719234467, 'total_duration': 201982.5979168415, 'accumulated_submission_time': 117645.53719234467, 'accumulated_eval_time': 84320.30842804909, 'accumulated_logging_time': 6.6605446338653564}
I0314 12:16:54.102626 139664567539456 logging_writer.py:48] [330584] accumulated_eval_time=84320.308428, accumulated_logging_time=6.660545, accumulated_submission_time=117645.537192, global_step=330584, preemption_count=0, score=117645.537192, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=201982.597917, train/accuracy=0.694746, train/bleu=35.728838, train/loss=1.377966, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 12:17:00.159596 139664575932160 logging_writer.py:48] [330600] global_step=330600, grad_norm=0.23316188156604767, loss=1.355315089225769
I0314 12:17:35.699731 139664567539456 logging_writer.py:48] [330700] global_step=330700, grad_norm=0.23678593337535858, loss=1.4327460527420044
I0314 12:18:11.282791 139664575932160 logging_writer.py:48] [330800] global_step=330800, grad_norm=0.2377004325389862, loss=1.4986785650253296
I0314 12:18:46.894430 139664567539456 logging_writer.py:48] [330900] global_step=330900, grad_norm=0.22381535172462463, loss=1.4524691104888916
I0314 12:19:22.496824 139664575932160 logging_writer.py:48] [331000] global_step=331000, grad_norm=0.23171110451221466, loss=1.4049465656280518
I0314 12:19:58.094087 139664567539456 logging_writer.py:48] [331100] global_step=331100, grad_norm=0.2355308085680008, loss=1.4448598623275757
I0314 12:20:33.682494 139664575932160 logging_writer.py:48] [331200] global_step=331200, grad_norm=0.23167645931243896, loss=1.476081371307373
I0314 12:21:09.260171 139664567539456 logging_writer.py:48] [331300] global_step=331300, grad_norm=0.22909417748451233, loss=1.4346399307250977
I0314 12:21:44.861476 139664575932160 logging_writer.py:48] [331400] global_step=331400, grad_norm=0.24035757780075073, loss=1.5269901752471924
I0314 12:22:20.433028 139664567539456 logging_writer.py:48] [331500] global_step=331500, grad_norm=0.24420742690563202, loss=1.470230221748352
I0314 12:22:56.039274 139664575932160 logging_writer.py:48] [331600] global_step=331600, grad_norm=0.5629348754882812, loss=1.4324116706848145
I0314 12:23:31.628043 139664567539456 logging_writer.py:48] [331700] global_step=331700, grad_norm=0.242838054895401, loss=1.5382376909255981
I0314 12:24:07.206413 139664575932160 logging_writer.py:48] [331800] global_step=331800, grad_norm=0.23439227044582367, loss=1.407873272895813
I0314 12:24:42.799324 139664567539456 logging_writer.py:48] [331900] global_step=331900, grad_norm=0.23642876744270325, loss=1.4125860929489136
I0314 12:25:18.407000 139664575932160 logging_writer.py:48] [332000] global_step=332000, grad_norm=0.24234095215797424, loss=1.4540656805038452
I0314 12:25:54.023475 139664567539456 logging_writer.py:48] [332100] global_step=332100, grad_norm=0.23547984659671783, loss=1.4427448511123657
I0314 12:26:29.676471 139664575932160 logging_writer.py:48] [332200] global_step=332200, grad_norm=0.2351873368024826, loss=1.44254732131958
I0314 12:27:05.255128 139664567539456 logging_writer.py:48] [332300] global_step=332300, grad_norm=0.2338436096906662, loss=1.4938565492630005
I0314 12:27:40.819461 139664575932160 logging_writer.py:48] [332400] global_step=332400, grad_norm=0.23355276882648468, loss=1.44950532913208
I0314 12:28:16.428394 139664567539456 logging_writer.py:48] [332500] global_step=332500, grad_norm=0.2470817118883133, loss=1.4589691162109375
I0314 12:28:51.997791 139664575932160 logging_writer.py:48] [332600] global_step=332600, grad_norm=0.23711372911930084, loss=1.4571533203125
I0314 12:29:27.570809 139664567539456 logging_writer.py:48] [332700] global_step=332700, grad_norm=0.23839044570922852, loss=1.4413881301879883
I0314 12:30:03.155438 139664575932160 logging_writer.py:48] [332800] global_step=332800, grad_norm=0.22923749685287476, loss=1.4530187845230103
I0314 12:30:38.777635 139664567539456 logging_writer.py:48] [332900] global_step=332900, grad_norm=0.23053881525993347, loss=1.4788049459457397
I0314 12:30:54.173850 139834281293632 spec.py:321] Evaluating on the training split.
I0314 12:30:57.162094 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:34:23.151085 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 12:34:25.841006 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:37:40.198355 139834281293632 spec.py:349] Evaluating on the test split.
I0314 12:37:42.867651 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:40:52.816521 139834281293632 submission_runner.py:420] Time since start: 203421.37s, 	Step: 332945, 	{'train/accuracy': 0.6956863403320312, 'train/loss': 1.3694723844528198, 'train/bleu': 35.02622983467754, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 118485.5244398117, 'total_duration': 203421.36900758743, 'accumulated_submission_time': 118485.5244398117, 'accumulated_eval_time': 84918.95106649399, 'accumulated_logging_time': 6.728163719177246}
I0314 12:40:52.875162 139664575932160 logging_writer.py:48] [332945] accumulated_eval_time=84918.951066, accumulated_logging_time=6.728164, accumulated_submission_time=118485.524440, global_step=332945, preemption_count=0, score=118485.524440, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=203421.369008, train/accuracy=0.695686, train/bleu=35.026230, train/loss=1.369472, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 12:41:12.768611 139664567539456 logging_writer.py:48] [333000] global_step=333000, grad_norm=0.2217082679271698, loss=1.4261879920959473
I0314 12:41:48.335163 139664575932160 logging_writer.py:48] [333100] global_step=333100, grad_norm=0.23957358300685883, loss=1.46086585521698
I0314 12:42:23.920346 139664567539456 logging_writer.py:48] [333200] global_step=333200, grad_norm=0.23597106337547302, loss=1.4000240564346313
I0314 12:42:59.502295 139664575932160 logging_writer.py:48] [333300] global_step=333300, grad_norm=0.24154849350452423, loss=1.5015554428100586
I0314 12:43:35.143019 139664567539456 logging_writer.py:48] [333400] global_step=333400, grad_norm=0.2329634428024292, loss=1.462534785270691
I0314 12:44:10.710462 139664575932160 logging_writer.py:48] [333500] global_step=333500, grad_norm=0.22870545089244843, loss=1.4668359756469727
I0314 12:44:46.351229 139664567539456 logging_writer.py:48] [333600] global_step=333600, grad_norm=0.237871453166008, loss=1.4378728866577148
I0314 12:45:21.947193 139664575932160 logging_writer.py:48] [333700] global_step=333700, grad_norm=0.23083823919296265, loss=1.4209392070770264
I0314 12:45:57.560914 139664567539456 logging_writer.py:48] [333800] global_step=333800, grad_norm=0.2404347062110901, loss=1.4231525659561157
I0314 12:46:33.170748 139664575932160 logging_writer.py:48] [333900] global_step=333900, grad_norm=0.23116368055343628, loss=1.4279316663742065
I0314 12:47:08.782682 139664567539456 logging_writer.py:48] [334000] global_step=334000, grad_norm=0.23777110874652863, loss=1.4891678094863892
I0314 12:47:44.381150 139664575932160 logging_writer.py:48] [334100] global_step=334100, grad_norm=0.22381837666034698, loss=1.370191216468811
I0314 12:48:19.962917 139664567539456 logging_writer.py:48] [334200] global_step=334200, grad_norm=0.23702523112297058, loss=1.4392529726028442
I0314 12:48:55.572235 139664575932160 logging_writer.py:48] [334300] global_step=334300, grad_norm=0.22885280847549438, loss=1.479360580444336
I0314 12:49:31.182719 139664567539456 logging_writer.py:48] [334400] global_step=334400, grad_norm=0.23591722548007965, loss=1.4541289806365967
I0314 12:50:06.799455 139664575932160 logging_writer.py:48] [334500] global_step=334500, grad_norm=0.23359070718288422, loss=1.4179826974868774
I0314 12:50:42.454139 139664567539456 logging_writer.py:48] [334600] global_step=334600, grad_norm=0.23229040205478668, loss=1.5070996284484863
I0314 12:51:18.070903 139664575932160 logging_writer.py:48] [334700] global_step=334700, grad_norm=0.24321919679641724, loss=1.5274430513381958
I0314 12:51:53.663595 139664567539456 logging_writer.py:48] [334800] global_step=334800, grad_norm=0.23122070729732513, loss=1.4600409269332886
I0314 12:52:29.265062 139664575932160 logging_writer.py:48] [334900] global_step=334900, grad_norm=0.24958625435829163, loss=1.4169378280639648
I0314 12:53:04.872272 139664567539456 logging_writer.py:48] [335000] global_step=335000, grad_norm=0.22781388461589813, loss=1.3754111528396606
I0314 12:53:40.507277 139664575932160 logging_writer.py:48] [335100] global_step=335100, grad_norm=0.23226609826087952, loss=1.440392255783081
I0314 12:54:16.116770 139664567539456 logging_writer.py:48] [335200] global_step=335200, grad_norm=0.2287723422050476, loss=1.4643707275390625
I0314 12:54:51.708975 139664575932160 logging_writer.py:48] [335300] global_step=335300, grad_norm=0.22935521602630615, loss=1.3880380392074585
I0314 12:54:52.851464 139834281293632 spec.py:321] Evaluating on the training split.
I0314 12:54:55.832558 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 12:58:25.535130 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 12:58:28.209286 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:01:42.383100 139834281293632 spec.py:349] Evaluating on the test split.
I0314 13:01:45.055899 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:04:55.140816 139834281293632 submission_runner.py:420] Time since start: 204863.69s, 	Step: 335305, 	{'train/accuracy': 0.695704460144043, 'train/loss': 1.3688232898712158, 'train/bleu': 35.480177783502256, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 119325.42063117027, 'total_duration': 204863.69330143929, 'accumulated_submission_time': 119325.42063117027, 'accumulated_eval_time': 85521.2403678894, 'accumulated_logging_time': 6.795787334442139}
I0314 13:04:55.199718 139664567539456 logging_writer.py:48] [335305] accumulated_eval_time=85521.240368, accumulated_logging_time=6.795787, accumulated_submission_time=119325.420631, global_step=335305, preemption_count=0, score=119325.420631, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=204863.693301, train/accuracy=0.695704, train/bleu=35.480178, train/loss=1.368823, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 13:05:29.334010 139664575932160 logging_writer.py:48] [335400] global_step=335400, grad_norm=0.24077382683753967, loss=1.4941061735153198
I0314 13:06:04.990926 139664567539456 logging_writer.py:48] [335500] global_step=335500, grad_norm=0.23724758625030518, loss=1.3720811605453491
I0314 13:06:40.575288 139664575932160 logging_writer.py:48] [335600] global_step=335600, grad_norm=0.2299654334783554, loss=1.3949575424194336
I0314 13:07:16.147746 139664567539456 logging_writer.py:48] [335700] global_step=335700, grad_norm=0.23533575236797333, loss=1.4643352031707764
I0314 13:07:51.719641 139664575932160 logging_writer.py:48] [335800] global_step=335800, grad_norm=0.23865744471549988, loss=1.5082224607467651
I0314 13:08:27.339963 139664567539456 logging_writer.py:48] [335900] global_step=335900, grad_norm=0.23112045228481293, loss=1.4532136917114258
I0314 13:09:02.982849 139664575932160 logging_writer.py:48] [336000] global_step=336000, grad_norm=0.24566295742988586, loss=1.4741792678833008
I0314 13:09:38.631071 139664567539456 logging_writer.py:48] [336100] global_step=336100, grad_norm=0.22845298051834106, loss=1.406799077987671
I0314 13:10:14.272001 139664575932160 logging_writer.py:48] [336200] global_step=336200, grad_norm=0.2301398664712906, loss=1.3731751441955566
I0314 13:10:49.933823 139664567539456 logging_writer.py:48] [336300] global_step=336300, grad_norm=0.22834962606430054, loss=1.4295716285705566
I0314 13:11:25.532014 139664575932160 logging_writer.py:48] [336400] global_step=336400, grad_norm=0.2419636845588684, loss=1.4595164060592651
I0314 13:12:01.136482 139664567539456 logging_writer.py:48] [336500] global_step=336500, grad_norm=0.22769668698310852, loss=1.3762882947921753
I0314 13:12:36.732316 139664575932160 logging_writer.py:48] [336600] global_step=336600, grad_norm=0.22932937741279602, loss=1.3646382093429565
I0314 13:13:12.325294 139664567539456 logging_writer.py:48] [336700] global_step=336700, grad_norm=0.22814474999904633, loss=1.4126002788543701
I0314 13:13:47.974980 139664575932160 logging_writer.py:48] [336800] global_step=336800, grad_norm=0.23333652317523956, loss=1.3885035514831543
I0314 13:14:23.567653 139664567539456 logging_writer.py:48] [336900] global_step=336900, grad_norm=0.23017822206020355, loss=1.4891597032546997
I0314 13:14:59.170801 139664575932160 logging_writer.py:48] [337000] global_step=337000, grad_norm=0.23113736510276794, loss=1.4405312538146973
I0314 13:15:34.753378 139664567539456 logging_writer.py:48] [337100] global_step=337100, grad_norm=0.22679339349269867, loss=1.3958210945129395
I0314 13:16:10.325200 139664575932160 logging_writer.py:48] [337200] global_step=337200, grad_norm=0.22364214062690735, loss=1.362975835800171
I0314 13:16:45.902654 139664567539456 logging_writer.py:48] [337300] global_step=337300, grad_norm=0.23610836267471313, loss=1.514827847480774
I0314 13:17:21.513777 139664575932160 logging_writer.py:48] [337400] global_step=337400, grad_norm=0.23865321278572083, loss=1.5147075653076172
I0314 13:17:57.065938 139664567539456 logging_writer.py:48] [337500] global_step=337500, grad_norm=0.24143964052200317, loss=1.412692904472351
I0314 13:18:32.662767 139664575932160 logging_writer.py:48] [337600] global_step=337600, grad_norm=0.23812973499298096, loss=1.4996070861816406
I0314 13:18:55.149679 139834281293632 spec.py:321] Evaluating on the training split.
I0314 13:18:58.140802 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:22:25.544775 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 13:22:28.213873 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:25:42.153880 139834281293632 spec.py:349] Evaluating on the test split.
I0314 13:25:44.818175 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:28:54.560288 139834281293632 submission_runner.py:420] Time since start: 206303.11s, 	Step: 337665, 	{'train/accuracy': 0.6960183382034302, 'train/loss': 1.3696424961090088, 'train/bleu': 35.55197694210756, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 120165.28549027443, 'total_duration': 206303.11276698112, 'accumulated_submission_time': 120165.28549027443, 'accumulated_eval_time': 86120.65092658997, 'accumulated_logging_time': 6.865016460418701}
I0314 13:28:54.620346 139664567539456 logging_writer.py:48] [337665] accumulated_eval_time=86120.650927, accumulated_logging_time=6.865016, accumulated_submission_time=120165.285490, global_step=337665, preemption_count=0, score=120165.285490, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=206303.112767, train/accuracy=0.696018, train/bleu=35.551977, train/loss=1.369642, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 13:29:07.416208 139664575932160 logging_writer.py:48] [337700] global_step=337700, grad_norm=0.23742365837097168, loss=1.5313570499420166
I0314 13:29:42.948879 139664567539456 logging_writer.py:48] [337800] global_step=337800, grad_norm=0.23567315936088562, loss=1.4221080541610718
I0314 13:30:18.554115 139664575932160 logging_writer.py:48] [337900] global_step=337900, grad_norm=0.22893573343753815, loss=1.4298664331436157
I0314 13:30:54.136922 139664567539456 logging_writer.py:48] [338000] global_step=338000, grad_norm=0.2432551234960556, loss=1.431930422782898
I0314 13:31:29.716036 139664575932160 logging_writer.py:48] [338100] global_step=338100, grad_norm=0.2325337529182434, loss=1.4227582216262817
I0314 13:32:05.302171 139664567539456 logging_writer.py:48] [338200] global_step=338200, grad_norm=0.23847998678684235, loss=1.4874807596206665
I0314 13:32:40.894843 139664575932160 logging_writer.py:48] [338300] global_step=338300, grad_norm=0.22991535067558289, loss=1.4381784200668335
I0314 13:33:16.500145 139664567539456 logging_writer.py:48] [338400] global_step=338400, grad_norm=0.23764127492904663, loss=1.4699889421463013
I0314 13:33:52.135420 139664575932160 logging_writer.py:48] [338500] global_step=338500, grad_norm=0.23428252339363098, loss=1.4336366653442383
I0314 13:34:27.733480 139664567539456 logging_writer.py:48] [338600] global_step=338600, grad_norm=0.23981352150440216, loss=1.4716283082962036
I0314 13:35:03.355488 139664575932160 logging_writer.py:48] [338700] global_step=338700, grad_norm=0.21839196979999542, loss=1.4278454780578613
I0314 13:35:38.916316 139664567539456 logging_writer.py:48] [338800] global_step=338800, grad_norm=0.23081552982330322, loss=1.438635230064392
I0314 13:36:14.512176 139664575932160 logging_writer.py:48] [338900] global_step=338900, grad_norm=0.23028653860092163, loss=1.3983571529388428
I0314 13:36:50.145065 139664567539456 logging_writer.py:48] [339000] global_step=339000, grad_norm=0.23608678579330444, loss=1.4032353162765503
I0314 13:37:25.783938 139664575932160 logging_writer.py:48] [339100] global_step=339100, grad_norm=0.23445314168930054, loss=1.4349011182785034
I0314 13:38:01.437069 139664567539456 logging_writer.py:48] [339200] global_step=339200, grad_norm=0.226888507604599, loss=1.3894470930099487
I0314 13:38:37.071541 139664575932160 logging_writer.py:48] [339300] global_step=339300, grad_norm=0.24532988667488098, loss=1.5522624254226685
I0314 13:39:12.667313 139664567539456 logging_writer.py:48] [339400] global_step=339400, grad_norm=0.23818162083625793, loss=1.4593240022659302
I0314 13:39:48.275985 139664575932160 logging_writer.py:48] [339500] global_step=339500, grad_norm=0.2268156260251999, loss=1.528720498085022
I0314 13:40:23.851917 139664567539456 logging_writer.py:48] [339600] global_step=339600, grad_norm=0.23758183419704437, loss=1.3667429685592651
I0314 13:40:59.456193 139664575932160 logging_writer.py:48] [339700] global_step=339700, grad_norm=0.23632194101810455, loss=1.4428131580352783
I0314 13:41:35.067691 139664567539456 logging_writer.py:48] [339800] global_step=339800, grad_norm=0.22947782278060913, loss=1.4789819717407227
I0314 13:42:10.674630 139664575932160 logging_writer.py:48] [339900] global_step=339900, grad_norm=0.23653458058834076, loss=1.4874738454818726
I0314 13:42:46.303349 139664567539456 logging_writer.py:48] [340000] global_step=340000, grad_norm=0.231891468167305, loss=1.38834547996521
I0314 13:42:54.567206 139834281293632 spec.py:321] Evaluating on the training split.
I0314 13:42:57.556869 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:46:45.360046 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 13:46:48.055658 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:50:03.187170 139834281293632 spec.py:349] Evaluating on the test split.
I0314 13:50:05.858890 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 13:53:15.681237 139834281293632 submission_runner.py:420] Time since start: 207764.23s, 	Step: 340025, 	{'train/accuracy': 0.6936178803443909, 'train/loss': 1.38359534740448, 'train/bleu': 35.268918831625335, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 121005.14982128143, 'total_duration': 207764.23371696472, 'accumulated_submission_time': 121005.14982128143, 'accumulated_eval_time': 86741.76490736008, 'accumulated_logging_time': 6.934324264526367}
I0314 13:53:15.739069 139664575932160 logging_writer.py:48] [340025] accumulated_eval_time=86741.764907, accumulated_logging_time=6.934324, accumulated_submission_time=121005.149821, global_step=340025, preemption_count=0, score=121005.149821, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=207764.233717, train/accuracy=0.693618, train/bleu=35.268919, train/loss=1.383595, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 13:53:42.746320 139664567539456 logging_writer.py:48] [340100] global_step=340100, grad_norm=0.23487257957458496, loss=1.363154411315918
I0314 13:54:18.275213 139664575932160 logging_writer.py:48] [340200] global_step=340200, grad_norm=0.23645462095737457, loss=1.4769307374954224
I0314 13:54:53.847454 139664567539456 logging_writer.py:48] [340300] global_step=340300, grad_norm=0.2246568650007248, loss=1.3829374313354492
I0314 13:55:29.436997 139664575932160 logging_writer.py:48] [340400] global_step=340400, grad_norm=0.2387618124485016, loss=1.4089092016220093
I0314 13:56:05.059351 139664567539456 logging_writer.py:48] [340500] global_step=340500, grad_norm=0.23693442344665527, loss=1.4583475589752197
I0314 13:56:40.672504 139664575932160 logging_writer.py:48] [340600] global_step=340600, grad_norm=0.2341284155845642, loss=1.4506492614746094
I0314 13:57:16.297361 139664567539456 logging_writer.py:48] [340700] global_step=340700, grad_norm=0.2351776510477066, loss=1.4347662925720215
I0314 13:57:51.914141 139664575932160 logging_writer.py:48] [340800] global_step=340800, grad_norm=0.23919707536697388, loss=1.4228119850158691
I0314 13:58:27.533150 139664567539456 logging_writer.py:48] [340900] global_step=340900, grad_norm=0.2293531745672226, loss=1.4221187829971313
I0314 13:59:03.156841 139664575932160 logging_writer.py:48] [341000] global_step=341000, grad_norm=0.22815868258476257, loss=1.4366828203201294
I0314 13:59:38.800954 139664567539456 logging_writer.py:48] [341100] global_step=341100, grad_norm=0.21731877326965332, loss=1.392654299736023
I0314 14:00:14.411610 139664575932160 logging_writer.py:48] [341200] global_step=341200, grad_norm=0.24339868128299713, loss=1.4908368587493896
I0314 14:00:50.036613 139664567539456 logging_writer.py:48] [341300] global_step=341300, grad_norm=0.2402232438325882, loss=1.3924717903137207
I0314 14:01:25.624002 139664575932160 logging_writer.py:48] [341400] global_step=341400, grad_norm=0.2330593764781952, loss=1.4513297080993652
I0314 14:02:01.302464 139664567539456 logging_writer.py:48] [341500] global_step=341500, grad_norm=0.2500007748603821, loss=1.4634134769439697
I0314 14:02:36.948904 139664575932160 logging_writer.py:48] [341600] global_step=341600, grad_norm=0.2299143373966217, loss=1.4630464315414429
I0314 14:03:12.571757 139664567539456 logging_writer.py:48] [341700] global_step=341700, grad_norm=0.2361592799425125, loss=1.4299665689468384
I0314 14:03:48.157355 139664575932160 logging_writer.py:48] [341800] global_step=341800, grad_norm=0.22988736629486084, loss=1.4202096462249756
I0314 14:04:23.770336 139664567539456 logging_writer.py:48] [341900] global_step=341900, grad_norm=0.23155473172664642, loss=1.4151338338851929
I0314 14:04:59.344820 139664575932160 logging_writer.py:48] [342000] global_step=342000, grad_norm=0.23358562588691711, loss=1.485675573348999
I0314 14:05:34.972846 139664567539456 logging_writer.py:48] [342100] global_step=342100, grad_norm=0.2362111657857895, loss=1.4380513429641724
I0314 14:06:10.572486 139664575932160 logging_writer.py:48] [342200] global_step=342200, grad_norm=0.23340155184268951, loss=1.4753369092941284
I0314 14:06:46.179254 139664567539456 logging_writer.py:48] [342300] global_step=342300, grad_norm=0.23213529586791992, loss=1.390061378479004
I0314 14:07:15.810719 139834281293632 spec.py:321] Evaluating on the training split.
I0314 14:07:18.783589 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:10:52.778241 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 14:10:55.456233 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:14:09.870677 139834281293632 spec.py:349] Evaluating on the test split.
I0314 14:14:12.553160 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:17:22.565410 139834281293632 submission_runner.py:420] Time since start: 209211.12s, 	Step: 342385, 	{'train/accuracy': 0.6969581842422485, 'train/loss': 1.3596528768539429, 'train/bleu': 35.45381204623339, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 121845.14006876945, 'total_duration': 209211.1178805828, 'accumulated_submission_time': 121845.14006876945, 'accumulated_eval_time': 87348.51953816414, 'accumulated_logging_time': 7.001046895980835}
I0314 14:17:22.624395 139664575932160 logging_writer.py:48] [342385] accumulated_eval_time=87348.519538, accumulated_logging_time=7.001047, accumulated_submission_time=121845.140069, global_step=342385, preemption_count=0, score=121845.140069, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=209211.117881, train/accuracy=0.696958, train/bleu=35.453812, train/loss=1.359653, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 14:17:28.332769 139664567539456 logging_writer.py:48] [342400] global_step=342400, grad_norm=0.23901008069515228, loss=1.4307301044464111
I0314 14:18:03.853531 139664575932160 logging_writer.py:48] [342500] global_step=342500, grad_norm=0.23001238703727722, loss=1.360381007194519
I0314 14:18:39.461896 139664567539456 logging_writer.py:48] [342600] global_step=342600, grad_norm=0.23595327138900757, loss=1.5556265115737915
I0314 14:19:15.050675 139664575932160 logging_writer.py:48] [342700] global_step=342700, grad_norm=0.2328023463487625, loss=1.4641988277435303
I0314 14:19:50.649917 139664567539456 logging_writer.py:48] [342800] global_step=342800, grad_norm=0.2349015325307846, loss=1.4575161933898926
I0314 14:20:26.256611 139664575932160 logging_writer.py:48] [342900] global_step=342900, grad_norm=0.23641233146190643, loss=1.484006643295288
I0314 14:21:01.876728 139664567539456 logging_writer.py:48] [343000] global_step=343000, grad_norm=0.23092952370643616, loss=1.4253060817718506
I0314 14:21:37.457382 139664575932160 logging_writer.py:48] [343100] global_step=343100, grad_norm=0.2441757172346115, loss=1.42848801612854
I0314 14:22:13.049741 139664567539456 logging_writer.py:48] [343200] global_step=343200, grad_norm=0.246095210313797, loss=1.4576345682144165
I0314 14:22:48.632276 139664575932160 logging_writer.py:48] [343300] global_step=343300, grad_norm=0.24656546115875244, loss=1.4191337823867798
I0314 14:23:24.270588 139664567539456 logging_writer.py:48] [343400] global_step=343400, grad_norm=0.22839362919330597, loss=1.3892953395843506
I0314 14:23:59.861137 139664575932160 logging_writer.py:48] [343500] global_step=343500, grad_norm=0.230520561337471, loss=1.4087769985198975
I0314 14:24:35.452551 139664567539456 logging_writer.py:48] [343600] global_step=343600, grad_norm=0.2251703143119812, loss=1.424957513809204
I0314 14:25:11.060913 139664575932160 logging_writer.py:48] [343700] global_step=343700, grad_norm=0.24116043746471405, loss=1.415158748626709
I0314 14:25:46.665560 139664567539456 logging_writer.py:48] [343800] global_step=343800, grad_norm=0.23110869526863098, loss=1.4179449081420898
I0314 14:26:22.263567 139664575932160 logging_writer.py:48] [343900] global_step=343900, grad_norm=0.22643867135047913, loss=1.3791464567184448
I0314 14:26:57.853081 139664567539456 logging_writer.py:48] [344000] global_step=344000, grad_norm=0.22992315888404846, loss=1.4822412729263306
I0314 14:27:33.457370 139664575932160 logging_writer.py:48] [344100] global_step=344100, grad_norm=0.2217327505350113, loss=1.4124400615692139
I0314 14:28:09.047452 139664567539456 logging_writer.py:48] [344200] global_step=344200, grad_norm=0.23147685825824738, loss=1.4134330749511719
I0314 14:28:44.618979 139664575932160 logging_writer.py:48] [344300] global_step=344300, grad_norm=0.23156511783599854, loss=1.47739839553833
I0314 14:29:20.246349 139664567539456 logging_writer.py:48] [344400] global_step=344400, grad_norm=0.22587637603282928, loss=1.5250250101089478
I0314 14:29:55.838471 139664575932160 logging_writer.py:48] [344500] global_step=344500, grad_norm=0.2310432493686676, loss=1.4976781606674194
I0314 14:30:31.442890 139664567539456 logging_writer.py:48] [344600] global_step=344600, grad_norm=0.2385401427745819, loss=1.360302209854126
I0314 14:31:07.028358 139664575932160 logging_writer.py:48] [344700] global_step=344700, grad_norm=0.23125089704990387, loss=1.4362914562225342
I0314 14:31:22.775992 139834281293632 spec.py:321] Evaluating on the training split.
I0314 14:31:25.767473 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:34:59.777211 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 14:35:02.462583 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:38:16.660924 139834281293632 spec.py:349] Evaluating on the test split.
I0314 14:38:19.347136 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:41:30.885511 139834281293632 submission_runner.py:420] Time since start: 210659.44s, 	Step: 344746, 	{'train/accuracy': 0.694857656955719, 'train/loss': 1.3747631311416626, 'train/bleu': 35.20722528657935, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 122685.2105679512, 'total_duration': 210659.43799066544, 'accumulated_submission_time': 122685.2105679512, 'accumulated_eval_time': 87956.62901735306, 'accumulated_logging_time': 7.070030212402344}
I0314 14:41:30.946188 139664567539456 logging_writer.py:48] [344746] accumulated_eval_time=87956.629017, accumulated_logging_time=7.070030, accumulated_submission_time=122685.210568, global_step=344746, preemption_count=0, score=122685.210568, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=210659.437991, train/accuracy=0.694858, train/bleu=35.207225, train/loss=1.374763, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 14:41:50.508429 139664575932160 logging_writer.py:48] [344800] global_step=344800, grad_norm=0.23911292850971222, loss=1.4725117683410645
I0314 14:42:26.042995 139664567539456 logging_writer.py:48] [344900] global_step=344900, grad_norm=0.22946982085704803, loss=1.4724903106689453
I0314 14:43:01.595867 139664575932160 logging_writer.py:48] [345000] global_step=345000, grad_norm=0.23489652574062347, loss=1.454961895942688
I0314 14:43:37.200578 139664567539456 logging_writer.py:48] [345100] global_step=345100, grad_norm=0.22809059917926788, loss=1.3864504098892212
I0314 14:44:12.812277 139664575932160 logging_writer.py:48] [345200] global_step=345200, grad_norm=0.2251039743423462, loss=1.431159257888794
I0314 14:44:48.426457 139664567539456 logging_writer.py:48] [345300] global_step=345300, grad_norm=0.2444457709789276, loss=1.4809595346450806
I0314 14:45:24.089145 139664575932160 logging_writer.py:48] [345400] global_step=345400, grad_norm=0.23006410896778107, loss=1.473028302192688
I0314 14:45:59.672699 139664567539456 logging_writer.py:48] [345500] global_step=345500, grad_norm=0.2489643543958664, loss=1.5093721151351929
I0314 14:46:35.302318 139664575932160 logging_writer.py:48] [345600] global_step=345600, grad_norm=0.2353886365890503, loss=1.4190455675125122
I0314 14:47:10.908831 139664567539456 logging_writer.py:48] [345700] global_step=345700, grad_norm=0.23313018679618835, loss=1.458074688911438
I0314 14:47:46.523330 139664575932160 logging_writer.py:48] [345800] global_step=345800, grad_norm=0.23822543025016785, loss=1.5297791957855225
I0314 14:48:22.100736 139664567539456 logging_writer.py:48] [345900] global_step=345900, grad_norm=0.2299090474843979, loss=1.4202022552490234
I0314 14:48:57.736087 139664575932160 logging_writer.py:48] [346000] global_step=346000, grad_norm=0.23694176971912384, loss=1.4950212240219116
I0314 14:49:33.286643 139664567539456 logging_writer.py:48] [346100] global_step=346100, grad_norm=0.23837953805923462, loss=1.438772439956665
I0314 14:50:08.921767 139664575932160 logging_writer.py:48] [346200] global_step=346200, grad_norm=0.2253599315881729, loss=1.4503059387207031
I0314 14:50:44.531727 139664567539456 logging_writer.py:48] [346300] global_step=346300, grad_norm=0.2357751876115799, loss=1.452544927597046
I0314 14:51:20.132009 139664575932160 logging_writer.py:48] [346400] global_step=346400, grad_norm=0.24156533181667328, loss=1.4313886165618896
I0314 14:51:55.727895 139664567539456 logging_writer.py:48] [346500] global_step=346500, grad_norm=0.22682592272758484, loss=1.399760365486145
I0314 14:52:31.316618 139664575932160 logging_writer.py:48] [346600] global_step=346600, grad_norm=0.2459908276796341, loss=1.478706955909729
I0314 14:53:06.917154 139664567539456 logging_writer.py:48] [346700] global_step=346700, grad_norm=0.23115767538547516, loss=1.4876000881195068
I0314 14:53:42.483272 139664575932160 logging_writer.py:48] [346800] global_step=346800, grad_norm=0.23304985463619232, loss=1.4550844430923462
I0314 14:54:18.088726 139664567539456 logging_writer.py:48] [346900] global_step=346900, grad_norm=0.24243029952049255, loss=1.4710522890090942
I0314 14:54:53.670974 139664575932160 logging_writer.py:48] [347000] global_step=347000, grad_norm=0.23416586220264435, loss=1.4278258085250854
I0314 14:55:29.273381 139664567539456 logging_writer.py:48] [347100] global_step=347100, grad_norm=0.23320552706718445, loss=1.4322891235351562
I0314 14:55:31.123767 139834281293632 spec.py:321] Evaluating on the training split.
I0314 14:55:34.093351 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 14:59:11.328181 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 14:59:14.005050 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:02:29.757291 139834281293632 spec.py:349] Evaluating on the test split.
I0314 15:02:32.433506 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:05:42.065962 139834281293632 submission_runner.py:420] Time since start: 212110.62s, 	Step: 347107, 	{'train/accuracy': 0.6944318413734436, 'train/loss': 1.3756641149520874, 'train/bleu': 35.670044646723696, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 123525.30701708794, 'total_duration': 212110.61843252182, 'accumulated_submission_time': 123525.30701708794, 'accumulated_eval_time': 88567.57115674019, 'accumulated_logging_time': 7.1397809982299805}
I0314 15:05:42.125640 139664575932160 logging_writer.py:48] [347107] accumulated_eval_time=88567.571157, accumulated_logging_time=7.139781, accumulated_submission_time=123525.307017, global_step=347107, preemption_count=0, score=123525.307017, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=212110.618433, train/accuracy=0.694432, train/bleu=35.670045, train/loss=1.375664, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 15:06:15.564511 139664567539456 logging_writer.py:48] [347200] global_step=347200, grad_norm=0.23544687032699585, loss=1.4491468667984009
I0314 15:06:51.127136 139664575932160 logging_writer.py:48] [347300] global_step=347300, grad_norm=0.2259572446346283, loss=1.4357103109359741
I0314 15:07:26.716107 139664567539456 logging_writer.py:48] [347400] global_step=347400, grad_norm=0.23684686422348022, loss=1.4684470891952515
I0314 15:08:02.340694 139664575932160 logging_writer.py:48] [347500] global_step=347500, grad_norm=0.2319268435239792, loss=1.47427499294281
I0314 15:08:37.915543 139664567539456 logging_writer.py:48] [347600] global_step=347600, grad_norm=0.2417842149734497, loss=1.418976068496704
I0314 15:09:13.542205 139664575932160 logging_writer.py:48] [347700] global_step=347700, grad_norm=0.23844735324382782, loss=1.5057775974273682
I0314 15:09:49.175275 139664567539456 logging_writer.py:48] [347800] global_step=347800, grad_norm=0.2301873415708542, loss=1.3696057796478271
I0314 15:10:24.778976 139664575932160 logging_writer.py:48] [347900] global_step=347900, grad_norm=0.2386179268360138, loss=1.4890673160552979
I0314 15:11:00.369204 139664567539456 logging_writer.py:48] [348000] global_step=348000, grad_norm=0.24408608675003052, loss=1.4752049446105957
I0314 15:11:35.946137 139664575932160 logging_writer.py:48] [348100] global_step=348100, grad_norm=0.23440945148468018, loss=1.481602668762207
I0314 15:12:11.553330 139664567539456 logging_writer.py:48] [348200] global_step=348200, grad_norm=0.23155833780765533, loss=1.4773383140563965
I0314 15:12:47.141843 139664575932160 logging_writer.py:48] [348300] global_step=348300, grad_norm=0.22562935948371887, loss=1.4202961921691895
I0314 15:13:22.716396 139664567539456 logging_writer.py:48] [348400] global_step=348400, grad_norm=0.23186253011226654, loss=1.428178310394287
I0314 15:13:58.325870 139664575932160 logging_writer.py:48] [348500] global_step=348500, grad_norm=0.23351316154003143, loss=1.3982216119766235
I0314 15:14:33.951911 139664567539456 logging_writer.py:48] [348600] global_step=348600, grad_norm=0.23999683558940887, loss=1.4583820104599
I0314 15:15:09.573662 139664575932160 logging_writer.py:48] [348700] global_step=348700, grad_norm=0.24884310364723206, loss=1.5062060356140137
I0314 15:15:45.219034 139664567539456 logging_writer.py:48] [348800] global_step=348800, grad_norm=0.2301478087902069, loss=1.4432238340377808
I0314 15:16:20.817461 139664575932160 logging_writer.py:48] [348900] global_step=348900, grad_norm=0.23971302807331085, loss=1.4652618169784546
I0314 15:16:56.419314 139664567539456 logging_writer.py:48] [349000] global_step=349000, grad_norm=0.22783814370632172, loss=1.4589954614639282
I0314 15:17:32.028885 139664575932160 logging_writer.py:48] [349100] global_step=349100, grad_norm=0.22633539140224457, loss=1.3520241975784302
I0314 15:18:07.626347 139664567539456 logging_writer.py:48] [349200] global_step=349200, grad_norm=0.2350437343120575, loss=1.3867039680480957
I0314 15:18:43.260075 139664575932160 logging_writer.py:48] [349300] global_step=349300, grad_norm=0.24924717843532562, loss=1.492732286453247
I0314 15:19:18.970938 139664567539456 logging_writer.py:48] [349400] global_step=349400, grad_norm=0.2270427793264389, loss=1.438613772392273
I0314 15:19:42.203547 139834281293632 spec.py:321] Evaluating on the training split.
I0314 15:19:45.183617 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:23:08.240964 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 15:23:10.917110 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:26:25.159940 139834281293632 spec.py:349] Evaluating on the test split.
I0314 15:26:27.842798 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:29:37.859902 139834281293632 submission_runner.py:420] Time since start: 213546.41s, 	Step: 349467, 	{'train/accuracy': 0.6982336044311523, 'train/loss': 1.3613815307617188, 'train/bleu': 35.713977052649426, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 124365.3011341095, 'total_duration': 213546.41238355637, 'accumulated_submission_time': 124365.3011341095, 'accumulated_eval_time': 89163.2274723053, 'accumulated_logging_time': 7.2094714641571045}
I0314 15:29:37.921352 139664575932160 logging_writer.py:48] [349467] accumulated_eval_time=89163.227472, accumulated_logging_time=7.209471, accumulated_submission_time=124365.301134, global_step=349467, preemption_count=0, score=124365.301134, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=213546.412384, train/accuracy=0.698234, train/bleu=35.713977, train/loss=1.361382, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 15:29:50.009905 139664567539456 logging_writer.py:48] [349500] global_step=349500, grad_norm=0.2260161191225052, loss=1.446612000465393
I0314 15:30:25.580313 139664575932160 logging_writer.py:48] [349600] global_step=349600, grad_norm=0.22312812507152557, loss=1.4118396043777466
I0314 15:31:01.211325 139664567539456 logging_writer.py:48] [349700] global_step=349700, grad_norm=0.23332791030406952, loss=1.4271031618118286
I0314 15:31:36.873673 139664575932160 logging_writer.py:48] [349800] global_step=349800, grad_norm=0.2292075753211975, loss=1.4119834899902344
I0314 15:32:12.504912 139664567539456 logging_writer.py:48] [349900] global_step=349900, grad_norm=0.23759502172470093, loss=1.5169953107833862
I0314 15:32:48.091562 139664575932160 logging_writer.py:48] [350000] global_step=350000, grad_norm=0.23659293353557587, loss=1.405645489692688
I0314 15:33:23.685057 139664567539456 logging_writer.py:48] [350100] global_step=350100, grad_norm=0.23297443985939026, loss=1.4574264287948608
I0314 15:33:59.281308 139664575932160 logging_writer.py:48] [350200] global_step=350200, grad_norm=0.23906826972961426, loss=1.3832041025161743
I0314 15:34:34.942211 139664567539456 logging_writer.py:48] [350300] global_step=350300, grad_norm=0.24490228295326233, loss=1.5125808715820312
I0314 15:35:10.568637 139664575932160 logging_writer.py:48] [350400] global_step=350400, grad_norm=0.23948918282985687, loss=1.477305293083191
I0314 15:35:46.174838 139664567539456 logging_writer.py:48] [350500] global_step=350500, grad_norm=0.2444974035024643, loss=1.4825836420059204
I0314 15:36:21.816472 139664575932160 logging_writer.py:48] [350600] global_step=350600, grad_norm=0.23775871098041534, loss=1.4364222288131714
I0314 15:36:57.437924 139664567539456 logging_writer.py:48] [350700] global_step=350700, grad_norm=0.23022228479385376, loss=1.4764407873153687
I0314 15:37:33.085218 139664575932160 logging_writer.py:48] [350800] global_step=350800, grad_norm=0.235788494348526, loss=1.4764652252197266
I0314 15:38:08.762242 139664567539456 logging_writer.py:48] [350900] global_step=350900, grad_norm=0.22969089448451996, loss=1.4332467317581177
I0314 15:38:44.355337 139664575932160 logging_writer.py:48] [351000] global_step=351000, grad_norm=0.29068613052368164, loss=1.4245291948318481
I0314 15:39:19.971504 139664567539456 logging_writer.py:48] [351100] global_step=351100, grad_norm=0.23639915883541107, loss=1.4086802005767822
I0314 15:39:55.583712 139664575932160 logging_writer.py:48] [351200] global_step=351200, grad_norm=0.2418869286775589, loss=1.5082648992538452
I0314 15:40:31.180611 139664567539456 logging_writer.py:48] [351300] global_step=351300, grad_norm=0.22759981453418732, loss=1.421364426612854
I0314 15:41:06.771318 139664575932160 logging_writer.py:48] [351400] global_step=351400, grad_norm=0.23232285678386688, loss=1.4839577674865723
I0314 15:41:42.373963 139664567539456 logging_writer.py:48] [351500] global_step=351500, grad_norm=0.23765712976455688, loss=1.4795498847961426
I0314 15:42:17.978049 139664575932160 logging_writer.py:48] [351600] global_step=351600, grad_norm=0.23242011666297913, loss=1.4232536554336548
I0314 15:42:53.601485 139664567539456 logging_writer.py:48] [351700] global_step=351700, grad_norm=0.23550854623317719, loss=1.4839810132980347
I0314 15:43:29.201419 139664575932160 logging_writer.py:48] [351800] global_step=351800, grad_norm=0.2235676795244217, loss=1.4369866847991943
I0314 15:43:38.171953 139834281293632 spec.py:321] Evaluating on the training split.
I0314 15:43:41.143665 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:47:15.280615 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 15:47:17.957800 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:50:31.699572 139834281293632 spec.py:349] Evaluating on the test split.
I0314 15:50:34.365584 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 15:53:44.151423 139834281293632 submission_runner.py:420] Time since start: 214992.70s, 	Step: 351827, 	{'train/accuracy': 0.6962584853172302, 'train/loss': 1.3689239025115967, 'train/bleu': 35.62077978803877, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 125205.46908950806, 'total_duration': 214992.70389819145, 'accumulated_submission_time': 125205.46908950806, 'accumulated_eval_time': 89769.20688509941, 'accumulated_logging_time': 7.279955148696899}
I0314 15:53:44.211773 139664567539456 logging_writer.py:48] [351827] accumulated_eval_time=89769.206885, accumulated_logging_time=7.279955, accumulated_submission_time=125205.469090, global_step=351827, preemption_count=0, score=125205.469090, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=214992.703898, train/accuracy=0.696258, train/bleu=35.620780, train/loss=1.368924, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 15:54:10.519103 139664575932160 logging_writer.py:48] [351900] global_step=351900, grad_norm=0.232425719499588, loss=1.5367344617843628
I0314 15:54:46.062286 139664567539456 logging_writer.py:48] [352000] global_step=352000, grad_norm=0.22537687420845032, loss=1.4213759899139404
I0314 15:55:21.631371 139664575932160 logging_writer.py:48] [352100] global_step=352100, grad_norm=0.2307196408510208, loss=1.4279377460479736
I0314 15:55:57.200605 139664567539456 logging_writer.py:48] [352200] global_step=352200, grad_norm=0.2360943853855133, loss=1.5567543506622314
I0314 15:56:32.796770 139664575932160 logging_writer.py:48] [352300] global_step=352300, grad_norm=0.23610953986644745, loss=1.4207360744476318
I0314 15:57:08.378042 139664567539456 logging_writer.py:48] [352400] global_step=352400, grad_norm=0.2333453893661499, loss=1.389083743095398
I0314 15:57:43.951575 139664575932160 logging_writer.py:48] [352500] global_step=352500, grad_norm=0.23339810967445374, loss=1.5464777946472168
I0314 15:58:19.554297 139664567539456 logging_writer.py:48] [352600] global_step=352600, grad_norm=0.2264305055141449, loss=1.4102884531021118
I0314 15:58:55.138644 139664575932160 logging_writer.py:48] [352700] global_step=352700, grad_norm=0.2557099759578705, loss=1.499969244003296
I0314 15:59:30.710381 139664567539456 logging_writer.py:48] [352800] global_step=352800, grad_norm=0.23595209419727325, loss=1.4829003810882568
I0314 16:00:06.298263 139664575932160 logging_writer.py:48] [352900] global_step=352900, grad_norm=0.24441854655742645, loss=1.4254083633422852
I0314 16:00:41.856771 139664567539456 logging_writer.py:48] [353000] global_step=353000, grad_norm=0.24411270022392273, loss=1.4812517166137695
I0314 16:01:17.424472 139664575932160 logging_writer.py:48] [353100] global_step=353100, grad_norm=0.23723317682743073, loss=1.3810182809829712
I0314 16:01:52.989514 139664567539456 logging_writer.py:48] [353200] global_step=353200, grad_norm=0.2366502583026886, loss=1.5007569789886475
I0314 16:02:28.593284 139664575932160 logging_writer.py:48] [353300] global_step=353300, grad_norm=0.22589223086833954, loss=1.3724616765975952
I0314 16:03:04.185876 139664567539456 logging_writer.py:48] [353400] global_step=353400, grad_norm=0.23385089635849, loss=1.4311045408248901
I0314 16:03:39.780424 139664575932160 logging_writer.py:48] [353500] global_step=353500, grad_norm=0.23818998038768768, loss=1.4448543787002563
I0314 16:04:15.385822 139664567539456 logging_writer.py:48] [353600] global_step=353600, grad_norm=0.2268170565366745, loss=1.3973780870437622
I0314 16:04:50.975394 139664575932160 logging_writer.py:48] [353700] global_step=353700, grad_norm=0.22460466623306274, loss=1.393646240234375
I0314 16:05:26.568445 139664567539456 logging_writer.py:48] [353800] global_step=353800, grad_norm=0.22955843806266785, loss=1.3961119651794434
I0314 16:06:02.146145 139664575932160 logging_writer.py:48] [353900] global_step=353900, grad_norm=0.23018255829811096, loss=1.4688342809677124
I0314 16:06:37.767794 139664567539456 logging_writer.py:48] [354000] global_step=354000, grad_norm=0.23873071372509003, loss=1.500056266784668
I0314 16:07:13.353329 139664575932160 logging_writer.py:48] [354100] global_step=354100, grad_norm=0.23058807849884033, loss=1.450989007949829
I0314 16:07:44.371563 139834281293632 spec.py:321] Evaluating on the training split.
I0314 16:07:47.339452 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:11:16.921232 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 16:11:19.604552 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:14:33.687921 139834281293632 spec.py:349] Evaluating on the test split.
I0314 16:14:36.374638 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:17:46.182728 139834281293632 submission_runner.py:420] Time since start: 216434.74s, 	Step: 354189, 	{'train/accuracy': 0.6944153308868408, 'train/loss': 1.3725401163101196, 'train/bleu': 35.38775796358296, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 126045.54736018181, 'total_duration': 216434.73520970345, 'accumulated_submission_time': 126045.54736018181, 'accumulated_eval_time': 90371.01800084114, 'accumulated_logging_time': 7.350381135940552}
I0314 16:17:46.243376 139664567539456 logging_writer.py:48] [354189] accumulated_eval_time=90371.018001, accumulated_logging_time=7.350381, accumulated_submission_time=126045.547360, global_step=354189, preemption_count=0, score=126045.547360, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=216434.735210, train/accuracy=0.694415, train/bleu=35.387758, train/loss=1.372540, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 16:17:50.519835 139664575932160 logging_writer.py:48] [354200] global_step=354200, grad_norm=0.2295302003622055, loss=1.422894835472107
I0314 16:18:26.044443 139664567539456 logging_writer.py:48] [354300] global_step=354300, grad_norm=0.2322016954421997, loss=1.4200869798660278
I0314 16:19:01.627914 139664575932160 logging_writer.py:48] [354400] global_step=354400, grad_norm=0.23028302192687988, loss=1.4048192501068115
I0314 16:19:37.209616 139664567539456 logging_writer.py:48] [354500] global_step=354500, grad_norm=0.23851516842842102, loss=1.4332178831100464
I0314 16:20:12.791595 139664575932160 logging_writer.py:48] [354600] global_step=354600, grad_norm=0.22873333096504211, loss=1.417037010192871
I0314 16:20:48.353155 139664567539456 logging_writer.py:48] [354700] global_step=354700, grad_norm=0.2312549352645874, loss=1.506762981414795
I0314 16:21:23.938730 139664575932160 logging_writer.py:48] [354800] global_step=354800, grad_norm=0.23200975358486176, loss=1.4492299556732178
I0314 16:21:59.514286 139664567539456 logging_writer.py:48] [354900] global_step=354900, grad_norm=0.23230239748954773, loss=1.422989845275879
I0314 16:22:35.088232 139664575932160 logging_writer.py:48] [355000] global_step=355000, grad_norm=0.2308502197265625, loss=1.4809424877166748
I0314 16:23:10.676748 139664567539456 logging_writer.py:48] [355100] global_step=355100, grad_norm=0.23607973754405975, loss=1.4455734491348267
I0314 16:23:46.272883 139664575932160 logging_writer.py:48] [355200] global_step=355200, grad_norm=0.23820193111896515, loss=1.4481984376907349
I0314 16:24:21.882040 139664567539456 logging_writer.py:48] [355300] global_step=355300, grad_norm=0.22555461525917053, loss=1.4441660642623901
I0314 16:24:57.478778 139664575932160 logging_writer.py:48] [355400] global_step=355400, grad_norm=0.2396053671836853, loss=1.4280754327774048
I0314 16:25:33.078970 139664567539456 logging_writer.py:48] [355500] global_step=355500, grad_norm=0.24318771064281464, loss=1.4949398040771484
I0314 16:26:08.667224 139664575932160 logging_writer.py:48] [355600] global_step=355600, grad_norm=0.24646203219890594, loss=1.4662750959396362
I0314 16:26:44.255118 139664567539456 logging_writer.py:48] [355700] global_step=355700, grad_norm=0.23698757588863373, loss=1.4042186737060547
I0314 16:27:19.832014 139664575932160 logging_writer.py:48] [355800] global_step=355800, grad_norm=0.24599307775497437, loss=1.5072498321533203
I0314 16:27:55.428207 139664567539456 logging_writer.py:48] [355900] global_step=355900, grad_norm=0.2262565642595291, loss=1.4348952770233154
I0314 16:28:31.006799 139664575932160 logging_writer.py:48] [356000] global_step=356000, grad_norm=0.23650191724300385, loss=1.3922288417816162
I0314 16:29:06.594611 139664567539456 logging_writer.py:48] [356100] global_step=356100, grad_norm=0.22428232431411743, loss=1.4407905340194702
I0314 16:29:42.188129 139664575932160 logging_writer.py:48] [356200] global_step=356200, grad_norm=0.24427086114883423, loss=1.4409335851669312
I0314 16:30:17.827768 139664567539456 logging_writer.py:48] [356300] global_step=356300, grad_norm=0.23962019383907318, loss=1.4299581050872803
I0314 16:30:53.447346 139664575932160 logging_writer.py:48] [356400] global_step=356400, grad_norm=0.2276623547077179, loss=1.454874873161316
I0314 16:31:29.028650 139664567539456 logging_writer.py:48] [356500] global_step=356500, grad_norm=0.24151600897312164, loss=1.488668441772461
I0314 16:31:46.537544 139834281293632 spec.py:321] Evaluating on the training split.
I0314 16:31:49.505581 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:35:25.650478 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 16:35:28.328320 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:38:42.590605 139834281293632 spec.py:349] Evaluating on the test split.
I0314 16:38:45.265645 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:41:57.068609 139834281293632 submission_runner.py:420] Time since start: 217885.62s, 	Step: 356551, 	{'train/accuracy': 0.6954219341278076, 'train/loss': 1.3780286312103271, 'train/bleu': 35.53209505714748, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 126885.76126217842, 'total_duration': 217885.62106704712, 'accumulated_submission_time': 126885.76126217842, 'accumulated_eval_time': 90981.54898738861, 'accumulated_logging_time': 7.420124530792236}
I0314 16:41:57.143017 139664575932160 logging_writer.py:48] [356551] accumulated_eval_time=90981.548987, accumulated_logging_time=7.420125, accumulated_submission_time=126885.761262, global_step=356551, preemption_count=0, score=126885.761262, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=217885.621067, train/accuracy=0.695422, train/bleu=35.532095, train/loss=1.378029, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 16:42:14.947031 139664567539456 logging_writer.py:48] [356600] global_step=356600, grad_norm=0.23425419628620148, loss=1.4451465606689453
I0314 16:42:50.501305 139664575932160 logging_writer.py:48] [356700] global_step=356700, grad_norm=0.22859950363636017, loss=1.4665203094482422
I0314 16:43:26.097871 139664567539456 logging_writer.py:48] [356800] global_step=356800, grad_norm=0.24095194041728973, loss=1.4072028398513794
I0314 16:44:01.749532 139664575932160 logging_writer.py:48] [356900] global_step=356900, grad_norm=0.2324385792016983, loss=1.3430451154708862
I0314 16:44:37.403802 139664567539456 logging_writer.py:48] [357000] global_step=357000, grad_norm=0.22505831718444824, loss=1.4695743322372437
I0314 16:45:13.017215 139664575932160 logging_writer.py:48] [357100] global_step=357100, grad_norm=0.2256765067577362, loss=1.3874530792236328
I0314 16:45:48.630728 139664567539456 logging_writer.py:48] [357200] global_step=357200, grad_norm=0.23792500793933868, loss=1.4906762838363647
I0314 16:46:24.179106 139664575932160 logging_writer.py:48] [357300] global_step=357300, grad_norm=0.23516272008419037, loss=1.4275294542312622
I0314 16:46:59.791242 139664567539456 logging_writer.py:48] [357400] global_step=357400, grad_norm=0.23920784890651703, loss=1.409581184387207
I0314 16:47:35.390899 139664575932160 logging_writer.py:48] [357500] global_step=357500, grad_norm=0.23344872891902924, loss=1.500261664390564
I0314 16:48:10.961983 139664567539456 logging_writer.py:48] [357600] global_step=357600, grad_norm=0.23525848984718323, loss=1.5370174646377563
I0314 16:48:46.580340 139664575932160 logging_writer.py:48] [357700] global_step=357700, grad_norm=0.2428423911333084, loss=1.469933032989502
I0314 16:49:22.205537 139664567539456 logging_writer.py:48] [357800] global_step=357800, grad_norm=0.23252291977405548, loss=1.5033069849014282
I0314 16:49:57.793764 139664575932160 logging_writer.py:48] [357900] global_step=357900, grad_norm=0.2297702133655548, loss=1.4364898204803467
I0314 16:50:33.408142 139664567539456 logging_writer.py:48] [358000] global_step=358000, grad_norm=0.23046623170375824, loss=1.416178822517395
I0314 16:51:09.014574 139664575932160 logging_writer.py:48] [358100] global_step=358100, grad_norm=0.23444396257400513, loss=1.4282387495040894
I0314 16:51:44.593261 139664567539456 logging_writer.py:48] [358200] global_step=358200, grad_norm=0.2400418221950531, loss=1.514322280883789
I0314 16:52:20.184693 139664575932160 logging_writer.py:48] [358300] global_step=358300, grad_norm=0.23119966685771942, loss=1.4648789167404175
I0314 16:52:55.783873 139664567539456 logging_writer.py:48] [358400] global_step=358400, grad_norm=0.2340613454580307, loss=1.4238654375076294
I0314 16:53:31.387966 139664575932160 logging_writer.py:48] [358500] global_step=358500, grad_norm=0.23367711901664734, loss=1.436040997505188
I0314 16:54:06.995825 139664567539456 logging_writer.py:48] [358600] global_step=358600, grad_norm=0.23964886367321014, loss=1.4833428859710693
I0314 16:54:42.583369 139664575932160 logging_writer.py:48] [358700] global_step=358700, grad_norm=0.240676149725914, loss=1.4609348773956299
I0314 16:55:18.180046 139664567539456 logging_writer.py:48] [358800] global_step=358800, grad_norm=0.24055103957653046, loss=1.455941081047058
I0314 16:55:53.801980 139664575932160 logging_writer.py:48] [358900] global_step=358900, grad_norm=0.22742559015750885, loss=1.3733856678009033
I0314 16:55:57.078500 139834281293632 spec.py:321] Evaluating on the training split.
I0314 16:56:00.049952 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 16:59:20.921805 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 16:59:23.614075 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:02:39.320899 139834281293632 spec.py:349] Evaluating on the test split.
I0314 17:02:41.997975 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:05:52.856440 139834281293632 submission_runner.py:420] Time since start: 219321.41s, 	Step: 358911, 	{'train/accuracy': 0.6971335411071777, 'train/loss': 1.3615541458129883, 'train/bleu': 35.45657819681476, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 127725.61231327057, 'total_duration': 219321.40892481804, 'accumulated_submission_time': 127725.61231327057, 'accumulated_eval_time': 91577.32687735558, 'accumulated_logging_time': 7.504851579666138}
I0314 17:05:52.918313 139664567539456 logging_writer.py:48] [358911] accumulated_eval_time=91577.326877, accumulated_logging_time=7.504852, accumulated_submission_time=127725.612313, global_step=358911, preemption_count=0, score=127725.612313, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=219321.408925, train/accuracy=0.697134, train/bleu=35.456578, train/loss=1.361554, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 17:06:24.914784 139664575932160 logging_writer.py:48] [359000] global_step=359000, grad_norm=0.22998462617397308, loss=1.4290157556533813
I0314 17:07:00.485888 139664567539456 logging_writer.py:48] [359100] global_step=359100, grad_norm=0.23376718163490295, loss=1.4752562046051025
I0314 17:07:36.072300 139664575932160 logging_writer.py:48] [359200] global_step=359200, grad_norm=0.24284106492996216, loss=1.4451889991760254
I0314 17:08:11.666221 139664567539456 logging_writer.py:48] [359300] global_step=359300, grad_norm=0.24034222960472107, loss=1.3741705417633057
I0314 17:08:47.284049 139664575932160 logging_writer.py:48] [359400] global_step=359400, grad_norm=0.22639073431491852, loss=1.3833653926849365
I0314 17:09:22.887943 139664567539456 logging_writer.py:48] [359500] global_step=359500, grad_norm=0.2390938401222229, loss=1.3904682397842407
I0314 17:09:58.466423 139664575932160 logging_writer.py:48] [359600] global_step=359600, grad_norm=0.2298002541065216, loss=1.4576338529586792
I0314 17:10:34.072601 139664567539456 logging_writer.py:48] [359700] global_step=359700, grad_norm=0.23806725442409515, loss=1.4983494281768799
I0314 17:11:09.677096 139664575932160 logging_writer.py:48] [359800] global_step=359800, grad_norm=0.23906730115413666, loss=1.443911075592041
I0314 17:11:45.248407 139664567539456 logging_writer.py:48] [359900] global_step=359900, grad_norm=0.23072287440299988, loss=1.4556444883346558
I0314 17:12:20.861341 139664575932160 logging_writer.py:48] [360000] global_step=360000, grad_norm=0.24715860188007355, loss=1.493562936782837
I0314 17:12:56.462436 139664567539456 logging_writer.py:48] [360100] global_step=360100, grad_norm=0.23645474016666412, loss=1.440041184425354
I0314 17:13:32.071871 139664575932160 logging_writer.py:48] [360200] global_step=360200, grad_norm=0.2309895008802414, loss=1.4508228302001953
I0314 17:14:07.680738 139664567539456 logging_writer.py:48] [360300] global_step=360300, grad_norm=0.23184239864349365, loss=1.4089266061782837
I0314 17:14:43.287785 139664575932160 logging_writer.py:48] [360400] global_step=360400, grad_norm=0.24760280549526215, loss=1.5356626510620117
I0314 17:15:18.904061 139664567539456 logging_writer.py:48] [360500] global_step=360500, grad_norm=0.23390434682369232, loss=1.388913631439209
I0314 17:15:54.529011 139664575932160 logging_writer.py:48] [360600] global_step=360600, grad_norm=0.23314788937568665, loss=1.4412661790847778
I0314 17:16:30.150835 139664567539456 logging_writer.py:48] [360700] global_step=360700, grad_norm=0.23789434134960175, loss=1.3949577808380127
I0314 17:17:05.798376 139664575932160 logging_writer.py:48] [360800] global_step=360800, grad_norm=0.23163524270057678, loss=1.4305906295776367
I0314 17:17:41.411330 139664567539456 logging_writer.py:48] [360900] global_step=360900, grad_norm=0.23083941638469696, loss=1.4379125833511353
I0314 17:18:17.035030 139664575932160 logging_writer.py:48] [361000] global_step=361000, grad_norm=0.2350916564464569, loss=1.4062937498092651
I0314 17:18:52.627520 139664567539456 logging_writer.py:48] [361100] global_step=361100, grad_norm=0.2564674913883209, loss=1.4484636783599854
I0314 17:19:28.259419 139664575932160 logging_writer.py:48] [361200] global_step=361200, grad_norm=0.23243694007396698, loss=1.4176005125045776
I0314 17:19:52.917675 139834281293632 spec.py:321] Evaluating on the training split.
I0314 17:19:55.896012 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:23:34.117283 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 17:23:36.810160 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:26:50.924350 139834281293632 spec.py:349] Evaluating on the test split.
I0314 17:26:53.604237 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:30:03.494562 139834281293632 submission_runner.py:420] Time since start: 220772.05s, 	Step: 361271, 	{'train/accuracy': 0.69556725025177, 'train/loss': 1.368157982826233, 'train/bleu': 35.47585553964436, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 128565.52987861633, 'total_duration': 220772.04701256752, 'accumulated_submission_time': 128565.52987861633, 'accumulated_eval_time': 92187.90368366241, 'accumulated_logging_time': 7.576892137527466}
I0314 17:30:03.556589 139664567539456 logging_writer.py:48] [361271] accumulated_eval_time=92187.903684, accumulated_logging_time=7.576892, accumulated_submission_time=128565.529879, global_step=361271, preemption_count=0, score=128565.529879, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=220772.047013, train/accuracy=0.695567, train/bleu=35.475856, train/loss=1.368158, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 17:30:14.222490 139664575932160 logging_writer.py:48] [361300] global_step=361300, grad_norm=0.2333783209323883, loss=1.3923944234848022
I0314 17:30:49.732121 139664567539456 logging_writer.py:48] [361400] global_step=361400, grad_norm=0.22384560108184814, loss=1.4032928943634033
I0314 17:31:25.330667 139664575932160 logging_writer.py:48] [361500] global_step=361500, grad_norm=0.24571552872657776, loss=1.4270764589309692
I0314 17:32:00.908603 139664567539456 logging_writer.py:48] [361600] global_step=361600, grad_norm=0.23131512105464935, loss=1.4438560009002686
I0314 17:32:36.516369 139664575932160 logging_writer.py:48] [361700] global_step=361700, grad_norm=0.2324061244726181, loss=1.4577887058258057
I0314 17:33:12.108398 139664567539456 logging_writer.py:48] [361800] global_step=361800, grad_norm=0.23892949521541595, loss=1.3754615783691406
I0314 17:33:47.734783 139664575932160 logging_writer.py:48] [361900] global_step=361900, grad_norm=0.24215728044509888, loss=1.4679523706436157
I0314 17:34:23.339075 139664567539456 logging_writer.py:48] [362000] global_step=362000, grad_norm=0.2350199818611145, loss=1.4035613536834717
I0314 17:34:58.929751 139664575932160 logging_writer.py:48] [362100] global_step=362100, grad_norm=0.2433231770992279, loss=1.4184306859970093
I0314 17:35:34.529365 139664567539456 logging_writer.py:48] [362200] global_step=362200, grad_norm=0.24100284278392792, loss=1.432819128036499
I0314 17:36:10.132826 139664575932160 logging_writer.py:48] [362300] global_step=362300, grad_norm=0.23781532049179077, loss=1.5004502534866333
I0314 17:36:45.720608 139664567539456 logging_writer.py:48] [362400] global_step=362400, grad_norm=0.22392964363098145, loss=1.4695918560028076
I0314 17:37:21.313642 139664575932160 logging_writer.py:48] [362500] global_step=362500, grad_norm=0.2343311905860901, loss=1.4787521362304688
I0314 17:37:56.902503 139664567539456 logging_writer.py:48] [362600] global_step=362600, grad_norm=0.23782393336296082, loss=1.4282076358795166
I0314 17:38:32.549106 139664575932160 logging_writer.py:48] [362700] global_step=362700, grad_norm=0.23690560460090637, loss=1.44999098777771
I0314 17:39:08.162192 139664567539456 logging_writer.py:48] [362800] global_step=362800, grad_norm=0.23055779933929443, loss=1.4266948699951172
I0314 17:39:43.758484 139664575932160 logging_writer.py:48] [362900] global_step=362900, grad_norm=0.24220633506774902, loss=1.506482720375061
I0314 17:40:19.367311 139664567539456 logging_writer.py:48] [363000] global_step=363000, grad_norm=0.24077335000038147, loss=1.3529976606369019
I0314 17:40:55.001483 139664575932160 logging_writer.py:48] [363100] global_step=363100, grad_norm=0.2312568873167038, loss=1.45964777469635
I0314 17:41:30.671782 139664567539456 logging_writer.py:48] [363200] global_step=363200, grad_norm=0.22686469554901123, loss=1.433962106704712
I0314 17:42:06.281079 139664575932160 logging_writer.py:48] [363300] global_step=363300, grad_norm=0.23300820589065552, loss=1.4690921306610107
I0314 17:42:41.932798 139664567539456 logging_writer.py:48] [363400] global_step=363400, grad_norm=0.2295830398797989, loss=1.3890221118927002
I0314 17:43:17.546755 139664575932160 logging_writer.py:48] [363500] global_step=363500, grad_norm=0.22870361804962158, loss=1.4409807920455933
I0314 17:43:53.162689 139664567539456 logging_writer.py:48] [363600] global_step=363600, grad_norm=0.2311553955078125, loss=1.42741858959198
I0314 17:44:03.558355 139834281293632 spec.py:321] Evaluating on the training split.
I0314 17:44:06.522160 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:47:24.404063 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 17:47:27.072921 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:50:41.075351 139834281293632 spec.py:349] Evaluating on the test split.
I0314 17:50:43.752038 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 17:53:53.916419 139834281293632 submission_runner.py:420] Time since start: 222202.47s, 	Step: 363631, 	{'train/accuracy': 0.6939976811408997, 'train/loss': 1.379857063293457, 'train/bleu': 35.58371781025625, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 129405.45047450066, 'total_duration': 222202.46889948845, 'accumulated_submission_time': 129405.45047450066, 'accumulated_eval_time': 92778.26169657707, 'accumulated_logging_time': 7.64824366569519}
I0314 17:53:53.981085 139664575932160 logging_writer.py:48] [363631] accumulated_eval_time=92778.261697, accumulated_logging_time=7.648244, accumulated_submission_time=129405.450475, global_step=363631, preemption_count=0, score=129405.450475, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=222202.468899, train/accuracy=0.693998, train/bleu=35.583718, train/loss=1.379857, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 17:54:18.837029 139664567539456 logging_writer.py:48] [363700] global_step=363700, grad_norm=0.23189905285835266, loss=1.4684957265853882
I0314 17:54:54.393282 139664575932160 logging_writer.py:48] [363800] global_step=363800, grad_norm=0.23120953142642975, loss=1.4340465068817139
I0314 17:55:29.964658 139664567539456 logging_writer.py:48] [363900] global_step=363900, grad_norm=0.2349645346403122, loss=1.4960262775421143
I0314 17:56:05.547605 139664575932160 logging_writer.py:48] [364000] global_step=364000, grad_norm=0.24720680713653564, loss=1.5090844631195068
I0314 17:56:41.144844 139664567539456 logging_writer.py:48] [364100] global_step=364100, grad_norm=0.23088182508945465, loss=1.4261938333511353
I0314 17:57:16.742523 139664575932160 logging_writer.py:48] [364200] global_step=364200, grad_norm=0.24136947095394135, loss=1.4983317852020264
I0314 17:57:52.347876 139664567539456 logging_writer.py:48] [364300] global_step=364300, grad_norm=0.23724648356437683, loss=1.4256296157836914
I0314 17:58:27.937799 139664575932160 logging_writer.py:48] [364400] global_step=364400, grad_norm=0.23113839328289032, loss=1.410812497138977
I0314 17:59:03.558093 139664567539456 logging_writer.py:48] [364500] global_step=364500, grad_norm=0.2402443289756775, loss=1.4086247682571411
I0314 17:59:39.132291 139664575932160 logging_writer.py:48] [364600] global_step=364600, grad_norm=0.22668646275997162, loss=1.4074218273162842
I0314 18:00:14.758785 139664567539456 logging_writer.py:48] [364700] global_step=364700, grad_norm=0.25055041909217834, loss=1.47264564037323
I0314 18:00:50.324717 139664575932160 logging_writer.py:48] [364800] global_step=364800, grad_norm=0.23085087537765503, loss=1.3675347566604614
I0314 18:01:25.891956 139664567539456 logging_writer.py:48] [364900] global_step=364900, grad_norm=0.224884033203125, loss=1.4536352157592773
I0314 18:02:01.474576 139664575932160 logging_writer.py:48] [365000] global_step=365000, grad_norm=0.24079613387584686, loss=1.4058623313903809
I0314 18:02:37.064000 139664567539456 logging_writer.py:48] [365100] global_step=365100, grad_norm=0.22626616060733795, loss=1.3979464769363403
I0314 18:03:12.678993 139664575932160 logging_writer.py:48] [365200] global_step=365200, grad_norm=0.24399334192276, loss=1.4016966819763184
I0314 18:03:48.276941 139664567539456 logging_writer.py:48] [365300] global_step=365300, grad_norm=0.23215685784816742, loss=1.4173563718795776
I0314 18:04:23.903664 139664575932160 logging_writer.py:48] [365400] global_step=365400, grad_norm=0.2299877405166626, loss=1.394261360168457
I0314 18:04:59.541439 139664567539456 logging_writer.py:48] [365500] global_step=365500, grad_norm=0.22888684272766113, loss=1.4384738206863403
I0314 18:05:35.144461 139664575932160 logging_writer.py:48] [365600] global_step=365600, grad_norm=0.24007898569107056, loss=1.4543328285217285
I0314 18:06:10.788743 139664567539456 logging_writer.py:48] [365700] global_step=365700, grad_norm=0.23171988129615784, loss=1.4190123081207275
I0314 18:06:46.398054 139664575932160 logging_writer.py:48] [365800] global_step=365800, grad_norm=0.22916190326213837, loss=1.4661240577697754
I0314 18:07:22.016475 139664567539456 logging_writer.py:48] [365900] global_step=365900, grad_norm=0.23096610605716705, loss=1.4366828203201294
I0314 18:07:54.115492 139834281293632 spec.py:321] Evaluating on the training split.
I0314 18:07:57.080068 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:11:16.705334 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 18:11:19.372468 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:14:33.506811 139834281293632 spec.py:349] Evaluating on the test split.
I0314 18:14:36.188303 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:17:47.156262 139834281293632 submission_runner.py:420] Time since start: 223635.71s, 	Step: 365992, 	{'train/accuracy': 0.6946594715118408, 'train/loss': 1.3834761381149292, 'train/bleu': 35.4348874325702, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 130245.50266051292, 'total_duration': 223635.70874118805, 'accumulated_submission_time': 130245.50266051292, 'accumulated_eval_time': 93371.30241632462, 'accumulated_logging_time': 7.723176717758179}
I0314 18:17:47.220604 139664575932160 logging_writer.py:48] [365992] accumulated_eval_time=93371.302416, accumulated_logging_time=7.723177, accumulated_submission_time=130245.502661, global_step=365992, preemption_count=0, score=130245.502661, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=223635.708741, train/accuracy=0.694659, train/bleu=35.434887, train/loss=1.383476, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 18:17:50.440421 139664567539456 logging_writer.py:48] [366000] global_step=366000, grad_norm=0.2333332598209381, loss=1.3896359205245972
I0314 18:18:25.980346 139664575932160 logging_writer.py:48] [366100] global_step=366100, grad_norm=0.22358883917331696, loss=1.4093143939971924
I0314 18:19:01.545471 139664567539456 logging_writer.py:48] [366200] global_step=366200, grad_norm=0.22740799188613892, loss=1.3922576904296875
I0314 18:19:37.171237 139664575932160 logging_writer.py:48] [366300] global_step=366300, grad_norm=0.23849965631961823, loss=1.3837077617645264
I0314 18:20:12.783625 139664567539456 logging_writer.py:48] [366400] global_step=366400, grad_norm=0.23775921761989594, loss=1.489730954170227
I0314 18:20:48.416024 139664575932160 logging_writer.py:48] [366500] global_step=366500, grad_norm=0.22598879039287567, loss=1.385951042175293
I0314 18:21:24.064028 139664567539456 logging_writer.py:48] [366600] global_step=366600, grad_norm=0.23135896027088165, loss=1.417341709136963
I0314 18:21:59.685487 139664575932160 logging_writer.py:48] [366700] global_step=366700, grad_norm=0.24590706825256348, loss=1.478665828704834
I0314 18:22:35.309517 139664567539456 logging_writer.py:48] [366800] global_step=366800, grad_norm=0.2336665540933609, loss=1.3650680780410767
I0314 18:23:10.914193 139664575932160 logging_writer.py:48] [366900] global_step=366900, grad_norm=0.23150242865085602, loss=1.4032992124557495
I0314 18:23:46.506546 139664567539456 logging_writer.py:48] [367000] global_step=367000, grad_norm=0.23234808444976807, loss=1.4463928937911987
I0314 18:24:22.123802 139664575932160 logging_writer.py:48] [367100] global_step=367100, grad_norm=0.24529597163200378, loss=1.5171324014663696
I0314 18:24:57.725300 139664567539456 logging_writer.py:48] [367200] global_step=367200, grad_norm=0.22510850429534912, loss=1.3884752988815308
I0314 18:25:33.390635 139664575932160 logging_writer.py:48] [367300] global_step=367300, grad_norm=0.24641358852386475, loss=1.4885425567626953
I0314 18:26:09.107022 139664567539456 logging_writer.py:48] [367400] global_step=367400, grad_norm=0.22331735491752625, loss=1.3948267698287964
I0314 18:26:44.759218 139664575932160 logging_writer.py:48] [367500] global_step=367500, grad_norm=0.23888927698135376, loss=1.5347089767456055
I0314 18:27:20.422975 139664567539456 logging_writer.py:48] [367600] global_step=367600, grad_norm=0.23265229165554047, loss=1.4973787069320679
I0314 18:27:56.062170 139664575932160 logging_writer.py:48] [367700] global_step=367700, grad_norm=7.0258097648620605, loss=1.504036784172058
I0314 18:28:31.662703 139664567539456 logging_writer.py:48] [367800] global_step=367800, grad_norm=0.22983570396900177, loss=1.4365867376327515
I0314 18:29:07.278099 139664575932160 logging_writer.py:48] [367900] global_step=367900, grad_norm=0.23636125028133392, loss=1.4314231872558594
I0314 18:29:42.875259 139664567539456 logging_writer.py:48] [368000] global_step=368000, grad_norm=0.237186461687088, loss=1.4547865390777588
I0314 18:30:18.493010 139664575932160 logging_writer.py:48] [368100] global_step=368100, grad_norm=0.24556468427181244, loss=1.4333895444869995
I0314 18:30:54.090092 139664567539456 logging_writer.py:48] [368200] global_step=368200, grad_norm=0.2352912575006485, loss=1.385989785194397
I0314 18:31:29.697015 139664575932160 logging_writer.py:48] [368300] global_step=368300, grad_norm=0.23044425249099731, loss=1.3592462539672852
I0314 18:31:47.213677 139834281293632 spec.py:321] Evaluating on the training split.
I0314 18:31:50.190477 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:35:22.003412 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 18:35:24.683493 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:38:38.623547 139834281293632 spec.py:349] Evaluating on the test split.
I0314 18:38:41.292975 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:41:51.092699 139834281293632 submission_runner.py:420] Time since start: 225079.65s, 	Step: 368351, 	{'train/accuracy': 0.6929248571395874, 'train/loss': 1.3840899467468262, 'train/bleu': 35.70822070105537, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 131085.41303038597, 'total_duration': 225079.64515781403, 'accumulated_submission_time': 131085.41303038597, 'accumulated_eval_time': 93975.18136429787, 'accumulated_logging_time': 7.796278238296509}
I0314 18:41:51.156969 139664567539456 logging_writer.py:48] [368351] accumulated_eval_time=93975.181364, accumulated_logging_time=7.796278, accumulated_submission_time=131085.413030, global_step=368351, preemption_count=0, score=131085.413030, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=225079.645158, train/accuracy=0.692925, train/bleu=35.708221, train/loss=1.384090, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 18:42:08.919697 139664575932160 logging_writer.py:48] [368400] global_step=368400, grad_norm=0.23660527169704437, loss=1.405128002166748
I0314 18:42:44.500416 139664567539456 logging_writer.py:48] [368500] global_step=368500, grad_norm=0.2405862957239151, loss=1.4205008745193481
I0314 18:43:20.144963 139664575932160 logging_writer.py:48] [368600] global_step=368600, grad_norm=0.2275306135416031, loss=1.493991494178772
I0314 18:43:55.744322 139664567539456 logging_writer.py:48] [368700] global_step=368700, grad_norm=0.23868416249752045, loss=1.4313870668411255
I0314 18:44:31.340978 139664575932160 logging_writer.py:48] [368800] global_step=368800, grad_norm=0.23879337310791016, loss=1.4803720712661743
I0314 18:45:06.947575 139664567539456 logging_writer.py:48] [368900] global_step=368900, grad_norm=0.24081192910671234, loss=1.467138648033142
I0314 18:45:42.556364 139664575932160 logging_writer.py:48] [369000] global_step=369000, grad_norm=0.2355806678533554, loss=1.4839640855789185
I0314 18:46:18.158851 139664567539456 logging_writer.py:48] [369100] global_step=369100, grad_norm=0.22902308404445648, loss=1.4325170516967773
I0314 18:46:53.759264 139664575932160 logging_writer.py:48] [369200] global_step=369200, grad_norm=0.23693117499351501, loss=1.4348154067993164
I0314 18:47:29.400899 139664567539456 logging_writer.py:48] [369300] global_step=369300, grad_norm=0.23084095120429993, loss=1.4277939796447754
I0314 18:48:05.033433 139664575932160 logging_writer.py:48] [369400] global_step=369400, grad_norm=0.2441028207540512, loss=1.448594570159912
I0314 18:48:40.631077 139664567539456 logging_writer.py:48] [369500] global_step=369500, grad_norm=0.24235625565052032, loss=1.4036394357681274
I0314 18:49:16.242321 139664575932160 logging_writer.py:48] [369600] global_step=369600, grad_norm=0.22756218910217285, loss=1.4352761507034302
I0314 18:49:51.840308 139664567539456 logging_writer.py:48] [369700] global_step=369700, grad_norm=0.23313860595226288, loss=1.4311087131500244
I0314 18:50:27.446290 139664575932160 logging_writer.py:48] [369800] global_step=369800, grad_norm=0.23283565044403076, loss=1.437239408493042
I0314 18:51:03.074896 139664567539456 logging_writer.py:48] [369900] global_step=369900, grad_norm=0.2380892038345337, loss=1.464171290397644
I0314 18:51:38.662509 139664575932160 logging_writer.py:48] [370000] global_step=370000, grad_norm=0.23200654983520508, loss=1.4667303562164307
I0314 18:52:14.276702 139664567539456 logging_writer.py:48] [370100] global_step=370100, grad_norm=0.23277093470096588, loss=1.4360021352767944
I0314 18:52:49.913944 139664575932160 logging_writer.py:48] [370200] global_step=370200, grad_norm=0.2277630865573883, loss=1.415199875831604
I0314 18:53:25.560253 139664567539456 logging_writer.py:48] [370300] global_step=370300, grad_norm=0.23121151328086853, loss=1.464176893234253
I0314 18:54:01.144376 139664575932160 logging_writer.py:48] [370400] global_step=370400, grad_norm=0.24205589294433594, loss=1.5013285875320435
I0314 18:54:36.754485 139664567539456 logging_writer.py:48] [370500] global_step=370500, grad_norm=0.23978878557682037, loss=1.459700584411621
I0314 18:55:12.352713 139664575932160 logging_writer.py:48] [370600] global_step=370600, grad_norm=0.23633675277233124, loss=1.4832398891448975
I0314 18:55:47.971883 139664567539456 logging_writer.py:48] [370700] global_step=370700, grad_norm=0.23274262249469757, loss=1.4870400428771973
I0314 18:55:51.253004 139834281293632 spec.py:321] Evaluating on the training split.
I0314 18:55:54.231452 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 18:59:23.148770 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 18:59:25.819822 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:02:39.789705 139834281293632 spec.py:349] Evaluating on the test split.
I0314 19:02:42.467617 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:05:52.278461 139834281293632 submission_runner.py:420] Time since start: 226520.83s, 	Step: 370711, 	{'train/accuracy': 0.6935230493545532, 'train/loss': 1.3815573453903198, 'train/bleu': 35.63322774950154, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 131925.42628622055, 'total_duration': 226520.8309454918, 'accumulated_submission_time': 131925.42628622055, 'accumulated_eval_time': 94576.20677280426, 'accumulated_logging_time': 7.871663808822632}
I0314 19:05:52.341231 139664575932160 logging_writer.py:48] [370711] accumulated_eval_time=94576.206773, accumulated_logging_time=7.871664, accumulated_submission_time=131925.426286, global_step=370711, preemption_count=0, score=131925.426286, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=226520.830945, train/accuracy=0.693523, train/bleu=35.633228, train/loss=1.381557, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 19:06:24.317110 139664567539456 logging_writer.py:48] [370800] global_step=370800, grad_norm=0.23555314540863037, loss=1.473647952079773
I0314 19:06:59.855849 139664575932160 logging_writer.py:48] [370900] global_step=370900, grad_norm=0.23238003253936768, loss=1.4741910696029663
I0314 19:07:35.461065 139664567539456 logging_writer.py:48] [371000] global_step=371000, grad_norm=0.23314936459064484, loss=1.4404207468032837
I0314 19:08:11.056444 139664575932160 logging_writer.py:48] [371100] global_step=371100, grad_norm=0.226482555270195, loss=1.4476450681686401
I0314 19:08:46.673017 139664567539456 logging_writer.py:48] [371200] global_step=371200, grad_norm=0.23380765318870544, loss=1.4427820444107056
I0314 19:09:22.279729 139664575932160 logging_writer.py:48] [371300] global_step=371300, grad_norm=0.23897318542003632, loss=1.4687743186950684
I0314 19:09:57.903803 139664567539456 logging_writer.py:48] [371400] global_step=371400, grad_norm=0.23419228196144104, loss=1.4863855838775635
I0314 19:10:33.529501 139664575932160 logging_writer.py:48] [371500] global_step=371500, grad_norm=0.23670190572738647, loss=1.4649161100387573
I0314 19:11:09.171234 139664567539456 logging_writer.py:48] [371600] global_step=371600, grad_norm=0.22848013043403625, loss=1.3843939304351807
I0314 19:11:44.782534 139664575932160 logging_writer.py:48] [371700] global_step=371700, grad_norm=0.23213377594947815, loss=1.4558980464935303
I0314 19:12:20.381688 139664567539456 logging_writer.py:48] [371800] global_step=371800, grad_norm=0.2337672859430313, loss=1.431754469871521
I0314 19:12:55.977970 139664575932160 logging_writer.py:48] [371900] global_step=371900, grad_norm=0.23768816888332367, loss=1.4534387588500977
I0314 19:13:31.579730 139664567539456 logging_writer.py:48] [372000] global_step=372000, grad_norm=0.22966143488883972, loss=1.4704731702804565
I0314 19:14:07.198344 139664575932160 logging_writer.py:48] [372100] global_step=372100, grad_norm=0.22464388608932495, loss=1.452496886253357
I0314 19:14:42.810194 139664567539456 logging_writer.py:48] [372200] global_step=372200, grad_norm=0.2373984009027481, loss=1.4729901552200317
I0314 19:15:18.419780 139664575932160 logging_writer.py:48] [372300] global_step=372300, grad_norm=0.23016981780529022, loss=1.4140571355819702
I0314 19:15:54.031334 139664567539456 logging_writer.py:48] [372400] global_step=372400, grad_norm=0.2296794354915619, loss=1.4347938299179077
I0314 19:16:29.696728 139664575932160 logging_writer.py:48] [372500] global_step=372500, grad_norm=0.23704612255096436, loss=1.3701592683792114
I0314 19:17:05.337043 139664567539456 logging_writer.py:48] [372600] global_step=372600, grad_norm=0.2354201227426529, loss=1.440626859664917
I0314 19:17:40.936038 139664575932160 logging_writer.py:48] [372700] global_step=372700, grad_norm=0.2323724925518036, loss=1.4438289403915405
I0314 19:18:16.520722 139664567539456 logging_writer.py:48] [372800] global_step=372800, grad_norm=0.23606012761592865, loss=1.43058443069458
I0314 19:18:52.126686 139664575932160 logging_writer.py:48] [372900] global_step=372900, grad_norm=0.23897843062877655, loss=1.4678542613983154
I0314 19:19:27.736489 139664567539456 logging_writer.py:48] [373000] global_step=373000, grad_norm=0.23164135217666626, loss=1.4180119037628174
I0314 19:19:52.405760 139834281293632 spec.py:321] Evaluating on the training split.
I0314 19:19:55.384323 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:23:18.548779 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 19:23:21.227385 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:26:35.255278 139834281293632 spec.py:349] Evaluating on the test split.
I0314 19:26:37.918990 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:29:47.814960 139834281293632 submission_runner.py:420] Time since start: 227956.37s, 	Step: 373071, 	{'train/accuracy': 0.693621814250946, 'train/loss': 1.3835389614105225, 'train/bleu': 35.99297319457071, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 132765.4077756405, 'total_duration': 227956.36740851402, 'accumulated_submission_time': 132765.4077756405, 'accumulated_eval_time': 95171.61591076851, 'accumulated_logging_time': 7.943910598754883}
I0314 19:29:47.880301 139664575932160 logging_writer.py:48] [373071] accumulated_eval_time=95171.615911, accumulated_logging_time=7.943911, accumulated_submission_time=132765.407776, global_step=373071, preemption_count=0, score=132765.407776, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=227956.367409, train/accuracy=0.693622, train/bleu=35.992973, train/loss=1.383539, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 19:29:58.552509 139664567539456 logging_writer.py:48] [373100] global_step=373100, grad_norm=0.2357092946767807, loss=1.479738712310791
I0314 19:30:34.103695 139664575932160 logging_writer.py:48] [373200] global_step=373200, grad_norm=0.24084414541721344, loss=1.486167073249817
I0314 19:31:09.691426 139664567539456 logging_writer.py:48] [373300] global_step=373300, grad_norm=0.23083266615867615, loss=1.4689083099365234
I0314 19:31:45.297189 139664575932160 logging_writer.py:48] [373400] global_step=373400, grad_norm=0.23625676333904266, loss=1.4526605606079102
I0314 19:32:20.914697 139664567539456 logging_writer.py:48] [373500] global_step=373500, grad_norm=0.23489931225776672, loss=1.472330093383789
I0314 19:32:56.520728 139664575932160 logging_writer.py:48] [373600] global_step=373600, grad_norm=0.23836074769496918, loss=1.380237102508545
I0314 19:33:32.145179 139664567539456 logging_writer.py:48] [373700] global_step=373700, grad_norm=0.23512911796569824, loss=1.4717943668365479
I0314 19:34:07.770113 139664575932160 logging_writer.py:48] [373800] global_step=373800, grad_norm=0.2308318316936493, loss=1.4308192729949951
I0314 19:34:43.366906 139664567539456 logging_writer.py:48] [373900] global_step=373900, grad_norm=0.2350645214319229, loss=1.4301693439483643
I0314 19:35:18.938352 139664575932160 logging_writer.py:48] [374000] global_step=374000, grad_norm=0.23086851835250854, loss=1.5042766332626343
I0314 19:35:54.534970 139664567539456 logging_writer.py:48] [374100] global_step=374100, grad_norm=0.24955208599567413, loss=1.4469919204711914
I0314 19:36:30.142593 139664575932160 logging_writer.py:48] [374200] global_step=374200, grad_norm=0.23720137774944305, loss=1.4493350982666016
I0314 19:37:05.776053 139664567539456 logging_writer.py:48] [374300] global_step=374300, grad_norm=0.23691099882125854, loss=1.3909794092178345
I0314 19:37:41.392202 139664575932160 logging_writer.py:48] [374400] global_step=374400, grad_norm=0.24499179422855377, loss=1.4788347482681274
I0314 19:38:16.996357 139664567539456 logging_writer.py:48] [374500] global_step=374500, grad_norm=0.23622392117977142, loss=1.4570995569229126
I0314 19:38:52.635706 139664575932160 logging_writer.py:48] [374600] global_step=374600, grad_norm=0.2214522510766983, loss=1.428764820098877
I0314 19:39:28.274506 139664567539456 logging_writer.py:48] [374700] global_step=374700, grad_norm=0.234861359000206, loss=1.4473564624786377
I0314 19:40:03.920772 139664575932160 logging_writer.py:48] [374800] global_step=374800, grad_norm=0.23597531020641327, loss=1.4544764757156372
I0314 19:40:39.526257 139664567539456 logging_writer.py:48] [374900] global_step=374900, grad_norm=0.22785316407680511, loss=1.4269627332687378
I0314 19:41:15.146625 139664575932160 logging_writer.py:48] [375000] global_step=375000, grad_norm=0.23169724643230438, loss=1.4533063173294067
I0314 19:41:50.746697 139664567539456 logging_writer.py:48] [375100] global_step=375100, grad_norm=0.23028817772865295, loss=1.4371442794799805
I0314 19:42:26.403045 139664575932160 logging_writer.py:48] [375200] global_step=375200, grad_norm=0.22637881338596344, loss=1.3914247751235962
I0314 19:43:02.034777 139664567539456 logging_writer.py:48] [375300] global_step=375300, grad_norm=0.23239803314208984, loss=1.3954501152038574
I0314 19:43:37.679470 139664575932160 logging_writer.py:48] [375400] global_step=375400, grad_norm=0.22389782965183258, loss=1.3811019659042358
I0314 19:43:48.076478 139834281293632 spec.py:321] Evaluating on the training split.
I0314 19:43:51.068157 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:47:17.519475 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 19:47:20.201707 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:50:34.248352 139834281293632 spec.py:349] Evaluating on the test split.
I0314 19:50:36.925014 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 19:53:48.046769 139834281293632 submission_runner.py:420] Time since start: 229396.60s, 	Step: 375431, 	{'train/accuracy': 0.6953145861625671, 'train/loss': 1.3713488578796387, 'train/bleu': 35.52966497028867, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 133605.52108120918, 'total_duration': 229396.59922361374, 'accumulated_submission_time': 133605.52108120918, 'accumulated_eval_time': 95771.58613538742, 'accumulated_logging_time': 8.019490957260132}
I0314 19:53:48.119853 139664567539456 logging_writer.py:48] [375431] accumulated_eval_time=95771.586135, accumulated_logging_time=8.019491, accumulated_submission_time=133605.521081, global_step=375431, preemption_count=0, score=133605.521081, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=229396.599224, train/accuracy=0.695315, train/bleu=35.529665, train/loss=1.371349, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 19:54:13.014975 139664575932160 logging_writer.py:48] [375500] global_step=375500, grad_norm=0.24099496006965637, loss=1.4537041187286377
I0314 19:54:48.601922 139664567539456 logging_writer.py:48] [375600] global_step=375600, grad_norm=0.23705750703811646, loss=1.4248591661453247
I0314 19:55:24.232106 139664575932160 logging_writer.py:48] [375700] global_step=375700, grad_norm=0.2380995750427246, loss=1.4633138179779053
I0314 19:55:59.852982 139664567539456 logging_writer.py:48] [375800] global_step=375800, grad_norm=0.23286113142967224, loss=1.4685410261154175
I0314 19:56:35.488026 139664575932160 logging_writer.py:48] [375900] global_step=375900, grad_norm=0.23599007725715637, loss=1.480493426322937
I0314 19:57:11.094391 139664567539456 logging_writer.py:48] [376000] global_step=376000, grad_norm=0.24359537661075592, loss=1.4722484350204468
I0314 19:57:46.702628 139664575932160 logging_writer.py:48] [376100] global_step=376100, grad_norm=0.2288195937871933, loss=1.4293164014816284
I0314 19:58:22.313423 139664567539456 logging_writer.py:48] [376200] global_step=376200, grad_norm=0.2192278355360031, loss=1.3716634511947632
I0314 19:58:57.983648 139664575932160 logging_writer.py:48] [376300] global_step=376300, grad_norm=0.22994768619537354, loss=1.492483139038086
I0314 19:59:33.600057 139664567539456 logging_writer.py:48] [376400] global_step=376400, grad_norm=0.22481022775173187, loss=1.365189790725708
I0314 20:00:09.217380 139664575932160 logging_writer.py:48] [376500] global_step=376500, grad_norm=0.2399311512708664, loss=1.471570611000061
I0314 20:00:44.821442 139664567539456 logging_writer.py:48] [376600] global_step=376600, grad_norm=0.23549965023994446, loss=1.5237255096435547
I0314 20:01:20.444517 139664575932160 logging_writer.py:48] [376700] global_step=376700, grad_norm=0.2274683266878128, loss=1.3657563924789429
I0314 20:01:56.052178 139664567539456 logging_writer.py:48] [376800] global_step=376800, grad_norm=0.23667219281196594, loss=1.4181056022644043
I0314 20:02:31.700442 139664575932160 logging_writer.py:48] [376900] global_step=376900, grad_norm=0.22395901381969452, loss=1.3884155750274658
I0314 20:03:07.314337 139664567539456 logging_writer.py:48] [377000] global_step=377000, grad_norm=0.22703194618225098, loss=1.4572224617004395
I0314 20:03:42.945468 139664575932160 logging_writer.py:48] [377100] global_step=377100, grad_norm=0.24789005517959595, loss=1.5019965171813965
I0314 20:04:18.569302 139664567539456 logging_writer.py:48] [377200] global_step=377200, grad_norm=0.23056527972221375, loss=1.377194881439209
I0314 20:04:54.200430 139664575932160 logging_writer.py:48] [377300] global_step=377300, grad_norm=0.23651233315467834, loss=1.4366875886917114
I0314 20:05:29.823471 139664567539456 logging_writer.py:48] [377400] global_step=377400, grad_norm=0.22522079944610596, loss=1.3768264055252075
I0314 20:06:05.453738 139664575932160 logging_writer.py:48] [377500] global_step=377500, grad_norm=0.22460395097732544, loss=1.4010695219039917
I0314 20:06:41.119272 139664567539456 logging_writer.py:48] [377600] global_step=377600, grad_norm=0.23357641696929932, loss=1.5264379978179932
I0314 20:07:16.791012 139664575932160 logging_writer.py:48] [377700] global_step=377700, grad_norm=0.24031345546245575, loss=1.3882018327713013
I0314 20:07:48.239250 139834281293632 spec.py:321] Evaluating on the training split.
I0314 20:07:51.207089 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:11:15.648759 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 20:11:18.329324 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:14:32.129159 139834281293632 spec.py:349] Evaluating on the test split.
I0314 20:14:34.798429 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:17:44.349549 139834281293632 submission_runner.py:420] Time since start: 230832.90s, 	Step: 377790, 	{'train/accuracy': 0.6948238611221313, 'train/loss': 1.373692512512207, 'train/bleu': 35.914049220026286, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 134445.55723881721, 'total_duration': 230832.90201711655, 'accumulated_submission_time': 134445.55723881721, 'accumulated_eval_time': 96367.69637274742, 'accumulated_logging_time': 8.102221727371216}
I0314 20:17:44.413838 139664567539456 logging_writer.py:48] [377790] accumulated_eval_time=96367.696373, accumulated_logging_time=8.102222, accumulated_submission_time=134445.557239, global_step=377790, preemption_count=0, score=134445.557239, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=230832.902017, train/accuracy=0.694824, train/bleu=35.914049, train/loss=1.373693, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 20:17:48.340245 139664575932160 logging_writer.py:48] [377800] global_step=377800, grad_norm=0.23662258684635162, loss=1.4811210632324219
I0314 20:18:23.863337 139664567539456 logging_writer.py:48] [377900] global_step=377900, grad_norm=0.2390003502368927, loss=1.425441026687622
I0314 20:18:59.446902 139664575932160 logging_writer.py:48] [378000] global_step=378000, grad_norm=0.22719137370586395, loss=1.4062533378601074
I0314 20:19:35.055757 139664567539456 logging_writer.py:48] [378100] global_step=378100, grad_norm=0.2378177046775818, loss=1.4583770036697388
I0314 20:20:10.689500 139664575932160 logging_writer.py:48] [378200] global_step=378200, grad_norm=0.23859302699565887, loss=1.445495367050171
I0314 20:20:46.300096 139664567539456 logging_writer.py:48] [378300] global_step=378300, grad_norm=0.2258625626564026, loss=1.4358209371566772
I0314 20:21:21.918069 139664575932160 logging_writer.py:48] [378400] global_step=378400, grad_norm=0.22283852100372314, loss=1.3521515130996704
I0314 20:21:57.533425 139664567539456 logging_writer.py:48] [378500] global_step=378500, grad_norm=0.23425105214118958, loss=1.5310966968536377
I0314 20:22:33.224262 139664575932160 logging_writer.py:48] [378600] global_step=378600, grad_norm=0.24624721705913544, loss=1.3802233934402466
I0314 20:23:08.872862 139664567539456 logging_writer.py:48] [378700] global_step=378700, grad_norm=0.240574449300766, loss=1.4553687572479248
I0314 20:23:44.544017 139664575932160 logging_writer.py:48] [378800] global_step=378800, grad_norm=0.22686812281608582, loss=1.4459550380706787
I0314 20:24:20.168597 139664567539456 logging_writer.py:48] [378900] global_step=378900, grad_norm=0.2384130358695984, loss=1.4870706796646118
I0314 20:24:55.771631 139664575932160 logging_writer.py:48] [379000] global_step=379000, grad_norm=0.2323930412530899, loss=1.419748306274414
I0314 20:25:31.388339 139664567539456 logging_writer.py:48] [379100] global_step=379100, grad_norm=0.23877762258052826, loss=1.5060824155807495
I0314 20:26:07.000514 139664575932160 logging_writer.py:48] [379200] global_step=379200, grad_norm=0.23708443343639374, loss=1.3580808639526367
I0314 20:26:42.688090 139664567539456 logging_writer.py:48] [379300] global_step=379300, grad_norm=0.2970944941043854, loss=1.4192564487457275
I0314 20:27:18.335110 139664575932160 logging_writer.py:48] [379400] global_step=379400, grad_norm=0.23600763082504272, loss=1.4679380655288696
I0314 20:27:53.959717 139664567539456 logging_writer.py:48] [379500] global_step=379500, grad_norm=0.23594163358211517, loss=1.4256134033203125
I0314 20:28:29.580432 139664575932160 logging_writer.py:48] [379600] global_step=379600, grad_norm=0.22687359154224396, loss=1.4368566274642944
I0314 20:29:05.181265 139664567539456 logging_writer.py:48] [379700] global_step=379700, grad_norm=0.2315751314163208, loss=1.5146647691726685
I0314 20:29:40.839685 139664575932160 logging_writer.py:48] [379800] global_step=379800, grad_norm=0.23400425910949707, loss=1.422389268875122
I0314 20:30:16.471621 139664567539456 logging_writer.py:48] [379900] global_step=379900, grad_norm=0.23006407916545868, loss=1.412306547164917
I0314 20:30:52.100794 139664575932160 logging_writer.py:48] [380000] global_step=380000, grad_norm=0.23735523223876953, loss=1.4369009733200073
I0314 20:31:27.701072 139664567539456 logging_writer.py:48] [380100] global_step=380100, grad_norm=0.22995814681053162, loss=1.4308290481567383
I0314 20:31:44.496613 139834281293632 spec.py:321] Evaluating on the training split.
I0314 20:31:47.467548 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:35:11.186813 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 20:35:13.889107 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:38:29.759014 139834281293632 spec.py:349] Evaluating on the test split.
I0314 20:38:32.446685 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:41:42.137986 139834281293632 submission_runner.py:420] Time since start: 232270.69s, 	Step: 380149, 	{'train/accuracy': 0.6977683305740356, 'train/loss': 1.3623616695404053, 'train/bleu': 35.690857062219756, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 135285.55623698235, 'total_duration': 232270.6904451847, 'accumulated_submission_time': 135285.55623698235, 'accumulated_eval_time': 96965.33767175674, 'accumulated_logging_time': 8.175270795822144}
I0314 20:41:42.203039 139664575932160 logging_writer.py:48] [380149] accumulated_eval_time=96965.337672, accumulated_logging_time=8.175271, accumulated_submission_time=135285.556237, global_step=380149, preemption_count=0, score=135285.556237, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=232270.690445, train/accuracy=0.697768, train/bleu=35.690857, train/loss=1.362362, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 20:42:00.705348 139664567539456 logging_writer.py:48] [380200] global_step=380200, grad_norm=0.2349652200937271, loss=1.449942708015442
I0314 20:42:36.252282 139664575932160 logging_writer.py:48] [380300] global_step=380300, grad_norm=0.23486068844795227, loss=1.464741826057434
I0314 20:43:11.837975 139664567539456 logging_writer.py:48] [380400] global_step=380400, grad_norm=0.22242951393127441, loss=1.3677743673324585
I0314 20:43:47.479964 139664575932160 logging_writer.py:48] [380500] global_step=380500, grad_norm=0.23439010977745056, loss=1.478118658065796
I0314 20:44:23.096177 139664567539456 logging_writer.py:48] [380600] global_step=380600, grad_norm=0.23083339631557465, loss=1.377123475074768
I0314 20:44:58.701642 139664575932160 logging_writer.py:48] [380700] global_step=380700, grad_norm=0.22494235634803772, loss=1.3658721446990967
I0314 20:45:34.270508 139664567539456 logging_writer.py:48] [380800] global_step=380800, grad_norm=0.23999381065368652, loss=1.5348113775253296
I0314 20:46:09.873710 139664575932160 logging_writer.py:48] [380900] global_step=380900, grad_norm=0.2363654524087906, loss=1.4737422466278076
I0314 20:46:45.483565 139664567539456 logging_writer.py:48] [381000] global_step=381000, grad_norm=0.23293323814868927, loss=1.465360403060913
I0314 20:47:21.111469 139664575932160 logging_writer.py:48] [381100] global_step=381100, grad_norm=0.23086823523044586, loss=1.382677435874939
I0314 20:47:56.733141 139664567539456 logging_writer.py:48] [381200] global_step=381200, grad_norm=0.22751998901367188, loss=1.4750748872756958
I0314 20:48:32.313721 139664575932160 logging_writer.py:48] [381300] global_step=381300, grad_norm=0.23545297980308533, loss=1.4399958848953247
I0314 20:49:07.937464 139664567539456 logging_writer.py:48] [381400] global_step=381400, grad_norm=0.23727387189865112, loss=1.5338526964187622
I0314 20:49:43.514726 139664575932160 logging_writer.py:48] [381500] global_step=381500, grad_norm=0.23479314148426056, loss=1.4369103908538818
I0314 20:50:19.111768 139664567539456 logging_writer.py:48] [381600] global_step=381600, grad_norm=0.2377600222826004, loss=1.403507947921753
I0314 20:50:54.750046 139664575932160 logging_writer.py:48] [381700] global_step=381700, grad_norm=0.23356953263282776, loss=1.4221045970916748
I0314 20:51:30.377541 139664567539456 logging_writer.py:48] [381800] global_step=381800, grad_norm=0.23868823051452637, loss=1.4656994342803955
I0314 20:52:05.973063 139664575932160 logging_writer.py:48] [381900] global_step=381900, grad_norm=0.23400798439979553, loss=1.4399951696395874
I0314 20:52:41.573390 139664567539456 logging_writer.py:48] [382000] global_step=382000, grad_norm=0.2346310317516327, loss=1.4940189123153687
I0314 20:53:17.238357 139664575932160 logging_writer.py:48] [382100] global_step=382100, grad_norm=0.23285679519176483, loss=1.484904408454895
I0314 20:53:52.900403 139664567539456 logging_writer.py:48] [382200] global_step=382200, grad_norm=0.22651979327201843, loss=1.435014247894287
I0314 20:54:28.491876 139664575932160 logging_writer.py:48] [382300] global_step=382300, grad_norm=0.22853946685791016, loss=1.4154455661773682
I0314 20:55:04.095762 139664567539456 logging_writer.py:48] [382400] global_step=382400, grad_norm=0.23145335912704468, loss=1.522897720336914
I0314 20:55:39.715503 139664575932160 logging_writer.py:48] [382500] global_step=382500, grad_norm=0.2344876378774643, loss=1.4447084665298462
I0314 20:55:42.288567 139834281293632 spec.py:321] Evaluating on the training split.
I0314 20:55:45.260601 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 20:59:13.386590 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 20:59:16.062829 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:02:29.975095 139834281293632 spec.py:349] Evaluating on the test split.
I0314 21:02:32.647878 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:05:42.152908 139834281293632 submission_runner.py:420] Time since start: 233710.71s, 	Step: 382509, 	{'train/accuracy': 0.6954215168952942, 'train/loss': 1.3679215908050537, 'train/bleu': 35.408766270120154, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 136125.55805802345, 'total_duration': 233710.7053785324, 'accumulated_submission_time': 136125.55805802345, 'accumulated_eval_time': 97565.20195937157, 'accumulated_logging_time': 8.250718832015991}
I0314 21:05:42.231542 139664567539456 logging_writer.py:48] [382509] accumulated_eval_time=97565.201959, accumulated_logging_time=8.250719, accumulated_submission_time=136125.558058, global_step=382509, preemption_count=0, score=136125.558058, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=233710.705379, train/accuracy=0.695422, train/bleu=35.408766, train/loss=1.367922, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 21:06:14.907990 139664575932160 logging_writer.py:48] [382600] global_step=382600, grad_norm=0.24455875158309937, loss=1.4608219861984253
I0314 21:06:50.462615 139664567539456 logging_writer.py:48] [382700] global_step=382700, grad_norm=0.23997461795806885, loss=1.4410289525985718
I0314 21:07:26.056305 139664575932160 logging_writer.py:48] [382800] global_step=382800, grad_norm=0.2320105880498886, loss=1.3990602493286133
I0314 21:08:01.661360 139664567539456 logging_writer.py:48] [382900] global_step=382900, grad_norm=0.23405729234218597, loss=1.374295949935913
I0314 21:08:37.264792 139664575932160 logging_writer.py:48] [383000] global_step=383000, grad_norm=0.24257925152778625, loss=1.398589849472046
I0314 21:09:12.888190 139664567539456 logging_writer.py:48] [383100] global_step=383100, grad_norm=0.2325466424226761, loss=1.3649975061416626
I0314 21:09:48.492737 139664575932160 logging_writer.py:48] [383200] global_step=383200, grad_norm=0.2424357384443283, loss=1.4493712186813354
I0314 21:10:24.076184 139664567539456 logging_writer.py:48] [383300] global_step=383300, grad_norm=0.23029731214046478, loss=1.4407806396484375
I0314 21:10:59.662685 139664575932160 logging_writer.py:48] [383400] global_step=383400, grad_norm=0.23228196799755096, loss=1.4662144184112549
I0314 21:11:35.272638 139664567539456 logging_writer.py:48] [383500] global_step=383500, grad_norm=0.2250548005104065, loss=1.3992712497711182
I0314 21:12:10.854375 139664575932160 logging_writer.py:48] [383600] global_step=383600, grad_norm=0.24504297971725464, loss=1.4639531373977661
I0314 21:12:46.470956 139664567539456 logging_writer.py:48] [383700] global_step=383700, grad_norm=0.23838059604167938, loss=1.4500309228897095
I0314 21:13:22.076360 139664575932160 logging_writer.py:48] [383800] global_step=383800, grad_norm=0.23353268206119537, loss=1.465438723564148
I0314 21:13:57.732859 139664567539456 logging_writer.py:48] [383900] global_step=383900, grad_norm=0.23189394176006317, loss=1.425331711769104
I0314 21:14:33.342175 139664575932160 logging_writer.py:48] [384000] global_step=384000, grad_norm=0.23620465397834778, loss=1.4658640623092651
I0314 21:15:08.951296 139664567539456 logging_writer.py:48] [384100] global_step=384100, grad_norm=0.24270039796829224, loss=1.5021858215332031
I0314 21:15:44.553096 139664575932160 logging_writer.py:48] [384200] global_step=384200, grad_norm=0.23203331232070923, loss=1.448751449584961
I0314 21:16:20.186684 139664567539456 logging_writer.py:48] [384300] global_step=384300, grad_norm=0.2307635396718979, loss=1.4867295026779175
I0314 21:16:55.764930 139664575932160 logging_writer.py:48] [384400] global_step=384400, grad_norm=0.23014242947101593, loss=1.4753361940383911
I0314 21:17:31.359151 139664567539456 logging_writer.py:48] [384500] global_step=384500, grad_norm=0.23148049414157867, loss=1.441260576248169
I0314 21:18:06.962040 139664575932160 logging_writer.py:48] [384600] global_step=384600, grad_norm=0.23370110988616943, loss=1.4831565618515015
I0314 21:18:42.603932 139664567539456 logging_writer.py:48] [384700] global_step=384700, grad_norm=0.22681701183319092, loss=1.4965686798095703
I0314 21:19:18.212347 139664575932160 logging_writer.py:48] [384800] global_step=384800, grad_norm=0.23598815500736237, loss=1.3899164199829102
I0314 21:19:42.505710 139834281293632 spec.py:321] Evaluating on the training split.
I0314 21:19:45.470197 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:23:19.819843 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 21:23:22.489797 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:26:36.458323 139834281293632 spec.py:349] Evaluating on the test split.
I0314 21:26:39.127807 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:29:48.830559 139834281293632 submission_runner.py:420] Time since start: 235157.38s, 	Step: 384870, 	{'train/accuracy': 0.6941781640052795, 'train/loss': 1.3764313459396362, 'train/bleu': 35.4485266232941, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 136965.75103139877, 'total_duration': 235157.38303089142, 'accumulated_submission_time': 136965.75103139877, 'accumulated_eval_time': 98171.5267598629, 'accumulated_logging_time': 8.339428424835205}
I0314 21:29:48.895885 139664567539456 logging_writer.py:48] [384870] accumulated_eval_time=98171.526760, accumulated_logging_time=8.339428, accumulated_submission_time=136965.751031, global_step=384870, preemption_count=0, score=136965.751031, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=235157.383031, train/accuracy=0.694178, train/bleu=35.448527, train/loss=1.376431, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 21:29:59.920119 139664575932160 logging_writer.py:48] [384900] global_step=384900, grad_norm=0.24642059206962585, loss=1.4946752786636353
I0314 21:30:35.441658 139664567539456 logging_writer.py:48] [385000] global_step=385000, grad_norm=0.2366636097431183, loss=1.5064115524291992
I0314 21:31:11.036314 139664575932160 logging_writer.py:48] [385100] global_step=385100, grad_norm=0.2296765148639679, loss=1.4239187240600586
I0314 21:31:46.623948 139664567539456 logging_writer.py:48] [385200] global_step=385200, grad_norm=0.23386132717132568, loss=1.375780701637268
I0314 21:32:22.244506 139664575932160 logging_writer.py:48] [385300] global_step=385300, grad_norm=0.24119329452514648, loss=1.4394099712371826
I0314 21:32:57.848859 139664567539456 logging_writer.py:48] [385400] global_step=385400, grad_norm=0.2371104210615158, loss=1.384148120880127
I0314 21:33:33.445259 139664575932160 logging_writer.py:48] [385500] global_step=385500, grad_norm=0.24018967151641846, loss=1.48501455783844
I0314 21:34:09.066068 139664567539456 logging_writer.py:48] [385600] global_step=385600, grad_norm=0.24520695209503174, loss=1.4128612279891968
I0314 21:34:44.702697 139664575932160 logging_writer.py:48] [385700] global_step=385700, grad_norm=0.23011766374111176, loss=1.3886663913726807
I0314 21:35:20.360476 139664567539456 logging_writer.py:48] [385800] global_step=385800, grad_norm=0.2363968938589096, loss=1.5090092420578003
I0314 21:35:56.009830 139664575932160 logging_writer.py:48] [385900] global_step=385900, grad_norm=0.2273227423429489, loss=1.477293610572815
I0314 21:36:31.619283 139664567539456 logging_writer.py:48] [386000] global_step=386000, grad_norm=0.23019897937774658, loss=1.4494199752807617
I0314 21:37:07.232515 139664575932160 logging_writer.py:48] [386100] global_step=386100, grad_norm=0.2292201966047287, loss=1.4791957139968872
I0314 21:37:42.824960 139664567539456 logging_writer.py:48] [386200] global_step=386200, grad_norm=0.24503231048583984, loss=1.450539231300354
I0314 21:38:18.442614 139664575932160 logging_writer.py:48] [386300] global_step=386300, grad_norm=0.23060718178749084, loss=1.380181908607483
I0314 21:38:54.055343 139664567539456 logging_writer.py:48] [386400] global_step=386400, grad_norm=0.2339334785938263, loss=1.4382017850875854
I0314 21:39:29.680984 139664575932160 logging_writer.py:48] [386500] global_step=386500, grad_norm=0.2259366363286972, loss=1.4567567110061646
I0314 21:40:05.300043 139664567539456 logging_writer.py:48] [386600] global_step=386600, grad_norm=0.2813732326030731, loss=1.4535176753997803
I0314 21:40:40.904137 139664575932160 logging_writer.py:48] [386700] global_step=386700, grad_norm=0.23385848104953766, loss=1.4748352766036987
I0314 21:41:16.504873 139664567539456 logging_writer.py:48] [386800] global_step=386800, grad_norm=0.2301357239484787, loss=1.389868974685669
I0314 21:41:52.104580 139664575932160 logging_writer.py:48] [386900] global_step=386900, grad_norm=0.23010535538196564, loss=1.4476691484451294
I0314 21:42:27.693647 139664567539456 logging_writer.py:48] [387000] global_step=387000, grad_norm=0.2335912585258484, loss=1.35712468624115
I0314 21:43:03.363933 139664575932160 logging_writer.py:48] [387100] global_step=387100, grad_norm=0.2324533313512802, loss=1.4647973775863647
I0314 21:43:39.023892 139664567539456 logging_writer.py:48] [387200] global_step=387200, grad_norm=0.23335832357406616, loss=1.470956802368164
I0314 21:43:49.064515 139834281293632 spec.py:321] Evaluating on the training split.
I0314 21:43:52.039922 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:47:29.161843 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 21:47:31.851200 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:50:46.114057 139834281293632 spec.py:349] Evaluating on the test split.
I0314 21:50:48.791535 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 21:53:58.335848 139834281293632 submission_runner.py:420] Time since start: 236606.89s, 	Step: 387230, 	{'train/accuracy': 0.6959114074707031, 'train/loss': 1.3697524070739746, 'train/bleu': 35.79706608134528, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 137805.83716368675, 'total_duration': 236606.8883280754, 'accumulated_submission_time': 137805.83716368675, 'accumulated_eval_time': 98780.79803609848, 'accumulated_logging_time': 8.41462779045105}
I0314 21:53:58.402896 139664575932160 logging_writer.py:48] [387230] accumulated_eval_time=98780.798036, accumulated_logging_time=8.414628, accumulated_submission_time=137805.837164, global_step=387230, preemption_count=0, score=137805.837164, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=236606.888328, train/accuracy=0.695911, train/bleu=35.797066, train/loss=1.369752, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 21:54:23.658411 139664567539456 logging_writer.py:48] [387300] global_step=387300, grad_norm=0.2347685843706131, loss=1.4694410562515259
I0314 21:54:59.205776 139664575932160 logging_writer.py:48] [387400] global_step=387400, grad_norm=0.22638875246047974, loss=1.4092391729354858
I0314 21:55:34.777597 139664567539456 logging_writer.py:48] [387500] global_step=387500, grad_norm=0.22469782829284668, loss=1.37625253200531
I0314 21:56:10.369392 139664575932160 logging_writer.py:48] [387600] global_step=387600, grad_norm=0.23919622600078583, loss=1.540884256362915
I0314 21:56:45.946739 139664567539456 logging_writer.py:48] [387700] global_step=387700, grad_norm=0.23101113736629486, loss=1.4381964206695557
I0314 21:57:21.559437 139664575932160 logging_writer.py:48] [387800] global_step=387800, grad_norm=0.24623802304267883, loss=1.4503940343856812
I0314 21:57:57.129534 139664567539456 logging_writer.py:48] [387900] global_step=387900, grad_norm=0.23819106817245483, loss=1.4430813789367676
I0314 21:58:32.730914 139664575932160 logging_writer.py:48] [388000] global_step=388000, grad_norm=0.2270641326904297, loss=1.3696709871292114
I0314 21:59:08.338177 139664567539456 logging_writer.py:48] [388100] global_step=388100, grad_norm=0.22882221639156342, loss=1.4129503965377808
I0314 21:59:43.958835 139664575932160 logging_writer.py:48] [388200] global_step=388200, grad_norm=0.24177823960781097, loss=1.4171961545944214
I0314 22:00:19.552839 139664567539456 logging_writer.py:48] [388300] global_step=388300, grad_norm=0.2315007448196411, loss=1.467719316482544
I0314 22:00:55.153847 139664575932160 logging_writer.py:48] [388400] global_step=388400, grad_norm=0.23869167268276215, loss=1.4170446395874023
I0314 22:01:30.754173 139664567539456 logging_writer.py:48] [388500] global_step=388500, grad_norm=0.24858491122722626, loss=1.4840848445892334
I0314 22:02:06.356296 139664575932160 logging_writer.py:48] [388600] global_step=388600, grad_norm=0.22667838633060455, loss=1.4475624561309814
I0314 22:02:41.948779 139664567539456 logging_writer.py:48] [388700] global_step=388700, grad_norm=0.24213336408138275, loss=1.4838039875030518
I0314 22:03:17.542346 139664575932160 logging_writer.py:48] [388800] global_step=388800, grad_norm=0.24715377390384674, loss=1.465742826461792
I0314 22:03:53.132649 139664567539456 logging_writer.py:48] [388900] global_step=388900, grad_norm=0.22565139830112457, loss=1.4445745944976807
I0314 22:04:28.750408 139664575932160 logging_writer.py:48] [389000] global_step=389000, grad_norm=0.2242889702320099, loss=1.4048775434494019
I0314 22:05:04.348507 139664567539456 logging_writer.py:48] [389100] global_step=389100, grad_norm=0.23181071877479553, loss=1.4089189767837524
I0314 22:05:39.947422 139664575932160 logging_writer.py:48] [389200] global_step=389200, grad_norm=0.22949029505252838, loss=1.3798174858093262
I0314 22:06:15.547089 139664567539456 logging_writer.py:48] [389300] global_step=389300, grad_norm=0.24663065373897552, loss=1.4860953092575073
I0314 22:06:51.133645 139664575932160 logging_writer.py:48] [389400] global_step=389400, grad_norm=0.23908351361751556, loss=1.41569983959198
I0314 22:07:26.732238 139664567539456 logging_writer.py:48] [389500] global_step=389500, grad_norm=0.22483786940574646, loss=1.3843917846679688
I0314 22:07:58.485626 139834281293632 spec.py:321] Evaluating on the training split.
I0314 22:08:01.465387 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:11:16.394866 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 22:11:19.071149 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:14:33.758484 139834281293632 spec.py:349] Evaluating on the test split.
I0314 22:14:36.443130 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:17:48.051614 139834281293632 submission_runner.py:420] Time since start: 238036.60s, 	Step: 389591, 	{'train/accuracy': 0.6969903111457825, 'train/loss': 1.3658790588378906, 'train/bleu': 35.59087751015178, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 138645.8393995762, 'total_duration': 238036.60406136513, 'accumulated_submission_time': 138645.8393995762, 'accumulated_eval_time': 99370.363945961, 'accumulated_logging_time': 8.490809679031372}
I0314 22:17:48.134550 139664575932160 logging_writer.py:48] [389591] accumulated_eval_time=99370.363946, accumulated_logging_time=8.490810, accumulated_submission_time=138645.839400, global_step=389591, preemption_count=0, score=138645.839400, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=238036.604061, train/accuracy=0.696990, train/bleu=35.590878, train/loss=1.365879, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 22:17:51.707233 139664567539456 logging_writer.py:48] [389600] global_step=389600, grad_norm=0.23853547871112823, loss=1.429948329925537
I0314 22:18:27.237653 139664575932160 logging_writer.py:48] [389700] global_step=389700, grad_norm=0.22673086822032928, loss=1.4162596464157104
I0314 22:19:02.816230 139664567539456 logging_writer.py:48] [389800] global_step=389800, grad_norm=0.24184398353099823, loss=1.4287798404693604
I0314 22:19:38.400028 139664575932160 logging_writer.py:48] [389900] global_step=389900, grad_norm=0.23725305497646332, loss=1.4550193548202515
I0314 22:20:13.999674 139664567539456 logging_writer.py:48] [390000] global_step=390000, grad_norm=0.23442023992538452, loss=1.5137826204299927
I0314 22:20:49.639811 139664575932160 logging_writer.py:48] [390100] global_step=390100, grad_norm=0.23304064571857452, loss=1.4437590837478638
I0314 22:21:25.254406 139664567539456 logging_writer.py:48] [390200] global_step=390200, grad_norm=0.22248788177967072, loss=1.4200345277786255
I0314 22:22:00.872285 139664575932160 logging_writer.py:48] [390300] global_step=390300, grad_norm=0.2434869408607483, loss=1.4987279176712036
I0314 22:22:36.516914 139664567539456 logging_writer.py:48] [390400] global_step=390400, grad_norm=0.23899008333683014, loss=1.4894672632217407
I0314 22:23:12.109428 139664575932160 logging_writer.py:48] [390500] global_step=390500, grad_norm=0.23110626637935638, loss=1.493294358253479
I0314 22:23:47.698542 139664567539456 logging_writer.py:48] [390600] global_step=390600, grad_norm=0.23290283977985382, loss=1.4166215658187866
I0314 22:24:23.312952 139664575932160 logging_writer.py:48] [390700] global_step=390700, grad_norm=0.22933334112167358, loss=1.4089874029159546
I0314 22:24:58.911108 139664567539456 logging_writer.py:48] [390800] global_step=390800, grad_norm=0.23533226549625397, loss=1.4592770338058472
I0314 22:25:34.515161 139664575932160 logging_writer.py:48] [390900] global_step=390900, grad_norm=0.23377572000026703, loss=1.4462846517562866
I0314 22:26:10.136771 139664567539456 logging_writer.py:48] [391000] global_step=391000, grad_norm=0.22793139517307281, loss=1.4376240968704224
I0314 22:26:45.758461 139664575932160 logging_writer.py:48] [391100] global_step=391100, grad_norm=0.24356569349765778, loss=1.4955201148986816
I0314 22:27:21.408924 139664567539456 logging_writer.py:48] [391200] global_step=391200, grad_norm=0.2534433901309967, loss=1.5040466785430908
I0314 22:27:57.041126 139664575932160 logging_writer.py:48] [391300] global_step=391300, grad_norm=0.23180948197841644, loss=1.4721345901489258
I0314 22:28:32.663282 139664567539456 logging_writer.py:48] [391400] global_step=391400, grad_norm=0.2333139181137085, loss=1.4738801717758179
I0314 22:29:08.288665 139664575932160 logging_writer.py:48] [391500] global_step=391500, grad_norm=0.2502530515193939, loss=1.4446660280227661
I0314 22:29:43.911619 139664567539456 logging_writer.py:48] [391600] global_step=391600, grad_norm=0.23748064041137695, loss=1.4752532243728638
I0314 22:30:19.485144 139664575932160 logging_writer.py:48] [391700] global_step=391700, grad_norm=0.236626997590065, loss=1.4417359828948975
I0314 22:30:55.099731 139664567539456 logging_writer.py:48] [391800] global_step=391800, grad_norm=0.2255777269601822, loss=1.4072787761688232
I0314 22:31:30.725147 139664575932160 logging_writer.py:48] [391900] global_step=391900, grad_norm=0.23822784423828125, loss=1.3912731409072876
I0314 22:31:48.242185 139834281293632 spec.py:321] Evaluating on the training split.
I0314 22:31:51.212964 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:35:18.083114 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 22:35:20.769376 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:38:34.754691 139834281293632 spec.py:349] Evaluating on the test split.
I0314 22:38:37.433002 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:41:47.194119 139834281293632 submission_runner.py:420] Time since start: 239475.75s, 	Step: 391951, 	{'train/accuracy': 0.6969181299209595, 'train/loss': 1.3643102645874023, 'train/bleu': 35.6003938792746, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 139485.86350560188, 'total_duration': 239475.74658942223, 'accumulated_submission_time': 139485.86350560188, 'accumulated_eval_time': 99969.31581687927, 'accumulated_logging_time': 8.585108041763306}
I0314 22:41:47.260783 139664567539456 logging_writer.py:48] [391951] accumulated_eval_time=99969.315817, accumulated_logging_time=8.585108, accumulated_submission_time=139485.863506, global_step=391951, preemption_count=0, score=139485.863506, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=239475.746589, train/accuracy=0.696918, train/bleu=35.600394, train/loss=1.364310, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 22:42:05.038361 139664575932160 logging_writer.py:48] [392000] global_step=392000, grad_norm=0.2266554832458496, loss=1.4037703275680542
I0314 22:42:40.640542 139664567539456 logging_writer.py:48] [392100] global_step=392100, grad_norm=0.24208377301692963, loss=1.4193480014801025
I0314 22:43:16.234633 139664575932160 logging_writer.py:48] [392200] global_step=392200, grad_norm=0.22215409576892853, loss=1.3802002668380737
I0314 22:43:51.826047 139664567539456 logging_writer.py:48] [392300] global_step=392300, grad_norm=0.22718185186386108, loss=1.3999309539794922
I0314 22:44:27.428522 139664575932160 logging_writer.py:48] [392400] global_step=392400, grad_norm=0.23963439464569092, loss=1.4059081077575684
I0314 22:45:03.049438 139664567539456 logging_writer.py:48] [392500] global_step=392500, grad_norm=0.23203371465206146, loss=1.520093321800232
I0314 22:45:38.649302 139664575932160 logging_writer.py:48] [392600] global_step=392600, grad_norm=0.24368971586227417, loss=1.5956562757492065
I0314 22:46:14.290595 139664567539456 logging_writer.py:48] [392700] global_step=392700, grad_norm=0.23725493252277374, loss=1.4114941358566284
I0314 22:46:49.877705 139664575932160 logging_writer.py:48] [392800] global_step=392800, grad_norm=0.23768049478530884, loss=1.4188611507415771
I0314 22:47:25.456175 139664567539456 logging_writer.py:48] [392900] global_step=392900, grad_norm=0.23529042303562164, loss=1.480883240699768
I0314 22:48:01.061479 139664575932160 logging_writer.py:48] [393000] global_step=393000, grad_norm=0.2445220947265625, loss=1.4529411792755127
I0314 22:48:36.687101 139664567539456 logging_writer.py:48] [393100] global_step=393100, grad_norm=0.22298243641853333, loss=1.3307743072509766
I0314 22:49:12.312364 139664575932160 logging_writer.py:48] [393200] global_step=393200, grad_norm=0.24270759522914886, loss=1.5006992816925049
I0314 22:49:47.937662 139664567539456 logging_writer.py:48] [393300] global_step=393300, grad_norm=0.23231551051139832, loss=1.4837884902954102
I0314 22:50:23.528926 139664575932160 logging_writer.py:48] [393400] global_step=393400, grad_norm=0.22681666910648346, loss=1.4177738428115845
I0314 22:50:59.118239 139664567539456 logging_writer.py:48] [393500] global_step=393500, grad_norm=0.23610597848892212, loss=1.4076141119003296
I0314 22:51:34.699983 139664575932160 logging_writer.py:48] [393600] global_step=393600, grad_norm=0.22925011813640594, loss=1.4276424646377563
I0314 22:52:10.329497 139664567539456 logging_writer.py:48] [393700] global_step=393700, grad_norm=0.2360466718673706, loss=1.432087779045105
I0314 22:52:45.938001 139664575932160 logging_writer.py:48] [393800] global_step=393800, grad_norm=0.24523267149925232, loss=1.4443432092666626
I0314 22:53:21.590571 139664567539456 logging_writer.py:48] [393900] global_step=393900, grad_norm=0.2237778753042221, loss=1.4419035911560059
I0314 22:53:57.239563 139664575932160 logging_writer.py:48] [394000] global_step=394000, grad_norm=0.23706041276454926, loss=1.4296677112579346
I0314 22:54:32.861459 139664567539456 logging_writer.py:48] [394100] global_step=394100, grad_norm=0.23982545733451843, loss=1.4057031869888306
I0314 22:55:08.482570 139664575932160 logging_writer.py:48] [394200] global_step=394200, grad_norm=0.22751756012439728, loss=1.3882185220718384
I0314 22:55:44.070508 139664567539456 logging_writer.py:48] [394300] global_step=394300, grad_norm=0.2444402426481247, loss=1.4874945878982544
I0314 22:55:47.351114 139834281293632 spec.py:321] Evaluating on the training split.
I0314 22:55:50.323113 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 22:59:31.260721 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 22:59:33.936483 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:02:48.062955 139834281293632 spec.py:349] Evaluating on the test split.
I0314 23:02:50.737184 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:06:00.187591 139834281293632 submission_runner.py:420] Time since start: 240928.74s, 	Step: 394311, 	{'train/accuracy': 0.6953207850456238, 'train/loss': 1.3696646690368652, 'train/bleu': 35.67714170119371, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 140325.8711452484, 'total_duration': 240928.74007606506, 'accumulated_submission_time': 140325.8711452484, 'accumulated_eval_time': 100582.15226006508, 'accumulated_logging_time': 8.660670042037964}
I0314 23:06:00.256596 139664575932160 logging_writer.py:48] [394311] accumulated_eval_time=100582.152260, accumulated_logging_time=8.660670, accumulated_submission_time=140325.871145, global_step=394311, preemption_count=0, score=140325.871145, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=240928.740076, train/accuracy=0.695321, train/bleu=35.677142, train/loss=1.369665, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 23:06:32.235509 139664567539456 logging_writer.py:48] [394400] global_step=394400, grad_norm=0.22224009037017822, loss=1.3392186164855957
I0314 23:07:07.827265 139664575932160 logging_writer.py:48] [394500] global_step=394500, grad_norm=0.23260782659053802, loss=1.4243645668029785
I0314 23:07:43.447226 139664567539456 logging_writer.py:48] [394600] global_step=394600, grad_norm=0.2356305867433548, loss=1.511162519454956
I0314 23:08:19.056787 139664575932160 logging_writer.py:48] [394700] global_step=394700, grad_norm=0.24075809121131897, loss=1.4920257329940796
I0314 23:08:54.661609 139664567539456 logging_writer.py:48] [394800] global_step=394800, grad_norm=0.2224438488483429, loss=1.3634378910064697
I0314 23:09:30.312809 139664575932160 logging_writer.py:48] [394900] global_step=394900, grad_norm=0.22917672991752625, loss=1.4336471557617188
I0314 23:10:05.933968 139664567539456 logging_writer.py:48] [395000] global_step=395000, grad_norm=0.2238437980413437, loss=1.414897084236145
I0314 23:10:41.560784 139664575932160 logging_writer.py:48] [395100] global_step=395100, grad_norm=0.2286594808101654, loss=1.4278677701950073
I0314 23:11:17.181128 139664567539456 logging_writer.py:48] [395200] global_step=395200, grad_norm=0.22678899765014648, loss=1.394365668296814
I0314 23:11:52.820012 139664575932160 logging_writer.py:48] [395300] global_step=395300, grad_norm=0.2345663607120514, loss=1.403656005859375
I0314 23:12:28.450305 139664567539456 logging_writer.py:48] [395400] global_step=395400, grad_norm=0.23618002235889435, loss=1.4225952625274658
I0314 23:13:04.075459 139664575932160 logging_writer.py:48] [395500] global_step=395500, grad_norm=0.23381838202476501, loss=1.4708648920059204
I0314 23:13:39.689068 139664567539456 logging_writer.py:48] [395600] global_step=395600, grad_norm=0.23340994119644165, loss=1.4125468730926514
I0314 23:14:15.286814 139664575932160 logging_writer.py:48] [395700] global_step=395700, grad_norm=0.23828710615634918, loss=1.5506315231323242
I0314 23:14:50.916686 139664567539456 logging_writer.py:48] [395800] global_step=395800, grad_norm=0.23781107366085052, loss=1.440017819404602
I0314 23:15:26.525259 139664575932160 logging_writer.py:48] [395900] global_step=395900, grad_norm=0.24446254968643188, loss=1.4996743202209473
I0314 23:16:02.162175 139664567539456 logging_writer.py:48] [396000] global_step=396000, grad_norm=0.23834079504013062, loss=1.4789241552352905
I0314 23:16:37.793639 139664575932160 logging_writer.py:48] [396100] global_step=396100, grad_norm=0.2334892600774765, loss=1.4225726127624512
I0314 23:17:13.421144 139664567539456 logging_writer.py:48] [396200] global_step=396200, grad_norm=0.23419997096061707, loss=1.470489740371704
I0314 23:17:49.041459 139664575932160 logging_writer.py:48] [396300] global_step=396300, grad_norm=0.232500821352005, loss=1.4788662195205688
I0314 23:18:24.628766 139664567539456 logging_writer.py:48] [396400] global_step=396400, grad_norm=0.23260441422462463, loss=1.3872356414794922
I0314 23:19:00.238250 139664575932160 logging_writer.py:48] [396500] global_step=396500, grad_norm=0.25064048171043396, loss=1.4166734218597412
I0314 23:19:35.823455 139664567539456 logging_writer.py:48] [396600] global_step=396600, grad_norm=0.21902762353420258, loss=1.3789383172988892
I0314 23:20:00.451692 139834281293632 spec.py:321] Evaluating on the training split.
I0314 23:20:03.442902 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:23:32.327684 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 23:23:34.995929 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:26:49.187736 139834281293632 spec.py:349] Evaluating on the test split.
I0314 23:26:51.862509 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:30:01.995361 139834281293632 submission_runner.py:420] Time since start: 242370.55s, 	Step: 396671, 	{'train/accuracy': 0.6944743990898132, 'train/loss': 1.3710854053497314, 'train/bleu': 35.42201402751068, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 141165.98493552208, 'total_duration': 242370.54783678055, 'accumulated_submission_time': 141165.98493552208, 'accumulated_eval_time': 101183.69587278366, 'accumulated_logging_time': 8.738826513290405}
I0314 23:30:02.063144 139664575932160 logging_writer.py:48] [396671] accumulated_eval_time=101183.695873, accumulated_logging_time=8.738827, accumulated_submission_time=141165.984936, global_step=396671, preemption_count=0, score=141165.984936, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=242370.547837, train/accuracy=0.694474, train/bleu=35.422014, train/loss=1.371085, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 23:30:12.729480 139664567539456 logging_writer.py:48] [396700] global_step=396700, grad_norm=0.2408810406923294, loss=1.4560445547103882
I0314 23:30:48.241053 139664575932160 logging_writer.py:48] [396800] global_step=396800, grad_norm=0.23617666959762573, loss=1.5142072439193726
I0314 23:31:23.824159 139664567539456 logging_writer.py:48] [396900] global_step=396900, grad_norm=0.23364782333374023, loss=1.473922610282898
I0314 23:31:59.469753 139664575932160 logging_writer.py:48] [397000] global_step=397000, grad_norm=0.24385298788547516, loss=1.4472230672836304
I0314 23:32:35.065742 139664567539456 logging_writer.py:48] [397100] global_step=397100, grad_norm=0.23545095324516296, loss=1.548783302307129
I0314 23:33:10.670899 139664575932160 logging_writer.py:48] [397200] global_step=397200, grad_norm=0.23502513766288757, loss=1.4641751050949097
I0314 23:33:46.267354 139664567539456 logging_writer.py:48] [397300] global_step=397300, grad_norm=0.23908698558807373, loss=1.4242091178894043
I0314 23:34:21.889522 139664575932160 logging_writer.py:48] [397400] global_step=397400, grad_norm=0.2387440949678421, loss=1.4701175689697266
I0314 23:34:57.491431 139664567539456 logging_writer.py:48] [397500] global_step=397500, grad_norm=0.22368796169757843, loss=1.386048674583435
I0314 23:35:33.096610 139664575932160 logging_writer.py:48] [397600] global_step=397600, grad_norm=0.24748237431049347, loss=1.5364869832992554
I0314 23:36:08.707362 139664567539456 logging_writer.py:48] [397700] global_step=397700, grad_norm=0.22981932759284973, loss=1.3904365301132202
I0314 23:36:44.303133 139664575932160 logging_writer.py:48] [397800] global_step=397800, grad_norm=0.22708825767040253, loss=1.4089728593826294
I0314 23:37:19.917784 139664567539456 logging_writer.py:48] [397900] global_step=397900, grad_norm=0.23371337354183197, loss=1.4659764766693115
I0314 23:37:55.500529 139664575932160 logging_writer.py:48] [398000] global_step=398000, grad_norm=0.22693756222724915, loss=1.432761549949646
I0314 23:38:31.093163 139664567539456 logging_writer.py:48] [398100] global_step=398100, grad_norm=0.23975425958633423, loss=1.4704865217208862
I0314 23:39:06.733367 139664575932160 logging_writer.py:48] [398200] global_step=398200, grad_norm=0.23294392228126526, loss=1.4977692365646362
I0314 23:39:42.333788 139664567539456 logging_writer.py:48] [398300] global_step=398300, grad_norm=0.24670538306236267, loss=1.5141479969024658
I0314 23:40:17.930937 139664575932160 logging_writer.py:48] [398400] global_step=398400, grad_norm=0.24165424704551697, loss=1.464958906173706
I0314 23:40:53.517267 139664567539456 logging_writer.py:48] [398500] global_step=398500, grad_norm=0.2317621409893036, loss=1.5090630054473877
I0314 23:41:29.102841 139664575932160 logging_writer.py:48] [398600] global_step=398600, grad_norm=0.2258976697921753, loss=1.4720343351364136
I0314 23:42:04.711679 139664567539456 logging_writer.py:48] [398700] global_step=398700, grad_norm=0.23987263441085815, loss=1.4135159254074097
I0314 23:42:40.315608 139664575932160 logging_writer.py:48] [398800] global_step=398800, grad_norm=0.2546018362045288, loss=1.5047580003738403
I0314 23:43:15.931797 139664567539456 logging_writer.py:48] [398900] global_step=398900, grad_norm=0.23693794012069702, loss=1.3985590934753418
I0314 23:43:51.560923 139664575932160 logging_writer.py:48] [399000] global_step=399000, grad_norm=0.23465855419635773, loss=1.4788624048233032
I0314 23:44:02.324544 139834281293632 spec.py:321] Evaluating on the training split.
I0314 23:44:05.290438 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:47:35.803292 139834281293632 spec.py:333] Evaluating on the validation split.
I0314 23:47:38.475495 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:50:52.438629 139834281293632 spec.py:349] Evaluating on the test split.
I0314 23:50:55.108354 139834281293632 workload.py:181] Translating evaluation dataset.
I0314 23:54:04.893769 139834281293632 submission_runner.py:420] Time since start: 243813.45s, 	Step: 399032, 	{'train/accuracy': 0.6948761343955994, 'train/loss': 1.3741616010665894, 'train/bleu': 35.58164949728439, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 142006.1650776863, 'total_duration': 243813.446236372, 'accumulated_submission_time': 142006.1650776863, 'accumulated_eval_time': 101786.26503157616, 'accumulated_logging_time': 8.816017627716064}
I0314 23:54:04.961805 139664567539456 logging_writer.py:48] [399032] accumulated_eval_time=101786.265032, accumulated_logging_time=8.816018, accumulated_submission_time=142006.165078, global_step=399032, preemption_count=0, score=142006.165078, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=243813.446236, train/accuracy=0.694876, train/bleu=35.581649, train/loss=1.374162, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0314 23:54:29.465672 139664575932160 logging_writer.py:48] [399100] global_step=399100, grad_norm=0.23030342161655426, loss=1.5276930332183838
I0314 23:55:05.027544 139664567539456 logging_writer.py:48] [399200] global_step=399200, grad_norm=0.2353667914867401, loss=1.3915530443191528
I0314 23:55:40.610272 139664575932160 logging_writer.py:48] [399300] global_step=399300, grad_norm=0.22891545295715332, loss=1.466235876083374
I0314 23:56:16.209135 139664567539456 logging_writer.py:48] [399400] global_step=399400, grad_norm=0.23974579572677612, loss=1.4359707832336426
I0314 23:56:51.821679 139664575932160 logging_writer.py:48] [399500] global_step=399500, grad_norm=0.23117192089557648, loss=1.4694085121154785
I0314 23:57:27.438092 139664567539456 logging_writer.py:48] [399600] global_step=399600, grad_norm=0.23678803443908691, loss=1.4947378635406494
I0314 23:58:03.079509 139664575932160 logging_writer.py:48] [399700] global_step=399700, grad_norm=0.23613862693309784, loss=1.4086408615112305
I0314 23:58:38.716352 139664567539456 logging_writer.py:48] [399800] global_step=399800, grad_norm=0.22537633776664734, loss=1.3796015977859497
I0314 23:59:14.340223 139664575932160 logging_writer.py:48] [399900] global_step=399900, grad_norm=0.24007339775562286, loss=1.4081590175628662
I0314 23:59:48.955071 139834281293632 spec.py:321] Evaluating on the training split.
I0314 23:59:51.926108 139834281293632 workload.py:181] Translating evaluation dataset.
I0315 00:03:16.110088 139834281293632 spec.py:333] Evaluating on the validation split.
I0315 00:03:18.789158 139834281293632 workload.py:181] Translating evaluation dataset.
I0315 00:06:32.699487 139834281293632 spec.py:349] Evaluating on the test split.
I0315 00:06:35.383861 139834281293632 workload.py:181] Translating evaluation dataset.
I0315 00:09:46.731528 139834281293632 submission_runner.py:420] Time since start: 244755.28s, 	Step: 399999, 	{'train/accuracy': 0.6971225738525391, 'train/loss': 1.3681684732437134, 'train/bleu': 35.5268341500284, 'validation/accuracy': 0.6957632303237915, 'validation/loss': 1.3586411476135254, 'validation/bleu': 30.80970163535293, 'validation/num_examples': 3000, 'test/accuracy': 0.7146011590957642, 'test/loss': 1.2520129680633545, 'test/bleu': 31.016732013552563, 'test/num_examples': 3003, 'score': 142350.1185748577, 'total_duration': 244755.2840101719, 'accumulated_submission_time': 142350.1185748577, 'accumulated_eval_time': 102384.04146027565, 'accumulated_logging_time': 8.893699645996094}
I0315 00:09:46.801107 139664567539456 logging_writer.py:48] [399999] accumulated_eval_time=102384.041460, accumulated_logging_time=8.893700, accumulated_submission_time=142350.118575, global_step=399999, preemption_count=0, score=142350.118575, test/accuracy=0.714601, test/bleu=31.016732, test/loss=1.252013, test/num_examples=3003, total_duration=244755.284010, train/accuracy=0.697123, train/bleu=35.526834, train/loss=1.368168, validation/accuracy=0.695763, validation/bleu=30.809702, validation/loss=1.358641, validation/num_examples=3000
I0315 00:09:46.864708 139664575932160 logging_writer.py:48] [399999] global_step=399999, preemption_count=0, score=142350.118575
I0315 00:09:47.944402 139834281293632 checkpoints.py:490] Saving checkpoint at step: 399999
I0315 00:09:51.644397 139834281293632 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax/trial_1/checkpoint_399999
I0315 00:09:51.648829 139834281293632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_3/wmt_jax/trial_1/checkpoint_399999.
I0315 00:09:51.687343 139834281293632 submission_runner.py:683] Final wmt score: 142350.1185748577
