python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_3 --overwrite=true --save_checkpoints=false --rng_seed=3168275751 --max_global_steps=144000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=self 2>&1 | tee -a /logs/librispeech_deepspeech_jax_03-15-2024-00-20-37.log
I0315 00:20:58.265620 139818008614720 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax because --overwrite was set.
I0315 00:20:58.274777 139818008614720 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax.
I0315 00:20:59.348942 139818008614720 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0315 00:20:59.349712 139818008614720 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0315 00:20:59.349927 139818008614720 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0315 00:21:00.312879 139818008614720 submission_runner.py:612] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax/trial_1.
I0315 00:21:00.525541 139818008614720 submission_runner.py:209] Initializing dataset.
I0315 00:21:00.525764 139818008614720 submission_runner.py:220] Initializing model.
I0315 00:21:03.216572 139818008614720 submission_runner.py:262] Initializing optimizer.
I0315 00:21:03.913947 139818008614720 submission_runner.py:269] Initializing metrics bundle.
I0315 00:21:03.914144 139818008614720 submission_runner.py:287] Initializing checkpoint and logger.
I0315 00:21:03.914894 139818008614720 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0315 00:21:03.915079 139818008614720 submission_runner.py:307] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0315 00:21:03.915338 139818008614720 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0315 00:21:03.915416 139818008614720 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0315 00:21:04.190887 139818008614720 logger_utils.py:220] Unable to record git information. Continuing without it.
I0315 00:21:04.446078 139818008614720 submission_runner.py:311] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0315 00:21:04.458419 139818008614720 submission_runner.py:321] Starting training loop.
I0315 00:21:04.746156 139818008614720 input_pipeline.py:20] Loading split = train-clean-100
I0315 00:21:04.786588 139818008614720 input_pipeline.py:20] Loading split = train-clean-360
I0315 00:21:04.916522 139818008614720 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0315 00:21:46.157544 139654554208000 logging_writer.py:48] [0] global_step=0, grad_norm=31.242565155029297, loss=33.32367706298828
I0315 00:21:46.188990 139818008614720 spec.py:321] Evaluating on the training split.
I0315 00:21:46.444578 139818008614720 input_pipeline.py:20] Loading split = train-clean-100
I0315 00:21:46.479019 139818008614720 input_pipeline.py:20] Loading split = train-clean-360
I0315 00:21:46.838211 139818008614720 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0315 00:23:25.593176 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 00:23:25.784303 139818008614720 input_pipeline.py:20] Loading split = dev-clean
I0315 00:23:25.789656 139818008614720 input_pipeline.py:20] Loading split = dev-other
I0315 00:24:32.794296 139818008614720 spec.py:349] Evaluating on the test split.
I0315 00:24:32.984425 139818008614720 input_pipeline.py:20] Loading split = test-clean
I0315 00:25:12.032759 139818008614720 submission_runner.py:420] Time since start: 247.57s, 	Step: 1, 	{'train/ctc_loss': Array(30.523008, dtype=float32), 'train/wer': 2.7218780584303355, 'validation/ctc_loss': Array(29.339128, dtype=float32), 'validation/wer': 2.4762157621865857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.561045, dtype=float32), 'test/wer': 2.7003635772754047, 'test/num_examples': 2472, 'score': 41.73051595687866, 'total_duration': 247.57225155830383, 'accumulated_submission_time': 41.73051595687866, 'accumulated_eval_time': 205.84168791770935, 'accumulated_logging_time': 0}
I0315 00:25:12.057498 139644911834880 logging_writer.py:48] [1] accumulated_eval_time=205.841688, accumulated_logging_time=0, accumulated_submission_time=41.730516, global_step=1, preemption_count=0, score=41.730516, test/ctc_loss=29.561044692993164, test/num_examples=2472, test/wer=2.700364, total_duration=247.572252, train/ctc_loss=30.523008346557617, train/wer=2.721878, validation/ctc_loss=29.339128494262695, validation/num_examples=5348, validation/wer=2.476216
I0315 00:26:36.805263 139660881159936 logging_writer.py:48] [100] global_step=100, grad_norm=1.0791733264923096, loss=6.168745040893555
I0315 00:27:53.457957 139660889552640 logging_writer.py:48] [200] global_step=200, grad_norm=0.3299722969532013, loss=5.8328046798706055
I0315 00:29:10.090183 139660881159936 logging_writer.py:48] [300] global_step=300, grad_norm=0.49131104350090027, loss=5.675682067871094
I0315 00:30:28.355966 139660889552640 logging_writer.py:48] [400] global_step=400, grad_norm=0.6991408467292786, loss=5.373201847076416
I0315 00:31:45.638463 139660881159936 logging_writer.py:48] [500] global_step=500, grad_norm=1.3359931707382202, loss=4.629803657531738
I0315 00:33:01.641084 139660889552640 logging_writer.py:48] [600] global_step=600, grad_norm=3.4998085498809814, loss=3.8419954776763916
I0315 00:34:19.593155 139660881159936 logging_writer.py:48] [700] global_step=700, grad_norm=1.5315918922424316, loss=3.463824510574341
I0315 00:35:36.045760 139660889552640 logging_writer.py:48] [800] global_step=800, grad_norm=2.719207763671875, loss=3.1801435947418213
I0315 00:36:54.421993 139660881159936 logging_writer.py:48] [900] global_step=900, grad_norm=2.5564849376678467, loss=2.9534263610839844
I0315 00:38:15.855202 139660889552640 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.2074408531188965, loss=2.828451156616211
I0315 00:39:36.184348 139653565220608 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.7519820928573608, loss=2.6512625217437744
I0315 00:40:51.279702 139653556827904 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.7686066627502441, loss=2.5469396114349365
I0315 00:42:08.294280 139653565220608 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8007121086120605, loss=2.5457677841186523
I0315 00:43:23.408141 139653556827904 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.759451389312744, loss=2.435497283935547
I0315 00:44:39.289144 139653565220608 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.027266502380371, loss=2.447011947631836
I0315 00:45:54.620081 139653556827904 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.6311323642730713, loss=2.2825915813446045
I0315 00:47:11.148277 139653565220608 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.238372325897217, loss=2.1292498111724854
I0315 00:48:27.775609 139653556827904 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.3260107040405273, loss=2.1467182636260986
I0315 00:49:12.039081 139818008614720 spec.py:321] Evaluating on the training split.
I0315 00:50:05.534752 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 00:50:53.511811 139818008614720 spec.py:349] Evaluating on the test split.
I0315 00:51:17.607362 139818008614720 submission_runner.py:420] Time since start: 1813.14s, 	Step: 1858, 	{'train/ctc_loss': Array(1.1975354, dtype=float32), 'train/wer': 0.330683199110214, 'validation/ctc_loss': Array(1.635861, dtype=float32), 'validation/wer': 0.40884559313361074, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2089342, dtype=float32), 'test/wer': 0.3347348323279101, 'test/num_examples': 2472, 'score': 1481.632533788681, 'total_duration': 1813.1430945396423, 'accumulated_submission_time': 1481.632533788681, 'accumulated_eval_time': 331.4041941165924, 'accumulated_logging_time': 0.038832664489746094}
I0315 00:51:17.642303 139661578483456 logging_writer.py:48] [1858] accumulated_eval_time=331.404194, accumulated_logging_time=0.038833, accumulated_submission_time=1481.632534, global_step=1858, preemption_count=0, score=1481.632534, test/ctc_loss=1.2089341878890991, test/num_examples=2472, test/wer=0.334735, total_duration=1813.143095, train/ctc_loss=1.1975353956222534, train/wer=0.330683, validation/ctc_loss=1.6358610391616821, validation/num_examples=5348, validation/wer=0.408846
I0315 00:51:50.765029 139661570090752 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.0227503776550293, loss=2.1801512241363525
I0315 00:53:07.955297 139661578483456 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.9590463638305664, loss=2.1440021991729736
I0315 00:54:26.629617 139661578483456 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.7220046520233154, loss=2.079770803451538
I0315 00:55:42.004404 139661570090752 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.8286652565002441, loss=2.0059080123901367
I0315 00:56:57.597581 139661578483456 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.749324321746826, loss=2.018767833709717
I0315 00:58:12.606970 139661570090752 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.639889717102051, loss=1.9552000761032104
I0315 00:59:27.172693 139661578483456 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.862276315689087, loss=1.9188264608383179
I0315 01:00:44.031045 139661570090752 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.4719314575195312, loss=1.9472362995147705
I0315 01:02:04.335969 139661578483456 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.554062008857727, loss=1.8985202312469482
I0315 01:03:27.417994 139661570090752 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.7594597339630127, loss=1.9689580202102661
I0315 01:04:48.911094 139661578483456 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.146056652069092, loss=1.8978824615478516
I0315 01:06:10.685196 139661570090752 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.764617681503296, loss=1.8747286796569824
I0315 01:07:34.321501 139662233843456 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.1289610862731934, loss=1.839422583580017
I0315 01:08:50.803099 139662225450752 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.1094679832458496, loss=1.8747879266738892
I0315 01:10:05.666799 139662233843456 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.7753067016601562, loss=1.810341715812683
I0315 01:11:20.743730 139662225450752 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.9393683671951294, loss=1.906197190284729
I0315 01:12:36.456262 139662233843456 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.3827316761016846, loss=1.8138455152511597
I0315 01:13:56.966911 139662225450752 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.6122970581054688, loss=1.8271187543869019
I0315 01:15:18.170604 139818008614720 spec.py:321] Evaluating on the training split.
I0315 01:16:14.426710 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 01:17:03.556473 139818008614720 spec.py:349] Evaluating on the test split.
I0315 01:17:29.143195 139818008614720 submission_runner.py:420] Time since start: 3384.68s, 	Step: 3700, 	{'train/ctc_loss': Array(0.5168572, dtype=float32), 'train/wer': 0.16965612764030577, 'validation/ctc_loss': Array(0.8609228, dtype=float32), 'validation/wer': 0.24393446421502843, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55478996, dtype=float32), 'test/wer': 0.1749233237868909, 'test/num_examples': 2472, 'score': 2922.079998731613, 'total_duration': 3384.678767681122, 'accumulated_submission_time': 2922.079998731613, 'accumulated_eval_time': 462.3708300590515, 'accumulated_logging_time': 0.08902764320373535}
I0315 01:17:29.173710 139662233843456 logging_writer.py:48] [3700] accumulated_eval_time=462.370830, accumulated_logging_time=0.089028, accumulated_submission_time=2922.079999, global_step=3700, preemption_count=0, score=2922.079999, test/ctc_loss=0.5547899603843689, test/num_examples=2472, test/wer=0.174923, total_duration=3384.678768, train/ctc_loss=0.5168572068214417, train/wer=0.169656, validation/ctc_loss=0.8609228134155273, validation/num_examples=5348, validation/wer=0.243934
I0315 01:17:30.062193 139662225450752 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.249178886413574, loss=1.8517179489135742
I0315 01:18:47.133199 139662233843456 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.7018357515335083, loss=1.8291015625
I0315 01:20:03.659680 139662225450752 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.183103322982788, loss=1.7749069929122925
I0315 01:21:18.797321 139662233843456 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.0163345336914062, loss=1.8219009637832642
I0315 01:22:34.769518 139662225450752 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.531805992126465, loss=1.830063819885254
I0315 01:23:53.538058 139661578483456 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.7500739097595215, loss=1.763925552368164
I0315 01:25:08.393749 139661570090752 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.1720499992370605, loss=1.7744392156600952
I0315 01:26:23.852838 139661578483456 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.1448917388916016, loss=1.674607753753662
I0315 01:27:40.978584 139661570090752 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.7698121070861816, loss=1.7270690202713013
I0315 01:29:02.915155 139661578483456 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.1101694107055664, loss=1.6738284826278687
I0315 01:30:21.420702 139661570090752 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.4830942153930664, loss=1.770686149597168
I0315 01:31:45.644038 139661578483456 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.8540716171264648, loss=1.7659226655960083
I0315 01:33:10.646468 139661570090752 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.054300308227539, loss=1.6977583169937134
I0315 01:34:32.721971 139661578483456 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.9452178478240967, loss=1.7437613010406494
I0315 01:35:55.845777 139661570090752 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.5342047214508057, loss=1.7917327880859375
I0315 01:37:18.152228 139661578483456 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.028742790222168, loss=1.7844783067703247
I0315 01:38:34.009901 139661570090752 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.3881545066833496, loss=1.7553918361663818
I0315 01:39:49.455638 139661578483456 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.2271392345428467, loss=1.7064392566680908
I0315 01:41:04.335260 139661570090752 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.0420641899108887, loss=1.6817891597747803
I0315 01:41:29.791400 139818008614720 spec.py:321] Evaluating on the training split.
I0315 01:42:23.733920 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 01:43:13.367536 139818008614720 spec.py:349] Evaluating on the test split.
I0315 01:43:39.459084 139818008614720 submission_runner.py:420] Time since start: 4954.99s, 	Step: 5535, 	{'train/ctc_loss': Array(0.40652406, dtype=float32), 'train/wer': 0.13997079768700812, 'validation/ctc_loss': Array(0.7689421, dtype=float32), 'validation/wer': 0.21863927319771764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4684899, dtype=float32), 'test/wer': 0.14910730607519346, 'test/num_examples': 2472, 'score': 4362.616629600525, 'total_duration': 4954.994887113571, 'accumulated_submission_time': 4362.616629600525, 'accumulated_eval_time': 592.0327932834625, 'accumulated_logging_time': 0.13383126258850098}
I0315 01:43:39.492406 139662233843456 logging_writer.py:48] [5535] accumulated_eval_time=592.032793, accumulated_logging_time=0.133831, accumulated_submission_time=4362.616630, global_step=5535, preemption_count=0, score=4362.616630, test/ctc_loss=0.4684898853302002, test/num_examples=2472, test/wer=0.149107, total_duration=4954.994887, train/ctc_loss=0.40652406215667725, train/wer=0.139971, validation/ctc_loss=0.76894211769104, validation/num_examples=5348, validation/wer=0.218639
I0315 01:44:29.539679 139662225450752 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.7742533683776855, loss=1.7184878587722778
I0315 01:45:45.327070 139662233843456 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.3731099367141724, loss=1.6641781330108643
I0315 01:47:00.844151 139662225450752 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.221693754196167, loss=1.7191014289855957
I0315 01:48:17.836280 139662233843456 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.203077554702759, loss=1.691887378692627
I0315 01:49:34.135492 139662225450752 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.528092861175537, loss=1.7448264360427856
I0315 01:50:53.142470 139662233843456 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.4189834594726562, loss=1.7708511352539062
I0315 01:52:17.270265 139662233843456 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.2747960090637207, loss=1.6686969995498657
I0315 01:53:32.182644 139662225450752 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.426961660385132, loss=1.7134166955947876
I0315 01:54:48.344222 139662233843456 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.0959622859954834, loss=1.7225244045257568
I0315 01:56:03.544312 139662225450752 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.9915952682495117, loss=1.6983188390731812
I0315 01:57:18.484421 139662233843456 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.64779806137085, loss=1.644769549369812
I0315 01:58:34.519255 139662225450752 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.353119373321533, loss=1.6328181028366089
I0315 01:59:51.676596 139662233843456 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.131856918334961, loss=1.694656491279602
I0315 02:01:16.322883 139662225450752 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.648350715637207, loss=1.7154244184494019
I0315 02:02:39.827427 139662233843456 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.883948802947998, loss=1.6784545183181763
I0315 02:04:03.482463 139662225450752 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.8152436017990112, loss=1.7470756769180298
I0315 02:05:26.544846 139662233843456 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.2496538162231445, loss=1.6895543336868286
I0315 02:06:46.915151 139662233843456 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.8777687549591064, loss=1.6946966648101807
I0315 02:07:39.786326 139818008614720 spec.py:321] Evaluating on the training split.
I0315 02:08:34.397581 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 02:09:25.329108 139818008614720 spec.py:349] Evaluating on the test split.
I0315 02:09:50.472399 139818008614720 submission_runner.py:420] Time since start: 6526.01s, 	Step: 7371, 	{'train/ctc_loss': Array(0.40096128, dtype=float32), 'train/wer': 0.13494224465645258, 'validation/ctc_loss': Array(0.74107677, dtype=float32), 'validation/wer': 0.21182308813732778, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4458511, dtype=float32), 'test/wer': 0.14492312067109459, 'test/num_examples': 2472, 'score': 5802.827211141586, 'total_duration': 6526.008309602737, 'accumulated_submission_time': 5802.827211141586, 'accumulated_eval_time': 722.7132506370544, 'accumulated_logging_time': 0.18260669708251953}
I0315 02:09:50.506186 139661435123456 logging_writer.py:48] [7371] accumulated_eval_time=722.713251, accumulated_logging_time=0.182607, accumulated_submission_time=5802.827211, global_step=7371, preemption_count=0, score=5802.827211, test/ctc_loss=0.44585108757019043, test/num_examples=2472, test/wer=0.144923, total_duration=6526.008310, train/ctc_loss=0.4009612798690796, train/wer=0.134942, validation/ctc_loss=0.7410767674446106, validation/num_examples=5348, validation/wer=0.211823
I0315 02:10:13.840589 139661426730752 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.5686023235321045, loss=1.6524930000305176
I0315 02:11:29.765748 139661435123456 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.808730363845825, loss=1.6971023082733154
I0315 02:12:44.427446 139661426730752 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.9123175144195557, loss=1.5968049764633179
I0315 02:14:01.347702 139661435123456 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.6579372882843018, loss=1.6762118339538574
I0315 02:15:17.331904 139661426730752 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.3336493968963623, loss=1.694420576095581
I0315 02:16:41.406140 139661435123456 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.073199510574341, loss=1.6226747035980225
I0315 02:18:05.406000 139661426730752 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.455975294113159, loss=1.6976238489151
I0315 02:19:25.613129 139661435123456 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.328123092651367, loss=1.6793040037155151
I0315 02:20:47.181839 139661426730752 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.4807913303375244, loss=1.657922625541687
I0315 02:22:09.270248 139661435123456 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.328843355178833, loss=1.6657261848449707
I0315 02:23:25.134313 139661426730752 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.7541663646698, loss=1.6264625787734985
I0315 02:24:40.976608 139661435123456 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.94851016998291, loss=1.6652687788009644
I0315 02:25:56.642460 139661426730752 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.9538360834121704, loss=1.6881648302078247
I0315 02:27:16.014955 139661435123456 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.5297110080718994, loss=1.7773422002792358
I0315 02:28:38.113213 139661426730752 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.6690402030944824, loss=1.6343075037002563
I0315 02:30:00.959386 139661435123456 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.9475072622299194, loss=1.6664131879806519
I0315 02:31:25.235466 139661426730752 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.792724132537842, loss=1.6936630010604858
I0315 02:32:46.532343 139661435123456 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.4678256511688232, loss=1.6127289533615112
I0315 02:33:50.632781 139818008614720 spec.py:321] Evaluating on the training split.
I0315 02:34:43.717025 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 02:35:33.212555 139818008614720 spec.py:349] Evaluating on the test split.
I0315 02:35:58.184042 139818008614720 submission_runner.py:420] Time since start: 8093.72s, 	Step: 9182, 	{'train/ctc_loss': Array(0.3648077, dtype=float32), 'train/wer': 0.1252038530953054, 'validation/ctc_loss': Array(0.69743854, dtype=float32), 'validation/wer': 0.20044025217953793, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41574928, dtype=float32), 'test/wer': 0.1366156846017915, 'test/num_examples': 2472, 'score': 7242.870648384094, 'total_duration': 8093.72025179863, 'accumulated_submission_time': 7242.870648384094, 'accumulated_eval_time': 850.259199142456, 'accumulated_logging_time': 0.23354673385620117}
I0315 02:35:58.214187 139661138159360 logging_writer.py:48] [9182] accumulated_eval_time=850.259199, accumulated_logging_time=0.233547, accumulated_submission_time=7242.870648, global_step=9182, preemption_count=0, score=7242.870648, test/ctc_loss=0.41574928164482117, test/num_examples=2472, test/wer=0.136616, total_duration=8093.720252, train/ctc_loss=0.36480769515037537, train/wer=0.125204, validation/ctc_loss=0.6974385380744934, validation/num_examples=5348, validation/wer=0.200440
I0315 02:36:12.554154 139661129766656 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.1801233291625977, loss=1.6890658140182495
I0315 02:37:32.012058 139660482799360 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.201993703842163, loss=1.6185170412063599
I0315 02:38:49.780787 139660474406656 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.8130056858062744, loss=1.662872314453125
I0315 02:40:05.865499 139660482799360 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.368885040283203, loss=1.6401602029800415
I0315 02:41:20.877182 139660474406656 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.044086456298828, loss=1.5995020866394043
I0315 02:42:36.172919 139660482799360 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8493098020553589, loss=1.6056954860687256
I0315 02:43:59.466670 139660474406656 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.651144027709961, loss=1.6831610202789307
I0315 02:45:26.543304 139660482799360 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.1668548583984375, loss=1.6734193563461304
I0315 02:46:53.560945 139660474406656 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.2045600414276123, loss=1.6163190603256226
I0315 02:48:18.864671 139660482799360 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.797978401184082, loss=1.6191763877868652
I0315 02:49:43.031727 139660474406656 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.642096996307373, loss=1.6630516052246094
I0315 02:51:05.933181 139660482799360 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.203681230545044, loss=1.5910180807113647
I0315 02:52:21.603167 139660474406656 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.588285446166992, loss=1.6274569034576416
I0315 02:53:36.616834 139660482799360 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.307992458343506, loss=1.6059136390686035
I0315 02:54:52.812779 139660474406656 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.366036891937256, loss=1.6503698825836182
I0315 02:56:12.988980 139660482799360 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.038010835647583, loss=1.6061053276062012
I0315 02:57:39.079494 139660474406656 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.9515644311904907, loss=1.6799430847167969
I0315 02:59:04.316072 139660482799360 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.981285572052002, loss=1.5743136405944824
I0315 02:59:58.897747 139818008614720 spec.py:321] Evaluating on the training split.
I0315 03:00:53.531146 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 03:01:43.343745 139818008614720 spec.py:349] Evaluating on the test split.
I0315 03:02:11.195297 139818008614720 submission_runner.py:420] Time since start: 9666.73s, 	Step: 10970, 	{'train/ctc_loss': Array(0.38352492, dtype=float32), 'train/wer': 0.1273506142938688, 'validation/ctc_loss': Array(0.6855383, dtype=float32), 'validation/wer': 0.19660735491470113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40698555, dtype=float32), 'test/wer': 0.1310300002031158, 'test/num_examples': 2472, 'score': 8683.471186876297, 'total_duration': 9666.73115491867, 'accumulated_submission_time': 8683.471186876297, 'accumulated_eval_time': 982.551082611084, 'accumulated_logging_time': 0.2804255485534668}
I0315 03:02:11.227229 139660923119360 logging_writer.py:48] [10970] accumulated_eval_time=982.551083, accumulated_logging_time=0.280426, accumulated_submission_time=8683.471187, global_step=10970, preemption_count=0, score=8683.471187, test/ctc_loss=0.4069855511188507, test/num_examples=2472, test/wer=0.131030, total_duration=9666.731155, train/ctc_loss=0.38352492451667786, train/wer=0.127351, validation/ctc_loss=0.6855382919311523, validation/num_examples=5348, validation/wer=0.196607
I0315 03:02:34.391340 139660914726656 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.010571002960205, loss=1.6324918270111084
I0315 03:03:50.786537 139660923119360 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.263651132583618, loss=1.6140724420547485
I0315 03:05:07.935660 139660914726656 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.270447015762329, loss=1.64536452293396
I0315 03:06:25.336559 139660923119360 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.724914073944092, loss=1.6269257068634033
I0315 03:07:48.162158 139660595439360 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.7718218564987183, loss=1.5784920454025269
I0315 03:09:03.142488 139660587046656 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.4951794147491455, loss=1.6273112297058105
I0315 03:10:18.988792 139660595439360 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.0751101970672607, loss=1.5961421728134155
I0315 03:11:35.205232 139660587046656 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.555906295776367, loss=1.6009899377822876
I0315 03:12:55.998482 139660595439360 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.865025997161865, loss=1.664736270904541
I0315 03:14:21.997380 139660587046656 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.207789421081543, loss=1.5901731252670288
I0315 03:15:47.899292 139660595439360 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.849159002304077, loss=1.6593332290649414
I0315 03:17:14.327551 139660587046656 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.805187940597534, loss=1.618791103363037
I0315 03:18:41.413832 139660595439360 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.191739559173584, loss=1.600035548210144
I0315 03:20:04.653201 139660587046656 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.0754668712615967, loss=1.5788094997406006
I0315 03:21:25.352670 139660923119360 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.759997844696045, loss=1.5797538757324219
I0315 03:22:42.302775 139660914726656 logging_writer.py:48] [12500] global_step=12500, grad_norm=6.650871276855469, loss=1.5968607664108276
I0315 03:23:58.088675 139660923119360 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.079418420791626, loss=1.558739423751831
I0315 03:25:13.181245 139660914726656 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.0668675899505615, loss=1.6179934740066528
I0315 03:26:11.865668 139818008614720 spec.py:321] Evaluating on the training split.
I0315 03:27:06.334172 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 03:27:55.622218 139818008614720 spec.py:349] Evaluating on the test split.
I0315 03:28:21.962746 139818008614720 submission_runner.py:420] Time since start: 11237.50s, 	Step: 12775, 	{'train/ctc_loss': Array(0.35075814, dtype=float32), 'train/wer': 0.11953608576351223, 'validation/ctc_loss': Array(0.6756054, dtype=float32), 'validation/wer': 0.19374957760892864, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3975374, dtype=float32), 'test/wer': 0.1268458147990169, 'test/num_examples': 2472, 'score': 10124.027910232544, 'total_duration': 11237.498494386673, 'accumulated_submission_time': 10124.027910232544, 'accumulated_eval_time': 1112.6423828601837, 'accumulated_logging_time': 0.32646632194519043}
I0315 03:28:21.993945 139660923119360 logging_writer.py:48] [12775] accumulated_eval_time=1112.642383, accumulated_logging_time=0.326466, accumulated_submission_time=10124.027910, global_step=12775, preemption_count=0, score=10124.027910, test/ctc_loss=0.3975374102592468, test/num_examples=2472, test/wer=0.126846, total_duration=11237.498494, train/ctc_loss=0.3507581353187561, train/wer=0.119536, validation/ctc_loss=0.6756054162979126, validation/num_examples=5348, validation/wer=0.193750
I0315 03:28:42.097956 139660914726656 logging_writer.py:48] [12800] global_step=12800, grad_norm=4.224318981170654, loss=1.606976866722107
I0315 03:29:58.600616 139660923119360 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.3696296215057373, loss=1.5265487432479858
I0315 03:31:13.715971 139660914726656 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.454484462738037, loss=1.5685020685195923
I0315 03:32:29.266118 139660923119360 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.900843620300293, loss=1.6079492568969727
I0315 03:33:47.013320 139660914726656 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.7632032632827759, loss=1.5701462030410767
I0315 03:35:11.837611 139660923119360 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.02351975440979, loss=1.631423830986023
I0315 03:36:37.274886 139660595439360 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.4230198860168457, loss=1.57060706615448
I0315 03:37:53.001416 139660587046656 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.319877862930298, loss=1.5903408527374268
I0315 03:39:07.836927 139660595439360 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.6065993309020996, loss=1.5576246976852417
I0315 03:40:23.943730 139660587046656 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.1719729900360107, loss=1.6506147384643555
I0315 03:41:44.770220 139660595439360 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.130796432495117, loss=1.7002369165420532
I0315 03:43:07.835471 139660587046656 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.680112361907959, loss=1.5886797904968262
I0315 03:44:33.001878 139660595439360 logging_writer.py:48] [14000] global_step=14000, grad_norm=6.070019721984863, loss=1.6369882822036743
I0315 03:45:54.647116 139660587046656 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.054942607879639, loss=1.64044189453125
I0315 03:47:21.623827 139660595439360 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.485311269760132, loss=1.6087660789489746
I0315 03:48:46.621515 139660587046656 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.2152493000030518, loss=1.602738857269287
I0315 03:50:11.564626 139660595439360 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.7972991466522217, loss=1.5714526176452637
I0315 03:51:31.362051 139660595439360 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.6426620483398438, loss=1.5207781791687012
I0315 03:52:22.678503 139818008614720 spec.py:321] Evaluating on the training split.
I0315 03:53:17.475111 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 03:54:08.480032 139818008614720 spec.py:349] Evaluating on the test split.
I0315 03:54:34.356162 139818008614720 submission_runner.py:420] Time since start: 12809.89s, 	Step: 14568, 	{'train/ctc_loss': Array(0.32302096, dtype=float32), 'train/wer': 0.11241061910086066, 'validation/ctc_loss': Array(0.6762659, dtype=float32), 'validation/wer': 0.19071801654807535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39017347, dtype=float32), 'test/wer': 0.1259114821359657, 'test/num_examples': 2472, 'score': 11564.629160404205, 'total_duration': 12809.892186164856, 'accumulated_submission_time': 11564.629160404205, 'accumulated_eval_time': 1244.3145473003387, 'accumulated_logging_time': 0.3751053810119629}
I0315 03:54:34.388532 139660626155264 logging_writer.py:48] [14568] accumulated_eval_time=1244.314547, accumulated_logging_time=0.375105, accumulated_submission_time=11564.629160, global_step=14568, preemption_count=0, score=11564.629160, test/ctc_loss=0.39017346501350403, test/num_examples=2472, test/wer=0.125911, total_duration=12809.892186, train/ctc_loss=0.32302096486091614, train/wer=0.112411, validation/ctc_loss=0.6762658953666687, validation/num_examples=5348, validation/wer=0.190718
I0315 03:54:59.073553 139660617762560 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.1155073642730713, loss=1.7122701406478882
I0315 03:56:14.949568 139660626155264 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.457354784011841, loss=1.6130284070968628
I0315 03:57:30.373114 139660617762560 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.7008929252624512, loss=1.5342845916748047
I0315 03:58:46.350729 139660626155264 logging_writer.py:48] [14900] global_step=14900, grad_norm=4.046997547149658, loss=1.5821036100387573
I0315 04:00:02.074833 139660617762560 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.7585039138793945, loss=1.5747861862182617
I0315 04:01:24.681187 139660626155264 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.4671518802642822, loss=1.5536985397338867
I0315 04:02:44.870983 139660617762560 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.8345589637756348, loss=1.6460609436035156
I0315 04:04:09.465576 139660626155264 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.08321213722229, loss=1.5852175951004028
I0315 04:05:33.011505 139660617762560 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.525228261947632, loss=1.538119912147522
I0315 04:06:55.251302 139660298475264 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.454068660736084, loss=1.5173521041870117
I0315 04:08:10.621922 139660290082560 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.923682928085327, loss=1.6804298162460327
I0315 04:09:26.166048 139660298475264 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.303093194961548, loss=1.5232791900634766
I0315 04:10:42.120910 139660290082560 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.2291207313537598, loss=1.5694327354431152
I0315 04:12:02.410079 139660298475264 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.3696930408477783, loss=1.5365062952041626
I0315 04:13:24.607801 139660290082560 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.9274190664291382, loss=1.5577189922332764
I0315 04:14:50.286932 139660298475264 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.4874212741851807, loss=1.6035377979278564
I0315 04:16:16.704950 139660290082560 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.68154239654541, loss=1.6405616998672485
I0315 04:17:36.940851 139660298475264 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.8781509399414062, loss=1.525672197341919
I0315 04:18:34.540991 139818008614720 spec.py:321] Evaluating on the training split.
I0315 04:19:29.154751 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 04:20:20.571407 139818008614720 spec.py:349] Evaluating on the test split.
I0315 04:20:45.832480 139818008614720 submission_runner.py:420] Time since start: 14381.37s, 	Step: 16370, 	{'train/ctc_loss': Array(0.2960854, dtype=float32), 'train/wer': 0.10138373880799494, 'validation/ctc_loss': Array(0.64389575, dtype=float32), 'validation/wer': 0.18535968409975187, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3699334, dtype=float32), 'test/wer': 0.11987894298539598, 'test/num_examples': 2472, 'score': 13004.699365615845, 'total_duration': 14381.36817574501, 'accumulated_submission_time': 13004.699365615845, 'accumulated_eval_time': 1375.6002094745636, 'accumulated_logging_time': 0.42122817039489746}
I0315 04:20:45.864792 139660298475264 logging_writer.py:48] [16370] accumulated_eval_time=1375.600209, accumulated_logging_time=0.421228, accumulated_submission_time=13004.699366, global_step=16370, preemption_count=0, score=13004.699366, test/ctc_loss=0.3699333965778351, test/num_examples=2472, test/wer=0.119879, total_duration=14381.368176, train/ctc_loss=0.296085387468338, train/wer=0.101384, validation/ctc_loss=0.6438957452774048, validation/num_examples=5348, validation/wer=0.185360
I0315 04:21:09.360816 139660290082560 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.9612804651260376, loss=1.5440995693206787
I0315 04:22:28.185416 139660626155264 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.953935980796814, loss=1.54207444190979
I0315 04:23:43.232882 139660617762560 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.237827301025391, loss=1.5627741813659668
I0315 04:25:00.694175 139660626155264 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.316410541534424, loss=1.5005789995193481
I0315 04:26:16.292805 139660617762560 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.396209239959717, loss=1.5572547912597656
I0315 04:27:33.671511 139660626155264 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.8714406490325928, loss=1.5321197509765625
I0315 04:28:56.765311 139660617762560 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.225770950317383, loss=1.5562238693237305
I0315 04:30:23.636099 139660626155264 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.1940665245056152, loss=1.5294456481933594
I0315 04:31:46.118499 139660617762560 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.8088135719299316, loss=1.6275330781936646
I0315 04:33:13.358579 139660626155264 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.3690807819366455, loss=1.5811322927474976
I0315 04:34:35.330822 139660617762560 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.7575695514678955, loss=1.5115478038787842
I0315 04:36:00.486229 139660626155264 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.1005542278289795, loss=1.483060598373413
I0315 04:37:18.813809 139660298475264 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.7904577255249023, loss=1.4849931001663208
I0315 04:38:35.070894 139660290082560 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.118736505508423, loss=1.533540964126587
I0315 04:39:51.944646 139660298475264 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.6292836666107178, loss=1.5411814451217651
I0315 04:41:06.680973 139660290082560 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.9176220893859863, loss=1.512539029121399
I0315 04:42:28.622887 139660298475264 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.8763997554779053, loss=1.6348284482955933
I0315 04:43:51.547549 139660290082560 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.187007427215576, loss=1.5074018239974976
I0315 04:44:46.324895 139818008614720 spec.py:321] Evaluating on the training split.
I0315 04:45:41.394933 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 04:46:31.946683 139818008614720 spec.py:349] Evaluating on the test split.
I0315 04:46:58.421059 139818008614720 submission_runner.py:420] Time since start: 15953.96s, 	Step: 18166, 	{'train/ctc_loss': Array(0.29231447, dtype=float32), 'train/wer': 0.10365909249357795, 'validation/ctc_loss': Array(0.61532944, dtype=float32), 'validation/wer': 0.1773270127537967, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35234314, dtype=float32), 'test/wer': 0.11451668596266731, 'test/num_examples': 2472, 'score': 14445.077192544937, 'total_duration': 15953.95691728592, 'accumulated_submission_time': 14445.077192544937, 'accumulated_eval_time': 1507.6907210350037, 'accumulated_logging_time': 0.46775197982788086}
I0315 04:46:58.453719 139660298475264 logging_writer.py:48] [18166] accumulated_eval_time=1507.690721, accumulated_logging_time=0.467752, accumulated_submission_time=14445.077193, global_step=18166, preemption_count=0, score=14445.077193, test/ctc_loss=0.3523431420326233, test/num_examples=2472, test/wer=0.114517, total_duration=15953.956917, train/ctc_loss=0.29231446981430054, train/wer=0.103659, validation/ctc_loss=0.6153294444084167, validation/num_examples=5348, validation/wer=0.177327
I0315 04:47:24.792638 139660290082560 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.7242769002914429, loss=1.5480369329452515
I0315 04:48:40.062437 139660298475264 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.112603187561035, loss=1.5321407318115234
I0315 04:49:54.902446 139660290082560 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.2179973125457764, loss=1.4467532634735107
I0315 04:51:12.164886 139660298475264 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7137691974639893, loss=1.489543080329895
I0315 04:52:31.890385 139660298475264 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.718395471572876, loss=1.494584083557129
I0315 04:53:48.045368 139660290082560 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.9674949645996094, loss=1.5277715921401978
I0315 04:55:05.408897 139660298475264 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.455251932144165, loss=1.485498309135437
I0315 04:56:20.538087 139660290082560 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.8555727005004883, loss=1.5233125686645508
I0315 04:57:41.771678 139660298475264 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.7549450397491455, loss=1.4606021642684937
I0315 04:59:06.783889 139660290082560 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.299752712249756, loss=1.4906482696533203
I0315 05:00:33.445223 139660298475264 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.1540517807006836, loss=1.5111452341079712
I0315 05:01:57.181407 139660290082560 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.7851592302322388, loss=1.5553107261657715
I0315 05:03:24.110711 139660298475264 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.2277421951293945, loss=1.5618778467178345
I0315 05:04:47.085405 139660290082560 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.1875410079956055, loss=1.4886506795883179
I0315 05:06:12.021699 139660626155264 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.580836772918701, loss=1.54466712474823
I0315 05:07:26.711315 139660617762560 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.1732165813446045, loss=1.4524859189987183
I0315 05:08:43.306633 139660626155264 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.7480087280273438, loss=1.5109434127807617
I0315 05:10:02.853323 139660617762560 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.898711204528809, loss=1.5018779039382935
I0315 05:10:59.126344 139818008614720 spec.py:321] Evaluating on the training split.
I0315 05:11:53.836543 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 05:12:45.283978 139818008614720 spec.py:349] Evaluating on the test split.
I0315 05:13:11.976279 139818008614720 submission_runner.py:420] Time since start: 17527.51s, 	Step: 19969, 	{'train/ctc_loss': Array(0.3047939, dtype=float32), 'train/wer': 0.10215500145429546, 'validation/ctc_loss': Array(0.6024862, dtype=float32), 'validation/wer': 0.1741120132848026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3425359, dtype=float32), 'test/wer': 0.11000751528446367, 'test/num_examples': 2472, 'score': 15885.665563583374, 'total_duration': 17527.51263141632, 'accumulated_submission_time': 15885.665563583374, 'accumulated_eval_time': 1640.5355043411255, 'accumulated_logging_time': 0.5157749652862549}
I0315 05:13:12.011793 139660626155264 logging_writer.py:48] [19969] accumulated_eval_time=1640.535504, accumulated_logging_time=0.515775, accumulated_submission_time=15885.665564, global_step=19969, preemption_count=0, score=15885.665564, test/ctc_loss=0.34253591299057007, test/num_examples=2472, test/wer=0.110008, total_duration=17527.512631, train/ctc_loss=0.3047938942909241, train/wer=0.102155, validation/ctc_loss=0.6024861931800842, validation/num_examples=5348, validation/wer=0.174112
I0315 05:13:36.533871 139660617762560 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.215761661529541, loss=1.4226075410842896
I0315 05:14:52.933116 139660626155264 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.7043819427490234, loss=1.4517368078231812
I0315 05:16:07.777483 139660617762560 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.0508012771606445, loss=1.5146945714950562
I0315 05:17:27.766476 139660626155264 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.603729724884033, loss=1.4870082139968872
I0315 05:18:54.223422 139660617762560 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.1796987056732178, loss=1.4953747987747192
I0315 05:20:18.814988 139660626155264 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.3166158199310303, loss=1.463079810142517
I0315 05:21:49.058374 139660298475264 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.744584083557129, loss=1.482545018196106
I0315 05:23:04.583302 139660290082560 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.615234851837158, loss=1.542221188545227
I0315 05:24:19.939176 139660298475264 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.2638134956359863, loss=1.5717997550964355
I0315 05:25:37.765804 139660290082560 logging_writer.py:48] [20900] global_step=20900, grad_norm=5.269535541534424, loss=1.4525340795516968
I0315 05:26:57.583622 139660298475264 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.019158363342285, loss=1.4933274984359741
I0315 05:28:22.621842 139660290082560 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.912553071975708, loss=1.574210286140442
I0315 05:29:43.027944 139660298475264 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.455533027648926, loss=1.500958800315857
I0315 05:31:06.749660 139660290082560 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.9242533445358276, loss=1.5314934253692627
I0315 05:32:34.170935 139660298475264 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.6778459548950195, loss=1.5341355800628662
I0315 05:33:55.559030 139660290082560 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.1928329467773438, loss=1.5105462074279785
I0315 05:35:21.513049 139660298475264 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.420935153961182, loss=1.496978759765625
I0315 05:36:44.157421 139660298475264 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.9492555856704712, loss=1.4844321012496948
I0315 05:37:12.432061 139818008614720 spec.py:321] Evaluating on the training split.
I0315 05:38:07.694186 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 05:38:58.832610 139818008614720 spec.py:349] Evaluating on the test split.
I0315 05:39:25.502501 139818008614720 submission_runner.py:420] Time since start: 19101.04s, 	Step: 21738, 	{'train/ctc_loss': Array(0.29166546, dtype=float32), 'train/wer': 0.09895270002901839, 'validation/ctc_loss': Array(0.5862438, dtype=float32), 'validation/wer': 0.16791372602025545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3305221, dtype=float32), 'test/wer': 0.10738732151199398, 'test/num_examples': 2472, 'score': 17326.005275011063, 'total_duration': 19101.038394212723, 'accumulated_submission_time': 17326.005275011063, 'accumulated_eval_time': 1773.6003246307373, 'accumulated_logging_time': 0.5648343563079834}
I0315 05:39:25.536102 139660298475264 logging_writer.py:48] [21738] accumulated_eval_time=1773.600325, accumulated_logging_time=0.564834, accumulated_submission_time=17326.005275, global_step=21738, preemption_count=0, score=17326.005275, test/ctc_loss=0.3305220901966095, test/num_examples=2472, test/wer=0.107387, total_duration=19101.038394, train/ctc_loss=0.2916654646396637, train/wer=0.098953, validation/ctc_loss=0.5862438082695007, validation/num_examples=5348, validation/wer=0.167914
I0315 05:40:13.141441 139660290082560 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.8737338781356812, loss=1.4349769353866577
I0315 05:41:27.796960 139660298475264 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.8671157360076904, loss=1.4293501377105713
I0315 05:42:42.507350 139660290082560 logging_writer.py:48] [22000] global_step=22000, grad_norm=5.849254131317139, loss=1.4766556024551392
I0315 05:43:59.061369 139660298475264 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.859281539916992, loss=1.4730443954467773
I0315 05:45:18.852982 139660290082560 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.5149929523468018, loss=1.444220781326294
I0315 05:46:40.535429 139660298475264 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.9011318683624268, loss=1.4609357118606567
I0315 05:48:03.321362 139660290082560 logging_writer.py:48] [22400] global_step=22400, grad_norm=4.671393871307373, loss=1.4734957218170166
I0315 05:49:28.535762 139660298475264 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.308957815170288, loss=1.3977187871932983
I0315 05:50:53.540545 139660290082560 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.81408953666687, loss=1.5338042974472046
I0315 05:52:15.863379 139660626155264 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.9441254138946533, loss=1.3793505430221558
I0315 05:53:32.620581 139660617762560 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.6151468753814697, loss=1.447716474533081
I0315 05:54:48.575626 139660626155264 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.201366901397705, loss=1.3952136039733887
I0315 05:56:04.797285 139660617762560 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.665208101272583, loss=1.4419424533843994
I0315 05:57:22.767993 139660626155264 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.6154003143310547, loss=1.4344747066497803
I0315 05:58:45.773799 139660617762560 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.182689666748047, loss=1.4877837896347046
I0315 06:00:06.163323 139660626155264 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5585954189300537, loss=1.4529415369033813
I0315 06:01:31.797836 139660617762560 logging_writer.py:48] [23400] global_step=23400, grad_norm=9.494917869567871, loss=1.5211116075515747
I0315 06:02:55.600096 139660626155264 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.294543981552124, loss=1.449666976928711
I0315 06:03:26.055004 139818008614720 spec.py:321] Evaluating on the training split.
I0315 06:04:21.981055 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 06:05:12.126847 139818008614720 spec.py:349] Evaluating on the test split.
I0315 06:05:38.068274 139818008614720 submission_runner.py:420] Time since start: 20673.60s, 	Step: 23537, 	{'train/ctc_loss': Array(0.27430362, dtype=float32), 'train/wer': 0.09441300545323593, 'validation/ctc_loss': Array(0.57207096, dtype=float32), 'validation/wer': 0.16316363671471465, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32181597, dtype=float32), 'test/wer': 0.10478743931915585, 'test/num_examples': 2472, 'score': 18766.441838026047, 'total_duration': 20673.603135347366, 'accumulated_submission_time': 18766.441838026047, 'accumulated_eval_time': 1905.6069421768188, 'accumulated_logging_time': 0.6118886470794678}
I0315 06:05:38.102523 139660626155264 logging_writer.py:48] [23537] accumulated_eval_time=1905.606942, accumulated_logging_time=0.611889, accumulated_submission_time=18766.441838, global_step=23537, preemption_count=0, score=18766.441838, test/ctc_loss=0.32181596755981445, test/num_examples=2472, test/wer=0.104787, total_duration=20673.603135, train/ctc_loss=0.2743036150932312, train/wer=0.094413, validation/ctc_loss=0.5720709562301636, validation/num_examples=5348, validation/wer=0.163164
I0315 06:06:26.287368 139660617762560 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.2260491847991943, loss=1.3684766292572021
I0315 06:07:44.562370 139660626155264 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.7372548580169678, loss=1.3742780685424805
I0315 06:09:00.340548 139660617762560 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.748056650161743, loss=1.460321068763733
I0315 06:10:16.154014 139660626155264 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.361102819442749, loss=1.390739917755127
I0315 06:11:32.513927 139660617762560 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.132446050643921, loss=1.3902751207351685
I0315 06:12:51.602110 139660626155264 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.8088716268539429, loss=1.456882357597351
I0315 06:14:16.137230 139660617762560 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.5580170154571533, loss=1.4423291683197021
I0315 06:15:43.386950 139660626155264 logging_writer.py:48] [24300] global_step=24300, grad_norm=5.498563289642334, loss=1.4520219564437866
I0315 06:17:08.611376 139660617762560 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.4240647554397583, loss=1.3358854055404663
I0315 06:18:33.590160 139660626155264 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.3602514266967773, loss=1.4183053970336914
I0315 06:20:00.370077 139660617762560 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.697151780128479, loss=1.4691929817199707
I0315 06:21:23.792573 139660626155264 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.6768298149108887, loss=1.400175929069519
I0315 06:22:43.933186 139660298475264 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.4135069847106934, loss=1.480430245399475
I0315 06:23:59.246002 139660290082560 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.1265368461608887, loss=1.3368288278579712
I0315 06:25:19.037690 139660298475264 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.087932586669922, loss=1.435829520225525
I0315 06:26:40.453689 139660290082560 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.698138475418091, loss=1.354715347290039
I0315 06:27:59.345057 139660298475264 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.2378227710723877, loss=1.4388269186019897
I0315 06:29:27.775716 139660290082560 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.9173691272735596, loss=1.3955234289169312
I0315 06:29:38.520203 139818008614720 spec.py:321] Evaluating on the training split.
I0315 06:30:33.504876 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 06:31:23.721152 139818008614720 spec.py:349] Evaluating on the test split.
I0315 06:31:49.310656 139818008614720 submission_runner.py:420] Time since start: 22244.85s, 	Step: 25314, 	{'train/ctc_loss': Array(0.24832816, dtype=float32), 'train/wer': 0.08623474515434315, 'validation/ctc_loss': Array(0.554332, dtype=float32), 'validation/wer': 0.15956245112331888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31350023, dtype=float32), 'test/wer': 0.10157820973737128, 'test/num_examples': 2472, 'score': 20206.777960777283, 'total_duration': 22244.84517145157, 'accumulated_submission_time': 20206.777960777283, 'accumulated_eval_time': 2036.3903830051422, 'accumulated_logging_time': 0.6608626842498779}
I0315 06:31:49.346930 139660298475264 logging_writer.py:48] [25314] accumulated_eval_time=2036.390383, accumulated_logging_time=0.660863, accumulated_submission_time=20206.777961, global_step=25314, preemption_count=0, score=20206.777961, test/ctc_loss=0.31350022554397583, test/num_examples=2472, test/wer=0.101578, total_duration=22244.845171, train/ctc_loss=0.24832816421985626, train/wer=0.086235, validation/ctc_loss=0.5543320178985596, validation/num_examples=5348, validation/wer=0.159562
I0315 06:32:54.612807 139660290082560 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.458275556564331, loss=1.483986258506775
I0315 06:34:09.531761 139660298475264 logging_writer.py:48] [25500] global_step=25500, grad_norm=10.237591743469238, loss=1.3833051919937134
I0315 06:35:28.799804 139660290082560 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.2062337398529053, loss=1.4261118173599243
I0315 06:36:55.754973 139660298475264 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.5833897590637207, loss=1.4527925252914429
I0315 06:38:17.769856 139660298475264 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.254483938217163, loss=1.3398405313491821
I0315 06:39:33.217688 139660290082560 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.622060775756836, loss=1.3368738889694214
I0315 06:40:50.497243 139660298475264 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.2182953357696533, loss=1.394339919090271
I0315 06:42:10.577342 139660290082560 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.0699398517608643, loss=1.4582828283309937
I0315 06:43:31.015037 139660298475264 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7122621536254883, loss=1.3473395109176636
I0315 06:44:56.378103 139660290082560 logging_writer.py:48] [26300] global_step=26300, grad_norm=6.747374057769775, loss=1.386529564857483
I0315 06:46:24.200785 139660298475264 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.0362935066223145, loss=1.4183433055877686
I0315 06:47:51.445441 139660290082560 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.9753365516662598, loss=1.4517194032669067
I0315 06:49:15.360978 139660298475264 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.8149940967559814, loss=1.399887204170227
I0315 06:50:40.721798 139660290082560 logging_writer.py:48] [26700] global_step=26700, grad_norm=6.933096408843994, loss=1.3721505403518677
I0315 06:52:06.583778 139660626155264 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.036257028579712, loss=1.3455400466918945
I0315 06:53:23.707869 139660617762560 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.0414037704467773, loss=1.3399901390075684
I0315 06:54:40.243871 139660626155264 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6560848951339722, loss=1.3588696718215942
I0315 06:55:49.389641 139818008614720 spec.py:321] Evaluating on the training split.
I0315 06:56:44.400949 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 06:57:34.999834 139818008614720 spec.py:349] Evaluating on the test split.
I0315 06:58:01.599348 139818008614720 submission_runner.py:420] Time since start: 23817.13s, 	Step: 27091, 	{'train/ctc_loss': Array(0.22888745, dtype=float32), 'train/wer': 0.0808210960118939, 'validation/ctc_loss': Array(0.53358024, dtype=float32), 'validation/wer': 0.15388551512401402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29808536, dtype=float32), 'test/wer': 0.09641906851095809, 'test/num_examples': 2472, 'score': 21646.736126184464, 'total_duration': 23817.134423017502, 'accumulated_submission_time': 21646.736126184464, 'accumulated_eval_time': 2168.593649625778, 'accumulated_logging_time': 0.711815357208252}
I0315 06:58:01.639788 139660626155264 logging_writer.py:48] [27091] accumulated_eval_time=2168.593650, accumulated_logging_time=0.711815, accumulated_submission_time=21646.736126, global_step=27091, preemption_count=0, score=21646.736126, test/ctc_loss=0.29808536171913147, test/num_examples=2472, test/wer=0.096419, total_duration=23817.134423, train/ctc_loss=0.22888745367527008, train/wer=0.080821, validation/ctc_loss=0.5335802435874939, validation/num_examples=5348, validation/wer=0.153886
I0315 06:58:09.272239 139660617762560 logging_writer.py:48] [27100] global_step=27100, grad_norm=5.018438816070557, loss=1.3791120052337646
I0315 06:59:24.095290 139660626155264 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.838041067123413, loss=1.3927092552185059
I0315 07:00:39.299205 139660617762560 logging_writer.py:48] [27300] global_step=27300, grad_norm=4.457713603973389, loss=1.348204493522644
I0315 07:01:57.341969 139660626155264 logging_writer.py:48] [27400] global_step=27400, grad_norm=4.357057094573975, loss=1.3809200525283813
I0315 07:03:24.113976 139660617762560 logging_writer.py:48] [27500] global_step=27500, grad_norm=4.654340744018555, loss=1.3723664283752441
I0315 07:04:49.263507 139660626155264 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.1419143676757812, loss=1.4105021953582764
I0315 07:06:12.397968 139660617762560 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.5576999187469482, loss=1.3719792366027832
I0315 07:07:38.929862 139660626155264 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.7286616563796997, loss=1.3608672618865967
I0315 07:08:58.107588 139660298475264 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.4235527515411377, loss=1.3173388242721558
I0315 07:10:13.313371 139660290082560 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.087097644805908, loss=1.401552438735962
I0315 07:11:29.725368 139660298475264 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.6531895399093628, loss=1.415783405303955
I0315 07:12:51.845801 139660290082560 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.0992555618286133, loss=1.335593581199646
I0315 07:14:16.284406 139660298475264 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.000748872756958, loss=1.352571964263916
I0315 07:15:41.961542 139660290082560 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.3830416202545166, loss=1.3888535499572754
I0315 07:17:09.862965 139660298475264 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.1562225818634033, loss=1.3729268312454224
I0315 07:18:30.602877 139660290082560 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.7961552143096924, loss=1.3502296209335327
I0315 07:19:52.862353 139660298475264 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.976736307144165, loss=1.354129433631897
I0315 07:21:19.571757 139660290082560 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.4049506187438965, loss=1.367461085319519
I0315 07:22:02.092964 139818008614720 spec.py:321] Evaluating on the training split.
I0315 07:22:57.737704 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 07:23:47.827079 139818008614720 spec.py:349] Evaluating on the test split.
I0315 07:24:14.296746 139818008614720 submission_runner.py:420] Time since start: 25389.83s, 	Step: 28847, 	{'train/ctc_loss': Array(0.21742319, dtype=float32), 'train/wer': 0.07468472246371624, 'validation/ctc_loss': Array(0.5180734, dtype=float32), 'validation/wer': 0.14864303851241106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2901405, dtype=float32), 'test/wer': 0.09316921576991043, 'test/num_examples': 2472, 'score': 23087.10746240616, 'total_duration': 25389.833054065704, 'accumulated_submission_time': 23087.10746240616, 'accumulated_eval_time': 2300.7924242019653, 'accumulated_logging_time': 0.7672219276428223}
I0315 07:24:14.328691 139661803763456 logging_writer.py:48] [28847] accumulated_eval_time=2300.792424, accumulated_logging_time=0.767222, accumulated_submission_time=23087.107462, global_step=28847, preemption_count=0, score=23087.107462, test/ctc_loss=0.2901405096054077, test/num_examples=2472, test/wer=0.093169, total_duration=25389.833054, train/ctc_loss=0.2174231857061386, train/wer=0.074685, validation/ctc_loss=0.5180733799934387, validation/num_examples=5348, validation/wer=0.148643
I0315 07:24:55.347483 139661795370752 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.1883697509765625, loss=1.3765342235565186
I0315 07:26:10.677506 139661803763456 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.8508163690567017, loss=1.3691014051437378
I0315 07:27:26.249013 139661795370752 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.2292895317077637, loss=1.3806259632110596
I0315 07:28:40.984629 139661803763456 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.6342720985412598, loss=1.3982683420181274
I0315 07:29:55.834284 139661795370752 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.0048604011535645, loss=1.3647456169128418
I0315 07:31:24.119296 139661803763456 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.539236307144165, loss=1.3709216117858887
I0315 07:32:47.468276 139661795370752 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.8354640007019043, loss=1.3391035795211792
I0315 07:34:14.188560 139661803763456 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.0834996700286865, loss=1.3924362659454346
I0315 07:35:36.319196 139661795370752 logging_writer.py:48] [29700] global_step=29700, grad_norm=4.121688365936279, loss=1.4392048120498657
I0315 07:37:03.110945 139661803763456 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.919577121734619, loss=1.376140832901001
I0315 07:38:26.500006 139661476083456 logging_writer.py:48] [29900] global_step=29900, grad_norm=5.306334495544434, loss=1.3364337682724
I0315 07:39:42.252208 139661467690752 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.5805294513702393, loss=1.2901116609573364
I0315 07:40:59.795412 139661476083456 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.0644822120666504, loss=1.3235660791397095
I0315 07:42:15.238691 139661467690752 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.9993741512298584, loss=1.347427487373352
I0315 07:43:37.421322 139661476083456 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.3157551288604736, loss=1.326155424118042
I0315 07:44:59.850009 139661467690752 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.979297637939453, loss=1.419424295425415
I0315 07:46:24.639095 139661476083456 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.050307273864746, loss=1.3317439556121826
I0315 07:47:51.037126 139661467690752 logging_writer.py:48] [30600] global_step=30600, grad_norm=5.026739120483398, loss=1.321058750152588
I0315 07:48:14.788124 139818008614720 spec.py:321] Evaluating on the training split.
I0315 07:49:09.135232 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 07:49:59.408210 139818008614720 spec.py:349] Evaluating on the test split.
I0315 07:50:25.743378 139818008614720 submission_runner.py:420] Time since start: 26961.28s, 	Step: 30628, 	{'train/ctc_loss': Array(0.2305872, dtype=float32), 'train/wer': 0.07983944357774311, 'validation/ctc_loss': Array(0.5070502, dtype=float32), 'validation/wer': 0.1471948405534047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28189483, dtype=float32), 'test/wer': 0.09097556516970325, 'test/num_examples': 2472, 'score': 24527.48431277275, 'total_duration': 26961.279413223267, 'accumulated_submission_time': 24527.48431277275, 'accumulated_eval_time': 2431.742198228836, 'accumulated_logging_time': 0.8124017715454102}
I0315 07:50:25.781329 139662233843456 logging_writer.py:48] [30628] accumulated_eval_time=2431.742198, accumulated_logging_time=0.812402, accumulated_submission_time=24527.484313, global_step=30628, preemption_count=0, score=24527.484313, test/ctc_loss=0.28189483284950256, test/num_examples=2472, test/wer=0.090976, total_duration=26961.279413, train/ctc_loss=0.2305871993303299, train/wer=0.079839, validation/ctc_loss=0.5070502161979675, validation/num_examples=5348, validation/wer=0.147195
I0315 07:51:20.406995 139662225450752 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.92102313041687, loss=1.3749799728393555
I0315 07:52:37.484864 139662233843456 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.683917284011841, loss=1.333614468574524
I0315 07:53:56.716930 139662233843456 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.778195381164551, loss=1.3070526123046875
I0315 07:55:12.445445 139662225450752 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.595512866973877, loss=1.4156748056411743
I0315 07:56:29.052447 139662233843456 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.010789394378662, loss=1.2828325033187866
I0315 07:57:46.753295 139662225450752 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.690713405609131, loss=1.2668085098266602
I0315 07:59:03.486432 139662233843456 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.1881163120269775, loss=1.2967742681503296
I0315 08:00:23.590111 139662225450752 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.560943365097046, loss=1.3231794834136963
I0315 08:01:51.982789 139662233843456 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.6868977546691895, loss=1.3376686573028564
I0315 08:03:16.392791 139662225450752 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.5618317127227783, loss=1.3260469436645508
I0315 08:04:40.766217 139662233843456 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.7764067649841309, loss=1.3051203489303589
I0315 08:06:06.934857 139662225450752 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.8656744956970215, loss=1.3152419328689575
I0315 08:07:32.659414 139662233843456 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.4903030395507812, loss=1.3132591247558594
I0315 08:08:52.320964 139661578483456 logging_writer.py:48] [32000] global_step=32000, grad_norm=5.23460054397583, loss=1.264068603515625
I0315 08:10:07.892226 139661570090752 logging_writer.py:48] [32100] global_step=32100, grad_norm=8.513937950134277, loss=1.3237595558166504
I0315 08:11:23.805680 139661578483456 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.870398998260498, loss=1.3522263765335083
I0315 08:12:40.141243 139661570090752 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.5213444232940674, loss=1.2905583381652832
I0315 08:14:04.917357 139661578483456 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.2244133949279785, loss=1.2984880208969116
I0315 08:14:26.046861 139818008614720 spec.py:321] Evaluating on the training split.
I0315 08:15:19.966871 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 08:16:09.486638 139818008614720 spec.py:349] Evaluating on the test split.
I0315 08:16:35.890708 139818008614720 submission_runner.py:420] Time since start: 28531.43s, 	Step: 32427, 	{'train/ctc_loss': Array(0.20866887, dtype=float32), 'train/wer': 0.07075458901024965, 'validation/ctc_loss': Array(0.4951226, dtype=float32), 'validation/wer': 0.14222269422748293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27268323, dtype=float32), 'test/wer': 0.08835537139723357, 'test/num_examples': 2472, 'score': 25967.665912628174, 'total_duration': 28531.426872015, 'accumulated_submission_time': 25967.665912628174, 'accumulated_eval_time': 2561.5807065963745, 'accumulated_logging_time': 0.8636722564697266}
I0315 08:16:35.926500 139661148403456 logging_writer.py:48] [32427] accumulated_eval_time=2561.580707, accumulated_logging_time=0.863672, accumulated_submission_time=25967.665913, global_step=32427, preemption_count=0, score=25967.665913, test/ctc_loss=0.2726832330226898, test/num_examples=2472, test/wer=0.088355, total_duration=28531.426872, train/ctc_loss=0.20866887271404266, train/wer=0.070755, validation/ctc_loss=0.49512261152267456, validation/num_examples=5348, validation/wer=0.142223
I0315 08:17:32.541202 139661140010752 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.249206304550171, loss=1.3141037225723267
I0315 08:18:48.677196 139661148403456 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.022897958755493, loss=1.2729624509811401
I0315 08:20:04.577298 139661140010752 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.750699043273926, loss=1.3251216411590576
I0315 08:21:29.877882 139661148403456 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.271733522415161, loss=1.3717715740203857
I0315 08:22:56.836607 139661140010752 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.395214796066284, loss=1.3188731670379639
I0315 08:24:22.412162 139660493043456 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.535696506500244, loss=1.297210931777954
I0315 08:25:39.615787 139660484650752 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.5524022579193115, loss=1.2889853715896606
I0315 08:26:55.926014 139660493043456 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.6967129707336426, loss=1.2815287113189697
I0315 08:28:15.903798 139660484650752 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.885789632797241, loss=1.323172926902771
I0315 08:29:38.547563 139660493043456 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.4232590198516846, loss=1.330237865447998
I0315 08:31:07.267187 139660484650752 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6857423782348633, loss=1.2792439460754395
I0315 08:32:35.038637 139660493043456 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.4045746326446533, loss=1.3078535795211792
I0315 08:34:02.798235 139660484650752 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.8491238355636597, loss=1.2088079452514648
I0315 08:35:30.025953 139660493043456 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.5212600231170654, loss=1.3152084350585938
I0315 08:36:55.622513 139660484650752 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.8688900470733643, loss=1.321454644203186
I0315 08:38:25.544853 139660493043456 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.9470925331115723, loss=1.2687355279922485
I0315 08:39:41.624427 139660484650752 logging_writer.py:48] [34100] global_step=34100, grad_norm=5.21430778503418, loss=1.2805914878845215
I0315 08:40:36.354426 139818008614720 spec.py:321] Evaluating on the training split.
I0315 08:41:32.621805 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 08:42:22.496365 139818008614720 spec.py:349] Evaluating on the test split.
I0315 08:42:47.758898 139818008614720 submission_runner.py:420] Time since start: 30103.30s, 	Step: 34173, 	{'train/ctc_loss': Array(0.21239454, dtype=float32), 'train/wer': 0.0721840662868767, 'validation/ctc_loss': Array(0.47541836, dtype=float32), 'validation/wer': 0.1369222896975197, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25914782, dtype=float32), 'test/wer': 0.08339934596713586, 'test/num_examples': 2472, 'score': 27408.011342525482, 'total_duration': 30103.295015335083, 'accumulated_submission_time': 27408.011342525482, 'accumulated_eval_time': 2692.9797925949097, 'accumulated_logging_time': 0.9129633903503418}
I0315 08:42:47.793321 139660493043456 logging_writer.py:48] [34173] accumulated_eval_time=2692.979793, accumulated_logging_time=0.912963, accumulated_submission_time=27408.011343, global_step=34173, preemption_count=0, score=27408.011343, test/ctc_loss=0.2591478228569031, test/num_examples=2472, test/wer=0.083399, total_duration=30103.295015, train/ctc_loss=0.21239453554153442, train/wer=0.072184, validation/ctc_loss=0.475418359041214, validation/num_examples=5348, validation/wer=0.136922
I0315 08:43:08.801620 139660484650752 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.25292706489563, loss=1.2374340295791626
I0315 08:44:23.708231 139660493043456 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.3005170822143555, loss=1.308536171913147
I0315 08:45:38.553057 139660484650752 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.967025637626648, loss=1.2676315307617188
I0315 08:46:53.528941 139660493043456 logging_writer.py:48] [34500] global_step=34500, grad_norm=5.0336174964904785, loss=1.2972142696380615
I0315 08:48:14.417693 139660484650752 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.549804449081421, loss=1.2826614379882812
I0315 08:49:37.640034 139660493043456 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.0048327445983887, loss=1.303856611251831
I0315 08:51:01.674375 139660484650752 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.191333532333374, loss=1.2713896036148071
I0315 08:52:27.246499 139660493043456 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.7771518230438232, loss=1.2922782897949219
I0315 08:53:55.896050 139660484650752 logging_writer.py:48] [35000] global_step=35000, grad_norm=5.1210222244262695, loss=1.2809059619903564
I0315 08:55:18.110506 139662233843456 logging_writer.py:48] [35100] global_step=35100, grad_norm=5.566214084625244, loss=1.2531986236572266
I0315 08:56:34.629938 139662225450752 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.552299976348877, loss=1.2434316873550415
I0315 08:57:49.371500 139662233843456 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.8849170207977295, loss=1.229852318763733
I0315 08:59:06.158128 139662225450752 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.503108263015747, loss=1.2354360818862915
I0315 09:00:30.751842 139662233843456 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.0341086387634277, loss=1.3008720874786377
I0315 09:01:57.710648 139662225450752 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.6883490085601807, loss=1.2878748178482056
I0315 09:03:24.315497 139662233843456 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.8631343841552734, loss=1.2895617485046387
I0315 09:04:50.858005 139662225450752 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.5278823375701904, loss=1.2568162679672241
I0315 09:06:16.864840 139662233843456 logging_writer.py:48] [35900] global_step=35900, grad_norm=4.261380672454834, loss=1.2709800004959106
I0315 09:06:48.132112 139818008614720 spec.py:321] Evaluating on the training split.
I0315 09:07:43.137182 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 09:08:33.642063 139818008614720 spec.py:349] Evaluating on the test split.
I0315 09:09:00.060952 139818008614720 submission_runner.py:420] Time since start: 31675.60s, 	Step: 35938, 	{'train/ctc_loss': Array(0.20002264, dtype=float32), 'train/wer': 0.0666794547992796, 'validation/ctc_loss': Array(0.46640846, dtype=float32), 'validation/wer': 0.13569614875889435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2501728, dtype=float32), 'test/wer': 0.08031198586314058, 'test/num_examples': 2472, 'score': 28848.267106056213, 'total_duration': 31675.596952438354, 'accumulated_submission_time': 28848.267106056213, 'accumulated_eval_time': 2824.903109550476, 'accumulated_logging_time': 0.9611873626708984}
I0315 09:09:00.099379 139662233843456 logging_writer.py:48] [35938] accumulated_eval_time=2824.903110, accumulated_logging_time=0.961187, accumulated_submission_time=28848.267106, global_step=35938, preemption_count=0, score=28848.267106, test/ctc_loss=0.25017279386520386, test/num_examples=2472, test/wer=0.080312, total_duration=31675.596952, train/ctc_loss=0.2000226378440857, train/wer=0.066679, validation/ctc_loss=0.46640846133232117, validation/num_examples=5348, validation/wer=0.135696
I0315 09:09:47.292757 139662225450752 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.0855562686920166, loss=1.2209781408309937
I0315 09:11:07.320516 139661906163456 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.2776129245758057, loss=1.2391403913497925
I0315 09:12:24.908227 139661897770752 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.9418258666992188, loss=1.2470715045928955
I0315 09:13:42.393491 139661906163456 logging_writer.py:48] [36300] global_step=36300, grad_norm=6.163814544677734, loss=1.1888083219528198
I0315 09:15:01.797634 139661897770752 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.598127841949463, loss=1.291698932647705
I0315 09:16:20.631164 139661906163456 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.7353551387786865, loss=1.2662479877471924
I0315 09:17:41.996298 139661897770752 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.6357896327972412, loss=1.258737325668335
I0315 09:19:07.537244 139661906163456 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.143439531326294, loss=1.3299624919891357
I0315 09:20:36.706410 139661897770752 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.577054500579834, loss=1.262219786643982
I0315 09:21:57.655289 139661906163456 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7266582250595093, loss=1.2355268001556396
I0315 09:23:22.399141 139661897770752 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.4963924884796143, loss=1.2483714818954468
I0315 09:24:48.349974 139661906163456 logging_writer.py:48] [37100] global_step=37100, grad_norm=6.5937323570251465, loss=1.2891722917556763
I0315 09:26:03.101616 139661897770752 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.303415298461914, loss=1.2051512002944946
I0315 09:27:19.046130 139661906163456 logging_writer.py:48] [37300] global_step=37300, grad_norm=4.276432991027832, loss=1.2739028930664062
I0315 09:28:39.711615 139661897770752 logging_writer.py:48] [37400] global_step=37400, grad_norm=6.6426191329956055, loss=1.2051925659179688
I0315 09:30:02.394989 139661906163456 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.499347448348999, loss=1.2433226108551025
I0315 09:31:23.742967 139661897770752 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.007049083709717, loss=1.2589659690856934
I0315 09:32:43.573897 139661906163456 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6916414499282837, loss=1.2215627431869507
I0315 09:33:00.649547 139818008614720 spec.py:321] Evaluating on the training split.
I0315 09:33:56.608180 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 09:34:47.160582 139818008614720 spec.py:349] Evaluating on the test split.
I0315 09:35:13.470633 139818008614720 submission_runner.py:420] Time since start: 33249.01s, 	Step: 37722, 	{'train/ctc_loss': Array(0.15005884, dtype=float32), 'train/wer': 0.053837007882086445, 'validation/ctc_loss': Array(0.4560848, dtype=float32), 'validation/wer': 0.1316218851675565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24341933, dtype=float32), 'test/wer': 0.07868705949261674, 'test/num_examples': 2472, 'score': 30288.733597755432, 'total_duration': 33249.00675344467, 'accumulated_submission_time': 30288.733597755432, 'accumulated_eval_time': 2957.7188007831573, 'accumulated_logging_time': 1.0137641429901123}
I0315 09:35:13.507476 139661614323456 logging_writer.py:48] [37722] accumulated_eval_time=2957.718801, accumulated_logging_time=1.013764, accumulated_submission_time=30288.733598, global_step=37722, preemption_count=0, score=30288.733598, test/ctc_loss=0.2434193342924118, test/num_examples=2472, test/wer=0.078687, total_duration=33249.006753, train/ctc_loss=0.1500588357448578, train/wer=0.053837, validation/ctc_loss=0.4560847878456116, validation/num_examples=5348, validation/wer=0.131622
I0315 09:36:12.914462 139661605930752 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.235273838043213, loss=1.2707322835922241
I0315 09:37:28.983181 139661614323456 logging_writer.py:48] [37900] global_step=37900, grad_norm=4.846207141876221, loss=1.194833517074585
I0315 09:38:46.905609 139661605930752 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.4782211780548096, loss=1.253460168838501
I0315 09:40:08.066987 139661614323456 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.3231022357940674, loss=1.2264944314956665
I0315 09:41:28.675192 139661286643456 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.1351253986358643, loss=1.2162017822265625
I0315 09:42:45.432515 139661278250752 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.637937545776367, loss=1.282077670097351
I0315 09:44:01.341918 139661286643456 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.915515661239624, loss=1.2210474014282227
I0315 09:45:18.679966 139661278250752 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.2097201347351074, loss=1.1932077407836914
I0315 09:46:43.554529 139661286643456 logging_writer.py:48] [38600] global_step=38600, grad_norm=5.4307861328125, loss=1.2278642654418945
I0315 09:48:09.874174 139661278250752 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.994951605796814, loss=1.2386808395385742
I0315 09:49:37.478075 139661286643456 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.443833589553833, loss=1.2168627977371216
I0315 09:51:04.078062 139661278250752 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.143348217010498, loss=1.221529483795166
I0315 09:52:28.104021 139661286643456 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.1899919509887695, loss=1.2167410850524902
I0315 09:53:54.300226 139661278250752 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.3877618312835693, loss=1.217789649963379
I0315 09:55:16.146715 139661286643456 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.6079893112182617, loss=1.189941644668579
I0315 09:56:31.921756 139661278250752 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.6402021646499634, loss=1.2005406618118286
I0315 09:57:48.834113 139661286643456 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.660979747772217, loss=1.2059038877487183
I0315 09:59:07.800099 139661278250752 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.9044675827026367, loss=1.1950565576553345
I0315 09:59:13.998006 139818008614720 spec.py:321] Evaluating on the training split.
I0315 10:00:09.371485 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 10:00:58.835564 139818008614720 spec.py:349] Evaluating on the test split.
I0315 10:01:25.408878 139818008614720 submission_runner.py:420] Time since start: 34820.94s, 	Step: 39509, 	{'train/ctc_loss': Array(0.17008072, dtype=float32), 'train/wer': 0.05923317353382188, 'validation/ctc_loss': Array(0.44051743, dtype=float32), 'validation/wer': 0.12786622512720006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23879972, dtype=float32), 'test/wer': 0.07637153941462027, 'test/num_examples': 2472, 'score': 31729.1388194561, 'total_duration': 34820.944878816605, 'accumulated_submission_time': 31729.1388194561, 'accumulated_eval_time': 3089.1241643428802, 'accumulated_logging_time': 1.0664176940917969}
I0315 10:01:25.445041 139662018803456 logging_writer.py:48] [39509] accumulated_eval_time=3089.124164, accumulated_logging_time=1.066418, accumulated_submission_time=31729.138819, global_step=39509, preemption_count=0, score=31729.138819, test/ctc_loss=0.23879972100257874, test/num_examples=2472, test/wer=0.076372, total_duration=34820.944879, train/ctc_loss=0.17008072137832642, train/wer=0.059233, validation/ctc_loss=0.4405174255371094, validation/num_examples=5348, validation/wer=0.127866
I0315 10:02:34.549386 139662010410752 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.1438193321228027, loss=1.2078523635864258
I0315 10:03:49.430600 139662018803456 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.2984848022460938, loss=1.2403900623321533
I0315 10:05:08.801614 139662010410752 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.5226316452026367, loss=1.2051817178726196
I0315 10:06:37.833065 139662018803456 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.3160512447357178, loss=1.2699470520019531
I0315 10:08:02.078722 139662010410752 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.6796317100524902, loss=1.1776964664459229
I0315 10:09:25.084293 139662018803456 logging_writer.py:48] [40100] global_step=40100, grad_norm=5.896297931671143, loss=1.2810920476913452
I0315 10:10:49.185815 139662018803456 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.2972123622894287, loss=1.214861273765564
I0315 10:12:06.107259 139662010410752 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.8408937454223633, loss=1.178999900817871
I0315 10:13:22.176454 139662018803456 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.6197686195373535, loss=1.1747198104858398
I0315 10:14:38.433430 139662010410752 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.857699155807495, loss=1.2233155965805054
I0315 10:15:55.560879 139662018803456 logging_writer.py:48] [40600] global_step=40600, grad_norm=4.429656028747559, loss=1.20697820186615
I0315 10:17:14.237758 139662010410752 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.7871127128601074, loss=1.2011646032333374
I0315 10:18:38.695552 139662018803456 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.7789723873138428, loss=1.1968544721603394
I0315 10:20:02.527307 139662010410752 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.505582571029663, loss=1.2014580965042114
I0315 10:21:30.504057 139662018803456 logging_writer.py:48] [41000] global_step=41000, grad_norm=4.639225006103516, loss=1.2589548826217651
I0315 10:22:59.022325 139662010410752 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.093294620513916, loss=1.175171971321106
I0315 10:24:26.840086 139662018803456 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.6060988903045654, loss=1.1759520769119263
I0315 10:25:25.437409 139818008614720 spec.py:321] Evaluating on the training split.
I0315 10:26:19.292985 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 10:27:09.186427 139818008614720 spec.py:349] Evaluating on the test split.
I0315 10:27:35.603246 139818008614720 submission_runner.py:420] Time since start: 36391.14s, 	Step: 41278, 	{'train/ctc_loss': Array(0.21692227, dtype=float32), 'train/wer': 0.0749608110087165, 'validation/ctc_loss': Array(0.43239108, dtype=float32), 'validation/wer': 0.12404298251542331, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23437671, dtype=float32), 'test/wer': 0.07480754778299108, 'test/num_examples': 2472, 'score': 33169.0481672287, 'total_duration': 36391.13930130005, 'accumulated_submission_time': 33169.0481672287, 'accumulated_eval_time': 3219.284531354904, 'accumulated_logging_time': 1.1159639358520508}
I0315 10:27:35.643858 139661511923456 logging_writer.py:48] [41278] accumulated_eval_time=3219.284531, accumulated_logging_time=1.115964, accumulated_submission_time=33169.048167, global_step=41278, preemption_count=0, score=33169.048167, test/ctc_loss=0.2343767136335373, test/num_examples=2472, test/wer=0.074808, total_duration=36391.139301, train/ctc_loss=0.21692226827144623, train/wer=0.074961, validation/ctc_loss=0.43239107728004456, validation/num_examples=5348, validation/wer=0.124043
I0315 10:27:53.604530 139661503530752 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.6761581897735596, loss=1.2324854135513306
I0315 10:29:10.968754 139661511923456 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.0774283409118652, loss=1.187136173248291
I0315 10:30:25.817252 139661503530752 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.055380344390869, loss=1.2002705335617065
I0315 10:31:41.000206 139661511923456 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.5552366971969604, loss=1.1235679388046265
I0315 10:33:01.379869 139661503530752 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.757910132408142, loss=1.185214638710022
I0315 10:34:27.656807 139661511923456 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.780854344367981, loss=1.1584316492080688
I0315 10:35:51.369161 139661503530752 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.9944907426834106, loss=1.1762138605117798
I0315 10:37:20.436038 139661511923456 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9940381050109863, loss=1.2305039167404175
I0315 10:38:47.559762 139661503530752 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.0984652042388916, loss=1.2191977500915527
I0315 10:40:14.727500 139661511923456 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.9511793851852417, loss=1.1599732637405396
I0315 10:41:35.687881 139661511923456 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.34877347946167, loss=1.1906336545944214
I0315 10:42:52.517877 139661503530752 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.5725913047790527, loss=1.186704397201538
I0315 10:44:10.119847 139661511923456 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.0103676319122314, loss=1.1534028053283691
I0315 10:45:28.592319 139661503530752 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.9090592861175537, loss=1.2143291234970093
I0315 10:46:54.445284 139661511923456 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.006815195083618, loss=1.1527585983276367
I0315 10:48:17.251756 139661503530752 logging_writer.py:48] [42800] global_step=42800, grad_norm=4.141611099243164, loss=1.1753920316696167
I0315 10:49:45.899486 139661511923456 logging_writer.py:48] [42900] global_step=42900, grad_norm=5.988974571228027, loss=1.1455342769622803
I0315 10:51:12.979363 139661503530752 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.855919361114502, loss=1.19631028175354
I0315 10:51:35.641319 139818008614720 spec.py:321] Evaluating on the training split.
I0315 10:52:29.064678 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 10:53:20.935397 139818008614720 spec.py:349] Evaluating on the test split.
I0315 10:53:47.184149 139818008614720 submission_runner.py:420] Time since start: 37962.72s, 	Step: 43028, 	{'train/ctc_loss': Array(0.22430344, dtype=float32), 'train/wer': 0.07638630597822768, 'validation/ctc_loss': Array(0.4287209, dtype=float32), 'validation/wer': 0.123058207903299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22969964, dtype=float32), 'test/wer': 0.07369041090325594, 'test/num_examples': 2472, 'score': 34608.96385860443, 'total_duration': 37962.72094845772, 'accumulated_submission_time': 34608.96385860443, 'accumulated_eval_time': 3350.8226585388184, 'accumulated_logging_time': 1.1698634624481201}
I0315 10:53:47.220566 139661511923456 logging_writer.py:48] [43028] accumulated_eval_time=3350.822659, accumulated_logging_time=1.169863, accumulated_submission_time=34608.963859, global_step=43028, preemption_count=0, score=34608.963859, test/ctc_loss=0.22969964146614075, test/num_examples=2472, test/wer=0.073690, total_duration=37962.720948, train/ctc_loss=0.2243034392595291, train/wer=0.076386, validation/ctc_loss=0.4287208914756775, validation/num_examples=5348, validation/wer=0.123058
I0315 10:54:41.803790 139661503530752 logging_writer.py:48] [43100] global_step=43100, grad_norm=9.347397804260254, loss=1.1418429613113403
I0315 10:55:58.157280 139661511923456 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.952061891555786, loss=1.14763605594635
I0315 10:57:17.319485 139660528883456 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.9802565574645996, loss=1.2147852182388306
I0315 10:58:33.828748 139660520490752 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.2905948162078857, loss=1.114648461341858
I0315 10:59:51.467416 139660528883456 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7701992988586426, loss=1.1861927509307861
I0315 11:01:12.584557 139660520490752 logging_writer.py:48] [43600] global_step=43600, grad_norm=4.321925640106201, loss=1.1283060312271118
I0315 11:02:40.301739 139660528883456 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.2062466144561768, loss=1.1657299995422363
I0315 11:04:06.259731 139660520490752 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.5725724697113037, loss=1.2044322490692139
I0315 11:05:34.590460 139660528883456 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.103278398513794, loss=1.1781485080718994
I0315 11:06:59.660084 139660520490752 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.9848642349243164, loss=1.1399911642074585
I0315 11:08:23.133524 139660528883456 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.9128952026367188, loss=1.2048758268356323
I0315 11:09:52.253524 139660520490752 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.312215805053711, loss=1.1601758003234863
I0315 11:11:22.224667 139661511923456 logging_writer.py:48] [44300] global_step=44300, grad_norm=8.234174728393555, loss=1.1623975038528442
I0315 11:12:39.096604 139661503530752 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.138780117034912, loss=1.1482607126235962
I0315 11:13:54.406720 139661511923456 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.1570940017700195, loss=1.1595960855484009
I0315 11:15:10.411423 139661503530752 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.3220622539520264, loss=1.1573781967163086
I0315 11:16:28.489855 139661511923456 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.419161081314087, loss=1.1258100271224976
I0315 11:17:48.066206 139818008614720 spec.py:321] Evaluating on the training split.
I0315 11:18:43.906593 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 11:19:34.052486 139818008614720 spec.py:349] Evaluating on the test split.
I0315 11:20:01.415490 139818008614720 submission_runner.py:420] Time since start: 39536.95s, 	Step: 44794, 	{'train/ctc_loss': Array(0.2613986, dtype=float32), 'train/wer': 0.09016618797199197, 'validation/ctc_loss': Array(0.4249241, dtype=float32), 'validation/wer': 0.1222375623931954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22824307, dtype=float32), 'test/wer': 0.07318262141246724, 'test/num_examples': 2472, 'score': 36049.72664427757, 'total_duration': 39536.94931817055, 'accumulated_submission_time': 36049.72664427757, 'accumulated_eval_time': 3484.164263486862, 'accumulated_logging_time': 1.2190589904785156}
I0315 11:20:01.460723 139661511923456 logging_writer.py:48] [44794] accumulated_eval_time=3484.164263, accumulated_logging_time=1.219059, accumulated_submission_time=36049.726644, global_step=44794, preemption_count=0, score=36049.726644, test/ctc_loss=0.22824306786060333, test/num_examples=2472, test/wer=0.073183, total_duration=39536.949318, train/ctc_loss=0.2613986134529114, train/wer=0.090166, validation/ctc_loss=0.4249241054058075, validation/num_examples=5348, validation/wer=0.122238
I0315 11:20:06.953399 139661503530752 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7931712865829468, loss=1.1617846488952637
I0315 11:21:23.320917 139661511923456 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.79422926902771, loss=1.1012521982192993
I0315 11:22:39.377281 139661503530752 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.1364493370056152, loss=1.1547510623931885
I0315 11:23:55.516804 139661511923456 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.8112146854400635, loss=1.1678705215454102
I0315 11:25:24.388849 139661503530752 logging_writer.py:48] [45200] global_step=45200, grad_norm=6.147968292236328, loss=1.175571322441101
I0315 11:26:52.702861 139661511923456 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.8306379318237305, loss=1.1126196384429932
I0315 11:28:13.702707 139660856563456 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.6264530420303345, loss=1.1242585182189941
I0315 11:29:29.074377 139660848170752 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7778716087341309, loss=1.151993989944458
I0315 11:30:44.818593 139660856563456 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.7093746662139893, loss=1.1625477075576782
I0315 11:32:03.320204 139660848170752 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.364337921142578, loss=1.1151094436645508
I0315 11:33:24.185897 139660856563456 logging_writer.py:48] [45800] global_step=45800, grad_norm=4.812857151031494, loss=1.2120728492736816
I0315 11:34:52.056401 139660848170752 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.4821834564208984, loss=1.1721866130828857
I0315 11:36:19.331521 139660856563456 logging_writer.py:48] [46000] global_step=46000, grad_norm=4.49318265914917, loss=1.156490683555603
I0315 11:37:44.186480 139660848170752 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.5957367420196533, loss=1.158837080001831
I0315 11:39:06.379410 139660856563456 logging_writer.py:48] [46200] global_step=46200, grad_norm=6.569473743438721, loss=1.171656608581543
I0315 11:40:32.483006 139660848170752 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.975600242614746, loss=1.1511473655700684
I0315 11:41:57.430971 139660856563456 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.377647638320923, loss=1.1855052709579468
I0315 11:43:13.776612 139660848170752 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.5809561014175415, loss=1.0813703536987305
I0315 11:44:01.453716 139818008614720 spec.py:321] Evaluating on the training split.
I0315 11:44:53.333795 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 11:45:43.939709 139818008614720 spec.py:349] Evaluating on the test split.
I0315 11:46:11.252157 139818008614720 submission_runner.py:420] Time since start: 41106.79s, 	Step: 46564, 	{'train/ctc_loss': Array(0.23482683, dtype=float32), 'train/wer': 0.07781617789919282, 'validation/ctc_loss': Array(0.42247275, dtype=float32), 'validation/wer': 0.12163897390347278, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.226913, dtype=float32), 'test/wer': 0.0726951435013101, 'test/num_examples': 2472, 'score': 37489.63005280495, 'total_duration': 41106.78757286072, 'accumulated_submission_time': 37489.63005280495, 'accumulated_eval_time': 3613.9566090106964, 'accumulated_logging_time': 1.2820138931274414}
I0315 11:46:11.290905 139660923119360 logging_writer.py:48] [46564] accumulated_eval_time=3613.956609, accumulated_logging_time=1.282014, accumulated_submission_time=37489.630053, global_step=46564, preemption_count=0, score=37489.630053, test/ctc_loss=0.22691300511360168, test/num_examples=2472, test/wer=0.072695, total_duration=41106.787573, train/ctc_loss=0.23482683300971985, train/wer=0.077816, validation/ctc_loss=0.42247274518013, validation/num_examples=5348, validation/wer=0.121639
I0315 11:46:39.200623 139660914726656 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.531217336654663, loss=1.169920563697815
I0315 11:47:53.983667 139660923119360 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.7594048976898193, loss=1.1667656898498535
I0315 11:49:09.834249 139660914726656 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.3040366172790527, loss=1.1825780868530273
I0315 11:50:24.712076 139660923119360 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.2483091354370117, loss=1.1649279594421387
I0315 11:51:48.547654 139660914726656 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.152902126312256, loss=1.1355599164962769
I0315 11:53:12.535347 139660923119360 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.5340576171875, loss=1.1007952690124512
I0315 11:54:38.954080 139660914726656 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.235337495803833, loss=1.1495848894119263
I0315 11:56:06.647216 139660923119360 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.5537137985229492, loss=1.1087274551391602
I0315 11:57:34.084862 139660595439360 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.3677035570144653, loss=1.1105910539627075
I0315 11:58:49.276076 139660587046656 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.0644357204437256, loss=1.1475780010223389
I0315 12:00:04.878568 139660595439360 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.283369302749634, loss=1.1521353721618652
I0315 12:01:21.460676 139660587046656 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.145598888397217, loss=1.1287022829055786
I0315 12:02:41.531264 139660595439360 logging_writer.py:48] [47800] global_step=47800, grad_norm=9.42904281616211, loss=1.1940803527832031
I0315 12:04:06.014775 139660587046656 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.9377143383026123, loss=1.1242009401321411
I0315 12:05:32.019424 139660595439360 logging_writer.py:48] [48000] global_step=48000, grad_norm=2.5708024501800537, loss=1.1430079936981201
I0315 12:06:57.875262 139660587046656 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.375596046447754, loss=1.1502788066864014
I0315 12:08:24.706382 139660595439360 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.516286611557007, loss=1.2083971500396729
I0315 12:09:54.194444 139660587046656 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.1899545192718506, loss=1.1264667510986328
I0315 12:10:11.868356 139818008614720 spec.py:321] Evaluating on the training split.
I0315 12:11:08.547533 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 12:11:58.398596 139818008614720 spec.py:349] Evaluating on the test split.
I0315 12:12:25.763644 139818008614720 submission_runner.py:420] Time since start: 42681.30s, 	Step: 48322, 	{'train/ctc_loss': Array(0.21850312, dtype=float32), 'train/wer': 0.0765535056172269, 'validation/ctc_loss': Array(0.42252696, dtype=float32), 'validation/wer': 0.12179344835243346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22681805, dtype=float32), 'test/wer': 0.07285763613836248, 'test/num_examples': 2472, 'score': 38930.12107872963, 'total_duration': 42681.298971414566, 'accumulated_submission_time': 38930.12107872963, 'accumulated_eval_time': 3747.8457283973694, 'accumulated_logging_time': 1.3374078273773193}
I0315 12:12:25.805435 139660298475264 logging_writer.py:48] [48322] accumulated_eval_time=3747.845728, accumulated_logging_time=1.337408, accumulated_submission_time=38930.121079, global_step=48322, preemption_count=0, score=38930.121079, test/ctc_loss=0.2268180549144745, test/num_examples=2472, test/wer=0.072858, total_duration=42681.298971, train/ctc_loss=0.21850311756134033, train/wer=0.076554, validation/ctc_loss=0.4225269556045532, validation/num_examples=5348, validation/wer=0.121793
I0315 12:13:25.824860 139660290082560 logging_writer.py:48] [48400] global_step=48400, grad_norm=2.201239824295044, loss=1.1098185777664185
I0315 12:14:45.254299 139660298475264 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.4187166690826416, loss=1.158729076385498
I0315 12:16:03.770543 139660290082560 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.466789722442627, loss=1.1859493255615234
I0315 12:17:25.859240 139660298475264 logging_writer.py:48] [48700] global_step=48700, grad_norm=4.825474739074707, loss=1.213020920753479
I0315 12:18:47.961738 139660290082560 logging_writer.py:48] [48800] global_step=48800, grad_norm=2.1052098274230957, loss=1.1060409545898438
I0315 12:20:11.862127 139660298475264 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.4375128746032715, loss=1.1414121389389038
I0315 12:21:38.532979 139660290082560 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.361602783203125, loss=1.1713916063308716
I0315 12:23:00.922100 139660298475264 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.658111095428467, loss=1.1981123685836792
I0315 12:24:26.455858 139660290082560 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.9430327415466309, loss=1.1625795364379883
I0315 12:25:48.587252 139660298475264 logging_writer.py:48] [49300] global_step=49300, grad_norm=2.443317413330078, loss=1.1973493099212646
I0315 12:27:15.232008 139660290082560 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.1948606967926025, loss=1.1870267391204834
I0315 12:28:37.126895 139660298475264 logging_writer.py:48] [49500] global_step=49500, grad_norm=4.882152080535889, loss=1.1557179689407349
I0315 12:29:52.360119 139660290082560 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.298470973968506, loss=1.1183537244796753
I0315 12:31:09.471856 139660298475264 logging_writer.py:48] [49700] global_step=49700, grad_norm=5.257525444030762, loss=1.1546536684036255
I0315 12:32:29.429472 139660290082560 logging_writer.py:48] [49800] global_step=49800, grad_norm=4.281478404998779, loss=1.123186469078064
I0315 12:33:53.027142 139660298475264 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.458651542663574, loss=1.215256929397583
I0315 12:35:20.167788 139660290082560 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.400899648666382, loss=1.146876335144043
I0315 12:36:26.245056 139818008614720 spec.py:321] Evaluating on the training split.
I0315 12:37:20.831206 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 12:38:12.591754 139818008614720 spec.py:349] Evaluating on the test split.
I0315 12:38:39.807880 139818008614720 submission_runner.py:420] Time since start: 44255.34s, 	Step: 50079, 	{'train/ctc_loss': Array(0.19701824, dtype=float32), 'train/wer': 0.0689132018836529, 'validation/ctc_loss': Array(0.42258406, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268369, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 40370.472751140594, 'total_duration': 44255.343381881714, 'accumulated_submission_time': 40370.472751140594, 'accumulated_eval_time': 3881.402559518814, 'accumulated_logging_time': 1.3948400020599365}
I0315 12:38:39.851752 139660298475264 logging_writer.py:48] [50079] accumulated_eval_time=3881.402560, accumulated_logging_time=1.394840, accumulated_submission_time=40370.472751, global_step=50079, preemption_count=0, score=40370.472751, test/ctc_loss=0.2268369048833847, test/num_examples=2472, test/wer=0.072939, total_duration=44255.343382, train/ctc_loss=0.19701823592185974, train/wer=0.068913, validation/ctc_loss=0.42258405685424805, validation/num_examples=5348, validation/wer=0.121851
I0315 12:38:56.353000 139660290082560 logging_writer.py:48] [50100] global_step=50100, grad_norm=2.7337028980255127, loss=1.1472266912460327
I0315 12:40:12.475491 139660298475264 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.635422706604004, loss=1.1159368753433228
I0315 12:41:27.264381 139660290082560 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.4819695949554443, loss=1.1307584047317505
I0315 12:42:45.956575 139660298475264 logging_writer.py:48] [50400] global_step=50400, grad_norm=2.1037497520446777, loss=1.14523184299469
I0315 12:44:11.919517 139660298475264 logging_writer.py:48] [50500] global_step=50500, grad_norm=5.674553871154785, loss=1.1755155324935913
I0315 12:45:28.370503 139660290082560 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.4012651443481445, loss=1.129777431488037
I0315 12:46:43.199343 139660298475264 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.016653299331665, loss=1.138837456703186
I0315 12:48:00.376183 139660290082560 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.7301509380340576, loss=1.1748172044754028
I0315 12:49:21.571308 139660298475264 logging_writer.py:48] [50900] global_step=50900, grad_norm=2.97944974899292, loss=1.1686789989471436
I0315 12:50:43.296010 139660290082560 logging_writer.py:48] [51000] global_step=51000, grad_norm=4.057518005371094, loss=1.1348189115524292
I0315 12:52:12.043984 139660298475264 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.9989173412323, loss=1.1174805164337158
I0315 12:53:39.007463 139660290082560 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.425729274749756, loss=1.2325439453125
I0315 12:55:07.692008 139660298475264 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.929203748703003, loss=1.124258279800415
I0315 12:56:32.665561 139660290082560 logging_writer.py:48] [51400] global_step=51400, grad_norm=5.2981977462768555, loss=1.1448025703430176
I0315 12:57:59.689419 139660298475264 logging_writer.py:48] [51500] global_step=51500, grad_norm=4.139111518859863, loss=1.1428358554840088
I0315 12:59:14.618252 139660290082560 logging_writer.py:48] [51600] global_step=51600, grad_norm=5.702473163604736, loss=1.144791841506958
I0315 13:00:31.623847 139660298475264 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.4057230949401855, loss=1.1233240365982056
I0315 13:01:48.284589 139660290082560 logging_writer.py:48] [51800] global_step=51800, grad_norm=4.180043697357178, loss=1.19091796875
I0315 13:02:40.377589 139818008614720 spec.py:321] Evaluating on the training split.
I0315 13:03:34.876659 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 13:04:26.416190 139818008614720 spec.py:349] Evaluating on the test split.
I0315 13:04:51.815721 139818008614720 submission_runner.py:420] Time since start: 45827.35s, 	Step: 51869, 	{'train/ctc_loss': Array(0.22619057, dtype=float32), 'train/wer': 0.07824490704785284, 'validation/ctc_loss': Array(0.4225461, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682315, dtype=float32), 'test/wer': 0.07289825929762557, 'test/num_examples': 2472, 'score': 41810.91124749184, 'total_duration': 45827.35194015503, 'accumulated_submission_time': 41810.91124749184, 'accumulated_eval_time': 4012.8354184627533, 'accumulated_logging_time': 1.4524447917938232}
I0315 13:04:51.851190 139660626155264 logging_writer.py:48] [51869] accumulated_eval_time=4012.835418, accumulated_logging_time=1.452445, accumulated_submission_time=41810.911247, global_step=51869, preemption_count=0, score=41810.911247, test/ctc_loss=0.22682315111160278, test/num_examples=2472, test/wer=0.072898, total_duration=45827.351940, train/ctc_loss=0.22619056701660156, train/wer=0.078245, validation/ctc_loss=0.4225460886955261, validation/num_examples=5348, validation/wer=0.121851
I0315 13:05:16.881694 139660617762560 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.147700071334839, loss=1.1526925563812256
I0315 13:06:33.301932 139660626155264 logging_writer.py:48] [52000] global_step=52000, grad_norm=8.18604850769043, loss=1.1880255937576294
I0315 13:07:49.846469 139660617762560 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.9955735206604004, loss=1.1438161134719849
I0315 13:09:08.150512 139660626155264 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.871582269668579, loss=1.1724622249603271
I0315 13:10:35.007871 139660617762560 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.0061988830566406, loss=1.077167272567749
I0315 13:11:59.116434 139660626155264 logging_writer.py:48] [52400] global_step=52400, grad_norm=2.939056158065796, loss=1.1366509199142456
I0315 13:13:25.012446 139660617762560 logging_writer.py:48] [52500] global_step=52500, grad_norm=6.343631267547607, loss=1.1387745141983032
I0315 13:14:46.311199 139660298475264 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.6763619184494019, loss=1.202759861946106
I0315 13:16:02.795310 139660290082560 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.9659888744354248, loss=1.1890130043029785
I0315 13:17:17.745830 139660298475264 logging_writer.py:48] [52800] global_step=52800, grad_norm=2.2014243602752686, loss=1.1245428323745728
I0315 13:18:37.913183 139660290082560 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.998990297317505, loss=1.1777727603912354
I0315 13:20:03.919845 139660298475264 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.2983808517456055, loss=1.1153690814971924
I0315 13:21:31.171185 139660290082560 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.089775800704956, loss=1.1390421390533447
I0315 13:22:58.453300 139660298475264 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.5380427837371826, loss=1.1546250581741333
I0315 13:24:24.909223 139660290082560 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.8992528915405273, loss=1.1742717027664185
I0315 13:25:47.260774 139660298475264 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.5695390701293945, loss=1.1412053108215332
I0315 13:27:12.958671 139660290082560 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.0203795433044434, loss=1.182276964187622
I0315 13:28:39.302722 139660626155264 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.1741878986358643, loss=1.126483678817749
I0315 13:28:51.837558 139818008614720 spec.py:321] Evaluating on the training split.
I0315 13:29:46.581310 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 13:30:36.791485 139818008614720 spec.py:349] Evaluating on the test split.
I0315 13:31:02.976082 139818008614720 submission_runner.py:420] Time since start: 47398.51s, 	Step: 53618, 	{'train/ctc_loss': Array(0.21388717, dtype=float32), 'train/wer': 0.07441268682356983, 'validation/ctc_loss': Array(0.42257115, dtype=float32), 'validation/wer': 0.12181275765855354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268347, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 43250.74343371391, 'total_duration': 47398.51116228104, 'accumulated_submission_time': 43250.74343371391, 'accumulated_eval_time': 4143.967495203018, 'accumulated_logging_time': 1.5726919174194336}
I0315 13:31:03.023463 139660626155264 logging_writer.py:48] [53618] accumulated_eval_time=4143.967495, accumulated_logging_time=1.572692, accumulated_submission_time=43250.743434, global_step=53618, preemption_count=0, score=43250.743434, test/ctc_loss=0.22683469951152802, test/num_examples=2472, test/wer=0.072919, total_duration=47398.511162, train/ctc_loss=0.21388716995716095, train/wer=0.074413, validation/ctc_loss=0.4225711524486542, validation/num_examples=5348, validation/wer=0.121813
I0315 13:32:05.501310 139660617762560 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.5912792682647705, loss=1.0802282094955444
I0315 13:33:21.833139 139660626155264 logging_writer.py:48] [53800] global_step=53800, grad_norm=2.099443197250366, loss=1.152640700340271
I0315 13:34:37.220243 139660617762560 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.1073853969573975, loss=1.1869255304336548
I0315 13:35:52.657311 139660626155264 logging_writer.py:48] [54000] global_step=54000, grad_norm=7.363567352294922, loss=1.1257236003875732
I0315 13:37:07.730880 139660617762560 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.833009123802185, loss=1.1394753456115723
I0315 13:38:31.975390 139660626155264 logging_writer.py:48] [54200] global_step=54200, grad_norm=9.830779075622559, loss=1.1129841804504395
I0315 13:39:55.700038 139660617762560 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.0193262100219727, loss=1.1857192516326904
I0315 13:41:23.877274 139660626155264 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.1978862285614014, loss=1.1613653898239136
I0315 13:42:45.920703 139660617762560 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.4557089805603027, loss=1.135575532913208
I0315 13:44:16.398401 139660626155264 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.8603172302246094, loss=1.1861066818237305
I0315 13:45:31.350301 139660617762560 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.7625842094421387, loss=1.1860483884811401
I0315 13:46:48.302496 139660626155264 logging_writer.py:48] [54800] global_step=54800, grad_norm=5.677431106567383, loss=1.2099090814590454
I0315 13:48:04.520540 139660617762560 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.8333992958068848, loss=1.0763449668884277
I0315 13:49:21.849983 139660626155264 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.318021059036255, loss=1.1266305446624756
I0315 13:50:46.211963 139660617762560 logging_writer.py:48] [55100] global_step=55100, grad_norm=2.142890214920044, loss=1.182273030281067
I0315 13:52:14.186079 139660626155264 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.0428144931793213, loss=1.1433932781219482
I0315 13:53:39.582817 139660617762560 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.8583760261535645, loss=1.1053675413131714
I0315 13:55:03.626320 139660626155264 logging_writer.py:48] [55400] global_step=55400, grad_norm=5.759933948516846, loss=1.1534581184387207
I0315 13:55:03.632268 139818008614720 spec.py:321] Evaluating on the training split.
I0315 13:55:58.013667 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 13:56:49.084697 139818008614720 spec.py:349] Evaluating on the test split.
I0315 13:57:14.557258 139818008614720 submission_runner.py:420] Time since start: 48970.09s, 	Step: 55401, 	{'train/ctc_loss': Array(0.21749085, dtype=float32), 'train/wer': 0.0768450514759327, 'validation/ctc_loss': Array(0.42252436, dtype=float32), 'validation/wer': 0.12184172161773367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22681281, dtype=float32), 'test/wer': 0.07289825929762557, 'test/num_examples': 2472, 'score': 44691.26508259773, 'total_duration': 48970.09406757355, 'accumulated_submission_time': 44691.26508259773, 'accumulated_eval_time': 4274.8877511024475, 'accumulated_logging_time': 1.6357917785644531}
I0315 13:57:14.595929 139660923119360 logging_writer.py:48] [55401] accumulated_eval_time=4274.887751, accumulated_logging_time=1.635792, accumulated_submission_time=44691.265083, global_step=55401, preemption_count=0, score=44691.265083, test/ctc_loss=0.22681280970573425, test/num_examples=2472, test/wer=0.072898, total_duration=48970.094068, train/ctc_loss=0.21749085187911987, train/wer=0.076845, validation/ctc_loss=0.4225243628025055, validation/num_examples=5348, validation/wer=0.121842
I0315 13:58:31.040118 139660914726656 logging_writer.py:48] [55500] global_step=55500, grad_norm=2.1320881843566895, loss=1.1707680225372314
I0315 13:59:46.560800 139660923119360 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.609574317932129, loss=1.120161771774292
I0315 14:01:06.665038 139660595439360 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.281247854232788, loss=1.145531177520752
I0315 14:02:23.814732 139660587046656 logging_writer.py:48] [55800] global_step=55800, grad_norm=5.428625583648682, loss=1.1363753080368042
I0315 14:03:39.104736 139660595439360 logging_writer.py:48] [55900] global_step=55900, grad_norm=4.562987804412842, loss=1.1331921815872192
I0315 14:04:54.582880 139660587046656 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.144880533218384, loss=1.2026981115341187
I0315 14:06:20.024521 139660595439360 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.171149969100952, loss=1.1602777242660522
I0315 14:07:48.025537 139660587046656 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.73157000541687, loss=1.2351124286651611
I0315 14:09:13.628852 139660595439360 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8763749599456787, loss=1.1702330112457275
I0315 14:10:33.714885 139660587046656 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.408494710922241, loss=1.1660596132278442
I0315 14:11:58.223411 139660595439360 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.845719337463379, loss=1.134150743484497
I0315 14:13:19.323381 139660587046656 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.2830698490142822, loss=1.1448203325271606
I0315 14:14:42.512732 139660595439360 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.266162633895874, loss=1.1413166522979736
I0315 14:15:57.508932 139660587046656 logging_writer.py:48] [56800] global_step=56800, grad_norm=2.3884525299072266, loss=1.1559524536132812
I0315 14:17:12.868594 139660595439360 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.3614869117736816, loss=1.1660213470458984
I0315 14:18:33.715682 139660587046656 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8924119472503662, loss=1.1221880912780762
I0315 14:19:56.770312 139660595439360 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.904655694961548, loss=1.1946845054626465
I0315 14:21:14.777374 139818008614720 spec.py:321] Evaluating on the training split.
I0315 14:22:11.950072 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 14:23:02.451143 139818008614720 spec.py:349] Evaluating on the test split.
I0315 14:23:29.503374 139818008614720 submission_runner.py:420] Time since start: 50545.04s, 	Step: 57190, 	{'train/ctc_loss': Array(0.22159734, dtype=float32), 'train/wer': 0.07648861283643893, 'validation/ctc_loss': Array(0.42251283, dtype=float32), 'validation/wer': 0.12181275765855354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22681028, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 46131.35928487778, 'total_duration': 50545.039157152176, 'accumulated_submission_time': 46131.35928487778, 'accumulated_eval_time': 4409.608050823212, 'accumulated_logging_time': 1.689574956893921}
I0315 14:23:29.545499 139661511923456 logging_writer.py:48] [57190] accumulated_eval_time=4409.608051, accumulated_logging_time=1.689575, accumulated_submission_time=46131.359285, global_step=57190, preemption_count=0, score=46131.359285, test/ctc_loss=0.2268102765083313, test/num_examples=2472, test/wer=0.072878, total_duration=50545.039157, train/ctc_loss=0.2215973436832428, train/wer=0.076489, validation/ctc_loss=0.42251282930374146, validation/num_examples=5348, validation/wer=0.121813
I0315 14:23:38.137760 139661503530752 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.026336908340454, loss=1.1572165489196777
I0315 14:24:54.320423 139661511923456 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.4721271991729736, loss=1.1781429052352905
I0315 14:26:10.069793 139661503530752 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.4079318046569824, loss=1.1310569047927856
I0315 14:27:25.912637 139661511923456 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8756972551345825, loss=1.1573879718780518
I0315 14:28:51.737782 139661503530752 logging_writer.py:48] [57600] global_step=57600, grad_norm=7.216372013092041, loss=1.204243540763855
I0315 14:30:17.519415 139660856563456 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.484234571456909, loss=1.1806750297546387
I0315 14:31:34.223968 139660848170752 logging_writer.py:48] [57800] global_step=57800, grad_norm=4.03744649887085, loss=1.1408079862594604
I0315 14:32:53.355182 139660856563456 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.874406099319458, loss=1.1560063362121582
I0315 14:34:11.480962 139660848170752 logging_writer.py:48] [58000] global_step=58000, grad_norm=4.389871597290039, loss=1.1669784784317017
I0315 14:35:32.893198 139660856563456 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.878720283508301, loss=1.1354559659957886
I0315 14:36:58.834373 139660848170752 logging_writer.py:48] [58200] global_step=58200, grad_norm=9.623161315917969, loss=1.1349811553955078
I0315 14:38:20.648823 139660856563456 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.299445867538452, loss=1.1820765733718872
I0315 14:39:44.241106 139660848170752 logging_writer.py:48] [58400] global_step=58400, grad_norm=7.799999713897705, loss=1.188281536102295
I0315 14:41:12.032077 139660856563456 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.830025315284729, loss=1.1345738172531128
I0315 14:42:39.049411 139660848170752 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.6922502517700195, loss=1.1316732168197632
I0315 14:44:05.709216 139660856563456 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.602676510810852, loss=1.134625792503357
I0315 14:45:25.660053 139661511923456 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.034252882003784, loss=1.1616705656051636
I0315 14:46:41.275681 139661503530752 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.6049022674560547, loss=1.1779650449752808
I0315 14:47:30.097942 139818008614720 spec.py:321] Evaluating on the training split.
I0315 14:48:26.754885 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 14:49:20.057651 139818008614720 spec.py:349] Evaluating on the test split.
I0315 14:49:47.384500 139818008614720 submission_runner.py:420] Time since start: 52122.92s, 	Step: 58964, 	{'train/ctc_loss': Array(0.23678349, dtype=float32), 'train/wer': 0.08127582615065874, 'validation/ctc_loss': Array(0.42260745, dtype=float32), 'validation/wer': 0.12186103092385375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684751, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 47571.82388663292, 'total_duration': 52122.92017412186, 'accumulated_submission_time': 47571.82388663292, 'accumulated_eval_time': 4546.888776302338, 'accumulated_logging_time': 1.7484843730926514}
I0315 14:49:47.427721 139661511923456 logging_writer.py:48] [58964] accumulated_eval_time=4546.888776, accumulated_logging_time=1.748484, accumulated_submission_time=47571.823887, global_step=58964, preemption_count=0, score=47571.823887, test/ctc_loss=0.22684751451015472, test/num_examples=2472, test/wer=0.072980, total_duration=52122.920174, train/ctc_loss=0.2367834895849228, train/wer=0.081276, validation/ctc_loss=0.4226074516773224, validation/num_examples=5348, validation/wer=0.121861
I0315 14:50:15.519606 139661503530752 logging_writer.py:48] [59000] global_step=59000, grad_norm=2.432079553604126, loss=1.1792857646942139
I0315 14:51:30.828969 139661511923456 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.6296303272247314, loss=1.1687647104263306
I0315 14:52:46.445058 139661503530752 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.6552579402923584, loss=1.1355632543563843
I0315 14:54:02.930545 139661511923456 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.5416077375411987, loss=1.139196753501892
I0315 14:55:28.822703 139661503530752 logging_writer.py:48] [59400] global_step=59400, grad_norm=4.388542652130127, loss=1.1574324369430542
I0315 14:56:54.380799 139661511923456 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.2828235626220703, loss=1.1880890130996704
I0315 14:58:23.638233 139661503530752 logging_writer.py:48] [59600] global_step=59600, grad_norm=7.057580947875977, loss=1.134117841720581
I0315 14:59:51.031094 139661511923456 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.6859818696975708, loss=1.156461477279663
I0315 15:01:11.934567 139660856563456 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.555762529373169, loss=1.1632989645004272
I0315 15:02:26.881692 139660848170752 logging_writer.py:48] [59900] global_step=59900, grad_norm=4.371490955352783, loss=1.2057089805603027
I0315 15:03:43.500771 139660856563456 logging_writer.py:48] [60000] global_step=60000, grad_norm=4.863318920135498, loss=1.1290767192840576
I0315 15:04:59.090865 139660848170752 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.207777976989746, loss=1.1672331094741821
I0315 15:06:22.513031 139660856563456 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.0953195095062256, loss=1.1750972270965576
I0315 15:07:45.211479 139660848170752 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.7341896295547485, loss=1.2252027988433838
I0315 15:09:13.910853 139660856563456 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.1569643020629883, loss=1.133890151977539
I0315 15:10:41.902830 139660848170752 logging_writer.py:48] [60500] global_step=60500, grad_norm=4.367262840270996, loss=1.1635664701461792
I0315 15:12:06.262718 139660856563456 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.901480793952942, loss=1.2163821458816528
I0315 15:13:35.054616 139660848170752 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.17020845413208, loss=1.2051801681518555
I0315 15:13:47.817255 139818008614720 spec.py:321] Evaluating on the training split.
I0315 15:14:43.001360 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 15:15:33.972546 139818008614720 spec.py:349] Evaluating on the test split.
I0315 15:16:00.648473 139818008614720 submission_runner.py:420] Time since start: 53696.18s, 	Step: 60715, 	{'train/ctc_loss': Array(0.2431013, dtype=float32), 'train/wer': 0.08164432198128928, 'validation/ctc_loss': Array(0.42257443, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268356, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 49012.130105018616, 'total_duration': 53696.184061050415, 'accumulated_submission_time': 49012.130105018616, 'accumulated_eval_time': 4679.714070558548, 'accumulated_logging_time': 1.8052341938018799}
I0315 15:16:00.688270 139660856563456 logging_writer.py:48] [60715] accumulated_eval_time=4679.714071, accumulated_logging_time=1.805234, accumulated_submission_time=49012.130105, global_step=60715, preemption_count=0, score=49012.130105, test/ctc_loss=0.22683559358119965, test/num_examples=2472, test/wer=0.072919, total_duration=53696.184061, train/ctc_loss=0.2431012988090515, train/wer=0.081644, validation/ctc_loss=0.4225744307041168, validation/num_examples=5348, validation/wer=0.121832
I0315 15:17:09.187009 139660528883456 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.364502429962158, loss=1.1190032958984375
I0315 15:18:25.466899 139660520490752 logging_writer.py:48] [60900] global_step=60900, grad_norm=4.334808349609375, loss=1.1823281049728394
I0315 15:19:41.509324 139660528883456 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.94303297996521, loss=1.1624844074249268
I0315 15:20:58.315957 139660520490752 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.0639233589172363, loss=1.0998843908309937
I0315 15:22:23.326184 139660528883456 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.9379210472106934, loss=1.1296095848083496
I0315 15:23:48.848560 139660520490752 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.7053065299987793, loss=1.1221472024917603
I0315 15:25:11.542295 139660528883456 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.3185744285583496, loss=1.161424994468689
I0315 15:26:37.184351 139660520490752 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.0291194915771484, loss=1.1848114728927612
I0315 15:28:06.382969 139660528883456 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.8809422254562378, loss=1.1661922931671143
I0315 15:29:35.800785 139660520490752 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.602521300315857, loss=1.1848337650299072
I0315 15:31:03.607115 139660856563456 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.472080707550049, loss=1.1083226203918457
I0315 15:32:20.498635 139660848170752 logging_writer.py:48] [61900] global_step=61900, grad_norm=4.808457851409912, loss=1.1584458351135254
I0315 15:33:37.119993 139660856563456 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.279383420944214, loss=1.141845464706421
I0315 15:34:51.864094 139660848170752 logging_writer.py:48] [62100] global_step=62100, grad_norm=5.287083148956299, loss=1.1533198356628418
I0315 15:36:10.261907 139660856563456 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.450521945953369, loss=1.1813105344772339
I0315 15:37:33.212208 139660848170752 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.199427843093872, loss=1.117329478263855
I0315 15:38:58.081711 139660856563456 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.8430256843566895, loss=1.145706057548523
I0315 15:40:00.722123 139818008614720 spec.py:321] Evaluating on the training split.
I0315 15:40:54.195297 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 15:41:44.411039 139818008614720 spec.py:349] Evaluating on the test split.
I0315 15:42:11.325891 139818008614720 submission_runner.py:420] Time since start: 55266.86s, 	Step: 62476, 	{'train/ctc_loss': Array(0.21817155, dtype=float32), 'train/wer': 0.076703208016679, 'validation/ctc_loss': Array(0.42259833, dtype=float32), 'validation/wer': 0.12186103092385375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685315, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 50452.07918572426, 'total_duration': 55266.86240553856, 'accumulated_submission_time': 50452.07918572426, 'accumulated_eval_time': 4810.312841653824, 'accumulated_logging_time': 1.8586266040802002}
I0315 15:42:11.364931 139660856563456 logging_writer.py:48] [62476] accumulated_eval_time=4810.312842, accumulated_logging_time=1.858627, accumulated_submission_time=50452.079186, global_step=62476, preemption_count=0, score=50452.079186, test/ctc_loss=0.226853147149086, test/num_examples=2472, test/wer=0.072919, total_duration=55266.862406, train/ctc_loss=0.21817155182361603, train/wer=0.076703, validation/ctc_loss=0.42259833216667175, validation/num_examples=5348, validation/wer=0.121861
I0315 15:42:30.660917 139660848170752 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.876246690750122, loss=1.1854530572891235
I0315 15:43:46.304788 139660856563456 logging_writer.py:48] [62600] global_step=62600, grad_norm=4.103448390960693, loss=1.0881212949752808
I0315 15:45:02.897247 139660848170752 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.3193886280059814, loss=1.132460355758667
I0315 15:46:20.268951 139660856563456 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.6703168153762817, loss=1.1880359649658203
I0315 15:47:41.810869 139661511923456 logging_writer.py:48] [62900] global_step=62900, grad_norm=4.083205223083496, loss=1.138214349746704
I0315 15:48:59.205508 139661503530752 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.4488344192504883, loss=1.1298725605010986
I0315 15:50:15.360146 139661511923456 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.589838743209839, loss=1.2247763872146606
I0315 15:51:34.036519 139661503530752 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.7756996154785156, loss=1.149169921875
I0315 15:52:56.197012 139661511923456 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.825078010559082, loss=1.2104884386062622
I0315 15:54:22.997256 139661503530752 logging_writer.py:48] [63400] global_step=63400, grad_norm=4.106704235076904, loss=1.1662263870239258
I0315 15:55:47.209541 139661511923456 logging_writer.py:48] [63500] global_step=63500, grad_norm=4.103425025939941, loss=1.195571780204773
I0315 15:57:12.028614 139661503530752 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.719974994659424, loss=1.1921255588531494
I0315 15:58:38.796071 139661511923456 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.7445158958435059, loss=1.1699610948562622
I0315 16:00:04.438772 139661503530752 logging_writer.py:48] [63800] global_step=63800, grad_norm=4.436611652374268, loss=1.1448071002960205
I0315 16:01:31.014164 139661511923456 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9847214221954346, loss=1.151185393333435
I0315 16:02:46.059550 139661503530752 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.4654009342193604, loss=1.1374770402908325
I0315 16:04:03.212882 139661511923456 logging_writer.py:48] [64100] global_step=64100, grad_norm=4.798170566558838, loss=1.1611968278884888
I0315 16:05:20.988375 139661503530752 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.6756162643432617, loss=1.160527229309082
I0315 16:06:11.521924 139818008614720 spec.py:321] Evaluating on the training split.
I0315 16:07:04.824246 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 16:07:56.646130 139818008614720 spec.py:349] Evaluating on the test split.
I0315 16:08:22.237451 139818008614720 submission_runner.py:420] Time since start: 56837.77s, 	Step: 64265, 	{'train/ctc_loss': Array(0.22722098, dtype=float32), 'train/wer': 0.07949774086075487, 'validation/ctc_loss': Array(0.4225785, dtype=float32), 'validation/wer': 0.12182241231161359, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683159, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 51892.149191617966, 'total_duration': 56837.77284407616, 'accumulated_submission_time': 51892.149191617966, 'accumulated_eval_time': 4941.022246599197, 'accumulated_logging_time': 1.9132137298583984}
I0315 16:08:22.275928 139661511923456 logging_writer.py:48] [64265] accumulated_eval_time=4941.022247, accumulated_logging_time=1.913214, accumulated_submission_time=51892.149192, global_step=64265, preemption_count=0, score=51892.149192, test/ctc_loss=0.2268315851688385, test/num_examples=2472, test/wer=0.072919, total_duration=56837.772844, train/ctc_loss=0.22722098231315613, train/wer=0.079498, validation/ctc_loss=0.42257851362228394, validation/num_examples=5348, validation/wer=0.121822
I0315 16:08:49.440114 139661503530752 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.8110315799713135, loss=1.1782596111297607
I0315 16:10:04.257566 139661511923456 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8419876098632812, loss=1.136613130569458
I0315 16:11:19.590962 139661503530752 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.006840467453003, loss=1.1542913913726807
I0315 16:12:46.017291 139661511923456 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.665189743041992, loss=1.129516363143921
I0315 16:14:11.230664 139661503530752 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.5717692375183105, loss=1.1482850313186646
I0315 16:15:40.588577 139661511923456 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.4087555408477783, loss=1.1356332302093506
I0315 16:17:07.038904 139660528883456 logging_writer.py:48] [64900] global_step=64900, grad_norm=5.220799446105957, loss=1.1228270530700684
I0315 16:18:22.743008 139660520490752 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.813066005706787, loss=1.153328537940979
I0315 16:19:37.495429 139660528883456 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.4005496501922607, loss=1.124666690826416
I0315 16:20:55.515334 139660520490752 logging_writer.py:48] [65200] global_step=65200, grad_norm=4.29685115814209, loss=1.150881290435791
I0315 16:22:18.578361 139660528883456 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.5983569622039795, loss=1.1953091621398926
I0315 16:23:44.065395 139660520490752 logging_writer.py:48] [65400] global_step=65400, grad_norm=4.4284281730651855, loss=1.206558346748352
I0315 16:25:12.335267 139660528883456 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.5967836380004883, loss=1.1101025342941284
I0315 16:26:35.765904 139660520490752 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.5658538341522217, loss=1.146653413772583
I0315 16:28:00.058944 139660528883456 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.2606074810028076, loss=1.1210441589355469
I0315 16:29:27.210666 139660520490752 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.165148973464966, loss=1.0951533317565918
I0315 16:30:53.159021 139660528883456 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.8460686206817627, loss=1.1735528707504272
I0315 16:32:15.225717 139661511923456 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.0311038494110107, loss=1.2178800106048584
I0315 16:32:22.598481 139818008614720 spec.py:321] Evaluating on the training split.
I0315 16:33:16.174214 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 16:34:08.104947 139818008614720 spec.py:349] Evaluating on the test split.
I0315 16:34:34.319759 139818008614720 submission_runner.py:420] Time since start: 58409.86s, 	Step: 66011, 	{'train/ctc_loss': Array(0.22163488, dtype=float32), 'train/wer': 0.07671405615319377, 'validation/ctc_loss': Array(0.42258117, dtype=float32), 'validation/wer': 0.12181275765855354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683603, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 53332.38691663742, 'total_duration': 58409.85575699806, 'accumulated_submission_time': 53332.38691663742, 'accumulated_eval_time': 5072.738001346588, 'accumulated_logging_time': 1.9674947261810303}
I0315 16:34:34.363129 139661511923456 logging_writer.py:48] [66011] accumulated_eval_time=5072.738001, accumulated_logging_time=1.967495, accumulated_submission_time=53332.386917, global_step=66011, preemption_count=0, score=53332.386917, test/ctc_loss=0.22683602571487427, test/num_examples=2472, test/wer=0.072919, total_duration=58409.855757, train/ctc_loss=0.2216348797082901, train/wer=0.076714, validation/ctc_loss=0.42258116602897644, validation/num_examples=5348, validation/wer=0.121813
I0315 16:35:42.550870 139661503530752 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.8465545177459717, loss=1.176015853881836
I0315 16:36:58.353656 139661511923456 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.6723337173461914, loss=1.1586971282958984
I0315 16:38:13.454163 139661503530752 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.8018860816955566, loss=1.2024192810058594
I0315 16:39:29.160766 139661511923456 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.7665505409240723, loss=1.160715103149414
I0315 16:40:53.147630 139661503530752 logging_writer.py:48] [66500] global_step=66500, grad_norm=4.281502723693848, loss=1.1655546426773071
I0315 16:42:19.709724 139661511923456 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.084383010864258, loss=1.145849347114563
I0315 16:43:44.653671 139661503530752 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.066862106323242, loss=1.1414406299591064
I0315 16:45:11.028483 139661511923456 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.9154719114303589, loss=1.1506688594818115
I0315 16:46:34.185073 139661503530752 logging_writer.py:48] [66900] global_step=66900, grad_norm=6.426499366760254, loss=1.1605676412582397
I0315 16:47:56.271948 139660856563456 logging_writer.py:48] [67000] global_step=67000, grad_norm=11.372747421264648, loss=1.1661901473999023
I0315 16:49:12.181951 139660848170752 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.7666239738464355, loss=1.1667510271072388
I0315 16:50:28.091304 139660856563456 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.311077356338501, loss=1.1315175294876099
I0315 16:51:47.915564 139660848170752 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.399348020553589, loss=1.1420774459838867
I0315 16:53:10.311768 139660856563456 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.974199056625366, loss=1.192488193511963
I0315 16:54:39.286124 139660848170752 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.6888962984085083, loss=1.149469017982483
I0315 16:56:07.802659 139660856563456 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.8092010021209717, loss=1.1533969640731812
I0315 16:57:34.250294 139660848170752 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.6458616256713867, loss=1.1501405239105225
I0315 16:58:34.610148 139818008614720 spec.py:321] Evaluating on the training split.
I0315 16:59:27.538419 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 17:00:19.180951 139818008614720 spec.py:349] Evaluating on the test split.
I0315 17:00:45.558402 139818008614720 submission_runner.py:420] Time since start: 59981.09s, 	Step: 67774, 	{'train/ctc_loss': Array(0.23132047, dtype=float32), 'train/wer': 0.07875333785022517, 'validation/ctc_loss': Array(0.4225996, dtype=float32), 'validation/wer': 0.12186103092385375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684477, dtype=float32), 'test/wer': 0.07295919403652022, 'test/num_examples': 2472, 'score': 54772.54679131508, 'total_duration': 59981.093438625336, 'accumulated_submission_time': 54772.54679131508, 'accumulated_eval_time': 5203.679777383804, 'accumulated_logging_time': 2.0264124870300293}
I0315 17:00:45.598150 139660564723456 logging_writer.py:48] [67774] accumulated_eval_time=5203.679777, accumulated_logging_time=2.026412, accumulated_submission_time=54772.546791, global_step=67774, preemption_count=0, score=54772.546791, test/ctc_loss=0.22684477269649506, test/num_examples=2472, test/wer=0.072959, total_duration=59981.093439, train/ctc_loss=0.23132047057151794, train/wer=0.078753, validation/ctc_loss=0.4225996136665344, validation/num_examples=5348, validation/wer=0.121861
I0315 17:01:06.871285 139660556330752 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.226414680480957, loss=1.1470955610275269
I0315 17:02:22.509384 139660564723456 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.0609467029571533, loss=1.1437808275222778
I0315 17:03:42.190820 139661511923456 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.5651967525482178, loss=1.1352200508117676
I0315 17:04:57.918810 139661503530752 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.005504608154297, loss=1.1821085214614868
I0315 17:06:15.109515 139661511923456 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.368335723876953, loss=1.194066047668457
I0315 17:07:31.168400 139661503530752 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.717104911804199, loss=1.1614184379577637
I0315 17:08:55.654304 139661511923456 logging_writer.py:48] [68400] global_step=68400, grad_norm=4.51535701751709, loss=1.152036428451538
I0315 17:10:22.917131 139661503530752 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9546171426773071, loss=1.1669753789901733
I0315 17:11:46.430288 139661511923456 logging_writer.py:48] [68600] global_step=68600, grad_norm=4.707743167877197, loss=1.2180187702178955
I0315 17:13:13.690542 139661503530752 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.913691759109497, loss=1.147449254989624
I0315 17:14:39.466265 139661511923456 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.0565900802612305, loss=1.1505024433135986
I0315 17:16:05.905209 139661503530752 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.66250741481781, loss=1.1176310777664185
I0315 17:17:32.563224 139661511923456 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8064641952514648, loss=1.1262067556381226
I0315 17:18:51.688483 139660564723456 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3810721635818481, loss=1.145002007484436
I0315 17:20:09.581799 139660556330752 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.0831167697906494, loss=1.1491636037826538
I0315 17:21:25.855947 139660564723456 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.82537841796875, loss=1.1518677473068237
I0315 17:22:49.521305 139660556330752 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.602094888687134, loss=1.1307696104049683
I0315 17:24:15.076612 139660564723456 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.6085723638534546, loss=1.1559667587280273
I0315 17:24:45.576083 139818008614720 spec.py:321] Evaluating on the training split.
I0315 17:25:40.027417 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 17:26:31.552243 139818008614720 spec.py:349] Evaluating on the test split.
I0315 17:26:57.677683 139818008614720 submission_runner.py:420] Time since start: 61553.21s, 	Step: 69539, 	{'train/ctc_loss': Array(0.2290587, dtype=float32), 'train/wer': 0.07869904519129715, 'validation/ctc_loss': Array(0.4225713, dtype=float32), 'validation/wer': 0.12182241231161359, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268291, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 56212.43574166298, 'total_duration': 61553.21344566345, 'accumulated_submission_time': 56212.43574166298, 'accumulated_eval_time': 5335.775634050369, 'accumulated_logging_time': 2.084322690963745}
I0315 17:26:57.717345 139660564723456 logging_writer.py:48] [69539] accumulated_eval_time=5335.775634, accumulated_logging_time=2.084323, accumulated_submission_time=56212.435742, global_step=69539, preemption_count=0, score=56212.435742, test/ctc_loss=0.22682909667491913, test/num_examples=2472, test/wer=0.072919, total_duration=61553.213446, train/ctc_loss=0.22905869781970978, train/wer=0.078699, validation/ctc_loss=0.4225713014602661, validation/num_examples=5348, validation/wer=0.121822
I0315 17:27:44.156030 139660556330752 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.6639552116394043, loss=1.1582067012786865
I0315 17:28:59.542188 139660564723456 logging_writer.py:48] [69700] global_step=69700, grad_norm=5.493202209472656, loss=1.1573026180267334
I0315 17:30:14.981318 139660556330752 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.160484790802002, loss=1.1260225772857666
I0315 17:31:43.068911 139660564723456 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.542687177658081, loss=1.0772452354431152
I0315 17:33:06.560929 139660556330752 logging_writer.py:48] [70000] global_step=70000, grad_norm=4.098805904388428, loss=1.2006717920303345
I0315 17:34:31.389703 139660564723456 logging_writer.py:48] [70100] global_step=70100, grad_norm=5.261757850646973, loss=1.1954699754714966
I0315 17:35:48.207895 139660556330752 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.976138114929199, loss=1.1773173809051514
I0315 17:37:05.020093 139660564723456 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.6764421463012695, loss=1.1747305393218994
I0315 17:38:21.217927 139660556330752 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.6226160526275635, loss=1.1699659824371338
I0315 17:39:40.187903 139660564723456 logging_writer.py:48] [70500] global_step=70500, grad_norm=5.735535144805908, loss=1.1192516088485718
I0315 17:41:05.564196 139660556330752 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.057950973510742, loss=1.1572425365447998
I0315 17:42:30.072605 139660564723456 logging_writer.py:48] [70700] global_step=70700, grad_norm=5.537991523742676, loss=1.1873928308486938
I0315 17:43:55.804630 139660556330752 logging_writer.py:48] [70800] global_step=70800, grad_norm=11.455912590026855, loss=1.1472018957138062
I0315 17:45:19.072939 139660564723456 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.66696035861969, loss=1.1930556297302246
I0315 17:46:42.217187 139660556330752 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.314824342727661, loss=1.1761549711227417
I0315 17:48:09.247329 139660564723456 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.6451183557510376, loss=1.1223188638687134
I0315 17:49:25.617266 139660556330752 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.728336215019226, loss=1.125110149383545
I0315 17:50:40.589202 139660564723456 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.295098304748535, loss=1.1666640043258667
I0315 17:50:57.992359 139818008614720 spec.py:321] Evaluating on the training split.
I0315 17:51:52.056285 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 17:52:42.329072 139818008614720 spec.py:349] Evaluating on the test split.
I0315 17:53:09.191111 139818008614720 submission_runner.py:420] Time since start: 63124.73s, 	Step: 71324, 	{'train/ctc_loss': Array(0.2350017, dtype=float32), 'train/wer': 0.0823583454169111, 'validation/ctc_loss': Array(0.4226408, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685763, dtype=float32), 'test/wer': 0.07295919403652022, 'test/num_examples': 2472, 'score': 57652.624497413635, 'total_duration': 63124.72708106041, 'accumulated_submission_time': 57652.624497413635, 'accumulated_eval_time': 5466.968840122223, 'accumulated_logging_time': 2.1383402347564697}
I0315 17:53:09.230385 139661511923456 logging_writer.py:48] [71324] accumulated_eval_time=5466.968840, accumulated_logging_time=2.138340, accumulated_submission_time=57652.624497, global_step=71324, preemption_count=0, score=57652.624497, test/ctc_loss=0.22685763239860535, test/num_examples=2472, test/wer=0.072959, total_duration=63124.727081, train/ctc_loss=0.23500169813632965, train/wer=0.082358, validation/ctc_loss=0.4226408004760742, validation/num_examples=5348, validation/wer=0.121851
I0315 17:54:08.060468 139661503530752 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.6574457883834839, loss=1.1285489797592163
I0315 17:55:23.457905 139661511923456 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.7853693962097168, loss=1.0989736318588257
I0315 17:56:38.533987 139661503530752 logging_writer.py:48] [71600] global_step=71600, grad_norm=4.718800067901611, loss=1.1192972660064697
I0315 17:57:56.831575 139661511923456 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.6418933868408203, loss=1.1396421194076538
I0315 17:59:25.376157 139661503530752 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.5182108879089355, loss=1.163535237312317
I0315 18:00:53.307353 139661511923456 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.201375961303711, loss=1.1401879787445068
I0315 18:02:20.116160 139661503530752 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.25838041305542, loss=1.1510391235351562
I0315 18:03:48.500918 139661511923456 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.5818092823028564, loss=1.1265040636062622
I0315 18:05:03.896178 139661503530752 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.6235923767089844, loss=1.1893166303634644
I0315 18:06:19.960230 139661511923456 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.5871214866638184, loss=1.1580631732940674
I0315 18:07:37.644429 139661503530752 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.3904407024383545, loss=1.169415831565857
I0315 18:08:55.748659 139661511923456 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0068440437316895, loss=1.141681432723999
I0315 18:10:18.915348 139661503530752 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.2341575622558594, loss=1.1717298030853271
I0315 18:11:45.744740 139661511923456 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.501568078994751, loss=1.13577401638031
I0315 18:13:11.761680 139661503530752 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.9440908432006836, loss=1.1404540538787842
I0315 18:14:40.608072 139661511923456 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.1369054317474365, loss=1.1521409749984741
I0315 18:16:08.520701 139661503530752 logging_writer.py:48] [73000] global_step=73000, grad_norm=7.444215774536133, loss=1.1248419284820557
I0315 18:17:09.847203 139818008614720 spec.py:321] Evaluating on the training split.
I0315 18:18:04.141651 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 18:18:55.272056 139818008614720 spec.py:349] Evaluating on the test split.
I0315 18:19:21.453675 139818008614720 submission_runner.py:420] Time since start: 64696.99s, 	Step: 73071, 	{'train/ctc_loss': Array(0.20977716, dtype=float32), 'train/wer': 0.07325595201076479, 'validation/ctc_loss': Array(0.42251086, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22680901, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 59093.15683174133, 'total_duration': 64696.9893181324, 'accumulated_submission_time': 59093.15683174133, 'accumulated_eval_time': 5598.56943821907, 'accumulated_logging_time': 2.190993070602417}
I0315 18:19:21.493396 139661511923456 logging_writer.py:48] [73071] accumulated_eval_time=5598.569438, accumulated_logging_time=2.190993, accumulated_submission_time=59093.156832, global_step=73071, preemption_count=0, score=59093.156832, test/ctc_loss=0.22680900990962982, test/num_examples=2472, test/wer=0.072919, total_duration=64696.989318, train/ctc_loss=0.20977716147899628, train/wer=0.073256, validation/ctc_loss=0.42251086235046387, validation/num_examples=5348, validation/wer=0.121832
I0315 18:19:44.050165 139661503530752 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.9500675201416016, loss=1.0996946096420288
I0315 18:21:05.341026 139661511923456 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.530142068862915, loss=1.14033842086792
I0315 18:22:20.131986 139661503530752 logging_writer.py:48] [73300] global_step=73300, grad_norm=4.73205041885376, loss=1.1674907207489014
I0315 18:23:39.581350 139661511923456 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.9608619213104248, loss=1.188478946685791
I0315 18:24:59.695452 139661503530752 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.2526543140411377, loss=1.0982279777526855
I0315 18:26:23.484722 139661511923456 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.2163801193237305, loss=1.1636370420455933
I0315 18:27:53.502395 139661503530752 logging_writer.py:48] [73700] global_step=73700, grad_norm=5.506081581115723, loss=1.1373980045318604
I0315 18:29:18.848457 139661511923456 logging_writer.py:48] [73800] global_step=73800, grad_norm=8.05004596710205, loss=1.2150487899780273
I0315 18:30:46.426164 139661503530752 logging_writer.py:48] [73900] global_step=73900, grad_norm=7.012409210205078, loss=1.18594229221344
I0315 18:32:13.159755 139661511923456 logging_writer.py:48] [74000] global_step=74000, grad_norm=4.020606994628906, loss=1.1565896272659302
I0315 18:33:37.182948 139661503530752 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.8750494718551636, loss=1.1589369773864746
I0315 18:35:02.053261 139661511923456 logging_writer.py:48] [74200] global_step=74200, grad_norm=4.094371795654297, loss=1.1387183666229248
I0315 18:36:18.425554 139661503530752 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.654643535614014, loss=1.1169910430908203
I0315 18:37:33.326495 139661511923456 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.1694931983947754, loss=1.1668106317520142
I0315 18:38:55.216566 139661503530752 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.591977119445801, loss=1.1681691408157349
I0315 18:40:20.194214 139661511923456 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.541693925857544, loss=1.1588627099990845
I0315 18:41:48.555938 139661503530752 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.796318769454956, loss=1.1293280124664307
I0315 18:43:12.900871 139661511923456 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.634777545928955, loss=1.1362255811691284
I0315 18:43:22.159549 139818008614720 spec.py:321] Evaluating on the training split.
I0315 18:44:15.593950 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 18:45:06.838757 139818008614720 spec.py:349] Evaluating on the test split.
I0315 18:45:34.212373 139818008614720 submission_runner.py:420] Time since start: 66269.75s, 	Step: 74813, 	{'train/ctc_loss': Array(0.20217852, dtype=float32), 'train/wer': 0.069316295956455, 'validation/ctc_loss': Array(0.42254487, dtype=float32), 'validation/wer': 0.12181275765855354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268203, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 60533.7385866642, 'total_duration': 66269.74801373482, 'accumulated_submission_time': 60533.7385866642, 'accumulated_eval_time': 5730.616383552551, 'accumulated_logging_time': 2.2462613582611084}
I0315 18:45:34.252770 139661220083456 logging_writer.py:48] [74813] accumulated_eval_time=5730.616384, accumulated_logging_time=2.246261, accumulated_submission_time=60533.738587, global_step=74813, preemption_count=0, score=60533.738587, test/ctc_loss=0.22682030498981476, test/num_examples=2472, test/wer=0.072939, total_duration=66269.748014, train/ctc_loss=0.20217852294445038, train/wer=0.069316, validation/ctc_loss=0.4225448668003082, validation/num_examples=5348, validation/wer=0.121813
I0315 18:46:40.105676 139661211690752 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.6610870361328125, loss=1.0942813158035278
I0315 18:47:55.077076 139661220083456 logging_writer.py:48] [75000] global_step=75000, grad_norm=5.432763576507568, loss=1.1578586101531982
I0315 18:49:13.063307 139661211690752 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.4584596157073975, loss=1.1681948900222778
I0315 18:50:35.695307 139661220083456 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.609372138977051, loss=1.156694769859314
I0315 18:51:52.648827 139661211690752 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1761202812194824, loss=1.1415413618087769
I0315 18:53:09.002099 139661220083456 logging_writer.py:48] [75400] global_step=75400, grad_norm=8.456075668334961, loss=1.1339155435562134
I0315 18:54:24.029766 139661211690752 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.6607751846313477, loss=1.1512750387191772
I0315 18:55:45.197921 139661220083456 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.8060708045959473, loss=1.168074369430542
I0315 18:57:12.315089 139661211690752 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.5443737506866455, loss=1.1715017557144165
I0315 18:58:40.464756 139661220083456 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.4238059520721436, loss=1.1062504053115845
I0315 19:00:01.956358 139661211690752 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.836281180381775, loss=1.1098257303237915
I0315 19:01:26.322778 139661220083456 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.341102123260498, loss=1.1757265329360962
I0315 19:02:53.469339 139661211690752 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.2902488708496094, loss=1.0964891910552979
I0315 19:04:19.169898 139661220083456 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.575507879257202, loss=1.1953784227371216
I0315 19:05:40.443841 139661220083456 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.8998045921325684, loss=1.168953537940979
I0315 19:06:56.503838 139661211690752 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.3137776851654053, loss=1.175851583480835
I0315 19:08:12.919608 139661220083456 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.032580852508545, loss=1.220485806465149
I0315 19:09:32.098632 139661211690752 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.7135653495788574, loss=1.204864263534546
I0315 19:09:34.949797 139818008614720 spec.py:321] Evaluating on the training split.
I0315 19:10:28.803240 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 19:11:18.676051 139818008614720 spec.py:349] Evaluating on the test split.
I0315 19:11:45.580801 139818008614720 submission_runner.py:420] Time since start: 67841.12s, 	Step: 76605, 	{'train/ctc_loss': Array(0.24120624, dtype=float32), 'train/wer': 0.084248732741005, 'validation/ctc_loss': Array(0.422617, dtype=float32), 'validation/wer': 0.12188034022997384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685282, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 61974.348662376404, 'total_duration': 67841.11692786217, 'accumulated_submission_time': 61974.348662376404, 'accumulated_eval_time': 5861.242010354996, 'accumulated_logging_time': 2.3004186153411865}
I0315 19:11:45.619798 139661511923456 logging_writer.py:48] [76605] accumulated_eval_time=5861.242010, accumulated_logging_time=2.300419, accumulated_submission_time=61974.348662, global_step=76605, preemption_count=0, score=61974.348662, test/ctc_loss=0.22685281932353973, test/num_examples=2472, test/wer=0.072980, total_duration=67841.116928, train/ctc_loss=0.24120624363422394, train/wer=0.084249, validation/ctc_loss=0.42261698842048645, validation/num_examples=5348, validation/wer=0.121880
I0315 19:12:58.124796 139661503530752 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.412670850753784, loss=1.156067967414856
I0315 19:14:13.723356 139661511923456 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.048560619354248, loss=1.2463557720184326
I0315 19:15:36.251323 139661503530752 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.213751792907715, loss=1.1759053468704224
I0315 19:17:05.013242 139661511923456 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.5410571098327637, loss=1.1218721866607666
I0315 19:18:29.583209 139661503530752 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.469499111175537, loss=1.1300805807113647
I0315 19:19:57.292292 139661511923456 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.612330198287964, loss=1.1887102127075195
I0315 19:21:23.582655 139660564723456 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.4907050132751465, loss=1.1764748096466064
I0315 19:22:41.260939 139660556330752 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.06101393699646, loss=1.2131211757659912
I0315 19:23:57.602167 139660564723456 logging_writer.py:48] [77500] global_step=77500, grad_norm=6.387131690979004, loss=1.1282578706741333
I0315 19:25:16.855578 139660556330752 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.0383238792419434, loss=1.0957369804382324
I0315 19:26:37.896272 139660564723456 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.920198917388916, loss=1.1221445798873901
I0315 19:28:02.027416 139660556330752 logging_writer.py:48] [77800] global_step=77800, grad_norm=4.319937229156494, loss=1.169277310371399
I0315 19:29:31.174273 139660564723456 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.1306140422821045, loss=1.2005950212478638
I0315 19:30:59.322620 139660556330752 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.445624828338623, loss=1.1257545948028564
I0315 19:32:22.792208 139660564723456 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.9050965309143066, loss=1.164589762687683
I0315 19:33:51.739338 139660556330752 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.1832122802734375, loss=1.1283055543899536
I0315 19:35:17.778363 139660564723456 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1330084800720215, loss=1.1701666116714478
I0315 19:35:45.619390 139818008614720 spec.py:321] Evaluating on the training split.
I0315 19:36:38.009239 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 19:37:28.185868 139818008614720 spec.py:349] Evaluating on the test split.
I0315 19:37:53.794770 139818008614720 submission_runner.py:420] Time since start: 69409.33s, 	Step: 78338, 	{'train/ctc_loss': Array(0.23317532, dtype=float32), 'train/wer': 0.07823094425483504, 'validation/ctc_loss': Array(0.42259285, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684146, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 63414.18882107735, 'total_duration': 69409.33088636398, 'accumulated_submission_time': 63414.18882107735, 'accumulated_eval_time': 5989.411997795105, 'accumulated_logging_time': 2.428640842437744}
I0315 19:37:53.835657 139660564723456 logging_writer.py:48] [78338] accumulated_eval_time=5989.411998, accumulated_logging_time=2.428641, accumulated_submission_time=63414.188821, global_step=78338, preemption_count=0, score=63414.188821, test/ctc_loss=0.22684146463871002, test/num_examples=2472, test/wer=0.072919, total_duration=69409.330886, train/ctc_loss=0.23317532241344452, train/wer=0.078231, validation/ctc_loss=0.4225928485393524, validation/num_examples=5348, validation/wer=0.121851
I0315 19:38:41.089126 139660556330752 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.8614909648895264, loss=1.1843698024749756
I0315 19:39:58.191722 139660564723456 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.7852870225906372, loss=1.1651811599731445
I0315 19:41:15.059138 139660556330752 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.437071323394775, loss=1.127448320388794
I0315 19:42:31.171070 139660564723456 logging_writer.py:48] [78700] global_step=78700, grad_norm=4.064172267913818, loss=1.0785573720932007
I0315 19:43:50.125631 139660556330752 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.465562105178833, loss=1.1515341997146606
I0315 19:45:13.525503 139660564723456 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.786102533340454, loss=1.13944673538208
I0315 19:46:36.887333 139660556330752 logging_writer.py:48] [79000] global_step=79000, grad_norm=6.319571495056152, loss=1.1901391744613647
I0315 19:48:04.010613 139660564723456 logging_writer.py:48] [79100] global_step=79100, grad_norm=4.26855993270874, loss=1.1873987913131714
I0315 19:49:30.795325 139660556330752 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.7419815063476562, loss=1.187110424041748
I0315 19:50:56.069817 139660564723456 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.5051188468933105, loss=1.1478019952774048
I0315 19:52:16.735949 139661511923456 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.0060982704162598, loss=1.1759486198425293
I0315 19:53:31.712800 139661503530752 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.6652817726135254, loss=1.1185038089752197
I0315 19:54:46.721523 139661511923456 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.1981492042541504, loss=1.1789658069610596
I0315 19:56:04.064532 139661503530752 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.320505380630493, loss=1.1584932804107666
I0315 19:57:31.563113 139661511923456 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.3059165477752686, loss=1.175881028175354
I0315 19:58:57.544730 139661503530752 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.583966016769409, loss=1.148651361465454
I0315 20:00:20.854797 139661511923456 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.378601551055908, loss=1.2092010974884033
I0315 20:01:45.711236 139661503530752 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.8281058073043823, loss=1.1313670873641968
I0315 20:01:54.350885 139818008614720 spec.py:321] Evaluating on the training split.
I0315 20:02:49.196527 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 20:03:39.336568 139818008614720 spec.py:349] Evaluating on the test split.
I0315 20:04:06.464191 139818008614720 submission_runner.py:420] Time since start: 70982.00s, 	Step: 80112, 	{'train/ctc_loss': Array(0.23077081, dtype=float32), 'train/wer': 0.07769953433405433, 'validation/ctc_loss': Array(0.42259255, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684368, dtype=float32), 'test/wer': 0.07295919403652022, 'test/num_examples': 2472, 'score': 64854.61954379082, 'total_duration': 70982.00092959404, 'accumulated_submission_time': 64854.61954379082, 'accumulated_eval_time': 6121.52056145668, 'accumulated_logging_time': 2.482978343963623}
I0315 20:04:06.509062 139661511923456 logging_writer.py:48] [80112] accumulated_eval_time=6121.520561, accumulated_logging_time=2.482978, accumulated_submission_time=64854.619544, global_step=80112, preemption_count=0, score=64854.619544, test/ctc_loss=0.2268436849117279, test/num_examples=2472, test/wer=0.072959, total_duration=70982.000930, train/ctc_loss=0.23077081143856049, train/wer=0.077700, validation/ctc_loss=0.42259255051612854, validation/num_examples=5348, validation/wer=0.121832
I0315 20:05:13.940705 139661503530752 logging_writer.py:48] [80200] global_step=80200, grad_norm=5.7066874504089355, loss=1.1508584022521973
I0315 20:06:30.036437 139661511923456 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.1046621799468994, loss=1.1600823402404785
I0315 20:07:50.592669 139661511923456 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.0399365425109863, loss=1.1876858472824097
I0315 20:09:08.468372 139661503530752 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.1833648681640625, loss=1.1677464246749878
I0315 20:10:25.880921 139661511923456 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.7265195846557617, loss=1.1112449169158936
I0315 20:11:45.740655 139661503530752 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.8895816802978516, loss=1.1437721252441406
I0315 20:13:07.271666 139661511923456 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.709419012069702, loss=1.1738885641098022
I0315 20:14:32.807382 139661503530752 logging_writer.py:48] [80900] global_step=80900, grad_norm=5.282439231872559, loss=1.1762484312057495
I0315 20:16:00.713785 139661511923456 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.329435110092163, loss=1.1259757280349731
I0315 20:17:24.810167 139661503530752 logging_writer.py:48] [81100] global_step=81100, grad_norm=5.716370105743408, loss=1.136939287185669
I0315 20:18:46.932448 139661511923456 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.5942695140838623, loss=1.1186187267303467
I0315 20:20:14.203091 139661503530752 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.972780466079712, loss=1.1987011432647705
I0315 20:21:40.336542 139660564723456 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.5629055500030518, loss=1.13776695728302
I0315 20:22:55.846980 139660556330752 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.406431198120117, loss=1.1626907587051392
I0315 20:24:12.176994 139660564723456 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.212599039077759, loss=1.0661888122558594
I0315 20:25:28.544924 139660556330752 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.783064365386963, loss=1.1633557081222534
I0315 20:26:50.341431 139660564723456 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.9898991584777832, loss=1.1163707971572876
I0315 20:28:06.901999 139818008614720 spec.py:321] Evaluating on the training split.
I0315 20:29:03.641678 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 20:29:55.295517 139818008614720 spec.py:349] Evaluating on the test split.
I0315 20:30:21.147460 139818008614720 submission_runner.py:420] Time since start: 72556.68s, 	Step: 81892, 	{'train/ctc_loss': Array(0.21406859, dtype=float32), 'train/wer': 0.07513247957606535, 'validation/ctc_loss': Array(0.42261186, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684759, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 66294.92736124992, 'total_duration': 72556.6824965477, 'accumulated_submission_time': 66294.92736124992, 'accumulated_eval_time': 6255.759551286697, 'accumulated_logging_time': 2.5418665409088135}
I0315 20:30:21.190872 139660564723456 logging_writer.py:48] [81892] accumulated_eval_time=6255.759551, accumulated_logging_time=2.541867, accumulated_submission_time=66294.927361, global_step=81892, preemption_count=0, score=66294.927361, test/ctc_loss=0.2268475890159607, test/num_examples=2472, test/wer=0.072919, total_duration=72556.682497, train/ctc_loss=0.21406859159469604, train/wer=0.075132, validation/ctc_loss=0.42261186242103577, validation/num_examples=5348, validation/wer=0.121851
I0315 20:30:28.061370 139660556330752 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.288790464401245, loss=1.1121537685394287
I0315 20:31:43.791961 139660564723456 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1447811126708984, loss=1.1878679990768433
I0315 20:33:00.132884 139660556330752 logging_writer.py:48] [82100] global_step=82100, grad_norm=5.853924751281738, loss=1.1775739192962646
I0315 20:34:17.566827 139660564723456 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.372148275375366, loss=1.191091537475586
I0315 20:35:42.291593 139660556330752 logging_writer.py:48] [82300] global_step=82300, grad_norm=5.603241920471191, loss=1.160975694656372
I0315 20:37:14.053053 139661511923456 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.895042896270752, loss=1.1481715440750122
I0315 20:38:31.001107 139661503530752 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.3672971725463867, loss=1.1461395025253296
I0315 20:39:48.672762 139661511923456 logging_writer.py:48] [82600] global_step=82600, grad_norm=5.302699565887451, loss=1.145390272140503
I0315 20:41:05.558002 139661503530752 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.708763837814331, loss=1.2013177871704102
I0315 20:42:26.638937 139661511923456 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.1674954891204834, loss=1.074609637260437
I0315 20:43:52.732117 139661503530752 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.47421932220459, loss=1.1154667139053345
I0315 20:45:18.424514 139661511923456 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.9888103008270264, loss=1.1313388347625732
I0315 20:46:47.037714 139661503530752 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.3863003253936768, loss=1.2008613348007202
I0315 20:48:14.323102 139661511923456 logging_writer.py:48] [83200] global_step=83200, grad_norm=6.6004319190979, loss=1.1646405458450317
I0315 20:49:39.507197 139661503530752 logging_writer.py:48] [83300] global_step=83300, grad_norm=6.7296600341796875, loss=1.1057358980178833
I0315 20:51:05.367534 139661511923456 logging_writer.py:48] [83400] global_step=83400, grad_norm=4.026650428771973, loss=1.1087441444396973
I0315 20:52:28.228378 139661511923456 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.3423571586608887, loss=1.1284615993499756
I0315 20:53:44.167788 139661503530752 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.1675758361816406, loss=1.2183542251586914
I0315 20:54:21.418968 139818008614720 spec.py:321] Evaluating on the training split.
I0315 20:55:13.406672 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 20:56:04.999405 139818008614720 spec.py:349] Evaluating on the test split.
I0315 20:56:31.342146 139818008614720 submission_runner.py:420] Time since start: 74126.88s, 	Step: 83650, 	{'train/ctc_loss': Array(0.21813707, dtype=float32), 'train/wer': 0.07587590126046788, 'validation/ctc_loss': Array(0.4225675, dtype=float32), 'validation/wer': 0.12186103092385375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683038, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 67735.07097840309, 'total_duration': 74126.87830376625, 'accumulated_submission_time': 67735.07097840309, 'accumulated_eval_time': 6385.677375793457, 'accumulated_logging_time': 2.598761558532715}
I0315 20:56:31.383336 139661511923456 logging_writer.py:48] [83650] accumulated_eval_time=6385.677376, accumulated_logging_time=2.598762, accumulated_submission_time=67735.070978, global_step=83650, preemption_count=0, score=67735.070978, test/ctc_loss=0.2268303781747818, test/num_examples=2472, test/wer=0.072919, total_duration=74126.878304, train/ctc_loss=0.21813707053661346, train/wer=0.075876, validation/ctc_loss=0.4225674867630005, validation/num_examples=5348, validation/wer=0.121861
I0315 20:57:10.823476 139661503530752 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.738954544067383, loss=1.2091492414474487
I0315 20:58:25.958643 139661511923456 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.1003851890563965, loss=1.119128942489624
I0315 20:59:40.985392 139661503530752 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.690599203109741, loss=1.1777515411376953
I0315 21:00:57.753475 139661511923456 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.941216230392456, loss=1.1545372009277344
I0315 21:02:23.557958 139661503530752 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1624386310577393, loss=1.153334379196167
I0315 21:03:50.963861 139661511923456 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.3307056427001953, loss=1.0894975662231445
I0315 21:05:18.345223 139661503530752 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.0993399620056152, loss=1.1347030401229858
I0315 21:06:42.195890 139661511923456 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.046950578689575, loss=1.083189606666565
I0315 21:08:04.745896 139661511923456 logging_writer.py:48] [84500] global_step=84500, grad_norm=5.650385856628418, loss=1.1529256105422974
I0315 21:09:20.595908 139661503530752 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.7153946161270142, loss=1.1132789850234985
I0315 21:10:37.730081 139661511923456 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.581007480621338, loss=1.1379343271255493
I0315 21:11:53.448268 139661503530752 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.6948347091674805, loss=1.1471402645111084
I0315 21:13:11.507927 139661511923456 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.4485461711883545, loss=1.19094979763031
I0315 21:14:34.922727 139661503530752 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.8422964811325073, loss=1.1873782873153687
I0315 21:15:57.908703 139661511923456 logging_writer.py:48] [85100] global_step=85100, grad_norm=5.191224575042725, loss=1.1660711765289307
I0315 21:17:25.466908 139661503530752 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.8899245262145996, loss=1.200429081916809
I0315 21:18:49.553930 139661511923456 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.8553810119628906, loss=1.1952778100967407
I0315 21:20:16.427344 139661503530752 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.5647841691970825, loss=1.1376347541809082
I0315 21:20:31.956529 139818008614720 spec.py:321] Evaluating on the training split.
I0315 21:21:27.358360 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 21:22:18.950753 139818008614720 spec.py:349] Evaluating on the test split.
I0315 21:22:44.692271 139818008614720 submission_runner.py:420] Time since start: 75700.23s, 	Step: 85420, 	{'train/ctc_loss': Array(0.21527421, dtype=float32), 'train/wer': 0.07514021366415868, 'validation/ctc_loss': Array(0.42256075, dtype=float32), 'validation/wer': 0.12182241231161359, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683091, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 69175.55707406998, 'total_duration': 75700.22792291641, 'accumulated_submission_time': 69175.55707406998, 'accumulated_eval_time': 6518.407257318497, 'accumulated_logging_time': 2.6557695865631104}
I0315 21:22:44.740135 139661511923456 logging_writer.py:48] [85420] accumulated_eval_time=6518.407257, accumulated_logging_time=2.655770, accumulated_submission_time=69175.557074, global_step=85420, preemption_count=0, score=69175.557074, test/ctc_loss=0.22683091461658478, test/num_examples=2472, test/wer=0.072919, total_duration=75700.227923, train/ctc_loss=0.21527421474456787, train/wer=0.075140, validation/ctc_loss=0.42256075143814087, validation/num_examples=5348, validation/wer=0.121822
I0315 21:23:49.212923 139660856563456 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.1174492835998535, loss=1.1442490816116333
I0315 21:25:04.985415 139660848170752 logging_writer.py:48] [85600] global_step=85600, grad_norm=7.670774936676025, loss=1.1955199241638184
I0315 21:26:21.903892 139660856563456 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.6466550827026367, loss=1.1387590169906616
I0315 21:27:37.424564 139660848170752 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.6315487623214722, loss=1.1442755460739136
I0315 21:28:54.128833 139660856563456 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.6324515342712402, loss=1.1963354349136353
I0315 21:30:12.475186 139660848170752 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.0703139305114746, loss=1.1450306177139282
I0315 21:31:33.730865 139660856563456 logging_writer.py:48] [86100] global_step=86100, grad_norm=5.169814586639404, loss=1.1859495639801025
I0315 21:33:00.045429 139660848170752 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.1029486656188965, loss=1.1888604164123535
I0315 21:34:26.805630 139660856563456 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.1457648277282715, loss=1.14484703540802
I0315 21:35:52.201886 139660848170752 logging_writer.py:48] [86400] global_step=86400, grad_norm=4.024158954620361, loss=1.1923480033874512
I0315 21:37:20.204435 139660856563456 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.8775972127914429, loss=1.1328264474868774
I0315 21:38:40.043841 139660856563456 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.919313430786133, loss=1.1146997213363647
I0315 21:39:56.352785 139660848170752 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.691840171813965, loss=1.1384772062301636
I0315 21:41:11.750672 139660856563456 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.694368839263916, loss=1.1636403799057007
I0315 21:42:26.618457 139660848170752 logging_writer.py:48] [86900] global_step=86900, grad_norm=5.47859001159668, loss=1.1672320365905762
I0315 21:43:41.630681 139660856563456 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.5306812524795532, loss=1.1267822980880737
I0315 21:45:05.887430 139660848170752 logging_writer.py:48] [87100] global_step=87100, grad_norm=5.606624603271484, loss=1.1142367124557495
I0315 21:46:32.632891 139660856563456 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.3144540786743164, loss=1.1114217042922974
I0315 21:46:45.110338 139818008614720 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0315 21:47:58.033679 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 21:48:49.740888 139818008614720 spec.py:349] Evaluating on the test split.
I0315 21:49:15.987856 139818008614720 submission_runner.py:420] Time since start: 77291.52s, 	Step: 87217, 	{'train/ctc_loss': Array(0.136696, dtype=float32), 'train/wer': 0.04877996939296038, 'validation/ctc_loss': Array(0.4225517, dtype=float32), 'validation/wer': 0.1218031030054935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682695, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 70615.84012532234, 'total_duration': 77291.52449822426, 'accumulated_submission_time': 70615.84012532234, 'accumulated_eval_time': 6669.279905557632, 'accumulated_logging_time': 2.7173826694488525}
I0315 21:49:16.030620 139660564723456 logging_writer.py:48] [87217] accumulated_eval_time=6669.279906, accumulated_logging_time=2.717383, accumulated_submission_time=70615.840125, global_step=87217, preemption_count=0, score=70615.840125, test/ctc_loss=0.22682695090770721, test/num_examples=2472, test/wer=0.072919, total_duration=77291.524498, train/ctc_loss=0.136695995926857, train/wer=0.048780, validation/ctc_loss=0.422551691532135, validation/num_examples=5348, validation/wer=0.121803
I0315 21:50:19.741564 139660556330752 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2241642475128174, loss=1.1800947189331055
I0315 21:51:35.987738 139660564723456 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.283998966217041, loss=1.1600018739700317
I0315 21:52:51.825141 139660556330752 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.7354230880737305, loss=1.1165481805801392
I0315 21:54:11.005525 139660564723456 logging_writer.py:48] [87600] global_step=87600, grad_norm=6.942433834075928, loss=1.1404109001159668
I0315 21:55:27.078922 139660556330752 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.8956563472747803, loss=1.1975873708724976
I0315 21:56:44.777899 139660564723456 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.682227849960327, loss=1.1804282665252686
I0315 21:58:00.648525 139660556330752 logging_writer.py:48] [87900] global_step=87900, grad_norm=3.916569232940674, loss=1.158915400505066
I0315 21:59:20.022767 139660564723456 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.7294013500213623, loss=1.1281603574752808
I0315 22:00:42.079811 139660556330752 logging_writer.py:48] [88100] global_step=88100, grad_norm=6.258891582489014, loss=1.225788950920105
I0315 22:02:06.326204 139660564723456 logging_writer.py:48] [88200] global_step=88200, grad_norm=5.353451251983643, loss=1.1481733322143555
I0315 22:03:34.934744 139660556330752 logging_writer.py:48] [88300] global_step=88300, grad_norm=4.074913501739502, loss=1.1389127969741821
I0315 22:05:03.622640 139660564723456 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.783238649368286, loss=1.1672508716583252
I0315 22:06:26.704626 139660556330752 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.9503072500228882, loss=1.1402568817138672
I0315 22:07:53.893472 139660564723456 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.3671904802322388, loss=1.1303472518920898
I0315 22:09:10.469054 139660556330752 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.491029977798462, loss=1.2194772958755493
I0315 22:10:28.376258 139660564723456 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.946408748626709, loss=1.173930048942566
I0315 22:11:45.105892 139660556330752 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.8268967866897583, loss=1.1681129932403564
I0315 22:13:05.820296 139660564723456 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.769413471221924, loss=1.2145847082138062
I0315 22:13:16.213108 139818008614720 spec.py:321] Evaluating on the training split.
I0315 22:14:10.930290 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 22:15:03.049563 139818008614720 spec.py:349] Evaluating on the test split.
I0315 22:15:29.378207 139818008614720 submission_runner.py:420] Time since start: 78864.91s, 	Step: 89013, 	{'train/ctc_loss': Array(0.13775001, dtype=float32), 'train/wer': 0.047716020842316346, 'validation/ctc_loss': Array(0.42260024, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683926, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 72055.93694233894, 'total_duration': 78864.91400146484, 'accumulated_submission_time': 72055.93694233894, 'accumulated_eval_time': 6802.439275741577, 'accumulated_logging_time': 2.773588180541992}
I0315 22:15:29.426428 139660626155264 logging_writer.py:48] [89013] accumulated_eval_time=6802.439276, accumulated_logging_time=2.773588, accumulated_submission_time=72055.936942, global_step=89013, preemption_count=0, score=72055.936942, test/ctc_loss=0.22683925926685333, test/num_examples=2472, test/wer=0.072939, total_duration=78864.914001, train/ctc_loss=0.13775001466274261, train/wer=0.047716, validation/ctc_loss=0.42260023951530457, validation/num_examples=5348, validation/wer=0.121871
I0315 22:16:36.443547 139660617762560 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.3072116374969482, loss=1.1455504894256592
I0315 22:17:51.893738 139660626155264 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.734752655029297, loss=1.1733324527740479
I0315 22:19:09.391412 139660617762560 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.987607479095459, loss=1.0899858474731445
I0315 22:20:37.447040 139660626155264 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.5229363441467285, loss=1.123937964439392
I0315 22:22:05.513782 139660617762560 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.822338581085205, loss=1.2190229892730713
I0315 22:23:33.950146 139660626155264 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.36584210395813, loss=1.1853883266448975
I0315 22:24:54.531170 139660626155264 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.527469277381897, loss=1.0897663831710815
I0315 22:26:11.058378 139660617762560 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.58685827255249, loss=1.1286053657531738
I0315 22:27:28.445861 139660626155264 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.9307267665863037, loss=1.1631642580032349
I0315 22:28:48.480987 139660617762560 logging_writer.py:48] [90000] global_step=90000, grad_norm=10.968027114868164, loss=1.1943011283874512
I0315 22:30:13.574085 139660626155264 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.453900098800659, loss=1.1709575653076172
I0315 22:31:41.171186 139660617762560 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.1332123279571533, loss=1.1299148797988892
I0315 22:33:11.301114 139660626155264 logging_writer.py:48] [90300] global_step=90300, grad_norm=5.4151530265808105, loss=1.1654562950134277
I0315 22:34:34.382000 139660617762560 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.7120072841644287, loss=1.173166275024414
I0315 22:35:59.285935 139660626155264 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.6864174604415894, loss=1.1216942071914673
I0315 22:37:26.758842 139660617762560 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.194220542907715, loss=1.2059619426727295
I0315 22:38:51.825435 139660626155264 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.285769939422607, loss=1.1148736476898193
I0315 22:39:29.939925 139818008614720 spec.py:321] Evaluating on the training split.
I0315 22:40:25.212820 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 22:41:15.405057 139818008614720 spec.py:349] Evaluating on the test split.
I0315 22:41:42.367390 139818008614720 submission_runner.py:420] Time since start: 80437.90s, 	Step: 90750, 	{'train/ctc_loss': Array(0.15148643, dtype=float32), 'train/wer': 0.05301937154787389, 'validation/ctc_loss': Array(0.42255753, dtype=float32), 'validation/wer': 0.12184172161773367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682476, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 73496.3667254448, 'total_duration': 80437.90277504921, 'accumulated_submission_time': 73496.3667254448, 'accumulated_eval_time': 6934.860629796982, 'accumulated_logging_time': 2.8354685306549072}
I0315 22:41:42.413773 139661511923456 logging_writer.py:48] [90750] accumulated_eval_time=6934.860630, accumulated_logging_time=2.835469, accumulated_submission_time=73496.366725, global_step=90750, preemption_count=0, score=73496.366725, test/ctc_loss=0.22682476043701172, test/num_examples=2472, test/wer=0.072939, total_duration=80437.902775, train/ctc_loss=0.15148642659187317, train/wer=0.053019, validation/ctc_loss=0.422557532787323, validation/num_examples=5348, validation/wer=0.121842
I0315 22:42:20.956344 139661503530752 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.401742935180664, loss=1.131441354751587
I0315 22:43:36.623389 139661511923456 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.550227165222168, loss=1.1164665222167969
I0315 22:44:52.314963 139661503530752 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.616848111152649, loss=1.1550785303115845
I0315 22:46:07.462388 139661511923456 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.3077468872070312, loss=1.1091194152832031
I0315 22:47:35.353158 139661503530752 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.663695812225342, loss=1.159907579421997
I0315 22:49:00.062124 139661511923456 logging_writer.py:48] [91300] global_step=91300, grad_norm=6.573205947875977, loss=1.1244093179702759
I0315 22:50:28.062306 139661503530752 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.473428249359131, loss=1.158035397529602
I0315 22:51:55.406246 139661511923456 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.3732672929763794, loss=1.1370329856872559
I0315 22:53:20.179207 139661503530752 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.2666726112365723, loss=1.1284865140914917
I0315 22:54:48.976651 139660564723456 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.265862464904785, loss=1.1724430322647095
I0315 22:56:03.813374 139660556330752 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.229928731918335, loss=1.142342448234558
I0315 22:57:19.635924 139660564723456 logging_writer.py:48] [91900] global_step=91900, grad_norm=3.3261654376983643, loss=1.1761289834976196
I0315 22:58:39.171129 139660556330752 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.3381597995758057, loss=1.1351287364959717
I0315 22:59:59.283033 139660564723456 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.377793073654175, loss=1.1753116846084595
I0315 23:01:24.377718 139660556330752 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.4269940853118896, loss=1.1838229894638062
I0315 23:02:53.461885 139660564723456 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.766503095626831, loss=1.1733477115631104
I0315 23:04:20.711924 139660556330752 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.1428945064544678, loss=1.135054111480713
I0315 23:05:42.933393 139818008614720 spec.py:321] Evaluating on the training split.
I0315 23:06:38.328395 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 23:07:28.588792 139818008614720 spec.py:349] Evaluating on the test split.
I0315 23:07:56.068014 139818008614720 submission_runner.py:420] Time since start: 82011.60s, 	Step: 92494, 	{'train/ctc_loss': Array(0.13619804, dtype=float32), 'train/wer': 0.047990232907588276, 'validation/ctc_loss': Array(0.4226031, dtype=float32), 'validation/wer': 0.12188999488303388, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684576, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 74936.79920172691, 'total_duration': 82011.60424017906, 'accumulated_submission_time': 74936.79920172691, 'accumulated_eval_time': 7067.9899690151215, 'accumulated_logging_time': 2.8983287811279297}
I0315 23:07:56.108101 139660564723456 logging_writer.py:48] [92494] accumulated_eval_time=7067.989969, accumulated_logging_time=2.898329, accumulated_submission_time=74936.799202, global_step=92494, preemption_count=0, score=74936.799202, test/ctc_loss=0.22684575617313385, test/num_examples=2472, test/wer=0.072980, total_duration=82011.604240, train/ctc_loss=0.1361980438232422, train/wer=0.047990, validation/ctc_loss=0.4226031005382538, validation/num_examples=5348, validation/wer=0.121890
I0315 23:08:01.460285 139660556330752 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.0865161418914795, loss=1.2124238014221191
I0315 23:09:16.400530 139660564723456 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3972115516662598, loss=1.207385540008545
I0315 23:10:34.480206 139660564723456 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.7624876499176025, loss=1.1215490102767944
I0315 23:11:51.387273 139660556330752 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.056555986404419, loss=1.1031891107559204
I0315 23:13:06.397613 139660564723456 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.96629524230957, loss=1.1278403997421265
I0315 23:14:23.238565 139660556330752 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.1404216289520264, loss=1.088360071182251
I0315 23:15:44.257254 139660564723456 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.6754443645477295, loss=1.1936346292495728
I0315 23:17:08.263628 139660556330752 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.9963879585266113, loss=1.1077064275741577
I0315 23:18:38.383388 139660564723456 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.365603446960449, loss=1.117721676826477
I0315 23:20:02.469263 139660556330752 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.126858949661255, loss=1.1478408575057983
I0315 23:21:31.331959 139660564723456 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.807149887084961, loss=1.1462948322296143
I0315 23:22:56.532778 139660556330752 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.0651750564575195, loss=1.1452122926712036
I0315 23:24:22.306597 139660564723456 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.499154567718506, loss=1.1689687967300415
I0315 23:25:42.577301 139660564723456 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.631150245666504, loss=1.173013687133789
I0315 23:26:59.655564 139660556330752 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.142408847808838, loss=1.1883705854415894
I0315 23:28:16.378992 139660564723456 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.600313663482666, loss=1.1320427656173706
I0315 23:29:35.487530 139660556330752 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.7743680477142334, loss=1.1861308813095093
I0315 23:30:58.587238 139660564723456 logging_writer.py:48] [94200] global_step=94200, grad_norm=3.4058444499969482, loss=1.1651674509048462
I0315 23:31:56.472679 139818008614720 spec.py:321] Evaluating on the training split.
I0315 23:32:51.581546 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 23:33:42.343214 139818008614720 spec.py:349] Evaluating on the test split.
I0315 23:34:08.497189 139818008614720 submission_runner.py:420] Time since start: 83584.03s, 	Step: 94269, 	{'train/ctc_loss': Array(0.14904647, dtype=float32), 'train/wer': 0.05232685223195483, 'validation/ctc_loss': Array(0.4226395, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22686507, dtype=float32), 'test/wer': 0.07299981719578331, 'test/num_examples': 2472, 'score': 76377.0752260685, 'total_duration': 83584.03257799149, 'accumulated_submission_time': 76377.0752260685, 'accumulated_eval_time': 7200.00839805603, 'accumulated_logging_time': 2.9541354179382324}
I0315 23:34:08.542239 139660856563456 logging_writer.py:48] [94269] accumulated_eval_time=7200.008398, accumulated_logging_time=2.954135, accumulated_submission_time=76377.075226, global_step=94269, preemption_count=0, score=76377.075226, test/ctc_loss=0.22686506807804108, test/num_examples=2472, test/wer=0.073000, total_duration=83584.032578, train/ctc_loss=0.14904646575450897, train/wer=0.052327, validation/ctc_loss=0.42263948917388916, validation/num_examples=5348, validation/wer=0.121851
I0315 23:34:33.796675 139660848170752 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.6274254322052, loss=1.1201775074005127
I0315 23:35:49.548269 139660856563456 logging_writer.py:48] [94400] global_step=94400, grad_norm=3.0099987983703613, loss=1.1928235292434692
I0315 23:37:05.963675 139660848170752 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.175758123397827, loss=1.1141687631607056
I0315 23:38:23.554639 139660856563456 logging_writer.py:48] [94600] global_step=94600, grad_norm=5.546672344207764, loss=1.143945336341858
I0315 23:39:49.681072 139660848170752 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.9091522693634033, loss=1.1657702922821045
I0315 23:41:16.861424 139661511923456 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.1080472469329834, loss=1.1292355060577393
I0315 23:42:33.856719 139661503530752 logging_writer.py:48] [94900] global_step=94900, grad_norm=3.0326924324035645, loss=1.1425350904464722
I0315 23:43:51.192505 139661511923456 logging_writer.py:48] [95000] global_step=95000, grad_norm=6.958645343780518, loss=1.1841827630996704
I0315 23:45:08.425673 139661503530752 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.903489112854004, loss=1.1471946239471436
I0315 23:46:27.632228 139661511923456 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.2879719734191895, loss=1.1673173904418945
I0315 23:47:54.087842 139661503530752 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.5179505348205566, loss=1.180087685585022
I0315 23:49:21.971374 139661511923456 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.9535748958587646, loss=1.1432901620864868
I0315 23:50:47.978447 139661503530752 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.729429244995117, loss=1.1970813274383545
I0315 23:52:12.944197 139661511923456 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.8775227069854736, loss=1.1564136743545532
I0315 23:53:38.781525 139661503530752 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.751206874847412, loss=1.155503273010254
I0315 23:55:09.645313 139660528883456 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.878835678100586, loss=1.1458547115325928
I0315 23:56:26.227159 139660520490752 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.478128910064697, loss=1.1177499294281006
I0315 23:57:42.226252 139660528883456 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.888895034790039, loss=1.1346591711044312
I0315 23:58:09.225642 139818008614720 spec.py:321] Evaluating on the training split.
I0315 23:59:05.372194 139818008614720 spec.py:333] Evaluating on the validation split.
I0315 23:59:57.552441 139818008614720 spec.py:349] Evaluating on the test split.
I0316 00:00:25.153229 139818008614720 submission_runner.py:420] Time since start: 85160.69s, 	Step: 96036, 	{'train/ctc_loss': Array(0.13960445, dtype=float32), 'train/wer': 0.04994817656298418, 'validation/ctc_loss': Array(0.42258808, dtype=float32), 'validation/wer': 0.12184172161773367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268396, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 77817.67358899117, 'total_duration': 85160.68888759613, 'accumulated_submission_time': 77817.67358899117, 'accumulated_eval_time': 7335.9301290512085, 'accumulated_logging_time': 3.0127499103546143}
I0316 00:00:25.194723 139660528883456 logging_writer.py:48] [96036] accumulated_eval_time=7335.930129, accumulated_logging_time=3.012750, accumulated_submission_time=77817.673589, global_step=96036, preemption_count=0, score=77817.673589, test/ctc_loss=0.2268396019935608, test/num_examples=2472, test/wer=0.072919, total_duration=85160.688888, train/ctc_loss=0.13960444927215576, train/wer=0.049948, validation/ctc_loss=0.4225880801677704, validation/num_examples=5348, validation/wer=0.121842
I0316 00:01:13.801158 139660520490752 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.6200506687164307, loss=1.1242426633834839
I0316 00:02:28.819743 139660528883456 logging_writer.py:48] [96200] global_step=96200, grad_norm=7.8105998039245605, loss=1.1430416107177734
I0316 00:03:44.481672 139660520490752 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.8910739421844482, loss=1.1389914751052856
I0316 00:05:07.102077 139660528883456 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.6962895393371582, loss=1.147868275642395
I0316 00:06:36.518048 139660520490752 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.525405406951904, loss=1.1498082876205444
I0316 00:08:02.394564 139660528883456 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.9743268489837646, loss=1.1359163522720337
I0316 00:09:32.980843 139660520490752 logging_writer.py:48] [96700] global_step=96700, grad_norm=6.352264881134033, loss=1.2036480903625488
I0316 00:11:04.090045 139660528883456 logging_writer.py:48] [96800] global_step=96800, grad_norm=3.4257445335388184, loss=1.105739951133728
I0316 00:12:23.445030 139660856563456 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.881772041320801, loss=1.2181196212768555
I0316 00:13:40.323607 139660848170752 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.453263759613037, loss=1.157993197441101
I0316 00:14:55.645274 139660856563456 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.796280860900879, loss=1.1817612648010254
I0316 00:16:17.425288 139660848170752 logging_writer.py:48] [97200] global_step=97200, grad_norm=3.4546308517456055, loss=1.178323745727539
I0316 00:17:41.115817 139660856563456 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.149533748626709, loss=1.1537091732025146
I0316 00:19:11.040439 139660848170752 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.611258864402771, loss=1.1340891122817993
I0316 00:20:36.009128 139660856563456 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.6052544116973877, loss=1.2005717754364014
I0316 00:22:05.905210 139660848170752 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.844048023223877, loss=1.1698112487792969
I0316 00:23:35.565252 139660856563456 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.745363235473633, loss=1.1097511053085327
I0316 00:24:25.194606 139818008614720 spec.py:321] Evaluating on the training split.
I0316 00:25:20.368790 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 00:26:10.816411 139818008614720 spec.py:349] Evaluating on the test split.
I0316 00:26:37.728997 139818008614720 submission_runner.py:420] Time since start: 86733.26s, 	Step: 97761, 	{'train/ctc_loss': Array(0.16996482, dtype=float32), 'train/wer': 0.05590072109193922, 'validation/ctc_loss': Array(0.4225012, dtype=float32), 'validation/wer': 0.12181275765855354, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22680737, dtype=float32), 'test/wer': 0.07289825929762557, 'test/num_examples': 2472, 'score': 79257.58918118477, 'total_duration': 86733.26485586166, 'accumulated_submission_time': 79257.58918118477, 'accumulated_eval_time': 7468.458901882172, 'accumulated_logging_time': 3.0681488513946533}
I0316 00:26:37.776629 139660923119360 logging_writer.py:48] [97761] accumulated_eval_time=7468.458902, accumulated_logging_time=3.068149, accumulated_submission_time=79257.589181, global_step=97761, preemption_count=0, score=79257.589181, test/ctc_loss=0.2268073707818985, test/num_examples=2472, test/wer=0.072898, total_duration=86733.264856, train/ctc_loss=0.16996482014656067, train/wer=0.055901, validation/ctc_loss=0.42250120639801025, validation/num_examples=5348, validation/wer=0.121813
I0316 00:27:08.747323 139660914726656 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.3831069469451904, loss=1.1556944847106934
I0316 00:28:28.391859 139660595439360 logging_writer.py:48] [97900] global_step=97900, grad_norm=12.800312042236328, loss=1.1671043634414673
I0316 00:29:44.169147 139660587046656 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.445438861846924, loss=1.1649110317230225
I0316 00:31:03.923804 139660595439360 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.587794542312622, loss=1.1328213214874268
I0316 00:32:22.433032 139660587046656 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.0054593086242676, loss=1.1969295740127563
I0316 00:33:44.300589 139660595439360 logging_writer.py:48] [98300] global_step=98300, grad_norm=3.289102554321289, loss=1.2089402675628662
I0316 00:35:07.365816 139660587046656 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.429170846939087, loss=1.1824331283569336
I0316 00:36:33.997057 139660595439360 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.7291200160980225, loss=1.1330617666244507
I0316 00:38:01.737879 139660587046656 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.2493038177490234, loss=1.1540884971618652
I0316 00:39:30.544897 139660595439360 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.668024063110352, loss=1.1574169397354126
I0316 00:41:00.274394 139660587046656 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.643974542617798, loss=1.1751625537872314
I0316 00:42:26.846982 139660595439360 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.1330368518829346, loss=1.1554416418075562
I0316 00:43:43.418244 139660587046656 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.770507335662842, loss=1.061500906944275
I0316 00:45:00.193510 139660595439360 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.493866920471191, loss=1.1872189044952393
I0316 00:46:20.206900 139660587046656 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.578545331954956, loss=1.1489512920379639
I0316 00:47:41.218554 139660595439360 logging_writer.py:48] [99300] global_step=99300, grad_norm=3.2036871910095215, loss=1.1161952018737793
I0316 00:49:05.951807 139660587046656 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.4192557334899902, loss=1.160057544708252
I0316 00:50:33.585518 139660595439360 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.1088197231292725, loss=1.1450920104980469
I0316 00:50:38.321650 139818008614720 spec.py:321] Evaluating on the training split.
I0316 00:51:36.904212 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 00:52:30.054004 139818008614720 spec.py:349] Evaluating on the test split.
I0316 00:52:56.648854 139818008614720 submission_runner.py:420] Time since start: 88312.19s, 	Step: 99506, 	{'train/ctc_loss': Array(0.15189572, dtype=float32), 'train/wer': 0.053917514843087364, 'validation/ctc_loss': Array(0.42260715, dtype=float32), 'validation/wer': 0.12186103092385375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684963, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 80698.04615569115, 'total_duration': 88312.18506860733, 'accumulated_submission_time': 80698.04615569115, 'accumulated_eval_time': 7606.780797481537, 'accumulated_logging_time': 3.13254976272583}
I0316 00:52:56.693190 139660595439360 logging_writer.py:48] [99506] accumulated_eval_time=7606.780797, accumulated_logging_time=3.132550, accumulated_submission_time=80698.046156, global_step=99506, preemption_count=0, score=80698.046156, test/ctc_loss=0.22684963047504425, test/num_examples=2472, test/wer=0.072939, total_duration=88312.185069, train/ctc_loss=0.15189571678638458, train/wer=0.053918, validation/ctc_loss=0.4226071536540985, validation/num_examples=5348, validation/wer=0.121861
I0316 00:54:08.005690 139660587046656 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.695577621459961, loss=1.2066233158111572
I0316 00:55:24.628662 139660595439360 logging_writer.py:48] [99700] global_step=99700, grad_norm=5.394626140594482, loss=1.169264793395996
I0316 00:56:45.934792 139660587046656 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.6409857273101807, loss=1.0853333473205566
I0316 00:58:09.333740 139660595439360 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.3653182983398438, loss=1.1324330568313599
I0316 00:59:30.461322 139660595439360 logging_writer.py:48] [100000] global_step=100000, grad_norm=5.141921520233154, loss=1.1606310606002808
I0316 01:00:48.777304 139660587046656 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.7894365787506104, loss=1.1490877866744995
I0316 01:02:04.834713 139660595439360 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.787041664123535, loss=1.1387848854064941
I0316 01:03:23.501883 139660587046656 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.221288204193115, loss=1.1530669927597046
I0316 01:04:49.670231 139660595439360 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.0926389694213867, loss=1.148780345916748
I0316 01:06:17.632023 139660587046656 logging_writer.py:48] [100500] global_step=100500, grad_norm=5.694562911987305, loss=1.161205768585205
I0316 01:07:45.011907 139660595439360 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.6613199710845947, loss=1.1732815504074097
I0316 01:09:12.315565 139660587046656 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.5394957065582275, loss=1.148837924003601
I0316 01:10:38.563730 139660595439360 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.8282333612442017, loss=1.1478949785232544
I0316 01:12:04.825037 139660587046656 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.6263864040374756, loss=1.137241244316101
I0316 01:13:28.692674 139660595439360 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.8355354070663452, loss=1.1278241872787476
I0316 01:14:46.046698 139660587046656 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.1315436363220215, loss=1.1263753175735474
I0316 01:16:02.448604 139660595439360 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.573394298553467, loss=1.1874089241027832
I0316 01:16:57.576558 139818008614720 spec.py:321] Evaluating on the training split.
I0316 01:17:54.139708 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 01:18:46.386006 139818008614720 spec.py:349] Evaluating on the test split.
I0316 01:19:13.892180 139818008614720 submission_runner.py:420] Time since start: 89889.43s, 	Step: 101269, 	{'train/ctc_loss': Array(0.13218991, dtype=float32), 'train/wer': 0.04816817882834442, 'validation/ctc_loss': Array(0.42258465, dtype=float32), 'validation/wer': 0.12184172161773367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683719, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 82138.8430378437, 'total_duration': 89889.42877030373, 'accumulated_submission_time': 82138.8430378437, 'accumulated_eval_time': 7743.091492176056, 'accumulated_logging_time': 3.1927475929260254}
I0316 01:19:13.940099 139661511923456 logging_writer.py:48] [101269] accumulated_eval_time=7743.091492, accumulated_logging_time=3.192748, accumulated_submission_time=82138.843038, global_step=101269, preemption_count=0, score=82138.843038, test/ctc_loss=0.2268371880054474, test/num_examples=2472, test/wer=0.072939, total_duration=89889.428770, train/ctc_loss=0.13218991458415985, train/wer=0.048168, validation/ctc_loss=0.4225846529006958, validation/num_examples=5348, validation/wer=0.121842
I0316 01:19:37.925000 139661503530752 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.8295679092407227, loss=1.1697945594787598
I0316 01:20:54.261260 139661511923456 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.182126045227051, loss=1.222800850868225
I0316 01:22:10.881411 139661503530752 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.420797824859619, loss=1.2197588682174683
I0316 01:23:33.607217 139661511923456 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.4422266483306885, loss=1.1525260210037231
I0316 01:25:01.288588 139661503530752 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.312131881713867, loss=1.155669093132019
I0316 01:26:28.004040 139661511923456 logging_writer.py:48] [101800] global_step=101800, grad_norm=3.1800696849823, loss=1.1429306268692017
I0316 01:27:54.433315 139661503530752 logging_writer.py:48] [101900] global_step=101900, grad_norm=6.455933570861816, loss=1.1522698402404785
I0316 01:29:20.407114 139661511923456 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.048654794692993, loss=1.1680927276611328
I0316 01:30:36.260642 139661503530752 logging_writer.py:48] [102100] global_step=102100, grad_norm=5.212044715881348, loss=1.1854913234710693
I0316 01:31:54.954127 139661511923456 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.62323260307312, loss=1.1148009300231934
I0316 01:33:15.703302 139661503530752 logging_writer.py:48] [102300] global_step=102300, grad_norm=3.66462779045105, loss=1.1803876161575317
I0316 01:34:33.091881 139661511923456 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.2453701496124268, loss=1.138968586921692
I0316 01:36:01.323865 139661503530752 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.3853566646575928, loss=1.2087687253952026
I0316 01:37:29.582918 139661511923456 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.573629856109619, loss=1.1295359134674072
I0316 01:38:54.643007 139661503530752 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.621255874633789, loss=1.1618859767913818
I0316 01:40:22.606954 139661511923456 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.566676616668701, loss=1.110155463218689
I0316 01:41:47.664566 139661503530752 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.3639543056488037, loss=1.1717779636383057
I0316 01:43:14.617470 139818008614720 spec.py:321] Evaluating on the training split.
I0316 01:44:08.777671 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 01:44:59.308938 139818008614720 spec.py:349] Evaluating on the test split.
I0316 01:45:25.966479 139818008614720 submission_runner.py:420] Time since start: 91461.50s, 	Step: 103000, 	{'train/ctc_loss': Array(0.13685067, dtype=float32), 'train/wer': 0.0485069051633973, 'validation/ctc_loss': Array(0.42259264, dtype=float32), 'validation/wer': 0.12186103092385375, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684465, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 83579.43496155739, 'total_duration': 91461.50216126442, 'accumulated_submission_time': 83579.43496155739, 'accumulated_eval_time': 7874.434673547745, 'accumulated_logging_time': 3.255798816680908}
I0316 01:45:26.015382 139661511923456 logging_writer.py:48] [103000] accumulated_eval_time=7874.434674, accumulated_logging_time=3.255799, accumulated_submission_time=83579.434962, global_step=103000, preemption_count=0, score=83579.434962, test/ctc_loss=0.2268446534872055, test/num_examples=2472, test/wer=0.072939, total_duration=91461.502161, train/ctc_loss=0.13685066998004913, train/wer=0.048507, validation/ctc_loss=0.4225926399230957, validation/num_examples=5348, validation/wer=0.121861
I0316 01:45:30.777222 139661511923456 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.3260414600372314, loss=1.1778688430786133
I0316 01:46:46.698077 139661503530752 logging_writer.py:48] [103100] global_step=103100, grad_norm=5.091689586639404, loss=1.1621657609939575
I0316 01:48:01.562392 139661511923456 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.320662021636963, loss=1.1060913801193237
I0316 01:49:17.824980 139661503530752 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.982411861419678, loss=1.1579885482788086
I0316 01:50:38.970052 139661511923456 logging_writer.py:48] [103400] global_step=103400, grad_norm=9.376363754272461, loss=1.1651662588119507
I0316 01:52:03.663333 139661503530752 logging_writer.py:48] [103500] global_step=103500, grad_norm=3.1155643463134766, loss=1.1141399145126343
I0316 01:53:25.698252 139661511923456 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.230494976043701, loss=1.1936736106872559
I0316 01:54:50.867233 139661503530752 logging_writer.py:48] [103700] global_step=103700, grad_norm=3.234128713607788, loss=1.156046986579895
I0316 01:56:18.701903 139661511923456 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.992043137550354, loss=1.1011546850204468
I0316 01:57:46.113394 139661503530752 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.9896974563598633, loss=1.1434489488601685
I0316 01:59:14.510587 139661511923456 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.0653676986694336, loss=1.1796698570251465
I0316 02:00:36.576704 139661511923456 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.285897731781006, loss=1.1316399574279785
I0316 02:01:52.429821 139661503530752 logging_writer.py:48] [104200] global_step=104200, grad_norm=3.2967982292175293, loss=1.1451678276062012
I0316 02:03:11.921327 139661511923456 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.7329654693603516, loss=1.1564992666244507
I0316 02:04:33.698002 139661503530752 logging_writer.py:48] [104400] global_step=104400, grad_norm=3.8656787872314453, loss=1.1672711372375488
I0316 02:05:58.995116 139661511923456 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.579437732696533, loss=1.1723542213439941
I0316 02:07:19.986872 139661503530752 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.823293924331665, loss=1.2426241636276245
I0316 02:08:45.306909 139661511923456 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.710849404335022, loss=1.1362618207931519
I0316 02:09:26.500155 139818008614720 spec.py:321] Evaluating on the training split.
I0316 02:10:21.354322 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 02:11:13.318256 139818008614720 spec.py:349] Evaluating on the test split.
I0316 02:11:39.344917 139818008614720 submission_runner.py:420] Time since start: 93034.88s, 	Step: 104750, 	{'train/ctc_loss': Array(0.13595581, dtype=float32), 'train/wer': 0.04978996122361051, 'validation/ctc_loss': Array(0.42256644, dtype=float32), 'validation/wer': 0.1218031030054935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683273, dtype=float32), 'test/wer': 0.07289825929762557, 'test/num_examples': 2472, 'score': 85019.83351564407, 'total_duration': 93034.88031411171, 'accumulated_submission_time': 85019.83351564407, 'accumulated_eval_time': 8007.273331642151, 'accumulated_logging_time': 3.319591999053955}
I0316 02:11:39.392926 139661511923456 logging_writer.py:48] [104750] accumulated_eval_time=8007.273332, accumulated_logging_time=3.319592, accumulated_submission_time=85019.833516, global_step=104750, preemption_count=0, score=85019.833516, test/ctc_loss=0.22683273255825043, test/num_examples=2472, test/wer=0.072898, total_duration=93034.880314, train/ctc_loss=0.135955810546875, train/wer=0.049790, validation/ctc_loss=0.4225664436817169, validation/num_examples=5348, validation/wer=0.121803
I0316 02:12:17.693612 139661503530752 logging_writer.py:48] [104800] global_step=104800, grad_norm=3.7595107555389404, loss=1.1425219774246216
I0316 02:13:32.575531 139661511923456 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.9237194061279297, loss=1.1282376050949097
I0316 02:14:50.264189 139661503530752 logging_writer.py:48] [105000] global_step=105000, grad_norm=3.0810389518737793, loss=1.1774253845214844
I0316 02:16:15.552300 139661511923456 logging_writer.py:48] [105100] global_step=105100, grad_norm=5.089209079742432, loss=1.125592827796936
I0316 02:17:33.149200 139661503530752 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.284864902496338, loss=1.148867130279541
I0316 02:18:52.109156 139661511923456 logging_writer.py:48] [105300] global_step=105300, grad_norm=6.056952953338623, loss=1.1615773439407349
I0316 02:20:12.601448 139661503530752 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.8978912830352783, loss=1.1357723474502563
I0316 02:21:36.116960 139661511923456 logging_writer.py:48] [105500] global_step=105500, grad_norm=5.223747730255127, loss=1.1137057542800903
I0316 02:23:00.951736 139661503530752 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.245539903640747, loss=1.1618893146514893
I0316 02:24:23.263428 139661511923456 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.6501970291137695, loss=1.1382935047149658
I0316 02:25:52.226270 139661503530752 logging_writer.py:48] [105800] global_step=105800, grad_norm=5.287374019622803, loss=1.1673561334609985
I0316 02:27:21.159599 139661511923456 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.122710943222046, loss=1.130798578262329
I0316 02:28:48.119521 139661503530752 logging_writer.py:48] [106000] global_step=106000, grad_norm=3.5407631397247314, loss=1.0512573719024658
I0316 02:30:17.438722 139661511923456 logging_writer.py:48] [106100] global_step=106100, grad_norm=3.520116090774536, loss=1.1441892385482788
I0316 02:31:33.983969 139661503530752 logging_writer.py:48] [106200] global_step=106200, grad_norm=5.242588520050049, loss=1.1266816854476929
I0316 02:32:50.722490 139661511923456 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.6191076040267944, loss=1.1377209424972534
I0316 02:34:09.237045 139661503530752 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.930772066116333, loss=1.1638834476470947
I0316 02:35:29.340486 139661511923456 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.9483580589294434, loss=1.1666545867919922
I0316 02:35:39.368177 139818008614720 spec.py:321] Evaluating on the training split.
I0316 02:36:34.580577 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 02:37:27.865998 139818008614720 spec.py:349] Evaluating on the test split.
I0316 02:37:54.653179 139818008614720 submission_runner.py:420] Time since start: 94610.19s, 	Step: 106513, 	{'train/ctc_loss': Array(0.15283024, dtype=float32), 'train/wer': 0.052183996907615, 'validation/ctc_loss': Array(0.42258832, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684817, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 86459.7220981121, 'total_duration': 94610.18890833855, 'accumulated_submission_time': 86459.7220981121, 'accumulated_eval_time': 8142.5525443553925, 'accumulated_logging_time': 3.381679058074951}
I0316 02:37:54.701698 139661511923456 logging_writer.py:48] [106513] accumulated_eval_time=8142.552544, accumulated_logging_time=3.381679, accumulated_submission_time=86459.722098, global_step=106513, preemption_count=0, score=86459.722098, test/ctc_loss=0.22684817016124725, test/num_examples=2472, test/wer=0.072939, total_duration=94610.188908, train/ctc_loss=0.15283024311065674, train/wer=0.052184, validation/ctc_loss=0.4225883185863495, validation/num_examples=5348, validation/wer=0.121832
I0316 02:39:02.181924 139661503530752 logging_writer.py:48] [106600] global_step=106600, grad_norm=3.346158027648926, loss=1.1231412887573242
I0316 02:40:17.566109 139661511923456 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.9053738117218018, loss=1.157765507698059
I0316 02:41:33.502929 139661503530752 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.4133896827697754, loss=1.1821118593215942
I0316 02:42:59.214029 139661511923456 logging_writer.py:48] [106900] global_step=106900, grad_norm=8.643158912658691, loss=1.1843053102493286
I0316 02:44:25.561757 139661503530752 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.1176950931549072, loss=1.1957437992095947
I0316 02:45:53.418016 139661511923456 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.4558542966842651, loss=1.162214756011963
I0316 02:47:14.445477 139661511923456 logging_writer.py:48] [107200] global_step=107200, grad_norm=7.0272746086120605, loss=1.1171514987945557
I0316 02:48:30.959731 139661503530752 logging_writer.py:48] [107300] global_step=107300, grad_norm=5.673848628997803, loss=1.1459296941757202
I0316 02:49:51.695425 139661511923456 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.924833297729492, loss=1.138177514076233
I0316 02:51:10.933749 139661503530752 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4805277585983276, loss=1.1194933652877808
I0316 02:52:33.778224 139661511923456 logging_writer.py:48] [107600] global_step=107600, grad_norm=5.482319355010986, loss=1.197365641593933
I0316 02:54:00.554574 139661503530752 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.689115285873413, loss=1.1610842943191528
I0316 02:55:25.693612 139661511923456 logging_writer.py:48] [107800] global_step=107800, grad_norm=3.7421908378601074, loss=1.134819507598877
I0316 02:56:48.666139 139661503530752 logging_writer.py:48] [107900] global_step=107900, grad_norm=5.375263214111328, loss=1.1441102027893066
I0316 02:58:15.294280 139661511923456 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.2381696701049805, loss=1.1530786752700806
I0316 02:59:44.441645 139661503530752 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.437193393707275, loss=1.143074631690979
I0316 03:01:07.550993 139661511923456 logging_writer.py:48] [108200] global_step=108200, grad_norm=6.18285608291626, loss=1.149005651473999
I0316 03:01:54.686444 139818008614720 spec.py:321] Evaluating on the training split.
I0316 03:02:50.031709 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 03:03:40.925097 139818008614720 spec.py:349] Evaluating on the test split.
I0316 03:04:06.881299 139818008614720 submission_runner.py:420] Time since start: 96182.42s, 	Step: 108264, 	{'train/ctc_loss': Array(0.152285, dtype=float32), 'train/wer': 0.053365922672466586, 'validation/ctc_loss': Array(0.42263636, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268602, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 87899.6213388443, 'total_duration': 96182.41661047935, 'accumulated_submission_time': 87899.6213388443, 'accumulated_eval_time': 8274.74119591713, 'accumulated_logging_time': 3.4438462257385254}
I0316 03:04:06.928701 139661511923456 logging_writer.py:48] [108264] accumulated_eval_time=8274.741196, accumulated_logging_time=3.443846, accumulated_submission_time=87899.621339, global_step=108264, preemption_count=0, score=87899.621339, test/ctc_loss=0.2268601953983307, test/num_examples=2472, test/wer=0.072980, total_duration=96182.416610, train/ctc_loss=0.15228499472141266, train/wer=0.053366, validation/ctc_loss=0.42263635993003845, validation/num_examples=5348, validation/wer=0.121851
I0316 03:04:34.581491 139661503530752 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.111881971359253, loss=1.1481584310531616
I0316 03:05:50.261627 139661511923456 logging_writer.py:48] [108400] global_step=108400, grad_norm=3.686946392059326, loss=1.1525720357894897
I0316 03:07:05.174710 139661503530752 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.9167850017547607, loss=1.138235330581665
I0316 03:08:20.471908 139661511923456 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.3338451385498047, loss=1.1459810733795166
I0316 03:09:45.502649 139661503530752 logging_writer.py:48] [108700] global_step=108700, grad_norm=3.305880069732666, loss=1.199326515197754
I0316 03:11:09.527592 139661511923456 logging_writer.py:48] [108800] global_step=108800, grad_norm=3.369722604751587, loss=1.1379919052124023
I0316 03:12:36.262744 139661503530752 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.5987548828125, loss=1.1203902959823608
I0316 03:14:05.004940 139661511923456 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.9280128479003906, loss=1.174006700515747
I0316 03:15:30.832278 139661503530752 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.326282262802124, loss=1.1415632963180542
I0316 03:17:00.000101 139661511923456 logging_writer.py:48] [109200] global_step=109200, grad_norm=3.441309690475464, loss=1.1886208057403564
I0316 03:18:16.686059 139661503530752 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.579794406890869, loss=1.1424331665039062
I0316 03:19:33.577893 139661511923456 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.8076348304748535, loss=1.1603708267211914
I0316 03:20:51.131213 139661503530752 logging_writer.py:48] [109500] global_step=109500, grad_norm=3.69682240486145, loss=1.1976553201675415
I0316 03:22:13.853719 139661511923456 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.8551573753356934, loss=1.1582874059677124
I0316 03:23:38.855585 139661503530752 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.00058126449585, loss=1.135297179222107
I0316 03:25:08.302021 139661511923456 logging_writer.py:48] [109800] global_step=109800, grad_norm=3.1419224739074707, loss=1.1567888259887695
I0316 03:26:30.569957 139661503530752 logging_writer.py:48] [109900] global_step=109900, grad_norm=7.2252278327941895, loss=1.1094374656677246
I0316 03:27:55.677233 139661511923456 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.828553318977356, loss=1.1068989038467407
I0316 03:28:07.911079 139818008614720 spec.py:321] Evaluating on the training split.
I0316 03:29:04.664751 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 03:29:57.201444 139818008614720 spec.py:349] Evaluating on the test split.
I0316 03:30:23.034327 139818008614720 submission_runner.py:420] Time since start: 97758.57s, 	Step: 110014, 	{'train/ctc_loss': Array(0.14916688, dtype=float32), 'train/wer': 0.05324139928721803, 'validation/ctc_loss': Array(0.4225592, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682457, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 89340.51832222939, 'total_duration': 97758.5703458786, 'accumulated_submission_time': 89340.51832222939, 'accumulated_eval_time': 8409.858960390091, 'accumulated_logging_time': 3.5061757564544678}
I0316 03:30:23.083278 139661511923456 logging_writer.py:48] [110014] accumulated_eval_time=8409.858960, accumulated_logging_time=3.506176, accumulated_submission_time=89340.518322, global_step=110014, preemption_count=0, score=89340.518322, test/ctc_loss=0.2268245667219162, test/num_examples=2472, test/wer=0.072919, total_duration=97758.570346, train/ctc_loss=0.14916688203811646, train/wer=0.053241, validation/ctc_loss=0.4225592017173767, validation/num_examples=5348, validation/wer=0.121871
I0316 03:31:30.213787 139661503530752 logging_writer.py:48] [110100] global_step=110100, grad_norm=5.247521877288818, loss=1.1643365621566772
I0316 03:32:45.175492 139661511923456 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.0769779682159424, loss=1.1560266017913818
I0316 03:34:05.376774 139661511923456 logging_writer.py:48] [110300] global_step=110300, grad_norm=3.007046937942505, loss=1.1091504096984863
I0316 03:35:22.211718 139661503530752 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.9791951179504395, loss=1.106530785560608
I0316 03:36:40.884682 139661511923456 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.572474479675293, loss=1.1663239002227783
I0316 03:38:04.159851 139661503530752 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.840792655944824, loss=1.19769287109375
I0316 03:39:28.719969 139661511923456 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.845944404602051, loss=1.132936716079712
I0316 03:40:55.095029 139661503530752 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.1257660388946533, loss=1.2326916456222534
I0316 03:42:23.132778 139661511923456 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.239596366882324, loss=1.1909687519073486
I0316 03:43:50.442295 139661503530752 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.650436878204346, loss=1.1635379791259766
I0316 03:45:15.030184 139661511923456 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.52768874168396, loss=1.1430894136428833
I0316 03:46:39.902763 139661503530752 logging_writer.py:48] [111200] global_step=111200, grad_norm=3.2627310752868652, loss=1.1963417530059814
I0316 03:48:04.970632 139661511923456 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.318678617477417, loss=1.1636180877685547
I0316 03:49:22.821053 139661503530752 logging_writer.py:48] [111400] global_step=111400, grad_norm=3.6556897163391113, loss=1.149128794670105
I0316 03:50:42.246505 139661511923456 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.9108773469924927, loss=1.1059741973876953
I0316 03:52:01.402051 139661503530752 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.492255210876465, loss=1.2091516256332397
I0316 03:53:26.666823 139661511923456 logging_writer.py:48] [111700] global_step=111700, grad_norm=3.88775634765625, loss=1.188130259513855
I0316 03:54:23.240319 139818008614720 spec.py:321] Evaluating on the training split.
I0316 03:55:21.850302 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 03:56:13.321079 139818008614720 spec.py:349] Evaluating on the test split.
I0316 03:56:40.040095 139818008614720 submission_runner.py:420] Time since start: 99335.57s, 	Step: 111767, 	{'train/ctc_loss': Array(0.14817163, dtype=float32), 'train/wer': 0.05172621573897962, 'validation/ctc_loss': Array(0.4225653, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682922, dtype=float32), 'test/wer': 0.07289825929762557, 'test/num_examples': 2472, 'score': 90780.58996725082, 'total_duration': 99335.57467722893, 'accumulated_submission_time': 90780.58996725082, 'accumulated_eval_time': 8546.651809930801, 'accumulated_logging_time': 3.56905198097229}
I0316 03:56:40.089160 139661511923456 logging_writer.py:48] [111767] accumulated_eval_time=8546.651810, accumulated_logging_time=3.569052, accumulated_submission_time=90780.589967, global_step=111767, preemption_count=0, score=90780.589967, test/ctc_loss=0.22682921588420868, test/num_examples=2472, test/wer=0.072898, total_duration=99335.574677, train/ctc_loss=0.14817163348197937, train/wer=0.051726, validation/ctc_loss=0.4225653111934662, validation/num_examples=5348, validation/wer=0.121832
I0316 03:57:05.699149 139661503530752 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.3047842979431152, loss=1.1708543300628662
I0316 03:58:20.453783 139661511923456 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.4726271629333496, loss=1.1809310913085938
I0316 03:59:37.136808 139661503530752 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.740889072418213, loss=1.1793200969696045
I0316 04:01:00.256296 139661511923456 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.4936881065368652, loss=1.094382643699646
I0316 04:02:26.662686 139661503530752 logging_writer.py:48] [112200] global_step=112200, grad_norm=3.208164930343628, loss=1.1603612899780273
I0316 04:03:50.694207 139661511923456 logging_writer.py:48] [112300] global_step=112300, grad_norm=6.251324653625488, loss=1.1791434288024902
I0316 04:05:06.939435 139661503530752 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.7323203086853027, loss=1.2379493713378906
I0316 04:06:23.653817 139661511923456 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.8991901874542236, loss=1.1763614416122437
I0316 04:07:39.926528 139661503530752 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.0966010093688965, loss=1.1794688701629639
I0316 04:09:03.151389 139661511923456 logging_writer.py:48] [112700] global_step=112700, grad_norm=3.0155272483825684, loss=1.168687105178833
I0316 04:10:27.979639 139661503530752 logging_writer.py:48] [112800] global_step=112800, grad_norm=3.167041301727295, loss=1.1505581140518188
I0316 04:11:49.203574 139661511923456 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.3082051277160645, loss=1.1956703662872314
I0316 04:13:14.830982 139661503530752 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.368467092514038, loss=1.2187211513519287
I0316 04:14:39.568790 139661511923456 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.8699438571929932, loss=1.1338837146759033
I0316 04:16:04.364598 139661503530752 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.171206474304199, loss=1.1427172422409058
I0316 04:17:32.178149 139661511923456 logging_writer.py:48] [113300] global_step=113300, grad_norm=3.825035810470581, loss=1.222981333732605
I0316 04:18:48.870048 139661503530752 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.6332467794418335, loss=1.1136542558670044
I0316 04:20:06.103021 139661511923456 logging_writer.py:48] [113500] global_step=113500, grad_norm=3.325502634048462, loss=1.2223553657531738
I0316 04:20:40.604070 139818008614720 spec.py:321] Evaluating on the training split.
I0316 04:21:36.419692 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 04:22:28.221802 139818008614720 spec.py:349] Evaluating on the test split.
I0316 04:22:55.707334 139818008614720 submission_runner.py:420] Time since start: 100911.24s, 	Step: 113547, 	{'train/ctc_loss': Array(0.1355164, dtype=float32), 'train/wer': 0.04865793927379515, 'validation/ctc_loss': Array(0.4225174, dtype=float32), 'validation/wer': 0.12182241231161359, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22680725, dtype=float32), 'test/wer': 0.07285763613836248, 'test/num_examples': 2472, 'score': 92221.01919841766, 'total_duration': 100911.24274611473, 'accumulated_submission_time': 92221.01919841766, 'accumulated_eval_time': 8681.74896121025, 'accumulated_logging_time': 3.6316757202148438}
I0316 04:22:55.757149 139660626155264 logging_writer.py:48] [113547] accumulated_eval_time=8681.748961, accumulated_logging_time=3.631676, accumulated_submission_time=92221.019198, global_step=113547, preemption_count=0, score=92221.019198, test/ctc_loss=0.22680725157260895, test/num_examples=2472, test/wer=0.072858, total_duration=100911.242746, train/ctc_loss=0.13551640510559082, train/wer=0.048658, validation/ctc_loss=0.4225173890590668, validation/num_examples=5348, validation/wer=0.121822
I0316 04:23:36.126467 139660617762560 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.7182766199111938, loss=1.2140778303146362
I0316 04:24:51.115365 139660626155264 logging_writer.py:48] [113700] global_step=113700, grad_norm=3.3779101371765137, loss=1.1614547967910767
I0316 04:26:06.075790 139660617762560 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.0802266597747803, loss=1.0994009971618652
I0316 04:27:24.519469 139660626155264 logging_writer.py:48] [113900] global_step=113900, grad_norm=3.687020778656006, loss=1.2101207971572876
I0316 04:28:51.642912 139660617762560 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.883761882781982, loss=1.085971713066101
I0316 04:30:17.646165 139660626155264 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.9935399293899536, loss=1.2210465669631958
I0316 04:31:46.343220 139660617762560 logging_writer.py:48] [114200] global_step=114200, grad_norm=8.633291244506836, loss=1.1016649007797241
I0316 04:33:13.229573 139660626155264 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.062706232070923, loss=1.152503252029419
I0316 04:34:35.145133 139660626155264 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.777923345565796, loss=1.1348769664764404
I0316 04:35:51.490587 139660617762560 logging_writer.py:48] [114500] global_step=114500, grad_norm=3.824312686920166, loss=1.234618902206421
I0316 04:37:07.971630 139660626155264 logging_writer.py:48] [114600] global_step=114600, grad_norm=5.902799606323242, loss=1.1449623107910156
I0316 04:38:24.906232 139660617762560 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.0359303951263428, loss=1.1711558103561401
I0316 04:39:50.297129 139660626155264 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.695204496383667, loss=1.1763274669647217
I0316 04:41:14.980398 139660617762560 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.7898054122924805, loss=1.1542519330978394
I0316 04:42:40.559917 139660626155264 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.3560791015625, loss=1.1551272869110107
I0316 04:44:05.242114 139660617762560 logging_writer.py:48] [115100] global_step=115100, grad_norm=5.164953231811523, loss=1.1069624423980713
I0316 04:45:30.036917 139660626155264 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.6011741161346436, loss=1.202458143234253
I0316 04:46:56.182067 139660617762560 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.7700891494750977, loss=1.1159738302230835
I0316 04:46:56.187889 139818008614720 spec.py:321] Evaluating on the training split.
I0316 04:47:56.866231 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 04:48:49.212625 139818008614720 spec.py:349] Evaluating on the test split.
I0316 04:49:16.343746 139818008614720 submission_runner.py:420] Time since start: 102491.88s, 	Step: 115301, 	{'train/ctc_loss': Array(0.14265695, dtype=float32), 'train/wer': 0.04942687918725694, 'validation/ctc_loss': Array(0.42258564, dtype=float32), 'validation/wer': 0.12184172161773367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684036, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 93661.36353373528, 'total_duration': 102491.87964940071, 'accumulated_submission_time': 93661.36353373528, 'accumulated_eval_time': 8821.899176359177, 'accumulated_logging_time': 3.6960880756378174}
I0316 04:49:16.388465 139660626155264 logging_writer.py:48] [115301] accumulated_eval_time=8821.899176, accumulated_logging_time=3.696088, accumulated_submission_time=93661.363534, global_step=115301, preemption_count=0, score=93661.363534, test/ctc_loss=0.22684036195278168, test/num_examples=2472, test/wer=0.072919, total_duration=102491.879649, train/ctc_loss=0.14265695214271545, train/wer=0.049427, validation/ctc_loss=0.4225856363773346, validation/num_examples=5348, validation/wer=0.121842
I0316 04:50:35.010162 139660626155264 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.625454902648926, loss=1.1606441736221313
I0316 04:51:51.295022 139660617762560 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.3009748458862305, loss=1.2487242221832275
I0316 04:53:07.279538 139660626155264 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.9641196727752686, loss=1.173154592514038
I0316 04:54:27.266065 139660617762560 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.6556925773620605, loss=1.1506531238555908
I0316 04:55:53.026934 139660626155264 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.8685154914855957, loss=1.139550805091858
I0316 04:57:19.128557 139660617762560 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.9719042778015137, loss=1.1610078811645508
I0316 04:58:45.927445 139660626155264 logging_writer.py:48] [116000] global_step=116000, grad_norm=8.256111145019531, loss=1.0976195335388184
I0316 05:00:09.377129 139660617762560 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.2078211307525635, loss=1.1618523597717285
I0316 05:01:35.577130 139660626155264 logging_writer.py:48] [116200] global_step=116200, grad_norm=3.302356243133545, loss=1.1352699995040894
I0316 05:02:59.351245 139660617762560 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.7720364332199097, loss=1.1634172201156616
I0316 05:04:30.560025 139660626155264 logging_writer.py:48] [116400] global_step=116400, grad_norm=3.227950096130371, loss=1.1650125980377197
I0316 05:05:47.173598 139660617762560 logging_writer.py:48] [116500] global_step=116500, grad_norm=6.1305131912231445, loss=1.1671855449676514
I0316 05:07:03.601273 139660626155264 logging_writer.py:48] [116600] global_step=116600, grad_norm=6.807319164276123, loss=1.195753574371338
I0316 05:08:22.058967 139660617762560 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.791089057922363, loss=1.1530593633651733
I0316 05:09:43.635704 139660626155264 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.639116644859314, loss=1.1394237279891968
I0316 05:11:09.845458 139660617762560 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.9518251419067383, loss=1.1344845294952393
I0316 05:12:33.195480 139660626155264 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.4278411865234375, loss=1.145527958869934
I0316 05:13:16.375643 139818008614720 spec.py:321] Evaluating on the training split.
I0316 05:14:13.364953 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 05:15:04.815527 139818008614720 spec.py:349] Evaluating on the test split.
I0316 05:15:31.253171 139818008614720 submission_runner.py:420] Time since start: 104066.79s, 	Step: 117052, 	{'train/ctc_loss': Array(0.15840484, dtype=float32), 'train/wer': 0.054679723003678855, 'validation/ctc_loss': Array(0.4226101, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685356, dtype=float32), 'test/wer': 0.07295919403652022, 'test/num_examples': 2472, 'score': 95101.26403808594, 'total_duration': 104066.78872728348, 'accumulated_submission_time': 95101.26403808594, 'accumulated_eval_time': 8956.770744800568, 'accumulated_logging_time': 3.7561357021331787}
I0316 05:15:31.301375 139661726963456 logging_writer.py:48] [117052] accumulated_eval_time=8956.770745, accumulated_logging_time=3.756136, accumulated_submission_time=95101.264038, global_step=117052, preemption_count=0, score=95101.264038, test/ctc_loss=0.22685356438159943, test/num_examples=2472, test/wer=0.072959, total_duration=104066.788727, train/ctc_loss=0.15840484201908112, train/wer=0.054680, validation/ctc_loss=0.4226101040840149, validation/num_examples=5348, validation/wer=0.121832
I0316 05:16:09.001270 139661718570752 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.8129165172576904, loss=1.164840579032898
I0316 05:17:23.674012 139661726963456 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.4549720287323, loss=1.1372923851013184
I0316 05:18:39.527354 139661718570752 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.9210257530212402, loss=1.171148657798767
I0316 05:20:05.293826 139661726963456 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.118356466293335, loss=1.132908821105957
I0316 05:21:26.041324 139661726963456 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.74479603767395, loss=1.0752977132797241
I0316 05:22:43.589398 139661718570752 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.312368869781494, loss=1.152984380722046
I0316 05:24:00.832677 139661726963456 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.784332752227783, loss=1.2330193519592285
I0316 05:25:21.420511 139661718570752 logging_writer.py:48] [117800] global_step=117800, grad_norm=3.777005434036255, loss=1.1513190269470215
I0316 05:26:44.953474 139661726963456 logging_writer.py:48] [117900] global_step=117900, grad_norm=3.0105693340301514, loss=1.0815831422805786
I0316 05:28:11.786351 139661718570752 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.5846755504608154, loss=1.1656301021575928
I0316 05:29:41.715928 139661726963456 logging_writer.py:48] [118100] global_step=118100, grad_norm=3.578364849090576, loss=1.1751950979232788
I0316 05:31:05.292495 139661718570752 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.0044054985046387, loss=1.1408624649047852
I0316 05:32:32.892562 139661726963456 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.4146368503570557, loss=1.176029920578003
I0316 05:33:58.642631 139661718570752 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.902538299560547, loss=1.1236228942871094
I0316 05:35:22.452957 139661726963456 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.6331043243408203, loss=1.210979700088501
I0316 05:36:40.169385 139661718570752 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.0347278118133545, loss=1.1448988914489746
I0316 05:37:57.934759 139661726963456 logging_writer.py:48] [118700] global_step=118700, grad_norm=3.6620426177978516, loss=1.175398349761963
I0316 05:39:13.813593 139661718570752 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.8103883266448975, loss=1.2078038454055786
I0316 05:39:31.847474 139818008614720 spec.py:321] Evaluating on the training split.
I0316 05:40:26.259704 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 05:41:17.007194 139818008614720 spec.py:349] Evaluating on the test split.
I0316 05:41:44.010862 139818008614720 submission_runner.py:420] Time since start: 105639.55s, 	Step: 118823, 	{'train/ctc_loss': Array(0.15105623, dtype=float32), 'train/wer': 0.05144603450436071, 'validation/ctc_loss': Array(0.42255726, dtype=float32), 'validation/wer': 0.12184172161773367, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683339, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 96541.71780490875, 'total_duration': 105639.54648947716, 'accumulated_submission_time': 96541.71780490875, 'accumulated_eval_time': 9088.92824625969, 'accumulated_logging_time': 3.8233048915863037}
I0316 05:41:44.055526 139661726963456 logging_writer.py:48] [118823] accumulated_eval_time=9088.928246, accumulated_logging_time=3.823305, accumulated_submission_time=96541.717805, global_step=118823, preemption_count=0, score=96541.717805, test/ctc_loss=0.22683338820934296, test/num_examples=2472, test/wer=0.072939, total_duration=105639.546489, train/ctc_loss=0.1510562300682068, train/wer=0.051446, validation/ctc_loss=0.4225572645664215, validation/num_examples=5348, validation/wer=0.121842
I0316 05:42:42.905959 139661718570752 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.5082571506500244, loss=1.1547088623046875
I0316 05:43:57.872261 139661726963456 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.987868070602417, loss=1.2211191654205322
I0316 05:45:13.697030 139661718570752 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.125417470932007, loss=1.173444390296936
I0316 05:46:41.946892 139661726963456 logging_writer.py:48] [119200] global_step=119200, grad_norm=3.7182536125183105, loss=1.0739076137542725
I0316 05:48:07.256732 139661718570752 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.877875804901123, loss=1.1355764865875244
I0316 05:49:33.926220 139661726963456 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.891935348510742, loss=1.128757119178772
I0316 05:51:01.027325 139661399283456 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.1076321601867676, loss=1.1222989559173584
I0316 05:52:19.378728 139661390890752 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.611625909805298, loss=1.149883508682251
I0316 05:53:36.587169 139661399283456 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.3413772583007812, loss=1.1282070875167847
I0316 05:54:57.400939 139661390890752 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.226806879043579, loss=1.1564573049545288
I0316 05:56:18.840822 139661399283456 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.0404856204986572, loss=1.124241590499878
I0316 05:57:44.061944 139661390890752 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.072420358657837, loss=1.172642469406128
I0316 05:59:09.135473 139661399283456 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.667097806930542, loss=1.1441062688827515
I0316 06:00:36.112786 139661390890752 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9074325561523438, loss=1.1876702308654785
I0316 06:02:02.839219 139661399283456 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.9089834690093994, loss=1.202398419380188
I0316 06:03:25.018663 139661390890752 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.255910873413086, loss=1.1221529245376587
I0316 06:04:49.426450 139661399283456 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.2226951122283936, loss=1.198643445968628
I0316 06:05:44.624661 139818008614720 spec.py:321] Evaluating on the training split.
I0316 06:06:41.004311 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 06:07:35.131080 139818008614720 spec.py:349] Evaluating on the test split.
I0316 06:08:03.066787 139818008614720 submission_runner.py:420] Time since start: 107218.60s, 	Step: 120570, 	{'train/ctc_loss': Array(0.14774881, dtype=float32), 'train/wer': 0.05287780855647892, 'validation/ctc_loss': Array(0.42251122, dtype=float32), 'validation/wer': 0.12179344835243346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22680296, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 97982.20047354698, 'total_duration': 107218.60173940659, 'accumulated_submission_time': 97982.20047354698, 'accumulated_eval_time': 9227.363971710205, 'accumulated_logging_time': 3.88323712348938}
I0316 06:08:03.120391 139660892403456 logging_writer.py:48] [120570] accumulated_eval_time=9227.363972, accumulated_logging_time=3.883237, accumulated_submission_time=97982.200474, global_step=120570, preemption_count=0, score=97982.200474, test/ctc_loss=0.22680296003818512, test/num_examples=2472, test/wer=0.072878, total_duration=107218.601739, train/ctc_loss=0.14774881303310394, train/wer=0.052878, validation/ctc_loss=0.4225112199783325, validation/num_examples=5348, validation/wer=0.121793
I0316 06:08:26.420622 139660884010752 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.677858352661133, loss=1.106040120124817
I0316 06:09:41.813861 139660892403456 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.8922717571258545, loss=1.1547285318374634
I0316 06:10:56.779626 139660884010752 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.1426074504852295, loss=1.2018487453460693
I0316 06:12:11.736503 139660892403456 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.0457968711853027, loss=1.1505588293075562
I0316 06:13:34.325935 139660884010752 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.269014358520508, loss=1.1704350709915161
I0316 06:15:03.929429 139660892403456 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.4737281799316406, loss=1.1894127130508423
I0316 06:16:32.770776 139660884010752 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.569950580596924, loss=1.2208884954452515
I0316 06:17:58.616267 139660892403456 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.256706714630127, loss=1.156445026397705
I0316 06:19:24.781996 139660884010752 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.9891259670257568, loss=1.1500494480133057
I0316 06:20:52.226785 139660892403456 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.650437355041504, loss=1.1769386529922485
I0316 06:22:15.179933 139660892403456 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.17372989654541, loss=1.1300082206726074
I0316 06:23:32.021480 139660884010752 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.5891036987304688, loss=1.1844927072525024
I0316 06:24:47.726162 139660892403456 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.8063485622406006, loss=1.1315159797668457
I0316 06:26:02.612146 139660884010752 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.188218832015991, loss=1.1399266719818115
I0316 06:27:27.298120 139660892403456 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.377246856689453, loss=1.169152021408081
I0316 06:28:54.132621 139660884010752 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.8820693492889404, loss=1.151757836341858
I0316 06:30:21.928840 139660892403456 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.426278591156006, loss=1.1289523839950562
I0316 06:31:49.302458 139660884010752 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.811936140060425, loss=1.1567820310592651
I0316 06:32:03.670183 139818008614720 spec.py:321] Evaluating on the training split.
I0316 06:32:59.199861 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 06:33:51.090743 139818008614720 spec.py:349] Evaluating on the test split.
I0316 06:34:17.859202 139818008614720 submission_runner.py:420] Time since start: 108793.39s, 	Step: 122319, 	{'train/ctc_loss': Array(0.18443608, dtype=float32), 'train/wer': 0.05921269557633194, 'validation/ctc_loss': Array(0.4226183, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685014, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 99422.66232037544, 'total_duration': 108793.39494419098, 'accumulated_submission_time': 99422.66232037544, 'accumulated_eval_time': 9361.547214984894, 'accumulated_logging_time': 3.952324867248535}
I0316 06:34:17.906874 139660595439360 logging_writer.py:48] [122319] accumulated_eval_time=9361.547215, accumulated_logging_time=3.952325, accumulated_submission_time=99422.662320, global_step=122319, preemption_count=0, score=99422.662320, test/ctc_loss=0.22685013711452484, test/num_examples=2472, test/wer=0.072980, total_duration=108793.394944, train/ctc_loss=0.18443608283996582, train/wer=0.059213, validation/ctc_loss=0.4226182997226715, validation/num_examples=5348, validation/wer=0.121871
I0316 06:35:20.205654 139660587046656 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.7794570922851562, loss=1.1278995275497437
I0316 06:36:35.107624 139660595439360 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.4268300533294678, loss=1.1467896699905396
I0316 06:37:55.754095 139660595439360 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.045238494873047, loss=1.160645842552185
I0316 06:39:12.558574 139660587046656 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.0329442024230957, loss=1.1469618082046509
I0316 06:40:29.485818 139660595439360 logging_writer.py:48] [122800] global_step=122800, grad_norm=6.517879486083984, loss=1.176832675933838
I0316 06:41:46.056377 139660587046656 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.231424808502197, loss=1.1226505041122437
I0316 06:43:10.281782 139660595439360 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.0757644176483154, loss=1.1476507186889648
I0316 06:44:39.588024 139660587046656 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.5004098415374756, loss=1.1734503507614136
I0316 06:46:08.807107 139660595439360 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.0614094734191895, loss=1.2012115716934204
I0316 06:47:36.594830 139660587046656 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.0172040462493896, loss=1.1475684642791748
I0316 06:49:01.291445 139660595439360 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.86344051361084, loss=1.10982346534729
I0316 06:50:23.948971 139660587046656 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.033606052398682, loss=1.138569712638855
I0316 06:51:54.201169 139660595439360 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.5497701168060303, loss=1.158685564994812
I0316 06:53:09.509325 139660587046656 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.376589059829712, loss=1.1391931772232056
I0316 06:54:24.945580 139660595439360 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.1148650646209717, loss=1.1618090867996216
I0316 06:55:43.133699 139660587046656 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.5295584201812744, loss=1.1510416269302368
I0316 06:57:05.750340 139660595439360 logging_writer.py:48] [124000] global_step=124000, grad_norm=6.634083271026611, loss=1.1459859609603882
I0316 06:58:18.263558 139818008614720 spec.py:321] Evaluating on the training split.
I0316 06:59:15.505182 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 07:00:05.900020 139818008614720 spec.py:349] Evaluating on the test split.
I0316 07:00:32.147045 139818008614720 submission_runner.py:420] Time since start: 110367.68s, 	Step: 124089, 	{'train/ctc_loss': Array(0.1285202, dtype=float32), 'train/wer': 0.045434501712574606, 'validation/ctc_loss': Array(0.42256793, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268248, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 100862.93128800392, 'total_duration': 110367.68206977844, 'accumulated_submission_time': 100862.93128800392, 'accumulated_eval_time': 9495.424234628677, 'accumulated_logging_time': 4.015031814575195}
I0316 07:00:32.200746 139660595439360 logging_writer.py:48] [124089] accumulated_eval_time=9495.424235, accumulated_logging_time=4.015032, accumulated_submission_time=100862.931288, global_step=124089, preemption_count=0, score=100862.931288, test/ctc_loss=0.2268248051404953, test/num_examples=2472, test/wer=0.072919, total_duration=110367.682070, train/ctc_loss=0.128520205616951, train/wer=0.045435, validation/ctc_loss=0.4225679337978363, validation/num_examples=5348, validation/wer=0.121871
I0316 07:00:41.562098 139660587046656 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.732051372528076, loss=1.115803837776184
I0316 07:01:56.837511 139660595439360 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.6049697399139404, loss=1.2093979120254517
I0316 07:03:12.373223 139660587046656 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.1985859870910645, loss=1.1343854665756226
I0316 07:04:35.503960 139660595439360 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.6876678466796875, loss=1.160659670829773
I0316 07:06:01.947291 139660587046656 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.6769874095916748, loss=1.1752082109451294
I0316 07:07:28.593678 139660595439360 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.8828682899475098, loss=1.136661171913147
I0316 07:08:50.775239 139660595439360 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.697589874267578, loss=1.16765296459198
I0316 07:10:07.837682 139660587046656 logging_writer.py:48] [124800] global_step=124800, grad_norm=4.23298978805542, loss=1.1271737813949585
I0316 07:11:23.660254 139660595439360 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.8401081562042236, loss=1.125535488128662
I0316 07:12:46.226976 139660587046656 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.3260138034820557, loss=1.148581862449646
I0316 07:14:12.626879 139660595439360 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.976107120513916, loss=1.1622047424316406
I0316 07:15:39.258024 139660587046656 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.7999258041381836, loss=1.1911345720291138
I0316 07:17:04.378633 139660595439360 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.0868279933929443, loss=1.1777793169021606
I0316 07:18:30.188015 139660587046656 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.150264024734497, loss=1.1063467264175415
I0316 07:19:58.232191 139660595439360 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.136906862258911, loss=1.150050401687622
I0316 07:21:28.439765 139660587046656 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.6533002853393555, loss=1.2286202907562256
I0316 07:22:52.805386 139660595439360 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.1774840354919434, loss=1.1734923124313354
I0316 07:24:07.911335 139660587046656 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.8682029247283936, loss=1.1603924036026
I0316 07:24:32.512880 139818008614720 spec.py:321] Evaluating on the training split.
I0316 07:25:28.036459 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 07:26:18.820933 139818008614720 spec.py:349] Evaluating on the test split.
I0316 07:26:44.804036 139818008614720 submission_runner.py:420] Time since start: 111940.34s, 	Step: 125833, 	{'train/ctc_loss': Array(0.14595449, dtype=float32), 'train/wer': 0.050949620204195786, 'validation/ctc_loss': Array(0.42254525, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22681937, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 102303.157143116, 'total_duration': 111940.33980464935, 'accumulated_submission_time': 102303.157143116, 'accumulated_eval_time': 9627.709649085999, 'accumulated_logging_time': 4.083231449127197}
I0316 07:26:44.854332 139660595439360 logging_writer.py:48] [125833] accumulated_eval_time=9627.709649, accumulated_logging_time=4.083231, accumulated_submission_time=102303.157143, global_step=125833, preemption_count=0, score=102303.157143, test/ctc_loss=0.22681936621665955, test/num_examples=2472, test/wer=0.072919, total_duration=111940.339805, train/ctc_loss=0.14595448970794678, train/wer=0.050950, validation/ctc_loss=0.42254525423049927, validation/num_examples=5348, validation/wer=0.121832
I0316 07:27:35.614538 139660587046656 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.8267858028411865, loss=1.0611652135849
I0316 07:28:51.682229 139660595439360 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.004586219787598, loss=1.164149284362793
I0316 07:30:07.693659 139660587046656 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.882324457168579, loss=1.1557197570800781
I0316 07:31:23.860105 139660595439360 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.551583766937256, loss=1.144972562789917
I0316 07:32:53.301415 139660587046656 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.547694206237793, loss=1.1513444185256958
I0316 07:34:18.171109 139660595439360 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.876373291015625, loss=1.1252448558807373
I0316 07:35:48.606784 139660587046656 logging_writer.py:48] [126500] global_step=126500, grad_norm=4.6512322425842285, loss=1.1722556352615356
I0316 07:37:16.391683 139660595439360 logging_writer.py:48] [126600] global_step=126600, grad_norm=4.079885005950928, loss=1.1371419429779053
I0316 07:38:43.788405 139660595439360 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.7069575786590576, loss=1.1822985410690308
I0316 07:40:01.210428 139660587046656 logging_writer.py:48] [126800] global_step=126800, grad_norm=4.728011608123779, loss=1.1364777088165283
I0316 07:41:16.514963 139660595439360 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.3241279125213623, loss=1.1034959554672241
I0316 07:42:34.435968 139660587046656 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.8898813724517822, loss=1.162513017654419
I0316 07:43:56.804666 139660595439360 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.1101455688476562, loss=1.08607816696167
I0316 07:45:20.512053 139660587046656 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.7434580326080322, loss=1.1892755031585693
I0316 07:46:49.105160 139660595439360 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.9854251146316528, loss=1.1412177085876465
I0316 07:48:17.115550 139660587046656 logging_writer.py:48] [127400] global_step=127400, grad_norm=4.955169200897217, loss=1.1303291320800781
I0316 07:49:41.006059 139660595439360 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.810446262359619, loss=1.1590157747268677
I0316 07:50:45.144695 139818008614720 spec.py:321] Evaluating on the training split.
I0316 07:51:40.318027 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 07:52:31.255727 139818008614720 spec.py:349] Evaluating on the test split.
I0316 07:52:57.024441 139818008614720 submission_runner.py:420] Time since start: 113512.56s, 	Step: 127576, 	{'train/ctc_loss': Array(0.20160694, dtype=float32), 'train/wer': 0.07048167608327655, 'validation/ctc_loss': Array(0.4225641, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682925, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 103743.35876011848, 'total_duration': 113512.56096434593, 'accumulated_submission_time': 103743.35876011848, 'accumulated_eval_time': 9759.584423303604, 'accumulated_logging_time': 4.149665832519531}
I0316 07:52:57.076744 139661138159360 logging_writer.py:48] [127576] accumulated_eval_time=9759.584423, accumulated_logging_time=4.149666, accumulated_submission_time=103743.358760, global_step=127576, preemption_count=0, score=103743.358760, test/ctc_loss=0.22682924568653107, test/num_examples=2472, test/wer=0.072939, total_duration=113512.560964, train/ctc_loss=0.20160694420337677, train/wer=0.070482, validation/ctc_loss=0.4225640892982483, validation/num_examples=5348, validation/wer=0.121851
I0316 07:53:16.691660 139661129766656 logging_writer.py:48] [127600] global_step=127600, grad_norm=6.4096455574035645, loss=1.1789692640304565
I0316 07:54:33.139234 139661138159360 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.18686580657959, loss=1.1704777479171753
I0316 07:55:53.755475 139660482799360 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.898573875427246, loss=1.1132309436798096
I0316 07:57:11.396459 139660474406656 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.3766582012176514, loss=1.1665470600128174
I0316 07:58:29.725138 139660482799360 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.8422539234161377, loss=1.1613739728927612
I0316 07:59:51.630049 139660474406656 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.976424217224121, loss=1.0855069160461426
I0316 08:01:16.965427 139660482799360 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.096879243850708, loss=1.239186406135559
I0316 08:02:44.118543 139660474406656 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.004249334335327, loss=1.170920729637146
I0316 08:04:13.044238 139660482799360 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.015343427658081, loss=1.1279666423797607
I0316 08:05:45.132749 139660474406656 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.2357523441314697, loss=1.1885364055633545
I0316 08:07:13.448400 139660482799360 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.6358728408813477, loss=1.1435625553131104
I0316 08:08:42.262227 139660474406656 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.5144171714782715, loss=1.176832675933838
I0316 08:10:05.776127 139661138159360 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.7705295085906982, loss=1.1311408281326294
I0316 08:11:20.533302 139661129766656 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.3612306118011475, loss=1.1251674890518188
I0316 08:12:39.788201 139661138159360 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.0188381671905518, loss=1.1410224437713623
I0316 08:13:58.275202 139661129766656 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.8856465816497803, loss=1.1154372692108154
I0316 08:15:20.557402 139661138159360 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.5122478008270264, loss=1.1601015329360962
I0316 08:16:46.347064 139661129766656 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.2602219581604004, loss=1.1831395626068115
I0316 08:16:57.846540 139818008614720 spec.py:321] Evaluating on the training split.
I0316 08:17:52.033546 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 08:18:44.645928 139818008614720 spec.py:349] Evaluating on the test split.
I0316 08:19:12.186017 139818008614720 submission_runner.py:420] Time since start: 115087.72s, 	Step: 129314, 	{'train/ctc_loss': Array(0.22234, dtype=float32), 'train/wer': 0.07533589517519783, 'validation/ctc_loss': Array(0.42261353, dtype=float32), 'validation/wer': 0.12182241231161359, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2268536, dtype=float32), 'test/wer': 0.07304044035504641, 'test/num_examples': 2472, 'score': 105184.04212832451, 'total_duration': 115087.72128272057, 'accumulated_submission_time': 105184.04212832451, 'accumulated_eval_time': 9893.917643547058, 'accumulated_logging_time': 4.215802431106567}
I0316 08:19:12.243566 139661138159360 logging_writer.py:48] [129314] accumulated_eval_time=9893.917644, accumulated_logging_time=4.215802, accumulated_submission_time=105184.042128, global_step=129314, preemption_count=0, score=105184.042128, test/ctc_loss=0.22685359418392181, test/num_examples=2472, test/wer=0.073040, total_duration=115087.721283, train/ctc_loss=0.22234000265598297, train/wer=0.075336, validation/ctc_loss=0.4226135313510895, validation/num_examples=5348, validation/wer=0.121822
I0316 08:20:18.406414 139661129766656 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.8219263553619385, loss=1.130829095840454
I0316 08:21:34.463875 139661138159360 logging_writer.py:48] [129500] global_step=129500, grad_norm=8.028885841369629, loss=1.156485676765442
I0316 08:22:50.803989 139661129766656 logging_writer.py:48] [129600] global_step=129600, grad_norm=4.307255268096924, loss=1.0926921367645264
I0316 08:24:16.577248 139661138159360 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.2414674758911133, loss=1.1395050287246704
I0316 08:25:45.176169 139661138159360 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.141425609588623, loss=1.1801221370697021
I0316 08:27:01.475378 139661129766656 logging_writer.py:48] [129900] global_step=129900, grad_norm=6.654822826385498, loss=1.1972731351852417
I0316 08:28:18.417332 139661138159360 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.6278798580169678, loss=1.172897458076477
I0316 08:29:37.307176 139661129766656 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.085170269012451, loss=1.1709998846054077
I0316 08:30:56.637994 139661138159360 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.2364518642425537, loss=1.1842987537384033
I0316 08:32:20.434308 139661129766656 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.2822861671447754, loss=1.1535874605178833
I0316 08:33:45.699664 139661138159360 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.0208892822265625, loss=1.0899779796600342
I0316 08:35:11.243135 139661129766656 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.4142327308654785, loss=1.1040477752685547
I0316 08:36:38.305321 139661138159360 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.277356147766113, loss=1.1495541334152222
I0316 08:38:05.740639 139661129766656 logging_writer.py:48] [130700] global_step=130700, grad_norm=4.546909809112549, loss=1.182648777961731
I0316 08:39:30.872526 139661138159360 logging_writer.py:48] [130800] global_step=130800, grad_norm=9.027190208435059, loss=1.1766786575317383
I0316 08:40:50.346215 139661138159360 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.2631025314331055, loss=1.201616883277893
I0316 08:42:07.679574 139661129766656 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.5146453380584717, loss=1.1369447708129883
I0316 08:43:12.719942 139818008614720 spec.py:321] Evaluating on the training split.
I0316 08:44:05.785382 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 08:44:56.661906 139818008614720 spec.py:349] Evaluating on the test split.
I0316 08:45:23.213026 139818008614720 submission_runner.py:420] Time since start: 116658.75s, 	Step: 131085, 	{'train/ctc_loss': Array(0.2571062, dtype=float32), 'train/wer': 0.08830865973640245, 'validation/ctc_loss': Array(0.42260465, dtype=float32), 'validation/wer': 0.12188999488303388, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685236, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 106624.42895913124, 'total_duration': 116658.7487475872, 'accumulated_submission_time': 106624.42895913124, 'accumulated_eval_time': 10024.404949426651, 'accumulated_logging_time': 4.290512323379517}
I0316 08:45:23.262550 139661138159360 logging_writer.py:48] [131085] accumulated_eval_time=10024.404949, accumulated_logging_time=4.290512, accumulated_submission_time=106624.428959, global_step=131085, preemption_count=0, score=106624.428959, test/ctc_loss=0.22685235738754272, test/num_examples=2472, test/wer=0.072939, total_duration=116658.748748, train/ctc_loss=0.257106214761734, train/wer=0.088309, validation/ctc_loss=0.42260465025901794, validation/num_examples=5348, validation/wer=0.121890
I0316 08:45:35.703217 139661129766656 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.7232729196548462, loss=1.136343002319336
I0316 08:46:50.537874 139661138159360 logging_writer.py:48] [131200] global_step=131200, grad_norm=6.572636604309082, loss=1.1556123495101929
I0316 08:48:05.794007 139661129766656 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.7315354347229004, loss=1.1208211183547974
I0316 08:49:27.240281 139661138159360 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.078098773956299, loss=1.1627459526062012
I0316 08:50:52.345430 139661129766656 logging_writer.py:48] [131500] global_step=131500, grad_norm=4.235734462738037, loss=1.0887434482574463
I0316 08:52:19.475683 139661138159360 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.248688220977783, loss=1.0640636682510376
I0316 08:53:47.949072 139661129766656 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.471688985824585, loss=1.1911447048187256
I0316 08:55:14.144004 139661138159360 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.7361910343170166, loss=1.190232515335083
I0316 08:56:37.805713 139661138159360 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.8296639919281006, loss=1.1272244453430176
I0316 08:57:53.500261 139661129766656 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.7186524868011475, loss=1.1149471998214722
I0316 08:59:10.768449 139661138159360 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.6405856609344482, loss=1.1491948366165161
I0316 09:00:29.218761 139661129766656 logging_writer.py:48] [132200] global_step=132200, grad_norm=4.095647811889648, loss=1.139939785003662
I0316 09:01:51.555243 139661138159360 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.708993911743164, loss=1.1681455373764038
I0316 09:03:16.344795 139661129766656 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.76979923248291, loss=1.1767804622650146
I0316 09:04:44.492105 139661138159360 logging_writer.py:48] [132500] global_step=132500, grad_norm=6.219412326812744, loss=1.1209160089492798
I0316 09:06:09.657490 139661129766656 logging_writer.py:48] [132600] global_step=132600, grad_norm=4.163291931152344, loss=1.1856367588043213
I0316 09:07:39.642257 139661138159360 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.838297963142395, loss=1.11056649684906
I0316 09:09:08.545150 139661129766656 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.4366360902786255, loss=1.1648164987564087
I0316 09:09:23.515444 139818008614720 spec.py:321] Evaluating on the training split.
I0316 09:10:16.208452 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 09:11:08.307048 139818008614720 spec.py:349] Evaluating on the test split.
I0316 09:11:35.314597 139818008614720 submission_runner.py:420] Time since start: 118230.85s, 	Step: 132820, 	{'train/ctc_loss': Array(0.23016225, dtype=float32), 'train/wer': 0.07640568141800681, 'validation/ctc_loss': Array(0.42252997, dtype=float32), 'validation/wer': 0.12179344835243346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682151, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 108064.59675383568, 'total_duration': 118230.8504126072, 'accumulated_submission_time': 108064.59675383568, 'accumulated_eval_time': 10156.198406934738, 'accumulated_logging_time': 4.354108810424805}
I0316 09:11:35.372844 139660626155264 logging_writer.py:48] [132820] accumulated_eval_time=10156.198407, accumulated_logging_time=4.354109, accumulated_submission_time=108064.596754, global_step=132820, preemption_count=0, score=108064.596754, test/ctc_loss=0.22682151198387146, test/num_examples=2472, test/wer=0.072919, total_duration=118230.850413, train/ctc_loss=0.23016224801540375, train/wer=0.076406, validation/ctc_loss=0.4225299656391144, validation/num_examples=5348, validation/wer=0.121793
I0316 09:12:40.334643 139660626155264 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.8982036113739014, loss=1.2092621326446533
I0316 09:13:59.363010 139660617762560 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.6109459400177, loss=1.1910418272018433
I0316 09:15:16.975654 139660626155264 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.134805679321289, loss=1.175961971282959
I0316 09:16:38.128708 139660617762560 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.683075428009033, loss=1.1528984308242798
I0316 09:17:59.715470 139660626155264 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.95048451423645, loss=1.1766122579574585
I0316 09:19:28.966843 139660617762560 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.6086082458496094, loss=1.1842315196990967
I0316 09:20:54.455657 139660626155264 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.832437753677368, loss=1.1563377380371094
I0316 09:22:19.922588 139660617762560 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.2042465209960938, loss=1.2010366916656494
I0316 09:23:45.009636 139660626155264 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.225191831588745, loss=1.1469069719314575
I0316 09:25:11.991153 139660617762560 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.5726616382598877, loss=1.2001287937164307
I0316 09:26:39.192186 139660626155264 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.428729772567749, loss=1.1939598321914673
I0316 09:27:54.253433 139660617762560 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.022817611694336, loss=1.1757935285568237
I0316 09:29:10.538205 139660626155264 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.953679323196411, loss=1.1476203203201294
I0316 09:30:27.791983 139660617762560 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.4166579246521, loss=1.2034857273101807
I0316 09:31:48.188277 139660626155264 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.3394527435302734, loss=1.1633832454681396
I0316 09:33:12.386745 139660617762560 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.621987819671631, loss=1.1661932468414307
I0316 09:34:39.184383 139660626155264 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.593202590942383, loss=1.1580133438110352
I0316 09:35:35.409369 139818008614720 spec.py:321] Evaluating on the training split.
I0316 09:36:28.748542 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 09:37:19.306018 139818008614720 spec.py:349] Evaluating on the test split.
I0316 09:37:45.585609 139818008614720 submission_runner.py:420] Time since start: 119801.12s, 	Step: 134567, 	{'train/ctc_loss': Array(0.2190213, dtype=float32), 'train/wer': 0.07620294562230626, 'validation/ctc_loss': Array(0.42256498, dtype=float32), 'validation/wer': 0.12187068557691379, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683118, dtype=float32), 'test/wer': 0.07293888245688868, 'test/num_examples': 2472, 'score': 109504.54699015617, 'total_duration': 119801.12107515335, 'accumulated_submission_time': 109504.54699015617, 'accumulated_eval_time': 10286.368607997894, 'accumulated_logging_time': 4.4263763427734375}
I0316 09:37:45.639765 139661803763456 logging_writer.py:48] [134567] accumulated_eval_time=10286.368608, accumulated_logging_time=4.426376, accumulated_submission_time=109504.546990, global_step=134567, preemption_count=0, score=109504.546990, test/ctc_loss=0.22683118283748627, test/num_examples=2472, test/wer=0.072939, total_duration=119801.121075, train/ctc_loss=0.21902130544185638, train/wer=0.076203, validation/ctc_loss=0.4225649833679199, validation/num_examples=5348, validation/wer=0.121871
I0316 09:38:11.397413 139661795370752 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.46606707572937, loss=1.0738918781280518
I0316 09:39:28.175564 139661803763456 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.6252508163452148, loss=1.0969569683074951
I0316 09:40:45.611002 139661795370752 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.8394174575805664, loss=1.1353577375411987
I0316 09:42:07.306727 139661803763456 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.8359262943267822, loss=1.1832669973373413
I0316 09:43:27.719107 139661476083456 logging_writer.py:48] [135000] global_step=135000, grad_norm=4.052344799041748, loss=1.1479283571243286
I0316 09:44:45.359594 139661467690752 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.993188381195068, loss=1.1607930660247803
I0316 09:46:05.216189 139661476083456 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.1768100261688232, loss=1.1065456867218018
I0316 09:47:27.619361 139661467690752 logging_writer.py:48] [135300] global_step=135300, grad_norm=4.7008562088012695, loss=1.2338311672210693
I0316 09:48:52.646929 139661476083456 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.0093514919281006, loss=1.1929458379745483
I0316 09:50:22.042351 139661467690752 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.8973568677902222, loss=1.1617093086242676
I0316 09:51:48.342515 139661476083456 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.8919596672058105, loss=1.1610008478164673
I0316 09:53:14.294254 139661467690752 logging_writer.py:48] [135700] global_step=135700, grad_norm=12.423439979553223, loss=1.2103458642959595
I0316 09:54:40.347635 139661476083456 logging_writer.py:48] [135800] global_step=135800, grad_norm=4.002532958984375, loss=1.1595916748046875
I0316 09:56:06.183295 139661467690752 logging_writer.py:48] [135900] global_step=135900, grad_norm=4.615424156188965, loss=1.0977914333343506
I0316 09:57:32.175036 139661476083456 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.338387966156006, loss=1.197538137435913
I0316 09:58:49.975408 139661467690752 logging_writer.py:48] [136100] global_step=136100, grad_norm=4.564831733703613, loss=1.1952890157699585
I0316 10:00:06.306679 139661476083456 logging_writer.py:48] [136200] global_step=136200, grad_norm=4.9046759605407715, loss=1.1784117221832275
I0316 10:01:24.943202 139661467690752 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.562628746032715, loss=1.1599141359329224
I0316 10:01:45.941509 139818008614720 spec.py:321] Evaluating on the training split.
I0316 10:02:42.939104 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 10:03:34.497546 139818008614720 spec.py:349] Evaluating on the test split.
I0316 10:04:00.043579 139818008614720 submission_runner.py:420] Time since start: 121375.58s, 	Step: 136327, 	{'train/ctc_loss': Array(0.19412705, dtype=float32), 'train/wer': 0.06823183359197157, 'validation/ctc_loss': Array(0.4225796, dtype=float32), 'validation/wer': 0.12183206696467362, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684185, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 110944.7632997036, 'total_duration': 121375.58246660233, 'accumulated_submission_time': 110944.7632997036, 'accumulated_eval_time': 10420.468059301376, 'accumulated_logging_time': 4.494283437728882}
I0316 10:04:00.088063 139662233843456 logging_writer.py:48] [136327] accumulated_eval_time=10420.468059, accumulated_logging_time=4.494283, accumulated_submission_time=110944.763300, global_step=136327, preemption_count=0, score=110944.763300, test/ctc_loss=0.22684185206890106, test/num_examples=2472, test/wer=0.072919, total_duration=121375.582467, train/ctc_loss=0.19412705302238464, train/wer=0.068232, validation/ctc_loss=0.4225795865058899, validation/num_examples=5348, validation/wer=0.121832
I0316 10:04:57.042801 139662225450752 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.486123561859131, loss=1.1630052328109741
I0316 10:06:12.010003 139662233843456 logging_writer.py:48] [136500] global_step=136500, grad_norm=4.349343299865723, loss=1.1813442707061768
I0316 10:07:27.411386 139662225450752 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.1859054565429688, loss=1.202722191810608
I0316 10:08:55.364149 139662233843456 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.63934588432312, loss=1.1625839471817017
I0316 10:10:21.316735 139662225450752 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.766204595565796, loss=1.101676344871521
I0316 10:11:47.117312 139662233843456 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.6804418563842773, loss=1.0961381196975708
I0316 10:13:16.188728 139661578483456 logging_writer.py:48] [137000] global_step=137000, grad_norm=4.171130180358887, loss=1.1205676794052124
I0316 10:14:33.246743 139661570090752 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.397554874420166, loss=1.1677114963531494
I0316 10:15:49.297293 139661578483456 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.1032676696777344, loss=1.146883487701416
I0316 10:17:05.890552 139661570090752 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.1033365726470947, loss=1.13451087474823
I0316 10:18:27.461482 139661578483456 logging_writer.py:48] [137400] global_step=137400, grad_norm=8.432221412658691, loss=1.099544644355774
I0316 10:19:51.449114 139661570090752 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.5404679775238037, loss=1.145607352256775
I0316 10:21:17.382416 139661578483456 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.842725992202759, loss=1.1710317134857178
I0316 10:22:41.961775 139661570090752 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.1632840633392334, loss=1.1369454860687256
I0316 10:24:10.887029 139661578483456 logging_writer.py:48] [137800] global_step=137800, grad_norm=4.303554534912109, loss=1.13079833984375
I0316 10:25:35.426717 139661570090752 logging_writer.py:48] [137900] global_step=137900, grad_norm=4.961524963378906, loss=1.1473747491836548
I0316 10:27:04.812037 139661578483456 logging_writer.py:48] [138000] global_step=138000, grad_norm=4.06980037689209, loss=1.1733099222183228
I0316 10:28:00.045485 139818008614720 spec.py:321] Evaluating on the training split.
I0316 10:28:54.534916 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 10:29:45.388906 139818008614720 spec.py:349] Evaluating on the test split.
I0316 10:30:11.762949 139818008614720 submission_runner.py:420] Time since start: 122947.30s, 	Step: 138066, 	{'train/ctc_loss': Array(0.2334239, dtype=float32), 'train/wer': 0.08039746078837721, 'validation/ctc_loss': Array(0.42255595, dtype=float32), 'validation/wer': 0.12182241231161359, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682582, dtype=float32), 'test/wer': 0.07289825929762557, 'test/num_examples': 2472, 'score': 112384.63683605194, 'total_duration': 122947.29821825027, 'accumulated_submission_time': 112384.63683605194, 'accumulated_eval_time': 10552.179483890533, 'accumulated_logging_time': 4.551663398742676}
I0316 10:30:11.814923 139661286643456 logging_writer.py:48] [138066] accumulated_eval_time=10552.179484, accumulated_logging_time=4.551663, accumulated_submission_time=112384.636836, global_step=138066, preemption_count=0, score=112384.636836, test/ctc_loss=0.22682581841945648, test/num_examples=2472, test/wer=0.072898, total_duration=122947.298218, train/ctc_loss=0.23342390358448029, train/wer=0.080397, validation/ctc_loss=0.42255595326423645, validation/num_examples=5348, validation/wer=0.121822
I0316 10:30:38.596640 139661278250752 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.1330442428588867, loss=1.1298223733901978
I0316 10:31:55.073997 139661286643456 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.2303850650787354, loss=1.198174238204956
I0316 10:33:11.843330 139661278250752 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.2823996543884277, loss=1.1099210977554321
I0316 10:34:27.298118 139661286643456 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.6473379135131836, loss=1.1628152132034302
I0316 10:35:42.965871 139661278250752 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.0526978969573975, loss=1.2363239526748657
I0316 10:37:07.212850 139661286643456 logging_writer.py:48] [138600] global_step=138600, grad_norm=4.1128106117248535, loss=1.1693627834320068
I0316 10:38:33.895453 139661278250752 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.9104652404785156, loss=1.1637002229690552
I0316 10:40:02.773778 139661286643456 logging_writer.py:48] [138800] global_step=138800, grad_norm=7.868422985076904, loss=1.178904414176941
I0316 10:41:26.199091 139661278250752 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.830689787864685, loss=1.2041094303131104
I0316 10:42:53.980050 139661286643456 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.7648630142211914, loss=1.1641075611114502
I0316 10:44:20.576122 139662233843456 logging_writer.py:48] [139100] global_step=139100, grad_norm=4.9324631690979, loss=1.1256687641143799
I0316 10:45:37.670640 139662225450752 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.8244638442993164, loss=1.2073065042495728
I0316 10:46:55.049269 139662233843456 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.6965359449386597, loss=1.1324779987335205
I0316 10:48:11.661431 139662225450752 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.8298847675323486, loss=1.137208342552185
I0316 10:49:34.235976 139662233843456 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.2105865478515625, loss=1.126898169517517
I0316 10:51:00.437272 139662225450752 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.421945333480835, loss=1.2003116607666016
I0316 10:52:24.398409 139662233843456 logging_writer.py:48] [139700] global_step=139700, grad_norm=4.83339262008667, loss=1.1136746406555176
I0316 10:53:51.926931 139662225450752 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.9610040187835693, loss=1.1054935455322266
I0316 10:54:12.081460 139818008614720 spec.py:321] Evaluating on the training split.
I0316 10:55:07.780963 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 10:55:58.796819 139818008614720 spec.py:349] Evaluating on the test split.
I0316 10:56:26.462935 139818008614720 submission_runner.py:420] Time since start: 124522.00s, 	Step: 139824, 	{'train/ctc_loss': Array(0.21257159, dtype=float32), 'train/wer': 0.07374077638755214, 'validation/ctc_loss': Array(0.42262492, dtype=float32), 'validation/wer': 0.1218031030054935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22685347, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 113824.81503725052, 'total_duration': 124521.99733042717, 'accumulated_submission_time': 113824.81503725052, 'accumulated_eval_time': 10686.553840637207, 'accumulated_logging_time': 4.618646621704102}
I0316 10:56:26.522949 139662233843456 logging_writer.py:48] [139824] accumulated_eval_time=10686.553841, accumulated_logging_time=4.618647, accumulated_submission_time=113824.815037, global_step=139824, preemption_count=0, score=113824.815037, test/ctc_loss=0.22685347497463226, test/num_examples=2472, test/wer=0.072980, total_duration=124521.997330, train/ctc_loss=0.21257159113883972, train/wer=0.073741, validation/ctc_loss=0.4226249158382416, validation/num_examples=5348, validation/wer=0.121803
I0316 10:57:25.123806 139662225450752 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.9438371658325195, loss=1.1354262828826904
I0316 10:58:39.882729 139662233843456 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.8640711307525635, loss=1.1922129392623901
I0316 10:59:59.303830 139661906163456 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.104611396789551, loss=1.1354875564575195
I0316 11:01:16.968454 139661897770752 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.6530697345733643, loss=1.1588108539581299
I0316 11:02:33.015425 139661906163456 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.553128719329834, loss=1.1937906742095947
I0316 11:03:50.721658 139661897770752 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.1853010654449463, loss=1.1346900463104248
I0316 11:05:15.318903 139661906163456 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.2271175384521484, loss=1.147709846496582
I0316 11:06:40.046487 139661897770752 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.9203343391418457, loss=1.1734588146209717
I0316 11:08:09.453433 139661906163456 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.8618111610412598, loss=1.135932207107544
I0316 11:09:38.420841 139661897770752 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.9565931558609009, loss=1.160784363746643
I0316 11:11:04.788251 139661906163456 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.509333372116089, loss=1.1529077291488647
I0316 11:12:31.287969 139661897770752 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.366153240203857, loss=1.1298563480377197
I0316 11:13:56.647641 139661906163456 logging_writer.py:48] [141100] global_step=141100, grad_norm=7.960747718811035, loss=1.1594123840332031
I0316 11:15:17.605073 139661906163456 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.7515252828598022, loss=1.134987473487854
I0316 11:16:33.063467 139661897770752 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.1811106204986572, loss=1.1894348859786987
I0316 11:17:50.105420 139661906163456 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.012287139892578, loss=1.1711039543151855
I0316 11:19:08.366158 139661897770752 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.5325647592544556, loss=1.1471009254455566
I0316 11:20:27.112030 139818008614720 spec.py:321] Evaluating on the training split.
I0316 11:21:21.544016 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 11:22:12.808716 139818008614720 spec.py:349] Evaluating on the test split.
I0316 11:22:39.523281 139818008614720 submission_runner.py:420] Time since start: 126095.06s, 	Step: 141595, 	{'train/ctc_loss': Array(0.21481861, dtype=float32), 'train/wer': 0.07603881210126885, 'validation/ctc_loss': Array(0.42257047, dtype=float32), 'validation/wer': 0.1218513762707937, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683278, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 115265.31365394592, 'total_duration': 126095.0590775013, 'accumulated_submission_time': 115265.31365394592, 'accumulated_eval_time': 10818.959375619888, 'accumulated_logging_time': 4.695364475250244}
I0316 11:22:39.579772 139661322483456 logging_writer.py:48] [141595] accumulated_eval_time=10818.959376, accumulated_logging_time=4.695364, accumulated_submission_time=115265.313654, global_step=141595, preemption_count=0, score=115265.313654, test/ctc_loss=0.226832777261734, test/num_examples=2472, test/wer=0.072878, total_duration=126095.059078, train/ctc_loss=0.21481861174106598, train/wer=0.076039, validation/ctc_loss=0.42257046699523926, validation/num_examples=5348, validation/wer=0.121851
I0316 11:22:44.188299 139661314090752 logging_writer.py:48] [141600] global_step=141600, grad_norm=5.260246753692627, loss=1.1408960819244385
I0316 11:24:00.945373 139661322483456 logging_writer.py:48] [141700] global_step=141700, grad_norm=4.835113525390625, loss=1.1402580738067627
I0316 11:25:15.760558 139661314090752 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.7688045501708984, loss=1.131834626197815
I0316 11:26:34.677482 139661322483456 logging_writer.py:48] [141900] global_step=141900, grad_norm=5.510278701782227, loss=1.1539877653121948
I0316 11:28:03.432767 139661314090752 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.1013455390930176, loss=1.1352901458740234
I0316 11:29:30.539986 139661322483456 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.5275025367736816, loss=1.1894577741622925
I0316 11:30:53.178806 139660994803456 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.9319627285003662, loss=1.1500757932662964
I0316 11:32:10.411331 139660986410752 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.0560758113861084, loss=1.188412070274353
I0316 11:33:28.814620 139660994803456 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.3500220775604248, loss=1.1830711364746094
I0316 11:34:45.448213 139660986410752 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.9922571182250977, loss=1.178337574005127
I0316 11:36:09.533602 139660994803456 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.4567971229553223, loss=1.1602104902267456
I0316 11:37:34.373807 139660986410752 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.8246986865997314, loss=1.1800811290740967
I0316 11:38:59.336540 139660994803456 logging_writer.py:48] [142800] global_step=142800, grad_norm=4.075538635253906, loss=1.1376092433929443
I0316 11:40:24.801416 139660986410752 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.5940001010894775, loss=1.117885947227478
I0316 11:41:46.440870 139660994803456 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.948713541030884, loss=1.1788814067840576
I0316 11:43:14.409301 139660986410752 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.472215175628662, loss=1.1392791271209717
I0316 11:44:40.656370 139660994803456 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.652904748916626, loss=1.1733771562576294
I0316 11:45:57.159257 139660986410752 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.2259624004364014, loss=1.1951874494552612
I0316 11:46:39.911606 139818008614720 spec.py:321] Evaluating on the training split.
I0316 11:47:33.955009 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 11:48:26.046252 139818008614720 spec.py:349] Evaluating on the test split.
I0316 11:48:52.015177 139818008614720 submission_runner.py:420] Time since start: 127667.55s, 	Step: 143357, 	{'train/ctc_loss': Array(0.22314472, dtype=float32), 'train/wer': 0.0771731146606444, 'validation/ctc_loss': Array(0.4225849, dtype=float32), 'validation/wer': 0.12179344835243346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22684102, dtype=float32), 'test/wer': 0.07297950561615177, 'test/num_examples': 2472, 'score': 116705.55896472931, 'total_duration': 127667.55155706406, 'accumulated_submission_time': 116705.55896472931, 'accumulated_eval_time': 10951.057807445526, 'accumulated_logging_time': 4.766013145446777}
I0316 11:48:52.067700 139660626155264 logging_writer.py:48] [143357] accumulated_eval_time=10951.057807, accumulated_logging_time=4.766013, accumulated_submission_time=116705.558965, global_step=143357, preemption_count=0, score=116705.558965, test/ctc_loss=0.2268410176038742, test/num_examples=2472, test/wer=0.072980, total_duration=127667.551557, train/ctc_loss=0.22314472496509552, train/wer=0.077173, validation/ctc_loss=0.4225848913192749, validation/num_examples=5348, validation/wer=0.121793
I0316 11:49:25.442752 139660617762560 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.7052651643753052, loss=1.1280555725097656
I0316 11:50:41.204743 139660626155264 logging_writer.py:48] [143500] global_step=143500, grad_norm=4.371747970581055, loss=1.1854366064071655
I0316 11:51:57.134472 139660617762560 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.4879586696624756, loss=1.1452964544296265
I0316 11:53:13.880179 139660626155264 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.7342655658721924, loss=1.1903674602508545
I0316 11:54:42.873588 139660617762560 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.0069739818573, loss=1.1511446237564087
I0316 11:56:08.317826 139660626155264 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.7821195125579834, loss=1.1688982248306274
I0316 11:57:33.278421 139818008614720 spec.py:321] Evaluating on the training split.
I0316 11:58:25.230749 139818008614720 spec.py:333] Evaluating on the validation split.
I0316 11:59:18.118057 139818008614720 spec.py:349] Evaluating on the test split.
I0316 11:59:43.579115 139818008614720 submission_runner.py:420] Time since start: 128319.12s, 	Step: 144000, 	{'train/ctc_loss': Array(0.23591998, dtype=float32), 'train/wer': 0.08150679031841282, 'validation/ctc_loss': Array(0.42256364, dtype=float32), 'validation/wer': 0.12179344835243346, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22682914, dtype=float32), 'test/wer': 0.07291857087725713, 'test/num_examples': 2472, 'score': 117226.72539401054, 'total_duration': 128319.11793255806, 'accumulated_submission_time': 117226.72539401054, 'accumulated_eval_time': 11081.355797290802, 'accumulated_logging_time': 4.835377931594849}
I0316 11:59:43.623824 139661803763456 logging_writer.py:48] [144000] accumulated_eval_time=11081.355797, accumulated_logging_time=4.835378, accumulated_submission_time=117226.725394, global_step=144000, preemption_count=0, score=117226.725394, test/ctc_loss=0.2268291413784027, test/num_examples=2472, test/wer=0.072919, total_duration=128319.117933, train/ctc_loss=0.2359199821949005, train/wer=0.081507, validation/ctc_loss=0.4225636422634125, validation/num_examples=5348, validation/wer=0.121793
I0316 11:59:43.657198 139661795370752 logging_writer.py:48] [144000] global_step=144000, preemption_count=0, score=117226.725394
I0316 11:59:43.866671 139818008614720 checkpoints.py:490] Saving checkpoint at step: 144000
I0316 11:59:44.858749 139818008614720 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax/trial_1/checkpoint_144000
I0316 11:59:44.879866 139818008614720 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_3/librispeech_deepspeech_jax/trial_1/checkpoint_144000.
I0316 11:59:46.039361 139818008614720 submission_runner.py:683] Final librispeech_deepspeech score: 117226.72539401054
