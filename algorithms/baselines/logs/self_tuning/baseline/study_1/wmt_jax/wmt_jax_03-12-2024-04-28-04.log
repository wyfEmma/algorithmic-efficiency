python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_1 --overwrite=true --save_checkpoints=false --rng_seed=830112138 --max_global_steps=399999 --tuning_ruleset=self 2>&1 | tee -a /logs/wmt_jax_03-12-2024-04-28-04.log
I0312 04:28:28.258350 140197733885760 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax because --overwrite was set.
I0312 04:28:28.266384 140197733885760 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax.
I0312 04:28:29.303285 140197733885760 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0312 04:28:29.305092 140197733885760 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0312 04:28:29.305331 140197733885760 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0312 04:28:30.350179 140197733885760 submission_runner.py:612] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax/trial_1.
I0312 04:28:30.558654 140197733885760 submission_runner.py:209] Initializing dataset.
I0312 04:28:30.570327 140197733885760 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:28:30.574580 140197733885760 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:28:30.752160 140197733885760 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:28:32.880048 140197733885760 submission_runner.py:220] Initializing model.
I0312 04:28:41.659922 140197733885760 submission_runner.py:262] Initializing optimizer.
I0312 04:28:42.721393 140197733885760 submission_runner.py:269] Initializing metrics bundle.
I0312 04:28:42.721594 140197733885760 submission_runner.py:287] Initializing checkpoint and logger.
I0312 04:28:42.722446 140197733885760 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax/trial_1 with prefix checkpoint_
I0312 04:28:42.722574 140197733885760 submission_runner.py:307] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax/trial_1/meta_data_0.json.
I0312 04:28:42.722777 140197733885760 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0312 04:28:42.722839 140197733885760 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0312 04:28:43.027530 140197733885760 logger_utils.py:220] Unable to record git information. Continuing without it.
I0312 04:28:43.310194 140197733885760 submission_runner.py:311] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax/trial_1/flags_0.json.
I0312 04:28:43.319935 140197733885760 submission_runner.py:321] Starting training loop.
I0312 04:29:21.274662 140032903997184 logging_writer.py:48] [0] global_step=0, grad_norm=4.7252278327941895, loss=11.120248794555664
I0312 04:29:21.292022 140197733885760 spec.py:321] Evaluating on the training split.
I0312 04:29:21.295299 140197733885760 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:29:21.298274 140197733885760 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:29:21.334684 140197733885760 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:29:29.069744 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 04:34:28.930776 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 04:34:28.964797 140197733885760 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:34:28.973151 140197733885760 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:34:29.009898 140197733885760 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:34:35.913516 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 04:39:27.475166 140197733885760 spec.py:349] Evaluating on the test split.
I0312 04:39:27.477668 140197733885760 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:39:27.481106 140197733885760 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:39:27.513047 140197733885760 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:39:30.390458 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 04:44:21.768644 140197733885760 submission_runner.py:420] Time since start: 938.45s, 	Step: 1, 	{'train/accuracy': 0.0006708127912133932, 'train/loss': 11.12966251373291, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.123063087463379, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.114374160766602, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.97205352783203, 'total_duration': 938.4486303329468, 'accumulated_submission_time': 37.97205352783203, 'accumulated_eval_time': 900.4765393733978, 'accumulated_logging_time': 0}
I0312 04:44:21.850258 140028045932288 logging_writer.py:48] [1] accumulated_eval_time=900.476539, accumulated_logging_time=0, accumulated_submission_time=37.972054, global_step=1, preemption_count=0, score=37.972054, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.114374, test/num_examples=3003, total_duration=938.448630, train/accuracy=0.000671, train/bleu=0.000000, train/loss=11.129663, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.123063, validation/num_examples=3000
I0312 04:44:58.238905 140028037539584 logging_writer.py:48] [100] global_step=100, grad_norm=0.21887525916099548, loss=8.272217750549316
I0312 04:45:34.719573 140028045932288 logging_writer.py:48] [200] global_step=200, grad_norm=0.38372334837913513, loss=7.4841837882995605
I0312 04:46:11.223524 140028037539584 logging_writer.py:48] [300] global_step=300, grad_norm=0.6049387454986572, loss=6.904033184051514
I0312 04:46:47.752495 140028045932288 logging_writer.py:48] [400] global_step=400, grad_norm=0.4638378918170929, loss=6.318811893463135
I0312 04:47:24.270429 140028037539584 logging_writer.py:48] [500] global_step=500, grad_norm=0.38834691047668457, loss=5.914494037628174
I0312 04:48:00.796729 140028045932288 logging_writer.py:48] [600] global_step=600, grad_norm=0.4833255410194397, loss=5.589923858642578
I0312 04:48:37.320723 140028037539584 logging_writer.py:48] [700] global_step=700, grad_norm=0.5148507356643677, loss=5.310546398162842
I0312 04:49:13.860782 140028045932288 logging_writer.py:48] [800] global_step=800, grad_norm=0.3553869426250458, loss=5.034418106079102
I0312 04:49:50.409542 140028037539584 logging_writer.py:48] [900] global_step=900, grad_norm=0.47955673933029175, loss=4.755875587463379
I0312 04:50:26.953038 140028045932288 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6570615768432617, loss=4.66224479675293
I0312 04:51:03.494005 140028037539584 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6227917075157166, loss=4.3103790283203125
I0312 04:51:40.038733 140028045932288 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5754828453063965, loss=4.075313568115234
I0312 04:52:16.554933 140028037539584 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.4759928286075592, loss=4.016409873962402
I0312 04:52:53.088857 140028045932288 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.524948239326477, loss=3.78517746925354
I0312 04:53:29.584078 140028037539584 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5289648771286011, loss=3.5601987838745117
I0312 04:54:06.100492 140028045932288 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4495357573032379, loss=3.3836448192596436
I0312 04:54:42.594610 140028037539584 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.45033881068229675, loss=3.42852520942688
I0312 04:55:19.108292 140028045932288 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.39155906438827515, loss=3.325991630554199
I0312 04:55:55.640675 140028037539584 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5687533020973206, loss=3.269824743270874
I0312 04:56:32.163368 140028045932288 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4430849552154541, loss=3.159562110900879
I0312 04:57:08.673153 140028037539584 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.357894629240036, loss=3.066498041152954
I0312 04:57:45.191364 140028045932288 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4188666343688965, loss=3.1343932151794434
I0312 04:58:21.730677 140028037539584 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.3830323815345764, loss=3.080165147781372
I0312 04:58:21.812186 140197733885760 spec.py:321] Evaluating on the training split.
I0312 04:58:24.867433 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:01:06.447885 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 05:01:09.181116 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:03:49.716392 140197733885760 spec.py:349] Evaluating on the test split.
I0312 05:03:52.440651 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:06:11.735544 140197733885760 submission_runner.py:420] Time since start: 2248.42s, 	Step: 2302, 	{'train/accuracy': 0.5118868350982666, 'train/loss': 2.8945295810699463, 'train/bleu': 21.798427704443863, 'validation/accuracy': 0.5098015069961548, 'validation/loss': 2.891077995300293, 'validation/bleu': 17.50913139290973, 'validation/num_examples': 3000, 'test/accuracy': 0.5070478320121765, 'test/loss': 2.9452285766601562, 'test/bleu': 16.23465916489031, 'test/num_examples': 3003, 'score': 877.8537795543671, 'total_duration': 2248.415539741516, 'accumulated_submission_time': 877.8537795543671, 'accumulated_eval_time': 1370.3998427391052, 'accumulated_logging_time': 0.09156942367553711}
I0312 05:06:11.749337 140028045932288 logging_writer.py:48] [2302] accumulated_eval_time=1370.399843, accumulated_logging_time=0.091569, accumulated_submission_time=877.853780, global_step=2302, preemption_count=0, score=877.853780, test/accuracy=0.507048, test/bleu=16.234659, test/loss=2.945229, test/num_examples=3003, total_duration=2248.415540, train/accuracy=0.511887, train/bleu=21.798428, train/loss=2.894530, validation/accuracy=0.509802, validation/bleu=17.509131, validation/loss=2.891078, validation/num_examples=3000
I0312 05:06:47.692127 140028037539584 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3950837552547455, loss=2.895630359649658
I0312 05:07:24.042535 140028045932288 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.33646702766418457, loss=2.9960992336273193
I0312 05:08:00.428510 140028037539584 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.39575138688087463, loss=2.8347134590148926
I0312 05:08:36.858811 140028045932288 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3132043480873108, loss=2.856144428253174
I0312 05:09:13.282246 140028037539584 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.2672930657863617, loss=2.702606678009033
I0312 05:09:49.769805 140028045932288 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2695361375808716, loss=2.754368782043457
I0312 05:10:26.196926 140028037539584 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.24678704142570496, loss=2.709902048110962
I0312 05:11:02.697934 140028045932288 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.30836302042007446, loss=2.7077295780181885
I0312 05:11:39.179102 140028037539584 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3510158360004425, loss=2.5858142375946045
I0312 05:12:15.674016 140028045932288 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.23874787986278534, loss=2.5429775714874268
I0312 05:12:52.143468 140028037539584 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.24817940592765808, loss=2.5790414810180664
I0312 05:13:28.640724 140028045932288 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.22397053241729736, loss=2.4988787174224854
I0312 05:14:05.108059 140028037539584 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.26213961839675903, loss=2.5137555599212646
I0312 05:14:41.601789 140028045932288 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.20281320810317993, loss=2.562542200088501
I0312 05:15:18.088815 140028037539584 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.2089911848306656, loss=2.4696173667907715
I0312 05:15:54.557968 140028045932288 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.23044610023498535, loss=2.464923858642578
I0312 05:16:31.033938 140028037539584 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.19710102677345276, loss=2.6012518405914307
I0312 05:17:07.540270 140028045932288 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.2232498824596405, loss=2.3901448249816895
I0312 05:17:44.041634 140028037539584 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.20932771265506744, loss=2.3625617027282715
I0312 05:18:20.525697 140028045932288 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.22953754663467407, loss=2.356822967529297
I0312 05:18:57.040102 140028037539584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.18655264377593994, loss=2.3360683917999268
I0312 05:19:33.517925 140028045932288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.17303691804409027, loss=2.323054790496826
I0312 05:20:10.024339 140028037539584 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.18371367454528809, loss=2.328223466873169
I0312 05:20:11.926423 140197733885760 spec.py:321] Evaluating on the training split.
I0312 05:20:14.971505 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:22:44.208193 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 05:22:46.943696 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:25:19.616021 140197733885760 spec.py:349] Evaluating on the test split.
I0312 05:25:22.338532 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:27:41.057145 140197733885760 submission_runner.py:420] Time since start: 3537.74s, 	Step: 4607, 	{'train/accuracy': 0.5724933743476868, 'train/loss': 2.2758736610412598, 'train/bleu': 26.642778119238887, 'validation/accuracy': 0.5883870124816895, 'validation/loss': 2.16740345954895, 'validation/bleu': 23.35456405805425, 'validation/num_examples': 3000, 'test/accuracy': 0.5886119604110718, 'test/loss': 2.1475672721862793, 'test/bleu': 21.836452376702415, 'test/num_examples': 3003, 'score': 1717.9510114192963, 'total_duration': 3537.737144947052, 'accumulated_submission_time': 1717.9510114192963, 'accumulated_eval_time': 1819.5305128097534, 'accumulated_logging_time': 0.11616659164428711}
I0312 05:27:41.070816 140028045932288 logging_writer.py:48] [4607] accumulated_eval_time=1819.530513, accumulated_logging_time=0.116167, accumulated_submission_time=1717.951011, global_step=4607, preemption_count=0, score=1717.951011, test/accuracy=0.588612, test/bleu=21.836452, test/loss=2.147567, test/num_examples=3003, total_duration=3537.737145, train/accuracy=0.572493, train/bleu=26.642778, train/loss=2.275874, validation/accuracy=0.588387, validation/bleu=23.354564, validation/loss=2.167403, validation/num_examples=3000
I0312 05:28:15.188242 140028037539584 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1933363825082779, loss=2.366304397583008
I0312 05:28:51.528121 140028045932288 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.16526278853416443, loss=2.2958836555480957
I0312 05:29:27.869029 140028037539584 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.18257583677768707, loss=2.316488265991211
I0312 05:30:04.290046 140028045932288 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.16423742473125458, loss=2.2195310592651367
I0312 05:30:40.726544 140028037539584 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.19056810438632965, loss=2.2659192085266113
I0312 05:31:17.180598 140028045932288 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.19630339741706848, loss=2.2097580432891846
I0312 05:31:53.621969 140028037539584 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.17518897354602814, loss=2.211074113845825
I0312 05:32:30.100218 140028045932288 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.17478536069393158, loss=2.2640678882598877
I0312 05:33:06.575021 140028037539584 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.15040072798728943, loss=2.2820966243743896
I0312 05:33:43.057173 140028045932288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.1792723387479782, loss=2.1624414920806885
I0312 05:34:19.530424 140028037539584 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.15802925825119019, loss=2.1998865604400635
I0312 05:34:56.020874 140028045932288 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.19136491417884827, loss=2.141296148300171
I0312 05:35:32.495387 140028037539584 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.16127152740955353, loss=2.134141683578491
I0312 05:36:08.973594 140028045932288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.15836380422115326, loss=2.1834874153137207
I0312 05:36:45.443134 140028037539584 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.1867782026529312, loss=2.255276918411255
I0312 05:37:21.908781 140028045932288 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.15530622005462646, loss=2.137289524078369
I0312 05:37:58.394636 140028037539584 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.19860497117042542, loss=2.215632438659668
I0312 05:38:34.904222 140028045932288 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.16156110167503357, loss=2.213284730911255
I0312 05:39:11.401900 140028037539584 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1753271222114563, loss=2.234127998352051
I0312 05:39:47.893293 140028045932288 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.16268835961818695, loss=2.115736722946167
I0312 05:40:24.400944 140028037539584 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.18521268665790558, loss=2.161083698272705
I0312 05:41:00.878251 140028045932288 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.14746332168579102, loss=2.1763203144073486
I0312 05:41:37.408730 140028037539584 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.1707661747932434, loss=2.094633102416992
I0312 05:41:41.142781 140197733885760 spec.py:321] Evaluating on the training split.
I0312 05:41:44.200216 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:44:14.774369 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 05:44:17.503448 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:46:50.063296 140197733885760 spec.py:349] Evaluating on the test split.
I0312 05:46:52.786032 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 05:49:11.262472 140197733885760 submission_runner.py:420] Time since start: 4827.94s, 	Step: 6912, 	{'train/accuracy': 0.6066136360168457, 'train/loss': 2.0027647018432617, 'train/bleu': 29.017854244266523, 'validation/accuracy': 0.6156030297279358, 'validation/loss': 1.9432120323181152, 'validation/bleu': 25.35619328040451, 'validation/num_examples': 3000, 'test/accuracy': 0.6200685501098633, 'test/loss': 1.8967156410217285, 'test/bleu': 24.104531101372096, 'test/num_examples': 3003, 'score': 2557.9466502666473, 'total_duration': 4827.942455291748, 'accumulated_submission_time': 2557.9466502666473, 'accumulated_eval_time': 2269.650138616562, 'accumulated_logging_time': 0.13889813423156738}
I0312 05:49:11.277056 140028045932288 logging_writer.py:48] [6912] accumulated_eval_time=2269.650139, accumulated_logging_time=0.138898, accumulated_submission_time=2557.946650, global_step=6912, preemption_count=0, score=2557.946650, test/accuracy=0.620069, test/bleu=24.104531, test/loss=1.896716, test/num_examples=3003, total_duration=4827.942455, train/accuracy=0.606614, train/bleu=29.017854, train/loss=2.002765, validation/accuracy=0.615603, validation/bleu=25.356193, validation/loss=1.943212, validation/num_examples=3000
I0312 05:49:43.537034 140028037539584 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.18688659369945526, loss=2.2466039657592773
I0312 05:50:19.902031 140028045932288 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.1617860496044159, loss=2.1798858642578125
I0312 05:50:56.295783 140028037539584 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.18504375219345093, loss=2.1500673294067383
I0312 05:51:32.767611 140028045932288 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1570691168308258, loss=2.0037429332733154
I0312 05:52:09.193165 140028037539584 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.17228512465953827, loss=2.1404242515563965
I0312 05:52:45.641466 140028045932288 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.14443425834178925, loss=2.069365978240967
I0312 05:53:22.134270 140028037539584 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1633143424987793, loss=2.164280891418457
I0312 05:53:58.629646 140028045932288 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.1599397510290146, loss=2.1808533668518066
I0312 05:54:35.123540 140028037539584 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1501280516386032, loss=1.9865309000015259
I0312 05:55:11.607485 140028045932288 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.17578469216823578, loss=2.067308187484741
I0312 05:55:48.148319 140028037539584 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.180782288312912, loss=2.167759656906128
I0312 05:56:24.662748 140028045932288 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.23737773299217224, loss=2.0866434574127197
I0312 05:57:01.214460 140028037539584 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.18243369460105896, loss=2.081759452819824
I0312 05:57:37.727195 140028045932288 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.1701120138168335, loss=2.097144603729248
I0312 05:58:14.217298 140028037539584 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.16869422793388367, loss=2.0851900577545166
I0312 05:58:50.732980 140028045932288 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1956157684326172, loss=2.0455479621887207
I0312 05:59:27.252748 140028037539584 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.1651713103055954, loss=2.004131555557251
I0312 06:00:03.806106 140028045932288 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.17566609382629395, loss=2.0450117588043213
I0312 06:00:40.367389 140028037539584 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.16353321075439453, loss=2.079068660736084
I0312 06:01:16.904007 140028045932288 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.17540805041790009, loss=2.0843074321746826
I0312 06:01:53.428332 140028037539584 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15904420614242554, loss=2.0574169158935547
I0312 06:02:29.958951 140028045932288 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.15968851745128632, loss=1.9399925470352173
I0312 06:03:06.472591 140028037539584 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.18134678900241852, loss=2.162222146987915
I0312 06:03:11.302285 140197733885760 spec.py:321] Evaluating on the training split.
I0312 06:03:14.358260 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:06:00.560214 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 06:06:03.296358 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:08:30.656253 140197733885760 spec.py:349] Evaluating on the test split.
I0312 06:08:33.390933 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:10:50.028579 140197733885760 submission_runner.py:420] Time since start: 6126.71s, 	Step: 9215, 	{'train/accuracy': 0.6120672225952148, 'train/loss': 1.959139347076416, 'train/bleu': 29.916192478814693, 'validation/accuracy': 0.6295396089553833, 'validation/loss': 1.8218027353286743, 'validation/bleu': 26.2413759186834, 'validation/num_examples': 3000, 'test/accuracy': 0.6364883184432983, 'test/loss': 1.7690547704696655, 'test/bleu': 25.23227095534961, 'test/num_examples': 3003, 'score': 3397.8931415081024, 'total_duration': 6126.708575963974, 'accumulated_submission_time': 3397.8931415081024, 'accumulated_eval_time': 2728.376378059387, 'accumulated_logging_time': 0.16465997695922852}
I0312 06:10:50.043487 140028045932288 logging_writer.py:48] [9215] accumulated_eval_time=2728.376378, accumulated_logging_time=0.164660, accumulated_submission_time=3397.893142, global_step=9215, preemption_count=0, score=3397.893142, test/accuracy=0.636488, test/bleu=25.232271, test/loss=1.769055, test/num_examples=3003, total_duration=6126.708576, train/accuracy=0.612067, train/bleu=29.916192, train/loss=1.959139, validation/accuracy=0.629540, validation/bleu=26.241376, validation/loss=1.821803, validation/num_examples=3000
I0312 06:11:21.283245 140028037539584 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.16704238951206207, loss=2.015805721282959
I0312 06:11:57.645353 140028045932288 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.19519399106502533, loss=1.985942006111145
I0312 06:12:34.037118 140028037539584 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.20411349833011627, loss=2.05606746673584
I0312 06:13:10.454221 140028045932288 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.17902250587940216, loss=2.097520589828491
I0312 06:13:46.933897 140028037539584 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.20747028291225433, loss=1.9740381240844727
I0312 06:14:23.403867 140028045932288 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.16681234538555145, loss=2.064847469329834
I0312 06:14:59.881432 140028037539584 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.18827080726623535, loss=1.9613676071166992
I0312 06:15:36.372794 140028045932288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1737549602985382, loss=1.9480527639389038
I0312 06:16:12.875498 140028037539584 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2023872286081314, loss=2.046213150024414
I0312 06:16:49.364133 140028045932288 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.18111009895801544, loss=1.9192091226577759
I0312 06:17:25.834234 140028037539584 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.167051762342453, loss=2.0296757221221924
I0312 06:18:02.361899 140028045932288 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18359501659870148, loss=1.892471194267273
I0312 06:18:38.867927 140028037539584 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.17497672140598297, loss=2.0670814514160156
I0312 06:19:15.403414 140028045932288 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.19588187336921692, loss=1.9625056982040405
I0312 06:19:51.915477 140028037539584 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.18217286467552185, loss=1.9956430196762085
I0312 06:20:28.400640 140028045932288 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1769246757030487, loss=2.0994346141815186
I0312 06:21:04.902853 140028037539584 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1813085377216339, loss=1.9136073589324951
I0312 06:21:41.409629 140028045932288 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.20167995989322662, loss=2.0956904888153076
I0312 06:22:17.914304 140028037539584 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.1880701184272766, loss=1.8671208620071411
I0312 06:22:54.418502 140028045932288 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.1832863688468933, loss=1.988210916519165
I0312 06:23:30.935739 140028037539584 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.18194526433944702, loss=2.0085136890411377
I0312 06:24:07.441791 140028045932288 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1825602948665619, loss=2.0217530727386475
I0312 06:24:43.935738 140028037539584 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.19362780451774597, loss=1.9847482442855835
I0312 06:24:50.225924 140197733885760 spec.py:321] Evaluating on the training split.
I0312 06:24:53.283958 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:28:02.206566 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 06:28:04.929734 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:30:41.175300 140197733885760 spec.py:349] Evaluating on the test split.
I0312 06:30:43.896286 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:33:09.903320 140197733885760 submission_runner.py:420] Time since start: 7466.58s, 	Step: 11519, 	{'train/accuracy': 0.6196917295455933, 'train/loss': 1.887145757675171, 'train/bleu': 30.07146527648772, 'validation/accuracy': 0.6392481327056885, 'validation/loss': 1.7518912553787231, 'validation/bleu': 26.88392878478057, 'validation/num_examples': 3000, 'test/accuracy': 0.6449480056762695, 'test/loss': 1.7010329961776733, 'test/bleu': 25.399824245978028, 'test/num_examples': 3003, 'score': 4237.998886823654, 'total_duration': 7466.583305358887, 'accumulated_submission_time': 4237.998886823654, 'accumulated_eval_time': 3228.05371260643, 'accumulated_logging_time': 0.18877077102661133}
I0312 06:33:09.918328 140028045932288 logging_writer.py:48] [11519] accumulated_eval_time=3228.053713, accumulated_logging_time=0.188771, accumulated_submission_time=4237.998887, global_step=11519, preemption_count=0, score=4237.998887, test/accuracy=0.644948, test/bleu=25.399824, test/loss=1.701033, test/num_examples=3003, total_duration=7466.583305, train/accuracy=0.619692, train/bleu=30.071465, train/loss=1.887146, validation/accuracy=0.639248, validation/bleu=26.883929, validation/loss=1.751891, validation/num_examples=3000
I0312 06:33:39.659353 140028037539584 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.19617114961147308, loss=1.8582898378372192
I0312 06:34:15.969499 140028045932288 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.16742776334285736, loss=1.9844164848327637
I0312 06:34:52.319974 140028037539584 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.26806893944740295, loss=1.8809294700622559
I0312 06:35:28.716266 140028045932288 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.18868488073349, loss=2.0458505153656006
I0312 06:36:05.161449 140028037539584 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.24352167546749115, loss=1.917297601699829
I0312 06:36:41.600246 140028045932288 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.1727440059185028, loss=1.9128416776657104
I0312 06:37:18.060463 140028037539584 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1895464062690735, loss=1.998325228691101
I0312 06:37:54.505790 140028045932288 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.25966784358024597, loss=1.9732307195663452
I0312 06:38:30.978276 140028037539584 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.22509978711605072, loss=1.8585853576660156
I0312 06:39:07.468794 140028045932288 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.19753746688365936, loss=1.988187551498413
I0312 06:39:43.967654 140028037539584 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.19299574196338654, loss=1.9044908285140991
I0312 06:40:20.477659 140028045932288 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.26574626564979553, loss=1.9485822916030884
I0312 06:40:56.977992 140028037539584 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1785714477300644, loss=1.9045549631118774
I0312 06:41:33.463217 140028045932288 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.2446547895669937, loss=1.9572691917419434
I0312 06:42:09.969851 140028037539584 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2538924813270569, loss=1.8901299238204956
I0312 06:42:46.437896 140028045932288 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2537749409675598, loss=1.928451657295227
I0312 06:43:22.940109 140028037539584 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.25322845578193665, loss=2.0437965393066406
I0312 06:43:59.454791 140028045932288 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.21802939474582672, loss=1.977273941040039
I0312 06:44:35.934622 140028037539584 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.2144518345594406, loss=1.9545236825942993
I0312 06:45:12.427669 140028045932288 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16347837448120117, loss=1.9281028509140015
I0312 06:45:48.892151 140028037539584 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.22107595205307007, loss=1.9339895248413086
I0312 06:46:25.402770 140028045932288 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2020670473575592, loss=1.9219893217086792
I0312 06:47:01.906753 140028037539584 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.19050057232379913, loss=1.8847668170928955
I0312 06:47:10.004265 140197733885760 spec.py:321] Evaluating on the training split.
I0312 06:47:13.049201 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:50:15.779328 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 06:50:18.493352 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:52:36.217941 140197733885760 spec.py:349] Evaluating on the test split.
I0312 06:52:38.937825 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 06:54:44.670902 140197733885760 submission_runner.py:420] Time since start: 8761.35s, 	Step: 13824, 	{'train/accuracy': 0.6305181980133057, 'train/loss': 1.8076016902923584, 'train/bleu': 30.405409117211498, 'validation/accuracy': 0.6429430246353149, 'validation/loss': 1.710771918296814, 'validation/bleu': 27.113700672480107, 'validation/num_examples': 3000, 'test/accuracy': 0.6524780988693237, 'test/loss': 1.6474509239196777, 'test/bleu': 26.427204107878673, 'test/num_examples': 3003, 'score': 5078.007721185684, 'total_duration': 8761.350906610489, 'accumulated_submission_time': 5078.007721185684, 'accumulated_eval_time': 3682.7203097343445, 'accumulated_logging_time': 0.214097261428833}
I0312 06:54:44.686147 140028045932288 logging_writer.py:48] [13824] accumulated_eval_time=3682.720310, accumulated_logging_time=0.214097, accumulated_submission_time=5078.007721, global_step=13824, preemption_count=0, score=5078.007721, test/accuracy=0.652478, test/bleu=26.427204, test/loss=1.647451, test/num_examples=3003, total_duration=8761.350907, train/accuracy=0.630518, train/bleu=30.405409, train/loss=1.807602, validation/accuracy=0.642943, validation/bleu=27.113701, validation/loss=1.710772, validation/num_examples=3000
I0312 06:55:12.630148 140028037539584 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1939903348684311, loss=1.9362454414367676
I0312 06:55:48.965831 140028045932288 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1899593025445938, loss=1.8591722249984741
I0312 06:56:25.352689 140028037539584 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.28497427701950073, loss=1.880200982093811
I0312 06:57:01.769240 140028045932288 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2218967080116272, loss=1.948725938796997
I0312 06:57:38.196273 140028037539584 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1789519488811493, loss=1.8664427995681763
I0312 06:58:14.644833 140028045932288 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.20045402646064758, loss=2.021880626678467
I0312 06:58:51.134088 140028037539584 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.18691398203372955, loss=1.9748066663742065
I0312 06:59:27.664972 140028045932288 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.19554853439331055, loss=1.8745331764221191
I0312 07:00:04.154665 140028037539584 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17413194477558136, loss=1.8735291957855225
I0312 07:00:40.652218 140028045932288 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2173546701669693, loss=1.928542137145996
I0312 07:01:17.129204 140028037539584 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.22916612029075623, loss=1.9125522375106812
I0312 07:01:53.630850 140028045932288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.19526949524879456, loss=1.9357248544692993
I0312 07:02:30.166877 140028037539584 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.17317423224449158, loss=1.8637515306472778
I0312 07:03:06.658551 140028045932288 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.20120510458946228, loss=1.9112440347671509
I0312 07:03:43.151225 140028037539584 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.24223151803016663, loss=1.9193363189697266
I0312 07:04:19.656013 140028045932288 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.18475069105625153, loss=1.9276537895202637
I0312 07:04:56.148200 140028037539584 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18922081589698792, loss=1.8741029500961304
I0312 07:05:32.657660 140028045932288 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.21002843976020813, loss=1.9642276763916016
I0312 07:06:09.161756 140028037539584 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1784595549106598, loss=1.8770583868026733
I0312 07:06:45.668416 140028045932288 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.20422831177711487, loss=1.8951536417007446
I0312 07:07:22.168591 140028037539584 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.22421526908874512, loss=1.9599252939224243
I0312 07:07:58.684620 140028045932288 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.19351571798324585, loss=1.9563443660736084
I0312 07:08:35.194032 140028037539584 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1856655478477478, loss=1.8978080749511719
I0312 07:08:44.772607 140197733885760 spec.py:321] Evaluating on the training split.
I0312 07:08:47.825462 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:12:42.256424 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 07:12:44.957324 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:15:11.320567 140197733885760 spec.py:349] Evaluating on the test split.
I0312 07:15:14.056146 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:17:32.376501 140197733885760 submission_runner.py:420] Time since start: 10129.06s, 	Step: 16128, 	{'train/accuracy': 0.6288556456565857, 'train/loss': 1.8313424587249756, 'train/bleu': 30.45977095033596, 'validation/accuracy': 0.6467743515968323, 'validation/loss': 1.6920876502990723, 'validation/bleu': 27.353765269924896, 'validation/num_examples': 3000, 'test/accuracy': 0.6546278595924377, 'test/loss': 1.6145895719528198, 'test/bleu': 26.11191716618898, 'test/num_examples': 3003, 'score': 5918.018773555756, 'total_duration': 10129.056498527527, 'accumulated_submission_time': 5918.018773555756, 'accumulated_eval_time': 4210.324150323868, 'accumulated_logging_time': 0.2381901741027832}
I0312 07:17:32.392383 140028045932288 logging_writer.py:48] [16128] accumulated_eval_time=4210.324150, accumulated_logging_time=0.238190, accumulated_submission_time=5918.018774, global_step=16128, preemption_count=0, score=5918.018774, test/accuracy=0.654628, test/bleu=26.111917, test/loss=1.614590, test/num_examples=3003, total_duration=10129.056499, train/accuracy=0.628856, train/bleu=30.459771, train/loss=1.831342, validation/accuracy=0.646774, validation/bleu=27.353765, validation/loss=1.692088, validation/num_examples=3000
I0312 07:17:58.890811 140028037539584 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.23309628665447235, loss=1.8227934837341309
I0312 07:18:35.196746 140028045932288 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.23193292319774628, loss=1.8497376441955566
I0312 07:19:11.571029 140028037539584 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.20610499382019043, loss=1.905712604522705
I0312 07:19:47.958287 140028045932288 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.29089853167533875, loss=1.8826979398727417
I0312 07:20:24.418131 140028037539584 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.20648297667503357, loss=1.8835407495498657
I0312 07:21:00.848066 140028045932288 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2099909782409668, loss=1.8298522233963013
I0312 07:21:37.293704 140028037539584 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.2712761163711548, loss=1.86956787109375
I0312 07:22:13.776188 140028045932288 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.20412680506706238, loss=1.9104046821594238
I0312 07:22:50.248380 140028037539584 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17713448405265808, loss=1.945279598236084
I0312 07:23:26.726754 140028045932288 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.18155881762504578, loss=2.02748441696167
I0312 07:24:03.227608 140028037539584 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2330305427312851, loss=1.833776593208313
I0312 07:24:39.759570 140028045932288 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.34055259823799133, loss=1.9013280868530273
I0312 07:25:16.257578 140028037539584 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.20663270354270935, loss=1.8826042413711548
I0312 07:25:52.727314 140028045932288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.23360905051231384, loss=1.8366085290908813
I0312 07:26:29.224659 140028037539584 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.1798960566520691, loss=1.8543256521224976
I0312 07:27:05.756042 140028045932288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2530091404914856, loss=1.8907616138458252
I0312 07:27:42.279322 140028037539584 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.22472403943538666, loss=1.8461521863937378
I0312 07:28:18.804453 140028045932288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.21531164646148682, loss=1.89460027217865
I0312 07:28:55.335543 140028037539584 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.19905999302864075, loss=1.815306305885315
I0312 07:29:31.860939 140028045932288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.18849867582321167, loss=1.9244577884674072
I0312 07:30:08.381215 140028037539584 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1877562254667282, loss=1.872441291809082
I0312 07:30:44.881788 140028045932288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.2166045904159546, loss=1.781040906906128
I0312 07:31:21.372125 140028037539584 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2624984085559845, loss=1.8262403011322021
I0312 07:31:32.409880 140197733885760 spec.py:321] Evaluating on the training split.
I0312 07:31:35.468182 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:34:30.030805 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 07:34:32.747106 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:37:01.612371 140197733885760 spec.py:349] Evaluating on the test split.
I0312 07:37:04.336098 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:39:20.565306 140197733885760 submission_runner.py:420] Time since start: 11437.25s, 	Step: 18432, 	{'train/accuracy': 0.6309792995452881, 'train/loss': 1.8097506761550903, 'train/bleu': 30.742780959834842, 'validation/accuracy': 0.6506924629211426, 'validation/loss': 1.6586767435073853, 'validation/bleu': 27.71685002958468, 'validation/num_examples': 3000, 'test/accuracy': 0.661158561706543, 'test/loss': 1.5822104215621948, 'test/bleu': 26.921020440753615, 'test/num_examples': 3003, 'score': 6757.959350347519, 'total_duration': 11437.245300531387, 'accumulated_submission_time': 6757.959350347519, 'accumulated_eval_time': 4678.479519367218, 'accumulated_logging_time': 0.26436829566955566}
I0312 07:39:20.582125 140028045932288 logging_writer.py:48] [18432] accumulated_eval_time=4678.479519, accumulated_logging_time=0.264368, accumulated_submission_time=6757.959350, global_step=18432, preemption_count=0, score=6757.959350, test/accuracy=0.661159, test/bleu=26.921020, test/loss=1.582210, test/num_examples=3003, total_duration=11437.245301, train/accuracy=0.630979, train/bleu=30.742781, train/loss=1.809751, validation/accuracy=0.650692, validation/bleu=27.716850, validation/loss=1.658677, validation/num_examples=3000
I0312 07:39:45.583380 140028037539584 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.21943989396095276, loss=1.9572474956512451
I0312 07:40:21.916851 140028045932288 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.2579745054244995, loss=1.8786277770996094
I0312 07:40:58.268611 140028037539584 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.2286672741174698, loss=1.8663156032562256
I0312 07:41:34.702612 140028045932288 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.21179775893688202, loss=1.9065053462982178
I0312 07:42:11.104668 140028037539584 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.2792968451976776, loss=1.8740317821502686
I0312 07:42:47.566151 140028045932288 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.19104786217212677, loss=1.9164962768554688
I0312 07:43:24.029006 140028037539584 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.2135990560054779, loss=1.9203400611877441
I0312 07:44:00.499496 140028045932288 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.17767271399497986, loss=1.7975224256515503
I0312 07:44:36.994374 140028037539584 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2041025012731552, loss=1.8898406028747559
I0312 07:45:13.490198 140028045932288 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.25414150953292847, loss=1.818628191947937
I0312 07:45:49.995612 140028037539584 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8870879411697388, loss=1.904344916343689
I0312 07:46:26.464901 140028045932288 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.20382454991340637, loss=1.9470683336257935
I0312 07:47:02.947630 140028037539584 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2315177619457245, loss=1.8457481861114502
I0312 07:47:39.429220 140028045932288 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17765188217163086, loss=1.882154107093811
I0312 07:48:15.904422 140028037539584 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.19381573796272278, loss=1.861106038093567
I0312 07:48:52.390988 140028045932288 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.24116425216197968, loss=1.8825032711029053
I0312 07:49:28.889528 140028037539584 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.1849648356437683, loss=1.8177858591079712
I0312 07:50:05.409595 140028045932288 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.20742060244083405, loss=1.882234811782837
I0312 07:50:41.919299 140028037539584 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.19372642040252686, loss=1.9187021255493164
I0312 07:51:18.407555 140028045932288 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.20954754948616028, loss=1.8643369674682617
I0312 07:51:54.934772 140028037539584 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.19141046702861786, loss=1.8388512134552002
I0312 07:52:31.422836 140028045932288 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.20090076327323914, loss=1.7451503276824951
I0312 07:53:07.940329 140028037539584 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.20141556859016418, loss=1.9007943868637085
I0312 07:53:20.794579 140197733885760 spec.py:321] Evaluating on the training split.
I0312 07:53:23.837693 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 07:57:39.533633 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 07:57:42.246073 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:00:12.059321 140197733885760 spec.py:349] Evaluating on the test split.
I0312 08:00:14.787061 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:02:42.258370 140197733885760 submission_runner.py:420] Time since start: 12838.94s, 	Step: 20737, 	{'train/accuracy': 0.6350600719451904, 'train/loss': 1.7639600038528442, 'train/bleu': 31.086818959270097, 'validation/accuracy': 0.6527507305145264, 'validation/loss': 1.6468219757080078, 'validation/bleu': 27.800610918108475, 'validation/num_examples': 3000, 'test/accuracy': 0.6625065803527832, 'test/loss': 1.5711008310317993, 'test/bleu': 27.02671804348759, 'test/num_examples': 3003, 'score': 7598.0957589149475, 'total_duration': 12838.938373327255, 'accumulated_submission_time': 7598.0957589149475, 'accumulated_eval_time': 5239.9432628154755, 'accumulated_logging_time': 0.2907545566558838}
I0312 08:02:42.275900 140028045932288 logging_writer.py:48] [20737] accumulated_eval_time=5239.943263, accumulated_logging_time=0.290755, accumulated_submission_time=7598.095759, global_step=20737, preemption_count=0, score=7598.095759, test/accuracy=0.662507, test/bleu=27.026718, test/loss=1.571101, test/num_examples=3003, total_duration=12838.938373, train/accuracy=0.635060, train/bleu=31.086819, train/loss=1.763960, validation/accuracy=0.652751, validation/bleu=27.800611, validation/loss=1.646822, validation/num_examples=3000
I0312 08:03:05.453679 140028037539584 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.21672323346138, loss=1.8000900745391846
I0312 08:03:41.719697 140028045932288 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.21489150822162628, loss=1.8449922800064087
I0312 08:04:18.040317 140028037539584 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.17029879987239838, loss=1.8415383100509644
I0312 08:04:54.413005 140028045932288 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.2261650562286377, loss=1.848543405532837
I0312 08:05:30.789262 140028037539584 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.19599774479866028, loss=1.879248023033142
I0312 08:06:07.230957 140028045932288 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2379477471113205, loss=1.908098578453064
I0312 08:06:43.655948 140028037539584 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.1866198182106018, loss=1.802045464515686
I0312 08:07:20.116502 140028045932288 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.18899452686309814, loss=1.9449913501739502
I0312 08:07:56.574010 140028037539584 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.2193818837404251, loss=1.8581544160842896
I0312 08:08:33.071109 140028045932288 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.23376843333244324, loss=1.8573975563049316
I0312 08:09:09.548207 140028037539584 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.20007829368114471, loss=1.9125057458877563
I0312 08:09:46.033634 140028045932288 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.23279163241386414, loss=1.9556597471237183
I0312 08:10:22.525679 140028037539584 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.22941499948501587, loss=1.842159390449524
I0312 08:10:59.041680 140028045932288 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.215249165892601, loss=1.8975619077682495
I0312 08:11:35.565467 140028037539584 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.20759360492229462, loss=1.8998509645462036
I0312 08:12:12.089096 140028045932288 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.2064211219549179, loss=1.9120712280273438
I0312 08:12:48.610954 140028037539584 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.22296690940856934, loss=1.9138896465301514
I0312 08:13:25.099253 140028045932288 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.19819915294647217, loss=1.9067614078521729
I0312 08:14:01.637669 140028037539584 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.22123679518699646, loss=1.8231748342514038
I0312 08:14:38.145349 140028045932288 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.2615474462509155, loss=1.8653379678726196
I0312 08:15:14.661999 140028037539584 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.1848503202199936, loss=1.9060356616973877
I0312 08:15:51.242151 140028045932288 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.21461455523967743, loss=1.8878676891326904
I0312 08:16:27.762753 140028037539584 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.19744938611984253, loss=1.8228344917297363
I0312 08:16:42.444998 140197733885760 spec.py:321] Evaluating on the training split.
I0312 08:16:45.491053 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:20:31.203958 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 08:20:33.913350 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:23:03.112006 140197733885760 spec.py:349] Evaluating on the test split.
I0312 08:23:05.842186 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:25:14.624323 140197733885760 submission_runner.py:420] Time since start: 14191.30s, 	Step: 23042, 	{'train/accuracy': 0.6321536302566528, 'train/loss': 1.7879424095153809, 'train/bleu': 30.843176318892503, 'validation/accuracy': 0.6557885408401489, 'validation/loss': 1.6294809579849243, 'validation/bleu': 27.80706443069435, 'validation/num_examples': 3000, 'test/accuracy': 0.6654813885688782, 'test/loss': 1.552293062210083, 'test/bleu': 27.471505111759694, 'test/num_examples': 3003, 'score': 8438.187176465988, 'total_duration': 14191.30432844162, 'accumulated_submission_time': 8438.187176465988, 'accumulated_eval_time': 5752.122546672821, 'accumulated_logging_time': 0.31887221336364746}
I0312 08:25:14.641075 140028045932288 logging_writer.py:48] [23042] accumulated_eval_time=5752.122547, accumulated_logging_time=0.318872, accumulated_submission_time=8438.187176, global_step=23042, preemption_count=0, score=8438.187176, test/accuracy=0.665481, test/bleu=27.471505, test/loss=1.552293, test/num_examples=3003, total_duration=14191.304328, train/accuracy=0.632154, train/bleu=30.843176, train/loss=1.787942, validation/accuracy=0.655789, validation/bleu=27.807064, validation/loss=1.629481, validation/num_examples=3000
I0312 08:25:36.024698 140028037539584 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.21679504215717316, loss=1.8202388286590576
I0312 08:26:12.389514 140028045932288 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.22278448939323425, loss=1.935860276222229
I0312 08:26:48.725301 140028037539584 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.21218480169773102, loss=1.813879370689392
I0312 08:27:25.133352 140028045932288 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.27858754992485046, loss=1.9360181093215942
I0312 08:28:01.527865 140028037539584 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.20382872223854065, loss=1.819143295288086
I0312 08:28:38.001151 140028045932288 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.344687819480896, loss=1.9308604001998901
I0312 08:29:14.458878 140028037539584 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.18789413571357727, loss=1.7567243576049805
I0312 08:29:50.928888 140028045932288 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.22213082015514374, loss=1.8209731578826904
I0312 08:30:27.413699 140028037539584 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.2538389563560486, loss=1.9195098876953125
I0312 08:31:03.916127 140028045932288 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.19316145777702332, loss=1.8263481855392456
I0312 08:31:40.420063 140028037539584 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.28567805886268616, loss=1.8243201971054077
I0312 08:32:16.884016 140028045932288 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.21510450541973114, loss=1.9091213941574097
I0312 08:32:53.382536 140028037539584 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2113659530878067, loss=1.8703370094299316
I0312 08:33:29.913539 140028045932288 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.23184016346931458, loss=1.8694725036621094
I0312 08:34:06.421025 140028037539584 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.22499248385429382, loss=1.7698403596878052
I0312 08:34:42.926078 140028045932288 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.21198613941669464, loss=1.8850059509277344
I0312 08:35:19.486047 140028037539584 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.22189565002918243, loss=1.8937255144119263
I0312 08:35:56.012317 140028045932288 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.2292209416627884, loss=1.8402493000030518
I0312 08:36:32.505884 140028037539584 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.2153048813343048, loss=1.8586182594299316
I0312 08:37:09.005752 140028045932288 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.19325922429561615, loss=1.835032343864441
I0312 08:37:45.500825 140028037539584 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.1920965313911438, loss=1.805881381034851
I0312 08:38:22.021489 140028045932288 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.23737195134162903, loss=1.8633023500442505
I0312 08:38:58.569645 140028037539584 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.2930099666118622, loss=1.8435542583465576
I0312 08:39:14.707413 140197733885760 spec.py:321] Evaluating on the training split.
I0312 08:39:17.759793 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:42:50.652982 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 08:42:53.361213 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:45:52.753406 140197733885760 spec.py:349] Evaluating on the test split.
I0312 08:45:55.465198 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 08:48:15.569944 140197733885760 submission_runner.py:420] Time since start: 15572.25s, 	Step: 25346, 	{'train/accuracy': 0.6534295678138733, 'train/loss': 1.6287175416946411, 'train/bleu': 31.96841674854174, 'validation/accuracy': 0.6557636857032776, 'validation/loss': 1.620172142982483, 'validation/bleu': 27.763272642865847, 'validation/num_examples': 3000, 'test/accuracy': 0.6647841930389404, 'test/loss': 1.544036865234375, 'test/bleu': 26.938452691417634, 'test/num_examples': 3003, 'score': 9278.177888393402, 'total_duration': 15572.249928712845, 'accumulated_submission_time': 9278.177888393402, 'accumulated_eval_time': 6292.985018253326, 'accumulated_logging_time': 0.34461355209350586}
I0312 08:48:15.590417 140028045932288 logging_writer.py:48] [25346] accumulated_eval_time=6292.985018, accumulated_logging_time=0.344614, accumulated_submission_time=9278.177888, global_step=25346, preemption_count=0, score=9278.177888, test/accuracy=0.664784, test/bleu=26.938453, test/loss=1.544037, test/num_examples=3003, total_duration=15572.249929, train/accuracy=0.653430, train/bleu=31.968417, train/loss=1.628718, validation/accuracy=0.655764, validation/bleu=27.763273, validation/loss=1.620172, validation/num_examples=3000
I0312 08:48:35.517954 140028037539584 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.19549444317817688, loss=1.8638181686401367
I0312 08:49:11.793603 140028045932288 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.20152883231639862, loss=1.737320065498352
I0312 08:49:48.117940 140028037539584 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.27310967445373535, loss=1.7633776664733887
I0312 08:50:24.492063 140028045932288 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.2043900340795517, loss=1.8007680177688599
I0312 08:51:00.914456 140028037539584 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.19584430754184723, loss=1.875169277191162
I0312 08:51:37.367070 140028045932288 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.21887336671352386, loss=1.8186239004135132
I0312 08:52:13.844258 140028037539584 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.21371488273143768, loss=1.759017825126648
I0312 08:52:50.292477 140028045932288 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.20598340034484863, loss=1.9042809009552002
I0312 08:53:26.806950 140028037539584 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.18478426337242126, loss=1.8143430948257446
I0312 08:54:03.246398 140028045932288 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.24724546074867249, loss=1.8136844635009766
I0312 08:54:39.726847 140028037539584 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.21436890959739685, loss=1.7674318552017212
I0312 08:55:16.186817 140028045932288 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.23175309598445892, loss=1.837282657623291
I0312 08:55:52.696239 140028037539584 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19222429394721985, loss=1.8838382959365845
I0312 08:56:29.174260 140028045932288 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.22270679473876953, loss=1.7855130434036255
I0312 08:57:05.687982 140028037539584 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.22052381932735443, loss=1.8404841423034668
I0312 08:57:42.202517 140028045932288 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.23231202363967896, loss=1.8225113153457642
I0312 08:58:18.709856 140028037539584 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.19157995283603668, loss=1.777266263961792
I0312 08:58:55.229816 140028045932288 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.21766944229602814, loss=1.8232779502868652
I0312 08:59:31.725188 140028037539584 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2409026324748993, loss=1.8460979461669922
I0312 09:00:08.234550 140028045932288 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.18524707853794098, loss=1.8033411502838135
I0312 09:00:44.732520 140028037539584 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2745155990123749, loss=1.7728081941604614
I0312 09:01:21.262627 140028045932288 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2467569261789322, loss=1.8288390636444092
I0312 09:01:57.783633 140028037539584 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2444804310798645, loss=1.7745709419250488
I0312 09:02:15.732356 140197733885760 spec.py:321] Evaluating on the training split.
I0312 09:02:18.779964 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:06:25.156957 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 09:06:27.882938 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:09:26.120291 140197733885760 spec.py:349] Evaluating on the test split.
I0312 09:09:28.850596 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:11:54.082147 140197733885760 submission_runner.py:420] Time since start: 16990.76s, 	Step: 27651, 	{'train/accuracy': 0.6354812383651733, 'train/loss': 1.7555207014083862, 'train/bleu': 30.85710459621641, 'validation/accuracy': 0.655738890171051, 'validation/loss': 1.6140739917755127, 'validation/bleu': 27.76954025263315, 'validation/num_examples': 3000, 'test/accuracy': 0.6674219965934753, 'test/loss': 1.5395432710647583, 'test/bleu': 27.347128709992166, 'test/num_examples': 3003, 'score': 10118.242518424988, 'total_duration': 16990.762141942978, 'accumulated_submission_time': 10118.242518424988, 'accumulated_eval_time': 6871.334754228592, 'accumulated_logging_time': 0.3750591278076172}
I0312 09:11:54.099999 140028045932288 logging_writer.py:48] [27651] accumulated_eval_time=6871.334754, accumulated_logging_time=0.375059, accumulated_submission_time=10118.242518, global_step=27651, preemption_count=0, score=10118.242518, test/accuracy=0.667422, test/bleu=27.347129, test/loss=1.539543, test/num_examples=3003, total_duration=16990.762142, train/accuracy=0.635481, train/bleu=30.857105, train/loss=1.755521, validation/accuracy=0.655739, validation/bleu=27.769540, validation/loss=1.614074, validation/num_examples=3000
I0312 09:12:12.188509 140028037539584 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.20209208130836487, loss=1.8285883665084839
I0312 09:12:48.486332 140028045932288 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.21543140709400177, loss=1.8016835451126099
I0312 09:13:24.844379 140028037539584 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.20263688266277313, loss=1.749326467514038
I0312 09:14:01.194375 140028045932288 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.21286962926387787, loss=1.8503228425979614
I0312 09:14:37.607392 140028037539584 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.19960427284240723, loss=1.8399311304092407
I0312 09:15:14.030636 140028045932288 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.18308676779270172, loss=1.8590282201766968
I0312 09:15:50.488561 140028037539584 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2282576709985733, loss=1.76520574092865
I0312 09:16:26.916420 140028045932288 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.3275461494922638, loss=1.8577001094818115
I0312 09:17:03.397980 140028037539584 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.20178565382957458, loss=1.9027888774871826
I0312 09:17:39.883667 140028045932288 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.20633605122566223, loss=1.8458000421524048
I0312 09:18:16.346036 140028037539584 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.17387723922729492, loss=1.7602072954177856
I0312 09:18:52.842224 140028045932288 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.47922489047050476, loss=1.9239275455474854
I0312 09:19:29.306909 140028037539584 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.19005127251148224, loss=1.9192818403244019
I0312 09:20:05.804768 140028045932288 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2777934968471527, loss=1.7693895101547241
I0312 09:20:42.304970 140028037539584 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.21997177600860596, loss=1.8778462409973145
I0312 09:21:18.821281 140028045932288 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.24151140451431274, loss=1.8446249961853027
I0312 09:21:55.314788 140028037539584 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.2284064143896103, loss=1.8595913648605347
I0312 09:22:31.807399 140028045932288 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.49759551882743835, loss=1.880001187324524
I0312 09:23:08.262945 140028037539584 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2067488580942154, loss=1.8564223051071167
I0312 09:23:44.769410 140028045932288 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.22233846783638, loss=1.7687768936157227
I0312 09:24:21.259875 140028037539584 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2539495527744293, loss=1.8111584186553955
I0312 09:24:57.785277 140028045932288 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.2468757927417755, loss=1.8742717504501343
I0312 09:25:34.280262 140028037539584 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.25491398572921753, loss=1.7650115489959717
I0312 09:25:54.426367 140197733885760 spec.py:321] Evaluating on the training split.
I0312 09:25:57.471626 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:29:50.816604 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 09:29:53.531701 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:32:33.123692 140197733885760 spec.py:349] Evaluating on the test split.
I0312 09:32:35.834782 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:34:52.379328 140197733885760 submission_runner.py:420] Time since start: 18369.06s, 	Step: 29957, 	{'train/accuracy': 0.6409510970115662, 'train/loss': 1.7328439950942993, 'train/bleu': 30.828748013400578, 'validation/accuracy': 0.6587767004966736, 'validation/loss': 1.6031513214111328, 'validation/bleu': 27.44061347257815, 'validation/num_examples': 3000, 'test/accuracy': 0.6674801111221313, 'test/loss': 1.530782699584961, 'test/bleu': 26.982702387093063, 'test/num_examples': 3003, 'score': 10958.493296861649, 'total_duration': 18369.059331417084, 'accumulated_submission_time': 10958.493296861649, 'accumulated_eval_time': 7409.2876698970795, 'accumulated_logging_time': 0.4019620418548584}
I0312 09:34:52.397814 140028045932288 logging_writer.py:48] [29957] accumulated_eval_time=7409.287670, accumulated_logging_time=0.401962, accumulated_submission_time=10958.493297, global_step=29957, preemption_count=0, score=10958.493297, test/accuracy=0.667480, test/bleu=26.982702, test/loss=1.530783, test/num_examples=3003, total_duration=18369.059331, train/accuracy=0.640951, train/bleu=30.828748, train/loss=1.732844, validation/accuracy=0.658777, validation/bleu=27.440613, validation/loss=1.603151, validation/num_examples=3000
I0312 09:35:08.338870 140028037539584 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.19241420924663544, loss=1.7607817649841309
I0312 09:35:44.667717 140028045932288 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19808822870254517, loss=1.7956397533416748
I0312 09:36:21.000044 140028037539584 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.18407827615737915, loss=1.8392295837402344
I0312 09:36:57.361675 140028045932288 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.19516855478286743, loss=1.8122780323028564
I0312 09:37:33.734258 140028037539584 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.20580127835273743, loss=1.8139771223068237
I0312 09:38:10.165311 140028045932288 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.19276560842990875, loss=1.8140093088150024
I0312 09:38:46.574169 140028037539584 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.205399751663208, loss=1.7246317863464355
I0312 09:39:23.049828 140028045932288 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.24389663338661194, loss=1.7786805629730225
I0312 09:39:59.516157 140028037539584 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.23670458793640137, loss=1.82326340675354
I0312 09:40:35.983572 140028045932288 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.1840350180864334, loss=1.8235582113265991
I0312 09:41:12.456240 140028037539584 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.19593682885169983, loss=1.7989206314086914
I0312 09:41:48.945817 140028045932288 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.21113285422325134, loss=1.8468152284622192
I0312 09:42:25.413178 140028037539584 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.1931198537349701, loss=1.8113030195236206
I0312 09:43:01.880376 140028045932288 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.1938745379447937, loss=1.8692409992218018
I0312 09:43:38.390159 140028037539584 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20393307507038116, loss=1.841555118560791
I0312 09:44:14.873022 140028045932288 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.21350400149822235, loss=1.7694716453552246
I0312 09:44:51.366563 140028037539584 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.2160273790359497, loss=1.7323734760284424
I0312 09:45:27.875279 140028045932288 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.21285639703273773, loss=1.865923523902893
I0312 09:46:04.393966 140028037539584 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.34121233224868774, loss=1.8504793643951416
I0312 09:46:40.886584 140028045932288 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1960393190383911, loss=1.7478994131088257
I0312 09:47:17.381506 140028037539584 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.19256554543972015, loss=1.8032102584838867
I0312 09:47:53.886572 140028045932288 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.1851411759853363, loss=1.8417726755142212
I0312 09:48:30.390728 140028037539584 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.20124635100364685, loss=1.8141757249832153
I0312 09:48:52.743326 140197733885760 spec.py:321] Evaluating on the training split.
I0312 09:48:55.790903 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:52:47.277452 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 09:52:49.996075 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:55:26.284429 140197733885760 spec.py:349] Evaluating on the test split.
I0312 09:55:29.014655 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 09:57:59.275076 140197733885760 submission_runner.py:420] Time since start: 19755.96s, 	Step: 32263, 	{'train/accuracy': 0.6501732468605042, 'train/loss': 1.671488881111145, 'train/bleu': 31.79653325067196, 'validation/accuracy': 0.6608969569206238, 'validation/loss': 1.5915027856826782, 'validation/bleu': 27.934553907088265, 'validation/num_examples': 3000, 'test/accuracy': 0.6707687377929688, 'test/loss': 1.5176211595535278, 'test/bleu': 27.344738706710345, 'test/num_examples': 3003, 'score': 11798.761286258698, 'total_duration': 19755.955070257187, 'accumulated_submission_time': 11798.761286258698, 'accumulated_eval_time': 7955.819365501404, 'accumulated_logging_time': 0.4307701587677002}
I0312 09:57:59.293524 140028045932288 logging_writer.py:48] [32263] accumulated_eval_time=7955.819366, accumulated_logging_time=0.430770, accumulated_submission_time=11798.761286, global_step=32263, preemption_count=0, score=11798.761286, test/accuracy=0.670769, test/bleu=27.344739, test/loss=1.517621, test/num_examples=3003, total_duration=19755.955070, train/accuracy=0.650173, train/bleu=31.796533, train/loss=1.671489, validation/accuracy=0.660897, validation/bleu=27.934554, validation/loss=1.591503, validation/num_examples=3000
I0312 09:58:13.082659 140028037539584 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.19431567192077637, loss=1.6990481615066528
I0312 09:58:49.409132 140028045932288 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2244386374950409, loss=1.7860925197601318
I0312 09:59:25.776605 140028037539584 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.21269568800926208, loss=1.7607457637786865
I0312 10:00:02.119498 140028045932288 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.22579871118068695, loss=1.8453434705734253
I0312 10:00:38.557196 140028037539584 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.19660162925720215, loss=1.8175523281097412
I0312 10:01:14.975320 140028045932288 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.248385950922966, loss=1.8939130306243896
I0312 10:01:51.434888 140028037539584 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.1826772689819336, loss=1.8018611669540405
I0312 10:02:27.899529 140028045932288 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.20767872035503387, loss=1.7913563251495361
I0312 10:03:04.371733 140028037539584 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.18302211165428162, loss=1.7886332273483276
I0312 10:03:40.851088 140028045932288 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.23524779081344604, loss=1.8387854099273682
I0312 10:04:17.321185 140028037539584 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.29138392210006714, loss=1.8046960830688477
I0312 10:04:53.812393 140028045932288 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2046322375535965, loss=1.8545809984207153
I0312 10:05:30.290251 140028037539584 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.20145122706890106, loss=1.9026060104370117
I0312 10:06:06.815767 140028045932288 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2080930769443512, loss=1.8335843086242676
I0312 10:06:43.319627 140028037539584 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.18386735022068024, loss=1.7497249841690063
I0312 10:07:19.816523 140028045932288 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.18122218549251556, loss=1.7396416664123535
I0312 10:07:56.315318 140028037539584 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.21464382112026215, loss=1.7876956462860107
I0312 10:08:32.807190 140028045932288 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2018125206232071, loss=1.8903597593307495
I0312 10:09:09.282074 140028037539584 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.20069542527198792, loss=1.8086676597595215
I0312 10:09:45.809541 140028045932288 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.19453252851963043, loss=1.8621506690979004
I0312 10:10:22.296027 140028037539584 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2141208052635193, loss=1.770153284072876
I0312 10:10:58.825376 140028045932288 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.24888019263744354, loss=1.8194068670272827
I0312 10:11:35.351875 140028037539584 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.20145007967948914, loss=1.7991111278533936
I0312 10:11:59.549716 140197733885760 spec.py:321] Evaluating on the training split.
I0312 10:12:02.627837 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 10:15:42.278902 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 10:15:44.997506 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 10:18:07.228531 140197733885760 spec.py:349] Evaluating on the test split.
I0312 10:18:09.964868 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 10:20:47.702365 140197733885760 submission_runner.py:420] Time since start: 21124.38s, 	Step: 34568, 	{'train/accuracy': 0.639989972114563, 'train/loss': 1.737483024597168, 'train/bleu': 31.40815149263302, 'validation/accuracy': 0.6610333323478699, 'validation/loss': 1.5890058279037476, 'validation/bleu': 28.182242715730233, 'validation/num_examples': 3000, 'test/accuracy': 0.6739526987075806, 'test/loss': 1.5071828365325928, 'test/bleu': 27.71240378788925, 'test/num_examples': 3003, 'score': 12638.940054178238, 'total_duration': 21124.38235092163, 'accumulated_submission_time': 12638.940054178238, 'accumulated_eval_time': 8483.97195148468, 'accumulated_logging_time': 0.45881009101867676}
I0312 10:20:47.720444 140028045932288 logging_writer.py:48] [34568] accumulated_eval_time=8483.971951, accumulated_logging_time=0.458810, accumulated_submission_time=12638.940054, global_step=34568, preemption_count=0, score=12638.940054, test/accuracy=0.673953, test/bleu=27.712404, test/loss=1.507183, test/num_examples=3003, total_duration=21124.382351, train/accuracy=0.639990, train/bleu=31.408151, train/loss=1.737483, validation/accuracy=0.661033, validation/bleu=28.182243, validation/loss=1.589006, validation/num_examples=3000
I0312 10:20:59.644522 140028037539584 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.22468671202659607, loss=1.8104817867279053
I0312 10:21:35.905077 140028045932288 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.18113282322883606, loss=1.819737434387207
I0312 10:22:12.230268 140028037539584 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.19566410779953003, loss=1.718907117843628
I0312 10:22:48.608767 140028045932288 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.2343965768814087, loss=1.806503415107727
I0312 10:23:25.024992 140028037539584 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.18641604483127594, loss=1.8068768978118896
I0312 10:24:01.470364 140028045932288 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2300310581922531, loss=1.7609025239944458
I0312 10:24:37.925974 140028037539584 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.2354465126991272, loss=1.7763441801071167
I0312 10:25:14.403386 140028045932288 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.24533070623874664, loss=1.8014655113220215
I0312 10:25:50.860429 140028037539584 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.19903339445590973, loss=1.8355613946914673
I0312 10:26:27.322338 140028045932288 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.22576749324798584, loss=1.7855372428894043
I0312 10:27:03.780842 140028037539584 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.2158087193965912, loss=1.7588601112365723
I0312 10:27:40.277371 140028045932288 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2014436572790146, loss=1.8094372749328613
I0312 10:28:16.750812 140028037539584 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.20805473625659943, loss=1.7694270610809326
I0312 10:28:53.225521 140028045932288 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.19273488223552704, loss=1.7388455867767334
I0312 10:29:29.712814 140028037539584 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.19623157382011414, loss=1.7588021755218506
I0312 10:30:06.213895 140028045932288 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.7499222159385681, loss=1.799925446510315
I0312 10:30:42.752234 140028037539584 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.7347967624664307, loss=1.7979844808578491
I0312 10:31:19.273992 140028045932288 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.41598251461982727, loss=1.7875409126281738
I0312 10:31:55.782573 140028037539584 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.21611270308494568, loss=1.8233871459960938
I0312 10:32:32.265155 140028045932288 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.20779886841773987, loss=1.8248249292373657
I0312 10:33:08.773959 140028037539584 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.19740429520606995, loss=1.7536877393722534
I0312 10:33:45.273443 140028045932288 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.29695162177085876, loss=1.8274662494659424
I0312 10:34:21.771649 140028037539584 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.21770790219306946, loss=1.7468478679656982
I0312 10:34:47.765053 140197733885760 spec.py:321] Evaluating on the training split.
I0312 10:34:50.821414 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 10:38:47.509580 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 10:38:50.225056 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 10:41:22.796427 140197733885760 spec.py:349] Evaluating on the test split.
I0312 10:41:25.522217 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 10:43:57.511559 140197733885760 submission_runner.py:420] Time since start: 22514.19s, 	Step: 36873, 	{'train/accuracy': 0.6420833468437195, 'train/loss': 1.7289228439331055, 'train/bleu': 31.196160112128922, 'validation/accuracy': 0.662744402885437, 'validation/loss': 1.5755648612976074, 'validation/bleu': 28.243285991467825, 'validation/num_examples': 3000, 'test/accuracy': 0.6739643216133118, 'test/loss': 1.4984428882598877, 'test/bleu': 27.51490274102142, 'test/num_examples': 3003, 'score': 13478.908013343811, 'total_duration': 22514.191542625427, 'accumulated_submission_time': 13478.908013343811, 'accumulated_eval_time': 9033.718392133713, 'accumulated_logging_time': 0.4855470657348633}
I0312 10:43:57.530523 140028045932288 logging_writer.py:48] [36873] accumulated_eval_time=9033.718392, accumulated_logging_time=0.485547, accumulated_submission_time=13478.908013, global_step=36873, preemption_count=0, score=13478.908013, test/accuracy=0.673964, test/bleu=27.514903, test/loss=1.498443, test/num_examples=3003, total_duration=22514.191543, train/accuracy=0.642083, train/bleu=31.196160, train/loss=1.728923, validation/accuracy=0.662744, validation/bleu=28.243286, validation/loss=1.575565, validation/num_examples=3000
I0312 10:44:07.691297 140028037539584 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.22276420891284943, loss=1.8332688808441162
I0312 10:44:43.952997 140028045932288 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.20423458516597748, loss=1.7966104745864868
I0312 10:45:20.307574 140028037539584 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.22478720545768738, loss=1.7516196966171265
I0312 10:45:56.671507 140028045932288 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.20316889882087708, loss=1.8169821500778198
I0312 10:46:33.098173 140028037539584 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2251766473054886, loss=1.7128404378890991
I0312 10:47:09.497138 140028045932288 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.2155546247959137, loss=1.7800025939941406
I0312 10:47:45.954658 140028037539584 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.20230312645435333, loss=1.868507742881775
I0312 10:48:22.413755 140028045932288 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2625482678413391, loss=1.783772587776184
I0312 10:48:58.912253 140028037539584 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.20664721727371216, loss=1.801403522491455
I0312 10:49:35.385193 140028045932288 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.2035868614912033, loss=1.816257119178772
I0312 10:50:11.854435 140028037539584 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2082296758890152, loss=1.787874698638916
I0312 10:50:48.342401 140028045932288 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.17681296169757843, loss=1.7639724016189575
I0312 10:51:24.837333 140028037539584 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.18371373414993286, loss=1.796869158744812
I0312 10:52:01.331542 140028045932288 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.21692012250423431, loss=1.849509358406067
I0312 10:52:37.813377 140028037539584 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.21688683331012726, loss=1.8261380195617676
I0312 10:53:14.289233 140028045932288 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.19987615942955017, loss=1.7958779335021973
I0312 10:53:50.790007 140028037539584 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22210660576820374, loss=1.7970870733261108
I0312 10:54:27.263637 140028045932288 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.21221208572387695, loss=1.8562343120574951
I0312 10:55:03.731255 140028037539584 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2537853717803955, loss=1.7460737228393555
I0312 10:55:40.237156 140028045932288 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.22148920595645905, loss=1.7173792123794556
I0312 10:56:16.743196 140028037539584 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.20059911906719208, loss=1.7432386875152588
I0312 10:56:53.269345 140028045932288 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.22517836093902588, loss=1.8114449977874756
I0312 10:57:29.788571 140028037539584 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.19995714724063873, loss=1.750474214553833
I0312 10:57:57.615116 140197733885760 spec.py:321] Evaluating on the training split.
I0312 10:58:00.671568 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:02:07.306672 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 11:02:10.020779 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:05:02.218137 140197733885760 spec.py:349] Evaluating on the test split.
I0312 11:05:04.948516 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:08:28.201400 140197733885760 submission_runner.py:420] Time since start: 23984.88s, 	Step: 39178, 	{'train/accuracy': 0.6463394165039062, 'train/loss': 1.6828906536102295, 'train/bleu': 31.62721259487478, 'validation/accuracy': 0.6632651686668396, 'validation/loss': 1.5713107585906982, 'validation/bleu': 28.164725355893932, 'validation/num_examples': 3000, 'test/accuracy': 0.6731741428375244, 'test/loss': 1.4919666051864624, 'test/bleu': 27.401873969473264, 'test/num_examples': 3003, 'score': 14318.914702415466, 'total_duration': 23984.881393671036, 'accumulated_submission_time': 14318.914702415466, 'accumulated_eval_time': 9664.304621696472, 'accumulated_logging_time': 0.5149462223052979}
I0312 11:08:28.220711 140028045932288 logging_writer.py:48] [39178] accumulated_eval_time=9664.304622, accumulated_logging_time=0.514946, accumulated_submission_time=14318.914702, global_step=39178, preemption_count=0, score=14318.914702, test/accuracy=0.673174, test/bleu=27.401874, test/loss=1.491967, test/num_examples=3003, total_duration=23984.881394, train/accuracy=0.646339, train/bleu=31.627213, train/loss=1.682891, validation/accuracy=0.663265, validation/bleu=28.164725, validation/loss=1.571311, validation/num_examples=3000
I0312 11:08:36.527380 140028037539584 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3627128601074219, loss=1.8349107503890991
I0312 11:09:12.729785 140028045932288 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.21362809836864471, loss=1.8763108253479004
I0312 11:09:49.033304 140028037539584 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.19904960691928864, loss=1.770965337753296
I0312 11:10:25.366644 140028045932288 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.23550589382648468, loss=1.828256607055664
I0312 11:11:01.772303 140028037539584 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.336096853017807, loss=1.7042962312698364
I0312 11:11:38.156583 140028045932288 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2430277168750763, loss=1.8229210376739502
I0312 11:12:14.603445 140028037539584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.23989585041999817, loss=1.7705780267715454
I0312 11:12:51.019294 140028045932288 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.20227085053920746, loss=1.7586296796798706
I0312 11:13:27.493525 140028037539584 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.20518793165683746, loss=1.750654935836792
I0312 11:14:03.930380 140028045932288 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.1947602778673172, loss=1.749734878540039
I0312 11:14:40.411429 140028037539584 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2268771529197693, loss=1.7823773622512817
I0312 11:15:16.880201 140028045932288 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.20014135539531708, loss=1.8020800352096558
I0312 11:15:53.356172 140028037539584 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.22359491884708405, loss=1.8064143657684326
I0312 11:16:29.827204 140028045932288 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.20938293635845184, loss=1.7993443012237549
I0312 11:17:06.336045 140028037539584 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.19624975323677063, loss=1.8080261945724487
I0312 11:17:42.832894 140028045932288 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.20161911845207214, loss=1.7635116577148438
I0312 11:18:19.336682 140028037539584 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.20934152603149414, loss=1.709523320198059
I0312 11:18:55.852532 140028045932288 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.20889994502067566, loss=1.7726256847381592
I0312 11:19:32.368957 140028037539584 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2260078489780426, loss=1.7733179330825806
I0312 11:20:08.891377 140028045932288 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5316219329833984, loss=1.7847249507904053
I0312 11:20:45.384212 140028037539584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.22770816087722778, loss=1.7524547576904297
I0312 11:21:21.918119 140028045932288 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.18748606741428375, loss=1.7264541387557983
I0312 11:21:58.412588 140028037539584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.1990409642457962, loss=1.7501730918884277
I0312 11:22:28.410117 140197733885760 spec.py:321] Evaluating on the training split.
I0312 11:22:31.459644 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:26:21.822265 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 11:26:24.543460 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:29:18.600757 140197733885760 spec.py:349] Evaluating on the test split.
I0312 11:29:21.326346 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:32:05.618562 140197733885760 submission_runner.py:420] Time since start: 25402.30s, 	Step: 41484, 	{'train/accuracy': 0.6445146203041077, 'train/loss': 1.7057585716247559, 'train/bleu': 31.45555725558557, 'validation/accuracy': 0.6643810868263245, 'validation/loss': 1.5640370845794678, 'validation/bleu': 28.44802331863066, 'validation/num_examples': 3000, 'test/accuracy': 0.677241325378418, 'test/loss': 1.4807902574539185, 'test/bleu': 27.910100164262925, 'test/num_examples': 3003, 'score': 15159.026709794998, 'total_duration': 25402.29855298996, 'accumulated_submission_time': 15159.026709794998, 'accumulated_eval_time': 10241.513010263443, 'accumulated_logging_time': 0.5448994636535645}
I0312 11:32:05.638018 140028045932288 logging_writer.py:48] [41484] accumulated_eval_time=10241.513010, accumulated_logging_time=0.544899, accumulated_submission_time=15159.026710, global_step=41484, preemption_count=0, score=15159.026710, test/accuracy=0.677241, test/bleu=27.910100, test/loss=1.480790, test/num_examples=3003, total_duration=25402.298553, train/accuracy=0.644515, train/bleu=31.455557, train/loss=1.705759, validation/accuracy=0.664381, validation/bleu=28.448023, validation/loss=1.564037, validation/num_examples=3000
I0312 11:32:11.800241 140028037539584 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.25434088706970215, loss=1.7784773111343384
I0312 11:32:48.015880 140028045932288 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2321431189775467, loss=1.778849482536316
I0312 11:33:24.353470 140028037539584 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.22668714821338654, loss=1.8026797771453857
I0312 11:34:00.673030 140028045932288 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.24731437861919403, loss=1.7449369430541992
I0312 11:34:37.048801 140028037539584 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.22578337788581848, loss=1.820181131362915
I0312 11:35:13.465301 140028045932288 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.21343404054641724, loss=1.7496906518936157
I0312 11:35:49.883395 140028037539584 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.19623275101184845, loss=1.833541750907898
I0312 11:36:26.295663 140028045932288 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.23399673402309418, loss=1.8325817584991455
I0312 11:37:02.751786 140028037539584 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.2363164871931076, loss=1.800121784210205
I0312 11:37:39.195963 140028045932288 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.21772396564483643, loss=1.7787669897079468
I0312 11:38:15.671921 140028037539584 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.30535924434661865, loss=1.7221390008926392
I0312 11:38:52.150300 140028045932288 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.22024627029895782, loss=1.77143394947052
I0312 11:39:28.659740 140028037539584 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.1919853836297989, loss=1.8103914260864258
I0312 11:40:05.137762 140028045932288 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.24801690876483917, loss=1.779516577720642
I0312 11:40:41.658390 140028037539584 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.20883715152740479, loss=1.7174338102340698
I0312 11:41:18.149554 140028045932288 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.2522830069065094, loss=1.7733179330825806
I0312 11:41:54.631218 140028037539584 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.24597789347171783, loss=1.7387821674346924
I0312 11:42:31.139374 140028045932288 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.22508299350738525, loss=1.7211642265319824
I0312 11:43:07.660054 140028037539584 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.19102251529693604, loss=1.765210509300232
I0312 11:43:44.157591 140028045932288 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.19308412075042725, loss=1.7335162162780762
I0312 11:44:20.646985 140028037539584 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2039434164762497, loss=1.8212668895721436
I0312 11:44:57.132945 140028045932288 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.21195898950099945, loss=1.726898431777954
I0312 11:45:33.647263 140028037539584 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.19812048971652985, loss=1.8253504037857056
I0312 11:46:05.855294 140197733885760 spec.py:321] Evaluating on the training split.
I0312 11:46:08.907379 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:49:22.071009 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 11:49:24.792454 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:51:56.422430 140197733885760 spec.py:349] Evaluating on the test split.
I0312 11:51:59.141789 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 11:54:14.441098 140197733885760 submission_runner.py:420] Time since start: 26731.12s, 	Step: 43790, 	{'train/accuracy': 0.678406298160553, 'train/loss': 1.4894598722457886, 'train/bleu': 34.31987309324694, 'validation/accuracy': 0.666340172290802, 'validation/loss': 1.5590335130691528, 'validation/bleu': 28.83492635339662, 'validation/num_examples': 3000, 'test/accuracy': 0.6773110628128052, 'test/loss': 1.4803187847137451, 'test/bleu': 28.130290097852814, 'test/num_examples': 3003, 'score': 15999.166652917862, 'total_duration': 26731.12109351158, 'accumulated_submission_time': 15999.166652917862, 'accumulated_eval_time': 10730.098761081696, 'accumulated_logging_time': 0.5752518177032471}
I0312 11:54:14.461500 140028045932288 logging_writer.py:48] [43790] accumulated_eval_time=10730.098761, accumulated_logging_time=0.575252, accumulated_submission_time=15999.166653, global_step=43790, preemption_count=0, score=15999.166653, test/accuracy=0.677311, test/bleu=28.130290, test/loss=1.480319, test/num_examples=3003, total_duration=26731.121094, train/accuracy=0.678406, train/bleu=34.319873, train/loss=1.489460, validation/accuracy=0.666340, validation/bleu=28.834926, validation/loss=1.559034, validation/num_examples=3000
I0312 11:54:18.461854 140028037539584 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2064596265554428, loss=1.8062682151794434
I0312 11:54:54.740224 140028045932288 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.20607545971870422, loss=1.7567164897918701
I0312 11:55:31.062252 140028037539584 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.22395674884319305, loss=1.7515103816986084
I0312 11:56:07.472849 140028045932288 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.20239368081092834, loss=1.7624986171722412
I0312 11:56:43.877242 140028037539584 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2278115302324295, loss=1.752286434173584
I0312 11:57:20.311084 140028045932288 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.21411274373531342, loss=1.7029922008514404
I0312 11:57:56.750371 140028037539584 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.25707390904426575, loss=1.7437407970428467
I0312 11:58:33.251787 140028045932288 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19099585711956024, loss=1.8071986436843872
I0312 11:59:09.732139 140028037539584 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.18781976401805878, loss=1.7359832525253296
I0312 11:59:46.219537 140028045932288 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2893381714820862, loss=1.795196771621704
I0312 12:00:22.692795 140028037539584 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.23018385469913483, loss=1.8026784658432007
I0312 12:00:59.141093 140028045932288 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.18952536582946777, loss=1.782564640045166
I0312 12:01:35.636867 140028037539584 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3129982054233551, loss=1.68412446975708
I0312 12:02:12.156950 140028045932288 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.1964934915304184, loss=1.8013464212417603
I0312 12:02:48.644648 140028037539584 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2394724041223526, loss=1.7868503332138062
I0312 12:03:25.124583 140028045932288 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.19486048817634583, loss=1.7495532035827637
I0312 12:04:01.625515 140028037539584 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.216600239276886, loss=1.8048731088638306
I0312 12:04:38.132076 140028045932288 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.20093387365341187, loss=1.760298252105713
I0312 12:05:14.640405 140028037539584 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.18832390010356903, loss=1.740058422088623
I0312 12:05:51.151109 140028045932288 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.21708673238754272, loss=1.7045875787734985
I0312 12:06:27.662937 140028037539584 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2091013640165329, loss=1.700365424156189
I0312 12:07:04.187807 140028045932288 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2930973768234253, loss=1.806614637374878
I0312 12:07:40.702775 140028037539584 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.20364941656589508, loss=1.7580010890960693
I0312 12:08:14.703294 140197733885760 spec.py:321] Evaluating on the training split.
I0312 12:08:17.752544 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:12:06.902171 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 12:12:09.614762 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:15:08.599439 140197733885760 spec.py:349] Evaluating on the test split.
I0312 12:15:11.311900 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:18:03.138552 140197733885760 submission_runner.py:420] Time since start: 28159.82s, 	Step: 46095, 	{'train/accuracy': 0.6512089371681213, 'train/loss': 1.6554925441741943, 'train/bleu': 31.58748186605699, 'validation/accuracy': 0.6669477224349976, 'validation/loss': 1.5520703792572021, 'validation/bleu': 28.66998281072759, 'validation/num_examples': 3000, 'test/accuracy': 0.6787520051002502, 'test/loss': 1.4696136713027954, 'test/bleu': 28.325597041806837, 'test/num_examples': 3003, 'score': 16839.33149933815, 'total_duration': 28159.8185441494, 'accumulated_submission_time': 16839.33149933815, 'accumulated_eval_time': 11318.533961772919, 'accumulated_logging_time': 0.605954647064209}
I0312 12:18:03.158293 140028045932288 logging_writer.py:48] [46095] accumulated_eval_time=11318.533962, accumulated_logging_time=0.605955, accumulated_submission_time=16839.331499, global_step=46095, preemption_count=0, score=16839.331499, test/accuracy=0.678752, test/bleu=28.325597, test/loss=1.469614, test/num_examples=3003, total_duration=28159.818544, train/accuracy=0.651209, train/bleu=31.587482, train/loss=1.655493, validation/accuracy=0.666948, validation/bleu=28.669983, validation/loss=1.552070, validation/num_examples=3000
I0312 12:18:05.334843 140028037539584 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.22283968329429626, loss=1.7405651807785034
I0312 12:18:41.458758 140028045932288 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.21205726265907288, loss=1.7326288223266602
I0312 12:19:17.712841 140028037539584 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19730499386787415, loss=1.7233561277389526
I0312 12:19:54.021585 140028045932288 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.22695879638195038, loss=1.8327677249908447
I0312 12:20:30.425713 140028037539584 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.27360033988952637, loss=1.8071459531784058
I0312 12:21:06.818463 140028045932288 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1982671618461609, loss=1.7856945991516113
I0312 12:21:43.218548 140028037539584 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.22592614591121674, loss=1.7814759016036987
I0312 12:22:19.639073 140028045932288 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2086847424507141, loss=1.7250535488128662
I0312 12:22:56.135560 140028037539584 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.19321295619010925, loss=1.7484025955200195
I0312 12:23:32.585997 140028045932288 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.21079520881175995, loss=1.7299319505691528
I0312 12:24:09.073075 140028037539584 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2127455770969391, loss=1.8528306484222412
I0312 12:24:45.536538 140028045932288 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.19963103532791138, loss=1.7117924690246582
I0312 12:25:22.016874 140028037539584 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.23395003378391266, loss=1.688274621963501
I0312 12:25:58.478724 140028045932288 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.19556286931037903, loss=1.77191960811615
I0312 12:26:34.959925 140028037539584 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.23737555742263794, loss=1.8210357427597046
I0312 12:27:11.450340 140028045932288 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.21559381484985352, loss=1.7080622911453247
I0312 12:27:47.951377 140028037539584 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.20792727172374725, loss=1.7766423225402832
I0312 12:28:24.461542 140028045932288 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.210951030254364, loss=1.6783866882324219
I0312 12:29:00.970885 140028037539584 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2030041664838791, loss=1.7019538879394531
I0312 12:29:37.447391 140028045932288 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2793281078338623, loss=1.7935441732406616
I0312 12:30:13.891011 140028037539584 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3037956655025482, loss=1.7096203565597534
I0312 12:30:50.407593 140028045932288 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18714261054992676, loss=1.7820028066635132
I0312 12:31:26.902035 140028037539584 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.21773889660835266, loss=1.7455065250396729
I0312 12:32:03.412498 140028045932288 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2007761150598526, loss=1.7407163381576538
I0312 12:32:03.418393 140197733885760 spec.py:321] Evaluating on the training split.
I0312 12:32:06.177866 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:36:23.980277 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 12:36:26.696469 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:39:04.566936 140197733885760 spec.py:349] Evaluating on the test split.
I0312 12:39:07.289631 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:41:50.650965 140197733885760 submission_runner.py:420] Time since start: 29587.33s, 	Step: 48401, 	{'train/accuracy': 0.648673415184021, 'train/loss': 1.6775165796279907, 'train/bleu': 31.704655840258024, 'validation/accuracy': 0.6679148077964783, 'validation/loss': 1.5459349155426025, 'validation/bleu': 28.496734334250775, 'validation/num_examples': 3000, 'test/accuracy': 0.6809017658233643, 'test/loss': 1.458929181098938, 'test/bleu': 28.30155804844667, 'test/num_examples': 3003, 'score': 17679.514671325684, 'total_duration': 29587.330949544907, 'accumulated_submission_time': 17679.514671325684, 'accumulated_eval_time': 11905.7664437294, 'accumulated_logging_time': 0.6351094245910645}
I0312 12:41:50.670609 140028037539584 logging_writer.py:48] [48401] accumulated_eval_time=11905.766444, accumulated_logging_time=0.635109, accumulated_submission_time=17679.514671, global_step=48401, preemption_count=0, score=17679.514671, test/accuracy=0.680902, test/bleu=28.301558, test/loss=1.458929, test/num_examples=3003, total_duration=29587.330950, train/accuracy=0.648673, train/bleu=31.704656, train/loss=1.677517, validation/accuracy=0.667915, validation/bleu=28.496734, validation/loss=1.545935, validation/num_examples=3000
I0312 12:42:26.871493 140028045932288 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.21070457994937897, loss=1.7761191129684448
I0312 12:43:03.125159 140028037539584 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.20464368164539337, loss=1.7747567892074585
I0312 12:43:39.453051 140028045932288 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.21381984651088715, loss=1.8136032819747925
I0312 12:44:15.863575 140028037539584 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.1912037581205368, loss=1.7014621496200562
I0312 12:44:52.253371 140028045932288 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.20037853717803955, loss=1.7393708229064941
I0312 12:45:28.688068 140028037539584 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.23005041480064392, loss=1.755419135093689
I0312 12:46:05.118249 140028045932288 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.18498173356056213, loss=1.7434293031692505
I0312 12:46:41.616258 140028037539584 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.18383993208408356, loss=1.7085723876953125
I0312 12:47:18.092912 140028045932288 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.384054571390152, loss=1.7583962678909302
I0312 12:47:54.599076 140028037539584 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.21044902503490448, loss=1.7904319763183594
I0312 12:48:31.067542 140028045932288 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.23652997612953186, loss=1.7335827350616455
I0312 12:49:07.543116 140028037539584 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.18315237760543823, loss=1.7845157384872437
I0312 12:49:44.013375 140028045932288 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.20563241839408875, loss=1.7593669891357422
I0312 12:50:20.506192 140028037539584 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.180121049284935, loss=1.725721001625061
I0312 12:50:57.017633 140028045932288 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.19689102470874786, loss=1.786083459854126
I0312 12:51:33.485902 140028037539584 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.20460720360279083, loss=1.7581543922424316
I0312 12:52:09.997914 140028045932288 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.20569273829460144, loss=1.74718177318573
I0312 12:52:46.479142 140028037539584 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.21193447709083557, loss=1.7677680253982544
I0312 12:53:22.983171 140028045932288 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.27419203519821167, loss=1.7135376930236816
I0312 12:53:59.478733 140028037539584 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.20406988263130188, loss=1.772163987159729
I0312 12:54:35.982739 140028045932288 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2062477022409439, loss=1.6858474016189575
I0312 12:55:12.491672 140028037539584 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.20117296278476715, loss=1.7370665073394775
I0312 12:55:48.954242 140028045932288 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.21445243060588837, loss=1.7729195356369019
I0312 12:55:50.860618 140197733885760 spec.py:321] Evaluating on the training split.
I0312 12:55:53.909544 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 12:59:59.640058 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 13:00:02.380300 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:02:35.155291 140197733885760 spec.py:349] Evaluating on the test split.
I0312 13:02:37.879970 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:04:58.593989 140197733885760 submission_runner.py:420] Time since start: 30975.27s, 	Step: 50707, 	{'train/accuracy': 0.6610821485519409, 'train/loss': 1.5793545246124268, 'train/bleu': 32.63553191464314, 'validation/accuracy': 0.6676668524742126, 'validation/loss': 1.5392777919769287, 'validation/bleu': 28.773472505424525, 'validation/num_examples': 3000, 'test/accuracy': 0.6794027090072632, 'test/loss': 1.4583882093429565, 'test/bleu': 27.870305365442896, 'test/num_examples': 3003, 'score': 18519.6294836998, 'total_duration': 30975.2739675045, 'accumulated_submission_time': 18519.6294836998, 'accumulated_eval_time': 12453.499742031097, 'accumulated_logging_time': 0.663583517074585}
I0312 13:04:58.614714 140028037539584 logging_writer.py:48] [50707] accumulated_eval_time=12453.499742, accumulated_logging_time=0.663584, accumulated_submission_time=18519.629484, global_step=50707, preemption_count=0, score=18519.629484, test/accuracy=0.679403, test/bleu=27.870305, test/loss=1.458388, test/num_examples=3003, total_duration=30975.273968, train/accuracy=0.661082, train/bleu=32.635532, train/loss=1.579355, validation/accuracy=0.667667, validation/bleu=28.773473, validation/loss=1.539278, validation/num_examples=3000
I0312 13:05:32.642348 140028045932288 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.20951208472251892, loss=1.8244409561157227
I0312 13:06:08.952920 140028037539584 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.26078173518180847, loss=1.7090378999710083
I0312 13:06:45.312840 140028045932288 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.21148467063903809, loss=1.6786603927612305
I0312 13:07:21.689040 140028037539584 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.20784343779087067, loss=1.74966561794281
I0312 13:07:58.154141 140028045932288 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.32811176776885986, loss=1.7453778982162476
I0312 13:08:34.590502 140028037539584 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.19823408126831055, loss=1.7859700918197632
I0312 13:09:11.054169 140028045932288 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.2133748084306717, loss=1.739385962486267
I0312 13:09:47.507089 140028037539584 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.22229842841625214, loss=1.7131201028823853
I0312 13:10:23.962023 140028045932288 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.195297971367836, loss=1.8042863607406616
I0312 13:11:00.457350 140028037539584 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.20650680363178253, loss=1.7073616981506348
I0312 13:11:36.930677 140028045932288 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.23260103166103363, loss=1.7808310985565186
I0312 13:12:13.429740 140028037539584 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.22023260593414307, loss=1.788381814956665
I0312 13:12:49.924056 140028045932288 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.19155055284500122, loss=1.6679383516311646
I0312 13:13:26.437119 140028037539584 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.20913174748420715, loss=1.7713825702667236
I0312 13:14:02.924908 140028045932288 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.2110304832458496, loss=1.7006629705429077
I0312 13:14:39.432628 140028037539584 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1832932084798813, loss=1.7100783586502075
I0312 13:15:15.927906 140028045932288 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.21473273634910583, loss=1.7039591073989868
I0312 13:15:52.449671 140028037539584 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19830641150474548, loss=1.7253341674804688
I0312 13:16:28.937113 140028045932288 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.19625960290431976, loss=1.7791534662246704
I0312 13:17:05.416902 140028037539584 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.22917057573795319, loss=1.70171320438385
I0312 13:17:41.940909 140028045932288 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.19406194984912872, loss=1.7409241199493408
I0312 13:18:18.432277 140028037539584 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18584924936294556, loss=1.7032076120376587
I0312 13:18:54.944495 140028045932288 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.23253995180130005, loss=1.7499699592590332
I0312 13:18:58.682581 140197733885760 spec.py:321] Evaluating on the training split.
I0312 13:19:01.748588 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:23:17.158527 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 13:23:19.881439 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:25:52.870891 140197733885760 spec.py:349] Evaluating on the test split.
I0312 13:25:55.591503 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:28:46.265699 140197733885760 submission_runner.py:420] Time since start: 32402.95s, 	Step: 53012, 	{'train/accuracy': 0.6509921550750732, 'train/loss': 1.663438320159912, 'train/bleu': 31.955037418769578, 'validation/accuracy': 0.6693159341812134, 'validation/loss': 1.5281200408935547, 'validation/bleu': 28.66539324958268, 'validation/num_examples': 3000, 'test/accuracy': 0.6824240684509277, 'test/loss': 1.4426162242889404, 'test/bleu': 28.27440388210995, 'test/num_examples': 3003, 'score': 19359.620164632797, 'total_duration': 32402.945701360703, 'accumulated_submission_time': 19359.620164632797, 'accumulated_eval_time': 13041.082812786102, 'accumulated_logging_time': 0.6946089267730713}
I0312 13:28:46.286751 140028037539584 logging_writer.py:48] [53012] accumulated_eval_time=13041.082813, accumulated_logging_time=0.694609, accumulated_submission_time=19359.620165, global_step=53012, preemption_count=0, score=19359.620165, test/accuracy=0.682424, test/bleu=28.274404, test/loss=1.442616, test/num_examples=3003, total_duration=32402.945701, train/accuracy=0.650992, train/bleu=31.955037, train/loss=1.663438, validation/accuracy=0.669316, validation/bleu=28.665393, validation/loss=1.528120, validation/num_examples=3000
I0312 13:29:18.431519 140028045932288 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.19009171426296234, loss=1.6734719276428223
I0312 13:29:54.697012 140028037539584 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.19275391101837158, loss=1.7374054193496704
I0312 13:30:31.060157 140028045932288 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.4065493941307068, loss=1.7233452796936035
I0312 13:31:07.417059 140028037539584 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.25014764070510864, loss=1.7148855924606323
I0312 13:31:43.810067 140028045932288 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.218855082988739, loss=1.709334373474121
I0312 13:32:20.261304 140028037539584 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.18150873482227325, loss=1.6706719398498535
I0312 13:32:56.677595 140028045932288 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.24292109906673431, loss=1.6852002143859863
I0312 13:33:33.153074 140028037539584 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2818150222301483, loss=1.7664761543273926
I0312 13:34:09.594134 140028045932288 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.2132088541984558, loss=1.7583340406417847
I0312 13:34:46.057541 140028037539584 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.19890616834163666, loss=1.7730501890182495
I0312 13:35:22.528051 140028045932288 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1970396190881729, loss=1.6505179405212402
I0312 13:35:58.988615 140028037539584 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.211722269654274, loss=1.7952282428741455
I0312 13:36:35.442950 140028045932288 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.22722481191158295, loss=1.729256510734558
I0312 13:37:11.924033 140028037539584 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.22345265746116638, loss=1.7251780033111572
I0312 13:37:48.385865 140028045932288 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.20951084792613983, loss=1.7572381496429443
I0312 13:38:24.917894 140028037539584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2157209813594818, loss=1.6987979412078857
I0312 13:39:01.453795 140028045932288 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3992968797683716, loss=1.647942304611206
I0312 13:39:37.946455 140028037539584 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.20195640623569489, loss=1.7194660902023315
I0312 13:40:14.446060 140028045932288 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.1957666277885437, loss=1.6908937692642212
I0312 13:40:50.933888 140028037539584 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.19241224229335785, loss=1.6664637327194214
I0312 13:41:27.441070 140028045932288 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.20349059998989105, loss=1.766259789466858
I0312 13:42:03.938565 140028037539584 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1973472386598587, loss=1.6271142959594727
I0312 13:42:40.454685 140028045932288 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19330288469791412, loss=1.7879642248153687
I0312 13:42:46.369968 140197733885760 spec.py:321] Evaluating on the training split.
I0312 13:42:49.417274 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:47:02.243319 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 13:47:04.956526 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:49:43.275016 140197733885760 spec.py:349] Evaluating on the test split.
I0312 13:49:45.991355 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 13:52:12.375859 140197733885760 submission_runner.py:420] Time since start: 33809.06s, 	Step: 55318, 	{'train/accuracy': 0.6490379571914673, 'train/loss': 1.6658644676208496, 'train/bleu': 31.859594305438492, 'validation/accuracy': 0.668584406375885, 'validation/loss': 1.5241024494171143, 'validation/bleu': 28.634566099582898, 'validation/num_examples': 3000, 'test/accuracy': 0.6829469799995422, 'test/loss': 1.442746877670288, 'test/bleu': 28.265151327877877, 'test/num_examples': 3003, 'score': 20199.62682914734, 'total_duration': 33809.05586147308, 'accumulated_submission_time': 20199.62682914734, 'accumulated_eval_time': 13607.088655233383, 'accumulated_logging_time': 0.7248268127441406}
I0312 13:52:12.396956 140028037539584 logging_writer.py:48] [55318] accumulated_eval_time=13607.088655, accumulated_logging_time=0.724827, accumulated_submission_time=20199.626829, global_step=55318, preemption_count=0, score=20199.626829, test/accuracy=0.682947, test/bleu=28.265151, test/loss=1.442747, test/num_examples=3003, total_duration=33809.055861, train/accuracy=0.649038, train/bleu=31.859594, train/loss=1.665864, validation/accuracy=0.668584, validation/bleu=28.634566, validation/loss=1.524102, validation/num_examples=3000
I0312 13:52:42.481249 140028045932288 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.2033129781484604, loss=1.6687991619110107
I0312 13:53:18.768952 140028037539584 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.19727376103401184, loss=1.6601051092147827
I0312 13:53:55.125082 140028045932288 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.1933630257844925, loss=1.7253751754760742
I0312 13:54:31.486525 140028037539584 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3785516023635864, loss=1.6596057415008545
I0312 13:55:07.905597 140028045932288 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2391742616891861, loss=1.7072211503982544
I0312 13:55:44.298672 140028037539584 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2010505348443985, loss=1.6618092060089111
I0312 13:56:20.742702 140028045932288 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.204627126455307, loss=1.820770502090454
I0312 13:56:57.180122 140028037539584 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.26749303936958313, loss=1.636434555053711
I0312 13:57:33.655707 140028045932288 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2802940607070923, loss=1.736630916595459
I0312 13:58:10.112422 140028037539584 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.19796951115131378, loss=1.6848933696746826
I0312 13:58:46.614921 140028045932288 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19285716116428375, loss=1.7406189441680908
I0312 13:59:23.114272 140028037539584 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.18455831706523895, loss=1.709756851196289
I0312 13:59:59.577364 140028045932288 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.19292470812797546, loss=1.709702730178833
I0312 14:00:36.069563 140028037539584 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.22518524527549744, loss=1.7193406820297241
I0312 14:01:12.554903 140028045932288 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.188340425491333, loss=1.6764297485351562
I0312 14:01:49.022143 140028037539584 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.21605101227760315, loss=1.7187952995300293
I0312 14:02:25.522080 140028045932288 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2014198899269104, loss=1.6714812517166138
I0312 14:03:02.043547 140028037539584 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.22947494685649872, loss=1.7782254219055176
I0312 14:03:38.558029 140028045932288 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1996224820613861, loss=1.6772347688674927
I0312 14:04:15.073760 140028037539584 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20570673048496246, loss=1.6665557622909546
I0312 14:04:51.587084 140028045932288 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2185153216123581, loss=1.7531262636184692
I0312 14:05:28.083870 140028037539584 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.20144470036029816, loss=1.699101448059082
I0312 14:06:04.570002 140028045932288 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19320346415042877, loss=1.6136363744735718
I0312 14:06:12.674992 140197733885760 spec.py:321] Evaluating on the training split.
I0312 14:06:15.724451 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:10:38.950808 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 14:10:41.675700 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:13:19.538146 140197733885760 spec.py:349] Evaluating on the test split.
I0312 14:13:22.264008 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:15:43.133065 140197733885760 submission_runner.py:420] Time since start: 35219.81s, 	Step: 57624, 	{'train/accuracy': 0.6594597697257996, 'train/loss': 1.5965825319290161, 'train/bleu': 32.67480079360041, 'validation/accuracy': 0.6716469526290894, 'validation/loss': 1.5170725584030151, 'validation/bleu': 29.14386669057417, 'validation/num_examples': 3000, 'test/accuracy': 0.6851897239685059, 'test/loss': 1.4309513568878174, 'test/bleu': 28.565840098324877, 'test/num_examples': 3003, 'score': 21039.827073812485, 'total_duration': 35219.81306219101, 'accumulated_submission_time': 21039.827073812485, 'accumulated_eval_time': 14177.54667854309, 'accumulated_logging_time': 0.7564868927001953}
I0312 14:15:43.153932 140028037539584 logging_writer.py:48] [57624] accumulated_eval_time=14177.546679, accumulated_logging_time=0.756487, accumulated_submission_time=21039.827074, global_step=57624, preemption_count=0, score=21039.827074, test/accuracy=0.685190, test/bleu=28.565840, test/loss=1.430951, test/num_examples=3003, total_duration=35219.813062, train/accuracy=0.659460, train/bleu=32.674801, train/loss=1.596583, validation/accuracy=0.671647, validation/bleu=29.143867, validation/loss=1.517073, validation/num_examples=3000
I0312 14:16:11.071476 140028045932288 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.22915272414684296, loss=1.7685778141021729
I0312 14:16:47.377990 140028037539584 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.1932455599308014, loss=1.6967694759368896
I0312 14:17:23.765638 140028045932288 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.22226440906524658, loss=1.6238776445388794
I0312 14:18:00.193749 140028037539584 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.19595575332641602, loss=1.6683112382888794
I0312 14:18:36.624939 140028045932288 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.19940485060214996, loss=1.7400587797164917
I0312 14:19:13.081432 140028037539584 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.1899905651807785, loss=1.6111862659454346
I0312 14:19:49.541641 140028045932288 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2108469158411026, loss=1.6650192737579346
I0312 14:20:25.969045 140028037539584 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.19968605041503906, loss=1.6650453805923462
I0312 14:21:02.436462 140028045932288 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.1994697004556656, loss=1.684999942779541
I0312 14:21:38.906131 140028037539584 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.18064823746681213, loss=1.6666666269302368
I0312 14:22:15.408207 140028045932288 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.21890296041965485, loss=1.6910163164138794
I0312 14:22:51.871963 140028037539584 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.19305101037025452, loss=1.729131817817688
I0312 14:23:28.351306 140028045932288 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.22190311551094055, loss=1.710236668586731
I0312 14:24:04.851124 140028037539584 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.1927790641784668, loss=1.713426113128662
I0312 14:24:41.351449 140028045932288 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.20040909945964813, loss=1.7408639192581177
I0312 14:25:17.855120 140028037539584 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.21330107748508453, loss=1.729124665260315
I0312 14:25:54.357596 140028045932288 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.26589494943618774, loss=1.7657922506332397
I0312 14:26:30.868271 140028037539584 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.21803344786167145, loss=1.735602855682373
I0312 14:27:07.404038 140028045932288 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.21589916944503784, loss=1.6633899211883545
I0312 14:27:43.949025 140028037539584 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.20465970039367676, loss=1.6243293285369873
I0312 14:28:20.457998 140028045932288 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.1821366250514984, loss=1.7090003490447998
I0312 14:28:56.989687 140028037539584 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.19344200193881989, loss=1.6981420516967773
I0312 14:29:33.511076 140028045932288 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19385665655136108, loss=1.648728847503662
I0312 14:29:43.458711 140197733885760 spec.py:321] Evaluating on the training split.
I0312 14:29:46.510035 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:33:31.617868 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 14:33:34.339619 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:36:28.300616 140197733885760 spec.py:349] Evaluating on the test split.
I0312 14:36:31.017561 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:39:23.189119 140197733885760 submission_runner.py:420] Time since start: 36639.87s, 	Step: 59929, 	{'train/accuracy': 0.6542331576347351, 'train/loss': 1.6329275369644165, 'train/bleu': 32.1923354088288, 'validation/accuracy': 0.6711261868476868, 'validation/loss': 1.514757752418518, 'validation/bleu': 28.66304524085199, 'validation/num_examples': 3000, 'test/accuracy': 0.6837139129638672, 'test/loss': 1.4270353317260742, 'test/bleu': 28.37492052326649, 'test/num_examples': 3003, 'score': 21880.053963899612, 'total_duration': 36639.86911034584, 'accumulated_submission_time': 21880.053963899612, 'accumulated_eval_time': 14757.277026891708, 'accumulated_logging_time': 0.7875394821166992}
I0312 14:39:23.210842 140028037539584 logging_writer.py:48] [59929] accumulated_eval_time=14757.277027, accumulated_logging_time=0.787539, accumulated_submission_time=21880.053964, global_step=59929, preemption_count=0, score=21880.053964, test/accuracy=0.683714, test/bleu=28.374921, test/loss=1.427035, test/num_examples=3003, total_duration=36639.869110, train/accuracy=0.654233, train/bleu=32.192335, train/loss=1.632928, validation/accuracy=0.671126, validation/bleu=28.663045, validation/loss=1.514758, validation/num_examples=3000
I0312 14:39:49.208305 140028045932288 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2121041715145111, loss=1.7158305644989014
I0312 14:40:25.467504 140028037539584 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.19325247406959534, loss=1.7409427165985107
I0312 14:41:01.806538 140028045932288 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.19117876887321472, loss=1.6777734756469727
I0312 14:41:38.113328 140028037539584 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.21989351511001587, loss=1.7815399169921875
I0312 14:42:14.538195 140028045932288 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20510078966617584, loss=1.7508208751678467
I0312 14:42:50.960356 140028037539584 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3064408302307129, loss=1.7487351894378662
I0312 14:43:27.381192 140028045932288 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2227669507265091, loss=1.6751941442489624
I0312 14:44:03.821216 140028037539584 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.19348500669002533, loss=1.648417353630066
I0312 14:44:40.329158 140028045932288 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.26403144001960754, loss=1.7107535600662231
I0312 14:45:16.816842 140028037539584 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.20278280973434448, loss=1.6648494005203247
I0312 14:45:53.311874 140028045932288 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2411404699087143, loss=1.6904847621917725
I0312 14:46:29.783680 140028037539584 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.19117334485054016, loss=1.6559778451919556
I0312 14:47:06.284414 140028045932288 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.23028580844402313, loss=1.7174350023269653
I0312 14:47:42.774327 140028037539584 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.1921444535255432, loss=1.6757698059082031
I0312 14:48:19.261975 140028045932288 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.21603840589523315, loss=1.7067430019378662
I0312 14:48:55.769450 140028037539584 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.1917351633310318, loss=1.666994333267212
I0312 14:49:32.284978 140028045932288 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.1880452185869217, loss=1.6541866064071655
I0312 14:50:08.789036 140028037539584 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.2150242179632187, loss=1.6588265895843506
I0312 14:50:45.306296 140028045932288 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.2226441502571106, loss=1.7484381198883057
I0312 14:51:21.829527 140028037539584 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2202758640050888, loss=1.7124545574188232
I0312 14:51:58.347838 140028045932288 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.25208455324172974, loss=1.7536293268203735
I0312 14:52:34.867856 140028037539584 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.22082117199897766, loss=1.6639586687088013
I0312 14:53:11.385545 140028045932288 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.22687573730945587, loss=1.7361202239990234
I0312 14:53:23.517969 140197733885760 spec.py:321] Evaluating on the training split.
I0312 14:53:26.577025 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:56:48.021276 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 14:56:50.748573 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 14:59:18.857891 140197733885760 spec.py:349] Evaluating on the test split.
I0312 14:59:21.591971 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:01:47.496698 140197733885760 submission_runner.py:420] Time since start: 37984.18s, 	Step: 62235, 	{'train/accuracy': 0.6547898054122925, 'train/loss': 1.6457945108413696, 'train/bleu': 32.29131481800968, 'validation/accuracy': 0.6748583316802979, 'validation/loss': 1.5035715103149414, 'validation/bleu': 29.144210761682988, 'validation/num_examples': 3000, 'test/accuracy': 0.6874208450317383, 'test/loss': 1.4156063795089722, 'test/bleu': 28.6996542227149, 'test/num_examples': 3003, 'score': 22720.285170316696, 'total_duration': 37984.17668604851, 'accumulated_submission_time': 22720.285170316696, 'accumulated_eval_time': 15261.255691766739, 'accumulated_logging_time': 0.8184239864349365}
I0312 15:01:47.518910 140028037539584 logging_writer.py:48] [62235] accumulated_eval_time=15261.255692, accumulated_logging_time=0.818424, accumulated_submission_time=22720.285170, global_step=62235, preemption_count=0, score=22720.285170, test/accuracy=0.687421, test/bleu=28.699654, test/loss=1.415606, test/num_examples=3003, total_duration=37984.176686, train/accuracy=0.654790, train/bleu=32.291315, train/loss=1.645795, validation/accuracy=0.674858, validation/bleu=29.144211, validation/loss=1.503572, validation/num_examples=3000
I0312 15:02:11.441567 140028045932288 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.2152385264635086, loss=1.7132916450500488
I0312 15:02:47.734098 140028037539584 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2075890302658081, loss=1.5922985076904297
I0312 15:03:24.130882 140028045932288 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20229588449001312, loss=1.6484136581420898
I0312 15:04:00.484314 140028037539584 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.20027883350849152, loss=1.6567776203155518
I0312 15:04:36.908103 140028045932288 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.1995074301958084, loss=1.6842187643051147
I0312 15:05:13.321122 140028037539584 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.22579269111156464, loss=1.6448532342910767
I0312 15:05:49.732591 140028045932288 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.21207036077976227, loss=1.7208424806594849
I0312 15:06:26.208061 140028037539584 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.25597453117370605, loss=1.7277753353118896
I0312 15:07:02.676514 140028045932288 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.17977286875247955, loss=1.6188565492630005
I0312 15:07:39.185069 140028037539584 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.1970548927783966, loss=1.7215298414230347
I0312 15:08:15.664315 140028045932288 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.19877347350120544, loss=1.6802377700805664
I0312 15:08:52.153547 140028037539584 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.21996349096298218, loss=1.7637193202972412
I0312 15:09:28.625127 140028045932288 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.21108976006507874, loss=1.71535325050354
I0312 15:10:05.135111 140028037539584 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.18803349137306213, loss=1.6683759689331055
I0312 15:10:41.618942 140028045932288 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2180895060300827, loss=1.6739120483398438
I0312 15:11:18.109053 140028037539584 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.20424768328666687, loss=1.7529053688049316
I0312 15:11:54.590020 140028045932288 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.24226835370063782, loss=1.7161436080932617
I0312 15:12:31.066580 140028037539584 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.19476155936717987, loss=1.749806523323059
I0312 15:13:07.583726 140028045932288 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.20469318330287933, loss=1.6548945903778076
I0312 15:13:44.089333 140028037539584 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.22859051823616028, loss=1.7538115978240967
I0312 15:14:20.599850 140028045932288 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.27705663442611694, loss=1.7922669649124146
I0312 15:14:57.100080 140028037539584 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.20596566796302795, loss=1.723328948020935
I0312 15:15:33.622696 140028045932288 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.20861968398094177, loss=1.730629801750183
I0312 15:15:47.568836 140197733885760 spec.py:321] Evaluating on the training split.
I0312 15:15:50.627016 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:20:14.768356 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 15:20:17.477591 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:22:43.288618 140197733885760 spec.py:349] Evaluating on the test split.
I0312 15:22:46.009152 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:25:28.761678 140197733885760 submission_runner.py:420] Time since start: 39405.44s, 	Step: 64540, 	{'train/accuracy': 0.6574546694755554, 'train/loss': 1.6062756776809692, 'train/bleu': 32.33868366983711, 'validation/accuracy': 0.6742879748344421, 'validation/loss': 1.499944806098938, 'validation/bleu': 29.16130690093247, 'validation/num_examples': 3000, 'test/accuracy': 0.6869560480117798, 'test/loss': 1.4149410724639893, 'test/bleu': 29.01646062679263, 'test/num_examples': 3003, 'score': 23560.25806093216, 'total_duration': 39405.44167017937, 'accumulated_submission_time': 23560.25806093216, 'accumulated_eval_time': 15842.44847869873, 'accumulated_logging_time': 0.8506321907043457}
I0312 15:25:28.783937 140028037539584 logging_writer.py:48] [64540] accumulated_eval_time=15842.448479, accumulated_logging_time=0.850632, accumulated_submission_time=23560.258061, global_step=64540, preemption_count=0, score=23560.258061, test/accuracy=0.686956, test/bleu=29.016461, test/loss=1.414941, test/num_examples=3003, total_duration=39405.441670, train/accuracy=0.657455, train/bleu=32.338684, train/loss=1.606276, validation/accuracy=0.674288, validation/bleu=29.161307, validation/loss=1.499945, validation/num_examples=3000
I0312 15:25:50.785439 140028045932288 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2201831191778183, loss=1.7849962711334229
I0312 15:26:26.999925 140028037539584 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.1956993043422699, loss=1.5985394716262817
I0312 15:27:03.326775 140028045932288 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.20387126505374908, loss=1.6177551746368408
I0312 15:27:39.626033 140028037539584 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20435461401939392, loss=1.637414574623108
I0312 15:28:16.005624 140028045932288 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2042704075574875, loss=1.6640740633010864
I0312 15:28:52.395950 140028037539584 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.1998564600944519, loss=1.6219745874404907
I0312 15:29:28.809656 140028045932288 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.21295927464962006, loss=1.7102686166763306
I0312 15:30:05.208144 140028037539584 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.20654189586639404, loss=1.6691685914993286
I0312 15:30:41.662361 140028045932288 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.19371099770069122, loss=1.6827499866485596
I0312 15:31:18.129931 140028037539584 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.22094948589801788, loss=1.7421820163726807
I0312 15:31:54.643348 140028045932288 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.18542850017547607, loss=1.6966524124145508
I0312 15:32:31.160850 140028037539584 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.1850329041481018, loss=1.578011155128479
I0312 15:33:07.634631 140028045932288 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.2103533148765564, loss=1.668907880783081
I0312 15:33:44.122325 140028037539584 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.21714815497398376, loss=1.729174256324768
I0312 15:34:20.587461 140028045932288 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19268910586833954, loss=1.6406822204589844
I0312 15:34:57.039999 140028037539584 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.19488565623760223, loss=1.7171804904937744
I0312 15:35:33.545903 140028045932288 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.22356249392032623, loss=1.6847920417785645
I0312 15:36:10.067782 140028037539584 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.1994466930627823, loss=1.6398980617523193
I0312 15:36:46.547630 140028045932288 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.1941797435283661, loss=1.5649300813674927
I0312 15:37:23.044557 140028037539584 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.18925398588180542, loss=1.594150185585022
I0312 15:37:59.581608 140028045932288 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.22174310684204102, loss=1.7344510555267334
I0312 15:38:36.061346 140028037539584 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.18717718124389648, loss=1.6666537523269653
I0312 15:39:12.534036 140028045932288 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.21821704506874084, loss=1.6636004447937012
I0312 15:39:29.025988 140197733885760 spec.py:321] Evaluating on the training split.
I0312 15:39:32.085841 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:43:03.435650 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 15:43:06.156320 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:45:48.764272 140197733885760 spec.py:349] Evaluating on the test split.
I0312 15:45:51.496923 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 15:48:26.564774 140197733885760 submission_runner.py:420] Time since start: 40783.24s, 	Step: 66847, 	{'train/accuracy': 0.6589851975440979, 'train/loss': 1.6024383306503296, 'train/bleu': 32.50040292950039, 'validation/accuracy': 0.675317108631134, 'validation/loss': 1.4933512210845947, 'validation/bleu': 29.146627221735972, 'validation/num_examples': 3000, 'test/accuracy': 0.6883156299591064, 'test/loss': 1.4015823602676392, 'test/bleu': 28.970973133800218, 'test/num_examples': 3003, 'score': 24400.42298603058, 'total_duration': 40783.24477171898, 'accumulated_submission_time': 24400.42298603058, 'accumulated_eval_time': 16379.987210035324, 'accumulated_logging_time': 0.8830595016479492}
I0312 15:48:26.586552 140028037539584 logging_writer.py:48] [66847] accumulated_eval_time=16379.987210, accumulated_logging_time=0.883060, accumulated_submission_time=24400.422986, global_step=66847, preemption_count=0, score=24400.422986, test/accuracy=0.688316, test/bleu=28.970973, test/loss=1.401582, test/num_examples=3003, total_duration=40783.244772, train/accuracy=0.658985, train/bleu=32.500403, train/loss=1.602438, validation/accuracy=0.675317, validation/bleu=29.146627, validation/loss=1.493351, validation/num_examples=3000
I0312 15:48:46.119351 140028045932288 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.19734711945056915, loss=1.6550617218017578
I0312 15:49:22.348955 140028037539584 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.20140883326530457, loss=1.7163193225860596
I0312 15:49:58.647636 140028045932288 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.20762751996517181, loss=1.6939579248428345
I0312 15:50:35.038987 140028037539584 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.20502586662769318, loss=1.6801496744155884
I0312 15:51:11.423693 140028045932288 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.2170422524213791, loss=1.7018176317214966
I0312 15:51:47.897439 140028037539584 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.21890179812908173, loss=1.682886004447937
I0312 15:52:24.378428 140028045932288 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.21401870250701904, loss=1.5870290994644165
I0312 15:53:00.827260 140028037539584 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.19976374506950378, loss=1.603090763092041
I0312 15:53:37.303389 140028045932288 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.19556379318237305, loss=1.6938437223434448
I0312 15:54:13.792358 140028037539584 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.1939907819032669, loss=1.6931681632995605
I0312 15:54:50.266044 140028045932288 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2090459018945694, loss=1.772748589515686
I0312 15:55:26.741298 140028037539584 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2889842092990875, loss=1.6349194049835205
I0312 15:56:03.220698 140028045932288 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.22944709658622742, loss=1.671266794204712
I0312 15:56:39.691178 140028037539584 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.21018512547016144, loss=1.7894498109817505
I0312 15:57:16.206596 140028045932288 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.21157020330429077, loss=1.6956958770751953
I0312 15:57:52.708593 140028037539584 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.18888460099697113, loss=1.6382639408111572
I0312 15:58:29.203037 140028045932288 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.21215397119522095, loss=1.6847378015518188
I0312 15:59:05.720392 140028037539584 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.19827042520046234, loss=1.6821430921554565
I0312 15:59:42.196570 140028045932288 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.20773319900035858, loss=1.73068106174469
I0312 16:00:18.721084 140028037539584 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.23521484434604645, loss=1.7223554849624634
I0312 16:00:55.221630 140028045932288 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.1948215663433075, loss=1.6595467329025269
I0312 16:01:31.697295 140028037539584 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.20278313755989075, loss=1.6445950269699097
I0312 16:02:08.201229 140028045932288 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.211684912443161, loss=1.7171822786331177
I0312 16:02:26.895426 140197733885760 spec.py:321] Evaluating on the training split.
I0312 16:02:29.942887 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:06:37.548606 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 16:06:40.264479 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:09:31.918584 140197733885760 spec.py:349] Evaluating on the test split.
I0312 16:09:34.645489 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:12:37.917992 140197733885760 submission_runner.py:420] Time since start: 42234.60s, 	Step: 69153, 	{'train/accuracy': 0.6765788197517395, 'train/loss': 1.4881170988082886, 'train/bleu': 33.541029229499564, 'validation/accuracy': 0.676656186580658, 'validation/loss': 1.4876593351364136, 'validation/bleu': 29.058710968380737, 'validation/num_examples': 3000, 'test/accuracy': 0.689233660697937, 'test/loss': 1.3974084854125977, 'test/bleu': 29.031414589198896, 'test/num_examples': 3003, 'score': 25240.656269311905, 'total_duration': 42234.59798383713, 'accumulated_submission_time': 25240.656269311905, 'accumulated_eval_time': 16991.00972509384, 'accumulated_logging_time': 0.9139461517333984}
I0312 16:12:37.940484 140028037539584 logging_writer.py:48] [69153] accumulated_eval_time=16991.009725, accumulated_logging_time=0.913946, accumulated_submission_time=25240.656269, global_step=69153, preemption_count=0, score=25240.656269, test/accuracy=0.689234, test/bleu=29.031415, test/loss=1.397408, test/num_examples=3003, total_duration=42234.597984, train/accuracy=0.676579, train/bleu=33.541029, train/loss=1.488117, validation/accuracy=0.676656, validation/bleu=29.058711, validation/loss=1.487659, validation/num_examples=3000
I0312 16:12:55.261339 140028045932288 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2326342910528183, loss=1.738992691040039
I0312 16:13:31.425893 140028037539584 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.21880893409252167, loss=1.6543080806732178
I0312 16:14:07.688164 140028045932288 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.204618439078331, loss=1.6294071674346924
I0312 16:14:44.014405 140028037539584 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.20466050505638123, loss=1.6659363508224487
I0312 16:15:20.392760 140028045932288 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2173425853252411, loss=1.6769112348556519
I0312 16:15:56.779339 140028037539584 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2162296622991562, loss=1.6416332721710205
I0312 16:16:33.188739 140028045932288 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2054450511932373, loss=1.6322263479232788
I0312 16:17:09.624329 140028037539584 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.20117929577827454, loss=1.7015776634216309
I0312 16:17:46.073763 140028045932288 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.1964867115020752, loss=1.5805946588516235
I0312 16:18:22.550930 140028037539584 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.19895397126674652, loss=1.6774431467056274
I0312 16:18:59.043842 140028045932288 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.1985442191362381, loss=1.6234608888626099
I0312 16:19:35.516591 140028037539584 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.24615569412708282, loss=1.6332423686981201
I0312 16:20:11.979368 140028045932288 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2308947592973709, loss=1.6966464519500732
I0312 16:20:48.492174 140028037539584 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.20132717490196228, loss=1.651874303817749
I0312 16:21:24.974383 140028045932288 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.19552288949489594, loss=1.7318205833435059
I0312 16:22:01.459301 140028037539584 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.22793498635292053, loss=1.6930960416793823
I0312 16:22:37.966117 140028045932288 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.20536528527736664, loss=1.633823275566101
I0312 16:23:14.487141 140028037539584 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.21686163544654846, loss=1.7040667533874512
I0312 16:23:51.021248 140028045932288 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20811717212200165, loss=1.678328037261963
I0312 16:24:27.519314 140028037539584 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2017207145690918, loss=1.6464658975601196
I0312 16:25:04.003092 140028045932288 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.22793084383010864, loss=1.6433565616607666
I0312 16:25:40.522817 140028037539584 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.20358461141586304, loss=1.711303949356079
I0312 16:26:17.041550 140028045932288 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2066691368818283, loss=1.664096474647522
I0312 16:26:37.927587 140197733885760 spec.py:321] Evaluating on the training split.
I0312 16:26:40.981891 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:31:10.329898 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 16:31:13.056481 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:34:13.036657 140197733885760 spec.py:349] Evaluating on the test split.
I0312 16:34:15.768178 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:37:17.377017 140197733885760 submission_runner.py:420] Time since start: 43714.06s, 	Step: 71459, 	{'train/accuracy': 0.665115475654602, 'train/loss': 1.5632176399230957, 'train/bleu': 32.916180646277745, 'validation/accuracy': 0.6775985360145569, 'validation/loss': 1.4751025438308716, 'validation/bleu': 29.321900177988866, 'validation/num_examples': 3000, 'test/accuracy': 0.6905351281166077, 'test/loss': 1.3871610164642334, 'test/bleu': 28.987678393285474, 'test/num_examples': 3003, 'score': 26080.56746864319, 'total_duration': 43714.0570063591, 'accumulated_submission_time': 26080.56746864319, 'accumulated_eval_time': 17630.459098100662, 'accumulated_logging_time': 0.9451935291290283}
I0312 16:37:17.399922 140028037539584 logging_writer.py:48] [71459] accumulated_eval_time=17630.459098, accumulated_logging_time=0.945194, accumulated_submission_time=26080.567469, global_step=71459, preemption_count=0, score=26080.567469, test/accuracy=0.690535, test/bleu=28.987678, test/loss=1.387161, test/num_examples=3003, total_duration=43714.057006, train/accuracy=0.665115, train/bleu=32.916181, train/loss=1.563218, validation/accuracy=0.677599, validation/bleu=29.321900, validation/loss=1.475103, validation/num_examples=3000
I0312 16:37:32.586968 140028045932288 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.19655553996562958, loss=1.7093782424926758
I0312 16:38:08.835005 140028037539584 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.19366396963596344, loss=1.6944373846054077
I0312 16:38:45.197659 140028045932288 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2675657868385315, loss=1.656341791152954
I0312 16:39:21.528748 140028037539584 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.20108525454998016, loss=1.6621352434158325
I0312 16:39:57.919947 140028045932288 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.1848118007183075, loss=1.6434006690979004
I0312 16:40:34.309138 140028037539584 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.20145417749881744, loss=1.6338448524475098
I0312 16:41:10.770345 140028045932288 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.21032048761844635, loss=1.657387614250183
I0312 16:41:47.213888 140028037539584 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2113841027021408, loss=1.6654680967330933
I0312 16:42:23.676273 140028045932288 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2291862517595291, loss=1.688015103340149
I0312 16:43:00.131998 140028037539584 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.2312280833721161, loss=1.5942497253417969
I0312 16:43:36.610235 140028045932288 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.21342206001281738, loss=1.6484616994857788
I0312 16:44:13.072284 140028037539584 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.20622478425502777, loss=1.642564296722412
I0312 16:44:49.568860 140028045932288 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.20275957882404327, loss=1.6114258766174316
I0312 16:45:26.054482 140028037539584 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.21983270347118378, loss=1.717132806777954
I0312 16:46:02.540837 140028045932288 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.20782309770584106, loss=1.6221506595611572
I0312 16:46:39.047489 140028037539584 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.21790891885757446, loss=1.6626979112625122
I0312 16:47:15.529522 140028045932288 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2249603420495987, loss=1.648282527923584
I0312 16:47:52.049311 140028037539584 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.21073277294635773, loss=1.6302788257598877
I0312 16:48:28.550652 140028045932288 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2548738718032837, loss=1.6645746231079102
I0312 16:49:05.068078 140028037539584 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.22751305997371674, loss=1.7401401996612549
I0312 16:49:41.543495 140028045932288 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.1923915594816208, loss=1.5707175731658936
I0312 16:50:18.045673 140028037539584 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.24219860136508942, loss=1.6739407777786255
I0312 16:50:54.559307 140028045932288 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.19346612691879272, loss=1.665993332862854
I0312 16:51:17.643901 140197733885760 spec.py:321] Evaluating on the training split.
I0312 16:51:20.694407 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:55:38.633938 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 16:55:41.358653 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 16:58:09.492392 140197733885760 spec.py:349] Evaluating on the test split.
I0312 16:58:12.223961 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:00:41.217937 140197733885760 submission_runner.py:420] Time since start: 45117.90s, 	Step: 73765, 	{'train/accuracy': 0.6569005846977234, 'train/loss': 1.6199302673339844, 'train/bleu': 33.01953946347801, 'validation/accuracy': 0.6786896586418152, 'validation/loss': 1.4717580080032349, 'validation/bleu': 29.54283922963444, 'validation/num_examples': 3000, 'test/accuracy': 0.6927546262741089, 'test/loss': 1.3817939758300781, 'test/bleu': 28.917819255835166, 'test/num_examples': 3003, 'score': 26920.736166715622, 'total_duration': 45117.89793419838, 'accumulated_submission_time': 26920.736166715622, 'accumulated_eval_time': 18194.033081531525, 'accumulated_logging_time': 0.9769787788391113}
I0312 17:00:41.241321 140028037539584 logging_writer.py:48] [73765] accumulated_eval_time=18194.033082, accumulated_logging_time=0.976979, accumulated_submission_time=26920.736167, global_step=73765, preemption_count=0, score=26920.736167, test/accuracy=0.692755, test/bleu=28.917819, test/loss=1.381794, test/num_examples=3003, total_duration=45117.897934, train/accuracy=0.656901, train/bleu=33.019539, train/loss=1.619930, validation/accuracy=0.678690, validation/bleu=29.542839, validation/loss=1.471758, validation/num_examples=3000
I0312 17:00:54.273263 140028045932288 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.2034052163362503, loss=1.676103115081787
I0312 17:01:30.543671 140028037539584 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.23760558664798737, loss=1.6377791166305542
I0312 17:02:06.901632 140028045932288 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.21688972413539886, loss=1.6797916889190674
I0312 17:02:43.235024 140028037539584 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.22506345808506012, loss=1.6786679029464722
I0312 17:03:19.680325 140028045932288 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.21553204953670502, loss=1.6878107786178589
I0312 17:03:56.085797 140028037539584 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.21475420892238617, loss=1.6495722532272339
I0312 17:04:32.553319 140028045932288 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2024521827697754, loss=1.6367567777633667
I0312 17:05:08.980550 140028037539584 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.22464409470558167, loss=1.677959680557251
I0312 17:05:45.427456 140028045932288 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2078724354505539, loss=1.617316484451294
I0312 17:06:21.926843 140028037539584 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.21569980680942535, loss=1.6642751693725586
I0312 17:06:58.404027 140028045932288 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.6292731761932373, loss=1.6771742105484009
I0312 17:07:34.871495 140028037539584 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.2008437216281891, loss=1.6229336261749268
I0312 17:08:11.349888 140028045932288 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2612362504005432, loss=1.5995680093765259
I0312 17:08:47.822715 140028037539584 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.19165337085723877, loss=1.6559373140335083
I0312 17:09:24.338681 140028045932288 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.21640010178089142, loss=1.6746407747268677
I0312 17:10:00.794770 140028037539584 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.21228206157684326, loss=1.6375864744186401
I0312 17:10:37.267423 140028045932288 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20552557706832886, loss=1.6205394268035889
I0312 17:11:13.785952 140028037539584 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.18867649137973785, loss=1.6096563339233398
I0312 17:11:50.280318 140028045932288 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2023421972990036, loss=1.6542342901229858
I0312 17:12:26.758391 140028037539584 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.19918066263198853, loss=1.716376781463623
I0312 17:13:03.247664 140028045932288 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2217799425125122, loss=1.73920476436615
I0312 17:13:39.774221 140028037539584 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.21526244282722473, loss=1.7008287906646729
I0312 17:14:16.259422 140028045932288 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.21037603914737701, loss=1.6362296342849731
I0312 17:14:41.520466 140197733885760 spec.py:321] Evaluating on the training split.
I0312 17:14:44.568250 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:18:39.923414 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 17:18:42.651910 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:21:14.241218 140197733885760 spec.py:349] Evaluating on the test split.
I0312 17:21:16.973575 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:23:52.784741 140197733885760 submission_runner.py:420] Time since start: 46509.46s, 	Step: 76071, 	{'train/accuracy': 0.6729021668434143, 'train/loss': 1.506928563117981, 'train/bleu': 33.29321228860028, 'validation/accuracy': 0.6776605248451233, 'validation/loss': 1.4689043760299683, 'validation/bleu': 29.190714508145124, 'validation/num_examples': 3000, 'test/accuracy': 0.6914531588554382, 'test/loss': 1.3808372020721436, 'test/bleu': 28.920786485322584, 'test/num_examples': 3003, 'score': 27760.940415620804, 'total_duration': 46509.464745759964, 'accumulated_submission_time': 27760.940415620804, 'accumulated_eval_time': 18745.29731321335, 'accumulated_logging_time': 1.009082555770874}
I0312 17:23:52.809027 140028037539584 logging_writer.py:48] [76071] accumulated_eval_time=18745.297313, accumulated_logging_time=1.009083, accumulated_submission_time=27760.940416, global_step=76071, preemption_count=0, score=27760.940416, test/accuracy=0.691453, test/bleu=28.920786, test/loss=1.380837, test/num_examples=3003, total_duration=46509.464746, train/accuracy=0.672902, train/bleu=33.293212, train/loss=1.506929, validation/accuracy=0.677661, validation/bleu=29.190715, validation/loss=1.468904, validation/num_examples=3000
I0312 17:24:03.642132 140028045932288 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.19836701452732086, loss=1.5759238004684448
I0312 17:24:39.876996 140028037539584 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21220548450946808, loss=1.6415066719055176
I0312 17:25:16.148438 140028045932288 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.192958801984787, loss=1.6062328815460205
I0312 17:25:52.515333 140028037539584 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2151368111371994, loss=1.7371083498001099
I0312 17:26:28.877541 140028045932288 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.20463044941425323, loss=1.6183604001998901
I0312 17:27:05.310285 140028037539584 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.20265594124794006, loss=1.602338194847107
I0312 17:27:41.740721 140028045932288 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.21425855159759521, loss=1.6576807498931885
I0312 17:28:18.196907 140028037539584 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.20925003290176392, loss=1.7484371662139893
I0312 17:28:54.669556 140028045932288 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.2074500024318695, loss=1.6400870084762573
I0312 17:29:31.150100 140028037539584 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.21385309100151062, loss=1.657084345817566
I0312 17:30:07.604080 140028045932288 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.21090765297412872, loss=1.6732687950134277
I0312 17:30:44.086779 140028037539584 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2218407243490219, loss=1.6142184734344482
I0312 17:31:20.582357 140028045932288 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21533001959323883, loss=1.65084969997406
I0312 17:31:57.059862 140028037539584 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.1996607631444931, loss=1.6036089658737183
I0312 17:32:33.573797 140028045932288 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.21840481460094452, loss=1.6366963386535645
I0312 17:33:10.065026 140028037539584 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.2080630511045456, loss=1.6472842693328857
I0312 17:33:46.529008 140028045932288 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.2094026505947113, loss=1.6300526857376099
I0312 17:34:23.006340 140028037539584 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.22886046767234802, loss=1.6946322917938232
I0312 17:34:59.485948 140028045932288 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.20224781334400177, loss=1.6635874509811401
I0312 17:35:35.950862 140028037539584 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.20822377502918243, loss=1.6582388877868652
I0312 17:36:12.447637 140028045932288 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2113012969493866, loss=1.6505507230758667
I0312 17:36:49.006188 140028037539584 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.2189721018075943, loss=1.7107676267623901
I0312 17:37:25.507940 140028045932288 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2097659409046173, loss=1.6336495876312256
I0312 17:37:52.943183 140197733885760 spec.py:321] Evaluating on the training split.
I0312 17:37:56.002193 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:42:21.408803 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 17:42:24.125016 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:45:53.190666 140197733885760 spec.py:349] Evaluating on the test split.
I0312 17:45:55.910632 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 17:49:27.809409 140197733885760 submission_runner.py:420] Time since start: 48044.49s, 	Step: 78377, 	{'train/accuracy': 0.6650096774101257, 'train/loss': 1.5661970376968384, 'train/bleu': 33.123997561018435, 'validation/accuracy': 0.6799543499946594, 'validation/loss': 1.4611165523529053, 'validation/bleu': 29.594720362186916, 'validation/num_examples': 3000, 'test/accuracy': 0.6954041123390198, 'test/loss': 1.366335391998291, 'test/bleu': 29.177742617319534, 'test/num_examples': 3003, 'score': 28600.997208356857, 'total_duration': 48044.48940658569, 'accumulated_submission_time': 28600.997208356857, 'accumulated_eval_time': 19440.16348552704, 'accumulated_logging_time': 1.0446865558624268}
I0312 17:49:27.833755 140028037539584 logging_writer.py:48] [78377] accumulated_eval_time=19440.163486, accumulated_logging_time=1.044687, accumulated_submission_time=28600.997208, global_step=78377, preemption_count=0, score=28600.997208, test/accuracy=0.695404, test/bleu=29.177743, test/loss=1.366335, test/num_examples=3003, total_duration=48044.489407, train/accuracy=0.665010, train/bleu=33.123998, train/loss=1.566197, validation/accuracy=0.679954, validation/bleu=29.594720, validation/loss=1.461117, validation/num_examples=3000
I0312 17:49:36.483119 140028045932288 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20609423518180847, loss=1.5514708757400513
I0312 17:50:12.584829 140028037539584 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.1986343264579773, loss=1.6385880708694458
I0312 17:50:48.803825 140028045932288 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2130349725484848, loss=1.7163091897964478
I0312 17:51:25.149453 140028037539584 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.21632549166679382, loss=1.6071746349334717
I0312 17:52:01.466675 140028045932288 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2248176485300064, loss=1.6604670286178589
I0312 17:52:37.877956 140028037539584 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.19273068010807037, loss=1.6992650032043457
I0312 17:53:14.304104 140028045932288 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.20815950632095337, loss=1.5679434537887573
I0312 17:53:50.742495 140028037539584 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.2230069935321808, loss=1.6086843013763428
I0312 17:54:27.210579 140028045932288 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.20472557842731476, loss=1.6727350950241089
I0312 17:55:03.669482 140028037539584 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20359592139720917, loss=1.5759484767913818
I0312 17:55:40.116192 140028045932288 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.2347969263792038, loss=1.6834913492202759
I0312 17:56:16.585588 140028037539584 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.1933339685201645, loss=1.6961854696273804
I0312 17:56:53.038850 140028045932288 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.21544741094112396, loss=1.619665503501892
I0312 17:57:29.509712 140028037539584 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.19898143410682678, loss=1.7220309972763062
I0312 17:58:05.985479 140028045932288 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.23088233172893524, loss=1.709194302558899
I0312 17:58:42.474069 140028037539584 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.20373749732971191, loss=1.6225389242172241
I0312 17:59:18.965267 140028045932288 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.22100520133972168, loss=1.65071439743042
I0312 17:59:55.480850 140028037539584 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.20743994414806366, loss=1.61980402469635
I0312 18:00:31.970556 140028045932288 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.21073660254478455, loss=1.574964165687561
I0312 18:01:08.466744 140028037539584 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2036544531583786, loss=1.7061816453933716
I0312 18:01:44.993583 140028045932288 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.219396710395813, loss=1.6729563474655151
I0312 18:02:21.475829 140028037539584 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.21574248373508453, loss=1.6432859897613525
I0312 18:02:57.948374 140028045932288 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.23083476722240448, loss=1.6139153242111206
I0312 18:03:27.950080 140197733885760 spec.py:321] Evaluating on the training split.
I0312 18:03:31.007312 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:07:45.605669 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 18:07:48.333609 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:10:31.955967 140197733885760 spec.py:349] Evaluating on the test split.
I0312 18:10:34.685891 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:13:10.816315 140197733885760 submission_runner.py:420] Time since start: 49467.50s, 	Step: 80684, 	{'train/accuracy': 0.66160649061203, 'train/loss': 1.584054946899414, 'train/bleu': 33.317098467297384, 'validation/accuracy': 0.6825581789016724, 'validation/loss': 1.45094633102417, 'validation/bleu': 29.821797940794205, 'validation/num_examples': 3000, 'test/accuracy': 0.6970890760421753, 'test/loss': 1.3583707809448242, 'test/bleu': 29.614426414284896, 'test/num_examples': 3003, 'score': 29441.037866830826, 'total_duration': 49467.49631810188, 'accumulated_submission_time': 29441.037866830826, 'accumulated_eval_time': 20023.029673337936, 'accumulated_logging_time': 1.0782907009124756}
I0312 18:13:10.841536 140028037539584 logging_writer.py:48] [80684] accumulated_eval_time=20023.029673, accumulated_logging_time=1.078291, accumulated_submission_time=29441.037867, global_step=80684, preemption_count=0, score=29441.037867, test/accuracy=0.697089, test/bleu=29.614426, test/loss=1.358371, test/num_examples=3003, total_duration=49467.496318, train/accuracy=0.661606, train/bleu=33.317098, train/loss=1.584055, validation/accuracy=0.682558, validation/bleu=29.821798, validation/loss=1.450946, validation/num_examples=3000
I0312 18:13:16.981971 140028045932288 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20196883380413055, loss=1.5804815292358398
I0312 18:13:53.204717 140028037539584 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.20211759209632874, loss=1.6075016260147095
I0312 18:14:29.506171 140028045932288 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20350486040115356, loss=1.6979225873947144
I0312 18:15:05.893079 140028037539584 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.19705688953399658, loss=1.5348199605941772
I0312 18:15:42.250329 140028045932288 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.18646281957626343, loss=1.612362265586853
I0312 18:16:18.686862 140028037539584 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.2179139405488968, loss=1.6037323474884033
I0312 18:16:55.080752 140028045932288 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.21687497198581696, loss=1.677668809890747
I0312 18:17:31.535077 140028037539584 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.2165285348892212, loss=1.6841007471084595
I0312 18:18:07.982630 140028045932288 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.20595760643482208, loss=1.601263403892517
I0312 18:18:44.420283 140028037539584 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.2188023179769516, loss=1.6704927682876587
I0312 18:19:20.863226 140028045932288 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.1977817565202713, loss=1.5544819831848145
I0312 18:19:57.317607 140028037539584 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.2130787968635559, loss=1.658048152923584
I0312 18:20:33.803868 140028045932288 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.19198811054229736, loss=1.6375681161880493
I0312 18:21:10.296093 140028037539584 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2522692382335663, loss=1.5865225791931152
I0312 18:21:46.787328 140028045932288 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.22239159047603607, loss=1.6145529747009277
I0312 18:22:23.302611 140028037539584 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.2043730765581131, loss=1.594767451286316
I0312 18:22:59.805804 140028045932288 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.21364150941371918, loss=1.6618787050247192
I0312 18:23:36.284979 140028037539584 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.22253800928592682, loss=1.645329236984253
I0312 18:24:12.792477 140028045932288 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.19105498492717743, loss=1.5950953960418701
I0312 18:24:49.282277 140028037539584 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.2056041955947876, loss=1.6565312147140503
I0312 18:25:25.765176 140028045932288 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.20683805644512177, loss=1.6684156656265259
I0312 18:26:02.256147 140028037539584 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.25321388244628906, loss=1.6476489305496216
I0312 18:26:38.763314 140028045932288 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2057703733444214, loss=1.6114286184310913
I0312 18:27:10.931662 140197733885760 spec.py:321] Evaluating on the training split.
I0312 18:27:13.977964 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:31:36.452730 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 18:31:39.175063 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:34:23.417006 140197733885760 spec.py:349] Evaluating on the test split.
I0312 18:34:26.143655 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:36:59.286080 140197733885760 submission_runner.py:420] Time since start: 50895.97s, 	Step: 82990, 	{'train/accuracy': 0.6741752624511719, 'train/loss': 1.5034538507461548, 'train/bleu': 33.5210667999066, 'validation/accuracy': 0.6817522048950195, 'validation/loss': 1.444745421409607, 'validation/bleu': 29.79512560255042, 'validation/num_examples': 3000, 'test/accuracy': 0.6958457231521606, 'test/loss': 1.355960488319397, 'test/bleu': 29.732062058134858, 'test/num_examples': 3003, 'score': 30281.050943374634, 'total_duration': 50895.96606874466, 'accumulated_submission_time': 30281.050943374634, 'accumulated_eval_time': 20611.38403081894, 'accumulated_logging_time': 1.1141114234924316}
I0312 18:36:59.310975 140028037539584 logging_writer.py:48] [82990] accumulated_eval_time=20611.384031, accumulated_logging_time=1.114111, accumulated_submission_time=30281.050943, global_step=82990, preemption_count=0, score=30281.050943, test/accuracy=0.695846, test/bleu=29.732062, test/loss=1.355960, test/num_examples=3003, total_duration=50895.966069, train/accuracy=0.674175, train/bleu=33.521067, train/loss=1.503454, validation/accuracy=0.681752, validation/bleu=29.795126, validation/loss=1.444745, validation/num_examples=3000
I0312 18:37:03.280916 140028045932288 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.21025997400283813, loss=1.5527805089950562
I0312 18:37:39.472865 140028037539584 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.19338108599185944, loss=1.5747255086898804
I0312 18:38:15.746852 140028045932288 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.2071010321378708, loss=1.5916740894317627
I0312 18:38:52.105064 140028037539584 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.22544662654399872, loss=1.6895511150360107
I0312 18:39:28.464901 140028045932288 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.2118566632270813, loss=1.588021159172058
I0312 18:40:04.896400 140028037539584 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.2025732696056366, loss=1.6301910877227783
I0312 18:40:41.342240 140028045932288 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.22088272869586945, loss=1.62855064868927
I0312 18:41:17.787278 140028037539584 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.20789024233818054, loss=1.6530417203903198
I0312 18:41:54.236126 140028045932288 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.20798403024673462, loss=1.6062573194503784
I0312 18:42:30.702598 140028037539584 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.204305037856102, loss=1.62888765335083
I0312 18:43:07.196316 140028045932288 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21821531653404236, loss=1.6569384336471558
I0312 18:43:43.682415 140028037539584 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.20823891460895538, loss=1.5993413925170898
I0312 18:44:20.205176 140028045932288 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.20264865458011627, loss=1.6008269786834717
I0312 18:44:56.667850 140028037539584 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.1943342536687851, loss=1.5464744567871094
I0312 18:45:33.160703 140028045932288 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.22664019465446472, loss=1.5975810289382935
I0312 18:46:09.630242 140028037539584 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.20657587051391602, loss=1.5704048871994019
I0312 18:46:46.128325 140028045932288 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.20959414541721344, loss=1.5651904344558716
I0312 18:47:22.601023 140028037539584 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.23389922082424164, loss=1.6051105260849
I0312 18:47:59.107547 140028045932288 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.20175951719284058, loss=1.6594667434692383
I0312 18:48:35.607133 140028037539584 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.19209375977516174, loss=1.508467197418213
I0312 18:49:12.113837 140028045932288 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2096649557352066, loss=1.5881036520004272
I0312 18:49:48.593715 140028037539584 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.21592290699481964, loss=1.502843976020813
I0312 18:50:25.124026 140028045932288 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.21195587515830994, loss=1.578543782234192
I0312 18:50:59.541094 140197733885760 spec.py:321] Evaluating on the training split.
I0312 18:51:02.613438 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:55:01.044822 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 18:55:03.779714 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 18:58:18.331611 140197733885760 spec.py:349] Evaluating on the test split.
I0312 18:58:21.061026 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:01:32.462620 140197733885760 submission_runner.py:420] Time since start: 52369.14s, 	Step: 85296, 	{'train/accuracy': 0.6712327003479004, 'train/loss': 1.5273113250732422, 'train/bleu': 33.137000770129596, 'validation/accuracy': 0.6831037402153015, 'validation/loss': 1.4382604360580444, 'validation/bleu': 29.524563702331893, 'validation/num_examples': 3000, 'test/accuracy': 0.6988786458969116, 'test/loss': 1.3426960706710815, 'test/bleu': 29.391099964317444, 'test/num_examples': 3003, 'score': 31121.205088377, 'total_duration': 52369.14261388779, 'accumulated_submission_time': 31121.205088377, 'accumulated_eval_time': 21244.305504322052, 'accumulated_logging_time': 1.1484274864196777}
I0312 19:01:32.490440 140028037539584 logging_writer.py:48] [85296] accumulated_eval_time=21244.305504, accumulated_logging_time=1.148427, accumulated_submission_time=31121.205088, global_step=85296, preemption_count=0, score=31121.205088, test/accuracy=0.698879, test/bleu=29.391100, test/loss=1.342696, test/num_examples=3003, total_duration=52369.142614, train/accuracy=0.671233, train/bleu=33.137001, train/loss=1.527311, validation/accuracy=0.683104, validation/bleu=29.524564, validation/loss=1.438260, validation/num_examples=3000
I0312 19:01:34.310265 140028045932288 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.20699100196361542, loss=1.6516096591949463
I0312 19:02:10.458358 140028037539584 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.21630162000656128, loss=1.61664617061615
I0312 19:02:46.727371 140028045932288 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.19894899427890778, loss=1.549526572227478
I0312 19:03:23.111256 140028037539584 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.1979535073041916, loss=1.6139522790908813
I0312 19:03:59.470048 140028045932288 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.26673081517219543, loss=1.6321505308151245
I0312 19:04:35.898016 140028037539584 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.22066408395767212, loss=1.5814049243927002
I0312 19:05:12.329936 140028045932288 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.1974828988313675, loss=1.5814646482467651
I0312 19:05:48.794937 140028037539584 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.20810678601264954, loss=1.6124687194824219
I0312 19:06:25.209936 140028045932288 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.20799453556537628, loss=1.585471510887146
I0312 19:07:01.697521 140028037539584 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.22020378708839417, loss=1.646645426750183
I0312 19:07:38.184021 140028045932288 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.20677052438259125, loss=1.6584322452545166
I0312 19:08:14.677670 140028037539584 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21408531069755554, loss=1.567903995513916
I0312 19:08:51.135778 140028045932288 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2203092724084854, loss=1.6778442859649658
I0312 19:09:27.638961 140028037539584 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.22362637519836426, loss=1.6196318864822388
I0312 19:10:04.110538 140028045932288 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.20751012861728668, loss=1.578160285949707
I0312 19:10:40.595294 140028037539584 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.20922061800956726, loss=1.558518886566162
I0312 19:11:17.077173 140028045932288 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2180451899766922, loss=1.6194826364517212
I0312 19:11:53.585560 140028037539584 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2160562127828598, loss=1.580457091331482
I0312 19:12:30.090798 140028045932288 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.22909413278102875, loss=1.5773676633834839
I0312 19:13:06.587153 140028037539584 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.20381580293178558, loss=1.5413936376571655
I0312 19:13:43.066498 140028045932288 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.20325450599193573, loss=1.6228806972503662
I0312 19:14:19.584986 140028037539584 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.22453798353672028, loss=1.6406610012054443
I0312 19:14:56.103189 140028045932288 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2101929783821106, loss=1.4960618019104004
I0312 19:15:32.610810 140028037539584 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.218196302652359, loss=1.654266357421875
I0312 19:15:32.616932 140197733885760 spec.py:321] Evaluating on the training split.
I0312 19:15:35.382269 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:19:32.779397 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 19:19:35.501017 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:22:17.328473 140197733885760 spec.py:349] Evaluating on the test split.
I0312 19:22:20.036657 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:25:14.175338 140197733885760 submission_runner.py:420] Time since start: 53790.86s, 	Step: 87601, 	{'train/accuracy': 0.7025133371353149, 'train/loss': 1.3403351306915283, 'train/bleu': 36.16159872807506, 'validation/accuracy': 0.6858935356140137, 'validation/loss': 1.4293599128723145, 'validation/bleu': 30.037808770372497, 'validation/num_examples': 3000, 'test/accuracy': 0.7010400295257568, 'test/loss': 1.3378076553344727, 'test/bleu': 29.907318585279327, 'test/num_examples': 3003, 'score': 31961.25281882286, 'total_duration': 53790.85534501076, 'accumulated_submission_time': 31961.25281882286, 'accumulated_eval_time': 21825.863842248917, 'accumulated_logging_time': 1.1863529682159424}
I0312 19:25:14.200457 140028045932288 logging_writer.py:48] [87601] accumulated_eval_time=21825.863842, accumulated_logging_time=1.186353, accumulated_submission_time=31961.252819, global_step=87601, preemption_count=0, score=31961.252819, test/accuracy=0.701040, test/bleu=29.907319, test/loss=1.337808, test/num_examples=3003, total_duration=53790.855345, train/accuracy=0.702513, train/bleu=36.161599, train/loss=1.340335, validation/accuracy=0.685894, validation/bleu=30.037809, validation/loss=1.429360, validation/num_examples=3000
I0312 19:25:50.340557 140028037539584 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.22014281153678894, loss=1.7069451808929443
I0312 19:26:26.537614 140028045932288 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.20252811908721924, loss=1.4849427938461304
I0312 19:27:02.851819 140028037539584 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.21786047518253326, loss=1.6448404788970947
I0312 19:27:39.232379 140028045932288 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.2043570876121521, loss=1.631003737449646
I0312 19:28:15.665229 140028037539584 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.21581079065799713, loss=1.5684881210327148
I0312 19:28:52.084323 140028045932288 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.22241654992103577, loss=1.5587553977966309
I0312 19:29:28.510442 140028037539584 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.19349688291549683, loss=1.568634033203125
I0312 19:30:04.956139 140028045932288 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.20872965455055237, loss=1.6603007316589355
I0312 19:30:41.387832 140028037539584 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21435090899467468, loss=1.6149870157241821
I0312 19:31:17.845082 140028045932288 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.20126844942569733, loss=1.587786316871643
I0312 19:31:54.318234 140028037539584 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.22555287182331085, loss=1.6022781133651733
I0312 19:32:30.812670 140028045932288 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.251787930727005, loss=1.616461157798767
I0312 19:33:07.277613 140028037539584 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.20073610544204712, loss=1.5435481071472168
I0312 19:33:43.776183 140028045932288 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.22316870093345642, loss=1.5810483694076538
I0312 19:34:20.268509 140028037539584 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.22553108632564545, loss=1.6011357307434082
I0312 19:34:56.793344 140028045932288 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2204698920249939, loss=1.6317483186721802
I0312 19:35:33.316268 140028037539584 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.2093852311372757, loss=1.5875253677368164
I0312 19:36:09.811784 140028045932288 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.32396069169044495, loss=1.5291589498519897
I0312 19:36:46.312239 140028037539584 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2189330905675888, loss=1.5336947441101074
I0312 19:37:22.809208 140028045932288 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.22398142516613007, loss=1.6051387786865234
I0312 19:37:59.311560 140028037539584 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.21339105069637299, loss=1.6017053127288818
I0312 19:38:35.834093 140028045932288 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.20971424877643585, loss=1.5311110019683838
I0312 19:39:12.330012 140028037539584 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2121255099773407, loss=1.5334992408752441
I0312 19:39:14.239778 140197733885760 spec.py:321] Evaluating on the training split.
I0312 19:39:17.294188 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:43:26.441649 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 19:43:29.153657 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:46:46.019657 140197733885760 spec.py:349] Evaluating on the test split.
I0312 19:46:48.742196 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 19:50:17.568750 140197733885760 submission_runner.py:420] Time since start: 55294.25s, 	Step: 89907, 	{'train/accuracy': 0.6764506101608276, 'train/loss': 1.486994981765747, 'train/bleu': 33.920997173484295, 'validation/accuracy': 0.6865506768226624, 'validation/loss': 1.4231069087982178, 'validation/bleu': 30.330018269000004, 'validation/num_examples': 3000, 'test/accuracy': 0.7012608647346497, 'test/loss': 1.3300431966781616, 'test/bleu': 29.77899500255573, 'test/num_examples': 3003, 'score': 32801.215609788895, 'total_duration': 55294.24873948097, 'accumulated_submission_time': 32801.215609788895, 'accumulated_eval_time': 22489.192754983902, 'accumulated_logging_time': 1.2217700481414795}
I0312 19:50:17.594419 140028045932288 logging_writer.py:48] [89907] accumulated_eval_time=22489.192755, accumulated_logging_time=1.221770, accumulated_submission_time=32801.215610, global_step=89907, preemption_count=0, score=32801.215610, test/accuracy=0.701261, test/bleu=29.778995, test/loss=1.330043, test/num_examples=3003, total_duration=55294.248739, train/accuracy=0.676451, train/bleu=33.920997, train/loss=1.486995, validation/accuracy=0.686551, validation/bleu=30.330018, validation/loss=1.423107, validation/num_examples=3000
I0312 19:50:51.529125 140028037539584 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20571164786815643, loss=1.6329258680343628
I0312 19:51:27.775600 140028045932288 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.21081194281578064, loss=1.5589818954467773
I0312 19:52:04.110381 140028037539584 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2152712047100067, loss=1.6198785305023193
I0312 19:52:40.503698 140028045932288 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2292550951242447, loss=1.5137557983398438
I0312 19:53:16.870727 140028037539584 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.297712117433548, loss=1.5527316331863403
I0312 19:53:53.303668 140028045932288 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.20873861014842987, loss=1.583396553993225
I0312 19:54:29.729255 140028037539584 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.20286323130130768, loss=1.6131796836853027
I0312 19:55:06.175705 140028045932288 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20329037308692932, loss=1.5157544612884521
I0312 19:55:42.604783 140028037539584 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.24590006470680237, loss=1.5867116451263428
I0312 19:56:19.092643 140028045932288 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.2258712500333786, loss=1.6014914512634277
I0312 19:56:55.553831 140028037539584 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21404825150966644, loss=1.5307071208953857
I0312 19:57:32.028661 140028045932288 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.21976901590824127, loss=1.5472933053970337
I0312 19:58:08.499245 140028037539584 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.22389549016952515, loss=1.6544827222824097
I0312 19:58:45.012780 140028045932288 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.2237551361322403, loss=1.5648925304412842
I0312 19:59:21.518852 140028037539584 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.20510873198509216, loss=1.5673831701278687
I0312 19:59:57.987066 140028045932288 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.20955754816532135, loss=1.586676836013794
I0312 20:00:34.464585 140028037539584 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.2050774246454239, loss=1.5208921432495117
I0312 20:01:10.946229 140028045932288 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.20985446870326996, loss=1.6175605058670044
I0312 20:01:47.441598 140028037539584 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.22386862337589264, loss=1.5704323053359985
I0312 20:02:23.951677 140028045932288 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.21330763399600983, loss=1.5397518873214722
I0312 20:03:00.435835 140028037539584 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22377009689807892, loss=1.6174958944320679
I0312 20:03:36.899793 140028045932288 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.21413180232048035, loss=1.6398099660873413
I0312 20:04:13.393942 140028037539584 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.2050967812538147, loss=1.5443974733352661
I0312 20:04:17.854167 140197733885760 spec.py:321] Evaluating on the training split.
I0312 20:04:20.909753 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:08:54.934911 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 20:08:57.657635 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:12:14.783237 140197733885760 spec.py:349] Evaluating on the test split.
I0312 20:12:17.509917 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:15:49.934944 140197733885760 submission_runner.py:420] Time since start: 56826.61s, 	Step: 92214, 	{'train/accuracy': 0.6773262619972229, 'train/loss': 1.4910597801208496, 'train/bleu': 33.9196685232179, 'validation/accuracy': 0.6862035393714905, 'validation/loss': 1.423099160194397, 'validation/bleu': 30.39234321071231, 'validation/num_examples': 3000, 'test/accuracy': 0.7024693489074707, 'test/loss': 1.3231168985366821, 'test/bleu': 29.701317958302273, 'test/num_examples': 3003, 'score': 33641.39935684204, 'total_duration': 56826.614934921265, 'accumulated_submission_time': 33641.39935684204, 'accumulated_eval_time': 23181.273468255997, 'accumulated_logging_time': 1.2571892738342285}
I0312 20:15:49.960827 140028045932288 logging_writer.py:48] [92214] accumulated_eval_time=23181.273468, accumulated_logging_time=1.257189, accumulated_submission_time=33641.399357, global_step=92214, preemption_count=0, score=33641.399357, test/accuracy=0.702469, test/bleu=29.701318, test/loss=1.323117, test/num_examples=3003, total_duration=56826.614935, train/accuracy=0.677326, train/bleu=33.919669, train/loss=1.491060, validation/accuracy=0.686204, validation/bleu=30.392343, validation/loss=1.423099, validation/num_examples=3000
I0312 20:16:21.418684 140028037539584 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.22337836027145386, loss=1.5589938163757324
I0312 20:16:57.734162 140028045932288 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.21836847066879272, loss=1.6223479509353638
I0312 20:17:34.052607 140028037539584 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22515231370925903, loss=1.5915067195892334
I0312 20:18:10.403182 140028045932288 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.21802780032157898, loss=1.5892040729522705
I0312 20:18:46.832262 140028037539584 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.22460134327411652, loss=1.6499083042144775
I0312 20:19:23.231351 140028045932288 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.20931704342365265, loss=1.545749545097351
I0312 20:19:59.676141 140028037539584 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2207254320383072, loss=1.691231608390808
I0312 20:20:36.134302 140028045932288 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.23653723299503326, loss=1.5976980924606323
I0312 20:21:12.577280 140028037539584 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.24783074855804443, loss=1.6597442626953125
I0312 20:21:49.020412 140028045932288 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.20849190652370453, loss=1.5908849239349365
I0312 20:22:25.501064 140028037539584 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.2073976695537567, loss=1.6338831186294556
I0312 20:23:01.982886 140028045932288 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.2228633612394333, loss=1.583322525024414
I0312 20:23:38.473439 140028037539584 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.23373492062091827, loss=1.5515385866165161
I0312 20:24:14.944465 140028045932288 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.20971260964870453, loss=1.600650429725647
I0312 20:24:51.414985 140028037539584 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.229802668094635, loss=1.5949634313583374
I0312 20:25:27.942189 140028045932288 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.21851760149002075, loss=1.5226836204528809
I0312 20:26:04.456313 140028037539584 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.2702086269855499, loss=1.6133285760879517
I0312 20:26:40.942393 140028045932288 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21713310480117798, loss=1.6549129486083984
I0312 20:27:17.439250 140028037539584 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.20824068784713745, loss=1.5196698904037476
I0312 20:27:53.919425 140028045932288 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.21570706367492676, loss=1.5724936723709106
I0312 20:28:30.393699 140028037539584 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.2204357385635376, loss=1.5879639387130737
I0312 20:29:06.899598 140028045932288 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.22566479444503784, loss=1.6256078481674194
I0312 20:29:43.369653 140028037539584 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.214257150888443, loss=1.5938276052474976
I0312 20:29:50.031349 140197733885760 spec.py:321] Evaluating on the training split.
I0312 20:29:53.084428 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:33:17.800912 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 20:33:20.522805 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:35:38.259502 140197733885760 spec.py:349] Evaluating on the test split.
I0312 20:35:40.984183 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:38:00.437740 140197733885760 submission_runner.py:420] Time since start: 58157.12s, 	Step: 94520, 	{'train/accuracy': 0.6835636496543884, 'train/loss': 1.435005784034729, 'train/bleu': 34.798970108395004, 'validation/accuracy': 0.6891916990280151, 'validation/loss': 1.411435604095459, 'validation/bleu': 30.261402817179807, 'validation/num_examples': 3000, 'test/accuracy': 0.7016210556030273, 'test/loss': 1.318655252456665, 'test/bleu': 29.794721474995853, 'test/num_examples': 3003, 'score': 34481.39404511452, 'total_duration': 58157.11773443222, 'accumulated_submission_time': 34481.39404511452, 'accumulated_eval_time': 23671.679799079895, 'accumulated_logging_time': 1.292123556137085}
I0312 20:38:00.464832 140028045932288 logging_writer.py:48] [94520] accumulated_eval_time=23671.679799, accumulated_logging_time=1.292124, accumulated_submission_time=34481.394045, global_step=94520, preemption_count=0, score=34481.394045, test/accuracy=0.701621, test/bleu=29.794721, test/loss=1.318655, test/num_examples=3003, total_duration=58157.117734, train/accuracy=0.683564, train/bleu=34.798970, train/loss=1.435006, validation/accuracy=0.689192, validation/bleu=30.261403, validation/loss=1.411436, validation/num_examples=3000
I0312 20:38:29.768040 140028037539584 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.22979281842708588, loss=1.5603480339050293
I0312 20:39:06.040923 140028045932288 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.22135743498802185, loss=1.5728685855865479
I0312 20:39:42.394404 140028037539584 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.213736429810524, loss=1.559433937072754
I0312 20:40:18.761141 140028045932288 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.22146983444690704, loss=1.5655046701431274
I0312 20:40:55.134856 140028037539584 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.2128203809261322, loss=1.5762759447097778
I0312 20:41:31.541168 140028045932288 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22154124081134796, loss=1.599152684211731
I0312 20:42:07.973108 140028037539584 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.22637605667114258, loss=1.588951826095581
I0312 20:42:44.381034 140028045932288 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21843872964382172, loss=1.5626676082611084
I0312 20:43:20.829355 140028037539584 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2106642872095108, loss=1.5118556022644043
I0312 20:43:57.313393 140028045932288 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.22937941551208496, loss=1.6023534536361694
I0312 20:44:33.773298 140028037539584 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.2085622102022171, loss=1.5579231977462769
I0312 20:45:10.243411 140028045932288 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.2231205552816391, loss=1.675086259841919
I0312 20:45:46.709149 140028037539584 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.2338317185640335, loss=1.6815999746322632
I0312 20:46:23.178438 140028045932288 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.24084819853305817, loss=1.6206010580062866
I0312 20:46:59.626709 140028037539584 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.20362801849842072, loss=1.4688475131988525
I0312 20:47:36.130388 140028045932288 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.21001873910427094, loss=1.5624604225158691
I0312 20:48:12.629743 140028037539584 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2208119034767151, loss=1.6060528755187988
I0312 20:48:49.132024 140028045932288 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.23338516056537628, loss=1.580974817276001
I0312 20:49:25.605336 140028037539584 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.21896907687187195, loss=1.633974313735962
I0312 20:50:02.104587 140028045932288 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.23627547919750214, loss=1.554902195930481
I0312 20:50:38.566865 140028037539584 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.21633610129356384, loss=1.597821831703186
I0312 20:51:15.106637 140028045932288 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.21622008085250854, loss=1.6777024269104004
I0312 20:51:51.602107 140028037539584 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.214286208152771, loss=1.5694202184677124
I0312 20:52:00.440847 140197733885760 spec.py:321] Evaluating on the training split.
I0312 20:52:03.507057 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 20:56:42.254076 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 20:56:44.973927 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:00:53.846338 140197733885760 spec.py:349] Evaluating on the test split.
I0312 21:00:56.567418 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:04:59.656162 140197733885760 submission_runner.py:420] Time since start: 59776.34s, 	Step: 96826, 	{'train/accuracy': 0.6781773567199707, 'train/loss': 1.476668119430542, 'train/bleu': 34.505094291513295, 'validation/accuracy': 0.6873690485954285, 'validation/loss': 1.410524845123291, 'validation/bleu': 30.01696468775817, 'validation/num_examples': 3000, 'test/accuracy': 0.7032014727592468, 'test/loss': 1.310566782951355, 'test/bleu': 29.931591119075662, 'test/num_examples': 3003, 'score': 35321.29417848587, 'total_duration': 59776.336155653, 'accumulated_submission_time': 35321.29417848587, 'accumulated_eval_time': 24450.895055770874, 'accumulated_logging_time': 1.328049898147583}
I0312 21:04:59.683761 140028045932288 logging_writer.py:48] [96826] accumulated_eval_time=24450.895056, accumulated_logging_time=1.328050, accumulated_submission_time=35321.294178, global_step=96826, preemption_count=0, score=35321.294178, test/accuracy=0.703201, test/bleu=29.931591, test/loss=1.310567, test/num_examples=3003, total_duration=59776.336156, train/accuracy=0.678177, train/bleu=34.505094, train/loss=1.476668, validation/accuracy=0.687369, validation/bleu=30.016965, validation/loss=1.410525, validation/num_examples=3000
I0312 21:05:26.756373 140028037539584 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.2241426408290863, loss=1.5653425455093384
I0312 21:06:02.964376 140028045932288 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.22225630283355713, loss=1.6337463855743408
I0312 21:06:39.279746 140028037539584 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.2032201588153839, loss=1.5698933601379395
I0312 21:07:15.592023 140028045932288 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.21138788759708405, loss=1.5767472982406616
I0312 21:07:51.991825 140028037539584 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.24582229554653168, loss=1.5330758094787598
I0312 21:08:28.356767 140028045932288 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.21625444293022156, loss=1.5721670389175415
I0312 21:09:04.794869 140028037539584 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.24156169593334198, loss=1.6072123050689697
I0312 21:09:41.250291 140028045932288 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21045368909835815, loss=1.5249533653259277
I0312 21:10:17.717985 140028037539584 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.2139161229133606, loss=1.4916882514953613
I0312 21:10:54.174619 140028045932288 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.22171056270599365, loss=1.5569264888763428
I0312 21:11:30.642120 140028037539584 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21514734625816345, loss=1.58482027053833
I0312 21:12:07.134204 140028045932288 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.22036324441432953, loss=1.4735366106033325
I0312 21:12:43.586556 140028037539584 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.21492384374141693, loss=1.5595470666885376
I0312 21:13:20.052234 140028045932288 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2120627611875534, loss=1.5792667865753174
I0312 21:13:56.536588 140028037539584 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.21918895840644836, loss=1.575304627418518
I0312 21:14:32.994986 140028045932288 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.2216443568468094, loss=1.5979610681533813
I0312 21:15:09.505845 140028037539584 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.20891816914081573, loss=1.5052556991577148
I0312 21:15:45.984462 140028045932288 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.20914648473262787, loss=1.5293747186660767
I0312 21:16:22.455683 140028037539584 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.23397815227508545, loss=1.5304111242294312
I0312 21:16:58.930126 140028045932288 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.22023887932300568, loss=1.5815001726150513
I0312 21:17:35.395002 140028037539584 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.22855699062347412, loss=1.5532279014587402
I0312 21:18:11.871314 140028045932288 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2154344618320465, loss=1.5595438480377197
I0312 21:18:48.362654 140028037539584 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21859674155712128, loss=1.6125359535217285
I0312 21:18:59.758261 140197733885760 spec.py:321] Evaluating on the training split.
I0312 21:19:02.835606 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:22:51.946369 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 21:22:54.672694 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:25:29.178222 140197733885760 spec.py:349] Evaluating on the test split.
I0312 21:25:31.922873 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:28:00.600552 140197733885760 submission_runner.py:420] Time since start: 61157.28s, 	Step: 99133, 	{'train/accuracy': 0.6766044497489929, 'train/loss': 1.4872363805770874, 'train/bleu': 34.415588200909774, 'validation/accuracy': 0.687480628490448, 'validation/loss': 1.401458740234375, 'validation/bleu': 30.11480655939417, 'validation/num_examples': 3000, 'test/accuracy': 0.7041775584220886, 'test/loss': 1.3046365976333618, 'test/bleu': 30.258098751022782, 'test/num_examples': 3003, 'score': 36161.29120993614, 'total_duration': 61157.28055071831, 'accumulated_submission_time': 36161.29120993614, 'accumulated_eval_time': 24991.73729467392, 'accumulated_logging_time': 1.3659710884094238}
I0312 21:28:00.627285 140028045932288 logging_writer.py:48] [99133] accumulated_eval_time=24991.737295, accumulated_logging_time=1.365971, accumulated_submission_time=36161.291210, global_step=99133, preemption_count=0, score=36161.291210, test/accuracy=0.704178, test/bleu=30.258099, test/loss=1.304637, test/num_examples=3003, total_duration=61157.280551, train/accuracy=0.676604, train/bleu=34.415588, train/loss=1.487236, validation/accuracy=0.687481, validation/bleu=30.114807, validation/loss=1.401459, validation/num_examples=3000
I0312 21:28:25.277576 140028037539584 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.20916342735290527, loss=1.518859624862671
I0312 21:29:01.567667 140028045932288 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.22162137925624847, loss=1.5472302436828613
I0312 21:29:37.877787 140028037539584 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.22225125133991241, loss=1.5166654586791992
I0312 21:30:14.254621 140028045932288 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.22929629683494568, loss=1.4869521856307983
I0312 21:30:50.613438 140028037539584 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.21841765940189362, loss=1.494017481803894
I0312 21:31:27.040011 140028045932288 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22726519405841827, loss=1.4759814739227295
I0312 21:32:03.478132 140028037539584 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.22249934077262878, loss=1.5527652502059937
I0312 21:32:39.962024 140028045932288 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.20759820938110352, loss=1.5396898984909058
I0312 21:33:16.428451 140028037539584 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22972851991653442, loss=1.6161956787109375
I0312 21:33:52.884078 140028045932288 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.22038550674915314, loss=1.6128439903259277
I0312 21:34:29.329663 140028037539584 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.2280730903148651, loss=1.5325101613998413
I0312 21:35:05.777605 140028045932288 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.21018022298812866, loss=1.5554245710372925
I0312 21:35:42.223889 140028037539584 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.22128748893737793, loss=1.5778172016143799
I0312 21:36:18.716215 140028045932288 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.23369570076465607, loss=1.5472197532653809
I0312 21:36:55.212542 140028037539584 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21949981153011322, loss=1.5341746807098389
I0312 21:37:31.689394 140028045932288 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.22125288844108582, loss=1.5531026124954224
I0312 21:38:08.170975 140028037539584 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.2457701563835144, loss=1.5268895626068115
I0312 21:38:44.682780 140028045932288 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.22056788206100464, loss=1.5899518728256226
I0312 21:39:21.181690 140028037539584 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.2313503921031952, loss=1.5630086660385132
I0312 21:39:57.662790 140028045932288 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.22273869812488556, loss=1.5744953155517578
I0312 21:40:34.154588 140028037539584 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.23979340493679047, loss=1.5748622417449951
I0312 21:41:10.630424 140028045932288 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.21371552348136902, loss=1.5345467329025269
I0312 21:41:47.124428 140028037539584 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.22264635562896729, loss=1.6136852502822876
I0312 21:42:00.711826 140197733885760 spec.py:321] Evaluating on the training split.
I0312 21:42:03.766498 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:46:22.832335 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 21:46:25.565424 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:49:03.544581 140197733885760 spec.py:349] Evaluating on the test split.
I0312 21:49:06.262639 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 21:51:49.204066 140197733885760 submission_runner.py:420] Time since start: 62585.88s, 	Step: 101439, 	{'train/accuracy': 0.6870901584625244, 'train/loss': 1.424283742904663, 'train/bleu': 34.776603339122985, 'validation/accuracy': 0.6906920075416565, 'validation/loss': 1.3949700593948364, 'validation/bleu': 30.415153484049604, 'validation/num_examples': 3000, 'test/accuracy': 0.7062227725982666, 'test/loss': 1.2959281206130981, 'test/bleu': 30.72127951020204, 'test/num_examples': 3003, 'score': 37001.29831576347, 'total_duration': 62585.88406729698, 'accumulated_submission_time': 37001.29831576347, 'accumulated_eval_time': 25580.229484319687, 'accumulated_logging_time': 1.402970314025879}
I0312 21:51:49.231273 140028045932288 logging_writer.py:48] [101439] accumulated_eval_time=25580.229484, accumulated_logging_time=1.402970, accumulated_submission_time=37001.298316, global_step=101439, preemption_count=0, score=37001.298316, test/accuracy=0.706223, test/bleu=30.721280, test/loss=1.295928, test/num_examples=3003, total_duration=62585.884067, train/accuracy=0.687090, train/bleu=34.776603, train/loss=1.424284, validation/accuracy=0.690692, validation/bleu=30.415153, validation/loss=1.394970, validation/num_examples=3000
I0312 21:52:11.627213 140028037539584 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.2428036630153656, loss=1.5108351707458496
I0312 21:52:47.871615 140028045932288 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22542674839496613, loss=1.5272539854049683
I0312 21:53:24.173294 140028037539584 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.23569588363170624, loss=1.4812105894088745
I0312 21:54:00.567040 140028045932288 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.2213752716779709, loss=1.5407589673995972
I0312 21:54:36.922779 140028037539584 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.2137904018163681, loss=1.6118758916854858
I0312 21:55:13.285457 140028045932288 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.20035311579704285, loss=1.5221903324127197
I0312 21:55:49.717532 140028037539584 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2176847755908966, loss=1.4951889514923096
I0312 21:56:26.175487 140028045932288 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2157636135816574, loss=1.4814037084579468
I0312 21:57:02.632175 140028037539584 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.23869574069976807, loss=1.5081502199172974
I0312 21:57:39.076284 140028045932288 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.2219824492931366, loss=1.5299066305160522
I0312 21:58:15.512156 140028037539584 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.23314331471920013, loss=1.4704294204711914
I0312 21:58:51.968453 140028045932288 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.21766027808189392, loss=1.5205026865005493
I0312 21:59:28.444780 140028037539584 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.28073650598526, loss=1.540941834449768
I0312 22:00:04.913104 140028045932288 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.22413133084774017, loss=1.5351414680480957
I0312 22:00:41.370965 140028037539584 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.23639968037605286, loss=1.534572958946228
I0312 22:01:17.797818 140028045932288 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.22653768956661224, loss=1.4811824560165405
I0312 22:01:54.294050 140028037539584 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.22116264700889587, loss=1.559521198272705
I0312 22:02:30.802364 140028045932288 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.22477683424949646, loss=1.5249375104904175
I0312 22:03:07.275871 140028037539584 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.2418896108865738, loss=1.572594165802002
I0312 22:03:43.758567 140028045932288 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.2210620492696762, loss=1.5738868713378906
I0312 22:04:20.257686 140028037539584 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.22037777304649353, loss=1.5018941164016724
I0312 22:04:56.748110 140028045932288 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.22986525297164917, loss=1.5360913276672363
I0312 22:05:33.223698 140028037539584 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.2248126119375229, loss=1.5286046266555786
I0312 22:05:49.352522 140197733885760 spec.py:321] Evaluating on the training split.
I0312 22:05:52.410391 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:09:44.183336 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 22:09:46.895434 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:12:23.264230 140197733885760 spec.py:349] Evaluating on the test split.
I0312 22:12:26.003688 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:15:04.380660 140197733885760 submission_runner.py:420] Time since start: 63981.06s, 	Step: 103746, 	{'train/accuracy': 0.6832632422447205, 'train/loss': 1.444430947303772, 'train/bleu': 34.29670099865465, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.3926249742507935, 'validation/bleu': 30.255927333565843, 'validation/num_examples': 3000, 'test/accuracy': 0.7068967819213867, 'test/loss': 1.29006826877594, 'test/bleu': 30.42128479296152, 'test/num_examples': 3003, 'score': 37841.34276676178, 'total_duration': 63981.060650110245, 'accumulated_submission_time': 37841.34276676178, 'accumulated_eval_time': 26135.257561206818, 'accumulated_logging_time': 1.4406273365020752}
I0312 22:15:04.408293 140028045932288 logging_writer.py:48] [103746] accumulated_eval_time=26135.257561, accumulated_logging_time=1.440627, accumulated_submission_time=37841.342767, global_step=103746, preemption_count=0, score=37841.342767, test/accuracy=0.706897, test/bleu=30.421285, test/loss=1.290068, test/num_examples=3003, total_duration=63981.060650, train/accuracy=0.683263, train/bleu=34.296701, train/loss=1.444431, validation/accuracy=0.690717, validation/bleu=30.255927, validation/loss=1.392625, validation/num_examples=3000
I0312 22:15:24.324090 140028037539584 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22098949551582336, loss=1.4828193187713623
I0312 22:16:00.602841 140028045932288 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.2238069474697113, loss=1.5464030504226685
I0312 22:16:36.919527 140028037539584 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.23358778655529022, loss=1.571053385734558
I0312 22:17:13.280267 140028045932288 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.21635660529136658, loss=1.5452712774276733
I0312 22:17:49.672356 140028037539584 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.2400064915418625, loss=1.5939950942993164
I0312 22:18:26.121589 140028045932288 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.22339345514774323, loss=1.5733946561813354
I0312 22:19:02.570880 140028037539584 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.24459853768348694, loss=1.5587897300720215
I0312 22:19:39.050955 140028045932288 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.22568488121032715, loss=1.5319517850875854
I0312 22:20:15.509857 140028037539584 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22336514294147491, loss=1.511179804801941
I0312 22:20:52.006806 140028045932288 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.2306804656982422, loss=1.5076241493225098
I0312 22:21:28.442164 140028037539584 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.22060145437717438, loss=1.4871618747711182
I0312 22:22:04.923140 140028045932288 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.22385506331920624, loss=1.500091791152954
I0312 22:22:41.383232 140028037539584 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22617264091968536, loss=1.5057123899459839
I0312 22:23:17.856675 140028045932288 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.23377995193004608, loss=1.4916012287139893
I0312 22:23:54.315122 140028037539584 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.22994495928287506, loss=1.5802080631256104
I0312 22:24:30.791482 140028045932288 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.2272612601518631, loss=1.519307017326355
I0312 22:25:07.282461 140028037539584 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.22617094218730927, loss=1.5798060894012451
I0312 22:25:43.765858 140028045932288 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.2438756227493286, loss=1.4714797735214233
I0312 22:26:20.275754 140028037539584 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.22340711951255798, loss=1.5097235441207886
I0312 22:26:56.730489 140028045932288 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.2256578952074051, loss=1.4722371101379395
I0312 22:27:33.240284 140028037539584 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.22310422360897064, loss=1.5136284828186035
I0312 22:28:09.730312 140028045932288 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.22774696350097656, loss=1.5640379190444946
I0312 22:28:46.221491 140028037539584 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.2358454018831253, loss=1.5678819417953491
I0312 22:29:04.535069 140197733885760 spec.py:321] Evaluating on the training split.
I0312 22:29:07.589820 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:33:00.985408 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 22:33:03.711252 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:35:40.810031 140197733885760 spec.py:349] Evaluating on the test split.
I0312 22:35:43.544940 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:38:12.514042 140197733885760 submission_runner.py:420] Time since start: 65369.19s, 	Step: 106052, 	{'train/accuracy': 0.6834360361099243, 'train/loss': 1.44869065284729, 'train/bleu': 34.66426654290793, 'validation/accuracy': 0.692055881023407, 'validation/loss': 1.3876729011535645, 'validation/bleu': 30.47464467803755, 'validation/num_examples': 3000, 'test/accuracy': 0.7068037986755371, 'test/loss': 1.2878295183181763, 'test/bleu': 30.07426208954982, 'test/num_examples': 3003, 'score': 38681.393530130386, 'total_duration': 65369.194038152695, 'accumulated_submission_time': 38681.393530130386, 'accumulated_eval_time': 26683.236490249634, 'accumulated_logging_time': 1.4774634838104248}
I0312 22:38:12.541442 140028045932288 logging_writer.py:48] [106052] accumulated_eval_time=26683.236490, accumulated_logging_time=1.477463, accumulated_submission_time=38681.393530, global_step=106052, preemption_count=0, score=38681.393530, test/accuracy=0.706804, test/bleu=30.074262, test/loss=1.287830, test/num_examples=3003, total_duration=65369.194038, train/accuracy=0.683436, train/bleu=34.664267, train/loss=1.448691, validation/accuracy=0.692056, validation/bleu=30.474645, validation/loss=1.387673, validation/num_examples=3000
I0312 22:38:30.282418 140028037539584 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.21858885884284973, loss=1.4807809591293335
I0312 22:39:06.597269 140028045932288 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.2239144742488861, loss=1.5382941961288452
I0312 22:39:42.926914 140028037539584 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.23556995391845703, loss=1.5882099866867065
I0312 22:40:19.262190 140028045932288 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.22690819203853607, loss=1.590600609779358
I0312 22:40:55.656113 140028037539584 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.21912753582000732, loss=1.4770684242248535
I0312 22:41:32.065197 140028045932288 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22115255892276764, loss=1.5149835348129272
I0312 22:42:08.497512 140028037539584 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.22120895981788635, loss=1.5129337310791016
I0312 22:42:44.946643 140028045932288 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.21655204892158508, loss=1.5206222534179688
I0312 22:43:21.413555 140028037539584 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.23013918101787567, loss=1.3770501613616943
I0312 22:43:57.890003 140028045932288 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.22315780818462372, loss=1.529518485069275
I0312 22:44:34.366307 140028037539584 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.2521807551383972, loss=1.5711684226989746
I0312 22:45:10.845137 140028045932288 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.22401578724384308, loss=1.4829857349395752
I0312 22:45:47.328824 140028037539584 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.22977519035339355, loss=1.5155830383300781
I0312 22:46:23.818217 140028045932288 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.2339155673980713, loss=1.5099644660949707
I0312 22:47:00.295184 140028037539584 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.23841725289821625, loss=1.4929684400558472
I0312 22:47:36.824403 140028045932288 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.2204972207546234, loss=1.508437156677246
I0312 22:48:13.325966 140028037539584 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.21494357287883759, loss=1.4879337549209595
I0312 22:48:49.855111 140028045932288 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.224607914686203, loss=1.522749423980713
I0312 22:49:26.366791 140028037539584 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.23156067728996277, loss=1.522642731666565
I0312 22:50:02.873208 140028045932288 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.22603553533554077, loss=1.5478163957595825
I0312 22:50:39.375050 140028037539584 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22831816971302032, loss=1.5596370697021484
I0312 22:51:15.876552 140028045932288 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.22313673794269562, loss=1.5197378396987915
I0312 22:51:52.369057 140028037539584 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.2286025434732437, loss=1.5401227474212646
I0312 22:52:12.531899 140197733885760 spec.py:321] Evaluating on the training split.
I0312 22:52:15.579499 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:56:38.014310 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 22:56:40.728315 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 22:59:33.051693 140197733885760 spec.py:349] Evaluating on the test split.
I0312 22:59:35.776660 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 23:02:22.454746 140197733885760 submission_runner.py:420] Time since start: 66819.13s, 	Step: 108357, 	{'train/accuracy': 0.6897974610328674, 'train/loss': 1.4114888906478882, 'train/bleu': 35.04683613009326, 'validation/accuracy': 0.6923534870147705, 'validation/loss': 1.3850077390670776, 'validation/bleu': 30.593538745220897, 'validation/num_examples': 3000, 'test/accuracy': 0.7074894309043884, 'test/loss': 1.2821885347366333, 'test/bleu': 30.3378661903933, 'test/num_examples': 3003, 'score': 39521.30562400818, 'total_duration': 66819.13472270966, 'accumulated_submission_time': 39521.30562400818, 'accumulated_eval_time': 27293.159260988235, 'accumulated_logging_time': 1.5152764320373535}
I0312 23:02:22.484118 140028045932288 logging_writer.py:48] [108357] accumulated_eval_time=27293.159261, accumulated_logging_time=1.515276, accumulated_submission_time=39521.305624, global_step=108357, preemption_count=0, score=39521.305624, test/accuracy=0.707489, test/bleu=30.337866, test/loss=1.282189, test/num_examples=3003, total_duration=66819.134723, train/accuracy=0.689797, train/bleu=35.046836, train/loss=1.411489, validation/accuracy=0.692353, validation/bleu=30.593539, validation/loss=1.385008, validation/num_examples=3000
I0312 23:02:38.364223 140028037539584 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.23082689940929413, loss=1.5386040210723877
I0312 23:03:14.586761 140028045932288 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.22224536538124084, loss=1.4727469682693481
I0312 23:03:50.882548 140028037539584 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.23465009033679962, loss=1.5592257976531982
I0312 23:04:27.243895 140028045932288 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.23989173769950867, loss=1.509752869606018
I0312 23:05:03.563629 140028037539584 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.226194366812706, loss=1.5055997371673584
I0312 23:05:40.010300 140028045932288 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.22706486284732819, loss=1.4782373905181885
I0312 23:06:16.399330 140028037539584 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.22110706567764282, loss=1.4795511960983276
I0312 23:06:52.865560 140028045932288 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.23603136837482452, loss=1.4854307174682617
I0312 23:07:29.298354 140028037539584 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.2247932106256485, loss=1.4977953433990479
I0312 23:08:05.783618 140028045932288 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.2291133850812912, loss=1.4612373113632202
I0312 23:08:42.229123 140028037539584 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.23178355395793915, loss=1.514763593673706
I0312 23:09:18.687884 140028045932288 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22728045284748077, loss=1.530951976776123
I0312 23:09:55.164726 140028037539584 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.2285998910665512, loss=1.5642718076705933
I0312 23:10:31.586459 140028045932288 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.21767854690551758, loss=1.4358688592910767
I0312 23:11:08.069347 140028037539584 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.22905927896499634, loss=1.4730037450790405
I0312 23:11:44.536226 140028045932288 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.23830842971801758, loss=1.5215815305709839
I0312 23:12:21.069291 140028037539584 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.23529553413391113, loss=1.5379289388656616
I0312 23:12:57.572017 140028045932288 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.228401318192482, loss=1.5019611120224
I0312 23:13:34.057457 140028037539584 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.23458316922187805, loss=1.4772136211395264
I0312 23:14:10.545772 140028045932288 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.22638607025146484, loss=1.462288498878479
I0312 23:14:47.064502 140028037539584 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.24415110051631927, loss=1.5061888694763184
I0312 23:15:23.531361 140028045932288 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.24153953790664673, loss=1.5241409540176392
I0312 23:16:00.021287 140028037539584 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.23226483166217804, loss=1.502406358718872
I0312 23:16:22.718305 140197733885760 spec.py:321] Evaluating on the training split.
I0312 23:16:25.781818 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 23:20:17.584151 140197733885760 spec.py:333] Evaluating on the validation split.
I0312 23:20:20.310643 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 23:23:18.990226 140197733885760 spec.py:349] Evaluating on the test split.
I0312 23:23:21.716703 140197733885760 workload.py:181] Translating evaluation dataset.
I0312 23:26:24.636823 140197733885760 submission_runner.py:420] Time since start: 68261.32s, 	Step: 110664, 	{'train/accuracy': 0.6857077479362488, 'train/loss': 1.4301151037216187, 'train/bleu': 34.912174504037615, 'validation/accuracy': 0.6944116950035095, 'validation/loss': 1.377577304840088, 'validation/bleu': 30.888292461111718, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.273249864578247, 'test/bleu': 30.811757487590313, 'test/num_examples': 3003, 'score': 40361.46251535416, 'total_duration': 68261.31683158875, 'accumulated_submission_time': 40361.46251535416, 'accumulated_eval_time': 27895.077735185623, 'accumulated_logging_time': 1.5552773475646973}
I0312 23:26:24.665418 140028045932288 logging_writer.py:48] [110664] accumulated_eval_time=27895.077735, accumulated_logging_time=1.555277, accumulated_submission_time=40361.462515, global_step=110664, preemption_count=0, score=40361.462515, test/accuracy=0.710127, test/bleu=30.811757, test/loss=1.273250, test/num_examples=3003, total_duration=68261.316832, train/accuracy=0.685708, train/bleu=34.912175, train/loss=1.430115, validation/accuracy=0.694412, validation/bleu=30.888292, validation/loss=1.377577, validation/num_examples=3000
I0312 23:26:24.692190 140028037539584 logging_writer.py:48] [110664] global_step=110664, preemption_count=0, score=40361.462515
I0312 23:26:25.784203 140197733885760 checkpoints.py:490] Saving checkpoint at step: 110664
I0312 23:26:29.511468 140197733885760 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax/trial_1/checkpoint_110664
I0312 23:26:29.516690 140197733885760 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_1/wmt_jax/trial_1/checkpoint_110664.
I0312 23:26:29.573282 140197733885760 submission_runner.py:683] Final wmt score: 40361.46251535416
