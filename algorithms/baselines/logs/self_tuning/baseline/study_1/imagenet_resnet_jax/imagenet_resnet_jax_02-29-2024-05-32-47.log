python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_1 --overwrite=true --save_checkpoints=false --rng_seed=325995729 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_resnet_jax_02-29-2024-05-32-47.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0229 05:33:08.817646 140688601454400 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax.
I0229 05:33:09.866857 140688601454400 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0229 05:33:09.867525 140688601454400 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0229 05:33:09.867658 140688601454400 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0229 05:33:10.683510 140688601454400 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax/trial_1.
I0229 05:33:10.882673 140688601454400 submission_runner.py:206] Initializing dataset.
I0229 05:33:10.901080 140688601454400 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:33:10.911211 140688601454400 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:33:11.275802 140688601454400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:33:12.459543 140688601454400 submission_runner.py:213] Initializing model.
I0229 05:33:22.658394 140688601454400 submission_runner.py:255] Initializing optimizer.
I0229 05:33:24.329876 140688601454400 submission_runner.py:262] Initializing metrics bundle.
I0229 05:33:24.330069 140688601454400 submission_runner.py:280] Initializing checkpoint and logger.
I0229 05:33:24.330950 140688601454400 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0229 05:33:24.331088 140688601454400 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0229 05:33:24.640805 140688601454400 logger_utils.py:220] Unable to record git information. Continuing without it.
I0229 05:33:24.920035 140688601454400 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax/trial_1/flags_0.json.
I0229 05:33:24.929101 140688601454400 submission_runner.py:314] Starting training loop.
I0229 05:34:15.550608 140523910182656 logging_writer.py:48] [0] global_step=0, grad_norm=0.6575260162353516, loss=6.928964614868164
I0229 05:34:15.566015 140688601454400 spec.py:321] Evaluating on the training split.
I0229 05:34:16.703806 140688601454400 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:34:16.713275 140688601454400 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:34:16.798687 140688601454400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:34:30.202533 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 05:34:31.989191 140688601454400 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:34:31.998212 140688601454400 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:34:32.038650 140688601454400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:34:47.549946 140688601454400 spec.py:349] Evaluating on the test split.
I0229 05:34:48.407919 140688601454400 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:34:48.413985 140688601454400 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0229 05:34:48.456140 140688601454400 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:34:52.591587 140688601454400 submission_runner.py:411] Time since start: 87.66s, 	Step: 1, 	{'train/accuracy': 0.0011559311533346772, 'train/loss': 6.912258625030518, 'validation/accuracy': 0.0011599999852478504, 'validation/loss': 6.9129204750061035, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.9126763343811035, 'test/num_examples': 10000, 'score': 50.636836767196655, 'total_duration': 87.66242718696594, 'accumulated_submission_time': 50.636836767196655, 'accumulated_eval_time': 37.02550530433655, 'accumulated_logging_time': 0}
I0229 05:34:52.608916 140503681070848 logging_writer.py:48] [1] accumulated_eval_time=37.025505, accumulated_logging_time=0, accumulated_submission_time=50.636837, global_step=1, preemption_count=0, score=50.636837, test/accuracy=0.001100, test/loss=6.912676, test/num_examples=10000, total_duration=87.662427, train/accuracy=0.001156, train/loss=6.912259, validation/accuracy=0.001160, validation/loss=6.912920, validation/num_examples=50000
I0229 05:35:26.334485 140503672678144 logging_writer.py:48] [100] global_step=100, grad_norm=0.6766614317893982, loss=6.807253360748291
I0229 05:36:00.177419 140503681070848 logging_writer.py:48] [200] global_step=200, grad_norm=0.7895325422286987, loss=6.571463584899902
I0229 05:36:34.068717 140503672678144 logging_writer.py:48] [300] global_step=300, grad_norm=1.0923726558685303, loss=6.297056198120117
I0229 05:37:07.972636 140503681070848 logging_writer.py:48] [400] global_step=400, grad_norm=1.7477684020996094, loss=6.067274570465088
I0229 05:37:41.874420 140503672678144 logging_writer.py:48] [500] global_step=500, grad_norm=2.9927756786346436, loss=5.859600067138672
I0229 05:38:15.790469 140503681070848 logging_writer.py:48] [600] global_step=600, grad_norm=2.154637575149536, loss=5.560821533203125
I0229 05:38:49.700724 140503672678144 logging_writer.py:48] [700] global_step=700, grad_norm=4.517483234405518, loss=5.448789596557617
I0229 05:39:23.622247 140503681070848 logging_writer.py:48] [800] global_step=800, grad_norm=3.3539161682128906, loss=5.328517436981201
I0229 05:39:57.576105 140503672678144 logging_writer.py:48] [900] global_step=900, grad_norm=3.642387628555298, loss=5.147103309631348
I0229 05:40:31.522040 140503681070848 logging_writer.py:48] [1000] global_step=1000, grad_norm=7.526960372924805, loss=5.082276344299316
I0229 05:41:05.465738 140503672678144 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.4165585041046143, loss=4.90718412399292
I0229 05:41:39.405112 140503681070848 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.394388675689697, loss=4.835025310516357
I0229 05:42:13.353706 140503672678144 logging_writer.py:48] [1300] global_step=1300, grad_norm=5.103870391845703, loss=4.753715991973877
I0229 05:42:47.280875 140503681070848 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.227055072784424, loss=4.713979721069336
I0229 05:43:21.187100 140503672678144 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.338866233825684, loss=4.6383233070373535
I0229 05:43:22.646258 140688601454400 spec.py:321] Evaluating on the training split.
I0229 05:43:29.991297 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 05:43:38.116901 140688601454400 spec.py:349] Evaluating on the test split.
I0229 05:43:40.442712 140688601454400 submission_runner.py:411] Time since start: 615.51s, 	Step: 1506, 	{'train/accuracy': 0.1685267835855484, 'train/loss': 4.295879364013672, 'validation/accuracy': 0.1553799957036972, 'validation/loss': 4.445466995239258, 'validation/num_examples': 50000, 'test/accuracy': 0.11570000648498535, 'test/loss': 4.921426296234131, 'test/num_examples': 10000, 'score': 560.612095117569, 'total_duration': 615.5135684013367, 'accumulated_submission_time': 560.612095117569, 'accumulated_eval_time': 54.821940660476685, 'accumulated_logging_time': 0.02597641944885254}
I0229 05:43:40.459126 140503764932352 logging_writer.py:48] [1506] accumulated_eval_time=54.821941, accumulated_logging_time=0.025976, accumulated_submission_time=560.612095, global_step=1506, preemption_count=0, score=560.612095, test/accuracy=0.115700, test/loss=4.921426, test/num_examples=10000, total_duration=615.513568, train/accuracy=0.168527, train/loss=4.295879, validation/accuracy=0.155380, validation/loss=4.445467, validation/num_examples=50000
I0229 05:44:12.686537 140503773325056 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.6026859283447266, loss=4.326952934265137
I0229 05:44:46.591690 140503764932352 logging_writer.py:48] [1700] global_step=1700, grad_norm=6.169818878173828, loss=4.237030506134033
I0229 05:45:20.519011 140503773325056 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.7188189029693604, loss=4.35934591293335
I0229 05:45:54.411132 140503764932352 logging_writer.py:48] [1900] global_step=1900, grad_norm=5.597654819488525, loss=4.1138458251953125
I0229 05:46:28.474819 140503773325056 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.130547523498535, loss=4.093710899353027
I0229 05:47:02.404438 140503764932352 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.6263577938079834, loss=3.9782679080963135
I0229 05:47:36.331008 140503773325056 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.684070587158203, loss=3.921891212463379
I0229 05:48:10.245004 140503764932352 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.168115615844727, loss=3.760606050491333
I0229 05:48:44.180119 140503773325056 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.4577531814575195, loss=3.7075934410095215
I0229 05:49:18.078454 140503764932352 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.4265143871307373, loss=3.8183891773223877
I0229 05:49:51.969005 140503773325056 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.7355082035064697, loss=3.6895592212677
I0229 05:50:25.895860 140503764932352 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.214271068572998, loss=3.5903522968292236
I0229 05:50:59.809357 140503773325056 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.320155620574951, loss=3.511385679244995
I0229 05:51:33.747394 140503764932352 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.285017490386963, loss=3.577918767929077
I0229 05:52:07.678119 140503773325056 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.145558834075928, loss=3.318617343902588
I0229 05:52:10.587306 140688601454400 spec.py:321] Evaluating on the training split.
I0229 05:52:17.770674 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 05:52:26.029984 140688601454400 spec.py:349] Evaluating on the test split.
I0229 05:52:28.283754 140688601454400 submission_runner.py:411] Time since start: 1143.35s, 	Step: 3010, 	{'train/accuracy': 0.33982381224632263, 'train/loss': 3.0561516284942627, 'validation/accuracy': 0.31035998463630676, 'validation/loss': 3.23838472366333, 'validation/num_examples': 50000, 'test/accuracy': 0.22990001738071442, 'test/loss': 3.900252103805542, 'test/num_examples': 10000, 'score': 1070.6800384521484, 'total_duration': 1143.3546075820923, 'accumulated_submission_time': 1070.6800384521484, 'accumulated_eval_time': 72.51834774017334, 'accumulated_logging_time': 0.05192422866821289}
I0229 05:52:28.300261 140526514837248 logging_writer.py:48] [3010] accumulated_eval_time=72.518348, accumulated_logging_time=0.051924, accumulated_submission_time=1070.680038, global_step=3010, preemption_count=0, score=1070.680038, test/accuracy=0.229900, test/loss=3.900252, test/num_examples=10000, total_duration=1143.354608, train/accuracy=0.339824, train/loss=3.056152, validation/accuracy=0.310360, validation/loss=3.238385, validation/num_examples=50000
I0229 05:52:59.101193 140526523229952 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.2464959621429443, loss=3.4448068141937256
I0229 05:53:32.943985 140526514837248 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.4527158737182617, loss=3.2646429538726807
I0229 05:54:06.837001 140526523229952 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.151901721954346, loss=3.210317611694336
I0229 05:54:40.719420 140526514837248 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.0780029296875, loss=3.1943721771240234
I0229 05:55:14.616261 140526523229952 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.2231006622314453, loss=3.1855320930480957
I0229 05:55:48.531636 140526514837248 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.6749696731567383, loss=3.258486270904541
I0229 05:56:22.400598 140526523229952 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2727952003479004, loss=3.102062702178955
I0229 05:56:56.305490 140526514837248 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.6664528846740723, loss=3.0043623447418213
I0229 05:57:30.192630 140526523229952 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.5453991889953613, loss=3.085463762283325
I0229 05:58:04.081079 140526514837248 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.520927429199219, loss=2.9764668941497803
I0229 05:58:37.979503 140526523229952 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.6866681575775146, loss=2.9609875679016113
I0229 05:59:11.861733 140526514837248 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.3391854763031006, loss=2.8132596015930176
I0229 05:59:45.719938 140526523229952 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.889339566230774, loss=2.9623074531555176
I0229 06:00:19.598828 140526514837248 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.5228209495544434, loss=2.9317822456359863
I0229 06:00:53.448032 140526523229952 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.583362579345703, loss=2.7691800594329834
I0229 06:00:58.285497 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:01:05.714883 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:01:13.981827 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:01:16.277337 140688601454400 submission_runner.py:411] Time since start: 1671.35s, 	Step: 4516, 	{'train/accuracy': 0.46709582209587097, 'train/loss': 2.3323709964752197, 'validation/accuracy': 0.43675997853279114, 'validation/loss': 2.5136337280273438, 'validation/num_examples': 50000, 'test/accuracy': 0.3290000259876251, 'test/loss': 3.260066509246826, 'test/num_examples': 10000, 'score': 1580.6047410964966, 'total_duration': 1671.3481891155243, 'accumulated_submission_time': 1580.6047410964966, 'accumulated_eval_time': 90.5101797580719, 'accumulated_logging_time': 0.07838964462280273}
I0229 06:01:16.294821 140525759887104 logging_writer.py:48] [4516] accumulated_eval_time=90.510180, accumulated_logging_time=0.078390, accumulated_submission_time=1580.604741, global_step=4516, preemption_count=0, score=1580.604741, test/accuracy=0.329000, test/loss=3.260067, test/num_examples=10000, total_duration=1671.348189, train/accuracy=0.467096, train/loss=2.332371, validation/accuracy=0.436760, validation/loss=2.513634, validation/num_examples=50000
I0229 06:01:45.032282 140525843748608 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.4197211265563965, loss=2.6936593055725098
I0229 06:02:18.876826 140525759887104 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.554697275161743, loss=2.771472930908203
I0229 06:02:52.717749 140525843748608 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.7908499240875244, loss=2.7218592166900635
I0229 06:03:26.580171 140525759887104 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.886998414993286, loss=2.7673227787017822
I0229 06:04:00.424471 140525843748608 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.4276318550109863, loss=2.479900360107422
I0229 06:04:34.333941 140525759887104 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.278559684753418, loss=2.641571044921875
I0229 06:05:08.172477 140525843748608 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.4764978885650635, loss=2.683577299118042
I0229 06:05:42.026491 140525759887104 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.9361541271209717, loss=2.560927152633667
I0229 06:06:15.877687 140525843748608 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.153738498687744, loss=2.5857715606689453
I0229 06:06:49.701096 140525759887104 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.437617778778076, loss=2.4532217979431152
I0229 06:07:23.510679 140525843748608 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.1043834686279297, loss=2.4809277057647705
I0229 06:07:57.348635 140525759887104 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9887441396713257, loss=2.596977710723877
I0229 06:08:31.220039 140525843748608 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.508662700653076, loss=2.36625337600708
I0229 06:09:05.045481 140525759887104 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.9563696384429932, loss=2.4987900257110596
I0229 06:09:38.844429 140525843748608 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.7836419343948364, loss=2.415971517562866
I0229 06:09:46.369055 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:09:53.404747 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:10:01.776481 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:10:04.099370 140688601454400 submission_runner.py:411] Time since start: 2199.17s, 	Step: 6024, 	{'train/accuracy': 0.5397201776504517, 'train/loss': 1.9583563804626465, 'validation/accuracy': 0.5008000135421753, 'validation/loss': 2.161590337753296, 'validation/num_examples': 50000, 'test/accuracy': 0.38520002365112305, 'test/loss': 2.8951804637908936, 'test/num_examples': 10000, 'score': 2090.61950135231, 'total_duration': 2199.1702225208282, 'accumulated_submission_time': 2090.61950135231, 'accumulated_eval_time': 108.24045300483704, 'accumulated_logging_time': 0.10556888580322266}
I0229 06:10:04.116857 140525743101696 logging_writer.py:48] [6024] accumulated_eval_time=108.240453, accumulated_logging_time=0.105569, accumulated_submission_time=2090.619501, global_step=6024, preemption_count=0, score=2090.619501, test/accuracy=0.385200, test/loss=2.895180, test/num_examples=10000, total_duration=2199.170223, train/accuracy=0.539720, train/loss=1.958356, validation/accuracy=0.500800, validation/loss=2.161590, validation/num_examples=50000
I0229 06:10:30.156980 140525751494400 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.987626552581787, loss=2.4405388832092285
I0229 06:11:03.971859 140525743101696 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.707117795944214, loss=2.4052255153656006
I0229 06:11:37.819045 140525751494400 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.547294855117798, loss=2.3594133853912354
I0229 06:12:11.670616 140525743101696 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.1158902645111084, loss=2.2873661518096924
I0229 06:12:45.518032 140525751494400 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.9126641750335693, loss=2.4140028953552246
I0229 06:13:19.388023 140525743101696 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.2499678134918213, loss=2.3849027156829834
I0229 06:13:53.231329 140525751494400 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.0178277492523193, loss=2.2122933864593506
I0229 06:14:27.071400 140525743101696 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.8515115976333618, loss=2.410256862640381
I0229 06:15:00.915553 140525751494400 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.9297481775283813, loss=2.3243579864501953
I0229 06:15:34.765053 140525743101696 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.0023815631866455, loss=2.3038079738616943
I0229 06:16:08.601824 140525751494400 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.8899760246276855, loss=2.1549196243286133
I0229 06:16:42.447910 140525743101696 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.81270170211792, loss=2.3308517932891846
I0229 06:17:16.283617 140525751494400 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.9996278285980225, loss=2.1906328201293945
I0229 06:17:50.121233 140525743101696 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.283381700515747, loss=2.3201851844787598
I0229 06:18:23.959824 140525751494400 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2674624919891357, loss=2.3207321166992188
I0229 06:18:34.198728 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:18:41.330017 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:18:49.723813 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:18:51.992076 140688601454400 submission_runner.py:411] Time since start: 2727.06s, 	Step: 7532, 	{'train/accuracy': 0.5772082209587097, 'train/loss': 1.7723355293273926, 'validation/accuracy': 0.5385599732398987, 'validation/loss': 1.980293869972229, 'validation/num_examples': 50000, 'test/accuracy': 0.41750001907348633, 'test/loss': 2.7358531951904297, 'test/num_examples': 10000, 'score': 2600.636000394821, 'total_duration': 2727.0629329681396, 'accumulated_submission_time': 2600.636000394821, 'accumulated_eval_time': 126.03376865386963, 'accumulated_logging_time': 0.13748550415039062}
I0229 06:18:52.010716 140525541775104 logging_writer.py:48] [7532] accumulated_eval_time=126.033769, accumulated_logging_time=0.137486, accumulated_submission_time=2600.636000, global_step=7532, preemption_count=0, score=2600.636000, test/accuracy=0.417500, test/loss=2.735853, test/num_examples=10000, total_duration=2727.062933, train/accuracy=0.577208, train/loss=1.772336, validation/accuracy=0.538560, validation/loss=1.980294, validation/num_examples=50000
I0229 06:19:15.316128 140525550167808 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.8938648700714111, loss=2.2189137935638428
I0229 06:19:49.103719 140525541775104 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.0402815341949463, loss=2.2864508628845215
I0229 06:20:22.947157 140525550167808 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.4774099588394165, loss=2.118788003921509
I0229 06:20:56.791883 140525541775104 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.0307085514068604, loss=2.217641592025757
I0229 06:21:30.628091 140525550167808 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9181910753250122, loss=2.2592525482177734
I0229 06:22:04.463419 140525541775104 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.59134840965271, loss=2.2665047645568848
I0229 06:22:38.298475 140525550167808 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.5273239612579346, loss=2.1704516410827637
I0229 06:23:12.183769 140525541775104 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.4717153310775757, loss=2.1894092559814453
I0229 06:23:45.992934 140525550167808 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.4844754934310913, loss=2.2654712200164795
I0229 06:24:19.821560 140525541775104 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.095806837081909, loss=2.19333815574646
I0229 06:24:53.655290 140525550167808 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.062910795211792, loss=2.181652545928955
I0229 06:25:27.482089 140525541775104 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.5253864526748657, loss=2.0567057132720947
I0229 06:26:01.341289 140525550167808 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.3026583194732666, loss=2.124115467071533
I0229 06:26:35.165175 140525541775104 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.7496341466903687, loss=2.215945243835449
I0229 06:27:08.977870 140525550167808 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.635273814201355, loss=2.12419056892395
I0229 06:27:22.281615 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:27:29.553537 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:27:37.867603 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:27:40.102021 140688601454400 submission_runner.py:411] Time since start: 3255.17s, 	Step: 9041, 	{'train/accuracy': 0.6067641973495483, 'train/loss': 1.6266725063323975, 'validation/accuracy': 0.5556600093841553, 'validation/loss': 1.8939425945281982, 'validation/num_examples': 50000, 'test/accuracy': 0.43070003390312195, 'test/loss': 2.655898094177246, 'test/num_examples': 10000, 'score': 3110.8438572883606, 'total_duration': 3255.172870874405, 'accumulated_submission_time': 3110.8438572883606, 'accumulated_eval_time': 143.8541338443756, 'accumulated_logging_time': 0.16742849349975586}
I0229 06:27:40.119559 140525852141312 logging_writer.py:48] [9041] accumulated_eval_time=143.854134, accumulated_logging_time=0.167428, accumulated_submission_time=3110.843857, global_step=9041, preemption_count=0, score=3110.843857, test/accuracy=0.430700, test/loss=2.655898, test/num_examples=10000, total_duration=3255.172871, train/accuracy=0.606764, train/loss=1.626673, validation/accuracy=0.555660, validation/loss=1.893943, validation/num_examples=50000
I0229 06:28:00.408865 140525860534016 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.5973907709121704, loss=2.188377618789673
I0229 06:28:34.209424 140525852141312 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.2739531993865967, loss=2.084899425506592
I0229 06:29:08.089944 140525860534016 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.09635853767395, loss=2.117291212081909
I0229 06:29:41.908350 140525852141312 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.1055305004119873, loss=2.173403739929199
I0229 06:30:15.775088 140525860534016 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.852542519569397, loss=2.0832135677337646
I0229 06:30:49.648427 140525852141312 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.940041184425354, loss=2.0993716716766357
I0229 06:31:23.526741 140525860534016 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.5856555700302124, loss=2.0492663383483887
I0229 06:31:57.396234 140525852141312 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.466928720474243, loss=2.0696475505828857
I0229 06:32:31.231981 140525860534016 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.5126726627349854, loss=2.073765754699707
I0229 06:33:05.081362 140525852141312 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4863595962524414, loss=2.0302772521972656
I0229 06:33:38.942270 140525860534016 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.6509438753128052, loss=2.037276268005371
I0229 06:34:12.778506 140525852141312 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.8251327276229858, loss=2.099381923675537
I0229 06:34:46.584292 140525860534016 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.5538675785064697, loss=2.0660150051116943
I0229 06:35:20.574612 140525852141312 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4870280027389526, loss=2.132744550704956
I0229 06:35:54.356657 140525860534016 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.013505220413208, loss=2.176990509033203
I0229 06:36:10.364830 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:36:17.780680 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:36:26.418043 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:36:28.662591 140688601454400 submission_runner.py:411] Time since start: 3783.73s, 	Step: 10549, 	{'train/accuracy': 0.6285674571990967, 'train/loss': 1.5007295608520508, 'validation/accuracy': 0.5639199614524841, 'validation/loss': 1.8578181266784668, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.585268497467041, 'test/num_examples': 10000, 'score': 3621.028754234314, 'total_duration': 3783.733434200287, 'accumulated_submission_time': 3621.028754234314, 'accumulated_eval_time': 162.1518476009369, 'accumulated_logging_time': 0.1943204402923584}
I0229 06:36:28.680958 140525264946944 logging_writer.py:48] [10549] accumulated_eval_time=162.151848, accumulated_logging_time=0.194320, accumulated_submission_time=3621.028754, global_step=10549, preemption_count=0, score=3621.028754, test/accuracy=0.444500, test/loss=2.585268, test/num_examples=10000, total_duration=3783.733434, train/accuracy=0.628567, train/loss=1.500730, validation/accuracy=0.563920, validation/loss=1.857818, validation/num_examples=50000
I0229 06:36:46.209371 140525273339648 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.5255846977233887, loss=2.087686061859131
I0229 06:37:19.980221 140525264946944 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.7192959785461426, loss=2.105154275894165
I0229 06:37:53.775424 140525273339648 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.2125906944274902, loss=1.9930427074432373
I0229 06:38:27.590360 140525264946944 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.7535006999969482, loss=2.0770320892333984
I0229 06:39:01.429961 140525273339648 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.0309407711029053, loss=2.161696195602417
I0229 06:39:35.292163 140525264946944 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.488585948944092, loss=1.976696491241455
I0229 06:40:09.110712 140525273339648 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.713517665863037, loss=2.0462698936462402
I0229 06:40:42.917993 140525264946944 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.5654674768447876, loss=1.9447680711746216
I0229 06:41:16.859913 140525273339648 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.5609896183013916, loss=2.048938751220703
I0229 06:41:50.677051 140525264946944 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.5206928253173828, loss=1.9624803066253662
I0229 06:42:24.479714 140525273339648 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.5373831987380981, loss=2.0752785205841064
I0229 06:42:58.324640 140525264946944 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.5727689266204834, loss=2.1589550971984863
I0229 06:43:32.156642 140525273339648 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.8757214546203613, loss=2.1064224243164062
I0229 06:44:06.000767 140525264946944 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.5821385383605957, loss=2.0201807022094727
I0229 06:44:39.850767 140525273339648 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.6746470928192139, loss=2.028860092163086
I0229 06:44:58.915722 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:45:06.251407 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:45:14.816892 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:45:17.115816 140688601454400 submission_runner.py:411] Time since start: 4312.19s, 	Step: 12058, 	{'train/accuracy': 0.6300023794174194, 'train/loss': 1.498435378074646, 'validation/accuracy': 0.5737599730491638, 'validation/loss': 1.8043254613876343, 'validation/num_examples': 50000, 'test/accuracy': 0.4433000087738037, 'test/loss': 2.5808615684509277, 'test/num_examples': 10000, 'score': 4131.20413517952, 'total_duration': 4312.186669111252, 'accumulated_submission_time': 4131.20413517952, 'accumulated_eval_time': 180.35190606117249, 'accumulated_logging_time': 0.22214174270629883}
I0229 06:45:17.143135 140524535142144 logging_writer.py:48] [12058] accumulated_eval_time=180.351906, accumulated_logging_time=0.222142, accumulated_submission_time=4131.204135, global_step=12058, preemption_count=0, score=4131.204135, test/accuracy=0.443300, test/loss=2.580862, test/num_examples=10000, total_duration=4312.186669, train/accuracy=0.630002, train/loss=1.498435, validation/accuracy=0.573760, validation/loss=1.804325, validation/num_examples=50000
I0229 06:45:31.675684 140524543534848 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.6582269668579102, loss=2.087336301803589
I0229 06:46:05.469220 140524535142144 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.8559520244598389, loss=1.9576082229614258
I0229 06:46:39.314424 140524543534848 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.3397630453109741, loss=2.0825209617614746
I0229 06:47:13.171986 140524535142144 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.569014072418213, loss=2.1018013954162598
I0229 06:47:47.035838 140524543534848 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.5111929178237915, loss=2.0273239612579346
I0229 06:48:20.888437 140524535142144 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.3649541139602661, loss=2.033830165863037
I0229 06:48:54.709120 140524543534848 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4105831384658813, loss=1.915770173072815
I0229 06:49:28.554285 140524535142144 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.5480529069900513, loss=1.9164198637008667
I0229 06:50:02.418479 140524543534848 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.9519033432006836, loss=1.9633393287658691
I0229 06:50:36.260276 140524535142144 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.384346842765808, loss=2.087409019470215
I0229 06:51:10.102846 140524543534848 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.5676782131195068, loss=2.060572862625122
I0229 06:51:43.953400 140524535142144 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.5260268449783325, loss=2.0233545303344727
I0229 06:52:17.798027 140524543534848 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.294997215270996, loss=1.9754536151885986
I0229 06:52:51.634256 140524535142144 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5909340381622314, loss=1.8545048236846924
I0229 06:53:25.506540 140524543534848 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.32404625415802, loss=1.9174535274505615
I0229 06:53:47.233800 140688601454400 spec.py:321] Evaluating on the training split.
I0229 06:53:54.711881 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 06:54:03.554971 140688601454400 spec.py:349] Evaluating on the test split.
I0229 06:54:05.853589 140688601454400 submission_runner.py:411] Time since start: 4840.92s, 	Step: 13566, 	{'train/accuracy': 0.6170080900192261, 'train/loss': 1.5616915225982666, 'validation/accuracy': 0.5643399953842163, 'validation/loss': 1.8516933917999268, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.5929534435272217, 'test/num_examples': 10000, 'score': 4641.235483169556, 'total_duration': 4840.924443721771, 'accumulated_submission_time': 4641.235483169556, 'accumulated_eval_time': 198.97168898582458, 'accumulated_logging_time': 0.2587699890136719}
I0229 06:54:05.873394 140524191205120 logging_writer.py:48] [13566] accumulated_eval_time=198.971689, accumulated_logging_time=0.258770, accumulated_submission_time=4641.235483, global_step=13566, preemption_count=0, score=4641.235483, test/accuracy=0.445600, test/loss=2.592953, test/num_examples=10000, total_duration=4840.924444, train/accuracy=0.617008, train/loss=1.561692, validation/accuracy=0.564340, validation/loss=1.851693, validation/num_examples=50000
I0229 06:54:17.688270 140524199597824 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.4719675779342651, loss=1.932811975479126
I0229 06:54:51.494653 140524191205120 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.3536607027053833, loss=1.9640910625457764
I0229 06:55:25.288272 140524199597824 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.4348480701446533, loss=1.866288423538208
I0229 06:55:59.115364 140524191205120 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9822999238967896, loss=1.9972302913665771
I0229 06:56:32.933345 140524199597824 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.7510077953338623, loss=2.024204730987549
I0229 06:57:06.777534 140524191205120 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.6680275201797485, loss=1.9707995653152466
I0229 06:57:40.609941 140524199597824 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.4641597270965576, loss=1.8997926712036133
I0229 06:58:14.442956 140524191205120 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7236509323120117, loss=1.7874799966812134
I0229 06:58:48.276631 140524199597824 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.5471612215042114, loss=1.919966459274292
I0229 06:59:22.098678 140524191205120 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.7531719207763672, loss=1.9320803880691528
I0229 06:59:55.909948 140524199597824 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.5468107461929321, loss=1.9859284162521362
I0229 07:00:29.703018 140524191205120 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.4041550159454346, loss=1.9572066068649292
I0229 07:01:03.475439 140524199597824 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.655902624130249, loss=1.8480010032653809
I0229 07:01:37.309390 140524191205120 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.5135200023651123, loss=1.8543683290481567
I0229 07:02:11.142253 140524199597824 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.7479631900787354, loss=1.8666090965270996
I0229 07:02:35.915095 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:02:44.072862 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:02:52.616902 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:02:54.889202 140688601454400 submission_runner.py:411] Time since start: 5369.96s, 	Step: 15075, 	{'train/accuracy': 0.6428770422935486, 'train/loss': 1.4448336362838745, 'validation/accuracy': 0.5908399820327759, 'validation/loss': 1.7287929058074951, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.5280418395996094, 'test/num_examples': 10000, 'score': 5151.217526912689, 'total_duration': 5369.960024833679, 'accumulated_submission_time': 5151.217526912689, 'accumulated_eval_time': 217.94574451446533, 'accumulated_logging_time': 0.2880523204803467}
I0229 07:02:54.923827 140524191205120 logging_writer.py:48] [15075] accumulated_eval_time=217.945745, accumulated_logging_time=0.288052, accumulated_submission_time=5151.217527, global_step=15075, preemption_count=0, score=5151.217527, test/accuracy=0.457100, test/loss=2.528042, test/num_examples=10000, total_duration=5369.960025, train/accuracy=0.642877, train/loss=1.444834, validation/accuracy=0.590840, validation/loss=1.728793, validation/num_examples=50000
I0229 07:03:03.714544 140524199597824 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5974723100662231, loss=2.0342938899993896
I0229 07:03:37.475373 140524191205120 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5440083742141724, loss=1.8865236043930054
I0229 07:04:11.276667 140524199597824 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4756474494934082, loss=1.836560606956482
I0229 07:04:45.064197 140524191205120 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.67684006690979, loss=1.9486451148986816
I0229 07:05:18.889363 140524199597824 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.501907467842102, loss=1.9192582368850708
I0229 07:05:52.826942 140524191205120 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5877580642700195, loss=1.9243966341018677
I0229 07:06:26.654541 140524199597824 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.7366567850112915, loss=2.0308499336242676
I0229 07:07:00.512049 140524191205120 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.5908609628677368, loss=1.8640313148498535
I0229 07:07:34.344138 140524199597824 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.833412528038025, loss=1.8706482648849487
I0229 07:08:08.145395 140524191205120 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.732161283493042, loss=1.9076359272003174
I0229 07:08:41.947523 140524199597824 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.6646029949188232, loss=1.8389546871185303
I0229 07:09:15.759467 140524191205120 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.7288181781768799, loss=1.8064649105072021
I0229 07:09:49.580080 140524199597824 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.6548482179641724, loss=1.8259589672088623
I0229 07:10:23.377974 140524191205120 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.6018799543380737, loss=1.7249397039413452
I0229 07:10:57.222185 140524199597824 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.7274088859558105, loss=1.987441897392273
I0229 07:11:25.062600 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:11:32.672944 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:11:41.228411 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:11:43.489726 140688601454400 submission_runner.py:411] Time since start: 5898.56s, 	Step: 16584, 	{'train/accuracy': 0.6567881107330322, 'train/loss': 1.3828415870666504, 'validation/accuracy': 0.6065399646759033, 'validation/loss': 1.6479262113571167, 'validation/num_examples': 50000, 'test/accuracy': 0.4767000079154968, 'test/loss': 2.3770153522491455, 'test/num_examples': 10000, 'score': 5661.294228553772, 'total_duration': 5898.560553789139, 'accumulated_submission_time': 5661.294228553772, 'accumulated_eval_time': 236.3728106021881, 'accumulated_logging_time': 0.33284974098205566}
I0229 07:11:43.513288 140525256554240 logging_writer.py:48] [16584] accumulated_eval_time=236.372811, accumulated_logging_time=0.332850, accumulated_submission_time=5661.294229, global_step=16584, preemption_count=0, score=5661.294229, test/accuracy=0.476700, test/loss=2.377015, test/num_examples=10000, total_duration=5898.560554, train/accuracy=0.656788, train/loss=1.382842, validation/accuracy=0.606540, validation/loss=1.647926, validation/num_examples=50000
I0229 07:11:49.261645 140525264946944 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.5413163900375366, loss=1.7763066291809082
I0229 07:12:23.161014 140525256554240 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.4880064725875854, loss=1.8561886548995972
I0229 07:12:56.993667 140525264946944 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.5194566249847412, loss=1.9170506000518799
I0229 07:13:30.800869 140525256554240 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.679490327835083, loss=1.964194655418396
I0229 07:14:04.656630 140525264946944 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.580169439315796, loss=1.7641112804412842
I0229 07:14:38.500354 140525256554240 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.483890414237976, loss=1.9240679740905762
I0229 07:15:12.346432 140525264946944 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.5187846422195435, loss=1.816379189491272
I0229 07:15:46.171837 140525256554240 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.05989670753479, loss=1.9206680059432983
I0229 07:16:20.020905 140525264946944 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.377573847770691, loss=1.8518450260162354
I0229 07:16:53.833450 140525256554240 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6963931322097778, loss=1.837634563446045
I0229 07:17:27.665604 140525264946944 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.567184329032898, loss=1.8985705375671387
I0229 07:18:01.714797 140525256554240 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.368257999420166, loss=1.8607525825500488
I0229 07:18:35.552381 140525264946944 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.7879397869110107, loss=2.0157408714294434
I0229 07:19:09.388379 140525256554240 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.593959927558899, loss=1.84980309009552
I0229 07:19:43.187064 140525264946944 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.651087760925293, loss=1.8814671039581299
I0229 07:20:13.726199 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:20:21.873039 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:20:35.512292 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:20:37.749497 140688601454400 submission_runner.py:411] Time since start: 6432.82s, 	Step: 18092, 	{'train/accuracy': 0.6660555005073547, 'train/loss': 1.3375362157821655, 'validation/accuracy': 0.6100599765777588, 'validation/loss': 1.6255896091461182, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.33859920501709, 'test/num_examples': 10000, 'score': 6171.443657398224, 'total_duration': 6432.820313692093, 'accumulated_submission_time': 6171.443657398224, 'accumulated_eval_time': 260.39603447914124, 'accumulated_logging_time': 0.37014007568359375}
I0229 07:20:37.778374 140524182812416 logging_writer.py:48] [18092] accumulated_eval_time=260.396034, accumulated_logging_time=0.370140, accumulated_submission_time=6171.443657, global_step=18092, preemption_count=0, score=6171.443657, test/accuracy=0.491900, test/loss=2.338599, test/num_examples=10000, total_duration=6432.820314, train/accuracy=0.666056, train/loss=1.337536, validation/accuracy=0.610060, validation/loss=1.625590, validation/num_examples=50000
I0229 07:20:40.817236 140524191205120 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.622137188911438, loss=1.7181316614151
I0229 07:21:14.565847 140524182812416 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.873201847076416, loss=1.8497120141983032
I0229 07:21:48.388129 140524191205120 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.676211953163147, loss=1.8262443542480469
I0229 07:22:22.181307 140524182812416 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.429977297782898, loss=1.9079188108444214
I0229 07:22:56.000693 140524191205120 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7019304037094116, loss=1.9087873697280884
I0229 07:23:29.839254 140524182812416 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.5499674081802368, loss=1.8464531898498535
I0229 07:24:03.668223 140524191205120 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.8406662940979004, loss=1.7957806587219238
I0229 07:24:37.555140 140524182812416 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.5432418584823608, loss=1.7482038736343384
I0229 07:25:11.393753 140524191205120 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.608896255493164, loss=2.0695712566375732
I0229 07:25:45.220059 140524182812416 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.7024669647216797, loss=1.9227769374847412
I0229 07:26:19.012904 140524191205120 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5459439754486084, loss=1.745445966720581
I0229 07:26:52.819325 140524182812416 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7957338094711304, loss=1.896902084350586
I0229 07:27:26.623839 140524191205120 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.6955572366714478, loss=1.9002654552459717
I0229 07:28:00.430838 140524182812416 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.852121353149414, loss=1.7810561656951904
I0229 07:28:34.273160 140524191205120 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.630515217781067, loss=1.821028232574463
I0229 07:29:08.099569 140524182812416 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.691107153892517, loss=1.8123905658721924
I0229 07:29:08.108298 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:29:15.843858 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:29:28.634614 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:29:30.847105 140688601454400 submission_runner.py:411] Time since start: 6965.92s, 	Step: 19601, 	{'train/accuracy': 0.6856465339660645, 'train/loss': 1.2491897344589233, 'validation/accuracy': 0.6026399731636047, 'validation/loss': 1.6453914642333984, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.384953022003174, 'test/num_examples': 10000, 'score': 6681.714090824127, 'total_duration': 6965.917953968048, 'accumulated_submission_time': 6681.714090824127, 'accumulated_eval_time': 283.1347830295563, 'accumulated_logging_time': 0.4076879024505615}
I0229 07:29:30.871788 140525256554240 logging_writer.py:48] [19601] accumulated_eval_time=283.134783, accumulated_logging_time=0.407688, accumulated_submission_time=6681.714091, global_step=19601, preemption_count=0, score=6681.714091, test/accuracy=0.482200, test/loss=2.384953, test/num_examples=10000, total_duration=6965.917954, train/accuracy=0.685647, train/loss=1.249190, validation/accuracy=0.602640, validation/loss=1.645391, validation/num_examples=50000
I0229 07:30:04.571065 140525264946944 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.7906280755996704, loss=1.9234386682510376
I0229 07:30:38.376861 140525256554240 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.4863101243972778, loss=1.7529292106628418
I0229 07:31:12.171287 140525264946944 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.801903486251831, loss=1.8824025392532349
I0229 07:31:45.987800 140525256554240 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.877866268157959, loss=1.778479814529419
I0229 07:32:19.795011 140525264946944 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.7151273488998413, loss=1.870221734046936
I0229 07:32:53.609633 140525256554240 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.710299015045166, loss=1.7885327339172363
I0229 07:33:27.420647 140525264946944 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.7924307584762573, loss=1.925269365310669
I0229 07:34:01.237160 140525256554240 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.5036778450012207, loss=1.8057348728179932
I0229 07:34:35.060934 140525264946944 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.4978289604187012, loss=1.62254798412323
I0229 07:35:08.901026 140525256554240 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.7851850986480713, loss=1.8929885625839233
I0229 07:35:42.685277 140525264946944 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.7286341190338135, loss=1.8459973335266113
I0229 07:36:16.521646 140525256554240 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.697312831878662, loss=1.8814444541931152
I0229 07:36:50.463412 140525264946944 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.6682090759277344, loss=1.8333415985107422
I0229 07:37:24.315596 140525256554240 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.6882426738739014, loss=1.742093563079834
I0229 07:37:58.120699 140525264946944 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.726563811302185, loss=1.7042547464370728
I0229 07:38:00.928910 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:38:08.635857 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:38:22.271442 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:38:24.364904 140688601454400 submission_runner.py:411] Time since start: 7499.44s, 	Step: 21110, 	{'train/accuracy': 0.6740672588348389, 'train/loss': 1.284901738166809, 'validation/accuracy': 0.612060010433197, 'validation/loss': 1.6220479011535645, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.3641345500946045, 'test/num_examples': 10000, 'score': 7191.711829662323, 'total_duration': 7499.435747146606, 'accumulated_submission_time': 7191.711829662323, 'accumulated_eval_time': 306.5707335472107, 'accumulated_logging_time': 0.4417273998260498}
I0229 07:38:24.395450 140524199597824 logging_writer.py:48] [21110] accumulated_eval_time=306.570734, accumulated_logging_time=0.441727, accumulated_submission_time=7191.711830, global_step=21110, preemption_count=0, score=7191.711830, test/accuracy=0.487400, test/loss=2.364135, test/num_examples=10000, total_duration=7499.435747, train/accuracy=0.674067, train/loss=1.284902, validation/accuracy=0.612060, validation/loss=1.622048, validation/num_examples=50000
I0229 07:38:55.047568 140524207990528 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.559640645980835, loss=1.9249680042266846
I0229 07:39:28.811385 140524199597824 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.879986047744751, loss=1.8029112815856934
I0229 07:40:02.594852 140524207990528 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.0661509037017822, loss=1.8225212097167969
I0229 07:40:36.397463 140524199597824 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.6983615159988403, loss=1.7541295289993286
I0229 07:41:10.189644 140524207990528 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.804284930229187, loss=1.6764452457427979
I0229 07:41:43.998814 140524199597824 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.6257566213607788, loss=1.7676535844802856
I0229 07:42:17.783356 140524207990528 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.7317070960998535, loss=1.794597864151001
I0229 07:42:51.688020 140524199597824 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5651419162750244, loss=1.7994449138641357
I0229 07:43:25.484207 140524207990528 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.7842708826065063, loss=1.9096177816390991
I0229 07:43:59.290709 140524199597824 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.778120994567871, loss=1.773366093635559
I0229 07:44:33.086313 140524207990528 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.7853631973266602, loss=1.7591995000839233
I0229 07:45:06.887435 140524199597824 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.558143973350525, loss=1.8582286834716797
I0229 07:45:40.704438 140524207990528 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.6249147653579712, loss=1.8633604049682617
I0229 07:46:14.500030 140524199597824 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.7460212707519531, loss=1.7482637166976929
I0229 07:46:48.299734 140524207990528 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.621450662612915, loss=1.7900385856628418
I0229 07:46:54.487810 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:47:02.707200 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:47:16.388971 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:47:18.645010 140688601454400 submission_runner.py:411] Time since start: 8033.72s, 	Step: 22620, 	{'train/accuracy': 0.6695631146430969, 'train/loss': 1.3065211772918701, 'validation/accuracy': 0.6083199977874756, 'validation/loss': 1.6256723403930664, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.382908344268799, 'test/num_examples': 10000, 'score': 7701.745020151138, 'total_duration': 8033.715864419937, 'accumulated_submission_time': 7701.745020151138, 'accumulated_eval_time': 330.72790360450745, 'accumulated_logging_time': 0.48030638694763184}
I0229 07:47:18.673317 140524199597824 logging_writer.py:48] [22620] accumulated_eval_time=330.727904, accumulated_logging_time=0.480306, accumulated_submission_time=7701.745020, global_step=22620, preemption_count=0, score=7701.745020, test/accuracy=0.483700, test/loss=2.382908, test/num_examples=10000, total_duration=8033.715864, train/accuracy=0.669563, train/loss=1.306521, validation/accuracy=0.608320, validation/loss=1.625672, validation/num_examples=50000
I0229 07:47:46.016805 140524207990528 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.75870943069458, loss=1.8644131422042847
I0229 07:48:19.741800 140524199597824 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.950665831565857, loss=1.868340253829956
I0229 07:48:53.534167 140524207990528 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.752350926399231, loss=1.7646199464797974
I0229 07:49:27.398470 140524199597824 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.5696797370910645, loss=1.8278841972351074
I0229 07:50:01.225374 140524207990528 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.872314691543579, loss=1.821382761001587
I0229 07:50:35.048321 140524199597824 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.8135398626327515, loss=1.8899340629577637
I0229 07:51:08.855540 140524207990528 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.8094598054885864, loss=1.844578742980957
I0229 07:51:42.678885 140524199597824 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.224688768386841, loss=1.6961569786071777
I0229 07:52:16.488467 140524207990528 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.736867904663086, loss=1.7157166004180908
I0229 07:52:50.332166 140524199597824 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.7188702821731567, loss=1.6617722511291504
I0229 07:53:24.130158 140524207990528 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.7334909439086914, loss=1.8271645307540894
I0229 07:53:57.929775 140524199597824 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.8799437284469604, loss=1.879288911819458
I0229 07:54:31.705508 140524207990528 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.5471514463424683, loss=1.647936224937439
I0229 07:55:05.510191 140524199597824 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7001885175704956, loss=1.8414970636367798
I0229 07:55:39.375694 140524207990528 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.849310040473938, loss=1.7290431261062622
I0229 07:55:48.936450 140688601454400 spec.py:321] Evaluating on the training split.
I0229 07:55:56.665247 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 07:56:11.226030 140688601454400 spec.py:349] Evaluating on the test split.
I0229 07:56:13.421664 140688601454400 submission_runner.py:411] Time since start: 8568.49s, 	Step: 24130, 	{'train/accuracy': 0.6800462007522583, 'train/loss': 1.271593689918518, 'validation/accuracy': 0.6187199950218201, 'validation/loss': 1.5880154371261597, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.347294330596924, 'test/num_examples': 10000, 'score': 8211.9476480484, 'total_duration': 8568.492515087128, 'accumulated_submission_time': 8211.9476480484, 'accumulated_eval_time': 355.2130854129791, 'accumulated_logging_time': 0.5175192356109619}
I0229 07:56:13.439880 140524191205120 logging_writer.py:48] [24130] accumulated_eval_time=355.213085, accumulated_logging_time=0.517519, accumulated_submission_time=8211.947648, global_step=24130, preemption_count=0, score=8211.947648, test/accuracy=0.492800, test/loss=2.347294, test/num_examples=10000, total_duration=8568.492515, train/accuracy=0.680046, train/loss=1.271594, validation/accuracy=0.618720, validation/loss=1.588015, validation/num_examples=50000
I0229 07:56:37.412490 140524199597824 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.7490121126174927, loss=1.7847980260849
I0229 07:57:11.153414 140524191205120 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.7963297367095947, loss=1.778276801109314
I0229 07:57:44.948291 140524199597824 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.6206878423690796, loss=1.7108192443847656
I0229 07:58:18.732256 140524191205120 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6071652173995972, loss=1.7952046394348145
I0229 07:58:52.521557 140524199597824 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.8036009073257446, loss=1.719712495803833
I0229 07:59:26.361062 140524191205120 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.8873275518417358, loss=1.6964625120162964
I0229 08:00:00.160302 140524199597824 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.7913819551467896, loss=1.7499185800552368
I0229 08:00:33.973537 140524191205120 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.8665002584457397, loss=1.814316749572754
I0229 08:01:07.787349 140524199597824 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.6369562149047852, loss=1.6531968116760254
I0229 08:01:41.611569 140524191205120 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.6516847610473633, loss=1.8595771789550781
I0229 08:02:15.419236 140524199597824 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.8326703310012817, loss=1.707953691482544
I0229 08:02:49.228760 140524191205120 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.5445597171783447, loss=1.7599588632583618
I0229 08:03:23.060079 140524199597824 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8236958980560303, loss=1.7556649446487427
I0229 08:03:56.841433 140524191205120 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.666654109954834, loss=1.7111462354660034
I0229 08:04:30.669145 140524199597824 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.4245243072509766, loss=1.7914403676986694
I0229 08:04:43.626487 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:04:51.302775 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:05:04.548429 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:05:06.688516 140688601454400 submission_runner.py:411] Time since start: 9101.76s, 	Step: 25640, 	{'train/accuracy': 0.6746651530265808, 'train/loss': 1.286739468574524, 'validation/accuracy': 0.6209200024604797, 'validation/loss': 1.5903011560440063, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.3183577060699463, 'test/num_examples': 10000, 'score': 8722.075249195099, 'total_duration': 9101.75936460495, 'accumulated_submission_time': 8722.075249195099, 'accumulated_eval_time': 378.2750778198242, 'accumulated_logging_time': 0.5450489521026611}
I0229 08:05:06.710273 140525256554240 logging_writer.py:48] [25640] accumulated_eval_time=378.275078, accumulated_logging_time=0.545049, accumulated_submission_time=8722.075249, global_step=25640, preemption_count=0, score=8722.075249, test/accuracy=0.497600, test/loss=2.318358, test/num_examples=10000, total_duration=9101.759365, train/accuracy=0.674665, train/loss=1.286739, validation/accuracy=0.620920, validation/loss=1.590301, validation/num_examples=50000
I0229 08:05:27.264025 140525264946944 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.813117265701294, loss=1.6794164180755615
I0229 08:06:01.021438 140525256554240 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.681586742401123, loss=1.6995131969451904
I0229 08:06:34.806874 140525264946944 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.9654914140701294, loss=1.695152997970581
I0229 08:07:08.612109 140525256554240 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8062587976455688, loss=1.7587213516235352
I0229 08:07:42.402624 140525264946944 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.632401466369629, loss=1.7186155319213867
I0229 08:08:16.213629 140525256554240 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.671021819114685, loss=1.6549022197723389
I0229 08:08:50.042662 140525264946944 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6789964437484741, loss=1.680585503578186
I0229 08:09:23.805504 140525256554240 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.7350040674209595, loss=1.6417021751403809
I0229 08:09:57.622807 140525264946944 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.5877892971038818, loss=1.6900385618209839
I0229 08:10:31.412176 140525256554240 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6204872131347656, loss=1.7454129457473755
I0229 08:11:05.245888 140525264946944 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.901903748512268, loss=1.8320472240447998
I0229 08:11:39.076276 140525256554240 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.6366546154022217, loss=1.8729954957962036
I0229 08:12:12.875190 140525264946944 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.8641027212142944, loss=1.7771676778793335
I0229 08:12:46.706289 140525256554240 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.8723584413528442, loss=1.8214545249938965
I0229 08:13:20.526331 140525264946944 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.6713868379592896, loss=1.5775299072265625
I0229 08:13:36.852195 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:13:44.585136 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:13:57.123458 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:13:59.814847 140688601454400 submission_runner.py:411] Time since start: 9634.89s, 	Step: 27150, 	{'train/accuracy': 0.6598373651504517, 'train/loss': 1.3535218238830566, 'validation/accuracy': 0.6033399701118469, 'validation/loss': 1.6538441181182861, 'validation/num_examples': 50000, 'test/accuracy': 0.47770002484321594, 'test/loss': 2.4134693145751953, 'test/num_examples': 10000, 'score': 9232.15855383873, 'total_duration': 9634.885572433472, 'accumulated_submission_time': 9232.15855383873, 'accumulated_eval_time': 401.23756408691406, 'accumulated_logging_time': 0.5751643180847168}
I0229 08:13:59.836830 140524191205120 logging_writer.py:48] [27150] accumulated_eval_time=401.237564, accumulated_logging_time=0.575164, accumulated_submission_time=9232.158554, global_step=27150, preemption_count=0, score=9232.158554, test/accuracy=0.477700, test/loss=2.413469, test/num_examples=10000, total_duration=9634.885572, train/accuracy=0.659837, train/loss=1.353522, validation/accuracy=0.603340, validation/loss=1.653844, validation/num_examples=50000
I0229 08:14:17.332842 140524199597824 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.7192505598068237, loss=1.611222267150879
I0229 08:14:51.048307 140524191205120 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.520904779434204, loss=1.7784255743026733
I0229 08:15:24.824520 140524199597824 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.7164164781570435, loss=1.7207958698272705
I0229 08:15:58.615482 140524191205120 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.938716173171997, loss=1.706063151359558
I0229 08:16:32.439619 140524199597824 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.7510203123092651, loss=1.7345077991485596
I0229 08:17:06.243865 140524191205120 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.0839428901672363, loss=1.8655929565429688
I0229 08:17:40.064205 140524199597824 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.9245946407318115, loss=1.6957547664642334
I0229 08:18:13.863439 140524191205120 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.6619679927825928, loss=1.8318219184875488
I0229 08:18:47.661595 140524199597824 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.8370717763900757, loss=1.6439305543899536
I0229 08:19:21.463753 140524191205120 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.7022275924682617, loss=1.7651827335357666
I0229 08:19:55.293003 140524199597824 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.7701059579849243, loss=1.641776442527771
I0229 08:20:29.165053 140524191205120 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.7764942646026611, loss=1.8119697570800781
I0229 08:21:02.981742 140524199597824 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.8422874212265015, loss=1.7494103908538818
I0229 08:21:36.785239 140524191205120 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.5966942310333252, loss=1.7259175777435303
I0229 08:22:10.604561 140524199597824 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.8848689794540405, loss=1.7318806648254395
I0229 08:22:29.990625 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:22:37.853897 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:22:49.754836 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:22:52.025626 140688601454400 submission_runner.py:411] Time since start: 10167.10s, 	Step: 28659, 	{'train/accuracy': 0.7111367583274841, 'train/loss': 1.1349786520004272, 'validation/accuracy': 0.6277799606323242, 'validation/loss': 1.5449466705322266, 'validation/num_examples': 50000, 'test/accuracy': 0.5024999976158142, 'test/loss': 2.2936408519744873, 'test/num_examples': 10000, 'score': 9742.014395713806, 'total_duration': 10167.096479654312, 'accumulated_submission_time': 9742.014395713806, 'accumulated_eval_time': 423.27252864837646, 'accumulated_logging_time': 0.8448653221130371}
I0229 08:22:52.044672 140524216383232 logging_writer.py:48] [28659] accumulated_eval_time=423.272529, accumulated_logging_time=0.844865, accumulated_submission_time=9742.014396, global_step=28659, preemption_count=0, score=9742.014396, test/accuracy=0.502500, test/loss=2.293641, test/num_examples=10000, total_duration=10167.096480, train/accuracy=0.711137, train/loss=1.134979, validation/accuracy=0.627780, validation/loss=1.544947, validation/num_examples=50000
I0229 08:23:06.212491 140525256554240 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.8206661939620972, loss=1.7525767087936401
I0229 08:23:39.954285 140524216383232 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.690269112586975, loss=1.6599512100219727
I0229 08:24:13.725587 140525256554240 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6437461376190186, loss=1.7588578462600708
I0229 08:24:47.533527 140524216383232 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.034484624862671, loss=1.8753979206085205
I0229 08:25:21.345956 140525256554240 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.021385431289673, loss=1.695493221282959
I0229 08:25:55.158752 140524216383232 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.8666938543319702, loss=1.7081800699234009
I0229 08:26:29.049160 140525256554240 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.791166067123413, loss=1.8441880941390991
I0229 08:27:02.856263 140524216383232 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.8191440105438232, loss=1.6306724548339844
I0229 08:27:36.666693 140525256554240 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.6795364618301392, loss=1.7132948637008667
I0229 08:28:10.481073 140524216383232 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.6342058181762695, loss=1.7576638460159302
I0229 08:28:44.316476 140525256554240 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.5504730939865112, loss=1.7059171199798584
I0229 08:29:18.124627 140524216383232 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7574057579040527, loss=1.714491367340088
I0229 08:29:51.934022 140525256554240 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.9127832651138306, loss=1.6129664182662964
I0229 08:30:25.732237 140524216383232 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.5415022373199463, loss=1.6054387092590332
I0229 08:30:59.564509 140525256554240 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.7698935270309448, loss=1.7274365425109863
I0229 08:31:22.339726 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:31:30.168665 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:31:44.889195 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:31:47.104403 140688601454400 submission_runner.py:411] Time since start: 10702.18s, 	Step: 30169, 	{'train/accuracy': 0.6936184763908386, 'train/loss': 1.211167335510254, 'validation/accuracy': 0.6198599934577942, 'validation/loss': 1.5693773031234741, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.3065173625946045, 'test/num_examples': 10000, 'score': 10252.25020313263, 'total_duration': 10702.17523431778, 'accumulated_submission_time': 10252.25020313263, 'accumulated_eval_time': 448.03714537620544, 'accumulated_logging_time': 0.8730454444885254}
I0229 08:31:47.126543 140524182812416 logging_writer.py:48] [30169] accumulated_eval_time=448.037145, accumulated_logging_time=0.873045, accumulated_submission_time=10252.250203, global_step=30169, preemption_count=0, score=10252.250203, test/accuracy=0.491900, test/loss=2.306517, test/num_examples=10000, total_duration=10702.175234, train/accuracy=0.693618, train/loss=1.211167, validation/accuracy=0.619860, validation/loss=1.569377, validation/num_examples=50000
I0229 08:31:57.949567 140524191205120 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.7428339719772339, loss=1.6591205596923828
I0229 08:32:31.702023 140524182812416 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.799344778060913, loss=1.758713722229004
I0229 08:33:05.537243 140524191205120 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.732253074645996, loss=1.6983907222747803
I0229 08:33:39.359376 140524182812416 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.043078660964966, loss=1.7105032205581665
I0229 08:34:13.164348 140524191205120 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.7278028726577759, loss=1.751955509185791
I0229 08:34:46.981305 140524182812416 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.7524560689926147, loss=1.6446374654769897
I0229 08:35:20.801509 140524191205120 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7038135528564453, loss=1.7646406888961792
I0229 08:35:54.587940 140524182812416 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.6698189973831177, loss=1.670993447303772
I0229 08:36:28.395346 140524191205120 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.9325709342956543, loss=1.7312986850738525
I0229 08:37:02.246223 140524182812416 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.8906099796295166, loss=1.841774582862854
I0229 08:37:36.060219 140524191205120 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.7925945520401, loss=1.7297195196151733
I0229 08:38:09.847959 140524182812416 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.958797574043274, loss=1.931957483291626
I0229 08:38:43.679772 140524191205120 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.8336422443389893, loss=1.6342447996139526
I0229 08:39:17.497089 140524182812416 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7471476793289185, loss=1.7170710563659668
I0229 08:39:51.311131 140524191205120 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.79141104221344, loss=1.6968543529510498
I0229 08:40:17.123773 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:40:25.167534 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:40:37.473470 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:40:39.710563 140688601454400 submission_runner.py:411] Time since start: 11234.78s, 	Step: 31678, 	{'train/accuracy': 0.6753029227256775, 'train/loss': 1.2694958448410034, 'validation/accuracy': 0.6157999634742737, 'validation/loss': 1.6059370040893555, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.335855722427368, 'test/num_examples': 10000, 'score': 10762.185815572739, 'total_duration': 11234.781381845474, 'accumulated_submission_time': 10762.185815572739, 'accumulated_eval_time': 470.6238646507263, 'accumulated_logging_time': 0.9068002700805664}
I0229 08:40:39.735603 140525256554240 logging_writer.py:48] [31678] accumulated_eval_time=470.623865, accumulated_logging_time=0.906800, accumulated_submission_time=10762.185816, global_step=31678, preemption_count=0, score=10762.185816, test/accuracy=0.488000, test/loss=2.335856, test/num_examples=10000, total_duration=11234.781382, train/accuracy=0.675303, train/loss=1.269496, validation/accuracy=0.615800, validation/loss=1.605937, validation/num_examples=50000
I0229 08:40:47.509845 140525264946944 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.7967770099639893, loss=1.733416199684143
I0229 08:41:21.229628 140525256554240 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7360209226608276, loss=1.6616857051849365
I0229 08:41:55.020405 140525264946944 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.7873092889785767, loss=1.7737537622451782
I0229 08:42:28.797912 140525256554240 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.7189443111419678, loss=1.669596552848816
I0229 08:43:02.573180 140525264946944 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.714766025543213, loss=1.7138152122497559
I0229 08:43:36.348341 140525256554240 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6508762836456299, loss=1.6182352304458618
I0229 08:44:10.120793 140525264946944 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.8352092504501343, loss=1.7224029302597046
I0229 08:44:43.938438 140525256554240 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.8829790353775024, loss=1.7498517036437988
I0229 08:45:17.792081 140525264946944 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7688864469528198, loss=1.6218583583831787
I0229 08:45:51.563693 140525256554240 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.773119330406189, loss=1.7772196531295776
I0229 08:46:25.339675 140525264946944 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.0977795124053955, loss=1.7414774894714355
I0229 08:46:59.050975 140525256554240 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.9130617380142212, loss=1.7602934837341309
I0229 08:47:32.852267 140525264946944 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.7555466890335083, loss=1.6035041809082031
I0229 08:48:06.630880 140525256554240 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.7157973051071167, loss=1.7812156677246094
I0229 08:48:40.459628 140525264946944 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.7735148668289185, loss=1.6682969331741333
I0229 08:49:09.912443 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:49:18.173202 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:49:29.687017 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:49:31.946468 140688601454400 submission_runner.py:411] Time since start: 11767.02s, 	Step: 33189, 	{'train/accuracy': 0.6901506781578064, 'train/loss': 1.2144831418991089, 'validation/accuracy': 0.6292600035667419, 'validation/loss': 1.5291776657104492, 'validation/num_examples': 50000, 'test/accuracy': 0.5026000142097473, 'test/loss': 2.2959067821502686, 'test/num_examples': 10000, 'score': 11272.298690795898, 'total_duration': 11767.017308950424, 'accumulated_submission_time': 11272.298690795898, 'accumulated_eval_time': 492.6578459739685, 'accumulated_logging_time': 0.9439327716827393}
I0229 08:49:31.972323 140522396051200 logging_writer.py:48] [33189] accumulated_eval_time=492.657846, accumulated_logging_time=0.943933, accumulated_submission_time=11272.298691, global_step=33189, preemption_count=0, score=11272.298691, test/accuracy=0.502600, test/loss=2.295907, test/num_examples=10000, total_duration=11767.017309, train/accuracy=0.690151, train/loss=1.214483, validation/accuracy=0.629260, validation/loss=1.529178, validation/num_examples=50000
I0229 08:49:36.057937 140522404443904 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.7787463665008545, loss=1.6391326189041138
I0229 08:50:09.776656 140522396051200 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8474477529525757, loss=1.769213080406189
I0229 08:50:43.541153 140522404443904 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.762816071510315, loss=1.669240117073059
I0229 08:51:17.382762 140522396051200 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.8212019205093384, loss=1.7304611206054688
I0229 08:51:51.140236 140522404443904 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.6681957244873047, loss=1.5751683712005615
I0229 08:52:24.954872 140522396051200 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.8911173343658447, loss=1.64417564868927
I0229 08:52:58.752336 140522404443904 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8005542755126953, loss=1.7150049209594727
I0229 08:53:32.557944 140522396051200 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.834337830543518, loss=1.7507591247558594
I0229 08:54:06.378876 140522404443904 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.6216199398040771, loss=1.7467288970947266
I0229 08:54:40.178185 140522396051200 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.739907145500183, loss=1.5997982025146484
I0229 08:55:13.974698 140522404443904 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.991379737854004, loss=1.7112939357757568
I0229 08:55:47.792911 140522396051200 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.7009302377700806, loss=1.585218906402588
I0229 08:56:21.585408 140522404443904 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.6323989629745483, loss=1.6937141418457031
I0229 08:56:55.446855 140522396051200 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.7747447490692139, loss=1.6096386909484863
I0229 08:57:29.305977 140522404443904 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.0537827014923096, loss=1.722255825996399
I0229 08:58:02.212411 140688601454400 spec.py:321] Evaluating on the training split.
I0229 08:58:10.520639 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 08:58:21.538902 140688601454400 spec.py:349] Evaluating on the test split.
I0229 08:58:23.820094 140688601454400 submission_runner.py:411] Time since start: 12298.89s, 	Step: 34699, 	{'train/accuracy': 0.6905093789100647, 'train/loss': 1.2288453578948975, 'validation/accuracy': 0.630079984664917, 'validation/loss': 1.5220123529434204, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.252868890762329, 'test/num_examples': 10000, 'score': 11782.477632045746, 'total_duration': 12298.890912532806, 'accumulated_submission_time': 11782.477632045746, 'accumulated_eval_time': 514.2654712200165, 'accumulated_logging_time': 0.9805140495300293}
I0229 08:58:23.846580 140525264946944 logging_writer.py:48] [34699] accumulated_eval_time=514.265471, accumulated_logging_time=0.980514, accumulated_submission_time=11782.477632, global_step=34699, preemption_count=0, score=11782.477632, test/accuracy=0.500500, test/loss=2.252869, test/num_examples=10000, total_duration=12298.890913, train/accuracy=0.690509, train/loss=1.228845, validation/accuracy=0.630080, validation/loss=1.522012, validation/num_examples=50000
I0229 08:58:24.545764 140525273339648 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.722312331199646, loss=1.7602481842041016
I0229 08:58:58.311632 140525264946944 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.0365843772888184, loss=1.5990105867385864
I0229 08:59:32.079923 140525273339648 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.6045560836791992, loss=1.6542468070983887
I0229 09:00:05.873978 140525264946944 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.5984985828399658, loss=1.6259841918945312
I0229 09:00:39.674008 140525273339648 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.9453985691070557, loss=1.658289909362793
I0229 09:01:13.491652 140525264946944 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.694620132446289, loss=1.7077261209487915
I0229 09:01:47.324750 140525273339648 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.6624234914779663, loss=1.590705394744873
I0229 09:02:21.177221 140525264946944 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.8659523725509644, loss=1.7358802556991577
I0229 09:02:55.009717 140525273339648 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.04512357711792, loss=1.673825740814209
I0229 09:03:28.863457 140525264946944 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.0927505493164062, loss=1.7598731517791748
I0229 09:04:02.664316 140525273339648 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.9419282674789429, loss=1.7932029962539673
I0229 09:04:36.463788 140525264946944 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.9174304008483887, loss=1.7141444683074951
I0229 09:05:10.278719 140525273339648 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.9896129369735718, loss=1.7063360214233398
I0229 09:05:44.114471 140525264946944 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.7522494792938232, loss=1.7077136039733887
I0229 09:06:17.950304 140525273339648 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7559834718704224, loss=1.654237151145935
I0229 09:06:51.783662 140525264946944 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6758019924163818, loss=1.7580504417419434
I0229 09:06:53.934183 140688601454400 spec.py:321] Evaluating on the training split.
I0229 09:07:02.319595 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 09:07:13.719939 140688601454400 spec.py:349] Evaluating on the test split.
I0229 09:07:16.029642 140688601454400 submission_runner.py:411] Time since start: 12831.10s, 	Step: 36208, 	{'train/accuracy': 0.6882174611091614, 'train/loss': 1.2332067489624023, 'validation/accuracy': 0.6226599812507629, 'validation/loss': 1.570199966430664, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.327148199081421, 'test/num_examples': 10000, 'score': 12292.503982305527, 'total_duration': 12831.100484848022, 'accumulated_submission_time': 12292.503982305527, 'accumulated_eval_time': 536.3608770370483, 'accumulated_logging_time': 1.0178706645965576}
I0229 09:07:16.052599 140522396051200 logging_writer.py:48] [36208] accumulated_eval_time=536.360877, accumulated_logging_time=1.017871, accumulated_submission_time=12292.503982, global_step=36208, preemption_count=0, score=12292.503982, test/accuracy=0.495700, test/loss=2.327148, test/num_examples=10000, total_duration=12831.100485, train/accuracy=0.688217, train/loss=1.233207, validation/accuracy=0.622660, validation/loss=1.570200, validation/num_examples=50000
I0229 09:07:47.438813 140522404443904 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.629010796546936, loss=1.6938159465789795
I0229 09:08:21.175133 140522396051200 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.039879083633423, loss=1.744853138923645
I0229 09:08:54.962271 140522404443904 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.8366492986679077, loss=1.7275910377502441
I0229 09:09:28.756625 140522396051200 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.7928470373153687, loss=1.7114365100860596
I0229 09:10:02.619461 140522404443904 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.95887291431427, loss=1.7308372259140015
I0229 09:10:36.434283 140522396051200 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.9468944072723389, loss=1.7275899648666382
I0229 09:11:10.242150 140522404443904 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.743928074836731, loss=1.7159990072250366
I0229 09:11:44.056062 140522396051200 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.8041467666625977, loss=1.743050456047058
I0229 09:12:17.881279 140522404443904 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7561113834381104, loss=1.6628308296203613
I0229 09:12:51.700650 140522396051200 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.7586311101913452, loss=1.6357280015945435
I0229 09:13:25.506701 140522404443904 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.9321364164352417, loss=1.6332008838653564
I0229 09:13:59.275937 140522396051200 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.569226861000061, loss=1.450665831565857
I0229 09:14:33.054464 140522404443904 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.7769091129302979, loss=1.6040112972259521
I0229 09:15:06.873840 140522396051200 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.673723816871643, loss=1.7047228813171387
I0229 09:15:40.663665 140522404443904 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.0009355545043945, loss=1.8005101680755615
I0229 09:15:46.264348 140688601454400 spec.py:321] Evaluating on the training split.
I0229 09:15:53.597017 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 09:16:03.169399 140688601454400 spec.py:349] Evaluating on the test split.
I0229 09:16:05.453977 140688601454400 submission_runner.py:411] Time since start: 13360.52s, 	Step: 37718, 	{'train/accuracy': 0.6916055083274841, 'train/loss': 1.2146902084350586, 'validation/accuracy': 0.6141799688339233, 'validation/loss': 1.6082170009613037, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.3108022212982178, 'test/num_examples': 10000, 'score': 12802.65355682373, 'total_duration': 13360.524811983109, 'accumulated_submission_time': 12802.65355682373, 'accumulated_eval_time': 555.5504469871521, 'accumulated_logging_time': 1.0512502193450928}
I0229 09:16:05.483644 140522396051200 logging_writer.py:48] [37718] accumulated_eval_time=555.550447, accumulated_logging_time=1.051250, accumulated_submission_time=12802.653557, global_step=37718, preemption_count=0, score=12802.653557, test/accuracy=0.486400, test/loss=2.310802, test/num_examples=10000, total_duration=13360.524812, train/accuracy=0.691606, train/loss=1.214690, validation/accuracy=0.614180, validation/loss=1.608217, validation/num_examples=50000
I0229 09:16:33.471648 140525264946944 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.2527709007263184, loss=1.8966383934020996
I0229 09:17:07.226116 140522396051200 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.0094010829925537, loss=1.5889227390289307
I0229 09:17:41.027477 140525264946944 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.0999574661254883, loss=1.6684088706970215
I0229 09:18:14.851048 140522396051200 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8346428871154785, loss=1.739704966545105
I0229 09:18:48.651027 140525264946944 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.8277417421340942, loss=1.7337502241134644
I0229 09:19:22.499908 140522396051200 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.8140199184417725, loss=1.6269679069519043
I0229 09:19:56.304680 140525264946944 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.782106637954712, loss=1.671781301498413
I0229 09:20:30.142797 140522396051200 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.779547095298767, loss=1.5004183053970337
I0229 09:21:03.913986 140525264946944 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.6669012308120728, loss=1.6192377805709839
I0229 09:21:37.749525 140522396051200 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.910582184791565, loss=1.6892950534820557
I0229 09:22:11.574189 140525264946944 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.667328119277954, loss=1.703716516494751
I0229 09:22:45.353684 140522396051200 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.7279952764511108, loss=1.5842208862304688
I0229 09:23:19.152377 140525264946944 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.6719478368759155, loss=1.572196125984192
I0229 09:23:52.975294 140522396051200 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.7019444704055786, loss=1.5309301614761353
I0229 09:24:26.774627 140525264946944 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.7302061319351196, loss=1.7752313613891602
I0229 09:24:35.717261 140688601454400 spec.py:321] Evaluating on the training split.
I0229 09:24:42.909749 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 09:24:52.696343 140688601454400 spec.py:349] Evaluating on the test split.
I0229 09:24:54.992806 140688601454400 submission_runner.py:411] Time since start: 13890.06s, 	Step: 39228, 	{'train/accuracy': 0.7033043503761292, 'train/loss': 1.156270980834961, 'validation/accuracy': 0.6322799921035767, 'validation/loss': 1.516471028327942, 'validation/num_examples': 50000, 'test/accuracy': 0.5144000053405762, 'test/loss': 2.233076572418213, 'test/num_examples': 10000, 'score': 13312.825942516327, 'total_duration': 13890.063405036926, 'accumulated_submission_time': 13312.825942516327, 'accumulated_eval_time': 574.8256969451904, 'accumulated_logging_time': 1.0908870697021484}
I0229 09:24:55.018047 140523092305664 logging_writer.py:48] [39228] accumulated_eval_time=574.825697, accumulated_logging_time=1.090887, accumulated_submission_time=13312.825943, global_step=39228, preemption_count=0, score=13312.825943, test/accuracy=0.514400, test/loss=2.233077, test/num_examples=10000, total_duration=13890.063405, train/accuracy=0.703304, train/loss=1.156271, validation/accuracy=0.632280, validation/loss=1.516471, validation/num_examples=50000
I0229 09:25:19.666702 140523947947776 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7277171611785889, loss=1.709810733795166
I0229 09:25:53.416976 140523092305664 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.7848756313323975, loss=1.72104811668396
I0229 09:26:27.202612 140523947947776 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.6815803050994873, loss=1.6840076446533203
I0229 09:27:01.020329 140523092305664 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7673572301864624, loss=1.7205039262771606
I0229 09:27:34.837898 140523947947776 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.9115583896636963, loss=1.730509877204895
I0229 09:28:08.721229 140523092305664 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.8267155885696411, loss=1.6575350761413574
I0229 09:28:42.554076 140523947947776 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.6974185705184937, loss=1.708086609840393
I0229 09:29:16.408229 140523092305664 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.9952014684677124, loss=1.6848284006118774
I0229 09:29:50.212538 140523947947776 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.820531964302063, loss=1.542263150215149
I0229 09:30:24.016655 140523092305664 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.8187447786331177, loss=1.672534465789795
I0229 09:30:57.833414 140523947947776 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.7989808320999146, loss=1.7378389835357666
I0229 09:31:31.630501 140523092305664 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.7292096614837646, loss=1.639580249786377
I0229 09:32:05.449363 140523947947776 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.8402416706085205, loss=1.563917875289917
I0229 09:32:39.269618 140523092305664 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.6302727460861206, loss=1.6489731073379517
I0229 09:33:13.102526 140523947947776 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.939666509628296, loss=1.6423653364181519
I0229 09:33:25.062875 140688601454400 spec.py:321] Evaluating on the training split.
I0229 09:33:32.001945 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 09:33:44.869273 140688601454400 spec.py:349] Evaluating on the test split.
I0229 09:33:46.981723 140688601454400 submission_runner.py:411] Time since start: 14422.05s, 	Step: 40737, 	{'train/accuracy': 0.698640763759613, 'train/loss': 1.1931952238082886, 'validation/accuracy': 0.6297799944877625, 'validation/loss': 1.5363556146621704, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2545292377471924, 'test/num_examples': 10000, 'score': 13822.809534072876, 'total_duration': 14422.052576303482, 'accumulated_submission_time': 13822.809534072876, 'accumulated_eval_time': 596.7445023059845, 'accumulated_logging_time': 1.1273493766784668}
I0229 09:33:47.001998 140522396051200 logging_writer.py:48] [40737] accumulated_eval_time=596.744502, accumulated_logging_time=1.127349, accumulated_submission_time=13822.809534, global_step=40737, preemption_count=0, score=13822.809534, test/accuracy=0.500100, test/loss=2.254529, test/num_examples=10000, total_duration=14422.052576, train/accuracy=0.698641, train/loss=1.193195, validation/accuracy=0.629780, validation/loss=1.536356, validation/num_examples=50000
I0229 09:34:08.632582 140522404443904 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.9594684839248657, loss=1.707679271697998
I0229 09:34:42.379967 140522396051200 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.8604862689971924, loss=1.6576918363571167
I0229 09:35:16.141334 140522404443904 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.7532798051834106, loss=1.6565110683441162
I0229 09:35:49.971960 140522396051200 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.9345645904541016, loss=1.7245913743972778
I0229 09:36:23.800534 140522404443904 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.7777971029281616, loss=1.7007393836975098
I0229 09:36:57.603170 140522396051200 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.713852047920227, loss=1.699961543083191
I0229 09:37:31.429486 140522404443904 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7224828004837036, loss=1.6745294332504272
I0229 09:38:05.248611 140522396051200 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.7705204486846924, loss=1.7394630908966064
I0229 09:38:39.051152 140522404443904 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.2268943786621094, loss=1.6595797538757324
I0229 09:39:12.845243 140522396051200 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.0387468338012695, loss=1.7908326387405396
I0229 09:39:46.645229 140522404443904 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.7799103260040283, loss=1.6043891906738281
I0229 09:40:20.481511 140522396051200 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.7474455833435059, loss=1.6077076196670532
I0229 09:40:54.277498 140522404443904 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7341042757034302, loss=1.6271780729293823
I0229 09:41:28.102412 140522396051200 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.0209500789642334, loss=1.7636603116989136
I0229 09:42:01.923732 140522404443904 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7096402645111084, loss=1.5739901065826416
I0229 09:42:17.259790 140688601454400 spec.py:321] Evaluating on the training split.
I0229 09:42:23.930874 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 09:42:39.957644 140688601454400 spec.py:349] Evaluating on the test split.
I0229 09:42:42.192210 140688601454400 submission_runner.py:411] Time since start: 14957.26s, 	Step: 42247, 	{'train/accuracy': 0.6972257494926453, 'train/loss': 1.1852744817733765, 'validation/accuracy': 0.6369199752807617, 'validation/loss': 1.5088731050491333, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.202585220336914, 'test/num_examples': 10000, 'score': 14333.007185459137, 'total_duration': 14957.263036727905, 'accumulated_submission_time': 14333.007185459137, 'accumulated_eval_time': 621.6768667697906, 'accumulated_logging_time': 1.1569111347198486}
I0229 09:42:42.217711 140525256554240 logging_writer.py:48] [42247] accumulated_eval_time=621.676867, accumulated_logging_time=1.156911, accumulated_submission_time=14333.007185, global_step=42247, preemption_count=0, score=14333.007185, test/accuracy=0.515500, test/loss=2.202585, test/num_examples=10000, total_duration=14957.263037, train/accuracy=0.697226, train/loss=1.185274, validation/accuracy=0.636920, validation/loss=1.508873, validation/num_examples=50000
I0229 09:43:00.409150 140525273339648 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.8359053134918213, loss=1.620011568069458
I0229 09:43:34.124573 140525256554240 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.7703745365142822, loss=1.56901216506958
I0229 09:44:07.896563 140525273339648 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.8951183557510376, loss=1.6245492696762085
I0229 09:44:41.679213 140525256554240 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.7634508609771729, loss=1.7562758922576904
I0229 09:45:15.498550 140525273339648 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.8732830286026, loss=1.5441826581954956
I0229 09:45:49.326184 140525256554240 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.8044264316558838, loss=1.5842702388763428
I0229 09:46:23.154912 140525273339648 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.706332802772522, loss=1.6424921751022339
I0229 09:46:56.954447 140525256554240 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.2994773387908936, loss=1.6299282312393188
I0229 09:47:30.747306 140525273339648 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8221021890640259, loss=1.5985543727874756
I0229 09:48:04.550154 140525256554240 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.9720418453216553, loss=1.7106034755706787
I0229 09:48:38.344471 140525273339648 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.8378043174743652, loss=1.6642457246780396
I0229 09:49:12.168195 140525256554240 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.832517385482788, loss=1.691440463066101
I0229 09:49:46.008883 140525273339648 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7878154516220093, loss=1.655591607093811
I0229 09:50:19.863793 140525256554240 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.7771066427230835, loss=1.6503183841705322
I0229 09:50:53.662666 140525273339648 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.9715005159378052, loss=1.7482424974441528
I0229 09:51:12.368710 140688601454400 spec.py:321] Evaluating on the training split.
I0229 09:51:19.005975 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 09:51:29.176430 140688601454400 spec.py:349] Evaluating on the test split.
I0229 09:51:31.417646 140688601454400 submission_runner.py:411] Time since start: 15486.49s, 	Step: 43757, 	{'train/accuracy': 0.6907684803009033, 'train/loss': 1.2137516736984253, 'validation/accuracy': 0.6337599754333496, 'validation/loss': 1.5132801532745361, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.261016845703125, 'test/num_examples': 10000, 'score': 14843.097437620163, 'total_duration': 15486.488491773605, 'accumulated_submission_time': 14843.097437620163, 'accumulated_eval_time': 640.7257559299469, 'accumulated_logging_time': 1.1918201446533203}
I0229 09:51:31.445256 140522396051200 logging_writer.py:48] [43757] accumulated_eval_time=640.725756, accumulated_logging_time=1.191820, accumulated_submission_time=14843.097438, global_step=43757, preemption_count=0, score=14843.097438, test/accuracy=0.504700, test/loss=2.261017, test/num_examples=10000, total_duration=15486.488492, train/accuracy=0.690768, train/loss=1.213752, validation/accuracy=0.633760, validation/loss=1.513280, validation/num_examples=50000
I0229 09:51:46.286644 140522404443904 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.76767897605896, loss=1.6503019332885742
I0229 09:52:20.018858 140522396051200 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7426916360855103, loss=1.6642918586730957
I0229 09:52:53.831993 140522404443904 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8399348258972168, loss=1.672748327255249
I0229 09:53:27.611315 140522396051200 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6037471294403076, loss=1.6726953983306885
I0229 09:54:01.449522 140522404443904 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7753708362579346, loss=1.6513315439224243
I0229 09:54:35.264110 140522396051200 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8071653842926025, loss=1.5543988943099976
I0229 09:55:09.100072 140522404443904 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.764487385749817, loss=1.648026704788208
I0229 09:55:42.886160 140522396051200 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.0887258052825928, loss=1.7651376724243164
I0229 09:56:16.686498 140522404443904 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6473517417907715, loss=1.6513659954071045
I0229 09:56:50.473684 140522396051200 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6952775716781616, loss=1.5500978231430054
I0229 09:57:24.254565 140522404443904 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.9939967393875122, loss=1.5654618740081787
I0229 09:57:58.065287 140522396051200 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7799861431121826, loss=1.7386746406555176
I0229 09:58:31.845470 140522404443904 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.7782560586929321, loss=1.628456473350525
I0229 09:59:05.695442 140522396051200 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.2300994396209717, loss=1.674019455909729
I0229 09:59:39.478232 140522404443904 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.9787797927856445, loss=1.6839418411254883
I0229 10:00:01.625863 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:00:08.244746 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:00:17.098786 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:00:19.364595 140688601454400 submission_runner.py:411] Time since start: 16014.44s, 	Step: 45267, 	{'train/accuracy': 0.7050183415412903, 'train/loss': 1.1576184034347534, 'validation/accuracy': 0.6333799958229065, 'validation/loss': 1.5275466442108154, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.2523016929626465, 'test/num_examples': 10000, 'score': 15353.215401887894, 'total_duration': 16014.435420036316, 'accumulated_submission_time': 15353.215401887894, 'accumulated_eval_time': 658.4644737243652, 'accumulated_logging_time': 1.2304582595825195}
I0229 10:00:19.395168 140525264946944 logging_writer.py:48] [45267] accumulated_eval_time=658.464474, accumulated_logging_time=1.230458, accumulated_submission_time=15353.215402, global_step=45267, preemption_count=0, score=15353.215402, test/accuracy=0.508100, test/loss=2.252302, test/num_examples=10000, total_duration=16014.435420, train/accuracy=0.705018, train/loss=1.157618, validation/accuracy=0.633380, validation/loss=1.527547, validation/num_examples=50000
I0229 10:00:30.852085 140525273339648 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.6280663013458252, loss=1.6528280973434448
I0229 10:01:04.585146 140525264946944 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.1077616214752197, loss=1.7196353673934937
I0229 10:01:38.357876 140525273339648 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.933989405632019, loss=1.6904690265655518
I0229 10:02:12.134227 140525264946944 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.748884916305542, loss=1.6377222537994385
I0229 10:02:45.920152 140525273339648 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.7505854368209839, loss=1.6294718980789185
I0229 10:03:19.694160 140525264946944 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.766071081161499, loss=1.5507676601409912
I0229 10:03:53.501840 140525273339648 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.8751208782196045, loss=1.6642335653305054
I0229 10:04:27.287765 140525264946944 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7822208404541016, loss=1.6792857646942139
I0229 10:05:01.151119 140525273339648 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8726415634155273, loss=1.7238678932189941
I0229 10:05:34.969107 140525264946944 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.864432454109192, loss=1.7253413200378418
I0229 10:06:08.775454 140525273339648 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.9225053787231445, loss=1.6909372806549072
I0229 10:06:42.572607 140525264946944 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.8587076663970947, loss=1.7631397247314453
I0229 10:07:16.381637 140525273339648 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.6629917621612549, loss=1.6650583744049072
I0229 10:07:50.187026 140525264946944 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.0064597129821777, loss=1.610494613647461
I0229 10:08:23.957816 140525273339648 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.9163131713867188, loss=1.6798779964447021
I0229 10:08:49.437743 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:08:56.182211 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:09:04.956472 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:09:07.279268 140688601454400 submission_runner.py:411] Time since start: 16542.35s, 	Step: 46777, 	{'train/accuracy': 0.7170957922935486, 'train/loss': 1.097066044807434, 'validation/accuracy': 0.6341800093650818, 'validation/loss': 1.5051928758621216, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.2467596530914307, 'test/num_examples': 10000, 'score': 15863.195637464523, 'total_duration': 16542.35011291504, 'accumulated_submission_time': 15863.195637464523, 'accumulated_eval_time': 676.3059678077698, 'accumulated_logging_time': 1.2712736129760742}
I0229 10:09:07.304708 140523092305664 logging_writer.py:48] [46777] accumulated_eval_time=676.305968, accumulated_logging_time=1.271274, accumulated_submission_time=15863.195637, global_step=46777, preemption_count=0, score=15863.195637, test/accuracy=0.508400, test/loss=2.246760, test/num_examples=10000, total_duration=16542.350113, train/accuracy=0.717096, train/loss=1.097066, validation/accuracy=0.634180, validation/loss=1.505193, validation/num_examples=50000
I0229 10:09:15.431675 140523947947776 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.9177414178848267, loss=1.5928043127059937
I0229 10:09:49.163726 140523092305664 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8302353620529175, loss=1.5836000442504883
I0229 10:10:22.891723 140523947947776 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.6472313404083252, loss=1.6538918018341064
I0229 10:10:56.707792 140523092305664 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.8240206241607666, loss=1.7086702585220337
I0229 10:11:30.578132 140523947947776 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.926978588104248, loss=1.5679717063903809
I0229 10:12:04.384083 140523092305664 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8781617879867554, loss=1.759917974472046
I0229 10:12:38.193646 140523947947776 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.786617636680603, loss=1.529645562171936
I0229 10:13:12.006510 140523092305664 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.855101227760315, loss=1.5492206811904907
I0229 10:13:45.793102 140523947947776 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.880226969718933, loss=1.6733806133270264
I0229 10:14:19.595213 140523092305664 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.2155284881591797, loss=1.707184910774231
I0229 10:14:53.397020 140523947947776 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.8205879926681519, loss=1.4848309755325317
I0229 10:15:27.210735 140523092305664 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.9349620342254639, loss=1.5896403789520264
I0229 10:16:00.993703 140523947947776 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.9575833082199097, loss=1.6933096647262573
I0229 10:16:34.812187 140523092305664 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.6750223636627197, loss=1.587461233139038
I0229 10:17:08.746592 140523947947776 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.8584492206573486, loss=1.6755151748657227
I0229 10:17:37.574725 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:17:44.086669 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:17:52.999546 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:17:55.262791 140688601454400 submission_runner.py:411] Time since start: 17070.33s, 	Step: 48287, 	{'train/accuracy': 0.6975845098495483, 'train/loss': 1.179138422012329, 'validation/accuracy': 0.620199978351593, 'validation/loss': 1.5764381885528564, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.2675833702087402, 'test/num_examples': 10000, 'score': 16373.403130292892, 'total_duration': 17070.333611249924, 'accumulated_submission_time': 16373.403130292892, 'accumulated_eval_time': 693.9939639568329, 'accumulated_logging_time': 1.3073816299438477}
I0229 10:17:55.294223 140525256554240 logging_writer.py:48] [48287] accumulated_eval_time=693.993964, accumulated_logging_time=1.307382, accumulated_submission_time=16373.403130, global_step=48287, preemption_count=0, score=16373.403130, test/accuracy=0.508000, test/loss=2.267583, test/num_examples=10000, total_duration=17070.333611, train/accuracy=0.697585, train/loss=1.179138, validation/accuracy=0.620200, validation/loss=1.576438, validation/num_examples=50000
I0229 10:18:00.041118 140525264946944 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.7522536516189575, loss=1.551068663597107
I0229 10:18:33.813987 140525256554240 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.7344123125076294, loss=1.5777846574783325
I0229 10:19:07.565403 140525264946944 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.8531383275985718, loss=1.6209688186645508
I0229 10:19:41.302643 140525256554240 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8644740581512451, loss=1.5822272300720215
I0229 10:20:15.086382 140525264946944 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.7320740222930908, loss=1.5389620065689087
I0229 10:20:48.866376 140525256554240 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.853548288345337, loss=1.6375555992126465
I0229 10:21:22.635763 140525264946944 logging_writer.py:48] [48900] global_step=48900, grad_norm=2.0752413272857666, loss=1.622241735458374
I0229 10:21:56.430411 140525256554240 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.8821125030517578, loss=1.627038598060608
I0229 10:22:30.221673 140525264946944 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.0405008792877197, loss=1.6071254014968872
I0229 10:23:04.010612 140525256554240 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.7552289962768555, loss=1.6294715404510498
I0229 10:23:37.899052 140525264946944 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.5917723178863525, loss=1.591905117034912
I0229 10:24:11.666101 140525256554240 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.0201644897460938, loss=1.597806453704834
I0229 10:24:45.478892 140525264946944 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7403687238693237, loss=1.6250426769256592
I0229 10:25:19.236934 140525256554240 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.9136873483657837, loss=1.5786267518997192
I0229 10:25:53.014277 140525264946944 logging_writer.py:48] [49700] global_step=49700, grad_norm=2.040245532989502, loss=1.642909288406372
I0229 10:26:25.558197 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:26:32.004451 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:26:42.775507 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:26:45.038967 140688601454400 submission_runner.py:411] Time since start: 17600.11s, 	Step: 49798, 	{'train/accuracy': 0.7109972834587097, 'train/loss': 1.1291890144348145, 'validation/accuracy': 0.6435399651527405, 'validation/loss': 1.4746588468551636, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.1758978366851807, 'test/num_examples': 10000, 'score': 16883.60203051567, 'total_duration': 17600.109800100327, 'accumulated_submission_time': 16883.60203051567, 'accumulated_eval_time': 713.4746758937836, 'accumulated_logging_time': 1.3526594638824463}
I0229 10:26:45.063803 140522396051200 logging_writer.py:48] [49798] accumulated_eval_time=713.474676, accumulated_logging_time=1.352659, accumulated_submission_time=16883.602031, global_step=49798, preemption_count=0, score=16883.602031, test/accuracy=0.519700, test/loss=2.175898, test/num_examples=10000, total_duration=17600.109800, train/accuracy=0.710997, train/loss=1.129189, validation/accuracy=0.643540, validation/loss=1.474659, validation/num_examples=50000
I0229 10:26:46.117177 140522404443904 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8674659729003906, loss=1.569868803024292
I0229 10:27:19.862106 140522396051200 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9514861106872559, loss=1.5703468322753906
I0229 10:27:53.583996 140522404443904 logging_writer.py:48] [50000] global_step=50000, grad_norm=2.020860195159912, loss=1.7149977684020996
I0229 10:28:27.346658 140522396051200 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.9140751361846924, loss=1.5378007888793945
I0229 10:29:01.112272 140522404443904 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.139538288116455, loss=1.7713537216186523
I0229 10:29:34.983258 140522396051200 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.972670555114746, loss=1.6620042324066162
I0229 10:30:08.776386 140522404443904 logging_writer.py:48] [50400] global_step=50400, grad_norm=2.106313467025757, loss=1.4781559705734253
I0229 10:30:42.574826 140522396051200 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.838607668876648, loss=1.6012312173843384
I0229 10:31:16.363897 140522404443904 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.9725418090820312, loss=1.6678386926651
I0229 10:31:50.160133 140522396051200 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.0254788398742676, loss=1.5735459327697754
I0229 10:32:24.001538 140522404443904 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8203719854354858, loss=1.5673255920410156
I0229 10:32:57.809910 140522396051200 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8827873468399048, loss=1.6001975536346436
I0229 10:33:31.615755 140522404443904 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.959126591682434, loss=1.732017159461975
I0229 10:34:05.401275 140522396051200 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8483601808547974, loss=1.5887072086334229
I0229 10:34:39.188602 140522404443904 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7408051490783691, loss=1.4824268817901611
I0229 10:35:12.993043 140522396051200 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.6900063753128052, loss=1.7468812465667725
I0229 10:35:15.158558 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:35:21.464982 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:35:31.611389 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:35:33.877005 140688601454400 submission_runner.py:411] Time since start: 18128.95s, 	Step: 51308, 	{'train/accuracy': 0.7145049571990967, 'train/loss': 1.1121189594268799, 'validation/accuracy': 0.6514599919319153, 'validation/loss': 1.4398891925811768, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.146376132965088, 'test/num_examples': 10000, 'score': 17393.63419032097, 'total_duration': 18128.947729110718, 'accumulated_submission_time': 17393.63419032097, 'accumulated_eval_time': 732.192950963974, 'accumulated_logging_time': 1.389472246170044}
I0229 10:35:33.905964 140525264946944 logging_writer.py:48] [51308] accumulated_eval_time=732.192951, accumulated_logging_time=1.389472, accumulated_submission_time=17393.634190, global_step=51308, preemption_count=0, score=17393.634190, test/accuracy=0.521700, test/loss=2.146376, test/num_examples=10000, total_duration=18128.947729, train/accuracy=0.714505, train/loss=1.112119, validation/accuracy=0.651460, validation/loss=1.439889, validation/num_examples=50000
I0229 10:36:05.317607 140525273339648 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.029097080230713, loss=1.69655442237854
I0229 10:36:39.087781 140525264946944 logging_writer.py:48] [51500] global_step=51500, grad_norm=2.001237392425537, loss=1.7095651626586914
I0229 10:37:12.871320 140525273339648 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.791329264640808, loss=1.7146962881088257
I0229 10:37:46.603239 140525264946944 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8268043994903564, loss=1.5708904266357422
I0229 10:38:20.422469 140525273339648 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8112545013427734, loss=1.649284839630127
I0229 10:38:54.186149 140525264946944 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.8920940160751343, loss=1.6933881044387817
I0229 10:39:27.969067 140525273339648 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.715596318244934, loss=1.473367691040039
I0229 10:40:01.747190 140525264946944 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.0468485355377197, loss=1.6131671667099
I0229 10:40:35.539974 140525273339648 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8273378610610962, loss=1.6661747694015503
I0229 10:41:09.333586 140525264946944 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.187915563583374, loss=1.6336655616760254
I0229 10:41:43.133429 140525273339648 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.9266233444213867, loss=1.7687065601348877
I0229 10:42:16.995930 140525264946944 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8418142795562744, loss=1.5571297407150269
I0229 10:42:50.786506 140525273339648 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.857899785041809, loss=1.6408802270889282
I0229 10:43:24.594774 140525264946944 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8745169639587402, loss=1.5770057439804077
I0229 10:43:58.382941 140525273339648 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.7886807918548584, loss=1.4632524251937866
I0229 10:44:03.916884 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:44:10.186269 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:44:20.453250 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:44:22.696321 140688601454400 submission_runner.py:411] Time since start: 18657.77s, 	Step: 52818, 	{'train/accuracy': 0.7062539458274841, 'train/loss': 1.140507698059082, 'validation/accuracy': 0.6458199620246887, 'validation/loss': 1.4497957229614258, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.1687963008880615, 'test/num_examples': 10000, 'score': 17903.583858013153, 'total_duration': 18657.767151117325, 'accumulated_submission_time': 17903.583858013153, 'accumulated_eval_time': 750.9723279476166, 'accumulated_logging_time': 1.429046869277954}
I0229 10:44:22.728296 140522387658496 logging_writer.py:48] [52818] accumulated_eval_time=750.972328, accumulated_logging_time=1.429047, accumulated_submission_time=17903.583858, global_step=52818, preemption_count=0, score=17903.583858, test/accuracy=0.520000, test/loss=2.168796, test/num_examples=10000, total_duration=18657.767151, train/accuracy=0.706254, train/loss=1.140508, validation/accuracy=0.645820, validation/loss=1.449796, validation/num_examples=50000
I0229 10:44:50.730625 140522396051200 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.0166611671447754, loss=1.7296926975250244
I0229 10:45:24.500676 140522387658496 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.0916075706481934, loss=1.567132592201233
I0229 10:45:58.253932 140522396051200 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.008531332015991, loss=1.5809444189071655
I0229 10:46:32.017676 140522387658496 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.1254351139068604, loss=1.7250148057937622
I0229 10:47:05.832603 140522396051200 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.7399822473526, loss=1.5835334062576294
I0229 10:47:39.615567 140522387658496 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.9592039585113525, loss=1.4955179691314697
I0229 10:48:13.532163 140522396051200 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.8987181186676025, loss=1.5709693431854248
I0229 10:48:47.280477 140522387658496 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.873699426651001, loss=1.5818572044372559
I0229 10:49:21.054508 140522396051200 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.8254510164260864, loss=1.6188523769378662
I0229 10:49:54.854301 140522387658496 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.766291856765747, loss=1.6169644594192505
I0229 10:50:28.600728 140522396051200 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.7511659860610962, loss=1.5058708190917969
I0229 10:51:02.341952 140522387658496 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.9001542329788208, loss=1.5991051197052002
I0229 10:51:36.114956 140522396051200 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.955845832824707, loss=1.6977201700210571
I0229 10:52:09.904217 140522387658496 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.1205122470855713, loss=1.6661148071289062
I0229 10:52:43.696728 140522396051200 logging_writer.py:48] [54300] global_step=54300, grad_norm=2.306546926498413, loss=1.5701467990875244
I0229 10:52:52.958049 140688601454400 spec.py:321] Evaluating on the training split.
I0229 10:52:59.132259 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 10:53:08.912281 140688601454400 spec.py:349] Evaluating on the test split.
I0229 10:53:11.144803 140688601454400 submission_runner.py:411] Time since start: 19186.22s, 	Step: 54329, 	{'train/accuracy': 0.7234932780265808, 'train/loss': 1.0733474493026733, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.484086513519287, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.193343162536621, 'test/num_examples': 10000, 'score': 18413.752345323563, 'total_duration': 19186.21563768387, 'accumulated_submission_time': 18413.752345323563, 'accumulated_eval_time': 769.1590249538422, 'accumulated_logging_time': 1.4705872535705566}
I0229 10:53:11.172219 140523947947776 logging_writer.py:48] [54329] accumulated_eval_time=769.159025, accumulated_logging_time=1.470587, accumulated_submission_time=18413.752345, global_step=54329, preemption_count=0, score=18413.752345, test/accuracy=0.512900, test/loss=2.193343, test/num_examples=10000, total_duration=19186.215638, train/accuracy=0.723493, train/loss=1.073347, validation/accuracy=0.642140, validation/loss=1.484087, validation/num_examples=50000
I0229 10:53:35.472967 140525256554240 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8150115013122559, loss=1.6615062952041626
I0229 10:54:09.319200 140523947947776 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.8144216537475586, loss=1.5639227628707886
I0229 10:54:43.096766 140525256554240 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.866110920906067, loss=1.6244902610778809
I0229 10:55:16.866828 140523947947776 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.8047577142715454, loss=1.6060136556625366
I0229 10:55:50.637546 140525256554240 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.9036023616790771, loss=1.6202446222305298
I0229 10:56:24.409044 140523947947776 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.8778878450393677, loss=1.4801384210586548
I0229 10:56:58.187573 140525256554240 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7857885360717773, loss=1.5189162492752075
I0229 10:57:31.975338 140523947947776 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8292515277862549, loss=1.5609240531921387
I0229 10:58:05.735776 140525256554240 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.8711826801300049, loss=1.5597069263458252
I0229 10:58:39.521586 140523947947776 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.824424147605896, loss=1.6358518600463867
I0229 10:59:13.329188 140525256554240 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.941461443901062, loss=1.707342267036438
I0229 10:59:47.141473 140523947947776 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.7432324886322021, loss=1.5671942234039307
I0229 11:00:21.002322 140525256554240 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.8221620321273804, loss=1.6688398122787476
I0229 11:00:54.801488 140523947947776 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.6639407873153687, loss=1.4979032278060913
I0229 11:01:28.611295 140525256554240 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.9886054992675781, loss=1.4395251274108887
I0229 11:01:41.264857 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:01:47.512871 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:01:57.421344 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:01:59.739617 140688601454400 submission_runner.py:411] Time since start: 19714.81s, 	Step: 55839, 	{'train/accuracy': 0.7308075428009033, 'train/loss': 1.0367093086242676, 'validation/accuracy': 0.6415799856185913, 'validation/loss': 1.4657151699066162, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.189545154571533, 'test/num_examples': 10000, 'score': 18923.7833173275, 'total_duration': 19714.81046271324, 'accumulated_submission_time': 18923.7833173275, 'accumulated_eval_time': 787.6337478160858, 'accumulated_logging_time': 1.508671760559082}
I0229 11:01:59.765789 140522387658496 logging_writer.py:48] [55839] accumulated_eval_time=787.633748, accumulated_logging_time=1.508672, accumulated_submission_time=18923.783317, global_step=55839, preemption_count=0, score=18923.783317, test/accuracy=0.517300, test/loss=2.189545, test/num_examples=10000, total_duration=19714.810463, train/accuracy=0.730808, train/loss=1.036709, validation/accuracy=0.641580, validation/loss=1.465715, validation/num_examples=50000
I0229 11:02:20.705863 140523092305664 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.726065754890442, loss=1.64314866065979
I0229 11:02:54.394181 140522387658496 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.1284077167510986, loss=1.6811962127685547
I0229 11:03:28.215255 140523092305664 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.9604606628417969, loss=1.6173101663589478
I0229 11:04:01.997467 140522387658496 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.6923041343688965, loss=1.430448293685913
I0229 11:04:35.809448 140523092305664 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8059443235397339, loss=1.5837090015411377
I0229 11:05:09.593682 140522387658496 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.085691213607788, loss=1.6368334293365479
I0229 11:05:43.408921 140523092305664 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.9888755083084106, loss=1.6023591756820679
I0229 11:06:17.203510 140522387658496 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9742833375930786, loss=1.5418167114257812
I0229 11:06:51.083429 140523092305664 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.8844094276428223, loss=1.6618516445159912
I0229 11:07:24.859728 140522387658496 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8856773376464844, loss=1.5555989742279053
I0229 11:07:58.579354 140523092305664 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.890472173690796, loss=1.5872504711151123
I0229 11:08:32.358327 140522387658496 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.09623646736145, loss=1.7013226747512817
I0229 11:09:06.189315 140523092305664 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.0828912258148193, loss=1.601000428199768
I0229 11:09:40.002441 140522387658496 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.9658774137496948, loss=1.4825034141540527
I0229 11:10:13.822669 140523092305664 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.097355365753174, loss=1.5891985893249512
I0229 11:10:29.871354 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:10:36.025855 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:10:45.208913 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:10:47.446372 140688601454400 submission_runner.py:411] Time since start: 20242.52s, 	Step: 57349, 	{'train/accuracy': 0.7161989808082581, 'train/loss': 1.0922472476959229, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.4972940683364868, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.2348709106445312, 'test/num_examples': 10000, 'score': 19433.826995134354, 'total_duration': 20242.517218351364, 'accumulated_submission_time': 19433.826995134354, 'accumulated_eval_time': 805.2087225914001, 'accumulated_logging_time': 1.5456082820892334}
I0229 11:10:47.476667 140525273339648 logging_writer.py:48] [57349] accumulated_eval_time=805.208723, accumulated_logging_time=1.545608, accumulated_submission_time=19433.826995, global_step=57349, preemption_count=0, score=19433.826995, test/accuracy=0.511100, test/loss=2.234871, test/num_examples=10000, total_duration=20242.517218, train/accuracy=0.716199, train/loss=1.092247, validation/accuracy=0.641300, validation/loss=1.497294, validation/num_examples=50000
I0229 11:11:05.046845 140525281732352 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9912564754486084, loss=1.5848989486694336
I0229 11:11:38.782595 140525273339648 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8754185438156128, loss=1.6358648538589478
I0229 11:12:12.562284 140525281732352 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9143438339233398, loss=1.6096805334091187
I0229 11:12:46.469912 140525273339648 logging_writer.py:48] [57700] global_step=57700, grad_norm=2.0091147422790527, loss=1.669338345527649
I0229 11:13:20.273309 140525281732352 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.7577831745147705, loss=1.4859540462493896
I0229 11:13:54.057311 140525273339648 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.7743964195251465, loss=1.4912075996398926
I0229 11:14:27.823442 140525281732352 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.97459077835083, loss=1.4811561107635498
I0229 11:15:01.617599 140525273339648 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9954556226730347, loss=1.637525200843811
I0229 11:15:35.443949 140525281732352 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.080035924911499, loss=1.6169681549072266
I0229 11:16:09.238035 140525273339648 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8784657716751099, loss=1.5804638862609863
I0229 11:16:43.027626 140525281732352 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9815055131912231, loss=1.5466291904449463
I0229 11:17:16.837690 140525273339648 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8750450611114502, loss=1.5690317153930664
I0229 11:17:50.625996 140525281732352 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9047738313674927, loss=1.515368938446045
I0229 11:18:24.438208 140525273339648 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.792839765548706, loss=1.4865728616714478
I0229 11:18:58.299429 140525281732352 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.9882019758224487, loss=1.6797797679901123
I0229 11:19:17.680067 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:19:23.862329 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:19:33.273812 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:19:35.584663 140688601454400 submission_runner.py:411] Time since start: 20770.66s, 	Step: 58859, 	{'train/accuracy': 0.7106584906578064, 'train/loss': 1.1283915042877197, 'validation/accuracy': 0.6424799561500549, 'validation/loss': 1.4816455841064453, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.208247423171997, 'test/num_examples': 10000, 'score': 19943.966212511063, 'total_duration': 20770.65550327301, 'accumulated_submission_time': 19943.966212511063, 'accumulated_eval_time': 823.1132650375366, 'accumulated_logging_time': 1.5886414051055908}
I0229 11:19:35.612408 140522387658496 logging_writer.py:48] [58859] accumulated_eval_time=823.113265, accumulated_logging_time=1.588641, accumulated_submission_time=19943.966213, global_step=58859, preemption_count=0, score=19943.966213, test/accuracy=0.523200, test/loss=2.208247, test/num_examples=10000, total_duration=20770.655503, train/accuracy=0.710658, train/loss=1.128392, validation/accuracy=0.642480, validation/loss=1.481646, validation/num_examples=50000
I0229 11:19:49.772775 140522396051200 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.872316598892212, loss=1.5870318412780762
I0229 11:20:23.565651 140522387658496 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9848750829696655, loss=1.5487329959869385
I0229 11:20:57.321241 140522396051200 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.9270784854888916, loss=1.5968338251113892
I0229 11:21:31.097961 140522387658496 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.7840927839279175, loss=1.5511764287948608
I0229 11:22:04.859916 140522396051200 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8724325895309448, loss=1.5791343450546265
I0229 11:22:38.611412 140522387658496 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.812445044517517, loss=1.523905634880066
I0229 11:23:12.381638 140522396051200 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.7786829471588135, loss=1.504479169845581
I0229 11:23:46.178590 140522387658496 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.9293321371078491, loss=1.5186158418655396
I0229 11:24:19.958806 140522396051200 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9091639518737793, loss=1.5815868377685547
I0229 11:24:53.821844 140522387658496 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.169506549835205, loss=1.5691051483154297
I0229 11:25:27.585883 140522396051200 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.8099929094314575, loss=1.5302037000656128
I0229 11:26:01.411724 140522387658496 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.8522599935531616, loss=1.5190409421920776
I0229 11:26:35.203682 140522396051200 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.825505018234253, loss=1.4836421012878418
I0229 11:27:09.022341 140522387658496 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.9296053647994995, loss=1.643924355506897
I0229 11:27:42.788384 140522396051200 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.113171100616455, loss=1.5587167739868164
I0229 11:28:05.595239 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:28:11.787942 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:28:20.843596 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:28:23.085880 140688601454400 submission_runner.py:411] Time since start: 21298.16s, 	Step: 60369, 	{'train/accuracy': 0.7179926633834839, 'train/loss': 1.0955880880355835, 'validation/accuracy': 0.6552000045776367, 'validation/loss': 1.42160165309906, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.127664804458618, 'test/num_examples': 10000, 'score': 20453.887604236603, 'total_duration': 21298.15672469139, 'accumulated_submission_time': 20453.887604236603, 'accumulated_eval_time': 840.6038625240326, 'accumulated_logging_time': 1.6262004375457764}
I0229 11:28:23.116920 140522387658496 logging_writer.py:48] [60369] accumulated_eval_time=840.603863, accumulated_logging_time=1.626200, accumulated_submission_time=20453.887604, global_step=60369, preemption_count=0, score=20453.887604, test/accuracy=0.524300, test/loss=2.127665, test/num_examples=10000, total_duration=21298.156725, train/accuracy=0.717993, train/loss=1.095588, validation/accuracy=0.655200, validation/loss=1.421602, validation/num_examples=50000
I0229 11:28:33.895101 140522396051200 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8786022663116455, loss=1.467725396156311
I0229 11:29:07.623624 140522387658496 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.897521734237671, loss=1.5061006546020508
I0229 11:29:41.390585 140522396051200 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.8215007781982422, loss=1.486106038093567
I0229 11:30:15.176188 140522387658496 logging_writer.py:48] [60700] global_step=60700, grad_norm=2.0469167232513428, loss=1.6182782649993896
I0229 11:30:48.938831 140522396051200 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.7658824920654297, loss=1.6193205118179321
I0229 11:31:22.756180 140522387658496 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9636255502700806, loss=1.548627495765686
I0229 11:31:56.542498 140522396051200 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.9737176895141602, loss=1.6156604290008545
I0229 11:32:30.358187 140522387658496 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.8781352043151855, loss=1.6274055242538452
I0229 11:33:04.137745 140522396051200 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.000319242477417, loss=1.5213671922683716
I0229 11:33:37.942413 140522387658496 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9203330278396606, loss=1.665055274963379
I0229 11:34:11.737954 140522396051200 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8019345998764038, loss=1.5464590787887573
I0229 11:34:45.510055 140522387658496 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9485424757003784, loss=1.5615955591201782
I0229 11:35:19.312500 140522396051200 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.0453686714172363, loss=1.503460168838501
I0229 11:35:53.073456 140522387658496 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.003582000732422, loss=1.5459566116333008
I0229 11:36:26.888924 140522396051200 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.0508196353912354, loss=1.528115153312683
I0229 11:36:53.383789 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:36:59.759725 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:37:08.929532 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:37:11.168323 140688601454400 submission_runner.py:411] Time since start: 21826.24s, 	Step: 61880, 	{'train/accuracy': 0.7214205861091614, 'train/loss': 1.0676918029785156, 'validation/accuracy': 0.6552199721336365, 'validation/loss': 1.4101110696792603, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1190402507781982, 'test/num_examples': 10000, 'score': 20964.092700004578, 'total_duration': 21826.239169597626, 'accumulated_submission_time': 20964.092700004578, 'accumulated_eval_time': 858.3883543014526, 'accumulated_logging_time': 1.667083978652954}
I0229 11:37:11.195859 140522387658496 logging_writer.py:48] [61880] accumulated_eval_time=858.388354, accumulated_logging_time=1.667084, accumulated_submission_time=20964.092700, global_step=61880, preemption_count=0, score=20964.092700, test/accuracy=0.528800, test/loss=2.119040, test/num_examples=10000, total_duration=21826.239170, train/accuracy=0.721421, train/loss=1.067692, validation/accuracy=0.655220, validation/loss=1.410111, validation/num_examples=50000
I0229 11:37:18.278207 140522396051200 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.8793519735336304, loss=1.6522157192230225
I0229 11:37:52.023630 140522387658496 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.8614015579223633, loss=1.6173899173736572
I0229 11:38:25.775836 140522396051200 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.836176872253418, loss=1.5133781433105469
I0229 11:38:59.549893 140522387658496 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.1651060581207275, loss=1.6112487316131592
I0229 11:39:33.325668 140522396051200 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.9678740501403809, loss=1.6690818071365356
I0229 11:40:07.081785 140522387658496 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.2228751182556152, loss=1.482945203781128
I0229 11:40:40.837506 140522396051200 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.760079026222229, loss=1.5645456314086914
I0229 11:41:14.599015 140522387658496 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9809846878051758, loss=1.6152251958847046
I0229 11:41:48.372525 140522396051200 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.894034504890442, loss=1.5135879516601562
I0229 11:42:22.138899 140522387658496 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.878112554550171, loss=1.5601248741149902
I0229 11:42:55.911376 140522396051200 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8668794631958008, loss=1.5085909366607666
I0229 11:43:29.726048 140522387658496 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.7837800979614258, loss=1.5819653272628784
I0229 11:44:03.518873 140522396051200 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.6962922811508179, loss=1.409264326095581
I0229 11:44:37.299014 140522387658496 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.1912753582000732, loss=1.494330883026123
I0229 11:45:11.049873 140522396051200 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.9541527032852173, loss=1.527565360069275
I0229 11:45:41.257488 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:45:47.417382 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:45:57.329019 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:45:59.582565 140688601454400 submission_runner.py:411] Time since start: 22354.65s, 	Step: 63391, 	{'train/accuracy': 0.7526506781578064, 'train/loss': 0.9483557343482971, 'validation/accuracy': 0.6526399850845337, 'validation/loss': 1.4119130373001099, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.121713399887085, 'test/num_examples': 10000, 'score': 21474.092804193497, 'total_duration': 22354.6533973217, 'accumulated_submission_time': 21474.092804193497, 'accumulated_eval_time': 876.7133748531342, 'accumulated_logging_time': 1.7043514251708984}
I0229 11:45:59.615479 140522396051200 logging_writer.py:48] [63391] accumulated_eval_time=876.713375, accumulated_logging_time=1.704351, accumulated_submission_time=21474.092804, global_step=63391, preemption_count=0, score=21474.092804, test/accuracy=0.527900, test/loss=2.121713, test/num_examples=10000, total_duration=22354.653397, train/accuracy=0.752651, train/loss=0.948356, validation/accuracy=0.652640, validation/loss=1.411913, validation/num_examples=50000
I0229 11:46:02.998762 140525264946944 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.7381068468093872, loss=1.4727739095687866
I0229 11:46:36.702569 140522396051200 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.885961651802063, loss=1.6188595294952393
I0229 11:47:10.492155 140525264946944 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9481431245803833, loss=1.5482513904571533
I0229 11:47:44.259428 140522396051200 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.8897738456726074, loss=1.658608078956604
I0229 11:48:18.040972 140525264946944 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.802700161933899, loss=1.5079939365386963
I0229 11:48:51.819190 140522396051200 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.0206825733184814, loss=1.4973140954971313
I0229 11:49:25.653207 140525264946944 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.1678249835968018, loss=1.68829345703125
I0229 11:49:59.436028 140522396051200 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.8483188152313232, loss=1.5057066679000854
I0229 11:50:33.230595 140525264946944 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9177544116973877, loss=1.4923722743988037
I0229 11:51:06.985737 140522396051200 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.8964205980300903, loss=1.5456594228744507
I0229 11:51:40.733342 140525264946944 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8784407377243042, loss=1.5393365621566772
I0229 11:52:14.528040 140522396051200 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9844456911087036, loss=1.662853717803955
I0229 11:52:48.312067 140525264946944 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8709330558776855, loss=1.4795444011688232
I0229 11:53:22.076533 140522396051200 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.8949356079101562, loss=1.5025020837783813
I0229 11:53:55.844732 140525264946944 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.9672778844833374, loss=1.5932165384292603
I0229 11:54:29.607014 140522396051200 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9656407833099365, loss=1.4406049251556396
I0229 11:54:29.615475 140688601454400 spec.py:321] Evaluating on the training split.
I0229 11:54:35.797067 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 11:54:45.928155 140688601454400 spec.py:349] Evaluating on the test split.
I0229 11:54:48.178415 140688601454400 submission_runner.py:411] Time since start: 22883.25s, 	Step: 64901, 	{'train/accuracy': 0.7511758208274841, 'train/loss': 0.9479004144668579, 'validation/accuracy': 0.6609999537467957, 'validation/loss': 1.3837952613830566, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.108168363571167, 'test/num_examples': 10000, 'score': 21984.03058242798, 'total_duration': 22883.249258756638, 'accumulated_submission_time': 21984.03058242798, 'accumulated_eval_time': 895.2762496471405, 'accumulated_logging_time': 1.7477846145629883}
I0229 11:54:48.208400 140522404443904 logging_writer.py:48] [64901] accumulated_eval_time=895.276250, accumulated_logging_time=1.747785, accumulated_submission_time=21984.030582, global_step=64901, preemption_count=0, score=21984.030582, test/accuracy=0.536800, test/loss=2.108168, test/num_examples=10000, total_duration=22883.249259, train/accuracy=0.751176, train/loss=0.947900, validation/accuracy=0.661000, validation/loss=1.383795, validation/num_examples=50000
I0229 11:55:21.960410 140523092305664 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.8558017015457153, loss=1.5098867416381836
I0229 11:55:55.752790 140522404443904 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.8402073383331299, loss=1.57437264919281
I0229 11:56:29.494754 140523092305664 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.9755558967590332, loss=1.423975944519043
I0229 11:57:03.278138 140522404443904 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.01580548286438, loss=1.4011666774749756
I0229 11:57:37.056590 140523092305664 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.0506532192230225, loss=1.485411524772644
I0229 11:58:10.865813 140522404443904 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.202918291091919, loss=1.5358638763427734
I0229 11:58:44.634629 140523092305664 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.8371225595474243, loss=1.4790260791778564
I0229 11:59:18.390028 140522404443904 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.0995287895202637, loss=1.5725419521331787
I0229 11:59:52.171441 140523092305664 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.1346702575683594, loss=1.6185013055801392
I0229 12:00:25.928336 140522404443904 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.8843718767166138, loss=1.5535138845443726
I0229 12:00:59.691565 140523092305664 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.9641894102096558, loss=1.6345887184143066
I0229 12:01:33.466236 140522404443904 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.8886520862579346, loss=1.568118929862976
I0229 12:02:07.297384 140523092305664 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.8563258647918701, loss=1.5623514652252197
I0229 12:02:41.064708 140522404443904 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.1533236503601074, loss=1.670149803161621
I0229 12:03:14.815119 140523092305664 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9957115650177002, loss=1.3924330472946167
I0229 12:03:18.333481 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:03:24.480368 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:03:33.141984 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:03:35.399630 140688601454400 submission_runner.py:411] Time since start: 23410.47s, 	Step: 66412, 	{'train/accuracy': 0.7248086333274841, 'train/loss': 1.053501844406128, 'validation/accuracy': 0.6454199552536011, 'validation/loss': 1.4561376571655273, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.121701955795288, 'test/num_examples': 10000, 'score': 22494.092856168747, 'total_duration': 23410.470444202423, 'accumulated_submission_time': 22494.092856168747, 'accumulated_eval_time': 912.3423182964325, 'accumulated_logging_time': 1.7881202697753906}
I0229 12:03:35.430117 140522396051200 logging_writer.py:48] [66412] accumulated_eval_time=912.342318, accumulated_logging_time=1.788120, accumulated_submission_time=22494.092856, global_step=66412, preemption_count=0, score=22494.092856, test/accuracy=0.527300, test/loss=2.121702, test/num_examples=10000, total_duration=23410.470444, train/accuracy=0.724809, train/loss=1.053502, validation/accuracy=0.645420, validation/loss=1.456138, validation/num_examples=50000
I0229 12:04:05.491923 140522404443904 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.9879428148269653, loss=1.5747060775756836
I0229 12:04:39.229773 140522396051200 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.9238179922103882, loss=1.454887866973877
I0229 12:05:12.975824 140522404443904 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.9723162651062012, loss=1.5678883790969849
I0229 12:05:46.736107 140522396051200 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.8544654846191406, loss=1.5085854530334473
I0229 12:06:20.505230 140522404443904 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.9031122922897339, loss=1.4357030391693115
I0229 12:06:54.285651 140522396051200 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.9117249250411987, loss=1.4955246448516846
I0229 12:07:28.071917 140522404443904 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.903041124343872, loss=1.495206356048584
I0229 12:08:01.929405 140522396051200 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.033108711242676, loss=1.5977941751480103
I0229 12:08:35.731305 140522404443904 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9253894090652466, loss=1.5227348804473877
I0229 12:09:09.507959 140522396051200 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.9936778545379639, loss=1.5281628370285034
I0229 12:09:43.274569 140522404443904 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.167924404144287, loss=1.521440029144287
I0229 12:10:17.043216 140522396051200 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.186392307281494, loss=1.7122981548309326
I0229 12:10:50.803017 140522404443904 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9667608737945557, loss=1.5895750522613525
I0229 12:11:24.565747 140522396051200 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.1193034648895264, loss=1.5694173574447632
I0229 12:11:58.333024 140522404443904 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.9011626243591309, loss=1.4807934761047363
I0229 12:12:05.565980 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:12:11.771793 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:12:20.519395 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:12:22.784104 140688601454400 submission_runner.py:411] Time since start: 23937.85s, 	Step: 67923, 	{'train/accuracy': 0.7313655614852905, 'train/loss': 1.0242691040039062, 'validation/accuracy': 0.6623199582099915, 'validation/loss': 1.3837313652038574, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.1165854930877686, 'test/num_examples': 10000, 'score': 23004.167016267776, 'total_duration': 23937.85494709015, 'accumulated_submission_time': 23004.167016267776, 'accumulated_eval_time': 929.560394525528, 'accumulated_logging_time': 1.828833818435669}
I0229 12:12:22.812791 140522404443904 logging_writer.py:48] [67923] accumulated_eval_time=929.560395, accumulated_logging_time=1.828834, accumulated_submission_time=23004.167016, global_step=67923, preemption_count=0, score=23004.167016, test/accuracy=0.534800, test/loss=2.116585, test/num_examples=10000, total_duration=23937.854947, train/accuracy=0.731366, train/loss=1.024269, validation/accuracy=0.662320, validation/loss=1.383731, validation/num_examples=50000
I0229 12:12:49.126299 140523092305664 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.7887272834777832, loss=1.494933009147644
I0229 12:13:22.880650 140522404443904 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9969896078109741, loss=1.556949496269226
I0229 12:13:56.643277 140523092305664 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.9539721012115479, loss=1.4612499475479126
I0229 12:14:30.579614 140522404443904 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.088002920150757, loss=1.4811711311340332
I0229 12:15:04.393130 140523092305664 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.9785698652267456, loss=1.585978388786316
I0229 12:15:38.146789 140522404443904 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.0949504375457764, loss=1.5288981199264526
I0229 12:16:11.889173 140523092305664 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.0138583183288574, loss=1.5914864540100098
I0229 12:16:45.658461 140522404443904 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.891686201095581, loss=1.6002202033996582
I0229 12:17:19.470458 140523092305664 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0963611602783203, loss=1.3670742511749268
I0229 12:17:53.264909 140522404443904 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.933575987815857, loss=1.4480708837509155
I0229 12:18:27.068876 140523092305664 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.0820698738098145, loss=1.4674750566482544
I0229 12:19:00.860588 140522404443904 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.0358290672302246, loss=1.539494514465332
I0229 12:19:34.648311 140523092305664 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.914063572883606, loss=1.4488799571990967
I0229 12:20:08.539100 140522404443904 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.141458511352539, loss=1.5019075870513916
I0229 12:20:42.291595 140523092305664 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.228116273880005, loss=1.568753957748413
I0229 12:20:52.917423 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:20:59.108001 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:21:07.687794 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:21:09.933217 140688601454400 submission_runner.py:411] Time since start: 24465.00s, 	Step: 69433, 	{'train/accuracy': 0.7354910373687744, 'train/loss': 1.011197805404663, 'validation/accuracy': 0.6612199544906616, 'validation/loss': 1.3969271183013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.1048424243927, 'test/num_examples': 10000, 'score': 23514.20720410347, 'total_duration': 24465.004062891006, 'accumulated_submission_time': 23514.20720410347, 'accumulated_eval_time': 946.5761430263519, 'accumulated_logging_time': 1.868699073791504}
I0229 12:21:09.960916 140522387658496 logging_writer.py:48] [69433] accumulated_eval_time=946.576143, accumulated_logging_time=1.868699, accumulated_submission_time=23514.207204, global_step=69433, preemption_count=0, score=23514.207204, test/accuracy=0.542000, test/loss=2.104842, test/num_examples=10000, total_duration=24465.004063, train/accuracy=0.735491, train/loss=1.011198, validation/accuracy=0.661220, validation/loss=1.396927, validation/num_examples=50000
I0229 12:21:32.923172 140522396051200 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.0962796211242676, loss=1.5367662906646729
I0229 12:22:06.644106 140522387658496 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.099057674407959, loss=1.6724811792373657
I0229 12:22:40.418214 140522396051200 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.1247832775115967, loss=1.557742714881897
I0229 12:23:14.158854 140522387658496 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.9801982641220093, loss=1.470150351524353
I0229 12:23:47.950868 140522396051200 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.09769868850708, loss=1.5519860982894897
I0229 12:24:21.750185 140522387658496 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.1078453063964844, loss=1.4850155115127563
I0229 12:24:55.531535 140522396051200 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.8527454137802124, loss=1.55859375
I0229 12:25:29.315142 140522387658496 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.070626735687256, loss=1.4579098224639893
I0229 12:26:03.097071 140522396051200 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.1381642818450928, loss=1.4526618719100952
I0229 12:26:36.947056 140522387658496 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.0212154388427734, loss=1.435103416442871
I0229 12:27:10.700194 140522396051200 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.004909038543701, loss=1.5705997943878174
I0229 12:27:44.468251 140522387658496 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.1447696685791016, loss=1.5335129499435425
I0229 12:28:18.247648 140522396051200 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.9748811721801758, loss=1.5543287992477417
I0229 12:28:52.029204 140522387658496 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0471606254577637, loss=1.5026845932006836
I0229 12:29:25.819450 140522396051200 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.968991994857788, loss=1.5222748517990112
I0229 12:29:40.137044 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:29:46.386016 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:29:55.046944 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:29:57.316741 140688601454400 submission_runner.py:411] Time since start: 24992.39s, 	Step: 70944, 	{'train/accuracy': 0.7195272445678711, 'train/loss': 1.0805131196975708, 'validation/accuracy': 0.6513800024986267, 'validation/loss': 1.4209266901016235, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.148491382598877, 'test/num_examples': 10000, 'score': 24024.321031093597, 'total_duration': 24992.387575387955, 'accumulated_submission_time': 24024.321031093597, 'accumulated_eval_time': 963.7557821273804, 'accumulated_logging_time': 1.9067182540893555}
I0229 12:29:57.345643 140525264946944 logging_writer.py:48] [70944] accumulated_eval_time=963.755782, accumulated_logging_time=1.906718, accumulated_submission_time=24024.321031, global_step=70944, preemption_count=0, score=24024.321031, test/accuracy=0.526300, test/loss=2.148491, test/num_examples=10000, total_duration=24992.387575, train/accuracy=0.719527, train/loss=1.080513, validation/accuracy=0.651380, validation/loss=1.420927, validation/num_examples=50000
I0229 12:30:16.591748 140525273339648 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.1389167308807373, loss=1.603264570236206
I0229 12:30:50.335047 140525264946944 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.8832757472991943, loss=1.3925001621246338
I0229 12:31:24.102777 140525273339648 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.8872065544128418, loss=1.4277232885360718
I0229 12:31:57.866529 140525264946944 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.3053536415100098, loss=1.5171352624893188
I0229 12:32:31.697016 140525273339648 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.0921554565429688, loss=1.6431241035461426
I0229 12:33:05.482971 140525264946944 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.0793230533599854, loss=1.5384933948516846
I0229 12:33:39.239127 140525273339648 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.0155491828918457, loss=1.583492398262024
I0229 12:34:12.969381 140525264946944 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.9129663705825806, loss=1.4166984558105469
I0229 12:34:46.733417 140525273339648 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.017350673675537, loss=1.495377540588379
I0229 12:35:20.530723 140525264946944 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.978591799736023, loss=1.5301878452301025
I0229 12:35:54.309950 140525273339648 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.0902037620544434, loss=1.5237760543823242
I0229 12:36:28.055190 140525264946944 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.286257028579712, loss=1.6059244871139526
I0229 12:37:01.831191 140525273339648 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.8574339151382446, loss=1.4276819229125977
I0229 12:37:35.656657 140525264946944 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.277653694152832, loss=1.5411896705627441
I0229 12:38:09.428154 140525273339648 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.1492834091186523, loss=1.4851011037826538
I0229 12:38:27.461060 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:38:33.676234 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:38:42.376066 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:38:44.936406 140688601454400 submission_runner.py:411] Time since start: 25520.01s, 	Step: 72455, 	{'train/accuracy': 0.7664819955825806, 'train/loss': 0.8874098658561707, 'validation/accuracy': 0.6557999849319458, 'validation/loss': 1.4078059196472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.126142978668213, 'test/num_examples': 10000, 'score': 24534.373774051666, 'total_duration': 25520.007241487503, 'accumulated_submission_time': 24534.373774051666, 'accumulated_eval_time': 981.2310724258423, 'accumulated_logging_time': 1.9461076259613037}
I0229 12:38:44.967638 140522387658496 logging_writer.py:48] [72455] accumulated_eval_time=981.231072, accumulated_logging_time=1.946108, accumulated_submission_time=24534.373774, global_step=72455, preemption_count=0, score=24534.373774, test/accuracy=0.533300, test/loss=2.126143, test/num_examples=10000, total_duration=25520.007241, train/accuracy=0.766482, train/loss=0.887410, validation/accuracy=0.655800, validation/loss=1.407806, validation/num_examples=50000
I0229 12:39:00.503422 140522404443904 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1013755798339844, loss=1.5183624029159546
I0229 12:39:34.212268 140522387658496 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.05008602142334, loss=1.552506446838379
I0229 12:40:07.971978 140522404443904 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.939734935760498, loss=1.4950273036956787
I0229 12:40:41.743817 140522387658496 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.035219192504883, loss=1.5270049571990967
I0229 12:41:15.516196 140522404443904 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.0106680393218994, loss=1.582241415977478
I0229 12:41:49.312934 140522387658496 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.200481653213501, loss=1.4680566787719727
I0229 12:42:23.080559 140522404443904 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.0919594764709473, loss=1.548327922821045
I0229 12:42:56.834052 140522387658496 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.1516103744506836, loss=1.4729423522949219
I0229 12:43:30.599228 140522404443904 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.2184345722198486, loss=1.6792501211166382
I0229 12:44:04.403082 140522387658496 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.1653685569763184, loss=1.5684130191802979
I0229 12:44:38.184202 140522404443904 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.0766515731811523, loss=1.4373224973678589
I0229 12:45:12.026412 140522387658496 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.245373487472534, loss=1.4764494895935059
I0229 12:45:45.802218 140522404443904 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0777382850646973, loss=1.507501482963562
I0229 12:46:19.560017 140522387658496 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.1722793579101562, loss=1.4850143194198608
I0229 12:46:53.343071 140522404443904 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.081646680831909, loss=1.5578993558883667
I0229 12:47:15.113457 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:47:21.375370 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:47:30.252425 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:47:32.517712 140688601454400 submission_runner.py:411] Time since start: 26047.59s, 	Step: 73966, 	{'train/accuracy': 0.7506775856018066, 'train/loss': 0.9532562494277954, 'validation/accuracy': 0.6530999541282654, 'validation/loss': 1.414706826210022, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1354970932006836, 'test/num_examples': 10000, 'score': 25044.457715272903, 'total_duration': 26047.58855366707, 'accumulated_submission_time': 25044.457715272903, 'accumulated_eval_time': 998.6352922916412, 'accumulated_logging_time': 1.987243413925171}
I0229 12:47:32.549136 140523092305664 logging_writer.py:48] [73966] accumulated_eval_time=998.635292, accumulated_logging_time=1.987243, accumulated_submission_time=25044.457715, global_step=73966, preemption_count=0, score=25044.457715, test/accuracy=0.529000, test/loss=2.135497, test/num_examples=10000, total_duration=26047.588554, train/accuracy=0.750678, train/loss=0.953256, validation/accuracy=0.653100, validation/loss=1.414707, validation/num_examples=50000
I0229 12:47:44.384857 140525256554240 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0850043296813965, loss=1.6355247497558594
I0229 12:48:18.133186 140523092305664 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.9774694442749023, loss=1.4682661294937134
I0229 12:48:51.934512 140525256554240 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.060622215270996, loss=1.3902626037597656
I0229 12:49:25.723837 140523092305664 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.4140374660491943, loss=1.5468648672103882
I0229 12:49:59.536449 140525256554240 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.1095919609069824, loss=1.4764398336410522
I0229 12:50:33.336425 140523092305664 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.9193884134292603, loss=1.430008888244629
I0229 12:51:07.216068 140525256554240 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.030212163925171, loss=1.5667153596878052
I0229 12:51:40.987866 140523092305664 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.039921998977661, loss=1.3439111709594727
I0229 12:52:14.791931 140525256554240 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.9611222743988037, loss=1.5050971508026123
I0229 12:52:48.567889 140523092305664 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.9620482921600342, loss=1.37101149559021
I0229 12:53:22.374777 140525256554240 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.052793264389038, loss=1.4912691116333008
I0229 12:53:56.163392 140523092305664 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.063584089279175, loss=1.614129900932312
I0229 12:54:29.950432 140525256554240 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.150289535522461, loss=1.4673621654510498
I0229 12:55:03.737629 140523092305664 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.2619566917419434, loss=1.3790464401245117
I0229 12:55:37.522226 140525256554240 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.873091459274292, loss=1.4403318166732788
I0229 12:56:02.661890 140688601454400 spec.py:321] Evaluating on the training split.
I0229 12:56:09.413276 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 12:56:18.083156 140688601454400 spec.py:349] Evaluating on the test split.
I0229 12:56:20.404642 140688601454400 submission_runner.py:411] Time since start: 26575.48s, 	Step: 75476, 	{'train/accuracy': 0.7411909699440002, 'train/loss': 0.9859520196914673, 'validation/accuracy': 0.6597200036048889, 'validation/loss': 1.3939448595046997, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.133927345275879, 'test/num_examples': 10000, 'score': 25554.508782863617, 'total_duration': 26575.475489616394, 'accumulated_submission_time': 25554.508782863617, 'accumulated_eval_time': 1016.3779957294464, 'accumulated_logging_time': 2.0287249088287354}
I0229 12:56:20.434606 140525281732352 logging_writer.py:48] [75476] accumulated_eval_time=1016.377996, accumulated_logging_time=2.028725, accumulated_submission_time=25554.508783, global_step=75476, preemption_count=0, score=25554.508783, test/accuracy=0.532600, test/loss=2.133927, test/num_examples=10000, total_duration=26575.475490, train/accuracy=0.741191, train/loss=0.985952, validation/accuracy=0.659720, validation/loss=1.393945, validation/num_examples=50000
I0229 12:56:28.891416 140525290125056 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.209946870803833, loss=1.4963233470916748
I0229 12:57:02.578249 140525281732352 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.1883246898651123, loss=1.5029371976852417
I0229 12:57:36.378125 140525290125056 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.0055339336395264, loss=1.419782280921936
I0229 12:58:10.155179 140525281732352 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9662894010543823, loss=1.431694746017456
I0229 12:58:43.953801 140525290125056 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.1827259063720703, loss=1.4755219221115112
I0229 12:59:17.726511 140525281732352 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.130018472671509, loss=1.539080023765564
I0229 12:59:51.546073 140525290125056 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.0759708881378174, loss=1.4559993743896484
I0229 13:00:25.317419 140525281732352 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.079718589782715, loss=1.4004342555999756
I0229 13:00:59.145342 140525290125056 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.232120990753174, loss=1.4881820678710938
I0229 13:01:32.923104 140525281732352 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.9250589609146118, loss=1.4618256092071533
I0229 13:02:06.722289 140525290125056 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.1341118812561035, loss=1.4468356370925903
I0229 13:02:40.485181 140525281732352 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.1258251667022705, loss=1.5031509399414062
I0229 13:03:14.268729 140525290125056 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.1368634700775146, loss=1.4437341690063477
I0229 13:03:48.115255 140525281732352 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.350569009780884, loss=1.5026496648788452
I0229 13:04:21.920299 140525290125056 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.2901358604431152, loss=1.5019816160202026
I0229 13:04:50.453475 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:04:56.624203 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:05:05.338785 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:05:07.600143 140688601454400 submission_runner.py:411] Time since start: 27102.67s, 	Step: 76986, 	{'train/accuracy': 0.7378627061843872, 'train/loss': 0.9981990456581116, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.3914191722869873, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.1226179599761963, 'test/num_examples': 10000, 'score': 26064.464957475662, 'total_duration': 27102.67097759247, 'accumulated_submission_time': 26064.464957475662, 'accumulated_eval_time': 1033.5246062278748, 'accumulated_logging_time': 2.0691707134246826}
I0229 13:05:07.632020 140523947947776 logging_writer.py:48] [76986] accumulated_eval_time=1033.524606, accumulated_logging_time=2.069171, accumulated_submission_time=26064.464957, global_step=76986, preemption_count=0, score=26064.464957, test/accuracy=0.537500, test/loss=2.122618, test/num_examples=10000, total_duration=27102.670978, train/accuracy=0.737863, train/loss=0.998199, validation/accuracy=0.662380, validation/loss=1.391419, validation/num_examples=50000
I0229 13:05:13.620680 140525256554240 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1832756996154785, loss=1.4958574771881104
I0229 13:05:47.382313 140523947947776 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.020927906036377, loss=1.4929804801940918
I0229 13:06:21.148666 140525256554240 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.1395440101623535, loss=1.4582124948501587
I0229 13:06:54.894576 140523947947776 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.050048589706421, loss=1.3660557270050049
I0229 13:07:28.655236 140525256554240 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.0623726844787598, loss=1.5509384870529175
I0229 13:08:02.432487 140523947947776 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.033726692199707, loss=1.4949116706848145
I0229 13:08:36.185546 140525256554240 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.1944520473480225, loss=1.5712273120880127
I0229 13:09:09.974136 140523947947776 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.25449800491333, loss=1.4123138189315796
I0229 13:09:43.833652 140525256554240 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.152646064758301, loss=1.4684423208236694
I0229 13:10:17.618030 140523947947776 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.000051736831665, loss=1.4328501224517822
I0229 13:10:51.363004 140525256554240 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.141955614089966, loss=1.6007212400436401
I0229 13:11:25.144839 140523947947776 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.2411398887634277, loss=1.5519630908966064
I0229 13:11:58.900339 140525256554240 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.025808811187744, loss=1.4993573427200317
I0229 13:12:32.663283 140523947947776 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1546599864959717, loss=1.4849252700805664
I0229 13:13:06.439918 140525256554240 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.223189115524292, loss=1.4325313568115234
I0229 13:13:37.647774 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:13:43.763270 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:13:52.495951 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:13:54.788763 140688601454400 submission_runner.py:411] Time since start: 27629.86s, 	Step: 78494, 	{'train/accuracy': 0.7308474183082581, 'train/loss': 1.024781346321106, 'validation/accuracy': 0.6592999696731567, 'validation/loss': 1.396071195602417, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.101693630218506, 'test/num_examples': 10000, 'score': 26573.513592481613, 'total_duration': 27629.85961341858, 'accumulated_submission_time': 26573.513592481613, 'accumulated_eval_time': 1050.6655519008636, 'accumulated_logging_time': 3.017031192779541}
I0229 13:13:54.818844 140522396051200 logging_writer.py:48] [78494] accumulated_eval_time=1050.665552, accumulated_logging_time=3.017031, accumulated_submission_time=26573.513592, global_step=78494, preemption_count=0, score=26573.513592, test/accuracy=0.534800, test/loss=2.101694, test/num_examples=10000, total_duration=27629.859613, train/accuracy=0.730847, train/loss=1.024781, validation/accuracy=0.659300, validation/loss=1.396071, validation/num_examples=50000
I0229 13:13:57.212244 140522404443904 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.116816520690918, loss=1.4402650594711304
I0229 13:14:30.901537 140522396051200 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.131869316101074, loss=1.4497123956680298
I0229 13:15:04.664094 140522404443904 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.1576082706451416, loss=1.4931219816207886
I0229 13:15:38.443704 140522396051200 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.9920532703399658, loss=1.4992315769195557
I0229 13:16:12.251279 140522404443904 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.002110004425049, loss=1.4743889570236206
I0229 13:16:46.024337 140522396051200 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.0524566173553467, loss=1.4991973638534546
I0229 13:17:19.784143 140522404443904 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.132174015045166, loss=1.5117957592010498
I0229 13:17:53.499659 140522396051200 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.13397479057312, loss=1.4761372804641724
I0229 13:18:27.257309 140522404443904 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.0783839225769043, loss=1.4658446311950684
I0229 13:19:01.025468 140522396051200 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.203676462173462, loss=1.5099010467529297
I0229 13:19:34.762392 140522404443904 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.1221113204956055, loss=1.4994721412658691
I0229 13:20:08.510490 140522396051200 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.0478477478027344, loss=1.4490294456481934
I0229 13:20:42.279869 140522404443904 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.185737371444702, loss=1.4905074834823608
I0229 13:21:16.043525 140522396051200 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.101531744003296, loss=1.5479049682617188
I0229 13:21:49.903383 140522404443904 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.168684959411621, loss=1.4474025964736938
I0229 13:22:23.692645 140522396051200 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.090379238128662, loss=1.4503822326660156
I0229 13:22:24.848935 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:22:31.040006 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:22:39.458902 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:22:41.742692 140688601454400 submission_runner.py:411] Time since start: 28156.81s, 	Step: 80005, 	{'train/accuracy': 0.73148512840271, 'train/loss': 1.0369644165039062, 'validation/accuracy': 0.6642000079154968, 'validation/loss': 1.3688702583312988, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.090766191482544, 'test/num_examples': 10000, 'score': 27083.481019496918, 'total_duration': 28156.813540935516, 'accumulated_submission_time': 27083.481019496918, 'accumulated_eval_time': 1067.5592651367188, 'accumulated_logging_time': 3.058248281478882}
I0229 13:22:41.773150 140522396051200 logging_writer.py:48] [80005] accumulated_eval_time=1067.559265, accumulated_logging_time=3.058248, accumulated_submission_time=27083.481019, global_step=80005, preemption_count=0, score=27083.481019, test/accuracy=0.532300, test/loss=2.090766, test/num_examples=10000, total_duration=28156.813541, train/accuracy=0.731485, train/loss=1.036964, validation/accuracy=0.664200, validation/loss=1.368870, validation/num_examples=50000
I0229 13:23:14.137208 140522404443904 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.2040457725524902, loss=1.3312134742736816
I0229 13:23:47.905180 140522396051200 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.2598044872283936, loss=1.4203022718429565
I0229 13:24:21.660559 140522404443904 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.294565200805664, loss=1.4577564001083374
I0229 13:24:55.435508 140522396051200 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.083831310272217, loss=1.4013843536376953
I0229 13:25:29.243056 140522404443904 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.060189723968506, loss=1.4272122383117676
I0229 13:26:03.029544 140522396051200 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.249936580657959, loss=1.4365814924240112
I0229 13:26:36.828679 140522404443904 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.374621629714966, loss=1.42431640625
I0229 13:27:10.582754 140522396051200 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.0862505435943604, loss=1.3828665018081665
I0229 13:27:44.436224 140522404443904 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1671135425567627, loss=1.434321641921997
I0229 13:28:18.297453 140522396051200 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.075273036956787, loss=1.4931538105010986
I0229 13:28:52.087092 140522404443904 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.2679009437561035, loss=1.4608558416366577
I0229 13:29:25.859674 140522396051200 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.9015072584152222, loss=1.3615021705627441
I0229 13:29:59.656381 140522404443904 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.3515405654907227, loss=1.568838357925415
I0229 13:30:33.447097 140522396051200 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.2738540172576904, loss=1.5711338520050049
I0229 13:31:07.269799 140522404443904 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.344320058822632, loss=1.5359145402908325
I0229 13:31:11.806193 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:31:17.938035 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:31:26.661695 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:31:28.948873 140688601454400 submission_runner.py:411] Time since start: 28684.02s, 	Step: 81515, 	{'train/accuracy': 0.7939453125, 'train/loss': 0.7794730067253113, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.3186352252960205, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 2.030189275741577, 'test/num_examples': 10000, 'score': 27593.453538179398, 'total_duration': 28684.019721269608, 'accumulated_submission_time': 27593.453538179398, 'accumulated_eval_time': 1084.7018973827362, 'accumulated_logging_time': 3.098496198654175}
I0229 13:31:28.982172 140522396051200 logging_writer.py:48] [81515] accumulated_eval_time=1084.701897, accumulated_logging_time=3.098496, accumulated_submission_time=27593.453538, global_step=81515, preemption_count=0, score=27593.453538, test/accuracy=0.549200, test/loss=2.030189, test/num_examples=10000, total_duration=28684.019721, train/accuracy=0.793945, train/loss=0.779473, validation/accuracy=0.676560, validation/loss=1.318635, validation/num_examples=50000
I0229 13:31:58.007597 140523947947776 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.145667552947998, loss=1.503300666809082
I0229 13:32:31.728209 140522396051200 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.116593360900879, loss=1.4026966094970703
I0229 13:33:05.502131 140523947947776 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.2011935710906982, loss=1.512969732284546
I0229 13:33:39.337932 140522396051200 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.046649932861328, loss=1.4240989685058594
I0229 13:34:13.291371 140523947947776 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.3597917556762695, loss=1.3258767127990723
I0229 13:34:47.099713 140522396051200 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.024066209793091, loss=1.5270769596099854
I0229 13:35:20.884921 140523947947776 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.9307811260223389, loss=1.496914029121399
I0229 13:35:54.661084 140522396051200 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.10231351852417, loss=1.3632129430770874
I0229 13:36:28.452041 140523947947776 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.1466450691223145, loss=1.4074152708053589
I0229 13:37:02.205318 140522396051200 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.0716261863708496, loss=1.426872968673706
I0229 13:37:36.015686 140523947947776 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.093043327331543, loss=1.436529517173767
I0229 13:38:09.792044 140522396051200 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.086763620376587, loss=1.4486948251724243
I0229 13:38:43.599673 140523947947776 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.3745803833007812, loss=1.5076322555541992
I0229 13:39:17.396314 140522396051200 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.060197114944458, loss=1.4751992225646973
I0229 13:39:51.197481 140523947947776 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.1064157485961914, loss=1.3837406635284424
I0229 13:39:59.129493 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:40:05.380456 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:40:14.307873 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:40:16.587316 140688601454400 submission_runner.py:411] Time since start: 29211.66s, 	Step: 83025, 	{'train/accuracy': 0.7580117583274841, 'train/loss': 0.9174767732620239, 'validation/accuracy': 0.6669999957084656, 'validation/loss': 1.3600958585739136, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0674638748168945, 'test/num_examples': 10000, 'score': 28103.538105487823, 'total_duration': 29211.658153772354, 'accumulated_submission_time': 28103.538105487823, 'accumulated_eval_time': 1102.1596643924713, 'accumulated_logging_time': 3.1425840854644775}
I0229 13:40:16.616828 140523947947776 logging_writer.py:48] [83025] accumulated_eval_time=1102.159664, accumulated_logging_time=3.142584, accumulated_submission_time=28103.538105, global_step=83025, preemption_count=0, score=28103.538105, test/accuracy=0.542100, test/loss=2.067464, test/num_examples=10000, total_duration=29211.658154, train/accuracy=0.758012, train/loss=0.917477, validation/accuracy=0.667000, validation/loss=1.360096, validation/num_examples=50000
I0229 13:40:42.226462 140525256554240 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1397526264190674, loss=1.4293818473815918
I0229 13:41:15.946394 140523947947776 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.2329416275024414, loss=1.466772437095642
I0229 13:41:49.698531 140525256554240 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.0420639514923096, loss=1.5287913084030151
I0229 13:42:23.498223 140523947947776 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1860058307647705, loss=1.4591436386108398
I0229 13:42:57.278506 140525256554240 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.0952742099761963, loss=1.4190930128097534
I0229 13:43:31.070270 140523947947776 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.4214353561401367, loss=1.4815679788589478
I0229 13:44:04.845160 140525256554240 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.192214250564575, loss=1.4420390129089355
I0229 13:44:38.609868 140523947947776 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.284945487976074, loss=1.473649024963379
I0229 13:45:12.387391 140525256554240 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.2315902709960938, loss=1.4585341215133667
I0229 13:45:46.137360 140523947947776 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.382225275039673, loss=1.473012924194336
I0229 13:46:19.974916 140525256554240 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.258188486099243, loss=1.4906039237976074
I0229 13:46:53.724083 140523947947776 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.1711132526397705, loss=1.479749083518982
I0229 13:47:27.519343 140525256554240 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.112307548522949, loss=1.3390569686889648
I0229 13:48:01.230868 140523947947776 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.155736207962036, loss=1.4318232536315918
I0229 13:48:34.975250 140525256554240 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.117788791656494, loss=1.397564172744751
I0229 13:48:46.922481 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:48:53.159538 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:49:01.639605 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:49:03.993119 140688601454400 submission_runner.py:411] Time since start: 29739.06s, 	Step: 84537, 	{'train/accuracy': 0.75390625, 'train/loss': 0.9180837273597717, 'validation/accuracy': 0.6711199879646301, 'validation/loss': 1.3440886735916138, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.0711793899536133, 'test/num_examples': 10000, 'score': 28613.781172037125, 'total_duration': 29739.063964366913, 'accumulated_submission_time': 28613.781172037125, 'accumulated_eval_time': 1119.2302613258362, 'accumulated_logging_time': 3.1828737258911133}
I0229 13:49:04.027999 140522605770496 logging_writer.py:48] [84537] accumulated_eval_time=1119.230261, accumulated_logging_time=3.182874, accumulated_submission_time=28613.781172, global_step=84537, preemption_count=0, score=28613.781172, test/accuracy=0.537800, test/loss=2.071179, test/num_examples=10000, total_duration=29739.063964, train/accuracy=0.753906, train/loss=0.918084, validation/accuracy=0.671120, validation/loss=1.344089, validation/num_examples=50000
I0229 13:49:25.600793 140523092305664 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.183335065841675, loss=1.4910989999771118
I0229 13:49:59.318505 140522605770496 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.133655071258545, loss=1.5033578872680664
I0229 13:50:33.093734 140523092305664 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.2028892040252686, loss=1.5041433572769165
I0229 13:51:06.870703 140522605770496 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.115294933319092, loss=1.4159820079803467
I0229 13:51:40.637423 140523092305664 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.232501983642578, loss=1.394264817237854
I0229 13:52:14.422795 140522605770496 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.1462223529815674, loss=1.3928403854370117
I0229 13:52:48.352597 140523092305664 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.151479482650757, loss=1.51726233959198
I0229 13:53:22.112621 140522605770496 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.351696491241455, loss=1.5444824695587158
I0229 13:53:55.875778 140523092305664 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.494541645050049, loss=1.4678970575332642
I0229 13:54:29.608794 140522605770496 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.227919340133667, loss=1.4350672960281372
I0229 13:55:03.383826 140523092305664 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.181095838546753, loss=1.4658888578414917
I0229 13:55:37.161080 140522605770496 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.3674182891845703, loss=1.4137537479400635
I0229 13:56:10.934032 140523092305664 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.259124994277954, loss=1.475855827331543
I0229 13:56:44.696127 140522605770496 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.3700003623962402, loss=1.450846552848816
I0229 13:57:18.459646 140523092305664 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.6762096881866455, loss=1.4890403747558594
I0229 13:57:34.119622 140688601454400 spec.py:321] Evaluating on the training split.
I0229 13:57:40.237423 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 13:57:48.668853 140688601454400 spec.py:349] Evaluating on the test split.
I0229 13:57:50.924284 140688601454400 submission_runner.py:411] Time since start: 30266.00s, 	Step: 86048, 	{'train/accuracy': 0.7520527839660645, 'train/loss': 0.9375916719436646, 'validation/accuracy': 0.670799970626831, 'validation/loss': 1.3429946899414062, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.0667781829833984, 'test/num_examples': 10000, 'score': 29123.81152534485, 'total_duration': 30265.995129585266, 'accumulated_submission_time': 29123.81152534485, 'accumulated_eval_time': 1136.0348734855652, 'accumulated_logging_time': 3.2278828620910645}
I0229 13:57:50.960743 140525264946944 logging_writer.py:48] [86048] accumulated_eval_time=1136.034873, accumulated_logging_time=3.227883, accumulated_submission_time=29123.811525, global_step=86048, preemption_count=0, score=29123.811525, test/accuracy=0.549100, test/loss=2.066778, test/num_examples=10000, total_duration=30265.995130, train/accuracy=0.752053, train/loss=0.937592, validation/accuracy=0.670800, validation/loss=1.342995, validation/num_examples=50000
I0229 13:58:08.845154 140525273339648 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.1855359077453613, loss=1.400596022605896
I0229 13:58:42.668373 140525264946944 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.3639485836029053, loss=1.3720653057098389
I0229 13:59:16.377166 140525273339648 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.180424690246582, loss=1.3247898817062378
I0229 13:59:50.136373 140525264946944 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.1799237728118896, loss=1.4048800468444824
I0229 14:00:23.952580 140525273339648 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.3233749866485596, loss=1.4655795097351074
I0229 14:00:57.715324 140525264946944 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.056244373321533, loss=1.433889627456665
I0229 14:01:31.508435 140525273339648 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.1690163612365723, loss=1.4354934692382812
I0229 14:02:05.273809 140525264946944 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.270754814147949, loss=1.4767003059387207
I0229 14:02:39.056058 140525273339648 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.246948719024658, loss=1.3973755836486816
I0229 14:03:12.829114 140525264946944 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.3299527168273926, loss=1.5147440433502197
I0229 14:03:46.541021 140525273339648 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.1979846954345703, loss=1.475322961807251
I0229 14:04:20.301249 140525264946944 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.2500598430633545, loss=1.4541852474212646
I0229 14:04:54.122810 140525273339648 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.467149019241333, loss=1.480676531791687
I0229 14:05:27.904340 140525264946944 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.413586378097534, loss=1.5156885385513306
I0229 14:06:01.717571 140525273339648 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.1689021587371826, loss=1.4465115070343018
I0229 14:06:21.108445 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:06:27.280182 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:06:35.936741 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:06:38.207525 140688601454400 submission_runner.py:411] Time since start: 30793.28s, 	Step: 87559, 	{'train/accuracy': 0.7551020383834839, 'train/loss': 0.9199984073638916, 'validation/accuracy': 0.6786999702453613, 'validation/loss': 1.3077771663665771, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.0441277027130127, 'test/num_examples': 10000, 'score': 29633.896688699722, 'total_duration': 30793.2783703804, 'accumulated_submission_time': 29633.896688699722, 'accumulated_eval_time': 1153.1339037418365, 'accumulated_logging_time': 3.2748494148254395}
I0229 14:06:38.243092 140522588985088 logging_writer.py:48] [87559] accumulated_eval_time=1153.133904, accumulated_logging_time=3.274849, accumulated_submission_time=29633.896689, global_step=87559, preemption_count=0, score=29633.896689, test/accuracy=0.552000, test/loss=2.044128, test/num_examples=10000, total_duration=30793.278370, train/accuracy=0.755102, train/loss=0.919998, validation/accuracy=0.678700, validation/loss=1.307777, validation/num_examples=50000
I0229 14:06:52.419189 140522597377792 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.3012449741363525, loss=1.4195151329040527
I0229 14:07:26.123999 140522588985088 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.2029807567596436, loss=1.4364383220672607
I0229 14:07:59.900214 140522597377792 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.3041646480560303, loss=1.4539692401885986
I0229 14:08:33.682213 140522588985088 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.05861234664917, loss=1.364708423614502
I0229 14:09:07.479414 140522597377792 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.628239393234253, loss=1.4465751647949219
I0229 14:09:41.258098 140522588985088 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.352712631225586, loss=1.5472806692123413
I0229 14:10:15.021446 140522597377792 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.5196433067321777, loss=1.3772947788238525
I0229 14:10:48.821200 140522588985088 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.1733508110046387, loss=1.4455668926239014
I0229 14:11:22.699389 140522597377792 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.361363649368286, loss=1.5085996389389038
I0229 14:11:56.488470 140522588985088 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.1563034057617188, loss=1.4071810245513916
I0229 14:12:30.282317 140522597377792 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.372563362121582, loss=1.4015816450119019
I0229 14:13:04.058156 140522588985088 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.4389562606811523, loss=1.4826185703277588
I0229 14:13:37.820361 140522597377792 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.175973892211914, loss=1.3442161083221436
I0229 14:14:11.605917 140522588985088 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.179429531097412, loss=1.3964885473251343
I0229 14:14:45.372158 140522597377792 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.5844669342041016, loss=1.4454149007797241
I0229 14:15:08.490801 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:15:14.762946 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:15:23.300964 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:15:25.608452 140688601454400 submission_runner.py:411] Time since start: 31320.68s, 	Step: 89070, 	{'train/accuracy': 0.7512954473495483, 'train/loss': 0.9448497891426086, 'validation/accuracy': 0.6762999892234802, 'validation/loss': 1.3257722854614258, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.053436517715454, 'test/num_examples': 10000, 'score': 30144.08286547661, 'total_duration': 31320.67922115326, 'accumulated_submission_time': 30144.08286547661, 'accumulated_eval_time': 1170.2514476776123, 'accumulated_logging_time': 3.3205323219299316}
I0229 14:15:25.647135 140525264946944 logging_writer.py:48] [89070] accumulated_eval_time=1170.251448, accumulated_logging_time=3.320532, accumulated_submission_time=30144.082865, global_step=89070, preemption_count=0, score=30144.082865, test/accuracy=0.552700, test/loss=2.053437, test/num_examples=10000, total_duration=31320.679221, train/accuracy=0.751295, train/loss=0.944850, validation/accuracy=0.676300, validation/loss=1.325772, validation/num_examples=50000
I0229 14:15:36.084209 140525273339648 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.1202945709228516, loss=1.3401813507080078
I0229 14:16:09.771423 140525264946944 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.440260410308838, loss=1.5474060773849487
I0229 14:16:43.509946 140525273339648 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.1905219554901123, loss=1.4455052614212036
I0229 14:17:17.317090 140525264946944 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.2575459480285645, loss=1.434556484222412
I0229 14:17:51.081327 140525273339648 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.1229913234710693, loss=1.372346043586731
I0229 14:18:24.836515 140525264946944 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.183837413787842, loss=1.4243205785751343
I0229 14:18:58.616890 140525273339648 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.5325210094451904, loss=1.4646474123001099
I0229 14:19:32.430857 140525264946944 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.0941593647003174, loss=1.5502352714538574
I0229 14:20:06.159583 140525273339648 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.22348690032959, loss=1.3096802234649658
I0229 14:20:39.931191 140525264946944 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.2767276763916016, loss=1.455933928489685
I0229 14:21:13.674715 140525273339648 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.216029405593872, loss=1.3196220397949219
I0229 14:21:47.393314 140525264946944 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.2563717365264893, loss=1.3829193115234375
I0229 14:22:21.155805 140525273339648 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.1903076171875, loss=1.3444350957870483
I0229 14:22:54.933495 140525264946944 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.31050968170166, loss=1.3520169258117676
I0229 14:23:28.721826 140525273339648 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.2313220500946045, loss=1.4119840860366821
I0229 14:23:55.894800 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:24:02.080887 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:24:10.880749 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:24:13.185288 140688601454400 submission_runner.py:411] Time since start: 31848.26s, 	Step: 90582, 	{'train/accuracy': 0.8028140664100647, 'train/loss': 0.7457603812217712, 'validation/accuracy': 0.6789199709892273, 'validation/loss': 1.2876888513565063, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 1.9878932237625122, 'test/num_examples': 10000, 'score': 30654.26819229126, 'total_duration': 31848.256128311157, 'accumulated_submission_time': 30654.26819229126, 'accumulated_eval_time': 1187.5418901443481, 'accumulated_logging_time': 3.3698208332061768}
I0229 14:24:13.217703 140525273339648 logging_writer.py:48] [90582] accumulated_eval_time=1187.541890, accumulated_logging_time=3.369821, accumulated_submission_time=30654.268192, global_step=90582, preemption_count=0, score=30654.268192, test/accuracy=0.554200, test/loss=1.987893, test/num_examples=10000, total_duration=31848.256128, train/accuracy=0.802814, train/loss=0.745760, validation/accuracy=0.678920, validation/loss=1.287689, validation/num_examples=50000
I0229 14:24:19.613054 140525281732352 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.296055793762207, loss=1.3341840505599976
I0229 14:24:53.270627 140525273339648 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.169097661972046, loss=1.4644267559051514
I0229 14:25:26.944168 140525281732352 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.326225757598877, loss=1.4159456491470337
I0229 14:26:00.713600 140525273339648 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.1743288040161133, loss=1.296401023864746
I0229 14:26:34.455771 140525281732352 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.198110580444336, loss=1.361051321029663
I0229 14:27:08.147888 140525273339648 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.595742702484131, loss=1.340230107307434
I0229 14:27:41.901760 140525281732352 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.3152530193328857, loss=1.4020379781723022
I0229 14:28:15.684280 140525273339648 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.1281039714813232, loss=1.4016178846359253
I0229 14:28:49.437428 140525281732352 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.5567033290863037, loss=1.4006024599075317
I0229 14:29:23.213981 140525273339648 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.1729750633239746, loss=1.4772425889968872
I0229 14:29:57.036163 140525281732352 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.3199496269226074, loss=1.2710014581680298
I0229 14:30:30.750818 140525273339648 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.6726412773132324, loss=1.4146850109100342
I0229 14:31:04.498495 140525281732352 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.6512279510498047, loss=1.5589967966079712
I0229 14:31:38.260857 140525273339648 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.2083613872528076, loss=1.3718116283416748
I0229 14:32:12.003529 140525281732352 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.200664520263672, loss=1.3278294801712036
I0229 14:32:43.506811 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:32:49.591157 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:32:58.269683 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:33:00.521610 140688601454400 submission_runner.py:411] Time since start: 32375.59s, 	Step: 92095, 	{'train/accuracy': 0.7693120241165161, 'train/loss': 0.856544017791748, 'validation/accuracy': 0.6754800081253052, 'validation/loss': 1.3307112455368042, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.0672965049743652, 'test/num_examples': 10000, 'score': 31164.4939289093, 'total_duration': 32375.592450141907, 'accumulated_submission_time': 31164.4939289093, 'accumulated_eval_time': 1204.5566387176514, 'accumulated_logging_time': 3.4134774208068848}
I0229 14:33:00.558561 140523092305664 logging_writer.py:48] [92095] accumulated_eval_time=1204.556639, accumulated_logging_time=3.413477, accumulated_submission_time=31164.493929, global_step=92095, preemption_count=0, score=31164.493929, test/accuracy=0.546800, test/loss=2.067297, test/num_examples=10000, total_duration=32375.592450, train/accuracy=0.769312, train/loss=0.856544, validation/accuracy=0.675480, validation/loss=1.330711, validation/num_examples=50000
I0229 14:33:02.590010 140523947947776 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.241331100463867, loss=1.4152635335922241
I0229 14:33:36.328186 140523092305664 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.266075611114502, loss=1.3108644485473633
I0229 14:34:10.106122 140523947947776 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.1902098655700684, loss=1.3154367208480835
I0229 14:34:43.877070 140523092305664 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.2619380950927734, loss=1.3366345167160034
I0229 14:35:17.644397 140523947947776 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4996559619903564, loss=1.3999377489089966
I0229 14:35:51.443617 140523092305664 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.2586026191711426, loss=1.3109242916107178
I0229 14:36:25.175289 140523947947776 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.33784556388855, loss=1.3306869268417358
I0229 14:36:58.933333 140523092305664 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.2706947326660156, loss=1.4040979146957397
I0229 14:37:32.714440 140523947947776 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.4697537422180176, loss=1.4546124935150146
I0229 14:38:06.472851 140523092305664 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.5406923294067383, loss=1.372086763381958
I0229 14:38:40.195953 140523947947776 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.2426817417144775, loss=1.4177511930465698
I0229 14:39:13.957829 140523092305664 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.297386646270752, loss=1.3017170429229736
I0229 14:39:47.729504 140523947947776 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.3921144008636475, loss=1.4270038604736328
I0229 14:40:21.490664 140523092305664 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.256991147994995, loss=1.4415318965911865
I0229 14:40:55.248102 140523947947776 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.5091702938079834, loss=1.4576408863067627
I0229 14:41:28.993234 140523092305664 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.3180909156799316, loss=1.367699384689331
I0229 14:41:30.813644 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:41:37.230012 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:41:46.029201 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:41:48.341825 140688601454400 submission_runner.py:411] Time since start: 32903.41s, 	Step: 93607, 	{'train/accuracy': 0.76566481590271, 'train/loss': 0.876564621925354, 'validation/accuracy': 0.6783599853515625, 'validation/loss': 1.3167260885238647, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 2.0210089683532715, 'test/num_examples': 10000, 'score': 31674.687499523163, 'total_duration': 32903.412647247314, 'accumulated_submission_time': 31674.687499523163, 'accumulated_eval_time': 1222.084743976593, 'accumulated_logging_time': 3.4607160091400146}
I0229 14:41:48.377836 140522597377792 logging_writer.py:48] [93607] accumulated_eval_time=1222.084744, accumulated_logging_time=3.460716, accumulated_submission_time=31674.687500, global_step=93607, preemption_count=0, score=31674.687500, test/accuracy=0.552900, test/loss=2.021009, test/num_examples=10000, total_duration=32903.412647, train/accuracy=0.765665, train/loss=0.876565, validation/accuracy=0.678360, validation/loss=1.316726, validation/num_examples=50000
I0229 14:42:20.083157 140522605770496 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.360705614089966, loss=1.4283406734466553
I0229 14:42:53.846232 140522597377792 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.7781426906585693, loss=1.4667749404907227
I0229 14:43:27.607204 140522605770496 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.244576930999756, loss=1.2999812364578247
I0229 14:44:01.347738 140522597377792 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.4125640392303467, loss=1.445102572441101
I0229 14:44:35.108730 140522605770496 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.3457794189453125, loss=1.3771737813949585
I0229 14:45:08.891915 140522597377792 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.3119394779205322, loss=1.4631826877593994
I0229 14:45:42.652132 140522605770496 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.3237967491149902, loss=1.2513887882232666
I0229 14:46:16.433844 140522597377792 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.1503231525421143, loss=1.3026882410049438
I0229 14:46:50.203884 140522605770496 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.2757906913757324, loss=1.3881540298461914
I0229 14:47:23.967851 140522597377792 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.1924550533294678, loss=1.3326365947723389
I0229 14:47:57.825776 140522605770496 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.178586006164551, loss=1.3049864768981934
I0229 14:48:31.560848 140522597377792 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.3106279373168945, loss=1.3716017007827759
I0229 14:49:05.318325 140522605770496 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.232537269592285, loss=1.4743701219558716
I0229 14:49:39.101888 140522597377792 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.8778939247131348, loss=1.4566737413406372
I0229 14:50:12.850950 140522605770496 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.53259539604187, loss=1.436368703842163
I0229 14:50:18.407617 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:50:24.552631 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:50:33.313977 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:50:35.647237 140688601454400 submission_runner.py:411] Time since start: 33430.72s, 	Step: 95118, 	{'train/accuracy': 0.7731783986091614, 'train/loss': 0.8373833894729614, 'validation/accuracy': 0.6893999576568604, 'validation/loss': 1.2670496702194214, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 1.996338963508606, 'test/num_examples': 10000, 'score': 32184.654997348785, 'total_duration': 33430.718074798584, 'accumulated_submission_time': 32184.654997348785, 'accumulated_eval_time': 1239.3243083953857, 'accumulated_logging_time': 3.507728338241577}
I0229 14:50:35.682732 140523947947776 logging_writer.py:48] [95118] accumulated_eval_time=1239.324308, accumulated_logging_time=3.507728, accumulated_submission_time=32184.654997, global_step=95118, preemption_count=0, score=32184.654997, test/accuracy=0.559300, test/loss=1.996339, test/num_examples=10000, total_duration=33430.718075, train/accuracy=0.773178, train/loss=0.837383, validation/accuracy=0.689400, validation/loss=1.267050, validation/num_examples=50000
I0229 14:51:03.693005 140525256554240 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.2639331817626953, loss=1.3541560173034668
I0229 14:51:37.446223 140523947947776 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.6062119007110596, loss=1.429891586303711
I0229 14:52:11.186887 140525256554240 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.281938076019287, loss=1.2773700952529907
I0229 14:52:44.962146 140523947947776 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.3246676921844482, loss=1.3176103830337524
I0229 14:53:18.760834 140525256554240 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.262423515319824, loss=1.21418035030365
I0229 14:53:52.614795 140523947947776 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.2223970890045166, loss=1.4071216583251953
I0229 14:54:26.398577 140525256554240 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.640770435333252, loss=1.5048224925994873
I0229 14:55:00.183665 140523947947776 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.3667354583740234, loss=1.3960556983947754
I0229 14:55:33.956214 140525256554240 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.2779598236083984, loss=1.3944590091705322
I0229 14:56:07.753283 140523947947776 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.3378846645355225, loss=1.3967175483703613
I0229 14:56:41.494399 140525256554240 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.426785945892334, loss=1.327520728111267
I0229 14:57:15.279488 140523947947776 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.3071343898773193, loss=1.4233027696609497
I0229 14:57:49.052680 140525256554240 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.3157153129577637, loss=1.413640022277832
I0229 14:58:22.842905 140523947947776 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.420607328414917, loss=1.3315229415893555
I0229 14:58:56.613115 140525256554240 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.2504353523254395, loss=1.281181812286377
I0229 14:59:05.882864 140688601454400 spec.py:321] Evaluating on the training split.
I0229 14:59:12.000431 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 14:59:20.735720 140688601454400 spec.py:349] Evaluating on the test split.
I0229 14:59:23.010884 140688601454400 submission_runner.py:411] Time since start: 33958.08s, 	Step: 96629, 	{'train/accuracy': 0.756257951259613, 'train/loss': 0.9207841157913208, 'validation/accuracy': 0.6714000105857849, 'validation/loss': 1.3359733819961548, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 2.0310440063476562, 'test/num_examples': 10000, 'score': 32694.792423963547, 'total_duration': 33958.081731557846, 'accumulated_submission_time': 32694.792423963547, 'accumulated_eval_time': 1256.4522774219513, 'accumulated_logging_time': 3.5546374320983887}
I0229 14:59:23.048534 140522597377792 logging_writer.py:48] [96629] accumulated_eval_time=1256.452277, accumulated_logging_time=3.554637, accumulated_submission_time=32694.792424, global_step=96629, preemption_count=0, score=32694.792424, test/accuracy=0.550500, test/loss=2.031044, test/num_examples=10000, total_duration=33958.081732, train/accuracy=0.756258, train/loss=0.920784, validation/accuracy=0.671400, validation/loss=1.335973, validation/num_examples=50000
I0229 14:59:47.336925 140522605770496 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.4015390872955322, loss=1.3624975681304932
I0229 15:00:21.142449 140522597377792 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.573467969894409, loss=1.4975230693817139
I0229 15:00:54.893206 140522605770496 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.1115739345550537, loss=1.366050362586975
I0229 15:01:28.659953 140522597377792 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.387072801589966, loss=1.3099019527435303
I0229 15:02:02.390832 140522605770496 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.713386058807373, loss=1.5086110830307007
I0229 15:02:36.162360 140522597377792 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.6296887397766113, loss=1.4193482398986816
I0229 15:03:09.979596 140522605770496 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.329653024673462, loss=1.3882495164871216
I0229 15:03:43.734990 140522597377792 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.4644203186035156, loss=1.4107321500778198
I0229 15:04:17.469571 140522605770496 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.38124680519104, loss=1.3002853393554688
I0229 15:04:51.237684 140522597377792 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.361433982849121, loss=1.3706408739089966
I0229 15:05:25.026638 140522605770496 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.456894874572754, loss=1.3747564554214478
I0229 15:05:58.809377 140522597377792 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.4645438194274902, loss=1.4736615419387817
I0229 15:06:32.651521 140522605770496 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.389080286026001, loss=1.3834794759750366
I0229 15:07:06.403422 140522597377792 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.5124332904815674, loss=1.3497836589813232
I0229 15:07:40.180421 140522605770496 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.483572483062744, loss=1.3735331296920776
I0229 15:07:53.136988 140688601454400 spec.py:321] Evaluating on the training split.
I0229 15:07:59.405212 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 15:08:08.094434 140688601454400 spec.py:349] Evaluating on the test split.
I0229 15:08:10.445892 140688601454400 submission_runner.py:411] Time since start: 34485.52s, 	Step: 98140, 	{'train/accuracy': 0.7623963356018066, 'train/loss': 0.890619695186615, 'validation/accuracy': 0.6840199828147888, 'validation/loss': 1.299177885055542, 'validation/num_examples': 50000, 'test/accuracy': 0.5555000305175781, 'test/loss': 2.0382800102233887, 'test/num_examples': 10000, 'score': 33204.81808638573, 'total_duration': 34485.51674103737, 'accumulated_submission_time': 33204.81808638573, 'accumulated_eval_time': 1273.7611339092255, 'accumulated_logging_time': 3.6036319732666016}
I0229 15:08:10.479941 140525256554240 logging_writer.py:48] [98140] accumulated_eval_time=1273.761134, accumulated_logging_time=3.603632, accumulated_submission_time=33204.818086, global_step=98140, preemption_count=0, score=33204.818086, test/accuracy=0.555500, test/loss=2.038280, test/num_examples=10000, total_duration=34485.516741, train/accuracy=0.762396, train/loss=0.890620, validation/accuracy=0.684020, validation/loss=1.299178, validation/num_examples=50000
I0229 15:08:31.105405 140525264946944 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.22330379486084, loss=1.3318758010864258
I0229 15:09:04.795289 140525256554240 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.4227752685546875, loss=1.3417044878005981
I0229 15:09:38.581948 140525264946944 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.2687811851501465, loss=1.295813798904419
I0229 15:10:12.357817 140525256554240 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.457406520843506, loss=1.3966870307922363
I0229 15:10:46.161039 140525264946944 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.4874966144561768, loss=1.3779795169830322
I0229 15:11:19.915494 140525256554240 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.6491622924804688, loss=1.3728753328323364
I0229 15:11:53.660591 140525264946944 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.623967409133911, loss=1.4325876235961914
I0229 15:12:27.525458 140525256554240 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.6543009281158447, loss=1.2943878173828125
I0229 15:13:01.299989 140525264946944 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.479966640472412, loss=1.3177683353424072
I0229 15:13:35.083175 140525256554240 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.398361921310425, loss=1.3501018285751343
I0229 15:14:08.794203 140525264946944 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.3381168842315674, loss=1.3473968505859375
I0229 15:14:42.558820 140525256554240 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.2750985622406006, loss=1.3089618682861328
I0229 15:15:16.347598 140525264946944 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.306460380554199, loss=1.3086316585540771
I0229 15:15:50.115708 140525256554240 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.4255383014678955, loss=1.3604774475097656
I0229 15:16:23.934219 140525264946944 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.4386751651763916, loss=1.3118622303009033
I0229 15:16:40.590353 140688601454400 spec.py:321] Evaluating on the training split.
I0229 15:16:46.697972 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 15:16:55.385350 140688601454400 spec.py:349] Evaluating on the test split.
I0229 15:16:57.652401 140688601454400 submission_runner.py:411] Time since start: 35012.72s, 	Step: 99651, 	{'train/accuracy': 0.807059109210968, 'train/loss': 0.7231523990631104, 'validation/accuracy': 0.6853199601173401, 'validation/loss': 1.2836941480636597, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 1.994417428970337, 'test/num_examples': 10000, 'score': 33714.866651535034, 'total_duration': 35012.723252773285, 'accumulated_submission_time': 33714.866651535034, 'accumulated_eval_time': 1290.823139667511, 'accumulated_logging_time': 3.6479039192199707}
I0229 15:16:57.684605 140522605770496 logging_writer.py:48] [99651] accumulated_eval_time=1290.823140, accumulated_logging_time=3.647904, accumulated_submission_time=33714.866652, global_step=99651, preemption_count=0, score=33714.866652, test/accuracy=0.558700, test/loss=1.994417, test/num_examples=10000, total_duration=35012.723253, train/accuracy=0.807059, train/loss=0.723152, validation/accuracy=0.685320, validation/loss=1.283694, validation/num_examples=50000
I0229 15:17:14.535558 140523092305664 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.6106507778167725, loss=1.4296330213546753
I0229 15:17:48.272637 140522605770496 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.8501124382019043, loss=1.4740159511566162
I0229 15:18:22.027427 140523092305664 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.5755615234375, loss=1.4169057607650757
I0229 15:18:55.872656 140522605770496 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.6177310943603516, loss=1.3323006629943848
I0229 15:19:29.615066 140523092305664 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.4730825424194336, loss=1.3963468074798584
I0229 15:20:03.353054 140522605770496 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.5998411178588867, loss=1.328847885131836
I0229 15:20:37.144836 140523092305664 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.4726061820983887, loss=1.367857813835144
I0229 15:21:10.927649 140522605770496 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.5679659843444824, loss=1.364064335823059
I0229 15:21:44.717791 140523092305664 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.802125930786133, loss=1.4433947801589966
I0229 15:22:18.510742 140522605770496 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.417802572250366, loss=1.370570421218872
I0229 15:22:52.281200 140523092305664 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.433102607727051, loss=1.2897744178771973
I0229 15:23:25.999165 140522605770496 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.4764368534088135, loss=1.3927229642868042
I0229 15:23:59.726003 140523092305664 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.687376022338867, loss=1.2937383651733398
I0229 15:24:33.586585 140522605770496 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.440098285675049, loss=1.4419702291488647
I0229 15:25:07.350234 140523092305664 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.2624199390411377, loss=1.3090970516204834
I0229 15:25:27.734586 140688601454400 spec.py:321] Evaluating on the training split.
I0229 15:25:33.844087 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 15:25:42.601810 140688601454400 spec.py:349] Evaluating on the test split.
I0229 15:25:44.870563 140688601454400 submission_runner.py:411] Time since start: 35539.94s, 	Step: 101162, 	{'train/accuracy': 0.7729192972183228, 'train/loss': 0.8433666229248047, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.3290696144104004, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.0852365493774414, 'test/num_examples': 10000, 'score': 34224.85311436653, 'total_duration': 35539.94133090973, 'accumulated_submission_time': 34224.85311436653, 'accumulated_eval_time': 1307.958990097046, 'accumulated_logging_time': 3.690621852874756}
I0229 15:25:44.904289 140523092305664 logging_writer.py:48] [101162] accumulated_eval_time=1307.958990, accumulated_logging_time=3.690622, accumulated_submission_time=34224.853114, global_step=101162, preemption_count=0, score=34224.853114, test/accuracy=0.545700, test/loss=2.085237, test/num_examples=10000, total_duration=35539.941331, train/accuracy=0.772919, train/loss=0.843367, validation/accuracy=0.676560, validation/loss=1.329070, validation/num_examples=50000
I0229 15:25:58.063837 140525264946944 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.3931691646575928, loss=1.4172446727752686
I0229 15:26:31.786672 140523092305664 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.3691704273223877, loss=1.289375901222229
I0229 15:27:05.545004 140525264946944 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.5725245475769043, loss=1.323585033416748
I0229 15:27:39.321072 140523092305664 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.979581356048584, loss=1.411916732788086
I0229 15:28:13.092481 140525264946944 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.4667251110076904, loss=1.2553542852401733
I0229 15:28:46.894589 140523092305664 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.5566234588623047, loss=1.308424949645996
I0229 15:29:20.662027 140525264946944 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.4575700759887695, loss=1.3352370262145996
I0229 15:29:54.404618 140523092305664 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.345954179763794, loss=1.3647788763046265
I0229 15:30:28.191128 140525264946944 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.544647455215454, loss=1.4091492891311646
I0229 15:31:02.060937 140523092305664 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.2003328800201416, loss=1.1772310733795166
I0229 15:31:35.821348 140525264946944 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.4598300457000732, loss=1.3729043006896973
I0229 15:32:09.531694 140523092305664 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.532733201980591, loss=1.3682852983474731
I0229 15:32:43.292666 140525264946944 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.462313175201416, loss=1.3792800903320312
I0229 15:33:17.111926 140523092305664 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.532205820083618, loss=1.364338755607605
I0229 15:33:50.860193 140525264946944 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.490466594696045, loss=1.2862277030944824
I0229 15:34:14.931339 140688601454400 spec.py:321] Evaluating on the training split.
I0229 15:34:21.069212 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 15:34:29.894446 140688601454400 spec.py:349] Evaluating on the test split.
I0229 15:34:32.131097 140688601454400 submission_runner.py:411] Time since start: 36067.20s, 	Step: 102673, 	{'train/accuracy': 0.7886638641357422, 'train/loss': 0.7821224331855774, 'validation/accuracy': 0.6950399875640869, 'validation/loss': 1.240763783454895, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.9514639377593994, 'test/num_examples': 10000, 'score': 34734.81702041626, 'total_duration': 36067.20191526413, 'accumulated_submission_time': 34734.81702041626, 'accumulated_eval_time': 1325.1586797237396, 'accumulated_logging_time': 3.736107349395752}
I0229 15:34:32.169893 140522605770496 logging_writer.py:48] [102673] accumulated_eval_time=1325.158680, accumulated_logging_time=3.736107, accumulated_submission_time=34734.817020, global_step=102673, preemption_count=0, score=34734.817020, test/accuracy=0.564900, test/loss=1.951464, test/num_examples=10000, total_duration=36067.201915, train/accuracy=0.788664, train/loss=0.782122, validation/accuracy=0.695040, validation/loss=1.240764, validation/num_examples=50000
I0229 15:34:41.643287 140523092305664 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.542551279067993, loss=1.2443782091140747
I0229 15:35:15.340048 140522605770496 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.6573939323425293, loss=1.3943077325820923
I0229 15:35:49.107732 140523092305664 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.4315364360809326, loss=1.3295496702194214
I0229 15:36:22.881075 140522605770496 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.623704433441162, loss=1.3009170293807983
I0229 15:36:56.723165 140523092305664 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.494926691055298, loss=1.3559157848358154
I0229 15:37:30.505451 140522605770496 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.3551318645477295, loss=1.3087496757507324
I0229 15:38:04.291278 140523092305664 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.5694525241851807, loss=1.3890427350997925
I0229 15:38:38.049705 140522605770496 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.5521433353424072, loss=1.3188029527664185
I0229 15:39:11.813343 140523092305664 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.795382499694824, loss=1.2910305261611938
I0229 15:39:45.605321 140522605770496 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.370626926422119, loss=1.2578362226486206
I0229 15:40:19.324681 140523092305664 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.4548089504241943, loss=1.2562899589538574
I0229 15:40:53.065922 140522605770496 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.6361477375030518, loss=1.4331024885177612
I0229 15:41:26.825350 140523092305664 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.3562638759613037, loss=1.2537639141082764
I0229 15:42:00.632849 140522605770496 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.411039352416992, loss=1.2848366498947144
I0229 15:42:34.419879 140523092305664 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.606902837753296, loss=1.4433094263076782
I0229 15:43:02.420726 140688601454400 spec.py:321] Evaluating on the training split.
I0229 15:43:08.546451 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 15:43:17.099356 140688601454400 spec.py:349] Evaluating on the test split.
I0229 15:43:19.346235 140688601454400 submission_runner.py:411] Time since start: 36594.42s, 	Step: 104184, 	{'train/accuracy': 0.7867307066917419, 'train/loss': 0.7857255339622498, 'validation/accuracy': 0.6966599822044373, 'validation/loss': 1.2365820407867432, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.9482532739639282, 'test/num_examples': 10000, 'score': 35245.00524544716, 'total_duration': 36594.41708111763, 'accumulated_submission_time': 35245.00524544716, 'accumulated_eval_time': 1342.0841455459595, 'accumulated_logging_time': 3.785308361053467}
I0229 15:43:19.381328 140525256554240 logging_writer.py:48] [104184] accumulated_eval_time=1342.084146, accumulated_logging_time=3.785308, accumulated_submission_time=35245.005245, global_step=104184, preemption_count=0, score=35245.005245, test/accuracy=0.572500, test/loss=1.948253, test/num_examples=10000, total_duration=36594.417081, train/accuracy=0.786731, train/loss=0.785726, validation/accuracy=0.696660, validation/loss=1.236582, validation/num_examples=50000
I0229 15:43:25.111936 140525264946944 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.6877079010009766, loss=1.3440616130828857
I0229 15:43:58.854426 140525256554240 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.6535744667053223, loss=1.2760987281799316
I0229 15:44:32.576256 140525264946944 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.372987747192383, loss=1.2183644771575928
I0229 15:45:06.346155 140525256554240 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.5658488273620605, loss=1.3310472965240479
I0229 15:45:40.160152 140525264946944 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.512812614440918, loss=1.3161654472351074
I0229 15:46:13.945681 140525256554240 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.5311172008514404, loss=1.3222181797027588
I0229 15:46:47.768586 140525264946944 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.3908774852752686, loss=1.2020339965820312
I0229 15:47:21.564227 140525256554240 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.4481513500213623, loss=1.3607978820800781
I0229 15:47:55.330440 140525264946944 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.587245225906372, loss=1.270801067352295
I0229 15:48:29.116220 140525256554240 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.6493287086486816, loss=1.2418023347854614
I0229 15:49:02.913149 140525264946944 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.572552442550659, loss=1.3692631721496582
I0229 15:49:36.743134 140525256554240 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.406677007675171, loss=1.2720028162002563
I0229 15:50:10.531615 140525264946944 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.9011476039886475, loss=1.245912790298462
I0229 15:50:44.286062 140525256554240 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.5492122173309326, loss=1.2551215887069702
I0229 15:51:18.059102 140525264946944 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.5409812927246094, loss=1.326735258102417
I0229 15:51:49.604538 140688601454400 spec.py:321] Evaluating on the training split.
I0229 15:51:55.754890 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 15:52:04.425632 140688601454400 spec.py:349] Evaluating on the test split.
I0229 15:52:06.723661 140688601454400 submission_runner.py:411] Time since start: 37121.79s, 	Step: 105695, 	{'train/accuracy': 0.7801339030265808, 'train/loss': 0.8069247603416443, 'validation/accuracy': 0.6951000094413757, 'validation/loss': 1.2463161945343018, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.96476411819458, 'test/num_examples': 10000, 'score': 35755.16663765907, 'total_duration': 37121.79448270798, 'accumulated_submission_time': 35755.16663765907, 'accumulated_eval_time': 1359.2032098770142, 'accumulated_logging_time': 3.831068515777588}
I0229 15:52:06.761065 140522605770496 logging_writer.py:48] [105695] accumulated_eval_time=1359.203210, accumulated_logging_time=3.831069, accumulated_submission_time=35755.166638, global_step=105695, preemption_count=0, score=35755.166638, test/accuracy=0.566200, test/loss=1.964764, test/num_examples=10000, total_duration=37121.794483, train/accuracy=0.780134, train/loss=0.806925, validation/accuracy=0.695100, validation/loss=1.246316, validation/num_examples=50000
I0229 15:52:08.792145 140523092305664 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.5052168369293213, loss=1.3523590564727783
I0229 15:52:42.461547 140522605770496 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.7236666679382324, loss=1.352486491203308
I0229 15:53:16.263840 140523092305664 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.7412126064300537, loss=1.4383831024169922
I0229 15:53:50.035394 140522605770496 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.675733804702759, loss=1.3017573356628418
I0229 15:54:23.789874 140523092305664 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.692519187927246, loss=1.3888075351715088
I0229 15:54:57.550362 140522605770496 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.8674063682556152, loss=1.2275059223175049
I0229 15:55:31.422613 140523092305664 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.566448211669922, loss=1.2998082637786865
I0229 15:56:05.174473 140522605770496 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.627039909362793, loss=1.288440465927124
I0229 15:56:38.951658 140523092305664 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.2949106693267822, loss=1.2731419801712036
I0229 15:57:12.720191 140522605770496 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.5694801807403564, loss=1.3114159107208252
I0229 15:57:46.540086 140523092305664 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.4549670219421387, loss=1.2977553606033325
I0229 15:58:20.290885 140522605770496 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.6375107765197754, loss=1.3940930366516113
I0229 15:58:54.056748 140523092305664 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.5742404460906982, loss=1.2838011980056763
I0229 15:59:27.803878 140522605770496 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.881934881210327, loss=1.3491876125335693
I0229 16:00:01.581334 140523092305664 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.584975242614746, loss=1.3399497270584106
I0229 16:00:35.343050 140522605770496 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.660125494003296, loss=1.3743795156478882
I0229 16:00:36.839850 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:00:43.003250 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:00:51.777878 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:00:54.048154 140688601454400 submission_runner.py:411] Time since start: 37649.12s, 	Step: 107206, 	{'train/accuracy': 0.7763273119926453, 'train/loss': 0.8324143290519714, 'validation/accuracy': 0.6882799863815308, 'validation/loss': 1.2644940614700317, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 2.0364840030670166, 'test/num_examples': 10000, 'score': 36265.1814968586, 'total_duration': 37649.1190032959, 'accumulated_submission_time': 36265.1814968586, 'accumulated_eval_time': 1376.4114780426025, 'accumulated_logging_time': 3.880370616912842}
I0229 16:00:54.082710 140525264946944 logging_writer.py:48] [107206] accumulated_eval_time=1376.411478, accumulated_logging_time=3.880371, accumulated_submission_time=36265.181497, global_step=107206, preemption_count=0, score=36265.181497, test/accuracy=0.554900, test/loss=2.036484, test/num_examples=10000, total_duration=37649.119003, train/accuracy=0.776327, train/loss=0.832414, validation/accuracy=0.688280, validation/loss=1.264494, validation/num_examples=50000
I0229 16:01:26.233247 140525273339648 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.711341619491577, loss=1.4441032409667969
I0229 16:01:59.974000 140525264946944 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.7949743270874023, loss=1.4344067573547363
I0229 16:02:33.723592 140525273339648 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.8915445804595947, loss=1.342602252960205
I0229 16:03:07.512146 140525264946944 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.584251642227173, loss=1.2539644241333008
I0229 16:03:41.282155 140525273339648 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.8304991722106934, loss=1.36879563331604
I0229 16:04:15.068450 140525264946944 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.6789791584014893, loss=1.3060016632080078
I0229 16:04:48.853350 140525273339648 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.428152322769165, loss=1.2078514099121094
I0229 16:05:22.631502 140525264946944 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.4158904552459717, loss=1.283124566078186
I0229 16:05:56.429030 140525273339648 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.812492847442627, loss=1.3217658996582031
I0229 16:06:30.201927 140525264946944 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.5427093505859375, loss=1.290436029434204
I0229 16:07:03.933648 140525273339648 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.390129804611206, loss=1.2840999364852905
I0229 16:07:37.759839 140525264946944 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.934352159500122, loss=1.2807673215866089
I0229 16:08:11.490148 140525273339648 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.4052727222442627, loss=1.1436914205551147
I0229 16:08:45.268793 140525264946944 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.8164992332458496, loss=1.3146188259124756
I0229 16:09:19.062607 140525273339648 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6973023414611816, loss=1.1797513961791992
I0229 16:09:24.271748 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:09:30.378306 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:09:38.878166 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:09:41.159394 140688601454400 submission_runner.py:411] Time since start: 38176.23s, 	Step: 108717, 	{'train/accuracy': 0.8256935477256775, 'train/loss': 0.6464024782180786, 'validation/accuracy': 0.6990599632263184, 'validation/loss': 1.226111888885498, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 1.9559884071350098, 'test/num_examples': 10000, 'score': 36775.30914545059, 'total_duration': 38176.230043172836, 'accumulated_submission_time': 36775.30914545059, 'accumulated_eval_time': 1393.2988758087158, 'accumulated_logging_time': 3.924722671508789}
I0229 16:09:41.199636 140522597377792 logging_writer.py:48] [108717] accumulated_eval_time=1393.298876, accumulated_logging_time=3.924723, accumulated_submission_time=36775.309145, global_step=108717, preemption_count=0, score=36775.309145, test/accuracy=0.561100, test/loss=1.955988, test/num_examples=10000, total_duration=38176.230043, train/accuracy=0.825694, train/loss=0.646402, validation/accuracy=0.699060, validation/loss=1.226112, validation/num_examples=50000
I0229 16:10:09.508206 140522605770496 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.8534839153289795, loss=1.2539536952972412
I0229 16:10:43.241023 140522597377792 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.631023645401001, loss=1.3766896724700928
I0229 16:11:17.039440 140522605770496 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.6378517150878906, loss=1.2622395753860474
I0229 16:11:50.756853 140522597377792 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.44675874710083, loss=1.2863445281982422
I0229 16:12:24.495830 140522605770496 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.931164264678955, loss=1.2935787439346313
I0229 16:12:58.260178 140522597377792 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.474487781524658, loss=1.1236841678619385
I0229 16:13:32.001573 140522605770496 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.8081893920898438, loss=1.313671588897705
I0229 16:14:05.810932 140522597377792 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.477290153503418, loss=1.3133474588394165
I0229 16:14:39.565805 140522605770496 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.7709665298461914, loss=1.2654598951339722
I0229 16:15:13.345858 140522597377792 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.555718421936035, loss=1.3254057168960571
I0229 16:15:47.118628 140522605770496 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.5728092193603516, loss=1.1675536632537842
I0229 16:16:20.887284 140522597377792 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.5511727333068848, loss=1.3903241157531738
I0229 16:16:54.670286 140522605770496 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.7905197143554688, loss=1.2783104181289673
I0229 16:17:28.428714 140522597377792 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.621077060699463, loss=1.2521518468856812
I0229 16:18:02.152019 140522605770496 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.6239161491394043, loss=1.3658170700073242
I0229 16:18:11.432867 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:18:17.561120 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:18:26.267148 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:18:28.599543 140688601454400 submission_runner.py:411] Time since start: 38703.67s, 	Step: 110229, 	{'train/accuracy': 0.8019770383834839, 'train/loss': 0.727361261844635, 'validation/accuracy': 0.6938199996948242, 'validation/loss': 1.250580072402954, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9688000679016113, 'test/num_examples': 10000, 'score': 37285.4779791832, 'total_duration': 38703.67038869858, 'accumulated_submission_time': 37285.4779791832, 'accumulated_eval_time': 1410.4655022621155, 'accumulated_logging_time': 3.9767305850982666}
I0229 16:18:28.634975 140525273339648 logging_writer.py:48] [110229] accumulated_eval_time=1410.465502, accumulated_logging_time=3.976731, accumulated_submission_time=37285.477979, global_step=110229, preemption_count=0, score=37285.477979, test/accuracy=0.565200, test/loss=1.968800, test/num_examples=10000, total_duration=38703.670389, train/accuracy=0.801977, train/loss=0.727361, validation/accuracy=0.693820, validation/loss=1.250580, validation/num_examples=50000
I0229 16:18:52.895162 140525281732352 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.9485626220703125, loss=1.353384017944336
I0229 16:19:26.628197 140525273339648 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.3178257942199707, loss=1.2115333080291748
I0229 16:20:00.483715 140525281732352 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.7519290447235107, loss=1.3533889055252075
I0229 16:20:34.257939 140525273339648 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.695284366607666, loss=1.3549641370773315
I0229 16:21:08.046245 140525281732352 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.538825511932373, loss=1.2232650518417358
I0229 16:21:41.834875 140525273339648 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.5353453159332275, loss=1.2564866542816162
I0229 16:22:15.648891 140525281732352 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.582460403442383, loss=1.2022454738616943
I0229 16:22:49.446223 140525273339648 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.737978935241699, loss=1.261155366897583
I0229 16:23:23.229269 140525281732352 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.6723461151123047, loss=1.2496379613876343
I0229 16:23:57.020153 140525273339648 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.78481388092041, loss=1.3371963500976562
I0229 16:24:30.804727 140525281732352 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.5418050289154053, loss=1.2755602598190308
I0229 16:25:04.592753 140525273339648 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.605896472930908, loss=1.3038536310195923
I0229 16:25:38.358726 140525281732352 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.5562551021575928, loss=1.202241063117981
I0229 16:26:12.200837 140525273339648 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.7162420749664307, loss=1.2269561290740967
I0229 16:26:45.978621 140525281732352 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.6624059677124023, loss=1.2086971998214722
I0229 16:26:58.625227 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:27:04.891438 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:27:13.379102 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:27:15.703328 140688601454400 submission_runner.py:411] Time since start: 39230.77s, 	Step: 111739, 	{'train/accuracy': 0.80765700340271, 'train/loss': 0.6981836557388306, 'validation/accuracy': 0.700939953327179, 'validation/loss': 1.21205735206604, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.9357373714447021, 'test/num_examples': 10000, 'score': 37795.405812978745, 'total_duration': 39230.77417373657, 'accumulated_submission_time': 37795.405812978745, 'accumulated_eval_time': 1427.5435523986816, 'accumulated_logging_time': 4.02356743812561}
I0229 16:27:15.740465 140522597377792 logging_writer.py:48] [111739] accumulated_eval_time=1427.543552, accumulated_logging_time=4.023567, accumulated_submission_time=37795.405813, global_step=111739, preemption_count=0, score=37795.405813, test/accuracy=0.571900, test/loss=1.935737, test/num_examples=10000, total_duration=39230.774174, train/accuracy=0.807657, train/loss=0.698184, validation/accuracy=0.700940, validation/loss=1.212057, validation/num_examples=50000
I0229 16:27:36.674203 140522605770496 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.8641598224639893, loss=1.3555541038513184
I0229 16:28:10.386556 140522597377792 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.7000699043273926, loss=1.2984256744384766
I0229 16:28:44.141339 140522605770496 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.7356882095336914, loss=1.2042042016983032
I0229 16:29:17.844682 140522597377792 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.656325340270996, loss=1.2062046527862549
I0229 16:29:51.612235 140522605770496 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.5343098640441895, loss=1.2229783535003662
I0229 16:30:25.412248 140522597377792 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.9501445293426514, loss=1.2856149673461914
I0229 16:30:59.192329 140522605770496 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.645991086959839, loss=1.23386549949646
I0229 16:31:33.008283 140522597377792 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.6733248233795166, loss=1.26823890209198
I0229 16:32:06.824111 140522605770496 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.6095893383026123, loss=1.263241171836853
I0229 16:32:40.648926 140522597377792 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.5217912197113037, loss=1.189127802848816
I0229 16:33:14.439047 140522605770496 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.6800334453582764, loss=1.2137115001678467
I0229 16:33:48.247091 140522597377792 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.896442413330078, loss=1.3036504983901978
I0229 16:34:22.001425 140522605770496 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.7951700687408447, loss=1.2949540615081787
I0229 16:34:55.754107 140522597377792 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.7747297286987305, loss=1.2322661876678467
I0229 16:35:29.520915 140522605770496 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.8056881427764893, loss=1.268593430519104
I0229 16:35:45.891515 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:35:52.039359 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:36:00.584625 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:36:02.853804 140688601454400 submission_runner.py:411] Time since start: 39757.92s, 	Step: 113250, 	{'train/accuracy': 0.8006218075752258, 'train/loss': 0.7307600378990173, 'validation/accuracy': 0.7016599774360657, 'validation/loss': 1.2059155702590942, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.932706356048584, 'test/num_examples': 10000, 'score': 38305.49462866783, 'total_duration': 39757.924556970596, 'accumulated_submission_time': 38305.49462866783, 'accumulated_eval_time': 1444.5057072639465, 'accumulated_logging_time': 4.071113586425781}
I0229 16:36:02.890917 140525273339648 logging_writer.py:48] [113250] accumulated_eval_time=1444.505707, accumulated_logging_time=4.071114, accumulated_submission_time=38305.494629, global_step=113250, preemption_count=0, score=38305.494629, test/accuracy=0.566800, test/loss=1.932706, test/num_examples=10000, total_duration=39757.924557, train/accuracy=0.800622, train/loss=0.730760, validation/accuracy=0.701660, validation/loss=1.205916, validation/num_examples=50000
I0229 16:36:20.091507 140525290125056 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.678279161453247, loss=1.2517774105072021
I0229 16:36:53.840636 140525273339648 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.7273354530334473, loss=1.322044849395752
I0229 16:37:27.594569 140525290125056 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.6011345386505127, loss=1.1936867237091064
I0229 16:38:01.463002 140525273339648 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8576388359069824, loss=1.3156870603561401
I0229 16:38:35.286054 140525290125056 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.931187391281128, loss=1.27354097366333
I0229 16:39:09.057560 140525273339648 logging_writer.py:48] [113800] global_step=113800, grad_norm=3.0279247760772705, loss=1.238763451576233
I0229 16:39:42.865891 140525290125056 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.8607656955718994, loss=1.239449143409729
I0229 16:40:16.600841 140525273339648 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.8476314544677734, loss=1.232582449913025
I0229 16:40:50.359471 140525290125056 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.89573335647583, loss=1.2500262260437012
I0229 16:41:24.148453 140525273339648 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.9112179279327393, loss=1.2281889915466309
I0229 16:41:57.922996 140525290125056 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.9335761070251465, loss=1.2530115842819214
I0229 16:42:31.728537 140525273339648 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.8278725147247314, loss=1.330643653869629
I0229 16:43:05.506426 140525290125056 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.886223077774048, loss=1.2660768032073975
I0229 16:43:39.269429 140525273339648 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.9453957080841064, loss=1.1784754991531372
I0229 16:44:13.146058 140525290125056 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.6850533485412598, loss=1.3161803483963013
I0229 16:44:32.883827 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:44:39.082114 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:44:47.578232 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:44:49.853377 140688601454400 submission_runner.py:411] Time since start: 40284.92s, 	Step: 114760, 	{'train/accuracy': 0.7948819994926453, 'train/loss': 0.7492088079452515, 'validation/accuracy': 0.6979399919509888, 'validation/loss': 1.2191317081451416, 'validation/num_examples': 50000, 'test/accuracy': 0.5746999979019165, 'test/loss': 1.9173393249511719, 'test/num_examples': 10000, 'score': 38815.42671918869, 'total_duration': 40284.924193143845, 'accumulated_submission_time': 38815.42671918869, 'accumulated_eval_time': 1461.475175857544, 'accumulated_logging_time': 4.1177613735198975}
I0229 16:44:49.889343 140523947947776 logging_writer.py:48] [114760] accumulated_eval_time=1461.475176, accumulated_logging_time=4.117761, accumulated_submission_time=38815.426719, global_step=114760, preemption_count=0, score=38815.426719, test/accuracy=0.574700, test/loss=1.917339, test/num_examples=10000, total_duration=40284.924193, train/accuracy=0.794882, train/loss=0.749209, validation/accuracy=0.697940, validation/loss=1.219132, validation/num_examples=50000
I0229 16:45:03.746276 140525256554240 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.6993184089660645, loss=1.1888374090194702
I0229 16:45:37.467533 140523947947776 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.657630443572998, loss=1.2359275817871094
I0229 16:46:11.272378 140525256554240 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.7121033668518066, loss=1.2353942394256592
I0229 16:46:45.054380 140523947947776 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.693939447402954, loss=1.2729228734970093
I0229 16:47:18.845108 140525256554240 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.855966091156006, loss=1.2206144332885742
I0229 16:47:52.621371 140523947947776 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.8511602878570557, loss=1.196082592010498
I0229 16:48:26.420687 140525256554240 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.0600924491882324, loss=1.3044297695159912
I0229 16:49:00.217341 140523947947776 logging_writer.py:48] [115500] global_step=115500, grad_norm=3.008470058441162, loss=1.358696460723877
I0229 16:49:34.020655 140525256554240 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.910402297973633, loss=1.2272930145263672
I0229 16:50:07.872434 140523947947776 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.8632566928863525, loss=1.3167859315872192
I0229 16:50:41.621141 140525256554240 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.557093858718872, loss=1.1488748788833618
I0229 16:51:15.437467 140523947947776 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.9210453033447266, loss=1.255130410194397
I0229 16:51:49.226154 140525256554240 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.804260730743408, loss=1.1745457649230957
I0229 16:52:22.995413 140523947947776 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.920888662338257, loss=1.3062264919281006
I0229 16:52:56.796939 140525256554240 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.90523362159729, loss=1.2516402006149292
I0229 16:53:19.917992 140688601454400 spec.py:321] Evaluating on the training split.
I0229 16:53:26.075163 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 16:53:34.818092 140688601454400 spec.py:349] Evaluating on the test split.
I0229 16:53:37.081499 140688601454400 submission_runner.py:411] Time since start: 40812.15s, 	Step: 116270, 	{'train/accuracy': 0.7964963316917419, 'train/loss': 0.7375555038452148, 'validation/accuracy': 0.7020999789237976, 'validation/loss': 1.2137975692749023, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 1.9192442893981934, 'test/num_examples': 10000, 'score': 39325.390781879425, 'total_duration': 40812.15231490135, 'accumulated_submission_time': 39325.390781879425, 'accumulated_eval_time': 1478.6386225223541, 'accumulated_logging_time': 4.166247367858887}
I0229 16:53:37.118028 140522597377792 logging_writer.py:48] [116270] accumulated_eval_time=1478.638623, accumulated_logging_time=4.166247, accumulated_submission_time=39325.390782, global_step=116270, preemption_count=0, score=39325.390782, test/accuracy=0.581400, test/loss=1.919244, test/num_examples=10000, total_duration=40812.152315, train/accuracy=0.796496, train/loss=0.737556, validation/accuracy=0.702100, validation/loss=1.213798, validation/num_examples=50000
I0229 16:53:47.574228 140522605770496 logging_writer.py:48] [116300] global_step=116300, grad_norm=3.018190383911133, loss=1.2498102188110352
I0229 16:54:21.314642 140522597377792 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.9822096824645996, loss=1.1165380477905273
I0229 16:54:55.051966 140522605770496 logging_writer.py:48] [116500] global_step=116500, grad_norm=3.062133550643921, loss=1.203312635421753
I0229 16:55:28.857555 140522597377792 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.7416892051696777, loss=1.1443310976028442
I0229 16:56:02.598267 140522605770496 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.8978471755981445, loss=1.196972370147705
I0229 16:56:36.418790 140522597377792 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.774752378463745, loss=1.2431905269622803
I0229 16:57:10.161065 140522605770496 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.014801025390625, loss=1.2320928573608398
I0229 16:57:43.939466 140522597377792 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.6060631275177, loss=1.2203187942504883
I0229 16:58:17.725609 140522605770496 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.8048975467681885, loss=1.286333680152893
I0229 16:58:51.485565 140522597377792 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.72104811668396, loss=1.1264808177947998
I0229 16:59:25.248826 140522605770496 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.237778902053833, loss=1.3089218139648438
I0229 16:59:59.027949 140522597377792 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.7293686866760254, loss=1.1849846839904785
I0229 17:00:32.812669 140522605770496 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.948270082473755, loss=1.1983641386032104
I0229 17:01:06.600268 140522597377792 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.799696207046509, loss=1.1524256467819214
I0229 17:01:40.420485 140522605770496 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.0571322441101074, loss=1.2882778644561768
I0229 17:02:07.230575 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:02:13.761837 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:02:22.294015 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:02:24.597984 140688601454400 submission_runner.py:411] Time since start: 41339.67s, 	Step: 117781, 	{'train/accuracy': 0.8391461968421936, 'train/loss': 0.5893431305885315, 'validation/accuracy': 0.702299952507019, 'validation/loss': 1.215503215789795, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.9403326511383057, 'test/num_examples': 10000, 'score': 39835.441056251526, 'total_duration': 41339.66882777214, 'accumulated_submission_time': 39835.441056251526, 'accumulated_eval_time': 1496.0060350894928, 'accumulated_logging_time': 4.21298360824585}
I0229 17:02:24.635383 140523947947776 logging_writer.py:48] [117781] accumulated_eval_time=1496.006035, accumulated_logging_time=4.212984, accumulated_submission_time=39835.441056, global_step=117781, preemption_count=0, score=39835.441056, test/accuracy=0.573600, test/loss=1.940333, test/num_examples=10000, total_duration=41339.668828, train/accuracy=0.839146, train/loss=0.589343, validation/accuracy=0.702300, validation/loss=1.215503, validation/num_examples=50000
I0229 17:02:31.387653 140525256554240 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.9014508724212646, loss=1.15811288356781
I0229 17:03:05.109587 140523947947776 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.836824655532837, loss=1.2097276449203491
I0229 17:03:38.865854 140525256554240 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.880146026611328, loss=1.21116042137146
I0229 17:04:12.636900 140523947947776 logging_writer.py:48] [118100] global_step=118100, grad_norm=3.023247241973877, loss=1.2204681634902954
I0229 17:04:46.398007 140525256554240 logging_writer.py:48] [118200] global_step=118200, grad_norm=3.060410261154175, loss=1.2253936529159546
I0229 17:05:20.171053 140523947947776 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.1231582164764404, loss=1.1891371011734009
I0229 17:05:53.943597 140525256554240 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.9190657138824463, loss=1.170170783996582
I0229 17:06:27.725164 140523947947776 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.8660976886749268, loss=1.1790472269058228
I0229 17:07:01.522159 140525256554240 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.760709047317505, loss=1.1606793403625488
I0229 17:07:35.322528 140523947947776 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.962202548980713, loss=1.2651286125183105
I0229 17:08:09.109745 140525256554240 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.678722620010376, loss=1.1637887954711914
I0229 17:08:42.984402 140523947947776 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.936135768890381, loss=1.2081156969070435
I0229 17:09:16.761448 140525256554240 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.850362539291382, loss=1.1676838397979736
I0229 17:09:50.520377 140523947947776 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.989323616027832, loss=1.177775502204895
I0229 17:10:24.320072 140525256554240 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.9177799224853516, loss=1.2380433082580566
I0229 17:10:54.860918 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:11:00.977733 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:11:09.814199 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:11:12.126174 140688601454400 submission_runner.py:411] Time since start: 41867.20s, 	Step: 119292, 	{'train/accuracy': 0.8243981003761292, 'train/loss': 0.6250141263008118, 'validation/accuracy': 0.7067999839782715, 'validation/loss': 1.1909725666046143, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.933236002922058, 'test/num_examples': 10000, 'score': 40345.60301208496, 'total_duration': 41867.19692969322, 'accumulated_submission_time': 40345.60301208496, 'accumulated_eval_time': 1513.2711565494537, 'accumulated_logging_time': 4.26213812828064}
I0229 17:11:12.171140 140522588985088 logging_writer.py:48] [119292] accumulated_eval_time=1513.271157, accumulated_logging_time=4.262138, accumulated_submission_time=40345.603012, global_step=119292, preemption_count=0, score=40345.603012, test/accuracy=0.577900, test/loss=1.933236, test/num_examples=10000, total_duration=41867.196930, train/accuracy=0.824398, train/loss=0.625014, validation/accuracy=0.706800, validation/loss=1.190973, validation/num_examples=50000
I0229 17:11:15.205043 140522597377792 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.844200372695923, loss=1.0595227479934692
I0229 17:11:48.924874 140522588985088 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.927060604095459, loss=1.2748010158538818
I0229 17:12:22.674903 140522597377792 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.8046326637268066, loss=1.1849356889724731
I0229 17:12:56.443156 140522588985088 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.8585574626922607, loss=1.2031134366989136
I0229 17:13:30.225987 140522597377792 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.6645638942718506, loss=1.186493992805481
I0229 17:14:03.973535 140522588985088 logging_writer.py:48] [119800] global_step=119800, grad_norm=3.0157437324523926, loss=1.220043420791626
I0229 17:14:37.799006 140522597377792 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.114485502243042, loss=1.1870956420898438
I0229 17:15:11.556113 140522588985088 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.696885585784912, loss=1.1451421976089478
I0229 17:15:45.322495 140522597377792 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.929713249206543, loss=1.1979085206985474
I0229 17:16:19.040065 140522588985088 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.8712217807769775, loss=1.2353745698928833
I0229 17:16:52.820450 140522597377792 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.923534870147705, loss=1.239128589630127
I0229 17:17:26.633938 140522588985088 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.91689395904541, loss=1.1435959339141846
I0229 17:18:00.415967 140522597377792 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.1355037689208984, loss=1.0941276550292969
I0229 17:18:34.219178 140522588985088 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.879384756088257, loss=1.2301691770553589
I0229 17:19:07.977910 140522597377792 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.9326441287994385, loss=1.1582791805267334
I0229 17:19:41.738678 140522588985088 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.8091821670532227, loss=1.175854206085205
I0229 17:19:42.212748 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:19:48.354082 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:19:57.005355 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:19:59.297017 140688601454400 submission_runner.py:411] Time since start: 42394.37s, 	Step: 120803, 	{'train/accuracy': 0.8103874325752258, 'train/loss': 0.6829017996788025, 'validation/accuracy': 0.7016199827194214, 'validation/loss': 1.216865062713623, 'validation/num_examples': 50000, 'test/accuracy': 0.5700000524520874, 'test/loss': 1.950163722038269, 'test/num_examples': 10000, 'score': 40855.583112478256, 'total_duration': 42394.367864370346, 'accumulated_submission_time': 40855.583112478256, 'accumulated_eval_time': 1530.355375289917, 'accumulated_logging_time': 4.317422151565552}
I0229 17:19:59.337358 140525256554240 logging_writer.py:48] [120803] accumulated_eval_time=1530.355375, accumulated_logging_time=4.317422, accumulated_submission_time=40855.583112, global_step=120803, preemption_count=0, score=40855.583112, test/accuracy=0.570000, test/loss=1.950164, test/num_examples=10000, total_duration=42394.367864, train/accuracy=0.810387, train/loss=0.682902, validation/accuracy=0.701620, validation/loss=1.216865, validation/num_examples=50000
I0229 17:20:32.419128 140525264946944 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.930164098739624, loss=1.1391735076904297
I0229 17:21:06.241940 140525256554240 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.9404923915863037, loss=1.2289506196975708
I0229 17:21:40.003941 140525264946944 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.199249029159546, loss=1.2638462781906128
I0229 17:22:13.764201 140525256554240 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.9966492652893066, loss=1.1999397277832031
I0229 17:22:47.553331 140525264946944 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.8444652557373047, loss=1.1394370794296265
I0229 17:23:21.310031 140525256554240 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.9053874015808105, loss=1.182363748550415
I0229 17:23:55.088696 140525264946944 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.838484048843384, loss=1.1158640384674072
I0229 17:24:28.853819 140525256554240 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.9354000091552734, loss=1.2042771577835083
I0229 17:25:02.656706 140525264946944 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.7255022525787354, loss=1.0857181549072266
I0229 17:25:36.397511 140525256554240 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.80808424949646, loss=1.1535096168518066
I0229 17:26:10.176447 140525264946944 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.0261969566345215, loss=1.1539571285247803
I0229 17:26:44.027620 140525256554240 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.1437692642211914, loss=1.1649670600891113
I0229 17:27:17.788789 140525264946944 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.8391952514648438, loss=1.126470923423767
I0229 17:27:51.570152 140525256554240 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.887094497680664, loss=1.0970613956451416
I0229 17:28:25.335054 140525264946944 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.9430038928985596, loss=1.2331796884536743
I0229 17:28:29.523642 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:28:35.622170 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:28:44.128621 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:28:46.401413 140688601454400 submission_runner.py:411] Time since start: 42921.47s, 	Step: 122314, 	{'train/accuracy': 0.8113440275192261, 'train/loss': 0.6753456592559814, 'validation/accuracy': 0.7064399719238281, 'validation/loss': 1.2009263038635254, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.9371180534362793, 'test/num_examples': 10000, 'score': 41365.70845270157, 'total_duration': 42921.472259521484, 'accumulated_submission_time': 41365.70845270157, 'accumulated_eval_time': 1547.2330920696259, 'accumulated_logging_time': 4.367457628250122}
I0229 17:28:46.439591 140523092305664 logging_writer.py:48] [122314] accumulated_eval_time=1547.233092, accumulated_logging_time=4.367458, accumulated_submission_time=41365.708453, global_step=122314, preemption_count=0, score=41365.708453, test/accuracy=0.578100, test/loss=1.937118, test/num_examples=10000, total_duration=42921.472260, train/accuracy=0.811344, train/loss=0.675346, validation/accuracy=0.706440, validation/loss=1.200926, validation/num_examples=50000
I0229 17:29:15.769053 140523947947776 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.9853949546813965, loss=1.1294052600860596
I0229 17:29:49.504127 140523092305664 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.032773494720459, loss=1.1718106269836426
I0229 17:30:23.254552 140523947947776 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.1306910514831543, loss=1.3196725845336914
I0229 17:30:57.061861 140523092305664 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.1506941318511963, loss=1.1492196321487427
I0229 17:31:30.807816 140523947947776 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.8390378952026367, loss=1.0478262901306152
I0229 17:32:04.586717 140523092305664 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.1257190704345703, loss=1.1899832487106323
I0229 17:32:38.353961 140523947947776 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.9597294330596924, loss=1.122772216796875
I0229 17:33:12.190695 140523092305664 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.197288990020752, loss=1.2642090320587158
I0229 17:33:45.977138 140523947947776 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.914667844772339, loss=1.075692892074585
I0229 17:34:19.730630 140523092305664 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.257267475128174, loss=1.2468256950378418
I0229 17:34:53.520897 140523947947776 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.9434704780578613, loss=1.1949063539505005
I0229 17:35:27.305951 140523092305664 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.9553723335266113, loss=1.1102954149246216
I0229 17:36:01.060114 140523947947776 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.2247633934020996, loss=1.276674509048462
I0229 17:36:34.847187 140523092305664 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.3450376987457275, loss=1.1857144832611084
I0229 17:37:08.605382 140523947947776 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.312464714050293, loss=1.1651639938354492
I0229 17:37:16.504048 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:37:22.763620 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:37:31.386943 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:37:33.677129 140688601454400 submission_runner.py:411] Time since start: 43448.75s, 	Step: 123825, 	{'train/accuracy': 0.8163862824440002, 'train/loss': 0.6607792973518372, 'validation/accuracy': 0.7098199725151062, 'validation/loss': 1.1880916357040405, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 1.9101765155792236, 'test/num_examples': 10000, 'score': 41875.71085715294, 'total_duration': 43448.74796462059, 'accumulated_submission_time': 41875.71085715294, 'accumulated_eval_time': 1564.4061267375946, 'accumulated_logging_time': 4.415964365005493}
I0229 17:37:33.717422 140525256554240 logging_writer.py:48] [123825] accumulated_eval_time=1564.406127, accumulated_logging_time=4.415964, accumulated_submission_time=41875.710857, global_step=123825, preemption_count=0, score=41875.710857, test/accuracy=0.585500, test/loss=1.910177, test/num_examples=10000, total_duration=43448.747965, train/accuracy=0.816386, train/loss=0.660779, validation/accuracy=0.709820, validation/loss=1.188092, validation/num_examples=50000
I0229 17:37:59.376345 140525264946944 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.4855048656463623, loss=1.1849634647369385
I0229 17:38:33.067914 140525256554240 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.1875667572021484, loss=1.1518386602401733
I0229 17:39:06.913522 140525264946944 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.971527338027954, loss=1.130267858505249
I0229 17:39:40.692300 140525256554240 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.913766860961914, loss=1.0716583728790283
I0229 17:40:14.442044 140525264946944 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.8767333030700684, loss=1.0137680768966675
I0229 17:40:48.258414 140525256554240 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.975008010864258, loss=1.2114739418029785
I0229 17:41:22.025852 140525264946944 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.154742956161499, loss=1.13875150680542
I0229 17:41:55.723774 140525256554240 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.313819169998169, loss=1.2147208452224731
I0229 17:42:29.479435 140525264946944 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.932220220565796, loss=1.0875409841537476
I0229 17:43:03.268080 140525256554240 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.9648244380950928, loss=1.1772114038467407
I0229 17:43:37.045928 140525264946944 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.1317479610443115, loss=1.150820255279541
I0229 17:44:10.774611 140525256554240 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.965073585510254, loss=1.0382460355758667
I0229 17:44:44.521800 140525264946944 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.0118188858032227, loss=1.0734848976135254
I0229 17:45:18.330454 140525256554240 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.9200034141540527, loss=1.0332199335098267
I0229 17:45:52.075197 140525264946944 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.195904493331909, loss=1.129726767539978
I0229 17:46:03.679084 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:46:09.904977 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:46:18.556602 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:46:20.829134 140688601454400 submission_runner.py:411] Time since start: 43975.90s, 	Step: 125336, 	{'train/accuracy': 0.8089126348495483, 'train/loss': 0.6776189208030701, 'validation/accuracy': 0.7065799832344055, 'validation/loss': 1.1986366510391235, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.9390764236450195, 'test/num_examples': 10000, 'score': 42385.60908794403, 'total_duration': 43975.89997935295, 'accumulated_submission_time': 42385.60908794403, 'accumulated_eval_time': 1581.5561335086823, 'accumulated_logging_time': 4.467309474945068}
I0229 17:46:20.867477 140522605770496 logging_writer.py:48] [125336] accumulated_eval_time=1581.556134, accumulated_logging_time=4.467309, accumulated_submission_time=42385.609088, global_step=125336, preemption_count=0, score=42385.609088, test/accuracy=0.580500, test/loss=1.939076, test/num_examples=10000, total_duration=43975.899979, train/accuracy=0.808913, train/loss=0.677619, validation/accuracy=0.706580, validation/loss=1.198637, validation/num_examples=50000
I0229 17:46:42.812743 140523092305664 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.024845600128174, loss=1.1625332832336426
I0229 17:47:16.547328 140522605770496 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.4985122680664062, loss=1.1277656555175781
I0229 17:47:50.302183 140523092305664 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.872399091720581, loss=1.0860768556594849
I0229 17:48:24.074067 140522605770496 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.987422466278076, loss=1.2003023624420166
I0229 17:48:57.848368 140523092305664 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.0203003883361816, loss=1.0392273664474487
I0229 17:49:31.609759 140522605770496 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.449068069458008, loss=1.1489418745040894
I0229 17:50:05.364497 140523092305664 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.072136878967285, loss=1.1077231168746948
I0229 17:50:39.126199 140522605770496 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.040482759475708, loss=1.0970436334609985
I0229 17:51:12.865143 140523092305664 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.0837562084198, loss=1.2013567686080933
I0229 17:51:46.726450 140522605770496 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.9260246753692627, loss=1.086698293685913
I0229 17:52:20.468343 140523092305664 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.9954473972320557, loss=1.1532447338104248
I0229 17:52:54.224715 140522605770496 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.8886852264404297, loss=1.051302433013916
I0229 17:53:27.990250 140523092305664 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.070150852203369, loss=1.1283416748046875
I0229 17:54:01.742110 140522605770496 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.2666923999786377, loss=1.0912039279937744
I0229 17:54:35.511177 140523092305664 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.392106294631958, loss=1.2913239002227783
I0229 17:54:50.833155 140688601454400 spec.py:321] Evaluating on the training split.
I0229 17:54:56.944843 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 17:55:05.737282 140688601454400 spec.py:349] Evaluating on the test split.
I0229 17:55:08.063994 140688601454400 submission_runner.py:411] Time since start: 44503.13s, 	Step: 126847, 	{'train/accuracy': 0.8555285334587097, 'train/loss': 0.5188804268836975, 'validation/accuracy': 0.7141599655151367, 'validation/loss': 1.1573100090026855, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.8532426357269287, 'test/num_examples': 10000, 'score': 42895.51242017746, 'total_duration': 44503.13484168053, 'accumulated_submission_time': 42895.51242017746, 'accumulated_eval_time': 1598.7869279384613, 'accumulated_logging_time': 4.516381502151489}
I0229 17:55:08.103338 140522597377792 logging_writer.py:48] [126847] accumulated_eval_time=1598.786928, accumulated_logging_time=4.516382, accumulated_submission_time=42895.512420, global_step=126847, preemption_count=0, score=42895.512420, test/accuracy=0.592400, test/loss=1.853243, test/num_examples=10000, total_duration=44503.134842, train/accuracy=0.855529, train/loss=0.518880, validation/accuracy=0.714160, validation/loss=1.157310, validation/num_examples=50000
I0229 17:55:26.317198 140522605770496 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.021859645843506, loss=1.016860008239746
I0229 17:56:00.033812 140522597377792 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.0007569789886475, loss=1.056112289428711
I0229 17:56:33.775612 140522605770496 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.0946860313415527, loss=1.221379280090332
I0229 17:57:07.763076 140522597377792 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.1430888175964355, loss=1.051213264465332
I0229 17:57:41.728126 140522605770496 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.210974931716919, loss=1.145015001296997
I0229 17:58:15.489038 140522597377792 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.283797025680542, loss=1.1309900283813477
I0229 17:58:49.253569 140522605770496 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.327890634536743, loss=1.133258581161499
I0229 17:59:22.980923 140522597377792 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.428029775619507, loss=1.219618320465088
I0229 17:59:56.726732 140522605770496 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.2841780185699463, loss=1.149099588394165
I0229 18:00:30.483130 140522597377792 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.4467060565948486, loss=1.198197603225708
I0229 18:01:04.198750 140522605770496 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.1851892471313477, loss=1.2248929738998413
I0229 18:01:37.937922 140522597377792 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.8872694969177246, loss=1.0307092666625977
I0229 18:02:11.754454 140522605770496 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.014580011367798, loss=1.1613520383834839
I0229 18:02:45.535585 140522597377792 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.5093817710876465, loss=1.1160858869552612
I0229 18:03:19.328115 140522605770496 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.38987135887146, loss=1.162279725074768
I0229 18:03:38.124052 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:03:44.222890 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:03:52.991788 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:03:55.227117 140688601454400 submission_runner.py:411] Time since start: 45030.30s, 	Step: 128357, 	{'train/accuracy': 0.8460618257522583, 'train/loss': 0.5457704067230225, 'validation/accuracy': 0.7174400091171265, 'validation/loss': 1.141788363456726, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8696740865707397, 'test/num_examples': 10000, 'score': 43405.469561100006, 'total_duration': 45030.29795050621, 'accumulated_submission_time': 43405.469561100006, 'accumulated_eval_time': 1615.8899295330048, 'accumulated_logging_time': 4.567685127258301}
I0229 18:03:55.271602 140522597377792 logging_writer.py:48] [128357] accumulated_eval_time=1615.889930, accumulated_logging_time=4.567685, accumulated_submission_time=43405.469561, global_step=128357, preemption_count=0, score=43405.469561, test/accuracy=0.590700, test/loss=1.869674, test/num_examples=10000, total_duration=45030.297951, train/accuracy=0.846062, train/loss=0.545770, validation/accuracy=0.717440, validation/loss=1.141788, validation/num_examples=50000
I0229 18:04:10.156200 140522605770496 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.541252613067627, loss=1.0802922248840332
I0229 18:04:43.906821 140522597377792 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.9421656131744385, loss=1.0343576669692993
I0229 18:05:17.651591 140522605770496 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.315131187438965, loss=1.130785346031189
I0229 18:05:51.403601 140522597377792 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.3279032707214355, loss=1.1247780323028564
I0229 18:06:25.184384 140522605770496 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.607889413833618, loss=1.2078251838684082
I0229 18:06:58.940245 140522597377792 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.0479321479797363, loss=1.134891390800476
I0229 18:07:32.713608 140522605770496 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.2681078910827637, loss=1.1182851791381836
I0229 18:08:06.477391 140522597377792 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.295280694961548, loss=1.1270616054534912
I0229 18:08:40.270101 140522605770496 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.1833245754241943, loss=1.0145251750946045
I0229 18:09:14.015247 140522597377792 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.125232219696045, loss=1.095560073852539
I0229 18:09:47.790122 140522605770496 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.512057304382324, loss=1.0826222896575928
I0229 18:10:21.620491 140522597377792 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.1488208770751953, loss=1.1196385622024536
I0229 18:10:55.406369 140522605770496 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.612186908721924, loss=1.1618996858596802
I0229 18:11:29.162103 140522597377792 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.8761816024780273, loss=1.0409777164459229
I0229 18:12:02.934661 140522605770496 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.3138997554779053, loss=1.0699056386947632
I0229 18:12:25.341732 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:12:31.527071 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:12:40.240313 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:12:42.536680 140688601454400 submission_runner.py:411] Time since start: 45557.61s, 	Step: 129868, 	{'train/accuracy': 0.833426296710968, 'train/loss': 0.5857810974121094, 'validation/accuracy': 0.7130599617958069, 'validation/loss': 1.1719478368759155, 'validation/num_examples': 50000, 'test/accuracy': 0.5879999995231628, 'test/loss': 1.8958600759506226, 'test/num_examples': 10000, 'score': 43915.472928762436, 'total_duration': 45557.60752797127, 'accumulated_submission_time': 43915.472928762436, 'accumulated_eval_time': 1633.0848352909088, 'accumulated_logging_time': 4.626312971115112}
I0229 18:12:42.576894 140522588985088 logging_writer.py:48] [129868] accumulated_eval_time=1633.084835, accumulated_logging_time=4.626313, accumulated_submission_time=43915.472929, global_step=129868, preemption_count=0, score=43915.472929, test/accuracy=0.588000, test/loss=1.895860, test/num_examples=10000, total_duration=45557.607528, train/accuracy=0.833426, train/loss=0.585781, validation/accuracy=0.713060, validation/loss=1.171948, validation/num_examples=50000
I0229 18:12:53.691986 140522597377792 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.467280626296997, loss=1.1097923517227173
I0229 18:13:27.418233 140522588985088 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.271371603012085, loss=1.0790579319000244
I0229 18:14:01.143862 140522597377792 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.1905925273895264, loss=1.0461689233779907
I0229 18:14:34.886750 140522588985088 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.884936809539795, loss=1.0930051803588867
I0229 18:15:08.636305 140522597377792 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.0980868339538574, loss=1.0066510438919067
I0229 18:15:42.393440 140522588985088 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.224583864212036, loss=1.1231364011764526
I0229 18:16:16.229243 140522597377792 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.0875165462493896, loss=1.0826698541641235
I0229 18:16:50.017054 140522588985088 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.460836410522461, loss=1.142939567565918
I0229 18:17:23.776527 140522597377792 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.0653843879699707, loss=0.9775680303573608
I0229 18:17:57.520415 140522588985088 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.2263123989105225, loss=1.0819313526153564
I0229 18:18:31.289174 140522597377792 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.974788188934326, loss=1.0218907594680786
I0229 18:19:05.068097 140522588985088 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.2392101287841797, loss=1.1095659732818604
I0229 18:19:38.858650 140522597377792 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.25314998626709, loss=1.0538721084594727
I0229 18:20:12.649822 140522588985088 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.179455280303955, loss=1.054206132888794
I0229 18:20:46.415826 140522597377792 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.465932607650757, loss=0.9859750270843506
I0229 18:21:12.868779 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:21:18.938672 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:21:27.656558 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:21:29.957928 140688601454400 submission_runner.py:411] Time since start: 46085.03s, 	Step: 131380, 	{'train/accuracy': 0.8418765664100647, 'train/loss': 0.564940869808197, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.134805679321289, 'validation/num_examples': 50000, 'test/accuracy': 0.5936000347137451, 'test/loss': 1.8518024682998657, 'test/num_examples': 10000, 'score': 44425.701722860336, 'total_duration': 46085.02875685692, 'accumulated_submission_time': 44425.701722860336, 'accumulated_eval_time': 1650.1739237308502, 'accumulated_logging_time': 4.677285432815552}
I0229 18:21:30.000680 140522588985088 logging_writer.py:48] [131380] accumulated_eval_time=1650.173924, accumulated_logging_time=4.677285, accumulated_submission_time=44425.701723, global_step=131380, preemption_count=0, score=44425.701723, test/accuracy=0.593600, test/loss=1.851802, test/num_examples=10000, total_duration=46085.028757, train/accuracy=0.841877, train/loss=0.564941, validation/accuracy=0.720580, validation/loss=1.134806, validation/num_examples=50000
I0229 18:21:37.092324 140522597377792 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.2238802909851074, loss=1.0707463026046753
I0229 18:22:10.871301 140522588985088 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.4722537994384766, loss=1.1421958208084106
I0229 18:22:44.619232 140522597377792 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.160644292831421, loss=1.1018363237380981
I0229 18:23:18.325574 140522588985088 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.226550340652466, loss=0.9803885221481323
I0229 18:23:52.052726 140522597377792 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.8664143085479736, loss=1.0677015781402588
I0229 18:24:25.839618 140522588985088 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.29129695892334, loss=1.0653698444366455
I0229 18:24:59.607374 140522597377792 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.7887089252471924, loss=1.1685166358947754
I0229 18:25:33.357867 140522588985088 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.3344273567199707, loss=1.152024269104004
I0229 18:26:07.133424 140522597377792 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.4400722980499268, loss=1.0782583951950073
I0229 18:26:40.923725 140522588985088 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.175179958343506, loss=1.0298988819122314
I0229 18:27:14.688205 140522597377792 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.418891668319702, loss=1.0434205532073975
I0229 18:27:48.420504 140522588985088 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.507549524307251, loss=1.1584582328796387
I0229 18:28:22.216050 140522597377792 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.1177256107330322, loss=1.0285522937774658
I0229 18:28:55.989851 140522588985088 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.387676477432251, loss=1.0228756666183472
I0229 18:29:29.764640 140522597377792 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.142855644226074, loss=1.0190834999084473
I0229 18:30:00.002638 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:30:06.321710 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:30:15.012622 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:30:17.289133 140688601454400 submission_runner.py:411] Time since start: 46612.36s, 	Step: 132891, 	{'train/accuracy': 0.8443080186843872, 'train/loss': 0.5527390241622925, 'validation/accuracy': 0.7223599553108215, 'validation/loss': 1.1370505094528198, 'validation/num_examples': 50000, 'test/accuracy': 0.5984000563621521, 'test/loss': 1.8685274124145508, 'test/num_examples': 10000, 'score': 44935.64200139046, 'total_duration': 46612.3599793911, 'accumulated_submission_time': 44935.64200139046, 'accumulated_eval_time': 1667.4603853225708, 'accumulated_logging_time': 4.7297937870025635}
I0229 18:30:17.329671 140523947947776 logging_writer.py:48] [132891] accumulated_eval_time=1667.460385, accumulated_logging_time=4.729794, accumulated_submission_time=44935.642001, global_step=132891, preemption_count=0, score=44935.642001, test/accuracy=0.598400, test/loss=1.868527, test/num_examples=10000, total_duration=46612.359979, train/accuracy=0.844308, train/loss=0.552739, validation/accuracy=0.722360, validation/loss=1.137051, validation/num_examples=50000
I0229 18:30:20.704733 140525256554240 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.573345184326172, loss=1.0718175172805786
I0229 18:30:54.431339 140523947947776 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.30041241645813, loss=1.02508544921875
I0229 18:31:28.166690 140525256554240 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.4092507362365723, loss=1.0203602313995361
I0229 18:32:01.935462 140523947947776 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.28228759765625, loss=1.089561104774475
I0229 18:32:35.706995 140525256554240 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.369272470474243, loss=1.0689764022827148
I0229 18:33:09.448317 140523947947776 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.2810778617858887, loss=1.0194672346115112
I0229 18:33:43.198632 140525256554240 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.360018491744995, loss=1.0807905197143555
I0229 18:34:16.963859 140523947947776 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.502133369445801, loss=1.0880887508392334
I0229 18:34:50.706511 140525256554240 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.3135268688201904, loss=1.0784512758255005
I0229 18:35:24.480836 140523947947776 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.5633819103240967, loss=1.0072636604309082
I0229 18:35:58.270661 140525256554240 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.537919759750366, loss=1.096021056175232
I0229 18:36:32.057846 140523947947776 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.203160047531128, loss=0.9696149230003357
I0229 18:37:05.844341 140525256554240 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.3216397762298584, loss=1.0487092733383179
I0229 18:37:39.619304 140523947947776 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.4750561714172363, loss=1.0500953197479248
I0229 18:38:13.395210 140525256554240 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.8527207374572754, loss=1.0951110124588013
I0229 18:38:47.139360 140523947947776 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.3079848289489746, loss=1.0049643516540527
I0229 18:38:47.293682 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:38:53.397462 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:39:02.147105 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:39:04.443610 140688601454400 submission_runner.py:411] Time since start: 47139.51s, 	Step: 134402, 	{'train/accuracy': 0.8469586968421936, 'train/loss': 0.5429478883743286, 'validation/accuracy': 0.722599983215332, 'validation/loss': 1.1435267925262451, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.8864117860794067, 'test/num_examples': 10000, 'score': 45445.54392409325, 'total_duration': 47139.51445841789, 'accumulated_submission_time': 45445.54392409325, 'accumulated_eval_time': 1684.6102840900421, 'accumulated_logging_time': 4.779802560806274}
I0229 18:39:04.482759 140522605770496 logging_writer.py:48] [134402] accumulated_eval_time=1684.610284, accumulated_logging_time=4.779803, accumulated_submission_time=45445.543924, global_step=134402, preemption_count=0, score=45445.543924, test/accuracy=0.595700, test/loss=1.886412, test/num_examples=10000, total_duration=47139.514458, train/accuracy=0.846959, train/loss=0.542948, validation/accuracy=0.722600, validation/loss=1.143527, validation/num_examples=50000
I0229 18:39:37.866883 140523092305664 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.3307104110717773, loss=0.9914247989654541
I0229 18:40:11.551459 140522605770496 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2553296089172363, loss=1.0286000967025757
I0229 18:40:45.468222 140523092305664 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.7833430767059326, loss=1.1277966499328613
I0229 18:41:19.244316 140522605770496 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.374901294708252, loss=1.105116367340088
I0229 18:41:53.009647 140523092305664 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.667729616165161, loss=1.0670640468597412
I0229 18:42:26.724631 140522605770496 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.8905930519104004, loss=1.2043607234954834
I0229 18:43:00.494603 140523092305664 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.476447343826294, loss=1.0981000661849976
I0229 18:43:34.272213 140522605770496 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.6901354789733887, loss=1.0211400985717773
I0229 18:44:07.977631 140523092305664 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.1354212760925293, loss=1.0322381258010864
I0229 18:44:41.743308 140522605770496 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.5803470611572266, loss=0.9942867159843445
I0229 18:45:15.503220 140523092305664 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.4036519527435303, loss=1.0255659818649292
I0229 18:45:49.282817 140522605770496 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.6898880004882812, loss=1.1453888416290283
I0229 18:46:23.171942 140523092305664 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.242945909500122, loss=1.0507055521011353
I0229 18:46:56.971510 140522605770496 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.448044538497925, loss=0.973163366317749
I0229 18:47:30.709592 140523092305664 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.568836212158203, loss=1.0242619514465332
I0229 18:47:34.569598 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:47:40.780536 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:47:49.486473 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:47:51.775506 140688601454400 submission_runner.py:411] Time since start: 47666.85s, 	Step: 135913, 	{'train/accuracy': 0.879902720451355, 'train/loss': 0.428302139043808, 'validation/accuracy': 0.7254799604415894, 'validation/loss': 1.13906729221344, 'validation/num_examples': 50000, 'test/accuracy': 0.5994000434875488, 'test/loss': 1.854590654373169, 'test/num_examples': 10000, 'score': 45955.56926059723, 'total_duration': 47666.84624814987, 'accumulated_submission_time': 45955.56926059723, 'accumulated_eval_time': 1701.8160552978516, 'accumulated_logging_time': 4.829279661178589}
I0229 18:47:51.814998 140522588985088 logging_writer.py:48] [135913] accumulated_eval_time=1701.816055, accumulated_logging_time=4.829280, accumulated_submission_time=45955.569261, global_step=135913, preemption_count=0, score=45955.569261, test/accuracy=0.599400, test/loss=1.854591, test/num_examples=10000, total_duration=47666.846248, train/accuracy=0.879903, train/loss=0.428302, validation/accuracy=0.725480, validation/loss=1.139067, validation/num_examples=50000
I0229 18:48:21.504965 140522597377792 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.3233790397644043, loss=0.9393048882484436
I0229 18:48:55.272762 140522588985088 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.4852402210235596, loss=1.028448462486267
I0229 18:49:29.025377 140522597377792 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.20188307762146, loss=0.9731127619743347
I0229 18:50:02.818915 140522588985088 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.2809853553771973, loss=1.0297530889511108
I0229 18:50:36.562652 140522597377792 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.3193724155426025, loss=0.9453461766242981
I0229 18:51:10.294985 140522588985088 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.7809174060821533, loss=1.1410759687423706
I0229 18:51:44.001996 140522597377792 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.4067580699920654, loss=1.077955722808838
I0229 18:52:17.773702 140522588985088 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.770815372467041, loss=1.0615030527114868
I0229 18:52:51.631116 140522597377792 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.464378595352173, loss=0.9788988828659058
I0229 18:53:25.377282 140522588985088 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.613654375076294, loss=1.0432231426239014
I0229 18:53:59.154426 140522597377792 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.7245471477508545, loss=0.9997652769088745
I0229 18:54:32.924350 140522588985088 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.6105291843414307, loss=1.0409854650497437
I0229 18:55:06.709914 140522597377792 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.345248222351074, loss=1.0085134506225586
I0229 18:55:40.514717 140522588985088 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.237074136734009, loss=1.005298137664795
I0229 18:56:14.264229 140522597377792 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.5126821994781494, loss=1.039015769958496
I0229 18:56:21.868168 140688601454400 spec.py:321] Evaluating on the training split.
I0229 18:56:28.056776 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 18:56:36.517833 140688601454400 spec.py:349] Evaluating on the test split.
I0229 18:56:38.799969 140688601454400 submission_runner.py:411] Time since start: 48193.87s, 	Step: 137424, 	{'train/accuracy': 0.8622847199440002, 'train/loss': 0.48595738410949707, 'validation/accuracy': 0.7216599583625793, 'validation/loss': 1.1468485593795776, 'validation/num_examples': 50000, 'test/accuracy': 0.5951000452041626, 'test/loss': 1.876573920249939, 'test/num_examples': 10000, 'score': 46465.56084442139, 'total_duration': 48193.87060427666, 'accumulated_submission_time': 46465.56084442139, 'accumulated_eval_time': 1718.7475936412811, 'accumulated_logging_time': 4.878940105438232}
I0229 18:56:38.842629 140525256554240 logging_writer.py:48] [137424] accumulated_eval_time=1718.747594, accumulated_logging_time=4.878940, accumulated_submission_time=46465.560844, global_step=137424, preemption_count=0, score=46465.560844, test/accuracy=0.595100, test/loss=1.876574, test/num_examples=10000, total_duration=48193.870604, train/accuracy=0.862285, train/loss=0.485957, validation/accuracy=0.721660, validation/loss=1.146849, validation/num_examples=50000
I0229 18:57:04.805410 140525264946944 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.3621978759765625, loss=1.0078569650650024
I0229 18:57:38.534889 140525256554240 logging_writer.py:48] [137600] global_step=137600, grad_norm=4.170530319213867, loss=1.0640429258346558
I0229 18:58:12.265019 140525264946944 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.465043306350708, loss=1.0343493223190308
I0229 18:58:46.129673 140525256554240 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.807011604309082, loss=1.0492388010025024
I0229 18:59:19.890314 140525264946944 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.71929931640625, loss=1.0744847059249878
I0229 18:59:53.664731 140525256554240 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.4003219604492188, loss=1.0512434244155884
I0229 19:00:27.438220 140525264946944 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.5482337474823, loss=1.0221037864685059
I0229 19:01:01.193803 140525256554240 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.3488476276397705, loss=1.0708425045013428
I0229 19:01:35.005897 140525264946944 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.6362321376800537, loss=1.0434514284133911
I0229 19:02:08.771107 140525256554240 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.663259744644165, loss=1.0691497325897217
I0229 19:02:42.539212 140525264946944 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.1490447521209717, loss=0.8713117837905884
I0229 19:03:16.305868 140525256554240 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.8204023838043213, loss=1.0047348737716675
I0229 19:03:50.042694 140525264946944 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.798973798751831, loss=1.0264843702316284
I0229 19:04:23.783320 140525256554240 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.4750120639801025, loss=1.054028868675232
I0229 19:04:57.626149 140525264946944 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.4943017959594727, loss=1.0249583721160889
I0229 19:05:08.921799 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:05:15.089868 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:05:23.610244 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:05:25.872953 140688601454400 submission_runner.py:411] Time since start: 48720.94s, 	Step: 138935, 	{'train/accuracy': 0.8673469424247742, 'train/loss': 0.46458643674850464, 'validation/accuracy': 0.7269600033760071, 'validation/loss': 1.113786220550537, 'validation/num_examples': 50000, 'test/accuracy': 0.6015000343322754, 'test/loss': 1.8418588638305664, 'test/num_examples': 10000, 'score': 46975.57577776909, 'total_duration': 48720.94380235672, 'accumulated_submission_time': 46975.57577776909, 'accumulated_eval_time': 1735.6987011432648, 'accumulated_logging_time': 4.933518886566162}
I0229 19:05:25.919049 140522597377792 logging_writer.py:48] [138935] accumulated_eval_time=1735.698701, accumulated_logging_time=4.933519, accumulated_submission_time=46975.575778, global_step=138935, preemption_count=0, score=46975.575778, test/accuracy=0.601500, test/loss=1.841859, test/num_examples=10000, total_duration=48720.943802, train/accuracy=0.867347, train/loss=0.464586, validation/accuracy=0.726960, validation/loss=1.113786, validation/num_examples=50000
I0229 19:05:48.178302 140522605770496 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.5661823749542236, loss=1.0109708309173584
I0229 19:06:21.900767 140522597377792 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.5895113945007324, loss=0.9821348190307617
I0229 19:06:55.677423 140522605770496 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.949939250946045, loss=1.0671550035476685
I0229 19:07:29.432429 140522597377792 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.8771886825561523, loss=1.0777829885482788
I0229 19:08:03.180277 140522605770496 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.712958335876465, loss=1.0011224746704102
I0229 19:08:36.929339 140522597377792 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.7505578994750977, loss=1.0098285675048828
I0229 19:09:10.725455 140522605770496 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.404287576675415, loss=0.9793105721473694
I0229 19:09:44.499971 140522597377792 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.4906516075134277, loss=0.9447668194770813
I0229 19:10:18.245051 140522605770496 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.239882469177246, loss=0.8621103763580322
I0229 19:10:52.096765 140522597377792 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.9319167137145996, loss=0.9968236684799194
I0229 19:11:25.854835 140522605770496 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.504366636276245, loss=1.0090793371200562
I0229 19:11:59.622197 140522597377792 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.4972739219665527, loss=1.0202627182006836
I0229 19:12:33.403479 140522605770496 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.5139319896698, loss=0.976790189743042
I0229 19:13:07.171728 140522597377792 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.7030107975006104, loss=0.9982524514198303
I0229 19:13:40.918482 140522605770496 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.680304527282715, loss=0.9855612516403198
I0229 19:13:55.923987 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:14:02.237913 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:14:11.040076 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:14:13.332748 140688601454400 submission_runner.py:411] Time since start: 49248.40s, 	Step: 140446, 	{'train/accuracy': 0.8664301633834839, 'train/loss': 0.4641014337539673, 'validation/accuracy': 0.7262399792671204, 'validation/loss': 1.1191344261169434, 'validation/num_examples': 50000, 'test/accuracy': 0.6013000011444092, 'test/loss': 1.8446753025054932, 'test/num_examples': 10000, 'score': 47485.51925230026, 'total_duration': 49248.403594732285, 'accumulated_submission_time': 47485.51925230026, 'accumulated_eval_time': 1753.1074166297913, 'accumulated_logging_time': 4.989470481872559}
I0229 19:14:13.376125 140525256554240 logging_writer.py:48] [140446] accumulated_eval_time=1753.107417, accumulated_logging_time=4.989470, accumulated_submission_time=47485.519252, global_step=140446, preemption_count=0, score=47485.519252, test/accuracy=0.601300, test/loss=1.844675, test/num_examples=10000, total_duration=49248.403595, train/accuracy=0.866430, train/loss=0.464101, validation/accuracy=0.726240, validation/loss=1.119134, validation/num_examples=50000
I0229 19:14:31.936084 140525264946944 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.760695219039917, loss=0.9270209670066833
I0229 19:15:05.648730 140525256554240 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.556835174560547, loss=1.0571317672729492
I0229 19:15:39.432991 140525264946944 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.9971463680267334, loss=1.0377672910690308
I0229 19:16:13.233720 140525256554240 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.337833881378174, loss=0.9246453046798706
I0229 19:16:47.076685 140525264946944 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.778390407562256, loss=1.037487506866455
I0229 19:17:20.864335 140525256554240 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.713449001312256, loss=0.9902687072753906
I0229 19:17:54.624452 140525264946944 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.1930062770843506, loss=0.8698910474777222
I0229 19:18:28.396293 140525256554240 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.8495283126831055, loss=1.0537068843841553
I0229 19:19:02.146940 140525264946944 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.7414231300354004, loss=0.9606733322143555
I0229 19:19:35.877164 140525256554240 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.7861554622650146, loss=0.9652432203292847
I0229 19:20:09.653838 140525264946944 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.735633611679077, loss=1.0130261182785034
I0229 19:20:43.426435 140525256554240 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.6124794483184814, loss=1.0176423788070679
I0229 19:21:17.186133 140525264946944 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.6364905834198, loss=1.0557715892791748
I0229 19:21:50.948774 140525256554240 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.649091958999634, loss=1.055602788925171
I0229 19:22:24.686608 140525264946944 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.615767002105713, loss=0.9468070268630981
I0229 19:22:43.437706 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:22:49.836796 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:22:58.607276 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:23:00.922080 140688601454400 submission_runner.py:411] Time since start: 49775.99s, 	Step: 141957, 	{'train/accuracy': 0.8687818646430969, 'train/loss': 0.45073121786117554, 'validation/accuracy': 0.7324999570846558, 'validation/loss': 1.1073507070541382, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.8624248504638672, 'test/num_examples': 10000, 'score': 47995.51835870743, 'total_duration': 49775.99292445183, 'accumulated_submission_time': 47995.51835870743, 'accumulated_eval_time': 1770.591741323471, 'accumulated_logging_time': 5.043493986129761}
I0229 19:23:00.963630 140522605770496 logging_writer.py:48] [141957] accumulated_eval_time=1770.591741, accumulated_logging_time=5.043494, accumulated_submission_time=47995.518359, global_step=141957, preemption_count=0, score=47995.518359, test/accuracy=0.604500, test/loss=1.862425, test/num_examples=10000, total_duration=49775.992924, train/accuracy=0.868782, train/loss=0.450731, validation/accuracy=0.732500, validation/loss=1.107351, validation/num_examples=50000
I0229 19:23:15.819004 140523092305664 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.9402732849121094, loss=0.9875816106796265
I0229 19:23:49.564090 140522605770496 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.7061469554901123, loss=1.0533850193023682
I0229 19:24:23.308325 140523092305664 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.784689426422119, loss=0.9794344902038574
I0229 19:24:57.000649 140522605770496 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.770730972290039, loss=0.9721097946166992
I0229 19:25:30.786845 140523092305664 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.768064260482788, loss=0.9215427041053772
I0229 19:26:04.599261 140522605770496 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.972346305847168, loss=0.9945037364959717
I0229 19:26:38.377124 140523092305664 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.8169140815734863, loss=0.9172952175140381
I0229 19:27:12.126588 140522605770496 logging_writer.py:48] [142700] global_step=142700, grad_norm=4.267502784729004, loss=0.986973226070404
I0229 19:27:45.898089 140523092305664 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.93953537940979, loss=0.9884134531021118
I0229 19:28:19.677705 140522605770496 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.747413396835327, loss=0.995032548904419
I0229 19:28:53.434787 140523092305664 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.676273822784424, loss=1.0535593032836914
I0229 19:29:27.248683 140522605770496 logging_writer.py:48] [143100] global_step=143100, grad_norm=4.122251510620117, loss=0.9972083568572998
I0229 19:30:01.028915 140523092305664 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.71642804145813, loss=0.9727552533149719
I0229 19:30:34.844316 140522605770496 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.416452646255493, loss=0.8868442177772522
I0229 19:31:08.610161 140523092305664 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.5902223587036133, loss=0.9006966948509216
I0229 19:31:31.028012 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:31:37.151906 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:31:45.658596 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:31:47.912086 140688601454400 submission_runner.py:411] Time since start: 50302.98s, 	Step: 143468, 	{'train/accuracy': 0.8729272484779358, 'train/loss': 0.4402921795845032, 'validation/accuracy': 0.7320399880409241, 'validation/loss': 1.117584228515625, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.8811101913452148, 'test/num_examples': 10000, 'score': 48505.51794219017, 'total_duration': 50302.9829390049, 'accumulated_submission_time': 48505.51794219017, 'accumulated_eval_time': 1787.475771188736, 'accumulated_logging_time': 5.096850633621216}
I0229 19:31:47.955417 140522597377792 logging_writer.py:48] [143468] accumulated_eval_time=1787.475771, accumulated_logging_time=5.096851, accumulated_submission_time=48505.517942, global_step=143468, preemption_count=0, score=48505.517942, test/accuracy=0.602500, test/loss=1.881110, test/num_examples=10000, total_duration=50302.982939, train/accuracy=0.872927, train/loss=0.440292, validation/accuracy=0.732040, validation/loss=1.117584, validation/num_examples=50000
I0229 19:31:59.076780 140522605770496 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.6584832668304443, loss=0.98279869556427
I0229 19:32:32.824877 140522597377792 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.7140657901763916, loss=0.961137056350708
I0229 19:33:06.558196 140522605770496 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.845824718475342, loss=1.0045506954193115
I0229 19:33:40.314061 140522597377792 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.5904760360717773, loss=0.8789204955101013
I0229 19:34:14.102719 140522605770496 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.5858027935028076, loss=0.9006898403167725
I0229 19:34:47.872016 140522597377792 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.631321430206299, loss=0.9466660022735596
I0229 19:35:21.680673 140522605770496 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.771049976348877, loss=1.0091856718063354
I0229 19:35:55.429467 140522597377792 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.712599277496338, loss=0.9320700764656067
I0229 19:36:29.193140 140522605770496 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.6599531173706055, loss=0.910010576248169
I0229 19:37:02.984391 140522597377792 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.7381479740142822, loss=0.9661239385604858
I0229 19:37:36.766493 140522605770496 logging_writer.py:48] [144500] global_step=144500, grad_norm=4.090451240539551, loss=0.9562795162200928
I0229 19:38:10.552003 140522597377792 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.786868095397949, loss=0.9136754274368286
I0229 19:38:44.312176 140522605770496 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.46584415435791, loss=0.8982313871383667
I0229 19:39:18.081848 140522597377792 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.888368606567383, loss=0.868552029132843
I0229 19:39:51.841654 140522605770496 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.652615547180176, loss=0.9736114740371704
I0229 19:40:17.950584 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:40:24.094632 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:40:32.637086 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:40:34.901954 140688601454400 submission_runner.py:411] Time since start: 50829.97s, 	Step: 144979, 	{'train/accuracy': 0.8941127061843872, 'train/loss': 0.3673299551010132, 'validation/accuracy': 0.7300800085067749, 'validation/loss': 1.1244101524353027, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.8705135583877563, 'test/num_examples': 10000, 'score': 49015.45076060295, 'total_duration': 50829.972801446915, 'accumulated_submission_time': 49015.45076060295, 'accumulated_eval_time': 1804.4271116256714, 'accumulated_logging_time': 5.150873422622681}
I0229 19:40:34.943869 140525273339648 logging_writer.py:48] [144979] accumulated_eval_time=1804.427112, accumulated_logging_time=5.150873, accumulated_submission_time=49015.450761, global_step=144979, preemption_count=0, score=49015.450761, test/accuracy=0.601100, test/loss=1.870514, test/num_examples=10000, total_duration=50829.972801, train/accuracy=0.894113, train/loss=0.367330, validation/accuracy=0.730080, validation/loss=1.124410, validation/num_examples=50000
I0229 19:40:42.395488 140525281732352 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.839290142059326, loss=0.8672205209732056
I0229 19:41:16.157042 140525273339648 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.655757427215576, loss=0.851560115814209
I0229 19:41:49.953440 140525281732352 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.854959487915039, loss=0.8860872387886047
I0229 19:42:23.680428 140525273339648 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.6366517543792725, loss=0.8672602772712708
I0229 19:42:57.480451 140525281732352 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.8013548851013184, loss=0.9219380021095276
I0229 19:43:31.254341 140525273339648 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.8573503494262695, loss=0.8959559798240662
I0229 19:44:05.004935 140525281732352 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.9743576049804688, loss=0.9498141407966614
I0229 19:44:38.772501 140525273339648 logging_writer.py:48] [145700] global_step=145700, grad_norm=4.02116584777832, loss=0.884992778301239
I0229 19:45:12.531529 140525281732352 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.652071475982666, loss=0.902860164642334
I0229 19:45:46.297885 140525273339648 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.8662335872650146, loss=0.9326151013374329
I0229 19:46:20.075735 140525281732352 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.5183322429656982, loss=0.8672707080841064
I0229 19:46:53.841658 140525273339648 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.587512254714966, loss=0.7868027091026306
I0229 19:47:27.690454 140525281732352 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.914320707321167, loss=0.9211024641990662
I0229 19:48:01.461591 140525273339648 logging_writer.py:48] [146300] global_step=146300, grad_norm=4.1290788650512695, loss=0.9314221143722534
I0229 19:48:35.203129 140525281732352 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.077681064605713, loss=0.964614987373352
I0229 19:49:05.059548 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:49:11.190626 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:49:19.851678 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:49:22.097985 140688601454400 submission_runner.py:411] Time since start: 51357.17s, 	Step: 146490, 	{'train/accuracy': 0.8969627022743225, 'train/loss': 0.3568805456161499, 'validation/accuracy': 0.7330399751663208, 'validation/loss': 1.1061513423919678, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.8569191694259644, 'test/num_examples': 10000, 'score': 49525.504454135895, 'total_duration': 51357.16882824898, 'accumulated_submission_time': 49525.504454135895, 'accumulated_eval_time': 1821.4655013084412, 'accumulated_logging_time': 5.202937126159668}
I0229 19:49:22.141892 140522597377792 logging_writer.py:48] [146490] accumulated_eval_time=1821.465501, accumulated_logging_time=5.202937, accumulated_submission_time=49525.504454, global_step=146490, preemption_count=0, score=49525.504454, test/accuracy=0.608000, test/loss=1.856919, test/num_examples=10000, total_duration=51357.168828, train/accuracy=0.896963, train/loss=0.356881, validation/accuracy=0.733040, validation/loss=1.106151, validation/num_examples=50000
I0229 19:49:25.862345 140522605770496 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.054636001586914, loss=0.988808274269104
I0229 19:49:59.572645 140522597377792 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.8333959579467773, loss=0.8787329792976379
I0229 19:50:33.342790 140522605770496 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.5727899074554443, loss=0.8839740753173828
I0229 19:51:07.083137 140522597377792 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.1690568923950195, loss=1.0006742477416992
I0229 19:51:40.848222 140522605770496 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.6275691986083984, loss=0.879630446434021
I0229 19:52:14.619509 140522597377792 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.7809014320373535, loss=0.9561358094215393
I0229 19:52:48.394555 140522605770496 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.9434361457824707, loss=0.9864501953125
I0229 19:53:22.154084 140522597377792 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.6560404300689697, loss=0.8679780960083008
I0229 19:53:55.992424 140522605770496 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.574578285217285, loss=0.7582968473434448
I0229 19:54:29.748007 140522597377792 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.848356246948242, loss=0.9065310955047607
I0229 19:55:03.502715 140522605770496 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.263530731201172, loss=0.9708304405212402
I0229 19:55:37.275803 140522597377792 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.818647861480713, loss=0.8903438448905945
I0229 19:56:11.023116 140522605770496 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.8713271617889404, loss=0.9522392749786377
I0229 19:56:44.781345 140522597377792 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.015126705169678, loss=0.9872953295707703
I0229 19:57:18.579569 140522605770496 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.0109171867370605, loss=0.895553469657898
I0229 19:57:52.365475 140522597377792 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.8967719078063965, loss=0.9153223633766174
I0229 19:57:52.372924 140688601454400 spec.py:321] Evaluating on the training split.
I0229 19:57:58.497029 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 19:58:07.368312 140688601454400 spec.py:349] Evaluating on the test split.
I0229 19:58:09.667329 140688601454400 submission_runner.py:411] Time since start: 51884.74s, 	Step: 148001, 	{'train/accuracy': 0.8917012214660645, 'train/loss': 0.37165364623069763, 'validation/accuracy': 0.7354599833488464, 'validation/loss': 1.0957401990890503, 'validation/num_examples': 50000, 'test/accuracy': 0.6052000522613525, 'test/loss': 1.8347212076187134, 'test/num_examples': 10000, 'score': 50035.673583984375, 'total_duration': 51884.738174676895, 'accumulated_submission_time': 50035.673583984375, 'accumulated_eval_time': 1838.759839296341, 'accumulated_logging_time': 5.256864070892334}
I0229 19:58:09.710975 140522597377792 logging_writer.py:48] [148001] accumulated_eval_time=1838.759839, accumulated_logging_time=5.256864, accumulated_submission_time=50035.673584, global_step=148001, preemption_count=0, score=50035.673584, test/accuracy=0.605200, test/loss=1.834721, test/num_examples=10000, total_duration=51884.738175, train/accuracy=0.891701, train/loss=0.371654, validation/accuracy=0.735460, validation/loss=1.095740, validation/num_examples=50000
I0229 19:58:43.452748 140525256554240 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.689892292022705, loss=0.9012276530265808
I0229 19:59:17.205833 140522597377792 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.6936614513397217, loss=0.952546238899231
I0229 19:59:51.062650 140525256554240 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.9628233909606934, loss=0.899368941783905
I0229 20:00:24.822373 140522597377792 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.1269426345825195, loss=0.8971952199935913
I0229 20:00:58.555079 140525256554240 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.934779167175293, loss=0.9133061170578003
I0229 20:01:32.287469 140522597377792 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.8629987239837646, loss=0.9403780698776245
I0229 20:02:06.065069 140525256554240 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.7180702686309814, loss=0.84767746925354
I0229 20:02:39.826619 140522597377792 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.513308048248291, loss=0.8175259828567505
I0229 20:03:13.563537 140525256554240 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.9105634689331055, loss=0.9449031949043274
I0229 20:03:47.338208 140522597377792 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8722760677337646, loss=0.8491194844245911
I0229 20:04:21.072002 140525256554240 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.292649269104004, loss=0.9312717318534851
I0229 20:04:54.828726 140522597377792 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.002564907073975, loss=0.8853811621665955
I0229 20:05:28.611459 140525256554240 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.092219829559326, loss=0.8431968688964844
I0229 20:06:02.438874 140522597377792 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.348185062408447, loss=0.9387833476066589
I0229 20:06:36.216714 140525256554240 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.200204372406006, loss=0.922156572341919
I0229 20:06:39.731324 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:06:45.808376 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:06:54.488833 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:06:56.794580 140688601454400 submission_runner.py:411] Time since start: 52411.87s, 	Step: 149512, 	{'train/accuracy': 0.8974609375, 'train/loss': 0.34971192479133606, 'validation/accuracy': 0.7398799657821655, 'validation/loss': 1.0879695415496826, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.8350088596343994, 'test/num_examples': 10000, 'score': 50545.63154053688, 'total_duration': 52411.86543726921, 'accumulated_submission_time': 50545.63154053688, 'accumulated_eval_time': 1855.8230559825897, 'accumulated_logging_time': 5.3106231689453125}
I0229 20:06:56.830853 140522597377792 logging_writer.py:48] [149512] accumulated_eval_time=1855.823056, accumulated_logging_time=5.310623, accumulated_submission_time=50545.631541, global_step=149512, preemption_count=0, score=50545.631541, test/accuracy=0.616400, test/loss=1.835009, test/num_examples=10000, total_duration=52411.865437, train/accuracy=0.897461, train/loss=0.349712, validation/accuracy=0.739880, validation/loss=1.087970, validation/num_examples=50000
I0229 20:07:26.836620 140522605770496 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.8019051551818848, loss=0.8663502931594849
I0229 20:08:00.546743 140522597377792 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.109740734100342, loss=0.9056805372238159
I0229 20:08:34.332069 140522605770496 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.698028564453125, loss=0.8291345238685608
I0229 20:09:08.094394 140522597377792 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.128307342529297, loss=0.8716400861740112
I0229 20:09:41.827959 140522605770496 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.9112119674682617, loss=0.8890684247016907
I0229 20:10:15.607084 140522597377792 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.422660827636719, loss=0.8202398419380188
I0229 20:10:49.380702 140522605770496 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.7460172176361084, loss=0.895332396030426
I0229 20:11:23.159441 140522597377792 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.7393946647644043, loss=0.8655220866203308
I0229 20:11:56.988572 140522605770496 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.125783443450928, loss=0.8162238001823425
I0229 20:12:30.774046 140522597377792 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.776304244995117, loss=0.8275331258773804
I0229 20:13:04.585973 140522605770496 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.266448497772217, loss=0.9271858930587769
I0229 20:13:38.321823 140522597377792 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.421041965484619, loss=0.9806625843048096
I0229 20:14:12.061271 140522605770496 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.934684991836548, loss=0.877257227897644
I0229 20:14:45.850851 140522597377792 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.787937879562378, loss=0.8113478422164917
I0229 20:15:19.632901 140522605770496 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.7988171577453613, loss=0.8163349628448486
I0229 20:15:26.860304 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:15:33.018795 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:15:41.421433 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:15:43.660803 140688601454400 submission_runner.py:411] Time since start: 52938.73s, 	Step: 151023, 	{'train/accuracy': 0.8940728306770325, 'train/loss': 0.3650061786174774, 'validation/accuracy': 0.7384399771690369, 'validation/loss': 1.0874049663543701, 'validation/num_examples': 50000, 'test/accuracy': 0.6171000003814697, 'test/loss': 1.8412714004516602, 'test/num_examples': 10000, 'score': 51055.59986066818, 'total_duration': 52938.73140335083, 'accumulated_submission_time': 51055.59986066818, 'accumulated_eval_time': 1872.6232559680939, 'accumulated_logging_time': 5.355967283248901}
I0229 20:15:43.702507 140522588985088 logging_writer.py:48] [151023] accumulated_eval_time=1872.623256, accumulated_logging_time=5.355967, accumulated_submission_time=51055.599861, global_step=151023, preemption_count=0, score=51055.599861, test/accuracy=0.617100, test/loss=1.841271, test/num_examples=10000, total_duration=52938.731403, train/accuracy=0.894073, train/loss=0.365006, validation/accuracy=0.738440, validation/loss=1.087405, validation/num_examples=50000
I0229 20:16:10.033043 140525256554240 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.7626233100891113, loss=0.8669651746749878
I0229 20:16:43.760825 140522588985088 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.2877912521362305, loss=0.8200729489326477
I0229 20:17:17.528560 140525256554240 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.072455883026123, loss=0.7877021431922913
I0229 20:17:51.282327 140522588985088 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.794203281402588, loss=0.8482991456985474
I0229 20:18:25.200839 140525256554240 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.597151041030884, loss=0.7967360019683838
I0229 20:18:58.950994 140522588985088 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.033581256866455, loss=0.9391403198242188
I0229 20:19:32.717178 140525256554240 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.995946168899536, loss=0.778410792350769
I0229 20:20:06.482922 140522588985088 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.1478447914123535, loss=0.8444903492927551
I0229 20:20:40.251005 140525256554240 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.020709991455078, loss=0.8866287469863892
I0229 20:21:13.971011 140522588985088 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.376603603363037, loss=0.9131307005882263
I0229 20:21:47.711077 140525256554240 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.8113174438476562, loss=0.8103894591331482
I0229 20:22:21.491014 140522588985088 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.8340952396392822, loss=0.8674668073654175
I0229 20:22:55.241912 140525256554240 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.07821798324585, loss=0.7691019177436829
I0229 20:23:28.992163 140522588985088 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.423544883728027, loss=0.8942970037460327
I0229 20:24:02.769170 140525256554240 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.349003314971924, loss=0.8484596014022827
I0229 20:24:13.830524 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:24:20.638569 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:24:29.330611 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:24:31.593624 140688601454400 submission_runner.py:411] Time since start: 53466.66s, 	Step: 152534, 	{'train/accuracy': 0.9021643400192261, 'train/loss': 0.3393253982067108, 'validation/accuracy': 0.7376399636268616, 'validation/loss': 1.0938892364501953, 'validation/num_examples': 50000, 'test/accuracy': 0.612000048160553, 'test/loss': 1.854403018951416, 'test/num_examples': 10000, 'score': 51565.666075229645, 'total_duration': 53466.66446304321, 'accumulated_submission_time': 51565.666075229645, 'accumulated_eval_time': 1890.3863129615784, 'accumulated_logging_time': 5.407841682434082}
I0229 20:24:31.638457 140523092305664 logging_writer.py:48] [152534] accumulated_eval_time=1890.386313, accumulated_logging_time=5.407842, accumulated_submission_time=51565.666075, global_step=152534, preemption_count=0, score=51565.666075, test/accuracy=0.612000, test/loss=1.854403, test/num_examples=10000, total_duration=53466.664463, train/accuracy=0.902164, train/loss=0.339325, validation/accuracy=0.737640, validation/loss=1.093889, validation/num_examples=50000
I0229 20:24:54.249492 140523947947776 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.8491404056549072, loss=0.8458234667778015
I0229 20:25:27.942722 140523092305664 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.7723214626312256, loss=0.7221496105194092
I0229 20:26:01.709322 140523947947776 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.9855258464813232, loss=0.8813972473144531
I0229 20:26:35.511853 140523092305664 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.8840200901031494, loss=0.888749361038208
I0229 20:27:09.287700 140523947947776 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.258109092712402, loss=0.8770426511764526
I0229 20:27:43.074550 140523092305664 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.058061122894287, loss=0.865983247756958
I0229 20:28:16.848565 140523947947776 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.995514154434204, loss=0.8211965560913086
I0229 20:28:50.593729 140523092305664 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.9478864669799805, loss=0.8467597365379333
I0229 20:29:24.385579 140523947947776 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.7215418815612793, loss=0.7703932523727417
I0229 20:29:58.166082 140523092305664 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.309044361114502, loss=0.9124271869659424
I0229 20:30:31.986844 140523947947776 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.913851261138916, loss=0.769193172454834
I0229 20:31:05.791325 140523092305664 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.341571807861328, loss=0.9973227977752686
I0229 20:31:39.551728 140523947947776 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.8529410362243652, loss=0.8275583386421204
I0229 20:32:13.332753 140523092305664 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.034485816955566, loss=0.9059607982635498
I0229 20:32:47.111168 140523947947776 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.006943702697754, loss=0.8451011180877686
I0229 20:33:01.785330 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:33:07.909852 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:33:16.543149 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:33:18.882208 140688601454400 submission_runner.py:411] Time since start: 53993.95s, 	Step: 154045, 	{'train/accuracy': 0.9244658350944519, 'train/loss': 0.26741597056388855, 'validation/accuracy': 0.74263995885849, 'validation/loss': 1.075542688369751, 'validation/num_examples': 50000, 'test/accuracy': 0.6180000305175781, 'test/loss': 1.8343791961669922, 'test/num_examples': 10000, 'score': 52075.750030994415, 'total_duration': 53993.95304322243, 'accumulated_submission_time': 52075.750030994415, 'accumulated_eval_time': 1907.4831624031067, 'accumulated_logging_time': 5.463268280029297}
I0229 20:33:18.933483 140522605770496 logging_writer.py:48] [154045] accumulated_eval_time=1907.483162, accumulated_logging_time=5.463268, accumulated_submission_time=52075.750031, global_step=154045, preemption_count=0, score=52075.750031, test/accuracy=0.618000, test/loss=1.834379, test/num_examples=10000, total_duration=53993.953043, train/accuracy=0.924466, train/loss=0.267416, validation/accuracy=0.742640, validation/loss=1.075543, validation/num_examples=50000
I0229 20:33:37.807776 140523092305664 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.171253681182861, loss=0.7809722423553467
I0229 20:34:11.577550 140522605770496 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.9638004302978516, loss=0.7916317582130432
I0229 20:34:45.353193 140523092305664 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.013314247131348, loss=0.7330728769302368
I0229 20:35:19.169986 140522605770496 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.975395441055298, loss=0.6978768706321716
I0229 20:35:52.973414 140523092305664 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.9197559356689453, loss=0.7643331289291382
I0229 20:36:26.842997 140522605770496 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.271890640258789, loss=0.8468348979949951
I0229 20:37:00.671009 140523092305664 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.947718381881714, loss=0.8153660297393799
I0229 20:37:34.501748 140522605770496 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.204366207122803, loss=0.870736300945282
I0229 20:38:08.311464 140523092305664 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.8496365547180176, loss=0.7692643404006958
I0229 20:38:42.118046 140522605770496 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.031401634216309, loss=0.7363781929016113
I0229 20:39:15.881862 140523092305664 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.9972312450408936, loss=0.7988513708114624
I0229 20:39:49.678083 140522605770496 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.174677848815918, loss=0.8603363037109375
I0229 20:40:23.445177 140523092305664 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.327809810638428, loss=0.8910651803016663
I0229 20:40:57.259357 140522605770496 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.044366836547852, loss=0.776846170425415
I0229 20:41:31.057832 140523092305664 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.886808156967163, loss=0.7700307369232178
I0229 20:41:49.098061 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:41:55.204323 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:42:04.068032 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:42:06.375665 140688601454400 submission_runner.py:411] Time since start: 54521.45s, 	Step: 155555, 	{'train/accuracy': 0.9222337007522583, 'train/loss': 0.26778116822242737, 'validation/accuracy': 0.7441799640655518, 'validation/loss': 1.0739376544952393, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.832737922668457, 'test/num_examples': 10000, 'score': 52585.85358142853, 'total_duration': 54521.44651532173, 'accumulated_submission_time': 52585.85358142853, 'accumulated_eval_time': 1924.7607214450836, 'accumulated_logging_time': 5.524292469024658}
I0229 20:42:06.419699 140522588985088 logging_writer.py:48] [155555] accumulated_eval_time=1924.760721, accumulated_logging_time=5.524292, accumulated_submission_time=52585.853581, global_step=155555, preemption_count=0, score=52585.853581, test/accuracy=0.617000, test/loss=1.832738, test/num_examples=10000, total_duration=54521.446515, train/accuracy=0.922234, train/loss=0.267781, validation/accuracy=0.744180, validation/loss=1.073938, validation/num_examples=50000
I0229 20:42:21.953310 140522597377792 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.354660511016846, loss=0.8707443475723267
I0229 20:42:55.774210 140522588985088 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.100440502166748, loss=0.8380559682846069
I0229 20:43:29.491348 140522597377792 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.9110653400421143, loss=0.7625399827957153
I0229 20:44:03.262365 140522588985088 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.954200267791748, loss=0.8175805807113647
I0229 20:44:37.096886 140522597377792 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.002416133880615, loss=0.7037520408630371
I0229 20:45:10.879844 140522588985088 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.202327728271484, loss=0.8416116237640381
I0229 20:45:44.703465 140522597377792 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.920478343963623, loss=0.7355331182479858
I0229 20:46:18.483904 140522588985088 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.434444904327393, loss=0.7928440570831299
I0229 20:46:52.302055 140522597377792 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.218000888824463, loss=0.7973300814628601
I0229 20:47:26.059992 140522588985088 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.384001731872559, loss=0.8310447335243225
I0229 20:47:59.845945 140522597377792 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.474896430969238, loss=0.8617708086967468
I0229 20:48:33.721341 140522588985088 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.628432750701904, loss=0.858872652053833
I0229 20:49:07.475492 140522597377792 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.176583766937256, loss=0.8337714672088623
I0229 20:49:41.240816 140522588985088 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.671461582183838, loss=0.8518018126487732
I0229 20:50:15.009656 140522597377792 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.8692891597747803, loss=0.7652139663696289
I0229 20:50:36.462244 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:50:42.551598 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:50:51.414896 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:50:53.661176 140688601454400 submission_runner.py:411] Time since start: 55048.73s, 	Step: 157065, 	{'train/accuracy': 0.9177096486091614, 'train/loss': 0.2826022207736969, 'validation/accuracy': 0.7434599995613098, 'validation/loss': 1.0783482789993286, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.843645453453064, 'test/num_examples': 10000, 'score': 53095.834916353226, 'total_duration': 55048.73201870918, 'accumulated_submission_time': 53095.834916353226, 'accumulated_eval_time': 1941.9596049785614, 'accumulated_logging_time': 5.5780956745147705}
I0229 20:50:53.705400 140525256554240 logging_writer.py:48] [157065] accumulated_eval_time=1941.959605, accumulated_logging_time=5.578096, accumulated_submission_time=53095.834916, global_step=157065, preemption_count=0, score=53095.834916, test/accuracy=0.614400, test/loss=1.843645, test/num_examples=10000, total_duration=55048.732019, train/accuracy=0.917710, train/loss=0.282602, validation/accuracy=0.743460, validation/loss=1.078348, validation/num_examples=50000
I0229 20:51:05.881342 140525264946944 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.163490295410156, loss=0.7725284695625305
I0229 20:51:39.587102 140525256554240 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.218114376068115, loss=0.7985169291496277
I0229 20:52:13.342735 140525264946944 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.419339179992676, loss=0.8211110234260559
I0229 20:52:47.130662 140525256554240 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.067739963531494, loss=0.7786984443664551
I0229 20:53:20.902132 140525264946944 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.370198726654053, loss=0.8052070736885071
I0229 20:53:54.663303 140525256554240 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.151105880737305, loss=0.7472218871116638
I0229 20:54:28.412870 140525264946944 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.5926103591918945, loss=0.7957686185836792
I0229 20:55:02.278065 140525256554240 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.266746997833252, loss=0.8540102243423462
I0229 20:55:36.044663 140525264946944 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.9846444129943848, loss=0.80806565284729
I0229 20:56:09.828313 140525256554240 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.586309909820557, loss=0.7792444825172424
I0229 20:56:43.581216 140525264946944 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.644996166229248, loss=0.8243163824081421
I0229 20:57:17.350854 140525256554240 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.32211446762085, loss=0.7853465676307678
I0229 20:57:51.114099 140525264946944 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.091410160064697, loss=0.7761270999908447
I0229 20:58:24.895968 140525256554240 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.27528190612793, loss=0.7529696822166443
I0229 20:58:58.692222 140525264946944 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.0117974281311035, loss=0.7548144459724426
I0229 20:59:23.859881 140688601454400 spec.py:321] Evaluating on the training split.
I0229 20:59:30.017868 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 20:59:38.731765 140688601454400 spec.py:349] Evaluating on the test split.
I0229 20:59:41.066167 140688601454400 submission_runner.py:411] Time since start: 55576.14s, 	Step: 158576, 	{'train/accuracy': 0.9215162396430969, 'train/loss': 0.27283674478530884, 'validation/accuracy': 0.7452399730682373, 'validation/loss': 1.074062705039978, 'validation/num_examples': 50000, 'test/accuracy': 0.6214000582695007, 'test/loss': 1.8301516771316528, 'test/num_examples': 10000, 'score': 53605.92672085762, 'total_duration': 55576.13701224327, 'accumulated_submission_time': 53605.92672085762, 'accumulated_eval_time': 1959.165843486786, 'accumulated_logging_time': 5.632391452789307}
I0229 20:59:41.112579 140522597377792 logging_writer.py:48] [158576] accumulated_eval_time=1959.165843, accumulated_logging_time=5.632391, accumulated_submission_time=53605.926721, global_step=158576, preemption_count=0, score=53605.926721, test/accuracy=0.621400, test/loss=1.830152, test/num_examples=10000, total_duration=55576.137012, train/accuracy=0.921516, train/loss=0.272837, validation/accuracy=0.745240, validation/loss=1.074063, validation/num_examples=50000
I0229 20:59:49.571193 140522605770496 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.13890266418457, loss=0.7602444887161255
I0229 21:00:23.308393 140522597377792 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.260851860046387, loss=0.7316924333572388
I0229 21:00:57.098234 140522605770496 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.369884490966797, loss=0.7536354064941406
I0229 21:01:30.856001 140522597377792 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.355139255523682, loss=0.8250275254249573
I0229 21:02:04.627326 140522605770496 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.53848123550415, loss=0.8723718523979187
I0229 21:02:38.427991 140522597377792 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.215134620666504, loss=0.813504695892334
I0229 21:03:12.198290 140522605770496 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.527340888977051, loss=0.8421396017074585
I0229 21:03:45.962739 140522597377792 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.3008246421813965, loss=0.791419267654419
I0229 21:04:19.718840 140522605770496 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.77543306350708, loss=0.8028333187103271
I0229 21:04:53.464376 140522597377792 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.386158466339111, loss=0.7653523087501526
I0229 21:05:27.235839 140522605770496 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.13135290145874, loss=0.712317168712616
I0229 21:06:00.967308 140522597377792 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.392561912536621, loss=0.8026663064956665
I0229 21:06:34.772332 140522605770496 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.095760345458984, loss=0.6932145357131958
I0229 21:07:08.625512 140522597377792 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.545289039611816, loss=0.8331241607666016
I0229 21:07:42.337675 140522605770496 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.394528865814209, loss=0.7614566087722778
I0229 21:08:11.171747 140688601454400 spec.py:321] Evaluating on the training split.
I0229 21:08:17.476225 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 21:08:26.027942 140688601454400 spec.py:349] Evaluating on the test split.
I0229 21:08:28.276048 140688601454400 submission_runner.py:411] Time since start: 56103.35s, 	Step: 160087, 	{'train/accuracy': 0.9222137928009033, 'train/loss': 0.2624360918998718, 'validation/accuracy': 0.7467199563980103, 'validation/loss': 1.0808501243591309, 'validation/num_examples': 50000, 'test/accuracy': 0.6155000329017639, 'test/loss': 1.869329571723938, 'test/num_examples': 10000, 'score': 54115.92363142967, 'total_duration': 56103.3468940258, 'accumulated_submission_time': 54115.92363142967, 'accumulated_eval_time': 1976.270096063614, 'accumulated_logging_time': 5.6887383460998535}
I0229 21:08:28.323171 140525256554240 logging_writer.py:48] [160087] accumulated_eval_time=1976.270096, accumulated_logging_time=5.688738, accumulated_submission_time=54115.923631, global_step=160087, preemption_count=0, score=54115.923631, test/accuracy=0.615500, test/loss=1.869330, test/num_examples=10000, total_duration=56103.346894, train/accuracy=0.922214, train/loss=0.262436, validation/accuracy=0.746720, validation/loss=1.080850, validation/num_examples=50000
I0229 21:08:33.042565 140525264946944 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.364764213562012, loss=0.7849646210670471
I0229 21:09:06.751081 140525256554240 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.10621976852417, loss=0.7027000188827515
I0229 21:09:40.465330 140525264946944 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.21660852432251, loss=0.7291684746742249
I0229 21:10:14.230266 140525256554240 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.860469341278076, loss=0.853690505027771
I0229 21:10:47.992547 140525264946944 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.9765372276306152, loss=0.7251703143119812
I0229 21:11:21.740765 140525256554240 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.480628967285156, loss=0.7763046026229858
I0229 21:11:55.499130 140525264946944 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.477372646331787, loss=0.8297962546348572
I0229 21:12:29.310024 140525256554240 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.101084232330322, loss=0.6772134304046631
I0229 21:13:03.060743 140525264946944 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.9826347827911377, loss=0.7326992154121399
I0229 21:13:36.943159 140525256554240 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.121133804321289, loss=0.7424371838569641
I0229 21:14:10.717656 140525264946944 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.296588897705078, loss=0.6822801232337952
I0229 21:14:44.472406 140525256554240 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.110419750213623, loss=0.7643508315086365
I0229 21:15:18.263638 140525264946944 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.667599201202393, loss=0.8273723721504211
I0229 21:15:51.976823 140525256554240 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.165794372558594, loss=0.7221309542655945
I0229 21:16:25.748802 140525264946944 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.426904201507568, loss=0.7232555150985718
I0229 21:16:58.370726 140688601454400 spec.py:321] Evaluating on the training split.
I0229 21:17:04.676958 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 21:17:13.140052 140688601454400 spec.py:349] Evaluating on the test split.
I0229 21:17:15.399906 140688601454400 submission_runner.py:411] Time since start: 56630.47s, 	Step: 161598, 	{'train/accuracy': 0.9258609414100647, 'train/loss': 0.25482872128486633, 'validation/accuracy': 0.7464199662208557, 'validation/loss': 1.0761977434158325, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.862369418144226, 'test/num_examples': 10000, 'score': 54625.90968847275, 'total_duration': 56630.47075533867, 'accumulated_submission_time': 54625.90968847275, 'accumulated_eval_time': 1993.2992358207703, 'accumulated_logging_time': 5.746572971343994}
I0229 21:17:15.447535 140522588985088 logging_writer.py:48] [161598] accumulated_eval_time=1993.299236, accumulated_logging_time=5.746573, accumulated_submission_time=54625.909688, global_step=161598, preemption_count=0, score=54625.909688, test/accuracy=0.621100, test/loss=1.862369, test/num_examples=10000, total_duration=56630.470755, train/accuracy=0.925861, train/loss=0.254829, validation/accuracy=0.746420, validation/loss=1.076198, validation/num_examples=50000
I0229 21:17:16.473741 140522597377792 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.328134536743164, loss=0.709540069103241
I0229 21:17:50.172578 140522588985088 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.5349931716918945, loss=0.8251831531524658
I0229 21:18:23.891737 140522597377792 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.172907829284668, loss=0.7049387097358704
I0229 21:18:57.657254 140522588985088 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.13250732421875, loss=0.7268509864807129
I0229 21:19:31.529738 140522597377792 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.212145805358887, loss=0.73014235496521
I0229 21:20:05.317994 140522588985088 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.131921291351318, loss=0.6968382596969604
I0229 21:20:39.114270 140522597377792 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.2250590324401855, loss=0.7568285465240479
I0229 21:21:12.872783 140522588985088 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.603010177612305, loss=0.884059488773346
I0229 21:21:46.647480 140522597377792 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.826217174530029, loss=0.7586673498153687
I0229 21:22:20.401832 140522588985088 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.289730072021484, loss=0.8314589262008667
I0229 21:22:54.182942 140522597377792 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.277478218078613, loss=0.720910906791687
I0229 21:23:27.981345 140522588985088 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.159974575042725, loss=0.7069488763809204
I0229 21:24:01.782164 140522597377792 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.949411630630493, loss=0.6438639163970947
I0229 21:24:35.578731 140522588985088 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.235015869140625, loss=0.7582363486289978
I0229 21:25:09.354875 140522597377792 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.412641525268555, loss=0.7390693426132202
I0229 21:25:43.162665 140522588985088 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.173922061920166, loss=0.7305335998535156
I0229 21:25:45.665111 140688601454400 spec.py:321] Evaluating on the training split.
I0229 21:25:51.804671 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 21:26:00.503664 140688601454400 spec.py:349] Evaluating on the test split.
I0229 21:26:02.813303 140688601454400 submission_runner.py:411] Time since start: 57157.88s, 	Step: 163109, 	{'train/accuracy': 0.9452128410339355, 'train/loss': 0.19644904136657715, 'validation/accuracy': 0.7478199601173401, 'validation/loss': 1.0691890716552734, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.8363096714019775, 'test/num_examples': 10000, 'score': 55136.06539058685, 'total_duration': 57157.884127378464, 'accumulated_submission_time': 55136.06539058685, 'accumulated_eval_time': 2010.4473536014557, 'accumulated_logging_time': 5.804094076156616}
I0229 21:26:02.876373 140525273339648 logging_writer.py:48] [163109] accumulated_eval_time=2010.447354, accumulated_logging_time=5.804094, accumulated_submission_time=55136.065391, global_step=163109, preemption_count=0, score=55136.065391, test/accuracy=0.624300, test/loss=1.836310, test/num_examples=10000, total_duration=57157.884127, train/accuracy=0.945213, train/loss=0.196449, validation/accuracy=0.747820, validation/loss=1.069189, validation/num_examples=50000
I0229 21:26:33.914451 140525281732352 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.607400417327881, loss=0.8149824738502502
I0229 21:27:07.651538 140525273339648 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.49098014831543, loss=0.6468247175216675
I0229 21:27:41.429269 140525281732352 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.06318998336792, loss=0.7226368188858032
I0229 21:28:15.225813 140525273339648 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.186840534210205, loss=0.7462126612663269
I0229 21:28:48.996559 140525281732352 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.570974349975586, loss=0.7286826968193054
I0229 21:29:22.748763 140525273339648 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.5613603591918945, loss=0.7471102476119995
I0229 21:29:56.567469 140525281732352 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.014456748962402, loss=0.7337778806686401
I0229 21:30:30.319695 140525273339648 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.566949367523193, loss=0.7614902257919312
I0229 21:31:04.136270 140525281732352 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.82047963142395, loss=0.6268218755722046
I0229 21:31:38.075551 140525273339648 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.827167987823486, loss=0.7697157263755798
I0229 21:32:11.831215 140525281732352 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.3786940574646, loss=0.7993373274803162
I0229 21:32:45.604305 140525273339648 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.286445617675781, loss=0.7046366930007935
I0229 21:33:19.377057 140525281732352 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.166798114776611, loss=0.6931049823760986
I0229 21:33:53.150985 140525273339648 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.604186534881592, loss=0.7376715540885925
I0229 21:34:26.923993 140525281732352 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.4374098777771, loss=0.6966599225997925
I0229 21:34:32.834194 140688601454400 spec.py:321] Evaluating on the training split.
I0229 21:34:38.970479 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 21:34:47.755216 140688601454400 spec.py:349] Evaluating on the test split.
I0229 21:34:50.041567 140688601454400 submission_runner.py:411] Time since start: 57685.11s, 	Step: 164619, 	{'train/accuracy': 0.9431201815605164, 'train/loss': 0.20017775893211365, 'validation/accuracy': 0.7487999796867371, 'validation/loss': 1.062160611152649, 'validation/num_examples': 50000, 'test/accuracy': 0.6244000196456909, 'test/loss': 1.8305258750915527, 'test/num_examples': 10000, 'score': 55645.96127533913, 'total_duration': 57685.11239886284, 'accumulated_submission_time': 55645.96127533913, 'accumulated_eval_time': 2027.654661655426, 'accumulated_logging_time': 5.87807035446167}
I0229 21:34:50.096055 140522597377792 logging_writer.py:48] [164619] accumulated_eval_time=2027.654662, accumulated_logging_time=5.878070, accumulated_submission_time=55645.961275, global_step=164619, preemption_count=0, score=55645.961275, test/accuracy=0.624400, test/loss=1.830526, test/num_examples=10000, total_duration=57685.112399, train/accuracy=0.943120, train/loss=0.200178, validation/accuracy=0.748800, validation/loss=1.062161, validation/num_examples=50000
I0229 21:35:17.756600 140522605770496 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.423502445220947, loss=0.7780410051345825
I0229 21:35:51.456757 140522597377792 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.593303203582764, loss=0.780788779258728
I0229 21:36:25.220345 140522605770496 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.846858501434326, loss=0.7594383358955383
I0229 21:36:59.012620 140522597377792 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.267212390899658, loss=0.7401989698410034
I0229 21:37:32.899557 140522605770496 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.291352272033691, loss=0.6985316872596741
I0229 21:38:06.687473 140522597377792 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.499044418334961, loss=0.6832464337348938
I0229 21:38:40.437736 140522605770496 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.166615962982178, loss=0.6857093572616577
I0229 21:39:14.155972 140522597377792 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.310873985290527, loss=0.7152449488639832
I0229 21:39:47.950242 140522605770496 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.167264461517334, loss=0.7101452946662903
I0229 21:40:21.764477 140522597377792 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.540637969970703, loss=0.7664557695388794
I0229 21:40:55.565166 140522605770496 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.242070198059082, loss=0.794152557849884
I0229 21:41:29.378343 140522597377792 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.855794906616211, loss=0.7874763607978821
I0229 21:42:03.156031 140522605770496 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.1478962898254395, loss=0.6566568613052368
I0229 21:42:36.976015 140522597377792 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.378521919250488, loss=0.653695285320282
I0229 21:43:10.773321 140522605770496 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.041285991668701, loss=0.6821348071098328
I0229 21:43:20.376723 140688601454400 spec.py:321] Evaluating on the training split.
I0229 21:43:26.480360 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 21:43:35.214501 140688601454400 spec.py:349] Evaluating on the test split.
I0229 21:43:37.509620 140688601454400 submission_runner.py:411] Time since start: 58212.58s, 	Step: 166130, 	{'train/accuracy': 0.9429408311843872, 'train/loss': 0.2010091245174408, 'validation/accuracy': 0.7507999539375305, 'validation/loss': 1.063918113708496, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8367518186569214, 'test/num_examples': 10000, 'score': 56156.17899298668, 'total_duration': 58212.58046770096, 'accumulated_submission_time': 56156.17899298668, 'accumulated_eval_time': 2044.7875108718872, 'accumulated_logging_time': 5.943117380142212}
I0229 21:43:37.554259 140525264946944 logging_writer.py:48] [166130] accumulated_eval_time=2044.787511, accumulated_logging_time=5.943117, accumulated_submission_time=56156.178993, global_step=166130, preemption_count=0, score=56156.178993, test/accuracy=0.617500, test/loss=1.836752, test/num_examples=10000, total_duration=58212.580468, train/accuracy=0.942941, train/loss=0.201009, validation/accuracy=0.750800, validation/loss=1.063918, validation/num_examples=50000
I0229 21:44:01.738188 140525273339648 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.186522960662842, loss=0.6895249485969543
I0229 21:44:35.499711 140525264946944 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.318853855133057, loss=0.6148511171340942
I0229 21:45:09.263803 140525273339648 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.555764198303223, loss=0.7355870008468628
I0229 21:45:43.046561 140525264946944 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.64282751083374, loss=0.684804379940033
I0229 21:46:16.832176 140525273339648 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.54810094833374, loss=0.7194079756736755
I0229 21:46:50.614524 140525264946944 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.753636837005615, loss=0.7519421577453613
I0229 21:47:24.397554 140525273339648 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.32313346862793, loss=0.7205065488815308
I0229 21:47:58.195816 140525264946944 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.347667217254639, loss=0.6905598044395447
I0229 21:48:32.006698 140525273339648 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.201731204986572, loss=0.6150661706924438
I0229 21:49:05.816920 140525264946944 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.252224445343018, loss=0.6614825129508972
I0229 21:49:39.604462 140525273339648 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.761893272399902, loss=0.7169950604438782
I0229 21:50:13.527626 140525264946944 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.24477481842041, loss=0.7411321997642517
I0229 21:50:47.332445 140525273339648 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.166675567626953, loss=0.6438718438148499
I0229 21:51:21.103488 140525264946944 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.61898946762085, loss=0.7256046533584595
I0229 21:51:54.926176 140525273339648 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.450859546661377, loss=0.6930133104324341
I0229 21:52:07.577747 140688601454400 spec.py:321] Evaluating on the training split.
I0229 21:52:13.768450 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 21:52:22.500353 140688601454400 spec.py:349] Evaluating on the test split.
I0229 21:52:24.792907 140688601454400 submission_runner.py:411] Time since start: 58739.86s, 	Step: 167639, 	{'train/accuracy': 0.9417450428009033, 'train/loss': 0.20107339322566986, 'validation/accuracy': 0.7512800097465515, 'validation/loss': 1.0629782676696777, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.8336199522018433, 'test/num_examples': 10000, 'score': 56666.13904762268, 'total_duration': 58739.86371612549, 'accumulated_submission_time': 56666.13904762268, 'accumulated_eval_time': 2062.002586364746, 'accumulated_logging_time': 5.999414920806885}
I0229 21:52:24.840975 140522605770496 logging_writer.py:48] [167639] accumulated_eval_time=2062.002586, accumulated_logging_time=5.999415, accumulated_submission_time=56666.139048, global_step=167639, preemption_count=0, score=56666.139048, test/accuracy=0.623400, test/loss=1.833620, test/num_examples=10000, total_duration=58739.863716, train/accuracy=0.941745, train/loss=0.201073, validation/accuracy=0.751280, validation/loss=1.062978, validation/num_examples=50000
I0229 21:52:45.790759 140523092305664 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.796656131744385, loss=0.7566734552383423
I0229 21:53:19.538743 140522605770496 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.6938557624816895, loss=0.6965413093566895
I0229 21:53:53.310212 140523092305664 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.39734411239624, loss=0.6817375421524048
I0229 21:54:27.088158 140522605770496 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.033230781555176, loss=0.7156369686126709
I0229 21:55:00.857584 140523092305664 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.59701681137085, loss=0.6843143701553345
I0229 21:55:34.633635 140522605770496 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.1131463050842285, loss=0.6747605204582214
I0229 21:56:08.489289 140523092305664 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.421874523162842, loss=0.6931958794593811
I0229 21:56:42.293784 140522605770496 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.727411270141602, loss=0.6989908218383789
I0229 21:57:16.060395 140523092305664 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.533280849456787, loss=0.7061793804168701
I0229 21:57:49.824622 140522605770496 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.376896858215332, loss=0.6370619535446167
I0229 21:58:23.568361 140523092305664 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.474177360534668, loss=0.6549561023712158
I0229 21:58:57.360903 140522605770496 logging_writer.py:48] [168800] global_step=168800, grad_norm=3.9818100929260254, loss=0.6170865297317505
I0229 21:59:31.130517 140523092305664 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.351502895355225, loss=0.6604247689247131
I0229 22:00:04.925948 140522605770496 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.9878883361816406, loss=0.6736787557601929
I0229 22:00:38.713580 140523092305664 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.339606285095215, loss=0.7219419479370117
I0229 22:00:55.086883 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:01:01.178447 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:01:09.875455 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:01:12.218447 140688601454400 submission_runner.py:411] Time since start: 59267.29s, 	Step: 169150, 	{'train/accuracy': 0.9476044178009033, 'train/loss': 0.19065237045288086, 'validation/accuracy': 0.7506200075149536, 'validation/loss': 1.0631998777389526, 'validation/num_examples': 50000, 'test/accuracy': 0.625, 'test/loss': 1.8395988941192627, 'test/num_examples': 10000, 'score': 57176.32164978981, 'total_duration': 59267.289298295975, 'accumulated_submission_time': 57176.32164978981, 'accumulated_eval_time': 2079.134115457535, 'accumulated_logging_time': 6.059413433074951}
I0229 22:01:12.264082 140522588985088 logging_writer.py:48] [169150] accumulated_eval_time=2079.134115, accumulated_logging_time=6.059413, accumulated_submission_time=57176.321650, global_step=169150, preemption_count=0, score=57176.321650, test/accuracy=0.625000, test/loss=1.839599, test/num_examples=10000, total_duration=59267.289298, train/accuracy=0.947604, train/loss=0.190652, validation/accuracy=0.750620, validation/loss=1.063200, validation/num_examples=50000
I0229 22:01:29.461780 140522597377792 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.127874851226807, loss=0.6240904331207275
I0229 22:02:03.169616 140522588985088 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.653617858886719, loss=0.6774670481681824
I0229 22:02:37.013870 140522597377792 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.658046722412109, loss=0.695239782333374
I0229 22:03:10.794022 140522588985088 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.026081085205078, loss=0.7128527164459229
I0229 22:03:44.554496 140522597377792 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.354574203491211, loss=0.6602773666381836
I0229 22:04:18.363511 140522588985088 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.481302738189697, loss=0.7350947856903076
I0229 22:04:52.175034 140522597377792 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.434589862823486, loss=0.6634906530380249
I0229 22:05:25.957827 140522588985088 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.462090969085693, loss=0.7680901885032654
I0229 22:05:59.751729 140522597377792 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.218068599700928, loss=0.6177520751953125
I0229 22:06:33.532402 140522588985088 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.442996025085449, loss=0.6478487253189087
I0229 22:07:07.323834 140522597377792 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.643443584442139, loss=0.6795673966407776
I0229 22:07:41.117810 140522588985088 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.347647190093994, loss=0.6552344560623169
I0229 22:08:14.970464 140522597377792 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.530552387237549, loss=0.710003137588501
I0229 22:08:48.778726 140522588985088 logging_writer.py:48] [170500] global_step=170500, grad_norm=3.7599995136260986, loss=0.5436922907829285
I0229 22:09:22.610963 140522597377792 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.248443603515625, loss=0.6770095229148865
I0229 22:09:42.346719 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:09:48.433518 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:09:56.930614 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:09:59.195394 140688601454400 submission_runner.py:411] Time since start: 59794.27s, 	Step: 170660, 	{'train/accuracy': 0.9497169852256775, 'train/loss': 0.18296679854393005, 'validation/accuracy': 0.7519999742507935, 'validation/loss': 1.061332106590271, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.8361148834228516, 'test/num_examples': 10000, 'score': 57686.33897137642, 'total_duration': 59794.26624298096, 'accumulated_submission_time': 57686.33897137642, 'accumulated_eval_time': 2095.9827468395233, 'accumulated_logging_time': 6.118293046951294}
I0229 22:09:59.243115 140525256554240 logging_writer.py:48] [170660] accumulated_eval_time=2095.982747, accumulated_logging_time=6.118293, accumulated_submission_time=57686.338971, global_step=170660, preemption_count=0, score=57686.338971, test/accuracy=0.625700, test/loss=1.836115, test/num_examples=10000, total_duration=59794.266243, train/accuracy=0.949717, train/loss=0.182967, validation/accuracy=0.752000, validation/loss=1.061332, validation/num_examples=50000
I0229 22:10:13.089322 140525264946944 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.326830863952637, loss=0.7082716226577759
I0229 22:10:46.857786 140525256554240 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.638217926025391, loss=0.6892321109771729
I0229 22:11:20.637870 140525264946944 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.6628522872924805, loss=0.6965689659118652
I0229 22:11:54.375869 140525256554240 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.496573448181152, loss=0.7063831090927124
I0229 22:12:28.186169 140525264946944 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.345559597015381, loss=0.609260082244873
I0229 22:13:01.958601 140525256554240 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.369704723358154, loss=0.6547731161117554
I0229 22:13:35.762802 140525264946944 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.495997428894043, loss=0.6150041222572327
I0229 22:14:09.510212 140525256554240 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.683816432952881, loss=0.7615877985954285
I0229 22:14:43.476069 140525264946944 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.942895889282227, loss=0.7733563184738159
I0229 22:15:17.257267 140525256554240 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.018374919891357, loss=0.681397557258606
I0229 22:15:51.055056 140525264946944 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.652283191680908, loss=0.7406313419342041
I0229 22:16:24.844991 140525256554240 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.989337921142578, loss=0.6773344278335571
I0229 22:16:58.625423 140525264946944 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.85420560836792, loss=0.6444438695907593
I0229 22:17:32.389519 140525256554240 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.308859348297119, loss=0.6186927556991577
I0229 22:18:06.177327 140525264946944 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.4969353675842285, loss=0.6890472173690796
I0229 22:18:29.270994 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:18:35.423040 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:18:44.221342 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:18:46.533432 140688601454400 submission_runner.py:411] Time since start: 60321.60s, 	Step: 172170, 	{'train/accuracy': 0.9553770422935486, 'train/loss': 0.1642269641160965, 'validation/accuracy': 0.7532199621200562, 'validation/loss': 1.0589675903320312, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8310208320617676, 'test/num_examples': 10000, 'score': 58196.301375865936, 'total_duration': 60321.604273319244, 'accumulated_submission_time': 58196.301375865936, 'accumulated_eval_time': 2113.2451345920563, 'accumulated_logging_time': 6.178622245788574}
I0229 22:18:46.582726 140522597377792 logging_writer.py:48] [172170] accumulated_eval_time=2113.245135, accumulated_logging_time=6.178622, accumulated_submission_time=58196.301376, global_step=172170, preemption_count=0, score=58196.301376, test/accuracy=0.627200, test/loss=1.831021, test/num_examples=10000, total_duration=60321.604273, train/accuracy=0.955377, train/loss=0.164227, validation/accuracy=0.753220, validation/loss=1.058968, validation/num_examples=50000
I0229 22:18:57.068336 140522605770496 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.440215587615967, loss=0.643027126789093
I0229 22:19:30.774987 140522597377792 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.63249397277832, loss=0.7182412147521973
I0229 22:20:04.511289 140522605770496 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.576632499694824, loss=0.754860520362854
I0229 22:20:38.363052 140522597377792 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.444958209991455, loss=0.6399973630905151
I0229 22:21:12.119033 140522605770496 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.489518165588379, loss=0.57277512550354
I0229 22:21:45.934404 140522597377792 logging_writer.py:48] [172700] global_step=172700, grad_norm=5.000790596008301, loss=0.7237117290496826
I0229 22:22:19.711445 140522605770496 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.600221633911133, loss=0.7029474377632141
I0229 22:22:53.523635 140522597377792 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.975079536437988, loss=0.7133711576461792
I0229 22:23:27.308390 140522605770496 logging_writer.py:48] [173000] global_step=173000, grad_norm=5.395133972167969, loss=0.7490243911743164
I0229 22:24:01.055922 140522597377792 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.53598165512085, loss=0.6657529473304749
I0229 22:24:34.824123 140522605770496 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.263998508453369, loss=0.6316661834716797
I0229 22:25:08.598315 140522597377792 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.5152506828308105, loss=0.6673126816749573
I0229 22:25:42.376644 140522605770496 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.252819538116455, loss=0.6209861040115356
I0229 22:26:16.187009 140522597377792 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.418410301208496, loss=0.6432435512542725
I0229 22:26:50.013318 140522605770496 logging_writer.py:48] [173600] global_step=173600, grad_norm=5.15371561050415, loss=0.6962440013885498
I0229 22:27:16.855525 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:27:23.018542 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:27:31.724482 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:27:34.002625 140688601454400 submission_runner.py:411] Time since start: 60849.07s, 	Step: 173681, 	{'train/accuracy': 0.9561144709587097, 'train/loss': 0.16302403807640076, 'validation/accuracy': 0.7536199688911438, 'validation/loss': 1.0499507188796997, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8309428691864014, 'test/num_examples': 10000, 'score': 58706.51188850403, 'total_duration': 60849.07347011566, 'accumulated_submission_time': 58706.51188850403, 'accumulated_eval_time': 2130.3921868801117, 'accumulated_logging_time': 6.2380759716033936}
I0229 22:27:34.048724 140525273339648 logging_writer.py:48] [173681] accumulated_eval_time=2130.392187, accumulated_logging_time=6.238076, accumulated_submission_time=58706.511889, global_step=173681, preemption_count=0, score=58706.511889, test/accuracy=0.629000, test/loss=1.830943, test/num_examples=10000, total_duration=60849.073470, train/accuracy=0.956114, train/loss=0.163024, validation/accuracy=0.753620, validation/loss=1.049951, validation/num_examples=50000
I0229 22:27:40.811272 140525281732352 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.590765476226807, loss=0.6754764318466187
I0229 22:28:14.528720 140525273339648 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.702563762664795, loss=0.6880789399147034
I0229 22:28:48.274560 140525281732352 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.546448230743408, loss=0.5634936094284058
I0229 22:29:22.055959 140525273339648 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.604216575622559, loss=0.610880970954895
I0229 22:29:55.828162 140525281732352 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.902711391448975, loss=0.6743344664573669
I0229 22:30:29.591999 140525273339648 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.640518665313721, loss=0.6790313720703125
I0229 22:31:03.378541 140525281732352 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.272895812988281, loss=0.5256086587905884
I0229 22:31:37.142536 140525273339648 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.13618803024292, loss=0.6090837121009827
I0229 22:32:10.934592 140525281732352 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.824007511138916, loss=0.7230655550956726
I0229 22:32:44.721270 140525273339648 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.471735954284668, loss=0.6914074420928955
I0229 22:33:18.580627 140525281732352 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.306844234466553, loss=0.7032968997955322
I0229 22:33:52.373633 140525273339648 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.871903419494629, loss=0.6973209381103516
I0229 22:34:26.141305 140525281732352 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.756619453430176, loss=0.6706701517105103
I0229 22:34:59.898164 140525273339648 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.935125350952148, loss=0.5950716733932495
I0229 22:35:33.679505 140525281732352 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.469750881195068, loss=0.6646694540977478
I0229 22:36:04.227341 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:36:10.467315 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:36:19.046730 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:36:21.308681 140688601454400 submission_runner.py:411] Time since start: 61376.38s, 	Step: 175192, 	{'train/accuracy': 0.9563137292861938, 'train/loss': 0.16133341193199158, 'validation/accuracy': 0.7529799938201904, 'validation/loss': 1.0494953393936157, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8195759057998657, 'test/num_examples': 10000, 'score': 59216.62665820122, 'total_duration': 61376.379529476166, 'accumulated_submission_time': 59216.62665820122, 'accumulated_eval_time': 2147.473479747772, 'accumulated_logging_time': 6.2952351570129395}
I0229 22:36:21.367430 140522597377792 logging_writer.py:48] [175192] accumulated_eval_time=2147.473480, accumulated_logging_time=6.295235, accumulated_submission_time=59216.626658, global_step=175192, preemption_count=0, score=59216.626658, test/accuracy=0.627600, test/loss=1.819576, test/num_examples=10000, total_duration=61376.379529, train/accuracy=0.956314, train/loss=0.161333, validation/accuracy=0.752980, validation/loss=1.049495, validation/num_examples=50000
I0229 22:36:24.417759 140522605770496 logging_writer.py:48] [175200] global_step=175200, grad_norm=5.029041290283203, loss=0.6419823169708252
I0229 22:36:58.124460 140522597377792 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.124393463134766, loss=0.5598925352096558
I0229 22:37:31.862078 140522605770496 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.318703651428223, loss=0.6831845641136169
I0229 22:38:05.618856 140522597377792 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.353082656860352, loss=0.6468562483787537
I0229 22:38:39.411706 140522605770496 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.71286678314209, loss=0.6712811589241028
I0229 22:39:13.217267 140522597377792 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.282066822052002, loss=0.6612195372581482
I0229 22:39:47.030757 140522605770496 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.083884239196777, loss=0.5705389976501465
I0229 22:40:20.822221 140522597377792 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.799162864685059, loss=0.675159752368927
I0229 22:40:54.610070 140522605770496 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.7685160636901855, loss=0.7827647924423218
I0229 22:41:28.389374 140522597377792 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.464943885803223, loss=0.6768597364425659
I0229 22:42:02.113654 140522605770496 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.2180023193359375, loss=0.6274393796920776
I0229 22:42:35.903341 140522597377792 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.540234565734863, loss=0.6286149621009827
I0229 22:43:09.732162 140522605770496 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.340577602386475, loss=0.6350926756858826
I0229 22:43:43.508731 140522597377792 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.430923938751221, loss=0.643498420715332
I0229 22:44:17.283196 140522605770496 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.442278861999512, loss=0.6745442152023315
I0229 22:44:51.084704 140522597377792 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.474059104919434, loss=0.6205723285675049
I0229 22:44:51.564268 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:44:57.677023 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:45:06.306490 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:45:08.899099 140688601454400 submission_runner.py:411] Time since start: 61903.97s, 	Step: 176703, 	{'train/accuracy': 0.9562739133834839, 'train/loss': 0.15974925458431244, 'validation/accuracy': 0.7539399862289429, 'validation/loss': 1.049542784690857, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8246986865997314, 'test/num_examples': 10000, 'score': 59726.75988483429, 'total_duration': 61903.96989917755, 'accumulated_submission_time': 59726.75988483429, 'accumulated_eval_time': 2164.8082122802734, 'accumulated_logging_time': 6.3645124435424805}
I0229 22:45:08.960126 140525256554240 logging_writer.py:48] [176703] accumulated_eval_time=2164.808212, accumulated_logging_time=6.364512, accumulated_submission_time=59726.759885, global_step=176703, preemption_count=0, score=59726.759885, test/accuracy=0.630100, test/loss=1.824699, test/num_examples=10000, total_duration=61903.969899, train/accuracy=0.956274, train/loss=0.159749, validation/accuracy=0.753940, validation/loss=1.049543, validation/num_examples=50000
I0229 22:45:42.014320 140525264946944 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.4704909324646, loss=0.6579272150993347
I0229 22:46:15.748081 140525256554240 logging_writer.py:48] [176900] global_step=176900, grad_norm=5.28399133682251, loss=0.6526342630386353
I0229 22:46:49.518636 140525264946944 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.609574317932129, loss=0.6362249255180359
I0229 22:47:23.312507 140525256554240 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.740291595458984, loss=0.6137207746505737
I0229 22:47:57.095067 140525264946944 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.441562652587891, loss=0.6664093136787415
I0229 22:48:31.168100 140525256554240 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.510894298553467, loss=0.5909972190856934
I0229 22:49:04.955314 140525264946944 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.331772804260254, loss=0.6340253353118896
I0229 22:49:38.743461 140525256554240 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.428234577178955, loss=0.6132485866546631
I0229 22:50:12.531462 140525264946944 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.312109470367432, loss=0.6322381496429443
I0229 22:50:46.310178 140525256554240 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.548003673553467, loss=0.6817312240600586
I0229 22:51:20.138004 140525264946944 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.757516860961914, loss=0.5973451733589172
I0229 22:51:53.951302 140525256554240 logging_writer.py:48] [177900] global_step=177900, grad_norm=5.215201377868652, loss=0.6769130229949951
I0229 22:52:27.722368 140525264946944 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.673346996307373, loss=0.6741855144500732
I0229 22:53:01.507739 140525256554240 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.687349319458008, loss=0.6312482357025146
I0229 22:53:35.279960 140525264946944 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.7248854637146, loss=0.6018343567848206
I0229 22:53:39.144871 140688601454400 spec.py:321] Evaluating on the training split.
I0229 22:53:45.467242 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 22:53:54.007000 140688601454400 spec.py:349] Evaluating on the test split.
I0229 22:53:56.406564 140688601454400 submission_runner.py:411] Time since start: 62431.48s, 	Step: 178213, 	{'train/accuracy': 0.9578084945678711, 'train/loss': 0.1552063673734665, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0473157167434692, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.8242660760879517, 'test/num_examples': 10000, 'score': 60236.88195681572, 'total_duration': 62431.477404117584, 'accumulated_submission_time': 60236.88195681572, 'accumulated_eval_time': 2182.069860935211, 'accumulated_logging_time': 6.435606241226196}
I0229 22:53:56.467989 140522605770496 logging_writer.py:48] [178213] accumulated_eval_time=2182.069861, accumulated_logging_time=6.435606, accumulated_submission_time=60236.881957, global_step=178213, preemption_count=0, score=60236.881957, test/accuracy=0.628300, test/loss=1.824266, test/num_examples=10000, total_duration=62431.477404, train/accuracy=0.957808, train/loss=0.155206, validation/accuracy=0.754920, validation/loss=1.047316, validation/num_examples=50000
I0229 22:54:26.218903 140523092305664 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.587405681610107, loss=0.604022741317749
I0229 22:54:59.937374 140522605770496 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.99914026260376, loss=0.595567524433136
I0229 22:55:33.744684 140523092305664 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.387895584106445, loss=0.5945380926132202
I0229 22:56:07.503701 140522605770496 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.57379674911499, loss=0.6270709037780762
I0229 22:56:41.302244 140523092305664 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.546781063079834, loss=0.6527276039123535
I0229 22:57:15.048347 140522605770496 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.3112969398498535, loss=0.5519615411758423
I0229 22:57:48.900813 140523092305664 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.446074485778809, loss=0.6312181949615479
I0229 22:58:22.668756 140522605770496 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.623193264007568, loss=0.6718521118164062
I0229 22:58:56.424223 140523092305664 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.638833522796631, loss=0.6336218118667603
I0229 22:59:30.217157 140522605770496 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.790469169616699, loss=0.6574849486351013
I0229 23:00:04.002330 140523092305664 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.869815349578857, loss=0.7033914923667908
I0229 23:00:37.778401 140522605770496 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.804437160491943, loss=0.6733664870262146
I0229 23:01:11.582412 140523092305664 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.240050315856934, loss=0.6182411909103394
I0229 23:01:45.381438 140522605770496 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.581494331359863, loss=0.6086825132369995
I0229 23:02:19.203417 140523092305664 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.762608051300049, loss=0.6205282211303711
I0229 23:02:26.445107 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:02:32.602060 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:02:41.329054 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:02:43.603288 140688601454400 submission_runner.py:411] Time since start: 62958.67s, 	Step: 179723, 	{'train/accuracy': 0.9579480290412903, 'train/loss': 0.15453115105628967, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0462963581085205, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.826785683631897, 'test/num_examples': 10000, 'score': 60746.79544234276, 'total_duration': 62958.6741335392, 'accumulated_submission_time': 60746.79544234276, 'accumulated_eval_time': 2199.227989912033, 'accumulated_logging_time': 6.507908821105957}
I0229 23:02:43.650593 140522588985088 logging_writer.py:48] [179723] accumulated_eval_time=2199.227990, accumulated_logging_time=6.507909, accumulated_submission_time=60746.795442, global_step=179723, preemption_count=0, score=60746.795442, test/accuracy=0.628900, test/loss=1.826786, test/num_examples=10000, total_duration=62958.674134, train/accuracy=0.957948, train/loss=0.154531, validation/accuracy=0.755700, validation/loss=1.046296, validation/num_examples=50000
I0229 23:03:09.949821 140522597377792 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.401269435882568, loss=0.5494526028633118
I0229 23:03:43.740175 140522588985088 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.516794204711914, loss=0.6057491302490234
I0229 23:04:17.549466 140522597377792 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.4852495193481445, loss=0.6709667444229126
I0229 23:04:51.335677 140522588985088 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.9069061279296875, loss=0.6678459644317627
I0229 23:05:25.139546 140522597377792 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.670009136199951, loss=0.6552206873893738
I0229 23:05:58.923933 140522588985088 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.467976093292236, loss=0.6579266786575317
I0229 23:06:32.694580 140522597377792 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.906704425811768, loss=0.5820057392120361
I0229 23:07:06.497035 140522588985088 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.809183597564697, loss=0.6962398290634155
I0229 23:07:40.227569 140522597377792 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.605619430541992, loss=0.676895797252655
I0229 23:08:14.003782 140522588985088 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.787228584289551, loss=0.6199654936790466
I0229 23:08:47.809961 140522597377792 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.9709577560424805, loss=0.6835864782333374
I0229 23:09:21.622123 140522588985088 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.214043617248535, loss=0.57155841588974
I0229 23:09:55.476735 140522597377792 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.4710869789123535, loss=0.6340924501419067
I0229 23:10:29.243171 140522588985088 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.503526210784912, loss=0.5997290015220642
I0229 23:11:03.006949 140522597377792 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.766181945800781, loss=0.6545163989067078
I0229 23:11:13.640281 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:11:19.736207 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:11:28.325245 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:11:30.609951 140688601454400 submission_runner.py:411] Time since start: 63485.68s, 	Step: 181233, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.14606840908527374, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0432047843933105, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.820835828781128, 'test/num_examples': 10000, 'score': 61256.721252441406, 'total_duration': 63485.68079519272, 'accumulated_submission_time': 61256.721252441406, 'accumulated_eval_time': 2216.197611093521, 'accumulated_logging_time': 6.566876411437988}
I0229 23:11:30.657559 140525256554240 logging_writer.py:48] [181233] accumulated_eval_time=2216.197611, accumulated_logging_time=6.566876, accumulated_submission_time=61256.721252, global_step=181233, preemption_count=0, score=61256.721252, test/accuracy=0.627800, test/loss=1.820836, test/num_examples=10000, total_duration=63485.680795, train/accuracy=0.961376, train/loss=0.146068, validation/accuracy=0.754760, validation/loss=1.043205, validation/num_examples=50000
I0229 23:11:53.609632 140525264946944 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.483191967010498, loss=0.6355293989181519
I0229 23:12:27.337728 140525256554240 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.538093566894531, loss=0.686476469039917
I0229 23:13:01.091621 140525264946944 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.691883087158203, loss=0.5974069237709045
I0229 23:13:34.864156 140525256554240 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.685550689697266, loss=0.6529726386070251
I0229 23:14:08.651546 140525264946944 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.238198757171631, loss=0.6763072609901428
I0229 23:14:42.420961 140525256554240 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.416998863220215, loss=0.5950989723205566
I0229 23:15:16.226582 140525264946944 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.166130542755127, loss=0.6047621965408325
I0229 23:15:50.012911 140525256554240 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.263840198516846, loss=0.6049073934555054
I0229 23:16:23.854900 140525264946944 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.46599006652832, loss=0.6282933950424194
I0229 23:16:57.640910 140525256554240 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.537115573883057, loss=0.6224247813224792
I0229 23:17:31.424504 140525264946944 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.46665096282959, loss=0.6358000040054321
I0229 23:18:05.197991 140525256554240 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.451717376708984, loss=0.6434328556060791
I0229 23:18:39.017709 140525264946944 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.565958023071289, loss=0.7133687734603882
I0229 23:19:12.795539 140525256554240 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.422542095184326, loss=0.5839517116546631
I0229 23:19:46.596569 140525264946944 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.408926010131836, loss=0.5684337019920349
I0229 23:20:00.629564 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:20:06.870353 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:20:15.633513 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:20:18.008771 140688601454400 submission_runner.py:411] Time since start: 64013.08s, 	Step: 182743, 	{'train/accuracy': 0.9588847160339355, 'train/loss': 0.15178032219409943, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0448073148727417, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.822758674621582, 'test/num_examples': 10000, 'score': 61766.63048315048, 'total_duration': 64013.07961964607, 'accumulated_submission_time': 61766.63048315048, 'accumulated_eval_time': 2233.5767714977264, 'accumulated_logging_time': 6.625716686248779}
I0229 23:20:18.058782 140523092305664 logging_writer.py:48] [182743] accumulated_eval_time=2233.576771, accumulated_logging_time=6.625717, accumulated_submission_time=61766.630483, global_step=182743, preemption_count=0, score=61766.630483, test/accuracy=0.630100, test/loss=1.822759, test/num_examples=10000, total_duration=64013.079620, train/accuracy=0.958885, train/loss=0.151780, validation/accuracy=0.755280, validation/loss=1.044807, validation/num_examples=50000
I0229 23:20:37.631623 140523947947776 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.252545356750488, loss=0.6492477655410767
I0229 23:21:11.424667 140523092305664 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.340684413909912, loss=0.6093586683273315
I0229 23:21:45.183858 140523947947776 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.332507133483887, loss=0.6056252121925354
I0229 23:22:19.018225 140523092305664 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.745493412017822, loss=0.5956628918647766
I0229 23:22:52.800542 140523947947776 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.200106143951416, loss=0.582744836807251
I0229 23:23:26.628308 140523092305664 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.318632125854492, loss=0.6190009117126465
I0229 23:24:00.415419 140523947947776 logging_writer.py:48] [183400] global_step=183400, grad_norm=3.961252212524414, loss=0.5402942299842834
I0229 23:24:34.200251 140523092305664 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.395366191864014, loss=0.5770208835601807
I0229 23:25:07.975340 140523947947776 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.4891862869262695, loss=0.5973341464996338
I0229 23:25:41.775713 140523092305664 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.136291027069092, loss=0.5787110924720764
I0229 23:26:15.564232 140523947947776 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.656787395477295, loss=0.6244177222251892
I0229 23:26:49.340278 140523092305664 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.437825679779053, loss=0.5710068941116333
I0229 23:27:23.143593 140523947947776 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.435572147369385, loss=0.6236485838890076
I0229 23:27:56.909971 140523092305664 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.405055522918701, loss=0.687149167060852
I0229 23:28:30.833880 140523947947776 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.537154197692871, loss=0.6095829606056213
I0229 23:28:48.192001 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:28:54.345629 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:29:03.175177 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:29:05.446175 140688601454400 submission_runner.py:411] Time since start: 64540.52s, 	Step: 184253, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.1485065221786499, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0438897609710693, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8217799663543701, 'test/num_examples': 10000, 'score': 62276.70138978958, 'total_duration': 64540.51701974869, 'accumulated_submission_time': 62276.70138978958, 'accumulated_eval_time': 2250.8308987617493, 'accumulated_logging_time': 6.685147285461426}
I0229 23:29:05.497831 140522588985088 logging_writer.py:48] [184253] accumulated_eval_time=2250.830899, accumulated_logging_time=6.685147, accumulated_submission_time=62276.701390, global_step=184253, preemption_count=0, score=62276.701390, test/accuracy=0.630700, test/loss=1.821780, test/num_examples=10000, total_duration=64540.517020, train/accuracy=0.960658, train/loss=0.148507, validation/accuracy=0.755340, validation/loss=1.043890, validation/num_examples=50000
I0229 23:29:21.700301 140522597377792 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.711544513702393, loss=0.6015719175338745
I0229 23:29:55.413822 140522588985088 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.722975730895996, loss=0.7414471507072449
I0229 23:30:29.174925 140522597377792 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.5942254066467285, loss=0.6260600686073303
I0229 23:31:02.962152 140522588985088 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.253427505493164, loss=0.5973101258277893
I0229 23:31:36.773595 140522597377792 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.263209819793701, loss=0.5841107368469238
I0229 23:32:10.574140 140522588985088 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.367469787597656, loss=0.6565176844596863
I0229 23:32:44.359896 140522597377792 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.768863677978516, loss=0.6832363605499268
I0229 23:33:18.121268 140522588985088 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.278886795043945, loss=0.553070604801178
I0229 23:33:51.900393 140522597377792 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.531182765960693, loss=0.5845711827278137
I0229 23:34:25.854666 140522588985088 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.453612327575684, loss=0.6001046895980835
I0229 23:34:59.636157 140522597377792 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.312819480895996, loss=0.5756136775016785
I0229 23:35:33.431427 140522588985088 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.044680595397949, loss=0.567759096622467
I0229 23:36:07.200695 140522597377792 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.482217788696289, loss=0.5900241732597351
I0229 23:36:40.970308 140522588985088 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.230076789855957, loss=0.570993185043335
I0229 23:37:14.742087 140522597377792 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.486318111419678, loss=0.580486536026001
I0229 23:37:35.497481 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:37:41.716861 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:37:50.089657 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:37:52.359954 140688601454400 submission_runner.py:411] Time since start: 65067.43s, 	Step: 185763, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14893734455108643, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0433369874954224, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8208897113800049, 'test/num_examples': 10000, 'score': 62786.637889146805, 'total_duration': 65067.43080019951, 'accumulated_submission_time': 62786.637889146805, 'accumulated_eval_time': 2267.693324804306, 'accumulated_logging_time': 6.7476887702941895}
I0229 23:37:52.410930 140523947947776 logging_writer.py:48] [185763] accumulated_eval_time=2267.693325, accumulated_logging_time=6.747689, accumulated_submission_time=62786.637889, global_step=185763, preemption_count=0, score=62786.637889, test/accuracy=0.631000, test/loss=1.820890, test/num_examples=10000, total_duration=65067.430800, train/accuracy=0.960579, train/loss=0.148937, validation/accuracy=0.755360, validation/loss=1.043337, validation/num_examples=50000
I0229 23:38:05.209036 140525264946944 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.494175910949707, loss=0.6019173860549927
I0229 23:38:38.955528 140523947947776 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.7939629554748535, loss=0.6225222945213318
I0229 23:39:12.689481 140525264946944 logging_writer.py:48] [186000] global_step=186000, grad_norm=4.423505783081055, loss=0.637647271156311
I0229 23:39:46.488310 140523947947776 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.443599224090576, loss=0.5979931354522705
I0229 23:40:20.262598 140525264946944 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.375853061676025, loss=0.6463028788566589
I0229 23:40:54.143151 140523947947776 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.260663986206055, loss=0.5686482191085815
I0229 23:41:27.892210 140525264946944 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.524010181427002, loss=0.6452576518058777
I0229 23:42:01.590954 140523947947776 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.328956127166748, loss=0.5690033435821533
I0229 23:42:35.376643 140525264946944 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.474537372589111, loss=0.579537570476532
I0229 23:43:09.144868 140523947947776 logging_writer.py:48] [186700] global_step=186700, grad_norm=4.781566619873047, loss=0.6949526071548462
I0229 23:43:42.879574 140525264946944 logging_writer.py:48] [186800] global_step=186800, grad_norm=4.427960395812988, loss=0.6086536645889282
I0229 23:44:16.674067 140523947947776 logging_writer.py:48] [186900] global_step=186900, grad_norm=4.568332195281982, loss=0.6116176843643188
I0229 23:44:50.463375 140525264946944 logging_writer.py:48] [187000] global_step=187000, grad_norm=4.577716827392578, loss=0.6440309286117554
I0229 23:45:24.251739 140523947947776 logging_writer.py:48] [187100] global_step=187100, grad_norm=4.291989803314209, loss=0.6411089301109314
I0229 23:45:58.066979 140525264946944 logging_writer.py:48] [187200] global_step=187200, grad_norm=4.65154504776001, loss=0.6385148763656616
I0229 23:46:22.544621 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:46:28.652182 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:46:37.301964 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:46:39.575631 140688601454400 submission_runner.py:411] Time since start: 65594.65s, 	Step: 187274, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14376719295978546, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.0444318056106567, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.822888970375061, 'test/num_examples': 10000, 'score': 63296.7076792717, 'total_duration': 65594.64647817612, 'accumulated_submission_time': 63296.7076792717, 'accumulated_eval_time': 2284.7242891788483, 'accumulated_logging_time': 6.810681581497192}
I0229 23:46:39.631131 140522597377792 logging_writer.py:48] [187274] accumulated_eval_time=2284.724289, accumulated_logging_time=6.810682, accumulated_submission_time=63296.707679, global_step=187274, preemption_count=0, score=63296.707679, test/accuracy=0.631000, test/loss=1.822889, test/num_examples=10000, total_duration=65594.646478, train/accuracy=0.961496, train/loss=0.143767, validation/accuracy=0.755300, validation/loss=1.044432, validation/num_examples=50000
I0229 23:46:48.766454 140522605770496 logging_writer.py:48] [187300] global_step=187300, grad_norm=4.131113052368164, loss=0.6124048233032227
I0229 23:47:22.477442 140522597377792 logging_writer.py:48] [187400] global_step=187400, grad_norm=4.277927875518799, loss=0.6337713003158569
I0229 23:47:56.259426 140522605770496 logging_writer.py:48] [187500] global_step=187500, grad_norm=4.267599582672119, loss=0.5247005224227905
I0229 23:48:30.014694 140522597377792 logging_writer.py:48] [187600] global_step=187600, grad_norm=4.683095932006836, loss=0.6396709680557251
I0229 23:49:03.754806 140522605770496 logging_writer.py:48] [187700] global_step=187700, grad_norm=4.803022384643555, loss=0.7008631825447083
I0229 23:49:37.527580 140522597377792 logging_writer.py:48] [187800] global_step=187800, grad_norm=4.3841142654418945, loss=0.6454065442085266
I0229 23:50:11.319474 140522605770496 logging_writer.py:48] [187900] global_step=187900, grad_norm=4.291182994842529, loss=0.6269827485084534
I0229 23:50:45.097516 140522597377792 logging_writer.py:48] [188000] global_step=188000, grad_norm=4.4111127853393555, loss=0.6295981407165527
I0229 23:51:18.875898 140522605770496 logging_writer.py:48] [188100] global_step=188100, grad_norm=4.434585094451904, loss=0.6972861289978027
I0229 23:51:52.654676 140522597377792 logging_writer.py:48] [188200] global_step=188200, grad_norm=4.451084136962891, loss=0.6102150678634644
I0229 23:52:26.472436 140522605770496 logging_writer.py:48] [188300] global_step=188300, grad_norm=4.091097354888916, loss=0.6443926095962524
I0229 23:53:00.424614 140522597377792 logging_writer.py:48] [188400] global_step=188400, grad_norm=5.068065643310547, loss=0.6417272686958313
I0229 23:53:34.215066 140522605770496 logging_writer.py:48] [188500] global_step=188500, grad_norm=4.755955219268799, loss=0.6673564314842224
I0229 23:54:08.001485 140522597377792 logging_writer.py:48] [188600] global_step=188600, grad_norm=4.851069927215576, loss=0.6617536544799805
I0229 23:54:41.796753 140522605770496 logging_writer.py:48] [188700] global_step=188700, grad_norm=4.314964294433594, loss=0.6175782680511475
I0229 23:55:09.632044 140688601454400 spec.py:321] Evaluating on the training split.
I0229 23:55:15.747058 140688601454400 spec.py:333] Evaluating on the validation split.
I0229 23:55:24.417507 140688601454400 spec.py:349] Evaluating on the test split.
I0229 23:55:26.677249 140688601454400 submission_runner.py:411] Time since start: 66121.75s, 	Step: 188784, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14800244569778442, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0436111688613892, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8203967809677124, 'test/num_examples': 10000, 'score': 63806.64422917366, 'total_duration': 66121.7480969429, 'accumulated_submission_time': 63806.64422917366, 'accumulated_eval_time': 2301.769454240799, 'accumulated_logging_time': 6.878018856048584}
I0229 23:55:26.729163 140525273339648 logging_writer.py:48] [188784] accumulated_eval_time=2301.769454, accumulated_logging_time=6.878019, accumulated_submission_time=63806.644229, global_step=188784, preemption_count=0, score=63806.644229, test/accuracy=0.630500, test/loss=1.820397, test/num_examples=10000, total_duration=66121.748097, train/accuracy=0.960858, train/loss=0.148002, validation/accuracy=0.755100, validation/loss=1.043611, validation/num_examples=50000
I0229 23:55:32.469044 140525281732352 logging_writer.py:48] [188800] global_step=188800, grad_norm=4.362667560577393, loss=0.6079380512237549
I0229 23:56:06.219601 140525273339648 logging_writer.py:48] [188900] global_step=188900, grad_norm=4.7782182693481445, loss=0.6632447242736816
I0229 23:56:39.971380 140525281732352 logging_writer.py:48] [189000] global_step=189000, grad_norm=4.615352153778076, loss=0.6589385867118835
I0229 23:57:13.778958 140525273339648 logging_writer.py:48] [189100] global_step=189100, grad_norm=4.651597499847412, loss=0.6812794208526611
I0229 23:57:47.580147 140525281732352 logging_writer.py:48] [189200] global_step=189200, grad_norm=4.390727519989014, loss=0.6222116947174072
I0229 23:58:21.388147 140525273339648 logging_writer.py:48] [189300] global_step=189300, grad_norm=4.20810079574585, loss=0.6154476404190063
I0229 23:58:55.264894 140525281732352 logging_writer.py:48] [189400] global_step=189400, grad_norm=4.548173904418945, loss=0.7644079923629761
I0229 23:59:29.027888 140525273339648 logging_writer.py:48] [189500] global_step=189500, grad_norm=5.352479457855225, loss=0.7344743609428406
I0301 00:00:02.831928 140525281732352 logging_writer.py:48] [189600] global_step=189600, grad_norm=4.4127583503723145, loss=0.5619512796401978
I0301 00:00:36.612495 140525273339648 logging_writer.py:48] [189700] global_step=189700, grad_norm=4.465992450714111, loss=0.5742669105529785
I0301 00:01:10.416118 140525281732352 logging_writer.py:48] [189800] global_step=189800, grad_norm=4.30614709854126, loss=0.6285336017608643
I0301 00:01:44.200176 140525273339648 logging_writer.py:48] [189900] global_step=189900, grad_norm=4.777441501617432, loss=0.7283174395561218
I0301 00:02:17.990365 140525281732352 logging_writer.py:48] [190000] global_step=190000, grad_norm=4.735174655914307, loss=0.6317289471626282
I0301 00:02:51.808535 140525273339648 logging_writer.py:48] [190100] global_step=190100, grad_norm=4.602499008178711, loss=0.5927013754844666
I0301 00:03:25.583046 140525281732352 logging_writer.py:48] [190200] global_step=190200, grad_norm=4.18631649017334, loss=0.539060652256012
I0301 00:03:56.822281 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:04:03.049661 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:04:11.774622 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:04:14.051123 140688601454400 submission_runner.py:411] Time since start: 66649.12s, 	Step: 190294, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14849664270877838, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0441384315490723, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8202807903289795, 'test/num_examples': 10000, 'score': 64316.67609715462, 'total_duration': 66649.12196302414, 'accumulated_submission_time': 64316.67609715462, 'accumulated_eval_time': 2318.9982414245605, 'accumulated_logging_time': 6.939778089523315}
I0301 00:04:14.099348 140522605770496 logging_writer.py:48] [190294] accumulated_eval_time=2318.998241, accumulated_logging_time=6.939778, accumulated_submission_time=64316.676097, global_step=190294, preemption_count=0, score=64316.676097, test/accuracy=0.630800, test/loss=1.820281, test/num_examples=10000, total_duration=66649.121963, train/accuracy=0.960200, train/loss=0.148497, validation/accuracy=0.755060, validation/loss=1.044138, validation/num_examples=50000
I0301 00:04:16.462904 140523092305664 logging_writer.py:48] [190300] global_step=190300, grad_norm=4.431865692138672, loss=0.6519296765327454
I0301 00:04:50.242555 140522605770496 logging_writer.py:48] [190400] global_step=190400, grad_norm=4.4002766609191895, loss=0.6375249624252319
I0301 00:05:24.137771 140523092305664 logging_writer.py:48] [190500] global_step=190500, grad_norm=4.563024044036865, loss=0.5963099002838135
I0301 00:05:57.904002 140522605770496 logging_writer.py:48] [190600] global_step=190600, grad_norm=4.541640281677246, loss=0.6162042021751404
I0301 00:06:31.666821 140523092305664 logging_writer.py:48] [190700] global_step=190700, grad_norm=4.38450813293457, loss=0.657315731048584
I0301 00:07:05.490160 140522605770496 logging_writer.py:48] [190800] global_step=190800, grad_norm=4.797049522399902, loss=0.5821670293807983
I0301 00:07:39.298551 140523092305664 logging_writer.py:48] [190900] global_step=190900, grad_norm=4.25480842590332, loss=0.5818996429443359
I0301 00:08:13.122215 140522605770496 logging_writer.py:48] [191000] global_step=191000, grad_norm=4.506539344787598, loss=0.6497658491134644
I0301 00:08:46.903804 140523092305664 logging_writer.py:48] [191100] global_step=191100, grad_norm=4.708872318267822, loss=0.6884182095527649
I0301 00:09:20.674274 140522605770496 logging_writer.py:48] [191200] global_step=191200, grad_norm=4.489592552185059, loss=0.5182350277900696
I0301 00:09:54.451800 140523092305664 logging_writer.py:48] [191300] global_step=191300, grad_norm=4.6222243309021, loss=0.6142855286598206
I0301 00:10:28.189517 140522605770496 logging_writer.py:48] [191400] global_step=191400, grad_norm=4.534037113189697, loss=0.5963797569274902
I0301 00:11:02.002968 140523092305664 logging_writer.py:48] [191500] global_step=191500, grad_norm=4.356947422027588, loss=0.5977724194526672
I0301 00:11:35.906983 140522605770496 logging_writer.py:48] [191600] global_step=191600, grad_norm=4.485701560974121, loss=0.6770353317260742
I0301 00:12:09.670611 140523092305664 logging_writer.py:48] [191700] global_step=191700, grad_norm=4.558224678039551, loss=0.7008737325668335
I0301 00:12:43.384911 140522605770496 logging_writer.py:48] [191800] global_step=191800, grad_norm=4.536302089691162, loss=0.5651078820228577
I0301 00:12:44.207679 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:12:51.045721 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:12:59.624221 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:13:01.942712 140688601454400 submission_runner.py:411] Time since start: 67177.01s, 	Step: 191804, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.14915305376052856, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.045419692993164, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8232477903366089, 'test/num_examples': 10000, 'score': 64826.72256541252, 'total_duration': 67177.01352787018, 'accumulated_submission_time': 64826.72256541252, 'accumulated_eval_time': 2336.7331914901733, 'accumulated_logging_time': 6.99896240234375}
I0301 00:13:02.011089 140522597377792 logging_writer.py:48] [191804] accumulated_eval_time=2336.733191, accumulated_logging_time=6.998962, accumulated_submission_time=64826.722565, global_step=191804, preemption_count=0, score=64826.722565, test/accuracy=0.630300, test/loss=1.823248, test/num_examples=10000, total_duration=67177.013528, train/accuracy=0.959901, train/loss=0.149153, validation/accuracy=0.754920, validation/loss=1.045420, validation/num_examples=50000
I0301 00:13:34.734695 140522605770496 logging_writer.py:48] [191900] global_step=191900, grad_norm=4.375216960906982, loss=0.6189175844192505
I0301 00:14:08.495959 140522597377792 logging_writer.py:48] [192000] global_step=192000, grad_norm=4.491387367248535, loss=0.594056248664856
I0301 00:14:42.290365 140522605770496 logging_writer.py:48] [192100] global_step=192100, grad_norm=4.232978343963623, loss=0.5748810768127441
I0301 00:15:16.071984 140522597377792 logging_writer.py:48] [192200] global_step=192200, grad_norm=4.626129627227783, loss=0.6550023555755615
I0301 00:15:49.822655 140522605770496 logging_writer.py:48] [192300] global_step=192300, grad_norm=4.505635738372803, loss=0.6635599136352539
I0301 00:16:23.587229 140522597377792 logging_writer.py:48] [192400] global_step=192400, grad_norm=4.344631671905518, loss=0.603541374206543
I0301 00:16:57.350373 140522605770496 logging_writer.py:48] [192500] global_step=192500, grad_norm=4.690820217132568, loss=0.6413487195968628
I0301 00:17:31.226249 140522597377792 logging_writer.py:48] [192600] global_step=192600, grad_norm=4.877740859985352, loss=0.7107359766960144
I0301 00:18:04.986995 140522605770496 logging_writer.py:48] [192700] global_step=192700, grad_norm=4.461272716522217, loss=0.6358872652053833
I0301 00:18:38.785727 140522597377792 logging_writer.py:48] [192800] global_step=192800, grad_norm=4.1243085861206055, loss=0.5748640298843384
I0301 00:19:12.556314 140522605770496 logging_writer.py:48] [192900] global_step=192900, grad_norm=4.850722312927246, loss=0.7477978467941284
I0301 00:19:46.352592 140522597377792 logging_writer.py:48] [193000] global_step=193000, grad_norm=5.190872669219971, loss=0.6597740650177002
I0301 00:20:20.144809 140522605770496 logging_writer.py:48] [193100] global_step=193100, grad_norm=4.627836227416992, loss=0.5996608734130859
I0301 00:20:53.962464 140522597377792 logging_writer.py:48] [193200] global_step=193200, grad_norm=4.550106048583984, loss=0.6654497385025024
I0301 00:21:27.736542 140522605770496 logging_writer.py:48] [193300] global_step=193300, grad_norm=4.1342058181762695, loss=0.6144521832466125
I0301 00:21:32.270038 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:21:38.339537 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:21:47.024580 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:21:49.309406 140688601454400 submission_runner.py:411] Time since start: 67704.38s, 	Step: 193315, 	{'train/accuracy': 0.9594427347183228, 'train/loss': 0.15063251554965973, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.0440016984939575, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8218053579330444, 'test/num_examples': 10000, 'score': 65336.91848921776, 'total_duration': 67704.38000226021, 'accumulated_submission_time': 65336.91848921776, 'accumulated_eval_time': 2353.7722561359406, 'accumulated_logging_time': 7.07848596572876}
I0301 00:21:49.363059 140522597377792 logging_writer.py:48] [193315] accumulated_eval_time=2353.772256, accumulated_logging_time=7.078486, accumulated_submission_time=65336.918489, global_step=193315, preemption_count=0, score=65336.918489, test/accuracy=0.630000, test/loss=1.821805, test/num_examples=10000, total_duration=67704.380002, train/accuracy=0.959443, train/loss=0.150633, validation/accuracy=0.755320, validation/loss=1.044002, validation/num_examples=50000
I0301 00:22:18.390417 140525256554240 logging_writer.py:48] [193400] global_step=193400, grad_norm=4.736012935638428, loss=0.6553820967674255
I0301 00:22:52.104741 140522597377792 logging_writer.py:48] [193500] global_step=193500, grad_norm=4.178778648376465, loss=0.6318613290786743
I0301 00:23:25.872638 140525256554240 logging_writer.py:48] [193600] global_step=193600, grad_norm=4.910080909729004, loss=0.6577954888343811
I0301 00:23:59.761697 140522597377792 logging_writer.py:48] [193700] global_step=193700, grad_norm=4.613595485687256, loss=0.7490453124046326
I0301 00:24:33.551280 140525256554240 logging_writer.py:48] [193800] global_step=193800, grad_norm=4.622559070587158, loss=0.6121764183044434
I0301 00:25:07.358931 140522597377792 logging_writer.py:48] [193900] global_step=193900, grad_norm=4.4367170333862305, loss=0.6727192997932434
I0301 00:25:41.140601 140525256554240 logging_writer.py:48] [194000] global_step=194000, grad_norm=4.5084733963012695, loss=0.6425961256027222
I0301 00:26:14.907938 140522597377792 logging_writer.py:48] [194100] global_step=194100, grad_norm=4.6268439292907715, loss=0.633280873298645
I0301 00:26:48.665135 140525256554240 logging_writer.py:48] [194200] global_step=194200, grad_norm=4.4651875495910645, loss=0.6127473711967468
I0301 00:27:22.417725 140522597377792 logging_writer.py:48] [194300] global_step=194300, grad_norm=4.512773513793945, loss=0.668791651725769
I0301 00:27:56.223292 140525256554240 logging_writer.py:48] [194400] global_step=194400, grad_norm=4.27937126159668, loss=0.6870253682136536
I0301 00:28:30.028087 140522597377792 logging_writer.py:48] [194500] global_step=194500, grad_norm=4.668524742126465, loss=0.6504923105239868
I0301 00:29:03.853465 140525256554240 logging_writer.py:48] [194600] global_step=194600, grad_norm=4.51475715637207, loss=0.6126421689987183
I0301 00:29:37.733102 140522597377792 logging_writer.py:48] [194700] global_step=194700, grad_norm=4.5905280113220215, loss=0.5958809852600098
I0301 00:30:11.487760 140525256554240 logging_writer.py:48] [194800] global_step=194800, grad_norm=4.183020114898682, loss=0.5978790521621704
I0301 00:30:19.371944 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:30:25.556750 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:30:34.056435 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:30:36.341600 140688601454400 submission_runner.py:411] Time since start: 68231.41s, 	Step: 194825, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.1465877741575241, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0440150499343872, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.820330023765564, 'test/num_examples': 10000, 'score': 65846.86537241936, 'total_duration': 68231.4124391079, 'accumulated_submission_time': 65846.86537241936, 'accumulated_eval_time': 2370.7418541908264, 'accumulated_logging_time': 7.142187833786011}
I0301 00:30:36.397289 140523092305664 logging_writer.py:48] [194825] accumulated_eval_time=2370.741854, accumulated_logging_time=7.142188, accumulated_submission_time=65846.865372, global_step=194825, preemption_count=0, score=65846.865372, test/accuracy=0.631200, test/loss=1.820330, test/num_examples=10000, total_duration=68231.412439, train/accuracy=0.961037, train/loss=0.146588, validation/accuracy=0.754920, validation/loss=1.044015, validation/num_examples=50000
I0301 00:31:02.014081 140523947947776 logging_writer.py:48] [194900] global_step=194900, grad_norm=4.42645788192749, loss=0.6174666285514832
I0301 00:31:35.760906 140523092305664 logging_writer.py:48] [195000] global_step=195000, grad_norm=4.534543037414551, loss=0.6658791303634644
I0301 00:32:09.531462 140523947947776 logging_writer.py:48] [195100] global_step=195100, grad_norm=4.820900917053223, loss=0.6571439504623413
I0301 00:32:43.340575 140523092305664 logging_writer.py:48] [195200] global_step=195200, grad_norm=4.724817276000977, loss=0.6002861261367798
I0301 00:33:17.144648 140523947947776 logging_writer.py:48] [195300] global_step=195300, grad_norm=4.395844459533691, loss=0.5753694176673889
I0301 00:33:50.973501 140523092305664 logging_writer.py:48] [195400] global_step=195400, grad_norm=4.649837493896484, loss=0.6265051364898682
I0301 00:34:24.765666 140523947947776 logging_writer.py:48] [195500] global_step=195500, grad_norm=4.443170547485352, loss=0.6205436587333679
I0301 00:34:58.583192 140523092305664 logging_writer.py:48] [195600] global_step=195600, grad_norm=4.0836968421936035, loss=0.6423104405403137
I0301 00:35:32.384660 140523947947776 logging_writer.py:48] [195700] global_step=195700, grad_norm=4.771376132965088, loss=0.6452153325080872
I0301 00:36:06.329846 140523092305664 logging_writer.py:48] [195800] global_step=195800, grad_norm=4.317525386810303, loss=0.6052932143211365
I0301 00:36:40.123823 140523947947776 logging_writer.py:48] [195900] global_step=195900, grad_norm=4.142555236816406, loss=0.6521445512771606
I0301 00:37:13.872257 140523092305664 logging_writer.py:48] [196000] global_step=196000, grad_norm=4.587453842163086, loss=0.6031326651573181
I0301 00:37:47.695718 140523947947776 logging_writer.py:48] [196100] global_step=196100, grad_norm=4.500219821929932, loss=0.6912468671798706
I0301 00:38:21.471914 140523092305664 logging_writer.py:48] [196200] global_step=196200, grad_norm=4.6682329177856445, loss=0.5514293313026428
I0301 00:38:55.286316 140523947947776 logging_writer.py:48] [196300] global_step=196300, grad_norm=4.652137756347656, loss=0.6005939841270447
I0301 00:39:06.543419 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:39:12.659716 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:39:21.089475 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:39:23.377338 140688601454400 submission_runner.py:411] Time since start: 68758.45s, 	Step: 196335, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.145789235830307, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0447232723236084, 'validation/num_examples': 50000, 'test/accuracy': 0.6327000260353088, 'test/loss': 1.8242987394332886, 'test/num_examples': 10000, 'score': 66356.94939851761, 'total_duration': 68758.44803285599, 'accumulated_submission_time': 66356.94939851761, 'accumulated_eval_time': 2387.5755808353424, 'accumulated_logging_time': 7.208361387252808}
I0301 00:39:23.429448 140522605770496 logging_writer.py:48] [196335] accumulated_eval_time=2387.575581, accumulated_logging_time=7.208361, accumulated_submission_time=66356.949399, global_step=196335, preemption_count=0, score=66356.949399, test/accuracy=0.632700, test/loss=1.824299, test/num_examples=10000, total_duration=68758.448033, train/accuracy=0.960858, train/loss=0.145789, validation/accuracy=0.754980, validation/loss=1.044723, validation/num_examples=50000
I0301 00:39:45.727857 140525256554240 logging_writer.py:48] [196400] global_step=196400, grad_norm=4.165466785430908, loss=0.6368572115898132
I0301 00:40:19.486795 140522605770496 logging_writer.py:48] [196500] global_step=196500, grad_norm=4.524720191955566, loss=0.6305403113365173
I0301 00:40:53.254949 140525256554240 logging_writer.py:48] [196600] global_step=196600, grad_norm=4.862228870391846, loss=0.6261003613471985
I0301 00:41:27.051488 140522605770496 logging_writer.py:48] [196700] global_step=196700, grad_norm=4.845032691955566, loss=0.627650260925293
I0301 00:42:00.874747 140525256554240 logging_writer.py:48] [196800] global_step=196800, grad_norm=4.881224155426025, loss=0.6477215886116028
I0301 00:42:34.646770 140522605770496 logging_writer.py:48] [196900] global_step=196900, grad_norm=4.568460941314697, loss=0.5282033085823059
I0301 00:43:08.418028 140525256554240 logging_writer.py:48] [197000] global_step=197000, grad_norm=4.609048843383789, loss=0.5939187407493591
I0301 00:43:42.190705 140522605770496 logging_writer.py:48] [197100] global_step=197100, grad_norm=4.299826145172119, loss=0.6330376863479614
I0301 00:44:15.972929 140525256554240 logging_writer.py:48] [197200] global_step=197200, grad_norm=4.647421360015869, loss=0.65899258852005
I0301 00:44:49.752942 140522605770496 logging_writer.py:48] [197300] global_step=197300, grad_norm=4.635787487030029, loss=0.6177562475204468
I0301 00:45:23.520484 140525256554240 logging_writer.py:48] [197400] global_step=197400, grad_norm=4.138610363006592, loss=0.5421355366706848
I0301 00:45:57.297463 140522605770496 logging_writer.py:48] [197500] global_step=197500, grad_norm=5.520247459411621, loss=0.6990552544593811
I0301 00:46:31.078058 140525256554240 logging_writer.py:48] [197600] global_step=197600, grad_norm=5.15314245223999, loss=0.6853846311569214
I0301 00:47:04.861290 140522605770496 logging_writer.py:48] [197700] global_step=197700, grad_norm=4.498007297515869, loss=0.6261235475540161
I0301 00:47:38.650142 140525256554240 logging_writer.py:48] [197800] global_step=197800, grad_norm=4.5802788734436035, loss=0.5744689106941223
I0301 00:47:53.623328 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:47:59.743885 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:48:08.684988 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:48:10.977829 140688601454400 submission_runner.py:411] Time since start: 69286.05s, 	Step: 197846, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.1475706398487091, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0447214841842651, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.822264552116394, 'test/num_examples': 10000, 'score': 66867.08155965805, 'total_duration': 69286.04866456985, 'accumulated_submission_time': 66867.08155965805, 'accumulated_eval_time': 2404.9300212860107, 'accumulated_logging_time': 7.270601749420166}
I0301 00:48:11.032917 140522588985088 logging_writer.py:48] [197846] accumulated_eval_time=2404.930021, accumulated_logging_time=7.270602, accumulated_submission_time=66867.081560, global_step=197846, preemption_count=0, score=66867.081560, test/accuracy=0.630900, test/loss=1.822265, test/num_examples=10000, total_duration=69286.048665, train/accuracy=0.960120, train/loss=0.147571, validation/accuracy=0.755240, validation/loss=1.044721, validation/num_examples=50000
I0301 00:48:29.611136 140522597377792 logging_writer.py:48] [197900] global_step=197900, grad_norm=4.483574867248535, loss=0.669817328453064
I0301 00:49:03.304158 140522588985088 logging_writer.py:48] [198000] global_step=198000, grad_norm=4.546894073486328, loss=0.6714518070220947
I0301 00:49:37.106316 140522597377792 logging_writer.py:48] [198100] global_step=198100, grad_norm=4.54674768447876, loss=0.6976296305656433
I0301 00:50:10.886972 140522588985088 logging_writer.py:48] [198200] global_step=198200, grad_norm=4.490563869476318, loss=0.6010096669197083
I0301 00:50:44.720350 140522597377792 logging_writer.py:48] [198300] global_step=198300, grad_norm=4.125696659088135, loss=0.6089659929275513
I0301 00:51:18.492235 140522588985088 logging_writer.py:48] [198400] global_step=198400, grad_norm=4.235963821411133, loss=0.6574000716209412
I0301 00:51:52.313660 140522597377792 logging_writer.py:48] [198500] global_step=198500, grad_norm=4.649908542633057, loss=0.6505374908447266
I0301 00:52:26.077802 140522588985088 logging_writer.py:48] [198600] global_step=198600, grad_norm=5.263942718505859, loss=0.6880283355712891
I0301 00:52:59.870764 140522597377792 logging_writer.py:48] [198700] global_step=198700, grad_norm=4.798356056213379, loss=0.635771632194519
I0301 00:53:33.627140 140522588985088 logging_writer.py:48] [198800] global_step=198800, grad_norm=4.609574317932129, loss=0.6133164763450623
I0301 00:54:07.397497 140522597377792 logging_writer.py:48] [198900] global_step=198900, grad_norm=3.8690412044525146, loss=0.5421527624130249
I0301 00:54:41.235636 140522588985088 logging_writer.py:48] [199000] global_step=199000, grad_norm=4.7177653312683105, loss=0.6663375496864319
I0301 00:55:15.023292 140522597377792 logging_writer.py:48] [199100] global_step=199100, grad_norm=4.739143371582031, loss=0.6977635025978088
I0301 00:55:48.816545 140522588985088 logging_writer.py:48] [199200] global_step=199200, grad_norm=4.541219711303711, loss=0.6473016142845154
I0301 00:56:22.632462 140522597377792 logging_writer.py:48] [199300] global_step=199300, grad_norm=4.908539772033691, loss=0.6819691061973572
I0301 00:56:41.010335 140688601454400 spec.py:321] Evaluating on the training split.
I0301 00:56:47.154628 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 00:56:55.811480 140688601454400 spec.py:349] Evaluating on the test split.
I0301 00:56:58.496749 140688601454400 submission_runner.py:411] Time since start: 69813.57s, 	Step: 199356, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.14824819564819336, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.042603850364685, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8182530403137207, 'test/num_examples': 10000, 'score': 67376.99536323547, 'total_duration': 69813.56760764122, 'accumulated_submission_time': 67376.99536323547, 'accumulated_eval_time': 2422.4164073467255, 'accumulated_logging_time': 7.336972713470459}
I0301 00:56:58.543271 140525264946944 logging_writer.py:48] [199356] accumulated_eval_time=2422.416407, accumulated_logging_time=7.336973, accumulated_submission_time=67376.995363, global_step=199356, preemption_count=0, score=67376.995363, test/accuracy=0.630500, test/loss=1.818253, test/num_examples=10000, total_duration=69813.567608, train/accuracy=0.960878, train/loss=0.148248, validation/accuracy=0.754660, validation/loss=1.042604, validation/num_examples=50000
I0301 00:57:13.705262 140525273339648 logging_writer.py:48] [199400] global_step=199400, grad_norm=4.269100189208984, loss=0.5867882370948792
I0301 00:57:47.421284 140525264946944 logging_writer.py:48] [199500] global_step=199500, grad_norm=4.366386890411377, loss=0.6321755647659302
I0301 00:58:21.157926 140525273339648 logging_writer.py:48] [199600] global_step=199600, grad_norm=4.348782539367676, loss=0.6280319094657898
I0301 00:58:54.914796 140525264946944 logging_writer.py:48] [199700] global_step=199700, grad_norm=4.349510669708252, loss=0.5886576175689697
I0301 00:59:28.690891 140525273339648 logging_writer.py:48] [199800] global_step=199800, grad_norm=4.506582260131836, loss=0.6326054930686951
I0301 01:00:02.477284 140525264946944 logging_writer.py:48] [199900] global_step=199900, grad_norm=4.5171685218811035, loss=0.5842303037643433
I0301 01:00:36.330329 140525273339648 logging_writer.py:48] [200000] global_step=200000, grad_norm=4.536459922790527, loss=0.6358586549758911
I0301 01:01:10.109572 140525264946944 logging_writer.py:48] [200100] global_step=200100, grad_norm=4.560096740722656, loss=0.6187173128128052
I0301 01:01:43.909079 140525273339648 logging_writer.py:48] [200200] global_step=200200, grad_norm=4.558210372924805, loss=0.6432462334632874
I0301 01:02:17.697729 140525264946944 logging_writer.py:48] [200300] global_step=200300, grad_norm=4.740705966949463, loss=0.6274045705795288
I0301 01:02:51.455632 140525273339648 logging_writer.py:48] [200400] global_step=200400, grad_norm=4.285716533660889, loss=0.6012216210365295
I0301 01:03:25.249732 140525264946944 logging_writer.py:48] [200500] global_step=200500, grad_norm=4.528363227844238, loss=0.6093053221702576
I0301 01:03:59.045855 140525273339648 logging_writer.py:48] [200600] global_step=200600, grad_norm=4.160290241241455, loss=0.620018720626831
I0301 01:04:32.818845 140525264946944 logging_writer.py:48] [200700] global_step=200700, grad_norm=4.195560932159424, loss=0.6241113543510437
I0301 01:05:06.597406 140525273339648 logging_writer.py:48] [200800] global_step=200800, grad_norm=4.397056579589844, loss=0.5923540592193604
I0301 01:05:28.677341 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:05:34.833479 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:05:43.485901 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:05:45.731049 140688601454400 submission_runner.py:411] Time since start: 70340.80s, 	Step: 200867, 	{'train/accuracy': 0.9594228267669678, 'train/loss': 0.15049663186073303, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0434798002243042, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8214901685714722, 'test/num_examples': 10000, 'score': 67887.0665371418, 'total_duration': 70340.8018951416, 'accumulated_submission_time': 67887.0665371418, 'accumulated_eval_time': 2439.470073223114, 'accumulated_logging_time': 7.393136739730835}
I0301 01:05:45.784651 140523092305664 logging_writer.py:48] [200867] accumulated_eval_time=2439.470073, accumulated_logging_time=7.393137, accumulated_submission_time=67887.066537, global_step=200867, preemption_count=0, score=67887.066537, test/accuracy=0.631200, test/loss=1.821490, test/num_examples=10000, total_duration=70340.801895, train/accuracy=0.959423, train/loss=0.150497, validation/accuracy=0.755180, validation/loss=1.043480, validation/num_examples=50000
I0301 01:05:57.282630 140523947947776 logging_writer.py:48] [200900] global_step=200900, grad_norm=4.907071590423584, loss=0.6113216280937195
I0301 01:06:31.095160 140523092305664 logging_writer.py:48] [201000] global_step=201000, grad_norm=4.674970626831055, loss=0.6686514019966125
I0301 01:07:04.851701 140523947947776 logging_writer.py:48] [201100] global_step=201100, grad_norm=4.4920172691345215, loss=0.624183177947998
I0301 01:07:38.653444 140523092305664 logging_writer.py:48] [201200] global_step=201200, grad_norm=4.293805122375488, loss=0.7130569219589233
I0301 01:08:12.433584 140523947947776 logging_writer.py:48] [201300] global_step=201300, grad_norm=4.33048677444458, loss=0.6272891163825989
I0301 01:08:46.238425 140523092305664 logging_writer.py:48] [201400] global_step=201400, grad_norm=4.5671491622924805, loss=0.5896948575973511
I0301 01:09:20.027485 140523947947776 logging_writer.py:48] [201500] global_step=201500, grad_norm=5.160995006561279, loss=0.6650999188423157
I0301 01:09:53.809568 140523092305664 logging_writer.py:48] [201600] global_step=201600, grad_norm=4.7507405281066895, loss=0.6695147752761841
I0301 01:10:27.571414 140523947947776 logging_writer.py:48] [201700] global_step=201700, grad_norm=4.174572467803955, loss=0.6412515640258789
I0301 01:11:01.322777 140523092305664 logging_writer.py:48] [201800] global_step=201800, grad_norm=4.81675386428833, loss=0.6305423974990845
I0301 01:11:35.116349 140523947947776 logging_writer.py:48] [201900] global_step=201900, grad_norm=4.749268531799316, loss=0.5960991382598877
I0301 01:12:08.900041 140523092305664 logging_writer.py:48] [202000] global_step=202000, grad_norm=4.423220634460449, loss=0.560549259185791
I0301 01:12:42.764785 140523947947776 logging_writer.py:48] [202100] global_step=202100, grad_norm=4.533055305480957, loss=0.5885729789733887
I0301 01:13:16.547355 140523092305664 logging_writer.py:48] [202200] global_step=202200, grad_norm=4.798939228057861, loss=0.6948288083076477
I0301 01:13:50.362582 140523947947776 logging_writer.py:48] [202300] global_step=202300, grad_norm=4.437495708465576, loss=0.6232401132583618
I0301 01:14:15.836127 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:14:21.978994 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:14:30.632850 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:14:33.033591 140688601454400 submission_runner.py:411] Time since start: 70868.10s, 	Step: 202377, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14340108633041382, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0438283681869507, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.822011947631836, 'test/num_examples': 10000, 'score': 68397.0556704998, 'total_duration': 70868.1044383049, 'accumulated_submission_time': 68397.0556704998, 'accumulated_eval_time': 2456.667491674423, 'accumulated_logging_time': 7.457369804382324}
I0301 01:14:33.085662 140522597377792 logging_writer.py:48] [202377] accumulated_eval_time=2456.667492, accumulated_logging_time=7.457370, accumulated_submission_time=68397.055670, global_step=202377, preemption_count=0, score=68397.055670, test/accuracy=0.631600, test/loss=1.822012, test/num_examples=10000, total_duration=70868.104438, train/accuracy=0.961217, train/loss=0.143401, validation/accuracy=0.755400, validation/loss=1.043828, validation/num_examples=50000
I0301 01:14:41.183325 140525264946944 logging_writer.py:48] [202400] global_step=202400, grad_norm=4.699331283569336, loss=0.64272141456604
I0301 01:15:14.902954 140522597377792 logging_writer.py:48] [202500] global_step=202500, grad_norm=4.606805801391602, loss=0.5939986109733582
I0301 01:15:48.634819 140525264946944 logging_writer.py:48] [202600] global_step=202600, grad_norm=4.54512357711792, loss=0.5706740021705627
I0301 01:16:22.436613 140522597377792 logging_writer.py:48] [202700] global_step=202700, grad_norm=4.483683109283447, loss=0.631604790687561
I0301 01:16:56.215080 140525264946944 logging_writer.py:48] [202800] global_step=202800, grad_norm=4.369155406951904, loss=0.6467543244361877
I0301 01:17:30.040332 140522597377792 logging_writer.py:48] [202900] global_step=202900, grad_norm=4.953503131866455, loss=0.7581762075424194
I0301 01:18:03.807315 140525264946944 logging_writer.py:48] [203000] global_step=203000, grad_norm=4.85658597946167, loss=0.707752525806427
I0301 01:18:37.590861 140522597377792 logging_writer.py:48] [203100] global_step=203100, grad_norm=4.447155952453613, loss=0.550654411315918
I0301 01:19:11.462716 140525264946944 logging_writer.py:48] [203200] global_step=203200, grad_norm=4.37379789352417, loss=0.5600728988647461
I0301 01:19:45.268747 140522597377792 logging_writer.py:48] [203300] global_step=203300, grad_norm=4.6360859870910645, loss=0.608944296836853
I0301 01:20:19.052120 140525264946944 logging_writer.py:48] [203400] global_step=203400, grad_norm=4.649636745452881, loss=0.6744871735572815
I0301 01:20:52.872013 140522597377792 logging_writer.py:48] [203500] global_step=203500, grad_norm=4.644059181213379, loss=0.6053023934364319
I0301 01:21:26.654934 140525264946944 logging_writer.py:48] [203600] global_step=203600, grad_norm=4.9411115646362305, loss=0.5912466049194336
I0301 01:22:00.478343 140522597377792 logging_writer.py:48] [203700] global_step=203700, grad_norm=4.385748863220215, loss=0.6553362607955933
I0301 01:22:34.272195 140525264946944 logging_writer.py:48] [203800] global_step=203800, grad_norm=4.684250831604004, loss=0.665144681930542
I0301 01:23:03.090767 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:23:09.311417 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:23:17.953185 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:23:20.194509 140688601454400 submission_runner.py:411] Time since start: 71395.27s, 	Step: 203887, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14597104489803314, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0434318780899048, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.822356104850769, 'test/num_examples': 10000, 'score': 68906.9993698597, 'total_duration': 71395.26534843445, 'accumulated_submission_time': 68906.9993698597, 'accumulated_eval_time': 2473.7711822986603, 'accumulated_logging_time': 7.51897931098938}
I0301 01:23:20.246221 140523092305664 logging_writer.py:48] [203887] accumulated_eval_time=2473.771182, accumulated_logging_time=7.518979, accumulated_submission_time=68906.999370, global_step=203887, preemption_count=0, score=68906.999370, test/accuracy=0.630100, test/loss=1.822356, test/num_examples=10000, total_duration=71395.265348, train/accuracy=0.961117, train/loss=0.145971, validation/accuracy=0.754720, validation/loss=1.043432, validation/num_examples=50000
I0301 01:23:24.978196 140523947947776 logging_writer.py:48] [203900] global_step=203900, grad_norm=4.546597480773926, loss=0.5811865925788879
I0301 01:23:58.691826 140523092305664 logging_writer.py:48] [204000] global_step=204000, grad_norm=4.376857280731201, loss=0.6800945401191711
I0301 01:24:32.449647 140523947947776 logging_writer.py:48] [204100] global_step=204100, grad_norm=4.621254920959473, loss=0.6685857772827148
I0301 01:25:06.311895 140523092305664 logging_writer.py:48] [204200] global_step=204200, grad_norm=4.427685737609863, loss=0.5923908948898315
I0301 01:25:40.086643 140523947947776 logging_writer.py:48] [204300] global_step=204300, grad_norm=4.526395797729492, loss=0.54715895652771
I0301 01:26:13.882434 140523092305664 logging_writer.py:48] [204400] global_step=204400, grad_norm=4.6666364669799805, loss=0.6447412371635437
I0301 01:26:47.665845 140523947947776 logging_writer.py:48] [204500] global_step=204500, grad_norm=4.823121070861816, loss=0.5970309972763062
I0301 01:27:21.465827 140523092305664 logging_writer.py:48] [204600] global_step=204600, grad_norm=4.791178226470947, loss=0.6859634518623352
I0301 01:27:55.258608 140523947947776 logging_writer.py:48] [204700] global_step=204700, grad_norm=4.469385623931885, loss=0.6123722195625305
I0301 01:28:29.039309 140523092305664 logging_writer.py:48] [204800] global_step=204800, grad_norm=4.313057899475098, loss=0.5864278078079224
I0301 01:29:02.837400 140523947947776 logging_writer.py:48] [204900] global_step=204900, grad_norm=4.3727126121521, loss=0.6913025379180908
I0301 01:29:36.642838 140523092305664 logging_writer.py:48] [205000] global_step=205000, grad_norm=5.05733585357666, loss=0.6263166069984436
I0301 01:30:10.407733 140523947947776 logging_writer.py:48] [205100] global_step=205100, grad_norm=4.823261737823486, loss=0.660548746585846
I0301 01:30:44.164625 140523092305664 logging_writer.py:48] [205200] global_step=205200, grad_norm=4.549308776855469, loss=0.6593191623687744
I0301 01:31:18.056168 140523947947776 logging_writer.py:48] [205300] global_step=205300, grad_norm=5.266181945800781, loss=0.6870703101158142
I0301 01:31:50.291218 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:31:56.595006 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:32:05.235937 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:32:07.512000 140688601454400 submission_runner.py:411] Time since start: 71922.58s, 	Step: 205397, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14629222452640533, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0448895692825317, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8229455947875977, 'test/num_examples': 10000, 'score': 69416.98277258873, 'total_duration': 71922.58283758163, 'accumulated_submission_time': 69416.98277258873, 'accumulated_eval_time': 2490.9919068813324, 'accumulated_logging_time': 7.580933094024658}
I0301 01:32:07.564423 140522605770496 logging_writer.py:48] [205397] accumulated_eval_time=2490.991907, accumulated_logging_time=7.580933, accumulated_submission_time=69416.982773, global_step=205397, preemption_count=0, score=69416.982773, test/accuracy=0.631000, test/loss=1.822946, test/num_examples=10000, total_duration=71922.582838, train/accuracy=0.960658, train/loss=0.146292, validation/accuracy=0.755420, validation/loss=1.044890, validation/num_examples=50000
I0301 01:32:08.917476 140523092305664 logging_writer.py:48] [205400] global_step=205400, grad_norm=4.603817462921143, loss=0.5697212219238281
I0301 01:32:42.643362 140522605770496 logging_writer.py:48] [205500] global_step=205500, grad_norm=4.319364070892334, loss=0.5649611949920654
I0301 01:33:16.438588 140523092305664 logging_writer.py:48] [205600] global_step=205600, grad_norm=4.308100700378418, loss=0.689180314540863
I0301 01:33:50.214326 140522605770496 logging_writer.py:48] [205700] global_step=205700, grad_norm=4.53996467590332, loss=0.5846632719039917
I0301 01:34:24.027541 140523092305664 logging_writer.py:48] [205800] global_step=205800, grad_norm=4.592029571533203, loss=0.7168295979499817
I0301 01:34:57.811538 140522605770496 logging_writer.py:48] [205900] global_step=205900, grad_norm=4.990364074707031, loss=0.644985556602478
I0301 01:35:31.640195 140523092305664 logging_writer.py:48] [206000] global_step=206000, grad_norm=4.091958999633789, loss=0.563092052936554
I0301 01:36:05.423676 140522605770496 logging_writer.py:48] [206100] global_step=206100, grad_norm=4.731502056121826, loss=0.5854727029800415
I0301 01:36:39.245326 140523092305664 logging_writer.py:48] [206200] global_step=206200, grad_norm=4.64805269241333, loss=0.5423263311386108
I0301 01:37:13.018894 140522605770496 logging_writer.py:48] [206300] global_step=206300, grad_norm=5.024162769317627, loss=0.6924369931221008
I0301 01:37:46.826771 140523092305664 logging_writer.py:48] [206400] global_step=206400, grad_norm=4.804879665374756, loss=0.7426252961158752
I0301 01:38:20.623444 140522605770496 logging_writer.py:48] [206500] global_step=206500, grad_norm=4.868706703186035, loss=0.6713848114013672
I0301 01:38:54.379439 140523092305664 logging_writer.py:48] [206600] global_step=206600, grad_norm=4.6024603843688965, loss=0.613760232925415
I0301 01:39:28.169416 140522605770496 logging_writer.py:48] [206700] global_step=206700, grad_norm=5.260116100311279, loss=0.640382707118988
I0301 01:40:01.994682 140523092305664 logging_writer.py:48] [206800] global_step=206800, grad_norm=4.48409366607666, loss=0.7082845568656921
I0301 01:40:35.811685 140522605770496 logging_writer.py:48] [206900] global_step=206900, grad_norm=4.5578155517578125, loss=0.5835432410240173
I0301 01:40:37.641621 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:40:43.796981 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:40:52.529659 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:40:54.789889 140688601454400 submission_runner.py:411] Time since start: 72449.86s, 	Step: 206907, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.14880415797233582, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.044255256652832, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8228908777236938, 'test/num_examples': 10000, 'score': 69926.99735879898, 'total_duration': 72449.86073350906, 'accumulated_submission_time': 69926.99735879898, 'accumulated_eval_time': 2508.140120267868, 'accumulated_logging_time': 7.6438798904418945}
I0301 01:40:54.846374 140522597377792 logging_writer.py:48] [206907] accumulated_eval_time=2508.140120, accumulated_logging_time=7.643880, accumulated_submission_time=69926.997359, global_step=206907, preemption_count=0, score=69926.997359, test/accuracy=0.632000, test/loss=1.822891, test/num_examples=10000, total_duration=72449.860734, train/accuracy=0.960918, train/loss=0.148804, validation/accuracy=0.755280, validation/loss=1.044255, validation/num_examples=50000
I0301 01:41:26.563072 140522605770496 logging_writer.py:48] [207000] global_step=207000, grad_norm=4.392845630645752, loss=0.600368857383728
I0301 01:42:00.323081 140522597377792 logging_writer.py:48] [207100] global_step=207100, grad_norm=4.7555036544799805, loss=0.7129424810409546
I0301 01:42:34.101781 140522605770496 logging_writer.py:48] [207200] global_step=207200, grad_norm=4.438429355621338, loss=0.6851152777671814
I0301 01:43:07.909711 140522597377792 logging_writer.py:48] [207300] global_step=207300, grad_norm=4.448172092437744, loss=0.571283221244812
I0301 01:43:41.755103 140522605770496 logging_writer.py:48] [207400] global_step=207400, grad_norm=4.586462020874023, loss=0.67972332239151
I0301 01:44:15.558648 140522597377792 logging_writer.py:48] [207500] global_step=207500, grad_norm=4.695764541625977, loss=0.5895975828170776
I0301 01:44:49.346723 140522605770496 logging_writer.py:48] [207600] global_step=207600, grad_norm=4.507029056549072, loss=0.6270030736923218
I0301 01:45:23.115610 140522597377792 logging_writer.py:48] [207700] global_step=207700, grad_norm=4.948182106018066, loss=0.7204910516738892
I0301 01:45:56.885742 140522605770496 logging_writer.py:48] [207800] global_step=207800, grad_norm=4.729465007781982, loss=0.6282204985618591
I0301 01:46:30.663309 140522597377792 logging_writer.py:48] [207900] global_step=207900, grad_norm=4.505363464355469, loss=0.6488037109375
I0301 01:47:04.448364 140522605770496 logging_writer.py:48] [208000] global_step=208000, grad_norm=4.735323429107666, loss=0.6018377542495728
I0301 01:47:38.249029 140522597377792 logging_writer.py:48] [208100] global_step=208100, grad_norm=4.545720100402832, loss=0.5829582214355469
I0301 01:48:12.034206 140522605770496 logging_writer.py:48] [208200] global_step=208200, grad_norm=4.720236778259277, loss=0.6461067795753479
I0301 01:48:45.828532 140522597377792 logging_writer.py:48] [208300] global_step=208300, grad_norm=5.656174659729004, loss=0.669087827205658
I0301 01:49:19.609000 140522605770496 logging_writer.py:48] [208400] global_step=208400, grad_norm=4.175302982330322, loss=0.5830913782119751
I0301 01:49:24.830568 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:49:31.005305 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:49:39.925513 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:49:42.207176 140688601454400 submission_runner.py:411] Time since start: 72977.28s, 	Step: 208417, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14933164417743683, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0437886714935303, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8218469619750977, 'test/num_examples': 10000, 'score': 70436.92092514038, 'total_duration': 72977.27802371979, 'accumulated_submission_time': 70436.92092514038, 'accumulated_eval_time': 2525.5167202949524, 'accumulated_logging_time': 7.710052490234375}
I0301 01:49:42.262543 140522597377792 logging_writer.py:48] [208417] accumulated_eval_time=2525.516720, accumulated_logging_time=7.710052, accumulated_submission_time=70436.920925, global_step=208417, preemption_count=0, score=70436.920925, test/accuracy=0.630600, test/loss=1.821847, test/num_examples=10000, total_duration=72977.278024, train/accuracy=0.960200, train/loss=0.149332, validation/accuracy=0.755240, validation/loss=1.043789, validation/num_examples=50000
I0301 01:50:10.615842 140525264946944 logging_writer.py:48] [208500] global_step=208500, grad_norm=4.906864643096924, loss=0.625286877155304
I0301 01:50:44.390223 140522597377792 logging_writer.py:48] [208600] global_step=208600, grad_norm=4.276301383972168, loss=0.6227253675460815
I0301 01:51:18.134009 140525264946944 logging_writer.py:48] [208700] global_step=208700, grad_norm=4.313923358917236, loss=0.624517023563385
I0301 01:51:51.918856 140522597377792 logging_writer.py:48] [208800] global_step=208800, grad_norm=4.801192283630371, loss=0.6277850866317749
I0301 01:52:25.704959 140525264946944 logging_writer.py:48] [208900] global_step=208900, grad_norm=4.726185321807861, loss=0.6224868893623352
I0301 01:52:59.461722 140522597377792 logging_writer.py:48] [209000] global_step=209000, grad_norm=4.315738677978516, loss=0.5823394060134888
I0301 01:53:33.183696 140525264946944 logging_writer.py:48] [209100] global_step=209100, grad_norm=4.558944225311279, loss=0.5544091463088989
I0301 01:54:06.949998 140522597377792 logging_writer.py:48] [209200] global_step=209200, grad_norm=4.589442253112793, loss=0.6654039025306702
I0301 01:54:40.738373 140525264946944 logging_writer.py:48] [209300] global_step=209300, grad_norm=4.621237754821777, loss=0.594752848148346
I0301 01:55:14.488373 140522597377792 logging_writer.py:48] [209400] global_step=209400, grad_norm=4.72050142288208, loss=0.6194306015968323
I0301 01:55:48.372099 140525264946944 logging_writer.py:48] [209500] global_step=209500, grad_norm=4.904018402099609, loss=0.6523221731185913
I0301 01:56:22.134247 140522597377792 logging_writer.py:48] [209600] global_step=209600, grad_norm=4.509100914001465, loss=0.5296642184257507
I0301 01:56:55.927169 140525264946944 logging_writer.py:48] [209700] global_step=209700, grad_norm=4.569056987762451, loss=0.608883798122406
I0301 01:57:29.692013 140522597377792 logging_writer.py:48] [209800] global_step=209800, grad_norm=4.362879753112793, loss=0.605304479598999
I0301 01:58:03.421954 140525264946944 logging_writer.py:48] [209900] global_step=209900, grad_norm=4.219452381134033, loss=0.6626163125038147
I0301 01:58:12.357910 140688601454400 spec.py:321] Evaluating on the training split.
I0301 01:58:18.496965 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 01:58:26.992887 140688601454400 spec.py:349] Evaluating on the test split.
I0301 01:58:29.286346 140688601454400 submission_runner.py:411] Time since start: 73504.36s, 	Step: 209928, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14807389676570892, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.044136643409729, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8214828968048096, 'test/num_examples': 10000, 'score': 70946.95390248299, 'total_duration': 73504.35719633102, 'accumulated_submission_time': 70946.95390248299, 'accumulated_eval_time': 2542.445108652115, 'accumulated_logging_time': 7.775446891784668}
I0301 01:58:29.340842 140522588985088 logging_writer.py:48] [209928] accumulated_eval_time=2542.445109, accumulated_logging_time=7.775447, accumulated_submission_time=70946.953902, global_step=209928, preemption_count=0, score=70946.953902, test/accuracy=0.631100, test/loss=1.821483, test/num_examples=10000, total_duration=73504.357196, train/accuracy=0.960738, train/loss=0.148074, validation/accuracy=0.755240, validation/loss=1.044137, validation/num_examples=50000
I0301 01:58:53.949192 140522597377792 logging_writer.py:48] [210000] global_step=210000, grad_norm=4.754286289215088, loss=0.6515462398529053
I0301 01:59:27.693246 140522588985088 logging_writer.py:48] [210100] global_step=210100, grad_norm=4.576086044311523, loss=0.6120462417602539
I0301 02:00:01.456422 140522597377792 logging_writer.py:48] [210200] global_step=210200, grad_norm=4.97564172744751, loss=0.6224898099899292
I0301 02:00:35.215158 140522588985088 logging_writer.py:48] [210300] global_step=210300, grad_norm=4.493890285491943, loss=0.6220152378082275
I0301 02:01:08.999130 140522597377792 logging_writer.py:48] [210400] global_step=210400, grad_norm=4.826849460601807, loss=0.6733482480049133
I0301 02:01:42.798916 140522588985088 logging_writer.py:48] [210500] global_step=210500, grad_norm=4.581255912780762, loss=0.6549715399742126
I0301 02:02:16.607542 140522597377792 logging_writer.py:48] [210600] global_step=210600, grad_norm=5.026148796081543, loss=0.5924006104469299
I0301 02:02:50.394466 140522588985088 logging_writer.py:48] [210700] global_step=210700, grad_norm=4.626679420471191, loss=0.5807761549949646
I0301 02:03:24.186610 140522597377792 logging_writer.py:48] [210800] global_step=210800, grad_norm=4.029239177703857, loss=0.5569905638694763
I0301 02:03:57.962722 140522588985088 logging_writer.py:48] [210900] global_step=210900, grad_norm=4.197746753692627, loss=0.5713648200035095
I0301 02:04:31.751592 140522597377792 logging_writer.py:48] [211000] global_step=211000, grad_norm=4.268350124359131, loss=0.5800279378890991
I0301 02:05:05.573757 140522588985088 logging_writer.py:48] [211100] global_step=211100, grad_norm=4.346189022064209, loss=0.5746921896934509
I0301 02:05:39.395057 140522597377792 logging_writer.py:48] [211200] global_step=211200, grad_norm=4.633462905883789, loss=0.679121196269989
I0301 02:06:13.181365 140522588985088 logging_writer.py:48] [211300] global_step=211300, grad_norm=4.158031940460205, loss=0.6087211966514587
I0301 02:06:46.979539 140522597377792 logging_writer.py:48] [211400] global_step=211400, grad_norm=4.498115062713623, loss=0.6610262989997864
I0301 02:06:59.619597 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:07:05.836409 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:07:14.536085 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:07:16.828862 140688601454400 submission_runner.py:411] Time since start: 74031.90s, 	Step: 211439, 	{'train/accuracy': 0.9592434167861938, 'train/loss': 0.1501782238483429, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0448315143585205, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8235745429992676, 'test/num_examples': 10000, 'score': 71457.17092013359, 'total_duration': 74031.89971113205, 'accumulated_submission_time': 71457.17092013359, 'accumulated_eval_time': 2559.6543271541595, 'accumulated_logging_time': 7.840409278869629}
I0301 02:07:16.884964 140522597377792 logging_writer.py:48] [211439] accumulated_eval_time=2559.654327, accumulated_logging_time=7.840409, accumulated_submission_time=71457.170920, global_step=211439, preemption_count=0, score=71457.170920, test/accuracy=0.631700, test/loss=1.823575, test/num_examples=10000, total_duration=74031.899711, train/accuracy=0.959243, train/loss=0.150178, validation/accuracy=0.755180, validation/loss=1.044832, validation/num_examples=50000
I0301 02:07:37.802165 140525264946944 logging_writer.py:48] [211500] global_step=211500, grad_norm=5.435941219329834, loss=0.6239417195320129
I0301 02:08:11.596436 140522597377792 logging_writer.py:48] [211600] global_step=211600, grad_norm=4.471187114715576, loss=0.6114258766174316
I0301 02:08:45.374108 140525264946944 logging_writer.py:48] [211700] global_step=211700, grad_norm=4.377923011779785, loss=0.6054946184158325
I0301 02:09:19.162273 140522597377792 logging_writer.py:48] [211800] global_step=211800, grad_norm=4.6766228675842285, loss=0.6244735717773438
I0301 02:09:52.948271 140525264946944 logging_writer.py:48] [211900] global_step=211900, grad_norm=4.630893230438232, loss=0.691608726978302
I0301 02:10:26.748758 140522597377792 logging_writer.py:48] [212000] global_step=212000, grad_norm=4.244679927825928, loss=0.5969889760017395
I0301 02:11:00.534927 140525264946944 logging_writer.py:48] [212100] global_step=212100, grad_norm=4.177576065063477, loss=0.5560407042503357
I0301 02:11:34.315862 140522597377792 logging_writer.py:48] [212200] global_step=212200, grad_norm=4.425124168395996, loss=0.6358644962310791
I0301 02:12:08.097964 140525264946944 logging_writer.py:48] [212300] global_step=212300, grad_norm=4.592689037322998, loss=0.6292963624000549
I0301 02:12:41.862602 140522597377792 logging_writer.py:48] [212400] global_step=212400, grad_norm=4.466188430786133, loss=0.5983660221099854
I0301 02:13:15.649368 140525264946944 logging_writer.py:48] [212500] global_step=212500, grad_norm=4.197127819061279, loss=0.5368808507919312
I0301 02:13:49.393383 140522597377792 logging_writer.py:48] [212600] global_step=212600, grad_norm=4.689786434173584, loss=0.6964924335479736
I0301 02:14:23.250182 140525264946944 logging_writer.py:48] [212700] global_step=212700, grad_norm=4.738229751586914, loss=0.6276503801345825
I0301 02:14:57.040794 140522597377792 logging_writer.py:48] [212800] global_step=212800, grad_norm=4.308947563171387, loss=0.5905636548995972
I0301 02:15:30.819985 140525264946944 logging_writer.py:48] [212900] global_step=212900, grad_norm=4.908111095428467, loss=0.6912704110145569
I0301 02:15:46.874052 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:15:52.996174 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:16:01.564237 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:16:03.920845 140688601454400 submission_runner.py:411] Time since start: 74558.99s, 	Step: 212949, 	{'train/accuracy': 0.9595423936843872, 'train/loss': 0.14972873032093048, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0440635681152344, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.822056531906128, 'test/num_examples': 10000, 'score': 71967.09794926643, 'total_duration': 74558.99166703224, 'accumulated_submission_time': 71967.09794926643, 'accumulated_eval_time': 2576.7010428905487, 'accumulated_logging_time': 7.906655311584473}
I0301 02:16:03.978064 140522597377792 logging_writer.py:48] [212949] accumulated_eval_time=2576.701043, accumulated_logging_time=7.906655, accumulated_submission_time=71967.097949, global_step=212949, preemption_count=0, score=71967.097949, test/accuracy=0.630700, test/loss=1.822057, test/num_examples=10000, total_duration=74558.991667, train/accuracy=0.959542, train/loss=0.149729, validation/accuracy=0.755340, validation/loss=1.044064, validation/num_examples=50000
I0301 02:16:21.508932 140522605770496 logging_writer.py:48] [213000] global_step=213000, grad_norm=4.593318939208984, loss=0.6872117519378662
I0301 02:16:55.319678 140522597377792 logging_writer.py:48] [213100] global_step=213100, grad_norm=4.340186595916748, loss=0.5957193374633789
I0301 02:17:29.083150 140522605770496 logging_writer.py:48] [213200] global_step=213200, grad_norm=4.386902332305908, loss=0.5919128060340881
I0301 02:18:02.827925 140522597377792 logging_writer.py:48] [213300] global_step=213300, grad_norm=4.733218193054199, loss=0.613323450088501
I0301 02:18:36.633166 140522605770496 logging_writer.py:48] [213400] global_step=213400, grad_norm=4.974812030792236, loss=0.6471133232116699
I0301 02:19:10.405839 140522597377792 logging_writer.py:48] [213500] global_step=213500, grad_norm=4.186424255371094, loss=0.6425973176956177
I0301 02:19:44.204358 140522605770496 logging_writer.py:48] [213600] global_step=213600, grad_norm=4.956277847290039, loss=0.685147225856781
I0301 02:20:18.101814 140522597377792 logging_writer.py:48] [213700] global_step=213700, grad_norm=4.384966850280762, loss=0.5716578364372253
I0301 02:20:51.874645 140522605770496 logging_writer.py:48] [213800] global_step=213800, grad_norm=4.66742467880249, loss=0.6600736379623413
I0301 02:21:25.671013 140522597377792 logging_writer.py:48] [213900] global_step=213900, grad_norm=4.628795623779297, loss=0.7025641798973083
I0301 02:21:59.451240 140522605770496 logging_writer.py:48] [214000] global_step=214000, grad_norm=4.4301981925964355, loss=0.616460919380188
I0301 02:22:33.206566 140522597377792 logging_writer.py:48] [214100] global_step=214100, grad_norm=4.266496658325195, loss=0.6461829543113708
I0301 02:23:06.975200 140522605770496 logging_writer.py:48] [214200] global_step=214200, grad_norm=5.0204644203186035, loss=0.6572934985160828
I0301 02:23:40.692156 140522597377792 logging_writer.py:48] [214300] global_step=214300, grad_norm=4.439421653747559, loss=0.6435486078262329
I0301 02:24:14.472923 140522605770496 logging_writer.py:48] [214400] global_step=214400, grad_norm=4.860817909240723, loss=0.6482430696487427
I0301 02:24:34.221173 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:24:40.317699 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:24:48.930502 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:24:51.215035 140688601454400 submission_runner.py:411] Time since start: 75086.29s, 	Step: 214460, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14846576750278473, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0437440872192383, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8225747346878052, 'test/num_examples': 10000, 'score': 72477.27730298042, 'total_duration': 75086.28588485718, 'accumulated_submission_time': 72477.27730298042, 'accumulated_eval_time': 2593.694860935211, 'accumulated_logging_time': 7.975393772125244}
I0301 02:24:51.271390 140525264946944 logging_writer.py:48] [214460] accumulated_eval_time=2593.694861, accumulated_logging_time=7.975394, accumulated_submission_time=72477.277303, global_step=214460, preemption_count=0, score=72477.277303, test/accuracy=0.632100, test/loss=1.822575, test/num_examples=10000, total_duration=75086.285885, train/accuracy=0.960200, train/loss=0.148466, validation/accuracy=0.755140, validation/loss=1.043744, validation/num_examples=50000
I0301 02:25:05.101018 140525273339648 logging_writer.py:48] [214500] global_step=214500, grad_norm=4.278863430023193, loss=0.6580584049224854
I0301 02:25:38.851452 140525264946944 logging_writer.py:48] [214600] global_step=214600, grad_norm=4.433141708374023, loss=0.6207959651947021
I0301 02:26:12.625149 140525273339648 logging_writer.py:48] [214700] global_step=214700, grad_norm=4.784063816070557, loss=0.6530871987342834
I0301 02:26:46.498039 140525264946944 logging_writer.py:48] [214800] global_step=214800, grad_norm=4.492331027984619, loss=0.6150023937225342
I0301 02:27:20.324551 140525273339648 logging_writer.py:48] [214900] global_step=214900, grad_norm=4.574579238891602, loss=0.6078413724899292
I0301 02:27:54.131325 140525264946944 logging_writer.py:48] [215000] global_step=215000, grad_norm=4.192269802093506, loss=0.5667039752006531
I0301 02:28:27.934535 140525273339648 logging_writer.py:48] [215100] global_step=215100, grad_norm=4.539440631866455, loss=0.6072801947593689
I0301 02:29:01.721938 140525264946944 logging_writer.py:48] [215200] global_step=215200, grad_norm=4.324776649475098, loss=0.540345311164856
I0301 02:29:35.530974 140525273339648 logging_writer.py:48] [215300] global_step=215300, grad_norm=4.284210205078125, loss=0.5914936065673828
I0301 02:30:09.339768 140525264946944 logging_writer.py:48] [215400] global_step=215400, grad_norm=4.986596584320068, loss=0.589559018611908
I0301 02:30:43.096324 140525273339648 logging_writer.py:48] [215500] global_step=215500, grad_norm=4.660978317260742, loss=0.652766227722168
I0301 02:31:16.887151 140525264946944 logging_writer.py:48] [215600] global_step=215600, grad_norm=4.898492336273193, loss=0.634223461151123
I0301 02:31:50.662948 140525273339648 logging_writer.py:48] [215700] global_step=215700, grad_norm=4.210952281951904, loss=0.5882444977760315
I0301 02:32:24.433379 140525264946944 logging_writer.py:48] [215800] global_step=215800, grad_norm=4.372720241546631, loss=0.7015883326530457
I0301 02:32:58.285300 140525273339648 logging_writer.py:48] [215900] global_step=215900, grad_norm=4.755495548248291, loss=0.6876544952392578
I0301 02:33:21.393919 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:33:27.494500 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:33:35.936749 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:33:38.208612 140688601454400 submission_runner.py:411] Time since start: 75613.28s, 	Step: 215970, 	{'train/accuracy': 0.9599210619926453, 'train/loss': 0.14851537346839905, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0446696281433105, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.823861837387085, 'test/num_examples': 10000, 'score': 72987.33560729027, 'total_duration': 75613.27943301201, 'accumulated_submission_time': 72987.33560729027, 'accumulated_eval_time': 2610.5095081329346, 'accumulated_logging_time': 8.043686389923096}
I0301 02:33:38.259507 140522597377792 logging_writer.py:48] [215970] accumulated_eval_time=2610.509508, accumulated_logging_time=8.043686, accumulated_submission_time=72987.335607, global_step=215970, preemption_count=0, score=72987.335607, test/accuracy=0.630600, test/loss=1.823862, test/num_examples=10000, total_duration=75613.279433, train/accuracy=0.959921, train/loss=0.148515, validation/accuracy=0.754740, validation/loss=1.044670, validation/num_examples=50000
I0301 02:33:48.723388 140522605770496 logging_writer.py:48] [216000] global_step=216000, grad_norm=4.125818252563477, loss=0.5556973814964294
I0301 02:34:22.421826 140522597377792 logging_writer.py:48] [216100] global_step=216100, grad_norm=4.35691499710083, loss=0.6693630814552307
I0301 02:34:56.192090 140522605770496 logging_writer.py:48] [216200] global_step=216200, grad_norm=4.457686424255371, loss=0.5953627228736877
I0301 02:35:29.950558 140522597377792 logging_writer.py:48] [216300] global_step=216300, grad_norm=4.133735656738281, loss=0.5892638564109802
I0301 02:36:03.705913 140522605770496 logging_writer.py:48] [216400] global_step=216400, grad_norm=4.521219253540039, loss=0.6577631235122681
I0301 02:36:37.480105 140522597377792 logging_writer.py:48] [216500] global_step=216500, grad_norm=4.761734962463379, loss=0.6848096251487732
I0301 02:37:11.255094 140522605770496 logging_writer.py:48] [216600] global_step=216600, grad_norm=4.47904634475708, loss=0.6647279262542725
I0301 02:37:45.040344 140522597377792 logging_writer.py:48] [216700] global_step=216700, grad_norm=4.572526931762695, loss=0.7129984498023987
I0301 02:38:18.774821 140522605770496 logging_writer.py:48] [216800] global_step=216800, grad_norm=4.40402889251709, loss=0.6303673982620239
I0301 02:38:52.640153 140522597377792 logging_writer.py:48] [216900] global_step=216900, grad_norm=4.513017177581787, loss=0.6240803003311157
I0301 02:39:26.400557 140522605770496 logging_writer.py:48] [217000] global_step=217000, grad_norm=4.381715774536133, loss=0.6157282590866089
I0301 02:40:00.173203 140522597377792 logging_writer.py:48] [217100] global_step=217100, grad_norm=4.642767429351807, loss=0.6955947279930115
I0301 02:40:33.950922 140522605770496 logging_writer.py:48] [217200] global_step=217200, grad_norm=4.125978469848633, loss=0.6214771270751953
I0301 02:41:07.726016 140522597377792 logging_writer.py:48] [217300] global_step=217300, grad_norm=4.468728065490723, loss=0.6349830031394958
I0301 02:41:41.501174 140522605770496 logging_writer.py:48] [217400] global_step=217400, grad_norm=4.605350494384766, loss=0.7154045104980469
I0301 02:42:08.315225 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:42:14.630766 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:42:23.137760 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:42:25.351853 140688601454400 submission_runner.py:411] Time since start: 76140.42s, 	Step: 217481, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14867544174194336, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0447502136230469, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8224695920944214, 'test/num_examples': 10000, 'score': 73497.32830357552, 'total_duration': 76140.42266964912, 'accumulated_submission_time': 73497.32830357552, 'accumulated_eval_time': 2627.5460596084595, 'accumulated_logging_time': 8.105120658874512}
I0301 02:42:25.410052 140525273339648 logging_writer.py:48] [217481] accumulated_eval_time=2627.546060, accumulated_logging_time=8.105121, accumulated_submission_time=73497.328304, global_step=217481, preemption_count=0, score=73497.328304, test/accuracy=0.631200, test/loss=1.822470, test/num_examples=10000, total_duration=76140.422670, train/accuracy=0.960539, train/loss=0.148675, validation/accuracy=0.754900, validation/loss=1.044750, validation/num_examples=50000
I0301 02:42:32.161960 140525281732352 logging_writer.py:48] [217500] global_step=217500, grad_norm=4.449127674102783, loss=0.6101241707801819
I0301 02:43:05.919439 140525273339648 logging_writer.py:48] [217600] global_step=217600, grad_norm=4.662449359893799, loss=0.6406096816062927
I0301 02:43:39.695199 140525281732352 logging_writer.py:48] [217700] global_step=217700, grad_norm=4.239201545715332, loss=0.5886763334274292
I0301 02:44:13.459817 140525273339648 logging_writer.py:48] [217800] global_step=217800, grad_norm=4.9586873054504395, loss=0.6155593395233154
I0301 02:44:47.265439 140525281732352 logging_writer.py:48] [217900] global_step=217900, grad_norm=4.397823810577393, loss=0.6821449398994446
I0301 02:45:21.121756 140525273339648 logging_writer.py:48] [218000] global_step=218000, grad_norm=4.542520046234131, loss=0.6001192927360535
I0301 02:45:54.896271 140525281732352 logging_writer.py:48] [218100] global_step=218100, grad_norm=4.667114734649658, loss=0.6359368562698364
I0301 02:46:28.669317 140525273339648 logging_writer.py:48] [218200] global_step=218200, grad_norm=4.6219024658203125, loss=0.6136409640312195
I0301 02:47:02.470349 140525281732352 logging_writer.py:48] [218300] global_step=218300, grad_norm=4.464852809906006, loss=0.6934141516685486
I0301 02:47:36.250286 140525273339648 logging_writer.py:48] [218400] global_step=218400, grad_norm=4.9076433181762695, loss=0.6421082615852356
I0301 02:48:10.001759 140525281732352 logging_writer.py:48] [218500] global_step=218500, grad_norm=4.590547561645508, loss=0.6005290150642395
I0301 02:48:43.779404 140525273339648 logging_writer.py:48] [218600] global_step=218600, grad_norm=4.610622406005859, loss=0.6421921849250793
I0301 02:49:17.561537 140525281732352 logging_writer.py:48] [218700] global_step=218700, grad_norm=4.658211708068848, loss=0.6160916090011597
I0301 02:49:51.349419 140525273339648 logging_writer.py:48] [218800] global_step=218800, grad_norm=4.592336654663086, loss=0.6144066452980042
I0301 02:50:25.148454 140525281732352 logging_writer.py:48] [218900] global_step=218900, grad_norm=4.627552032470703, loss=0.6114604473114014
I0301 02:50:55.352018 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:51:01.523742 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:51:10.064530 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:51:12.637286 140688601454400 submission_runner.py:411] Time since start: 76667.71s, 	Step: 218991, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.1492205560207367, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.043259620666504, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8205660581588745, 'test/num_examples': 10000, 'score': 74007.20657753944, 'total_duration': 76667.70803070068, 'accumulated_submission_time': 74007.20657753944, 'accumulated_eval_time': 2644.8311898708344, 'accumulated_logging_time': 8.175191640853882}
I0301 02:51:12.693740 140522597377792 logging_writer.py:48] [218991] accumulated_eval_time=2644.831190, accumulated_logging_time=8.175192, accumulated_submission_time=74007.206578, global_step=218991, preemption_count=0, score=74007.206578, test/accuracy=0.630100, test/loss=1.820566, test/num_examples=10000, total_duration=76667.708031, train/accuracy=0.959602, train/loss=0.149221, validation/accuracy=0.755120, validation/loss=1.043260, validation/num_examples=50000
I0301 02:51:16.079477 140522605770496 logging_writer.py:48] [219000] global_step=219000, grad_norm=4.456767559051514, loss=0.6696239113807678
I0301 02:51:49.836121 140522597377792 logging_writer.py:48] [219100] global_step=219100, grad_norm=4.453956127166748, loss=0.6034170389175415
I0301 02:52:23.558041 140522605770496 logging_writer.py:48] [219200] global_step=219200, grad_norm=4.321008205413818, loss=0.7018463611602783
I0301 02:52:57.337220 140522597377792 logging_writer.py:48] [219300] global_step=219300, grad_norm=4.608107566833496, loss=0.673026442527771
I0301 02:53:31.114821 140522605770496 logging_writer.py:48] [219400] global_step=219400, grad_norm=4.479465484619141, loss=0.6265861988067627
I0301 02:54:04.910275 140522597377792 logging_writer.py:48] [219500] global_step=219500, grad_norm=4.450404644012451, loss=0.6062050461769104
I0301 02:54:38.688355 140522605770496 logging_writer.py:48] [219600] global_step=219600, grad_norm=4.30751371383667, loss=0.6100621819496155
I0301 02:55:12.468694 140522597377792 logging_writer.py:48] [219700] global_step=219700, grad_norm=4.648642539978027, loss=0.7139459848403931
I0301 02:55:46.247769 140522605770496 logging_writer.py:48] [219800] global_step=219800, grad_norm=4.562438488006592, loss=0.64935302734375
I0301 02:56:20.020088 140522597377792 logging_writer.py:48] [219900] global_step=219900, grad_norm=4.577611446380615, loss=0.7179169654846191
I0301 02:56:53.794798 140522605770496 logging_writer.py:48] [220000] global_step=220000, grad_norm=4.668519496917725, loss=0.6297860145568848
I0301 02:57:27.734014 140522597377792 logging_writer.py:48] [220100] global_step=220100, grad_norm=4.488631248474121, loss=0.6104923486709595
I0301 02:58:01.528571 140522605770496 logging_writer.py:48] [220200] global_step=220200, grad_norm=4.214432716369629, loss=0.5997539758682251
I0301 02:58:35.284451 140522597377792 logging_writer.py:48] [220300] global_step=220300, grad_norm=4.3609771728515625, loss=0.5650327205657959
I0301 02:59:09.072128 140522605770496 logging_writer.py:48] [220400] global_step=220400, grad_norm=4.930068016052246, loss=0.7023104429244995
I0301 02:59:42.885231 140522597377792 logging_writer.py:48] [220500] global_step=220500, grad_norm=4.494868755340576, loss=0.658760130405426
I0301 02:59:42.892395 140688601454400 spec.py:321] Evaluating on the training split.
I0301 02:59:49.003491 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 02:59:57.443081 140688601454400 spec.py:349] Evaluating on the test split.
I0301 02:59:59.701776 140688601454400 submission_runner.py:411] Time since start: 77194.77s, 	Step: 220501, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14759352803230286, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0445195436477661, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8223117589950562, 'test/num_examples': 10000, 'score': 74517.34069633484, 'total_duration': 77194.7726213932, 'accumulated_submission_time': 74517.34069633484, 'accumulated_eval_time': 2661.6404991149902, 'accumulated_logging_time': 8.243839025497437}
I0301 02:59:59.757183 140523947947776 logging_writer.py:48] [220501] accumulated_eval_time=2661.640499, accumulated_logging_time=8.243839, accumulated_submission_time=74517.340696, global_step=220501, preemption_count=0, score=74517.340696, test/accuracy=0.631400, test/loss=1.822312, test/num_examples=10000, total_duration=77194.772621, train/accuracy=0.960619, train/loss=0.147594, validation/accuracy=0.754940, validation/loss=1.044520, validation/num_examples=50000
I0301 03:00:33.458749 140525256554240 logging_writer.py:48] [220600] global_step=220600, grad_norm=4.178010940551758, loss=0.662693440914154
I0301 03:01:07.184150 140523947947776 logging_writer.py:48] [220700] global_step=220700, grad_norm=4.726053714752197, loss=0.6610023975372314
I0301 03:01:40.991248 140525256554240 logging_writer.py:48] [220800] global_step=220800, grad_norm=4.747016429901123, loss=0.6899574995040894
I0301 03:02:14.774175 140523947947776 logging_writer.py:48] [220900] global_step=220900, grad_norm=4.2736616134643555, loss=0.5738400220870972
I0301 03:02:48.532186 140525256554240 logging_writer.py:48] [221000] global_step=221000, grad_norm=4.444272518157959, loss=0.5790349841117859
I0301 03:03:22.326736 140523947947776 logging_writer.py:48] [221100] global_step=221100, grad_norm=4.9539103507995605, loss=0.6914513111114502
I0301 03:03:56.215084 140525256554240 logging_writer.py:48] [221200] global_step=221200, grad_norm=5.102475643157959, loss=0.7480686902999878
I0301 03:04:29.976125 140523947947776 logging_writer.py:48] [221300] global_step=221300, grad_norm=4.586970806121826, loss=0.661945641040802
I0301 03:05:03.777443 140525256554240 logging_writer.py:48] [221400] global_step=221400, grad_norm=4.45479154586792, loss=0.5494964122772217
I0301 03:05:37.557346 140523947947776 logging_writer.py:48] [221500] global_step=221500, grad_norm=4.453851222991943, loss=0.5965620279312134
I0301 03:06:11.296728 140525256554240 logging_writer.py:48] [221600] global_step=221600, grad_norm=4.728397846221924, loss=0.6370201706886292
I0301 03:06:45.054295 140523947947776 logging_writer.py:48] [221700] global_step=221700, grad_norm=4.605590343475342, loss=0.6442752480506897
I0301 03:07:18.838802 140525256554240 logging_writer.py:48] [221800] global_step=221800, grad_norm=4.559409141540527, loss=0.6388828754425049
I0301 03:07:52.636974 140523947947776 logging_writer.py:48] [221900] global_step=221900, grad_norm=4.330603122711182, loss=0.562623143196106
I0301 03:08:26.454114 140525256554240 logging_writer.py:48] [222000] global_step=222000, grad_norm=4.1312785148620605, loss=0.5919408202171326
I0301 03:08:29.983726 140688601454400 spec.py:321] Evaluating on the training split.
I0301 03:08:36.134273 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 03:08:44.615786 140688601454400 spec.py:349] Evaluating on the test split.
I0301 03:08:46.864557 140688601454400 submission_runner.py:411] Time since start: 77721.94s, 	Step: 222012, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14808398485183716, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0450679063796997, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8228307962417603, 'test/num_examples': 10000, 'score': 75027.50423502922, 'total_duration': 77721.93538951874, 'accumulated_submission_time': 75027.50423502922, 'accumulated_eval_time': 2678.5212664604187, 'accumulated_logging_time': 8.310691595077515}
I0301 03:08:46.918844 140525264946944 logging_writer.py:48] [222012] accumulated_eval_time=2678.521266, accumulated_logging_time=8.310692, accumulated_submission_time=75027.504235, global_step=222012, preemption_count=0, score=75027.504235, test/accuracy=0.631400, test/loss=1.822831, test/num_examples=10000, total_duration=77721.935390, train/accuracy=0.960340, train/loss=0.148084, validation/accuracy=0.755220, validation/loss=1.045068, validation/num_examples=50000
I0301 03:09:16.915385 140525273339648 logging_writer.py:48] [222100] global_step=222100, grad_norm=4.746462345123291, loss=0.6577691435813904
I0301 03:09:50.840610 140525264946944 logging_writer.py:48] [222200] global_step=222200, grad_norm=4.3317999839782715, loss=0.600702702999115
I0301 03:10:24.583869 140525273339648 logging_writer.py:48] [222300] global_step=222300, grad_norm=3.997783660888672, loss=0.5554819107055664
I0301 03:10:58.363133 140525264946944 logging_writer.py:48] [222400] global_step=222400, grad_norm=4.148268699645996, loss=0.6017767190933228
I0301 03:11:32.137903 140525273339648 logging_writer.py:48] [222500] global_step=222500, grad_norm=4.125231742858887, loss=0.5742700695991516
I0301 03:12:05.938666 140525264946944 logging_writer.py:48] [222600] global_step=222600, grad_norm=4.249685764312744, loss=0.6469556093215942
I0301 03:12:39.701726 140525273339648 logging_writer.py:48] [222700] global_step=222700, grad_norm=4.569851875305176, loss=0.6326149702072144
I0301 03:13:13.484395 140525264946944 logging_writer.py:48] [222800] global_step=222800, grad_norm=4.60352087020874, loss=0.7189338803291321
I0301 03:13:47.269280 140525273339648 logging_writer.py:48] [222900] global_step=222900, grad_norm=4.510227203369141, loss=0.6553406119346619
I0301 03:14:21.050623 140525264946944 logging_writer.py:48] [223000] global_step=223000, grad_norm=4.683653354644775, loss=0.6612242460250854
I0301 03:14:54.812546 140525273339648 logging_writer.py:48] [223100] global_step=223100, grad_norm=4.308489799499512, loss=0.6453876495361328
I0301 03:15:28.592653 140525264946944 logging_writer.py:48] [223200] global_step=223200, grad_norm=4.735254287719727, loss=0.6157499551773071
I0301 03:16:02.459952 140525273339648 logging_writer.py:48] [223300] global_step=223300, grad_norm=4.862573623657227, loss=0.6506150960922241
I0301 03:16:36.243097 140525264946944 logging_writer.py:48] [223400] global_step=223400, grad_norm=4.267204761505127, loss=0.6266523003578186
I0301 03:17:10.016910 140525273339648 logging_writer.py:48] [223500] global_step=223500, grad_norm=4.439274787902832, loss=0.6031804084777832
I0301 03:17:16.905708 140688601454400 spec.py:321] Evaluating on the training split.
I0301 03:17:22.996622 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 03:17:31.679503 140688601454400 spec.py:349] Evaluating on the test split.
I0301 03:17:33.943335 140688601454400 submission_runner.py:411] Time since start: 78249.01s, 	Step: 223522, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14815884828567505, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.04426109790802, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8223483562469482, 'test/num_examples': 10000, 'score': 75537.42972803116, 'total_duration': 78249.01416754723, 'accumulated_submission_time': 75537.42972803116, 'accumulated_eval_time': 2695.558828353882, 'accumulated_logging_time': 8.374577283859253}
I0301 03:17:34.003439 140522605770496 logging_writer.py:48] [223522] accumulated_eval_time=2695.558828, accumulated_logging_time=8.374577, accumulated_submission_time=75537.429728, global_step=223522, preemption_count=0, score=75537.429728, test/accuracy=0.629700, test/loss=1.822348, test/num_examples=10000, total_duration=78249.014168, train/accuracy=0.960280, train/loss=0.148159, validation/accuracy=0.755240, validation/loss=1.044261, validation/num_examples=50000
I0301 03:18:00.636235 140523092305664 logging_writer.py:48] [223600] global_step=223600, grad_norm=4.80596399307251, loss=0.6802703142166138
I0301 03:18:34.420070 140522605770496 logging_writer.py:48] [223700] global_step=223700, grad_norm=4.505284786224365, loss=0.6253330111503601
I0301 03:19:08.193287 140523092305664 logging_writer.py:48] [223800] global_step=223800, grad_norm=4.4365458488464355, loss=0.5964158773422241
I0301 03:19:41.955186 140522605770496 logging_writer.py:48] [223900] global_step=223900, grad_norm=4.603296279907227, loss=0.6633963584899902
I0301 03:20:15.747102 140523092305664 logging_writer.py:48] [224000] global_step=224000, grad_norm=4.347880840301514, loss=0.5859957337379456
I0301 03:20:49.501065 140522605770496 logging_writer.py:48] [224100] global_step=224100, grad_norm=4.623230934143066, loss=0.6863570213317871
I0301 03:21:23.288307 140523092305664 logging_writer.py:48] [224200] global_step=224200, grad_norm=4.684229373931885, loss=0.6280225515365601
I0301 03:21:57.110962 140522605770496 logging_writer.py:48] [224300] global_step=224300, grad_norm=4.328017234802246, loss=0.5520952343940735
I0301 03:22:30.985664 140523092305664 logging_writer.py:48] [224400] global_step=224400, grad_norm=4.391639709472656, loss=0.5918155908584595
I0301 03:23:04.778450 140522605770496 logging_writer.py:48] [224500] global_step=224500, grad_norm=4.135192394256592, loss=0.5318283438682556
I0301 03:23:38.569217 140523092305664 logging_writer.py:48] [224600] global_step=224600, grad_norm=4.73127555847168, loss=0.5771185159683228
I0301 03:24:12.346054 140522605770496 logging_writer.py:48] [224700] global_step=224700, grad_norm=4.639904499053955, loss=0.6460683345794678
I0301 03:24:46.139466 140523092305664 logging_writer.py:48] [224800] global_step=224800, grad_norm=4.3173136711120605, loss=0.58498215675354
I0301 03:25:19.943544 140522605770496 logging_writer.py:48] [224900] global_step=224900, grad_norm=4.240981578826904, loss=0.6511154174804688
I0301 03:25:53.736606 140523092305664 logging_writer.py:48] [225000] global_step=225000, grad_norm=4.87745475769043, loss=0.6753671765327454
I0301 03:26:04.023586 140688601454400 spec.py:321] Evaluating on the training split.
I0301 03:26:10.221976 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 03:26:18.766579 140688601454400 spec.py:349] Evaluating on the test split.
I0301 03:26:21.046005 140688601454400 submission_runner.py:411] Time since start: 78776.12s, 	Step: 225032, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14908351004123688, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0442862510681152, 'validation/num_examples': 50000, 'test/accuracy': 0.6327000260353088, 'test/loss': 1.8209985494613647, 'test/num_examples': 10000, 'score': 76047.38732123375, 'total_duration': 78776.11684513092, 'accumulated_submission_time': 76047.38732123375, 'accumulated_eval_time': 2712.5811915397644, 'accumulated_logging_time': 8.44601035118103}
I0301 03:26:21.103368 140522597377792 logging_writer.py:48] [225032] accumulated_eval_time=2712.581192, accumulated_logging_time=8.446010, accumulated_submission_time=76047.387321, global_step=225032, preemption_count=0, score=76047.387321, test/accuracy=0.632700, test/loss=1.820999, test/num_examples=10000, total_duration=78776.116845, train/accuracy=0.960340, train/loss=0.149084, validation/accuracy=0.755280, validation/loss=1.044286, validation/num_examples=50000
I0301 03:26:44.394312 140522605770496 logging_writer.py:48] [225100] global_step=225100, grad_norm=4.743912696838379, loss=0.6038722991943359
I0301 03:27:18.123269 140522597377792 logging_writer.py:48] [225200] global_step=225200, grad_norm=4.932991027832031, loss=0.6429986953735352
I0301 03:27:51.874445 140522605770496 logging_writer.py:48] [225300] global_step=225300, grad_norm=4.309536457061768, loss=0.5653077363967896
I0301 03:28:25.777835 140522597377792 logging_writer.py:48] [225400] global_step=225400, grad_norm=4.401692867279053, loss=0.6179777979850769
I0301 03:28:59.545631 140522605770496 logging_writer.py:48] [225500] global_step=225500, grad_norm=4.771031856536865, loss=0.6316273808479309
I0301 03:29:33.294534 140522597377792 logging_writer.py:48] [225600] global_step=225600, grad_norm=4.519408226013184, loss=0.5478907823562622
I0301 03:30:07.052265 140522605770496 logging_writer.py:48] [225700] global_step=225700, grad_norm=4.874821186065674, loss=0.7322155237197876
I0301 03:30:40.822814 140522597377792 logging_writer.py:48] [225800] global_step=225800, grad_norm=5.954072952270508, loss=0.7159186005592346
I0301 03:31:14.571226 140522605770496 logging_writer.py:48] [225900] global_step=225900, grad_norm=4.488745212554932, loss=0.5825283527374268
I0301 03:31:48.264567 140522597377792 logging_writer.py:48] [226000] global_step=226000, grad_norm=4.354525089263916, loss=0.6184826493263245
I0301 03:32:22.047966 140522605770496 logging_writer.py:48] [226100] global_step=226100, grad_norm=4.510693073272705, loss=0.6584169268608093
I0301 03:32:55.835282 140522597377792 logging_writer.py:48] [226200] global_step=226200, grad_norm=4.770938873291016, loss=0.6414237022399902
I0301 03:33:29.597095 140522605770496 logging_writer.py:48] [226300] global_step=226300, grad_norm=4.372384548187256, loss=0.5989124178886414
I0301 03:34:03.362338 140522597377792 logging_writer.py:48] [226400] global_step=226400, grad_norm=4.690850734710693, loss=0.5788880586624146
I0301 03:34:37.241660 140522605770496 logging_writer.py:48] [226500] global_step=226500, grad_norm=4.495506286621094, loss=0.5905864238739014
I0301 03:34:51.247431 140688601454400 spec.py:321] Evaluating on the training split.
I0301 03:34:57.346558 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 03:35:06.023838 140688601454400 spec.py:349] Evaluating on the test split.
I0301 03:35:08.322424 140688601454400 submission_runner.py:411] Time since start: 79303.39s, 	Step: 226543, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14483234286308289, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.043460488319397, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.82106614112854, 'test/num_examples': 10000, 'score': 76557.46827101707, 'total_duration': 79303.39326834679, 'accumulated_submission_time': 76557.46827101707, 'accumulated_eval_time': 2729.6561329364777, 'accumulated_logging_time': 8.513875246047974}
I0301 03:35:08.377000 140523947947776 logging_writer.py:48] [226543] accumulated_eval_time=2729.656133, accumulated_logging_time=8.513875, accumulated_submission_time=76557.468271, global_step=226543, preemption_count=0, score=76557.468271, test/accuracy=0.631500, test/loss=1.821066, test/num_examples=10000, total_duration=79303.393268, train/accuracy=0.961416, train/loss=0.144832, validation/accuracy=0.755280, validation/loss=1.043460, validation/num_examples=50000
I0301 03:35:27.979103 140525256554240 logging_writer.py:48] [226600] global_step=226600, grad_norm=4.390505790710449, loss=0.6283976435661316
I0301 03:36:01.669349 140523947947776 logging_writer.py:48] [226700] global_step=226700, grad_norm=4.604000091552734, loss=0.6804487109184265
I0301 03:36:35.458019 140525256554240 logging_writer.py:48] [226800] global_step=226800, grad_norm=4.256262302398682, loss=0.6086450815200806
I0301 03:37:09.226009 140523947947776 logging_writer.py:48] [226900] global_step=226900, grad_norm=4.088361740112305, loss=0.5814269781112671
I0301 03:37:42.958692 140525256554240 logging_writer.py:48] [227000] global_step=227000, grad_norm=4.348464488983154, loss=0.5887928009033203
I0301 03:38:16.712085 140523947947776 logging_writer.py:48] [227100] global_step=227100, grad_norm=4.7456817626953125, loss=0.6709094047546387
I0301 03:38:50.515498 140525256554240 logging_writer.py:48] [227200] global_step=227200, grad_norm=4.9349751472473145, loss=0.6697590351104736
I0301 03:39:24.297255 140523947947776 logging_writer.py:48] [227300] global_step=227300, grad_norm=4.3717217445373535, loss=0.5956538915634155
I0301 03:39:58.066126 140525256554240 logging_writer.py:48] [227400] global_step=227400, grad_norm=4.675439834594727, loss=0.6189652681350708
I0301 03:40:32.152963 140523947947776 logging_writer.py:48] [227500] global_step=227500, grad_norm=5.021783828735352, loss=0.6878318786621094
I0301 03:41:05.858297 140525256554240 logging_writer.py:48] [227600] global_step=227600, grad_norm=5.266427516937256, loss=0.6535903215408325
I0301 03:41:39.596086 140523947947776 logging_writer.py:48] [227700] global_step=227700, grad_norm=4.981673717498779, loss=0.6458746790885925
I0301 03:42:13.392544 140525256554240 logging_writer.py:48] [227800] global_step=227800, grad_norm=4.420178413391113, loss=0.5991360545158386
I0301 03:42:47.163658 140523947947776 logging_writer.py:48] [227900] global_step=227900, grad_norm=4.565227508544922, loss=0.6532570123672485
I0301 03:43:20.903355 140525256554240 logging_writer.py:48] [228000] global_step=228000, grad_norm=4.313815116882324, loss=0.6025061011314392
I0301 03:43:38.617989 140688601454400 spec.py:321] Evaluating on the training split.
I0301 03:43:44.720670 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 03:43:53.214056 140688601454400 spec.py:349] Evaluating on the test split.
I0301 03:43:55.523523 140688601454400 submission_runner.py:411] Time since start: 79830.59s, 	Step: 228054, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14714834094047546, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0447500944137573, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.82277250289917, 'test/num_examples': 10000, 'score': 77067.64736413956, 'total_duration': 79830.59428882599, 'accumulated_submission_time': 77067.64736413956, 'accumulated_eval_time': 2746.561534643173, 'accumulated_logging_time': 8.578904390335083}
I0301 03:43:55.582184 140525273339648 logging_writer.py:48] [228054] accumulated_eval_time=2746.561535, accumulated_logging_time=8.578904, accumulated_submission_time=77067.647364, global_step=228054, preemption_count=0, score=77067.647364, test/accuracy=0.631400, test/loss=1.822773, test/num_examples=10000, total_duration=79830.594289, train/accuracy=0.960818, train/loss=0.147148, validation/accuracy=0.754840, validation/loss=1.044750, validation/num_examples=50000
I0301 03:44:11.465147 140525281732352 logging_writer.py:48] [228100] global_step=228100, grad_norm=4.452042579650879, loss=0.6372831463813782
I0301 03:44:45.177501 140525273339648 logging_writer.py:48] [228200] global_step=228200, grad_norm=4.288581371307373, loss=0.6580695509910583
I0301 03:45:18.917609 140525281732352 logging_writer.py:48] [228300] global_step=228300, grad_norm=4.416771411895752, loss=0.6708271503448486
I0301 03:45:52.688253 140525273339648 logging_writer.py:48] [228400] global_step=228400, grad_norm=4.386330604553223, loss=0.5955530405044556
I0301 03:46:26.479799 140525281732352 logging_writer.py:48] [228500] global_step=228500, grad_norm=4.72220516204834, loss=0.7154316902160645
I0301 03:47:00.290295 140525273339648 logging_writer.py:48] [228600] global_step=228600, grad_norm=4.488528728485107, loss=0.6266887187957764
I0301 03:47:34.089208 140525281732352 logging_writer.py:48] [228700] global_step=228700, grad_norm=4.172145366668701, loss=0.5961648225784302
I0301 03:48:07.875864 140525273339648 logging_writer.py:48] [228800] global_step=228800, grad_norm=4.21761417388916, loss=0.5748393535614014
I0301 03:48:41.687695 140525281732352 logging_writer.py:48] [228900] global_step=228900, grad_norm=4.534976959228516, loss=0.6327110528945923
I0301 03:49:15.464611 140525273339648 logging_writer.py:48] [229000] global_step=229000, grad_norm=4.7726335525512695, loss=0.5624116659164429
I0301 03:49:49.223192 140525281732352 logging_writer.py:48] [229100] global_step=229100, grad_norm=4.726261138916016, loss=0.6770395040512085
I0301 03:50:22.989161 140525273339648 logging_writer.py:48] [229200] global_step=229200, grad_norm=4.713621616363525, loss=0.6148176193237305
I0301 03:50:56.762008 140525281732352 logging_writer.py:48] [229300] global_step=229300, grad_norm=4.67636251449585, loss=0.6577492952346802
I0301 03:51:30.514157 140525273339648 logging_writer.py:48] [229400] global_step=229400, grad_norm=4.820639133453369, loss=0.6226114630699158
I0301 03:52:04.253584 140525281732352 logging_writer.py:48] [229500] global_step=229500, grad_norm=5.070105075836182, loss=0.6313707828521729
I0301 03:52:25.664204 140688601454400 spec.py:321] Evaluating on the training split.
I0301 03:52:32.458655 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 03:52:41.076541 140688601454400 spec.py:349] Evaluating on the test split.
I0301 03:52:43.347629 140688601454400 submission_runner.py:411] Time since start: 80358.42s, 	Step: 229565, 	{'train/accuracy': 0.9593231678009033, 'train/loss': 0.15072134137153625, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0427016019821167, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.81840181350708, 'test/num_examples': 10000, 'score': 77577.66748857498, 'total_duration': 80358.41845607758, 'accumulated_submission_time': 77577.66748857498, 'accumulated_eval_time': 2764.244913339615, 'accumulated_logging_time': 8.647281408309937}
I0301 03:52:43.405468 140523947947776 logging_writer.py:48] [229565] accumulated_eval_time=2764.244913, accumulated_logging_time=8.647281, accumulated_submission_time=77577.667489, global_step=229565, preemption_count=0, score=77577.667489, test/accuracy=0.631600, test/loss=1.818402, test/num_examples=10000, total_duration=80358.418456, train/accuracy=0.959323, train/loss=0.150721, validation/accuracy=0.754900, validation/loss=1.042702, validation/num_examples=50000
I0301 03:52:55.606418 140525256554240 logging_writer.py:48] [229600] global_step=229600, grad_norm=5.121166706085205, loss=0.7132513523101807
I0301 03:53:29.326026 140523947947776 logging_writer.py:48] [229700] global_step=229700, grad_norm=4.401185512542725, loss=0.5965741872787476
I0301 03:54:03.071841 140525256554240 logging_writer.py:48] [229800] global_step=229800, grad_norm=4.778791427612305, loss=0.6757574081420898
I0301 03:54:36.864650 140523947947776 logging_writer.py:48] [229900] global_step=229900, grad_norm=4.134889125823975, loss=0.5418009161949158
I0301 03:55:10.635074 140525256554240 logging_writer.py:48] [230000] global_step=230000, grad_norm=5.590954780578613, loss=0.5618907809257507
I0301 03:55:44.405449 140523947947776 logging_writer.py:48] [230100] global_step=230100, grad_norm=4.605024814605713, loss=0.7075874209403992
I0301 03:56:18.208032 140525256554240 logging_writer.py:48] [230200] global_step=230200, grad_norm=4.3422465324401855, loss=0.5463730096817017
I0301 03:56:51.982963 140523947947776 logging_writer.py:48] [230300] global_step=230300, grad_norm=5.235045433044434, loss=0.7326650619506836
I0301 03:57:25.748185 140525256554240 logging_writer.py:48] [230400] global_step=230400, grad_norm=4.243897914886475, loss=0.6315829157829285
I0301 03:57:59.512009 140523947947776 logging_writer.py:48] [230500] global_step=230500, grad_norm=4.553462505340576, loss=0.6508617401123047
I0301 03:58:33.299929 140525256554240 logging_writer.py:48] [230600] global_step=230600, grad_norm=4.75219202041626, loss=0.6636396050453186
I0301 03:59:07.266849 140523947947776 logging_writer.py:48] [230700] global_step=230700, grad_norm=5.095874786376953, loss=0.6427784562110901
I0301 03:59:41.050109 140525256554240 logging_writer.py:48] [230800] global_step=230800, grad_norm=4.677676200866699, loss=0.6064962148666382
I0301 04:00:14.850584 140523947947776 logging_writer.py:48] [230900] global_step=230900, grad_norm=4.200425148010254, loss=0.6049631237983704
I0301 04:00:48.629381 140525256554240 logging_writer.py:48] [231000] global_step=231000, grad_norm=4.401554107666016, loss=0.5837467312812805
I0301 04:01:13.450061 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:01:19.534404 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:01:27.956444 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:01:30.208329 140688601454400 submission_runner.py:411] Time since start: 80885.28s, 	Step: 231075, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14954286813735962, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.043567180633545, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8206231594085693, 'test/num_examples': 10000, 'score': 78087.64923286438, 'total_duration': 80885.27916908264, 'accumulated_submission_time': 78087.64923286438, 'accumulated_eval_time': 2781.0031356811523, 'accumulated_logging_time': 8.715314626693726}
I0301 04:01:30.266622 140525264946944 logging_writer.py:48] [231075] accumulated_eval_time=2781.003136, accumulated_logging_time=8.715315, accumulated_submission_time=78087.649233, global_step=231075, preemption_count=0, score=78087.649233, test/accuracy=0.631500, test/loss=1.820623, test/num_examples=10000, total_duration=80885.279169, train/accuracy=0.960061, train/loss=0.149543, validation/accuracy=0.755200, validation/loss=1.043567, validation/num_examples=50000
I0301 04:01:39.055366 140525273339648 logging_writer.py:48] [231100] global_step=231100, grad_norm=4.704275608062744, loss=0.6748440265655518
I0301 04:02:12.853589 140525264946944 logging_writer.py:48] [231200] global_step=231200, grad_norm=4.350668907165527, loss=0.6183909177780151
I0301 04:02:46.626810 140525273339648 logging_writer.py:48] [231300] global_step=231300, grad_norm=4.562872886657715, loss=0.5904005169868469
I0301 04:03:20.408961 140525264946944 logging_writer.py:48] [231400] global_step=231400, grad_norm=4.373770236968994, loss=0.6376954317092896
I0301 04:03:54.185517 140525273339648 logging_writer.py:48] [231500] global_step=231500, grad_norm=4.476535320281982, loss=0.5923458933830261
I0301 04:04:27.935305 140525264946944 logging_writer.py:48] [231600] global_step=231600, grad_norm=4.633012771606445, loss=0.6106998920440674
I0301 04:05:01.813856 140525273339648 logging_writer.py:48] [231700] global_step=231700, grad_norm=4.7910990715026855, loss=0.6886338591575623
I0301 04:05:35.662930 140525264946944 logging_writer.py:48] [231800] global_step=231800, grad_norm=4.072907447814941, loss=0.6323105692863464
I0301 04:06:09.451150 140525273339648 logging_writer.py:48] [231900] global_step=231900, grad_norm=4.627554416656494, loss=0.5923871994018555
I0301 04:06:43.249796 140525264946944 logging_writer.py:48] [232000] global_step=232000, grad_norm=4.62607479095459, loss=0.6265440583229065
I0301 04:07:17.022045 140525273339648 logging_writer.py:48] [232100] global_step=232100, grad_norm=4.250011444091797, loss=0.5698090195655823
I0301 04:07:50.808709 140525264946944 logging_writer.py:48] [232200] global_step=232200, grad_norm=4.328026294708252, loss=0.5972914695739746
I0301 04:08:24.565445 140525273339648 logging_writer.py:48] [232300] global_step=232300, grad_norm=4.439908027648926, loss=0.6529481410980225
I0301 04:08:58.339441 140525264946944 logging_writer.py:48] [232400] global_step=232400, grad_norm=5.156295299530029, loss=0.6708871126174927
I0301 04:09:32.150516 140525273339648 logging_writer.py:48] [232500] global_step=232500, grad_norm=4.43898344039917, loss=0.5992920994758606
I0301 04:10:00.328869 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:10:06.583636 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:10:15.201609 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:10:17.489814 140688601454400 submission_runner.py:411] Time since start: 81412.56s, 	Step: 232585, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.1485619843006134, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0447583198547363, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8235143423080444, 'test/num_examples': 10000, 'score': 78597.64774560928, 'total_duration': 81412.56066179276, 'accumulated_submission_time': 78597.64774560928, 'accumulated_eval_time': 2798.164034128189, 'accumulated_logging_time': 8.785706281661987}
I0301 04:10:17.545589 140522605770496 logging_writer.py:48] [232585] accumulated_eval_time=2798.164034, accumulated_logging_time=8.785706, accumulated_submission_time=78597.647746, global_step=232585, preemption_count=0, score=78597.647746, test/accuracy=0.631700, test/loss=1.823514, test/num_examples=10000, total_duration=81412.560662, train/accuracy=0.960678, train/loss=0.148562, validation/accuracy=0.755400, validation/loss=1.044758, validation/num_examples=50000
I0301 04:10:22.959875 140523092305664 logging_writer.py:48] [232600] global_step=232600, grad_norm=4.778835296630859, loss=0.655860185623169
I0301 04:10:56.694876 140522605770496 logging_writer.py:48] [232700] global_step=232700, grad_norm=4.719293117523193, loss=0.6469756364822388
I0301 04:11:30.571710 140523092305664 logging_writer.py:48] [232800] global_step=232800, grad_norm=4.090002536773682, loss=0.5664670467376709
I0301 04:12:04.370703 140522605770496 logging_writer.py:48] [232900] global_step=232900, grad_norm=4.918889999389648, loss=0.6095057129859924
I0301 04:12:38.152561 140523092305664 logging_writer.py:48] [233000] global_step=233000, grad_norm=4.699023246765137, loss=0.635298490524292
I0301 04:13:11.963384 140522605770496 logging_writer.py:48] [233100] global_step=233100, grad_norm=4.121663570404053, loss=0.6244800686836243
I0301 04:13:45.754948 140523092305664 logging_writer.py:48] [233200] global_step=233200, grad_norm=4.562376976013184, loss=0.61322420835495
I0301 04:14:19.565069 140522605770496 logging_writer.py:48] [233300] global_step=233300, grad_norm=4.537543773651123, loss=0.6319582462310791
I0301 04:14:53.330374 140523092305664 logging_writer.py:48] [233400] global_step=233400, grad_norm=4.901395320892334, loss=0.6501585841178894
I0301 04:15:27.142066 140522605770496 logging_writer.py:48] [233500] global_step=233500, grad_norm=4.3405985832214355, loss=0.6280087232589722
I0301 04:16:00.921619 140523092305664 logging_writer.py:48] [233600] global_step=233600, grad_norm=4.829561710357666, loss=0.6087143421173096
I0301 04:16:34.691280 140522605770496 logging_writer.py:48] [233700] global_step=233700, grad_norm=4.667642116546631, loss=0.6561294794082642
I0301 04:17:08.455796 140523092305664 logging_writer.py:48] [233800] global_step=233800, grad_norm=4.435597896575928, loss=0.6558652520179749
I0301 04:17:42.354457 140522605770496 logging_writer.py:48] [233900] global_step=233900, grad_norm=4.441266059875488, loss=0.6076316833496094
I0301 04:18:16.143695 140523092305664 logging_writer.py:48] [234000] global_step=234000, grad_norm=4.487856864929199, loss=0.6182170510292053
I0301 04:18:47.689608 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:18:53.787421 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:19:02.589768 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:19:04.849570 140688601454400 submission_runner.py:411] Time since start: 81939.92s, 	Step: 234095, 	{'train/accuracy': 0.9609375, 'train/loss': 0.1463867574930191, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0434765815734863, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.821435809135437, 'test/num_examples': 10000, 'score': 79107.72985172272, 'total_duration': 81939.9204120636, 'accumulated_submission_time': 79107.72985172272, 'accumulated_eval_time': 2815.323947429657, 'accumulated_logging_time': 8.851818084716797}
I0301 04:19:04.908301 140522597377792 logging_writer.py:48] [234095] accumulated_eval_time=2815.323947, accumulated_logging_time=8.851818, accumulated_submission_time=79107.729852, global_step=234095, preemption_count=0, score=79107.729852, test/accuracy=0.631400, test/loss=1.821436, test/num_examples=10000, total_duration=81939.920412, train/accuracy=0.960938, train/loss=0.146387, validation/accuracy=0.754980, validation/loss=1.043477, validation/num_examples=50000
I0301 04:19:06.944354 140525264946944 logging_writer.py:48] [234100] global_step=234100, grad_norm=4.5761260986328125, loss=0.612932026386261
I0301 04:19:40.677332 140522597377792 logging_writer.py:48] [234200] global_step=234200, grad_norm=4.396878719329834, loss=0.656702995300293
I0301 04:20:14.414067 140525264946944 logging_writer.py:48] [234300] global_step=234300, grad_norm=4.1770148277282715, loss=0.552661120891571
I0301 04:20:48.178588 140522597377792 logging_writer.py:48] [234400] global_step=234400, grad_norm=4.300816059112549, loss=0.6053590178489685
I0301 04:21:21.927257 140525264946944 logging_writer.py:48] [234500] global_step=234500, grad_norm=4.283773422241211, loss=0.5944515466690063
I0301 04:21:55.741714 140522597377792 logging_writer.py:48] [234600] global_step=234600, grad_norm=4.348336219787598, loss=0.5776465535163879
I0301 04:22:29.536296 140525264946944 logging_writer.py:48] [234700] global_step=234700, grad_norm=4.557799816131592, loss=0.7009851932525635
I0301 04:23:03.333799 140522597377792 logging_writer.py:48] [234800] global_step=234800, grad_norm=4.48992395401001, loss=0.6226797699928284
I0301 04:23:37.228147 140525264946944 logging_writer.py:48] [234900] global_step=234900, grad_norm=4.69235897064209, loss=0.6669711470603943
I0301 04:24:11.020785 140522597377792 logging_writer.py:48] [235000] global_step=235000, grad_norm=4.223544597625732, loss=0.5654581189155579
I0301 04:24:44.772707 140525264946944 logging_writer.py:48] [235100] global_step=235100, grad_norm=4.317965984344482, loss=0.5977329015731812
I0301 04:25:18.543560 140522597377792 logging_writer.py:48] [235200] global_step=235200, grad_norm=4.574239253997803, loss=0.6102733612060547
I0301 04:25:52.352008 140525264946944 logging_writer.py:48] [235300] global_step=235300, grad_norm=4.199673652648926, loss=0.5704938769340515
I0301 04:26:26.157780 140522597377792 logging_writer.py:48] [235400] global_step=235400, grad_norm=4.665511131286621, loss=0.6089844107627869
I0301 04:26:59.975200 140525264946944 logging_writer.py:48] [235500] global_step=235500, grad_norm=4.2763237953186035, loss=0.5639355778694153
I0301 04:27:33.764814 140522597377792 logging_writer.py:48] [235600] global_step=235600, grad_norm=4.31069278717041, loss=0.630965530872345
I0301 04:27:34.933004 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:27:41.019257 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:27:49.427033 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:27:51.702517 140688601454400 submission_runner.py:411] Time since start: 82466.77s, 	Step: 235605, 	{'train/accuracy': 0.9598214030265808, 'train/loss': 0.1489105075597763, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0432418584823608, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.820827841758728, 'test/num_examples': 10000, 'score': 79617.69214963913, 'total_duration': 82466.77335429192, 'accumulated_submission_time': 79617.69214963913, 'accumulated_eval_time': 2832.093398332596, 'accumulated_logging_time': 8.920358657836914}
I0301 04:27:51.755826 140522588985088 logging_writer.py:48] [235605] accumulated_eval_time=2832.093398, accumulated_logging_time=8.920359, accumulated_submission_time=79617.692150, global_step=235605, preemption_count=0, score=79617.692150, test/accuracy=0.631200, test/loss=1.820828, test/num_examples=10000, total_duration=82466.773354, train/accuracy=0.959821, train/loss=0.148911, validation/accuracy=0.755100, validation/loss=1.043242, validation/num_examples=50000
I0301 04:28:24.175969 140523092305664 logging_writer.py:48] [235700] global_step=235700, grad_norm=4.461544036865234, loss=0.652314305305481
I0301 04:28:57.967110 140522588985088 logging_writer.py:48] [235800] global_step=235800, grad_norm=4.74287748336792, loss=0.6735701560974121
I0301 04:29:31.879002 140523092305664 logging_writer.py:48] [235900] global_step=235900, grad_norm=4.633210182189941, loss=0.6858755946159363
I0301 04:30:05.602723 140522588985088 logging_writer.py:48] [236000] global_step=236000, grad_norm=4.41048526763916, loss=0.6447526216506958
I0301 04:30:39.368226 140523092305664 logging_writer.py:48] [236100] global_step=236100, grad_norm=4.6515350341796875, loss=0.6634931564331055
I0301 04:31:13.194024 140522588985088 logging_writer.py:48] [236200] global_step=236200, grad_norm=4.3719987869262695, loss=0.6332014203071594
I0301 04:31:46.968171 140523092305664 logging_writer.py:48] [236300] global_step=236300, grad_norm=4.046478748321533, loss=0.5599696040153503
I0301 04:32:20.771150 140522588985088 logging_writer.py:48] [236400] global_step=236400, grad_norm=4.408907413482666, loss=0.5975283980369568
I0301 04:32:54.523451 140523092305664 logging_writer.py:48] [236500] global_step=236500, grad_norm=4.42635440826416, loss=0.6016286611557007
I0301 04:33:28.266067 140522588985088 logging_writer.py:48] [236600] global_step=236600, grad_norm=4.424893379211426, loss=0.6072298288345337
I0301 04:34:02.025349 140523092305664 logging_writer.py:48] [236700] global_step=236700, grad_norm=4.133275032043457, loss=0.5771739482879639
I0301 04:34:35.832323 140522588985088 logging_writer.py:48] [236800] global_step=236800, grad_norm=4.3908185958862305, loss=0.5820045471191406
I0301 04:35:09.624360 140523092305664 logging_writer.py:48] [236900] global_step=236900, grad_norm=4.439223766326904, loss=0.5955739617347717
I0301 04:35:43.505810 140522588985088 logging_writer.py:48] [237000] global_step=237000, grad_norm=4.5672502517700195, loss=0.5882369875907898
I0301 04:36:17.294310 140523092305664 logging_writer.py:48] [237100] global_step=237100, grad_norm=4.289352893829346, loss=0.5874305367469788
I0301 04:36:21.841596 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:36:28.075175 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:36:36.751900 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:36:39.099954 140688601454400 submission_runner.py:411] Time since start: 82994.17s, 	Step: 237115, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14523626863956451, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.044255256652832, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8229202032089233, 'test/num_examples': 10000, 'score': 80127.71584177017, 'total_duration': 82994.17069745064, 'accumulated_submission_time': 80127.71584177017, 'accumulated_eval_time': 2849.3516008853912, 'accumulated_logging_time': 8.984033823013306}
I0301 04:36:39.156003 140522605770496 logging_writer.py:48] [237115] accumulated_eval_time=2849.351601, accumulated_logging_time=8.984034, accumulated_submission_time=80127.715842, global_step=237115, preemption_count=0, score=80127.715842, test/accuracy=0.631500, test/loss=1.822920, test/num_examples=10000, total_duration=82994.170697, train/accuracy=0.961117, train/loss=0.145236, validation/accuracy=0.755380, validation/loss=1.044255, validation/num_examples=50000
I0301 04:37:08.158998 140525256554240 logging_writer.py:48] [237200] global_step=237200, grad_norm=4.324195384979248, loss=0.6683086156845093
I0301 04:37:41.890621 140522605770496 logging_writer.py:48] [237300] global_step=237300, grad_norm=4.909621715545654, loss=0.6393896341323853
I0301 04:38:15.657883 140525256554240 logging_writer.py:48] [237400] global_step=237400, grad_norm=4.545631408691406, loss=0.6606314182281494
I0301 04:38:49.435560 140522605770496 logging_writer.py:48] [237500] global_step=237500, grad_norm=4.878351211547852, loss=0.6611472964286804
I0301 04:39:23.208380 140525256554240 logging_writer.py:48] [237600] global_step=237600, grad_norm=4.173980236053467, loss=0.627072811126709
I0301 04:39:56.961554 140522605770496 logging_writer.py:48] [237700] global_step=237700, grad_norm=4.516926288604736, loss=0.6540743112564087
I0301 04:40:30.696207 140525256554240 logging_writer.py:48] [237800] global_step=237800, grad_norm=4.54553747177124, loss=0.6827049851417542
I0301 04:41:07.343310 140522605770496 logging_writer.py:48] [237900] global_step=237900, grad_norm=4.912890434265137, loss=0.6138405799865723
I0301 04:41:59.379009 140525256554240 logging_writer.py:48] [238000] global_step=238000, grad_norm=4.511707305908203, loss=0.6940820813179016
I0301 04:42:33.242478 140522605770496 logging_writer.py:48] [238100] global_step=238100, grad_norm=4.426140785217285, loss=0.5941964387893677
I0301 04:43:07.011824 140525256554240 logging_writer.py:48] [238200] global_step=238200, grad_norm=4.3809814453125, loss=0.649874746799469
I0301 04:43:40.801877 140522605770496 logging_writer.py:48] [238300] global_step=238300, grad_norm=4.753421306610107, loss=0.6506714224815369
I0301 04:44:14.586917 140525256554240 logging_writer.py:48] [238400] global_step=238400, grad_norm=4.560342311859131, loss=0.6680852770805359
I0301 04:44:48.357915 140522605770496 logging_writer.py:48] [238500] global_step=238500, grad_norm=4.696582317352295, loss=0.6363508701324463
I0301 04:45:09.420723 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:45:15.516242 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:45:24.142240 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:45:26.406945 140688601454400 submission_runner.py:411] Time since start: 83521.48s, 	Step: 238564, 	{'train/accuracy': 0.9593430757522583, 'train/loss': 0.1516030877828598, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0438953638076782, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8205711841583252, 'test/num_examples': 10000, 'score': 80637.91840529442, 'total_duration': 83521.47772955894, 'accumulated_submission_time': 80637.91840529442, 'accumulated_eval_time': 2866.3377170562744, 'accumulated_logging_time': 9.051939964294434}
I0301 04:45:26.464905 140523947947776 logging_writer.py:48] [238564] accumulated_eval_time=2866.337717, accumulated_logging_time=9.051940, accumulated_submission_time=80637.918405, global_step=238564, preemption_count=0, score=80637.918405, test/accuracy=0.630900, test/loss=1.820571, test/num_examples=10000, total_duration=83521.477730, train/accuracy=0.959343, train/loss=0.151603, validation/accuracy=0.755160, validation/loss=1.043895, validation/num_examples=50000
I0301 04:45:38.948637 140525281732352 logging_writer.py:48] [238600] global_step=238600, grad_norm=4.204716682434082, loss=0.5814840793609619
I0301 04:46:12.659452 140523947947776 logging_writer.py:48] [238700] global_step=238700, grad_norm=4.479380130767822, loss=0.6001012921333313
I0301 04:46:46.402455 140525281732352 logging_writer.py:48] [238800] global_step=238800, grad_norm=4.897128582000732, loss=0.6227258443832397
I0301 04:47:20.159292 140523947947776 logging_writer.py:48] [238900] global_step=238900, grad_norm=4.594176292419434, loss=0.6556224822998047
I0301 04:47:53.860360 140525281732352 logging_writer.py:48] [239000] global_step=239000, grad_norm=4.405300617218018, loss=0.6024153232574463
I0301 04:48:27.678135 140523947947776 logging_writer.py:48] [239100] global_step=239100, grad_norm=4.62780237197876, loss=0.6076080203056335
I0301 04:49:01.438551 140525281732352 logging_writer.py:48] [239200] global_step=239200, grad_norm=4.645659923553467, loss=0.6553987860679626
I0301 04:49:35.258513 140523947947776 logging_writer.py:48] [239300] global_step=239300, grad_norm=4.315225124359131, loss=0.6977055072784424
I0301 04:50:09.023688 140525281732352 logging_writer.py:48] [239400] global_step=239400, grad_norm=4.43071985244751, loss=0.6311370134353638
I0301 04:50:42.800563 140523947947776 logging_writer.py:48] [239500] global_step=239500, grad_norm=4.840623378753662, loss=0.624481201171875
I0301 04:51:16.580197 140525281732352 logging_writer.py:48] [239600] global_step=239600, grad_norm=5.023852825164795, loss=0.7040486931800842
I0301 04:51:50.367382 140523947947776 logging_writer.py:48] [239700] global_step=239700, grad_norm=5.254558563232422, loss=0.6562153100967407
I0301 04:52:24.190428 140525281732352 logging_writer.py:48] [239800] global_step=239800, grad_norm=4.938777446746826, loss=0.6578701734542847
I0301 04:52:57.965397 140523947947776 logging_writer.py:48] [239900] global_step=239900, grad_norm=4.281118869781494, loss=0.573749840259552
I0301 04:53:31.767754 140525281732352 logging_writer.py:48] [240000] global_step=240000, grad_norm=4.198343276977539, loss=0.6012226343154907
I0301 04:53:56.545752 140688601454400 spec.py:321] Evaluating on the training split.
I0301 04:54:02.793292 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 04:54:11.416159 140688601454400 spec.py:349] Evaluating on the test split.
I0301 04:54:13.687541 140688601454400 submission_runner.py:411] Time since start: 84048.76s, 	Step: 240075, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14484253525733948, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0428738594055176, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8207663297653198, 'test/num_examples': 10000, 'score': 81147.93720412254, 'total_duration': 84048.75836634636, 'accumulated_submission_time': 81147.93720412254, 'accumulated_eval_time': 2883.4794404506683, 'accumulated_logging_time': 9.120002508163452}
I0301 04:54:13.744572 140522597377792 logging_writer.py:48] [240075] accumulated_eval_time=2883.479440, accumulated_logging_time=9.120003, accumulated_submission_time=81147.937204, global_step=240075, preemption_count=0, score=81147.937204, test/accuracy=0.631500, test/loss=1.820766, test/num_examples=10000, total_duration=84048.758366, train/accuracy=0.961535, train/loss=0.144843, validation/accuracy=0.755160, validation/loss=1.042874, validation/num_examples=50000
I0301 04:54:22.569554 140522605770496 logging_writer.py:48] [240100] global_step=240100, grad_norm=4.209436893463135, loss=0.5729370713233948
I0301 04:54:56.275270 140522597377792 logging_writer.py:48] [240200] global_step=240200, grad_norm=4.38895845413208, loss=0.6307699680328369
I0301 04:55:30.049671 140522605770496 logging_writer.py:48] [240300] global_step=240300, grad_norm=4.733099937438965, loss=0.6349725723266602
I0301 04:56:03.824933 140522597377792 logging_writer.py:48] [240400] global_step=240400, grad_norm=4.176606178283691, loss=0.5447097420692444
I0301 04:56:37.611513 140522605770496 logging_writer.py:48] [240500] global_step=240500, grad_norm=4.461583614349365, loss=0.7166107892990112
I0301 04:57:11.391230 140522597377792 logging_writer.py:48] [240600] global_step=240600, grad_norm=4.683437347412109, loss=0.7179124355316162
I0301 04:57:45.158833 140522605770496 logging_writer.py:48] [240700] global_step=240700, grad_norm=4.990654945373535, loss=0.60549396276474
I0301 04:58:18.915008 140522597377792 logging_writer.py:48] [240800] global_step=240800, grad_norm=4.686609745025635, loss=0.6844649910926819
I0301 04:58:52.688464 140522605770496 logging_writer.py:48] [240900] global_step=240900, grad_norm=4.295915126800537, loss=0.5497506856918335
I0301 04:59:26.487966 140522597377792 logging_writer.py:48] [241000] global_step=241000, grad_norm=4.312118053436279, loss=0.612750232219696
I0301 05:00:00.252198 140522605770496 logging_writer.py:48] [241100] global_step=241100, grad_norm=4.164474010467529, loss=0.5481966137886047
I0301 05:00:34.116803 140522597377792 logging_writer.py:48] [241200] global_step=241200, grad_norm=4.987616062164307, loss=0.6726586818695068
I0301 05:01:07.901095 140522605770496 logging_writer.py:48] [241300] global_step=241300, grad_norm=4.585640907287598, loss=0.6110178828239441
I0301 05:01:41.686039 140522597377792 logging_writer.py:48] [241400] global_step=241400, grad_norm=4.860722541809082, loss=0.6019922494888306
I0301 05:02:15.457956 140522605770496 logging_writer.py:48] [241500] global_step=241500, grad_norm=4.3354668617248535, loss=0.662519633769989
I0301 05:02:43.928880 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:02:50.042932 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:02:58.678989 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:03:01.016838 140688601454400 submission_runner.py:411] Time since start: 84576.09s, 	Step: 241586, 	{'train/accuracy': 0.9617147445678711, 'train/loss': 0.14256349205970764, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0439531803131104, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.82180655002594, 'test/num_examples': 10000, 'score': 81658.0580971241, 'total_duration': 84576.08765101433, 'accumulated_submission_time': 81658.0580971241, 'accumulated_eval_time': 2900.5673315525055, 'accumulated_logging_time': 9.188629388809204}
I0301 05:03:01.076642 140525264946944 logging_writer.py:48] [241586] accumulated_eval_time=2900.567332, accumulated_logging_time=9.188629, accumulated_submission_time=81658.058097, global_step=241586, preemption_count=0, score=81658.058097, test/accuracy=0.630500, test/loss=1.821807, test/num_examples=10000, total_duration=84576.087651, train/accuracy=0.961715, train/loss=0.142563, validation/accuracy=0.754920, validation/loss=1.043953, validation/num_examples=50000
I0301 05:03:06.161763 140525273339648 logging_writer.py:48] [241600] global_step=241600, grad_norm=4.492280960083008, loss=0.6270702481269836
I0301 05:03:39.869576 140525264946944 logging_writer.py:48] [241700] global_step=241700, grad_norm=3.9917922019958496, loss=0.5764721632003784
I0301 05:04:13.631658 140525273339648 logging_writer.py:48] [241800] global_step=241800, grad_norm=4.65831184387207, loss=0.6030884981155396
I0301 05:04:47.435514 140525264946944 logging_writer.py:48] [241900] global_step=241900, grad_norm=5.307069301605225, loss=0.7278205156326294
I0301 05:05:21.210765 140525273339648 logging_writer.py:48] [242000] global_step=242000, grad_norm=4.898556709289551, loss=0.6305876970291138
I0301 05:05:54.960353 140525264946944 logging_writer.py:48] [242100] global_step=242100, grad_norm=4.646127223968506, loss=0.6772319674491882
I0301 05:06:28.744540 140525273339648 logging_writer.py:48] [242200] global_step=242200, grad_norm=4.773060321807861, loss=0.5991679430007935
I0301 05:07:02.650450 140525264946944 logging_writer.py:48] [242300] global_step=242300, grad_norm=4.560257434844971, loss=0.6490030288696289
I0301 05:07:36.424365 140525273339648 logging_writer.py:48] [242400] global_step=242400, grad_norm=4.3143439292907715, loss=0.6475600004196167
I0301 05:08:10.179921 140525264946944 logging_writer.py:48] [242500] global_step=242500, grad_norm=4.5790181159973145, loss=0.6420214772224426
I0301 05:08:43.946019 140525273339648 logging_writer.py:48] [242600] global_step=242600, grad_norm=4.445738792419434, loss=0.6980988383293152
I0301 05:09:17.706631 140525264946944 logging_writer.py:48] [242700] global_step=242700, grad_norm=4.801774501800537, loss=0.6302211880683899
I0301 05:09:51.486001 140525273339648 logging_writer.py:48] [242800] global_step=242800, grad_norm=4.366562366485596, loss=0.6474363207817078
I0301 05:10:25.321944 140525264946944 logging_writer.py:48] [242900] global_step=242900, grad_norm=4.776060104370117, loss=0.6302555203437805
I0301 05:10:59.106653 140525273339648 logging_writer.py:48] [243000] global_step=243000, grad_norm=4.301305770874023, loss=0.5724235773086548
I0301 05:11:31.042527 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:11:37.142094 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:11:45.606911 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:11:47.891123 140688601454400 submission_runner.py:411] Time since start: 85102.96s, 	Step: 243096, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14817968010902405, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0450206995010376, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.821830153465271, 'test/num_examples': 10000, 'score': 82167.9572262764, 'total_duration': 85102.96196842194, 'accumulated_submission_time': 82167.9572262764, 'accumulated_eval_time': 2917.4158787727356, 'accumulated_logging_time': 9.261387825012207}
I0301 05:11:47.955458 140523092305664 logging_writer.py:48] [243096] accumulated_eval_time=2917.415879, accumulated_logging_time=9.261388, accumulated_submission_time=82167.957226, global_step=243096, preemption_count=0, score=82167.957226, test/accuracy=0.629800, test/loss=1.821830, test/num_examples=10000, total_duration=85102.961968, train/accuracy=0.960001, train/loss=0.148180, validation/accuracy=0.754920, validation/loss=1.045021, validation/num_examples=50000
I0301 05:11:49.651340 140523947947776 logging_writer.py:48] [243100] global_step=243100, grad_norm=4.078519821166992, loss=0.5625187754631042
I0301 05:12:23.406371 140523092305664 logging_writer.py:48] [243200] global_step=243200, grad_norm=4.455977439880371, loss=0.6059622764587402
I0301 05:12:57.215450 140523947947776 logging_writer.py:48] [243300] global_step=243300, grad_norm=4.285787105560303, loss=0.5590487122535706
I0301 05:13:30.980900 140523092305664 logging_writer.py:48] [243400] global_step=243400, grad_norm=4.597712993621826, loss=0.6119468212127686
I0301 05:14:04.760146 140523947947776 logging_writer.py:48] [243500] global_step=243500, grad_norm=4.564945697784424, loss=0.6430993676185608
I0301 05:14:38.548051 140523092305664 logging_writer.py:48] [243600] global_step=243600, grad_norm=3.971566915512085, loss=0.5910971164703369
I0301 05:15:12.325685 140523947947776 logging_writer.py:48] [243700] global_step=243700, grad_norm=3.8994665145874023, loss=0.5775811672210693
I0301 05:15:46.093809 140523092305664 logging_writer.py:48] [243800] global_step=243800, grad_norm=4.310940265655518, loss=0.655705988407135
I0301 05:16:19.876446 140523947947776 logging_writer.py:48] [243900] global_step=243900, grad_norm=4.619810104370117, loss=0.6862606406211853
I0301 05:16:53.626372 140523092305664 logging_writer.py:48] [244000] global_step=244000, grad_norm=4.512317657470703, loss=0.6327006816864014
I0301 05:17:27.438386 140523947947776 logging_writer.py:48] [244100] global_step=244100, grad_norm=4.583942890167236, loss=0.6694835424423218
I0301 05:18:01.195813 140523092305664 logging_writer.py:48] [244200] global_step=244200, grad_norm=4.336214542388916, loss=0.6058992147445679
I0301 05:18:34.990807 140523947947776 logging_writer.py:48] [244300] global_step=244300, grad_norm=4.343410968780518, loss=0.5431952476501465
I0301 05:19:08.887285 140523092305664 logging_writer.py:48] [244400] global_step=244400, grad_norm=4.276182651519775, loss=0.593005895614624
I0301 05:19:42.709783 140523947947776 logging_writer.py:48] [244500] global_step=244500, grad_norm=4.66683292388916, loss=0.6554732918739319
I0301 05:20:16.507626 140523092305664 logging_writer.py:48] [244600] global_step=244600, grad_norm=4.339021682739258, loss=0.5454258918762207
I0301 05:20:18.004417 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:20:24.168894 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:20:32.739265 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:20:35.014869 140688601454400 submission_runner.py:411] Time since start: 85630.09s, 	Step: 244606, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.14900000393390656, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0443910360336304, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8226317167282104, 'test/num_examples': 10000, 'score': 82677.94487500191, 'total_duration': 85630.08571457863, 'accumulated_submission_time': 82677.94487500191, 'accumulated_eval_time': 2934.426279783249, 'accumulated_logging_time': 9.335773944854736}
I0301 05:20:35.075531 140522605770496 logging_writer.py:48] [244606] accumulated_eval_time=2934.426280, accumulated_logging_time=9.335774, accumulated_submission_time=82677.944875, global_step=244606, preemption_count=0, score=82677.944875, test/accuracy=0.630800, test/loss=1.822632, test/num_examples=10000, total_duration=85630.085715, train/accuracy=0.960499, train/loss=0.149000, validation/accuracy=0.755040, validation/loss=1.044391, validation/num_examples=50000
I0301 05:21:07.153077 140523092305664 logging_writer.py:48] [244700] global_step=244700, grad_norm=5.148705959320068, loss=0.6809497475624084
I0301 05:21:40.836190 140522605770496 logging_writer.py:48] [244800] global_step=244800, grad_norm=4.6822919845581055, loss=0.6249313950538635
I0301 05:22:14.618038 140523092305664 logging_writer.py:48] [244900] global_step=244900, grad_norm=4.449961185455322, loss=0.6145309805870056
I0301 05:22:48.431159 140522605770496 logging_writer.py:48] [245000] global_step=245000, grad_norm=4.37687349319458, loss=0.6665323972702026
I0301 05:23:22.196020 140523092305664 logging_writer.py:48] [245100] global_step=245100, grad_norm=4.0578155517578125, loss=0.6173847317695618
I0301 05:23:55.985702 140522605770496 logging_writer.py:48] [245200] global_step=245200, grad_norm=4.632317543029785, loss=0.6636663675308228
I0301 05:24:29.761055 140523092305664 logging_writer.py:48] [245300] global_step=245300, grad_norm=3.989908456802368, loss=0.5930494070053101
I0301 05:25:03.495198 140522605770496 logging_writer.py:48] [245400] global_step=245400, grad_norm=4.545382022857666, loss=0.6632804870605469
I0301 05:25:37.420844 140523092305664 logging_writer.py:48] [245500] global_step=245500, grad_norm=4.237218379974365, loss=0.6604753136634827
I0301 05:26:11.219406 140522605770496 logging_writer.py:48] [245600] global_step=245600, grad_norm=4.4983811378479, loss=0.6750760674476624
I0301 05:26:44.997025 140523092305664 logging_writer.py:48] [245700] global_step=245700, grad_norm=4.7371368408203125, loss=0.6372173428535461
I0301 05:27:18.804383 140522605770496 logging_writer.py:48] [245800] global_step=245800, grad_norm=4.022542953491211, loss=0.5365209579467773
I0301 05:27:52.580158 140523092305664 logging_writer.py:48] [245900] global_step=245900, grad_norm=4.470577716827393, loss=0.6592322587966919
I0301 05:28:26.356096 140522605770496 logging_writer.py:48] [246000] global_step=246000, grad_norm=4.476341724395752, loss=0.5945743322372437
I0301 05:29:00.126603 140523092305664 logging_writer.py:48] [246100] global_step=246100, grad_norm=4.357756614685059, loss=0.6228122711181641
I0301 05:29:05.332956 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:29:11.445116 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:29:20.046658 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:29:22.289357 140688601454400 submission_runner.py:411] Time since start: 86157.36s, 	Step: 246117, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14785854518413544, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.044091820716858, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.820813775062561, 'test/num_examples': 10000, 'score': 83188.13959169388, 'total_duration': 86157.36020231247, 'accumulated_submission_time': 83188.13959169388, 'accumulated_eval_time': 2951.382652282715, 'accumulated_logging_time': 9.406443357467651}
I0301 05:29:22.347012 140525256554240 logging_writer.py:48] [246117] accumulated_eval_time=2951.382652, accumulated_logging_time=9.406443, accumulated_submission_time=83188.139592, global_step=246117, preemption_count=0, score=83188.139592, test/accuracy=0.631100, test/loss=1.820814, test/num_examples=10000, total_duration=86157.360202, train/accuracy=0.961057, train/loss=0.147859, validation/accuracy=0.754920, validation/loss=1.044092, validation/num_examples=50000
I0301 05:29:50.632995 140525264946944 logging_writer.py:48] [246200] global_step=246200, grad_norm=4.783288478851318, loss=0.7289177179336548
I0301 05:30:24.396603 140525256554240 logging_writer.py:48] [246300] global_step=246300, grad_norm=4.753947734832764, loss=0.6144645810127258
I0301 05:30:58.140956 140525264946944 logging_writer.py:48] [246400] global_step=246400, grad_norm=4.384066104888916, loss=0.6439918875694275
I0301 05:31:31.988085 140525256554240 logging_writer.py:48] [246500] global_step=246500, grad_norm=4.6798553466796875, loss=0.7027556896209717
I0301 05:32:05.744173 140525264946944 logging_writer.py:48] [246600] global_step=246600, grad_norm=4.165704250335693, loss=0.5664386749267578
I0301 05:32:39.506355 140525256554240 logging_writer.py:48] [246700] global_step=246700, grad_norm=4.815118312835693, loss=0.6016749143600464
I0301 05:33:13.294652 140525264946944 logging_writer.py:48] [246800] global_step=246800, grad_norm=4.336502552032471, loss=0.6332079172134399
I0301 05:33:47.108940 140525256554240 logging_writer.py:48] [246900] global_step=246900, grad_norm=3.9434542655944824, loss=0.5498746633529663
I0301 05:34:20.879234 140525264946944 logging_writer.py:48] [247000] global_step=247000, grad_norm=4.659524917602539, loss=0.6648133397102356
I0301 05:34:54.677851 140525256554240 logging_writer.py:48] [247100] global_step=247100, grad_norm=4.646153450012207, loss=0.6577377915382385
I0301 05:35:28.502558 140525264946944 logging_writer.py:48] [247200] global_step=247200, grad_norm=4.896055221557617, loss=0.6549249887466431
I0301 05:36:02.260382 140525256554240 logging_writer.py:48] [247300] global_step=247300, grad_norm=4.744791030883789, loss=0.6996351480484009
I0301 05:36:36.054115 140525264946944 logging_writer.py:48] [247400] global_step=247400, grad_norm=4.629316806793213, loss=0.6475130915641785
I0301 05:37:09.801651 140525256554240 logging_writer.py:48] [247500] global_step=247500, grad_norm=4.596103191375732, loss=0.5772624015808105
I0301 05:37:43.610070 140525264946944 logging_writer.py:48] [247600] global_step=247600, grad_norm=4.148287773132324, loss=0.6223044991493225
I0301 05:37:52.546992 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:37:58.642325 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:38:07.405114 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:38:09.666809 140688601454400 submission_runner.py:411] Time since start: 86684.74s, 	Step: 247628, 	{'train/accuracy': 0.9596220850944519, 'train/loss': 0.14834918081760406, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0434871912002563, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.822364330291748, 'test/num_examples': 10000, 'score': 83698.27619314194, 'total_duration': 86684.73764586449, 'accumulated_submission_time': 83698.27619314194, 'accumulated_eval_time': 2968.502408027649, 'accumulated_logging_time': 9.47542691230774}
I0301 05:38:09.723783 140522588985088 logging_writer.py:48] [247628] accumulated_eval_time=2968.502408, accumulated_logging_time=9.475427, accumulated_submission_time=83698.276193, global_step=247628, preemption_count=0, score=83698.276193, test/accuracy=0.631700, test/loss=1.822364, test/num_examples=10000, total_duration=86684.737646, train/accuracy=0.959622, train/loss=0.148349, validation/accuracy=0.755480, validation/loss=1.043487, validation/num_examples=50000
I0301 05:38:34.348858 140522597377792 logging_writer.py:48] [247700] global_step=247700, grad_norm=4.410447597503662, loss=0.6564216017723083
I0301 05:39:08.083337 140522588985088 logging_writer.py:48] [247800] global_step=247800, grad_norm=4.804006576538086, loss=0.6663232445716858
I0301 05:39:41.850450 140522597377792 logging_writer.py:48] [247900] global_step=247900, grad_norm=4.4846882820129395, loss=0.6603820323944092
I0301 05:40:15.648954 140522588985088 logging_writer.py:48] [248000] global_step=248000, grad_norm=4.837039470672607, loss=0.6104311943054199
I0301 05:40:49.440166 140522597377792 logging_writer.py:48] [248100] global_step=248100, grad_norm=4.605533123016357, loss=0.6270020008087158
I0301 05:41:23.240271 140522588985088 logging_writer.py:48] [248200] global_step=248200, grad_norm=4.672476291656494, loss=0.6844305396080017
I0301 05:41:57.013970 140522597377792 logging_writer.py:48] [248300] global_step=248300, grad_norm=4.26478910446167, loss=0.6071271896362305
I0301 05:42:30.807143 140522588985088 logging_writer.py:48] [248400] global_step=248400, grad_norm=4.301974773406982, loss=0.6178853511810303
I0301 05:43:04.609799 140522597377792 logging_writer.py:48] [248500] global_step=248500, grad_norm=4.4476776123046875, loss=0.620415210723877
I0301 05:43:38.563806 140522588985088 logging_writer.py:48] [248600] global_step=248600, grad_norm=4.814029216766357, loss=0.6585674285888672
I0301 05:44:12.403455 140522597377792 logging_writer.py:48] [248700] global_step=248700, grad_norm=4.502282619476318, loss=0.616027295589447
I0301 05:44:46.201463 140522588985088 logging_writer.py:48] [248800] global_step=248800, grad_norm=4.415621757507324, loss=0.6205430626869202
I0301 05:45:20.007371 140522597377792 logging_writer.py:48] [248900] global_step=248900, grad_norm=4.335565567016602, loss=0.6373345851898193
I0301 05:45:53.804916 140522588985088 logging_writer.py:48] [249000] global_step=249000, grad_norm=4.008137226104736, loss=0.6004053354263306
I0301 05:46:27.626461 140522597377792 logging_writer.py:48] [249100] global_step=249100, grad_norm=4.543398380279541, loss=0.575578510761261
I0301 05:46:39.946793 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:46:46.087490 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:46:54.673353 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:46:57.060144 140688601454400 submission_runner.py:411] Time since start: 87212.13s, 	Step: 249138, 	{'train/accuracy': 0.959382951259613, 'train/loss': 0.1505030393600464, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.044371485710144, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8221702575683594, 'test/num_examples': 10000, 'score': 84208.43679046631, 'total_duration': 87212.13094115257, 'accumulated_submission_time': 84208.43679046631, 'accumulated_eval_time': 2985.615665435791, 'accumulated_logging_time': 9.542508840560913}
I0301 05:46:57.155146 140522588985088 logging_writer.py:48] [249138] accumulated_eval_time=2985.615665, accumulated_logging_time=9.542509, accumulated_submission_time=84208.436790, global_step=249138, preemption_count=0, score=84208.436790, test/accuracy=0.630800, test/loss=1.822170, test/num_examples=10000, total_duration=87212.130941, train/accuracy=0.959383, train/loss=0.150503, validation/accuracy=0.755560, validation/loss=1.044371, validation/num_examples=50000
I0301 05:47:18.421365 140522597377792 logging_writer.py:48] [249200] global_step=249200, grad_norm=4.319072246551514, loss=0.5835177898406982
I0301 05:47:52.139221 140522588985088 logging_writer.py:48] [249300] global_step=249300, grad_norm=4.4998908042907715, loss=0.6461670398712158
I0301 05:48:25.869323 140522597377792 logging_writer.py:48] [249400] global_step=249400, grad_norm=4.49036169052124, loss=0.6155352592468262
I0301 05:48:59.654899 140522588985088 logging_writer.py:48] [249500] global_step=249500, grad_norm=4.443390369415283, loss=0.6527141332626343
I0301 05:49:33.435478 140522597377792 logging_writer.py:48] [249600] global_step=249600, grad_norm=4.278865337371826, loss=0.5827369093894958
I0301 05:50:07.333199 140522588985088 logging_writer.py:48] [249700] global_step=249700, grad_norm=4.074202537536621, loss=0.5685246586799622
I0301 05:50:41.124572 140522597377792 logging_writer.py:48] [249800] global_step=249800, grad_norm=4.768743515014648, loss=0.5824488401412964
I0301 05:51:14.905301 140522588985088 logging_writer.py:48] [249900] global_step=249900, grad_norm=4.801132678985596, loss=0.6539706587791443
I0301 05:51:48.708400 140522597377792 logging_writer.py:48] [250000] global_step=250000, grad_norm=4.289242744445801, loss=0.6005969047546387
I0301 05:52:22.511834 140522588985088 logging_writer.py:48] [250100] global_step=250100, grad_norm=4.235783576965332, loss=0.5911649465560913
I0301 05:52:56.343381 140522597377792 logging_writer.py:48] [250200] global_step=250200, grad_norm=4.645448684692383, loss=0.6032867431640625
I0301 05:53:30.109164 140522588985088 logging_writer.py:48] [250300] global_step=250300, grad_norm=4.567573547363281, loss=0.7094200849533081
I0301 05:54:03.880388 140522597377792 logging_writer.py:48] [250400] global_step=250400, grad_norm=4.733259201049805, loss=0.6625701785087585
I0301 05:54:37.674624 140522588985088 logging_writer.py:48] [250500] global_step=250500, grad_norm=4.7478461265563965, loss=0.6082971096038818
I0301 05:55:11.387607 140522597377792 logging_writer.py:48] [250600] global_step=250600, grad_norm=4.530243396759033, loss=0.6008286476135254
I0301 05:55:27.084682 140688601454400 spec.py:321] Evaluating on the training split.
I0301 05:55:33.165326 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 05:55:41.566334 140688601454400 spec.py:349] Evaluating on the test split.
I0301 05:55:43.828752 140688601454400 submission_runner.py:411] Time since start: 87738.90s, 	Step: 250648, 	{'train/accuracy': 0.9588448405265808, 'train/loss': 0.15175123512744904, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0438597202301025, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8228862285614014, 'test/num_examples': 10000, 'score': 84718.3010494709, 'total_duration': 87738.89959907532, 'accumulated_submission_time': 84718.3010494709, 'accumulated_eval_time': 3002.3596892356873, 'accumulated_logging_time': 9.651327848434448}
I0301 05:55:43.893505 140525264946944 logging_writer.py:48] [250648] accumulated_eval_time=3002.359689, accumulated_logging_time=9.651328, accumulated_submission_time=84718.301049, global_step=250648, preemption_count=0, score=84718.301049, test/accuracy=0.631500, test/loss=1.822886, test/num_examples=10000, total_duration=87738.899599, train/accuracy=0.958845, train/loss=0.151751, validation/accuracy=0.755400, validation/loss=1.043860, validation/num_examples=50000
I0301 05:56:01.777903 140525273339648 logging_writer.py:48] [250700] global_step=250700, grad_norm=4.2964277267456055, loss=0.5472205281257629
I0301 05:56:35.616813 140525264946944 logging_writer.py:48] [250800] global_step=250800, grad_norm=4.370271682739258, loss=0.5810349583625793
I0301 05:57:09.368842 140525273339648 logging_writer.py:48] [250900] global_step=250900, grad_norm=4.1226806640625, loss=0.5761563777923584
I0301 05:57:43.149581 140525264946944 logging_writer.py:48] [251000] global_step=251000, grad_norm=4.775018215179443, loss=0.6391282081604004
I0301 05:58:16.932541 140525273339648 logging_writer.py:48] [251100] global_step=251100, grad_norm=4.4255499839782715, loss=0.5893514156341553
I0301 05:58:50.712985 140525264946944 logging_writer.py:48] [251200] global_step=251200, grad_norm=4.481222629547119, loss=0.6845400333404541
I0301 05:59:24.506336 140525273339648 logging_writer.py:48] [251300] global_step=251300, grad_norm=4.599976062774658, loss=0.6517414450645447
I0301 05:59:58.298874 140525264946944 logging_writer.py:48] [251400] global_step=251400, grad_norm=4.573092937469482, loss=0.6793417930603027
I0301 06:00:32.123043 140525273339648 logging_writer.py:48] [251500] global_step=251500, grad_norm=4.241171360015869, loss=0.5746068358421326
I0301 06:01:05.915294 140525264946944 logging_writer.py:48] [251600] global_step=251600, grad_norm=4.4635725021362305, loss=0.6386864185333252
I0301 06:01:39.687875 140525273339648 logging_writer.py:48] [251700] global_step=251700, grad_norm=4.589482307434082, loss=0.6699291467666626
I0301 06:02:13.548057 140525264946944 logging_writer.py:48] [251800] global_step=251800, grad_norm=4.363106727600098, loss=0.6070194244384766
I0301 06:02:47.283868 140525273339648 logging_writer.py:48] [251900] global_step=251900, grad_norm=4.951169013977051, loss=0.7065654397010803
I0301 06:03:21.087979 140525264946944 logging_writer.py:48] [252000] global_step=252000, grad_norm=5.070484161376953, loss=0.5559719800949097
I0301 06:03:54.850530 140525273339648 logging_writer.py:48] [252100] global_step=252100, grad_norm=4.965123176574707, loss=0.6581754088401794
I0301 06:04:13.884364 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:04:19.985725 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:04:28.571584 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:04:30.853718 140688601454400 submission_runner.py:411] Time since start: 88265.92s, 	Step: 252158, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14626452326774597, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.043168544769287, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8210536241531372, 'test/num_examples': 10000, 'score': 85228.23076438904, 'total_duration': 88265.92456793785, 'accumulated_submission_time': 85228.23076438904, 'accumulated_eval_time': 3019.3289988040924, 'accumulated_logging_time': 9.725855588912964}
I0301 06:04:30.913267 140523092305664 logging_writer.py:48] [252158] accumulated_eval_time=3019.328999, accumulated_logging_time=9.725856, accumulated_submission_time=85228.230764, global_step=252158, preemption_count=0, score=85228.230764, test/accuracy=0.631800, test/loss=1.821054, test/num_examples=10000, total_duration=88265.924568, train/accuracy=0.960957, train/loss=0.146265, validation/accuracy=0.755220, validation/loss=1.043169, validation/num_examples=50000
I0301 06:04:45.425004 140523947947776 logging_writer.py:48] [252200] global_step=252200, grad_norm=4.711723804473877, loss=0.6150070428848267
I0301 06:05:19.135098 140523092305664 logging_writer.py:48] [252300] global_step=252300, grad_norm=4.540888786315918, loss=0.6386044025421143
I0301 06:05:52.923650 140523947947776 logging_writer.py:48] [252400] global_step=252400, grad_norm=4.534395217895508, loss=0.5842400789260864
I0301 06:06:26.665868 140523092305664 logging_writer.py:48] [252500] global_step=252500, grad_norm=4.53634786605835, loss=0.5588567852973938
I0301 06:07:00.445082 140523947947776 logging_writer.py:48] [252600] global_step=252600, grad_norm=4.229348182678223, loss=0.5217195749282837
I0301 06:07:34.266827 140523092305664 logging_writer.py:48] [252700] global_step=252700, grad_norm=5.246745586395264, loss=0.6786553859710693
I0301 06:08:08.036190 140523947947776 logging_writer.py:48] [252800] global_step=252800, grad_norm=3.972984552383423, loss=0.5513409972190857
I0301 06:08:41.865737 140523092305664 logging_writer.py:48] [252900] global_step=252900, grad_norm=4.34895133972168, loss=0.6395386457443237
I0301 06:09:15.645526 140523947947776 logging_writer.py:48] [253000] global_step=253000, grad_norm=4.5719685554504395, loss=0.592331051826477
I0301 06:09:49.417893 140523092305664 logging_writer.py:48] [253100] global_step=253100, grad_norm=4.292838096618652, loss=0.5940636992454529
I0301 06:10:23.228310 140523947947776 logging_writer.py:48] [253200] global_step=253200, grad_norm=4.954825401306152, loss=0.6797100305557251
I0301 06:10:57.010832 140523092305664 logging_writer.py:48] [253300] global_step=253300, grad_norm=4.398703098297119, loss=0.6524621248245239
I0301 06:11:30.791412 140523947947776 logging_writer.py:48] [253400] global_step=253400, grad_norm=4.387241363525391, loss=0.6028264760971069
I0301 06:12:04.577285 140523092305664 logging_writer.py:48] [253500] global_step=253500, grad_norm=4.455292701721191, loss=0.6017088890075684
I0301 06:12:38.365299 140523947947776 logging_writer.py:48] [253600] global_step=253600, grad_norm=4.74771785736084, loss=0.5942420959472656
I0301 06:13:01.167766 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:13:07.392541 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:13:16.002236 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:13:18.262325 140688601454400 submission_runner.py:411] Time since start: 88793.33s, 	Step: 253669, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14913074672222137, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0445446968078613, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8215969800949097, 'test/num_examples': 10000, 'score': 85738.42370462418, 'total_duration': 88793.33317494392, 'accumulated_submission_time': 85738.42370462418, 'accumulated_eval_time': 3036.42351937294, 'accumulated_logging_time': 9.79535961151123}
I0301 06:13:18.325613 140522605770496 logging_writer.py:48] [253669] accumulated_eval_time=3036.423519, accumulated_logging_time=9.795360, accumulated_submission_time=85738.423705, global_step=253669, preemption_count=0, score=85738.423705, test/accuracy=0.631000, test/loss=1.821597, test/num_examples=10000, total_duration=88793.333175, train/accuracy=0.959981, train/loss=0.149131, validation/accuracy=0.755180, validation/loss=1.044545, validation/num_examples=50000
I0301 06:13:29.126385 140523092305664 logging_writer.py:48] [253700] global_step=253700, grad_norm=4.6807098388671875, loss=0.646409273147583
I0301 06:14:02.881130 140522605770496 logging_writer.py:48] [253800] global_step=253800, grad_norm=5.101241588592529, loss=0.611971914768219
I0301 06:14:36.699987 140523092305664 logging_writer.py:48] [253900] global_step=253900, grad_norm=5.074331283569336, loss=0.6260708570480347
I0301 06:15:10.461969 140522605770496 logging_writer.py:48] [254000] global_step=254000, grad_norm=4.198391914367676, loss=0.5999853014945984
I0301 06:15:44.241365 140523092305664 logging_writer.py:48] [254100] global_step=254100, grad_norm=4.12054443359375, loss=0.5967497825622559
I0301 06:16:18.025845 140522605770496 logging_writer.py:48] [254200] global_step=254200, grad_norm=5.0898590087890625, loss=0.6002376079559326
I0301 06:16:51.801409 140523092305664 logging_writer.py:48] [254300] global_step=254300, grad_norm=4.345370292663574, loss=0.6600701808929443
I0301 06:17:25.584450 140522605770496 logging_writer.py:48] [254400] global_step=254400, grad_norm=4.414813995361328, loss=0.6191368103027344
I0301 06:17:59.388982 140523092305664 logging_writer.py:48] [254500] global_step=254500, grad_norm=4.595438480377197, loss=0.6885970830917358
I0301 06:18:33.191161 140522605770496 logging_writer.py:48] [254600] global_step=254600, grad_norm=4.526609897613525, loss=0.6694310307502747
I0301 06:19:06.978577 140523092305664 logging_writer.py:48] [254700] global_step=254700, grad_norm=4.421536445617676, loss=0.6948713064193726
I0301 06:19:40.777514 140522605770496 logging_writer.py:48] [254800] global_step=254800, grad_norm=4.443253517150879, loss=0.6131560206413269
I0301 06:20:14.573231 140523092305664 logging_writer.py:48] [254900] global_step=254900, grad_norm=4.8878865242004395, loss=0.7516739368438721
I0301 06:20:48.407166 140522605770496 logging_writer.py:48] [255000] global_step=255000, grad_norm=5.006734848022461, loss=0.7068576216697693
I0301 06:21:22.155879 140523092305664 logging_writer.py:48] [255100] global_step=255100, grad_norm=4.513322353363037, loss=0.6900572776794434
I0301 06:21:48.286028 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:21:54.385485 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:22:03.102116 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:22:05.404778 140688601454400 submission_runner.py:411] Time since start: 89320.48s, 	Step: 255179, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.1473139524459839, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0440517663955688, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8222875595092773, 'test/num_examples': 10000, 'score': 86248.32073330879, 'total_duration': 89320.47562503815, 'accumulated_submission_time': 86248.32073330879, 'accumulated_eval_time': 3053.542221546173, 'accumulated_logging_time': 9.870186805725098}
I0301 06:22:05.473863 140525256554240 logging_writer.py:48] [255179] accumulated_eval_time=3053.542222, accumulated_logging_time=9.870187, accumulated_submission_time=86248.320733, global_step=255179, preemption_count=0, score=86248.320733, test/accuracy=0.630500, test/loss=1.822288, test/num_examples=10000, total_duration=89320.475625, train/accuracy=0.960081, train/loss=0.147314, validation/accuracy=0.755340, validation/loss=1.044052, validation/num_examples=50000
I0301 06:22:12.912405 140525264946944 logging_writer.py:48] [255200] global_step=255200, grad_norm=4.834208965301514, loss=0.6590813994407654
I0301 06:22:46.638718 140525256554240 logging_writer.py:48] [255300] global_step=255300, grad_norm=4.543605804443359, loss=0.6516611576080322
I0301 06:23:20.384142 140525264946944 logging_writer.py:48] [255400] global_step=255400, grad_norm=4.596296787261963, loss=0.5904815196990967
I0301 06:23:54.145337 140525256554240 logging_writer.py:48] [255500] global_step=255500, grad_norm=4.777074337005615, loss=0.686874508857727
I0301 06:24:27.904631 140525264946944 logging_writer.py:48] [255600] global_step=255600, grad_norm=4.407537937164307, loss=0.6020809412002563
I0301 06:25:01.691843 140525256554240 logging_writer.py:48] [255700] global_step=255700, grad_norm=4.440830230712891, loss=0.5756657123565674
I0301 06:25:35.508635 140525264946944 logging_writer.py:48] [255800] global_step=255800, grad_norm=4.268716335296631, loss=0.5835999846458435
I0301 06:26:09.306017 140525256554240 logging_writer.py:48] [255900] global_step=255900, grad_norm=4.688380718231201, loss=0.7023197412490845
I0301 06:26:43.117554 140525264946944 logging_writer.py:48] [256000] global_step=256000, grad_norm=4.489091873168945, loss=0.6142851114273071
I0301 06:27:16.968456 140525256554240 logging_writer.py:48] [256100] global_step=256100, grad_norm=4.36424446105957, loss=0.643863320350647
I0301 06:27:50.735117 140525264946944 logging_writer.py:48] [256200] global_step=256200, grad_norm=4.264015197753906, loss=0.5896305441856384
I0301 06:28:24.510957 140525256554240 logging_writer.py:48] [256300] global_step=256300, grad_norm=4.564598083496094, loss=0.6578429937362671
I0301 06:28:58.337042 140525264946944 logging_writer.py:48] [256400] global_step=256400, grad_norm=4.534909725189209, loss=0.7136867046356201
I0301 06:29:32.104460 140525256554240 logging_writer.py:48] [256500] global_step=256500, grad_norm=4.172427177429199, loss=0.6262115240097046
I0301 06:30:05.911304 140525264946944 logging_writer.py:48] [256600] global_step=256600, grad_norm=4.442971229553223, loss=0.6592423915863037
I0301 06:30:35.736862 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:30:41.848724 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:30:50.396863 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:30:52.665072 140688601454400 submission_runner.py:411] Time since start: 89847.74s, 	Step: 256690, 	{'train/accuracy': 0.9594427347183228, 'train/loss': 0.15106740593910217, 'validation/accuracy': 0.7545799612998962, 'validation/loss': 1.0446027517318726, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8216809034347534, 'test/num_examples': 10000, 'score': 86758.5216627121, 'total_duration': 89847.73591923714, 'accumulated_submission_time': 86758.5216627121, 'accumulated_eval_time': 3070.470389842987, 'accumulated_logging_time': 9.949352264404297}
I0301 06:30:52.736233 140522597377792 logging_writer.py:48] [256690] accumulated_eval_time=3070.470390, accumulated_logging_time=9.949352, accumulated_submission_time=86758.521663, global_step=256690, preemption_count=0, score=86758.521663, test/accuracy=0.630400, test/loss=1.821681, test/num_examples=10000, total_duration=89847.735919, train/accuracy=0.959443, train/loss=0.151067, validation/accuracy=0.754580, validation/loss=1.044603, validation/num_examples=50000
I0301 06:30:56.472234 140522605770496 logging_writer.py:48] [256700] global_step=256700, grad_norm=4.3439483642578125, loss=0.5983147025108337
I0301 06:31:30.191165 140522597377792 logging_writer.py:48] [256800] global_step=256800, grad_norm=4.554288387298584, loss=0.6268838047981262
I0301 06:32:03.931992 140522605770496 logging_writer.py:48] [256900] global_step=256900, grad_norm=4.596993446350098, loss=0.6645355820655823
I0301 06:32:37.692059 140522597377792 logging_writer.py:48] [257000] global_step=257000, grad_norm=4.622498989105225, loss=0.6208744049072266
I0301 06:33:11.529799 140522605770496 logging_writer.py:48] [257100] global_step=257100, grad_norm=4.943702697753906, loss=0.6275606155395508
I0301 06:33:45.289587 140522597377792 logging_writer.py:48] [257200] global_step=257200, grad_norm=5.118716239929199, loss=0.6737017631530762
I0301 06:34:19.101503 140522605770496 logging_writer.py:48] [257300] global_step=257300, grad_norm=4.787402629852295, loss=0.6894150972366333
I0301 06:34:52.884345 140522597377792 logging_writer.py:48] [257400] global_step=257400, grad_norm=4.597794055938721, loss=0.638401448726654
I0301 06:35:26.700282 140522605770496 logging_writer.py:48] [257500] global_step=257500, grad_norm=4.488408088684082, loss=0.5969164371490479
I0301 06:36:00.484673 140522597377792 logging_writer.py:48] [257600] global_step=257600, grad_norm=4.243598461151123, loss=0.5949023962020874
I0301 06:36:34.290452 140522605770496 logging_writer.py:48] [257700] global_step=257700, grad_norm=4.6138081550598145, loss=0.6847792267799377
I0301 06:37:08.119493 140522597377792 logging_writer.py:48] [257800] global_step=257800, grad_norm=4.113797187805176, loss=0.5963300466537476
I0301 06:37:41.913012 140522605770496 logging_writer.py:48] [257900] global_step=257900, grad_norm=4.768065929412842, loss=0.6571131944656372
I0301 06:38:15.709573 140522597377792 logging_writer.py:48] [258000] global_step=258000, grad_norm=4.508004665374756, loss=0.7167282700538635
I0301 06:38:49.522373 140522605770496 logging_writer.py:48] [258100] global_step=258100, grad_norm=4.690426826477051, loss=0.6416717767715454
I0301 06:39:22.815401 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:39:29.088323 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:39:37.718810 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:39:39.998683 140688601454400 submission_runner.py:411] Time since start: 90375.07s, 	Step: 258200, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14667266607284546, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0436691045761108, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8211063146591187, 'test/num_examples': 10000, 'score': 87268.53696084023, 'total_duration': 90375.06952118874, 'accumulated_submission_time': 87268.53696084023, 'accumulated_eval_time': 3087.6536297798157, 'accumulated_logging_time': 10.03275179862976}
I0301 06:39:40.059143 140525264946944 logging_writer.py:48] [258200] accumulated_eval_time=3087.653630, accumulated_logging_time=10.032752, accumulated_submission_time=87268.536961, global_step=258200, preemption_count=0, score=87268.536961, test/accuracy=0.631900, test/loss=1.821106, test/num_examples=10000, total_duration=90375.069521, train/accuracy=0.960758, train/loss=0.146673, validation/accuracy=0.754860, validation/loss=1.043669, validation/num_examples=50000
I0301 06:39:40.399532 140525273339648 logging_writer.py:48] [258200] global_step=258200, grad_norm=4.604302883148193, loss=0.6461203098297119
I0301 06:40:14.149960 140525264946944 logging_writer.py:48] [258300] global_step=258300, grad_norm=4.182067394256592, loss=0.6300020813941956
I0301 06:40:47.887396 140525273339648 logging_writer.py:48] [258400] global_step=258400, grad_norm=4.269443511962891, loss=0.583932101726532
I0301 06:41:21.643383 140525264946944 logging_writer.py:48] [258500] global_step=258500, grad_norm=5.133575439453125, loss=0.5954325199127197
I0301 06:41:55.408723 140525273339648 logging_writer.py:48] [258600] global_step=258600, grad_norm=4.7248711585998535, loss=0.6399275660514832
I0301 06:42:29.169103 140525264946944 logging_writer.py:48] [258700] global_step=258700, grad_norm=4.4126996994018555, loss=0.6254591345787048
I0301 06:43:02.932302 140525273339648 logging_writer.py:48] [258800] global_step=258800, grad_norm=4.960067272186279, loss=0.6152868270874023
I0301 06:43:36.694414 140525264946944 logging_writer.py:48] [258900] global_step=258900, grad_norm=4.410621166229248, loss=0.648696780204773
I0301 06:44:10.470919 140525273339648 logging_writer.py:48] [259000] global_step=259000, grad_norm=4.525339126586914, loss=0.6106497645378113
I0301 06:44:44.232085 140525264946944 logging_writer.py:48] [259100] global_step=259100, grad_norm=4.8767194747924805, loss=0.6076918244361877
I0301 06:45:18.041933 140525273339648 logging_writer.py:48] [259200] global_step=259200, grad_norm=4.0743279457092285, loss=0.5284328460693359
I0301 06:45:51.792147 140525264946944 logging_writer.py:48] [259300] global_step=259300, grad_norm=4.455362796783447, loss=0.6409634351730347
I0301 06:46:25.575496 140525273339648 logging_writer.py:48] [259400] global_step=259400, grad_norm=4.2511796951293945, loss=0.5766023993492126
I0301 06:46:59.378922 140525264946944 logging_writer.py:48] [259500] global_step=259500, grad_norm=4.215966701507568, loss=0.5779275894165039
I0301 06:47:33.178998 140525273339648 logging_writer.py:48] [259600] global_step=259600, grad_norm=4.872927665710449, loss=0.6902710199356079
I0301 06:48:06.968010 140525264946944 logging_writer.py:48] [259700] global_step=259700, grad_norm=4.607116222381592, loss=0.599238395690918
I0301 06:48:10.147954 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:48:16.292064 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:48:24.864698 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:48:27.143018 140688601454400 submission_runner.py:411] Time since start: 90902.21s, 	Step: 259711, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14872534573078156, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0438485145568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8223766088485718, 'test/num_examples': 10000, 'score': 87778.56299948692, 'total_duration': 90902.21386170387, 'accumulated_submission_time': 87778.56299948692, 'accumulated_eval_time': 3104.648650407791, 'accumulated_logging_time': 10.103766679763794}
I0301 06:48:27.207050 140522605770496 logging_writer.py:48] [259711] accumulated_eval_time=3104.648650, accumulated_logging_time=10.103767, accumulated_submission_time=87778.562999, global_step=259711, preemption_count=0, score=87778.562999, test/accuracy=0.630900, test/loss=1.822377, test/num_examples=10000, total_duration=90902.213862, train/accuracy=0.960539, train/loss=0.148725, validation/accuracy=0.754760, validation/loss=1.043849, validation/num_examples=50000
I0301 06:48:57.523637 140523092305664 logging_writer.py:48] [259800] global_step=259800, grad_norm=4.071633815765381, loss=0.6094469428062439
I0301 06:49:31.279718 140522605770496 logging_writer.py:48] [259900] global_step=259900, grad_norm=4.556399345397949, loss=0.663228452205658
I0301 06:50:05.076154 140523092305664 logging_writer.py:48] [260000] global_step=260000, grad_norm=4.648280143737793, loss=0.6599741578102112
I0301 06:50:38.867120 140522605770496 logging_writer.py:48] [260100] global_step=260100, grad_norm=4.285324573516846, loss=0.6241548657417297
I0301 06:51:12.635349 140523092305664 logging_writer.py:48] [260200] global_step=260200, grad_norm=4.204811096191406, loss=0.5516445636749268
I0301 06:51:46.488078 140522605770496 logging_writer.py:48] [260300] global_step=260300, grad_norm=4.188662052154541, loss=0.6096242666244507
I0301 06:52:20.225623 140523092305664 logging_writer.py:48] [260400] global_step=260400, grad_norm=4.837774276733398, loss=0.5876225233078003
I0301 06:52:53.961635 140522605770496 logging_writer.py:48] [260500] global_step=260500, grad_norm=4.241609573364258, loss=0.5695316791534424
I0301 06:53:27.758392 140523092305664 logging_writer.py:48] [260600] global_step=260600, grad_norm=5.052529811859131, loss=0.6581970453262329
I0301 06:54:01.539001 140522605770496 logging_writer.py:48] [260700] global_step=260700, grad_norm=4.693851947784424, loss=0.6688788533210754
I0301 06:54:35.371259 140523092305664 logging_writer.py:48] [260800] global_step=260800, grad_norm=4.901683807373047, loss=0.597099781036377
I0301 06:55:09.142843 140522605770496 logging_writer.py:48] [260900] global_step=260900, grad_norm=4.445263385772705, loss=0.6432700753211975
I0301 06:55:42.963237 140523092305664 logging_writer.py:48] [261000] global_step=261000, grad_norm=4.698371887207031, loss=0.6166642308235168
I0301 06:56:16.725976 140522605770496 logging_writer.py:48] [261100] global_step=261100, grad_norm=4.403132438659668, loss=0.6030165553092957
I0301 06:56:50.476018 140523092305664 logging_writer.py:48] [261200] global_step=261200, grad_norm=4.43098783493042, loss=0.6413528323173523
I0301 06:56:57.372994 140688601454400 spec.py:321] Evaluating on the training split.
I0301 06:57:03.676739 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 06:57:12.283082 140688601454400 spec.py:349] Evaluating on the test split.
I0301 06:57:14.631753 140688601454400 submission_runner.py:411] Time since start: 91429.70s, 	Step: 261222, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.14965467154979706, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0438201427459717, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.820735216140747, 'test/num_examples': 10000, 'score': 88288.66622662544, 'total_duration': 91429.70258498192, 'accumulated_submission_time': 88288.66622662544, 'accumulated_eval_time': 3121.9073457717896, 'accumulated_logging_time': 10.17822551727295}
I0301 06:57:14.693329 140522588985088 logging_writer.py:48] [261222] accumulated_eval_time=3121.907346, accumulated_logging_time=10.178226, accumulated_submission_time=88288.666227, global_step=261222, preemption_count=0, score=88288.666227, test/accuracy=0.630900, test/loss=1.820735, test/num_examples=10000, total_duration=91429.702585, train/accuracy=0.959742, train/loss=0.149655, validation/accuracy=0.755120, validation/loss=1.043820, validation/num_examples=50000
I0301 06:57:41.406388 140522597377792 logging_writer.py:48] [261300] global_step=261300, grad_norm=4.695240497589111, loss=0.6093369126319885
I0301 06:58:15.167386 140522588985088 logging_writer.py:48] [261400] global_step=261400, grad_norm=4.257906913757324, loss=0.5976831912994385
I0301 06:58:48.943817 140522597377792 logging_writer.py:48] [261500] global_step=261500, grad_norm=4.318622589111328, loss=0.5606128573417664
I0301 06:59:22.747614 140522588985088 logging_writer.py:48] [261600] global_step=261600, grad_norm=4.611245155334473, loss=0.6051410436630249
I0301 06:59:56.503416 140522597377792 logging_writer.py:48] [261700] global_step=261700, grad_norm=3.9990456104278564, loss=0.5675899982452393
I0301 07:00:30.286345 140522588985088 logging_writer.py:48] [261800] global_step=261800, grad_norm=4.357668876647949, loss=0.5847902297973633
I0301 07:01:04.064679 140522597377792 logging_writer.py:48] [261900] global_step=261900, grad_norm=4.000539302825928, loss=0.5184863209724426
I0301 07:01:37.840362 140522588985088 logging_writer.py:48] [262000] global_step=262000, grad_norm=4.276822090148926, loss=0.5969048738479614
I0301 07:02:11.605110 140522597377792 logging_writer.py:48] [262100] global_step=262100, grad_norm=4.9852705001831055, loss=0.639856219291687
I0301 07:02:45.378062 140522588985088 logging_writer.py:48] [262200] global_step=262200, grad_norm=4.309776306152344, loss=0.6068883538246155
I0301 07:03:19.143694 140522597377792 logging_writer.py:48] [262300] global_step=262300, grad_norm=4.5557637214660645, loss=0.5920416116714478
I0301 07:03:53.000511 140522588985088 logging_writer.py:48] [262400] global_step=262400, grad_norm=4.22238302230835, loss=0.6149693727493286
I0301 07:04:26.782260 140522597377792 logging_writer.py:48] [262500] global_step=262500, grad_norm=4.518049240112305, loss=0.612453281879425
I0301 07:05:00.555379 140522588985088 logging_writer.py:48] [262600] global_step=262600, grad_norm=4.320854663848877, loss=0.6077015399932861
I0301 07:05:34.353482 140522597377792 logging_writer.py:48] [262700] global_step=262700, grad_norm=4.226806163787842, loss=0.5932484865188599
I0301 07:05:44.956984 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:05:51.005207 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:05:59.906242 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:06:02.173044 140688601454400 submission_runner.py:411] Time since start: 91957.24s, 	Step: 262733, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.14832642674446106, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0432186126708984, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8212600946426392, 'test/num_examples': 10000, 'score': 88798.86704826355, 'total_duration': 91957.24380397797, 'accumulated_submission_time': 88798.86704826355, 'accumulated_eval_time': 3139.123267889023, 'accumulated_logging_time': 10.250165700912476}
I0301 07:06:02.234024 140525273339648 logging_writer.py:48] [262733] accumulated_eval_time=3139.123268, accumulated_logging_time=10.250166, accumulated_submission_time=88798.867048, global_step=262733, preemption_count=0, score=88798.867048, test/accuracy=0.630900, test/loss=1.821260, test/num_examples=10000, total_duration=91957.243804, train/accuracy=0.961177, train/loss=0.148326, validation/accuracy=0.754860, validation/loss=1.043219, validation/num_examples=50000
I0301 07:06:25.201107 140525281732352 logging_writer.py:48] [262800] global_step=262800, grad_norm=4.78665828704834, loss=0.6719828248023987
I0301 07:06:58.946978 140525273339648 logging_writer.py:48] [262900] global_step=262900, grad_norm=4.599133491516113, loss=0.5477825403213501
I0301 07:07:32.732606 140525281732352 logging_writer.py:48] [263000] global_step=263000, grad_norm=4.263885498046875, loss=0.6350526809692383
I0301 07:08:06.500641 140525273339648 logging_writer.py:48] [263100] global_step=263100, grad_norm=4.586440086364746, loss=0.6101490259170532
I0301 07:08:40.279799 140525281732352 logging_writer.py:48] [263200] global_step=263200, grad_norm=4.320525646209717, loss=0.6382686495780945
I0301 07:09:14.104248 140525273339648 logging_writer.py:48] [263300] global_step=263300, grad_norm=4.690521240234375, loss=0.6906528472900391
I0301 07:09:47.871093 140525281732352 logging_writer.py:48] [263400] global_step=263400, grad_norm=4.841972351074219, loss=0.5934514403343201
I0301 07:10:21.733380 140525273339648 logging_writer.py:48] [263500] global_step=263500, grad_norm=4.632596015930176, loss=0.6368588209152222
I0301 07:10:55.549401 140525281732352 logging_writer.py:48] [263600] global_step=263600, grad_norm=4.688415050506592, loss=0.6016014218330383
I0301 07:11:29.319252 140525273339648 logging_writer.py:48] [263700] global_step=263700, grad_norm=4.691949844360352, loss=0.5977905988693237
I0301 07:12:03.090976 140525281732352 logging_writer.py:48] [263800] global_step=263800, grad_norm=4.724606037139893, loss=0.6566411256790161
I0301 07:12:36.921502 140525273339648 logging_writer.py:48] [263900] global_step=263900, grad_norm=4.736965179443359, loss=0.6169211268424988
I0301 07:13:10.689213 140525281732352 logging_writer.py:48] [264000] global_step=264000, grad_norm=4.2689738273620605, loss=0.6308239102363586
I0301 07:13:44.481547 140525273339648 logging_writer.py:48] [264100] global_step=264100, grad_norm=4.330038547515869, loss=0.5542553663253784
I0301 07:14:18.268011 140525281732352 logging_writer.py:48] [264200] global_step=264200, grad_norm=4.3260650634765625, loss=0.6213825345039368
I0301 07:14:32.267112 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:14:38.277899 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:14:46.862410 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:14:49.145264 140688601454400 submission_runner.py:411] Time since start: 92484.22s, 	Step: 264243, 	{'train/accuracy': 0.9611965417861938, 'train/loss': 0.1453111469745636, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0443156957626343, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8213574886322021, 'test/num_examples': 10000, 'score': 89308.83620667458, 'total_duration': 92484.21611142159, 'accumulated_submission_time': 89308.83620667458, 'accumulated_eval_time': 3156.001371860504, 'accumulated_logging_time': 10.322932720184326}
I0301 07:14:49.205735 140522605770496 logging_writer.py:48] [264243] accumulated_eval_time=3156.001372, accumulated_logging_time=10.322933, accumulated_submission_time=89308.836207, global_step=264243, preemption_count=0, score=89308.836207, test/accuracy=0.631000, test/loss=1.821357, test/num_examples=10000, total_duration=92484.216111, train/accuracy=0.961197, train/loss=0.145311, validation/accuracy=0.755580, validation/loss=1.044316, validation/num_examples=50000
I0301 07:15:08.808411 140523092305664 logging_writer.py:48] [264300] global_step=264300, grad_norm=4.2460198402404785, loss=0.5924264192581177
I0301 07:15:42.532949 140522605770496 logging_writer.py:48] [264400] global_step=264400, grad_norm=4.494097709655762, loss=0.5315992832183838
I0301 07:16:16.366087 140523092305664 logging_writer.py:48] [264500] global_step=264500, grad_norm=4.1463117599487305, loss=0.5756693482398987
I0301 07:16:50.133735 140522605770496 logging_writer.py:48] [264600] global_step=264600, grad_norm=4.603216648101807, loss=0.6080479025840759
I0301 07:17:23.917620 140523092305664 logging_writer.py:48] [264700] global_step=264700, grad_norm=4.62255334854126, loss=0.6005380749702454
I0301 07:17:57.706523 140522605770496 logging_writer.py:48] [264800] global_step=264800, grad_norm=4.353854179382324, loss=0.6228270530700684
I0301 07:18:31.474441 140523092305664 logging_writer.py:48] [264900] global_step=264900, grad_norm=4.510630130767822, loss=0.6758257150650024
I0301 07:19:05.266310 140522605770496 logging_writer.py:48] [265000] global_step=265000, grad_norm=4.6847758293151855, loss=0.7099795341491699
I0301 07:19:39.024561 140523092305664 logging_writer.py:48] [265100] global_step=265100, grad_norm=4.461305618286133, loss=0.6370069980621338
I0301 07:20:12.752774 140522605770496 logging_writer.py:48] [265200] global_step=265200, grad_norm=4.7845611572265625, loss=0.5710143446922302
I0301 07:20:46.503661 140523092305664 logging_writer.py:48] [265300] global_step=265300, grad_norm=5.089189529418945, loss=0.5980145931243896
I0301 07:21:20.274049 140522605770496 logging_writer.py:48] [265400] global_step=265400, grad_norm=6.289463520050049, loss=0.6138452291488647
I0301 07:21:54.057460 140523092305664 logging_writer.py:48] [265500] global_step=265500, grad_norm=4.5360612869262695, loss=0.5900610685348511
I0301 07:22:27.983001 140522605770496 logging_writer.py:48] [265600] global_step=265600, grad_norm=4.878156661987305, loss=0.6581157445907593
I0301 07:23:01.705873 140523092305664 logging_writer.py:48] [265700] global_step=265700, grad_norm=4.423771381378174, loss=0.6253097653388977
I0301 07:23:19.419102 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:23:25.471933 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:23:34.215715 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:23:36.474786 140688601454400 submission_runner.py:411] Time since start: 93011.55s, 	Step: 265754, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.1484108865261078, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0444883108139038, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8218551874160767, 'test/num_examples': 10000, 'score': 89818.98686289787, 'total_duration': 93011.54561305046, 'accumulated_submission_time': 89818.98686289787, 'accumulated_eval_time': 3173.0569911003113, 'accumulated_logging_time': 10.393508911132812}
I0301 07:23:36.539805 140525264946944 logging_writer.py:48] [265754] accumulated_eval_time=3173.056991, accumulated_logging_time=10.393509, accumulated_submission_time=89818.986863, global_step=265754, preemption_count=0, score=89818.986863, test/accuracy=0.631800, test/loss=1.821855, test/num_examples=10000, total_duration=93011.545613, train/accuracy=0.960798, train/loss=0.148411, validation/accuracy=0.755100, validation/loss=1.044488, validation/num_examples=50000
I0301 07:23:52.399930 140525273339648 logging_writer.py:48] [265800] global_step=265800, grad_norm=4.419517517089844, loss=0.6200610399246216
I0301 07:24:26.068986 140525264946944 logging_writer.py:48] [265900] global_step=265900, grad_norm=4.592678546905518, loss=0.5473750829696655
I0301 07:24:59.841790 140525273339648 logging_writer.py:48] [266000] global_step=266000, grad_norm=4.338377952575684, loss=0.6048902273178101
I0301 07:25:33.605638 140525264946944 logging_writer.py:48] [266100] global_step=266100, grad_norm=4.961907386779785, loss=0.6266771554946899
I0301 07:26:07.364542 140525273339648 logging_writer.py:48] [266200] global_step=266200, grad_norm=4.594505786895752, loss=0.6202453374862671
I0301 07:26:41.150044 140525264946944 logging_writer.py:48] [266300] global_step=266300, grad_norm=4.39750337600708, loss=0.6398559808731079
I0301 07:27:14.931126 140525273339648 logging_writer.py:48] [266400] global_step=266400, grad_norm=4.838024616241455, loss=0.6408233642578125
I0301 07:27:48.704896 140525264946944 logging_writer.py:48] [266500] global_step=266500, grad_norm=4.451170921325684, loss=0.6049447655677795
I0301 07:28:22.515075 140525273339648 logging_writer.py:48] [266600] global_step=266600, grad_norm=5.029290199279785, loss=0.6302133798599243
I0301 07:28:56.302120 140525264946944 logging_writer.py:48] [266700] global_step=266700, grad_norm=4.337514400482178, loss=0.6328545212745667
I0301 07:29:30.070567 140525273339648 logging_writer.py:48] [266800] global_step=266800, grad_norm=5.162941932678223, loss=0.6976414918899536
I0301 07:30:03.846676 140525264946944 logging_writer.py:48] [266900] global_step=266900, grad_norm=4.840646266937256, loss=0.7039265632629395
I0301 07:30:37.590494 140525273339648 logging_writer.py:48] [267000] global_step=267000, grad_norm=4.487087249755859, loss=0.6666038036346436
I0301 07:31:11.386195 140525264946944 logging_writer.py:48] [267100] global_step=267100, grad_norm=4.363475322723389, loss=0.6137526631355286
I0301 07:31:45.199974 140525273339648 logging_writer.py:48] [267200] global_step=267200, grad_norm=5.01157808303833, loss=0.7323616743087769
I0301 07:32:06.599714 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:32:12.555243 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:32:21.426649 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:32:23.686524 140688601454400 submission_runner.py:411] Time since start: 93538.76s, 	Step: 267265, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14792300760746002, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0435808897018433, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.82218599319458, 'test/num_examples': 10000, 'score': 90328.98372650146, 'total_duration': 93538.75735902786, 'accumulated_submission_time': 90328.98372650146, 'accumulated_eval_time': 3190.1437480449677, 'accumulated_logging_time': 10.469247817993164}
I0301 07:32:23.749815 140522588985088 logging_writer.py:48] [267265] accumulated_eval_time=3190.143748, accumulated_logging_time=10.469248, accumulated_submission_time=90328.983727, global_step=267265, preemption_count=0, score=90328.983727, test/accuracy=0.630800, test/loss=1.822186, test/num_examples=10000, total_duration=93538.757359, train/accuracy=0.960100, train/loss=0.147923, validation/accuracy=0.755360, validation/loss=1.043581, validation/num_examples=50000
I0301 07:32:35.887629 140522597377792 logging_writer.py:48] [267300] global_step=267300, grad_norm=4.427657604217529, loss=0.6529110670089722
I0301 07:33:09.632241 140522588985088 logging_writer.py:48] [267400] global_step=267400, grad_norm=4.367491722106934, loss=0.6534780263900757
I0301 07:33:43.428585 140522597377792 logging_writer.py:48] [267500] global_step=267500, grad_norm=4.509684085845947, loss=0.6148431897163391
I0301 07:34:17.190967 140522588985088 logging_writer.py:48] [267600] global_step=267600, grad_norm=4.392085552215576, loss=0.5503044128417969
I0301 07:34:51.133764 140522597377792 logging_writer.py:48] [267700] global_step=267700, grad_norm=4.269487380981445, loss=0.559500515460968
I0301 07:35:24.893513 140522588985088 logging_writer.py:48] [267800] global_step=267800, grad_norm=4.876255989074707, loss=0.645628035068512
I0301 07:35:58.628561 140522597377792 logging_writer.py:48] [267900] global_step=267900, grad_norm=4.58725643157959, loss=0.6536977887153625
I0301 07:36:32.386781 140522588985088 logging_writer.py:48] [268000] global_step=268000, grad_norm=4.443203926086426, loss=0.6100919246673584
I0301 07:37:06.223156 140522597377792 logging_writer.py:48] [268100] global_step=268100, grad_norm=4.344697952270508, loss=0.6326904892921448
I0301 07:37:39.985807 140522588985088 logging_writer.py:48] [268200] global_step=268200, grad_norm=3.9438061714172363, loss=0.6334215998649597
I0301 07:38:13.811786 140522597377792 logging_writer.py:48] [268300] global_step=268300, grad_norm=4.45407247543335, loss=0.6453387141227722
I0301 07:38:47.595170 140522588985088 logging_writer.py:48] [268400] global_step=268400, grad_norm=4.403232097625732, loss=0.5931512117385864
I0301 07:39:21.345355 140522597377792 logging_writer.py:48] [268500] global_step=268500, grad_norm=4.596097946166992, loss=0.6413357257843018
I0301 07:39:55.127294 140522588985088 logging_writer.py:48] [268600] global_step=268600, grad_norm=4.31055212020874, loss=0.6576348543167114
I0301 07:40:28.914785 140522597377792 logging_writer.py:48] [268700] global_step=268700, grad_norm=4.6275634765625, loss=0.6748337745666504
I0301 07:40:53.883297 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:41:00.481467 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:41:09.407248 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:41:11.693085 140688601454400 submission_runner.py:411] Time since start: 94066.76s, 	Step: 268775, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.1491859406232834, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0436774492263794, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8209495544433594, 'test/num_examples': 10000, 'score': 90839.05476880074, 'total_duration': 94066.76383280754, 'accumulated_submission_time': 90839.05476880074, 'accumulated_eval_time': 3207.9533960819244, 'accumulated_logging_time': 10.54270601272583}
I0301 07:41:11.771610 140522597377792 logging_writer.py:48] [268775] accumulated_eval_time=3207.953396, accumulated_logging_time=10.542706, accumulated_submission_time=90839.054769, global_step=268775, preemption_count=0, score=90839.054769, test/accuracy=0.631300, test/loss=1.820950, test/num_examples=10000, total_duration=94066.763833, train/accuracy=0.959901, train/loss=0.149186, validation/accuracy=0.754860, validation/loss=1.043677, validation/num_examples=50000
I0301 07:41:20.573611 140525264946944 logging_writer.py:48] [268800] global_step=268800, grad_norm=5.027712345123291, loss=0.636539101600647
I0301 07:41:54.280579 140522597377792 logging_writer.py:48] [268900] global_step=268900, grad_norm=4.4477081298828125, loss=0.6148960590362549
I0301 07:42:28.043613 140525264946944 logging_writer.py:48] [269000] global_step=269000, grad_norm=4.456033229827881, loss=0.596443772315979
I0301 07:43:01.826953 140522597377792 logging_writer.py:48] [269100] global_step=269100, grad_norm=4.871214389801025, loss=0.6280972361564636
I0301 07:43:35.615677 140525264946944 logging_writer.py:48] [269200] global_step=269200, grad_norm=4.201025009155273, loss=0.5417392253875732
I0301 07:44:09.410728 140522597377792 logging_writer.py:48] [269300] global_step=269300, grad_norm=4.54178524017334, loss=0.6462617516517639
I0301 07:44:43.200368 140525264946944 logging_writer.py:48] [269400] global_step=269400, grad_norm=4.209225654602051, loss=0.6039915084838867
I0301 07:45:16.981442 140522597377792 logging_writer.py:48] [269500] global_step=269500, grad_norm=4.4636735916137695, loss=0.6175784468650818
I0301 07:45:50.773406 140525264946944 logging_writer.py:48] [269600] global_step=269600, grad_norm=5.0229082107543945, loss=0.6188085675239563
I0301 07:46:24.567503 140522597377792 logging_writer.py:48] [269700] global_step=269700, grad_norm=4.458371162414551, loss=0.6622222661972046
I0301 07:46:58.416028 140525264946944 logging_writer.py:48] [269800] global_step=269800, grad_norm=4.506076335906982, loss=0.5794448256492615
I0301 07:47:32.213363 140522597377792 logging_writer.py:48] [269900] global_step=269900, grad_norm=5.323029041290283, loss=0.6076105237007141
I0301 07:48:05.995781 140525264946944 logging_writer.py:48] [270000] global_step=270000, grad_norm=4.797351360321045, loss=0.64659583568573
I0301 07:48:39.787413 140522597377792 logging_writer.py:48] [270100] global_step=270100, grad_norm=4.491850852966309, loss=0.6247739791870117
I0301 07:49:13.603209 140525264946944 logging_writer.py:48] [270200] global_step=270200, grad_norm=4.522681713104248, loss=0.6193368434906006
I0301 07:49:41.794666 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:49:47.857110 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:49:56.300963 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:49:59.242778 140688601454400 submission_runner.py:411] Time since start: 94594.31s, 	Step: 270285, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.15049897134304047, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0429737567901611, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8212668895721436, 'test/num_examples': 10000, 'score': 91349.01457190514, 'total_duration': 94594.31363272667, 'accumulated_submission_time': 91349.01457190514, 'accumulated_eval_time': 3225.401467323303, 'accumulated_logging_time': 10.631753206253052}
I0301 07:49:59.296023 140522597377792 logging_writer.py:48] [270285] accumulated_eval_time=3225.401467, accumulated_logging_time=10.631753, accumulated_submission_time=91349.014572, global_step=270285, preemption_count=0, score=91349.014572, test/accuracy=0.630900, test/loss=1.821267, test/num_examples=10000, total_duration=94594.313633, train/accuracy=0.960300, train/loss=0.150499, validation/accuracy=0.755560, validation/loss=1.042974, validation/num_examples=50000
I0301 07:50:04.696322 140522605770496 logging_writer.py:48] [270300] global_step=270300, grad_norm=4.596641540527344, loss=0.6035566926002502
I0301 07:50:38.399540 140522597377792 logging_writer.py:48] [270400] global_step=270400, grad_norm=4.02031135559082, loss=0.5509905815124512
I0301 07:51:12.162847 140522605770496 logging_writer.py:48] [270500] global_step=270500, grad_norm=4.711061954498291, loss=0.6723223328590393
I0301 07:51:45.935213 140522597377792 logging_writer.py:48] [270600] global_step=270600, grad_norm=4.754847049713135, loss=0.6507848501205444
I0301 07:52:19.718971 140522605770496 logging_writer.py:48] [270700] global_step=270700, grad_norm=4.3998284339904785, loss=0.6166369318962097
I0301 07:52:53.490405 140522597377792 logging_writer.py:48] [270800] global_step=270800, grad_norm=4.383147239685059, loss=0.6370605826377869
I0301 07:53:27.352663 140522605770496 logging_writer.py:48] [270900] global_step=270900, grad_norm=4.320840358734131, loss=0.6432217359542847
I0301 07:54:01.155816 140522597377792 logging_writer.py:48] [271000] global_step=271000, grad_norm=4.697957515716553, loss=0.6632936000823975
I0301 07:54:34.901784 140522605770496 logging_writer.py:48] [271100] global_step=271100, grad_norm=4.730448246002197, loss=0.6638094186782837
I0301 07:55:08.693781 140522597377792 logging_writer.py:48] [271200] global_step=271200, grad_norm=4.391735076904297, loss=0.6069833040237427
I0301 07:55:42.459239 140522605770496 logging_writer.py:48] [271300] global_step=271300, grad_norm=4.52679443359375, loss=0.619411051273346
I0301 07:56:16.220035 140522597377792 logging_writer.py:48] [271400] global_step=271400, grad_norm=4.3612284660339355, loss=0.6023962497711182
I0301 07:56:49.987720 140522605770496 logging_writer.py:48] [271500] global_step=271500, grad_norm=4.963251113891602, loss=0.6044915318489075
I0301 07:57:23.757956 140522597377792 logging_writer.py:48] [271600] global_step=271600, grad_norm=4.566527366638184, loss=0.5740450024604797
I0301 07:57:57.552385 140522605770496 logging_writer.py:48] [271700] global_step=271700, grad_norm=4.668576240539551, loss=0.5825390219688416
I0301 07:58:29.447057 140688601454400 spec.py:321] Evaluating on the training split.
I0301 07:58:35.475755 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 07:58:44.224463 140688601454400 spec.py:349] Evaluating on the test split.
I0301 07:58:46.509787 140688601454400 submission_runner.py:411] Time since start: 95121.58s, 	Step: 271796, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.1452290415763855, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0430622100830078, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8198816776275635, 'test/num_examples': 10000, 'score': 91859.10242986679, 'total_duration': 95121.58063220978, 'accumulated_submission_time': 91859.10242986679, 'accumulated_eval_time': 3242.4641485214233, 'accumulated_logging_time': 10.695482969284058}
I0301 07:58:46.570544 140522597377792 logging_writer.py:48] [271796] accumulated_eval_time=3242.464149, accumulated_logging_time=10.695483, accumulated_submission_time=91859.102430, global_step=271796, preemption_count=0, score=91859.102430, test/accuracy=0.631900, test/loss=1.819882, test/num_examples=10000, total_duration=95121.580632, train/accuracy=0.961535, train/loss=0.145229, validation/accuracy=0.755400, validation/loss=1.043062, validation/num_examples=50000
I0301 07:58:48.268167 140522605770496 logging_writer.py:48] [271800] global_step=271800, grad_norm=4.524201393127441, loss=0.6428344249725342
I0301 07:59:22.077737 140522597377792 logging_writer.py:48] [271900] global_step=271900, grad_norm=4.7984299659729, loss=0.6318157315254211
I0301 07:59:55.849291 140522605770496 logging_writer.py:48] [272000] global_step=272000, grad_norm=4.325841426849365, loss=0.6291532516479492
I0301 08:00:29.634789 140522597377792 logging_writer.py:48] [272100] global_step=272100, grad_norm=4.236743450164795, loss=0.5938863754272461
I0301 08:01:03.413070 140522605770496 logging_writer.py:48] [272200] global_step=272200, grad_norm=4.502447128295898, loss=0.6293966174125671
I0301 08:01:37.208075 140522597377792 logging_writer.py:48] [272300] global_step=272300, grad_norm=4.4721808433532715, loss=0.5908480882644653
I0301 08:02:10.978931 140522605770496 logging_writer.py:48] [272400] global_step=272400, grad_norm=4.285162448883057, loss=0.5987313985824585
I0301 08:02:44.773929 140522597377792 logging_writer.py:48] [272500] global_step=272500, grad_norm=4.504729747772217, loss=0.6176303625106812
I0301 08:03:18.549972 140522605770496 logging_writer.py:48] [272600] global_step=272600, grad_norm=4.249999046325684, loss=0.5521836876869202
I0301 08:03:52.312520 140522597377792 logging_writer.py:48] [272700] global_step=272700, grad_norm=4.580193519592285, loss=0.606020987033844
I0301 08:04:26.076290 140522605770496 logging_writer.py:48] [272800] global_step=272800, grad_norm=4.429199695587158, loss=0.5508600473403931
I0301 08:04:59.841943 140522597377792 logging_writer.py:48] [272900] global_step=272900, grad_norm=4.6072821617126465, loss=0.6361483931541443
I0301 08:05:33.712102 140522605770496 logging_writer.py:48] [273000] global_step=273000, grad_norm=4.1801066398620605, loss=0.6235134601593018
I0301 08:06:07.549316 140522597377792 logging_writer.py:48] [273100] global_step=273100, grad_norm=4.337417125701904, loss=0.65083247423172
I0301 08:06:41.340120 140522605770496 logging_writer.py:48] [273200] global_step=273200, grad_norm=4.375478744506836, loss=0.6134895086288452
I0301 08:07:15.165305 140522597377792 logging_writer.py:48] [273300] global_step=273300, grad_norm=4.437032222747803, loss=0.6052736639976501
I0301 08:07:16.656439 140688601454400 spec.py:321] Evaluating on the training split.
I0301 08:07:22.605442 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 08:07:31.513188 140688601454400 spec.py:349] Evaluating on the test split.
I0301 08:07:33.778625 140688601454400 submission_runner.py:411] Time since start: 95648.85s, 	Step: 273306, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.14882172644138336, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0445902347564697, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8220733404159546, 'test/num_examples': 10000, 'score': 92369.12679386139, 'total_duration': 95648.849401474, 'accumulated_submission_time': 92369.12679386139, 'accumulated_eval_time': 3259.586245775223, 'accumulated_logging_time': 10.765870571136475}
I0301 08:07:33.845531 140525264946944 logging_writer.py:48] [273306] accumulated_eval_time=3259.586246, accumulated_logging_time=10.765871, accumulated_submission_time=92369.126794, global_step=273306, preemption_count=0, score=92369.126794, test/accuracy=0.630500, test/loss=1.822073, test/num_examples=10000, total_duration=95648.849401, train/accuracy=0.959682, train/loss=0.148822, validation/accuracy=0.755340, validation/loss=1.044590, validation/num_examples=50000
I0301 08:08:05.908170 140525273339648 logging_writer.py:48] [273400] global_step=273400, grad_norm=4.356481075286865, loss=0.6285859942436218
I0301 08:08:39.672855 140525264946944 logging_writer.py:48] [273500] global_step=273500, grad_norm=4.6174445152282715, loss=0.649342954158783
I0301 08:09:13.464416 140525273339648 logging_writer.py:48] [273600] global_step=273600, grad_norm=4.572803974151611, loss=0.6845707893371582
I0301 08:09:47.255898 140525264946944 logging_writer.py:48] [273700] global_step=273700, grad_norm=4.754447937011719, loss=0.6035991311073303
I0301 08:10:21.031653 140525273339648 logging_writer.py:48] [273800] global_step=273800, grad_norm=4.7017011642456055, loss=0.6523009538650513
I0301 08:10:54.817396 140525264946944 logging_writer.py:48] [273900] global_step=273900, grad_norm=4.361740589141846, loss=0.662735641002655
I0301 08:11:28.724990 140525273339648 logging_writer.py:48] [274000] global_step=274000, grad_norm=4.2598114013671875, loss=0.5578269362449646
I0301 08:12:02.515909 140525264946944 logging_writer.py:48] [274100] global_step=274100, grad_norm=4.382782936096191, loss=0.6324563026428223
I0301 08:12:36.295552 140525273339648 logging_writer.py:48] [274200] global_step=274200, grad_norm=4.834092617034912, loss=0.6836803555488586
I0301 08:13:10.067184 140525264946944 logging_writer.py:48] [274300] global_step=274300, grad_norm=4.276105880737305, loss=0.6225361824035645
I0301 08:13:43.869501 140525273339648 logging_writer.py:48] [274400] global_step=274400, grad_norm=5.112589359283447, loss=0.6627809405326843
I0301 08:14:17.603507 140525264946944 logging_writer.py:48] [274500] global_step=274500, grad_norm=4.338080406188965, loss=0.6336532831192017
I0301 08:14:51.380323 140525273339648 logging_writer.py:48] [274600] global_step=274600, grad_norm=4.504069805145264, loss=0.6695016026496887
I0301 08:15:25.152629 140525264946944 logging_writer.py:48] [274700] global_step=274700, grad_norm=4.607499122619629, loss=0.5965743660926819
I0301 08:15:58.945214 140525273339648 logging_writer.py:48] [274800] global_step=274800, grad_norm=4.294669151306152, loss=0.5817317366600037
I0301 08:16:03.803157 140688601454400 spec.py:321] Evaluating on the training split.
I0301 08:16:09.774324 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 08:16:18.736970 140688601454400 spec.py:349] Evaluating on the test split.
I0301 08:16:21.108688 140688601454400 submission_runner.py:411] Time since start: 96176.18s, 	Step: 274816, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.1465967744588852, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0439108610153198, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8224775791168213, 'test/num_examples': 10000, 'score': 92879.02329158783, 'total_duration': 96176.17948126793, 'accumulated_submission_time': 92879.02329158783, 'accumulated_eval_time': 3276.8916738033295, 'accumulated_logging_time': 10.8425931930542}
I0301 08:16:21.169583 140522605770496 logging_writer.py:48] [274816] accumulated_eval_time=3276.891674, accumulated_logging_time=10.842593, accumulated_submission_time=92879.023292, global_step=274816, preemption_count=0, score=92879.023292, test/accuracy=0.630300, test/loss=1.822478, test/num_examples=10000, total_duration=96176.179481, train/accuracy=0.960798, train/loss=0.146597, validation/accuracy=0.755280, validation/loss=1.043911, validation/num_examples=50000
I0301 08:16:49.843155 140523092305664 logging_writer.py:48] [274900] global_step=274900, grad_norm=4.368856906890869, loss=0.6347215175628662
I0301 08:17:23.537297 140522605770496 logging_writer.py:48] [275000] global_step=275000, grad_norm=4.620932579040527, loss=0.617955207824707
I0301 08:17:57.400224 140523092305664 logging_writer.py:48] [275100] global_step=275100, grad_norm=4.4319257736206055, loss=0.6250500679016113
I0301 08:18:31.147096 140522605770496 logging_writer.py:48] [275200] global_step=275200, grad_norm=4.7437520027160645, loss=0.6566330790519714
I0301 08:19:04.929147 140523092305664 logging_writer.py:48] [275300] global_step=275300, grad_norm=4.7000732421875, loss=0.6110026836395264
I0301 08:19:38.662404 140522605770496 logging_writer.py:48] [275400] global_step=275400, grad_norm=4.292193412780762, loss=0.5835988521575928
I0301 08:20:12.418277 140523092305664 logging_writer.py:48] [275500] global_step=275500, grad_norm=4.267230033874512, loss=0.6906757354736328
I0301 08:20:46.246932 140522605770496 logging_writer.py:48] [275600] global_step=275600, grad_norm=4.345561504364014, loss=0.6368203163146973
I0301 08:21:20.027781 140523092305664 logging_writer.py:48] [275700] global_step=275700, grad_norm=4.358578681945801, loss=0.6344708800315857
I0301 08:21:53.843447 140522605770496 logging_writer.py:48] [275800] global_step=275800, grad_norm=4.479404449462891, loss=0.667493999004364
I0301 08:22:27.628049 140523092305664 logging_writer.py:48] [275900] global_step=275900, grad_norm=4.772488117218018, loss=0.7104376554489136
I0301 08:23:01.437322 140522605770496 logging_writer.py:48] [276000] global_step=276000, grad_norm=4.609978675842285, loss=0.6394537687301636
I0301 08:23:35.214849 140523092305664 logging_writer.py:48] [276100] global_step=276100, grad_norm=4.701054096221924, loss=0.7036899924278259
I0301 08:24:09.043331 140522605770496 logging_writer.py:48] [276200] global_step=276200, grad_norm=3.9326395988464355, loss=0.5894726514816284
I0301 08:24:42.807475 140523092305664 logging_writer.py:48] [276300] global_step=276300, grad_norm=4.25317907333374, loss=0.6225804090499878
I0301 08:24:51.402779 140688601454400 spec.py:321] Evaluating on the training split.
I0301 08:24:57.341631 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 08:25:06.407577 140688601454400 spec.py:349] Evaluating on the test split.
I0301 08:25:08.645017 140688601454400 submission_runner.py:411] Time since start: 96703.72s, 	Step: 276327, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14787167310714722, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.044476866722107, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.821550726890564, 'test/num_examples': 10000, 'score': 93389.19371938705, 'total_duration': 96703.71585345268, 'accumulated_submission_time': 93389.19371938705, 'accumulated_eval_time': 3294.133857011795, 'accumulated_logging_time': 10.913537740707397}
I0301 08:25:08.708363 140522605770496 logging_writer.py:48] [276327] accumulated_eval_time=3294.133857, accumulated_logging_time=10.913538, accumulated_submission_time=93389.193719, global_step=276327, preemption_count=0, score=93389.193719, test/accuracy=0.631000, test/loss=1.821551, test/num_examples=10000, total_duration=96703.715853, train/accuracy=0.960459, train/loss=0.147872, validation/accuracy=0.755180, validation/loss=1.044477, validation/num_examples=50000
I0301 08:25:33.694962 140525264946944 logging_writer.py:48] [276400] global_step=276400, grad_norm=4.6780781745910645, loss=0.6569172143936157
I0301 08:26:07.425780 140522605770496 logging_writer.py:48] [276500] global_step=276500, grad_norm=4.308889865875244, loss=0.5800464749336243
I0301 08:26:41.210188 140525264946944 logging_writer.py:48] [276600] global_step=276600, grad_norm=5.347527980804443, loss=0.638999879360199
I0301 08:27:14.991624 140522605770496 logging_writer.py:48] [276700] global_step=276700, grad_norm=4.227599620819092, loss=0.5759937763214111
I0301 08:27:48.811399 140525264946944 logging_writer.py:48] [276800] global_step=276800, grad_norm=4.339028835296631, loss=0.6484171748161316
I0301 08:28:22.599872 140522605770496 logging_writer.py:48] [276900] global_step=276900, grad_norm=4.256868839263916, loss=0.5403878092765808
I0301 08:28:56.387975 140525264946944 logging_writer.py:48] [277000] global_step=277000, grad_norm=4.541923999786377, loss=0.6489331126213074
I0301 08:29:30.155303 140522605770496 logging_writer.py:48] [277100] global_step=277100, grad_norm=4.369753837585449, loss=0.6461833715438843
I0301 08:30:03.965509 140525264946944 logging_writer.py:48] [277200] global_step=277200, grad_norm=4.889787673950195, loss=0.6230149269104004
I0301 08:30:37.739705 140522605770496 logging_writer.py:48] [277300] global_step=277300, grad_norm=4.614877700805664, loss=0.6319606304168701
I0301 08:31:11.524292 140525264946944 logging_writer.py:48] [277400] global_step=277400, grad_norm=4.4890947341918945, loss=0.6353875398635864
I0301 08:31:45.303668 140522605770496 logging_writer.py:48] [277500] global_step=277500, grad_norm=4.195384502410889, loss=0.6588090062141418
I0301 08:32:19.098836 140525264946944 logging_writer.py:48] [277600] global_step=277600, grad_norm=4.6930646896362305, loss=0.6711704134941101
I0301 08:32:53.177733 140522605770496 logging_writer.py:48] [277700] global_step=277700, grad_norm=4.291347503662109, loss=0.6208688020706177
I0301 08:33:26.958442 140525264946944 logging_writer.py:48] [277800] global_step=277800, grad_norm=4.649207592010498, loss=0.6359747648239136
I0301 08:33:38.913811 140688601454400 spec.py:321] Evaluating on the training split.
I0301 08:33:44.951096 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 08:33:53.695760 140688601454400 spec.py:349] Evaluating on the test split.
I0301 08:33:55.966039 140688601454400 submission_runner.py:411] Time since start: 97231.04s, 	Step: 277837, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.14968299865722656, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0444915294647217, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8213553428649902, 'test/num_examples': 10000, 'score': 93899.33606386185, 'total_duration': 97231.03688788414, 'accumulated_submission_time': 93899.33606386185, 'accumulated_eval_time': 3311.18604016304, 'accumulated_logging_time': 10.987645626068115}
I0301 08:33:56.030277 140523947947776 logging_writer.py:48] [277837] accumulated_eval_time=3311.186040, accumulated_logging_time=10.987646, accumulated_submission_time=93899.336064, global_step=277837, preemption_count=0, score=93899.336064, test/accuracy=0.631400, test/loss=1.821355, test/num_examples=10000, total_duration=97231.036888, train/accuracy=0.960240, train/loss=0.149683, validation/accuracy=0.755360, validation/loss=1.044492, validation/num_examples=50000
I0301 08:34:17.625175 140525256554240 logging_writer.py:48] [277900] global_step=277900, grad_norm=4.645803928375244, loss=0.5651406049728394
I0301 08:34:51.367164 140523947947776 logging_writer.py:48] [278000] global_step=278000, grad_norm=4.692261219024658, loss=0.6094467043876648
I0301 08:35:25.150215 140525256554240 logging_writer.py:48] [278100] global_step=278100, grad_norm=4.673247337341309, loss=0.6382966637611389
I0301 08:35:58.937030 140523947947776 logging_writer.py:48] [278200] global_step=278200, grad_norm=4.524574279785156, loss=0.5919908285140991
I0301 08:36:32.796705 140525256554240 logging_writer.py:48] [278300] global_step=278300, grad_norm=4.423844337463379, loss=0.579107403755188
I0301 08:37:06.580882 140523947947776 logging_writer.py:48] [278400] global_step=278400, grad_norm=4.808513641357422, loss=0.5958425998687744
I0301 08:37:40.388225 140525256554240 logging_writer.py:48] [278500] global_step=278500, grad_norm=4.294491767883301, loss=0.6282927393913269
I0301 08:38:14.179930 140523947947776 logging_writer.py:48] [278600] global_step=278600, grad_norm=4.556628227233887, loss=0.6378604173660278
I0301 08:38:47.964410 140525256554240 logging_writer.py:48] [278700] global_step=278700, grad_norm=4.514317989349365, loss=0.5628522634506226
I0301 08:39:21.733480 140523947947776 logging_writer.py:48] [278800] global_step=278800, grad_norm=4.704844951629639, loss=0.6361313462257385
I0301 08:39:55.458899 140525256554240 logging_writer.py:48] [278900] global_step=278900, grad_norm=4.574944972991943, loss=0.5807195901870728
I0301 08:40:29.236078 140523947947776 logging_writer.py:48] [279000] global_step=279000, grad_norm=4.3692498207092285, loss=0.6754273176193237
I0301 08:41:03.009366 140525256554240 logging_writer.py:48] [279100] global_step=279100, grad_norm=4.644046306610107, loss=0.6162102818489075
I0301 08:41:36.806711 140523947947776 logging_writer.py:48] [279200] global_step=279200, grad_norm=4.259757995605469, loss=0.630306601524353
I0301 08:42:10.748688 140525256554240 logging_writer.py:48] [279300] global_step=279300, grad_norm=4.925503730773926, loss=0.6494928598403931
I0301 08:42:26.105558 140688601454400 spec.py:321] Evaluating on the training split.
I0301 08:42:32.109018 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 08:42:40.708080 140688601454400 spec.py:349] Evaluating on the test split.
I0301 08:42:42.969640 140688601454400 submission_runner.py:411] Time since start: 97758.04s, 	Step: 279347, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14300675690174103, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.043919563293457, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8228868246078491, 'test/num_examples': 10000, 'score': 94409.34986424446, 'total_duration': 97758.04048991203, 'accumulated_submission_time': 94409.34986424446, 'accumulated_eval_time': 3328.050077676773, 'accumulated_logging_time': 11.06198787689209}
I0301 08:42:43.033717 140522605770496 logging_writer.py:48] [279347] accumulated_eval_time=3328.050078, accumulated_logging_time=11.061988, accumulated_submission_time=94409.349864, global_step=279347, preemption_count=0, score=94409.349864, test/accuracy=0.630800, test/loss=1.822887, test/num_examples=10000, total_duration=97758.040490, train/accuracy=0.961515, train/loss=0.143007, validation/accuracy=0.755100, validation/loss=1.043920, validation/num_examples=50000
I0301 08:43:01.277505 140523092305664 logging_writer.py:48] [279400] global_step=279400, grad_norm=4.443064212799072, loss=0.6259595155715942
I0301 08:43:35.027167 140522605770496 logging_writer.py:48] [279500] global_step=279500, grad_norm=4.889787673950195, loss=0.60992431640625
I0301 08:44:08.762997 140523092305664 logging_writer.py:48] [279600] global_step=279600, grad_norm=4.591992378234863, loss=0.5984800457954407
I0301 08:44:42.545373 140522605770496 logging_writer.py:48] [279700] global_step=279700, grad_norm=4.905026912689209, loss=0.5919877290725708
I0301 08:45:16.316171 140523092305664 logging_writer.py:48] [279800] global_step=279800, grad_norm=4.927435874938965, loss=0.6523677110671997
I0301 08:45:50.079228 140522605770496 logging_writer.py:48] [279900] global_step=279900, grad_norm=4.7908148765563965, loss=0.6107853651046753
I0301 08:46:23.868923 140523092305664 logging_writer.py:48] [280000] global_step=280000, grad_norm=4.287531852722168, loss=0.589801549911499
I0301 08:46:57.641158 140522605770496 logging_writer.py:48] [280100] global_step=280100, grad_norm=4.751326560974121, loss=0.6069403886795044
I0301 08:47:31.397831 140523092305664 logging_writer.py:48] [280200] global_step=280200, grad_norm=4.452675819396973, loss=0.6750021576881409
I0301 08:48:05.163316 140522605770496 logging_writer.py:48] [280300] global_step=280300, grad_norm=5.110385894775391, loss=0.6869500279426575
I0301 08:48:39.077113 140523092305664 logging_writer.py:48] [280400] global_step=280400, grad_norm=4.458989143371582, loss=0.6007451415061951
I0301 08:49:12.859131 140522605770496 logging_writer.py:48] [280500] global_step=280500, grad_norm=4.560307502746582, loss=0.6737678647041321
I0301 08:49:46.616463 140523092305664 logging_writer.py:48] [280600] global_step=280600, grad_norm=4.770155906677246, loss=0.7290498614311218
I0301 08:50:20.394543 140522605770496 logging_writer.py:48] [280700] global_step=280700, grad_norm=4.586217880249023, loss=0.591801106929779
I0301 08:50:54.170957 140523092305664 logging_writer.py:48] [280800] global_step=280800, grad_norm=4.536605358123779, loss=0.6329156756401062
I0301 08:51:13.240094 140688601454400 spec.py:321] Evaluating on the training split.
I0301 08:51:19.207726 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 08:51:28.020495 140688601454400 spec.py:349] Evaluating on the test split.
I0301 08:51:30.362580 140688601454400 submission_runner.py:411] Time since start: 98285.43s, 	Step: 280858, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14566275477409363, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0434741973876953, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.820765733718872, 'test/num_examples': 10000, 'score': 94919.4937672615, 'total_duration': 98285.43342876434, 'accumulated_submission_time': 94919.4937672615, 'accumulated_eval_time': 3345.172520637512, 'accumulated_logging_time': 11.137099504470825}
I0301 08:51:30.426424 140525256554240 logging_writer.py:48] [280858] accumulated_eval_time=3345.172521, accumulated_logging_time=11.137100, accumulated_submission_time=94919.493767, global_step=280858, preemption_count=0, score=94919.493767, test/accuracy=0.631100, test/loss=1.820766, test/num_examples=10000, total_duration=98285.433429, train/accuracy=0.960758, train/loss=0.145663, validation/accuracy=0.755260, validation/loss=1.043474, validation/num_examples=50000
I0301 08:51:44.921069 140525273339648 logging_writer.py:48] [280900] global_step=280900, grad_norm=4.654645919799805, loss=0.6365430355072021
I0301 08:52:18.666567 140525256554240 logging_writer.py:48] [281000] global_step=281000, grad_norm=4.641030311584473, loss=0.6679387092590332
I0301 08:52:52.405819 140525273339648 logging_writer.py:48] [281100] global_step=281100, grad_norm=4.5119829177856445, loss=0.6371192336082458
I0301 08:53:26.193086 140525256554240 logging_writer.py:48] [281200] global_step=281200, grad_norm=4.07365083694458, loss=0.5559671521186829
I0301 08:53:59.979023 140525273339648 logging_writer.py:48] [281300] global_step=281300, grad_norm=4.768276691436768, loss=0.6549618244171143
I0301 08:54:33.837211 140525256554240 logging_writer.py:48] [281400] global_step=281400, grad_norm=4.4707865715026855, loss=0.6411630511283875
I0301 08:55:07.637971 140525273339648 logging_writer.py:48] [281500] global_step=281500, grad_norm=4.513978958129883, loss=0.648059070110321
I0301 08:55:41.417745 140525256554240 logging_writer.py:48] [281600] global_step=281600, grad_norm=4.3919477462768555, loss=0.6983275413513184
I0301 08:56:15.199896 140525273339648 logging_writer.py:48] [281700] global_step=281700, grad_norm=4.514186859130859, loss=0.6107509732246399
I0301 08:56:48.969079 140525256554240 logging_writer.py:48] [281800] global_step=281800, grad_norm=4.510005474090576, loss=0.6369500756263733
I0301 08:57:22.729836 140525273339648 logging_writer.py:48] [281900] global_step=281900, grad_norm=4.279958724975586, loss=0.5935174822807312
I0301 08:57:56.497836 140525256554240 logging_writer.py:48] [282000] global_step=282000, grad_norm=5.014886379241943, loss=0.6347764730453491
I0301 08:58:30.259928 140525273339648 logging_writer.py:48] [282100] global_step=282100, grad_norm=5.056243896484375, loss=0.6367855072021484
I0301 08:59:04.040913 140525256554240 logging_writer.py:48] [282200] global_step=282200, grad_norm=5.190240383148193, loss=0.6014654040336609
I0301 08:59:37.817603 140525273339648 logging_writer.py:48] [282300] global_step=282300, grad_norm=4.670556545257568, loss=0.6688995957374573
I0301 09:00:00.620130 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:00:06.741735 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:00:15.574754 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:00:17.950935 140688601454400 submission_runner.py:411] Time since start: 98813.02s, 	Step: 282369, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14792662858963013, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0445046424865723, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.821438193321228, 'test/num_examples': 10000, 'score': 95429.6256942749, 'total_duration': 98813.02177882195, 'accumulated_submission_time': 95429.6256942749, 'accumulated_eval_time': 3362.503289937973, 'accumulated_logging_time': 11.210871934890747}
I0301 09:00:18.014392 140522597377792 logging_writer.py:48] [282369] accumulated_eval_time=3362.503290, accumulated_logging_time=11.210872, accumulated_submission_time=95429.625694, global_step=282369, preemption_count=0, score=95429.625694, test/accuracy=0.630900, test/loss=1.821438, test/num_examples=10000, total_duration=98813.021779, train/accuracy=0.960120, train/loss=0.147927, validation/accuracy=0.755460, validation/loss=1.044505, validation/num_examples=50000
I0301 09:00:28.841136 140522605770496 logging_writer.py:48] [282400] global_step=282400, grad_norm=4.260280609130859, loss=0.5800598859786987
I0301 09:01:02.642194 140522597377792 logging_writer.py:48] [282500] global_step=282500, grad_norm=4.733928680419922, loss=0.5916432738304138
I0301 09:01:36.393512 140522605770496 logging_writer.py:48] [282600] global_step=282600, grad_norm=3.9554290771484375, loss=0.5590566992759705
I0301 09:02:10.179026 140522597377792 logging_writer.py:48] [282700] global_step=282700, grad_norm=4.636483669281006, loss=0.5985908508300781
I0301 09:02:43.955028 140522605770496 logging_writer.py:48] [282800] global_step=282800, grad_norm=4.3078107833862305, loss=0.6036378145217896
I0301 09:03:17.770949 140522597377792 logging_writer.py:48] [282900] global_step=282900, grad_norm=4.659473896026611, loss=0.5948951244354248
I0301 09:03:51.579089 140522605770496 logging_writer.py:48] [283000] global_step=283000, grad_norm=4.541368007659912, loss=0.6384283304214478
I0301 09:04:25.376793 140522597377792 logging_writer.py:48] [283100] global_step=283100, grad_norm=4.064434051513672, loss=0.6256940364837646
I0301 09:04:59.117955 140522605770496 logging_writer.py:48] [283200] global_step=283200, grad_norm=4.322049617767334, loss=0.6285220384597778
I0301 09:05:32.881683 140522597377792 logging_writer.py:48] [283300] global_step=283300, grad_norm=4.6528120040893555, loss=0.6634065508842468
I0301 09:06:06.624456 140522605770496 logging_writer.py:48] [283400] global_step=283400, grad_norm=4.436540603637695, loss=0.6198740005493164
I0301 09:06:40.348088 140522597377792 logging_writer.py:48] [283500] global_step=283500, grad_norm=4.086839199066162, loss=0.6007094979286194
I0301 09:07:14.161042 140522605770496 logging_writer.py:48] [283600] global_step=283600, grad_norm=4.3091325759887695, loss=0.6038901209831238
I0301 09:07:47.982927 140522597377792 logging_writer.py:48] [283700] global_step=283700, grad_norm=4.007613658905029, loss=0.5255500078201294
I0301 09:08:21.771919 140522605770496 logging_writer.py:48] [283800] global_step=283800, grad_norm=4.850147724151611, loss=0.71422278881073
I0301 09:08:47.978752 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:08:54.058393 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:09:02.863209 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:09:05.136228 140688601454400 submission_runner.py:411] Time since start: 99340.21s, 	Step: 283879, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.14735518395900726, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.043495535850525, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.821829915046692, 'test/num_examples': 10000, 'score': 95939.5230576992, 'total_duration': 99340.20707345009, 'accumulated_submission_time': 95939.5230576992, 'accumulated_eval_time': 3379.660735845566, 'accumulated_logging_time': 11.28923773765564}
I0301 09:09:05.203087 140525256554240 logging_writer.py:48] [283879] accumulated_eval_time=3379.660736, accumulated_logging_time=11.289238, accumulated_submission_time=95939.523058, global_step=283879, preemption_count=0, score=95939.523058, test/accuracy=0.630700, test/loss=1.821830, test/num_examples=10000, total_duration=99340.207073, train/accuracy=0.961177, train/loss=0.147355, validation/accuracy=0.755060, validation/loss=1.043496, validation/num_examples=50000
I0301 09:09:12.635864 140525264946944 logging_writer.py:48] [283900] global_step=283900, grad_norm=4.368032932281494, loss=0.6424316763877869
I0301 09:09:46.343815 140525256554240 logging_writer.py:48] [284000] global_step=284000, grad_norm=4.460931777954102, loss=0.630582332611084
I0301 09:10:20.093156 140525264946944 logging_writer.py:48] [284100] global_step=284100, grad_norm=4.335973739624023, loss=0.5482253432273865
I0301 09:10:53.887585 140525256554240 logging_writer.py:48] [284200] global_step=284200, grad_norm=4.702352523803711, loss=0.6464375853538513
I0301 09:11:27.680428 140525264946944 logging_writer.py:48] [284300] global_step=284300, grad_norm=4.727248191833496, loss=0.6309176683425903
I0301 09:12:01.448922 140525256554240 logging_writer.py:48] [284400] global_step=284400, grad_norm=4.810105800628662, loss=0.6260092258453369
I0301 09:12:35.213812 140525264946944 logging_writer.py:48] [284500] global_step=284500, grad_norm=4.479583740234375, loss=0.6319558620452881
I0301 09:13:09.079295 140525256554240 logging_writer.py:48] [284600] global_step=284600, grad_norm=4.607215404510498, loss=0.6821798086166382
I0301 09:13:42.852403 140525264946944 logging_writer.py:48] [284700] global_step=284700, grad_norm=4.751931667327881, loss=0.6491096019744873
I0301 09:14:16.625556 140525256554240 logging_writer.py:48] [284800] global_step=284800, grad_norm=4.227054595947266, loss=0.5888571739196777
I0301 09:14:50.417292 140525264946944 logging_writer.py:48] [284900] global_step=284900, grad_norm=4.403738975524902, loss=0.616307258605957
I0301 09:15:24.204148 140525256554240 logging_writer.py:48] [285000] global_step=285000, grad_norm=4.86570930480957, loss=0.6828403472900391
I0301 09:15:57.996544 140525264946944 logging_writer.py:48] [285100] global_step=285100, grad_norm=4.771718978881836, loss=0.6517506837844849
I0301 09:16:31.777050 140525256554240 logging_writer.py:48] [285200] global_step=285200, grad_norm=4.174665451049805, loss=0.6412768959999084
I0301 09:17:05.587689 140525264946944 logging_writer.py:48] [285300] global_step=285300, grad_norm=4.716710090637207, loss=0.644222617149353
I0301 09:17:35.449035 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:17:41.553524 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:17:50.205839 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:17:52.478448 140688601454400 submission_runner.py:411] Time since start: 99867.55s, 	Step: 285390, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.1493917554616928, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.043468713760376, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.821364164352417, 'test/num_examples': 10000, 'score': 96449.70619988441, 'total_duration': 99867.54928565025, 'accumulated_submission_time': 96449.70619988441, 'accumulated_eval_time': 3396.690092563629, 'accumulated_logging_time': 11.36655569076538}
I0301 09:17:52.551792 140522597377792 logging_writer.py:48] [285390] accumulated_eval_time=3396.690093, accumulated_logging_time=11.366556, accumulated_submission_time=96449.706200, global_step=285390, preemption_count=0, score=96449.706200, test/accuracy=0.631300, test/loss=1.821364, test/num_examples=10000, total_duration=99867.549286, train/accuracy=0.960320, train/loss=0.149392, validation/accuracy=0.754980, validation/loss=1.043469, validation/num_examples=50000
I0301 09:17:56.271707 140522605770496 logging_writer.py:48] [285400] global_step=285400, grad_norm=4.701206207275391, loss=0.6057947278022766
I0301 09:18:30.005991 140522597377792 logging_writer.py:48] [285500] global_step=285500, grad_norm=4.695777416229248, loss=0.6050121784210205
I0301 09:19:03.788136 140522605770496 logging_writer.py:48] [285600] global_step=285600, grad_norm=4.772239685058594, loss=0.6589663028717041
I0301 09:19:37.636660 140522597377792 logging_writer.py:48] [285700] global_step=285700, grad_norm=4.716531753540039, loss=0.6184208393096924
I0301 09:20:11.462221 140522605770496 logging_writer.py:48] [285800] global_step=285800, grad_norm=4.491016387939453, loss=0.6779776215553284
I0301 09:20:45.260373 140522597377792 logging_writer.py:48] [285900] global_step=285900, grad_norm=4.22988748550415, loss=0.6601520776748657
I0301 09:21:19.044914 140522605770496 logging_writer.py:48] [286000] global_step=286000, grad_norm=4.316460132598877, loss=0.5933293104171753
I0301 09:21:52.848271 140522597377792 logging_writer.py:48] [286100] global_step=286100, grad_norm=4.542017459869385, loss=0.6287379264831543
I0301 09:22:26.625460 140522605770496 logging_writer.py:48] [286200] global_step=286200, grad_norm=4.979269027709961, loss=0.7214103937149048
I0301 09:23:00.426827 140522597377792 logging_writer.py:48] [286300] global_step=286300, grad_norm=4.3007917404174805, loss=0.6448420286178589
I0301 09:23:34.219489 140522605770496 logging_writer.py:48] [286400] global_step=286400, grad_norm=4.94648551940918, loss=0.6564706563949585
I0301 09:24:07.997667 140522597377792 logging_writer.py:48] [286500] global_step=286500, grad_norm=4.223677635192871, loss=0.5754924416542053
I0301 09:24:41.791425 140522605770496 logging_writer.py:48] [286600] global_step=286600, grad_norm=4.273139476776123, loss=0.6157264709472656
I0301 09:25:15.637741 140522597377792 logging_writer.py:48] [286700] global_step=286700, grad_norm=4.2306342124938965, loss=0.555158257484436
I0301 09:25:49.430122 140522605770496 logging_writer.py:48] [286800] global_step=286800, grad_norm=4.750001907348633, loss=0.6027854681015015
I0301 09:26:22.707417 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:26:28.758142 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:26:37.455063 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:26:39.720785 140688601454400 submission_runner.py:411] Time since start: 100394.79s, 	Step: 286900, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14755737781524658, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0439929962158203, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.820935606956482, 'test/num_examples': 10000, 'score': 96959.79819345474, 'total_duration': 100394.79163074493, 'accumulated_submission_time': 96959.79819345474, 'accumulated_eval_time': 3413.7034113407135, 'accumulated_logging_time': 11.450378656387329}
I0301 09:26:39.785867 140525256554240 logging_writer.py:48] [286900] accumulated_eval_time=3413.703411, accumulated_logging_time=11.450379, accumulated_submission_time=96959.798193, global_step=286900, preemption_count=0, score=96959.798193, test/accuracy=0.631200, test/loss=1.820936, test/num_examples=10000, total_duration=100394.791631, train/accuracy=0.960599, train/loss=0.147557, validation/accuracy=0.755220, validation/loss=1.043993, validation/num_examples=50000
I0301 09:26:40.134586 140525264946944 logging_writer.py:48] [286900] global_step=286900, grad_norm=4.719686508178711, loss=0.6290437579154968
I0301 09:27:13.849098 140525256554240 logging_writer.py:48] [287000] global_step=287000, grad_norm=4.627181529998779, loss=0.6308832764625549
I0301 09:27:47.597244 140525264946944 logging_writer.py:48] [287100] global_step=287100, grad_norm=4.5168914794921875, loss=0.5939974188804626
I0301 09:28:21.358655 140525256554240 logging_writer.py:48] [287200] global_step=287200, grad_norm=4.274171829223633, loss=0.6350775361061096
I0301 09:28:55.145510 140525264946944 logging_writer.py:48] [287300] global_step=287300, grad_norm=4.32393741607666, loss=0.5462396144866943
I0301 09:29:28.931137 140525256554240 logging_writer.py:48] [287400] global_step=287400, grad_norm=4.936052322387695, loss=0.681477427482605
I0301 09:30:02.760245 140525264946944 logging_writer.py:48] [287500] global_step=287500, grad_norm=3.9951305389404297, loss=0.5750645995140076
I0301 09:30:36.544372 140525256554240 logging_writer.py:48] [287600] global_step=287600, grad_norm=4.485381603240967, loss=0.5630108714103699
I0301 09:31:10.340119 140525264946944 logging_writer.py:48] [287700] global_step=287700, grad_norm=4.487078666687012, loss=0.6187852621078491
I0301 09:31:44.175655 140525256554240 logging_writer.py:48] [287800] global_step=287800, grad_norm=4.3830037117004395, loss=0.6676109433174133
I0301 09:32:17.969103 140525264946944 logging_writer.py:48] [287900] global_step=287900, grad_norm=4.8720703125, loss=0.6238864660263062
I0301 09:32:51.745361 140525256554240 logging_writer.py:48] [288000] global_step=288000, grad_norm=4.702600955963135, loss=0.644871711730957
I0301 09:33:25.555481 140525264946944 logging_writer.py:48] [288100] global_step=288100, grad_norm=4.850006580352783, loss=0.5987772941589355
I0301 09:33:59.341757 140525256554240 logging_writer.py:48] [288200] global_step=288200, grad_norm=4.726461410522461, loss=0.6738346219062805
I0301 09:34:33.087726 140525264946944 logging_writer.py:48] [288300] global_step=288300, grad_norm=5.105077743530273, loss=0.6394872069358826
I0301 09:35:06.848072 140525256554240 logging_writer.py:48] [288400] global_step=288400, grad_norm=4.770203113555908, loss=0.6530274152755737
I0301 09:35:10.027505 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:35:16.112004 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:35:24.990486 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:35:27.276535 140688601454400 submission_runner.py:411] Time since start: 100922.35s, 	Step: 288411, 	{'train/accuracy': 0.9584462642669678, 'train/loss': 0.1529013216495514, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.0433685779571533, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8218662738800049, 'test/num_examples': 10000, 'score': 97469.97769403458, 'total_duration': 100922.34735965729, 'accumulated_submission_time': 97469.97769403458, 'accumulated_eval_time': 3430.952383518219, 'accumulated_logging_time': 11.525851726531982}
I0301 09:35:27.348809 140522597377792 logging_writer.py:48] [288411] accumulated_eval_time=3430.952384, accumulated_logging_time=11.525852, accumulated_submission_time=97469.977694, global_step=288411, preemption_count=0, score=97469.977694, test/accuracy=0.630800, test/loss=1.821866, test/num_examples=10000, total_duration=100922.347360, train/accuracy=0.958446, train/loss=0.152901, validation/accuracy=0.755200, validation/loss=1.043369, validation/num_examples=50000
I0301 09:35:57.704665 140522605770496 logging_writer.py:48] [288500] global_step=288500, grad_norm=5.21427059173584, loss=0.7156652212142944
I0301 09:36:31.454129 140522597377792 logging_writer.py:48] [288600] global_step=288600, grad_norm=4.474981307983398, loss=0.5860458612442017
I0301 09:37:05.180002 140522605770496 logging_writer.py:48] [288700] global_step=288700, grad_norm=4.638910293579102, loss=0.5826558470726013
I0301 09:37:39.044373 140522597377792 logging_writer.py:48] [288800] global_step=288800, grad_norm=4.70749044418335, loss=0.7435927987098694
I0301 09:38:12.793379 140522605770496 logging_writer.py:48] [288900] global_step=288900, grad_norm=4.388251304626465, loss=0.5923761129379272
I0301 09:38:46.540710 140522597377792 logging_writer.py:48] [289000] global_step=289000, grad_norm=4.4270734786987305, loss=0.6167983412742615
I0301 09:39:20.342028 140522605770496 logging_writer.py:48] [289100] global_step=289100, grad_norm=4.876461029052734, loss=0.6594465374946594
I0301 09:39:54.108399 140522597377792 logging_writer.py:48] [289200] global_step=289200, grad_norm=4.29523229598999, loss=0.6119800806045532
I0301 09:40:27.868678 140522605770496 logging_writer.py:48] [289300] global_step=289300, grad_norm=4.729872703552246, loss=0.6616417765617371
I0301 09:41:01.618284 140522597377792 logging_writer.py:48] [289400] global_step=289400, grad_norm=4.41046667098999, loss=0.6396941542625427
I0301 09:41:35.410103 140522605770496 logging_writer.py:48] [289500] global_step=289500, grad_norm=4.3996710777282715, loss=0.6521385312080383
I0301 09:42:09.192957 140522597377792 logging_writer.py:48] [289600] global_step=289600, grad_norm=4.475825786590576, loss=0.6940884590148926
I0301 09:42:42.987772 140522605770496 logging_writer.py:48] [289700] global_step=289700, grad_norm=4.365187644958496, loss=0.6338032484054565
I0301 09:43:16.754723 140522597377792 logging_writer.py:48] [289800] global_step=289800, grad_norm=4.636802673339844, loss=0.6624202132225037
I0301 09:43:50.715729 140522605770496 logging_writer.py:48] [289900] global_step=289900, grad_norm=4.334957599639893, loss=0.6849949359893799
I0301 09:43:57.608407 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:44:03.773857 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:44:12.378956 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:44:14.694038 140688601454400 submission_runner.py:411] Time since start: 101449.76s, 	Step: 289922, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14966896176338196, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0449682474136353, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8234632015228271, 'test/num_examples': 10000, 'score': 97980.17452836037, 'total_duration': 101449.76487326622, 'accumulated_submission_time': 97980.17452836037, 'accumulated_eval_time': 3448.0379524230957, 'accumulated_logging_time': 11.608575582504272}
I0301 09:44:14.759221 140522597377792 logging_writer.py:48] [289922] accumulated_eval_time=3448.037952, accumulated_logging_time=11.608576, accumulated_submission_time=97980.174528, global_step=289922, preemption_count=0, score=97980.174528, test/accuracy=0.631200, test/loss=1.823463, test/num_examples=10000, total_duration=101449.764873, train/accuracy=0.960061, train/loss=0.149669, validation/accuracy=0.755100, validation/loss=1.044968, validation/num_examples=50000
I0301 09:44:41.346271 140525256554240 logging_writer.py:48] [290000] global_step=290000, grad_norm=4.7634124755859375, loss=0.6237414479255676
I0301 09:45:15.096925 140522597377792 logging_writer.py:48] [290100] global_step=290100, grad_norm=4.69285249710083, loss=0.692241370677948
I0301 09:45:48.798425 140525256554240 logging_writer.py:48] [290200] global_step=290200, grad_norm=4.501152992248535, loss=0.6175961494445801
I0301 09:46:22.541309 140522597377792 logging_writer.py:48] [290300] global_step=290300, grad_norm=4.5861897468566895, loss=0.6696894764900208
I0301 09:46:56.331158 140525256554240 logging_writer.py:48] [290400] global_step=290400, grad_norm=4.481532096862793, loss=0.6013798713684082
I0301 09:47:30.093524 140522597377792 logging_writer.py:48] [290500] global_step=290500, grad_norm=4.431891918182373, loss=0.566138744354248
I0301 09:48:03.894593 140525256554240 logging_writer.py:48] [290600] global_step=290600, grad_norm=4.284155368804932, loss=0.6103016138076782
I0301 09:48:37.666517 140522597377792 logging_writer.py:48] [290700] global_step=290700, grad_norm=4.77919340133667, loss=0.6848767399787903
I0301 09:49:11.400388 140525256554240 logging_writer.py:48] [290800] global_step=290800, grad_norm=4.5901899337768555, loss=0.6009671688079834
I0301 09:49:45.181263 140522597377792 logging_writer.py:48] [290900] global_step=290900, grad_norm=4.581470489501953, loss=0.6577191352844238
I0301 09:50:19.092424 140525256554240 logging_writer.py:48] [291000] global_step=291000, grad_norm=4.899832248687744, loss=0.7244281768798828
I0301 09:50:52.851073 140522597377792 logging_writer.py:48] [291100] global_step=291100, grad_norm=4.433133125305176, loss=0.6587300896644592
I0301 09:51:26.599883 140525256554240 logging_writer.py:48] [291200] global_step=291200, grad_norm=4.511775970458984, loss=0.6182889938354492
I0301 09:52:00.373683 140522597377792 logging_writer.py:48] [291300] global_step=291300, grad_norm=4.158166885375977, loss=0.5475725531578064
I0301 09:52:34.156152 140525256554240 logging_writer.py:48] [291400] global_step=291400, grad_norm=4.358778953552246, loss=0.6183502078056335
I0301 09:52:44.766129 140688601454400 spec.py:321] Evaluating on the training split.
I0301 09:52:50.734851 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 09:52:59.656392 140688601454400 spec.py:349] Evaluating on the test split.
I0301 09:53:01.906167 140688601454400 submission_runner.py:411] Time since start: 101976.98s, 	Step: 291433, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.14643806219100952, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.044140338897705, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8225699663162231, 'test/num_examples': 10000, 'score': 98490.11977744102, 'total_duration': 101976.97698068619, 'accumulated_submission_time': 98490.11977744102, 'accumulated_eval_time': 3465.1779096126556, 'accumulated_logging_time': 11.683691501617432}
I0301 09:53:01.975302 140523092305664 logging_writer.py:48] [291433] accumulated_eval_time=3465.177910, accumulated_logging_time=11.683692, accumulated_submission_time=98490.119777, global_step=291433, preemption_count=0, score=98490.119777, test/accuracy=0.632000, test/loss=1.822570, test/num_examples=10000, total_duration=101976.976981, train/accuracy=0.960300, train/loss=0.146438, validation/accuracy=0.755320, validation/loss=1.044140, validation/num_examples=50000
I0301 09:53:24.908106 140523947947776 logging_writer.py:48] [291500] global_step=291500, grad_norm=4.625394821166992, loss=0.6633848547935486
I0301 09:53:58.650371 140523092305664 logging_writer.py:48] [291600] global_step=291600, grad_norm=4.798363208770752, loss=0.6482057571411133
I0301 09:54:32.402199 140523947947776 logging_writer.py:48] [291700] global_step=291700, grad_norm=4.452786445617676, loss=0.6234442591667175
I0301 09:55:06.190781 140523092305664 logging_writer.py:48] [291800] global_step=291800, grad_norm=4.1861891746521, loss=0.5704547166824341
I0301 09:55:39.987216 140523947947776 logging_writer.py:48] [291900] global_step=291900, grad_norm=4.686159133911133, loss=0.6031386852264404
I0301 09:56:13.868830 140523092305664 logging_writer.py:48] [292000] global_step=292000, grad_norm=4.206815719604492, loss=0.5739493370056152
I0301 09:56:47.646077 140523947947776 logging_writer.py:48] [292100] global_step=292100, grad_norm=4.610393047332764, loss=0.6239813566207886
I0301 09:57:21.428992 140523092305664 logging_writer.py:48] [292200] global_step=292200, grad_norm=4.054164409637451, loss=0.5625203251838684
I0301 09:57:55.215990 140523947947776 logging_writer.py:48] [292300] global_step=292300, grad_norm=4.189757823944092, loss=0.6115598678588867
I0301 09:58:29.015466 140523092305664 logging_writer.py:48] [292400] global_step=292400, grad_norm=4.71285343170166, loss=0.6229851245880127
I0301 09:59:02.820123 140523947947776 logging_writer.py:48] [292500] global_step=292500, grad_norm=4.84116792678833, loss=0.6772698163986206
I0301 09:59:36.623260 140523092305664 logging_writer.py:48] [292600] global_step=292600, grad_norm=4.439037799835205, loss=0.6825838685035706
I0301 10:00:10.368578 140523947947776 logging_writer.py:48] [292700] global_step=292700, grad_norm=4.714814186096191, loss=0.6454455256462097
I0301 10:00:44.124041 140523092305664 logging_writer.py:48] [292800] global_step=292800, grad_norm=4.780301094055176, loss=0.6816914081573486
I0301 10:01:17.902270 140523947947776 logging_writer.py:48] [292900] global_step=292900, grad_norm=5.061650276184082, loss=0.6494626998901367
I0301 10:01:32.212324 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:01:38.166410 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:01:46.942322 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:01:49.245630 140688601454400 submission_runner.py:411] Time since start: 102504.32s, 	Step: 292944, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.14857828617095947, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0444979667663574, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8216490745544434, 'test/num_examples': 10000, 'score': 99000.29478907585, 'total_duration': 102504.31647467613, 'accumulated_submission_time': 99000.29478907585, 'accumulated_eval_time': 3482.211163520813, 'accumulated_logging_time': 11.76318645477295}
I0301 10:01:49.311698 140522605770496 logging_writer.py:48] [292944] accumulated_eval_time=3482.211164, accumulated_logging_time=11.763186, accumulated_submission_time=99000.294789, global_step=292944, preemption_count=0, score=99000.294789, test/accuracy=0.630800, test/loss=1.821649, test/num_examples=10000, total_duration=102504.316475, train/accuracy=0.960260, train/loss=0.148578, validation/accuracy=0.755600, validation/loss=1.044498, validation/num_examples=50000
I0301 10:02:08.516281 140523092305664 logging_writer.py:48] [293000] global_step=293000, grad_norm=4.300031661987305, loss=0.6040360331535339
I0301 10:02:42.291235 140522605770496 logging_writer.py:48] [293100] global_step=293100, grad_norm=4.560284614562988, loss=0.6213041543960571
I0301 10:03:16.062604 140523092305664 logging_writer.py:48] [293200] global_step=293200, grad_norm=4.503575325012207, loss=0.6764347553253174
I0301 10:03:49.849079 140522605770496 logging_writer.py:48] [293300] global_step=293300, grad_norm=4.328188896179199, loss=0.5818575620651245
I0301 10:04:23.633954 140523092305664 logging_writer.py:48] [293400] global_step=293400, grad_norm=4.514288425445557, loss=0.6159399747848511
I0301 10:04:57.416648 140522605770496 logging_writer.py:48] [293500] global_step=293500, grad_norm=4.4232611656188965, loss=0.5821500420570374
I0301 10:05:31.199668 140523092305664 logging_writer.py:48] [293600] global_step=293600, grad_norm=4.930098056793213, loss=0.6740500926971436
I0301 10:06:04.963711 140522605770496 logging_writer.py:48] [293700] global_step=293700, grad_norm=4.766860485076904, loss=0.561152994632721
I0301 10:06:38.748401 140523092305664 logging_writer.py:48] [293800] global_step=293800, grad_norm=4.344622611999512, loss=0.6139516234397888
I0301 10:07:12.483736 140522605770496 logging_writer.py:48] [293900] global_step=293900, grad_norm=4.153860092163086, loss=0.5802542567253113
I0301 10:07:46.278507 140523092305664 logging_writer.py:48] [294000] global_step=294000, grad_norm=4.6778740882873535, loss=0.6223498582839966
I0301 10:08:20.118419 140522605770496 logging_writer.py:48] [294100] global_step=294100, grad_norm=4.4257121086120605, loss=0.5976897478103638
I0301 10:08:53.922985 140523092305664 logging_writer.py:48] [294200] global_step=294200, grad_norm=4.741549968719482, loss=0.5710539817810059
I0301 10:09:27.728072 140522605770496 logging_writer.py:48] [294300] global_step=294300, grad_norm=4.710551738739014, loss=0.6743986010551453
I0301 10:10:01.514736 140523092305664 logging_writer.py:48] [294400] global_step=294400, grad_norm=4.900335788726807, loss=0.8143765926361084
I0301 10:10:19.560018 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:10:25.548107 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:10:34.407479 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:10:36.725850 140688601454400 submission_runner.py:411] Time since start: 103031.80s, 	Step: 294455, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14858688414096832, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0446248054504395, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.821648359298706, 'test/num_examples': 10000, 'score': 99510.4796898365, 'total_duration': 103031.7966837883, 'accumulated_submission_time': 99510.4796898365, 'accumulated_eval_time': 3499.3769342899323, 'accumulated_logging_time': 11.840510606765747}
I0301 10:10:36.791310 140525264946944 logging_writer.py:48] [294455] accumulated_eval_time=3499.376934, accumulated_logging_time=11.840511, accumulated_submission_time=99510.479690, global_step=294455, preemption_count=0, score=99510.479690, test/accuracy=0.631700, test/loss=1.821648, test/num_examples=10000, total_duration=103031.796684, train/accuracy=0.959801, train/loss=0.148587, validation/accuracy=0.754980, validation/loss=1.044625, validation/num_examples=50000
I0301 10:10:52.298021 140525273339648 logging_writer.py:48] [294500] global_step=294500, grad_norm=4.318536281585693, loss=0.6403203010559082
I0301 10:11:26.030112 140525264946944 logging_writer.py:48] [294600] global_step=294600, grad_norm=4.356689929962158, loss=0.6087688207626343
I0301 10:11:59.794129 140525273339648 logging_writer.py:48] [294700] global_step=294700, grad_norm=4.453860759735107, loss=0.5941870808601379
I0301 10:12:33.572705 140525264946944 logging_writer.py:48] [294800] global_step=294800, grad_norm=4.397071838378906, loss=0.6319202780723572
I0301 10:13:07.357524 140525273339648 logging_writer.py:48] [294900] global_step=294900, grad_norm=4.315347194671631, loss=0.6046406030654907
I0301 10:13:41.184058 140525264946944 logging_writer.py:48] [295000] global_step=295000, grad_norm=4.425780773162842, loss=0.5752304792404175
I0301 10:14:14.963669 140525273339648 logging_writer.py:48] [295100] global_step=295100, grad_norm=4.821524620056152, loss=0.6956694722175598
I0301 10:14:48.913171 140525264946944 logging_writer.py:48] [295200] global_step=295200, grad_norm=4.675867080688477, loss=0.7058844566345215
I0301 10:15:22.693627 140525273339648 logging_writer.py:48] [295300] global_step=295300, grad_norm=4.792685508728027, loss=0.6112734079360962
I0301 10:15:56.468741 140525264946944 logging_writer.py:48] [295400] global_step=295400, grad_norm=4.966712474822998, loss=0.6293290853500366
I0301 10:16:30.234902 140525273339648 logging_writer.py:48] [295500] global_step=295500, grad_norm=4.381374359130859, loss=0.6787328124046326
I0301 10:17:04.043501 140525264946944 logging_writer.py:48] [295600] global_step=295600, grad_norm=4.953260898590088, loss=0.6278506517410278
I0301 10:17:37.830720 140525273339648 logging_writer.py:48] [295700] global_step=295700, grad_norm=4.556535243988037, loss=0.6409995555877686
I0301 10:18:11.640944 140525264946944 logging_writer.py:48] [295800] global_step=295800, grad_norm=4.3289713859558105, loss=0.605148434638977
I0301 10:18:45.418821 140525273339648 logging_writer.py:48] [295900] global_step=295900, grad_norm=4.34456205368042, loss=0.6230478882789612
I0301 10:19:06.846693 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:19:12.915222 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:19:21.729947 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:19:24.008785 140688601454400 submission_runner.py:411] Time since start: 103559.08s, 	Step: 295965, 	{'train/accuracy': 0.9590441584587097, 'train/loss': 0.14959582686424255, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.043899655342102, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.821966528892517, 'test/num_examples': 10000, 'score': 100020.47172164917, 'total_duration': 103559.07963442802, 'accumulated_submission_time': 100020.47172164917, 'accumulated_eval_time': 3516.5390000343323, 'accumulated_logging_time': 11.916550636291504}
I0301 10:19:24.074546 140522588985088 logging_writer.py:48] [295965] accumulated_eval_time=3516.539000, accumulated_logging_time=11.916551, accumulated_submission_time=100020.471722, global_step=295965, preemption_count=0, score=100020.471722, test/accuracy=0.631100, test/loss=1.821967, test/num_examples=10000, total_duration=103559.079634, train/accuracy=0.959044, train/loss=0.149596, validation/accuracy=0.755020, validation/loss=1.043900, validation/num_examples=50000
I0301 10:19:36.214412 140522597377792 logging_writer.py:48] [296000] global_step=296000, grad_norm=4.162479400634766, loss=0.6009482145309448
I0301 10:20:09.961369 140522588985088 logging_writer.py:48] [296100] global_step=296100, grad_norm=4.973903656005859, loss=0.7313244938850403
I0301 10:20:43.720189 140522597377792 logging_writer.py:48] [296200] global_step=296200, grad_norm=4.236629962921143, loss=0.6821954250335693
I0301 10:21:17.610261 140522588985088 logging_writer.py:48] [296300] global_step=296300, grad_norm=4.483362197875977, loss=0.598721981048584
I0301 10:21:51.414283 140522597377792 logging_writer.py:48] [296400] global_step=296400, grad_norm=4.209904193878174, loss=0.6262356638908386
I0301 10:22:25.163708 140522588985088 logging_writer.py:48] [296500] global_step=296500, grad_norm=4.781855583190918, loss=0.5977080464363098
I0301 10:22:58.974417 140522597377792 logging_writer.py:48] [296600] global_step=296600, grad_norm=4.876373767852783, loss=0.7135476469993591
I0301 10:23:32.740587 140522588985088 logging_writer.py:48] [296700] global_step=296700, grad_norm=4.468124866485596, loss=0.6180293560028076
I0301 10:24:06.457144 140522597377792 logging_writer.py:48] [296800] global_step=296800, grad_norm=4.078832626342773, loss=0.5887711048126221
I0301 10:24:40.214323 140522588985088 logging_writer.py:48] [296900] global_step=296900, grad_norm=4.680357456207275, loss=0.7547888159751892
I0301 10:25:14.017608 140522597377792 logging_writer.py:48] [297000] global_step=297000, grad_norm=4.560614109039307, loss=0.6443989276885986
I0301 10:25:47.809506 140522588985088 logging_writer.py:48] [297100] global_step=297100, grad_norm=5.050933837890625, loss=0.677587628364563
I0301 10:26:21.607094 140522597377792 logging_writer.py:48] [297200] global_step=297200, grad_norm=4.261601448059082, loss=0.6148121356964111
I0301 10:26:55.469688 140522588985088 logging_writer.py:48] [297300] global_step=297300, grad_norm=4.272286891937256, loss=0.5863553881645203
I0301 10:27:29.261451 140522597377792 logging_writer.py:48] [297400] global_step=297400, grad_norm=4.195931911468506, loss=0.6193699836730957
I0301 10:27:54.074502 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:28:00.096461 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:28:09.073026 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:28:11.361590 140688601454400 submission_runner.py:411] Time since start: 104086.43s, 	Step: 297475, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14696070551872253, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0441290140151978, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8233592510223389, 'test/num_examples': 10000, 'score': 100530.40967082977, 'total_duration': 104086.43240571022, 'accumulated_submission_time': 100530.40967082977, 'accumulated_eval_time': 3533.8260176181793, 'accumulated_logging_time': 11.992268562316895}
I0301 10:28:11.436872 140525264946944 logging_writer.py:48] [297475] accumulated_eval_time=3533.826018, accumulated_logging_time=11.992269, accumulated_submission_time=100530.409671, global_step=297475, preemption_count=0, score=100530.409671, test/accuracy=0.630000, test/loss=1.823359, test/num_examples=10000, total_duration=104086.432406, train/accuracy=0.961735, train/loss=0.146961, validation/accuracy=0.755180, validation/loss=1.044129, validation/num_examples=50000
I0301 10:28:20.179291 140525273339648 logging_writer.py:48] [297500] global_step=297500, grad_norm=4.417689323425293, loss=0.5760896801948547
I0301 10:28:53.903410 140525264946944 logging_writer.py:48] [297600] global_step=297600, grad_norm=4.711868762969971, loss=0.6487069725990295
I0301 10:29:27.612060 140525273339648 logging_writer.py:48] [297700] global_step=297700, grad_norm=3.977405071258545, loss=0.5577552318572998
I0301 10:30:01.384553 140525264946944 logging_writer.py:48] [297800] global_step=297800, grad_norm=4.453220844268799, loss=0.6228594779968262
I0301 10:30:35.157817 140525273339648 logging_writer.py:48] [297900] global_step=297900, grad_norm=4.591533184051514, loss=0.6998664140701294
I0301 10:31:08.937596 140525264946944 logging_writer.py:48] [298000] global_step=298000, grad_norm=4.229493618011475, loss=0.6014460325241089
I0301 10:31:42.762607 140525273339648 logging_writer.py:48] [298100] global_step=298100, grad_norm=4.6509108543396, loss=0.6886101365089417
I0301 10:32:16.536857 140525264946944 logging_writer.py:48] [298200] global_step=298200, grad_norm=4.993020534515381, loss=0.730622410774231
I0301 10:32:50.359343 140525273339648 logging_writer.py:48] [298300] global_step=298300, grad_norm=4.487212657928467, loss=0.6155092716217041
I0301 10:33:24.307095 140525264946944 logging_writer.py:48] [298400] global_step=298400, grad_norm=5.482564449310303, loss=0.6475149989128113
I0301 10:33:58.132613 140525273339648 logging_writer.py:48] [298500] global_step=298500, grad_norm=4.482761859893799, loss=0.6394433975219727
I0301 10:34:31.888734 140525264946944 logging_writer.py:48] [298600] global_step=298600, grad_norm=4.796337127685547, loss=0.6252778172492981
I0301 10:35:05.617223 140525273339648 logging_writer.py:48] [298700] global_step=298700, grad_norm=4.570523738861084, loss=0.6941840648651123
I0301 10:35:39.399301 140525264946944 logging_writer.py:48] [298800] global_step=298800, grad_norm=4.379185676574707, loss=0.546075701713562
I0301 10:36:13.141352 140525273339648 logging_writer.py:48] [298900] global_step=298900, grad_norm=5.435730457305908, loss=0.6103110313415527
I0301 10:36:41.652199 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:36:47.775058 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:36:56.317992 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:36:59.202937 140688601454400 submission_runner.py:411] Time since start: 104614.27s, 	Step: 298986, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.14973214268684387, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0440407991409302, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8226197957992554, 'test/num_examples': 10000, 'score': 101040.5618698597, 'total_duration': 104614.27379345894, 'accumulated_submission_time': 101040.5618698597, 'accumulated_eval_time': 3551.3767199516296, 'accumulated_logging_time': 12.078696966171265}
I0301 10:36:59.259403 140522597377792 logging_writer.py:48] [298986] accumulated_eval_time=3551.376720, accumulated_logging_time=12.078697, accumulated_submission_time=101040.561870, global_step=298986, preemption_count=0, score=101040.561870, test/accuracy=0.631100, test/loss=1.822620, test/num_examples=10000, total_duration=104614.273793, train/accuracy=0.959702, train/loss=0.149732, validation/accuracy=0.755060, validation/loss=1.044041, validation/num_examples=50000
I0301 10:37:04.322834 140522605770496 logging_writer.py:48] [299000] global_step=299000, grad_norm=4.238697052001953, loss=0.6276698112487793
I0301 10:37:38.027865 140522597377792 logging_writer.py:48] [299100] global_step=299100, grad_norm=4.453744888305664, loss=0.6559198498725891
I0301 10:38:11.777176 140522605770496 logging_writer.py:48] [299200] global_step=299200, grad_norm=4.4568305015563965, loss=0.6517109274864197
I0301 10:38:45.576163 140522597377792 logging_writer.py:48] [299300] global_step=299300, grad_norm=4.244901180267334, loss=0.5892901420593262
I0301 10:39:19.483299 140522605770496 logging_writer.py:48] [299400] global_step=299400, grad_norm=4.729246616363525, loss=0.5759351849555969
I0301 10:39:53.279010 140522597377792 logging_writer.py:48] [299500] global_step=299500, grad_norm=4.258326530456543, loss=0.5843879580497742
I0301 10:40:27.065983 140522605770496 logging_writer.py:48] [299600] global_step=299600, grad_norm=4.656636714935303, loss=0.6439344882965088
I0301 10:41:00.858305 140522597377792 logging_writer.py:48] [299700] global_step=299700, grad_norm=4.812745094299316, loss=0.6717546582221985
I0301 10:41:34.621726 140522605770496 logging_writer.py:48] [299800] global_step=299800, grad_norm=4.804356575012207, loss=0.6388739943504333
I0301 10:42:08.412336 140522597377792 logging_writer.py:48] [299900] global_step=299900, grad_norm=4.296485900878906, loss=0.5762097835540771
I0301 10:42:42.235143 140522605770496 logging_writer.py:48] [300000] global_step=300000, grad_norm=4.91330099105835, loss=0.5979923009872437
I0301 10:43:16.018409 140522597377792 logging_writer.py:48] [300100] global_step=300100, grad_norm=4.48211145401001, loss=0.6369340419769287
I0301 10:43:49.781319 140522605770496 logging_writer.py:48] [300200] global_step=300200, grad_norm=4.093785762786865, loss=0.6097462177276611
I0301 10:44:23.556160 140522597377792 logging_writer.py:48] [300300] global_step=300300, grad_norm=4.598062515258789, loss=0.6526705026626587
I0301 10:44:57.369516 140522605770496 logging_writer.py:48] [300400] global_step=300400, grad_norm=4.63787317276001, loss=0.6006638407707214
I0301 10:45:29.349208 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:45:35.312832 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:45:43.938573 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:45:46.183007 140688601454400 submission_runner.py:411] Time since start: 105141.25s, 	Step: 300496, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.14931277930736542, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0435774326324463, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8219795227050781, 'test/num_examples': 10000, 'score': 101550.59058713913, 'total_duration': 105141.25382041931, 'accumulated_submission_time': 101550.59058713913, 'accumulated_eval_time': 3568.2104346752167, 'accumulated_logging_time': 12.144399642944336}
I0301 10:45:46.257925 140525273339648 logging_writer.py:48] [300496] accumulated_eval_time=3568.210435, accumulated_logging_time=12.144400, accumulated_submission_time=101550.590587, global_step=300496, preemption_count=0, score=101550.590587, test/accuracy=0.631400, test/loss=1.821980, test/num_examples=10000, total_duration=105141.253820, train/accuracy=0.959901, train/loss=0.149313, validation/accuracy=0.755340, validation/loss=1.043577, validation/num_examples=50000
I0301 10:45:47.968278 140525281732352 logging_writer.py:48] [300500] global_step=300500, grad_norm=4.905632019042969, loss=0.6840571761131287
I0301 10:46:21.705599 140525273339648 logging_writer.py:48] [300600] global_step=300600, grad_norm=4.619847774505615, loss=0.6021945476531982
I0301 10:46:55.463505 140525281732352 logging_writer.py:48] [300700] global_step=300700, grad_norm=4.492583751678467, loss=0.639819324016571
I0301 10:47:29.263458 140525273339648 logging_writer.py:48] [300800] global_step=300800, grad_norm=5.042420864105225, loss=0.6542161703109741
I0301 10:48:03.043497 140525281732352 logging_writer.py:48] [300900] global_step=300900, grad_norm=4.3816752433776855, loss=0.6317509412765503
I0301 10:48:36.840357 140525273339648 logging_writer.py:48] [301000] global_step=301000, grad_norm=4.366822242736816, loss=0.596824586391449
I0301 10:49:10.599327 140525281732352 logging_writer.py:48] [301100] global_step=301100, grad_norm=4.405206680297852, loss=0.6566689014434814
I0301 10:49:44.349478 140525273339648 logging_writer.py:48] [301200] global_step=301200, grad_norm=4.732354164123535, loss=0.6959601640701294
I0301 10:50:18.130389 140525281732352 logging_writer.py:48] [301300] global_step=301300, grad_norm=4.216730117797852, loss=0.5976679921150208
I0301 10:50:51.875970 140525273339648 logging_writer.py:48] [301400] global_step=301400, grad_norm=4.487298011779785, loss=0.6643075346946716
I0301 10:51:25.645537 140525281732352 logging_writer.py:48] [301500] global_step=301500, grad_norm=4.450643539428711, loss=0.6572577953338623
I0301 10:51:59.520188 140525273339648 logging_writer.py:48] [301600] global_step=301600, grad_norm=4.259365081787109, loss=0.5760090947151184
I0301 10:52:33.335954 140525281732352 logging_writer.py:48] [301700] global_step=301700, grad_norm=4.678336143493652, loss=0.5964546203613281
I0301 10:53:07.120276 140525273339648 logging_writer.py:48] [301800] global_step=301800, grad_norm=4.603781700134277, loss=0.6475204825401306
I0301 10:53:40.906520 140525281732352 logging_writer.py:48] [301900] global_step=301900, grad_norm=4.215754508972168, loss=0.5922949314117432
I0301 10:54:14.666811 140525273339648 logging_writer.py:48] [302000] global_step=302000, grad_norm=4.535240650177002, loss=0.5554297566413879
I0301 10:54:16.500635 140688601454400 spec.py:321] Evaluating on the training split.
I0301 10:54:22.602222 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 10:54:31.349901 140688601454400 spec.py:349] Evaluating on the test split.
I0301 10:54:33.614538 140688601454400 submission_runner.py:411] Time since start: 105668.69s, 	Step: 302007, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14699672162532806, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.043437123298645, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8211946487426758, 'test/num_examples': 10000, 'score': 102060.7697827816, 'total_duration': 105668.68538475037, 'accumulated_submission_time': 102060.7697827816, 'accumulated_eval_time': 3585.3242876529694, 'accumulated_logging_time': 12.231438398361206}
I0301 10:54:33.682126 140523092305664 logging_writer.py:48] [302007] accumulated_eval_time=3585.324288, accumulated_logging_time=12.231438, accumulated_submission_time=102060.769783, global_step=302007, preemption_count=0, score=102060.769783, test/accuracy=0.631100, test/loss=1.821195, test/num_examples=10000, total_duration=105668.685385, train/accuracy=0.960758, train/loss=0.146997, validation/accuracy=0.755720, validation/loss=1.043437, validation/num_examples=50000
I0301 10:55:05.353533 140525281732352 logging_writer.py:48] [302100] global_step=302100, grad_norm=4.572152137756348, loss=0.6435709595680237
I0301 10:55:39.124148 140523092305664 logging_writer.py:48] [302200] global_step=302200, grad_norm=4.832427501678467, loss=0.661756694316864
I0301 10:56:12.948047 140525281732352 logging_writer.py:48] [302300] global_step=302300, grad_norm=4.533871173858643, loss=0.663981020450592
I0301 10:56:46.741590 140523092305664 logging_writer.py:48] [302400] global_step=302400, grad_norm=4.436306953430176, loss=0.587867259979248
I0301 10:57:20.529221 140525281732352 logging_writer.py:48] [302500] global_step=302500, grad_norm=5.031861782073975, loss=0.6672422885894775
I0301 10:57:54.504998 140523092305664 logging_writer.py:48] [302600] global_step=302600, grad_norm=4.486846923828125, loss=0.6002368927001953
I0301 10:58:28.279105 140525281732352 logging_writer.py:48] [302700] global_step=302700, grad_norm=4.577110767364502, loss=0.6821780800819397
I0301 10:59:02.067102 140523092305664 logging_writer.py:48] [302800] global_step=302800, grad_norm=4.698533058166504, loss=0.6989126205444336
I0301 10:59:35.885961 140525281732352 logging_writer.py:48] [302900] global_step=302900, grad_norm=4.270833969116211, loss=0.5879149436950684
I0301 11:00:09.677261 140523092305664 logging_writer.py:48] [303000] global_step=303000, grad_norm=4.71042013168335, loss=0.6041156053543091
I0301 11:00:43.487015 140525281732352 logging_writer.py:48] [303100] global_step=303100, grad_norm=4.407433032989502, loss=0.6307641267776489
I0301 11:01:17.274078 140523092305664 logging_writer.py:48] [303200] global_step=303200, grad_norm=4.24394416809082, loss=0.5686755180358887
I0301 11:01:51.092051 140525281732352 logging_writer.py:48] [303300] global_step=303300, grad_norm=4.571481704711914, loss=0.597383975982666
I0301 11:02:24.887239 140523092305664 logging_writer.py:48] [303400] global_step=303400, grad_norm=4.369348049163818, loss=0.6007416248321533
I0301 11:02:58.666328 140525281732352 logging_writer.py:48] [303500] global_step=303500, grad_norm=4.59676456451416, loss=0.5778823494911194
I0301 11:03:03.880387 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:03:09.871828 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:03:18.714082 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:03:20.974806 140688601454400 submission_runner.py:411] Time since start: 106196.05s, 	Step: 303517, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14538729190826416, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.044714331626892, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8220094442367554, 'test/num_examples': 10000, 'score': 102570.90319132805, 'total_duration': 106196.04564929008, 'accumulated_submission_time': 102570.90319132805, 'accumulated_eval_time': 3602.4186551570892, 'accumulated_logging_time': 12.31143856048584}
I0301 11:03:21.042191 140522605770496 logging_writer.py:48] [303517] accumulated_eval_time=3602.418655, accumulated_logging_time=12.311439, accumulated_submission_time=102570.903191, global_step=303517, preemption_count=0, score=102570.903191, test/accuracy=0.632100, test/loss=1.822009, test/num_examples=10000, total_duration=106196.045649, train/accuracy=0.961316, train/loss=0.145387, validation/accuracy=0.754960, validation/loss=1.044714, validation/num_examples=50000
I0301 11:03:49.381330 140523092305664 logging_writer.py:48] [303600] global_step=303600, grad_norm=4.393567085266113, loss=0.6582232117652893
I0301 11:04:23.242615 140522605770496 logging_writer.py:48] [303700] global_step=303700, grad_norm=4.380322456359863, loss=0.6198112964630127
I0301 11:04:57.014585 140523092305664 logging_writer.py:48] [303800] global_step=303800, grad_norm=4.5046868324279785, loss=0.6108596920967102
I0301 11:05:30.771576 140522605770496 logging_writer.py:48] [303900] global_step=303900, grad_norm=4.252352714538574, loss=0.5794460773468018
I0301 11:06:04.523409 140523092305664 logging_writer.py:48] [304000] global_step=304000, grad_norm=4.436913013458252, loss=0.553862988948822
I0301 11:06:38.313095 140522605770496 logging_writer.py:48] [304100] global_step=304100, grad_norm=5.297814846038818, loss=0.7489712238311768
I0301 11:07:12.109441 140523092305664 logging_writer.py:48] [304200] global_step=304200, grad_norm=4.820613384246826, loss=0.7189487814903259
I0301 11:07:45.911061 140522605770496 logging_writer.py:48] [304300] global_step=304300, grad_norm=4.513944149017334, loss=0.6203786134719849
I0301 11:08:19.664405 140523092305664 logging_writer.py:48] [304400] global_step=304400, grad_norm=4.671884059906006, loss=0.6486481428146362
I0301 11:08:53.440983 140522605770496 logging_writer.py:48] [304500] global_step=304500, grad_norm=4.5079569816589355, loss=0.6686336994171143
I0301 11:09:27.210406 140523092305664 logging_writer.py:48] [304600] global_step=304600, grad_norm=4.626638412475586, loss=0.7068755030632019
I0301 11:10:00.940722 140522605770496 logging_writer.py:48] [304700] global_step=304700, grad_norm=4.10060977935791, loss=0.6068625450134277
I0301 11:10:34.867125 140523092305664 logging_writer.py:48] [304800] global_step=304800, grad_norm=4.532914161682129, loss=0.6561543941497803
I0301 11:11:08.637814 140522605770496 logging_writer.py:48] [304900] global_step=304900, grad_norm=4.350943565368652, loss=0.6478745937347412
I0301 11:11:42.395523 140523092305664 logging_writer.py:48] [305000] global_step=305000, grad_norm=4.7154436111450195, loss=0.5911070704460144
I0301 11:11:51.311217 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:11:57.316015 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:12:06.033618 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:12:08.286757 140688601454400 submission_runner.py:411] Time since start: 106723.36s, 	Step: 305028, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.1474113166332245, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0436537265777588, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8212631940841675, 'test/num_examples': 10000, 'score': 103081.1084947586, 'total_duration': 106723.35728740692, 'accumulated_submission_time': 103081.1084947586, 'accumulated_eval_time': 3619.393835544586, 'accumulated_logging_time': 12.389586448669434}
I0301 11:12:08.356810 140522597377792 logging_writer.py:48] [305028] accumulated_eval_time=3619.393836, accumulated_logging_time=12.389586, accumulated_submission_time=103081.108495, global_step=305028, preemption_count=0, score=103081.108495, test/accuracy=0.631600, test/loss=1.821263, test/num_examples=10000, total_duration=106723.357287, train/accuracy=0.961177, train/loss=0.147411, validation/accuracy=0.755120, validation/loss=1.043654, validation/num_examples=50000
I0301 11:12:32.979548 140525256554240 logging_writer.py:48] [305100] global_step=305100, grad_norm=4.515954494476318, loss=0.6311032176017761
I0301 11:13:06.696682 140522597377792 logging_writer.py:48] [305200] global_step=305200, grad_norm=4.421678066253662, loss=0.6174766421318054
I0301 11:13:40.464509 140525256554240 logging_writer.py:48] [305300] global_step=305300, grad_norm=4.569581031799316, loss=0.5710523724555969
I0301 11:14:14.272507 140522597377792 logging_writer.py:48] [305400] global_step=305400, grad_norm=4.529764652252197, loss=0.6093772649765015
I0301 11:14:48.059315 140525256554240 logging_writer.py:48] [305500] global_step=305500, grad_norm=4.242719650268555, loss=0.5951769351959229
I0301 11:15:21.883799 140522597377792 logging_writer.py:48] [305600] global_step=305600, grad_norm=4.1723527908325195, loss=0.5841712355613708
I0301 11:15:55.658283 140525256554240 logging_writer.py:48] [305700] global_step=305700, grad_norm=4.580050945281982, loss=0.6564832925796509
I0301 11:16:29.508885 140522597377792 logging_writer.py:48] [305800] global_step=305800, grad_norm=4.469735622406006, loss=0.5693700909614563
I0301 11:17:03.260299 140525256554240 logging_writer.py:48] [305900] global_step=305900, grad_norm=4.726778984069824, loss=0.6029354333877563
I0301 11:17:37.059897 140522597377792 logging_writer.py:48] [306000] global_step=306000, grad_norm=4.379931449890137, loss=0.6106818318367004
I0301 11:18:10.832499 140525256554240 logging_writer.py:48] [306100] global_step=306100, grad_norm=4.298532962799072, loss=0.6038939952850342
I0301 11:18:44.649980 140522597377792 logging_writer.py:48] [306200] global_step=306200, grad_norm=4.60387659072876, loss=0.6272680759429932
I0301 11:19:18.423542 140525256554240 logging_writer.py:48] [306300] global_step=306300, grad_norm=4.502066612243652, loss=0.6063823699951172
I0301 11:19:52.234344 140522597377792 logging_writer.py:48] [306400] global_step=306400, grad_norm=4.630128383636475, loss=0.6464183330535889
I0301 11:20:25.997957 140525256554240 logging_writer.py:48] [306500] global_step=306500, grad_norm=4.966701030731201, loss=0.562090277671814
I0301 11:20:38.299166 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:20:44.961965 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:20:53.757679 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:20:56.035884 140688601454400 submission_runner.py:411] Time since start: 107251.11s, 	Step: 306538, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.1488519310951233, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0441625118255615, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8231303691864014, 'test/num_examples': 10000, 'score': 103590.98895573616, 'total_duration': 107251.10673069954, 'accumulated_submission_time': 103590.98895573616, 'accumulated_eval_time': 3637.1305088996887, 'accumulated_logging_time': 12.470108985900879}
I0301 11:20:56.108110 140522605770496 logging_writer.py:48] [306538] accumulated_eval_time=3637.130509, accumulated_logging_time=12.470109, accumulated_submission_time=103590.988956, global_step=306538, preemption_count=0, score=103590.988956, test/accuracy=0.630600, test/loss=1.823130, test/num_examples=10000, total_duration=107251.106731, train/accuracy=0.959602, train/loss=0.148852, validation/accuracy=0.755280, validation/loss=1.044163, validation/num_examples=50000
I0301 11:21:17.374173 140523092305664 logging_writer.py:48] [306600] global_step=306600, grad_norm=4.503013610839844, loss=0.5706029534339905
I0301 11:21:51.082053 140522605770496 logging_writer.py:48] [306700] global_step=306700, grad_norm=4.5725202560424805, loss=0.6167138814926147
I0301 11:22:24.847658 140523092305664 logging_writer.py:48] [306800] global_step=306800, grad_norm=4.464834213256836, loss=0.6148793697357178
I0301 11:22:58.778291 140522605770496 logging_writer.py:48] [306900] global_step=306900, grad_norm=4.23091459274292, loss=0.5910181403160095
I0301 11:23:32.530268 140523092305664 logging_writer.py:48] [307000] global_step=307000, grad_norm=4.250207424163818, loss=0.582768440246582
I0301 11:24:06.293643 140522605770496 logging_writer.py:48] [307100] global_step=307100, grad_norm=5.016726493835449, loss=0.6686276197433472
I0301 11:24:40.100344 140523092305664 logging_writer.py:48] [307200] global_step=307200, grad_norm=4.674496650695801, loss=0.6520748138427734
I0301 11:25:13.881986 140522605770496 logging_writer.py:48] [307300] global_step=307300, grad_norm=4.569530487060547, loss=0.6697713136672974
I0301 11:25:47.654907 140523092305664 logging_writer.py:48] [307400] global_step=307400, grad_norm=4.546108722686768, loss=0.6467546224594116
I0301 11:26:21.434338 140522605770496 logging_writer.py:48] [307500] global_step=307500, grad_norm=4.383030414581299, loss=0.6018707752227783
I0301 11:26:55.222276 140523092305664 logging_writer.py:48] [307600] global_step=307600, grad_norm=4.434167385101318, loss=0.6833584308624268
I0301 11:27:28.992979 140522605770496 logging_writer.py:48] [307700] global_step=307700, grad_norm=4.084128379821777, loss=0.572147011756897
I0301 11:28:02.767123 140523092305664 logging_writer.py:48] [307800] global_step=307800, grad_norm=4.866759777069092, loss=0.7072278261184692
I0301 11:28:36.638000 140522605770496 logging_writer.py:48] [307900] global_step=307900, grad_norm=4.27260684967041, loss=0.5849615335464478
I0301 11:29:10.412197 140523092305664 logging_writer.py:48] [308000] global_step=308000, grad_norm=4.895974159240723, loss=0.663488507270813
I0301 11:29:26.114829 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:29:32.208237 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:29:40.903379 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:29:43.224916 140688601454400 submission_runner.py:411] Time since start: 107778.30s, 	Step: 308048, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.15103530883789062, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.044703722000122, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.822330355644226, 'test/num_examples': 10000, 'score': 104100.93312764168, 'total_duration': 107778.29575705528, 'accumulated_submission_time': 104100.93312764168, 'accumulated_eval_time': 3654.240547180176, 'accumulated_logging_time': 12.552121877670288}
I0301 11:29:43.293778 140525256554240 logging_writer.py:48] [308048] accumulated_eval_time=3654.240547, accumulated_logging_time=12.552122, accumulated_submission_time=104100.933128, global_step=308048, preemption_count=0, score=104100.933128, test/accuracy=0.630900, test/loss=1.822330, test/num_examples=10000, total_duration=107778.295757, train/accuracy=0.959682, train/loss=0.151035, validation/accuracy=0.755560, validation/loss=1.044704, validation/num_examples=50000
I0301 11:30:01.184800 140525264946944 logging_writer.py:48] [308100] global_step=308100, grad_norm=4.509024143218994, loss=0.617516279220581
I0301 11:30:34.936074 140525256554240 logging_writer.py:48] [308200] global_step=308200, grad_norm=4.340644836425781, loss=0.6410196423530579
I0301 11:31:08.722993 140525264946944 logging_writer.py:48] [308300] global_step=308300, grad_norm=4.79717493057251, loss=0.6476877927780151
I0301 11:31:42.514860 140525256554240 logging_writer.py:48] [308400] global_step=308400, grad_norm=4.923480987548828, loss=0.6159195303916931
I0301 11:32:16.312941 140525264946944 logging_writer.py:48] [308500] global_step=308500, grad_norm=4.637327671051025, loss=0.7361764311790466
I0301 11:32:50.063545 140525256554240 logging_writer.py:48] [308600] global_step=308600, grad_norm=4.622458457946777, loss=0.617130696773529
I0301 11:33:23.804977 140525264946944 logging_writer.py:48] [308700] global_step=308700, grad_norm=4.449474811553955, loss=0.7387301325798035
I0301 11:33:57.604437 140525256554240 logging_writer.py:48] [308800] global_step=308800, grad_norm=4.579859256744385, loss=0.6786199808120728
I0301 11:34:31.404022 140525264946944 logging_writer.py:48] [308900] global_step=308900, grad_norm=4.358719825744629, loss=0.5506040453910828
I0301 11:35:05.269237 140525256554240 logging_writer.py:48] [309000] global_step=309000, grad_norm=4.4144134521484375, loss=0.6659018397331238
I0301 11:35:39.041702 140525264946944 logging_writer.py:48] [309100] global_step=309100, grad_norm=4.359336853027344, loss=0.6019861698150635
I0301 11:36:12.834352 140525256554240 logging_writer.py:48] [309200] global_step=309200, grad_norm=4.994981288909912, loss=0.6499024033546448
I0301 11:36:46.654138 140525264946944 logging_writer.py:48] [309300] global_step=309300, grad_norm=4.777615070343018, loss=0.6247747540473938
I0301 11:37:20.458220 140525256554240 logging_writer.py:48] [309400] global_step=309400, grad_norm=4.3397650718688965, loss=0.5982153415679932
I0301 11:37:54.255917 140525264946944 logging_writer.py:48] [309500] global_step=309500, grad_norm=4.630165100097656, loss=0.5983223915100098
I0301 11:38:13.334155 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:38:19.398258 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:38:28.172132 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:38:30.452672 140688601454400 submission_runner.py:411] Time since start: 108305.52s, 	Step: 309558, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14920125901699066, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0423375368118286, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.820195198059082, 'test/num_examples': 10000, 'score': 104610.91036987305, 'total_duration': 108305.5235171318, 'accumulated_submission_time': 104610.91036987305, 'accumulated_eval_time': 3671.3590240478516, 'accumulated_logging_time': 12.631409883499146}
I0301 11:38:30.524740 140522605770496 logging_writer.py:48] [309558] accumulated_eval_time=3671.359024, accumulated_logging_time=12.631410, accumulated_submission_time=104610.910370, global_step=309558, preemption_count=0, score=104610.910370, test/accuracy=0.630900, test/loss=1.820195, test/num_examples=10000, total_duration=108305.523517, train/accuracy=0.960459, train/loss=0.149201, validation/accuracy=0.755860, validation/loss=1.042338, validation/num_examples=50000
I0301 11:38:45.039271 140523092305664 logging_writer.py:48] [309600] global_step=309600, grad_norm=4.656757354736328, loss=0.6151584386825562
I0301 11:39:18.780756 140522605770496 logging_writer.py:48] [309700] global_step=309700, grad_norm=4.587447643280029, loss=0.6702702641487122
I0301 11:39:52.573020 140523092305664 logging_writer.py:48] [309800] global_step=309800, grad_norm=4.655117988586426, loss=0.6402689218521118
I0301 11:40:26.352313 140522605770496 logging_writer.py:48] [309900] global_step=309900, grad_norm=4.316709518432617, loss=0.5680984258651733
I0301 11:41:00.179318 140523092305664 logging_writer.py:48] [310000] global_step=310000, grad_norm=4.5778937339782715, loss=0.6821174621582031
I0301 11:41:33.942603 140522605770496 logging_writer.py:48] [310100] global_step=310100, grad_norm=4.396378040313721, loss=0.645840048789978
I0301 11:42:07.684873 140523092305664 logging_writer.py:48] [310200] global_step=310200, grad_norm=4.607967376708984, loss=0.6880953311920166
I0301 11:42:41.444739 140522605770496 logging_writer.py:48] [310300] global_step=310300, grad_norm=4.291112899780273, loss=0.5635372996330261
I0301 11:43:15.215080 140523092305664 logging_writer.py:48] [310400] global_step=310400, grad_norm=4.663334846496582, loss=0.6572325229644775
I0301 11:43:48.974445 140522605770496 logging_writer.py:48] [310500] global_step=310500, grad_norm=4.587267875671387, loss=0.6028134822845459
I0301 11:44:22.701197 140523092305664 logging_writer.py:48] [310600] global_step=310600, grad_norm=4.770691871643066, loss=0.6181527376174927
I0301 11:44:56.487265 140522605770496 logging_writer.py:48] [310700] global_step=310700, grad_norm=4.849688529968262, loss=0.6084200143814087
I0301 11:45:30.261774 140523092305664 logging_writer.py:48] [310800] global_step=310800, grad_norm=4.431136608123779, loss=0.6255104541778564
I0301 11:46:04.034918 140522605770496 logging_writer.py:48] [310900] global_step=310900, grad_norm=4.511291980743408, loss=0.5708349943161011
I0301 11:46:37.817558 140523092305664 logging_writer.py:48] [311000] global_step=311000, grad_norm=4.474179744720459, loss=0.6345409154891968
I0301 11:47:00.573566 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:47:06.939254 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:47:15.574952 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:47:17.868761 140688601454400 submission_runner.py:411] Time since start: 108832.94s, 	Step: 311069, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.1465490460395813, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0438048839569092, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8213917016983032, 'test/num_examples': 10000, 'score': 105120.89776802063, 'total_duration': 108832.93958759308, 'accumulated_submission_time': 105120.89776802063, 'accumulated_eval_time': 3688.6541543006897, 'accumulated_logging_time': 12.713399410247803}
I0301 11:47:17.961615 140525264946944 logging_writer.py:48] [311069] accumulated_eval_time=3688.654154, accumulated_logging_time=12.713399, accumulated_submission_time=105120.897768, global_step=311069, preemption_count=0, score=105120.897768, test/accuracy=0.631700, test/loss=1.821392, test/num_examples=10000, total_duration=108832.939588, train/accuracy=0.960180, train/loss=0.146549, validation/accuracy=0.755480, validation/loss=1.043805, validation/num_examples=50000
I0301 11:47:28.760323 140525273339648 logging_writer.py:48] [311100] global_step=311100, grad_norm=4.314200401306152, loss=0.5512340068817139
I0301 11:48:02.540919 140525264946944 logging_writer.py:48] [311200] global_step=311200, grad_norm=4.8086419105529785, loss=0.6596344113349915
I0301 11:48:36.325450 140525273339648 logging_writer.py:48] [311300] global_step=311300, grad_norm=4.260119438171387, loss=0.6116137504577637
I0301 11:49:10.065718 140525264946944 logging_writer.py:48] [311400] global_step=311400, grad_norm=4.536795616149902, loss=0.6402645707130432
I0301 11:49:43.840530 140525273339648 logging_writer.py:48] [311500] global_step=311500, grad_norm=4.844438552856445, loss=0.5994768738746643
I0301 11:50:17.670908 140525264946944 logging_writer.py:48] [311600] global_step=311600, grad_norm=4.966060638427734, loss=0.6506369709968567
I0301 11:50:51.485758 140525273339648 logging_writer.py:48] [311700] global_step=311700, grad_norm=4.3089399337768555, loss=0.6077098846435547
I0301 11:51:25.282206 140525264946944 logging_writer.py:48] [311800] global_step=311800, grad_norm=4.718456745147705, loss=0.6339408159255981
I0301 11:51:59.087782 140525273339648 logging_writer.py:48] [311900] global_step=311900, grad_norm=4.179648399353027, loss=0.5969815254211426
I0301 11:52:32.899331 140525264946944 logging_writer.py:48] [312000] global_step=312000, grad_norm=4.644948482513428, loss=0.6846936941146851
I0301 11:53:06.696185 140525273339648 logging_writer.py:48] [312100] global_step=312100, grad_norm=4.932681560516357, loss=0.7330987453460693
I0301 11:53:40.552851 140525264946944 logging_writer.py:48] [312200] global_step=312200, grad_norm=4.335572242736816, loss=0.5975855588912964
I0301 11:54:14.324207 140525273339648 logging_writer.py:48] [312300] global_step=312300, grad_norm=4.569557189941406, loss=0.5977396368980408
I0301 11:54:48.091304 140525264946944 logging_writer.py:48] [312400] global_step=312400, grad_norm=4.650356769561768, loss=0.6989108324050903
I0301 11:55:21.862750 140525273339648 logging_writer.py:48] [312500] global_step=312500, grad_norm=4.20639705657959, loss=0.584371030330658
I0301 11:55:48.035268 140688601454400 spec.py:321] Evaluating on the training split.
I0301 11:55:54.003732 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 11:56:02.667009 140688601454400 spec.py:349] Evaluating on the test split.
I0301 11:56:04.968992 140688601454400 submission_runner.py:411] Time since start: 109360.04s, 	Step: 312579, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14747101068496704, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.043671727180481, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8213400840759277, 'test/num_examples': 10000, 'score': 105630.90949559212, 'total_duration': 109360.03984093666, 'accumulated_submission_time': 105630.90949559212, 'accumulated_eval_time': 3705.5878355503082, 'accumulated_logging_time': 12.816117763519287}
I0301 11:56:05.041161 140523092305664 logging_writer.py:48] [312579] accumulated_eval_time=3705.587836, accumulated_logging_time=12.816118, accumulated_submission_time=105630.909496, global_step=312579, preemption_count=0, score=105630.909496, test/accuracy=0.631200, test/loss=1.821340, test/num_examples=10000, total_duration=109360.039841, train/accuracy=0.960379, train/loss=0.147471, validation/accuracy=0.754860, validation/loss=1.043672, validation/num_examples=50000
I0301 11:56:12.478417 140523947947776 logging_writer.py:48] [312600] global_step=312600, grad_norm=6.150113582611084, loss=0.6104527711868286
I0301 11:56:46.207824 140523092305664 logging_writer.py:48] [312700] global_step=312700, grad_norm=4.417260646820068, loss=0.6390087008476257
I0301 11:57:19.972822 140523947947776 logging_writer.py:48] [312800] global_step=312800, grad_norm=4.625256061553955, loss=0.6686046719551086
I0301 11:57:53.741208 140523092305664 logging_writer.py:48] [312900] global_step=312900, grad_norm=4.785734176635742, loss=0.6365543603897095
I0301 11:58:27.558762 140523947947776 logging_writer.py:48] [313000] global_step=313000, grad_norm=4.314822196960449, loss=0.6448614001274109
I0301 11:59:01.343804 140523092305664 logging_writer.py:48] [313100] global_step=313100, grad_norm=4.741321563720703, loss=0.6795775890350342
I0301 11:59:35.180357 140523947947776 logging_writer.py:48] [313200] global_step=313200, grad_norm=4.722540378570557, loss=0.612041711807251
I0301 12:00:08.997501 140523092305664 logging_writer.py:48] [313300] global_step=313300, grad_norm=4.748701572418213, loss=0.6351784467697144
I0301 12:00:42.766068 140523947947776 logging_writer.py:48] [313400] global_step=313400, grad_norm=4.373376369476318, loss=0.6383822560310364
I0301 12:01:16.568043 140523092305664 logging_writer.py:48] [313500] global_step=313500, grad_norm=4.569232940673828, loss=0.6662124991416931
I0301 12:01:50.341425 140523947947776 logging_writer.py:48] [313600] global_step=313600, grad_norm=4.214603900909424, loss=0.6129537224769592
I0301 12:02:24.098318 140523092305664 logging_writer.py:48] [313700] global_step=313700, grad_norm=4.667937278747559, loss=0.6430190801620483
I0301 12:02:57.891929 140523947947776 logging_writer.py:48] [313800] global_step=313800, grad_norm=3.847156524658203, loss=0.5194633603096008
I0301 12:03:31.667902 140523092305664 logging_writer.py:48] [313900] global_step=313900, grad_norm=5.013876914978027, loss=0.613608181476593
I0301 12:04:05.448537 140523947947776 logging_writer.py:48] [314000] global_step=314000, grad_norm=4.101527690887451, loss=0.6080611944198608
I0301 12:04:34.982125 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:04:40.991461 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:04:49.598027 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:04:51.888718 140688601454400 submission_runner.py:411] Time since start: 109886.96s, 	Step: 314089, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1459231972694397, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0452815294265747, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8212146759033203, 'test/num_examples': 10000, 'score': 106140.7857196331, 'total_duration': 109886.95943021774, 'accumulated_submission_time': 106140.7857196331, 'accumulated_eval_time': 3722.494250059128, 'accumulated_logging_time': 12.90107798576355}
I0301 12:04:51.959319 140522597377792 logging_writer.py:48] [314089] accumulated_eval_time=3722.494250, accumulated_logging_time=12.901078, accumulated_submission_time=106140.785720, global_step=314089, preemption_count=0, score=106140.785720, test/accuracy=0.630200, test/loss=1.821215, test/num_examples=10000, total_duration=109886.959430, train/accuracy=0.961217, train/loss=0.145923, validation/accuracy=0.754720, validation/loss=1.045282, validation/num_examples=50000
I0301 12:04:56.009221 140522605770496 logging_writer.py:48] [314100] global_step=314100, grad_norm=4.271212577819824, loss=0.6089885234832764
I0301 12:05:29.754839 140522597377792 logging_writer.py:48] [314200] global_step=314200, grad_norm=4.53859281539917, loss=0.6340382695198059
I0301 12:06:03.600629 140522605770496 logging_writer.py:48] [314300] global_step=314300, grad_norm=4.468462944030762, loss=0.6404916048049927
I0301 12:06:37.382359 140522597377792 logging_writer.py:48] [314400] global_step=314400, grad_norm=4.437723636627197, loss=0.6313300132751465
I0301 12:07:11.173201 140522605770496 logging_writer.py:48] [314500] global_step=314500, grad_norm=4.4645676612854, loss=0.6313496232032776
I0301 12:07:44.964739 140522597377792 logging_writer.py:48] [314600] global_step=314600, grad_norm=5.372983932495117, loss=0.7132104635238647
I0301 12:08:18.756608 140522605770496 logging_writer.py:48] [314700] global_step=314700, grad_norm=4.882482528686523, loss=0.6424590945243835
I0301 12:08:52.567336 140522597377792 logging_writer.py:48] [314800] global_step=314800, grad_norm=4.312992572784424, loss=0.6469408273696899
I0301 12:09:26.349739 140522605770496 logging_writer.py:48] [314900] global_step=314900, grad_norm=4.7546916007995605, loss=0.6774876117706299
I0301 12:10:00.165154 140522597377792 logging_writer.py:48] [315000] global_step=315000, grad_norm=4.55773401260376, loss=0.6648176908493042
I0301 12:10:33.954352 140522605770496 logging_writer.py:48] [315100] global_step=315100, grad_norm=4.8260722160339355, loss=0.6227452754974365
I0301 12:11:07.750778 140522597377792 logging_writer.py:48] [315200] global_step=315200, grad_norm=4.583219528198242, loss=0.6393744945526123
I0301 12:11:41.603742 140522605770496 logging_writer.py:48] [315300] global_step=315300, grad_norm=4.362476825714111, loss=0.6140960454940796
I0301 12:12:15.399418 140522597377792 logging_writer.py:48] [315400] global_step=315400, grad_norm=4.181514739990234, loss=0.577053964138031
I0301 12:12:49.164428 140522605770496 logging_writer.py:48] [315500] global_step=315500, grad_norm=4.624536037445068, loss=0.6244816184043884
I0301 12:13:22.066457 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:13:28.086068 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:13:36.593364 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:13:38.879411 140688601454400 submission_runner.py:411] Time since start: 110413.95s, 	Step: 315599, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14923390746116638, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0437467098236084, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.820225715637207, 'test/num_examples': 10000, 'score': 106650.83040618896, 'total_duration': 110413.95022773743, 'accumulated_submission_time': 106650.83040618896, 'accumulated_eval_time': 3739.307128429413, 'accumulated_logging_time': 12.98228931427002}
I0301 12:13:38.950346 140525256554240 logging_writer.py:48] [315599] accumulated_eval_time=3739.307128, accumulated_logging_time=12.982289, accumulated_submission_time=106650.830406, global_step=315599, preemption_count=0, score=106650.830406, test/accuracy=0.630900, test/loss=1.820226, test/num_examples=10000, total_duration=110413.950228, train/accuracy=0.960579, train/loss=0.149234, validation/accuracy=0.755000, validation/loss=1.043747, validation/num_examples=50000
I0301 12:13:39.630364 140525264946944 logging_writer.py:48] [315600] global_step=315600, grad_norm=4.305354595184326, loss=0.6285467743873596
I0301 12:14:13.355636 140525256554240 logging_writer.py:48] [315700] global_step=315700, grad_norm=4.450944900512695, loss=0.6313530206680298
I0301 12:14:47.082723 140525264946944 logging_writer.py:48] [315800] global_step=315800, grad_norm=4.922214031219482, loss=0.5870634317398071
I0301 12:15:20.867744 140525256554240 logging_writer.py:48] [315900] global_step=315900, grad_norm=4.515112400054932, loss=0.6806048154830933
I0301 12:15:54.680086 140525264946944 logging_writer.py:48] [316000] global_step=316000, grad_norm=4.708625316619873, loss=0.7271056771278381
I0301 12:16:28.457236 140525256554240 logging_writer.py:48] [316100] global_step=316100, grad_norm=4.381059169769287, loss=0.6566691398620605
I0301 12:17:02.278976 140525264946944 logging_writer.py:48] [316200] global_step=316200, grad_norm=4.634941577911377, loss=0.6613351106643677
I0301 12:17:36.059331 140525256554240 logging_writer.py:48] [316300] global_step=316300, grad_norm=4.4602437019348145, loss=0.6078616380691528
I0301 12:18:09.880732 140525264946944 logging_writer.py:48] [316400] global_step=316400, grad_norm=4.408992767333984, loss=0.6300434470176697
I0301 12:18:43.667775 140525256554240 logging_writer.py:48] [316500] global_step=316500, grad_norm=4.258680820465088, loss=0.7018821239471436
I0301 12:19:17.496167 140525264946944 logging_writer.py:48] [316600] global_step=316600, grad_norm=4.241146564483643, loss=0.6105278730392456
I0301 12:19:51.272545 140525256554240 logging_writer.py:48] [316700] global_step=316700, grad_norm=4.823023796081543, loss=0.6534085869789124
I0301 12:20:25.098355 140525264946944 logging_writer.py:48] [316800] global_step=316800, grad_norm=4.603720664978027, loss=0.651348352432251
I0301 12:20:58.864916 140525256554240 logging_writer.py:48] [316900] global_step=316900, grad_norm=4.697257995605469, loss=0.5605705976486206
I0301 12:21:32.668395 140525264946944 logging_writer.py:48] [317000] global_step=317000, grad_norm=4.224584579467773, loss=0.5979112982749939
I0301 12:22:06.434013 140525256554240 logging_writer.py:48] [317100] global_step=317100, grad_norm=5.2896246910095215, loss=0.7146739363670349
I0301 12:22:08.933778 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:22:14.942185 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:22:23.675638 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:22:25.947385 140688601454400 submission_runner.py:411] Time since start: 110941.02s, 	Step: 317109, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14777058362960815, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0443214178085327, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.821238398551941, 'test/num_examples': 10000, 'score': 107160.75046777725, 'total_duration': 110941.01823163033, 'accumulated_submission_time': 107160.75046777725, 'accumulated_eval_time': 3756.3206837177277, 'accumulated_logging_time': 13.06489896774292}
I0301 12:22:26.020320 140523092305664 logging_writer.py:48] [317109] accumulated_eval_time=3756.320684, accumulated_logging_time=13.064899, accumulated_submission_time=107160.750468, global_step=317109, preemption_count=0, score=107160.750468, test/accuracy=0.630800, test/loss=1.821238, test/num_examples=10000, total_duration=110941.018232, train/accuracy=0.960519, train/loss=0.147771, validation/accuracy=0.755240, validation/loss=1.044321, validation/num_examples=50000
I0301 12:22:57.068824 140523947947776 logging_writer.py:48] [317200] global_step=317200, grad_norm=4.170775890350342, loss=0.6549167037010193
I0301 12:23:30.841227 140523092305664 logging_writer.py:48] [317300] global_step=317300, grad_norm=4.750784397125244, loss=0.6277965903282166
I0301 12:24:04.583387 140523947947776 logging_writer.py:48] [317400] global_step=317400, grad_norm=3.9389638900756836, loss=0.5137145519256592
I0301 12:24:38.456127 140523092305664 logging_writer.py:48] [317500] global_step=317500, grad_norm=4.729805946350098, loss=0.6615238189697266
I0301 12:25:12.236190 140523947947776 logging_writer.py:48] [317600] global_step=317600, grad_norm=4.476373195648193, loss=0.6592658162117004
I0301 12:25:46.047677 140523092305664 logging_writer.py:48] [317700] global_step=317700, grad_norm=4.257628440856934, loss=0.6382865905761719
I0301 12:26:19.827626 140523947947776 logging_writer.py:48] [317800] global_step=317800, grad_norm=4.387053489685059, loss=0.6266926527023315
I0301 12:26:53.660052 140523092305664 logging_writer.py:48] [317900] global_step=317900, grad_norm=4.6587958335876465, loss=0.5847656726837158
I0301 12:27:27.454774 140523947947776 logging_writer.py:48] [318000] global_step=318000, grad_norm=4.437911033630371, loss=0.601128876209259
I0301 12:28:01.265200 140523092305664 logging_writer.py:48] [318100] global_step=318100, grad_norm=4.292021751403809, loss=0.6228321194648743
I0301 12:28:35.053188 140523947947776 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.651700973510742, loss=0.664048433303833
I0301 12:29:08.850467 140523092305664 logging_writer.py:48] [318300] global_step=318300, grad_norm=4.122810363769531, loss=0.5731279850006104
I0301 12:29:42.649593 140523947947776 logging_writer.py:48] [318400] global_step=318400, grad_norm=4.286095142364502, loss=0.7003746032714844
I0301 12:30:16.537830 140523092305664 logging_writer.py:48] [318500] global_step=318500, grad_norm=4.7255024909973145, loss=0.6707751154899597
I0301 12:30:50.322457 140523947947776 logging_writer.py:48] [318600] global_step=318600, grad_norm=4.400393486022949, loss=0.6163282990455627
I0301 12:30:56.195129 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:31:02.258795 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:31:11.086044 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:31:13.453207 140688601454400 submission_runner.py:411] Time since start: 111468.52s, 	Step: 318619, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14350579679012299, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0443395376205444, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.82354736328125, 'test/num_examples': 10000, 'score': 107670.861992836, 'total_duration': 111468.52405381203, 'accumulated_submission_time': 107670.861992836, 'accumulated_eval_time': 3773.578710079193, 'accumulated_logging_time': 13.148986101150513}
I0301 12:31:13.526654 140525256554240 logging_writer.py:48] [318619] accumulated_eval_time=3773.578710, accumulated_logging_time=13.148986, accumulated_submission_time=107670.861993, global_step=318619, preemption_count=0, score=107670.861993, test/accuracy=0.630700, test/loss=1.823547, test/num_examples=10000, total_duration=111468.524054, train/accuracy=0.961515, train/loss=0.143506, validation/accuracy=0.754960, validation/loss=1.044340, validation/num_examples=50000
I0301 12:31:41.198368 140525264946944 logging_writer.py:48] [318700] global_step=318700, grad_norm=4.2721428871154785, loss=0.5557060241699219
I0301 12:32:14.927270 140525256554240 logging_writer.py:48] [318800] global_step=318800, grad_norm=4.100787162780762, loss=0.5901679992675781
I0301 12:32:48.712252 140525264946944 logging_writer.py:48] [318900] global_step=318900, grad_norm=4.882275104522705, loss=0.64100182056427
I0301 12:33:22.505694 140525256554240 logging_writer.py:48] [319000] global_step=319000, grad_norm=4.045515060424805, loss=0.5650579929351807
I0301 12:33:56.318767 140525264946944 logging_writer.py:48] [319100] global_step=319100, grad_norm=4.547558307647705, loss=0.6523076891899109
I0301 12:34:30.095242 140525256554240 logging_writer.py:48] [319200] global_step=319200, grad_norm=4.403573513031006, loss=0.622894823551178
I0301 12:35:03.910764 140525264946944 logging_writer.py:48] [319300] global_step=319300, grad_norm=5.024738788604736, loss=0.5925552248954773
I0301 12:35:37.691867 140525256554240 logging_writer.py:48] [319400] global_step=319400, grad_norm=5.0565409660339355, loss=0.6334097385406494
I0301 12:36:11.494983 140525264946944 logging_writer.py:48] [319500] global_step=319500, grad_norm=4.520083904266357, loss=0.615094006061554
I0301 12:36:45.369617 140525256554240 logging_writer.py:48] [319600] global_step=319600, grad_norm=4.764039039611816, loss=0.6339980363845825
I0301 12:37:19.120380 140525264946944 logging_writer.py:48] [319700] global_step=319700, grad_norm=4.181860446929932, loss=0.5838345289230347
I0301 12:37:52.913594 140525256554240 logging_writer.py:48] [319800] global_step=319800, grad_norm=4.410554885864258, loss=0.6433347463607788
I0301 12:38:26.725497 140525264946944 logging_writer.py:48] [319900] global_step=319900, grad_norm=4.234241962432861, loss=0.617737889289856
I0301 12:39:00.518352 140525256554240 logging_writer.py:48] [320000] global_step=320000, grad_norm=4.440535545349121, loss=0.6199501156806946
I0301 12:39:34.334383 140525264946944 logging_writer.py:48] [320100] global_step=320100, grad_norm=4.3481879234313965, loss=0.6193944215774536
I0301 12:39:43.616591 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:39:49.604935 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:39:58.305862 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:40:00.605733 140688601454400 submission_runner.py:411] Time since start: 111995.68s, 	Step: 320129, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.14651109278202057, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.044081687927246, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8216602802276611, 'test/num_examples': 10000, 'score': 108180.89000105858, 'total_duration': 111995.6765794754, 'accumulated_submission_time': 108180.89000105858, 'accumulated_eval_time': 3790.567802667618, 'accumulated_logging_time': 13.232507467269897}
I0301 12:40:00.676584 140522605770496 logging_writer.py:48] [320129] accumulated_eval_time=3790.567803, accumulated_logging_time=13.232507, accumulated_submission_time=108180.890001, global_step=320129, preemption_count=0, score=108180.890001, test/accuracy=0.630400, test/loss=1.821660, test/num_examples=10000, total_duration=111995.676579, train/accuracy=0.960360, train/loss=0.146511, validation/accuracy=0.754620, validation/loss=1.044082, validation/num_examples=50000
I0301 12:40:24.939270 140523092305664 logging_writer.py:48] [320200] global_step=320200, grad_norm=4.4588446617126465, loss=0.6659399271011353
I0301 12:40:58.680896 140522605770496 logging_writer.py:48] [320300] global_step=320300, grad_norm=4.267817497253418, loss=0.5904589295387268
I0301 12:41:32.474174 140523092305664 logging_writer.py:48] [320400] global_step=320400, grad_norm=4.513591766357422, loss=0.6152520775794983
I0301 12:42:06.232325 140522605770496 logging_writer.py:48] [320500] global_step=320500, grad_norm=4.319035530090332, loss=0.6213744878768921
I0301 12:42:40.051752 140523092305664 logging_writer.py:48] [320600] global_step=320600, grad_norm=4.607259273529053, loss=0.581577718257904
I0301 12:43:13.823291 140522605770496 logging_writer.py:48] [320700] global_step=320700, grad_norm=4.7928853034973145, loss=0.6169121861457825
I0301 12:43:47.532057 140523092305664 logging_writer.py:48] [320800] global_step=320800, grad_norm=4.17557430267334, loss=0.5843521952629089
I0301 12:44:21.299172 140522605770496 logging_writer.py:48] [320900] global_step=320900, grad_norm=4.644266605377197, loss=0.6951459050178528
I0301 12:44:55.121458 140523092305664 logging_writer.py:48] [321000] global_step=321000, grad_norm=4.629323959350586, loss=0.6060413122177124
I0301 12:45:28.896687 140522605770496 logging_writer.py:48] [321100] global_step=321100, grad_norm=4.252908229827881, loss=0.5979001522064209
I0301 12:46:02.673249 140523092305664 logging_writer.py:48] [321200] global_step=321200, grad_norm=5.344111442565918, loss=0.7014321684837341
I0301 12:46:36.470016 140522605770496 logging_writer.py:48] [321300] global_step=321300, grad_norm=4.393692970275879, loss=0.5844835638999939
I0301 12:47:10.252184 140523092305664 logging_writer.py:48] [321400] global_step=321400, grad_norm=4.787383556365967, loss=0.6625726222991943
I0301 12:47:44.050215 140522605770496 logging_writer.py:48] [321500] global_step=321500, grad_norm=4.73344087600708, loss=0.6237834095954895
I0301 12:48:17.795431 140523092305664 logging_writer.py:48] [321600] global_step=321600, grad_norm=4.332010269165039, loss=0.5854764580726624
I0301 12:48:30.795439 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:48:36.843492 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:48:45.407300 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:48:47.995896 140688601454400 submission_runner.py:411] Time since start: 112523.07s, 	Step: 321640, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14783208072185516, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0437196493148804, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8210450410842896, 'test/num_examples': 10000, 'score': 108690.94467353821, 'total_duration': 112523.06674385071, 'accumulated_submission_time': 108690.94467353821, 'accumulated_eval_time': 3807.768210887909, 'accumulated_logging_time': 13.314942359924316}
I0301 12:48:48.065033 140522605770496 logging_writer.py:48] [321640] accumulated_eval_time=3807.768211, accumulated_logging_time=13.314942, accumulated_submission_time=108690.944674, global_step=321640, preemption_count=0, score=108690.944674, test/accuracy=0.631300, test/loss=1.821045, test/num_examples=10000, total_duration=112523.066744, train/accuracy=0.960818, train/loss=0.147832, validation/accuracy=0.755420, validation/loss=1.043720, validation/num_examples=50000
I0301 12:49:08.611394 140523092305664 logging_writer.py:48] [321700] global_step=321700, grad_norm=4.728157997131348, loss=0.6860733032226562
I0301 12:49:42.306911 140522605770496 logging_writer.py:48] [321800] global_step=321800, grad_norm=4.571798324584961, loss=0.631071925163269
I0301 12:50:16.072398 140523092305664 logging_writer.py:48] [321900] global_step=321900, grad_norm=4.319903373718262, loss=0.5841147303581238
I0301 12:50:49.863683 140522605770496 logging_writer.py:48] [322000] global_step=322000, grad_norm=4.292187690734863, loss=0.6048611402511597
I0301 12:51:23.635477 140523092305664 logging_writer.py:48] [322100] global_step=322100, grad_norm=4.2113752365112305, loss=0.611941933631897
I0301 12:51:57.425197 140522605770496 logging_writer.py:48] [322200] global_step=322200, grad_norm=4.386916637420654, loss=0.619512140750885
I0301 12:52:31.209770 140523092305664 logging_writer.py:48] [322300] global_step=322300, grad_norm=4.629048824310303, loss=0.6550533771514893
I0301 12:53:05.044664 140522605770496 logging_writer.py:48] [322400] global_step=322400, grad_norm=4.376624584197998, loss=0.6857777833938599
I0301 12:53:38.845772 140523092305664 logging_writer.py:48] [322500] global_step=322500, grad_norm=4.822061538696289, loss=0.6810693144798279
I0301 12:54:12.625409 140522605770496 logging_writer.py:48] [322600] global_step=322600, grad_norm=4.582431316375732, loss=0.6408513784408569
I0301 12:54:46.437363 140523092305664 logging_writer.py:48] [322700] global_step=322700, grad_norm=4.742696285247803, loss=0.6891684532165527
I0301 12:55:20.304211 140522605770496 logging_writer.py:48] [322800] global_step=322800, grad_norm=4.2549943923950195, loss=0.5622138977050781
I0301 12:55:54.090672 140523092305664 logging_writer.py:48] [322900] global_step=322900, grad_norm=4.652382850646973, loss=0.6625372767448425
I0301 12:56:27.882756 140522605770496 logging_writer.py:48] [323000] global_step=323000, grad_norm=4.690530776977539, loss=0.6666488647460938
I0301 12:57:01.672868 140523092305664 logging_writer.py:48] [323100] global_step=323100, grad_norm=4.904127597808838, loss=0.6998542547225952
I0301 12:57:18.037247 140688601454400 spec.py:321] Evaluating on the training split.
I0301 12:57:24.069648 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 12:57:32.781724 140688601454400 spec.py:349] Evaluating on the test split.
I0301 12:57:35.069179 140688601454400 submission_runner.py:411] Time since start: 113050.14s, 	Step: 323150, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.1503463238477707, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.04445481300354, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8226555585861206, 'test/num_examples': 10000, 'score': 109200.85324692726, 'total_duration': 113050.1400270462, 'accumulated_submission_time': 109200.85324692726, 'accumulated_eval_time': 3824.800094604492, 'accumulated_logging_time': 13.394735336303711}
I0301 12:57:35.140382 140522588985088 logging_writer.py:48] [323150] accumulated_eval_time=3824.800095, accumulated_logging_time=13.394735, accumulated_submission_time=109200.853247, global_step=323150, preemption_count=0, score=109200.853247, test/accuracy=0.630800, test/loss=1.822656, test/num_examples=10000, total_duration=113050.140027, train/accuracy=0.960360, train/loss=0.150346, validation/accuracy=0.755240, validation/loss=1.044455, validation/num_examples=50000
I0301 12:57:52.374183 140522597377792 logging_writer.py:48] [323200] global_step=323200, grad_norm=4.836089134216309, loss=0.6248186230659485
I0301 12:58:26.108566 140522588985088 logging_writer.py:48] [323300] global_step=323300, grad_norm=4.636471748352051, loss=0.6276814341545105
I0301 12:58:59.922274 140522597377792 logging_writer.py:48] [323400] global_step=323400, grad_norm=4.06052303314209, loss=0.610815167427063
I0301 12:59:33.700253 140522588985088 logging_writer.py:48] [323500] global_step=323500, grad_norm=4.844453811645508, loss=0.6863073110580444
I0301 13:00:07.472112 140522597377792 logging_writer.py:48] [323600] global_step=323600, grad_norm=4.520370960235596, loss=0.6199917793273926
I0301 13:00:41.291149 140522588985088 logging_writer.py:48] [323700] global_step=323700, grad_norm=4.455389976501465, loss=0.6593787670135498
I0301 13:01:15.157267 140522597377792 logging_writer.py:48] [323800] global_step=323800, grad_norm=4.516538143157959, loss=0.6349389553070068
I0301 13:01:48.914288 140522588985088 logging_writer.py:48] [323900] global_step=323900, grad_norm=4.080872535705566, loss=0.5405083894729614
I0301 13:02:22.698689 140522597377792 logging_writer.py:48] [324000] global_step=324000, grad_norm=4.758315563201904, loss=0.6450132727622986
I0301 13:02:56.530674 140522588985088 logging_writer.py:48] [324100] global_step=324100, grad_norm=5.0950446128845215, loss=0.6533530354499817
I0301 13:03:30.318096 140522597377792 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.1605682373046875, loss=0.5485899448394775
I0301 13:04:04.151129 140522588985088 logging_writer.py:48] [324300] global_step=324300, grad_norm=4.251706123352051, loss=0.6428449153900146
I0301 13:04:37.935688 140522597377792 logging_writer.py:48] [324400] global_step=324400, grad_norm=4.529656410217285, loss=0.589123547077179
I0301 13:05:11.720725 140522588985088 logging_writer.py:48] [324500] global_step=324500, grad_norm=5.0395331382751465, loss=0.6452639102935791
I0301 13:05:45.515784 140522597377792 logging_writer.py:48] [324600] global_step=324600, grad_norm=4.270864009857178, loss=0.6617996692657471
I0301 13:06:05.244196 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:06:11.350202 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:06:20.231413 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:06:22.485476 140688601454400 submission_runner.py:411] Time since start: 113577.56s, 	Step: 324660, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14669185876846313, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0439778566360474, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8214871883392334, 'test/num_examples': 10000, 'score': 109710.8930542469, 'total_duration': 113577.55630922318, 'accumulated_submission_time': 109710.8930542469, 'accumulated_eval_time': 3842.041315317154, 'accumulated_logging_time': 13.478303670883179}
I0301 13:06:22.554659 140525264946944 logging_writer.py:48] [324660] accumulated_eval_time=3842.041315, accumulated_logging_time=13.478304, accumulated_submission_time=109710.893054, global_step=324660, preemption_count=0, score=109710.893054, test/accuracy=0.631100, test/loss=1.821487, test/num_examples=10000, total_duration=113577.556309, train/accuracy=0.960619, train/loss=0.146692, validation/accuracy=0.754800, validation/loss=1.043978, validation/num_examples=50000
I0301 13:06:36.398328 140525273339648 logging_writer.py:48] [324700] global_step=324700, grad_norm=5.181910037994385, loss=0.582493782043457
I0301 13:07:10.080968 140525264946944 logging_writer.py:48] [324800] global_step=324800, grad_norm=4.261688232421875, loss=0.5969668626785278
I0301 13:07:43.940864 140525273339648 logging_writer.py:48] [324900] global_step=324900, grad_norm=4.925477981567383, loss=0.6770672798156738
I0301 13:08:17.731941 140525264946944 logging_writer.py:48] [325000] global_step=325000, grad_norm=4.730942726135254, loss=0.6803860664367676
I0301 13:08:51.503474 140525273339648 logging_writer.py:48] [325100] global_step=325100, grad_norm=4.105463981628418, loss=0.6191816329956055
I0301 13:09:25.276879 140525264946944 logging_writer.py:48] [325200] global_step=325200, grad_norm=4.466709136962891, loss=0.5749390125274658
I0301 13:09:59.035651 140525273339648 logging_writer.py:48] [325300] global_step=325300, grad_norm=4.530673503875732, loss=0.6051161289215088
I0301 13:10:32.817437 140525264946944 logging_writer.py:48] [325400] global_step=325400, grad_norm=4.369442939758301, loss=0.6492626667022705
I0301 13:11:06.563832 140525273339648 logging_writer.py:48] [325500] global_step=325500, grad_norm=4.25674295425415, loss=0.6262711882591248
I0301 13:11:40.288747 140525264946944 logging_writer.py:48] [325600] global_step=325600, grad_norm=4.564369201660156, loss=0.6198607087135315
I0301 13:12:14.045828 140525273339648 logging_writer.py:48] [325700] global_step=325700, grad_norm=5.067010402679443, loss=0.6472358703613281
I0301 13:12:47.818276 140525264946944 logging_writer.py:48] [325800] global_step=325800, grad_norm=4.684453010559082, loss=0.6534740924835205
I0301 13:13:21.705858 140525273339648 logging_writer.py:48] [325900] global_step=325900, grad_norm=4.633228778839111, loss=0.6745984554290771
I0301 13:13:55.434030 140525264946944 logging_writer.py:48] [326000] global_step=326000, grad_norm=4.555008411407471, loss=0.5640547275543213
I0301 13:14:29.190288 140525273339648 logging_writer.py:48] [326100] global_step=326100, grad_norm=4.355062961578369, loss=0.6238818764686584
I0301 13:14:52.656602 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:14:58.608124 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:15:07.434601 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:15:09.741798 140688601454400 submission_runner.py:411] Time since start: 114104.81s, 	Step: 326171, 	{'train/accuracy': 0.9599409699440002, 'train/loss': 0.15057870745658875, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0444331169128418, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8210031986236572, 'test/num_examples': 10000, 'score': 110220.93134093285, 'total_duration': 114104.81264638901, 'accumulated_submission_time': 110220.93134093285, 'accumulated_eval_time': 3859.1264731884003, 'accumulated_logging_time': 13.558036088943481}
I0301 13:15:09.815367 140523092305664 logging_writer.py:48] [326171] accumulated_eval_time=3859.126473, accumulated_logging_time=13.558036, accumulated_submission_time=110220.931341, global_step=326171, preemption_count=0, score=110220.931341, test/accuracy=0.630900, test/loss=1.821003, test/num_examples=10000, total_duration=114104.812646, train/accuracy=0.959941, train/loss=0.150579, validation/accuracy=0.755840, validation/loss=1.044433, validation/num_examples=50000
I0301 13:15:19.943121 140523947947776 logging_writer.py:48] [326200] global_step=326200, grad_norm=4.550939559936523, loss=0.6137832999229431
I0301 13:15:53.656572 140523092305664 logging_writer.py:48] [326300] global_step=326300, grad_norm=4.388692378997803, loss=0.6247833371162415
I0301 13:16:27.401011 140523947947776 logging_writer.py:48] [326400] global_step=326400, grad_norm=4.350261211395264, loss=0.5933879613876343
I0301 13:17:01.141613 140523092305664 logging_writer.py:48] [326500] global_step=326500, grad_norm=4.462218284606934, loss=0.6479032635688782
I0301 13:17:34.921646 140523947947776 logging_writer.py:48] [326600] global_step=326600, grad_norm=4.2052717208862305, loss=0.5463144779205322
I0301 13:18:08.700301 140523092305664 logging_writer.py:48] [326700] global_step=326700, grad_norm=4.548201560974121, loss=0.6306405067443848
I0301 13:18:42.472722 140523947947776 logging_writer.py:48] [326800] global_step=326800, grad_norm=4.355128288269043, loss=0.6487749814987183
I0301 13:19:16.275482 140523092305664 logging_writer.py:48] [326900] global_step=326900, grad_norm=4.456792831420898, loss=0.6244010925292969
I0301 13:19:50.172522 140523947947776 logging_writer.py:48] [327000] global_step=327000, grad_norm=4.023494243621826, loss=0.5174708366394043
I0301 13:20:23.946517 140523092305664 logging_writer.py:48] [327100] global_step=327100, grad_norm=4.158629417419434, loss=0.5034690499305725
I0301 13:20:57.657431 140523947947776 logging_writer.py:48] [327200] global_step=327200, grad_norm=4.114695072174072, loss=0.5622043013572693
I0301 13:21:31.419480 140523092305664 logging_writer.py:48] [327300] global_step=327300, grad_norm=4.145337104797363, loss=0.5859070420265198
I0301 13:22:05.232420 140523947947776 logging_writer.py:48] [327400] global_step=327400, grad_norm=4.282484531402588, loss=0.5738387703895569
I0301 13:22:39.014946 140523092305664 logging_writer.py:48] [327500] global_step=327500, grad_norm=4.25272798538208, loss=0.5711756348609924
I0301 13:23:12.808360 140523947947776 logging_writer.py:48] [327600] global_step=327600, grad_norm=4.516352653503418, loss=0.6280340552330017
I0301 13:23:39.973747 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:23:45.930145 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:23:54.778121 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:23:57.041510 140688601454400 submission_runner.py:411] Time since start: 114632.11s, 	Step: 327682, 	{'train/accuracy': 0.9587252736091614, 'train/loss': 0.15132096409797668, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0429681539535522, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.820167899131775, 'test/num_examples': 10000, 'score': 110731.02626633644, 'total_duration': 114632.11230945587, 'accumulated_submission_time': 110731.02626633644, 'accumulated_eval_time': 3876.1941425800323, 'accumulated_logging_time': 13.642706871032715}
I0301 13:23:57.155805 140525273339648 logging_writer.py:48] [327682] accumulated_eval_time=3876.194143, accumulated_logging_time=13.642707, accumulated_submission_time=110731.026266, global_step=327682, preemption_count=0, score=110731.026266, test/accuracy=0.631600, test/loss=1.820168, test/num_examples=10000, total_duration=114632.112309, train/accuracy=0.958725, train/loss=0.151321, validation/accuracy=0.755480, validation/loss=1.042968, validation/num_examples=50000
I0301 13:24:04.219206 140525281732352 logging_writer.py:48] [327700] global_step=327700, grad_norm=4.655529975891113, loss=0.6087695360183716
I0301 13:24:37.926181 140525273339648 logging_writer.py:48] [327800] global_step=327800, grad_norm=5.149223327636719, loss=0.7004529237747192
I0301 13:25:11.662890 140525281732352 logging_writer.py:48] [327900] global_step=327900, grad_norm=4.314034461975098, loss=0.5184107422828674
I0301 13:25:45.502593 140525273339648 logging_writer.py:48] [328000] global_step=328000, grad_norm=4.236612319946289, loss=0.6362866759300232
I0301 13:26:19.278926 140525281732352 logging_writer.py:48] [328100] global_step=328100, grad_norm=4.341503143310547, loss=0.6432549357414246
I0301 13:26:53.051841 140525273339648 logging_writer.py:48] [328200] global_step=328200, grad_norm=4.620811462402344, loss=0.6071349382400513
I0301 13:27:26.849654 140525281732352 logging_writer.py:48] [328300] global_step=328300, grad_norm=4.6330037117004395, loss=0.6431428790092468
I0301 13:28:00.618594 140525273339648 logging_writer.py:48] [328400] global_step=328400, grad_norm=4.487015724182129, loss=0.6347975730895996
I0301 13:28:34.378830 140525281732352 logging_writer.py:48] [328500] global_step=328500, grad_norm=4.344412803649902, loss=0.634061336517334
I0301 13:29:08.178368 140525273339648 logging_writer.py:48] [328600] global_step=328600, grad_norm=4.402134418487549, loss=0.6172145009040833
I0301 13:29:41.947518 140525281732352 logging_writer.py:48] [328700] global_step=328700, grad_norm=5.001559257507324, loss=0.7359688878059387
I0301 13:30:15.727539 140525273339648 logging_writer.py:48] [328800] global_step=328800, grad_norm=4.263507843017578, loss=0.6001289486885071
I0301 13:30:49.535688 140525281732352 logging_writer.py:48] [328900] global_step=328900, grad_norm=4.499646186828613, loss=0.6200194358825684
I0301 13:31:23.336072 140525273339648 logging_writer.py:48] [329000] global_step=329000, grad_norm=4.628141403198242, loss=0.6615052819252014
I0301 13:31:57.152500 140525281732352 logging_writer.py:48] [329100] global_step=329100, grad_norm=4.828664779663086, loss=0.6056931614875793
I0301 13:32:27.338429 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:32:33.291217 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:32:42.028788 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:32:44.297088 140688601454400 submission_runner.py:411] Time since start: 115159.37s, 	Step: 329191, 	{'train/accuracy': 0.9612364172935486, 'train/loss': 0.14664992690086365, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0440337657928467, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.822582721710205, 'test/num_examples': 10000, 'score': 111240.5034327507, 'total_duration': 115159.3679318428, 'accumulated_submission_time': 111240.5034327507, 'accumulated_eval_time': 3893.1527485847473, 'accumulated_logging_time': 14.410885095596313}
I0301 13:32:44.367908 140523947947776 logging_writer.py:48] [329191] accumulated_eval_time=3893.152749, accumulated_logging_time=14.410885, accumulated_submission_time=111240.503433, global_step=329191, preemption_count=0, score=111240.503433, test/accuracy=0.630900, test/loss=1.822583, test/num_examples=10000, total_duration=115159.367932, train/accuracy=0.961236, train/loss=0.146650, validation/accuracy=0.755440, validation/loss=1.044034, validation/num_examples=50000
I0301 13:32:47.741890 140525256554240 logging_writer.py:48] [329200] global_step=329200, grad_norm=4.195572376251221, loss=0.566494882106781
I0301 13:33:21.477041 140523947947776 logging_writer.py:48] [329300] global_step=329300, grad_norm=4.610325813293457, loss=0.6364116668701172
I0301 13:33:55.184037 140525256554240 logging_writer.py:48] [329400] global_step=329400, grad_norm=4.460231781005859, loss=0.7063536047935486
I0301 13:34:28.962980 140523947947776 logging_writer.py:48] [329500] global_step=329500, grad_norm=4.47629976272583, loss=0.6302890181541443
I0301 13:35:02.740898 140525256554240 logging_writer.py:48] [329600] global_step=329600, grad_norm=4.776889324188232, loss=0.6064066290855408
I0301 13:35:36.535335 140523947947776 logging_writer.py:48] [329700] global_step=329700, grad_norm=4.460873603820801, loss=0.6868455410003662
I0301 13:36:10.322731 140525256554240 logging_writer.py:48] [329800] global_step=329800, grad_norm=4.756174564361572, loss=0.5883055925369263
I0301 13:36:44.103876 140523947947776 logging_writer.py:48] [329900] global_step=329900, grad_norm=4.595632553100586, loss=0.6122691631317139
I0301 13:37:17.876445 140525256554240 logging_writer.py:48] [330000] global_step=330000, grad_norm=4.779264450073242, loss=0.6287631988525391
I0301 13:37:51.662521 140523947947776 logging_writer.py:48] [330100] global_step=330100, grad_norm=4.400588035583496, loss=0.6282402873039246
I0301 13:38:25.485508 140525256554240 logging_writer.py:48] [330200] global_step=330200, grad_norm=4.233502388000488, loss=0.5717346668243408
I0301 13:38:59.253812 140523947947776 logging_writer.py:48] [330300] global_step=330300, grad_norm=4.93917989730835, loss=0.6226041913032532
I0301 13:39:33.022359 140525256554240 logging_writer.py:48] [330400] global_step=330400, grad_norm=4.127804756164551, loss=0.5585098266601562
I0301 13:40:06.794239 140523947947776 logging_writer.py:48] [330500] global_step=330500, grad_norm=4.301460266113281, loss=0.6245404481887817
I0301 13:40:40.537649 140525256554240 logging_writer.py:48] [330600] global_step=330600, grad_norm=4.734514236450195, loss=0.6419428586959839
I0301 13:41:14.289004 140523947947776 logging_writer.py:48] [330700] global_step=330700, grad_norm=4.263077735900879, loss=0.6509760618209839
I0301 13:41:14.439964 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:41:20.375786 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:41:29.020602 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:41:31.271771 140688601454400 submission_runner.py:411] Time since start: 115686.34s, 	Step: 330702, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.14893050491809845, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.044543743133545, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8222780227661133, 'test/num_examples': 10000, 'score': 111750.51376104355, 'total_duration': 115686.34260249138, 'accumulated_submission_time': 111750.51376104355, 'accumulated_eval_time': 3909.9845094680786, 'accumulated_logging_time': 14.49191951751709}
I0301 13:41:31.345504 140522597377792 logging_writer.py:48] [330702] accumulated_eval_time=3909.984509, accumulated_logging_time=14.491920, accumulated_submission_time=111750.513761, global_step=330702, preemption_count=0, score=111750.513761, test/accuracy=0.629900, test/loss=1.822278, test/num_examples=10000, total_duration=115686.342602, train/accuracy=0.959702, train/loss=0.148931, validation/accuracy=0.754920, validation/loss=1.044544, validation/num_examples=50000
I0301 13:42:04.733552 140522605770496 logging_writer.py:48] [330800] global_step=330800, grad_norm=4.877652168273926, loss=0.6921865940093994
I0301 13:42:38.448869 140522597377792 logging_writer.py:48] [330900] global_step=330900, grad_norm=4.806851387023926, loss=0.6388566493988037
I0301 13:43:12.204638 140522605770496 logging_writer.py:48] [331000] global_step=331000, grad_norm=4.945281028747559, loss=0.6965366005897522
I0301 13:43:45.989809 140522597377792 logging_writer.py:48] [331100] global_step=331100, grad_norm=4.111626625061035, loss=0.5450518727302551
I0301 13:44:19.813702 140522605770496 logging_writer.py:48] [331200] global_step=331200, grad_norm=4.828824520111084, loss=0.6455841064453125
I0301 13:44:53.611070 140522597377792 logging_writer.py:48] [331300] global_step=331300, grad_norm=4.271688938140869, loss=0.6040521264076233
I0301 13:45:27.354180 140522605770496 logging_writer.py:48] [331400] global_step=331400, grad_norm=4.394078254699707, loss=0.5732766389846802
I0301 13:46:01.109499 140522597377792 logging_writer.py:48] [331500] global_step=331500, grad_norm=4.326362133026123, loss=0.5870534181594849
I0301 13:46:34.932738 140522605770496 logging_writer.py:48] [331600] global_step=331600, grad_norm=4.964565753936768, loss=0.6654261350631714
I0301 13:47:08.710362 140522597377792 logging_writer.py:48] [331700] global_step=331700, grad_norm=4.21744966506958, loss=0.6632079482078552
I0301 13:47:42.517926 140522605770496 logging_writer.py:48] [331800] global_step=331800, grad_norm=4.319725513458252, loss=0.602234423160553
I0301 13:48:16.297973 140522597377792 logging_writer.py:48] [331900] global_step=331900, grad_norm=4.044281482696533, loss=0.6370599865913391
I0301 13:48:50.062177 140522605770496 logging_writer.py:48] [332000] global_step=332000, grad_norm=4.578792095184326, loss=0.6467251777648926
I0301 13:49:23.840126 140522597377792 logging_writer.py:48] [332100] global_step=332100, grad_norm=4.2653679847717285, loss=0.671129047870636
I0301 13:49:57.581517 140522605770496 logging_writer.py:48] [332200] global_step=332200, grad_norm=4.504567623138428, loss=0.6552950739860535
I0301 13:50:01.484300 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:50:07.488315 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:50:16.197611 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:50:18.511347 140688601454400 submission_runner.py:411] Time since start: 116213.58s, 	Step: 332213, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14840257167816162, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0446621179580688, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8226637840270996, 'test/num_examples': 10000, 'score': 112260.58879756927, 'total_duration': 116213.58219194412, 'accumulated_submission_time': 112260.58879756927, 'accumulated_eval_time': 3927.0115196704865, 'accumulated_logging_time': 14.57741928100586}
I0301 13:50:18.585776 140522605770496 logging_writer.py:48] [332213] accumulated_eval_time=3927.011520, accumulated_logging_time=14.577419, accumulated_submission_time=112260.588798, global_step=332213, preemption_count=0, score=112260.588798, test/accuracy=0.632000, test/loss=1.822664, test/num_examples=10000, total_duration=116213.582192, train/accuracy=0.959961, train/loss=0.148403, validation/accuracy=0.755360, validation/loss=1.044662, validation/num_examples=50000
I0301 13:50:48.407327 140523092305664 logging_writer.py:48] [332300] global_step=332300, grad_norm=4.22629976272583, loss=0.6305565237998962
I0301 13:51:22.150886 140522605770496 logging_writer.py:48] [332400] global_step=332400, grad_norm=4.547970294952393, loss=0.635055661201477
I0301 13:51:55.938446 140523092305664 logging_writer.py:48] [332500] global_step=332500, grad_norm=4.535326957702637, loss=0.6211231350898743
I0301 13:52:29.745500 140522605770496 logging_writer.py:48] [332600] global_step=332600, grad_norm=4.412587642669678, loss=0.5748670697212219
I0301 13:53:03.509307 140523092305664 logging_writer.py:48] [332700] global_step=332700, grad_norm=4.669170379638672, loss=0.6732915639877319
I0301 13:53:37.265784 140522605770496 logging_writer.py:48] [332800] global_step=332800, grad_norm=4.62838077545166, loss=0.6441044211387634
I0301 13:54:11.046793 140523092305664 logging_writer.py:48] [332900] global_step=332900, grad_norm=4.4263176918029785, loss=0.5950648188591003
I0301 13:54:44.796155 140522605770496 logging_writer.py:48] [333000] global_step=333000, grad_norm=4.495971202850342, loss=0.6651870012283325
I0301 13:55:18.617279 140523092305664 logging_writer.py:48] [333100] global_step=333100, grad_norm=4.2938337326049805, loss=0.6328380107879639
I0301 13:55:52.419701 140522605770496 logging_writer.py:48] [333200] global_step=333200, grad_norm=4.799259185791016, loss=0.5959751009941101
I0301 13:56:26.212664 140523092305664 logging_writer.py:48] [333300] global_step=333300, grad_norm=4.661245346069336, loss=0.6960910558700562
I0301 13:57:00.073596 140522605770496 logging_writer.py:48] [333400] global_step=333400, grad_norm=4.593593120574951, loss=0.6519694328308105
I0301 13:57:33.871702 140523092305664 logging_writer.py:48] [333500] global_step=333500, grad_norm=4.475957870483398, loss=0.6267179250717163
I0301 13:58:07.645392 140522605770496 logging_writer.py:48] [333600] global_step=333600, grad_norm=4.9442548751831055, loss=0.6592760682106018
I0301 13:58:41.386112 140523092305664 logging_writer.py:48] [333700] global_step=333700, grad_norm=4.976156711578369, loss=0.6315663456916809
I0301 13:58:48.619765 140688601454400 spec.py:321] Evaluating on the training split.
I0301 13:58:54.611957 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 13:59:03.479155 140688601454400 spec.py:349] Evaluating on the test split.
I0301 13:59:05.761638 140688601454400 submission_runner.py:411] Time since start: 116740.83s, 	Step: 333723, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.1488601267337799, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.0437555313110352, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8226979970932007, 'test/num_examples': 10000, 'score': 112770.559705019, 'total_duration': 116740.83248353004, 'accumulated_submission_time': 112770.559705019, 'accumulated_eval_time': 3944.153341770172, 'accumulated_logging_time': 14.663526058197021}
I0301 13:59:05.837175 140522588985088 logging_writer.py:48] [333723] accumulated_eval_time=3944.153342, accumulated_logging_time=14.663526, accumulated_submission_time=112770.559705, global_step=333723, preemption_count=0, score=112770.559705, test/accuracy=0.630800, test/loss=1.822698, test/num_examples=10000, total_duration=116740.832484, train/accuracy=0.959742, train/loss=0.148860, validation/accuracy=0.755500, validation/loss=1.043756, validation/num_examples=50000
I0301 13:59:32.114434 140522597377792 logging_writer.py:48] [333800] global_step=333800, grad_norm=4.592818260192871, loss=0.6218927502632141
I0301 14:00:05.855941 140522588985088 logging_writer.py:48] [333900] global_step=333900, grad_norm=4.291412353515625, loss=0.5229820609092712
I0301 14:00:39.547037 140522597377792 logging_writer.py:48] [334000] global_step=334000, grad_norm=4.827384948730469, loss=0.6626052856445312
I0301 14:01:13.334597 140522588985088 logging_writer.py:48] [334100] global_step=334100, grad_norm=5.030137538909912, loss=0.6629651784896851
I0301 14:01:47.089451 140522597377792 logging_writer.py:48] [334200] global_step=334200, grad_norm=4.842556476593018, loss=0.6238275170326233
I0301 14:02:20.844066 140522588985088 logging_writer.py:48] [334300] global_step=334300, grad_norm=4.321727275848389, loss=0.6309261918067932
I0301 14:02:54.813681 140522597377792 logging_writer.py:48] [334400] global_step=334400, grad_norm=4.231749057769775, loss=0.6241110563278198
I0301 14:03:28.588231 140522588985088 logging_writer.py:48] [334500] global_step=334500, grad_norm=4.553170204162598, loss=0.6039857268333435
I0301 14:04:02.352288 140522597377792 logging_writer.py:48] [334600] global_step=334600, grad_norm=4.709236145019531, loss=0.6121795773506165
I0301 14:04:36.163750 140522588985088 logging_writer.py:48] [334700] global_step=334700, grad_norm=4.1906657218933105, loss=0.5847334861755371
I0301 14:05:09.936481 140522597377792 logging_writer.py:48] [334800] global_step=334800, grad_norm=4.608964920043945, loss=0.7010321617126465
I0301 14:05:43.704824 140522588985088 logging_writer.py:48] [334900] global_step=334900, grad_norm=4.868135452270508, loss=0.6130069494247437
I0301 14:06:17.475081 140522597377792 logging_writer.py:48] [335000] global_step=335000, grad_norm=4.386235237121582, loss=0.6047450304031372
I0301 14:06:51.256360 140522588985088 logging_writer.py:48] [335100] global_step=335100, grad_norm=4.589451789855957, loss=0.6542378067970276
I0301 14:07:25.007619 140522597377792 logging_writer.py:48] [335200] global_step=335200, grad_norm=4.7014641761779785, loss=0.7109021544456482
I0301 14:07:35.984253 140688601454400 spec.py:321] Evaluating on the training split.
I0301 14:07:41.952234 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 14:07:50.840755 140688601454400 spec.py:349] Evaluating on the test split.
I0301 14:07:53.118133 140688601454400 submission_runner.py:411] Time since start: 117268.19s, 	Step: 335234, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.1474035531282425, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.0441795587539673, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8214054107666016, 'test/num_examples': 10000, 'score': 113280.64534378052, 'total_duration': 117268.18897390366, 'accumulated_submission_time': 113280.64534378052, 'accumulated_eval_time': 3961.2871708869934, 'accumulated_logging_time': 14.749180316925049}
I0301 14:07:53.195125 140525256554240 logging_writer.py:48] [335234] accumulated_eval_time=3961.287171, accumulated_logging_time=14.749180, accumulated_submission_time=113280.645344, global_step=335234, preemption_count=0, score=113280.645344, test/accuracy=0.630600, test/loss=1.821405, test/num_examples=10000, total_duration=117268.188974, train/accuracy=0.960898, train/loss=0.147404, validation/accuracy=0.755320, validation/loss=1.044180, validation/num_examples=50000
I0301 14:08:15.810251 140525264946944 logging_writer.py:48] [335300] global_step=335300, grad_norm=4.505566596984863, loss=0.6464554667472839
I0301 14:08:49.654353 140525256554240 logging_writer.py:48] [335400] global_step=335400, grad_norm=5.082211017608643, loss=0.7051626443862915
I0301 14:09:23.420244 140525264946944 logging_writer.py:48] [335500] global_step=335500, grad_norm=4.257422924041748, loss=0.6300464868545532
I0301 14:09:57.184204 140525256554240 logging_writer.py:48] [335600] global_step=335600, grad_norm=5.037984848022461, loss=0.6525101661682129
I0301 14:10:30.959800 140525264946944 logging_writer.py:48] [335700] global_step=335700, grad_norm=4.651327610015869, loss=0.5951521396636963
I0301 14:11:04.702699 140525256554240 logging_writer.py:48] [335800] global_step=335800, grad_norm=5.075249195098877, loss=0.7463522553443909
I0301 14:11:38.489785 140525264946944 logging_writer.py:48] [335900] global_step=335900, grad_norm=4.4064040184021, loss=0.5989749431610107
I0301 14:12:12.274438 140525256554240 logging_writer.py:48] [336000] global_step=336000, grad_norm=4.689799785614014, loss=0.6321189999580383
I0301 14:12:46.054839 140525264946944 logging_writer.py:48] [336100] global_step=336100, grad_norm=4.517185688018799, loss=0.6237735152244568
I0301 14:13:19.855571 140525256554240 logging_writer.py:48] [336200] global_step=336200, grad_norm=4.954001426696777, loss=0.6849362254142761
I0301 14:13:53.644474 140525264946944 logging_writer.py:48] [336300] global_step=336300, grad_norm=4.621770858764648, loss=0.601754903793335
I0301 14:14:27.450412 140525256554240 logging_writer.py:48] [336400] global_step=336400, grad_norm=5.013174057006836, loss=0.713614821434021
I0301 14:15:01.389674 140525264946944 logging_writer.py:48] [336500] global_step=336500, grad_norm=4.728622913360596, loss=0.6295943856239319
I0301 14:15:35.210571 140525256554240 logging_writer.py:48] [336600] global_step=336600, grad_norm=4.468340873718262, loss=0.6426934599876404
I0301 14:16:08.990857 140525264946944 logging_writer.py:48] [336700] global_step=336700, grad_norm=4.74509334564209, loss=0.58966064453125
I0301 14:16:23.331268 140688601454400 spec.py:321] Evaluating on the training split.
I0301 14:16:29.384958 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 14:16:38.157327 140688601454400 spec.py:349] Evaluating on the test split.
I0301 14:16:40.511326 140688601454400 submission_runner.py:411] Time since start: 117795.58s, 	Step: 336744, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14919085800647736, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0451172590255737, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8223639726638794, 'test/num_examples': 10000, 'score': 113790.71906757355, 'total_duration': 117795.58216953278, 'accumulated_submission_time': 113790.71906757355, 'accumulated_eval_time': 3978.467176914215, 'accumulated_logging_time': 14.835916996002197}
I0301 14:16:40.584439 140522597377792 logging_writer.py:48] [336744] accumulated_eval_time=3978.467177, accumulated_logging_time=14.835917, accumulated_submission_time=113790.719068, global_step=336744, preemption_count=0, score=113790.719068, test/accuracy=0.630900, test/loss=1.822364, test/num_examples=10000, total_duration=117795.582170, train/accuracy=0.960100, train/loss=0.149191, validation/accuracy=0.755280, validation/loss=1.045117, validation/num_examples=50000
I0301 14:16:59.799617 140522605770496 logging_writer.py:48] [336800] global_step=336800, grad_norm=4.459438800811768, loss=0.6072032451629639
I0301 14:17:33.536474 140522597377792 logging_writer.py:48] [336900] global_step=336900, grad_norm=4.127120494842529, loss=0.5644716620445251
I0301 14:18:07.304867 140522605770496 logging_writer.py:48] [337000] global_step=337000, grad_norm=4.107967376708984, loss=0.5855251550674438
I0301 14:18:41.062583 140522597377792 logging_writer.py:48] [337100] global_step=337100, grad_norm=4.48707389831543, loss=0.6101850271224976
I0301 14:19:14.811025 140522605770496 logging_writer.py:48] [337200] global_step=337200, grad_norm=4.35584020614624, loss=0.5600277185440063
I0301 14:19:48.570444 140522597377792 logging_writer.py:48] [337300] global_step=337300, grad_norm=4.723687171936035, loss=0.6360664963722229
I0301 14:20:22.358753 140522605770496 logging_writer.py:48] [337400] global_step=337400, grad_norm=4.418132305145264, loss=0.6474555730819702
I0301 14:20:56.287375 140522597377792 logging_writer.py:48] [337500] global_step=337500, grad_norm=4.366216659545898, loss=0.6440417766571045
I0301 14:21:30.058992 140522605770496 logging_writer.py:48] [337600] global_step=337600, grad_norm=4.629615783691406, loss=0.6088110208511353
I0301 14:22:03.807328 140522597377792 logging_writer.py:48] [337700] global_step=337700, grad_norm=4.712679862976074, loss=0.6302407383918762
I0301 14:22:37.514074 140522605770496 logging_writer.py:48] [337800] global_step=337800, grad_norm=4.375984191894531, loss=0.6212273240089417
I0301 14:23:11.257973 140522597377792 logging_writer.py:48] [337900] global_step=337900, grad_norm=4.876710891723633, loss=0.7067719101905823
I0301 14:23:45.013553 140522605770496 logging_writer.py:48] [338000] global_step=338000, grad_norm=4.12714958190918, loss=0.6257123947143555
I0301 14:24:18.770616 140522597377792 logging_writer.py:48] [338100] global_step=338100, grad_norm=3.873073101043701, loss=0.5704200863838196
I0301 14:24:52.557175 140522605770496 logging_writer.py:48] [338200] global_step=338200, grad_norm=4.316396236419678, loss=0.6172782778739929
I0301 14:25:10.589259 140688601454400 spec.py:321] Evaluating on the training split.
I0301 14:25:16.571114 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 14:25:25.089635 140688601454400 spec.py:349] Evaluating on the test split.
I0301 14:25:27.338627 140688601454400 submission_runner.py:411] Time since start: 118322.41s, 	Step: 338255, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14975686371326447, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0436608791351318, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8215768337249756, 'test/num_examples': 10000, 'score': 114300.66180181503, 'total_duration': 118322.40947365761, 'accumulated_submission_time': 114300.66180181503, 'accumulated_eval_time': 3995.216495990753, 'accumulated_logging_time': 14.918900728225708}
I0301 14:25:27.411640 140525264946944 logging_writer.py:48] [338255] accumulated_eval_time=3995.216496, accumulated_logging_time=14.918901, accumulated_submission_time=114300.661802, global_step=338255, preemption_count=0, score=114300.661802, test/accuracy=0.630300, test/loss=1.821577, test/num_examples=10000, total_duration=118322.409474, train/accuracy=0.959801, train/loss=0.149757, validation/accuracy=0.755140, validation/loss=1.043661, validation/num_examples=50000
I0301 14:25:42.927652 140525273339648 logging_writer.py:48] [338300] global_step=338300, grad_norm=4.254495143890381, loss=0.6418975591659546
I0301 14:26:16.652390 140525264946944 logging_writer.py:48] [338400] global_step=338400, grad_norm=4.965029716491699, loss=0.6178255081176758
I0301 14:26:50.399299 140525273339648 logging_writer.py:48] [338500] global_step=338500, grad_norm=4.1566643714904785, loss=0.5540128946304321
I0301 14:27:24.268987 140525264946944 logging_writer.py:48] [338600] global_step=338600, grad_norm=4.720920562744141, loss=0.7113048434257507
I0301 14:27:58.018058 140525273339648 logging_writer.py:48] [338700] global_step=338700, grad_norm=4.184581279754639, loss=0.5640971660614014
I0301 14:28:31.804634 140525264946944 logging_writer.py:48] [338800] global_step=338800, grad_norm=4.5874505043029785, loss=0.573257327079773
I0301 14:29:05.598676 140525273339648 logging_writer.py:48] [338900] global_step=338900, grad_norm=4.824206829071045, loss=0.6055731773376465
I0301 14:29:39.386349 140525264946944 logging_writer.py:48] [339000] global_step=339000, grad_norm=4.796586990356445, loss=0.5647449493408203
I0301 14:30:13.193569 140525273339648 logging_writer.py:48] [339100] global_step=339100, grad_norm=4.9715118408203125, loss=0.7310211062431335
I0301 14:30:46.960263 140525264946944 logging_writer.py:48] [339200] global_step=339200, grad_norm=4.294308662414551, loss=0.5714293122291565
I0301 14:31:20.766830 140525273339648 logging_writer.py:48] [339300] global_step=339300, grad_norm=4.643265724182129, loss=0.681067168712616
I0301 14:31:54.530805 140525264946944 logging_writer.py:48] [339400] global_step=339400, grad_norm=4.60294771194458, loss=0.5728662610054016
I0301 14:32:28.342598 140525273339648 logging_writer.py:48] [339500] global_step=339500, grad_norm=4.611236095428467, loss=0.615607738494873
I0301 14:33:02.112533 140525264946944 logging_writer.py:48] [339600] global_step=339600, grad_norm=4.944437503814697, loss=0.6306824684143066
I0301 14:33:35.983515 140525273339648 logging_writer.py:48] [339700] global_step=339700, grad_norm=4.558612823486328, loss=0.635947585105896
I0301 14:33:57.405569 140688601454400 spec.py:321] Evaluating on the training split.
I0301 14:34:03.534850 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 14:34:12.259007 140688601454400 spec.py:349] Evaluating on the test split.
I0301 14:34:14.474765 140688601454400 submission_runner.py:411] Time since start: 118849.55s, 	Step: 339765, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.15014895796775818, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0430506467819214, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.82118558883667, 'test/num_examples': 10000, 'score': 114810.59439182281, 'total_duration': 118849.54559993744, 'accumulated_submission_time': 114810.59439182281, 'accumulated_eval_time': 4012.285629749298, 'accumulated_logging_time': 15.001896142959595}
I0301 14:34:14.544765 140525264946944 logging_writer.py:48] [339765] accumulated_eval_time=4012.285630, accumulated_logging_time=15.001896, accumulated_submission_time=114810.594392, global_step=339765, preemption_count=0, score=114810.594392, test/accuracy=0.629900, test/loss=1.821186, test/num_examples=10000, total_duration=118849.545600, train/accuracy=0.960519, train/loss=0.150149, validation/accuracy=0.754840, validation/loss=1.043051, validation/num_examples=50000
I0301 14:34:26.709814 140525273339648 logging_writer.py:48] [339800] global_step=339800, grad_norm=4.423157215118408, loss=0.6022331714630127
I0301 14:35:00.417963 140525264946944 logging_writer.py:48] [339900] global_step=339900, grad_norm=4.27692174911499, loss=0.6699016690254211
I0301 14:35:34.174473 140525273339648 logging_writer.py:48] [340000] global_step=340000, grad_norm=4.662293434143066, loss=0.6780726909637451
I0301 14:36:07.922400 140525264946944 logging_writer.py:48] [340100] global_step=340100, grad_norm=4.146214485168457, loss=0.6094754934310913
I0301 14:36:41.694277 140525273339648 logging_writer.py:48] [340200] global_step=340200, grad_norm=4.933290004730225, loss=0.6200425624847412
I0301 14:37:15.462544 140525264946944 logging_writer.py:48] [340300] global_step=340300, grad_norm=4.81773042678833, loss=0.7553350925445557
I0301 14:37:49.238188 140525273339648 logging_writer.py:48] [340400] global_step=340400, grad_norm=4.943121910095215, loss=0.6170427799224854
I0301 14:38:23.054306 140525264946944 logging_writer.py:48] [340500] global_step=340500, grad_norm=4.623042106628418, loss=0.5874323844909668
I0301 14:38:56.801793 140525273339648 logging_writer.py:48] [340600] global_step=340600, grad_norm=4.226244926452637, loss=0.606094479560852
I0301 14:39:30.632359 140525264946944 logging_writer.py:48] [340700] global_step=340700, grad_norm=5.08735990524292, loss=0.6980355978012085
I0301 14:40:04.413632 140525273339648 logging_writer.py:48] [340800] global_step=340800, grad_norm=4.4184346199035645, loss=0.6193938255310059
I0301 14:40:38.206166 140525264946944 logging_writer.py:48] [340900] global_step=340900, grad_norm=4.582404613494873, loss=0.6226447224617004
I0301 14:41:12.025655 140525273339648 logging_writer.py:48] [341000] global_step=341000, grad_norm=4.459385871887207, loss=0.5779932737350464
I0301 14:41:45.817755 140525264946944 logging_writer.py:48] [341100] global_step=341100, grad_norm=4.434813976287842, loss=0.6117551326751709
I0301 14:42:19.620481 140525273339648 logging_writer.py:48] [341200] global_step=341200, grad_norm=4.495750427246094, loss=0.6351867318153381
I0301 14:42:44.721496 140688601454400 spec.py:321] Evaluating on the training split.
I0301 14:42:50.691094 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 14:42:59.485699 140688601454400 spec.py:349] Evaluating on the test split.
I0301 14:43:01.753888 140688601454400 submission_runner.py:411] Time since start: 119376.82s, 	Step: 341276, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.1441345363855362, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0443507432937622, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8239904642105103, 'test/num_examples': 10000, 'score': 115320.70908522606, 'total_duration': 119376.82467389107, 'accumulated_submission_time': 115320.70908522606, 'accumulated_eval_time': 4029.317915916443, 'accumulated_logging_time': 15.082010746002197}
I0301 14:43:01.875132 140522597377792 logging_writer.py:48] [341276] accumulated_eval_time=4029.317916, accumulated_logging_time=15.082011, accumulated_submission_time=115320.709085, global_step=341276, preemption_count=0, score=115320.709085, test/accuracy=0.630400, test/loss=1.823990, test/num_examples=10000, total_duration=119376.824674, train/accuracy=0.961496, train/loss=0.144135, validation/accuracy=0.755260, validation/loss=1.044351, validation/num_examples=50000
I0301 14:43:10.347460 140522605770496 logging_writer.py:48] [341300] global_step=341300, grad_norm=4.272789001464844, loss=0.5884125232696533
I0301 14:43:44.114665 140522597377792 logging_writer.py:48] [341400] global_step=341400, grad_norm=4.796395301818848, loss=0.6237285733222961
I0301 14:44:17.880408 140522605770496 logging_writer.py:48] [341500] global_step=341500, grad_norm=4.544702529907227, loss=0.6303843259811401
I0301 14:44:51.656482 140522597377792 logging_writer.py:48] [341600] global_step=341600, grad_norm=4.72570276260376, loss=0.6166918873786926
I0301 14:45:25.550123 140522605770496 logging_writer.py:48] [341700] global_step=341700, grad_norm=4.984755039215088, loss=0.5761827230453491
I0301 14:45:59.327410 140522597377792 logging_writer.py:48] [341800] global_step=341800, grad_norm=4.647645950317383, loss=0.6751574277877808
I0301 14:46:33.074612 140522605770496 logging_writer.py:48] [341900] global_step=341900, grad_norm=4.453183650970459, loss=0.5877342820167542
I0301 14:47:06.867025 140522597377792 logging_writer.py:48] [342000] global_step=342000, grad_norm=4.6947245597839355, loss=0.6888158917427063
I0301 14:47:40.645773 140522605770496 logging_writer.py:48] [342100] global_step=342100, grad_norm=3.965996742248535, loss=0.49472925066947937
I0301 14:48:14.419234 140522597377792 logging_writer.py:48] [342200] global_step=342200, grad_norm=4.380356788635254, loss=0.5603083968162537
I0301 14:48:48.175698 140522605770496 logging_writer.py:48] [342300] global_step=342300, grad_norm=4.472879886627197, loss=0.6279193162918091
I0301 14:49:21.927591 140522597377792 logging_writer.py:48] [342400] global_step=342400, grad_norm=4.420158863067627, loss=0.6565501689910889
I0301 14:49:55.667141 140522605770496 logging_writer.py:48] [342500] global_step=342500, grad_norm=4.810935020446777, loss=0.679935872554779
I0301 14:50:29.483547 140522597377792 logging_writer.py:48] [342600] global_step=342600, grad_norm=4.882195472717285, loss=0.5861301422119141
I0301 14:51:03.266427 140522605770496 logging_writer.py:48] [342700] global_step=342700, grad_norm=4.037408828735352, loss=0.6033221483230591
I0301 14:51:31.856064 140688601454400 spec.py:321] Evaluating on the training split.
I0301 14:51:37.838223 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 14:51:46.550377 140688601454400 spec.py:349] Evaluating on the test split.
I0301 14:51:48.825124 140688601454400 submission_runner.py:411] Time since start: 119903.90s, 	Step: 342786, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14583943784236908, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0440630912780762, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8213176727294922, 'test/num_examples': 10000, 'score': 115830.62089276314, 'total_duration': 119903.89597392082, 'accumulated_submission_time': 115830.62089276314, 'accumulated_eval_time': 4046.286930322647, 'accumulated_logging_time': 15.220804691314697}
I0301 14:51:48.902191 140522597377792 logging_writer.py:48] [342786] accumulated_eval_time=4046.286930, accumulated_logging_time=15.220805, accumulated_submission_time=115830.620893, global_step=342786, preemption_count=0, score=115830.620893, test/accuracy=0.630500, test/loss=1.821318, test/num_examples=10000, total_duration=119903.895974, train/accuracy=0.961655, train/loss=0.145839, validation/accuracy=0.754860, validation/loss=1.044063, validation/num_examples=50000
I0301 14:51:53.960052 140525256554240 logging_writer.py:48] [342800] global_step=342800, grad_norm=4.5837907791137695, loss=0.5956741571426392
I0301 14:52:27.702774 140522597377792 logging_writer.py:48] [342900] global_step=342900, grad_norm=4.537688732147217, loss=0.571336567401886
I0301 14:53:01.461711 140525256554240 logging_writer.py:48] [343000] global_step=343000, grad_norm=4.406800270080566, loss=0.5807022452354431
I0301 14:53:35.247442 140522597377792 logging_writer.py:48] [343100] global_step=343100, grad_norm=4.2815775871276855, loss=0.6399083137512207
I0301 14:54:09.061004 140525256554240 logging_writer.py:48] [343200] global_step=343200, grad_norm=4.8243303298950195, loss=0.6825659275054932
I0301 14:54:42.842581 140522597377792 logging_writer.py:48] [343300] global_step=343300, grad_norm=4.177298545837402, loss=0.5995746850967407
I0301 14:55:16.637814 140525256554240 logging_writer.py:48] [343400] global_step=343400, grad_norm=4.43914270401001, loss=0.6000292301177979
I0301 14:55:50.360442 140522597377792 logging_writer.py:48] [343500] global_step=343500, grad_norm=4.8396477699279785, loss=0.6425020694732666
I0301 14:56:24.139450 140525256554240 logging_writer.py:48] [343600] global_step=343600, grad_norm=5.195608139038086, loss=0.6514848470687866
I0301 14:56:57.952871 140522597377792 logging_writer.py:48] [343700] global_step=343700, grad_norm=4.241354465484619, loss=0.5912898778915405
I0301 14:57:31.928931 140525256554240 logging_writer.py:48] [343800] global_step=343800, grad_norm=4.782174110412598, loss=0.6311314702033997
I0301 14:58:05.760981 140522597377792 logging_writer.py:48] [343900] global_step=343900, grad_norm=4.722395896911621, loss=0.619185745716095
I0301 14:58:39.542523 140525256554240 logging_writer.py:48] [344000] global_step=344000, grad_norm=4.45413064956665, loss=0.6160629391670227
I0301 14:59:13.332963 140522597377792 logging_writer.py:48] [344100] global_step=344100, grad_norm=4.880241394042969, loss=0.5747957825660706
I0301 14:59:47.131506 140525256554240 logging_writer.py:48] [344200] global_step=344200, grad_norm=4.8437371253967285, loss=0.648054301738739
I0301 15:00:19.042692 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:00:25.053586 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:00:33.659790 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:00:35.997823 140688601454400 submission_runner.py:411] Time since start: 120431.07s, 	Step: 344296, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.14973142743110657, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.043811321258545, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8192788362503052, 'test/num_examples': 10000, 'score': 116340.69873285294, 'total_duration': 120431.06862139702, 'accumulated_submission_time': 116340.69873285294, 'accumulated_eval_time': 4063.241968870163, 'accumulated_logging_time': 15.308415651321411}
I0301 15:00:36.085432 140525264946944 logging_writer.py:48] [344296] accumulated_eval_time=4063.241969, accumulated_logging_time=15.308416, accumulated_submission_time=116340.698733, global_step=344296, preemption_count=0, score=116340.698733, test/accuracy=0.631000, test/loss=1.819279, test/num_examples=10000, total_duration=120431.068621, train/accuracy=0.959602, train/loss=0.149731, validation/accuracy=0.755320, validation/loss=1.043811, validation/num_examples=50000
I0301 15:00:37.770633 140525273339648 logging_writer.py:48] [344300] global_step=344300, grad_norm=4.242319107055664, loss=0.6246076822280884
I0301 15:01:11.518914 140525264946944 logging_writer.py:48] [344400] global_step=344400, grad_norm=4.300745964050293, loss=0.6328922510147095
I0301 15:01:45.287737 140525273339648 logging_writer.py:48] [344500] global_step=344500, grad_norm=4.266237735748291, loss=0.5833067893981934
I0301 15:02:19.064881 140525264946944 logging_writer.py:48] [344600] global_step=344600, grad_norm=4.494213581085205, loss=0.6713123321533203
I0301 15:02:52.846057 140525273339648 logging_writer.py:48] [344700] global_step=344700, grad_norm=4.4274067878723145, loss=0.6888086795806885
I0301 15:03:26.622064 140525264946944 logging_writer.py:48] [344800] global_step=344800, grad_norm=4.337468147277832, loss=0.6100854277610779
I0301 15:04:00.543709 140525273339648 logging_writer.py:48] [344900] global_step=344900, grad_norm=4.351063251495361, loss=0.6500579714775085
I0301 15:04:34.336316 140525264946944 logging_writer.py:48] [345000] global_step=345000, grad_norm=4.917141437530518, loss=0.7785188555717468
I0301 15:05:08.133417 140525273339648 logging_writer.py:48] [345100] global_step=345100, grad_norm=4.14467716217041, loss=0.5869027376174927
I0301 15:05:41.938193 140525264946944 logging_writer.py:48] [345200] global_step=345200, grad_norm=4.262505054473877, loss=0.597925066947937
I0301 15:06:15.706757 140525273339648 logging_writer.py:48] [345300] global_step=345300, grad_norm=4.737123489379883, loss=0.6365441083908081
I0301 15:06:49.507274 140525264946944 logging_writer.py:48] [345400] global_step=345400, grad_norm=4.633787631988525, loss=0.6148483753204346
I0301 15:07:23.306982 140525273339648 logging_writer.py:48] [345500] global_step=345500, grad_norm=5.3440632820129395, loss=0.58836829662323
I0301 15:07:57.098839 140525264946944 logging_writer.py:48] [345600] global_step=345600, grad_norm=4.701931953430176, loss=0.6091035604476929
I0301 15:08:30.892591 140525273339648 logging_writer.py:48] [345700] global_step=345700, grad_norm=4.2682108879089355, loss=0.6058185696601868
I0301 15:09:04.673337 140525264946944 logging_writer.py:48] [345800] global_step=345800, grad_norm=4.777164936065674, loss=0.5808233022689819
I0301 15:09:06.173770 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:09:12.889691 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:09:21.821955 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:09:24.181868 140688601454400 submission_runner.py:411] Time since start: 120959.25s, 	Step: 345806, 	{'train/accuracy': 0.9596420526504517, 'train/loss': 0.15026216208934784, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0439318418502808, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8209062814712524, 'test/num_examples': 10000, 'score': 116850.72356057167, 'total_duration': 120959.25270080566, 'accumulated_submission_time': 116850.72356057167, 'accumulated_eval_time': 4081.2500195503235, 'accumulated_logging_time': 15.40729808807373}
I0301 15:09:24.259583 140523092305664 logging_writer.py:48] [345806] accumulated_eval_time=4081.250020, accumulated_logging_time=15.407298, accumulated_submission_time=116850.723561, global_step=345806, preemption_count=0, score=116850.723561, test/accuracy=0.631100, test/loss=1.820906, test/num_examples=10000, total_duration=120959.252701, train/accuracy=0.959642, train/loss=0.150262, validation/accuracy=0.755340, validation/loss=1.043932, validation/num_examples=50000
I0301 15:09:56.451810 140523947947776 logging_writer.py:48] [345900] global_step=345900, grad_norm=4.614853382110596, loss=0.6852626800537109
I0301 15:10:30.187585 140523092305664 logging_writer.py:48] [346000] global_step=346000, grad_norm=4.904068470001221, loss=0.6475773453712463
I0301 15:11:03.967267 140523947947776 logging_writer.py:48] [346100] global_step=346100, grad_norm=4.587404727935791, loss=0.6014813780784607
I0301 15:11:37.752136 140523092305664 logging_writer.py:48] [346200] global_step=346200, grad_norm=4.581596851348877, loss=0.6638626456260681
I0301 15:12:11.543795 140523947947776 logging_writer.py:48] [346300] global_step=346300, grad_norm=5.2488861083984375, loss=0.7087981700897217
I0301 15:12:45.365820 140523092305664 logging_writer.py:48] [346400] global_step=346400, grad_norm=4.4717817306518555, loss=0.658690333366394
I0301 15:13:19.143170 140523947947776 logging_writer.py:48] [346500] global_step=346500, grad_norm=4.403322696685791, loss=0.60865718126297
I0301 15:13:52.953831 140523092305664 logging_writer.py:48] [346600] global_step=346600, grad_norm=4.174604892730713, loss=0.6033027768135071
I0301 15:14:26.732686 140523947947776 logging_writer.py:48] [346700] global_step=346700, grad_norm=4.373902797698975, loss=0.6289367079734802
I0301 15:15:00.521862 140523092305664 logging_writer.py:48] [346800] global_step=346800, grad_norm=4.367014408111572, loss=0.6072829365730286
I0301 15:15:34.289732 140523947947776 logging_writer.py:48] [346900] global_step=346900, grad_norm=4.29791259765625, loss=0.6043406128883362
I0301 15:16:08.135565 140523092305664 logging_writer.py:48] [347000] global_step=347000, grad_norm=4.593966960906982, loss=0.6393624544143677
I0301 15:16:41.940226 140523947947776 logging_writer.py:48] [347100] global_step=347100, grad_norm=4.527046203613281, loss=0.6437092423439026
I0301 15:17:15.769550 140523092305664 logging_writer.py:48] [347200] global_step=347200, grad_norm=4.673720836639404, loss=0.6140695810317993
I0301 15:17:49.534801 140523947947776 logging_writer.py:48] [347300] global_step=347300, grad_norm=4.551395893096924, loss=0.6079233884811401
I0301 15:17:54.418375 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:18:00.439893 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:18:09.166753 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:18:11.456542 140688601454400 submission_runner.py:411] Time since start: 121486.53s, 	Step: 347316, 	{'train/accuracy': 0.9595025181770325, 'train/loss': 0.15018466114997864, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.0441985130310059, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8224879503250122, 'test/num_examples': 10000, 'score': 117360.82094693184, 'total_duration': 121486.52737092972, 'accumulated_submission_time': 117360.82094693184, 'accumulated_eval_time': 4098.288117408752, 'accumulated_logging_time': 15.495001316070557}
I0301 15:18:11.533146 140525264946944 logging_writer.py:48] [347316] accumulated_eval_time=4098.288117, accumulated_logging_time=15.495001, accumulated_submission_time=117360.820947, global_step=347316, preemption_count=0, score=117360.820947, test/accuracy=0.631900, test/loss=1.822488, test/num_examples=10000, total_duration=121486.527371, train/accuracy=0.959503, train/loss=0.150185, validation/accuracy=0.755320, validation/loss=1.044199, validation/num_examples=50000
I0301 15:18:40.197042 140525273339648 logging_writer.py:48] [347400] global_step=347400, grad_norm=4.403838634490967, loss=0.6138865947723389
I0301 15:19:13.896494 140525264946944 logging_writer.py:48] [347500] global_step=347500, grad_norm=4.528132438659668, loss=0.6050353646278381
I0301 15:19:47.662215 140525273339648 logging_writer.py:48] [347600] global_step=347600, grad_norm=4.963080406188965, loss=0.6511695384979248
I0301 15:20:21.459238 140525264946944 logging_writer.py:48] [347700] global_step=347700, grad_norm=4.436306953430176, loss=0.6941816210746765
I0301 15:20:55.236385 140525273339648 logging_writer.py:48] [347800] global_step=347800, grad_norm=4.520970821380615, loss=0.6319224834442139
I0301 15:21:29.004571 140525264946944 logging_writer.py:48] [347900] global_step=347900, grad_norm=4.9054951667785645, loss=0.6257762312889099
I0301 15:22:02.847796 140525273339648 logging_writer.py:48] [348000] global_step=348000, grad_norm=4.063634872436523, loss=0.5858239531517029
I0301 15:22:36.651247 140525264946944 logging_writer.py:48] [348100] global_step=348100, grad_norm=4.505288124084473, loss=0.6001452803611755
I0301 15:23:10.434215 140525273339648 logging_writer.py:48] [348200] global_step=348200, grad_norm=4.599372386932373, loss=0.5863595604896545
I0301 15:23:44.176440 140525264946944 logging_writer.py:48] [348300] global_step=348300, grad_norm=4.429861545562744, loss=0.5721676349639893
I0301 15:24:17.940126 140525273339648 logging_writer.py:48] [348400] global_step=348400, grad_norm=4.838763236999512, loss=0.6592634916305542
I0301 15:24:51.741590 140525264946944 logging_writer.py:48] [348500] global_step=348500, grad_norm=4.535595417022705, loss=0.6166656017303467
I0301 15:25:25.525979 140525273339648 logging_writer.py:48] [348600] global_step=348600, grad_norm=4.334434986114502, loss=0.5876549482345581
I0301 15:25:59.337149 140525264946944 logging_writer.py:48] [348700] global_step=348700, grad_norm=4.415136814117432, loss=0.6303558349609375
I0301 15:26:33.111796 140525273339648 logging_writer.py:48] [348800] global_step=348800, grad_norm=4.366040229797363, loss=0.6525248885154724
I0301 15:26:41.692118 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:26:47.790344 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:26:56.481152 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:26:58.840780 140688601454400 submission_runner.py:411] Time since start: 122013.91s, 	Step: 348827, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14595791697502136, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0425646305084229, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8201829195022583, 'test/num_examples': 10000, 'score': 117870.91880130768, 'total_duration': 122013.91163277626, 'accumulated_submission_time': 117870.91880130768, 'accumulated_eval_time': 4115.436737775803, 'accumulated_logging_time': 15.581097602844238}
I0301 15:26:58.903823 140522597377792 logging_writer.py:48] [348827] accumulated_eval_time=4115.436738, accumulated_logging_time=15.581098, accumulated_submission_time=117870.918801, global_step=348827, preemption_count=0, score=117870.918801, test/accuracy=0.630800, test/loss=1.820183, test/num_examples=10000, total_duration=122013.911633, train/accuracy=0.961834, train/loss=0.145958, validation/accuracy=0.755160, validation/loss=1.042565, validation/num_examples=50000
I0301 15:27:23.867232 140522605770496 logging_writer.py:48] [348900] global_step=348900, grad_norm=4.52230978012085, loss=0.6680585145950317
I0301 15:27:57.563961 140522597377792 logging_writer.py:48] [349000] global_step=349000, grad_norm=4.665916919708252, loss=0.6030464172363281
I0301 15:28:31.371008 140522605770496 logging_writer.py:48] [349100] global_step=349100, grad_norm=4.747139930725098, loss=0.6167983412742615
I0301 15:29:05.142506 140522597377792 logging_writer.py:48] [349200] global_step=349200, grad_norm=4.278399467468262, loss=0.5168837308883667
I0301 15:29:38.949585 140522605770496 logging_writer.py:48] [349300] global_step=349300, grad_norm=4.383640766143799, loss=0.5820058584213257
I0301 15:30:12.706894 140522597377792 logging_writer.py:48] [349400] global_step=349400, grad_norm=4.395105361938477, loss=0.5860910415649414
I0301 15:30:46.494656 140522605770496 logging_writer.py:48] [349500] global_step=349500, grad_norm=4.911623477935791, loss=0.6404684782028198
I0301 15:31:20.266730 140522597377792 logging_writer.py:48] [349600] global_step=349600, grad_norm=4.5291571617126465, loss=0.5985219478607178
I0301 15:31:54.019874 140522605770496 logging_writer.py:48] [349700] global_step=349700, grad_norm=4.159470558166504, loss=0.6186083555221558
I0301 15:32:27.763921 140522597377792 logging_writer.py:48] [349800] global_step=349800, grad_norm=4.3241987228393555, loss=0.711119532585144
I0301 15:33:01.561117 140522605770496 logging_writer.py:48] [349900] global_step=349900, grad_norm=4.644537448883057, loss=0.690037190914154
I0301 15:33:35.322983 140522597377792 logging_writer.py:48] [350000] global_step=350000, grad_norm=4.250851154327393, loss=0.5779381990432739
I0301 15:34:09.041141 140522605770496 logging_writer.py:48] [350100] global_step=350100, grad_norm=4.470930576324463, loss=0.6110520362854004
I0301 15:34:42.971557 140522597377792 logging_writer.py:48] [350200] global_step=350200, grad_norm=4.542245864868164, loss=0.5981980562210083
I0301 15:35:16.747308 140522605770496 logging_writer.py:48] [350300] global_step=350300, grad_norm=4.3663153648376465, loss=0.6200762391090393
I0301 15:35:29.041957 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:35:34.991520 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:35:43.717729 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:35:45.974915 140688601454400 submission_runner.py:411] Time since start: 122541.05s, 	Step: 350338, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1469176858663559, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0435841083526611, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.821236252784729, 'test/num_examples': 10000, 'score': 118380.99407410622, 'total_duration': 122541.04575610161, 'accumulated_submission_time': 118380.99407410622, 'accumulated_eval_time': 4132.369649171829, 'accumulated_logging_time': 15.654744863510132}
I0301 15:35:46.050439 140525256554240 logging_writer.py:48] [350338] accumulated_eval_time=4132.369649, accumulated_logging_time=15.654745, accumulated_submission_time=118380.994074, global_step=350338, preemption_count=0, score=118380.994074, test/accuracy=0.631500, test/loss=1.821236, test/num_examples=10000, total_duration=122541.045756, train/accuracy=0.960479, train/loss=0.146918, validation/accuracy=0.755020, validation/loss=1.043584, validation/num_examples=50000
I0301 15:36:07.331597 140525264946944 logging_writer.py:48] [350400] global_step=350400, grad_norm=4.3552374839782715, loss=0.6322239637374878
I0301 15:36:41.074277 140525256554240 logging_writer.py:48] [350500] global_step=350500, grad_norm=4.916648864746094, loss=0.6669859290122986
I0301 15:37:14.794102 140525264946944 logging_writer.py:48] [350600] global_step=350600, grad_norm=4.7991485595703125, loss=0.6618139743804932
I0301 15:37:48.548373 140525256554240 logging_writer.py:48] [350700] global_step=350700, grad_norm=4.6836137771606445, loss=0.6471754312515259
I0301 15:38:22.347744 140525264946944 logging_writer.py:48] [350800] global_step=350800, grad_norm=4.592728614807129, loss=0.6384580135345459
I0301 15:38:56.122984 140525256554240 logging_writer.py:48] [350900] global_step=350900, grad_norm=4.579399108886719, loss=0.6655593514442444
I0301 15:39:29.884297 140525264946944 logging_writer.py:48] [351000] global_step=351000, grad_norm=4.6035871505737305, loss=0.6191059350967407
I0301 15:40:03.643803 140525256554240 logging_writer.py:48] [351100] global_step=351100, grad_norm=4.291257381439209, loss=0.6286014914512634
I0301 15:40:37.536856 140525264946944 logging_writer.py:48] [351200] global_step=351200, grad_norm=4.436674118041992, loss=0.6248708963394165
I0301 15:41:11.317674 140525256554240 logging_writer.py:48] [351300] global_step=351300, grad_norm=4.737445831298828, loss=0.6024689674377441
I0301 15:41:45.138052 140525264946944 logging_writer.py:48] [351400] global_step=351400, grad_norm=4.856099605560303, loss=0.6575932502746582
I0301 15:42:18.922196 140525256554240 logging_writer.py:48] [351500] global_step=351500, grad_norm=4.307961940765381, loss=0.6410310864448547
I0301 15:42:52.728694 140525264946944 logging_writer.py:48] [351600] global_step=351600, grad_norm=5.073543548583984, loss=0.7031053304672241
I0301 15:43:26.513064 140525256554240 logging_writer.py:48] [351700] global_step=351700, grad_norm=4.359187126159668, loss=0.6086449027061462
I0301 15:44:00.306545 140525264946944 logging_writer.py:48] [351800] global_step=351800, grad_norm=4.267125129699707, loss=0.6167454123497009
I0301 15:44:16.304782 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:44:22.605396 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:44:31.113585 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:44:33.416863 140688601454400 submission_runner.py:411] Time since start: 123068.49s, 	Step: 351849, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14718742668628693, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0444236993789673, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8209481239318848, 'test/num_examples': 10000, 'score': 118891.18662977219, 'total_duration': 123068.48770236969, 'accumulated_submission_time': 118891.18662977219, 'accumulated_eval_time': 4149.4816863536835, 'accumulated_logging_time': 15.740437507629395}
I0301 15:44:33.492832 140522605770496 logging_writer.py:48] [351849] accumulated_eval_time=4149.481686, accumulated_logging_time=15.740438, accumulated_submission_time=118891.186630, global_step=351849, preemption_count=0, score=118891.186630, test/accuracy=0.630100, test/loss=1.820948, test/num_examples=10000, total_duration=123068.487702, train/accuracy=0.960658, train/loss=0.147187, validation/accuracy=0.754780, validation/loss=1.044424, validation/num_examples=50000
I0301 15:44:51.039805 140523092305664 logging_writer.py:48] [351900] global_step=351900, grad_norm=4.461248397827148, loss=0.6727486252784729
I0301 15:45:24.789798 140522605770496 logging_writer.py:48] [352000] global_step=352000, grad_norm=4.366939544677734, loss=0.6244434118270874
I0301 15:45:58.560791 140523092305664 logging_writer.py:48] [352100] global_step=352100, grad_norm=3.8358657360076904, loss=0.5223379135131836
I0301 15:46:32.388080 140522605770496 logging_writer.py:48] [352200] global_step=352200, grad_norm=4.674436092376709, loss=0.6735601425170898
I0301 15:47:06.170016 140523092305664 logging_writer.py:48] [352300] global_step=352300, grad_norm=4.785098075866699, loss=0.6551325917243958
I0301 15:47:39.947386 140522605770496 logging_writer.py:48] [352400] global_step=352400, grad_norm=4.4894514083862305, loss=0.5407938361167908
I0301 15:48:13.747292 140523092305664 logging_writer.py:48] [352500] global_step=352500, grad_norm=4.613776683807373, loss=0.6167978048324585
I0301 15:48:47.490947 140522605770496 logging_writer.py:48] [352600] global_step=352600, grad_norm=4.792124271392822, loss=0.646136999130249
I0301 15:49:21.278390 140523092305664 logging_writer.py:48] [352700] global_step=352700, grad_norm=4.189478397369385, loss=0.5780434608459473
I0301 15:49:55.074384 140522605770496 logging_writer.py:48] [352800] global_step=352800, grad_norm=4.295626640319824, loss=0.6322134733200073
I0301 15:50:28.852641 140523092305664 logging_writer.py:48] [352900] global_step=352900, grad_norm=4.871513366699219, loss=0.6586340665817261
I0301 15:51:02.630691 140522605770496 logging_writer.py:48] [353000] global_step=353000, grad_norm=4.0744524002075195, loss=0.5794385671615601
I0301 15:51:36.408683 140523092305664 logging_writer.py:48] [353100] global_step=353100, grad_norm=4.8528852462768555, loss=0.6925801038742065
I0301 15:52:10.200599 140522605770496 logging_writer.py:48] [353200] global_step=353200, grad_norm=4.308560848236084, loss=0.6350319385528564
I0301 15:52:44.178885 140523092305664 logging_writer.py:48] [353300] global_step=353300, grad_norm=4.828897953033447, loss=0.6204878091812134
I0301 15:53:03.587341 140688601454400 spec.py:321] Evaluating on the training split.
I0301 15:53:09.635236 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 15:53:18.337291 140688601454400 spec.py:349] Evaluating on the test split.
I0301 15:53:20.617074 140688601454400 submission_runner.py:411] Time since start: 123595.69s, 	Step: 353359, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14895647764205933, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0429089069366455, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8223930597305298, 'test/num_examples': 10000, 'score': 119401.21796488762, 'total_duration': 123595.68791532516, 'accumulated_submission_time': 119401.21796488762, 'accumulated_eval_time': 4166.511382102966, 'accumulated_logging_time': 15.826533794403076}
I0301 15:53:20.697051 140525273339648 logging_writer.py:48] [353359] accumulated_eval_time=4166.511382, accumulated_logging_time=15.826534, accumulated_submission_time=119401.217965, global_step=353359, preemption_count=0, score=119401.217965, test/accuracy=0.630700, test/loss=1.822393, test/num_examples=10000, total_duration=123595.687915, train/accuracy=0.960579, train/loss=0.148956, validation/accuracy=0.755420, validation/loss=1.042909, validation/num_examples=50000
I0301 15:53:34.859491 140525281732352 logging_writer.py:48] [353400] global_step=353400, grad_norm=4.351803302764893, loss=0.6516839265823364
I0301 15:54:08.630810 140525273339648 logging_writer.py:48] [353500] global_step=353500, grad_norm=4.576205253601074, loss=0.6554611921310425
I0301 15:54:42.391377 140525281732352 logging_writer.py:48] [353600] global_step=353600, grad_norm=4.617704391479492, loss=0.5746867060661316
I0301 15:55:16.128968 140525273339648 logging_writer.py:48] [353700] global_step=353700, grad_norm=4.471307277679443, loss=0.5807193517684937
I0301 15:55:49.903163 140525281732352 logging_writer.py:48] [353800] global_step=353800, grad_norm=4.5887885093688965, loss=0.6712977290153503
I0301 15:56:23.709929 140525273339648 logging_writer.py:48] [353900] global_step=353900, grad_norm=4.249141216278076, loss=0.6041452288627625
I0301 15:56:57.469562 140525281732352 logging_writer.py:48] [354000] global_step=354000, grad_norm=4.182730197906494, loss=0.6065407395362854
I0301 15:57:31.177484 140525273339648 logging_writer.py:48] [354100] global_step=354100, grad_norm=4.799495220184326, loss=0.6710134744644165
I0301 15:58:04.934368 140525281732352 logging_writer.py:48] [354200] global_step=354200, grad_norm=4.690852165222168, loss=0.6007144451141357
I0301 15:58:38.889264 140525273339648 logging_writer.py:48] [354300] global_step=354300, grad_norm=4.59467077255249, loss=0.5945768356323242
I0301 15:59:12.638580 140525281732352 logging_writer.py:48] [354400] global_step=354400, grad_norm=4.492742538452148, loss=0.6242960691452026
I0301 15:59:46.422655 140525273339648 logging_writer.py:48] [354500] global_step=354500, grad_norm=4.371659755706787, loss=0.6176540851593018
I0301 16:00:20.188147 140525281732352 logging_writer.py:48] [354600] global_step=354600, grad_norm=4.5048627853393555, loss=0.5553749203681946
I0301 16:00:53.998903 140525273339648 logging_writer.py:48] [354700] global_step=354700, grad_norm=4.479428768157959, loss=0.6408263444900513
I0301 16:01:27.776969 140525281732352 logging_writer.py:48] [354800] global_step=354800, grad_norm=4.427504539489746, loss=0.6265043020248413
I0301 16:01:50.870708 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:01:56.852662 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:02:05.668972 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:02:07.917773 140688601454400 submission_runner.py:411] Time since start: 124122.99s, 	Step: 354870, 	{'train/accuracy': 0.9594626426696777, 'train/loss': 0.14952337741851807, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0451470613479614, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8234483003616333, 'test/num_examples': 10000, 'score': 119911.32834887505, 'total_duration': 124122.98861145973, 'accumulated_submission_time': 119911.32834887505, 'accumulated_eval_time': 4183.558404684067, 'accumulated_logging_time': 15.91768193244934}
I0301 16:02:07.996843 140522588985088 logging_writer.py:48] [354870] accumulated_eval_time=4183.558405, accumulated_logging_time=15.917682, accumulated_submission_time=119911.328349, global_step=354870, preemption_count=0, score=119911.328349, test/accuracy=0.630600, test/loss=1.823448, test/num_examples=10000, total_duration=124122.988611, train/accuracy=0.959463, train/loss=0.149523, validation/accuracy=0.754740, validation/loss=1.045147, validation/num_examples=50000
I0301 16:02:18.500867 140522597377792 logging_writer.py:48] [354900] global_step=354900, grad_norm=3.9639079570770264, loss=0.5647302269935608
I0301 16:02:52.198376 140522588985088 logging_writer.py:48] [355000] global_step=355000, grad_norm=5.131932735443115, loss=0.6748127341270447
I0301 16:03:25.972714 140522597377792 logging_writer.py:48] [355100] global_step=355100, grad_norm=4.2805328369140625, loss=0.5775637030601501
I0301 16:03:59.755601 140522588985088 logging_writer.py:48] [355200] global_step=355200, grad_norm=4.618402004241943, loss=0.6528037190437317
I0301 16:04:33.540442 140522597377792 logging_writer.py:48] [355300] global_step=355300, grad_norm=4.617383003234863, loss=0.601248025894165
I0301 16:05:07.350446 140522588985088 logging_writer.py:48] [355400] global_step=355400, grad_norm=4.671448230743408, loss=0.7043270468711853
I0301 16:05:41.125624 140522597377792 logging_writer.py:48] [355500] global_step=355500, grad_norm=4.546594619750977, loss=0.7029796838760376
I0301 16:06:14.914510 140522588985088 logging_writer.py:48] [355600] global_step=355600, grad_norm=4.26155424118042, loss=0.584242582321167
I0301 16:06:48.691749 140522597377792 logging_writer.py:48] [355700] global_step=355700, grad_norm=4.233365058898926, loss=0.5359529256820679
I0301 16:07:22.490418 140522588985088 logging_writer.py:48] [355800] global_step=355800, grad_norm=4.481881141662598, loss=0.6326214671134949
I0301 16:07:56.272668 140522597377792 logging_writer.py:48] [355900] global_step=355900, grad_norm=4.403467655181885, loss=0.5408354997634888
I0301 16:08:30.050280 140522588985088 logging_writer.py:48] [356000] global_step=356000, grad_norm=4.3043317794799805, loss=0.6467965841293335
I0301 16:09:03.820339 140522597377792 logging_writer.py:48] [356100] global_step=356100, grad_norm=4.359117031097412, loss=0.5865048766136169
I0301 16:09:37.646553 140522588985088 logging_writer.py:48] [356200] global_step=356200, grad_norm=4.62414026260376, loss=0.6333636045455933
I0301 16:10:11.433811 140522597377792 logging_writer.py:48] [356300] global_step=356300, grad_norm=4.4133477210998535, loss=0.6068033576011658
I0301 16:10:37.918935 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:10:43.902784 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:10:52.545102 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:10:55.118401 140688601454400 submission_runner.py:411] Time since start: 124650.19s, 	Step: 356380, 	{'train/accuracy': 0.9622528553009033, 'train/loss': 0.142277330160141, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0428621768951416, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8212215900421143, 'test/num_examples': 10000, 'score': 120421.18785190582, 'total_duration': 124650.18924379349, 'accumulated_submission_time': 120421.18785190582, 'accumulated_eval_time': 4200.7578365802765, 'accumulated_logging_time': 16.00704550743103}
I0301 16:10:55.197132 140525256554240 logging_writer.py:48] [356380] accumulated_eval_time=4200.757837, accumulated_logging_time=16.007046, accumulated_submission_time=120421.187852, global_step=356380, preemption_count=0, score=120421.187852, test/accuracy=0.631800, test/loss=1.821222, test/num_examples=10000, total_duration=124650.189244, train/accuracy=0.962253, train/loss=0.142277, validation/accuracy=0.755620, validation/loss=1.042862, validation/num_examples=50000
I0301 16:11:02.273923 140525264946944 logging_writer.py:48] [356400] global_step=356400, grad_norm=4.560021877288818, loss=0.6010973453521729
I0301 16:11:36.021026 140525256554240 logging_writer.py:48] [356500] global_step=356500, grad_norm=4.845000743865967, loss=0.6705779433250427
I0301 16:12:09.743169 140525264946944 logging_writer.py:48] [356600] global_step=356600, grad_norm=5.072632789611816, loss=0.6811050772666931
I0301 16:12:43.509361 140525256554240 logging_writer.py:48] [356700] global_step=356700, grad_norm=4.6464009284973145, loss=0.5813527703285217
I0301 16:13:17.326677 140525264946944 logging_writer.py:48] [356800] global_step=356800, grad_norm=4.56674337387085, loss=0.6422691345214844
I0301 16:13:51.091573 140525256554240 logging_writer.py:48] [356900] global_step=356900, grad_norm=4.587924003601074, loss=0.6089374423027039
I0301 16:14:24.859991 140525264946944 logging_writer.py:48] [357000] global_step=357000, grad_norm=4.556199550628662, loss=0.5710114240646362
I0301 16:14:58.625764 140525256554240 logging_writer.py:48] [357100] global_step=357100, grad_norm=4.3865275382995605, loss=0.6057085394859314
I0301 16:15:32.373620 140525264946944 logging_writer.py:48] [357200] global_step=357200, grad_norm=4.750734806060791, loss=0.6439078450202942
I0301 16:16:06.147617 140525256554240 logging_writer.py:48] [357300] global_step=357300, grad_norm=4.548679351806641, loss=0.6810556054115295
I0301 16:16:39.922373 140525264946944 logging_writer.py:48] [357400] global_step=357400, grad_norm=4.522777557373047, loss=0.6678261756896973
I0301 16:17:13.779617 140525256554240 logging_writer.py:48] [357500] global_step=357500, grad_norm=4.836164951324463, loss=0.66761314868927
I0301 16:17:47.581525 140525264946944 logging_writer.py:48] [357600] global_step=357600, grad_norm=4.259078502655029, loss=0.6517813801765442
I0301 16:18:21.358492 140525256554240 logging_writer.py:48] [357700] global_step=357700, grad_norm=4.613253593444824, loss=0.6649428009986877
I0301 16:18:55.158582 140525264946944 logging_writer.py:48] [357800] global_step=357800, grad_norm=4.091427803039551, loss=0.5634073615074158
I0301 16:19:25.359905 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:19:31.298773 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:19:39.863323 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:19:42.153015 140688601454400 submission_runner.py:411] Time since start: 125177.22s, 	Step: 357891, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14439095556735992, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.044643521308899, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8233301639556885, 'test/num_examples': 10000, 'score': 120931.28691577911, 'total_duration': 125177.22383451462, 'accumulated_submission_time': 120931.28691577911, 'accumulated_eval_time': 4217.550870895386, 'accumulated_logging_time': 16.096714735031128}
I0301 16:19:42.231337 140523092305664 logging_writer.py:48] [357891] accumulated_eval_time=4217.550871, accumulated_logging_time=16.096715, accumulated_submission_time=120931.286916, global_step=357891, preemption_count=0, score=120931.286916, test/accuracy=0.631200, test/loss=1.823330, test/num_examples=10000, total_duration=125177.223835, train/accuracy=0.961256, train/loss=0.144391, validation/accuracy=0.755320, validation/loss=1.044644, validation/num_examples=50000
I0301 16:19:45.617748 140523947947776 logging_writer.py:48] [357900] global_step=357900, grad_norm=4.310447692871094, loss=0.59096360206604
I0301 16:20:19.341129 140523092305664 logging_writer.py:48] [358000] global_step=358000, grad_norm=4.363699436187744, loss=0.6075985431671143
I0301 16:20:53.123755 140523947947776 logging_writer.py:48] [358100] global_step=358100, grad_norm=4.463298797607422, loss=0.6545788049697876
I0301 16:21:26.913376 140523092305664 logging_writer.py:48] [358200] global_step=358200, grad_norm=4.937991619110107, loss=0.5967695713043213
I0301 16:22:00.730990 140523947947776 logging_writer.py:48] [358300] global_step=358300, grad_norm=4.760007858276367, loss=0.6938921809196472
I0301 16:22:34.460583 140523092305664 logging_writer.py:48] [358400] global_step=358400, grad_norm=4.699531555175781, loss=0.6394599676132202
I0301 16:23:08.344443 140523947947776 logging_writer.py:48] [358500] global_step=358500, grad_norm=4.323240280151367, loss=0.6023212671279907
I0301 16:23:42.135526 140523092305664 logging_writer.py:48] [358600] global_step=358600, grad_norm=4.843597888946533, loss=0.6867510676383972
I0301 16:24:15.898636 140523947947776 logging_writer.py:48] [358700] global_step=358700, grad_norm=4.579063415527344, loss=0.625840425491333
I0301 16:24:49.661184 140523092305664 logging_writer.py:48] [358800] global_step=358800, grad_norm=4.3866496086120605, loss=0.5875913500785828
I0301 16:25:23.453571 140523947947776 logging_writer.py:48] [358900] global_step=358900, grad_norm=4.273090362548828, loss=0.6180508136749268
I0301 16:25:57.228565 140523092305664 logging_writer.py:48] [359000] global_step=359000, grad_norm=5.0257720947265625, loss=0.592389702796936
I0301 16:26:31.041080 140523947947776 logging_writer.py:48] [359100] global_step=359100, grad_norm=4.352653980255127, loss=0.6495108008384705
I0301 16:27:04.824670 140523092305664 logging_writer.py:48] [359200] global_step=359200, grad_norm=4.538394927978516, loss=0.6000516414642334
I0301 16:27:38.626602 140523947947776 logging_writer.py:48] [359300] global_step=359300, grad_norm=4.708589553833008, loss=0.664831817150116
I0301 16:28:12.398443 140523092305664 logging_writer.py:48] [359400] global_step=359400, grad_norm=5.010189533233643, loss=0.6625698804855347
I0301 16:28:12.406883 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:28:18.455665 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:28:27.004563 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:28:29.280764 140688601454400 submission_runner.py:411] Time since start: 125704.35s, 	Step: 359401, 	{'train/accuracy': 0.9583665132522583, 'train/loss': 0.1501764953136444, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0434049367904663, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8215309381484985, 'test/num_examples': 10000, 'score': 121441.40004301071, 'total_duration': 125704.35160374641, 'accumulated_submission_time': 121441.40004301071, 'accumulated_eval_time': 4234.424687862396, 'accumulated_logging_time': 16.18539547920227}
I0301 16:28:29.360732 140523092305664 logging_writer.py:48] [359401] accumulated_eval_time=4234.424688, accumulated_logging_time=16.185395, accumulated_submission_time=121441.400043, global_step=359401, preemption_count=0, score=121441.400043, test/accuracy=0.630300, test/loss=1.821531, test/num_examples=10000, total_duration=125704.351604, train/accuracy=0.958367, train/loss=0.150176, validation/accuracy=0.755000, validation/loss=1.043405, validation/num_examples=50000
I0301 16:29:03.094919 140525256554240 logging_writer.py:48] [359500] global_step=359500, grad_norm=4.789773464202881, loss=0.6836975812911987
I0301 16:29:36.936613 140523092305664 logging_writer.py:48] [359600] global_step=359600, grad_norm=4.555027484893799, loss=0.6921643614768982
I0301 16:30:10.719609 140525256554240 logging_writer.py:48] [359700] global_step=359700, grad_norm=4.329111099243164, loss=0.605743408203125
I0301 16:30:44.485715 140523092305664 logging_writer.py:48] [359800] global_step=359800, grad_norm=4.262136936187744, loss=0.6580106019973755
I0301 16:31:18.280371 140525256554240 logging_writer.py:48] [359900] global_step=359900, grad_norm=4.374380588531494, loss=0.6267054677009583
I0301 16:31:52.091279 140523092305664 logging_writer.py:48] [360000] global_step=360000, grad_norm=4.440274238586426, loss=0.5822470784187317
I0301 16:32:25.889812 140525256554240 logging_writer.py:48] [360100] global_step=360100, grad_norm=4.483090400695801, loss=0.6410933136940002
I0301 16:32:59.677927 140523092305664 logging_writer.py:48] [360200] global_step=360200, grad_norm=4.452658653259277, loss=0.5883573293685913
I0301 16:33:33.454134 140525256554240 logging_writer.py:48] [360300] global_step=360300, grad_norm=4.459783554077148, loss=0.5660502910614014
I0301 16:34:07.229532 140523092305664 logging_writer.py:48] [360400] global_step=360400, grad_norm=4.6926140785217285, loss=0.6259405016899109
I0301 16:34:41.023798 140525256554240 logging_writer.py:48] [360500] global_step=360500, grad_norm=5.116499900817871, loss=0.7253020405769348
I0301 16:35:14.810080 140523092305664 logging_writer.py:48] [360600] global_step=360600, grad_norm=4.34412956237793, loss=0.5300643444061279
I0301 16:35:48.647634 140525256554240 logging_writer.py:48] [360700] global_step=360700, grad_norm=4.430920124053955, loss=0.603975236415863
I0301 16:36:22.420008 140523092305664 logging_writer.py:48] [360800] global_step=360800, grad_norm=4.433883190155029, loss=0.6339919567108154
I0301 16:36:56.201717 140525256554240 logging_writer.py:48] [360900] global_step=360900, grad_norm=3.998908281326294, loss=0.575154185295105
I0301 16:36:59.386858 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:37:05.597527 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:37:14.039591 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:37:16.313210 140688601454400 submission_runner.py:411] Time since start: 126231.38s, 	Step: 360911, 	{'train/accuracy': 0.9622129797935486, 'train/loss': 0.14666838943958282, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0447399616241455, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8208733797073364, 'test/num_examples': 10000, 'score': 121951.36361050606, 'total_duration': 126231.38405561447, 'accumulated_submission_time': 121951.36361050606, 'accumulated_eval_time': 4251.351005554199, 'accumulated_logging_time': 16.275821685791016}
I0301 16:37:16.405731 140525264946944 logging_writer.py:48] [360911] accumulated_eval_time=4251.351006, accumulated_logging_time=16.275822, accumulated_submission_time=121951.363611, global_step=360911, preemption_count=0, score=121951.363611, test/accuracy=0.630500, test/loss=1.820873, test/num_examples=10000, total_duration=126231.384056, train/accuracy=0.962213, train/loss=0.146668, validation/accuracy=0.755000, validation/loss=1.044740, validation/num_examples=50000
I0301 16:37:46.715205 140525273339648 logging_writer.py:48] [361000] global_step=361000, grad_norm=4.513967037200928, loss=0.6536543965339661
I0301 16:38:20.483779 140525264946944 logging_writer.py:48] [361100] global_step=361100, grad_norm=4.469382286071777, loss=0.6010355949401855
I0301 16:38:54.282794 140525273339648 logging_writer.py:48] [361200] global_step=361200, grad_norm=4.995143890380859, loss=0.7204844951629639
I0301 16:39:28.050084 140525264946944 logging_writer.py:48] [361300] global_step=361300, grad_norm=4.755484104156494, loss=0.645886242389679
I0301 16:40:01.802230 140525273339648 logging_writer.py:48] [361400] global_step=361400, grad_norm=4.253036022186279, loss=0.6266363263130188
I0301 16:40:35.590594 140525264946944 logging_writer.py:48] [361500] global_step=361500, grad_norm=4.555093288421631, loss=0.6557839512825012
I0301 16:41:09.388221 140525273339648 logging_writer.py:48] [361600] global_step=361600, grad_norm=4.143441677093506, loss=0.5254795551300049
I0301 16:41:43.280644 140525264946944 logging_writer.py:48] [361700] global_step=361700, grad_norm=4.669550895690918, loss=0.6577868461608887
I0301 16:42:17.051006 140525273339648 logging_writer.py:48] [361800] global_step=361800, grad_norm=4.206511497497559, loss=0.5934023857116699
I0301 16:42:50.808577 140525264946944 logging_writer.py:48] [361900] global_step=361900, grad_norm=4.546477317810059, loss=0.6121646165847778
I0301 16:43:24.567733 140525273339648 logging_writer.py:48] [362000] global_step=362000, grad_norm=4.544574737548828, loss=0.6128237843513489
I0301 16:43:58.353460 140525264946944 logging_writer.py:48] [362100] global_step=362100, grad_norm=4.406899452209473, loss=0.6269909143447876
I0301 16:44:32.135546 140525273339648 logging_writer.py:48] [362200] global_step=362200, grad_norm=4.793529510498047, loss=0.6284480094909668
I0301 16:45:05.928264 140525264946944 logging_writer.py:48] [362300] global_step=362300, grad_norm=4.8590168952941895, loss=0.577650785446167
I0301 16:45:39.755840 140525273339648 logging_writer.py:48] [362400] global_step=362400, grad_norm=4.565146446228027, loss=0.6312302947044373
I0301 16:45:46.651770 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:45:52.629774 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:46:01.439921 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:46:03.820187 140688601454400 submission_runner.py:411] Time since start: 126758.89s, 	Step: 362422, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.15055051445960999, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0432157516479492, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8205649852752686, 'test/num_examples': 10000, 'score': 122461.54856848717, 'total_duration': 126758.89103007317, 'accumulated_submission_time': 122461.54856848717, 'accumulated_eval_time': 4268.519370794296, 'accumulated_logging_time': 16.37822437286377}
I0301 16:46:03.898014 140523092305664 logging_writer.py:48] [362422] accumulated_eval_time=4268.519371, accumulated_logging_time=16.378224, accumulated_submission_time=122461.548568, global_step=362422, preemption_count=0, score=122461.548568, test/accuracy=0.631300, test/loss=1.820565, test/num_examples=10000, total_duration=126758.891030, train/accuracy=0.960100, train/loss=0.150551, validation/accuracy=0.755420, validation/loss=1.043216, validation/num_examples=50000
I0301 16:46:30.539833 140523947947776 logging_writer.py:48] [362500] global_step=362500, grad_norm=4.452982425689697, loss=0.6533685326576233
I0301 16:47:04.274352 140523092305664 logging_writer.py:48] [362600] global_step=362600, grad_norm=4.193777084350586, loss=0.6339827179908752
I0301 16:47:38.088448 140523947947776 logging_writer.py:48] [362700] global_step=362700, grad_norm=4.4721174240112305, loss=0.6469020843505859
I0301 16:48:11.863768 140523092305664 logging_writer.py:48] [362800] global_step=362800, grad_norm=4.251076698303223, loss=0.648484468460083
I0301 16:48:45.622963 140523947947776 logging_writer.py:48] [362900] global_step=362900, grad_norm=4.537759304046631, loss=0.6346347332000732
I0301 16:49:19.388495 140523092305664 logging_writer.py:48] [363000] global_step=363000, grad_norm=4.448770046234131, loss=0.5959745645523071
I0301 16:49:53.141135 140523947947776 logging_writer.py:48] [363100] global_step=363100, grad_norm=4.426652908325195, loss=0.6020628213882446
I0301 16:50:26.926662 140523092305664 logging_writer.py:48] [363200] global_step=363200, grad_norm=4.21929407119751, loss=0.5735733509063721
I0301 16:51:00.699366 140523947947776 logging_writer.py:48] [363300] global_step=363300, grad_norm=4.71793794631958, loss=0.647813618183136
I0301 16:51:34.428771 140523092305664 logging_writer.py:48] [363400] global_step=363400, grad_norm=4.540447235107422, loss=0.573815107345581
I0301 16:52:08.152182 140523947947776 logging_writer.py:48] [363500] global_step=363500, grad_norm=4.521743297576904, loss=0.641784131526947
I0301 16:52:41.910087 140523092305664 logging_writer.py:48] [363600] global_step=363600, grad_norm=4.303824424743652, loss=0.5433017015457153
I0301 16:53:15.702800 140523947947776 logging_writer.py:48] [363700] global_step=363700, grad_norm=4.4907636642456055, loss=0.5765849351882935
I0301 16:53:49.635599 140523092305664 logging_writer.py:48] [363800] global_step=363800, grad_norm=4.400172233581543, loss=0.606736958026886
I0301 16:54:23.396960 140523947947776 logging_writer.py:48] [363900] global_step=363900, grad_norm=4.731828689575195, loss=0.6238963007926941
I0301 16:54:34.030881 140688601454400 spec.py:321] Evaluating on the training split.
I0301 16:54:40.060245 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 16:54:48.785090 140688601454400 spec.py:349] Evaluating on the test split.
I0301 16:54:51.055508 140688601454400 submission_runner.py:411] Time since start: 127286.13s, 	Step: 363933, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1459295153617859, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0441726446151733, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.821946144104004, 'test/num_examples': 10000, 'score': 122971.61852169037, 'total_duration': 127286.1263434887, 'accumulated_submission_time': 122971.61852169037, 'accumulated_eval_time': 4285.543945074081, 'accumulated_logging_time': 16.466397523880005}
I0301 16:54:51.138088 140522597377792 logging_writer.py:48] [363933] accumulated_eval_time=4285.543945, accumulated_logging_time=16.466398, accumulated_submission_time=122971.618522, global_step=363933, preemption_count=0, score=122971.618522, test/accuracy=0.630200, test/loss=1.821946, test/num_examples=10000, total_duration=127286.126343, train/accuracy=0.961217, train/loss=0.145930, validation/accuracy=0.755000, validation/loss=1.044173, validation/num_examples=50000
I0301 16:55:14.079770 140522605770496 logging_writer.py:48] [364000] global_step=364000, grad_norm=4.291474342346191, loss=0.5889953970909119
I0301 16:55:47.788079 140522597377792 logging_writer.py:48] [364100] global_step=364100, grad_norm=5.0157318115234375, loss=0.6498210430145264
I0301 16:56:21.570044 140522605770496 logging_writer.py:48] [364200] global_step=364200, grad_norm=4.802731990814209, loss=0.5985404253005981
I0301 16:56:55.358936 140522597377792 logging_writer.py:48] [364300] global_step=364300, grad_norm=4.156815528869629, loss=0.5888437032699585
I0301 16:57:29.117975 140522605770496 logging_writer.py:48] [364400] global_step=364400, grad_norm=4.1230692863464355, loss=0.631913959980011
I0301 16:58:02.871045 140522597377792 logging_writer.py:48] [364500] global_step=364500, grad_norm=4.639209747314453, loss=0.6002798080444336
I0301 16:58:36.661036 140522605770496 logging_writer.py:48] [364600] global_step=364600, grad_norm=4.718350410461426, loss=0.6724504828453064
I0301 16:59:10.459310 140522597377792 logging_writer.py:48] [364700] global_step=364700, grad_norm=4.3215227127075195, loss=0.6001207232475281
I0301 16:59:44.248140 140522605770496 logging_writer.py:48] [364800] global_step=364800, grad_norm=4.755619525909424, loss=0.641236424446106
I0301 17:00:18.153479 140522597377792 logging_writer.py:48] [364900] global_step=364900, grad_norm=4.5296454429626465, loss=0.6220181584358215
I0301 17:00:51.944042 140522605770496 logging_writer.py:48] [365000] global_step=365000, grad_norm=4.463311672210693, loss=0.5734467506408691
I0301 17:01:25.686954 140522597377792 logging_writer.py:48] [365100] global_step=365100, grad_norm=4.707340240478516, loss=0.6848998069763184
I0301 17:01:59.493581 140522605770496 logging_writer.py:48] [365200] global_step=365200, grad_norm=4.567588806152344, loss=0.6629613637924194
I0301 17:02:33.305522 140522597377792 logging_writer.py:48] [365300] global_step=365300, grad_norm=4.34091854095459, loss=0.6316653490066528
I0301 17:03:07.071138 140522605770496 logging_writer.py:48] [365400] global_step=365400, grad_norm=4.919163703918457, loss=0.7289633750915527
I0301 17:03:21.104584 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:03:27.116694 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:03:35.889961 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:03:38.144835 140688601454400 submission_runner.py:411] Time since start: 127813.22s, 	Step: 365443, 	{'train/accuracy': 0.9574099183082581, 'train/loss': 0.1546299159526825, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.0436898469924927, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8219724893569946, 'test/num_examples': 10000, 'score': 123481.5219783783, 'total_duration': 127813.21567821503, 'accumulated_submission_time': 123481.5219783783, 'accumulated_eval_time': 4302.584145069122, 'accumulated_logging_time': 16.560025453567505}
I0301 17:03:38.229123 140525273339648 logging_writer.py:48] [365443] accumulated_eval_time=4302.584145, accumulated_logging_time=16.560025, accumulated_submission_time=123481.521978, global_step=365443, preemption_count=0, score=123481.521978, test/accuracy=0.631200, test/loss=1.821972, test/num_examples=10000, total_duration=127813.215678, train/accuracy=0.957410, train/loss=0.154630, validation/accuracy=0.755380, validation/loss=1.043690, validation/num_examples=50000
I0301 17:03:57.829560 140525281732352 logging_writer.py:48] [365500] global_step=365500, grad_norm=4.371299743652344, loss=0.5683696269989014
I0301 17:04:31.551960 140525273339648 logging_writer.py:48] [365600] global_step=365600, grad_norm=4.8764872550964355, loss=0.6792547702789307
I0301 17:05:05.339456 140525281732352 logging_writer.py:48] [365700] global_step=365700, grad_norm=4.740137100219727, loss=0.7261884808540344
I0301 17:05:39.124475 140525273339648 logging_writer.py:48] [365800] global_step=365800, grad_norm=4.660059452056885, loss=0.6279968619346619
I0301 17:06:12.981945 140525281732352 logging_writer.py:48] [365900] global_step=365900, grad_norm=4.295711517333984, loss=0.5815321207046509
I0301 17:06:46.730775 140525273339648 logging_writer.py:48] [366000] global_step=366000, grad_norm=4.62306022644043, loss=0.59224534034729
I0301 17:07:20.506025 140525281732352 logging_writer.py:48] [366100] global_step=366100, grad_norm=4.462158679962158, loss=0.5902948379516602
I0301 17:07:54.272610 140525273339648 logging_writer.py:48] [366200] global_step=366200, grad_norm=5.070395469665527, loss=0.6636251211166382
I0301 17:08:28.056723 140525281732352 logging_writer.py:48] [366300] global_step=366300, grad_norm=4.9736223220825195, loss=0.6540869474411011
I0301 17:09:01.822036 140525273339648 logging_writer.py:48] [366400] global_step=366400, grad_norm=4.362870693206787, loss=0.5929879546165466
I0301 17:09:35.611586 140525281732352 logging_writer.py:48] [366500] global_step=366500, grad_norm=5.124826908111572, loss=0.6377630233764648
I0301 17:10:09.372730 140525273339648 logging_writer.py:48] [366600] global_step=366600, grad_norm=4.718874454498291, loss=0.6307248473167419
I0301 17:10:43.134634 140525281732352 logging_writer.py:48] [366700] global_step=366700, grad_norm=4.595113754272461, loss=0.6431400775909424
I0301 17:11:16.897983 140525273339648 logging_writer.py:48] [366800] global_step=366800, grad_norm=4.246492862701416, loss=0.6209479570388794
I0301 17:11:50.683739 140525281732352 logging_writer.py:48] [366900] global_step=366900, grad_norm=4.487307071685791, loss=0.6252214908599854
I0301 17:12:08.455361 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:12:14.544338 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:12:23.272172 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:12:25.577121 140688601454400 submission_runner.py:411] Time since start: 128340.65s, 	Step: 366954, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14792640507221222, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.043339490890503, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8199297189712524, 'test/num_examples': 10000, 'score': 123991.68515920639, 'total_duration': 128340.64795994759, 'accumulated_submission_time': 123991.68515920639, 'accumulated_eval_time': 4319.705858469009, 'accumulated_logging_time': 16.655137300491333}
I0301 17:12:25.655258 140522597377792 logging_writer.py:48] [366954] accumulated_eval_time=4319.705858, accumulated_logging_time=16.655137, accumulated_submission_time=123991.685159, global_step=366954, preemption_count=0, score=123991.685159, test/accuracy=0.631500, test/loss=1.819930, test/num_examples=10000, total_duration=128340.647960, train/accuracy=0.960379, train/loss=0.147926, validation/accuracy=0.754980, validation/loss=1.043339, validation/num_examples=50000
I0301 17:12:41.513371 140522605770496 logging_writer.py:48] [367000] global_step=367000, grad_norm=4.138176918029785, loss=0.6680134534835815
I0301 17:13:15.236853 140522597377792 logging_writer.py:48] [367100] global_step=367100, grad_norm=4.830751895904541, loss=0.6297987103462219
I0301 17:13:49.005511 140522605770496 logging_writer.py:48] [367200] global_step=367200, grad_norm=4.756441116333008, loss=0.6473847031593323
I0301 17:14:22.788065 140522597377792 logging_writer.py:48] [367300] global_step=367300, grad_norm=4.828426361083984, loss=0.6876780986785889
I0301 17:14:56.608616 140522605770496 logging_writer.py:48] [367400] global_step=367400, grad_norm=4.123600006103516, loss=0.5005161762237549
I0301 17:15:30.391008 140522597377792 logging_writer.py:48] [367500] global_step=367500, grad_norm=4.577672958374023, loss=0.5943117141723633
I0301 17:16:04.209537 140522605770496 logging_writer.py:48] [367600] global_step=367600, grad_norm=4.040100574493408, loss=0.5629032254219055
I0301 17:16:38.018298 140522597377792 logging_writer.py:48] [367700] global_step=367700, grad_norm=4.317486763000488, loss=0.602170467376709
I0301 17:17:11.830244 140522605770496 logging_writer.py:48] [367800] global_step=367800, grad_norm=4.348481178283691, loss=0.6412235498428345
I0301 17:17:45.609775 140522597377792 logging_writer.py:48] [367900] global_step=367900, grad_norm=4.405948162078857, loss=0.569632887840271
I0301 17:18:19.464328 140522605770496 logging_writer.py:48] [368000] global_step=368000, grad_norm=4.526240348815918, loss=0.6716769933700562
I0301 17:18:53.238971 140522597377792 logging_writer.py:48] [368100] global_step=368100, grad_norm=4.228084564208984, loss=0.6844297051429749
I0301 17:19:27.037552 140522605770496 logging_writer.py:48] [368200] global_step=368200, grad_norm=4.232728004455566, loss=0.5502170324325562
I0301 17:20:00.839763 140522597377792 logging_writer.py:48] [368300] global_step=368300, grad_norm=4.811739444732666, loss=0.6681645512580872
I0301 17:20:34.620948 140522605770496 logging_writer.py:48] [368400] global_step=368400, grad_norm=4.638164520263672, loss=0.6833837628364563
I0301 17:20:55.719639 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:21:01.817707 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:21:10.553081 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:21:12.826560 140688601454400 submission_runner.py:411] Time since start: 128867.90s, 	Step: 368464, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14658616483211517, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0438544750213623, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8230565786361694, 'test/num_examples': 10000, 'score': 124501.6871304512, 'total_duration': 128867.89737272263, 'accumulated_submission_time': 124501.6871304512, 'accumulated_eval_time': 4336.812728643417, 'accumulated_logging_time': 16.74353575706482}
I0301 17:21:12.901801 140525256554240 logging_writer.py:48] [368464] accumulated_eval_time=4336.812729, accumulated_logging_time=16.743536, accumulated_submission_time=124501.687130, global_step=368464, preemption_count=0, score=124501.687130, test/accuracy=0.631100, test/loss=1.823057, test/num_examples=10000, total_duration=128867.897373, train/accuracy=0.960758, train/loss=0.146586, validation/accuracy=0.755000, validation/loss=1.043854, validation/num_examples=50000
I0301 17:21:25.399314 140525264946944 logging_writer.py:48] [368500] global_step=368500, grad_norm=4.956281661987305, loss=0.6098639965057373
I0301 17:21:59.155711 140525256554240 logging_writer.py:48] [368600] global_step=368600, grad_norm=4.556972503662109, loss=0.6248078942298889
I0301 17:22:32.945121 140525264946944 logging_writer.py:48] [368700] global_step=368700, grad_norm=4.832498550415039, loss=0.607042670249939
I0301 17:23:06.729089 140525256554240 logging_writer.py:48] [368800] global_step=368800, grad_norm=4.967021465301514, loss=0.6289196014404297
I0301 17:23:40.537746 140525264946944 logging_writer.py:48] [368900] global_step=368900, grad_norm=4.241761207580566, loss=0.524705708026886
I0301 17:24:14.313941 140525256554240 logging_writer.py:48] [369000] global_step=369000, grad_norm=4.573510646820068, loss=0.5923517942428589
I0301 17:24:48.318150 140525264946944 logging_writer.py:48] [369100] global_step=369100, grad_norm=4.569281101226807, loss=0.6826713681221008
I0301 17:25:22.083718 140525256554240 logging_writer.py:48] [369200] global_step=369200, grad_norm=4.9395012855529785, loss=0.6323061585426331
I0301 17:25:55.857755 140525264946944 logging_writer.py:48] [369300] global_step=369300, grad_norm=4.477403163909912, loss=0.6465312242507935
I0301 17:26:29.656976 140525256554240 logging_writer.py:48] [369400] global_step=369400, grad_norm=4.601790428161621, loss=0.7160438299179077
I0301 17:27:03.436127 140525264946944 logging_writer.py:48] [369500] global_step=369500, grad_norm=4.438992977142334, loss=0.6874817609786987
I0301 17:27:37.210933 140525256554240 logging_writer.py:48] [369600] global_step=369600, grad_norm=4.849069595336914, loss=0.6635259389877319
I0301 17:28:10.994578 140525264946944 logging_writer.py:48] [369700] global_step=369700, grad_norm=4.612499237060547, loss=0.7144968509674072
I0301 17:28:44.788229 140525256554240 logging_writer.py:48] [369800] global_step=369800, grad_norm=4.792234897613525, loss=0.6559473872184753
I0301 17:29:18.563691 140525264946944 logging_writer.py:48] [369900] global_step=369900, grad_norm=4.499542236328125, loss=0.6454832553863525
I0301 17:29:43.028895 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:29:49.030652 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:29:57.774629 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:30:00.073334 140688601454400 submission_runner.py:411] Time since start: 129395.14s, 	Step: 369974, 	{'train/accuracy': 0.9587850570678711, 'train/loss': 0.15173330903053284, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0435724258422852, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8213281631469727, 'test/num_examples': 10000, 'score': 125011.75234508514, 'total_duration': 129395.14382815361, 'accumulated_submission_time': 125011.75234508514, 'accumulated_eval_time': 4353.856767416, 'accumulated_logging_time': 16.82906484603882}
I0301 17:30:00.150721 140522597377792 logging_writer.py:48] [369974] accumulated_eval_time=4353.856767, accumulated_logging_time=16.829065, accumulated_submission_time=125011.752345, global_step=369974, preemption_count=0, score=125011.752345, test/accuracy=0.630800, test/loss=1.821328, test/num_examples=10000, total_duration=129395.143828, train/accuracy=0.958785, train/loss=0.151733, validation/accuracy=0.755340, validation/loss=1.043572, validation/num_examples=50000
I0301 17:30:09.262205 140522605770496 logging_writer.py:48] [370000] global_step=370000, grad_norm=4.7902703285217285, loss=0.6673782467842102
I0301 17:30:43.070204 140522597377792 logging_writer.py:48] [370100] global_step=370100, grad_norm=4.701309680938721, loss=0.5792079567909241
I0301 17:31:16.839166 140522605770496 logging_writer.py:48] [370200] global_step=370200, grad_norm=4.413069248199463, loss=0.6670434474945068
I0301 17:31:50.622347 140522597377792 logging_writer.py:48] [370300] global_step=370300, grad_norm=4.027591228485107, loss=0.5819453597068787
I0301 17:32:24.405025 140522605770496 logging_writer.py:48] [370400] global_step=370400, grad_norm=4.172768592834473, loss=0.5880069732666016
I0301 17:32:58.178490 140522597377792 logging_writer.py:48] [370500] global_step=370500, grad_norm=4.693546772003174, loss=0.6700815558433533
I0301 17:33:31.962251 140522605770496 logging_writer.py:48] [370600] global_step=370600, grad_norm=4.85853910446167, loss=0.6000570058822632
I0301 17:34:05.752192 140522597377792 logging_writer.py:48] [370700] global_step=370700, grad_norm=4.474600315093994, loss=0.5852049589157104
I0301 17:34:39.531090 140522605770496 logging_writer.py:48] [370800] global_step=370800, grad_norm=4.582694053649902, loss=0.6688509583473206
I0301 17:35:13.316402 140522597377792 logging_writer.py:48] [370900] global_step=370900, grad_norm=4.177989959716797, loss=0.58278489112854
I0301 17:35:47.112286 140522605770496 logging_writer.py:48] [371000] global_step=371000, grad_norm=4.455674171447754, loss=0.6779199242591858
I0301 17:36:20.900167 140522597377792 logging_writer.py:48] [371100] global_step=371100, grad_norm=4.599879264831543, loss=0.6873478293418884
I0301 17:36:54.783104 140522605770496 logging_writer.py:48] [371200] global_step=371200, grad_norm=4.1638922691345215, loss=0.5604389905929565
I0301 17:37:28.560157 140522597377792 logging_writer.py:48] [371300] global_step=371300, grad_norm=4.4513678550720215, loss=0.6103659272193909
I0301 17:38:02.358044 140522605770496 logging_writer.py:48] [371400] global_step=371400, grad_norm=4.096255302429199, loss=0.5900583267211914
I0301 17:38:30.182013 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:38:36.180126 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:38:44.854089 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:38:47.196332 140688601454400 submission_runner.py:411] Time since start: 129922.27s, 	Step: 371484, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14732562005519867, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0428035259246826, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8214532136917114, 'test/num_examples': 10000, 'score': 125521.72247958183, 'total_duration': 129922.26717352867, 'accumulated_submission_time': 125521.72247958183, 'accumulated_eval_time': 4370.871035575867, 'accumulated_logging_time': 16.91614294052124}
I0301 17:38:47.277695 140525256554240 logging_writer.py:48] [371484] accumulated_eval_time=4370.871036, accumulated_logging_time=16.916143, accumulated_submission_time=125521.722480, global_step=371484, preemption_count=0, score=125521.722480, test/accuracy=0.631600, test/loss=1.821453, test/num_examples=10000, total_duration=129922.267174, train/accuracy=0.960619, train/loss=0.147326, validation/accuracy=0.755220, validation/loss=1.042804, validation/num_examples=50000
I0301 17:38:53.014917 140525264946944 logging_writer.py:48] [371500] global_step=371500, grad_norm=4.538961887359619, loss=0.6733291149139404
I0301 17:39:26.746220 140525256554240 logging_writer.py:48] [371600] global_step=371600, grad_norm=4.911783218383789, loss=0.6440468430519104
I0301 17:40:00.512169 140525264946944 logging_writer.py:48] [371700] global_step=371700, grad_norm=4.317805767059326, loss=0.6701954007148743
I0301 17:40:34.243700 140525256554240 logging_writer.py:48] [371800] global_step=371800, grad_norm=4.488080978393555, loss=0.5606096386909485
I0301 17:41:08.033133 140525264946944 logging_writer.py:48] [371900] global_step=371900, grad_norm=4.576740741729736, loss=0.6683045625686646
I0301 17:41:41.845348 140525256554240 logging_writer.py:48] [372000] global_step=372000, grad_norm=4.8449625968933105, loss=0.6650110483169556
I0301 17:42:15.633769 140525264946944 logging_writer.py:48] [372100] global_step=372100, grad_norm=4.370893478393555, loss=0.5871787071228027
I0301 17:42:49.535228 140525256554240 logging_writer.py:48] [372200] global_step=372200, grad_norm=4.685555458068848, loss=0.6363134384155273
I0301 17:43:23.324571 140525264946944 logging_writer.py:48] [372300] global_step=372300, grad_norm=4.538048267364502, loss=0.5466596484184265
I0301 17:43:57.109257 140525256554240 logging_writer.py:48] [372400] global_step=372400, grad_norm=4.205533504486084, loss=0.6029309034347534
I0301 17:44:30.879420 140525264946944 logging_writer.py:48] [372500] global_step=372500, grad_norm=4.76321268081665, loss=0.6940690279006958
I0301 17:45:04.708263 140525256554240 logging_writer.py:48] [372600] global_step=372600, grad_norm=4.520517349243164, loss=0.6768920421600342
I0301 17:45:38.484532 140525264946944 logging_writer.py:48] [372700] global_step=372700, grad_norm=4.743385314941406, loss=0.6550882458686829
I0301 17:46:12.221763 140525256554240 logging_writer.py:48] [372800] global_step=372800, grad_norm=4.29935884475708, loss=0.5572078227996826
I0301 17:46:45.964346 140525264946944 logging_writer.py:48] [372900] global_step=372900, grad_norm=4.429008960723877, loss=0.6228867173194885
I0301 17:47:17.202953 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:47:23.260915 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:47:31.961840 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:47:34.224415 140688601454400 submission_runner.py:411] Time since start: 130449.30s, 	Step: 372994, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.1476263850927353, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0441089868545532, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8195016384124756, 'test/num_examples': 10000, 'score': 126031.5854511261, 'total_duration': 130449.29526090622, 'accumulated_submission_time': 126031.5854511261, 'accumulated_eval_time': 4387.892452478409, 'accumulated_logging_time': 17.007720232009888}
I0301 17:47:34.303786 140522588985088 logging_writer.py:48] [372994] accumulated_eval_time=4387.892452, accumulated_logging_time=17.007720, accumulated_submission_time=126031.585451, global_step=372994, preemption_count=0, score=126031.585451, test/accuracy=0.630600, test/loss=1.819502, test/num_examples=10000, total_duration=130449.295261, train/accuracy=0.960439, train/loss=0.147626, validation/accuracy=0.754880, validation/loss=1.044109, validation/num_examples=50000
I0301 17:47:36.675195 140522597377792 logging_writer.py:48] [373000] global_step=373000, grad_norm=4.761336326599121, loss=0.6574046611785889
I0301 17:48:10.408037 140522588985088 logging_writer.py:48] [373100] global_step=373100, grad_norm=4.466333389282227, loss=0.6476817727088928
I0301 17:48:44.146395 140522597377792 logging_writer.py:48] [373200] global_step=373200, grad_norm=4.859304428100586, loss=0.6561754941940308
I0301 17:49:17.990601 140522588985088 logging_writer.py:48] [373300] global_step=373300, grad_norm=4.218313217163086, loss=0.5515928268432617
I0301 17:49:51.758294 140522597377792 logging_writer.py:48] [373400] global_step=373400, grad_norm=4.789742469787598, loss=0.5849501490592957
I0301 17:50:25.537775 140522588985088 logging_writer.py:48] [373500] global_step=373500, grad_norm=4.799805641174316, loss=0.6545364856719971
I0301 17:50:59.306557 140522597377792 logging_writer.py:48] [373600] global_step=373600, grad_norm=4.258083343505859, loss=0.6083682179450989
I0301 17:51:33.113877 140522588985088 logging_writer.py:48] [373700] global_step=373700, grad_norm=3.990518093109131, loss=0.5936716198921204
I0301 17:52:06.899948 140522597377792 logging_writer.py:48] [373800] global_step=373800, grad_norm=4.626220703125, loss=0.6456146836280823
I0301 17:52:40.656985 140522588985088 logging_writer.py:48] [373900] global_step=373900, grad_norm=4.557718753814697, loss=0.6108813285827637
I0301 17:53:14.455671 140522597377792 logging_writer.py:48] [374000] global_step=374000, grad_norm=4.432573318481445, loss=0.5216348767280579
I0301 17:53:48.232087 140522588985088 logging_writer.py:48] [374100] global_step=374100, grad_norm=4.425457954406738, loss=0.5614439845085144
I0301 17:54:22.009032 140522597377792 logging_writer.py:48] [374200] global_step=374200, grad_norm=4.540571689605713, loss=0.5567922592163086
I0301 17:54:55.858942 140522588985088 logging_writer.py:48] [374300] global_step=374300, grad_norm=4.534604549407959, loss=0.6135512590408325
I0301 17:55:29.621484 140522597377792 logging_writer.py:48] [374400] global_step=374400, grad_norm=4.91516637802124, loss=0.7338945865631104
I0301 17:56:03.414095 140522588985088 logging_writer.py:48] [374500] global_step=374500, grad_norm=4.442744731903076, loss=0.6410560011863708
I0301 17:56:04.237138 140688601454400 spec.py:321] Evaluating on the training split.
I0301 17:56:10.276452 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 17:56:18.987216 140688601454400 spec.py:349] Evaluating on the test split.
I0301 17:56:21.263067 140688601454400 submission_runner.py:411] Time since start: 130976.33s, 	Step: 374504, 	{'train/accuracy': 0.9590441584587097, 'train/loss': 0.1508360058069229, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0448273420333862, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8218950033187866, 'test/num_examples': 10000, 'score': 126541.45552277565, 'total_duration': 130976.33390402794, 'accumulated_submission_time': 126541.45552277565, 'accumulated_eval_time': 4404.918319940567, 'accumulated_logging_time': 17.09819459915161}
I0301 17:56:21.344128 140525264946944 logging_writer.py:48] [374504] accumulated_eval_time=4404.918320, accumulated_logging_time=17.098195, accumulated_submission_time=126541.455523, global_step=374504, preemption_count=0, score=126541.455523, test/accuracy=0.631400, test/loss=1.821895, test/num_examples=10000, total_duration=130976.333904, train/accuracy=0.959044, train/loss=0.150836, validation/accuracy=0.754980, validation/loss=1.044827, validation/num_examples=50000
I0301 17:56:54.068752 140525273339648 logging_writer.py:48] [374600] global_step=374600, grad_norm=4.578243732452393, loss=0.568092405796051
I0301 17:57:27.802613 140525264946944 logging_writer.py:48] [374700] global_step=374700, grad_norm=4.644394397735596, loss=0.6264314651489258
I0301 17:58:01.568791 140525273339648 logging_writer.py:48] [374800] global_step=374800, grad_norm=4.893625736236572, loss=0.6487562656402588
I0301 17:58:35.352505 140525264946944 logging_writer.py:48] [374900] global_step=374900, grad_norm=4.440666675567627, loss=0.6580080389976501
I0301 17:59:09.127243 140525273339648 logging_writer.py:48] [375000] global_step=375000, grad_norm=4.557539463043213, loss=0.5897244215011597
I0301 17:59:42.927585 140525264946944 logging_writer.py:48] [375100] global_step=375100, grad_norm=4.036845684051514, loss=0.5511915683746338
I0301 18:00:16.715160 140525273339648 logging_writer.py:48] [375200] global_step=375200, grad_norm=4.581729888916016, loss=0.6078731417655945
I0301 18:00:50.507941 140525264946944 logging_writer.py:48] [375300] global_step=375300, grad_norm=4.891362190246582, loss=0.7199491262435913
I0301 18:01:24.331942 140525273339648 logging_writer.py:48] [375400] global_step=375400, grad_norm=4.579064846038818, loss=0.6825094819068909
I0301 18:01:58.147370 140525264946944 logging_writer.py:48] [375500] global_step=375500, grad_norm=4.529575347900391, loss=0.623082160949707
I0301 18:02:31.942560 140525273339648 logging_writer.py:48] [375600] global_step=375600, grad_norm=4.606250762939453, loss=0.6257146596908569
I0301 18:03:05.769755 140525264946944 logging_writer.py:48] [375700] global_step=375700, grad_norm=4.412036895751953, loss=0.6592390537261963
I0301 18:03:39.541428 140525273339648 logging_writer.py:48] [375800] global_step=375800, grad_norm=4.7292022705078125, loss=0.6601743698120117
I0301 18:04:13.330173 140525264946944 logging_writer.py:48] [375900] global_step=375900, grad_norm=4.458888530731201, loss=0.6121380925178528
I0301 18:04:47.111388 140525273339648 logging_writer.py:48] [376000] global_step=376000, grad_norm=4.586936950683594, loss=0.6356064677238464
I0301 18:04:51.315382 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:04:57.476210 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:05:06.267988 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:05:08.559750 140688601454400 submission_runner.py:411] Time since start: 131503.63s, 	Step: 376014, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14662368595600128, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.0439865589141846, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8222171068191528, 'test/num_examples': 10000, 'score': 127051.36449313164, 'total_duration': 131503.63059139252, 'accumulated_submission_time': 127051.36449313164, 'accumulated_eval_time': 4422.162630319595, 'accumulated_logging_time': 17.189462661743164}
I0301 18:05:08.637232 140522605770496 logging_writer.py:48] [376014] accumulated_eval_time=4422.162630, accumulated_logging_time=17.189463, accumulated_submission_time=127051.364493, global_step=376014, preemption_count=0, score=127051.364493, test/accuracy=0.630700, test/loss=1.822217, test/num_examples=10000, total_duration=131503.630591, train/accuracy=0.961117, train/loss=0.146624, validation/accuracy=0.755380, validation/loss=1.043987, validation/num_examples=50000
I0301 18:05:38.001111 140523092305664 logging_writer.py:48] [376100] global_step=376100, grad_norm=4.844278335571289, loss=0.6551793217658997
I0301 18:06:11.749385 140522605770496 logging_writer.py:48] [376200] global_step=376200, grad_norm=4.5321831703186035, loss=0.7067949771881104
I0301 18:06:45.541704 140523092305664 logging_writer.py:48] [376300] global_step=376300, grad_norm=4.349759578704834, loss=0.6734614372253418
I0301 18:07:19.289824 140522605770496 logging_writer.py:48] [376400] global_step=376400, grad_norm=4.957488059997559, loss=0.6453266143798828
I0301 18:07:53.142642 140523092305664 logging_writer.py:48] [376500] global_step=376500, grad_norm=4.726309776306152, loss=0.6154080033302307
I0301 18:08:26.938343 140522605770496 logging_writer.py:48] [376600] global_step=376600, grad_norm=4.246066093444824, loss=0.5807358622550964
I0301 18:09:00.698168 140523092305664 logging_writer.py:48] [376700] global_step=376700, grad_norm=5.101686477661133, loss=0.6748232245445251
I0301 18:09:34.432729 140522605770496 logging_writer.py:48] [376800] global_step=376800, grad_norm=4.287271976470947, loss=0.5757632851600647
I0301 18:10:08.225412 140523092305664 logging_writer.py:48] [376900] global_step=376900, grad_norm=4.472466945648193, loss=0.5837860107421875
I0301 18:10:42.027933 140522605770496 logging_writer.py:48] [377000] global_step=377000, grad_norm=4.49631404876709, loss=0.6161678433418274
I0301 18:11:15.823869 140523092305664 logging_writer.py:48] [377100] global_step=377100, grad_norm=4.379655361175537, loss=0.6163384318351746
I0301 18:11:49.631097 140522605770496 logging_writer.py:48] [377200] global_step=377200, grad_norm=4.491283416748047, loss=0.6077954173088074
I0301 18:12:23.422680 140523092305664 logging_writer.py:48] [377300] global_step=377300, grad_norm=4.656883716583252, loss=0.6433817744255066
I0301 18:12:57.239503 140522605770496 logging_writer.py:48] [377400] global_step=377400, grad_norm=4.5518012046813965, loss=0.5765504837036133
I0301 18:13:31.100426 140523092305664 logging_writer.py:48] [377500] global_step=377500, grad_norm=4.464262962341309, loss=0.5996605157852173
I0301 18:13:38.692743 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:13:44.671359 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:13:53.306037 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:13:55.576173 140688601454400 submission_runner.py:411] Time since start: 132030.65s, 	Step: 377524, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.1494261622428894, 'validation/accuracy': 0.7545799612998962, 'validation/loss': 1.0441468954086304, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.822312355041504, 'test/num_examples': 10000, 'score': 127561.3567495346, 'total_duration': 132030.64701890945, 'accumulated_submission_time': 127561.3567495346, 'accumulated_eval_time': 4439.046007156372, 'accumulated_logging_time': 17.278419017791748}
I0301 18:13:55.657064 140522588985088 logging_writer.py:48] [377524] accumulated_eval_time=4439.046007, accumulated_logging_time=17.278419, accumulated_submission_time=127561.356750, global_step=377524, preemption_count=0, score=127561.356750, test/accuracy=0.630300, test/loss=1.822312, test/num_examples=10000, total_duration=132030.647019, train/accuracy=0.960320, train/loss=0.149426, validation/accuracy=0.754580, validation/loss=1.044147, validation/num_examples=50000
I0301 18:14:22.475029 140522597377792 logging_writer.py:48] [377600] global_step=377600, grad_norm=4.3066229820251465, loss=0.585918664932251
I0301 18:14:56.219670 140522588985088 logging_writer.py:48] [377700] global_step=377700, grad_norm=4.18939208984375, loss=0.6435500383377075
I0301 18:15:30.002009 140522597377792 logging_writer.py:48] [377800] global_step=377800, grad_norm=4.692258834838867, loss=0.6764355897903442
I0301 18:16:03.782199 140522588985088 logging_writer.py:48] [377900] global_step=377900, grad_norm=4.403678894042969, loss=0.5678175091743469
I0301 18:16:37.557705 140522597377792 logging_writer.py:48] [378000] global_step=378000, grad_norm=5.096881866455078, loss=0.6345242857933044
I0301 18:17:11.331657 140522588985088 logging_writer.py:48] [378100] global_step=378100, grad_norm=4.311732292175293, loss=0.6516138315200806
I0301 18:17:45.112474 140522597377792 logging_writer.py:48] [378200] global_step=378200, grad_norm=4.512260913848877, loss=0.6742269396781921
I0301 18:18:18.912461 140522588985088 logging_writer.py:48] [378300] global_step=378300, grad_norm=4.600997447967529, loss=0.6422737240791321
I0301 18:18:52.695002 140522597377792 logging_writer.py:48] [378400] global_step=378400, grad_norm=4.428154945373535, loss=0.6529964804649353
I0301 18:19:26.488909 140522588985088 logging_writer.py:48] [378500] global_step=378500, grad_norm=4.587964057922363, loss=0.6955662369728088
I0301 18:20:00.329130 140522597377792 logging_writer.py:48] [378600] global_step=378600, grad_norm=4.5146565437316895, loss=0.6241880059242249
I0301 18:20:34.119471 140522588985088 logging_writer.py:48] [378700] global_step=378700, grad_norm=4.6101861000061035, loss=0.6197170615196228
I0301 18:21:07.912925 140522597377792 logging_writer.py:48] [378800] global_step=378800, grad_norm=4.4385528564453125, loss=0.6518113017082214
I0301 18:21:41.697984 140522588985088 logging_writer.py:48] [378900] global_step=378900, grad_norm=4.954151153564453, loss=0.6364357471466064
I0301 18:22:15.481610 140522597377792 logging_writer.py:48] [379000] global_step=379000, grad_norm=4.205873489379883, loss=0.5730977058410645
I0301 18:22:25.747137 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:22:31.749496 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:22:40.244804 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:22:42.531510 140688601454400 submission_runner.py:411] Time since start: 132557.60s, 	Step: 379032, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.1464610993862152, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0441802740097046, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8221724033355713, 'test/num_examples': 10000, 'score': 128070.56897807121, 'total_duration': 132557.6023557186, 'accumulated_submission_time': 128070.56897807121, 'accumulated_eval_time': 4455.830331802368, 'accumulated_logging_time': 18.186094760894775}
I0301 18:22:42.613134 140522597377792 logging_writer.py:48] [379032] accumulated_eval_time=4455.830332, accumulated_logging_time=18.186095, accumulated_submission_time=128070.568978, global_step=379032, preemption_count=0, score=128070.568978, test/accuracy=0.631000, test/loss=1.822172, test/num_examples=10000, total_duration=132557.602356, train/accuracy=0.961416, train/loss=0.146461, validation/accuracy=0.755120, validation/loss=1.044180, validation/num_examples=50000
I0301 18:23:05.891574 140522605770496 logging_writer.py:48] [379100] global_step=379100, grad_norm=4.770814418792725, loss=0.601947546005249
I0301 18:23:39.641343 140522597377792 logging_writer.py:48] [379200] global_step=379200, grad_norm=4.4100236892700195, loss=0.5875262022018433
I0301 18:24:13.415273 140522605770496 logging_writer.py:48] [379300] global_step=379300, grad_norm=4.105559825897217, loss=0.5947900414466858
I0301 18:24:47.196075 140522597377792 logging_writer.py:48] [379400] global_step=379400, grad_norm=4.127349376678467, loss=0.6624706983566284
I0301 18:25:20.912449 140522605770496 logging_writer.py:48] [379500] global_step=379500, grad_norm=4.189748287200928, loss=0.575520396232605
I0301 18:25:54.782488 140522597377792 logging_writer.py:48] [379600] global_step=379600, grad_norm=4.550543785095215, loss=0.6460951566696167
I0301 18:26:28.544546 140522605770496 logging_writer.py:48] [379700] global_step=379700, grad_norm=4.655651092529297, loss=0.5553810596466064
I0301 18:27:02.311545 140522597377792 logging_writer.py:48] [379800] global_step=379800, grad_norm=4.658205032348633, loss=0.6147763729095459
I0301 18:27:36.134318 140522605770496 logging_writer.py:48] [379900] global_step=379900, grad_norm=4.935426235198975, loss=0.6247155666351318
I0301 18:28:09.920177 140522597377792 logging_writer.py:48] [380000] global_step=380000, grad_norm=4.464745998382568, loss=0.6347065567970276
I0301 18:28:43.744290 140522605770496 logging_writer.py:48] [380100] global_step=380100, grad_norm=4.956603050231934, loss=0.6361830234527588
I0301 18:29:17.515679 140522597377792 logging_writer.py:48] [380200] global_step=380200, grad_norm=4.491465091705322, loss=0.6960199475288391
I0301 18:29:51.300006 140522605770496 logging_writer.py:48] [380300] global_step=380300, grad_norm=4.267800807952881, loss=0.5992496013641357
I0301 18:30:25.080486 140522597377792 logging_writer.py:48] [380400] global_step=380400, grad_norm=4.479119300842285, loss=0.694410502910614
I0301 18:30:58.807765 140522605770496 logging_writer.py:48] [380500] global_step=380500, grad_norm=4.414218425750732, loss=0.5827481746673584
I0301 18:31:12.819003 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:31:18.800477 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:31:27.600436 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:31:29.866706 140688601454400 submission_runner.py:411] Time since start: 133084.94s, 	Step: 380543, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14477674663066864, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0452173948287964, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8225467205047607, 'test/num_examples': 10000, 'score': 128580.71305727959, 'total_duration': 133084.9375550747, 'accumulated_submission_time': 128580.71305727959, 'accumulated_eval_time': 4472.877988100052, 'accumulated_logging_time': 18.277392625808716}
I0301 18:31:29.950637 140525273339648 logging_writer.py:48] [380543] accumulated_eval_time=4472.877988, accumulated_logging_time=18.277393, accumulated_submission_time=128580.713057, global_step=380543, preemption_count=0, score=128580.713057, test/accuracy=0.630200, test/loss=1.822547, test/num_examples=10000, total_duration=133084.937555, train/accuracy=0.961356, train/loss=0.144777, validation/accuracy=0.755420, validation/loss=1.045217, validation/num_examples=50000
I0301 18:31:49.497572 140525281732352 logging_writer.py:48] [380600] global_step=380600, grad_norm=4.654422760009766, loss=0.6769846677780151
I0301 18:32:23.309659 140525273339648 logging_writer.py:48] [380700] global_step=380700, grad_norm=4.896900653839111, loss=0.5713654160499573
I0301 18:32:57.085392 140525281732352 logging_writer.py:48] [380800] global_step=380800, grad_norm=4.578242301940918, loss=0.6956788897514343
I0301 18:33:30.855196 140525273339648 logging_writer.py:48] [380900] global_step=380900, grad_norm=4.430421352386475, loss=0.6104172468185425
I0301 18:34:04.643108 140525281732352 logging_writer.py:48] [381000] global_step=381000, grad_norm=4.44490385055542, loss=0.6019600033760071
I0301 18:34:38.434014 140525273339648 logging_writer.py:48] [381100] global_step=381100, grad_norm=4.8018670082092285, loss=0.6567332744598389
I0301 18:35:12.170816 140525281732352 logging_writer.py:48] [381200] global_step=381200, grad_norm=4.563783645629883, loss=0.6080962419509888
I0301 18:35:45.937371 140525273339648 logging_writer.py:48] [381300] global_step=381300, grad_norm=4.5422492027282715, loss=0.5971778035163879
I0301 18:36:19.716224 140525281732352 logging_writer.py:48] [381400] global_step=381400, grad_norm=4.852166175842285, loss=0.675342857837677
I0301 18:36:53.510293 140525273339648 logging_writer.py:48] [381500] global_step=381500, grad_norm=4.181782245635986, loss=0.5729305148124695
I0301 18:37:27.312556 140525281732352 logging_writer.py:48] [381600] global_step=381600, grad_norm=4.621558666229248, loss=0.5898351669311523
I0301 18:38:01.121735 140525273339648 logging_writer.py:48] [381700] global_step=381700, grad_norm=4.911960601806641, loss=0.663321852684021
I0301 18:38:34.960415 140525281732352 logging_writer.py:48] [381800] global_step=381800, grad_norm=4.801918983459473, loss=0.6712108850479126
I0301 18:39:08.744261 140525273339648 logging_writer.py:48] [381900] global_step=381900, grad_norm=4.358126163482666, loss=0.5899344086647034
I0301 18:39:42.493620 140525281732352 logging_writer.py:48] [382000] global_step=382000, grad_norm=4.758957386016846, loss=0.6721934676170349
I0301 18:40:00.178546 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:40:06.299665 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:40:14.971643 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:40:17.233780 140688601454400 submission_runner.py:411] Time since start: 133612.30s, 	Step: 382054, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.14934468269348145, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.0439260005950928, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.822293996810913, 'test/num_examples': 10000, 'score': 129090.87953257561, 'total_duration': 133612.30461883545, 'accumulated_submission_time': 129090.87953257561, 'accumulated_eval_time': 4489.933167695999, 'accumulated_logging_time': 18.371665239334106}
I0301 18:40:17.315517 140523947947776 logging_writer.py:48] [382054] accumulated_eval_time=4489.933168, accumulated_logging_time=18.371665, accumulated_submission_time=129090.879533, global_step=382054, preemption_count=0, score=129090.879533, test/accuracy=0.631200, test/loss=1.822294, test/num_examples=10000, total_duration=133612.304619, train/accuracy=0.959762, train/loss=0.149345, validation/accuracy=0.755200, validation/loss=1.043926, validation/num_examples=50000
I0301 18:40:33.195196 140525281732352 logging_writer.py:48] [382100] global_step=382100, grad_norm=4.416130542755127, loss=0.643201470375061
I0301 18:41:06.897267 140523947947776 logging_writer.py:48] [382200] global_step=382200, grad_norm=4.516718864440918, loss=0.7109053730964661
I0301 18:41:40.646936 140525281732352 logging_writer.py:48] [382300] global_step=382300, grad_norm=4.5728607177734375, loss=0.6766941547393799
I0301 18:42:14.423008 140523947947776 logging_writer.py:48] [382400] global_step=382400, grad_norm=4.438531875610352, loss=0.5716075897216797
I0301 18:42:48.204257 140525281732352 logging_writer.py:48] [382500] global_step=382500, grad_norm=4.24838399887085, loss=0.6390931606292725
I0301 18:43:21.984172 140523947947776 logging_writer.py:48] [382600] global_step=382600, grad_norm=4.777601718902588, loss=0.6299703121185303
I0301 18:43:55.765194 140525281732352 logging_writer.py:48] [382700] global_step=382700, grad_norm=4.599664211273193, loss=0.6338668465614319
I0301 18:44:29.597476 140523947947776 logging_writer.py:48] [382800] global_step=382800, grad_norm=4.617586612701416, loss=0.6583319306373596
I0301 18:45:03.376298 140525281732352 logging_writer.py:48] [382900] global_step=382900, grad_norm=5.032540798187256, loss=0.6237784624099731
I0301 18:45:37.167502 140523947947776 logging_writer.py:48] [383000] global_step=383000, grad_norm=4.305663108825684, loss=0.6537817716598511
I0301 18:46:10.940225 140525281732352 logging_writer.py:48] [383100] global_step=383100, grad_norm=4.752148628234863, loss=0.6123430728912354
I0301 18:46:44.687727 140523947947776 logging_writer.py:48] [383200] global_step=383200, grad_norm=4.523291110992432, loss=0.5927021503448486
I0301 18:47:18.462305 140525281732352 logging_writer.py:48] [383300] global_step=383300, grad_norm=4.81019926071167, loss=0.6416362524032593
I0301 18:47:52.243672 140523947947776 logging_writer.py:48] [383400] global_step=383400, grad_norm=4.146945476531982, loss=0.6085942983627319
I0301 18:48:26.008174 140525281732352 logging_writer.py:48] [383500] global_step=383500, grad_norm=4.2201080322265625, loss=0.5622876882553101
I0301 18:48:47.440865 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:48:54.265578 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:49:03.035451 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:49:05.431787 140688601454400 submission_runner.py:411] Time since start: 134140.50s, 	Step: 383565, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14891788363456726, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0436407327651978, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8230477571487427, 'test/num_examples': 10000, 'score': 129600.94166183472, 'total_duration': 134140.50261998177, 'accumulated_submission_time': 129600.94166183472, 'accumulated_eval_time': 4507.924045085907, 'accumulated_logging_time': 18.46406841278076}
I0301 18:49:05.510329 140522605770496 logging_writer.py:48] [383565] accumulated_eval_time=4507.924045, accumulated_logging_time=18.464068, accumulated_submission_time=129600.941662, global_step=383565, preemption_count=0, score=129600.941662, test/accuracy=0.631100, test/loss=1.823048, test/num_examples=10000, total_duration=134140.502620, train/accuracy=0.959981, train/loss=0.148918, validation/accuracy=0.754980, validation/loss=1.043641, validation/num_examples=50000
I0301 18:49:17.651808 140523092305664 logging_writer.py:48] [383600] global_step=383600, grad_norm=4.382383823394775, loss=0.6152930855751038
I0301 18:49:51.437965 140522605770496 logging_writer.py:48] [383700] global_step=383700, grad_norm=4.483423233032227, loss=0.6288639307022095
I0301 18:50:25.206085 140523092305664 logging_writer.py:48] [383800] global_step=383800, grad_norm=4.511519432067871, loss=0.6240860223770142
I0301 18:50:59.095111 140522605770496 logging_writer.py:48] [383900] global_step=383900, grad_norm=4.451632022857666, loss=0.5776085257530212
I0301 18:51:32.873105 140523092305664 logging_writer.py:48] [384000] global_step=384000, grad_norm=4.66849422454834, loss=0.6308862566947937
I0301 18:52:06.699234 140522605770496 logging_writer.py:48] [384100] global_step=384100, grad_norm=4.693481922149658, loss=0.5477603077888489
I0301 18:52:40.499743 140523092305664 logging_writer.py:48] [384200] global_step=384200, grad_norm=4.3085784912109375, loss=0.568864107131958
I0301 18:53:14.305010 140522605770496 logging_writer.py:48] [384300] global_step=384300, grad_norm=4.835992336273193, loss=0.627020001411438
I0301 18:53:48.085761 140523092305664 logging_writer.py:48] [384400] global_step=384400, grad_norm=4.4824934005737305, loss=0.6642472147941589
I0301 18:54:21.907360 140522605770496 logging_writer.py:48] [384500] global_step=384500, grad_norm=4.578662872314453, loss=0.6271315217018127
I0301 18:54:55.690365 140523092305664 logging_writer.py:48] [384600] global_step=384600, grad_norm=4.910858631134033, loss=0.6745724081993103
I0301 18:55:29.499338 140522605770496 logging_writer.py:48] [384700] global_step=384700, grad_norm=4.357815265655518, loss=0.6595897674560547
I0301 18:56:03.279217 140523092305664 logging_writer.py:48] [384800] global_step=384800, grad_norm=4.752191543579102, loss=0.6006346940994263
I0301 18:56:37.098151 140522605770496 logging_writer.py:48] [384900] global_step=384900, grad_norm=4.062014102935791, loss=0.5197516083717346
I0301 18:57:10.951359 140523092305664 logging_writer.py:48] [385000] global_step=385000, grad_norm=5.377037048339844, loss=0.6690889596939087
I0301 18:57:35.437160 140688601454400 spec.py:321] Evaluating on the training split.
I0301 18:57:41.434931 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 18:57:50.062435 140688601454400 spec.py:349] Evaluating on the test split.
I0301 18:57:52.329204 140688601454400 submission_runner.py:411] Time since start: 134667.40s, 	Step: 385074, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14889252185821533, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0445308685302734, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8222934007644653, 'test/num_examples': 10000, 'score': 130110.80349063873, 'total_duration': 134667.40004825592, 'accumulated_submission_time': 130110.80349063873, 'accumulated_eval_time': 4524.816039323807, 'accumulated_logging_time': 18.555992364883423}
I0301 18:57:52.410389 140522597377792 logging_writer.py:48] [385074] accumulated_eval_time=4524.816039, accumulated_logging_time=18.555992, accumulated_submission_time=130110.803491, global_step=385074, preemption_count=0, score=130110.803491, test/accuracy=0.630100, test/loss=1.822293, test/num_examples=10000, total_duration=134667.400048, train/accuracy=0.959961, train/loss=0.148893, validation/accuracy=0.755100, validation/loss=1.044531, validation/num_examples=50000
I0301 18:58:01.529331 140522605770496 logging_writer.py:48] [385100] global_step=385100, grad_norm=4.346492767333984, loss=0.6372143626213074
I0301 18:58:35.265696 140522597377792 logging_writer.py:48] [385200] global_step=385200, grad_norm=4.526610851287842, loss=0.6875306963920593
I0301 18:59:09.003034 140522605770496 logging_writer.py:48] [385300] global_step=385300, grad_norm=4.71068811416626, loss=0.6761597394943237
I0301 18:59:42.795149 140522597377792 logging_writer.py:48] [385400] global_step=385400, grad_norm=4.324012756347656, loss=0.5655112266540527
I0301 19:00:16.595015 140522605770496 logging_writer.py:48] [385500] global_step=385500, grad_norm=4.264409065246582, loss=0.6353657841682434
I0301 19:00:50.379401 140522597377792 logging_writer.py:48] [385600] global_step=385600, grad_norm=4.792458534240723, loss=0.684283435344696
I0301 19:01:24.174825 140522605770496 logging_writer.py:48] [385700] global_step=385700, grad_norm=4.018638610839844, loss=0.59758460521698
I0301 19:01:57.941426 140522597377792 logging_writer.py:48] [385800] global_step=385800, grad_norm=4.336070537567139, loss=0.5591038465499878
I0301 19:02:31.720400 140522605770496 logging_writer.py:48] [385900] global_step=385900, grad_norm=4.372886657714844, loss=0.6146138310432434
I0301 19:03:05.545261 140522597377792 logging_writer.py:48] [386000] global_step=386000, grad_norm=4.234354496002197, loss=0.5935267210006714
I0301 19:03:39.333482 140522605770496 logging_writer.py:48] [386100] global_step=386100, grad_norm=4.328024864196777, loss=0.5967192649841309
I0301 19:04:13.108853 140522597377792 logging_writer.py:48] [386200] global_step=386200, grad_norm=4.6039934158325195, loss=0.6206084489822388
I0301 19:04:46.893934 140522605770496 logging_writer.py:48] [386300] global_step=386300, grad_norm=4.432837963104248, loss=0.6128309369087219
I0301 19:05:20.664552 140522597377792 logging_writer.py:48] [386400] global_step=386400, grad_norm=4.27938175201416, loss=0.6260466575622559
I0301 19:05:54.448117 140522605770496 logging_writer.py:48] [386500] global_step=386500, grad_norm=4.236632347106934, loss=0.5643874406814575
I0301 19:06:22.643563 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:06:28.650043 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:06:37.355976 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:06:39.645342 140688601454400 submission_runner.py:411] Time since start: 135194.72s, 	Step: 386585, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.14861316978931427, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0445588827133179, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8213427066802979, 'test/num_examples': 10000, 'score': 130620.97541832924, 'total_duration': 135194.71618199348, 'accumulated_submission_time': 130620.97541832924, 'accumulated_eval_time': 4541.817780017853, 'accumulated_logging_time': 18.64685320854187}
I0301 19:06:39.729228 140523092305664 logging_writer.py:48] [386585] accumulated_eval_time=4541.817780, accumulated_logging_time=18.646853, accumulated_submission_time=130620.975418, global_step=386585, preemption_count=0, score=130620.975418, test/accuracy=0.630500, test/loss=1.821343, test/num_examples=10000, total_duration=135194.716182, train/accuracy=0.960140, train/loss=0.148613, validation/accuracy=0.755220, validation/loss=1.044559, validation/num_examples=50000
I0301 19:06:45.125366 140523947947776 logging_writer.py:48] [386600] global_step=386600, grad_norm=4.386407852172852, loss=0.632245659828186
I0301 19:07:18.860922 140523092305664 logging_writer.py:48] [386700] global_step=386700, grad_norm=4.3817644119262695, loss=0.6633602976799011
I0301 19:07:52.670234 140523947947776 logging_writer.py:48] [386800] global_step=386800, grad_norm=4.245788097381592, loss=0.5778329372406006
I0301 19:08:26.431030 140523092305664 logging_writer.py:48] [386900] global_step=386900, grad_norm=4.562402248382568, loss=0.6862080097198486
I0301 19:09:00.262296 140523947947776 logging_writer.py:48] [387000] global_step=387000, grad_norm=4.287187099456787, loss=0.5651381015777588
I0301 19:09:34.111217 140523092305664 logging_writer.py:48] [387100] global_step=387100, grad_norm=4.555301666259766, loss=0.644419252872467
I0301 19:10:07.923152 140523947947776 logging_writer.py:48] [387200] global_step=387200, grad_norm=4.7393717765808105, loss=0.6276832222938538
I0301 19:10:41.700635 140523092305664 logging_writer.py:48] [387300] global_step=387300, grad_norm=4.434650421142578, loss=0.6140689253807068
I0301 19:11:15.505203 140523947947776 logging_writer.py:48] [387400] global_step=387400, grad_norm=4.020698547363281, loss=0.5986768007278442
I0301 19:11:49.303181 140523092305664 logging_writer.py:48] [387500] global_step=387500, grad_norm=4.228806018829346, loss=0.627629816532135
I0301 19:12:23.116081 140523947947776 logging_writer.py:48] [387600] global_step=387600, grad_norm=4.391686916351318, loss=0.651313066482544
I0301 19:12:56.900183 140523092305664 logging_writer.py:48] [387700] global_step=387700, grad_norm=4.690603733062744, loss=0.6897909641265869
I0301 19:13:30.722181 140523947947776 logging_writer.py:48] [387800] global_step=387800, grad_norm=4.848541736602783, loss=0.6141945123672485
I0301 19:14:04.496835 140523092305664 logging_writer.py:48] [387900] global_step=387900, grad_norm=4.449375629425049, loss=0.6059322357177734
I0301 19:14:38.315757 140523947947776 logging_writer.py:48] [388000] global_step=388000, grad_norm=4.247711181640625, loss=0.5551104545593262
I0301 19:15:09.870940 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:15:16.293542 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:15:24.900924 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:15:27.184143 140688601454400 submission_runner.py:411] Time since start: 135722.25s, 	Step: 388095, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.1466667354106903, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0440008640289307, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.821485996246338, 'test/num_examples': 10000, 'score': 131131.05405449867, 'total_duration': 135722.25498723984, 'accumulated_submission_time': 131131.05405449867, 'accumulated_eval_time': 4559.130956888199, 'accumulated_logging_time': 18.741585731506348}
I0301 19:15:27.276709 140525264946944 logging_writer.py:48] [388095] accumulated_eval_time=4559.130957, accumulated_logging_time=18.741586, accumulated_submission_time=131131.054054, global_step=388095, preemption_count=0, score=131131.054054, test/accuracy=0.630600, test/loss=1.821486, test/num_examples=10000, total_duration=135722.254987, train/accuracy=0.961376, train/loss=0.146667, validation/accuracy=0.755620, validation/loss=1.044001, validation/num_examples=50000
I0301 19:15:29.334455 140525273339648 logging_writer.py:48] [388100] global_step=388100, grad_norm=4.7566142082214355, loss=0.6177045702934265
I0301 19:16:03.048295 140525264946944 logging_writer.py:48] [388200] global_step=388200, grad_norm=4.773277282714844, loss=0.639806866645813
I0301 19:16:36.821703 140525273339648 logging_writer.py:48] [388300] global_step=388300, grad_norm=4.581729888916016, loss=0.6496859788894653
I0301 19:17:10.596114 140525264946944 logging_writer.py:48] [388400] global_step=388400, grad_norm=4.404065132141113, loss=0.6156607866287231
I0301 19:17:44.401832 140525273339648 logging_writer.py:48] [388500] global_step=388500, grad_norm=4.572812080383301, loss=0.7069579362869263
I0301 19:18:18.194786 140525264946944 logging_writer.py:48] [388600] global_step=388600, grad_norm=4.878991603851318, loss=0.6582481861114502
I0301 19:18:51.997014 140525273339648 logging_writer.py:48] [388700] global_step=388700, grad_norm=4.490422248840332, loss=0.6203259229660034
I0301 19:19:25.799723 140525264946944 logging_writer.py:48] [388800] global_step=388800, grad_norm=4.507380962371826, loss=0.611353874206543
I0301 19:19:59.550131 140525273339648 logging_writer.py:48] [388900] global_step=388900, grad_norm=4.440835952758789, loss=0.6281634569168091
I0301 19:20:33.312156 140525264946944 logging_writer.py:48] [389000] global_step=389000, grad_norm=4.411828994750977, loss=0.6495764255523682
I0301 19:21:07.107692 140525273339648 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.516657829284668, loss=0.6088979840278625
I0301 19:21:41.012459 140525264946944 logging_writer.py:48] [389200] global_step=389200, grad_norm=4.5150675773620605, loss=0.585058331489563
I0301 19:22:14.797733 140525273339648 logging_writer.py:48] [389300] global_step=389300, grad_norm=4.4649457931518555, loss=0.6208163499832153
I0301 19:22:48.576296 140525264946944 logging_writer.py:48] [389400] global_step=389400, grad_norm=4.37747049331665, loss=0.6298677325248718
I0301 19:23:22.293716 140525273339648 logging_writer.py:48] [389500] global_step=389500, grad_norm=5.116447448730469, loss=0.7402723431587219
I0301 19:23:56.069510 140525264946944 logging_writer.py:48] [389600] global_step=389600, grad_norm=4.567562103271484, loss=0.6390087604522705
I0301 19:23:57.232792 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:24:03.349603 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:24:12.018503 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:24:14.314173 140688601454400 submission_runner.py:411] Time since start: 136249.38s, 	Step: 389605, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.1472872942686081, 'validation/accuracy': 0.755079984664917, 'validation/loss': 1.043853998184204, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8217018842697144, 'test/num_examples': 10000, 'score': 131640.9479010105, 'total_duration': 136249.3849053383, 'accumulated_submission_time': 131640.9479010105, 'accumulated_eval_time': 4576.21217250824, 'accumulated_logging_time': 18.844163179397583}
I0301 19:24:14.399885 140522597377792 logging_writer.py:48] [389605] accumulated_eval_time=4576.212173, accumulated_logging_time=18.844163, accumulated_submission_time=131640.947901, global_step=389605, preemption_count=0, score=131640.947901, test/accuracy=0.630300, test/loss=1.821702, test/num_examples=10000, total_duration=136249.384905, train/accuracy=0.960619, train/loss=0.147287, validation/accuracy=0.755080, validation/loss=1.043854, validation/num_examples=50000
I0301 19:24:46.834438 140522605770496 logging_writer.py:48] [389700] global_step=389700, grad_norm=4.666779518127441, loss=0.6612082719802856
I0301 19:25:20.573350 140522597377792 logging_writer.py:48] [389800] global_step=389800, grad_norm=4.914323329925537, loss=0.6903637051582336
I0301 19:25:54.305629 140522605770496 logging_writer.py:48] [389900] global_step=389900, grad_norm=4.572396755218506, loss=0.6747845411300659
I0301 19:26:28.086075 140522597377792 logging_writer.py:48] [390000] global_step=390000, grad_norm=4.283865451812744, loss=0.6250579953193665
I0301 19:27:01.867146 140522605770496 logging_writer.py:48] [390100] global_step=390100, grad_norm=4.529251575469971, loss=0.6149482727050781
I0301 19:27:35.740725 140522597377792 logging_writer.py:48] [390200] global_step=390200, grad_norm=4.2634100914001465, loss=0.6209732294082642
I0301 19:28:09.514748 140522605770496 logging_writer.py:48] [390300] global_step=390300, grad_norm=4.114224910736084, loss=0.5463570356369019
I0301 19:28:43.278255 140522597377792 logging_writer.py:48] [390400] global_step=390400, grad_norm=4.407042980194092, loss=0.6216993927955627
I0301 19:29:17.049166 140522605770496 logging_writer.py:48] [390500] global_step=390500, grad_norm=4.376676559448242, loss=0.5949034690856934
I0301 19:29:50.805024 140522597377792 logging_writer.py:48] [390600] global_step=390600, grad_norm=4.366032123565674, loss=0.6130922436714172
I0301 19:30:24.564748 140522605770496 logging_writer.py:48] [390700] global_step=390700, grad_norm=4.523262977600098, loss=0.5912736058235168
I0301 19:30:58.328589 140522597377792 logging_writer.py:48] [390800] global_step=390800, grad_norm=5.242043972015381, loss=0.6187311410903931
I0301 19:31:32.106650 140522605770496 logging_writer.py:48] [390900] global_step=390900, grad_norm=4.457894802093506, loss=0.6388510465621948
I0301 19:32:05.887550 140522597377792 logging_writer.py:48] [391000] global_step=391000, grad_norm=4.456153392791748, loss=0.6017627120018005
I0301 19:32:39.671016 140522605770496 logging_writer.py:48] [391100] global_step=391100, grad_norm=4.758123874664307, loss=0.7056171298027039
I0301 19:32:44.532572 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:32:50.501036 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:32:59.194761 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:33:01.485779 140688601454400 submission_runner.py:411] Time since start: 136776.56s, 	Step: 391116, 	{'train/accuracy': 0.9608378410339355, 'train/loss': 0.14675454795360565, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0440353155136108, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8227473497390747, 'test/num_examples': 10000, 'score': 132151.01629567146, 'total_duration': 136776.55661320686, 'accumulated_submission_time': 132151.01629567146, 'accumulated_eval_time': 4593.165332555771, 'accumulated_logging_time': 18.94241428375244}
I0301 19:33:01.572010 140522597377792 logging_writer.py:48] [391116] accumulated_eval_time=4593.165333, accumulated_logging_time=18.942414, accumulated_submission_time=132151.016296, global_step=391116, preemption_count=0, score=132151.016296, test/accuracy=0.630300, test/loss=1.822747, test/num_examples=10000, total_duration=136776.556613, train/accuracy=0.960838, train/loss=0.146755, validation/accuracy=0.755220, validation/loss=1.044035, validation/num_examples=50000
I0301 19:33:30.294241 140525264946944 logging_writer.py:48] [391200] global_step=391200, grad_norm=4.398680686950684, loss=0.6325839161872864
I0301 19:34:04.092579 140522597377792 logging_writer.py:48] [391300] global_step=391300, grad_norm=4.186035633087158, loss=0.6001991033554077
I0301 19:34:37.910429 140525264946944 logging_writer.py:48] [391400] global_step=391400, grad_norm=4.749872207641602, loss=0.6217862367630005
I0301 19:35:11.703696 140522597377792 logging_writer.py:48] [391500] global_step=391500, grad_norm=4.037261009216309, loss=0.5945566296577454
I0301 19:35:45.479822 140525264946944 logging_writer.py:48] [391600] global_step=391600, grad_norm=4.388172626495361, loss=0.5996837019920349
I0301 19:36:19.221912 140522597377792 logging_writer.py:48] [391700] global_step=391700, grad_norm=4.197057247161865, loss=0.579198956489563
I0301 19:36:53.008841 140525264946944 logging_writer.py:48] [391800] global_step=391800, grad_norm=4.873796463012695, loss=0.6793784499168396
I0301 19:37:26.793274 140522597377792 logging_writer.py:48] [391900] global_step=391900, grad_norm=4.492883205413818, loss=0.5856819748878479
I0301 19:38:00.591873 140525264946944 logging_writer.py:48] [392000] global_step=392000, grad_norm=4.414247035980225, loss=0.6286851167678833
I0301 19:38:34.351028 140522597377792 logging_writer.py:48] [392100] global_step=392100, grad_norm=4.380179405212402, loss=0.6360915899276733
I0301 19:39:08.174686 140525264946944 logging_writer.py:48] [392200] global_step=392200, grad_norm=4.856922149658203, loss=0.5685856342315674
I0301 19:39:42.032716 140522597377792 logging_writer.py:48] [392300] global_step=392300, grad_norm=4.655034065246582, loss=0.6309456825256348
I0301 19:40:15.829484 140525264946944 logging_writer.py:48] [392400] global_step=392400, grad_norm=4.7652974128723145, loss=0.7062385678291321
I0301 19:40:49.596966 140522597377792 logging_writer.py:48] [392500] global_step=392500, grad_norm=4.4763288497924805, loss=0.6450879573822021
I0301 19:41:23.344404 140525264946944 logging_writer.py:48] [392600] global_step=392600, grad_norm=4.596961498260498, loss=0.6968849301338196
I0301 19:41:31.604809 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:41:37.658827 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:41:46.116786 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:41:48.405878 140688601454400 submission_runner.py:411] Time since start: 137303.48s, 	Step: 392626, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.15075649321079254, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0441441535949707, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.821921944618225, 'test/num_examples': 10000, 'score': 132660.9876010418, 'total_duration': 137303.47638607025, 'accumulated_submission_time': 132660.9876010418, 'accumulated_eval_time': 4609.966013908386, 'accumulated_logging_time': 19.038569688796997}
I0301 19:41:48.489847 140522597377792 logging_writer.py:48] [392626] accumulated_eval_time=4609.966014, accumulated_logging_time=19.038570, accumulated_submission_time=132660.987601, global_step=392626, preemption_count=0, score=132660.987601, test/accuracy=0.630000, test/loss=1.821922, test/num_examples=10000, total_duration=137303.476386, train/accuracy=0.960041, train/loss=0.150756, validation/accuracy=0.755160, validation/loss=1.044144, validation/num_examples=50000
I0301 19:42:13.813020 140522605770496 logging_writer.py:48] [392700] global_step=392700, grad_norm=4.497903823852539, loss=0.5112795233726501
I0301 19:42:47.563817 140522597377792 logging_writer.py:48] [392800] global_step=392800, grad_norm=4.956299304962158, loss=0.5463805794715881
I0301 19:43:21.366986 140522605770496 logging_writer.py:48] [392900] global_step=392900, grad_norm=4.491833209991455, loss=0.6411917209625244
I0301 19:43:55.161723 140522597377792 logging_writer.py:48] [393000] global_step=393000, grad_norm=4.537868022918701, loss=0.6423711180686951
I0301 19:44:28.949162 140522605770496 logging_writer.py:48] [393100] global_step=393100, grad_norm=4.4188032150268555, loss=0.6243354678153992
I0301 19:45:02.731939 140522597377792 logging_writer.py:48] [393200] global_step=393200, grad_norm=4.737185001373291, loss=0.6880912780761719
I0301 19:45:36.522130 140522605770496 logging_writer.py:48] [393300] global_step=393300, grad_norm=4.43080472946167, loss=0.6790533065795898
I0301 19:46:10.430279 140522597377792 logging_writer.py:48] [393400] global_step=393400, grad_norm=4.511288642883301, loss=0.5753353238105774
I0301 19:46:44.204707 140522605770496 logging_writer.py:48] [393500] global_step=393500, grad_norm=4.137167453765869, loss=0.5986876487731934
I0301 19:47:18.009460 140522597377792 logging_writer.py:48] [393600] global_step=393600, grad_norm=4.527254581451416, loss=0.6286983489990234
I0301 19:47:51.783404 140522605770496 logging_writer.py:48] [393700] global_step=393700, grad_norm=4.466812610626221, loss=0.6011863946914673
I0301 19:48:25.580969 140522597377792 logging_writer.py:48] [393800] global_step=393800, grad_norm=4.447963237762451, loss=0.6612898707389832
I0301 19:48:59.394962 140522605770496 logging_writer.py:48] [393900] global_step=393900, grad_norm=4.307349681854248, loss=0.5645597577095032
I0301 19:49:33.160887 140522597377792 logging_writer.py:48] [394000] global_step=394000, grad_norm=4.395803451538086, loss=0.663138747215271
I0301 19:50:06.985038 140522605770496 logging_writer.py:48] [394100] global_step=394100, grad_norm=4.5684494972229, loss=0.5991342067718506
I0301 19:50:18.618877 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:50:24.623444 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:50:33.204784 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:50:35.471997 140688601454400 submission_runner.py:411] Time since start: 137830.54s, 	Step: 394136, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14875565469264984, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0435575246810913, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8231137990951538, 'test/num_examples': 10000, 'score': 133171.0541651249, 'total_duration': 137830.54284071922, 'accumulated_submission_time': 133171.0541651249, 'accumulated_eval_time': 4626.819089174271, 'accumulated_logging_time': 19.13323402404785}
I0301 19:50:35.552861 140525256554240 logging_writer.py:48] [394136] accumulated_eval_time=4626.819089, accumulated_logging_time=19.133234, accumulated_submission_time=133171.054165, global_step=394136, preemption_count=0, score=133171.054165, test/accuracy=0.629600, test/loss=1.823114, test/num_examples=10000, total_duration=137830.542841, train/accuracy=0.959961, train/loss=0.148756, validation/accuracy=0.754900, validation/loss=1.043558, validation/num_examples=50000
I0301 19:50:57.500973 140525264946944 logging_writer.py:48] [394200] global_step=394200, grad_norm=4.668316841125488, loss=0.5961700677871704
I0301 19:51:31.205100 140525256554240 logging_writer.py:48] [394300] global_step=394300, grad_norm=4.45239782333374, loss=0.6237857341766357
I0301 19:52:05.130603 140525264946944 logging_writer.py:48] [394400] global_step=394400, grad_norm=4.3595871925354, loss=0.5958741307258606
I0301 19:52:38.949915 140525256554240 logging_writer.py:48] [394500] global_step=394500, grad_norm=4.544772624969482, loss=0.5969040989875793
I0301 19:53:12.701949 140525264946944 logging_writer.py:48] [394600] global_step=394600, grad_norm=4.74470329284668, loss=0.7066968679428101
I0301 19:53:46.482433 140525256554240 logging_writer.py:48] [394700] global_step=394700, grad_norm=4.272047519683838, loss=0.570663571357727
I0301 19:54:20.275009 140525264946944 logging_writer.py:48] [394800] global_step=394800, grad_norm=4.452983856201172, loss=0.6140717267990112
I0301 19:54:54.045771 140525256554240 logging_writer.py:48] [394900] global_step=394900, grad_norm=4.430415153503418, loss=0.5726310610771179
I0301 19:55:27.814824 140525264946944 logging_writer.py:48] [395000] global_step=395000, grad_norm=4.38531494140625, loss=0.5822466611862183
I0301 19:56:01.578524 140525256554240 logging_writer.py:48] [395100] global_step=395100, grad_norm=4.013670444488525, loss=0.5941535234451294
I0301 19:56:35.377613 140525264946944 logging_writer.py:48] [395200] global_step=395200, grad_norm=4.736383438110352, loss=0.6804397702217102
I0301 19:57:09.169828 140525256554240 logging_writer.py:48] [395300] global_step=395300, grad_norm=4.568791389465332, loss=0.6715973019599915
I0301 19:57:42.959664 140525264946944 logging_writer.py:48] [395400] global_step=395400, grad_norm=4.417581558227539, loss=0.6299973726272583
I0301 19:58:16.874697 140525256554240 logging_writer.py:48] [395500] global_step=395500, grad_norm=4.668593406677246, loss=0.6086776256561279
I0301 19:58:50.652059 140525264946944 logging_writer.py:48] [395600] global_step=395600, grad_norm=4.23898458480835, loss=0.5964006781578064
I0301 19:59:05.667149 140688601454400 spec.py:321] Evaluating on the training split.
I0301 19:59:11.687958 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 19:59:20.511290 140688601454400 spec.py:349] Evaluating on the test split.
I0301 19:59:22.801407 140688601454400 submission_runner.py:411] Time since start: 138357.87s, 	Step: 395646, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14062191545963287, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.044531226158142, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8228707313537598, 'test/num_examples': 10000, 'score': 133681.10687541962, 'total_duration': 138357.87222909927, 'accumulated_submission_time': 133681.10687541962, 'accumulated_eval_time': 4643.953272104263, 'accumulated_logging_time': 19.223914623260498}
I0301 19:59:22.898635 140522605770496 logging_writer.py:48] [395646] accumulated_eval_time=4643.953272, accumulated_logging_time=19.223915, accumulated_submission_time=133681.106875, global_step=395646, preemption_count=0, score=133681.106875, test/accuracy=0.631100, test/loss=1.822871, test/num_examples=10000, total_duration=138357.872229, train/accuracy=0.961934, train/loss=0.140622, validation/accuracy=0.754680, validation/loss=1.044531, validation/num_examples=50000
I0301 19:59:41.433947 140523092305664 logging_writer.py:48] [395700] global_step=395700, grad_norm=4.276974201202393, loss=0.6085401177406311
I0301 20:00:15.213525 140522605770496 logging_writer.py:48] [395800] global_step=395800, grad_norm=4.770840644836426, loss=0.631937563419342
I0301 20:00:48.972612 140523092305664 logging_writer.py:48] [395900] global_step=395900, grad_norm=4.66930627822876, loss=0.606270432472229
I0301 20:01:22.737532 140522605770496 logging_writer.py:48] [396000] global_step=396000, grad_norm=5.215138912200928, loss=0.6356871128082275
I0301 20:01:56.505838 140523092305664 logging_writer.py:48] [396100] global_step=396100, grad_norm=4.700946807861328, loss=0.6472251415252686
I0301 20:02:30.266067 140522605770496 logging_writer.py:48] [396200] global_step=396200, grad_norm=4.722715377807617, loss=0.6538338661193848
I0301 20:03:04.024765 140523092305664 logging_writer.py:48] [396300] global_step=396300, grad_norm=4.46390962600708, loss=0.6402883529663086
I0301 20:03:37.826529 140522605770496 logging_writer.py:48] [396400] global_step=396400, grad_norm=4.2963786125183105, loss=0.6034785509109497
I0301 20:04:11.609168 140523092305664 logging_writer.py:48] [396500] global_step=396500, grad_norm=4.172402381896973, loss=0.599881649017334
I0301 20:04:45.477668 140522605770496 logging_writer.py:48] [396600] global_step=396600, grad_norm=4.239604473114014, loss=0.5470424294471741
I0301 20:05:19.257040 140523092305664 logging_writer.py:48] [396700] global_step=396700, grad_norm=4.775567054748535, loss=0.6802695989608765
I0301 20:05:53.071522 140522605770496 logging_writer.py:48] [396800] global_step=396800, grad_norm=4.603507995605469, loss=0.5739288330078125
I0301 20:06:26.859409 140523092305664 logging_writer.py:48] [396900] global_step=396900, grad_norm=4.227470397949219, loss=0.5995397567749023
I0301 20:07:00.695317 140522605770496 logging_writer.py:48] [397000] global_step=397000, grad_norm=4.4447784423828125, loss=0.5784264802932739
I0301 20:07:34.500843 140523092305664 logging_writer.py:48] [397100] global_step=397100, grad_norm=4.559691429138184, loss=0.6297599673271179
I0301 20:07:52.865911 140688601454400 spec.py:321] Evaluating on the training split.
I0301 20:07:58.881383 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 20:08:07.473139 140688601454400 spec.py:349] Evaluating on the test split.
I0301 20:08:09.746407 140688601454400 submission_runner.py:411] Time since start: 138884.82s, 	Step: 397156, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.1473497450351715, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0434935092926025, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8224536180496216, 'test/num_examples': 10000, 'score': 134191.01133322716, 'total_duration': 138884.81725287437, 'accumulated_submission_time': 134191.01133322716, 'accumulated_eval_time': 4660.833715677261, 'accumulated_logging_time': 19.331422567367554}
I0301 20:08:09.839067 140525264946944 logging_writer.py:48] [397156] accumulated_eval_time=4660.833716, accumulated_logging_time=19.331423, accumulated_submission_time=134191.011333, global_step=397156, preemption_count=0, score=134191.011333, test/accuracy=0.630900, test/loss=1.822454, test/num_examples=10000, total_duration=138884.817253, train/accuracy=0.960459, train/loss=0.147350, validation/accuracy=0.755700, validation/loss=1.043494, validation/num_examples=50000
I0301 20:08:25.011089 140525273339648 logging_writer.py:48] [397200] global_step=397200, grad_norm=4.651660442352295, loss=0.6535316705703735
I0301 20:08:58.742524 140525264946944 logging_writer.py:48] [397300] global_step=397300, grad_norm=4.410518169403076, loss=0.6561650633811951
I0301 20:09:32.534260 140525273339648 logging_writer.py:48] [397400] global_step=397400, grad_norm=4.621838092803955, loss=0.6323866248130798
I0301 20:10:06.312697 140525264946944 logging_writer.py:48] [397500] global_step=397500, grad_norm=4.57828426361084, loss=0.58849036693573
I0301 20:10:40.207670 140525273339648 logging_writer.py:48] [397600] global_step=397600, grad_norm=4.692470550537109, loss=0.6715940237045288
I0301 20:11:13.983906 140525264946944 logging_writer.py:48] [397700] global_step=397700, grad_norm=4.274193286895752, loss=0.6053312420845032
I0301 20:11:47.774741 140525273339648 logging_writer.py:48] [397800] global_step=397800, grad_norm=4.14527702331543, loss=0.5582078099250793
I0301 20:12:21.561541 140525264946944 logging_writer.py:48] [397900] global_step=397900, grad_norm=4.487424373626709, loss=0.6050488948822021
I0301 20:12:55.377537 140525273339648 logging_writer.py:48] [398000] global_step=398000, grad_norm=4.560595989227295, loss=0.6249147653579712
I0301 20:13:29.158108 140525264946944 logging_writer.py:48] [398100] global_step=398100, grad_norm=4.515311241149902, loss=0.6302001476287842
I0301 20:14:02.987581 140525273339648 logging_writer.py:48] [398200] global_step=398200, grad_norm=4.52351188659668, loss=0.6193206310272217
I0301 20:14:36.788752 140525264946944 logging_writer.py:48] [398300] global_step=398300, grad_norm=4.310140132904053, loss=0.5986344218254089
I0301 20:15:10.592414 140525273339648 logging_writer.py:48] [398400] global_step=398400, grad_norm=4.608502388000488, loss=0.6235816478729248
I0301 20:15:44.400641 140525264946944 logging_writer.py:48] [398500] global_step=398500, grad_norm=3.9573636054992676, loss=0.5278221964836121
I0301 20:16:18.203500 140525273339648 logging_writer.py:48] [398600] global_step=398600, grad_norm=4.365305423736572, loss=0.6594703793525696
I0301 20:16:39.905580 140688601454400 spec.py:321] Evaluating on the training split.
I0301 20:16:45.956485 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 20:16:54.671704 140688601454400 spec.py:349] Evaluating on the test split.
I0301 20:16:56.996193 140688601454400 submission_runner.py:411] Time since start: 139412.07s, 	Step: 398665, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.1485028862953186, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.044009804725647, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8216071128845215, 'test/num_examples': 10000, 'score': 134701.0166823864, 'total_duration': 139412.06704068184, 'accumulated_submission_time': 134701.0166823864, 'accumulated_eval_time': 4677.92427277565, 'accumulated_logging_time': 19.43360996246338}
I0301 20:16:57.082738 140522588985088 logging_writer.py:48] [398665] accumulated_eval_time=4677.924273, accumulated_logging_time=19.433610, accumulated_submission_time=134701.016682, global_step=398665, preemption_count=0, score=134701.016682, test/accuracy=0.630900, test/loss=1.821607, test/num_examples=10000, total_duration=139412.067041, train/accuracy=0.961017, train/loss=0.148503, validation/accuracy=0.754920, validation/loss=1.044010, validation/num_examples=50000
I0301 20:17:09.219343 140522597377792 logging_writer.py:48] [398700] global_step=398700, grad_norm=4.5758585929870605, loss=0.582691490650177
I0301 20:17:42.933730 140522588985088 logging_writer.py:48] [398800] global_step=398800, grad_norm=4.496422290802002, loss=0.7018442749977112
I0301 20:18:16.723294 140522597377792 logging_writer.py:48] [398900] global_step=398900, grad_norm=3.9882638454437256, loss=0.5786950588226318
I0301 20:18:50.474383 140522588985088 logging_writer.py:48] [399000] global_step=399000, grad_norm=4.765058517456055, loss=0.6503922343254089
I0301 20:19:24.200658 140522597377792 logging_writer.py:48] [399100] global_step=399100, grad_norm=4.405703544616699, loss=0.6306631565093994
I0301 20:19:57.966663 140522588985088 logging_writer.py:48] [399200] global_step=399200, grad_norm=4.322521686553955, loss=0.6177331209182739
I0301 20:20:31.795332 140522597377792 logging_writer.py:48] [399300] global_step=399300, grad_norm=4.2344136238098145, loss=0.538110613822937
I0301 20:21:05.556667 140522588985088 logging_writer.py:48] [399400] global_step=399400, grad_norm=4.694998741149902, loss=0.6554619669914246
I0301 20:21:39.349571 140522597377792 logging_writer.py:48] [399500] global_step=399500, grad_norm=4.437368869781494, loss=0.5729178190231323
I0301 20:22:13.126288 140522588985088 logging_writer.py:48] [399600] global_step=399600, grad_norm=4.515066146850586, loss=0.6373842358589172
I0301 20:22:46.899583 140522597377792 logging_writer.py:48] [399700] global_step=399700, grad_norm=4.35349178314209, loss=0.5652545690536499
I0301 20:23:20.759947 140522588985088 logging_writer.py:48] [399800] global_step=399800, grad_norm=4.869703769683838, loss=0.6959686875343323
I0301 20:23:54.578812 140522597377792 logging_writer.py:48] [399900] global_step=399900, grad_norm=4.861105918884277, loss=0.6776755452156067
I0301 20:24:28.336139 140522588985088 logging_writer.py:48] [400000] global_step=400000, grad_norm=4.9437079429626465, loss=0.6131661534309387
I0301 20:25:02.100445 140522597377792 logging_writer.py:48] [400100] global_step=400100, grad_norm=4.622196197509766, loss=0.5861451625823975
I0301 20:25:27.234634 140688601454400 spec.py:321] Evaluating on the training split.
I0301 20:25:33.221894 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 20:25:41.915820 140688601454400 spec.py:349] Evaluating on the test split.
I0301 20:25:44.177281 140688601454400 submission_runner.py:411] Time since start: 139939.25s, 	Step: 400176, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14942507445812225, 'validation/accuracy': 0.755079984664917, 'validation/loss': 1.04414701461792, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8218629360198975, 'test/num_examples': 10000, 'score': 135211.10509705544, 'total_duration': 139939.248128891, 'accumulated_submission_time': 135211.10509705544, 'accumulated_eval_time': 4694.866877555847, 'accumulated_logging_time': 19.531213521957397}
I0301 20:25:44.263835 140525256554240 logging_writer.py:48] [400176] accumulated_eval_time=4694.866878, accumulated_logging_time=19.531214, accumulated_submission_time=135211.105097, global_step=400176, preemption_count=0, score=135211.105097, test/accuracy=0.631900, test/loss=1.821863, test/num_examples=10000, total_duration=139939.248129, train/accuracy=0.960120, train/loss=0.149425, validation/accuracy=0.755080, validation/loss=1.044147, validation/num_examples=50000
I0301 20:25:52.726446 140525264946944 logging_writer.py:48] [400200] global_step=400200, grad_norm=4.711286544799805, loss=0.6134238243103027
I0301 20:26:26.480757 140525256554240 logging_writer.py:48] [400300] global_step=400300, grad_norm=4.8428473472595215, loss=0.6672461032867432
I0301 20:27:00.250748 140525264946944 logging_writer.py:48] [400400] global_step=400400, grad_norm=4.329186916351318, loss=0.6441339254379272
I0301 20:27:34.030008 140525256554240 logging_writer.py:48] [400500] global_step=400500, grad_norm=4.504113674163818, loss=0.6135682463645935
I0301 20:28:07.789927 140525264946944 logging_writer.py:48] [400600] global_step=400600, grad_norm=4.194601058959961, loss=0.574652373790741
I0301 20:28:41.564632 140525256554240 logging_writer.py:48] [400700] global_step=400700, grad_norm=4.404592514038086, loss=0.614011287689209
I0301 20:29:15.420794 140525264946944 logging_writer.py:48] [400800] global_step=400800, grad_norm=4.721428871154785, loss=0.6269083619117737
I0301 20:29:49.211550 140525256554240 logging_writer.py:48] [400900] global_step=400900, grad_norm=5.1335344314575195, loss=0.7087053060531616
I0301 20:30:22.999132 140525264946944 logging_writer.py:48] [401000] global_step=401000, grad_norm=4.3732781410217285, loss=0.6520220637321472
I0301 20:30:56.773682 140525256554240 logging_writer.py:48] [401100] global_step=401100, grad_norm=4.646787643432617, loss=0.537930428981781
I0301 20:31:30.561750 140525264946944 logging_writer.py:48] [401200] global_step=401200, grad_norm=4.250210762023926, loss=0.624114990234375
I0301 20:32:04.336391 140525256554240 logging_writer.py:48] [401300] global_step=401300, grad_norm=4.3546013832092285, loss=0.6358683705329895
I0301 20:32:38.105540 140525264946944 logging_writer.py:48] [401400] global_step=401400, grad_norm=4.518080711364746, loss=0.66159987449646
I0301 20:33:11.868072 140525256554240 logging_writer.py:48] [401500] global_step=401500, grad_norm=4.771572589874268, loss=0.6114091873168945
I0301 20:33:45.671589 140525264946944 logging_writer.py:48] [401600] global_step=401600, grad_norm=4.594705104827881, loss=0.6671372056007385
I0301 20:34:14.497174 140688601454400 spec.py:321] Evaluating on the training split.
I0301 20:34:20.617872 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 20:34:29.250394 140688601454400 spec.py:349] Evaluating on the test split.
I0301 20:34:31.513240 140688601454400 submission_runner.py:411] Time since start: 140466.58s, 	Step: 401687, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14580205082893372, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0438878536224365, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8203022480010986, 'test/num_examples': 10000, 'score': 135721.27109241486, 'total_duration': 140466.58408665657, 'accumulated_submission_time': 135721.27109241486, 'accumulated_eval_time': 4711.882912874222, 'accumulated_logging_time': 19.63275671005249}
I0301 20:34:31.598650 140522597377792 logging_writer.py:48] [401687] accumulated_eval_time=4711.882913, accumulated_logging_time=19.632757, accumulated_submission_time=135721.271092, global_step=401687, preemption_count=0, score=135721.271092, test/accuracy=0.630800, test/loss=1.820302, test/num_examples=10000, total_duration=140466.584087, train/accuracy=0.961316, train/loss=0.145802, validation/accuracy=0.755220, validation/loss=1.043888, validation/num_examples=50000
I0301 20:34:36.312085 140522605770496 logging_writer.py:48] [401700] global_step=401700, grad_norm=4.771971225738525, loss=0.6079597473144531
I0301 20:35:09.997019 140522597377792 logging_writer.py:48] [401800] global_step=401800, grad_norm=3.9738757610321045, loss=0.5166020393371582
I0301 20:35:43.889297 140522605770496 logging_writer.py:48] [401900] global_step=401900, grad_norm=4.594574928283691, loss=0.6281096935272217
I0301 20:36:17.668745 140522597377792 logging_writer.py:48] [402000] global_step=402000, grad_norm=4.222951889038086, loss=0.6359930038452148
I0301 20:36:51.428210 140522605770496 logging_writer.py:48] [402100] global_step=402100, grad_norm=4.576642036437988, loss=0.5686087012290955
I0301 20:37:25.193319 140522597377792 logging_writer.py:48] [402200] global_step=402200, grad_norm=4.34616231918335, loss=0.630538821220398
I0301 20:37:58.964056 140522605770496 logging_writer.py:48] [402300] global_step=402300, grad_norm=4.7782721519470215, loss=0.6777340173721313
I0301 20:38:32.774723 140522597377792 logging_writer.py:48] [402400] global_step=402400, grad_norm=4.272831916809082, loss=0.6732649207115173
I0301 20:39:06.553308 140522605770496 logging_writer.py:48] [402500] global_step=402500, grad_norm=4.7318902015686035, loss=0.6689174175262451
I0301 20:39:40.369179 140522597377792 logging_writer.py:48] [402600] global_step=402600, grad_norm=4.516448020935059, loss=0.5677573680877686
I0301 20:40:14.171120 140522605770496 logging_writer.py:48] [402700] global_step=402700, grad_norm=4.647284030914307, loss=0.5827207565307617
I0301 20:40:47.993078 140522597377792 logging_writer.py:48] [402800] global_step=402800, grad_norm=4.354585647583008, loss=0.6482698917388916
I0301 20:41:21.770734 140522605770496 logging_writer.py:48] [402900] global_step=402900, grad_norm=4.72292947769165, loss=0.6341807842254639
I0301 20:41:55.648976 140522597377792 logging_writer.py:48] [403000] global_step=403000, grad_norm=4.7517781257629395, loss=0.6270279288291931
I0301 20:42:29.424381 140522605770496 logging_writer.py:48] [403100] global_step=403100, grad_norm=4.23261833190918, loss=0.6464359760284424
I0301 20:43:01.736144 140688601454400 spec.py:321] Evaluating on the training split.
I0301 20:43:07.739822 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 20:43:16.284843 140688601454400 spec.py:349] Evaluating on the test split.
I0301 20:43:18.573405 140688601454400 submission_runner.py:411] Time since start: 140993.64s, 	Step: 403197, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.1497427225112915, 'validation/accuracy': 0.755079984664917, 'validation/loss': 1.0438313484191895, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8218708038330078, 'test/num_examples': 10000, 'score': 136231.34775304794, 'total_duration': 140993.64425182343, 'accumulated_submission_time': 136231.34775304794, 'accumulated_eval_time': 4728.720160961151, 'accumulated_logging_time': 19.728036642074585}
I0301 20:43:18.655915 140522597377792 logging_writer.py:48] [403197] accumulated_eval_time=4728.720161, accumulated_logging_time=19.728037, accumulated_submission_time=136231.347753, global_step=403197, preemption_count=0, score=136231.347753, test/accuracy=0.631300, test/loss=1.821871, test/num_examples=10000, total_duration=140993.644252, train/accuracy=0.959841, train/loss=0.149743, validation/accuracy=0.755080, validation/loss=1.043831, validation/num_examples=50000
I0301 20:43:20.019420 140525264946944 logging_writer.py:48] [403200] global_step=403200, grad_norm=4.304348945617676, loss=0.5965431928634644
I0301 20:43:53.762323 140522597377792 logging_writer.py:48] [403300] global_step=403300, grad_norm=4.462982177734375, loss=0.5963652729988098
I0301 20:44:27.493115 140525264946944 logging_writer.py:48] [403400] global_step=403400, grad_norm=4.620821475982666, loss=0.6213834285736084
I0301 20:45:01.255801 140522597377792 logging_writer.py:48] [403500] global_step=403500, grad_norm=4.870522975921631, loss=0.6331760883331299
I0301 20:45:35.018088 140525264946944 logging_writer.py:48] [403600] global_step=403600, grad_norm=4.116743564605713, loss=0.6259966492652893
I0301 20:46:08.802093 140522597377792 logging_writer.py:48] [403700] global_step=403700, grad_norm=4.226539134979248, loss=0.6641102433204651
I0301 20:46:42.574249 140525264946944 logging_writer.py:48] [403800] global_step=403800, grad_norm=4.852598667144775, loss=0.6183913946151733
I0301 20:47:16.307449 140522597377792 logging_writer.py:48] [403900] global_step=403900, grad_norm=4.333963394165039, loss=0.6089246869087219
I0301 20:47:50.243427 140525264946944 logging_writer.py:48] [404000] global_step=404000, grad_norm=4.349303722381592, loss=0.6080686450004578
I0301 20:48:24.010522 140522597377792 logging_writer.py:48] [404100] global_step=404100, grad_norm=4.241527557373047, loss=0.6051762700080872
I0301 20:48:57.765569 140525264946944 logging_writer.py:48] [404200] global_step=404200, grad_norm=4.626677513122559, loss=0.6508655548095703
I0301 20:49:31.533496 140522597377792 logging_writer.py:48] [404300] global_step=404300, grad_norm=5.1620283126831055, loss=0.7031854391098022
I0301 20:50:05.310952 140525264946944 logging_writer.py:48] [404400] global_step=404400, grad_norm=4.434550762176514, loss=0.6269850730895996
I0301 20:50:39.073691 140522597377792 logging_writer.py:48] [404500] global_step=404500, grad_norm=4.919169902801514, loss=0.6533170342445374
I0301 20:51:12.843449 140525264946944 logging_writer.py:48] [404600] global_step=404600, grad_norm=4.703799247741699, loss=0.5822616815567017
I0301 20:51:46.592978 140522597377792 logging_writer.py:48] [404700] global_step=404700, grad_norm=4.449173927307129, loss=0.6230917572975159
I0301 20:51:48.757038 140688601454400 spec.py:321] Evaluating on the training split.
I0301 20:51:54.731705 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 20:52:03.558627 140688601454400 spec.py:349] Evaluating on the test split.
I0301 20:52:05.828521 140688601454400 submission_runner.py:411] Time since start: 141520.90s, 	Step: 404708, 	{'train/accuracy': 0.959004282951355, 'train/loss': 0.1519351452589035, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.0440607070922852, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8225207328796387, 'test/num_examples': 10000, 'score': 136741.38403439522, 'total_duration': 141520.89934945107, 'accumulated_submission_time': 136741.38403439522, 'accumulated_eval_time': 4745.791574001312, 'accumulated_logging_time': 19.823182582855225}
I0301 20:52:05.914478 140522605770496 logging_writer.py:48] [404708] accumulated_eval_time=4745.791574, accumulated_logging_time=19.823183, accumulated_submission_time=136741.384034, global_step=404708, preemption_count=0, score=136741.384034, test/accuracy=0.630800, test/loss=1.822521, test/num_examples=10000, total_duration=141520.899349, train/accuracy=0.959004, train/loss=0.151935, validation/accuracy=0.755320, validation/loss=1.044061, validation/num_examples=50000
I0301 20:52:37.252091 140523092305664 logging_writer.py:48] [404800] global_step=404800, grad_norm=4.187672138214111, loss=0.6269273161888123
I0301 20:53:11.012660 140522605770496 logging_writer.py:48] [404900] global_step=404900, grad_norm=4.759906768798828, loss=0.6646777987480164
I0301 20:53:44.864335 140523092305664 logging_writer.py:48] [405000] global_step=405000, grad_norm=4.877660751342773, loss=0.6445281505584717
I0301 20:54:18.681434 140522605770496 logging_writer.py:48] [405100] global_step=405100, grad_norm=4.538803577423096, loss=0.6117851138114929
I0301 20:54:52.462213 140523092305664 logging_writer.py:48] [405200] global_step=405200, grad_norm=4.673594951629639, loss=0.6719602346420288
I0301 20:55:26.269026 140522605770496 logging_writer.py:48] [405300] global_step=405300, grad_norm=4.044134616851807, loss=0.6218053698539734
I0301 20:56:00.045013 140523092305664 logging_writer.py:48] [405400] global_step=405400, grad_norm=4.563533782958984, loss=0.628564178943634
I0301 20:56:33.860151 140522605770496 logging_writer.py:48] [405500] global_step=405500, grad_norm=4.393251419067383, loss=0.6507329940795898
I0301 20:57:07.634831 140523092305664 logging_writer.py:48] [405600] global_step=405600, grad_norm=4.365910053253174, loss=0.6807729005813599
I0301 20:57:41.436110 140522605770496 logging_writer.py:48] [405700] global_step=405700, grad_norm=4.608008861541748, loss=0.6300996541976929
I0301 20:58:15.209260 140523092305664 logging_writer.py:48] [405800] global_step=405800, grad_norm=4.446003437042236, loss=0.5868862867355347
I0301 20:58:48.978739 140522605770496 logging_writer.py:48] [405900] global_step=405900, grad_norm=4.742206573486328, loss=0.6461738348007202
I0301 20:59:22.749078 140523092305664 logging_writer.py:48] [406000] global_step=406000, grad_norm=4.0914626121521, loss=0.55704265832901
I0301 20:59:56.603549 140522605770496 logging_writer.py:48] [406100] global_step=406100, grad_norm=4.34676456451416, loss=0.6068214178085327
I0301 21:00:30.380777 140523092305664 logging_writer.py:48] [406200] global_step=406200, grad_norm=4.608319282531738, loss=0.6414247751235962
I0301 21:00:35.940922 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:00:42.048375 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:00:50.556713 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:00:52.840998 140688601454400 submission_runner.py:411] Time since start: 142047.91s, 	Step: 406218, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14945334196090698, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.0442662239074707, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.82229483127594, 'test/num_examples': 10000, 'score': 137251.34723711014, 'total_duration': 142047.91184687614, 'accumulated_submission_time': 137251.34723711014, 'accumulated_eval_time': 4762.691604375839, 'accumulated_logging_time': 19.920071363449097}
I0301 21:00:52.935320 140525264946944 logging_writer.py:48] [406218] accumulated_eval_time=4762.691604, accumulated_logging_time=19.920071, accumulated_submission_time=137251.347237, global_step=406218, preemption_count=0, score=137251.347237, test/accuracy=0.631200, test/loss=1.822295, test/num_examples=10000, total_duration=142047.911847, train/accuracy=0.959961, train/loss=0.149453, validation/accuracy=0.755200, validation/loss=1.044266, validation/num_examples=50000
I0301 21:01:20.965558 140525273339648 logging_writer.py:48] [406300] global_step=406300, grad_norm=4.768580913543701, loss=0.6309322118759155
I0301 21:01:54.639985 140525264946944 logging_writer.py:48] [406400] global_step=406400, grad_norm=4.578421592712402, loss=0.6629613637924194
I0301 21:02:28.404588 140525273339648 logging_writer.py:48] [406500] global_step=406500, grad_norm=4.45262336730957, loss=0.4985648989677429
I0301 21:03:02.181549 140525264946944 logging_writer.py:48] [406600] global_step=406600, grad_norm=4.658469200134277, loss=0.6090452671051025
I0301 21:03:35.964552 140525273339648 logging_writer.py:48] [406700] global_step=406700, grad_norm=4.348333835601807, loss=0.6272107362747192
I0301 21:04:09.788689 140525264946944 logging_writer.py:48] [406800] global_step=406800, grad_norm=4.194738388061523, loss=0.6250325441360474
I0301 21:04:43.549283 140525273339648 logging_writer.py:48] [406900] global_step=406900, grad_norm=4.179964065551758, loss=0.6156178712844849
I0301 21:05:17.365970 140525264946944 logging_writer.py:48] [407000] global_step=407000, grad_norm=4.51324462890625, loss=0.6373807191848755
I0301 21:05:51.190373 140525273339648 logging_writer.py:48] [407100] global_step=407100, grad_norm=4.966256618499756, loss=0.6929752826690674
I0301 21:06:24.972204 140525264946944 logging_writer.py:48] [407200] global_step=407200, grad_norm=4.449308395385742, loss=0.5988735556602478
I0301 21:06:58.735519 140525273339648 logging_writer.py:48] [407300] global_step=407300, grad_norm=4.56747579574585, loss=0.6558387875556946
I0301 21:07:32.552664 140525264946944 logging_writer.py:48] [407400] global_step=407400, grad_norm=4.239834785461426, loss=0.6197673678398132
I0301 21:08:06.346163 140525273339648 logging_writer.py:48] [407500] global_step=407500, grad_norm=4.443603038787842, loss=0.5987924337387085
I0301 21:08:40.142811 140525264946944 logging_writer.py:48] [407600] global_step=407600, grad_norm=4.1779937744140625, loss=0.593689501285553
I0301 21:09:13.908567 140525273339648 logging_writer.py:48] [407700] global_step=407700, grad_norm=4.3416619300842285, loss=0.6372523307800293
I0301 21:09:23.170195 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:09:29.199196 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:09:37.857551 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:09:40.120295 140688601454400 submission_runner.py:411] Time since start: 142575.19s, 	Step: 407729, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14725780487060547, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.044488787651062, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8232182264328003, 'test/num_examples': 10000, 'score': 137761.51915931702, 'total_duration': 142575.1911354065, 'accumulated_submission_time': 137761.51915931702, 'accumulated_eval_time': 4779.641663074493, 'accumulated_logging_time': 20.02484154701233}
I0301 21:09:40.206441 140522605770496 logging_writer.py:48] [407729] accumulated_eval_time=4779.641663, accumulated_logging_time=20.024842, accumulated_submission_time=137761.519159, global_step=407729, preemption_count=0, score=137761.519159, test/accuracy=0.630300, test/loss=1.823218, test/num_examples=10000, total_duration=142575.191135, train/accuracy=0.960340, train/loss=0.147258, validation/accuracy=0.754880, validation/loss=1.044489, validation/num_examples=50000
I0301 21:10:04.512198 140523092305664 logging_writer.py:48] [407800] global_step=407800, grad_norm=4.581640720367432, loss=0.5677125453948975
I0301 21:10:38.218076 140522605770496 logging_writer.py:48] [407900] global_step=407900, grad_norm=4.416715621948242, loss=0.5362890958786011
I0301 21:11:11.998391 140523092305664 logging_writer.py:48] [408000] global_step=408000, grad_norm=4.426267147064209, loss=0.6106216907501221
I0301 21:11:45.783647 140522605770496 logging_writer.py:48] [408100] global_step=408100, grad_norm=4.205521583557129, loss=0.6609642505645752
I0301 21:12:19.610912 140523092305664 logging_writer.py:48] [408200] global_step=408200, grad_norm=4.696351051330566, loss=0.6417158842086792
I0301 21:12:53.370193 140522605770496 logging_writer.py:48] [408300] global_step=408300, grad_norm=4.322048187255859, loss=0.5775690078735352
I0301 21:13:27.153206 140523092305664 logging_writer.py:48] [408400] global_step=408400, grad_norm=4.594317436218262, loss=0.665647029876709
I0301 21:14:00.921849 140522605770496 logging_writer.py:48] [408500] global_step=408500, grad_norm=4.036259651184082, loss=0.6281125545501709
I0301 21:14:34.680351 140523092305664 logging_writer.py:48] [408600] global_step=408600, grad_norm=4.342919826507568, loss=0.5975747108459473
I0301 21:15:08.477112 140522605770496 logging_writer.py:48] [408700] global_step=408700, grad_norm=4.520498275756836, loss=0.629574716091156
I0301 21:15:42.273400 140523092305664 logging_writer.py:48] [408800] global_step=408800, grad_norm=4.967544078826904, loss=0.6246564388275146
I0301 21:16:16.070351 140522605770496 logging_writer.py:48] [408900] global_step=408900, grad_norm=4.589638710021973, loss=0.6201713681221008
I0301 21:16:49.866923 140523092305664 logging_writer.py:48] [409000] global_step=409000, grad_norm=4.23912239074707, loss=0.6313260197639465
I0301 21:17:23.669834 140522605770496 logging_writer.py:48] [409100] global_step=409100, grad_norm=4.6354851722717285, loss=0.6201493740081787
I0301 21:17:57.529549 140523092305664 logging_writer.py:48] [409200] global_step=409200, grad_norm=4.59805154800415, loss=0.6934975981712341
I0301 21:18:10.178365 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:18:16.131961 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:18:24.625193 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:18:26.925766 140688601454400 submission_runner.py:411] Time since start: 143102.00s, 	Step: 409239, 	{'train/accuracy': 0.9592832922935486, 'train/loss': 0.14898401498794556, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0428025722503662, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8203986883163452, 'test/num_examples': 10000, 'score': 138271.42797732353, 'total_duration': 143101.9966094494, 'accumulated_submission_time': 138271.42797732353, 'accumulated_eval_time': 4796.389009952545, 'accumulated_logging_time': 20.122021436691284}
I0301 21:18:27.014621 140522597377792 logging_writer.py:48] [409239] accumulated_eval_time=4796.389010, accumulated_logging_time=20.122021, accumulated_submission_time=138271.427977, global_step=409239, preemption_count=0, score=138271.427977, test/accuracy=0.630700, test/loss=1.820399, test/num_examples=10000, total_duration=143101.996609, train/accuracy=0.959283, train/loss=0.148984, validation/accuracy=0.755460, validation/loss=1.042803, validation/num_examples=50000
I0301 21:18:47.945576 140525256554240 logging_writer.py:48] [409300] global_step=409300, grad_norm=4.5400071144104, loss=0.5729358196258545
I0301 21:19:21.677244 140522597377792 logging_writer.py:48] [409400] global_step=409400, grad_norm=4.106280326843262, loss=0.6059515476226807
I0301 21:19:55.418859 140525256554240 logging_writer.py:48] [409500] global_step=409500, grad_norm=4.347029685974121, loss=0.5778482556343079
I0301 21:20:29.202753 140522597377792 logging_writer.py:48] [409600] global_step=409600, grad_norm=4.682755470275879, loss=0.6472315788269043
I0301 21:21:02.982742 140525256554240 logging_writer.py:48] [409700] global_step=409700, grad_norm=5.233854293823242, loss=0.713916540145874
I0301 21:21:36.761686 140522597377792 logging_writer.py:48] [409800] global_step=409800, grad_norm=4.185685634613037, loss=0.5570706129074097
I0301 21:22:10.540439 140525256554240 logging_writer.py:48] [409900] global_step=409900, grad_norm=4.616270065307617, loss=0.6117696762084961
I0301 21:22:44.347995 140522597377792 logging_writer.py:48] [410000] global_step=410000, grad_norm=4.248584747314453, loss=0.6125876903533936
I0301 21:23:18.118115 140525256554240 logging_writer.py:48] [410100] global_step=410100, grad_norm=4.519655227661133, loss=0.620042085647583
I0301 21:23:51.890033 140522597377792 logging_writer.py:48] [410200] global_step=410200, grad_norm=4.304152965545654, loss=0.5911192297935486
I0301 21:24:25.851903 140525256554240 logging_writer.py:48] [410300] global_step=410300, grad_norm=4.588648319244385, loss=0.6403920650482178
I0301 21:24:59.639007 140522597377792 logging_writer.py:48] [410400] global_step=410400, grad_norm=4.417787551879883, loss=0.542756974697113
I0301 21:25:33.449889 140525256554240 logging_writer.py:48] [410500] global_step=410500, grad_norm=4.7288055419921875, loss=0.6285481452941895
I0301 21:26:07.218282 140522597377792 logging_writer.py:48] [410600] global_step=410600, grad_norm=4.4836931228637695, loss=0.6625158190727234
I0301 21:26:41.011807 140525256554240 logging_writer.py:48] [410700] global_step=410700, grad_norm=4.836556434631348, loss=0.6265348196029663
I0301 21:26:57.013104 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:27:03.244146 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:27:11.976173 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:27:14.260631 140688601454400 submission_runner.py:411] Time since start: 143629.33s, 	Step: 410749, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14994953572750092, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0459305047988892, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8239178657531738, 'test/num_examples': 10000, 'score': 138781.3643064499, 'total_duration': 143629.33147835732, 'accumulated_submission_time': 138781.3643064499, 'accumulated_eval_time': 4813.636505126953, 'accumulated_logging_time': 20.22056555747986}
I0301 21:27:14.346184 140523947947776 logging_writer.py:48] [410749] accumulated_eval_time=4813.636505, accumulated_logging_time=20.220566, accumulated_submission_time=138781.364306, global_step=410749, preemption_count=0, score=138781.364306, test/accuracy=0.631100, test/loss=1.823918, test/num_examples=10000, total_duration=143629.331478, train/accuracy=0.959981, train/loss=0.149950, validation/accuracy=0.755040, validation/loss=1.045931, validation/num_examples=50000
I0301 21:27:31.917334 140525281732352 logging_writer.py:48] [410800] global_step=410800, grad_norm=4.476941108703613, loss=0.697817325592041
I0301 21:28:05.638978 140523947947776 logging_writer.py:48] [410900] global_step=410900, grad_norm=4.492650508880615, loss=0.6696843504905701
I0301 21:28:39.422783 140525281732352 logging_writer.py:48] [411000] global_step=411000, grad_norm=4.366626739501953, loss=0.672545313835144
I0301 21:29:13.209004 140523947947776 logging_writer.py:48] [411100] global_step=411100, grad_norm=5.155786514282227, loss=0.5860607624053955
I0301 21:29:46.987274 140525281732352 logging_writer.py:48] [411200] global_step=411200, grad_norm=4.770477771759033, loss=0.6144629120826721
I0301 21:30:20.834062 140523947947776 logging_writer.py:48] [411300] global_step=411300, grad_norm=4.497357368469238, loss=0.6523765325546265
I0301 21:30:54.630777 140525281732352 logging_writer.py:48] [411400] global_step=411400, grad_norm=4.80211877822876, loss=0.6392844915390015
I0301 21:31:28.402328 140523947947776 logging_writer.py:48] [411500] global_step=411500, grad_norm=4.6716461181640625, loss=0.6897661089897156
I0301 21:32:02.180952 140525281732352 logging_writer.py:48] [411600] global_step=411600, grad_norm=4.3335676193237305, loss=0.6284853219985962
I0301 21:32:35.952404 140523947947776 logging_writer.py:48] [411700] global_step=411700, grad_norm=4.291059970855713, loss=0.5890299677848816
I0301 21:33:09.726575 140525281732352 logging_writer.py:48] [411800] global_step=411800, grad_norm=4.685606956481934, loss=0.6571745872497559
I0301 21:33:43.516585 140523947947776 logging_writer.py:48] [411900] global_step=411900, grad_norm=4.760190010070801, loss=0.7442591786384583
I0301 21:34:17.302502 140525281732352 logging_writer.py:48] [412000] global_step=412000, grad_norm=4.117464542388916, loss=0.6004632115364075
I0301 21:34:51.066184 140523947947776 logging_writer.py:48] [412100] global_step=412100, grad_norm=4.348477363586426, loss=0.6147419810295105
I0301 21:35:24.869978 140525281732352 logging_writer.py:48] [412200] global_step=412200, grad_norm=4.501579284667969, loss=0.602803647518158
I0301 21:35:44.580122 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:35:50.595093 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:35:59.077745 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:36:01.345666 140688601454400 submission_runner.py:411] Time since start: 144156.42s, 	Step: 412260, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14634431898593903, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0433546304702759, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.822560429573059, 'test/num_examples': 10000, 'score': 139291.53511548042, 'total_duration': 144156.41612815857, 'accumulated_submission_time': 139291.53511548042, 'accumulated_eval_time': 4830.401615142822, 'accumulated_logging_time': 20.31696081161499}
I0301 21:36:01.474377 140522605770496 logging_writer.py:48] [412260] accumulated_eval_time=4830.401615, accumulated_logging_time=20.316961, accumulated_submission_time=139291.535115, global_step=412260, preemption_count=0, score=139291.535115, test/accuracy=0.630800, test/loss=1.822560, test/num_examples=10000, total_duration=144156.416128, train/accuracy=0.960778, train/loss=0.146344, validation/accuracy=0.755180, validation/loss=1.043355, validation/num_examples=50000
I0301 21:36:15.314078 140523092305664 logging_writer.py:48] [412300] global_step=412300, grad_norm=4.1669769287109375, loss=0.5516093373298645
I0301 21:36:49.123479 140522605770496 logging_writer.py:48] [412400] global_step=412400, grad_norm=4.326207637786865, loss=0.5924499034881592
I0301 21:37:22.895614 140523092305664 logging_writer.py:48] [412500] global_step=412500, grad_norm=4.103657245635986, loss=0.5475038290023804
I0301 21:37:56.686128 140522605770496 logging_writer.py:48] [412600] global_step=412600, grad_norm=4.328349590301514, loss=0.674048125743866
I0301 21:38:30.434427 140523092305664 logging_writer.py:48] [412700] global_step=412700, grad_norm=4.35054874420166, loss=0.5973386168479919
I0301 21:39:04.182122 140522605770496 logging_writer.py:48] [412800] global_step=412800, grad_norm=4.586043357849121, loss=0.6336092352867126
I0301 21:39:37.948070 140523092305664 logging_writer.py:48] [412900] global_step=412900, grad_norm=4.282052993774414, loss=0.534986138343811
I0301 21:40:11.757112 140522605770496 logging_writer.py:48] [413000] global_step=413000, grad_norm=4.71989107131958, loss=0.7198387384414673
I0301 21:40:45.535808 140523092305664 logging_writer.py:48] [413100] global_step=413100, grad_norm=4.85543155670166, loss=0.6555356979370117
I0301 21:41:19.327209 140522605770496 logging_writer.py:48] [413200] global_step=413200, grad_norm=4.500582695007324, loss=0.6810916662216187
I0301 21:41:53.098426 140523092305664 logging_writer.py:48] [413300] global_step=413300, grad_norm=4.047581195831299, loss=0.5496602654457092
I0301 21:42:26.893241 140522605770496 logging_writer.py:48] [413400] global_step=413400, grad_norm=4.762505054473877, loss=0.5992032289505005
I0301 21:43:00.740040 140523092305664 logging_writer.py:48] [413500] global_step=413500, grad_norm=4.176239013671875, loss=0.5694995522499084
I0301 21:43:34.538154 140522605770496 logging_writer.py:48] [413600] global_step=413600, grad_norm=4.274536609649658, loss=0.5794313549995422
I0301 21:44:08.305417 140523092305664 logging_writer.py:48] [413700] global_step=413700, grad_norm=4.922631740570068, loss=0.6992578506469727
I0301 21:44:31.435648 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:44:37.430523 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:44:45.961102 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:44:48.235802 140688601454400 submission_runner.py:411] Time since start: 144683.31s, 	Step: 413770, 	{'train/accuracy': 0.9599409699440002, 'train/loss': 0.1482083797454834, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0445153713226318, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8213326930999756, 'test/num_examples': 10000, 'score': 139801.42785167694, 'total_duration': 144683.30664777756, 'accumulated_submission_time': 139801.42785167694, 'accumulated_eval_time': 4847.201719760895, 'accumulated_logging_time': 20.461642026901245}
I0301 21:44:48.325028 140525256554240 logging_writer.py:48] [413770] accumulated_eval_time=4847.201720, accumulated_logging_time=20.461642, accumulated_submission_time=139801.427852, global_step=413770, preemption_count=0, score=139801.427852, test/accuracy=0.630900, test/loss=1.821333, test/num_examples=10000, total_duration=144683.306648, train/accuracy=0.959941, train/loss=0.148208, validation/accuracy=0.755340, validation/loss=1.044515, validation/num_examples=50000
I0301 21:44:58.808820 140525264946944 logging_writer.py:48] [413800] global_step=413800, grad_norm=4.721216201782227, loss=0.6778228282928467
I0301 21:45:32.538009 140525256554240 logging_writer.py:48] [413900] global_step=413900, grad_norm=4.503554344177246, loss=0.5967476963996887
I0301 21:46:06.301100 140525264946944 logging_writer.py:48] [414000] global_step=414000, grad_norm=4.330949306488037, loss=0.6095448732376099
I0301 21:46:40.133281 140525256554240 logging_writer.py:48] [414100] global_step=414100, grad_norm=4.262153148651123, loss=0.6483609080314636
I0301 21:47:13.919149 140525264946944 logging_writer.py:48] [414200] global_step=414200, grad_norm=4.626774311065674, loss=0.6601235866546631
I0301 21:47:47.742864 140525256554240 logging_writer.py:48] [414300] global_step=414300, grad_norm=4.931478500366211, loss=0.6585348844528198
I0301 21:48:21.529706 140525264946944 logging_writer.py:48] [414400] global_step=414400, grad_norm=4.25701379776001, loss=0.5766780972480774
I0301 21:48:55.390703 140525256554240 logging_writer.py:48] [414500] global_step=414500, grad_norm=4.350442886352539, loss=0.6321633458137512
I0301 21:49:29.139467 140525264946944 logging_writer.py:48] [414600] global_step=414600, grad_norm=4.595872402191162, loss=0.6134759187698364
I0301 21:50:02.897051 140525256554240 logging_writer.py:48] [414700] global_step=414700, grad_norm=4.123789310455322, loss=0.5713672041893005
I0301 21:50:36.669822 140525264946944 logging_writer.py:48] [414800] global_step=414800, grad_norm=4.477055072784424, loss=0.6078662872314453
I0301 21:51:10.466198 140525256554240 logging_writer.py:48] [414900] global_step=414900, grad_norm=4.449522018432617, loss=0.5797209739685059
I0301 21:51:44.231586 140525264946944 logging_writer.py:48] [415000] global_step=415000, grad_norm=4.359022617340088, loss=0.5865519642829895
I0301 21:52:18.046078 140525256554240 logging_writer.py:48] [415100] global_step=415100, grad_norm=4.692694187164307, loss=0.6110628843307495
I0301 21:52:51.831063 140525264946944 logging_writer.py:48] [415200] global_step=415200, grad_norm=4.235872745513916, loss=0.5866782069206238
I0301 21:53:18.341801 140688601454400 spec.py:321] Evaluating on the training split.
I0301 21:53:24.537607 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 21:53:33.159616 140688601454400 spec.py:349] Evaluating on the test split.
I0301 21:53:35.435474 140688601454400 submission_runner.py:411] Time since start: 145210.51s, 	Step: 415280, 	{'train/accuracy': 0.9595025181770325, 'train/loss': 0.1520635187625885, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0431699752807617, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8208905458450317, 'test/num_examples': 10000, 'score': 140311.3820786476, 'total_duration': 145210.5059850216, 'accumulated_submission_time': 140311.3820786476, 'accumulated_eval_time': 4864.29502248764, 'accumulated_logging_time': 20.562307596206665}
I0301 21:53:35.523792 140523092305664 logging_writer.py:48] [415280] accumulated_eval_time=4864.295022, accumulated_logging_time=20.562308, accumulated_submission_time=140311.382079, global_step=415280, preemption_count=0, score=140311.382079, test/accuracy=0.631700, test/loss=1.820891, test/num_examples=10000, total_duration=145210.505985, train/accuracy=0.959503, train/loss=0.152064, validation/accuracy=0.754860, validation/loss=1.043170, validation/num_examples=50000
I0301 21:53:42.604252 140523947947776 logging_writer.py:48] [415300] global_step=415300, grad_norm=4.361753463745117, loss=0.591685950756073
I0301 21:54:16.295617 140523092305664 logging_writer.py:48] [415400] global_step=415400, grad_norm=4.513477802276611, loss=0.6496320962905884
I0301 21:54:50.163879 140523947947776 logging_writer.py:48] [415500] global_step=415500, grad_norm=4.8542399406433105, loss=0.6475533843040466
I0301 21:55:23.911489 140523092305664 logging_writer.py:48] [415600] global_step=415600, grad_norm=4.489136219024658, loss=0.6293267607688904
I0301 21:55:57.696754 140523947947776 logging_writer.py:48] [415700] global_step=415700, grad_norm=4.1263532638549805, loss=0.5672636032104492
I0301 21:56:31.468354 140523092305664 logging_writer.py:48] [415800] global_step=415800, grad_norm=4.565803050994873, loss=0.6293047666549683
I0301 21:57:05.251777 140523947947776 logging_writer.py:48] [415900] global_step=415900, grad_norm=4.138345718383789, loss=0.6322060823440552
I0301 21:57:39.027410 140523092305664 logging_writer.py:48] [416000] global_step=416000, grad_norm=4.197295665740967, loss=0.6000133752822876
I0301 21:58:12.797303 140523947947776 logging_writer.py:48] [416100] global_step=416100, grad_norm=4.4437575340271, loss=0.6096235513687134
I0301 21:58:46.603492 140523092305664 logging_writer.py:48] [416200] global_step=416200, grad_norm=4.986929416656494, loss=0.6196267604827881
I0301 21:59:20.383639 140523947947776 logging_writer.py:48] [416300] global_step=416300, grad_norm=4.402435302734375, loss=0.5955663919448853
I0301 21:59:54.156163 140523092305664 logging_writer.py:48] [416400] global_step=416400, grad_norm=4.440390110015869, loss=0.6007236838340759
I0301 22:00:27.923657 140523947947776 logging_writer.py:48] [416500] global_step=416500, grad_norm=4.539361000061035, loss=0.6410561800003052
I0301 22:01:01.767824 140523092305664 logging_writer.py:48] [416600] global_step=416600, grad_norm=4.352126598358154, loss=0.65700364112854
I0301 22:01:35.553955 140523947947776 logging_writer.py:48] [416700] global_step=416700, grad_norm=4.64981746673584, loss=0.6258570551872253
I0301 22:02:05.447312 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:02:11.514698 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:02:20.174836 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:02:22.442693 140688601454400 submission_runner.py:411] Time since start: 145737.51s, 	Step: 416790, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14694274961948395, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0446892976760864, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8210554122924805, 'test/num_examples': 10000, 'score': 140821.24357557297, 'total_duration': 145737.5135421753, 'accumulated_submission_time': 140821.24357557297, 'accumulated_eval_time': 4881.290358066559, 'accumulated_logging_time': 20.661017656326294}
I0301 22:02:22.531530 140522597377792 logging_writer.py:48] [416790] accumulated_eval_time=4881.290358, accumulated_logging_time=20.661018, accumulated_submission_time=140821.243576, global_step=416790, preemption_count=0, score=140821.243576, test/accuracy=0.630500, test/loss=1.821055, test/num_examples=10000, total_duration=145737.513542, train/accuracy=0.961356, train/loss=0.146943, validation/accuracy=0.754920, validation/loss=1.044689, validation/num_examples=50000
I0301 22:02:26.257393 140522605770496 logging_writer.py:48] [416800] global_step=416800, grad_norm=4.715917587280273, loss=0.6327664852142334
I0301 22:02:59.935360 140522597377792 logging_writer.py:48] [416900] global_step=416900, grad_norm=4.707029342651367, loss=0.6223046183586121
I0301 22:03:33.729515 140522605770496 logging_writer.py:48] [417000] global_step=417000, grad_norm=4.355611324310303, loss=0.5717109441757202
I0301 22:04:07.516947 140522597377792 logging_writer.py:48] [417100] global_step=417100, grad_norm=4.6590704917907715, loss=0.6690676212310791
I0301 22:04:41.348651 140522605770496 logging_writer.py:48] [417200] global_step=417200, grad_norm=5.072075843811035, loss=0.683535099029541
I0301 22:05:15.105440 140522597377792 logging_writer.py:48] [417300] global_step=417300, grad_norm=4.73133659362793, loss=0.650781512260437
I0301 22:05:48.882515 140522605770496 logging_writer.py:48] [417400] global_step=417400, grad_norm=3.9773473739624023, loss=0.539033055305481
I0301 22:06:22.656626 140522597377792 logging_writer.py:48] [417500] global_step=417500, grad_norm=4.457729339599609, loss=0.7448588013648987
I0301 22:06:56.535417 140522605770496 logging_writer.py:48] [417600] global_step=417600, grad_norm=4.266192436218262, loss=0.6118319034576416
I0301 22:07:30.314660 140522597377792 logging_writer.py:48] [417700] global_step=417700, grad_norm=4.608669281005859, loss=0.6362411379814148
I0301 22:08:04.114559 140522605770496 logging_writer.py:48] [417800] global_step=417800, grad_norm=4.741885185241699, loss=0.7246641516685486
I0301 22:08:37.895315 140522597377792 logging_writer.py:48] [417900] global_step=417900, grad_norm=4.787755966186523, loss=0.6258497834205627
I0301 22:09:11.680106 140522605770496 logging_writer.py:48] [418000] global_step=418000, grad_norm=4.583202362060547, loss=0.5295453071594238
I0301 22:09:45.442759 140522597377792 logging_writer.py:48] [418100] global_step=418100, grad_norm=4.298493385314941, loss=0.5749834179878235
I0301 22:10:19.246343 140522605770496 logging_writer.py:48] [418200] global_step=418200, grad_norm=4.460250377655029, loss=0.5661655068397522
I0301 22:10:52.490986 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:10:58.501604 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:11:07.109835 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:11:09.364313 140688601454400 submission_runner.py:411] Time since start: 146264.44s, 	Step: 418300, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14551134407520294, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0434072017669678, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.82075834274292, 'test/num_examples': 10000, 'score': 141331.1424012184, 'total_duration': 146264.43515825272, 'accumulated_submission_time': 141331.1424012184, 'accumulated_eval_time': 4898.1636373996735, 'accumulated_logging_time': 20.759366750717163}
I0301 22:11:09.450866 140522597377792 logging_writer.py:48] [418300] accumulated_eval_time=4898.163637, accumulated_logging_time=20.759367, accumulated_submission_time=141331.142401, global_step=418300, preemption_count=0, score=141331.142401, test/accuracy=0.630200, test/loss=1.820758, test/num_examples=10000, total_duration=146264.435158, train/accuracy=0.961476, train/loss=0.145511, validation/accuracy=0.755160, validation/loss=1.043407, validation/num_examples=50000
I0301 22:11:09.801235 140523092305664 logging_writer.py:48] [418300] global_step=418300, grad_norm=4.55288553237915, loss=0.6342005729675293
I0301 22:11:43.523221 140522597377792 logging_writer.py:48] [418400] global_step=418400, grad_norm=4.445087909698486, loss=0.63459712266922
I0301 22:12:17.295421 140523092305664 logging_writer.py:48] [418500] global_step=418500, grad_norm=4.646306991577148, loss=0.6761547327041626
I0301 22:12:51.081572 140522597377792 logging_writer.py:48] [418600] global_step=418600, grad_norm=4.426868438720703, loss=0.5906703472137451
I0301 22:13:24.978642 140523092305664 logging_writer.py:48] [418700] global_step=418700, grad_norm=4.258171081542969, loss=0.5541446208953857
I0301 22:13:58.784397 140522597377792 logging_writer.py:48] [418800] global_step=418800, grad_norm=4.354518890380859, loss=0.5924152135848999
I0301 22:14:32.593459 140523092305664 logging_writer.py:48] [418900] global_step=418900, grad_norm=4.788269519805908, loss=0.7640941143035889
I0301 22:15:06.389049 140522597377792 logging_writer.py:48] [419000] global_step=419000, grad_norm=4.36751651763916, loss=0.5809696316719055
I0301 22:15:40.177071 140523092305664 logging_writer.py:48] [419100] global_step=419100, grad_norm=4.109920024871826, loss=0.6061481833457947
I0301 22:16:13.975203 140522597377792 logging_writer.py:48] [419200] global_step=419200, grad_norm=4.150158405303955, loss=0.6436323523521423
I0301 22:16:47.768271 140523092305664 logging_writer.py:48] [419300] global_step=419300, grad_norm=4.507394790649414, loss=0.6294999122619629
I0301 22:17:21.535873 140522597377792 logging_writer.py:48] [419400] global_step=419400, grad_norm=4.569843769073486, loss=0.5877376198768616
I0301 22:17:55.365339 140523092305664 logging_writer.py:48] [419500] global_step=419500, grad_norm=4.549807548522949, loss=0.6149390935897827
I0301 22:18:29.126920 140522597377792 logging_writer.py:48] [419600] global_step=419600, grad_norm=4.290805816650391, loss=0.6046168208122253
I0301 22:19:02.898428 140523092305664 logging_writer.py:48] [419700] global_step=419700, grad_norm=4.588361740112305, loss=0.617881178855896
I0301 22:19:36.789851 140522597377792 logging_writer.py:48] [419800] global_step=419800, grad_norm=4.3782219886779785, loss=0.5951435565948486
I0301 22:19:39.631026 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:19:45.860985 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:19:54.312989 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:19:56.603036 140688601454400 submission_runner.py:411] Time since start: 146791.67s, 	Step: 419810, 	{'train/accuracy': 0.9610769748687744, 'train/loss': 0.14620260894298553, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0435361862182617, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.822310447692871, 'test/num_examples': 10000, 'score': 141841.25842308998, 'total_duration': 146791.6738820076, 'accumulated_submission_time': 141841.25842308998, 'accumulated_eval_time': 4915.135595321655, 'accumulated_logging_time': 20.857829093933105}
I0301 22:19:56.691952 140525256554240 logging_writer.py:48] [419810] accumulated_eval_time=4915.135595, accumulated_logging_time=20.857829, accumulated_submission_time=141841.258423, global_step=419810, preemption_count=0, score=141841.258423, test/accuracy=0.631100, test/loss=1.822310, test/num_examples=10000, total_duration=146791.673882, train/accuracy=0.961077, train/loss=0.146203, validation/accuracy=0.755100, validation/loss=1.043536, validation/num_examples=50000
I0301 22:20:27.450087 140525264946944 logging_writer.py:48] [419900] global_step=419900, grad_norm=4.773200511932373, loss=0.6699063181877136
I0301 22:21:01.198576 140525256554240 logging_writer.py:48] [420000] global_step=420000, grad_norm=4.453476428985596, loss=0.6617847681045532
I0301 22:21:34.969678 140525264946944 logging_writer.py:48] [420100] global_step=420100, grad_norm=4.7552490234375, loss=0.6592626571655273
I0301 22:22:08.754711 140525256554240 logging_writer.py:48] [420200] global_step=420200, grad_norm=4.584261894226074, loss=0.6924696564674377
I0301 22:22:42.565785 140525264946944 logging_writer.py:48] [420300] global_step=420300, grad_norm=4.561102390289307, loss=0.5650042295455933
I0301 22:23:16.349977 140525256554240 logging_writer.py:48] [420400] global_step=420400, grad_norm=4.658801078796387, loss=0.6674178838729858
I0301 22:23:50.171985 140525264946944 logging_writer.py:48] [420500] global_step=420500, grad_norm=4.428450107574463, loss=0.6178948879241943
I0301 22:24:23.967634 140525256554240 logging_writer.py:48] [420600] global_step=420600, grad_norm=5.007737636566162, loss=0.7334305644035339
I0301 22:24:57.768748 140525264946944 logging_writer.py:48] [420700] global_step=420700, grad_norm=5.106288433074951, loss=0.6084140539169312
I0301 22:25:31.640956 140525256554240 logging_writer.py:48] [420800] global_step=420800, grad_norm=4.676057815551758, loss=0.5754437446594238
I0301 22:26:05.377021 140525264946944 logging_writer.py:48] [420900] global_step=420900, grad_norm=4.530523777008057, loss=0.610858678817749
I0301 22:26:39.122112 140525256554240 logging_writer.py:48] [421000] global_step=421000, grad_norm=4.6345415115356445, loss=0.5902994275093079
I0301 22:27:12.919803 140525264946944 logging_writer.py:48] [421100] global_step=421100, grad_norm=4.898165225982666, loss=0.6038932800292969
I0301 22:27:46.710740 140525256554240 logging_writer.py:48] [421200] global_step=421200, grad_norm=4.766628742218018, loss=0.577252984046936
I0301 22:28:20.465157 140525264946944 logging_writer.py:48] [421300] global_step=421300, grad_norm=4.403807640075684, loss=0.5940319895744324
I0301 22:28:26.703454 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:28:32.761143 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:28:41.384680 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:28:43.715754 140688601454400 submission_runner.py:411] Time since start: 147318.79s, 	Step: 421320, 	{'train/accuracy': 0.9599409699440002, 'train/loss': 0.14980603754520416, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0440946817398071, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8217707872390747, 'test/num_examples': 10000, 'score': 142351.206646204, 'total_duration': 147318.78655314445, 'accumulated_submission_time': 142351.206646204, 'accumulated_eval_time': 4932.147796154022, 'accumulated_logging_time': 20.957899570465088}
I0301 22:28:43.802596 140522588985088 logging_writer.py:48] [421320] accumulated_eval_time=4932.147796, accumulated_logging_time=20.957900, accumulated_submission_time=142351.206646, global_step=421320, preemption_count=0, score=142351.206646, test/accuracy=0.631600, test/loss=1.821771, test/num_examples=10000, total_duration=147318.786553, train/accuracy=0.959941, train/loss=0.149806, validation/accuracy=0.755000, validation/loss=1.044095, validation/num_examples=50000
I0301 22:29:11.123882 140522605770496 logging_writer.py:48] [421400] global_step=421400, grad_norm=4.438932418823242, loss=0.6163584589958191
I0301 22:29:44.845883 140522588985088 logging_writer.py:48] [421500] global_step=421500, grad_norm=4.475920677185059, loss=0.5955959558486938
I0301 22:30:18.630533 140522605770496 logging_writer.py:48] [421600] global_step=421600, grad_norm=4.741033554077148, loss=0.6490212082862854
I0301 22:30:52.412888 140522588985088 logging_writer.py:48] [421700] global_step=421700, grad_norm=4.773438930511475, loss=0.7157478332519531
I0301 22:31:26.218981 140522605770496 logging_writer.py:48] [421800] global_step=421800, grad_norm=4.2841877937316895, loss=0.5952309370040894
I0301 22:32:00.152524 140522588985088 logging_writer.py:48] [421900] global_step=421900, grad_norm=4.330414295196533, loss=0.6546019911766052
I0301 22:32:33.959443 140522605770496 logging_writer.py:48] [422000] global_step=422000, grad_norm=4.522346019744873, loss=0.5711817145347595
I0301 22:33:07.752596 140522588985088 logging_writer.py:48] [422100] global_step=422100, grad_norm=4.794912338256836, loss=0.7232605218887329
I0301 22:33:41.550094 140522605770496 logging_writer.py:48] [422200] global_step=422200, grad_norm=4.782957077026367, loss=0.631049633026123
I0301 22:34:15.335449 140522588985088 logging_writer.py:48] [422300] global_step=422300, grad_norm=3.856672525405884, loss=0.522997260093689
I0301 22:34:49.128157 140522605770496 logging_writer.py:48] [422400] global_step=422400, grad_norm=4.339632987976074, loss=0.6205968856811523
I0301 22:35:22.895812 140522588985088 logging_writer.py:48] [422500] global_step=422500, grad_norm=4.475902080535889, loss=0.6790151000022888
I0301 22:35:56.698512 140522605770496 logging_writer.py:48] [422600] global_step=422600, grad_norm=4.350194454193115, loss=0.6459237337112427
I0301 22:36:30.465082 140522588985088 logging_writer.py:48] [422700] global_step=422700, grad_norm=4.288804054260254, loss=0.6596782803535461
I0301 22:37:04.272186 140522605770496 logging_writer.py:48] [422800] global_step=422800, grad_norm=4.251955032348633, loss=0.5602340698242188
I0301 22:37:13.871550 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:37:20.553882 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:37:29.336653 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:37:31.637769 140688601454400 submission_runner.py:411] Time since start: 147846.71s, 	Step: 422830, 	{'train/accuracy': 0.9587252736091614, 'train/loss': 0.15140672028064728, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0427519083023071, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8215205669403076, 'test/num_examples': 10000, 'score': 142861.21098184586, 'total_duration': 147846.70860219002, 'accumulated_submission_time': 142861.21098184586, 'accumulated_eval_time': 4949.913951873779, 'accumulated_logging_time': 21.057546615600586}
I0301 22:37:31.724142 140525264946944 logging_writer.py:48] [422830] accumulated_eval_time=4949.913952, accumulated_logging_time=21.057547, accumulated_submission_time=142861.210982, global_step=422830, preemption_count=0, score=142861.210982, test/accuracy=0.630400, test/loss=1.821521, test/num_examples=10000, total_duration=147846.708602, train/accuracy=0.958725, train/loss=0.151407, validation/accuracy=0.755220, validation/loss=1.042752, validation/num_examples=50000
I0301 22:37:55.804194 140525273339648 logging_writer.py:48] [422900] global_step=422900, grad_norm=4.718602657318115, loss=0.6606482863426208
I0301 22:38:29.544917 140525264946944 logging_writer.py:48] [423000] global_step=423000, grad_norm=4.517679214477539, loss=0.628937304019928
I0301 22:39:03.323645 140525273339648 logging_writer.py:48] [423100] global_step=423100, grad_norm=4.5723724365234375, loss=0.6378558278083801
I0301 22:39:37.108298 140525264946944 logging_writer.py:48] [423200] global_step=423200, grad_norm=4.529402732849121, loss=0.6046873927116394
I0301 22:40:10.882920 140525273339648 logging_writer.py:48] [423300] global_step=423300, grad_norm=4.919272422790527, loss=0.6251645088195801
I0301 22:40:44.672798 140525264946944 logging_writer.py:48] [423400] global_step=423400, grad_norm=4.112966060638428, loss=0.6412577033042908
I0301 22:41:18.436848 140525273339648 logging_writer.py:48] [423500] global_step=423500, grad_norm=4.438286781311035, loss=0.5899427533149719
I0301 22:41:52.227938 140525264946944 logging_writer.py:48] [423600] global_step=423600, grad_norm=4.56740665435791, loss=0.6490365266799927
I0301 22:42:26.049144 140525273339648 logging_writer.py:48] [423700] global_step=423700, grad_norm=4.398025035858154, loss=0.6151571273803711
I0301 22:42:59.839744 140525264946944 logging_writer.py:48] [423800] global_step=423800, grad_norm=5.254139423370361, loss=0.7035185098648071
I0301 22:43:33.656427 140525273339648 logging_writer.py:48] [423900] global_step=423900, grad_norm=4.204742431640625, loss=0.5568728446960449
I0301 22:44:07.541348 140525264946944 logging_writer.py:48] [424000] global_step=424000, grad_norm=4.7003278732299805, loss=0.6707075834274292
I0301 22:44:41.334823 140525273339648 logging_writer.py:48] [424100] global_step=424100, grad_norm=4.556136131286621, loss=0.6435110569000244
I0301 22:45:15.121311 140525264946944 logging_writer.py:48] [424200] global_step=424200, grad_norm=4.222843170166016, loss=0.6027172803878784
I0301 22:45:48.899776 140525273339648 logging_writer.py:48] [424300] global_step=424300, grad_norm=4.772851467132568, loss=0.6253926753997803
I0301 22:46:01.869211 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:46:07.876727 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:46:16.590380 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:46:18.878848 140688601454400 submission_runner.py:411] Time since start: 148373.95s, 	Step: 424340, 	{'train/accuracy': 0.9601601958274841, 'train/loss': 0.1487235575914383, 'validation/accuracy': 0.7555199861526489, 'validation/loss': 1.0439789295196533, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.822304129600525, 'test/num_examples': 10000, 'score': 143371.2946677208, 'total_duration': 148373.94969177246, 'accumulated_submission_time': 143371.2946677208, 'accumulated_eval_time': 4966.923537492752, 'accumulated_logging_time': 21.153748273849487}
I0301 22:46:18.966747 140522597377792 logging_writer.py:48] [424340] accumulated_eval_time=4966.923537, accumulated_logging_time=21.153748, accumulated_submission_time=143371.294668, global_step=424340, preemption_count=0, score=143371.294668, test/accuracy=0.630200, test/loss=1.822304, test/num_examples=10000, total_duration=148373.949692, train/accuracy=0.960160, train/loss=0.148724, validation/accuracy=0.755520, validation/loss=1.043979, validation/num_examples=50000
I0301 22:46:39.544785 140522605770496 logging_writer.py:48] [424400] global_step=424400, grad_norm=4.6804728507995605, loss=0.6070740222930908
I0301 22:47:13.332243 140522597377792 logging_writer.py:48] [424500] global_step=424500, grad_norm=4.577844619750977, loss=0.5593997836112976
I0301 22:47:47.086534 140522605770496 logging_writer.py:48] [424600] global_step=424600, grad_norm=4.71170711517334, loss=0.6905978322029114
I0301 22:48:20.880158 140522597377792 logging_writer.py:48] [424700] global_step=424700, grad_norm=4.74923038482666, loss=0.6754641532897949
I0301 22:48:54.652648 140522605770496 logging_writer.py:48] [424800] global_step=424800, grad_norm=4.640775203704834, loss=0.5933631658554077
I0301 22:49:28.423851 140522597377792 logging_writer.py:48] [424900] global_step=424900, grad_norm=4.483248233795166, loss=0.639617919921875
I0301 22:50:02.206770 140522605770496 logging_writer.py:48] [425000] global_step=425000, grad_norm=4.593360900878906, loss=0.6255828142166138
I0301 22:50:36.037571 140522597377792 logging_writer.py:48] [425100] global_step=425100, grad_norm=4.708992958068848, loss=0.6719317436218262
I0301 22:51:09.794110 140522605770496 logging_writer.py:48] [425200] global_step=425200, grad_norm=4.968790054321289, loss=0.6218059062957764
I0301 22:51:43.602864 140522597377792 logging_writer.py:48] [425300] global_step=425300, grad_norm=4.331427574157715, loss=0.5172443389892578
I0301 22:52:17.387123 140522605770496 logging_writer.py:48] [425400] global_step=425400, grad_norm=4.443627834320068, loss=0.6321336627006531
I0301 22:52:51.170960 140522597377792 logging_writer.py:48] [425500] global_step=425500, grad_norm=4.993367671966553, loss=0.5833724737167358
I0301 22:53:24.947977 140522605770496 logging_writer.py:48] [425600] global_step=425600, grad_norm=4.998610019683838, loss=0.6537007093429565
I0301 22:53:58.673025 140522597377792 logging_writer.py:48] [425700] global_step=425700, grad_norm=4.613189220428467, loss=0.6413893699645996
I0301 22:54:32.460764 140522605770496 logging_writer.py:48] [425800] global_step=425800, grad_norm=4.929195404052734, loss=0.6729521751403809
I0301 22:54:49.150635 140688601454400 spec.py:321] Evaluating on the training split.
I0301 22:54:55.155434 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 22:55:03.970137 140688601454400 spec.py:349] Evaluating on the test split.
I0301 22:55:06.201133 140688601454400 submission_runner.py:411] Time since start: 148901.27s, 	Step: 425851, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14807626605033875, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0441192388534546, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.820621132850647, 'test/num_examples': 10000, 'score': 143881.41513490677, 'total_duration': 148901.27196621895, 'accumulated_submission_time': 143881.41513490677, 'accumulated_eval_time': 4983.973979711533, 'accumulated_logging_time': 21.251810312271118}
I0301 22:55:06.298526 140525264946944 logging_writer.py:48] [425851] accumulated_eval_time=4983.973980, accumulated_logging_time=21.251810, accumulated_submission_time=143881.415135, global_step=425851, preemption_count=0, score=143881.415135, test/accuracy=0.630900, test/loss=1.820621, test/num_examples=10000, total_duration=148901.271966, train/accuracy=0.961217, train/loss=0.148076, validation/accuracy=0.755340, validation/loss=1.044119, validation/num_examples=50000
I0301 22:55:23.184979 140525273339648 logging_writer.py:48] [425900] global_step=425900, grad_norm=4.631136894226074, loss=0.6509345769882202
I0301 22:55:56.923340 140525264946944 logging_writer.py:48] [426000] global_step=426000, grad_norm=5.198497295379639, loss=0.6720500588417053
I0301 22:56:30.730376 140525273339648 logging_writer.py:48] [426100] global_step=426100, grad_norm=4.514616012573242, loss=0.660145103931427
I0301 22:57:04.506186 140525264946944 logging_writer.py:48] [426200] global_step=426200, grad_norm=4.523733615875244, loss=0.6605963706970215
I0301 22:57:38.282080 140525273339648 logging_writer.py:48] [426300] global_step=426300, grad_norm=4.899161338806152, loss=0.6936227679252625
I0301 22:58:12.045048 140525264946944 logging_writer.py:48] [426400] global_step=426400, grad_norm=4.374824523925781, loss=0.5982894897460938
I0301 22:58:45.827481 140525273339648 logging_writer.py:48] [426500] global_step=426500, grad_norm=4.3482584953308105, loss=0.5871750116348267
I0301 22:59:19.565632 140525264946944 logging_writer.py:48] [426600] global_step=426600, grad_norm=4.621984958648682, loss=0.6695813536643982
I0301 22:59:53.337919 140525273339648 logging_writer.py:48] [426700] global_step=426700, grad_norm=4.394516944885254, loss=0.6208365559577942
I0301 23:00:27.081537 140525264946944 logging_writer.py:48] [426800] global_step=426800, grad_norm=4.785830974578857, loss=0.5662069916725159
I0301 23:01:00.864360 140525273339648 logging_writer.py:48] [426900] global_step=426900, grad_norm=4.359454154968262, loss=0.6318833827972412
I0301 23:01:34.642131 140525264946944 logging_writer.py:48] [427000] global_step=427000, grad_norm=4.923051357269287, loss=0.7493447065353394
I0301 23:02:08.430431 140525273339648 logging_writer.py:48] [427100] global_step=427100, grad_norm=4.682762622833252, loss=0.670489490032196
I0301 23:02:42.242107 140525264946944 logging_writer.py:48] [427200] global_step=427200, grad_norm=4.220869541168213, loss=0.6323800683021545
I0301 23:03:16.020670 140525273339648 logging_writer.py:48] [427300] global_step=427300, grad_norm=4.336970329284668, loss=0.5898758172988892
I0301 23:03:36.446104 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:03:42.463584 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:03:51.162486 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:03:53.424633 140688601454400 submission_runner.py:411] Time since start: 149428.50s, 	Step: 427362, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14516471326351166, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0440856218338013, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8233418464660645, 'test/num_examples': 10000, 'score': 144391.50057291985, 'total_duration': 149428.4954817295, 'accumulated_submission_time': 144391.50057291985, 'accumulated_eval_time': 5000.952465295792, 'accumulated_logging_time': 21.359248638153076}
I0301 23:03:53.521157 140523092305664 logging_writer.py:48] [427362] accumulated_eval_time=5000.952465, accumulated_logging_time=21.359249, accumulated_submission_time=144391.500573, global_step=427362, preemption_count=0, score=144391.500573, test/accuracy=0.631100, test/loss=1.823342, test/num_examples=10000, total_duration=149428.495482, train/accuracy=0.960938, train/loss=0.145165, validation/accuracy=0.754940, validation/loss=1.044086, validation/num_examples=50000
I0301 23:04:06.665317 140525281732352 logging_writer.py:48] [427400] global_step=427400, grad_norm=4.435294151306152, loss=0.6296517848968506
I0301 23:04:40.389516 140523092305664 logging_writer.py:48] [427500] global_step=427500, grad_norm=4.773784637451172, loss=0.610244631767273
I0301 23:05:14.154061 140525281732352 logging_writer.py:48] [427600] global_step=427600, grad_norm=4.464138031005859, loss=0.6738903522491455
I0301 23:05:48.223969 140523092305664 logging_writer.py:48] [427700] global_step=427700, grad_norm=4.485899925231934, loss=0.6137265563011169
I0301 23:06:21.983172 140525281732352 logging_writer.py:48] [427800] global_step=427800, grad_norm=4.251833438873291, loss=0.6006978750228882
I0301 23:06:55.748650 140523092305664 logging_writer.py:48] [427900] global_step=427900, grad_norm=4.498961925506592, loss=0.5436453223228455
I0301 23:07:29.498798 140525281732352 logging_writer.py:48] [428000] global_step=428000, grad_norm=4.366846084594727, loss=0.6549595594406128
I0301 23:08:03.286512 140523092305664 logging_writer.py:48] [428100] global_step=428100, grad_norm=4.11460018157959, loss=0.6285149455070496
I0301 23:08:37.170063 140525281732352 logging_writer.py:48] [428200] global_step=428200, grad_norm=4.647759437561035, loss=0.6920507550239563
I0301 23:09:10.961613 140523092305664 logging_writer.py:48] [428300] global_step=428300, grad_norm=4.433953285217285, loss=0.6189941167831421
I0301 23:09:44.764414 140525281732352 logging_writer.py:48] [428400] global_step=428400, grad_norm=4.372926235198975, loss=0.6155182123184204
I0301 23:10:18.535562 140523092305664 logging_writer.py:48] [428500] global_step=428500, grad_norm=4.515355587005615, loss=0.6523007154464722
I0301 23:10:52.343668 140525281732352 logging_writer.py:48] [428600] global_step=428600, grad_norm=4.227441787719727, loss=0.5958563089370728
I0301 23:11:26.116983 140523092305664 logging_writer.py:48] [428700] global_step=428700, grad_norm=4.546302318572998, loss=0.6492687463760376
I0301 23:11:59.917723 140525281732352 logging_writer.py:48] [428800] global_step=428800, grad_norm=4.448453426361084, loss=0.5832281112670898
I0301 23:12:23.713343 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:12:29.678729 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:12:38.194963 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:12:40.475349 140688601454400 submission_runner.py:411] Time since start: 149955.55s, 	Step: 428872, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.1478695422410965, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0440679788589478, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8220691680908203, 'test/num_examples': 10000, 'score': 144901.63012313843, 'total_duration': 149955.54619264603, 'accumulated_submission_time': 144901.63012313843, 'accumulated_eval_time': 5017.714422941208, 'accumulated_logging_time': 21.466845512390137}
I0301 23:12:40.566487 140525264946944 logging_writer.py:48] [428872] accumulated_eval_time=5017.714423, accumulated_logging_time=21.466846, accumulated_submission_time=144901.630123, global_step=428872, preemption_count=0, score=144901.630123, test/accuracy=0.630400, test/loss=1.822069, test/num_examples=10000, total_duration=149955.546193, train/accuracy=0.960061, train/loss=0.147870, validation/accuracy=0.755240, validation/loss=1.044068, validation/num_examples=50000
I0301 23:12:50.364730 140525281732352 logging_writer.py:48] [428900] global_step=428900, grad_norm=4.651461124420166, loss=0.6918336749076843
I0301 23:13:24.078266 140525264946944 logging_writer.py:48] [429000] global_step=429000, grad_norm=4.440923690795898, loss=0.5961127877235413
I0301 23:13:57.830874 140525281732352 logging_writer.py:48] [429100] global_step=429100, grad_norm=4.440268039703369, loss=0.5955897569656372
I0301 23:14:31.577484 140525264946944 logging_writer.py:48] [429200] global_step=429200, grad_norm=4.142629623413086, loss=0.5447028279304504
I0301 23:15:05.446659 140525281732352 logging_writer.py:48] [429300] global_step=429300, grad_norm=4.386274337768555, loss=0.6800327897071838
I0301 23:15:39.212269 140525264946944 logging_writer.py:48] [429400] global_step=429400, grad_norm=4.792577743530273, loss=0.6595010161399841
I0301 23:16:12.970384 140525281732352 logging_writer.py:48] [429500] global_step=429500, grad_norm=4.22286319732666, loss=0.6480671167373657
I0301 23:16:46.730602 140525264946944 logging_writer.py:48] [429600] global_step=429600, grad_norm=4.817570209503174, loss=0.5841404795646667
I0301 23:17:20.499241 140525281732352 logging_writer.py:48] [429700] global_step=429700, grad_norm=4.953869819641113, loss=0.6099632382392883
I0301 23:17:54.266469 140525264946944 logging_writer.py:48] [429800] global_step=429800, grad_norm=4.520864963531494, loss=0.5900999903678894
I0301 23:18:28.053362 140525281732352 logging_writer.py:48] [429900] global_step=429900, grad_norm=4.317840099334717, loss=0.6500257253646851
I0301 23:19:01.804246 140525264946944 logging_writer.py:48] [430000] global_step=430000, grad_norm=4.816654205322266, loss=0.6927944421768188
I0301 23:19:35.573236 140525281732352 logging_writer.py:48] [430100] global_step=430100, grad_norm=5.064945220947266, loss=0.667194128036499
I0301 23:20:09.384758 140525264946944 logging_writer.py:48] [430200] global_step=430200, grad_norm=4.201411724090576, loss=0.5796977281570435
I0301 23:20:43.089125 140525281732352 logging_writer.py:48] [430300] global_step=430300, grad_norm=4.768198013305664, loss=0.6949895024299622
I0301 23:21:10.626302 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:21:16.599088 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:21:25.126827 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:21:27.387969 140688601454400 submission_runner.py:411] Time since start: 150482.46s, 	Step: 430383, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14715412259101868, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0442098379135132, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8212717771530151, 'test/num_examples': 10000, 'score': 145411.6282696724, 'total_duration': 150482.45879983902, 'accumulated_submission_time': 145411.6282696724, 'accumulated_eval_time': 5034.476024627686, 'accumulated_logging_time': 21.568559885025024}
I0301 23:21:27.475800 140522404443904 logging_writer.py:48] [430383] accumulated_eval_time=5034.476025, accumulated_logging_time=21.568560, accumulated_submission_time=145411.628270, global_step=430383, preemption_count=0, score=145411.628270, test/accuracy=0.631400, test/loss=1.821272, test/num_examples=10000, total_duration=150482.458800, train/accuracy=0.960698, train/loss=0.147154, validation/accuracy=0.755060, validation/loss=1.044210, validation/num_examples=50000
I0301 23:21:33.550758 140522554390272 logging_writer.py:48] [430400] global_step=430400, grad_norm=4.970178127288818, loss=0.637515664100647
I0301 23:22:07.273168 140522404443904 logging_writer.py:48] [430500] global_step=430500, grad_norm=5.335400581359863, loss=0.7087348699569702
I0301 23:22:40.982635 140522554390272 logging_writer.py:48] [430600] global_step=430600, grad_norm=4.242117404937744, loss=0.6005327105522156
I0301 23:23:14.788019 140522404443904 logging_writer.py:48] [430700] global_step=430700, grad_norm=4.325376510620117, loss=0.6017178297042847
I0301 23:23:48.563719 140522554390272 logging_writer.py:48] [430800] global_step=430800, grad_norm=4.760018348693848, loss=0.6830495595932007
I0301 23:24:22.347429 140522404443904 logging_writer.py:48] [430900] global_step=430900, grad_norm=4.677319049835205, loss=0.6471683979034424
I0301 23:24:56.114120 140522554390272 logging_writer.py:48] [431000] global_step=431000, grad_norm=4.471074104309082, loss=0.5967018008232117
I0301 23:25:29.883795 140522404443904 logging_writer.py:48] [431100] global_step=431100, grad_norm=4.39204740524292, loss=0.5803530216217041
I0301 23:26:03.663856 140522554390272 logging_writer.py:48] [431200] global_step=431200, grad_norm=4.3643388748168945, loss=0.5616600513458252
I0301 23:26:37.483766 140522404443904 logging_writer.py:48] [431300] global_step=431300, grad_norm=4.17832088470459, loss=0.5675106048583984
I0301 23:27:11.326214 140522554390272 logging_writer.py:48] [431400] global_step=431400, grad_norm=4.5026044845581055, loss=0.6180424094200134
I0301 23:27:45.109355 140522404443904 logging_writer.py:48] [431500] global_step=431500, grad_norm=4.175561428070068, loss=0.6062120795249939
I0301 23:28:18.908610 140522554390272 logging_writer.py:48] [431600] global_step=431600, grad_norm=4.214549541473389, loss=0.5410739779472351
I0301 23:28:52.661893 140522404443904 logging_writer.py:48] [431700] global_step=431700, grad_norm=4.512565612792969, loss=0.636681079864502
I0301 23:29:26.459398 140522554390272 logging_writer.py:48] [431800] global_step=431800, grad_norm=4.384005069732666, loss=0.5946046710014343
I0301 23:29:57.679909 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:30:03.857673 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:30:12.531774 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:30:14.795732 140688601454400 submission_runner.py:411] Time since start: 151009.87s, 	Step: 431894, 	{'train/accuracy': 0.9595025181770325, 'train/loss': 0.15168562531471252, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.0435458421707153, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8222107887268066, 'test/num_examples': 10000, 'score': 145921.7688229084, 'total_duration': 151009.8665678501, 'accumulated_submission_time': 145921.7688229084, 'accumulated_eval_time': 5051.591791629791, 'accumulated_logging_time': 21.6670138835907}
I0301 23:30:14.886542 140522396051200 logging_writer.py:48] [431894] accumulated_eval_time=5051.591792, accumulated_logging_time=21.667014, accumulated_submission_time=145921.768823, global_step=431894, preemption_count=0, score=145921.768823, test/accuracy=0.630800, test/loss=1.822211, test/num_examples=10000, total_duration=151009.866568, train/accuracy=0.959503, train/loss=0.151686, validation/accuracy=0.755420, validation/loss=1.043546, validation/num_examples=50000
I0301 23:30:17.254719 140522404443904 logging_writer.py:48] [431900] global_step=431900, grad_norm=4.811083793640137, loss=0.6498881578445435
I0301 23:30:50.991385 140522396051200 logging_writer.py:48] [432000] global_step=432000, grad_norm=4.793205261230469, loss=0.6870843172073364
I0301 23:31:24.724003 140522404443904 logging_writer.py:48] [432100] global_step=432100, grad_norm=4.412971019744873, loss=0.6170212030410767
I0301 23:31:58.511426 140522396051200 logging_writer.py:48] [432200] global_step=432200, grad_norm=4.313453197479248, loss=0.6052279472351074
I0301 23:32:32.295320 140522404443904 logging_writer.py:48] [432300] global_step=432300, grad_norm=4.503416061401367, loss=0.6453664302825928
I0301 23:33:06.111093 140522396051200 logging_writer.py:48] [432400] global_step=432400, grad_norm=4.223410606384277, loss=0.6650904417037964
I0301 23:33:39.932846 140522404443904 logging_writer.py:48] [432500] global_step=432500, grad_norm=4.148090839385986, loss=0.5862903594970703
I0301 23:34:13.726105 140522396051200 logging_writer.py:48] [432600] global_step=432600, grad_norm=4.6328535079956055, loss=0.6831283569335938
I0301 23:34:47.490603 140522404443904 logging_writer.py:48] [432700] global_step=432700, grad_norm=4.638996601104736, loss=0.673977255821228
I0301 23:35:21.321391 140522396051200 logging_writer.py:48] [432800] global_step=432800, grad_norm=4.563700199127197, loss=0.5488653182983398
I0301 23:35:55.122641 140522404443904 logging_writer.py:48] [432900] global_step=432900, grad_norm=4.562947750091553, loss=0.6336197853088379
I0301 23:36:28.941730 140522396051200 logging_writer.py:48] [433000] global_step=433000, grad_norm=4.436980724334717, loss=0.6300536394119263
I0301 23:37:02.719803 140522404443904 logging_writer.py:48] [433100] global_step=433100, grad_norm=4.743030071258545, loss=0.6774179935455322
I0301 23:37:36.485888 140522396051200 logging_writer.py:48] [433200] global_step=433200, grad_norm=4.007693767547607, loss=0.604193925857544
I0301 23:38:10.252522 140522404443904 logging_writer.py:48] [433300] global_step=433300, grad_norm=4.717127799987793, loss=0.6698512434959412
I0301 23:38:44.025297 140522396051200 logging_writer.py:48] [433400] global_step=433400, grad_norm=4.674705505371094, loss=0.6491012573242188
I0301 23:38:44.856310 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:38:50.857597 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:38:59.340411 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:39:01.651443 140688601454400 submission_runner.py:411] Time since start: 151536.72s, 	Step: 433404, 	{'train/accuracy': 0.962312638759613, 'train/loss': 0.14179004728794098, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.044046401977539, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8215786218643188, 'test/num_examples': 10000, 'score': 146431.67700624466, 'total_duration': 151536.72223758698, 'accumulated_submission_time': 146431.67700624466, 'accumulated_eval_time': 5068.386824846268, 'accumulated_logging_time': 21.767746925354004}
I0301 23:39:01.786144 140523092305664 logging_writer.py:48] [433404] accumulated_eval_time=5068.386825, accumulated_logging_time=21.767747, accumulated_submission_time=146431.677006, global_step=433404, preemption_count=0, score=146431.677006, test/accuracy=0.631500, test/loss=1.821579, test/num_examples=10000, total_duration=151536.722238, train/accuracy=0.962313, train/loss=0.141790, validation/accuracy=0.755300, validation/loss=1.044046, validation/num_examples=50000
I0301 23:39:34.627764 140523947947776 logging_writer.py:48] [433500] global_step=433500, grad_norm=4.421830654144287, loss=0.6469026803970337
I0301 23:40:08.375578 140523092305664 logging_writer.py:48] [433600] global_step=433600, grad_norm=4.627832889556885, loss=0.6705167293548584
I0301 23:40:42.131189 140523947947776 logging_writer.py:48] [433700] global_step=433700, grad_norm=4.527007102966309, loss=0.6169697046279907
I0301 23:41:15.907983 140523092305664 logging_writer.py:48] [433800] global_step=433800, grad_norm=4.436763286590576, loss=0.5787906646728516
I0301 23:41:49.693458 140523947947776 logging_writer.py:48] [433900] global_step=433900, grad_norm=4.791573524475098, loss=0.668858528137207
I0301 23:42:23.476645 140523092305664 logging_writer.py:48] [434000] global_step=434000, grad_norm=4.756532669067383, loss=0.5967205762863159
I0301 23:42:57.268419 140523947947776 logging_writer.py:48] [434100] global_step=434100, grad_norm=4.661696434020996, loss=0.6623955965042114
I0301 23:43:31.068567 140523092305664 logging_writer.py:48] [434200] global_step=434200, grad_norm=4.745924472808838, loss=0.6644279956817627
I0301 23:44:04.867329 140523947947776 logging_writer.py:48] [434300] global_step=434300, grad_norm=4.306668758392334, loss=0.5985158085823059
I0301 23:44:38.677443 140523092305664 logging_writer.py:48] [434400] global_step=434400, grad_norm=4.249149322509766, loss=0.595181941986084
I0301 23:45:12.489449 140523947947776 logging_writer.py:48] [434500] global_step=434500, grad_norm=4.360635757446289, loss=0.6257951259613037
I0301 23:45:46.356060 140523092305664 logging_writer.py:48] [434600] global_step=434600, grad_norm=4.521358966827393, loss=0.6108605861663818
I0301 23:46:20.132036 140523947947776 logging_writer.py:48] [434700] global_step=434700, grad_norm=4.268963813781738, loss=0.6071357727050781
I0301 23:46:53.931602 140523092305664 logging_writer.py:48] [434800] global_step=434800, grad_norm=4.866312503814697, loss=0.664506196975708
I0301 23:47:27.701082 140523947947776 logging_writer.py:48] [434900] global_step=434900, grad_norm=4.437471389770508, loss=0.5673142075538635
I0301 23:47:31.909351 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:47:37.896743 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:47:46.566499 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:47:48.836380 140688601454400 submission_runner.py:411] Time since start: 152063.91s, 	Step: 434914, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14392603933811188, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0445345640182495, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8237618207931519, 'test/num_examples': 10000, 'score': 146941.73819971085, 'total_duration': 152063.90723013878, 'accumulated_submission_time': 146941.73819971085, 'accumulated_eval_time': 5085.313814640045, 'accumulated_logging_time': 21.91230607032776}
I0301 23:47:48.923996 140522404443904 logging_writer.py:48] [434914] accumulated_eval_time=5085.313815, accumulated_logging_time=21.912306, accumulated_submission_time=146941.738200, global_step=434914, preemption_count=0, score=146941.738200, test/accuracy=0.631100, test/loss=1.823762, test/num_examples=10000, total_duration=152063.907230, train/accuracy=0.961276, train/loss=0.143926, validation/accuracy=0.754920, validation/loss=1.044535, validation/num_examples=50000
I0301 23:48:18.238302 140522554390272 logging_writer.py:48] [435000] global_step=435000, grad_norm=4.481603622436523, loss=0.5633677244186401
I0301 23:48:51.952376 140522404443904 logging_writer.py:48] [435100] global_step=435100, grad_norm=5.105391979217529, loss=0.6435957551002502
I0301 23:49:25.713686 140522554390272 logging_writer.py:48] [435200] global_step=435200, grad_norm=4.303703308105469, loss=0.6026437282562256
I0301 23:49:59.511276 140522404443904 logging_writer.py:48] [435300] global_step=435300, grad_norm=4.517386436462402, loss=0.6710466742515564
I0301 23:50:33.294883 140522554390272 logging_writer.py:48] [435400] global_step=435400, grad_norm=4.877131462097168, loss=0.6400202512741089
I0301 23:51:07.098942 140522404443904 logging_writer.py:48] [435500] global_step=435500, grad_norm=4.190773010253906, loss=0.568270206451416
I0301 23:51:40.906095 140522554390272 logging_writer.py:48] [435600] global_step=435600, grad_norm=4.113007068634033, loss=0.5622791051864624
I0301 23:52:14.765270 140522404443904 logging_writer.py:48] [435700] global_step=435700, grad_norm=4.513208389282227, loss=0.684273898601532
I0301 23:52:48.567771 140522554390272 logging_writer.py:48] [435800] global_step=435800, grad_norm=4.560414791107178, loss=0.7515178322792053
I0301 23:53:22.318367 140522404443904 logging_writer.py:48] [435900] global_step=435900, grad_norm=4.463180065155029, loss=0.6355714201927185
I0301 23:53:56.089342 140522554390272 logging_writer.py:48] [436000] global_step=436000, grad_norm=4.1052751541137695, loss=0.5631236433982849
I0301 23:54:29.878905 140522404443904 logging_writer.py:48] [436100] global_step=436100, grad_norm=4.672128677368164, loss=0.6979837417602539
I0301 23:55:03.659133 140522554390272 logging_writer.py:48] [436200] global_step=436200, grad_norm=4.246841907501221, loss=0.6439116597175598
I0301 23:55:37.464050 140522404443904 logging_writer.py:48] [436300] global_step=436300, grad_norm=4.416221618652344, loss=0.6122768521308899
I0301 23:56:11.249601 140522554390272 logging_writer.py:48] [436400] global_step=436400, grad_norm=4.167027950286865, loss=0.6469805836677551
I0301 23:56:18.843379 140688601454400 spec.py:321] Evaluating on the training split.
I0301 23:56:24.830544 140688601454400 spec.py:333] Evaluating on the validation split.
I0301 23:56:33.350379 140688601454400 spec.py:349] Evaluating on the test split.
I0301 23:56:35.639612 140688601454400 submission_runner.py:411] Time since start: 152590.71s, 	Step: 436424, 	{'train/accuracy': 0.9592036008834839, 'train/loss': 0.1486395001411438, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.045411467552185, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8222869634628296, 'test/num_examples': 10000, 'score': 147451.59631967545, 'total_duration': 152590.7104575634, 'accumulated_submission_time': 147451.59631967545, 'accumulated_eval_time': 5102.110006809235, 'accumulated_logging_time': 22.009751081466675}
I0301 23:56:35.734221 140522404443904 logging_writer.py:48] [436424] accumulated_eval_time=5102.110007, accumulated_logging_time=22.009751, accumulated_submission_time=147451.596320, global_step=436424, preemption_count=0, score=147451.596320, test/accuracy=0.630700, test/loss=1.822287, test/num_examples=10000, total_duration=152590.710458, train/accuracy=0.959204, train/loss=0.148640, validation/accuracy=0.754800, validation/loss=1.045411, validation/num_examples=50000
I0301 23:57:01.678360 140523947947776 logging_writer.py:48] [436500] global_step=436500, grad_norm=4.724207401275635, loss=0.5887764692306519
I0301 23:57:35.424858 140522404443904 logging_writer.py:48] [436600] global_step=436600, grad_norm=4.696056842803955, loss=0.6324504017829895
I0301 23:58:09.254909 140523947947776 logging_writer.py:48] [436700] global_step=436700, grad_norm=4.589882850646973, loss=0.6762515306472778
I0301 23:58:43.027607 140522404443904 logging_writer.py:48] [436800] global_step=436800, grad_norm=5.000338077545166, loss=0.6503764986991882
I0301 23:59:16.787896 140523947947776 logging_writer.py:48] [436900] global_step=436900, grad_norm=4.529641151428223, loss=0.5811004638671875
I0301 23:59:50.583982 140522404443904 logging_writer.py:48] [437000] global_step=437000, grad_norm=4.931578159332275, loss=0.6551088094711304
I0302 00:00:24.355119 140523947947776 logging_writer.py:48] [437100] global_step=437100, grad_norm=4.573035717010498, loss=0.6328742504119873
I0302 00:00:58.182987 140522404443904 logging_writer.py:48] [437200] global_step=437200, grad_norm=4.7357916831970215, loss=0.6322286128997803
I0302 00:01:31.989495 140523947947776 logging_writer.py:48] [437300] global_step=437300, grad_norm=4.806100368499756, loss=0.705344557762146
I0302 00:02:05.782718 140522404443904 logging_writer.py:48] [437400] global_step=437400, grad_norm=4.443447113037109, loss=0.6546807289123535
I0302 00:02:39.567673 140523947947776 logging_writer.py:48] [437500] global_step=437500, grad_norm=4.52567720413208, loss=0.6408352851867676
I0302 00:03:13.387951 140522404443904 logging_writer.py:48] [437600] global_step=437600, grad_norm=4.537896156311035, loss=0.6798863410949707
I0302 00:03:47.178371 140523947947776 logging_writer.py:48] [437700] global_step=437700, grad_norm=4.286093711853027, loss=0.6527345180511475
I0302 00:04:20.998916 140522404443904 logging_writer.py:48] [437800] global_step=437800, grad_norm=4.232166767120361, loss=0.6348868012428284
I0302 00:04:54.781374 140523947947776 logging_writer.py:48] [437900] global_step=437900, grad_norm=4.639864444732666, loss=0.6387717127799988
I0302 00:05:05.719608 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:05:11.737977 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:05:20.271378 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:05:22.568472 140688601454400 submission_runner.py:411] Time since start: 153117.64s, 	Step: 437934, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.15037216246128082, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.042616367340088, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8189860582351685, 'test/num_examples': 10000, 'score': 147961.5187318325, 'total_duration': 153117.63931131363, 'accumulated_submission_time': 147961.5187318325, 'accumulated_eval_time': 5118.958813905716, 'accumulated_logging_time': 22.115402460098267}
I0302 00:05:22.659172 140525256554240 logging_writer.py:48] [437934] accumulated_eval_time=5118.958814, accumulated_logging_time=22.115402, accumulated_submission_time=147961.518732, global_step=437934, preemption_count=0, score=147961.518732, test/accuracy=0.630600, test/loss=1.818986, test/num_examples=10000, total_duration=153117.639311, train/accuracy=0.960738, train/loss=0.150372, validation/accuracy=0.754980, validation/loss=1.042616, validation/num_examples=50000
I0302 00:05:45.236760 140525264946944 logging_writer.py:48] [438000] global_step=438000, grad_norm=4.688049793243408, loss=0.586254358291626
I0302 00:06:18.996088 140525256554240 logging_writer.py:48] [438100] global_step=438100, grad_norm=4.745445251464844, loss=0.6365121006965637
I0302 00:06:52.749911 140525264946944 logging_writer.py:48] [438200] global_step=438200, grad_norm=4.401061534881592, loss=0.6857103109359741
I0302 00:07:26.534973 140525256554240 logging_writer.py:48] [438300] global_step=438300, grad_norm=4.4502644538879395, loss=0.6554529666900635
I0302 00:08:00.264107 140525264946944 logging_writer.py:48] [438400] global_step=438400, grad_norm=4.377872943878174, loss=0.6609012484550476
I0302 00:08:34.011523 140525256554240 logging_writer.py:48] [438500] global_step=438500, grad_norm=4.361068248748779, loss=0.5848676562309265
I0302 00:09:07.777885 140525264946944 logging_writer.py:48] [438600] global_step=438600, grad_norm=4.626306533813477, loss=0.6910141706466675
I0302 00:09:41.564449 140525256554240 logging_writer.py:48] [438700] global_step=438700, grad_norm=4.557036876678467, loss=0.613080620765686
I0302 00:10:15.339452 140525264946944 logging_writer.py:48] [438800] global_step=438800, grad_norm=4.874541759490967, loss=0.5756223201751709
I0302 00:10:49.213138 140525256554240 logging_writer.py:48] [438900] global_step=438900, grad_norm=4.178845405578613, loss=0.6051532626152039
I0302 00:11:23.042394 140525264946944 logging_writer.py:48] [439000] global_step=439000, grad_norm=5.042102336883545, loss=0.6174725294113159
I0302 00:11:56.862356 140525256554240 logging_writer.py:48] [439100] global_step=439100, grad_norm=5.211867809295654, loss=0.6942050457000732
I0302 00:12:30.645760 140525264946944 logging_writer.py:48] [439200] global_step=439200, grad_norm=4.229194641113281, loss=0.6069482564926147
I0302 00:13:04.450319 140525256554240 logging_writer.py:48] [439300] global_step=439300, grad_norm=4.3138933181762695, loss=0.5838059782981873
I0302 00:13:38.243408 140525264946944 logging_writer.py:48] [439400] global_step=439400, grad_norm=4.575766086578369, loss=0.7057978510856628
I0302 00:13:52.595765 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:13:58.741832 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:14:07.545487 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:14:09.803882 140688601454400 submission_runner.py:411] Time since start: 153644.87s, 	Step: 439444, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.1458749920129776, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0434237718582153, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8204666376113892, 'test/num_examples': 10000, 'score': 148471.39300227165, 'total_duration': 153644.87471556664, 'accumulated_submission_time': 148471.39300227165, 'accumulated_eval_time': 5136.166883468628, 'accumulated_logging_time': 22.215853929519653}
I0302 00:14:09.893934 140522404443904 logging_writer.py:48] [439444] accumulated_eval_time=5136.166883, accumulated_logging_time=22.215854, accumulated_submission_time=148471.393002, global_step=439444, preemption_count=0, score=148471.393002, test/accuracy=0.632000, test/loss=1.820467, test/num_examples=10000, total_duration=153644.874716, train/accuracy=0.960858, train/loss=0.145875, validation/accuracy=0.755020, validation/loss=1.043424, validation/num_examples=50000
I0302 00:14:29.120524 140522554390272 logging_writer.py:48] [439500] global_step=439500, grad_norm=4.548542499542236, loss=0.6816335916519165
I0302 00:15:02.837472 140522404443904 logging_writer.py:48] [439600] global_step=439600, grad_norm=4.319997310638428, loss=0.6572479009628296
I0302 00:15:36.594013 140522554390272 logging_writer.py:48] [439700] global_step=439700, grad_norm=4.716182231903076, loss=0.6011064648628235
I0302 00:16:10.359021 140522404443904 logging_writer.py:48] [439800] global_step=439800, grad_norm=4.237198829650879, loss=0.6096105575561523
I0302 00:16:44.197224 140522554390272 logging_writer.py:48] [439900] global_step=439900, grad_norm=4.539857387542725, loss=0.6260077953338623
I0302 00:17:17.999413 140522404443904 logging_writer.py:48] [440000] global_step=440000, grad_norm=4.2614593505859375, loss=0.6601099371910095
I0302 00:17:51.800921 140522554390272 logging_writer.py:48] [440100] global_step=440100, grad_norm=4.436349391937256, loss=0.6251469254493713
I0302 00:18:25.529717 140522404443904 logging_writer.py:48] [440200] global_step=440200, grad_norm=4.468533515930176, loss=0.6323826313018799
I0302 00:18:59.309620 140522554390272 logging_writer.py:48] [440300] global_step=440300, grad_norm=3.6570799350738525, loss=0.48898518085479736
I0302 00:19:33.105358 140522404443904 logging_writer.py:48] [440400] global_step=440400, grad_norm=4.259061813354492, loss=0.6670415997505188
I0302 00:20:06.911273 140522554390272 logging_writer.py:48] [440500] global_step=440500, grad_norm=4.077841758728027, loss=0.6123757362365723
I0302 00:20:40.677038 140522404443904 logging_writer.py:48] [440600] global_step=440600, grad_norm=4.13184928894043, loss=0.622054934501648
I0302 00:21:14.461503 140522554390272 logging_writer.py:48] [440700] global_step=440700, grad_norm=4.3911356925964355, loss=0.6236326694488525
I0302 00:21:48.270907 140522404443904 logging_writer.py:48] [440800] global_step=440800, grad_norm=4.358722686767578, loss=0.6522771120071411
I0302 00:22:22.103599 140522554390272 logging_writer.py:48] [440900] global_step=440900, grad_norm=4.202583312988281, loss=0.5711694359779358
I0302 00:22:39.872593 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:22:45.919469 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:22:54.586469 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:22:56.840817 140688601454400 submission_runner.py:411] Time since start: 154171.91s, 	Step: 440954, 	{'train/accuracy': 0.9615752100944519, 'train/loss': 0.1486123651266098, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.0435887575149536, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8217567205429077, 'test/num_examples': 10000, 'score': 148981.3084001541, 'total_duration': 154171.9116613865, 'accumulated_submission_time': 148981.3084001541, 'accumulated_eval_time': 5153.135057687759, 'accumulated_logging_time': 22.316500425338745}
I0302 00:22:56.938491 140522396051200 logging_writer.py:48] [440954] accumulated_eval_time=5153.135058, accumulated_logging_time=22.316500, accumulated_submission_time=148981.308400, global_step=440954, preemption_count=0, score=148981.308400, test/accuracy=0.630200, test/loss=1.821757, test/num_examples=10000, total_duration=154171.911661, train/accuracy=0.961575, train/loss=0.148612, validation/accuracy=0.755300, validation/loss=1.043589, validation/num_examples=50000
I0302 00:23:12.801815 140522404443904 logging_writer.py:48] [441000] global_step=441000, grad_norm=4.762357711791992, loss=0.6646362543106079
I0302 00:23:46.488967 140522396051200 logging_writer.py:48] [441100] global_step=441100, grad_norm=4.707937717437744, loss=0.6252659559249878
I0302 00:24:20.246122 140522404443904 logging_writer.py:48] [441200] global_step=441200, grad_norm=4.714454650878906, loss=0.6720554828643799
I0302 00:24:54.017938 140522396051200 logging_writer.py:48] [441300] global_step=441300, grad_norm=4.623484134674072, loss=0.6614637970924377
I0302 00:25:27.812389 140522404443904 logging_writer.py:48] [441400] global_step=441400, grad_norm=4.55213737487793, loss=0.6369472146034241
I0302 00:26:01.623580 140522396051200 logging_writer.py:48] [441500] global_step=441500, grad_norm=4.1056013107299805, loss=0.5797672867774963
I0302 00:26:35.423914 140522404443904 logging_writer.py:48] [441600] global_step=441600, grad_norm=4.383605480194092, loss=0.5990625619888306
I0302 00:27:09.183821 140522396051200 logging_writer.py:48] [441700] global_step=441700, grad_norm=4.489039421081543, loss=0.6231621503829956
I0302 00:27:42.969704 140522404443904 logging_writer.py:48] [441800] global_step=441800, grad_norm=4.1948418617248535, loss=0.619765043258667
I0302 00:28:16.762380 140522396051200 logging_writer.py:48] [441900] global_step=441900, grad_norm=4.488875389099121, loss=0.6230494379997253
I0302 00:28:50.568323 140522404443904 logging_writer.py:48] [442000] global_step=442000, grad_norm=4.722859859466553, loss=0.6433314681053162
I0302 00:29:24.392625 140522396051200 logging_writer.py:48] [442100] global_step=442100, grad_norm=4.114715576171875, loss=0.6257282495498657
I0302 00:29:58.180556 140522404443904 logging_writer.py:48] [442200] global_step=442200, grad_norm=4.371933937072754, loss=0.6522908806800842
I0302 00:30:31.983933 140522396051200 logging_writer.py:48] [442300] global_step=442300, grad_norm=4.276325702667236, loss=0.5865195393562317
I0302 00:31:05.777189 140522404443904 logging_writer.py:48] [442400] global_step=442400, grad_norm=4.437244892120361, loss=0.5970384478569031
I0302 00:31:26.857679 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:31:32.820821 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:31:41.350903 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:31:43.649111 140688601454400 submission_runner.py:411] Time since start: 154698.72s, 	Step: 442464, 	{'train/accuracy': 0.9580875039100647, 'train/loss': 0.15265513956546783, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0435420274734497, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8232500553131104, 'test/num_examples': 10000, 'score': 149491.16506505013, 'total_duration': 154698.7199485302, 'accumulated_submission_time': 149491.16506505013, 'accumulated_eval_time': 5169.926432609558, 'accumulated_logging_time': 22.424886465072632}
I0302 00:31:43.745603 140525273339648 logging_writer.py:48] [442464] accumulated_eval_time=5169.926433, accumulated_logging_time=22.424886, accumulated_submission_time=149491.165065, global_step=442464, preemption_count=0, score=149491.165065, test/accuracy=0.631100, test/loss=1.823250, test/num_examples=10000, total_duration=154698.719949, train/accuracy=0.958088, train/loss=0.152655, validation/accuracy=0.755440, validation/loss=1.043542, validation/num_examples=50000
I0302 00:31:56.239150 140525281732352 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.6912007331848145, loss=0.6872738003730774
I0302 00:32:29.920308 140525273339648 logging_writer.py:48] [442600] global_step=442600, grad_norm=4.534170627593994, loss=0.6312974691390991
I0302 00:33:03.683819 140525281732352 logging_writer.py:48] [442700] global_step=442700, grad_norm=4.4146223068237305, loss=0.6199265122413635
I0302 00:33:37.499954 140525273339648 logging_writer.py:48] [442800] global_step=442800, grad_norm=4.743030071258545, loss=0.5566675066947937
I0302 00:34:11.274150 140525281732352 logging_writer.py:48] [442900] global_step=442900, grad_norm=4.875488758087158, loss=0.6083739995956421
I0302 00:34:45.087609 140525273339648 logging_writer.py:48] [443000] global_step=443000, grad_norm=4.720037937164307, loss=0.7088319063186646
I0302 00:35:18.939337 140525281732352 logging_writer.py:48] [443100] global_step=443100, grad_norm=4.779074668884277, loss=0.7168603539466858
I0302 00:35:52.713382 140525273339648 logging_writer.py:48] [443200] global_step=443200, grad_norm=4.176425457000732, loss=0.5989623665809631
I0302 00:36:26.483314 140525281732352 logging_writer.py:48] [443300] global_step=443300, grad_norm=4.744661808013916, loss=0.6927772164344788
I0302 00:37:00.290207 140525273339648 logging_writer.py:48] [443400] global_step=443400, grad_norm=4.334228038787842, loss=0.5864405035972595
I0302 00:37:34.069904 140525281732352 logging_writer.py:48] [443500] global_step=443500, grad_norm=3.997075080871582, loss=0.5992542505264282
I0302 00:38:07.828915 140525273339648 logging_writer.py:48] [443600] global_step=443600, grad_norm=5.60605001449585, loss=0.6315466165542603
I0302 00:38:41.589654 140525281732352 logging_writer.py:48] [443700] global_step=443700, grad_norm=4.582334995269775, loss=0.6357765793800354
I0302 00:39:15.382733 140525273339648 logging_writer.py:48] [443800] global_step=443800, grad_norm=4.500126838684082, loss=0.6127772331237793
I0302 00:39:49.161098 140525281732352 logging_writer.py:48] [443900] global_step=443900, grad_norm=4.67436408996582, loss=0.5733809471130371
I0302 00:40:13.650277 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:40:19.624238 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:40:28.126522 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:40:30.386048 140688601454400 submission_runner.py:411] Time since start: 155225.46s, 	Step: 443974, 	{'train/accuracy': 0.9595822691917419, 'train/loss': 0.1498272866010666, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.043811559677124, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8222272396087646, 'test/num_examples': 10000, 'score': 150001.00571346283, 'total_duration': 155225.45688009262, 'accumulated_submission_time': 150001.00571346283, 'accumulated_eval_time': 5186.662141799927, 'accumulated_logging_time': 22.533340454101562}
I0302 00:40:30.474592 140522554390272 logging_writer.py:48] [443974] accumulated_eval_time=5186.662142, accumulated_logging_time=22.533340, accumulated_submission_time=150001.005713, global_step=443974, preemption_count=0, score=150001.005713, test/accuracy=0.630800, test/loss=1.822227, test/num_examples=10000, total_duration=155225.456880, train/accuracy=0.959582, train/loss=0.149827, validation/accuracy=0.755600, validation/loss=1.043812, validation/num_examples=50000
I0302 00:40:39.592308 140523092305664 logging_writer.py:48] [444000] global_step=444000, grad_norm=4.596650123596191, loss=0.6158542037010193
I0302 00:41:13.398085 140522554390272 logging_writer.py:48] [444100] global_step=444100, grad_norm=4.3087334632873535, loss=0.6100323796272278
I0302 00:41:47.154951 140523092305664 logging_writer.py:48] [444200] global_step=444200, grad_norm=4.785836696624756, loss=0.6762744188308716
I0302 00:42:20.872866 140522554390272 logging_writer.py:48] [444300] global_step=444300, grad_norm=4.385028839111328, loss=0.6141154766082764
I0302 00:42:54.651790 140523092305664 logging_writer.py:48] [444400] global_step=444400, grad_norm=4.441558361053467, loss=0.6622875332832336
I0302 00:43:28.419372 140522554390272 logging_writer.py:48] [444500] global_step=444500, grad_norm=4.9796881675720215, loss=0.6541132926940918
I0302 00:44:02.198190 140523092305664 logging_writer.py:48] [444600] global_step=444600, grad_norm=4.631937026977539, loss=0.6280835270881653
I0302 00:44:35.996290 140522554390272 logging_writer.py:48] [444700] global_step=444700, grad_norm=4.679872512817383, loss=0.6696298122406006
I0302 00:45:09.774832 140523092305664 logging_writer.py:48] [444800] global_step=444800, grad_norm=4.287811756134033, loss=0.5835489630699158
I0302 00:45:43.544427 140522554390272 logging_writer.py:48] [444900] global_step=444900, grad_norm=4.82169771194458, loss=0.6470627188682556
I0302 00:46:17.325418 140523092305664 logging_writer.py:48] [445000] global_step=445000, grad_norm=4.6483235359191895, loss=0.6730046272277832
I0302 00:46:51.118242 140522554390272 logging_writer.py:48] [445100] global_step=445100, grad_norm=4.537167549133301, loss=0.6198782920837402
I0302 00:47:24.964447 140523092305664 logging_writer.py:48] [445200] global_step=445200, grad_norm=4.516083717346191, loss=0.6077157258987427
I0302 00:47:58.763894 140522554390272 logging_writer.py:48] [445300] global_step=445300, grad_norm=4.39350700378418, loss=0.6280003786087036
I0302 00:48:32.545560 140523092305664 logging_writer.py:48] [445400] global_step=445400, grad_norm=4.553101062774658, loss=0.6112726926803589
I0302 00:49:00.720982 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:49:06.821342 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:49:15.451349 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:49:17.729212 140688601454400 submission_runner.py:411] Time since start: 155752.80s, 	Step: 445485, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.14875340461730957, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0437120199203491, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8205363750457764, 'test/num_examples': 10000, 'score': 150511.18306398392, 'total_duration': 155752.8000433445, 'accumulated_submission_time': 150511.18306398392, 'accumulated_eval_time': 5203.670311450958, 'accumulated_logging_time': 22.639395475387573}
I0302 00:49:17.818591 140522404443904 logging_writer.py:48] [445485] accumulated_eval_time=5203.670311, accumulated_logging_time=22.639395, accumulated_submission_time=150511.183064, global_step=445485, preemption_count=0, score=150511.183064, test/accuracy=0.630800, test/loss=1.820536, test/num_examples=10000, total_duration=155752.800043, train/accuracy=0.959702, train/loss=0.148753, validation/accuracy=0.755120, validation/loss=1.043712, validation/num_examples=50000
I0302 00:49:23.219385 140522554390272 logging_writer.py:48] [445500] global_step=445500, grad_norm=4.283055305480957, loss=0.6630708575248718
I0302 00:49:56.934757 140522404443904 logging_writer.py:48] [445600] global_step=445600, grad_norm=4.786007881164551, loss=0.5983256697654724
I0302 00:50:30.634034 140522554390272 logging_writer.py:48] [445700] global_step=445700, grad_norm=4.379998683929443, loss=0.673757016658783
I0302 00:51:04.406623 140522404443904 logging_writer.py:48] [445800] global_step=445800, grad_norm=4.682733535766602, loss=0.6718159914016724
I0302 00:51:38.192727 140522554390272 logging_writer.py:48] [445900] global_step=445900, grad_norm=4.499504089355469, loss=0.6516612768173218
I0302 00:52:11.969234 140522404443904 logging_writer.py:48] [446000] global_step=446000, grad_norm=4.343483924865723, loss=0.576585054397583
I0302 00:52:45.712453 140522554390272 logging_writer.py:48] [446100] global_step=446100, grad_norm=4.81328821182251, loss=0.6715042591094971
I0302 00:53:19.470466 140522404443904 logging_writer.py:48] [446200] global_step=446200, grad_norm=5.159458637237549, loss=0.723982572555542
I0302 00:53:53.356606 140522554390272 logging_writer.py:48] [446300] global_step=446300, grad_norm=4.968429088592529, loss=0.6567206382751465
I0302 00:54:27.162163 140522404443904 logging_writer.py:48] [446400] global_step=446400, grad_norm=4.375460624694824, loss=0.6710883378982544
I0302 00:55:00.930250 140522554390272 logging_writer.py:48] [446500] global_step=446500, grad_norm=4.466743469238281, loss=0.5890222191810608
I0302 00:55:34.726035 140522404443904 logging_writer.py:48] [446600] global_step=446600, grad_norm=4.331785202026367, loss=0.5999384522438049
I0302 00:56:08.513556 140522554390272 logging_writer.py:48] [446700] global_step=446700, grad_norm=4.276052951812744, loss=0.5667862296104431
I0302 00:56:42.303859 140522404443904 logging_writer.py:48] [446800] global_step=446800, grad_norm=4.041396141052246, loss=0.5406855940818787
I0302 00:57:16.116308 140522554390272 logging_writer.py:48] [446900] global_step=446900, grad_norm=4.547459602355957, loss=0.6475614905357361
I0302 00:57:48.023499 140688601454400 spec.py:321] Evaluating on the training split.
I0302 00:57:54.008903 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 00:58:02.828332 140688601454400 spec.py:349] Evaluating on the test split.
I0302 00:58:05.119951 140688601454400 submission_runner.py:411] Time since start: 156280.19s, 	Step: 446996, 	{'train/accuracy': 0.9610769748687744, 'train/loss': 0.14668411016464233, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0435166358947754, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8197071552276611, 'test/num_examples': 10000, 'score': 151021.32336831093, 'total_duration': 156280.19078469276, 'accumulated_submission_time': 151021.32336831093, 'accumulated_eval_time': 5220.766705274582, 'accumulated_logging_time': 22.740891456604004}
I0302 00:58:05.210725 140525256554240 logging_writer.py:48] [446996] accumulated_eval_time=5220.766705, accumulated_logging_time=22.740891, accumulated_submission_time=151021.323368, global_step=446996, preemption_count=0, score=151021.323368, test/accuracy=0.631400, test/loss=1.819707, test/num_examples=10000, total_duration=156280.190785, train/accuracy=0.961077, train/loss=0.146684, validation/accuracy=0.754940, validation/loss=1.043517, validation/num_examples=50000
I0302 00:58:06.906034 140525264946944 logging_writer.py:48] [447000] global_step=447000, grad_norm=5.209372043609619, loss=0.6849789619445801
I0302 00:58:40.663029 140525256554240 logging_writer.py:48] [447100] global_step=447100, grad_norm=4.564515590667725, loss=0.6618442535400391
I0302 00:59:14.471232 140525264946944 logging_writer.py:48] [447200] global_step=447200, grad_norm=4.653034687042236, loss=0.6557983160018921
I0302 00:59:48.288749 140525256554240 logging_writer.py:48] [447300] global_step=447300, grad_norm=5.064647674560547, loss=0.6177815794944763
I0302 01:00:22.089731 140525264946944 logging_writer.py:48] [447400] global_step=447400, grad_norm=4.872376918792725, loss=0.6917192935943604
I0302 01:00:55.871955 140525256554240 logging_writer.py:48] [447500] global_step=447500, grad_norm=4.853247165679932, loss=0.6109743118286133
I0302 01:01:29.687478 140525264946944 logging_writer.py:48] [447600] global_step=447600, grad_norm=4.5357255935668945, loss=0.5596673488616943
I0302 01:02:03.450324 140525256554240 logging_writer.py:48] [447700] global_step=447700, grad_norm=5.025669574737549, loss=0.6409110426902771
I0302 01:02:37.222636 140525264946944 logging_writer.py:48] [447800] global_step=447800, grad_norm=4.266251564025879, loss=0.5784176588058472
I0302 01:03:10.991778 140525256554240 logging_writer.py:48] [447900] global_step=447900, grad_norm=4.563430309295654, loss=0.5612387657165527
I0302 01:03:44.772274 140525264946944 logging_writer.py:48] [448000] global_step=448000, grad_norm=4.308236598968506, loss=0.6470577716827393
I0302 01:04:18.540982 140525256554240 logging_writer.py:48] [448100] global_step=448100, grad_norm=4.117837905883789, loss=0.547773003578186
I0302 01:04:52.343317 140525264946944 logging_writer.py:48] [448200] global_step=448200, grad_norm=4.818967342376709, loss=0.7522767782211304
I0302 01:05:26.146289 140525256554240 logging_writer.py:48] [448300] global_step=448300, grad_norm=4.906437873840332, loss=0.6146184206008911
I0302 01:05:59.997926 140525264946944 logging_writer.py:48] [448400] global_step=448400, grad_norm=4.862549304962158, loss=0.6192756295204163
I0302 01:06:33.771571 140525256554240 logging_writer.py:48] [448500] global_step=448500, grad_norm=4.561187744140625, loss=0.6210330724716187
I0302 01:06:35.271960 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:06:41.255427 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:06:49.952783 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:06:52.245686 140688601454400 submission_runner.py:411] Time since start: 156807.32s, 	Step: 448506, 	{'train/accuracy': 0.9595623016357422, 'train/loss': 0.15052373707294464, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.044321060180664, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8209294080734253, 'test/num_examples': 10000, 'score': 151531.32101392746, 'total_duration': 156807.316532135, 'accumulated_submission_time': 151531.32101392746, 'accumulated_eval_time': 5237.740381240845, 'accumulated_logging_time': 22.843069553375244}
I0302 01:06:52.337041 140522404443904 logging_writer.py:48] [448506] accumulated_eval_time=5237.740381, accumulated_logging_time=22.843070, accumulated_submission_time=151531.321014, global_step=448506, preemption_count=0, score=151531.321014, test/accuracy=0.630600, test/loss=1.820929, test/num_examples=10000, total_duration=156807.316532, train/accuracy=0.959562, train/loss=0.150524, validation/accuracy=0.755440, validation/loss=1.044321, validation/num_examples=50000
I0302 01:07:24.380810 140522554390272 logging_writer.py:48] [448600] global_step=448600, grad_norm=4.470515251159668, loss=0.6153752207756042
I0302 01:07:58.153193 140522404443904 logging_writer.py:48] [448700] global_step=448700, grad_norm=4.5489912033081055, loss=0.6614552736282349
I0302 01:08:31.934072 140522554390272 logging_writer.py:48] [448800] global_step=448800, grad_norm=4.842031478881836, loss=0.6747210025787354
I0302 01:09:05.697872 140522404443904 logging_writer.py:48] [448900] global_step=448900, grad_norm=4.5782060623168945, loss=0.7087894082069397
I0302 01:09:39.488679 140522554390272 logging_writer.py:48] [449000] global_step=449000, grad_norm=4.537893772125244, loss=0.611407995223999
I0302 01:10:13.281345 140522404443904 logging_writer.py:48] [449100] global_step=449100, grad_norm=4.515880107879639, loss=0.6745462417602539
I0302 01:10:47.049630 140522554390272 logging_writer.py:48] [449200] global_step=449200, grad_norm=4.502469539642334, loss=0.6229363083839417
I0302 01:11:20.844784 140522404443904 logging_writer.py:48] [449300] global_step=449300, grad_norm=4.608153820037842, loss=0.6205801963806152
I0302 01:11:54.670396 140522554390272 logging_writer.py:48] [449400] global_step=449400, grad_norm=4.356546878814697, loss=0.6336898803710938
I0302 01:12:28.544122 140522404443904 logging_writer.py:48] [449500] global_step=449500, grad_norm=4.99479341506958, loss=0.6383389830589294
I0302 01:13:02.329200 140522554390272 logging_writer.py:48] [449600] global_step=449600, grad_norm=4.813055992126465, loss=0.6625431776046753
I0302 01:13:36.121946 140522404443904 logging_writer.py:48] [449700] global_step=449700, grad_norm=4.646880149841309, loss=0.6958209872245789
I0302 01:14:09.897572 140522554390272 logging_writer.py:48] [449800] global_step=449800, grad_norm=4.221043109893799, loss=0.6067451238632202
I0302 01:14:43.685048 140522404443904 logging_writer.py:48] [449900] global_step=449900, grad_norm=5.053861618041992, loss=0.6821584701538086
I0302 01:15:17.475358 140522554390272 logging_writer.py:48] [450000] global_step=450000, grad_norm=4.49254035949707, loss=0.618844211101532
I0302 01:15:22.342355 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:15:28.323507 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:15:36.855956 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:15:39.122831 140688601454400 submission_runner.py:411] Time since start: 157334.19s, 	Step: 450016, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.14856944978237152, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0434327125549316, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8213356733322144, 'test/num_examples': 10000, 'score': 152041.26451206207, 'total_duration': 157334.1936788559, 'accumulated_submission_time': 152041.26451206207, 'accumulated_eval_time': 5254.520807504654, 'accumulated_logging_time': 22.94432306289673}
I0302 01:15:39.215984 140525256554240 logging_writer.py:48] [450016] accumulated_eval_time=5254.520808, accumulated_logging_time=22.944323, accumulated_submission_time=152041.264512, global_step=450016, preemption_count=0, score=152041.264512, test/accuracy=0.631800, test/loss=1.821336, test/num_examples=10000, total_duration=157334.193679, train/accuracy=0.959742, train/loss=0.148569, validation/accuracy=0.755120, validation/loss=1.043433, validation/num_examples=50000
I0302 01:16:07.947975 140525264946944 logging_writer.py:48] [450100] global_step=450100, grad_norm=4.8559746742248535, loss=0.6820826530456543
I0302 01:16:41.679920 140525256554240 logging_writer.py:48] [450200] global_step=450200, grad_norm=4.433149337768555, loss=0.5989606380462646
I0302 01:17:15.414685 140525264946944 logging_writer.py:48] [450300] global_step=450300, grad_norm=4.590728282928467, loss=0.6809596419334412
I0302 01:17:49.164866 140525256554240 logging_writer.py:48] [450400] global_step=450400, grad_norm=4.34029483795166, loss=0.5846624970436096
I0302 01:18:23.078825 140525264946944 logging_writer.py:48] [450500] global_step=450500, grad_norm=4.76296329498291, loss=0.6913421154022217
I0302 01:18:56.892132 140525256554240 logging_writer.py:48] [450600] global_step=450600, grad_norm=4.806150913238525, loss=0.6916914582252502
I0302 01:19:30.692894 140525264946944 logging_writer.py:48] [450700] global_step=450700, grad_norm=4.733248710632324, loss=0.6121284365653992
I0302 01:20:04.501918 140525256554240 logging_writer.py:48] [450800] global_step=450800, grad_norm=4.30329704284668, loss=0.6491239666938782
I0302 01:20:38.262093 140525264946944 logging_writer.py:48] [450900] global_step=450900, grad_norm=4.526074409484863, loss=0.5894461274147034
I0302 01:21:12.054953 140525256554240 logging_writer.py:48] [451000] global_step=451000, grad_norm=4.437739372253418, loss=0.6647639274597168
I0302 01:21:45.830197 140525264946944 logging_writer.py:48] [451100] global_step=451100, grad_norm=4.419297695159912, loss=0.5829365849494934
I0302 01:22:19.631381 140525256554240 logging_writer.py:48] [451200] global_step=451200, grad_norm=4.367953777313232, loss=0.6396817564964294
I0302 01:22:53.420251 140525264946944 logging_writer.py:48] [451300] global_step=451300, grad_norm=4.375421524047852, loss=0.5655459761619568
I0302 01:23:27.193471 140525256554240 logging_writer.py:48] [451400] global_step=451400, grad_norm=4.1679158210754395, loss=0.5749215483665466
I0302 01:24:00.973069 140525264946944 logging_writer.py:48] [451500] global_step=451500, grad_norm=4.338763236999512, loss=0.6678499579429626
I0302 01:24:09.216123 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:24:15.382137 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:24:23.842567 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:24:26.454955 140688601454400 submission_runner.py:411] Time since start: 157861.53s, 	Step: 451526, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14705683290958405, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0432263612747192, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8225295543670654, 'test/num_examples': 10000, 'score': 152551.20199108124, 'total_duration': 157861.52579021454, 'accumulated_submission_time': 152551.20199108124, 'accumulated_eval_time': 5271.759593009949, 'accumulated_logging_time': 23.047905683517456}
I0302 01:24:26.546207 140522554390272 logging_writer.py:48] [451526] accumulated_eval_time=5271.759593, accumulated_logging_time=23.047906, accumulated_submission_time=152551.201991, global_step=451526, preemption_count=0, score=152551.201991, test/accuracy=0.630900, test/loss=1.822530, test/num_examples=10000, total_duration=157861.525790, train/accuracy=0.961097, train/loss=0.147057, validation/accuracy=0.754900, validation/loss=1.043226, validation/num_examples=50000
I0302 01:24:51.866650 140523092305664 logging_writer.py:48] [451600] global_step=451600, grad_norm=4.671699047088623, loss=0.586230993270874
I0302 01:25:25.614784 140522554390272 logging_writer.py:48] [451700] global_step=451700, grad_norm=4.134428024291992, loss=0.618259072303772
I0302 01:25:59.388284 140523092305664 logging_writer.py:48] [451800] global_step=451800, grad_norm=4.829925537109375, loss=0.5959827899932861
I0302 01:26:33.167310 140522554390272 logging_writer.py:48] [451900] global_step=451900, grad_norm=4.506839752197266, loss=0.6260595321655273
I0302 01:27:06.990679 140523092305664 logging_writer.py:48] [452000] global_step=452000, grad_norm=4.38053560256958, loss=0.6263905167579651
I0302 01:27:40.796381 140522554390272 logging_writer.py:48] [452100] global_step=452100, grad_norm=4.227531433105469, loss=0.5618747472763062
I0302 01:28:14.598601 140523092305664 logging_writer.py:48] [452200] global_step=452200, grad_norm=4.082826614379883, loss=0.5355900526046753
I0302 01:28:48.391339 140522554390272 logging_writer.py:48] [452300] global_step=452300, grad_norm=4.839177131652832, loss=0.6162439584732056
I0302 01:29:22.184472 140523092305664 logging_writer.py:48] [452400] global_step=452400, grad_norm=4.308647155761719, loss=0.6455162763595581
I0302 01:29:55.961142 140522554390272 logging_writer.py:48] [452500] global_step=452500, grad_norm=4.5324249267578125, loss=0.6417221426963806
I0302 01:30:29.736825 140523092305664 logging_writer.py:48] [452600] global_step=452600, grad_norm=4.971586227416992, loss=0.6085543036460876
I0302 01:31:03.597889 140522554390272 logging_writer.py:48] [452700] global_step=452700, grad_norm=3.990931510925293, loss=0.5478546619415283
I0302 01:31:37.405205 140523092305664 logging_writer.py:48] [452800] global_step=452800, grad_norm=4.536463737487793, loss=0.6685276627540588
I0302 01:32:11.182950 140522554390272 logging_writer.py:48] [452900] global_step=452900, grad_norm=4.08855676651001, loss=0.5805134773254395
I0302 01:32:44.973767 140523092305664 logging_writer.py:48] [453000] global_step=453000, grad_norm=4.729816436767578, loss=0.7187561392784119
I0302 01:32:56.623852 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:33:02.715132 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:33:11.232679 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:33:13.534523 140688601454400 submission_runner.py:411] Time since start: 158388.61s, 	Step: 453036, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.1490989327430725, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0449022054672241, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8207491636276245, 'test/num_examples': 10000, 'score': 153061.21781492233, 'total_duration': 158388.60536050797, 'accumulated_submission_time': 153061.21781492233, 'accumulated_eval_time': 5288.670228481293, 'accumulated_logging_time': 23.149638652801514}
I0302 01:33:13.627079 140523947947776 logging_writer.py:48] [453036] accumulated_eval_time=5288.670228, accumulated_logging_time=23.149639, accumulated_submission_time=153061.217815, global_step=453036, preemption_count=0, score=153061.217815, test/accuracy=0.630800, test/loss=1.820749, test/num_examples=10000, total_duration=158388.605361, train/accuracy=0.960100, train/loss=0.149099, validation/accuracy=0.754800, validation/loss=1.044902, validation/num_examples=50000
I0302 01:33:35.508202 140525256554240 logging_writer.py:48] [453100] global_step=453100, grad_norm=4.5397186279296875, loss=0.5990029573440552
I0302 01:34:09.253136 140523947947776 logging_writer.py:48] [453200] global_step=453200, grad_norm=4.190257549285889, loss=0.5677230358123779
I0302 01:34:43.047675 140525256554240 logging_writer.py:48] [453300] global_step=453300, grad_norm=4.121292591094971, loss=0.6137784123420715
I0302 01:35:16.780703 140523947947776 logging_writer.py:48] [453400] global_step=453400, grad_norm=4.59869384765625, loss=0.6839253306388855
I0302 01:35:50.552854 140525256554240 logging_writer.py:48] [453500] global_step=453500, grad_norm=4.741906642913818, loss=0.6633448004722595
I0302 01:36:24.355214 140523947947776 logging_writer.py:48] [453600] global_step=453600, grad_norm=4.524547100067139, loss=0.6045821905136108
I0302 01:36:58.240417 140525256554240 logging_writer.py:48] [453700] global_step=453700, grad_norm=4.80325984954834, loss=0.6021329164505005
I0302 01:37:32.026463 140523947947776 logging_writer.py:48] [453800] global_step=453800, grad_norm=4.69432258605957, loss=0.643107533454895
I0302 01:38:05.824361 140525256554240 logging_writer.py:48] [453900] global_step=453900, grad_norm=4.678102016448975, loss=0.644911527633667
I0302 01:38:39.567243 140523947947776 logging_writer.py:48] [454000] global_step=454000, grad_norm=4.988227367401123, loss=0.6799308061599731
I0302 01:39:13.378105 140525256554240 logging_writer.py:48] [454100] global_step=454100, grad_norm=4.887603282928467, loss=0.5836763978004456
I0302 01:39:47.150765 140523947947776 logging_writer.py:48] [454200] global_step=454200, grad_norm=4.790571212768555, loss=0.6915071606636047
I0302 01:40:20.921579 140525256554240 logging_writer.py:48] [454300] global_step=454300, grad_norm=4.146286487579346, loss=0.5819619297981262
I0302 01:40:54.735029 140523947947776 logging_writer.py:48] [454400] global_step=454400, grad_norm=4.9244842529296875, loss=0.6734596490859985
I0302 01:41:28.545477 140525256554240 logging_writer.py:48] [454500] global_step=454500, grad_norm=4.339590072631836, loss=0.6027344465255737
I0302 01:41:43.582626 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:41:49.694739 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:41:58.418529 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:42:00.674477 140688601454400 submission_runner.py:411] Time since start: 158915.75s, 	Step: 454546, 	{'train/accuracy': 0.9602199792861938, 'train/loss': 0.14820550382137299, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0442836284637451, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8212093114852905, 'test/num_examples': 10000, 'score': 153571.1104207039, 'total_duration': 158915.745316267, 'accumulated_submission_time': 153571.1104207039, 'accumulated_eval_time': 5305.7620232105255, 'accumulated_logging_time': 23.25257182121277}
I0302 01:42:00.765867 140522554390272 logging_writer.py:48] [454546] accumulated_eval_time=5305.762023, accumulated_logging_time=23.252572, accumulated_submission_time=153571.110421, global_step=454546, preemption_count=0, score=153571.110421, test/accuracy=0.630500, test/loss=1.821209, test/num_examples=10000, total_duration=158915.745316, train/accuracy=0.960220, train/loss=0.148206, validation/accuracy=0.754800, validation/loss=1.044284, validation/num_examples=50000
I0302 01:42:19.315756 140523092305664 logging_writer.py:48] [454600] global_step=454600, grad_norm=4.6270575523376465, loss=0.6052153706550598
I0302 01:42:53.098223 140522554390272 logging_writer.py:48] [454700] global_step=454700, grad_norm=4.313233852386475, loss=0.6655840277671814
I0302 01:43:26.936923 140523092305664 logging_writer.py:48] [454800] global_step=454800, grad_norm=4.916715621948242, loss=0.6989806294441223
I0302 01:44:00.725975 140522554390272 logging_writer.py:48] [454900] global_step=454900, grad_norm=4.250650405883789, loss=0.5929561853408813
I0302 01:44:34.523995 140523092305664 logging_writer.py:48] [455000] global_step=455000, grad_norm=4.295295238494873, loss=0.5476222038269043
I0302 01:45:08.282904 140522554390272 logging_writer.py:48] [455100] global_step=455100, grad_norm=4.688451766967773, loss=0.7151666879653931
I0302 01:45:42.058780 140523092305664 logging_writer.py:48] [455200] global_step=455200, grad_norm=5.00941276550293, loss=0.6440555453300476
I0302 01:46:15.887003 140522554390272 logging_writer.py:48] [455300] global_step=455300, grad_norm=4.385159492492676, loss=0.6164531707763672
I0302 01:46:49.680939 140523092305664 logging_writer.py:48] [455400] global_step=455400, grad_norm=4.368279457092285, loss=0.5722836256027222
I0302 01:47:23.454290 140522554390272 logging_writer.py:48] [455500] global_step=455500, grad_norm=4.151867389678955, loss=0.5634034872055054
I0302 01:47:57.250181 140523092305664 logging_writer.py:48] [455600] global_step=455600, grad_norm=4.953057289123535, loss=0.7334084510803223
I0302 01:48:31.032877 140522554390272 logging_writer.py:48] [455700] global_step=455700, grad_norm=4.590017795562744, loss=0.5857289433479309
I0302 01:49:04.815948 140523092305664 logging_writer.py:48] [455800] global_step=455800, grad_norm=4.678802967071533, loss=0.6400277614593506
I0302 01:49:38.710456 140522554390272 logging_writer.py:48] [455900] global_step=455900, grad_norm=4.761734962463379, loss=0.5915044546127319
I0302 01:50:12.506401 140523092305664 logging_writer.py:48] [456000] global_step=456000, grad_norm=4.493969917297363, loss=0.6293127536773682
I0302 01:50:30.918980 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:50:36.935744 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:50:45.564523 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:50:47.853092 140688601454400 submission_runner.py:411] Time since start: 159442.92s, 	Step: 456056, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.1483684480190277, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0436128377914429, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8223989009857178, 'test/num_examples': 10000, 'score': 154081.2016234398, 'total_duration': 159442.92390727997, 'accumulated_submission_time': 154081.2016234398, 'accumulated_eval_time': 5322.696059703827, 'accumulated_logging_time': 23.354546785354614}
I0302 01:50:47.961076 140522404443904 logging_writer.py:48] [456056] accumulated_eval_time=5322.696060, accumulated_logging_time=23.354547, accumulated_submission_time=154081.201623, global_step=456056, preemption_count=0, score=154081.201623, test/accuracy=0.631900, test/loss=1.822399, test/num_examples=10000, total_duration=159442.923907, train/accuracy=0.960379, train/loss=0.148368, validation/accuracy=0.755260, validation/loss=1.043613, validation/num_examples=50000
I0302 01:51:03.146600 140525264946944 logging_writer.py:48] [456100] global_step=456100, grad_norm=4.467639446258545, loss=0.6757488250732422
I0302 01:51:36.911851 140522404443904 logging_writer.py:48] [456200] global_step=456200, grad_norm=4.381585121154785, loss=0.5751960277557373
I0302 01:52:10.688438 140525264946944 logging_writer.py:48] [456300] global_step=456300, grad_norm=5.505338191986084, loss=0.725131630897522
I0302 01:52:44.464092 140522404443904 logging_writer.py:48] [456400] global_step=456400, grad_norm=4.462656497955322, loss=0.6019846796989441
I0302 01:53:18.234741 140525264946944 logging_writer.py:48] [456500] global_step=456500, grad_norm=4.4867682456970215, loss=0.6145316362380981
I0302 01:53:51.992111 140522404443904 logging_writer.py:48] [456600] global_step=456600, grad_norm=4.312223434448242, loss=0.5667029023170471
I0302 01:54:25.752969 140525264946944 logging_writer.py:48] [456700] global_step=456700, grad_norm=3.9818801879882812, loss=0.5216533541679382
I0302 01:54:59.558161 140522404443904 logging_writer.py:48] [456800] global_step=456800, grad_norm=4.4341654777526855, loss=0.6083939075469971
I0302 01:55:33.381895 140525264946944 logging_writer.py:48] [456900] global_step=456900, grad_norm=5.014423370361328, loss=0.6306359767913818
I0302 01:56:07.151427 140522404443904 logging_writer.py:48] [457000] global_step=457000, grad_norm=4.477147102355957, loss=0.5948339104652405
I0302 01:56:40.934387 140525264946944 logging_writer.py:48] [457100] global_step=457100, grad_norm=4.47986364364624, loss=0.5739905834197998
I0302 01:57:14.711848 140522404443904 logging_writer.py:48] [457200] global_step=457200, grad_norm=4.259110450744629, loss=0.6014183759689331
I0302 01:57:48.453849 140525264946944 logging_writer.py:48] [457300] global_step=457300, grad_norm=4.449968338012695, loss=0.6162970066070557
I0302 01:58:22.210500 140522404443904 logging_writer.py:48] [457400] global_step=457400, grad_norm=4.4338765144348145, loss=0.6390567421913147
I0302 01:58:56.011397 140525264946944 logging_writer.py:48] [457500] global_step=457500, grad_norm=4.972365379333496, loss=0.6491440534591675
I0302 01:59:18.108617 140688601454400 spec.py:321] Evaluating on the training split.
I0302 01:59:24.153401 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 01:59:32.641550 140688601454400 spec.py:349] Evaluating on the test split.
I0302 01:59:34.923444 140688601454400 submission_runner.py:411] Time since start: 159969.99s, 	Step: 457567, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14615195989608765, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0445982217788696, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8232157230377197, 'test/num_examples': 10000, 'score': 154591.28657960892, 'total_duration': 159969.99428796768, 'accumulated_submission_time': 154591.28657960892, 'accumulated_eval_time': 5339.510857105255, 'accumulated_logging_time': 23.473296642303467}
I0302 01:59:35.015759 140525256554240 logging_writer.py:48] [457567] accumulated_eval_time=5339.510857, accumulated_logging_time=23.473297, accumulated_submission_time=154591.286580, global_step=457567, preemption_count=0, score=154591.286580, test/accuracy=0.631100, test/loss=1.823216, test/num_examples=10000, total_duration=159969.994288, train/accuracy=0.960938, train/loss=0.146152, validation/accuracy=0.755280, validation/loss=1.044598, validation/num_examples=50000
I0302 01:59:46.466638 140525273339648 logging_writer.py:48] [457600] global_step=457600, grad_norm=4.517119884490967, loss=0.5971508622169495
I0302 02:00:20.191278 140525256554240 logging_writer.py:48] [457700] global_step=457700, grad_norm=4.209879398345947, loss=0.5786241292953491
I0302 02:00:53.900394 140525273339648 logging_writer.py:48] [457800] global_step=457800, grad_norm=4.266646385192871, loss=0.6243095397949219
I0302 02:01:27.680908 140525256554240 logging_writer.py:48] [457900] global_step=457900, grad_norm=4.946792125701904, loss=0.6022779941558838
I0302 02:02:01.488400 140525273339648 logging_writer.py:48] [458000] global_step=458000, grad_norm=4.377704620361328, loss=0.596909761428833
I0302 02:02:35.285674 140525256554240 logging_writer.py:48] [458100] global_step=458100, grad_norm=4.539879322052002, loss=0.620228111743927
I0302 02:03:09.051578 140525273339648 logging_writer.py:48] [458200] global_step=458200, grad_norm=4.298092842102051, loss=0.5655092000961304
I0302 02:03:42.823752 140525256554240 logging_writer.py:48] [458300] global_step=458300, grad_norm=4.601517677307129, loss=0.6724828481674194
I0302 02:04:16.645595 140525273339648 logging_writer.py:48] [458400] global_step=458400, grad_norm=4.717703342437744, loss=0.6654750108718872
I0302 02:04:50.403213 140525256554240 logging_writer.py:48] [458500] global_step=458500, grad_norm=4.481177806854248, loss=0.6392074823379517
I0302 02:05:24.187420 140525273339648 logging_writer.py:48] [458600] global_step=458600, grad_norm=4.130007266998291, loss=0.5795214772224426
I0302 02:05:57.937576 140525256554240 logging_writer.py:48] [458700] global_step=458700, grad_norm=4.515510559082031, loss=0.580471396446228
I0302 02:06:31.653602 140525273339648 logging_writer.py:48] [458800] global_step=458800, grad_norm=4.121764183044434, loss=0.5949408411979675
I0302 02:07:05.427467 140525256554240 logging_writer.py:48] [458900] global_step=458900, grad_norm=4.558913230895996, loss=0.6497204303741455
I0302 02:07:39.313745 140525273339648 logging_writer.py:48] [459000] global_step=459000, grad_norm=4.911412239074707, loss=0.6071475148200989
I0302 02:08:05.123829 140688601454400 spec.py:321] Evaluating on the training split.
I0302 02:08:11.080739 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 02:08:19.725558 140688601454400 spec.py:349] Evaluating on the test split.
I0302 02:08:22.005847 140688601454400 submission_runner.py:411] Time since start: 160497.08s, 	Step: 459078, 	{'train/accuracy': 0.9615553021430969, 'train/loss': 0.14675858616828918, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0421971082687378, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8196227550506592, 'test/num_examples': 10000, 'score': 155101.33123731613, 'total_duration': 160497.07613682747, 'accumulated_submission_time': 155101.33123731613, 'accumulated_eval_time': 5356.392268419266, 'accumulated_logging_time': 23.577007055282593}
I0302 02:08:22.097676 140522396051200 logging_writer.py:48] [459078] accumulated_eval_time=5356.392268, accumulated_logging_time=23.577007, accumulated_submission_time=155101.331237, global_step=459078, preemption_count=0, score=155101.331237, test/accuracy=0.631500, test/loss=1.819623, test/num_examples=10000, total_duration=160497.076137, train/accuracy=0.961555, train/loss=0.146759, validation/accuracy=0.755040, validation/loss=1.042197, validation/num_examples=50000
I0302 02:08:29.886045 140522404443904 logging_writer.py:48] [459100] global_step=459100, grad_norm=4.859767436981201, loss=0.7139520645141602
I0302 02:09:03.619157 140522396051200 logging_writer.py:48] [459200] global_step=459200, grad_norm=4.7624125480651855, loss=0.6795392632484436
I0302 02:09:37.384462 140522404443904 logging_writer.py:48] [459300] global_step=459300, grad_norm=4.299633026123047, loss=0.6658227443695068
I0302 02:10:11.159057 140522396051200 logging_writer.py:48] [459400] global_step=459400, grad_norm=4.337836742401123, loss=0.5953927636146545
I0302 02:10:44.932536 140522404443904 logging_writer.py:48] [459500] global_step=459500, grad_norm=4.6165900230407715, loss=0.6494104266166687
I0302 02:11:18.703118 140522396051200 logging_writer.py:48] [459600] global_step=459600, grad_norm=4.5222063064575195, loss=0.6365352869033813
I0302 02:11:52.545779 140522404443904 logging_writer.py:48] [459700] global_step=459700, grad_norm=4.293211936950684, loss=0.6209613084793091
I0302 02:12:26.325920 140522396051200 logging_writer.py:48] [459800] global_step=459800, grad_norm=4.806662082672119, loss=0.6028367877006531
I0302 02:13:00.126593 140522404443904 logging_writer.py:48] [459900] global_step=459900, grad_norm=4.972962379455566, loss=0.6299974918365479
I0302 02:13:33.943011 140522396051200 logging_writer.py:48] [460000] global_step=460000, grad_norm=4.526882171630859, loss=0.6382036209106445
I0302 02:14:07.771361 140522404443904 logging_writer.py:48] [460100] global_step=460100, grad_norm=4.6374125480651855, loss=0.6751098036766052
I0302 02:14:41.566176 140522396051200 logging_writer.py:48] [460200] global_step=460200, grad_norm=4.515847682952881, loss=0.5539240837097168
I0302 02:15:15.351077 140522404443904 logging_writer.py:48] [460300] global_step=460300, grad_norm=4.793483257293701, loss=0.6583447456359863
I0302 02:15:49.125332 140522396051200 logging_writer.py:48] [460400] global_step=460400, grad_norm=4.415546417236328, loss=0.597258985042572
I0302 02:16:22.921983 140522404443904 logging_writer.py:48] [460500] global_step=460500, grad_norm=4.754782199859619, loss=0.5826902389526367
I0302 02:16:52.060460 140688601454400 spec.py:321] Evaluating on the training split.
I0302 02:16:58.002484 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 02:17:07.290343 140688601454400 spec.py:349] Evaluating on the test split.
I0302 02:17:09.583783 140688601454400 submission_runner.py:411] Time since start: 161024.65s, 	Step: 460588, 	{'train/accuracy': 0.9595623016357422, 'train/loss': 0.148551806807518, 'validation/accuracy': 0.755079984664917, 'validation/loss': 1.0454641580581665, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.822048306465149, 'test/num_examples': 10000, 'score': 155611.23100996017, 'total_duration': 161024.65453124046, 'accumulated_submission_time': 155611.23100996017, 'accumulated_eval_time': 5373.915447711945, 'accumulated_logging_time': 23.67885732650757}
I0302 02:17:09.677629 140522554390272 logging_writer.py:48] [460588] accumulated_eval_time=5373.915448, accumulated_logging_time=23.678857, accumulated_submission_time=155611.231010, global_step=460588, preemption_count=0, score=155611.231010, test/accuracy=0.630900, test/loss=1.822048, test/num_examples=10000, total_duration=161024.654531, train/accuracy=0.959562, train/loss=0.148552, validation/accuracy=0.755080, validation/loss=1.045464, validation/num_examples=50000
I0302 02:17:14.066990 140525256554240 logging_writer.py:48] [460600] global_step=460600, grad_norm=5.154562950134277, loss=0.6648117303848267
I0302 02:17:47.790960 140522554390272 logging_writer.py:48] [460700] global_step=460700, grad_norm=3.9596636295318604, loss=0.5082582831382751
I0302 02:18:21.503971 140525256554240 logging_writer.py:48] [460800] global_step=460800, grad_norm=4.523662090301514, loss=0.5981219410896301
I0302 02:18:55.313816 140522554390272 logging_writer.py:48] [460900] global_step=460900, grad_norm=4.873504638671875, loss=0.6415573358535767
I0302 02:19:29.066904 140525256554240 logging_writer.py:48] [461000] global_step=461000, grad_norm=4.616491317749023, loss=0.6634812355041504
I0302 02:20:02.809225 140522554390272 logging_writer.py:48] [461100] global_step=461100, grad_norm=4.21024227142334, loss=0.5813585519790649
I0302 02:20:36.666806 140525256554240 logging_writer.py:48] [461200] global_step=461200, grad_norm=4.498002052307129, loss=0.6538743376731873
I0302 02:21:10.473377 140522554390272 logging_writer.py:48] [461300] global_step=461300, grad_norm=4.237249851226807, loss=0.5787692070007324
I0302 02:21:44.241930 140525256554240 logging_writer.py:48] [461400] global_step=461400, grad_norm=4.473961353302002, loss=0.5814284682273865
I0302 02:22:17.977385 140522554390272 logging_writer.py:48] [461500] global_step=461500, grad_norm=4.102809429168701, loss=0.579692006111145
I0302 02:22:51.784770 140525256554240 logging_writer.py:48] [461600] global_step=461600, grad_norm=4.722589492797852, loss=0.6762118935585022
I0302 02:23:25.545897 140522554390272 logging_writer.py:48] [461700] global_step=461700, grad_norm=4.2496442794799805, loss=0.5626170039176941
I0302 02:23:59.345928 140525256554240 logging_writer.py:48] [461800] global_step=461800, grad_norm=4.836323261260986, loss=0.5523691773414612
I0302 02:24:33.129455 140522554390272 logging_writer.py:48] [461900] global_step=461900, grad_norm=4.509376525878906, loss=0.6559303998947144
I0302 02:25:06.929502 140525256554240 logging_writer.py:48] [462000] global_step=462000, grad_norm=4.6069016456604, loss=0.6625098586082458
I0302 02:25:39.844078 140688601454400 spec.py:321] Evaluating on the training split.
I0302 02:25:45.858588 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 02:25:54.425161 140688601454400 spec.py:349] Evaluating on the test split.
I0302 02:25:56.708780 140688601454400 submission_runner.py:411] Time since start: 161551.78s, 	Step: 462099, 	{'train/accuracy': 0.9593630433082581, 'train/loss': 0.15155340731143951, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0442078113555908, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8219547271728516, 'test/num_examples': 10000, 'score': 156121.33530831337, 'total_duration': 161551.77962732315, 'accumulated_submission_time': 156121.33530831337, 'accumulated_eval_time': 5390.780106306076, 'accumulated_logging_time': 23.7828586101532}
I0302 02:25:56.803405 140523092305664 logging_writer.py:48] [462099] accumulated_eval_time=5390.780106, accumulated_logging_time=23.782859, accumulated_submission_time=156121.335308, global_step=462099, preemption_count=0, score=156121.335308, test/accuracy=0.631200, test/loss=1.821955, test/num_examples=10000, total_duration=161551.779627, train/accuracy=0.959363, train/loss=0.151553, validation/accuracy=0.755400, validation/loss=1.044208, validation/num_examples=50000
I0302 02:25:57.499230 140523947947776 logging_writer.py:48] [462100] global_step=462100, grad_norm=4.301734447479248, loss=0.5498798489570618
I0302 02:26:31.262781 140523092305664 logging_writer.py:48] [462200] global_step=462200, grad_norm=4.766194820404053, loss=0.6352901458740234
I0302 02:27:05.009007 140523947947776 logging_writer.py:48] [462300] global_step=462300, grad_norm=4.676578998565674, loss=0.6677234768867493
I0302 02:27:38.800249 140523092305664 logging_writer.py:48] [462400] global_step=462400, grad_norm=4.537384986877441, loss=0.6216226816177368
I0302 02:28:12.582061 140523947947776 logging_writer.py:48] [462500] global_step=462500, grad_norm=4.282032012939453, loss=0.5512027740478516
I0302 02:28:46.388561 140523092305664 logging_writer.py:48] [462600] global_step=462600, grad_norm=4.89942741394043, loss=0.616988480091095
I0302 02:29:20.171081 140523947947776 logging_writer.py:48] [462700] global_step=462700, grad_norm=4.515820503234863, loss=0.6346559524536133
I0302 02:29:53.969272 140523092305664 logging_writer.py:48] [462800] global_step=462800, grad_norm=4.084898948669434, loss=0.6503573060035706
I0302 02:30:27.770114 140523947947776 logging_writer.py:48] [462900] global_step=462900, grad_norm=4.427699089050293, loss=0.5874764919281006
I0302 02:31:01.566262 140523092305664 logging_writer.py:48] [463000] global_step=463000, grad_norm=4.005651950836182, loss=0.5514160990715027
I0302 02:31:35.378987 140523947947776 logging_writer.py:48] [463100] global_step=463100, grad_norm=4.24488639831543, loss=0.5861532688140869
I0302 02:32:09.169269 140523092305664 logging_writer.py:48] [463200] global_step=463200, grad_norm=4.458263397216797, loss=0.6578495502471924
I0302 02:32:43.071577 140523947947776 logging_writer.py:48] [463300] global_step=463300, grad_norm=4.367999076843262, loss=0.6097902655601501
I0302 02:33:16.870021 140523092305664 logging_writer.py:48] [463400] global_step=463400, grad_norm=4.704827308654785, loss=0.6653033494949341
I0302 02:33:50.643124 140523947947776 logging_writer.py:48] [463500] global_step=463500, grad_norm=4.890958309173584, loss=0.6695914268493652
I0302 02:34:24.441706 140523092305664 logging_writer.py:48] [463600] global_step=463600, grad_norm=4.698111534118652, loss=0.6572728753089905
I0302 02:34:26.947604 140688601454400 spec.py:321] Evaluating on the training split.
I0302 02:34:32.996267 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 02:34:41.540808 140688601454400 spec.py:349] Evaluating on the test split.
I0302 02:34:43.818616 140688601454400 submission_runner.py:411] Time since start: 162078.89s, 	Step: 463609, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.14840573072433472, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.043148398399353, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8227782249450684, 'test/num_examples': 10000, 'score': 156631.41730761528, 'total_duration': 162078.88946652412, 'accumulated_submission_time': 156631.41730761528, 'accumulated_eval_time': 5407.651072978973, 'accumulated_logging_time': 23.887704133987427}
I0302 02:34:43.908584 140522396051200 logging_writer.py:48] [463609] accumulated_eval_time=5407.651073, accumulated_logging_time=23.887704, accumulated_submission_time=156631.417308, global_step=463609, preemption_count=0, score=156631.417308, test/accuracy=0.631500, test/loss=1.822778, test/num_examples=10000, total_duration=162078.889467, train/accuracy=0.959841, train/loss=0.148406, validation/accuracy=0.755040, validation/loss=1.043148, validation/num_examples=50000
I0302 02:35:14.936929 140522404443904 logging_writer.py:48] [463700] global_step=463700, grad_norm=4.641538143157959, loss=0.6696320176124573
I0302 02:35:48.669744 140522396051200 logging_writer.py:48] [463800] global_step=463800, grad_norm=4.342559337615967, loss=0.6480819582939148
I0302 02:36:22.431590 140522404443904 logging_writer.py:48] [463900] global_step=463900, grad_norm=4.156662464141846, loss=0.6348136067390442
I0302 02:36:56.206999 140522396051200 logging_writer.py:48] [464000] global_step=464000, grad_norm=4.564535617828369, loss=0.6270785331726074
I0302 02:37:29.964052 140522404443904 logging_writer.py:48] [464100] global_step=464100, grad_norm=4.545432090759277, loss=0.5767873525619507
I0302 02:38:03.754903 140522396051200 logging_writer.py:48] [464200] global_step=464200, grad_norm=5.020229816436768, loss=0.60667884349823
I0302 02:38:37.598806 140522404443904 logging_writer.py:48] [464300] global_step=464300, grad_norm=4.438743591308594, loss=0.6326732635498047
I0302 02:39:11.367146 140522396051200 logging_writer.py:48] [464400] global_step=464400, grad_norm=4.855595588684082, loss=0.6138404607772827
I0302 02:39:45.158318 140522404443904 logging_writer.py:48] [464500] global_step=464500, grad_norm=4.681421279907227, loss=0.6354305148124695
I0302 02:40:18.948466 140522396051200 logging_writer.py:48] [464600] global_step=464600, grad_norm=4.850991725921631, loss=0.607437252998352
I0302 02:40:52.747585 140522404443904 logging_writer.py:48] [464700] global_step=464700, grad_norm=4.278262615203857, loss=0.6467118263244629
I0302 02:41:26.531460 140522396051200 logging_writer.py:48] [464800] global_step=464800, grad_norm=4.392557144165039, loss=0.6363757848739624
I0302 02:42:00.332375 140522404443904 logging_writer.py:48] [464900] global_step=464900, grad_norm=4.5054473876953125, loss=0.5768190026283264
I0302 02:42:34.139287 140522396051200 logging_writer.py:48] [465000] global_step=465000, grad_norm=4.997661113739014, loss=0.6474579572677612
I0302 02:43:07.942650 140522404443904 logging_writer.py:48] [465100] global_step=465100, grad_norm=4.697903633117676, loss=0.6117739677429199
I0302 02:43:13.844258 140688601454400 spec.py:321] Evaluating on the training split.
I0302 02:43:19.830364 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 02:43:28.474394 140688601454400 spec.py:349] Evaluating on the test split.
I0302 02:43:30.751208 140688601454400 submission_runner.py:411] Time since start: 162605.82s, 	Step: 465119, 	{'train/accuracy': 0.9618741869926453, 'train/loss': 0.14662395417690277, 'validation/accuracy': 0.755079984664917, 'validation/loss': 1.044501543045044, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8231332302093506, 'test/num_examples': 10000, 'score': 157141.29112553596, 'total_duration': 162605.82205343246, 'accumulated_submission_time': 157141.29112553596, 'accumulated_eval_time': 5424.55797290802, 'accumulated_logging_time': 23.987500429153442}
I0302 02:43:30.845419 140525256554240 logging_writer.py:48] [465119] accumulated_eval_time=5424.557973, accumulated_logging_time=23.987500, accumulated_submission_time=157141.291126, global_step=465119, preemption_count=0, score=157141.291126, test/accuracy=0.630500, test/loss=1.823133, test/num_examples=10000, total_duration=162605.822053, train/accuracy=0.961874, train/loss=0.146624, validation/accuracy=0.755080, validation/loss=1.044502, validation/num_examples=50000
I0302 02:43:58.454954 140525264946944 logging_writer.py:48] [465200] global_step=465200, grad_norm=5.704007625579834, loss=0.693767786026001
I0302 02:44:32.253077 140525256554240 logging_writer.py:48] [465300] global_step=465300, grad_norm=4.729267120361328, loss=0.6270002126693726
I0302 02:45:06.081755 140525264946944 logging_writer.py:48] [465400] global_step=465400, grad_norm=4.2890543937683105, loss=0.6206429600715637
I0302 02:45:39.875953 140525256554240 logging_writer.py:48] [465500] global_step=465500, grad_norm=4.666466236114502, loss=0.6570261120796204
I0302 02:46:13.667495 140525264946944 logging_writer.py:48] [465600] global_step=465600, grad_norm=4.471304416656494, loss=0.649895429611206
I0302 02:46:47.471037 140525256554240 logging_writer.py:48] [465700] global_step=465700, grad_norm=4.792424201965332, loss=0.6917846202850342
I0302 02:47:21.243739 140525264946944 logging_writer.py:48] [465800] global_step=465800, grad_norm=4.863421440124512, loss=0.626891553401947
I0302 02:47:55.079631 140525256554240 logging_writer.py:48] [465900] global_step=465900, grad_norm=4.1743245124816895, loss=0.6473284363746643
I0302 02:48:28.860307 140525264946944 logging_writer.py:48] [466000] global_step=466000, grad_norm=4.2496185302734375, loss=0.6765598058700562
I0302 02:49:02.688871 140525256554240 logging_writer.py:48] [466100] global_step=466100, grad_norm=4.2776408195495605, loss=0.5943781137466431
I0302 02:49:36.453469 140525264946944 logging_writer.py:48] [466200] global_step=466200, grad_norm=4.477441310882568, loss=0.6655012965202332
I0302 02:50:10.222871 140525256554240 logging_writer.py:48] [466300] global_step=466300, grad_norm=4.433810234069824, loss=0.5683459639549255
I0302 02:50:44.011069 140525264946944 logging_writer.py:48] [466400] global_step=466400, grad_norm=4.440812110900879, loss=0.6245195865631104
I0302 02:51:17.907284 140525256554240 logging_writer.py:48] [466500] global_step=466500, grad_norm=4.424454212188721, loss=0.6285812854766846
I0302 02:51:51.670923 140525264946944 logging_writer.py:48] [466600] global_step=466600, grad_norm=4.5358052253723145, loss=0.6868230700492859
I0302 02:52:00.938407 140688601454400 spec.py:321] Evaluating on the training split.
I0302 02:52:06.999231 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 02:52:15.523321 140688601454400 spec.py:349] Evaluating on the test split.
I0302 02:52:17.845724 140688601454400 submission_runner.py:411] Time since start: 163132.92s, 	Step: 466629, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14668235182762146, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.043945550918579, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.822500228881836, 'test/num_examples': 10000, 'score': 157651.32108592987, 'total_duration': 163132.91656827927, 'accumulated_submission_time': 157651.32108592987, 'accumulated_eval_time': 5441.46524143219, 'accumulated_logging_time': 24.0924711227417}
I0302 02:52:17.936936 140522554390272 logging_writer.py:48] [466629] accumulated_eval_time=5441.465241, accumulated_logging_time=24.092471, accumulated_submission_time=157651.321086, global_step=466629, preemption_count=0, score=157651.321086, test/accuracy=0.630400, test/loss=1.822500, test/num_examples=10000, total_duration=163132.916568, train/accuracy=0.960100, train/loss=0.146682, validation/accuracy=0.755420, validation/loss=1.043946, validation/num_examples=50000
I0302 02:52:42.222349 140523092305664 logging_writer.py:48] [466700] global_step=466700, grad_norm=4.250821113586426, loss=0.659562885761261
I0302 02:53:15.950114 140522554390272 logging_writer.py:48] [466800] global_step=466800, grad_norm=5.069523811340332, loss=0.7172251343727112
I0302 02:53:49.701021 140523092305664 logging_writer.py:48] [466900] global_step=466900, grad_norm=4.406640529632568, loss=0.5819845199584961
I0302 02:54:23.458201 140522554390272 logging_writer.py:48] [467000] global_step=467000, grad_norm=4.162346363067627, loss=0.6060378551483154
I0302 02:54:57.220016 140523092305664 logging_writer.py:48] [467100] global_step=467100, grad_norm=4.70104455947876, loss=0.665868878364563
I0302 02:55:30.998186 140522554390272 logging_writer.py:48] [467200] global_step=467200, grad_norm=4.825894832611084, loss=0.6374433040618896
I0302 02:56:04.761663 140523092305664 logging_writer.py:48] [467300] global_step=467300, grad_norm=4.762514591217041, loss=0.664907693862915
I0302 02:56:38.550035 140522554390272 logging_writer.py:48] [467400] global_step=467400, grad_norm=4.881542682647705, loss=0.6985450983047485
I0302 02:57:12.421474 140523092305664 logging_writer.py:48] [467500] global_step=467500, grad_norm=4.340188026428223, loss=0.5786232948303223
I0302 02:57:46.199476 140522554390272 logging_writer.py:48] [467600] global_step=467600, grad_norm=4.5629072189331055, loss=0.6907954216003418
I0302 02:58:19.988316 140523092305664 logging_writer.py:48] [467700] global_step=467700, grad_norm=4.2534966468811035, loss=0.5270156860351562
I0302 02:58:53.742285 140522554390272 logging_writer.py:48] [467800] global_step=467800, grad_norm=4.6569504737854, loss=0.7177255153656006
I0302 02:59:27.498476 140523092305664 logging_writer.py:48] [467900] global_step=467900, grad_norm=4.461198806762695, loss=0.6385295391082764
I0302 03:00:01.283838 140522554390272 logging_writer.py:48] [468000] global_step=468000, grad_norm=4.376302719116211, loss=0.5637668967247009
I0302 03:00:35.059475 140523092305664 logging_writer.py:48] [468100] global_step=468100, grad_norm=4.3623785972595215, loss=0.6189380884170532
I0302 03:00:48.055791 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:00:54.042480 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:01:02.759397 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:01:05.049637 140688601454400 submission_runner.py:411] Time since start: 163660.12s, 	Step: 468140, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14661850035190582, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0439494848251343, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8211979866027832, 'test/num_examples': 10000, 'score': 158161.37888002396, 'total_duration': 163660.12048506737, 'accumulated_submission_time': 158161.37888002396, 'accumulated_eval_time': 5458.459059715271, 'accumulated_logging_time': 24.193307638168335}
I0302 03:01:05.142237 140522404443904 logging_writer.py:48] [468140] accumulated_eval_time=5458.459060, accumulated_logging_time=24.193308, accumulated_submission_time=158161.378880, global_step=468140, preemption_count=0, score=158161.378880, test/accuracy=0.630400, test/loss=1.821198, test/num_examples=10000, total_duration=163660.120485, train/accuracy=0.960658, train/loss=0.146619, validation/accuracy=0.754940, validation/loss=1.043949, validation/num_examples=50000
I0302 03:01:25.758449 140522554390272 logging_writer.py:48] [468200] global_step=468200, grad_norm=4.653138160705566, loss=0.6386635303497314
I0302 03:01:59.415723 140522404443904 logging_writer.py:48] [468300] global_step=468300, grad_norm=4.393749237060547, loss=0.6049984693527222
I0302 03:02:33.185102 140522554390272 logging_writer.py:48] [468400] global_step=468400, grad_norm=4.655406951904297, loss=0.6531623005867004
I0302 03:03:06.946024 140522404443904 logging_writer.py:48] [468500] global_step=468500, grad_norm=4.232371807098389, loss=0.5677241086959839
I0302 03:03:40.793546 140522554390272 logging_writer.py:48] [468600] global_step=468600, grad_norm=5.0559000968933105, loss=0.6612066626548767
I0302 03:04:14.584612 140522404443904 logging_writer.py:48] [468700] global_step=468700, grad_norm=4.272148132324219, loss=0.6595338582992554
I0302 03:04:48.398756 140522554390272 logging_writer.py:48] [468800] global_step=468800, grad_norm=4.4673991203308105, loss=0.6437193155288696
I0302 03:05:22.162928 140522404443904 logging_writer.py:48] [468900] global_step=468900, grad_norm=4.581874370574951, loss=0.6725451946258545
I0302 03:05:55.963963 140522554390272 logging_writer.py:48] [469000] global_step=469000, grad_norm=4.380130767822266, loss=0.6274697184562683
I0302 03:06:29.749835 140522404443904 logging_writer.py:48] [469100] global_step=469100, grad_norm=4.439999103546143, loss=0.6355298161506653
I0302 03:07:03.511095 140522554390272 logging_writer.py:48] [469200] global_step=469200, grad_norm=4.267212867736816, loss=0.5777802467346191
I0302 03:07:37.274843 140522404443904 logging_writer.py:48] [469300] global_step=469300, grad_norm=4.3866143226623535, loss=0.604755699634552
I0302 03:08:10.984988 140522554390272 logging_writer.py:48] [469400] global_step=469400, grad_norm=4.447197437286377, loss=0.6032702326774597
I0302 03:08:44.756729 140522404443904 logging_writer.py:48] [469500] global_step=469500, grad_norm=4.557094573974609, loss=0.6664848327636719
I0302 03:09:18.538197 140522554390272 logging_writer.py:48] [469600] global_step=469600, grad_norm=4.560403823852539, loss=0.547155499458313
I0302 03:09:35.334763 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:09:41.322006 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:09:49.914091 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:09:52.283106 140688601454400 submission_runner.py:411] Time since start: 164187.35s, 	Step: 469651, 	{'train/accuracy': 0.9607182741165161, 'train/loss': 0.14770126342773438, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.0445095300674438, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.822662591934204, 'test/num_examples': 10000, 'score': 158671.50946760178, 'total_duration': 164187.35395240784, 'accumulated_submission_time': 158671.50946760178, 'accumulated_eval_time': 5475.40735244751, 'accumulated_logging_time': 24.295584201812744}
I0302 03:09:52.377777 140522404443904 logging_writer.py:48] [469651] accumulated_eval_time=5475.407352, accumulated_logging_time=24.295584, accumulated_submission_time=158671.509468, global_step=469651, preemption_count=0, score=158671.509468, test/accuracy=0.630700, test/loss=1.822663, test/num_examples=10000, total_duration=164187.353952, train/accuracy=0.960718, train/loss=0.147701, validation/accuracy=0.755540, validation/loss=1.044510, validation/num_examples=50000
I0302 03:10:09.286226 140525256554240 logging_writer.py:48] [469700] global_step=469700, grad_norm=4.765326023101807, loss=0.6361905336380005
I0302 03:10:42.976962 140522404443904 logging_writer.py:48] [469800] global_step=469800, grad_norm=5.307816505432129, loss=0.6443361043930054
I0302 03:11:16.773032 140525256554240 logging_writer.py:48] [469900] global_step=469900, grad_norm=4.65086030960083, loss=0.7080752849578857
I0302 03:11:50.552177 140522404443904 logging_writer.py:48] [470000] global_step=470000, grad_norm=4.981439113616943, loss=0.6590864658355713
I0302 03:12:24.344179 140525256554240 logging_writer.py:48] [470100] global_step=470100, grad_norm=4.327877044677734, loss=0.5995122194290161
I0302 03:12:58.107010 140522404443904 logging_writer.py:48] [470200] global_step=470200, grad_norm=4.620450496673584, loss=0.5672987103462219
I0302 03:13:31.862769 140525256554240 logging_writer.py:48] [470300] global_step=470300, grad_norm=4.356498718261719, loss=0.6423540115356445
I0302 03:14:05.605057 140522404443904 logging_writer.py:48] [470400] global_step=470400, grad_norm=4.658906936645508, loss=0.6576367616653442
I0302 03:14:39.420799 140525256554240 logging_writer.py:48] [470500] global_step=470500, grad_norm=4.764578819274902, loss=0.6678439378738403
I0302 03:15:13.216163 140522404443904 logging_writer.py:48] [470600] global_step=470600, grad_norm=4.810609340667725, loss=0.6545140743255615
I0302 03:15:47.048760 140525256554240 logging_writer.py:48] [470700] global_step=470700, grad_norm=4.320399284362793, loss=0.6131624579429626
I0302 03:16:20.826647 140522404443904 logging_writer.py:48] [470800] global_step=470800, grad_norm=4.920376777648926, loss=0.6739566326141357
I0302 03:16:54.630055 140525256554240 logging_writer.py:48] [470900] global_step=470900, grad_norm=4.307997703552246, loss=0.5581045150756836
I0302 03:17:28.394115 140522404443904 logging_writer.py:48] [471000] global_step=471000, grad_norm=4.56260347366333, loss=0.6073207259178162
I0302 03:18:02.135680 140525256554240 logging_writer.py:48] [471100] global_step=471100, grad_norm=4.142548561096191, loss=0.5575568675994873
I0302 03:18:22.525569 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:18:28.532523 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:18:37.157931 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:18:39.457482 140688601454400 submission_runner.py:411] Time since start: 164714.53s, 	Step: 471162, 	{'train/accuracy': 0.9594826102256775, 'train/loss': 0.1514231264591217, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.043542504310608, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.821772575378418, 'test/num_examples': 10000, 'score': 159181.595911026, 'total_duration': 164714.5281674862, 'accumulated_submission_time': 159181.595911026, 'accumulated_eval_time': 5492.339055299759, 'accumulated_logging_time': 24.399834871292114}
I0302 03:18:39.577401 140523947947776 logging_writer.py:48] [471162] accumulated_eval_time=5492.339055, accumulated_logging_time=24.399835, accumulated_submission_time=159181.595911, global_step=471162, preemption_count=0, score=159181.595911, test/accuracy=0.630800, test/loss=1.821773, test/num_examples=10000, total_duration=164714.528167, train/accuracy=0.959483, train/loss=0.151423, validation/accuracy=0.755180, validation/loss=1.043543, validation/num_examples=50000
I0302 03:18:52.717488 140525273339648 logging_writer.py:48] [471200] global_step=471200, grad_norm=5.178603172302246, loss=0.6479538083076477
I0302 03:19:26.479455 140523947947776 logging_writer.py:48] [471300] global_step=471300, grad_norm=4.212251663208008, loss=0.6093411445617676
I0302 03:20:00.246592 140525273339648 logging_writer.py:48] [471400] global_step=471400, grad_norm=4.297483921051025, loss=0.6759944558143616
I0302 03:20:34.000323 140523947947776 logging_writer.py:48] [471500] global_step=471500, grad_norm=4.770772457122803, loss=0.6411550641059875
I0302 03:21:07.784398 140525273339648 logging_writer.py:48] [471600] global_step=471600, grad_norm=4.7047438621521, loss=0.5857832431793213
I0302 03:21:41.741677 140523947947776 logging_writer.py:48] [471700] global_step=471700, grad_norm=4.262057304382324, loss=0.5791312456130981
I0302 03:22:15.536995 140525273339648 logging_writer.py:48] [471800] global_step=471800, grad_norm=4.2387166023254395, loss=0.6564841270446777
I0302 03:22:49.292059 140523947947776 logging_writer.py:48] [471900] global_step=471900, grad_norm=4.874680042266846, loss=0.6731804609298706
I0302 03:23:23.062670 140525273339648 logging_writer.py:48] [472000] global_step=472000, grad_norm=5.096248149871826, loss=0.6356496214866638
I0302 03:23:56.813414 140523947947776 logging_writer.py:48] [472100] global_step=472100, grad_norm=4.591917991638184, loss=0.6319423317909241
I0302 03:24:30.583559 140525273339648 logging_writer.py:48] [472200] global_step=472200, grad_norm=4.795486927032471, loss=0.6191282868385315
I0302 03:25:04.360641 140523947947776 logging_writer.py:48] [472300] global_step=472300, grad_norm=4.017131805419922, loss=0.5591151714324951
I0302 03:25:38.130524 140525273339648 logging_writer.py:48] [472400] global_step=472400, grad_norm=4.749545097351074, loss=0.6034799218177795
I0302 03:26:11.882688 140523947947776 logging_writer.py:48] [472500] global_step=472500, grad_norm=5.066995620727539, loss=0.7539555430412292
I0302 03:26:45.629521 140525273339648 logging_writer.py:48] [472600] global_step=472600, grad_norm=4.496399402618408, loss=0.6242256164550781
I0302 03:27:09.773309 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:27:15.816213 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:27:24.465038 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:27:26.754939 140688601454400 submission_runner.py:411] Time since start: 165241.83s, 	Step: 472673, 	{'train/accuracy': 0.9615553021430969, 'train/loss': 0.14318634569644928, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.0438499450683594, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8215082883834839, 'test/num_examples': 10000, 'score': 159691.7286350727, 'total_duration': 165241.82578277588, 'accumulated_submission_time': 159691.7286350727, 'accumulated_eval_time': 5509.320637464523, 'accumulated_logging_time': 24.530014991760254}
I0302 03:27:26.852066 140522554390272 logging_writer.py:48] [472673] accumulated_eval_time=5509.320637, accumulated_logging_time=24.530015, accumulated_submission_time=159691.728635, global_step=472673, preemption_count=0, score=159691.728635, test/accuracy=0.630200, test/loss=1.821508, test/num_examples=10000, total_duration=165241.825783, train/accuracy=0.961555, train/loss=0.143186, validation/accuracy=0.755300, validation/loss=1.043850, validation/num_examples=50000
I0302 03:27:36.313254 140523092305664 logging_writer.py:48] [472700] global_step=472700, grad_norm=4.8743462562561035, loss=0.6824039816856384
I0302 03:28:10.126249 140522554390272 logging_writer.py:48] [472800] global_step=472800, grad_norm=4.742260456085205, loss=0.5644677877426147
I0302 03:28:43.877842 140523092305664 logging_writer.py:48] [472900] global_step=472900, grad_norm=4.578921318054199, loss=0.6230542659759521
I0302 03:29:17.624064 140522554390272 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.568967819213867, loss=0.65140700340271
I0302 03:29:51.425319 140523092305664 logging_writer.py:48] [473100] global_step=473100, grad_norm=4.488603115081787, loss=0.5997987389564514
I0302 03:30:25.219813 140522554390272 logging_writer.py:48] [473200] global_step=473200, grad_norm=4.358714580535889, loss=0.6508455276489258
I0302 03:30:59.007270 140523092305664 logging_writer.py:48] [473300] global_step=473300, grad_norm=4.773502349853516, loss=0.638935387134552
I0302 03:31:32.819437 140522554390272 logging_writer.py:48] [473400] global_step=473400, grad_norm=4.207102298736572, loss=0.5684330463409424
I0302 03:32:06.633492 140523092305664 logging_writer.py:48] [473500] global_step=473500, grad_norm=4.981958866119385, loss=0.7478668689727783
I0302 03:32:40.439756 140522554390272 logging_writer.py:48] [473600] global_step=473600, grad_norm=4.502956867218018, loss=0.5769180655479431
I0302 03:33:14.220801 140523092305664 logging_writer.py:48] [473700] global_step=473700, grad_norm=4.856838703155518, loss=0.6458332538604736
I0302 03:33:48.003359 140522554390272 logging_writer.py:48] [473800] global_step=473800, grad_norm=5.1519012451171875, loss=0.6166962385177612
I0302 03:34:21.860194 140523092305664 logging_writer.py:48] [473900] global_step=473900, grad_norm=4.743036270141602, loss=0.6787065267562866
I0302 03:34:55.646529 140522554390272 logging_writer.py:48] [474000] global_step=474000, grad_norm=4.522775650024414, loss=0.6033018231391907
I0302 03:35:29.436061 140523092305664 logging_writer.py:48] [474100] global_step=474100, grad_norm=4.726775646209717, loss=0.6371883749961853
I0302 03:35:56.995326 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:36:03.127161 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:36:11.749108 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:36:14.102036 140688601454400 submission_runner.py:411] Time since start: 165769.17s, 	Step: 474183, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14378973841667175, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.045137643814087, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.823304533958435, 'test/num_examples': 10000, 'score': 160201.8107573986, 'total_duration': 165769.17288303375, 'accumulated_submission_time': 160201.8107573986, 'accumulated_eval_time': 5526.427300453186, 'accumulated_logging_time': 24.63716220855713}
I0302 03:36:14.198074 140522404443904 logging_writer.py:48] [474183] accumulated_eval_time=5526.427300, accumulated_logging_time=24.637162, accumulated_submission_time=160201.810757, global_step=474183, preemption_count=0, score=160201.810757, test/accuracy=0.631000, test/loss=1.823305, test/num_examples=10000, total_duration=165769.172883, train/accuracy=0.961476, train/loss=0.143790, validation/accuracy=0.754860, validation/loss=1.045138, validation/num_examples=50000
I0302 03:36:20.268563 140522554390272 logging_writer.py:48] [474200] global_step=474200, grad_norm=4.2533392906188965, loss=0.5469887852668762
I0302 03:36:54.017086 140522404443904 logging_writer.py:48] [474300] global_step=474300, grad_norm=4.502624034881592, loss=0.6477221846580505
I0302 03:37:27.793643 140522554390272 logging_writer.py:48] [474400] global_step=474400, grad_norm=4.441598892211914, loss=0.624894380569458
I0302 03:38:01.573611 140522404443904 logging_writer.py:48] [474500] global_step=474500, grad_norm=4.52977180480957, loss=0.5613812804222107
I0302 03:38:35.361192 140522554390272 logging_writer.py:48] [474600] global_step=474600, grad_norm=4.424977779388428, loss=0.6048742532730103
I0302 03:39:09.146261 140522404443904 logging_writer.py:48] [474700] global_step=474700, grad_norm=4.370115756988525, loss=0.5804108381271362
I0302 03:39:42.950630 140522554390272 logging_writer.py:48] [474800] global_step=474800, grad_norm=4.231902122497559, loss=0.6205370426177979
I0302 03:40:16.809984 140522404443904 logging_writer.py:48] [474900] global_step=474900, grad_norm=4.529240608215332, loss=0.620749831199646
I0302 03:40:50.605341 140522554390272 logging_writer.py:48] [475000] global_step=475000, grad_norm=4.581729888916016, loss=0.6919195652008057
I0302 03:41:24.397136 140522404443904 logging_writer.py:48] [475100] global_step=475100, grad_norm=4.548184394836426, loss=0.6065486669540405
I0302 03:41:58.170875 140522554390272 logging_writer.py:48] [475200] global_step=475200, grad_norm=4.403591156005859, loss=0.6376923322677612
I0302 03:42:31.965480 140522404443904 logging_writer.py:48] [475300] global_step=475300, grad_norm=4.536911487579346, loss=0.6041767597198486
I0302 03:43:05.767787 140522554390272 logging_writer.py:48] [475400] global_step=475400, grad_norm=4.616195201873779, loss=0.6689683198928833
I0302 03:43:39.591910 140522404443904 logging_writer.py:48] [475500] global_step=475500, grad_norm=4.457624435424805, loss=0.5626366138458252
I0302 03:44:13.380870 140522554390272 logging_writer.py:48] [475600] global_step=475600, grad_norm=4.454061985015869, loss=0.5549689531326294
I0302 03:44:44.253010 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:44:50.260607 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:44:58.795406 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:45:01.054239 140688601454400 submission_runner.py:411] Time since start: 166296.13s, 	Step: 475693, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.1477307230234146, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.0440150499343872, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8224012851715088, 'test/num_examples': 10000, 'score': 160711.80350279808, 'total_duration': 166296.12507390976, 'accumulated_submission_time': 160711.80350279808, 'accumulated_eval_time': 5543.228472948074, 'accumulated_logging_time': 24.7429301738739}
I0302 03:45:01.148672 140525264946944 logging_writer.py:48] [475693] accumulated_eval_time=5543.228473, accumulated_logging_time=24.742930, accumulated_submission_time=160711.803503, global_step=475693, preemption_count=0, score=160711.803503, test/accuracy=0.630500, test/loss=1.822401, test/num_examples=10000, total_duration=166296.125074, train/accuracy=0.960280, train/loss=0.147731, validation/accuracy=0.755200, validation/loss=1.044015, validation/num_examples=50000
I0302 03:45:03.849362 140525273339648 logging_writer.py:48] [475700] global_step=475700, grad_norm=4.52957010269165, loss=0.6182458996772766
I0302 03:45:37.596997 140525264946944 logging_writer.py:48] [475800] global_step=475800, grad_norm=4.108006000518799, loss=0.5592201352119446
I0302 03:46:11.339100 140525273339648 logging_writer.py:48] [475900] global_step=475900, grad_norm=5.253438472747803, loss=0.6660894155502319
I0302 03:46:45.199914 140525264946944 logging_writer.py:48] [476000] global_step=476000, grad_norm=4.693139553070068, loss=0.6356600522994995
I0302 03:47:18.997834 140525273339648 logging_writer.py:48] [476100] global_step=476100, grad_norm=4.7211127281188965, loss=0.549444854259491
I0302 03:47:52.765098 140525264946944 logging_writer.py:48] [476200] global_step=476200, grad_norm=4.24625825881958, loss=0.5429336428642273
I0302 03:48:26.518301 140525273339648 logging_writer.py:48] [476300] global_step=476300, grad_norm=4.470226764678955, loss=0.5984984040260315
I0302 03:49:00.279450 140525264946944 logging_writer.py:48] [476400] global_step=476400, grad_norm=4.263089656829834, loss=0.5896629691123962
I0302 03:49:34.040609 140525273339648 logging_writer.py:48] [476500] global_step=476500, grad_norm=4.666871547698975, loss=0.6516796350479126
I0302 03:50:07.845615 140525264946944 logging_writer.py:48] [476600] global_step=476600, grad_norm=3.9525320529937744, loss=0.48801612854003906
I0302 03:50:41.624984 140525273339648 logging_writer.py:48] [476700] global_step=476700, grad_norm=4.089923858642578, loss=0.548866868019104
I0302 03:51:15.450004 140525264946944 logging_writer.py:48] [476800] global_step=476800, grad_norm=4.53823709487915, loss=0.635736882686615
I0302 03:51:49.227343 140525273339648 logging_writer.py:48] [476900] global_step=476900, grad_norm=4.570192337036133, loss=0.6219015717506409
I0302 03:52:23.022958 140525264946944 logging_writer.py:48] [477000] global_step=477000, grad_norm=4.112339496612549, loss=0.5596193075180054
I0302 03:52:56.903206 140525273339648 logging_writer.py:48] [477100] global_step=477100, grad_norm=4.342740535736084, loss=0.5878292322158813
I0302 03:53:30.673689 140525264946944 logging_writer.py:48] [477200] global_step=477200, grad_norm=5.1537675857543945, loss=0.6659483909606934
I0302 03:53:31.166718 140688601454400 spec.py:321] Evaluating on the training split.
I0302 03:53:37.150370 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 03:53:45.681879 140688601454400 spec.py:349] Evaluating on the test split.
I0302 03:53:47.986173 140688601454400 submission_runner.py:411] Time since start: 166823.06s, 	Step: 477203, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.15131667256355286, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0432075262069702, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8201723098754883, 'test/num_examples': 10000, 'score': 161221.76031327248, 'total_duration': 166823.05702018738, 'accumulated_submission_time': 161221.76031327248, 'accumulated_eval_time': 5560.047877073288, 'accumulated_logging_time': 24.84749460220337}
I0302 03:53:48.081342 140522404443904 logging_writer.py:48] [477203] accumulated_eval_time=5560.047877, accumulated_logging_time=24.847495, accumulated_submission_time=161221.760313, global_step=477203, preemption_count=0, score=161221.760313, test/accuracy=0.629900, test/loss=1.820172, test/num_examples=10000, total_duration=166823.057020, train/accuracy=0.960041, train/loss=0.151317, validation/accuracy=0.754720, validation/loss=1.043208, validation/num_examples=50000
I0302 03:54:21.148046 140522554390272 logging_writer.py:48] [477300] global_step=477300, grad_norm=4.141936779022217, loss=0.6159300208091736
I0302 03:54:54.942271 140522404443904 logging_writer.py:48] [477400] global_step=477400, grad_norm=4.50827169418335, loss=0.6159117817878723
I0302 03:55:28.689451 140522554390272 logging_writer.py:48] [477500] global_step=477500, grad_norm=4.659183979034424, loss=0.6402443647384644
I0302 03:56:02.492074 140522404443904 logging_writer.py:48] [477600] global_step=477600, grad_norm=4.791139125823975, loss=0.6798424124717712
I0302 03:56:36.283696 140522554390272 logging_writer.py:48] [477700] global_step=477700, grad_norm=4.459030628204346, loss=0.5287511348724365
I0302 03:57:10.319312 140522404443904 logging_writer.py:48] [477800] global_step=477800, grad_norm=4.49498176574707, loss=0.595375120639801
I0302 03:57:44.090216 140522554390272 logging_writer.py:48] [477900] global_step=477900, grad_norm=4.204747676849365, loss=0.5896369218826294
I0302 03:58:17.875384 140522404443904 logging_writer.py:48] [478000] global_step=478000, grad_norm=4.2226762771606445, loss=0.5850709676742554
I0302 03:58:51.731951 140522554390272 logging_writer.py:48] [478100] global_step=478100, grad_norm=4.509727478027344, loss=0.6362666487693787
I0302 03:59:25.519173 140522404443904 logging_writer.py:48] [478200] global_step=478200, grad_norm=4.585940837860107, loss=0.6473864316940308
I0302 03:59:59.292779 140522554390272 logging_writer.py:48] [478300] global_step=478300, grad_norm=4.539441108703613, loss=0.6137374639511108
I0302 04:00:33.094528 140522404443904 logging_writer.py:48] [478400] global_step=478400, grad_norm=4.187095642089844, loss=0.586238443851471
I0302 04:01:06.870037 140522554390272 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.673642158508301, loss=0.6823687553405762
I0302 04:01:40.684016 140522404443904 logging_writer.py:48] [478600] global_step=478600, grad_norm=4.205955505371094, loss=0.5478134751319885
I0302 04:02:14.474866 140522554390272 logging_writer.py:48] [478700] global_step=478700, grad_norm=4.051159381866455, loss=0.5400824546813965
I0302 04:02:18.001655 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:02:24.023422 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:02:32.638770 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:02:34.912840 140688601454400 submission_runner.py:411] Time since start: 167349.98s, 	Step: 478712, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.1452740877866745, 'validation/accuracy': 0.7547000050544739, 'validation/loss': 1.045116901397705, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.824659824371338, 'test/num_examples': 10000, 'score': 161731.6183450222, 'total_duration': 167349.98368883133, 'accumulated_submission_time': 161731.6183450222, 'accumulated_eval_time': 5576.959014892578, 'accumulated_logging_time': 24.95271611213684}
I0302 04:02:35.010596 140522404443904 logging_writer.py:48] [478712] accumulated_eval_time=5576.959015, accumulated_logging_time=24.952716, accumulated_submission_time=161731.618345, global_step=478712, preemption_count=0, score=161731.618345, test/accuracy=0.630300, test/loss=1.824660, test/num_examples=10000, total_duration=167349.983689, train/accuracy=0.961017, train/loss=0.145274, validation/accuracy=0.754700, validation/loss=1.045117, validation/num_examples=50000
I0302 04:03:05.019052 140522554390272 logging_writer.py:48] [478800] global_step=478800, grad_norm=4.402378559112549, loss=0.6436546444892883
I0302 04:03:38.759378 140522404443904 logging_writer.py:48] [478900] global_step=478900, grad_norm=4.603094100952148, loss=0.6797065138816833
I0302 04:04:12.495838 140522554390272 logging_writer.py:48] [479000] global_step=479000, grad_norm=4.402554988861084, loss=0.6040950417518616
I0302 04:04:46.357711 140522404443904 logging_writer.py:48] [479100] global_step=479100, grad_norm=5.211844444274902, loss=0.6670462489128113
I0302 04:05:20.146179 140522554390272 logging_writer.py:48] [479200] global_step=479200, grad_norm=4.661947250366211, loss=0.6826469302177429
I0302 04:05:53.925218 140522404443904 logging_writer.py:48] [479300] global_step=479300, grad_norm=4.282835006713867, loss=0.612114429473877
I0302 04:06:27.718828 140522554390272 logging_writer.py:48] [479400] global_step=479400, grad_norm=4.039834976196289, loss=0.5446040630340576
I0302 04:07:01.510568 140522404443904 logging_writer.py:48] [479500] global_step=479500, grad_norm=4.674588203430176, loss=0.6143835186958313
I0302 04:07:35.307360 140522554390272 logging_writer.py:48] [479600] global_step=479600, grad_norm=4.418337821960449, loss=0.6307582855224609
I0302 04:08:09.083376 140522404443904 logging_writer.py:48] [479700] global_step=479700, grad_norm=4.642446994781494, loss=0.6541315317153931
I0302 04:08:42.812881 140522554390272 logging_writer.py:48] [479800] global_step=479800, grad_norm=4.684513568878174, loss=0.6595279574394226
I0302 04:09:16.630728 140522404443904 logging_writer.py:48] [479900] global_step=479900, grad_norm=4.381663799285889, loss=0.616396963596344
I0302 04:09:50.424208 140522554390272 logging_writer.py:48] [480000] global_step=480000, grad_norm=4.879338264465332, loss=0.594253420829773
I0302 04:10:24.217580 140522404443904 logging_writer.py:48] [480100] global_step=480100, grad_norm=4.330061912536621, loss=0.6435545086860657
I0302 04:10:58.070214 140522554390272 logging_writer.py:48] [480200] global_step=480200, grad_norm=4.893418312072754, loss=0.6305838823318481
I0302 04:11:05.001386 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:11:11.125143 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:11:19.703984 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:11:21.981707 140688601454400 submission_runner.py:411] Time since start: 167877.05s, 	Step: 480222, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.15065304934978485, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0434825420379639, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.821131706237793, 'test/num_examples': 10000, 'score': 162241.5458357334, 'total_duration': 167877.05255436897, 'accumulated_submission_time': 162241.5458357334, 'accumulated_eval_time': 5593.939316511154, 'accumulated_logging_time': 25.06274676322937}
I0302 04:11:22.076347 140522396051200 logging_writer.py:48] [480222] accumulated_eval_time=5593.939317, accumulated_logging_time=25.062747, accumulated_submission_time=162241.545836, global_step=480222, preemption_count=0, score=162241.545836, test/accuracy=0.631000, test/loss=1.821132, test/num_examples=10000, total_duration=167877.052554, train/accuracy=0.960260, train/loss=0.150653, validation/accuracy=0.755140, validation/loss=1.043483, validation/num_examples=50000
I0302 04:11:48.785225 140522404443904 logging_writer.py:48] [480300] global_step=480300, grad_norm=4.387803554534912, loss=0.6704753041267395
I0302 04:12:22.509680 140522396051200 logging_writer.py:48] [480400] global_step=480400, grad_norm=4.452258110046387, loss=0.5690485835075378
I0302 04:12:56.256374 140522404443904 logging_writer.py:48] [480500] global_step=480500, grad_norm=4.90995979309082, loss=0.6446163654327393
I0302 04:13:30.048779 140522396051200 logging_writer.py:48] [480600] global_step=480600, grad_norm=4.238966941833496, loss=0.5534554123878479
I0302 04:14:03.838322 140522404443904 logging_writer.py:48] [480700] global_step=480700, grad_norm=4.713631629943848, loss=0.6608250141143799
I0302 04:14:37.632689 140522396051200 logging_writer.py:48] [480800] global_step=480800, grad_norm=4.4866557121276855, loss=0.5743203163146973
I0302 04:15:11.453366 140522404443904 logging_writer.py:48] [480900] global_step=480900, grad_norm=4.529781341552734, loss=0.6150833368301392
I0302 04:15:45.238298 140522396051200 logging_writer.py:48] [481000] global_step=481000, grad_norm=4.438168048858643, loss=0.5954405665397644
I0302 04:16:19.056750 140522404443904 logging_writer.py:48] [481100] global_step=481100, grad_norm=4.903956890106201, loss=0.6737388372421265
I0302 04:16:52.845596 140522396051200 logging_writer.py:48] [481200] global_step=481200, grad_norm=5.258862018585205, loss=0.6677732467651367
I0302 04:17:26.723867 140522404443904 logging_writer.py:48] [481300] global_step=481300, grad_norm=4.351167678833008, loss=0.6151429414749146
I0302 04:18:00.500972 140522396051200 logging_writer.py:48] [481400] global_step=481400, grad_norm=4.830692768096924, loss=0.6281267404556274
I0302 04:18:34.294191 140522404443904 logging_writer.py:48] [481500] global_step=481500, grad_norm=4.112547397613525, loss=0.5628978610038757
I0302 04:19:08.082147 140522396051200 logging_writer.py:48] [481600] global_step=481600, grad_norm=4.7717485427856445, loss=0.5698928833007812
I0302 04:19:41.875835 140522404443904 logging_writer.py:48] [481700] global_step=481700, grad_norm=4.658957004547119, loss=0.6539932489395142
I0302 04:19:52.165056 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:19:58.132193 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:20:06.747692 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:20:09.044518 140688601454400 submission_runner.py:411] Time since start: 168404.12s, 	Step: 481732, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.1497306227684021, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0443615913391113, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8221760988235474, 'test/num_examples': 10000, 'score': 162751.5722372532, 'total_duration': 168404.1153049469, 'accumulated_submission_time': 162751.5722372532, 'accumulated_eval_time': 5610.8186683654785, 'accumulated_logging_time': 25.167808771133423}
I0302 04:20:09.141999 140522404443904 logging_writer.py:48] [481732] accumulated_eval_time=5610.818668, accumulated_logging_time=25.167809, accumulated_submission_time=162751.572237, global_step=481732, preemption_count=0, score=162751.572237, test/accuracy=0.631700, test/loss=1.822176, test/num_examples=10000, total_duration=168404.115305, train/accuracy=0.959901, train/loss=0.149731, validation/accuracy=0.754860, validation/loss=1.044362, validation/num_examples=50000
I0302 04:20:32.404420 140522554390272 logging_writer.py:48] [481800] global_step=481800, grad_norm=4.9448041915893555, loss=0.6216211915016174
I0302 04:21:06.108900 140522404443904 logging_writer.py:48] [481900] global_step=481900, grad_norm=4.294707298278809, loss=0.5822707414627075
I0302 04:21:39.900979 140522554390272 logging_writer.py:48] [482000] global_step=482000, grad_norm=4.022989273071289, loss=0.6208751201629639
I0302 04:22:13.672765 140522404443904 logging_writer.py:48] [482100] global_step=482100, grad_norm=4.462093830108643, loss=0.6432760953903198
I0302 04:22:47.458248 140522554390272 logging_writer.py:48] [482200] global_step=482200, grad_norm=4.432854175567627, loss=0.6112147569656372
I0302 04:23:21.302655 140522404443904 logging_writer.py:48] [482300] global_step=482300, grad_norm=4.326559543609619, loss=0.5745452642440796
I0302 04:23:55.106434 140522554390272 logging_writer.py:48] [482400] global_step=482400, grad_norm=4.473761558532715, loss=0.6130642294883728
I0302 04:24:28.891728 140522404443904 logging_writer.py:48] [482500] global_step=482500, grad_norm=4.169063568115234, loss=0.5825915336608887
I0302 04:25:02.638552 140522554390272 logging_writer.py:48] [482600] global_step=482600, grad_norm=5.200685024261475, loss=0.6286020874977112
I0302 04:25:36.431605 140522404443904 logging_writer.py:48] [482700] global_step=482700, grad_norm=4.391436576843262, loss=0.5742601752281189
I0302 04:26:10.227227 140522554390272 logging_writer.py:48] [482800] global_step=482800, grad_norm=4.635645389556885, loss=0.6416769623756409
I0302 04:26:43.992547 140522404443904 logging_writer.py:48] [482900] global_step=482900, grad_norm=4.449389457702637, loss=0.6504802107810974
I0302 04:27:17.768178 140522554390272 logging_writer.py:48] [483000] global_step=483000, grad_norm=4.586882591247559, loss=0.6128799319267273
I0302 04:27:51.538049 140522404443904 logging_writer.py:48] [483100] global_step=483100, grad_norm=4.372809886932373, loss=0.6281923651695251
I0302 04:28:25.342304 140522554390272 logging_writer.py:48] [483200] global_step=483200, grad_norm=4.436182975769043, loss=0.5980252027511597
I0302 04:28:39.340503 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:28:45.356026 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:28:53.845701 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:28:56.121440 140688601454400 submission_runner.py:411] Time since start: 168931.19s, 	Step: 483243, 	{'train/accuracy': 0.9590840339660645, 'train/loss': 0.14985357224941254, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0456264019012451, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8228379487991333, 'test/num_examples': 10000, 'score': 163261.70915150642, 'total_duration': 168931.19228696823, 'accumulated_submission_time': 163261.70915150642, 'accumulated_eval_time': 5627.59955739975, 'accumulated_logging_time': 25.27507972717285}
I0302 04:28:56.217846 140522396051200 logging_writer.py:48] [483243] accumulated_eval_time=5627.599557, accumulated_logging_time=25.275080, accumulated_submission_time=163261.709152, global_step=483243, preemption_count=0, score=163261.709152, test/accuracy=0.630800, test/loss=1.822838, test/num_examples=10000, total_duration=168931.192287, train/accuracy=0.959084, train/loss=0.149854, validation/accuracy=0.754880, validation/loss=1.045626, validation/num_examples=50000
I0302 04:29:15.765804 140522404443904 logging_writer.py:48] [483300] global_step=483300, grad_norm=4.405109405517578, loss=0.6043450832366943
I0302 04:29:49.533819 140522396051200 logging_writer.py:48] [483400] global_step=483400, grad_norm=4.6894378662109375, loss=0.6250278353691101
I0302 04:30:23.273224 140522404443904 logging_writer.py:48] [483500] global_step=483500, grad_norm=4.271551132202148, loss=0.569185197353363
I0302 04:30:57.023183 140522396051200 logging_writer.py:48] [483600] global_step=483600, grad_norm=4.016927719116211, loss=0.6064205169677734
I0302 04:31:30.831875 140522404443904 logging_writer.py:48] [483700] global_step=483700, grad_norm=4.32859992980957, loss=0.6449019908905029
I0302 04:32:04.578338 140522396051200 logging_writer.py:48] [483800] global_step=483800, grad_norm=4.480440139770508, loss=0.6368699073791504
I0302 04:32:38.352565 140522404443904 logging_writer.py:48] [483900] global_step=483900, grad_norm=4.273789882659912, loss=0.590404748916626
I0302 04:33:12.123130 140522396051200 logging_writer.py:48] [484000] global_step=484000, grad_norm=4.476207256317139, loss=0.6314572095870972
I0302 04:33:45.893229 140522404443904 logging_writer.py:48] [484100] global_step=484100, grad_norm=4.793360710144043, loss=0.7023789882659912
I0302 04:34:19.707669 140522396051200 logging_writer.py:48] [484200] global_step=484200, grad_norm=4.253752708435059, loss=0.6308631300926208
I0302 04:34:53.495986 140522404443904 logging_writer.py:48] [484300] global_step=484300, grad_norm=4.3520731925964355, loss=0.5891937017440796
I0302 04:35:27.374538 140522396051200 logging_writer.py:48] [484400] global_step=484400, grad_norm=4.901820182800293, loss=0.6350732445716858
I0302 04:36:01.142448 140522404443904 logging_writer.py:48] [484500] global_step=484500, grad_norm=3.9652292728424072, loss=0.5475023984909058
I0302 04:36:34.909822 140522396051200 logging_writer.py:48] [484600] global_step=484600, grad_norm=4.209312915802002, loss=0.6004009246826172
I0302 04:37:08.723127 140522404443904 logging_writer.py:48] [484700] global_step=484700, grad_norm=4.794836044311523, loss=0.7082160711288452
I0302 04:37:26.122190 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:37:32.075245 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:37:40.708829 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:37:43.013786 140688601454400 submission_runner.py:411] Time since start: 169458.08s, 	Step: 484753, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14704766869544983, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.0443992614746094, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8222293853759766, 'test/num_examples': 10000, 'score': 163771.55219697952, 'total_duration': 169458.08463525772, 'accumulated_submission_time': 163771.55219697952, 'accumulated_eval_time': 5644.491109132767, 'accumulated_logging_time': 25.381606101989746}
I0302 04:37:43.112436 140522554390272 logging_writer.py:48] [484753] accumulated_eval_time=5644.491109, accumulated_logging_time=25.381606, accumulated_submission_time=163771.552197, global_step=484753, preemption_count=0, score=163771.552197, test/accuracy=0.630500, test/loss=1.822229, test/num_examples=10000, total_duration=169458.084635, train/accuracy=0.960698, train/loss=0.147048, validation/accuracy=0.755400, validation/loss=1.044399, validation/num_examples=50000
I0302 04:37:59.302155 140525256554240 logging_writer.py:48] [484800] global_step=484800, grad_norm=4.59350061416626, loss=0.6632446646690369
I0302 04:38:33.067826 140522554390272 logging_writer.py:48] [484900] global_step=484900, grad_norm=4.569199562072754, loss=0.666782557964325
I0302 04:39:06.815500 140525256554240 logging_writer.py:48] [485000] global_step=485000, grad_norm=4.491856098175049, loss=0.6474198698997498
I0302 04:39:40.624103 140522554390272 logging_writer.py:48] [485100] global_step=485100, grad_norm=4.596987724304199, loss=0.6505335569381714
I0302 04:40:14.395681 140525256554240 logging_writer.py:48] [485200] global_step=485200, grad_norm=4.379790782928467, loss=0.6096701622009277
I0302 04:40:48.164296 140522554390272 logging_writer.py:48] [485300] global_step=485300, grad_norm=5.382715225219727, loss=0.6558995246887207
I0302 04:41:21.932589 140525256554240 logging_writer.py:48] [485400] global_step=485400, grad_norm=4.498178482055664, loss=0.5587219595909119
I0302 04:41:55.804913 140522554390272 logging_writer.py:48] [485500] global_step=485500, grad_norm=4.211663246154785, loss=0.6647005081176758
I0302 04:42:29.587421 140525256554240 logging_writer.py:48] [485600] global_step=485600, grad_norm=4.599026679992676, loss=0.5702667832374573
I0302 04:43:03.312725 140522554390272 logging_writer.py:48] [485700] global_step=485700, grad_norm=4.1256537437438965, loss=0.587604820728302
I0302 04:43:37.086019 140525256554240 logging_writer.py:48] [485800] global_step=485800, grad_norm=4.624532699584961, loss=0.5656588077545166
I0302 04:44:10.867226 140522554390272 logging_writer.py:48] [485900] global_step=485900, grad_norm=4.39729118347168, loss=0.6166918277740479
I0302 04:44:44.640552 140525256554240 logging_writer.py:48] [486000] global_step=486000, grad_norm=4.273306369781494, loss=0.4735648036003113
I0302 04:45:18.404851 140522554390272 logging_writer.py:48] [486100] global_step=486100, grad_norm=4.6953630447387695, loss=0.6815138459205627
I0302 04:45:52.158155 140525256554240 logging_writer.py:48] [486200] global_step=486200, grad_norm=4.332528114318848, loss=0.6178988218307495
I0302 04:46:13.216857 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:46:19.312086 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:46:27.818461 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:46:30.169379 140688601454400 submission_runner.py:411] Time since start: 169985.24s, 	Step: 486264, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14761821925640106, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.044253945350647, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8217581510543823, 'test/num_examples': 10000, 'score': 164281.594363451, 'total_duration': 169985.24022579193, 'accumulated_submission_time': 164281.594363451, 'accumulated_eval_time': 5661.443585395813, 'accumulated_logging_time': 25.490796089172363}
I0302 04:46:30.267987 140523947947776 logging_writer.py:48] [486264] accumulated_eval_time=5661.443585, accumulated_logging_time=25.490796, accumulated_submission_time=164281.594363, global_step=486264, preemption_count=0, score=164281.594363, test/accuracy=0.630700, test/loss=1.821758, test/num_examples=10000, total_duration=169985.240226, train/accuracy=0.960459, train/loss=0.147618, validation/accuracy=0.755040, validation/loss=1.044254, validation/num_examples=50000
I0302 04:46:42.788135 140525273339648 logging_writer.py:48] [486300] global_step=486300, grad_norm=4.095378875732422, loss=0.616517186164856
I0302 04:47:16.548431 140523947947776 logging_writer.py:48] [486400] global_step=486400, grad_norm=4.467940330505371, loss=0.6081894636154175
I0302 04:47:50.277710 140525273339648 logging_writer.py:48] [486500] global_step=486500, grad_norm=4.97018575668335, loss=0.5722284317016602
I0302 04:48:24.135148 140523947947776 logging_writer.py:48] [486600] global_step=486600, grad_norm=4.644253253936768, loss=0.6852107644081116
I0302 04:48:57.897709 140525273339648 logging_writer.py:48] [486700] global_step=486700, grad_norm=4.651704788208008, loss=0.6698572635650635
I0302 04:49:31.648015 140523947947776 logging_writer.py:48] [486800] global_step=486800, grad_norm=4.403666019439697, loss=0.56163090467453
I0302 04:50:05.435782 140525273339648 logging_writer.py:48] [486900] global_step=486900, grad_norm=4.538544178009033, loss=0.5747793912887573
I0302 04:50:39.219468 140523947947776 logging_writer.py:48] [487000] global_step=487000, grad_norm=4.580683708190918, loss=0.6749328970909119
I0302 04:51:13.003358 140525273339648 logging_writer.py:48] [487100] global_step=487100, grad_norm=5.163488388061523, loss=0.5990869998931885
I0302 04:51:46.773226 140523947947776 logging_writer.py:48] [487200] global_step=487200, grad_norm=4.473224639892578, loss=0.6401444673538208
I0302 04:52:20.517375 140525273339648 logging_writer.py:48] [487300] global_step=487300, grad_norm=4.140368461608887, loss=0.608802855014801
I0302 04:52:54.285074 140523947947776 logging_writer.py:48] [487400] global_step=487400, grad_norm=4.599277496337891, loss=0.7008431553840637
I0302 04:53:28.046346 140525273339648 logging_writer.py:48] [487500] global_step=487500, grad_norm=4.663832187652588, loss=0.5391236543655396
I0302 04:54:01.878949 140523947947776 logging_writer.py:48] [487600] global_step=487600, grad_norm=4.062612533569336, loss=0.5618582367897034
I0302 04:54:35.649221 140525273339648 logging_writer.py:48] [487700] global_step=487700, grad_norm=4.98371696472168, loss=0.6489701271057129
I0302 04:55:00.448162 140688601454400 spec.py:321] Evaluating on the training split.
I0302 04:55:06.562321 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 04:55:15.115322 140688601454400 spec.py:349] Evaluating on the test split.
I0302 04:55:17.382022 140688601454400 submission_runner.py:411] Time since start: 170512.45s, 	Step: 487775, 	{'train/accuracy': 0.9592434167861938, 'train/loss': 0.15171094238758087, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0429028272628784, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.819995641708374, 'test/num_examples': 10000, 'score': 164791.71216368675, 'total_duration': 170512.45286893845, 'accumulated_submission_time': 164791.71216368675, 'accumulated_eval_time': 5678.37740445137, 'accumulated_logging_time': 25.59970498085022}
I0302 04:55:17.480207 140523092305664 logging_writer.py:48] [487775] accumulated_eval_time=5678.377404, accumulated_logging_time=25.599705, accumulated_submission_time=164791.712164, global_step=487775, preemption_count=0, score=164791.712164, test/accuracy=0.632100, test/loss=1.819996, test/num_examples=10000, total_duration=170512.452869, train/accuracy=0.959243, train/loss=0.151711, validation/accuracy=0.755340, validation/loss=1.042903, validation/num_examples=50000
I0302 04:55:26.280208 140525256554240 logging_writer.py:48] [487800] global_step=487800, grad_norm=4.771604537963867, loss=0.6915621161460876
I0302 04:55:59.968882 140523092305664 logging_writer.py:48] [487900] global_step=487900, grad_norm=4.94700288772583, loss=0.5991562008857727
I0302 04:56:33.769966 140525256554240 logging_writer.py:48] [488000] global_step=488000, grad_norm=4.5295610427856445, loss=0.6053409576416016
I0302 04:57:07.479186 140523092305664 logging_writer.py:48] [488100] global_step=488100, grad_norm=4.828818321228027, loss=0.6521816253662109
I0302 04:57:41.234794 140525256554240 logging_writer.py:48] [488200] global_step=488200, grad_norm=4.808533668518066, loss=0.6126164197921753
I0302 04:58:14.998457 140523092305664 logging_writer.py:48] [488300] global_step=488300, grad_norm=4.337338924407959, loss=0.5826457738876343
I0302 04:58:48.740628 140525256554240 logging_writer.py:48] [488400] global_step=488400, grad_norm=4.658769130706787, loss=0.6625739336013794
I0302 04:59:22.544578 140523092305664 logging_writer.py:48] [488500] global_step=488500, grad_norm=4.4839372634887695, loss=0.6452541947364807
I0302 04:59:56.297466 140525256554240 logging_writer.py:48] [488600] global_step=488600, grad_norm=5.155663013458252, loss=0.7025731801986694
I0302 05:00:30.136072 140523092305664 logging_writer.py:48] [488700] global_step=488700, grad_norm=4.725278377532959, loss=0.6061154007911682
I0302 05:01:03.877755 140525256554240 logging_writer.py:48] [488800] global_step=488800, grad_norm=4.255495548248291, loss=0.666880190372467
I0302 05:01:37.663687 140523092305664 logging_writer.py:48] [488900] global_step=488900, grad_norm=4.459565162658691, loss=0.5980286002159119
I0302 05:02:11.493039 140525256554240 logging_writer.py:48] [489000] global_step=489000, grad_norm=4.520814418792725, loss=0.5864483714103699
I0302 05:02:45.231811 140523092305664 logging_writer.py:48] [489100] global_step=489100, grad_norm=5.157794952392578, loss=0.6836168766021729
I0302 05:03:18.941804 140525256554240 logging_writer.py:48] [489200] global_step=489200, grad_norm=4.243771553039551, loss=0.6083129048347473
I0302 05:03:47.451952 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:03:53.464647 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:04:02.202954 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:04:04.562567 140688601454400 submission_runner.py:411] Time since start: 171039.63s, 	Step: 489286, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.1463460773229599, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0446679592132568, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.820755958557129, 'test/num_examples': 10000, 'score': 165301.6225683689, 'total_duration': 171039.63341140747, 'accumulated_submission_time': 165301.6225683689, 'accumulated_eval_time': 5695.487970590591, 'accumulated_logging_time': 25.707472562789917}
I0302 05:04:04.658118 140522404443904 logging_writer.py:48] [489286] accumulated_eval_time=5695.487971, accumulated_logging_time=25.707473, accumulated_submission_time=165301.622568, global_step=489286, preemption_count=0, score=165301.622568, test/accuracy=0.630300, test/loss=1.820756, test/num_examples=10000, total_duration=171039.633411, train/accuracy=0.960738, train/loss=0.146346, validation/accuracy=0.754840, validation/loss=1.044668, validation/num_examples=50000
I0302 05:04:09.735557 140522554390272 logging_writer.py:48] [489300] global_step=489300, grad_norm=4.492688179016113, loss=0.6658180356025696
I0302 05:04:43.473029 140522404443904 logging_writer.py:48] [489400] global_step=489400, grad_norm=4.366320610046387, loss=0.6530638933181763
I0302 05:05:17.230004 140522554390272 logging_writer.py:48] [489500] global_step=489500, grad_norm=4.612131595611572, loss=0.6047607660293579
I0302 05:05:51.006616 140522404443904 logging_writer.py:48] [489600] global_step=489600, grad_norm=4.298069000244141, loss=0.5898328423500061
I0302 05:06:24.855013 140522554390272 logging_writer.py:48] [489700] global_step=489700, grad_norm=4.3669281005859375, loss=0.625066339969635
I0302 05:06:58.660246 140522404443904 logging_writer.py:48] [489800] global_step=489800, grad_norm=4.439045429229736, loss=0.6107037663459778
I0302 05:07:32.448191 140522554390272 logging_writer.py:48] [489900] global_step=489900, grad_norm=4.9593963623046875, loss=0.5835665464401245
I0302 05:08:06.262327 140522404443904 logging_writer.py:48] [490000] global_step=490000, grad_norm=4.735151290893555, loss=0.6904857754707336
I0302 05:08:40.063806 140522554390272 logging_writer.py:48] [490100] global_step=490100, grad_norm=4.243828773498535, loss=0.635766863822937
I0302 05:09:13.837320 140522404443904 logging_writer.py:48] [490200] global_step=490200, grad_norm=4.692958354949951, loss=0.6089686751365662
I0302 05:09:47.621419 140522554390272 logging_writer.py:48] [490300] global_step=490300, grad_norm=5.774904251098633, loss=0.6836196780204773
I0302 05:10:21.397508 140522404443904 logging_writer.py:48] [490400] global_step=490400, grad_norm=4.711777210235596, loss=0.5953709483146667
I0302 05:10:55.158371 140522554390272 logging_writer.py:48] [490500] global_step=490500, grad_norm=4.614444732666016, loss=0.6356301307678223
I0302 05:11:28.958584 140522404443904 logging_writer.py:48] [490600] global_step=490600, grad_norm=4.119581699371338, loss=0.5099766850471497
I0302 05:12:02.740684 140522554390272 logging_writer.py:48] [490700] global_step=490700, grad_norm=4.364814758300781, loss=0.6527924537658691
I0302 05:12:34.687590 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:12:40.693237 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:12:49.099407 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:12:51.424290 140688601454400 submission_runner.py:411] Time since start: 171566.50s, 	Step: 490796, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14878235757350922, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.0443212985992432, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8227946758270264, 'test/num_examples': 10000, 'score': 165811.5897936821, 'total_duration': 171566.49512457848, 'accumulated_submission_time': 165811.5897936821, 'accumulated_eval_time': 5712.22460770607, 'accumulated_logging_time': 25.813186407089233}
I0302 05:12:51.521722 140525256554240 logging_writer.py:48] [490796] accumulated_eval_time=5712.224608, accumulated_logging_time=25.813186, accumulated_submission_time=165811.589794, global_step=490796, preemption_count=0, score=165811.589794, test/accuracy=0.631700, test/loss=1.822795, test/num_examples=10000, total_duration=171566.495125, train/accuracy=0.960439, train/loss=0.148782, validation/accuracy=0.755300, validation/loss=1.044321, validation/num_examples=50000
I0302 05:12:53.208064 140525264946944 logging_writer.py:48] [490800] global_step=490800, grad_norm=4.50578498840332, loss=0.6038703918457031
I0302 05:13:26.913093 140525256554240 logging_writer.py:48] [490900] global_step=490900, grad_norm=5.460717678070068, loss=0.6284673810005188
I0302 05:14:00.651433 140525264946944 logging_writer.py:48] [491000] global_step=491000, grad_norm=4.862906455993652, loss=0.6998759508132935
I0302 05:14:34.356668 140525256554240 logging_writer.py:48] [491100] global_step=491100, grad_norm=4.8102216720581055, loss=0.6050540804862976
I0302 05:15:08.152342 140525264946944 logging_writer.py:48] [491200] global_step=491200, grad_norm=4.500943660736084, loss=0.6324355602264404
I0302 05:15:41.974697 140525256554240 logging_writer.py:48] [491300] global_step=491300, grad_norm=4.6679606437683105, loss=0.5838058590888977
I0302 05:16:15.759708 140525264946944 logging_writer.py:48] [491400] global_step=491400, grad_norm=4.637424945831299, loss=0.6231956481933594
I0302 05:16:49.497917 140525256554240 logging_writer.py:48] [491500] global_step=491500, grad_norm=4.849141597747803, loss=0.6872963905334473
I0302 05:17:23.283324 140525264946944 logging_writer.py:48] [491600] global_step=491600, grad_norm=4.2470011711120605, loss=0.6444389820098877
I0302 05:17:57.065323 140525256554240 logging_writer.py:48] [491700] global_step=491700, grad_norm=4.222858428955078, loss=0.5625134110450745
I0302 05:18:30.854542 140525264946944 logging_writer.py:48] [491800] global_step=491800, grad_norm=4.331205368041992, loss=0.6416659951210022
I0302 05:19:04.725605 140525256554240 logging_writer.py:48] [491900] global_step=491900, grad_norm=4.275794982910156, loss=0.5669698715209961
I0302 05:19:38.494144 140525264946944 logging_writer.py:48] [492000] global_step=492000, grad_norm=4.353925704956055, loss=0.6652845740318298
I0302 05:20:12.227053 140525256554240 logging_writer.py:48] [492100] global_step=492100, grad_norm=4.182045936584473, loss=0.6144864559173584
I0302 05:20:46.003045 140525264946944 logging_writer.py:48] [492200] global_step=492200, grad_norm=4.737873077392578, loss=0.6884631514549255
I0302 05:21:19.830449 140525256554240 logging_writer.py:48] [492300] global_step=492300, grad_norm=4.860278129577637, loss=0.6801853775978088
I0302 05:21:21.666359 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:21:27.670689 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:21:36.255661 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:21:38.543658 140688601454400 submission_runner.py:411] Time since start: 172093.61s, 	Step: 492307, 	{'train/accuracy': 0.9592036008834839, 'train/loss': 0.15085409581661224, 'validation/accuracy': 0.7555199861526489, 'validation/loss': 1.044857144355774, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.823703646659851, 'test/num_examples': 10000, 'score': 166321.67253041267, 'total_duration': 172093.6145040989, 'accumulated_submission_time': 166321.67253041267, 'accumulated_eval_time': 5729.101854324341, 'accumulated_logging_time': 25.920092582702637}
I0302 05:21:38.642581 140522554390272 logging_writer.py:48] [492307] accumulated_eval_time=5729.101854, accumulated_logging_time=25.920093, accumulated_submission_time=166321.672530, global_step=492307, preemption_count=0, score=166321.672530, test/accuracy=0.632200, test/loss=1.823704, test/num_examples=10000, total_duration=172093.614504, train/accuracy=0.959204, train/loss=0.150854, validation/accuracy=0.755520, validation/loss=1.044857, validation/num_examples=50000
I0302 05:22:10.357254 140523092305664 logging_writer.py:48] [492400] global_step=492400, grad_norm=4.641472816467285, loss=0.6077219247817993
I0302 05:22:44.105172 140522554390272 logging_writer.py:48] [492500] global_step=492500, grad_norm=4.28359317779541, loss=0.5403099060058594
I0302 05:23:17.882499 140523092305664 logging_writer.py:48] [492600] global_step=492600, grad_norm=4.6982808113098145, loss=0.5553117990493774
I0302 05:23:51.697387 140522554390272 logging_writer.py:48] [492700] global_step=492700, grad_norm=5.2380290031433105, loss=0.7100223302841187
I0302 05:24:25.469758 140523092305664 logging_writer.py:48] [492800] global_step=492800, grad_norm=4.891209125518799, loss=0.6328299641609192
I0302 05:24:59.349386 140522554390272 logging_writer.py:48] [492900] global_step=492900, grad_norm=4.933223247528076, loss=0.6598970293998718
I0302 05:25:33.169693 140523092305664 logging_writer.py:48] [493000] global_step=493000, grad_norm=4.26740837097168, loss=0.5940976142883301
I0302 05:26:06.962926 140522554390272 logging_writer.py:48] [493100] global_step=493100, grad_norm=4.493356704711914, loss=0.6128964424133301
I0302 05:26:40.765249 140523092305664 logging_writer.py:48] [493200] global_step=493200, grad_norm=4.651585578918457, loss=0.6504148840904236
I0302 05:27:14.551640 140522554390272 logging_writer.py:48] [493300] global_step=493300, grad_norm=5.006965637207031, loss=0.6497845649719238
I0302 05:27:48.368172 140523092305664 logging_writer.py:48] [493400] global_step=493400, grad_norm=4.475053310394287, loss=0.6808453798294067
I0302 05:28:22.157142 140522554390272 logging_writer.py:48] [493500] global_step=493500, grad_norm=4.669651031494141, loss=0.6025539636611938
I0302 05:28:55.966841 140523092305664 logging_writer.py:48] [493600] global_step=493600, grad_norm=4.392920017242432, loss=0.6128706336021423
I0302 05:29:29.741254 140522554390272 logging_writer.py:48] [493700] global_step=493700, grad_norm=4.810540199279785, loss=0.5795775651931763
I0302 05:30:03.497745 140523092305664 logging_writer.py:48] [493800] global_step=493800, grad_norm=4.774825096130371, loss=0.6542509198188782
I0302 05:30:08.715894 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:30:14.882890 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:30:23.345831 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:30:25.624034 140688601454400 submission_runner.py:411] Time since start: 172620.69s, 	Step: 493817, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.148208349943161, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0426260232925415, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8204790353775024, 'test/num_examples': 10000, 'score': 166831.68437623978, 'total_duration': 172620.69486761093, 'accumulated_submission_time': 166831.68437623978, 'accumulated_eval_time': 5746.009928226471, 'accumulated_logging_time': 26.028761386871338}
I0302 05:30:25.722750 140523947947776 logging_writer.py:48] [493817] accumulated_eval_time=5746.009928, accumulated_logging_time=26.028761, accumulated_submission_time=166831.684376, global_step=493817, preemption_count=0, score=166831.684376, test/accuracy=0.631100, test/loss=1.820479, test/num_examples=10000, total_duration=172620.694868, train/accuracy=0.960738, train/loss=0.148208, validation/accuracy=0.755100, validation/loss=1.042626, validation/num_examples=50000
I0302 05:30:54.036899 140525256554240 logging_writer.py:48] [493900] global_step=493900, grad_norm=4.184481620788574, loss=0.6041650176048279
I0302 05:31:27.993418 140523947947776 logging_writer.py:48] [494000] global_step=494000, grad_norm=4.385232925415039, loss=0.625573456287384
I0302 05:32:01.781804 140525256554240 logging_writer.py:48] [494100] global_step=494100, grad_norm=4.624529838562012, loss=0.6333113312721252
I0302 05:32:35.595557 140523947947776 logging_writer.py:48] [494200] global_step=494200, grad_norm=4.305948734283447, loss=0.5701056122779846
I0302 05:33:09.366504 140525256554240 logging_writer.py:48] [494300] global_step=494300, grad_norm=4.828526020050049, loss=0.6546306014060974
I0302 05:33:43.158978 140523947947776 logging_writer.py:48] [494400] global_step=494400, grad_norm=4.828690052032471, loss=0.6729537844657898
I0302 05:34:16.948850 140525256554240 logging_writer.py:48] [494500] global_step=494500, grad_norm=4.327113151550293, loss=0.5745381116867065
I0302 05:34:50.700083 140523947947776 logging_writer.py:48] [494600] global_step=494600, grad_norm=4.538400650024414, loss=0.6400438547134399
I0302 05:35:24.448957 140525256554240 logging_writer.py:48] [494700] global_step=494700, grad_norm=4.31511926651001, loss=0.6027708053588867
I0302 05:35:58.258588 140523947947776 logging_writer.py:48] [494800] global_step=494800, grad_norm=5.070489883422852, loss=0.6425719857215881
I0302 05:36:32.084451 140525256554240 logging_writer.py:48] [494900] global_step=494900, grad_norm=4.3957295417785645, loss=0.6305373907089233
I0302 05:37:05.865869 140523947947776 logging_writer.py:48] [495000] global_step=495000, grad_norm=4.41877555847168, loss=0.6073434352874756
I0302 05:37:39.722532 140525256554240 logging_writer.py:48] [495100] global_step=495100, grad_norm=4.245203495025635, loss=0.6455202102661133
I0302 05:38:13.479438 140523947947776 logging_writer.py:48] [495200] global_step=495200, grad_norm=4.468255996704102, loss=0.5879119634628296
I0302 05:38:47.308853 140525256554240 logging_writer.py:48] [495300] global_step=495300, grad_norm=4.331595420837402, loss=0.5591022968292236
I0302 05:38:55.903905 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:39:02.059110 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:39:10.690407 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:39:12.959677 140688601454400 submission_runner.py:411] Time since start: 173148.03s, 	Step: 495327, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14538776874542236, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0442039966583252, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8217428922653198, 'test/num_examples': 10000, 'score': 167341.80224132538, 'total_duration': 173148.03052449226, 'accumulated_submission_time': 167341.80224132538, 'accumulated_eval_time': 5763.065656900406, 'accumulated_logging_time': 26.138359785079956}
I0302 05:39:13.059626 140522554390272 logging_writer.py:48] [495327] accumulated_eval_time=5763.065657, accumulated_logging_time=26.138360, accumulated_submission_time=167341.802241, global_step=495327, preemption_count=0, score=167341.802241, test/accuracy=0.632200, test/loss=1.821743, test/num_examples=10000, total_duration=173148.030524, train/accuracy=0.961535, train/loss=0.145388, validation/accuracy=0.755000, validation/loss=1.044204, validation/num_examples=50000
I0302 05:39:37.988720 140523092305664 logging_writer.py:48] [495400] global_step=495400, grad_norm=4.648281097412109, loss=0.6797230243682861
I0302 05:40:11.760081 140522554390272 logging_writer.py:48] [495500] global_step=495500, grad_norm=4.621620178222656, loss=0.6103002429008484
I0302 05:40:45.530203 140523092305664 logging_writer.py:48] [495600] global_step=495600, grad_norm=4.494726657867432, loss=0.593990683555603
I0302 05:41:19.338284 140522554390272 logging_writer.py:48] [495700] global_step=495700, grad_norm=4.350638389587402, loss=0.5963820219039917
I0302 05:41:53.148042 140523092305664 logging_writer.py:48] [495800] global_step=495800, grad_norm=4.878456115722656, loss=0.6530942320823669
I0302 05:42:26.974413 140522554390272 logging_writer.py:48] [495900] global_step=495900, grad_norm=4.132255554199219, loss=0.606726884841919
I0302 05:43:00.786311 140523092305664 logging_writer.py:48] [496000] global_step=496000, grad_norm=4.719529151916504, loss=0.6202863454818726
I0302 05:43:34.641754 140522554390272 logging_writer.py:48] [496100] global_step=496100, grad_norm=4.645761013031006, loss=0.6650136709213257
I0302 05:44:08.480667 140523092305664 logging_writer.py:48] [496200] global_step=496200, grad_norm=4.505565643310547, loss=0.6668533086776733
I0302 05:44:42.291147 140522554390272 logging_writer.py:48] [496300] global_step=496300, grad_norm=4.957300662994385, loss=0.655745804309845
I0302 05:45:16.078178 140523092305664 logging_writer.py:48] [496400] global_step=496400, grad_norm=4.682657241821289, loss=0.7262015342712402
I0302 05:45:49.834621 140522554390272 logging_writer.py:48] [496500] global_step=496500, grad_norm=4.3835835456848145, loss=0.6223602890968323
I0302 05:46:23.621168 140523092305664 logging_writer.py:48] [496600] global_step=496600, grad_norm=4.423549652099609, loss=0.6736109852790833
I0302 05:46:57.425683 140522554390272 logging_writer.py:48] [496700] global_step=496700, grad_norm=4.56989049911499, loss=0.672492265701294
I0302 05:47:31.212333 140523092305664 logging_writer.py:48] [496800] global_step=496800, grad_norm=4.300911903381348, loss=0.6408708095550537
I0302 05:47:43.155911 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:47:49.248942 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:47:57.706237 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:48:00.095273 140688601454400 submission_runner.py:411] Time since start: 173675.17s, 	Step: 496837, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14715123176574707, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0421451330184937, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8195481300354004, 'test/num_examples': 10000, 'score': 167851.83716320992, 'total_duration': 173675.16613030434, 'accumulated_submission_time': 167851.83716320992, 'accumulated_eval_time': 5780.004978179932, 'accumulated_logging_time': 26.24807047843933}
I0302 05:48:00.174759 140525264946944 logging_writer.py:48] [496837] accumulated_eval_time=5780.004978, accumulated_logging_time=26.248070, accumulated_submission_time=167851.837163, global_step=496837, preemption_count=0, score=167851.837163, test/accuracy=0.630400, test/loss=1.819548, test/num_examples=10000, total_duration=173675.166130, train/accuracy=0.960340, train/loss=0.147151, validation/accuracy=0.755180, validation/loss=1.042145, validation/num_examples=50000
I0302 05:48:21.715526 140525273339648 logging_writer.py:48] [496900] global_step=496900, grad_norm=4.6185078620910645, loss=0.6592016220092773
I0302 05:48:55.442075 140525264946944 logging_writer.py:48] [497000] global_step=497000, grad_norm=4.3529205322265625, loss=0.61572265625
I0302 05:49:29.170958 140525273339648 logging_writer.py:48] [497100] global_step=497100, grad_norm=4.595067024230957, loss=0.613089382648468
I0302 05:50:03.029851 140525264946944 logging_writer.py:48] [497200] global_step=497200, grad_norm=4.828476905822754, loss=0.6572939157485962
I0302 05:50:36.804620 140525273339648 logging_writer.py:48] [497300] global_step=497300, grad_norm=4.6265482902526855, loss=0.6498048901557922
I0302 05:51:10.592882 140525264946944 logging_writer.py:48] [497400] global_step=497400, grad_norm=4.455578327178955, loss=0.6026782393455505
I0302 05:51:44.352073 140525273339648 logging_writer.py:48] [497500] global_step=497500, grad_norm=4.661928653717041, loss=0.601790189743042
I0302 05:52:18.144352 140525264946944 logging_writer.py:48] [497600] global_step=497600, grad_norm=4.4021735191345215, loss=0.6426090002059937
I0302 05:52:51.944936 140525273339648 logging_writer.py:48] [497700] global_step=497700, grad_norm=4.890204429626465, loss=0.6892704963684082
I0302 05:53:25.751773 140525264946944 logging_writer.py:48] [497800] global_step=497800, grad_norm=4.841358184814453, loss=0.701401948928833
I0302 05:53:59.562062 140525273339648 logging_writer.py:48] [497900] global_step=497900, grad_norm=5.326322078704834, loss=0.708304762840271
I0302 05:54:33.324408 140525264946944 logging_writer.py:48] [498000] global_step=498000, grad_norm=4.4262566566467285, loss=0.682105302810669
I0302 05:55:07.116738 140525273339648 logging_writer.py:48] [498100] global_step=498100, grad_norm=4.569248676300049, loss=0.6161417961120605
I0302 05:55:40.992048 140525264946944 logging_writer.py:48] [498200] global_step=498200, grad_norm=4.476109504699707, loss=0.6340934038162231
I0302 05:56:14.787287 140525273339648 logging_writer.py:48] [498300] global_step=498300, grad_norm=4.728801727294922, loss=0.6994644999504089
I0302 05:56:30.112059 140688601454400 spec.py:321] Evaluating on the training split.
I0302 05:56:36.116590 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 05:56:44.730776 140688601454400 spec.py:349] Evaluating on the test split.
I0302 05:56:46.987956 140688601454400 submission_runner.py:411] Time since start: 174202.06s, 	Step: 498347, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.1485576331615448, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.044399380683899, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8199979066848755, 'test/num_examples': 10000, 'score': 168361.71323132515, 'total_duration': 174202.0587952137, 'accumulated_submission_time': 168361.71323132515, 'accumulated_eval_time': 5796.880818128586, 'accumulated_logging_time': 26.336756229400635}
I0302 05:56:47.087117 140523092305664 logging_writer.py:48] [498347] accumulated_eval_time=5796.880818, accumulated_logging_time=26.336756, accumulated_submission_time=168361.713231, global_step=498347, preemption_count=0, score=168361.713231, test/accuracy=0.631900, test/loss=1.819998, test/num_examples=10000, total_duration=174202.058795, train/accuracy=0.959602, train/loss=0.148558, validation/accuracy=0.755020, validation/loss=1.044399, validation/num_examples=50000
I0302 05:57:05.331318 140523947947776 logging_writer.py:48] [498400] global_step=498400, grad_norm=4.567094326019287, loss=0.6418874263763428
I0302 05:57:39.075858 140523092305664 logging_writer.py:48] [498500] global_step=498500, grad_norm=4.616075038909912, loss=0.5937038660049438
I0302 05:58:12.825658 140523947947776 logging_writer.py:48] [498600] global_step=498600, grad_norm=4.597287178039551, loss=0.6712498068809509
I0302 05:58:46.605524 140523092305664 logging_writer.py:48] [498700] global_step=498700, grad_norm=4.836106300354004, loss=0.6569872498512268
I0302 05:59:20.421974 140523947947776 logging_writer.py:48] [498800] global_step=498800, grad_norm=4.552781105041504, loss=0.620263934135437
I0302 05:59:54.198210 140523092305664 logging_writer.py:48] [498900] global_step=498900, grad_norm=4.606253147125244, loss=0.705093264579773
I0302 06:00:28.000573 140523947947776 logging_writer.py:48] [499000] global_step=499000, grad_norm=4.00092887878418, loss=0.5558508634567261
I0302 06:01:01.777285 140523092305664 logging_writer.py:48] [499100] global_step=499100, grad_norm=4.613990783691406, loss=0.6107563972473145
I0302 06:01:35.571179 140523947947776 logging_writer.py:48] [499200] global_step=499200, grad_norm=4.478569030761719, loss=0.6360988616943359
I0302 06:02:09.517712 140523092305664 logging_writer.py:48] [499300] global_step=499300, grad_norm=4.669410705566406, loss=0.6170656681060791
I0302 06:02:43.311045 140523947947776 logging_writer.py:48] [499400] global_step=499400, grad_norm=4.465691566467285, loss=0.6230644583702087
I0302 06:03:17.122148 140523092305664 logging_writer.py:48] [499500] global_step=499500, grad_norm=4.797250747680664, loss=0.6957692503929138
I0302 06:03:50.874903 140523947947776 logging_writer.py:48] [499600] global_step=499600, grad_norm=4.902307033538818, loss=0.7635136842727661
I0302 06:04:24.681733 140523092305664 logging_writer.py:48] [499700] global_step=499700, grad_norm=4.94082498550415, loss=0.6656463146209717
I0302 06:04:58.452973 140523947947776 logging_writer.py:48] [499800] global_step=499800, grad_norm=5.012558937072754, loss=0.6297413110733032
I0302 06:05:17.213803 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:05:23.903747 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:05:32.550625 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:05:34.821167 140688601454400 submission_runner.py:411] Time since start: 174729.89s, 	Step: 499857, 	{'train/accuracy': 0.9595423936843872, 'train/loss': 0.15101097524166107, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.043644905090332, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.821232557296753, 'test/num_examples': 10000, 'score': 168871.77570962906, 'total_duration': 174729.8920071125, 'accumulated_submission_time': 168871.77570962906, 'accumulated_eval_time': 5814.488134860992, 'accumulated_logging_time': 26.447570085525513}
I0302 06:05:34.921566 140525264946944 logging_writer.py:48] [499857] accumulated_eval_time=5814.488135, accumulated_logging_time=26.447570, accumulated_submission_time=168871.775710, global_step=499857, preemption_count=0, score=168871.775710, test/accuracy=0.631100, test/loss=1.821233, test/num_examples=10000, total_duration=174729.892007, train/accuracy=0.959542, train/loss=0.151011, validation/accuracy=0.755040, validation/loss=1.043645, validation/num_examples=50000
I0302 06:05:49.789783 140525273339648 logging_writer.py:48] [499900] global_step=499900, grad_norm=4.671990394592285, loss=0.6467048525810242
I0302 06:06:23.523621 140525264946944 logging_writer.py:48] [500000] global_step=500000, grad_norm=4.374223232269287, loss=0.5968347191810608
I0302 06:06:57.274767 140525273339648 logging_writer.py:48] [500100] global_step=500100, grad_norm=5.03261661529541, loss=0.6987273693084717
I0302 06:07:31.073865 140525264946944 logging_writer.py:48] [500200] global_step=500200, grad_norm=4.76692533493042, loss=0.696719229221344
I0302 06:08:04.989787 140525273339648 logging_writer.py:48] [500300] global_step=500300, grad_norm=4.582046985626221, loss=0.6849127411842346
I0302 06:08:38.792830 140525264946944 logging_writer.py:48] [500400] global_step=500400, grad_norm=4.235959529876709, loss=0.5771955847740173
I0302 06:09:12.587052 140525273339648 logging_writer.py:48] [500500] global_step=500500, grad_norm=4.5662522315979, loss=0.6419081687927246
I0302 06:09:46.359407 140525264946944 logging_writer.py:48] [500600] global_step=500600, grad_norm=4.626654148101807, loss=0.6404836773872375
I0302 06:10:20.188744 140525273339648 logging_writer.py:48] [500700] global_step=500700, grad_norm=4.233565807342529, loss=0.5545659065246582
I0302 06:10:53.996243 140525264946944 logging_writer.py:48] [500800] global_step=500800, grad_norm=4.708789348602295, loss=0.6664995551109314
I0302 06:11:27.823430 140525273339648 logging_writer.py:48] [500900] global_step=500900, grad_norm=4.70709228515625, loss=0.6020208597183228
I0302 06:12:01.611642 140525264946944 logging_writer.py:48] [501000] global_step=501000, grad_norm=4.125674724578857, loss=0.5802122950553894
I0302 06:12:35.415670 140525273339648 logging_writer.py:48] [501100] global_step=501100, grad_norm=4.499857425689697, loss=0.6366762518882751
I0302 06:13:09.194482 140525264946944 logging_writer.py:48] [501200] global_step=501200, grad_norm=4.294217586517334, loss=0.5837710499763489
I0302 06:13:42.990916 140525273339648 logging_writer.py:48] [501300] global_step=501300, grad_norm=4.592291831970215, loss=0.6448155045509338
I0302 06:14:04.852284 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:14:10.825528 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:14:19.499610 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:14:21.766258 140688601454400 submission_runner.py:411] Time since start: 175256.84s, 	Step: 501366, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14730046689510345, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0455933809280396, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.822257161140442, 'test/num_examples': 10000, 'score': 169381.6427283287, 'total_duration': 175256.8369383812, 'accumulated_submission_time': 169381.6427283287, 'accumulated_eval_time': 5831.401889562607, 'accumulated_logging_time': 26.559768199920654}
I0302 06:14:21.866465 140523092305664 logging_writer.py:48] [501366] accumulated_eval_time=5831.401890, accumulated_logging_time=26.559768, accumulated_submission_time=169381.642728, global_step=501366, preemption_count=0, score=169381.642728, test/accuracy=0.631200, test/loss=1.822257, test/num_examples=10000, total_duration=175256.836938, train/accuracy=0.961834, train/loss=0.147300, validation/accuracy=0.755060, validation/loss=1.045593, validation/num_examples=50000
I0302 06:14:33.691786 140523947947776 logging_writer.py:48] [501400] global_step=501400, grad_norm=4.467663288116455, loss=0.5641836524009705
I0302 06:15:07.390472 140523092305664 logging_writer.py:48] [501500] global_step=501500, grad_norm=4.718743801116943, loss=0.6261093616485596
I0302 06:15:41.145934 140523947947776 logging_writer.py:48] [501600] global_step=501600, grad_norm=4.69843864440918, loss=0.6978194117546082
I0302 06:16:14.931770 140523092305664 logging_writer.py:48] [501700] global_step=501700, grad_norm=4.4375691413879395, loss=0.6128499507904053
I0302 06:16:48.713726 140523947947776 logging_writer.py:48] [501800] global_step=501800, grad_norm=4.413365840911865, loss=0.6454469561576843
I0302 06:17:22.508241 140523092305664 logging_writer.py:48] [501900] global_step=501900, grad_norm=4.6518025398254395, loss=0.6420568823814392
I0302 06:17:56.300039 140523947947776 logging_writer.py:48] [502000] global_step=502000, grad_norm=3.994577407836914, loss=0.5565711259841919
I0302 06:18:30.105754 140523092305664 logging_writer.py:48] [502100] global_step=502100, grad_norm=4.258611679077148, loss=0.5889654159545898
I0302 06:19:03.852989 140523947947776 logging_writer.py:48] [502200] global_step=502200, grad_norm=4.597248077392578, loss=0.6127358675003052
I0302 06:19:37.590648 140523092305664 logging_writer.py:48] [502300] global_step=502300, grad_norm=4.616626262664795, loss=0.6557889580726624
I0302 06:20:11.419574 140523947947776 logging_writer.py:48] [502400] global_step=502400, grad_norm=4.4338884353637695, loss=0.6117433905601501
I0302 06:20:45.227074 140523092305664 logging_writer.py:48] [502500] global_step=502500, grad_norm=4.50780725479126, loss=0.629741907119751
I0302 06:21:18.992387 140523947947776 logging_writer.py:48] [502600] global_step=502600, grad_norm=4.324806213378906, loss=0.601057231426239
I0302 06:21:52.794756 140523092305664 logging_writer.py:48] [502700] global_step=502700, grad_norm=4.800576686859131, loss=0.6774370074272156
I0302 06:22:26.559421 140523947947776 logging_writer.py:48] [502800] global_step=502800, grad_norm=4.5116353034973145, loss=0.6084251403808594
I0302 06:22:52.062009 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:22:58.148693 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:23:06.840922 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:23:09.131113 140688601454400 submission_runner.py:411] Time since start: 175784.20s, 	Step: 502877, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14883457124233246, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.043159008026123, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8203349113464355, 'test/num_examples': 10000, 'score': 169891.77300667763, 'total_duration': 175784.2019557953, 'accumulated_submission_time': 169891.77300667763, 'accumulated_eval_time': 5848.470964431763, 'accumulated_logging_time': 26.671674251556396}
I0302 06:23:09.238410 140525256554240 logging_writer.py:48] [502877] accumulated_eval_time=5848.470964, accumulated_logging_time=26.671674, accumulated_submission_time=169891.773007, global_step=502877, preemption_count=0, score=169891.773007, test/accuracy=0.631700, test/loss=1.820335, test/num_examples=10000, total_duration=175784.201956, train/accuracy=0.960041, train/loss=0.148835, validation/accuracy=0.755480, validation/loss=1.043159, validation/num_examples=50000
I0302 06:23:17.312302 140525264946944 logging_writer.py:48] [502900] global_step=502900, grad_norm=4.734387397766113, loss=0.6711576581001282
I0302 06:23:51.060874 140525256554240 logging_writer.py:48] [503000] global_step=503000, grad_norm=4.956571102142334, loss=0.6973684430122375
I0302 06:24:24.781006 140525264946944 logging_writer.py:48] [503100] global_step=503100, grad_norm=4.647291660308838, loss=0.64378821849823
I0302 06:24:58.546428 140525256554240 logging_writer.py:48] [503200] global_step=503200, grad_norm=4.2941083908081055, loss=0.5970837473869324
I0302 06:25:32.287942 140525264946944 logging_writer.py:48] [503300] global_step=503300, grad_norm=4.643164157867432, loss=0.7046600580215454
I0302 06:26:06.099173 140525256554240 logging_writer.py:48] [503400] global_step=503400, grad_norm=4.194462776184082, loss=0.6043856739997864
I0302 06:26:39.925875 140525264946944 logging_writer.py:48] [503500] global_step=503500, grad_norm=4.578795433044434, loss=0.6506975889205933
I0302 06:27:13.729722 140525256554240 logging_writer.py:48] [503600] global_step=503600, grad_norm=4.289294242858887, loss=0.635504961013794
I0302 06:27:47.526849 140525264946944 logging_writer.py:48] [503700] global_step=503700, grad_norm=4.807318687438965, loss=0.58164381980896
I0302 06:28:21.289032 140525256554240 logging_writer.py:48] [503800] global_step=503800, grad_norm=4.303133964538574, loss=0.5857101082801819
I0302 06:28:55.087311 140525264946944 logging_writer.py:48] [503900] global_step=503900, grad_norm=4.400407791137695, loss=0.5778674483299255
I0302 06:29:28.884518 140525256554240 logging_writer.py:48] [504000] global_step=504000, grad_norm=4.202491283416748, loss=0.573465883731842
I0302 06:30:02.657099 140525264946944 logging_writer.py:48] [504100] global_step=504100, grad_norm=5.016890048980713, loss=0.6690322160720825
I0302 06:30:36.447654 140525256554240 logging_writer.py:48] [504200] global_step=504200, grad_norm=4.264876365661621, loss=0.5867434144020081
I0302 06:31:10.234862 140525264946944 logging_writer.py:48] [504300] global_step=504300, grad_norm=4.860246658325195, loss=0.6666148900985718
I0302 06:31:39.414816 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:31:45.402060 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:31:53.985687 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:31:56.279315 140688601454400 submission_runner.py:411] Time since start: 176311.35s, 	Step: 504388, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14607682824134827, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0443062782287598, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8220136165618896, 'test/num_examples': 10000, 'score': 170401.88774085045, 'total_duration': 176311.35015702248, 'accumulated_submission_time': 170401.88774085045, 'accumulated_eval_time': 5865.335414886475, 'accumulated_logging_time': 26.788559198379517}
I0302 06:31:56.380321 140523092305664 logging_writer.py:48] [504388] accumulated_eval_time=5865.335415, accumulated_logging_time=26.788559, accumulated_submission_time=170401.887741, global_step=504388, preemption_count=0, score=170401.887741, test/accuracy=0.630400, test/loss=1.822014, test/num_examples=10000, total_duration=176311.350157, train/accuracy=0.961137, train/loss=0.146077, validation/accuracy=0.755020, validation/loss=1.044306, validation/num_examples=50000
I0302 06:32:00.770480 140523947947776 logging_writer.py:48] [504400] global_step=504400, grad_norm=4.29470157623291, loss=0.5673384070396423
I0302 06:32:34.576143 140523092305664 logging_writer.py:48] [504500] global_step=504500, grad_norm=4.501508712768555, loss=0.6141459941864014
I0302 06:33:08.352017 140523947947776 logging_writer.py:48] [504600] global_step=504600, grad_norm=4.373651504516602, loss=0.6676726341247559
I0302 06:33:42.125349 140523092305664 logging_writer.py:48] [504700] global_step=504700, grad_norm=4.484886646270752, loss=0.5684149861335754
I0302 06:34:15.920703 140523947947776 logging_writer.py:48] [504800] global_step=504800, grad_norm=4.761640548706055, loss=0.6341103315353394
I0302 06:34:49.713438 140523092305664 logging_writer.py:48] [504900] global_step=504900, grad_norm=4.5010986328125, loss=0.6256623268127441
I0302 06:35:23.490437 140523947947776 logging_writer.py:48] [505000] global_step=505000, grad_norm=5.232333660125732, loss=0.6378706097602844
I0302 06:35:57.274771 140523092305664 logging_writer.py:48] [505100] global_step=505100, grad_norm=4.228662967681885, loss=0.5352941751480103
I0302 06:36:31.088608 140523947947776 logging_writer.py:48] [505200] global_step=505200, grad_norm=4.465554714202881, loss=0.6742334961891174
I0302 06:37:04.866380 140523092305664 logging_writer.py:48] [505300] global_step=505300, grad_norm=4.50419282913208, loss=0.6718764901161194
I0302 06:37:38.663590 140523947947776 logging_writer.py:48] [505400] global_step=505400, grad_norm=4.5914435386657715, loss=0.6300269961357117
I0302 06:38:12.452393 140523092305664 logging_writer.py:48] [505500] global_step=505500, grad_norm=4.343321323394775, loss=0.5938416719436646
I0302 06:38:46.305794 140523947947776 logging_writer.py:48] [505600] global_step=505600, grad_norm=4.471441745758057, loss=0.6349081993103027
I0302 06:39:20.106864 140523092305664 logging_writer.py:48] [505700] global_step=505700, grad_norm=5.106565952301025, loss=0.6426920294761658
I0302 06:39:53.899415 140523947947776 logging_writer.py:48] [505800] global_step=505800, grad_norm=4.3811492919921875, loss=0.6437344551086426
I0302 06:40:26.461743 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:40:32.457156 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:40:40.951365 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:40:43.191035 140688601454400 submission_runner.py:411] Time since start: 176838.26s, 	Step: 505898, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.146673321723938, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0442750453948975, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8225374221801758, 'test/num_examples': 10000, 'score': 170911.90631198883, 'total_duration': 176838.2618751526, 'accumulated_submission_time': 170911.90631198883, 'accumulated_eval_time': 5882.064656734467, 'accumulated_logging_time': 26.900224924087524}
I0302 06:40:43.291773 140525256554240 logging_writer.py:48] [505898] accumulated_eval_time=5882.064657, accumulated_logging_time=26.900225, accumulated_submission_time=170911.906312, global_step=505898, preemption_count=0, score=170911.906312, test/accuracy=0.631300, test/loss=1.822537, test/num_examples=10000, total_duration=176838.261875, train/accuracy=0.960360, train/loss=0.146673, validation/accuracy=0.755100, validation/loss=1.044275, validation/num_examples=50000
I0302 06:40:44.315422 140525264946944 logging_writer.py:48] [505900] global_step=505900, grad_norm=4.698649883270264, loss=0.681749701499939
I0302 06:41:18.039360 140525256554240 logging_writer.py:48] [506000] global_step=506000, grad_norm=4.402435302734375, loss=0.583608865737915
I0302 06:41:51.739098 140525264946944 logging_writer.py:48] [506100] global_step=506100, grad_norm=4.276064395904541, loss=0.5612321496009827
I0302 06:42:25.497901 140525256554240 logging_writer.py:48] [506200] global_step=506200, grad_norm=4.05601692199707, loss=0.5852176547050476
I0302 06:42:59.322880 140525264946944 logging_writer.py:48] [506300] global_step=506300, grad_norm=4.714869976043701, loss=0.6445633769035339
I0302 06:43:33.088272 140525256554240 logging_writer.py:48] [506400] global_step=506400, grad_norm=4.456500053405762, loss=0.6397092342376709
I0302 06:44:06.874361 140525264946944 logging_writer.py:48] [506500] global_step=506500, grad_norm=4.651458263397217, loss=0.6064069867134094
I0302 06:44:40.639225 140525256554240 logging_writer.py:48] [506600] global_step=506600, grad_norm=4.661269187927246, loss=0.594192385673523
I0302 06:45:14.522694 140525264946944 logging_writer.py:48] [506700] global_step=506700, grad_norm=5.412540435791016, loss=0.6659538149833679
I0302 06:45:48.308033 140525256554240 logging_writer.py:48] [506800] global_step=506800, grad_norm=4.950223922729492, loss=0.6196070909500122
I0302 06:46:22.130002 140525264946944 logging_writer.py:48] [506900] global_step=506900, grad_norm=4.3418288230896, loss=0.6233757734298706
I0302 06:46:55.897832 140525256554240 logging_writer.py:48] [507000] global_step=507000, grad_norm=4.959038734436035, loss=0.6904979944229126
I0302 06:47:29.633973 140525264946944 logging_writer.py:48] [507100] global_step=507100, grad_norm=4.869771957397461, loss=0.6552200317382812
I0302 06:48:03.391458 140525256554240 logging_writer.py:48] [507200] global_step=507200, grad_norm=5.136381149291992, loss=0.6671651601791382
I0302 06:48:37.183889 140525264946944 logging_writer.py:48] [507300] global_step=507300, grad_norm=4.744243144989014, loss=0.6635984182357788
I0302 06:49:10.996440 140525256554240 logging_writer.py:48] [507400] global_step=507400, grad_norm=4.4706878662109375, loss=0.6462629437446594
I0302 06:49:13.511764 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:49:19.567178 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:49:28.347847 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:49:30.667100 140688601454400 submission_runner.py:411] Time since start: 177365.74s, 	Step: 507409, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14857043325901031, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.044175386428833, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8244786262512207, 'test/num_examples': 10000, 'score': 171422.06424570084, 'total_duration': 177365.73793911934, 'accumulated_submission_time': 171422.06424570084, 'accumulated_eval_time': 5899.219933986664, 'accumulated_logging_time': 27.011072874069214}
I0302 06:49:30.768723 140525273339648 logging_writer.py:48] [507409] accumulated_eval_time=5899.219934, accumulated_logging_time=27.011073, accumulated_submission_time=171422.064246, global_step=507409, preemption_count=0, score=171422.064246, test/accuracy=0.631300, test/loss=1.824479, test/num_examples=10000, total_duration=177365.737939, train/accuracy=0.960041, train/loss=0.148570, validation/accuracy=0.755220, validation/loss=1.044175, validation/num_examples=50000
I0302 06:50:01.820111 140525281732352 logging_writer.py:48] [507500] global_step=507500, grad_norm=4.402279853820801, loss=0.555243194103241
I0302 06:50:35.603940 140525273339648 logging_writer.py:48] [507600] global_step=507600, grad_norm=4.729641437530518, loss=0.7051113843917847
I0302 06:51:09.443537 140525281732352 logging_writer.py:48] [507700] global_step=507700, grad_norm=4.858851909637451, loss=0.7261088490486145
I0302 06:51:43.229060 140525273339648 logging_writer.py:48] [507800] global_step=507800, grad_norm=4.274753093719482, loss=0.5896846055984497
I0302 06:52:17.029405 140525281732352 logging_writer.py:48] [507900] global_step=507900, grad_norm=4.478568077087402, loss=0.6514450311660767
I0302 06:52:50.830430 140525273339648 logging_writer.py:48] [508000] global_step=508000, grad_norm=4.626708030700684, loss=0.6220362186431885
I0302 06:53:24.603468 140525281732352 logging_writer.py:48] [508100] global_step=508100, grad_norm=4.237359046936035, loss=0.6207586526870728
I0302 06:53:58.400883 140525273339648 logging_writer.py:48] [508200] global_step=508200, grad_norm=5.114043235778809, loss=0.7176056504249573
I0302 06:54:32.196242 140525281732352 logging_writer.py:48] [508300] global_step=508300, grad_norm=4.229860782623291, loss=0.5817183256149292
I0302 06:55:05.969829 140525273339648 logging_writer.py:48] [508400] global_step=508400, grad_norm=4.46086311340332, loss=0.6071635484695435
I0302 06:55:39.743387 140525281732352 logging_writer.py:48] [508500] global_step=508500, grad_norm=4.631606578826904, loss=0.6119116544723511
I0302 06:56:13.512291 140525273339648 logging_writer.py:48] [508600] global_step=508600, grad_norm=4.860735893249512, loss=0.6381725072860718
I0302 06:56:47.310031 140525281732352 logging_writer.py:48] [508700] global_step=508700, grad_norm=4.257485389709473, loss=0.6405652165412903
I0302 06:57:21.229999 140525273339648 logging_writer.py:48] [508800] global_step=508800, grad_norm=4.518436908721924, loss=0.5637043714523315
I0302 06:57:55.018762 140525281732352 logging_writer.py:48] [508900] global_step=508900, grad_norm=4.733188152313232, loss=0.6605339646339417
I0302 06:58:00.904714 140688601454400 spec.py:321] Evaluating on the training split.
I0302 06:58:07.011720 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 06:58:15.558637 140688601454400 spec.py:349] Evaluating on the test split.
I0302 06:58:17.923393 140688601454400 submission_runner.py:411] Time since start: 177892.99s, 	Step: 508919, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.151060089468956, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0434767007827759, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8214324712753296, 'test/num_examples': 10000, 'score': 171932.13884925842, 'total_duration': 177892.9942240715, 'accumulated_submission_time': 171932.13884925842, 'accumulated_eval_time': 5916.238545417786, 'accumulated_logging_time': 27.122536659240723}
I0302 06:58:18.026013 140523092305664 logging_writer.py:48] [508919] accumulated_eval_time=5916.238545, accumulated_logging_time=27.122537, accumulated_submission_time=171932.138849, global_step=508919, preemption_count=0, score=171932.138849, test/accuracy=0.630100, test/loss=1.821432, test/num_examples=10000, total_duration=177892.994224, train/accuracy=0.960300, train/loss=0.151060, validation/accuracy=0.755900, validation/loss=1.043477, validation/num_examples=50000
I0302 06:58:45.678218 140523947947776 logging_writer.py:48] [509000] global_step=509000, grad_norm=4.598049163818359, loss=0.6001785397529602
I0302 06:59:19.431528 140523092305664 logging_writer.py:48] [509100] global_step=509100, grad_norm=4.220452785491943, loss=0.6660491824150085
I0302 06:59:53.114659 140523947947776 logging_writer.py:48] [509200] global_step=509200, grad_norm=4.710639476776123, loss=0.6473458409309387
I0302 07:00:26.890047 140523092305664 logging_writer.py:48] [509300] global_step=509300, grad_norm=4.509087562561035, loss=0.630859375
I0302 07:01:00.717941 140523947947776 logging_writer.py:48] [509400] global_step=509400, grad_norm=5.58597993850708, loss=0.6548871994018555
I0302 07:01:34.491091 140523092305664 logging_writer.py:48] [509500] global_step=509500, grad_norm=4.641759395599365, loss=0.6123666167259216
I0302 07:02:08.262751 140523947947776 logging_writer.py:48] [509600] global_step=509600, grad_norm=4.596589088439941, loss=0.6431871652603149
I0302 07:02:42.050528 140523092305664 logging_writer.py:48] [509700] global_step=509700, grad_norm=3.950695753097534, loss=0.5058272480964661
I0302 07:03:15.916283 140523947947776 logging_writer.py:48] [509800] global_step=509800, grad_norm=4.513734340667725, loss=0.5979174971580505
I0302 07:03:49.703944 140523092305664 logging_writer.py:48] [509900] global_step=509900, grad_norm=4.40078592300415, loss=0.6421049237251282
I0302 07:04:23.492733 140523947947776 logging_writer.py:48] [510000] global_step=510000, grad_norm=4.359931468963623, loss=0.5525434613227844
I0302 07:04:57.269489 140523092305664 logging_writer.py:48] [510100] global_step=510100, grad_norm=4.204870223999023, loss=0.5407003164291382
I0302 07:05:31.064079 140523947947776 logging_writer.py:48] [510200] global_step=510200, grad_norm=4.261712074279785, loss=0.6106247305870056
I0302 07:06:04.840707 140523092305664 logging_writer.py:48] [510300] global_step=510300, grad_norm=4.936100482940674, loss=0.6462875008583069
I0302 07:06:38.626992 140523947947776 logging_writer.py:48] [510400] global_step=510400, grad_norm=4.096136569976807, loss=0.5926786661148071
I0302 07:06:48.223069 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:06:54.343895 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:07:02.930372 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:07:05.207963 140688601454400 submission_runner.py:411] Time since start: 178420.28s, 	Step: 510430, 	{'train/accuracy': 0.9616748690605164, 'train/loss': 0.14183931052684784, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0449419021606445, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8233567476272583, 'test/num_examples': 10000, 'score': 172442.27340078354, 'total_duration': 178420.2787978649, 'accumulated_submission_time': 172442.27340078354, 'accumulated_eval_time': 5933.223411083221, 'accumulated_logging_time': 27.235366582870483}
I0302 07:07:05.312275 140522554390272 logging_writer.py:48] [510430] accumulated_eval_time=5933.223411, accumulated_logging_time=27.235367, accumulated_submission_time=172442.273401, global_step=510430, preemption_count=0, score=172442.273401, test/accuracy=0.631200, test/loss=1.823357, test/num_examples=10000, total_duration=178420.278798, train/accuracy=0.961675, train/loss=0.141839, validation/accuracy=0.755220, validation/loss=1.044942, validation/num_examples=50000
I0302 07:07:29.245287 140525256554240 logging_writer.py:48] [510500] global_step=510500, grad_norm=4.226990222930908, loss=0.536758303642273
I0302 07:08:02.982894 140522554390272 logging_writer.py:48] [510600] global_step=510600, grad_norm=4.721287727355957, loss=0.6743001937866211
I0302 07:08:36.784332 140525256554240 logging_writer.py:48] [510700] global_step=510700, grad_norm=4.334752082824707, loss=0.6085910797119141
I0302 07:09:10.569952 140522554390272 logging_writer.py:48] [510800] global_step=510800, grad_norm=4.399517059326172, loss=0.5925395488739014
I0302 07:09:44.389264 140525256554240 logging_writer.py:48] [510900] global_step=510900, grad_norm=4.320679664611816, loss=0.6376371383666992
I0302 07:10:18.159233 140522554390272 logging_writer.py:48] [511000] global_step=511000, grad_norm=4.579283714294434, loss=0.5934069156646729
I0302 07:10:51.979938 140525256554240 logging_writer.py:48] [511100] global_step=511100, grad_norm=4.802903652191162, loss=0.6279796361923218
I0302 07:11:25.761009 140522554390272 logging_writer.py:48] [511200] global_step=511200, grad_norm=4.945893287658691, loss=0.6876407861709595
I0302 07:11:59.595443 140525256554240 logging_writer.py:48] [511300] global_step=511300, grad_norm=5.248185157775879, loss=0.618858277797699
I0302 07:12:33.393234 140522554390272 logging_writer.py:48] [511400] global_step=511400, grad_norm=4.7249345779418945, loss=0.6376090049743652
I0302 07:13:07.128651 140525256554240 logging_writer.py:48] [511500] global_step=511500, grad_norm=4.550602436065674, loss=0.6446638107299805
I0302 07:13:40.915172 140522554390272 logging_writer.py:48] [511600] global_step=511600, grad_norm=4.464306354522705, loss=0.5536026954650879
I0302 07:14:14.694796 140525256554240 logging_writer.py:48] [511700] global_step=511700, grad_norm=4.293334484100342, loss=0.6031166315078735
I0302 07:14:48.458706 140522554390272 logging_writer.py:48] [511800] global_step=511800, grad_norm=4.4670939445495605, loss=0.6658571362495422
I0302 07:15:22.251160 140525256554240 logging_writer.py:48] [511900] global_step=511900, grad_norm=4.0860466957092285, loss=0.5904710292816162
I0302 07:15:35.292918 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:15:41.268778 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:15:49.916940 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:15:52.223691 140688601454400 submission_runner.py:411] Time since start: 178947.29s, 	Step: 511940, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.1435249298810959, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0442218780517578, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8227651119232178, 'test/num_examples': 10000, 'score': 172952.19137954712, 'total_duration': 178947.29454159737, 'accumulated_submission_time': 172952.19137954712, 'accumulated_eval_time': 5950.1541385650635, 'accumulated_logging_time': 27.349958181381226}
I0302 07:15:52.323354 140522396051200 logging_writer.py:48] [511940] accumulated_eval_time=5950.154139, accumulated_logging_time=27.349958, accumulated_submission_time=172952.191380, global_step=511940, preemption_count=0, score=172952.191380, test/accuracy=0.630600, test/loss=1.822765, test/num_examples=10000, total_duration=178947.294542, train/accuracy=0.961436, train/loss=0.143525, validation/accuracy=0.755720, validation/loss=1.044222, validation/num_examples=50000
I0302 07:16:12.915912 140522404443904 logging_writer.py:48] [512000] global_step=512000, grad_norm=4.885035514831543, loss=0.6258647441864014
I0302 07:16:46.669309 140522396051200 logging_writer.py:48] [512100] global_step=512100, grad_norm=4.831577301025391, loss=0.6427217721939087
I0302 07:17:20.439844 140522404443904 logging_writer.py:48] [512200] global_step=512200, grad_norm=4.287208557128906, loss=0.5617198944091797
I0302 07:17:54.231385 140522396051200 logging_writer.py:48] [512300] global_step=512300, grad_norm=4.821671009063721, loss=0.658334493637085
I0302 07:18:28.004458 140522404443904 logging_writer.py:48] [512400] global_step=512400, grad_norm=4.635417938232422, loss=0.5938566327095032
I0302 07:19:01.805019 140522396051200 logging_writer.py:48] [512500] global_step=512500, grad_norm=4.292996883392334, loss=0.6750930547714233
I0302 07:19:35.585581 140522404443904 logging_writer.py:48] [512600] global_step=512600, grad_norm=4.629878997802734, loss=0.6558324098587036
I0302 07:20:09.368629 140522396051200 logging_writer.py:48] [512700] global_step=512700, grad_norm=4.961223602294922, loss=0.5969423055648804
I0302 07:20:43.170988 140522404443904 logging_writer.py:48] [512800] global_step=512800, grad_norm=4.2771501541137695, loss=0.6861360669136047
I0302 07:21:16.969359 140522396051200 logging_writer.py:48] [512900] global_step=512900, grad_norm=4.5856146812438965, loss=0.6382832527160645
I0302 07:21:50.862106 140522404443904 logging_writer.py:48] [513000] global_step=513000, grad_norm=4.9345927238464355, loss=0.6426863670349121
I0302 07:22:24.644928 140522396051200 logging_writer.py:48] [513100] global_step=513100, grad_norm=4.883779048919678, loss=0.6289939880371094
I0302 07:22:58.419256 140522404443904 logging_writer.py:48] [513200] global_step=513200, grad_norm=4.617938995361328, loss=0.5936136245727539
I0302 07:23:32.165456 140522396051200 logging_writer.py:48] [513300] global_step=513300, grad_norm=4.859621047973633, loss=0.7079121470451355
I0302 07:24:05.918019 140522404443904 logging_writer.py:48] [513400] global_step=513400, grad_norm=4.6954193115234375, loss=0.6442521214485168
I0302 07:24:22.297693 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:24:28.283405 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:24:36.902013 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:24:39.228033 140688601454400 submission_runner.py:411] Time since start: 179474.30s, 	Step: 513450, 	{'train/accuracy': 0.9593430757522583, 'train/loss': 0.14875221252441406, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.043470025062561, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8211781978607178, 'test/num_examples': 10000, 'score': 173462.10426592827, 'total_duration': 179474.29887628555, 'accumulated_submission_time': 173462.10426592827, 'accumulated_eval_time': 5967.08442735672, 'accumulated_logging_time': 27.459717750549316}
I0302 07:24:39.327452 140522554390272 logging_writer.py:48] [513450] accumulated_eval_time=5967.084427, accumulated_logging_time=27.459718, accumulated_submission_time=173462.104266, global_step=513450, preemption_count=0, score=173462.104266, test/accuracy=0.630400, test/loss=1.821178, test/num_examples=10000, total_duration=179474.298876, train/accuracy=0.959343, train/loss=0.148752, validation/accuracy=0.755140, validation/loss=1.043470, validation/num_examples=50000
I0302 07:24:56.526250 140525264946944 logging_writer.py:48] [513500] global_step=513500, grad_norm=4.390345573425293, loss=0.5776290893554688
I0302 07:25:30.297285 140522554390272 logging_writer.py:48] [513600] global_step=513600, grad_norm=4.737636566162109, loss=0.6984912157058716
I0302 07:26:04.042703 140525264946944 logging_writer.py:48] [513700] global_step=513700, grad_norm=4.3165693283081055, loss=0.5810688734054565
I0302 07:26:37.777609 140522554390272 logging_writer.py:48] [513800] global_step=513800, grad_norm=4.539535045623779, loss=0.5933687686920166
I0302 07:27:11.583153 140525264946944 logging_writer.py:48] [513900] global_step=513900, grad_norm=4.579109191894531, loss=0.6255788803100586
I0302 07:27:45.366223 140522554390272 logging_writer.py:48] [514000] global_step=514000, grad_norm=4.360972881317139, loss=0.5585790276527405
I0302 07:28:19.221033 140525264946944 logging_writer.py:48] [514100] global_step=514100, grad_norm=4.96269416809082, loss=0.6538907289505005
I0302 07:28:52.968371 140522554390272 logging_writer.py:48] [514200] global_step=514200, grad_norm=4.769035816192627, loss=0.6366117000579834
I0302 07:29:26.761475 140525264946944 logging_writer.py:48] [514300] global_step=514300, grad_norm=4.276242733001709, loss=0.5866199731826782
I0302 07:30:00.524581 140522554390272 logging_writer.py:48] [514400] global_step=514400, grad_norm=4.979995250701904, loss=0.6551971435546875
I0302 07:30:34.311057 140525264946944 logging_writer.py:48] [514500] global_step=514500, grad_norm=4.4877519607543945, loss=0.6170401573181152
I0302 07:31:08.119228 140522554390272 logging_writer.py:48] [514600] global_step=514600, grad_norm=4.6973958015441895, loss=0.6207566261291504
I0302 07:31:41.901623 140525264946944 logging_writer.py:48] [514700] global_step=514700, grad_norm=4.743564128875732, loss=0.6526132822036743
I0302 07:32:15.714697 140522554390272 logging_writer.py:48] [514800] global_step=514800, grad_norm=4.9030632972717285, loss=0.7329314947128296
I0302 07:32:49.505267 140525264946944 logging_writer.py:48] [514900] global_step=514900, grad_norm=5.024501800537109, loss=0.6658554077148438
I0302 07:33:09.252192 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:33:15.222894 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:33:23.828754 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:33:26.073818 140688601454400 submission_runner.py:411] Time since start: 180001.14s, 	Step: 514960, 	{'train/accuracy': 0.9610769748687744, 'train/loss': 0.15016885101795197, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0442010164260864, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.82271409034729, 'test/num_examples': 10000, 'score': 173971.96740150452, 'total_duration': 180001.14465355873, 'accumulated_submission_time': 173971.96740150452, 'accumulated_eval_time': 5983.905996799469, 'accumulated_logging_time': 27.568768978118896}
I0302 07:33:26.178341 140523092305664 logging_writer.py:48] [514960] accumulated_eval_time=5983.905997, accumulated_logging_time=27.568769, accumulated_submission_time=173971.967402, global_step=514960, preemption_count=0, score=173971.967402, test/accuracy=0.631500, test/loss=1.822714, test/num_examples=10000, total_duration=180001.144654, train/accuracy=0.961077, train/loss=0.150169, validation/accuracy=0.755000, validation/loss=1.044201, validation/num_examples=50000
I0302 07:33:40.041539 140523947947776 logging_writer.py:48] [515000] global_step=515000, grad_norm=4.8150129318237305, loss=0.5911657810211182
I0302 07:34:13.814180 140523092305664 logging_writer.py:48] [515100] global_step=515100, grad_norm=4.588932037353516, loss=0.6420395374298096
I0302 07:34:47.562098 140523947947776 logging_writer.py:48] [515200] global_step=515200, grad_norm=4.201463222503662, loss=0.5669682025909424
I0302 07:35:21.341674 140523092305664 logging_writer.py:48] [515300] global_step=515300, grad_norm=4.158421039581299, loss=0.5646501779556274
I0302 07:35:55.123709 140523947947776 logging_writer.py:48] [515400] global_step=515400, grad_norm=4.556538105010986, loss=0.6268115043640137
I0302 07:36:28.879678 140523092305664 logging_writer.py:48] [515500] global_step=515500, grad_norm=5.244724750518799, loss=0.7803524732589722
I0302 07:37:02.667651 140523947947776 logging_writer.py:48] [515600] global_step=515600, grad_norm=4.521499156951904, loss=0.651491105556488
I0302 07:37:36.434439 140523092305664 logging_writer.py:48] [515700] global_step=515700, grad_norm=4.798403739929199, loss=0.6053881645202637
I0302 07:38:10.230797 140523947947776 logging_writer.py:48] [515800] global_step=515800, grad_norm=4.922502517700195, loss=0.660033106803894
I0302 07:38:44.029785 140523092305664 logging_writer.py:48] [515900] global_step=515900, grad_norm=4.445735931396484, loss=0.6544584631919861
I0302 07:39:17.789096 140523947947776 logging_writer.py:48] [516000] global_step=516000, grad_norm=4.297800064086914, loss=0.6595467329025269
I0302 07:39:51.587558 140523092305664 logging_writer.py:48] [516100] global_step=516100, grad_norm=4.48944616317749, loss=0.6399785280227661
I0302 07:40:25.407264 140523947947776 logging_writer.py:48] [516200] global_step=516200, grad_norm=4.850985050201416, loss=0.6445755958557129
I0302 07:40:59.176459 140523092305664 logging_writer.py:48] [516300] global_step=516300, grad_norm=4.117291450500488, loss=0.6176123023033142
I0302 07:41:32.934309 140523947947776 logging_writer.py:48] [516400] global_step=516400, grad_norm=4.666144371032715, loss=0.6235066056251526
I0302 07:41:56.371755 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:42:02.489351 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:42:11.079520 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:42:13.385549 140688601454400 submission_runner.py:411] Time since start: 180528.46s, 	Step: 516471, 	{'train/accuracy': 0.9609972834587097, 'train/loss': 0.14672991633415222, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0436804294586182, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8224447965621948, 'test/num_examples': 10000, 'score': 174482.09878492355, 'total_duration': 180528.45638537407, 'accumulated_submission_time': 174482.09878492355, 'accumulated_eval_time': 6000.9197318553925, 'accumulated_logging_time': 27.683451890945435}
I0302 07:42:13.493046 140522396051200 logging_writer.py:48] [516471] accumulated_eval_time=6000.919732, accumulated_logging_time=27.683452, accumulated_submission_time=174482.098785, global_step=516471, preemption_count=0, score=174482.098785, test/accuracy=0.630000, test/loss=1.822445, test/num_examples=10000, total_duration=180528.456385, train/accuracy=0.960997, train/loss=0.146730, validation/accuracy=0.755220, validation/loss=1.043680, validation/num_examples=50000
I0302 07:42:23.600145 140522404443904 logging_writer.py:48] [516500] global_step=516500, grad_norm=4.500883102416992, loss=0.571205198764801
I0302 07:42:57.358176 140522396051200 logging_writer.py:48] [516600] global_step=516600, grad_norm=4.783952713012695, loss=0.6817067861557007
I0302 07:43:31.141205 140522404443904 logging_writer.py:48] [516700] global_step=516700, grad_norm=4.573394775390625, loss=0.6067570447921753
I0302 07:44:04.909446 140522396051200 logging_writer.py:48] [516800] global_step=516800, grad_norm=4.49022912979126, loss=0.6364657878875732
I0302 07:44:38.667841 140522404443904 logging_writer.py:48] [516900] global_step=516900, grad_norm=5.068999290466309, loss=0.6540116667747498
I0302 07:45:12.443662 140522396051200 logging_writer.py:48] [517000] global_step=517000, grad_norm=4.638818264007568, loss=0.5654194355010986
I0302 07:45:46.222903 140522404443904 logging_writer.py:48] [517100] global_step=517100, grad_norm=5.334813594818115, loss=0.6412531137466431
I0302 07:46:20.026384 140522396051200 logging_writer.py:48] [517200] global_step=517200, grad_norm=4.75201940536499, loss=0.6600072383880615
I0302 07:46:53.948381 140522404443904 logging_writer.py:48] [517300] global_step=517300, grad_norm=4.5450358390808105, loss=0.6105366349220276
I0302 07:47:27.724797 140522396051200 logging_writer.py:48] [517400] global_step=517400, grad_norm=4.417723178863525, loss=0.6157514452934265
I0302 07:48:01.530608 140522404443904 logging_writer.py:48] [517500] global_step=517500, grad_norm=4.290398120880127, loss=0.6083495616912842
I0302 07:48:35.298690 140522396051200 logging_writer.py:48] [517600] global_step=517600, grad_norm=4.340598106384277, loss=0.6269007325172424
I0302 07:49:09.101750 140522404443904 logging_writer.py:48] [517700] global_step=517700, grad_norm=4.571042537689209, loss=0.6812606453895569
I0302 07:49:42.880434 140522396051200 logging_writer.py:48] [517800] global_step=517800, grad_norm=4.3775458335876465, loss=0.6035689115524292
I0302 07:50:16.654332 140522404443904 logging_writer.py:48] [517900] global_step=517900, grad_norm=4.374123573303223, loss=0.5814539194107056
I0302 07:50:43.504691 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:50:49.563443 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:50:58.151388 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:51:01.003603 140688601454400 submission_runner.py:411] Time since start: 181056.07s, 	Step: 517981, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14737054705619812, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.043861746788025, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8219637870788574, 'test/num_examples': 10000, 'score': 174992.04834723473, 'total_duration': 181056.0744562149, 'accumulated_submission_time': 174992.04834723473, 'accumulated_eval_time': 6018.418604850769, 'accumulated_logging_time': 27.80076766014099}
I0302 07:51:01.086084 140523947947776 logging_writer.py:48] [517981] accumulated_eval_time=6018.418605, accumulated_logging_time=27.800768, accumulated_submission_time=174992.048347, global_step=517981, preemption_count=0, score=174992.048347, test/accuracy=0.631400, test/loss=1.821964, test/num_examples=10000, total_duration=181056.074456, train/accuracy=0.960559, train/loss=0.147371, validation/accuracy=0.755140, validation/loss=1.043862, validation/num_examples=50000
I0302 07:51:07.822329 140525273339648 logging_writer.py:48] [518000] global_step=518000, grad_norm=5.418980598449707, loss=0.6966559290885925
I0302 07:51:41.549824 140523947947776 logging_writer.py:48] [518100] global_step=518100, grad_norm=4.248876571655273, loss=0.6378216743469238
I0302 07:52:15.309292 140525273339648 logging_writer.py:48] [518200] global_step=518200, grad_norm=4.647695064544678, loss=0.6543140411376953
I0302 07:52:49.258979 140523947947776 logging_writer.py:48] [518300] global_step=518300, grad_norm=4.714910984039307, loss=0.6250059604644775
I0302 07:53:23.008525 140525273339648 logging_writer.py:48] [518400] global_step=518400, grad_norm=4.624564170837402, loss=0.6184442043304443
I0302 07:53:56.821393 140523947947776 logging_writer.py:48] [518500] global_step=518500, grad_norm=4.479509353637695, loss=0.6445189714431763
I0302 07:54:30.600538 140525273339648 logging_writer.py:48] [518600] global_step=518600, grad_norm=4.802472114562988, loss=0.5798434019088745
I0302 07:55:04.372358 140523947947776 logging_writer.py:48] [518700] global_step=518700, grad_norm=4.517200946807861, loss=0.6062543392181396
I0302 07:55:38.144515 140525273339648 logging_writer.py:48] [518800] global_step=518800, grad_norm=4.395301342010498, loss=0.5779894590377808
I0302 07:56:11.948676 140523947947776 logging_writer.py:48] [518900] global_step=518900, grad_norm=4.538496017456055, loss=0.6062783002853394
I0302 07:56:45.746635 140525273339648 logging_writer.py:48] [519000] global_step=519000, grad_norm=4.265610694885254, loss=0.5702921152114868
I0302 07:57:19.544224 140523947947776 logging_writer.py:48] [519100] global_step=519100, grad_norm=4.53209924697876, loss=0.5865715742111206
I0302 07:57:53.317564 140525273339648 logging_writer.py:48] [519200] global_step=519200, grad_norm=4.245471477508545, loss=0.575393795967102
I0302 07:58:27.105149 140523947947776 logging_writer.py:48] [519300] global_step=519300, grad_norm=4.387913703918457, loss=0.591717004776001
I0302 07:59:00.987267 140525273339648 logging_writer.py:48] [519400] global_step=519400, grad_norm=4.1456828117370605, loss=0.5604779720306396
I0302 07:59:31.197951 140688601454400 spec.py:321] Evaluating on the training split.
I0302 07:59:37.221829 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 07:59:45.797286 140688601454400 spec.py:349] Evaluating on the test split.
I0302 07:59:48.062975 140688601454400 submission_runner.py:411] Time since start: 181583.13s, 	Step: 519491, 	{'train/accuracy': 0.9587651491165161, 'train/loss': 0.15323005616664886, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.043881893157959, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.821869969367981, 'test/num_examples': 10000, 'score': 175502.09851241112, 'total_duration': 181583.13382434845, 'accumulated_submission_time': 175502.09851241112, 'accumulated_eval_time': 6035.283583641052, 'accumulated_logging_time': 27.892229795455933}
I0302 07:59:48.166766 140522554390272 logging_writer.py:48] [519491] accumulated_eval_time=6035.283584, accumulated_logging_time=27.892230, accumulated_submission_time=175502.098512, global_step=519491, preemption_count=0, score=175502.098512, test/accuracy=0.631000, test/loss=1.821870, test/num_examples=10000, total_duration=181583.133824, train/accuracy=0.958765, train/loss=0.153230, validation/accuracy=0.755240, validation/loss=1.043882, validation/num_examples=50000
I0302 07:59:51.564450 140523092305664 logging_writer.py:48] [519500] global_step=519500, grad_norm=4.519002437591553, loss=0.7294507026672363
I0302 08:00:25.330507 140522554390272 logging_writer.py:48] [519600] global_step=519600, grad_norm=4.858032703399658, loss=0.6163243651390076
I0302 08:00:59.046525 140523092305664 logging_writer.py:48] [519700] global_step=519700, grad_norm=4.550633430480957, loss=0.6785544157028198
I0302 08:01:32.813333 140522554390272 logging_writer.py:48] [519800] global_step=519800, grad_norm=4.319151401519775, loss=0.6428081393241882
I0302 08:02:06.585838 140523092305664 logging_writer.py:48] [519900] global_step=519900, grad_norm=4.5327863693237305, loss=0.6305127739906311
I0302 08:02:40.343616 140522554390272 logging_writer.py:48] [520000] global_step=520000, grad_norm=4.905416488647461, loss=0.6137133836746216
I0302 08:03:14.131309 140523092305664 logging_writer.py:48] [520100] global_step=520100, grad_norm=4.100956439971924, loss=0.5625596642494202
I0302 08:03:47.862592 140522554390272 logging_writer.py:48] [520200] global_step=520200, grad_norm=4.764790058135986, loss=0.6660886406898499
I0302 08:04:21.655060 140523092305664 logging_writer.py:48] [520300] global_step=520300, grad_norm=4.5656232833862305, loss=0.6383002400398254
I0302 08:04:55.438826 140522554390272 logging_writer.py:48] [520400] global_step=520400, grad_norm=4.455767631530762, loss=0.6676509380340576
I0302 08:05:29.289382 140523092305664 logging_writer.py:48] [520500] global_step=520500, grad_norm=5.078790664672852, loss=0.7465715408325195
I0302 08:06:03.078980 140522554390272 logging_writer.py:48] [520600] global_step=520600, grad_norm=4.497072696685791, loss=0.6112399697303772
I0302 08:06:36.837880 140523092305664 logging_writer.py:48] [520700] global_step=520700, grad_norm=4.470289707183838, loss=0.586887001991272
I0302 08:07:10.599074 140522554390272 logging_writer.py:48] [520800] global_step=520800, grad_norm=4.805333137512207, loss=0.6695868372917175
I0302 08:07:44.366000 140523092305664 logging_writer.py:48] [520900] global_step=520900, grad_norm=4.445252418518066, loss=0.6290338635444641
I0302 08:08:18.068799 140522554390272 logging_writer.py:48] [521000] global_step=521000, grad_norm=4.175355911254883, loss=0.6314128637313843
I0302 08:08:18.076183 140688601454400 spec.py:321] Evaluating on the training split.
I0302 08:08:24.061476 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 08:08:32.636986 140688601454400 spec.py:349] Evaluating on the test split.
I0302 08:08:34.939383 140688601454400 submission_runner.py:411] Time since start: 182110.01s, 	Step: 521001, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.14842480421066284, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.04348886013031, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8207950592041016, 'test/num_examples': 10000, 'score': 176011.94000339508, 'total_duration': 182110.01022839546, 'accumulated_submission_time': 176011.94000339508, 'accumulated_eval_time': 6052.146709918976, 'accumulated_logging_time': 28.011739253997803}
I0302 08:08:35.048882 140523947947776 logging_writer.py:48] [521001] accumulated_eval_time=6052.146710, accumulated_logging_time=28.011739, accumulated_submission_time=176011.940003, global_step=521001, preemption_count=0, score=176011.940003, test/accuracy=0.630700, test/loss=1.820795, test/num_examples=10000, total_duration=182110.010228, train/accuracy=0.959702, train/loss=0.148425, validation/accuracy=0.755060, validation/loss=1.043489, validation/num_examples=50000
I0302 08:09:08.759219 140525264946944 logging_writer.py:48] [521100] global_step=521100, grad_norm=4.301445007324219, loss=0.5988398194313049
I0302 08:09:42.500574 140523947947776 logging_writer.py:48] [521200] global_step=521200, grad_norm=4.2129034996032715, loss=0.5695499777793884
I0302 08:10:16.253637 140525264946944 logging_writer.py:48] [521300] global_step=521300, grad_norm=4.294666290283203, loss=0.6056521534919739
I0302 08:10:50.005527 140523947947776 logging_writer.py:48] [521400] global_step=521400, grad_norm=4.357152938842773, loss=0.5498078465461731
I0302 08:11:23.868292 140525264946944 logging_writer.py:48] [521500] global_step=521500, grad_norm=4.549960613250732, loss=0.6701027154922485
I0302 08:11:57.660966 140523947947776 logging_writer.py:48] [521600] global_step=521600, grad_norm=4.527153491973877, loss=0.6013758778572083
I0302 08:12:31.434420 140525264946944 logging_writer.py:48] [521700] global_step=521700, grad_norm=4.013975620269775, loss=0.5746582746505737
I0302 08:13:05.228115 140523947947776 logging_writer.py:48] [521800] global_step=521800, grad_norm=4.410584926605225, loss=0.6568505764007568
I0302 08:13:39.000053 140525264946944 logging_writer.py:48] [521900] global_step=521900, grad_norm=4.7708353996276855, loss=0.6139766573905945
I0302 08:14:12.806448 140523947947776 logging_writer.py:48] [522000] global_step=522000, grad_norm=4.609342098236084, loss=0.6602474451065063
I0302 08:14:46.563127 140525264946944 logging_writer.py:48] [522100] global_step=522100, grad_norm=4.440948009490967, loss=0.5910651087760925
I0302 08:15:20.372534 140523947947776 logging_writer.py:48] [522200] global_step=522200, grad_norm=4.9466776847839355, loss=0.621415376663208
I0302 08:15:54.124373 140525264946944 logging_writer.py:48] [522300] global_step=522300, grad_norm=4.120552062988281, loss=0.5894427299499512
I0302 08:16:27.945724 140523947947776 logging_writer.py:48] [522400] global_step=522400, grad_norm=4.4332146644592285, loss=0.6441856622695923
I0302 08:17:01.728528 140525264946944 logging_writer.py:48] [522500] global_step=522500, grad_norm=4.869202136993408, loss=0.7600013613700867
I0302 08:17:05.250280 140688601454400 spec.py:321] Evaluating on the training split.
I0302 08:17:11.424672 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 08:17:20.035250 140688601454400 spec.py:349] Evaluating on the test split.
I0302 08:17:22.322860 140688601454400 submission_runner.py:411] Time since start: 182637.39s, 	Step: 522512, 	{'train/accuracy': 0.9596619606018066, 'train/loss': 0.15068073570728302, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0442814826965332, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8228248357772827, 'test/num_examples': 10000, 'score': 176522.079351902, 'total_duration': 182637.39370679855, 'accumulated_submission_time': 176522.079351902, 'accumulated_eval_time': 6069.219238519669, 'accumulated_logging_time': 28.131194591522217}
I0302 08:17:22.428699 140522404443904 logging_writer.py:48] [522512] accumulated_eval_time=6069.219239, accumulated_logging_time=28.131195, accumulated_submission_time=176522.079352, global_step=522512, preemption_count=0, score=176522.079352, test/accuracy=0.630300, test/loss=1.822825, test/num_examples=10000, total_duration=182637.393707, train/accuracy=0.959662, train/loss=0.150681, validation/accuracy=0.754920, validation/loss=1.044281, validation/num_examples=50000
I0302 08:17:52.420228 140522554390272 logging_writer.py:48] [522600] global_step=522600, grad_norm=4.347799301147461, loss=0.5532307028770447
I0302 08:18:26.158609 140522404443904 logging_writer.py:48] [522700] global_step=522700, grad_norm=4.5211334228515625, loss=0.5977686643600464
I0302 08:18:59.922020 140522554390272 logging_writer.py:48] [522800] global_step=522800, grad_norm=4.414617538452148, loss=0.6171607971191406
I0302 08:19:33.724114 140522404443904 logging_writer.py:48] [522900] global_step=522900, grad_norm=4.707835674285889, loss=0.5667834281921387
I0302 08:20:07.494479 140522554390272 logging_writer.py:48] [523000] global_step=523000, grad_norm=4.423864841461182, loss=0.6022181510925293
I0302 08:20:41.291231 140522404443904 logging_writer.py:48] [523100] global_step=523100, grad_norm=4.682122230529785, loss=0.643840491771698
I0302 08:21:15.096248 140522554390272 logging_writer.py:48] [523200] global_step=523200, grad_norm=4.460134029388428, loss=0.6447328925132751
I0302 08:21:48.870738 140522404443904 logging_writer.py:48] [523300] global_step=523300, grad_norm=4.408655643463135, loss=0.6287760734558105
I0302 08:22:22.650478 140522554390272 logging_writer.py:48] [523400] global_step=523400, grad_norm=4.400020599365234, loss=0.5714128017425537
I0302 08:22:56.430075 140522404443904 logging_writer.py:48] [523500] global_step=523500, grad_norm=4.512192249298096, loss=0.6691715717315674
I0302 08:23:30.315405 140522554390272 logging_writer.py:48] [523600] global_step=523600, grad_norm=4.117634296417236, loss=0.5426063537597656
I0302 08:24:04.066830 140522404443904 logging_writer.py:48] [523700] global_step=523700, grad_norm=4.768423080444336, loss=0.699016809463501
I0302 08:24:37.846221 140522554390272 logging_writer.py:48] [523800] global_step=523800, grad_norm=4.406682014465332, loss=0.6483525633811951
I0302 08:25:11.615978 140522404443904 logging_writer.py:48] [523900] global_step=523900, grad_norm=4.8024396896362305, loss=0.6245385408401489
I0302 08:25:45.391007 140522554390272 logging_writer.py:48] [524000] global_step=524000, grad_norm=5.097986698150635, loss=0.6523470878601074
I0302 08:25:52.628141 140688601454400 spec.py:321] Evaluating on the training split.
I0302 08:25:58.583925 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 08:26:07.388032 140688601454400 spec.py:349] Evaluating on the test split.
I0302 08:26:09.661048 140688601454400 submission_runner.py:411] Time since start: 183164.73s, 	Step: 524023, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.1465355008840561, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.0426671504974365, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.819411039352417, 'test/num_examples': 10000, 'score': 177032.21652126312, 'total_duration': 183164.73188519478, 'accumulated_submission_time': 177032.21652126312, 'accumulated_eval_time': 6086.252090454102, 'accumulated_logging_time': 28.246938705444336}
I0302 08:26:09.768134 140525264946944 logging_writer.py:48] [524023] accumulated_eval_time=6086.252090, accumulated_logging_time=28.246939, accumulated_submission_time=177032.216521, global_step=524023, preemption_count=0, score=177032.216521, test/accuracy=0.631300, test/loss=1.819411, test/num_examples=10000, total_duration=183164.731885, train/accuracy=0.960559, train/loss=0.146536, validation/accuracy=0.755300, validation/loss=1.042667, validation/num_examples=50000
I0302 08:26:36.097268 140525273339648 logging_writer.py:48] [524100] global_step=524100, grad_norm=4.846426010131836, loss=0.6891720294952393
I0302 08:27:09.824243 140525264946944 logging_writer.py:48] [524200] global_step=524200, grad_norm=4.578257083892822, loss=0.6350515484809875
I0302 08:27:43.585948 140525273339648 logging_writer.py:48] [524300] global_step=524300, grad_norm=4.657677173614502, loss=0.6404225826263428
I0302 08:28:17.395200 140525264946944 logging_writer.py:48] [524400] global_step=524400, grad_norm=4.741259574890137, loss=0.6088109612464905
I0302 08:28:51.195060 140525273339648 logging_writer.py:48] [524500] global_step=524500, grad_norm=4.874377727508545, loss=0.618680477142334
I0302 08:29:25.034937 140525264946944 logging_writer.py:48] [524600] global_step=524600, grad_norm=4.9593658447265625, loss=0.6601656675338745
I0302 08:29:58.864814 140525273339648 logging_writer.py:48] [524700] global_step=524700, grad_norm=4.334229946136475, loss=0.5645815134048462
I0302 08:30:32.644889 140525264946944 logging_writer.py:48] [524800] global_step=524800, grad_norm=4.642370700836182, loss=0.6239413022994995
I0302 08:31:06.433524 140525273339648 logging_writer.py:48] [524900] global_step=524900, grad_norm=4.516989707946777, loss=0.6239135265350342
I0302 08:31:40.230670 140525264946944 logging_writer.py:48] [525000] global_step=525000, grad_norm=4.720837116241455, loss=0.6197128295898438
I0302 08:32:14.022455 140525273339648 logging_writer.py:48] [525100] global_step=525100, grad_norm=4.558533191680908, loss=0.6783407926559448
I0302 08:32:47.823500 140525264946944 logging_writer.py:48] [525200] global_step=525200, grad_norm=4.552647590637207, loss=0.6079658269882202
I0302 08:33:21.656051 140525273339648 logging_writer.py:48] [525300] global_step=525300, grad_norm=5.212770938873291, loss=0.6755146980285645
I0302 08:33:55.449537 140525264946944 logging_writer.py:48] [525400] global_step=525400, grad_norm=4.394732475280762, loss=0.671485424041748
I0302 08:34:29.251451 140525273339648 logging_writer.py:48] [525500] global_step=525500, grad_norm=5.154385089874268, loss=0.6560325622558594
I0302 08:34:39.868438 140688601454400 spec.py:321] Evaluating on the training split.
I0302 08:34:45.935643 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 08:34:54.453212 140688601454400 spec.py:349] Evaluating on the test split.
I0302 08:34:56.747637 140688601454400 submission_runner.py:411] Time since start: 183691.82s, 	Step: 525533, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.14972807466983795, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.044359564781189, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.823042869567871, 'test/num_examples': 10000, 'score': 177542.2542464733, 'total_duration': 183691.81848621368, 'accumulated_submission_time': 177542.2542464733, 'accumulated_eval_time': 6103.131259441376, 'accumulated_logging_time': 28.364127159118652}
I0302 08:34:56.848801 140522396051200 logging_writer.py:48] [525533] accumulated_eval_time=6103.131259, accumulated_logging_time=28.364127, accumulated_submission_time=177542.254246, global_step=525533, preemption_count=0, score=177542.254246, test/accuracy=0.631000, test/loss=1.823043, test/num_examples=10000, total_duration=183691.818486, train/accuracy=0.959861, train/loss=0.149728, validation/accuracy=0.755220, validation/loss=1.044360, validation/num_examples=50000
I0302 08:35:19.776294 140522404443904 logging_writer.py:48] [525600] global_step=525600, grad_norm=4.691497802734375, loss=0.6211538314819336
I0302 08:35:53.601598 140522396051200 logging_writer.py:48] [525700] global_step=525700, grad_norm=4.434454441070557, loss=0.6522286534309387
I0302 08:36:27.356883 140522404443904 logging_writer.py:48] [525800] global_step=525800, grad_norm=4.967545986175537, loss=0.6332830786705017
I0302 08:37:01.131944 140522396051200 logging_writer.py:48] [525900] global_step=525900, grad_norm=4.075041770935059, loss=0.585595965385437
I0302 08:37:34.907626 140522404443904 logging_writer.py:48] [526000] global_step=526000, grad_norm=4.329557418823242, loss=0.6263159513473511
I0302 08:38:08.707016 140522396051200 logging_writer.py:48] [526100] global_step=526100, grad_norm=3.965120315551758, loss=0.5415953993797302
I0302 08:38:42.479971 140522404443904 logging_writer.py:48] [526200] global_step=526200, grad_norm=4.676660537719727, loss=0.6819427013397217
I0302 08:39:16.284623 140522396051200 logging_writer.py:48] [526300] global_step=526300, grad_norm=4.032011985778809, loss=0.5423802137374878
I0302 08:39:50.049758 140522404443904 logging_writer.py:48] [526400] global_step=526400, grad_norm=4.604212760925293, loss=0.6465319991111755
I0302 08:40:23.823609 140522396051200 logging_writer.py:48] [526500] global_step=526500, grad_norm=4.692321300506592, loss=0.6433243155479431
I0302 08:40:57.616563 140522404443904 logging_writer.py:48] [526600] global_step=526600, grad_norm=4.542627334594727, loss=0.652396559715271
I0302 08:41:31.325701 140522396051200 logging_writer.py:48] [526700] global_step=526700, grad_norm=4.8956828117370605, loss=0.6789637207984924
I0302 08:42:05.267001 140522404443904 logging_writer.py:48] [526800] global_step=526800, grad_norm=4.278369426727295, loss=0.6346948742866516
I0302 08:42:39.078944 140522396051200 logging_writer.py:48] [526900] global_step=526900, grad_norm=4.289287090301514, loss=0.6139940023422241
I0302 08:43:12.841430 140522404443904 logging_writer.py:48] [527000] global_step=527000, grad_norm=4.537997722625732, loss=0.6367578506469727
I0302 08:43:26.813136 140688601454400 spec.py:321] Evaluating on the training split.
I0302 08:43:32.808106 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 08:43:41.285300 140688601454400 spec.py:349] Evaluating on the test split.
I0302 08:43:43.523040 140688601454400 submission_runner.py:411] Time since start: 184218.59s, 	Step: 527043, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.1480627954006195, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.043128252029419, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8209943771362305, 'test/num_examples': 10000, 'score': 178052.15637159348, 'total_duration': 184218.59387516975, 'accumulated_submission_time': 178052.15637159348, 'accumulated_eval_time': 6119.841101884842, 'accumulated_logging_time': 28.47494602203369}
I0302 08:43:43.621888 140525264946944 logging_writer.py:48] [527043] accumulated_eval_time=6119.841102, accumulated_logging_time=28.474946, accumulated_submission_time=178052.156372, global_step=527043, preemption_count=0, score=178052.156372, test/accuracy=0.631800, test/loss=1.820994, test/num_examples=10000, total_duration=184218.593875, train/accuracy=0.960200, train/loss=0.148063, validation/accuracy=0.755140, validation/loss=1.043128, validation/num_examples=50000
I0302 08:44:03.154676 140525273339648 logging_writer.py:48] [527100] global_step=527100, grad_norm=4.605151653289795, loss=0.6492411494255066
I0302 08:44:36.898869 140525264946944 logging_writer.py:48] [527200] global_step=527200, grad_norm=4.350048542022705, loss=0.5783401131629944
I0302 08:45:10.636197 140525273339648 logging_writer.py:48] [527300] global_step=527300, grad_norm=5.546900272369385, loss=0.6895856857299805
I0302 08:45:44.422010 140525264946944 logging_writer.py:48] [527400] global_step=527400, grad_norm=4.272386074066162, loss=0.5674201250076294
I0302 08:46:18.196052 140525273339648 logging_writer.py:48] [527500] global_step=527500, grad_norm=4.739468574523926, loss=0.6970633864402771
I0302 08:46:51.986307 140525264946944 logging_writer.py:48] [527600] global_step=527600, grad_norm=4.6461310386657715, loss=0.6026715040206909
I0302 08:47:25.772796 140525273339648 logging_writer.py:48] [527700] global_step=527700, grad_norm=4.219346523284912, loss=0.5907482504844666
I0302 08:47:59.537796 140525264946944 logging_writer.py:48] [527800] global_step=527800, grad_norm=4.808263778686523, loss=0.7191128730773926
I0302 08:48:33.388573 140525273339648 logging_writer.py:48] [527900] global_step=527900, grad_norm=4.379650115966797, loss=0.5815868973731995
I0302 08:49:07.500197 140525264946944 logging_writer.py:48] [528000] global_step=528000, grad_norm=4.68080997467041, loss=0.5996975302696228
I0302 08:49:41.285365 140525273339648 logging_writer.py:48] [528100] global_step=528100, grad_norm=4.475035190582275, loss=0.6973356008529663
I0302 08:50:15.047937 140525264946944 logging_writer.py:48] [528200] global_step=528200, grad_norm=4.313473224639893, loss=0.6066372394561768
I0302 08:50:48.828846 140525273339648 logging_writer.py:48] [528300] global_step=528300, grad_norm=4.682793617248535, loss=0.7022489309310913
I0302 08:51:22.602319 140525264946944 logging_writer.py:48] [528400] global_step=528400, grad_norm=4.49679708480835, loss=0.7214662432670593
I0302 08:51:56.400659 140525273339648 logging_writer.py:48] [528500] global_step=528500, grad_norm=4.185689926147461, loss=0.562802255153656
I0302 08:52:13.797503 140688601454400 spec.py:321] Evaluating on the training split.
I0302 08:52:19.767251 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 08:52:28.308245 140688601454400 spec.py:349] Evaluating on the test split.
I0302 08:52:30.574082 140688601454400 submission_runner.py:411] Time since start: 184745.64s, 	Step: 528553, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.1473826766014099, 'validation/accuracy': 0.7552799582481384, 'validation/loss': 1.0443631410598755, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8213881254196167, 'test/num_examples': 10000, 'score': 178562.26889562607, 'total_duration': 184745.6449303627, 'accumulated_submission_time': 178562.26889562607, 'accumulated_eval_time': 6136.617638349533, 'accumulated_logging_time': 28.584404706954956}
I0302 08:52:30.678674 140522404443904 logging_writer.py:48] [528553] accumulated_eval_time=6136.617638, accumulated_logging_time=28.584405, accumulated_submission_time=178562.268896, global_step=528553, preemption_count=0, score=178562.268896, test/accuracy=0.631000, test/loss=1.821388, test/num_examples=10000, total_duration=184745.644930, train/accuracy=0.960878, train/loss=0.147383, validation/accuracy=0.755280, validation/loss=1.044363, validation/num_examples=50000
I0302 08:52:46.908712 140522554390272 logging_writer.py:48] [528600] global_step=528600, grad_norm=4.6955952644348145, loss=0.6210582256317139
I0302 08:53:20.561114 140522404443904 logging_writer.py:48] [528700] global_step=528700, grad_norm=5.26942777633667, loss=0.6908630728721619
I0302 08:53:54.367349 140522554390272 logging_writer.py:48] [528800] global_step=528800, grad_norm=4.323285102844238, loss=0.6150051951408386
I0302 08:54:28.198208 140522404443904 logging_writer.py:48] [528900] global_step=528900, grad_norm=4.044312953948975, loss=0.5290820598602295
I0302 08:55:01.985457 140522554390272 logging_writer.py:48] [529000] global_step=529000, grad_norm=4.952943325042725, loss=0.6057991981506348
I0302 08:55:35.799824 140522404443904 logging_writer.py:48] [529100] global_step=529100, grad_norm=4.469118595123291, loss=0.5782228112220764
I0302 08:56:09.602662 140522554390272 logging_writer.py:48] [529200] global_step=529200, grad_norm=4.632244110107422, loss=0.687346875667572
I0302 08:56:43.401496 140522404443904 logging_writer.py:48] [529300] global_step=529300, grad_norm=4.402589321136475, loss=0.6582314968109131
I0302 08:57:17.173209 140522554390272 logging_writer.py:48] [529400] global_step=529400, grad_norm=5.12147331237793, loss=0.7230855226516724
I0302 08:57:50.945605 140522404443904 logging_writer.py:48] [529500] global_step=529500, grad_norm=4.9610748291015625, loss=0.6242175102233887
I0302 08:58:24.702443 140522554390272 logging_writer.py:48] [529600] global_step=529600, grad_norm=4.589529037475586, loss=0.6351631283760071
I0302 08:58:58.468405 140522404443904 logging_writer.py:48] [529700] global_step=529700, grad_norm=4.295400142669678, loss=0.6048166751861572
I0302 08:59:32.283201 140522554390272 logging_writer.py:48] [529800] global_step=529800, grad_norm=4.925252914428711, loss=0.6060802340507507
I0302 09:00:06.075329 140522404443904 logging_writer.py:48] [529900] global_step=529900, grad_norm=4.467747688293457, loss=0.560612678527832
I0302 09:00:39.954994 140522554390272 logging_writer.py:48] [530000] global_step=530000, grad_norm=4.560166358947754, loss=0.6372272968292236
I0302 09:01:00.683748 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:01:06.777665 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:01:15.413879 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:01:17.718772 140688601454400 submission_runner.py:411] Time since start: 185272.79s, 	Step: 530063, 	{'train/accuracy': 0.9594626426696777, 'train/loss': 0.15038684010505676, 'validation/accuracy': 0.755079984664917, 'validation/loss': 1.0445592403411865, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8239092826843262, 'test/num_examples': 10000, 'score': 179072.21165585518, 'total_duration': 185272.78962039948, 'accumulated_submission_time': 179072.21165585518, 'accumulated_eval_time': 6153.652619361877, 'accumulated_logging_time': 28.698846340179443}
I0302 09:01:17.826554 140522404443904 logging_writer.py:48] [530063] accumulated_eval_time=6153.652619, accumulated_logging_time=28.698846, accumulated_submission_time=179072.211656, global_step=530063, preemption_count=0, score=179072.211656, test/accuracy=0.630100, test/loss=1.823909, test/num_examples=10000, total_duration=185272.789620, train/accuracy=0.959463, train/loss=0.150387, validation/accuracy=0.755080, validation/loss=1.044559, validation/num_examples=50000
I0302 09:01:30.660390 140525264946944 logging_writer.py:48] [530100] global_step=530100, grad_norm=4.386266708374023, loss=0.5359030961990356
I0302 09:02:04.430806 140522404443904 logging_writer.py:48] [530200] global_step=530200, grad_norm=4.425745010375977, loss=0.6070080399513245
I0302 09:02:38.185792 140525264946944 logging_writer.py:48] [530300] global_step=530300, grad_norm=4.7583794593811035, loss=0.6831579208374023
I0302 09:03:11.965871 140522404443904 logging_writer.py:48] [530400] global_step=530400, grad_norm=4.703573226928711, loss=0.6492587327957153
I0302 09:03:45.741900 140525264946944 logging_writer.py:48] [530500] global_step=530500, grad_norm=4.358975887298584, loss=0.6264059543609619
I0302 09:04:19.533916 140522404443904 logging_writer.py:48] [530600] global_step=530600, grad_norm=4.393650531768799, loss=0.6213516592979431
I0302 09:04:53.342476 140525264946944 logging_writer.py:48] [530700] global_step=530700, grad_norm=4.403533935546875, loss=0.6068902611732483
I0302 09:05:27.162002 140522404443904 logging_writer.py:48] [530800] global_step=530800, grad_norm=4.250249862670898, loss=0.5806549191474915
I0302 09:06:00.932450 140525264946944 logging_writer.py:48] [530900] global_step=530900, grad_norm=4.68895959854126, loss=0.6317249536514282
I0302 09:06:34.709364 140522404443904 logging_writer.py:48] [531000] global_step=531000, grad_norm=5.309706687927246, loss=0.703244149684906
I0302 09:07:08.567497 140525264946944 logging_writer.py:48] [531100] global_step=531100, grad_norm=4.347844123840332, loss=0.6663552522659302
I0302 09:07:42.342294 140522404443904 logging_writer.py:48] [531200] global_step=531200, grad_norm=4.418733596801758, loss=0.6079283952713013
I0302 09:08:16.122770 140525264946944 logging_writer.py:48] [531300] global_step=531300, grad_norm=4.305948734283447, loss=0.6305094957351685
I0302 09:08:49.902344 140522404443904 logging_writer.py:48] [531400] global_step=531400, grad_norm=4.770900726318359, loss=0.5988467931747437
I0302 09:09:23.698354 140525264946944 logging_writer.py:48] [531500] global_step=531500, grad_norm=4.278718948364258, loss=0.5713496208190918
I0302 09:09:47.844291 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:09:53.795563 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:10:02.381230 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:10:04.649415 140688601454400 submission_runner.py:411] Time since start: 185799.72s, 	Step: 531573, 	{'train/accuracy': 0.9595224857330322, 'train/loss': 0.15040116012096405, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0430076122283936, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8203206062316895, 'test/num_examples': 10000, 'score': 179582.16641449928, 'total_duration': 185799.72025728226, 'accumulated_submission_time': 179582.16641449928, 'accumulated_eval_time': 6170.457691907883, 'accumulated_logging_time': 28.817756414413452}
I0302 09:10:04.754309 140522554390272 logging_writer.py:48] [531573] accumulated_eval_time=6170.457692, accumulated_logging_time=28.817756, accumulated_submission_time=179582.166414, global_step=531573, preemption_count=0, score=179582.166414, test/accuracy=0.631200, test/loss=1.820321, test/num_examples=10000, total_duration=185799.720257, train/accuracy=0.959522, train/loss=0.150401, validation/accuracy=0.755360, validation/loss=1.043008, validation/num_examples=50000
I0302 09:10:14.213881 140523092305664 logging_writer.py:48] [531600] global_step=531600, grad_norm=4.4164886474609375, loss=0.573047935962677
I0302 09:10:47.912767 140522554390272 logging_writer.py:48] [531700] global_step=531700, grad_norm=4.707536697387695, loss=0.5889346599578857
I0302 09:11:21.683503 140523092305664 logging_writer.py:48] [531800] global_step=531800, grad_norm=4.149982452392578, loss=0.5809401273727417
I0302 09:11:55.504935 140522554390272 logging_writer.py:48] [531900] global_step=531900, grad_norm=4.91519021987915, loss=0.6020534634590149
I0302 09:12:29.282181 140523092305664 logging_writer.py:48] [532000] global_step=532000, grad_norm=4.966789245605469, loss=0.6817580461502075
I0302 09:13:03.122169 140522554390272 logging_writer.py:48] [532100] global_step=532100, grad_norm=4.8621392250061035, loss=0.7213679552078247
I0302 09:13:36.912036 140523092305664 logging_writer.py:48] [532200] global_step=532200, grad_norm=4.304328441619873, loss=0.6422850489616394
I0302 09:14:10.697930 140522554390272 logging_writer.py:48] [532300] global_step=532300, grad_norm=4.355563163757324, loss=0.5599427819252014
I0302 09:14:44.472452 140523092305664 logging_writer.py:48] [532400] global_step=532400, grad_norm=4.606964588165283, loss=0.6818198561668396
I0302 09:15:18.275435 140522554390272 logging_writer.py:48] [532500] global_step=532500, grad_norm=5.1052350997924805, loss=0.5922874808311462
I0302 09:15:52.062344 140523092305664 logging_writer.py:48] [532600] global_step=532600, grad_norm=4.877137660980225, loss=0.6866950392723083
I0302 09:16:25.855048 140522554390272 logging_writer.py:48] [532700] global_step=532700, grad_norm=4.540436744689941, loss=0.6098601818084717
I0302 09:16:59.635905 140523092305664 logging_writer.py:48] [532800] global_step=532800, grad_norm=4.568025588989258, loss=0.6724794507026672
I0302 09:17:33.463409 140522554390272 logging_writer.py:48] [532900] global_step=532900, grad_norm=4.794417381286621, loss=0.6046450138092041
I0302 09:18:07.250785 140523092305664 logging_writer.py:48] [533000] global_step=533000, grad_norm=4.783989429473877, loss=0.6750755906105042
I0302 09:18:34.765923 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:18:40.790986 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:18:49.230842 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:18:51.490588 140688601454400 submission_runner.py:411] Time since start: 186326.56s, 	Step: 533083, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.1487494260072708, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.0439097881317139, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8219304084777832, 'test/num_examples': 10000, 'score': 180092.11389112473, 'total_duration': 186326.5613975525, 'accumulated_submission_time': 180092.11389112473, 'accumulated_eval_time': 6187.182283878326, 'accumulated_logging_time': 28.934704780578613}
I0302 09:18:51.596680 140522554390272 logging_writer.py:48] [533083] accumulated_eval_time=6187.182284, accumulated_logging_time=28.934705, accumulated_submission_time=180092.113891, global_step=533083, preemption_count=0, score=180092.113891, test/accuracy=0.632000, test/loss=1.821930, test/num_examples=10000, total_duration=186326.561398, train/accuracy=0.960539, train/loss=0.148749, validation/accuracy=0.755200, validation/loss=1.043910, validation/num_examples=50000
I0302 09:18:57.679252 140525264946944 logging_writer.py:48] [533100] global_step=533100, grad_norm=4.719447135925293, loss=0.6475717425346375
I0302 09:19:31.475830 140522554390272 logging_writer.py:48] [533200] global_step=533200, grad_norm=4.236325263977051, loss=0.5706255435943604
I0302 09:20:05.229668 140525264946944 logging_writer.py:48] [533300] global_step=533300, grad_norm=4.425393104553223, loss=0.5662776231765747
I0302 09:20:38.979218 140522554390272 logging_writer.py:48] [533400] global_step=533400, grad_norm=4.105413913726807, loss=0.5545190572738647
I0302 09:21:12.726366 140525264946944 logging_writer.py:48] [533500] global_step=533500, grad_norm=4.539801120758057, loss=0.629744827747345
I0302 09:21:46.492971 140522554390272 logging_writer.py:48] [533600] global_step=533600, grad_norm=4.241236686706543, loss=0.5851466655731201
I0302 09:22:20.259926 140525264946944 logging_writer.py:48] [533700] global_step=533700, grad_norm=4.750932216644287, loss=0.651146650314331
I0302 09:22:54.013437 140522554390272 logging_writer.py:48] [533800] global_step=533800, grad_norm=4.3173041343688965, loss=0.6494669914245605
I0302 09:23:27.801371 140525264946944 logging_writer.py:48] [533900] global_step=533900, grad_norm=4.243439197540283, loss=0.6187228560447693
I0302 09:24:01.572065 140522554390272 logging_writer.py:48] [534000] global_step=534000, grad_norm=4.467795372009277, loss=0.6210564374923706
I0302 09:24:35.389410 140525264946944 logging_writer.py:48] [534100] global_step=534100, grad_norm=4.580251216888428, loss=0.6037585735321045
I0302 09:25:09.161059 140522554390272 logging_writer.py:48] [534200] global_step=534200, grad_norm=4.738065719604492, loss=0.6894442439079285
I0302 09:25:43.002158 140525264946944 logging_writer.py:48] [534300] global_step=534300, grad_norm=4.741330623626709, loss=0.6262762546539307
I0302 09:26:16.806321 140522554390272 logging_writer.py:48] [534400] global_step=534400, grad_norm=4.987762928009033, loss=0.7289524078369141
I0302 09:26:50.585039 140525264946944 logging_writer.py:48] [534500] global_step=534500, grad_norm=4.772633075714111, loss=0.6986558437347412
I0302 09:27:21.786344 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:27:27.765796 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:27:36.336766 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:27:38.602444 140688601454400 submission_runner.py:411] Time since start: 186853.67s, 	Step: 534594, 	{'train/accuracy': 0.9620535373687744, 'train/loss': 0.14273086190223694, 'validation/accuracy': 0.7545999884605408, 'validation/loss': 1.04520845413208, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8219239711761475, 'test/num_examples': 10000, 'score': 180602.2413403988, 'total_duration': 186853.67328119278, 'accumulated_submission_time': 180602.2413403988, 'accumulated_eval_time': 6203.998332023621, 'accumulated_logging_time': 29.051042318344116}
I0302 09:27:38.703920 140523947947776 logging_writer.py:48] [534594] accumulated_eval_time=6203.998332, accumulated_logging_time=29.051042, accumulated_submission_time=180602.241340, global_step=534594, preemption_count=0, score=180602.241340, test/accuracy=0.632000, test/loss=1.821924, test/num_examples=10000, total_duration=186853.673281, train/accuracy=0.962054, train/loss=0.142731, validation/accuracy=0.754600, validation/loss=1.045208, validation/num_examples=50000
I0302 09:27:41.076663 140525256554240 logging_writer.py:48] [534600] global_step=534600, grad_norm=5.237556457519531, loss=0.6277744770050049
I0302 09:28:14.811102 140523947947776 logging_writer.py:48] [534700] global_step=534700, grad_norm=4.678627014160156, loss=0.602053165435791
I0302 09:28:48.519521 140525256554240 logging_writer.py:48] [534800] global_step=534800, grad_norm=4.908827304840088, loss=0.6292327642440796
I0302 09:29:22.328611 140523947947776 logging_writer.py:48] [534900] global_step=534900, grad_norm=4.3033342361450195, loss=0.5589765310287476
I0302 09:29:56.111265 140525256554240 logging_writer.py:48] [535000] global_step=535000, grad_norm=4.789563179016113, loss=0.6377071142196655
I0302 09:30:29.899394 140523947947776 logging_writer.py:48] [535100] global_step=535100, grad_norm=4.340052604675293, loss=0.5998336672782898
I0302 09:31:03.680548 140525256554240 logging_writer.py:48] [535200] global_step=535200, grad_norm=4.806510925292969, loss=0.6147410869598389
I0302 09:31:37.550149 140523947947776 logging_writer.py:48] [535300] global_step=535300, grad_norm=4.742532730102539, loss=0.6407273411750793
I0302 09:32:11.288582 140525256554240 logging_writer.py:48] [535400] global_step=535400, grad_norm=4.591207027435303, loss=0.6632170677185059
I0302 09:32:45.065488 140523947947776 logging_writer.py:48] [535500] global_step=535500, grad_norm=4.693722724914551, loss=0.6219874024391174
I0302 09:33:18.868613 140525256554240 logging_writer.py:48] [535600] global_step=535600, grad_norm=4.481743335723877, loss=0.5678809285163879
I0302 09:33:52.624170 140523947947776 logging_writer.py:48] [535700] global_step=535700, grad_norm=4.039669990539551, loss=0.5991520285606384
I0302 09:34:26.420978 140525256554240 logging_writer.py:48] [535800] global_step=535800, grad_norm=4.4035444259643555, loss=0.670549750328064
I0302 09:35:00.185984 140523947947776 logging_writer.py:48] [535900] global_step=535900, grad_norm=4.598932266235352, loss=0.6701838970184326
I0302 09:35:33.909732 140525256554240 logging_writer.py:48] [536000] global_step=536000, grad_norm=3.9648890495300293, loss=0.5987078547477722
I0302 09:36:07.720625 140523947947776 logging_writer.py:48] [536100] global_step=536100, grad_norm=4.4389472007751465, loss=0.6218543648719788
I0302 09:36:08.891319 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:36:15.044038 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:36:23.645159 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:36:25.965016 140688601454400 submission_runner.py:411] Time since start: 187381.04s, 	Step: 536105, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.1484987586736679, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0436646938323975, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8213005065917969, 'test/num_examples': 10000, 'score': 181112.36427617073, 'total_duration': 187381.0358531475, 'accumulated_submission_time': 181112.36427617073, 'accumulated_eval_time': 6221.071969509125, 'accumulated_logging_time': 29.16448450088501}
I0302 09:36:26.069414 140522554390272 logging_writer.py:48] [536105] accumulated_eval_time=6221.071970, accumulated_logging_time=29.164485, accumulated_submission_time=181112.364276, global_step=536105, preemption_count=0, score=181112.364276, test/accuracy=0.632100, test/loss=1.821301, test/num_examples=10000, total_duration=187381.035853, train/accuracy=0.960778, train/loss=0.148499, validation/accuracy=0.755440, validation/loss=1.043665, validation/num_examples=50000
I0302 09:36:58.432279 140523092305664 logging_writer.py:48] [536200] global_step=536200, grad_norm=4.853910446166992, loss=0.7210488319396973
I0302 09:37:32.158345 140522554390272 logging_writer.py:48] [536300] global_step=536300, grad_norm=4.508302211761475, loss=0.6264203786849976
I0302 09:38:06.026050 140523092305664 logging_writer.py:48] [536400] global_step=536400, grad_norm=4.3892107009887695, loss=0.591234564781189
I0302 09:38:39.818919 140522554390272 logging_writer.py:48] [536500] global_step=536500, grad_norm=4.459690570831299, loss=0.6286669373512268
I0302 09:39:13.552967 140523092305664 logging_writer.py:48] [536600] global_step=536600, grad_norm=4.984455108642578, loss=0.6193324327468872
I0302 09:39:47.318554 140522554390272 logging_writer.py:48] [536700] global_step=536700, grad_norm=4.375953674316406, loss=0.6248852014541626
I0302 09:40:21.079076 140523092305664 logging_writer.py:48] [536800] global_step=536800, grad_norm=4.444221019744873, loss=0.6748247146606445
I0302 09:40:54.903046 140522554390272 logging_writer.py:48] [536900] global_step=536900, grad_norm=4.511326313018799, loss=0.5932705402374268
I0302 09:41:28.681335 140523092305664 logging_writer.py:48] [537000] global_step=537000, grad_norm=4.705410480499268, loss=0.6295658349990845
I0302 09:42:02.488551 140522554390272 logging_writer.py:48] [537100] global_step=537100, grad_norm=4.273857116699219, loss=0.566113293170929
I0302 09:42:36.271437 140523092305664 logging_writer.py:48] [537200] global_step=537200, grad_norm=4.403480052947998, loss=0.6297156810760498
I0302 09:43:10.047267 140522554390272 logging_writer.py:48] [537300] global_step=537300, grad_norm=4.6295318603515625, loss=0.6309860348701477
I0302 09:43:43.841682 140523092305664 logging_writer.py:48] [537400] global_step=537400, grad_norm=4.622714042663574, loss=0.6477581858634949
I0302 09:44:17.728263 140522554390272 logging_writer.py:48] [537500] global_step=537500, grad_norm=4.4907426834106445, loss=0.6352348327636719
I0302 09:44:51.478110 140523092305664 logging_writer.py:48] [537600] global_step=537600, grad_norm=4.232776165008545, loss=0.6138635873794556
I0302 09:44:56.013441 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:45:02.144962 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:45:10.982718 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:45:13.263991 140688601454400 submission_runner.py:411] Time since start: 187908.33s, 	Step: 537615, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.1466994285583496, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0449239015579224, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8213962316513062, 'test/num_examples': 10000, 'score': 181622.24652838707, 'total_duration': 187908.334836483, 'accumulated_submission_time': 181622.24652838707, 'accumulated_eval_time': 6238.322468757629, 'accumulated_logging_time': 29.278345346450806}
I0302 09:45:13.365314 140522404443904 logging_writer.py:48] [537615] accumulated_eval_time=6238.322469, accumulated_logging_time=29.278345, accumulated_submission_time=181622.246528, global_step=537615, preemption_count=0, score=181622.246528, test/accuracy=0.630100, test/loss=1.821396, test/num_examples=10000, total_duration=187908.334836, train/accuracy=0.960439, train/loss=0.146699, validation/accuracy=0.755260, validation/loss=1.044924, validation/num_examples=50000
I0302 09:45:42.363175 140522554390272 logging_writer.py:48] [537700] global_step=537700, grad_norm=4.999731063842773, loss=0.6839129328727722
I0302 09:46:16.091073 140522404443904 logging_writer.py:48] [537800] global_step=537800, grad_norm=4.612668991088867, loss=0.6430665254592896
I0302 09:46:49.854776 140522554390272 logging_writer.py:48] [537900] global_step=537900, grad_norm=4.399099349975586, loss=0.5673130750656128
I0302 09:47:23.623567 140522404443904 logging_writer.py:48] [538000] global_step=538000, grad_norm=4.145979404449463, loss=0.6523227691650391
I0302 09:47:57.384650 140522554390272 logging_writer.py:48] [538100] global_step=538100, grad_norm=4.195980072021484, loss=0.6844613552093506
I0302 09:48:31.155956 140522404443904 logging_writer.py:48] [538200] global_step=538200, grad_norm=4.376354694366455, loss=0.6763981580734253
I0302 09:49:04.960812 140522554390272 logging_writer.py:48] [538300] global_step=538300, grad_norm=3.9963910579681396, loss=0.586121141910553
I0302 09:49:38.746462 140522404443904 logging_writer.py:48] [538400] global_step=538400, grad_norm=4.495079517364502, loss=0.6528176069259644
I0302 09:50:12.617430 140522554390272 logging_writer.py:48] [538500] global_step=538500, grad_norm=4.622918128967285, loss=0.7059078216552734
I0302 09:50:46.413511 140522404443904 logging_writer.py:48] [538600] global_step=538600, grad_norm=4.283611297607422, loss=0.5578571557998657
I0302 09:51:20.220958 140522554390272 logging_writer.py:48] [538700] global_step=538700, grad_norm=4.711901664733887, loss=0.6229692101478577
I0302 09:51:54.019371 140522404443904 logging_writer.py:48] [538800] global_step=538800, grad_norm=4.785951137542725, loss=0.7066659927368164
I0302 09:52:27.826510 140522554390272 logging_writer.py:48] [538900] global_step=538900, grad_norm=4.782260894775391, loss=0.6563876271247864
I0302 09:53:01.623663 140522404443904 logging_writer.py:48] [539000] global_step=539000, grad_norm=5.010861396789551, loss=0.6146329641342163
I0302 09:53:35.416937 140522554390272 logging_writer.py:48] [539100] global_step=539100, grad_norm=4.9144206047058105, loss=0.6612321734428406
I0302 09:53:43.326182 140688601454400 spec.py:321] Evaluating on the training split.
I0302 09:53:49.305682 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 09:53:57.920441 140688601454400 spec.py:349] Evaluating on the test split.
I0302 09:54:00.314409 140688601454400 submission_runner.py:411] Time since start: 188435.38s, 	Step: 539125, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.15160095691680908, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.043686032295227, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8224132061004639, 'test/num_examples': 10000, 'score': 182132.1446402073, 'total_duration': 188435.38492751122, 'accumulated_submission_time': 182132.1446402073, 'accumulated_eval_time': 6255.310319900513, 'accumulated_logging_time': 29.389899730682373}
I0302 09:54:00.423735 140523092305664 logging_writer.py:48] [539125] accumulated_eval_time=6255.310320, accumulated_logging_time=29.389900, accumulated_submission_time=182132.144640, global_step=539125, preemption_count=0, score=182132.144640, test/accuracy=0.631000, test/loss=1.822413, test/num_examples=10000, total_duration=188435.384928, train/accuracy=0.959602, train/loss=0.151601, validation/accuracy=0.755560, validation/loss=1.043686, validation/num_examples=50000
I0302 09:54:26.097628 140525264946944 logging_writer.py:48] [539200] global_step=539200, grad_norm=4.336902618408203, loss=0.6354720592498779
I0302 09:54:59.849934 140523092305664 logging_writer.py:48] [539300] global_step=539300, grad_norm=4.356377601623535, loss=0.549669086933136
I0302 09:55:33.611634 140525264946944 logging_writer.py:48] [539400] global_step=539400, grad_norm=4.982844352722168, loss=0.6313652396202087
I0302 09:56:07.465264 140523092305664 logging_writer.py:48] [539500] global_step=539500, grad_norm=4.378321170806885, loss=0.627778172492981
I0302 09:56:41.268138 140525264946944 logging_writer.py:48] [539600] global_step=539600, grad_norm=4.54470157623291, loss=0.5700740814208984
I0302 09:57:15.008106 140523092305664 logging_writer.py:48] [539700] global_step=539700, grad_norm=5.119678020477295, loss=0.6551662683486938
I0302 09:57:48.749444 140525264946944 logging_writer.py:48] [539800] global_step=539800, grad_norm=4.6418986320495605, loss=0.6375611424446106
I0302 09:58:22.539222 140523092305664 logging_writer.py:48] [539900] global_step=539900, grad_norm=4.491906642913818, loss=0.6404081583023071
I0302 09:58:56.363854 140525264946944 logging_writer.py:48] [540000] global_step=540000, grad_norm=4.591737270355225, loss=0.6989347338676453
I0302 09:59:30.143594 140523092305664 logging_writer.py:48] [540100] global_step=540100, grad_norm=4.896080017089844, loss=0.6016892790794373
I0302 10:00:03.932744 140525264946944 logging_writer.py:48] [540200] global_step=540200, grad_norm=4.465275287628174, loss=0.6579194664955139
I0302 10:00:37.733214 140523092305664 logging_writer.py:48] [540300] global_step=540300, grad_norm=4.625818729400635, loss=0.6465384364128113
I0302 10:01:11.532435 140525264946944 logging_writer.py:48] [540400] global_step=540400, grad_norm=4.665247440338135, loss=0.5997098684310913
I0302 10:01:45.320961 140523092305664 logging_writer.py:48] [540500] global_step=540500, grad_norm=4.520625114440918, loss=0.6064034104347229
I0302 10:02:19.157729 140525264946944 logging_writer.py:48] [540600] global_step=540600, grad_norm=4.5962982177734375, loss=0.700098991394043
I0302 10:02:30.478037 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:02:36.448818 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:02:44.872391 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:02:47.225769 140688601454400 submission_runner.py:411] Time since start: 188962.30s, 	Step: 540635, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.14935335516929626, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0445722341537476, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8208487033843994, 'test/num_examples': 10000, 'score': 182642.13536715508, 'total_duration': 188962.29661369324, 'accumulated_submission_time': 182642.13536715508, 'accumulated_eval_time': 6272.058003902435, 'accumulated_logging_time': 29.509883642196655}
I0302 10:02:47.331815 140522554390272 logging_writer.py:48] [540635] accumulated_eval_time=6272.058004, accumulated_logging_time=29.509884, accumulated_submission_time=182642.135367, global_step=540635, preemption_count=0, score=182642.135367, test/accuracy=0.630800, test/loss=1.820849, test/num_examples=10000, total_duration=188962.296614, train/accuracy=0.959861, train/loss=0.149353, validation/accuracy=0.755140, validation/loss=1.044572, validation/num_examples=50000
I0302 10:03:09.571680 140523947947776 logging_writer.py:48] [540700] global_step=540700, grad_norm=4.365558624267578, loss=0.5831027626991272
I0302 10:03:43.300983 140522554390272 logging_writer.py:48] [540800] global_step=540800, grad_norm=4.284167289733887, loss=0.6309829950332642
I0302 10:04:17.083642 140523947947776 logging_writer.py:48] [540900] global_step=540900, grad_norm=4.64996337890625, loss=0.6283479332923889
I0302 10:04:50.807295 140522554390272 logging_writer.py:48] [541000] global_step=541000, grad_norm=4.616255283355713, loss=0.6726215481758118
I0302 10:05:24.561398 140523947947776 logging_writer.py:48] [541100] global_step=541100, grad_norm=4.0793561935424805, loss=0.5825614929199219
I0302 10:05:58.344321 140522554390272 logging_writer.py:48] [541200] global_step=541200, grad_norm=4.178323745727539, loss=0.6213433742523193
I0302 10:06:32.125383 140523947947776 logging_writer.py:48] [541300] global_step=541300, grad_norm=4.179329872131348, loss=0.5750877261161804
I0302 10:07:05.889310 140522554390272 logging_writer.py:48] [541400] global_step=541400, grad_norm=4.656505584716797, loss=0.619036078453064
I0302 10:07:39.644771 140523947947776 logging_writer.py:48] [541500] global_step=541500, grad_norm=4.536056995391846, loss=0.6144148111343384
I0302 10:08:13.606205 140522554390272 logging_writer.py:48] [541600] global_step=541600, grad_norm=4.356082439422607, loss=0.6174489259719849
I0302 10:08:47.353961 140523947947776 logging_writer.py:48] [541700] global_step=541700, grad_norm=4.342884540557861, loss=0.6905096173286438
I0302 10:09:21.114837 140522554390272 logging_writer.py:48] [541800] global_step=541800, grad_norm=5.081784725189209, loss=0.6139750480651855
I0302 10:09:54.924194 140523947947776 logging_writer.py:48] [541900] global_step=541900, grad_norm=5.331035614013672, loss=0.6651479005813599
I0302 10:10:28.713068 140522554390272 logging_writer.py:48] [542000] global_step=542000, grad_norm=4.6638102531433105, loss=0.6021566390991211
I0302 10:11:02.502404 140523947947776 logging_writer.py:48] [542100] global_step=542100, grad_norm=4.556050777435303, loss=0.6772838830947876
I0302 10:11:17.509408 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:11:23.466166 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:11:31.861705 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:11:34.144111 140688601454400 submission_runner.py:411] Time since start: 189489.21s, 	Step: 542146, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14494061470031738, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.044356346130371, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8221149444580078, 'test/num_examples': 10000, 'score': 183152.2509188652, 'total_duration': 189489.21495723724, 'accumulated_submission_time': 183152.2509188652, 'accumulated_eval_time': 6288.69265794754, 'accumulated_logging_time': 29.62590265274048}
I0302 10:11:34.251879 140523092305664 logging_writer.py:48] [542146] accumulated_eval_time=6288.692658, accumulated_logging_time=29.625903, accumulated_submission_time=183152.250919, global_step=542146, preemption_count=0, score=183152.250919, test/accuracy=0.631000, test/loss=1.822115, test/num_examples=10000, total_duration=189489.214957, train/accuracy=0.961515, train/loss=0.144941, validation/accuracy=0.754920, validation/loss=1.044356, validation/num_examples=50000
I0302 10:11:52.842936 140523947947776 logging_writer.py:48] [542200] global_step=542200, grad_norm=4.245023727416992, loss=0.553712010383606
I0302 10:12:26.552450 140523092305664 logging_writer.py:48] [542300] global_step=542300, grad_norm=4.016692161560059, loss=0.5441417694091797
I0302 10:13:00.301571 140523947947776 logging_writer.py:48] [542400] global_step=542400, grad_norm=4.441878795623779, loss=0.5882672071456909
I0302 10:13:34.097430 140523092305664 logging_writer.py:48] [542500] global_step=542500, grad_norm=4.785393238067627, loss=0.651697039604187
I0302 10:14:07.868741 140523947947776 logging_writer.py:48] [542600] global_step=542600, grad_norm=4.605983257293701, loss=0.6256762742996216
I0302 10:14:41.730627 140523092305664 logging_writer.py:48] [542700] global_step=542700, grad_norm=5.418835163116455, loss=0.7352945804595947
I0302 10:15:15.502699 140523947947776 logging_writer.py:48] [542800] global_step=542800, grad_norm=4.423035144805908, loss=0.5805188417434692
I0302 10:15:49.280131 140523092305664 logging_writer.py:48] [542900] global_step=542900, grad_norm=4.527285099029541, loss=0.5685312747955322
I0302 10:16:23.010104 140523947947776 logging_writer.py:48] [543000] global_step=543000, grad_norm=4.412071704864502, loss=0.6327894926071167
I0302 10:16:56.780461 140523092305664 logging_writer.py:48] [543100] global_step=543100, grad_norm=4.680258274078369, loss=0.6640986800193787
I0302 10:17:30.559483 140523947947776 logging_writer.py:48] [543200] global_step=543200, grad_norm=4.905261039733887, loss=0.6047251224517822
I0302 10:18:04.360652 140523092305664 logging_writer.py:48] [543300] global_step=543300, grad_norm=4.172518730163574, loss=0.621077835559845
I0302 10:18:38.155941 140523947947776 logging_writer.py:48] [543400] global_step=543400, grad_norm=4.489372253417969, loss=0.611366868019104
I0302 10:19:11.946445 140523092305664 logging_writer.py:48] [543500] global_step=543500, grad_norm=4.591020107269287, loss=0.6751137375831604
I0302 10:19:45.728446 140523947947776 logging_writer.py:48] [543600] global_step=543600, grad_norm=4.326303958892822, loss=0.5461713671684265
I0302 10:20:04.450361 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:20:10.376619 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:20:18.904956 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:20:21.236774 140688601454400 submission_runner.py:411] Time since start: 190016.31s, 	Step: 543657, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.1462753862142563, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0432575941085815, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8207831382751465, 'test/num_examples': 10000, 'score': 183662.38544940948, 'total_duration': 190016.3076210022, 'accumulated_submission_time': 183662.38544940948, 'accumulated_eval_time': 6305.479026794434, 'accumulated_logging_time': 29.744391202926636}
I0302 10:20:21.340260 140522554390272 logging_writer.py:48] [543657] accumulated_eval_time=6305.479027, accumulated_logging_time=29.744391, accumulated_submission_time=183662.385449, global_step=543657, preemption_count=0, score=183662.385449, test/accuracy=0.631900, test/loss=1.820783, test/num_examples=10000, total_duration=190016.307621, train/accuracy=0.960140, train/loss=0.146275, validation/accuracy=0.755140, validation/loss=1.043258, validation/num_examples=50000
I0302 10:20:36.384034 140525256554240 logging_writer.py:48] [543700] global_step=543700, grad_norm=4.554905414581299, loss=0.599846601486206
I0302 10:21:10.077339 140522554390272 logging_writer.py:48] [543800] global_step=543800, grad_norm=4.325037002563477, loss=0.6241922974586487
I0302 10:21:43.806488 140525256554240 logging_writer.py:48] [543900] global_step=543900, grad_norm=5.068056583404541, loss=0.6121184229850769
I0302 10:22:17.564256 140522554390272 logging_writer.py:48] [544000] global_step=544000, grad_norm=4.416856288909912, loss=0.6775121688842773
I0302 10:22:51.343627 140525256554240 logging_writer.py:48] [544100] global_step=544100, grad_norm=4.483331203460693, loss=0.5825120210647583
I0302 10:23:25.078375 140522554390272 logging_writer.py:48] [544200] global_step=544200, grad_norm=4.798765182495117, loss=0.6420679688453674
I0302 10:23:58.844545 140525256554240 logging_writer.py:48] [544300] global_step=544300, grad_norm=4.726018905639648, loss=0.6311943531036377
I0302 10:24:32.656578 140522554390272 logging_writer.py:48] [544400] global_step=544400, grad_norm=4.545010566711426, loss=0.6543968319892883
I0302 10:25:06.395977 140525256554240 logging_writer.py:48] [544500] global_step=544500, grad_norm=4.410912990570068, loss=0.6441493034362793
I0302 10:25:40.140610 140522554390272 logging_writer.py:48] [544600] global_step=544600, grad_norm=4.671609878540039, loss=0.6196523904800415
I0302 10:26:13.910088 140525256554240 logging_writer.py:48] [544700] global_step=544700, grad_norm=4.855447292327881, loss=0.620566725730896
I0302 10:26:47.742894 140522554390272 logging_writer.py:48] [544800] global_step=544800, grad_norm=4.543449401855469, loss=0.5713337659835815
I0302 10:27:21.525520 140525256554240 logging_writer.py:48] [544900] global_step=544900, grad_norm=4.700129985809326, loss=0.5996637940406799
I0302 10:27:55.332444 140522554390272 logging_writer.py:48] [545000] global_step=545000, grad_norm=5.4918694496154785, loss=0.7156035900115967
I0302 10:28:29.103395 140525256554240 logging_writer.py:48] [545100] global_step=545100, grad_norm=4.597202301025391, loss=0.6218029260635376
I0302 10:28:51.528316 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:28:57.726856 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:29:06.436227 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:29:08.755651 140688601454400 submission_runner.py:411] Time since start: 190543.83s, 	Step: 545168, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.1489509791135788, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.044825792312622, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8213889598846436, 'test/num_examples': 10000, 'score': 184172.5107626915, 'total_duration': 190543.8264966011, 'accumulated_submission_time': 184172.5107626915, 'accumulated_eval_time': 6322.7063319683075, 'accumulated_logging_time': 29.857580184936523}
I0302 10:29:08.860588 140522396051200 logging_writer.py:48] [545168] accumulated_eval_time=6322.706332, accumulated_logging_time=29.857580, accumulated_submission_time=184172.510763, global_step=545168, preemption_count=0, score=184172.510763, test/accuracy=0.631100, test/loss=1.821389, test/num_examples=10000, total_duration=190543.826497, train/accuracy=0.960001, train/loss=0.148951, validation/accuracy=0.755000, validation/loss=1.044826, validation/num_examples=50000
I0302 10:29:20.021446 140522404443904 logging_writer.py:48] [545200] global_step=545200, grad_norm=4.265564918518066, loss=0.6014761328697205
I0302 10:29:53.695720 140522396051200 logging_writer.py:48] [545300] global_step=545300, grad_norm=4.039340496063232, loss=0.6127623319625854
I0302 10:30:27.437911 140522404443904 logging_writer.py:48] [545400] global_step=545400, grad_norm=4.5675177574157715, loss=0.5956712961196899
I0302 10:31:01.193034 140522396051200 logging_writer.py:48] [545500] global_step=545500, grad_norm=5.506322860717773, loss=0.7456414699554443
I0302 10:31:34.973015 140522404443904 logging_writer.py:48] [545600] global_step=545600, grad_norm=4.214224815368652, loss=0.6448596715927124
I0302 10:32:08.739681 140522396051200 logging_writer.py:48] [545700] global_step=545700, grad_norm=4.738776206970215, loss=0.656745433807373
I0302 10:32:42.607436 140522404443904 logging_writer.py:48] [545800] global_step=545800, grad_norm=4.433077812194824, loss=0.6018466949462891
I0302 10:33:16.391776 140522396051200 logging_writer.py:48] [545900] global_step=545900, grad_norm=4.426503658294678, loss=0.5979387760162354
I0302 10:33:50.118814 140522404443904 logging_writer.py:48] [546000] global_step=546000, grad_norm=4.4841628074646, loss=0.6105263233184814
I0302 10:34:23.885199 140522396051200 logging_writer.py:48] [546100] global_step=546100, grad_norm=4.249979496002197, loss=0.6110535860061646
I0302 10:34:57.675959 140522404443904 logging_writer.py:48] [546200] global_step=546200, grad_norm=5.081788063049316, loss=0.6919324398040771
I0302 10:35:31.437914 140522396051200 logging_writer.py:48] [546300] global_step=546300, grad_norm=4.757996559143066, loss=0.6259045004844666
I0302 10:36:05.234701 140522404443904 logging_writer.py:48] [546400] global_step=546400, grad_norm=4.735446929931641, loss=0.6458742022514343
I0302 10:36:39.003302 140522396051200 logging_writer.py:48] [546500] global_step=546500, grad_norm=4.761399745941162, loss=0.5970844030380249
I0302 10:37:12.797622 140522404443904 logging_writer.py:48] [546600] global_step=546600, grad_norm=4.262252330780029, loss=0.5853402614593506
I0302 10:37:38.951652 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:37:44.934596 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:37:53.519753 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:37:55.784117 140688601454400 submission_runner.py:411] Time since start: 191070.85s, 	Step: 546679, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14726100862026215, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.043584942817688, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8213204145431519, 'test/num_examples': 10000, 'score': 184682.5400466919, 'total_duration': 191070.85496354103, 'accumulated_submission_time': 184682.5400466919, 'accumulated_eval_time': 6339.538756847382, 'accumulated_logging_time': 29.97205090522766}
I0302 10:37:55.892126 140522396051200 logging_writer.py:48] [546679] accumulated_eval_time=6339.538757, accumulated_logging_time=29.972051, accumulated_submission_time=184682.540047, global_step=546679, preemption_count=0, score=184682.540047, test/accuracy=0.631500, test/loss=1.821320, test/num_examples=10000, total_duration=191070.854964, train/accuracy=0.960678, train/loss=0.147261, validation/accuracy=0.755000, validation/loss=1.043585, validation/num_examples=50000
I0302 10:38:03.323435 140525256554240 logging_writer.py:48] [546700] global_step=546700, grad_norm=4.278648853302002, loss=0.6139782667160034
I0302 10:38:37.043660 140522396051200 logging_writer.py:48] [546800] global_step=546800, grad_norm=4.778563976287842, loss=0.6299235820770264
I0302 10:39:10.980360 140525256554240 logging_writer.py:48] [546900] global_step=546900, grad_norm=5.528581142425537, loss=0.6276894807815552
I0302 10:39:44.762955 140522396051200 logging_writer.py:48] [547000] global_step=547000, grad_norm=4.149754047393799, loss=0.5947422385215759
I0302 10:40:18.519518 140525256554240 logging_writer.py:48] [547100] global_step=547100, grad_norm=4.429816246032715, loss=0.6045120358467102
I0302 10:40:52.294984 140522396051200 logging_writer.py:48] [547200] global_step=547200, grad_norm=4.786934852600098, loss=0.6107810735702515
I0302 10:41:26.079983 140525256554240 logging_writer.py:48] [547300] global_step=547300, grad_norm=4.171025276184082, loss=0.5255208611488342
I0302 10:41:59.899012 140522396051200 logging_writer.py:48] [547400] global_step=547400, grad_norm=4.7298970222473145, loss=0.6476225852966309
I0302 10:42:33.620940 140525256554240 logging_writer.py:48] [547500] global_step=547500, grad_norm=4.662783622741699, loss=0.674586296081543
I0302 10:43:07.403919 140522396051200 logging_writer.py:48] [547600] global_step=547600, grad_norm=4.457886219024658, loss=0.6521010994911194
I0302 10:43:41.240670 140525256554240 logging_writer.py:48] [547700] global_step=547700, grad_norm=5.0434746742248535, loss=0.660342812538147
I0302 10:44:15.041415 140522396051200 logging_writer.py:48] [547800] global_step=547800, grad_norm=4.489596366882324, loss=0.6354040503501892
I0302 10:44:48.863870 140525256554240 logging_writer.py:48] [547900] global_step=547900, grad_norm=5.050807476043701, loss=0.633782684803009
I0302 10:45:22.767452 140522396051200 logging_writer.py:48] [548000] global_step=548000, grad_norm=4.445603370666504, loss=0.6163444519042969
I0302 10:45:56.570457 140525256554240 logging_writer.py:48] [548100] global_step=548100, grad_norm=4.626519203186035, loss=0.6351891756057739
I0302 10:46:26.112339 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:46:32.122296 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:46:40.540434 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:46:42.824855 140688601454400 submission_runner.py:411] Time since start: 191597.90s, 	Step: 548189, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.1502102166414261, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0438426733016968, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8203867673873901, 'test/num_examples': 10000, 'score': 185192.69767308235, 'total_duration': 191597.89569282532, 'accumulated_submission_time': 185192.69767308235, 'accumulated_eval_time': 6356.251218318939, 'accumulated_logging_time': 30.08995032310486}
I0302 10:46:42.930512 140522554390272 logging_writer.py:48] [548189] accumulated_eval_time=6356.251218, accumulated_logging_time=30.089950, accumulated_submission_time=185192.697673, global_step=548189, preemption_count=0, score=185192.697673, test/accuracy=0.630900, test/loss=1.820387, test/num_examples=10000, total_duration=191597.895693, train/accuracy=0.959742, train/loss=0.150210, validation/accuracy=0.755180, validation/loss=1.043843, validation/num_examples=50000
I0302 10:46:46.987535 140523092305664 logging_writer.py:48] [548200] global_step=548200, grad_norm=4.654984474182129, loss=0.6138870120048523
I0302 10:47:20.714087 140522554390272 logging_writer.py:48] [548300] global_step=548300, grad_norm=4.403326988220215, loss=0.6726201772689819
I0302 10:47:54.471143 140523092305664 logging_writer.py:48] [548400] global_step=548400, grad_norm=4.56579065322876, loss=0.6053965091705322
I0302 10:48:28.268415 140522554390272 logging_writer.py:48] [548500] global_step=548500, grad_norm=4.3178629875183105, loss=0.6151087880134583
I0302 10:49:02.058182 140523092305664 logging_writer.py:48] [548600] global_step=548600, grad_norm=4.274548053741455, loss=0.6626989841461182
I0302 10:49:35.886649 140522554390272 logging_writer.py:48] [548700] global_step=548700, grad_norm=4.503199100494385, loss=0.60282301902771
I0302 10:50:09.687461 140523092305664 logging_writer.py:48] [548800] global_step=548800, grad_norm=4.380047798156738, loss=0.5872139930725098
I0302 10:50:43.475267 140522554390272 logging_writer.py:48] [548900] global_step=548900, grad_norm=4.631894111633301, loss=0.5975701808929443
I0302 10:51:17.299352 140523092305664 logging_writer.py:48] [549000] global_step=549000, grad_norm=5.076034069061279, loss=0.6769084334373474
I0302 10:51:51.092496 140522554390272 logging_writer.py:48] [549100] global_step=549100, grad_norm=4.239797115325928, loss=0.5665737390518188
I0302 10:52:24.880910 140523092305664 logging_writer.py:48] [549200] global_step=549200, grad_norm=4.562627792358398, loss=0.6776923537254333
I0302 10:52:58.646532 140522554390272 logging_writer.py:48] [549300] global_step=549300, grad_norm=4.7685675621032715, loss=0.7315896153450012
I0302 10:53:32.443047 140523092305664 logging_writer.py:48] [549400] global_step=549400, grad_norm=4.490370750427246, loss=0.6208683252334595
I0302 10:54:06.192930 140522554390272 logging_writer.py:48] [549500] global_step=549500, grad_norm=4.317859649658203, loss=0.5437712669372559
I0302 10:54:39.993165 140523092305664 logging_writer.py:48] [549600] global_step=549600, grad_norm=4.2182698249816895, loss=0.6163493394851685
I0302 10:55:12.902096 140688601454400 spec.py:321] Evaluating on the training split.
I0302 10:55:18.897515 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 10:55:27.556595 140688601454400 spec.py:349] Evaluating on the test split.
I0302 10:55:29.900244 140688601454400 submission_runner.py:411] Time since start: 192124.97s, 	Step: 549699, 	{'train/accuracy': 0.9620535373687744, 'train/loss': 0.14193937182426453, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.043861746788025, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8231457471847534, 'test/num_examples': 10000, 'score': 185702.6072921753, 'total_duration': 192124.97103381157, 'accumulated_submission_time': 185702.6072921753, 'accumulated_eval_time': 6373.249268293381, 'accumulated_logging_time': 30.20629596710205}
I0302 10:55:30.010753 140523092305664 logging_writer.py:48] [549699] accumulated_eval_time=6373.249268, accumulated_logging_time=30.206296, accumulated_submission_time=185702.607292, global_step=549699, preemption_count=0, score=185702.607292, test/accuracy=0.630700, test/loss=1.823146, test/num_examples=10000, total_duration=192124.971034, train/accuracy=0.962054, train/loss=0.141939, validation/accuracy=0.754780, validation/loss=1.043862, validation/num_examples=50000
I0302 10:55:30.697737 140525256554240 logging_writer.py:48] [549700] global_step=549700, grad_norm=4.490987300872803, loss=0.5910851359367371
I0302 10:56:04.485057 140523092305664 logging_writer.py:48] [549800] global_step=549800, grad_norm=4.546327114105225, loss=0.6029011011123657
I0302 10:56:38.244714 140525256554240 logging_writer.py:48] [549900] global_step=549900, grad_norm=4.702089786529541, loss=0.7065258622169495
I0302 10:57:12.013312 140523092305664 logging_writer.py:48] [550000] global_step=550000, grad_norm=4.527166843414307, loss=0.6851545572280884
I0302 10:57:45.990258 140525256554240 logging_writer.py:48] [550100] global_step=550100, grad_norm=4.276503562927246, loss=0.618398904800415
I0302 10:58:19.776956 140523092305664 logging_writer.py:48] [550200] global_step=550200, grad_norm=4.5470428466796875, loss=0.6429858207702637
I0302 10:58:53.566880 140525256554240 logging_writer.py:48] [550300] global_step=550300, grad_norm=4.15109920501709, loss=0.6415299773216248
I0302 10:59:27.354172 140523092305664 logging_writer.py:48] [550400] global_step=550400, grad_norm=4.372968673706055, loss=0.5371605157852173
I0302 11:00:01.140421 140525256554240 logging_writer.py:48] [550500] global_step=550500, grad_norm=4.609381198883057, loss=0.6209758520126343
I0302 11:00:34.917631 140523092305664 logging_writer.py:48] [550600] global_step=550600, grad_norm=4.567566871643066, loss=0.6307604312896729
I0302 11:01:08.700276 140525256554240 logging_writer.py:48] [550700] global_step=550700, grad_norm=4.5321221351623535, loss=0.6090604662895203
I0302 11:01:42.484443 140523092305664 logging_writer.py:48] [550800] global_step=550800, grad_norm=4.214084625244141, loss=0.5903391242027283
I0302 11:02:16.260814 140525256554240 logging_writer.py:48] [550900] global_step=550900, grad_norm=4.644328594207764, loss=0.6506634950637817
I0302 11:02:50.050751 140523092305664 logging_writer.py:48] [551000] global_step=551000, grad_norm=4.424436569213867, loss=0.5783208012580872
I0302 11:03:23.916072 140525256554240 logging_writer.py:48] [551100] global_step=551100, grad_norm=4.5116472244262695, loss=0.5999435186386108
I0302 11:03:57.709877 140523092305664 logging_writer.py:48] [551200] global_step=551200, grad_norm=4.563910484313965, loss=0.6206689476966858
I0302 11:04:00.216555 140688601454400 spec.py:321] Evaluating on the training split.
I0302 11:04:06.281355 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 11:04:14.887740 140688601454400 spec.py:349] Evaluating on the test split.
I0302 11:04:17.170052 140688601454400 submission_runner.py:411] Time since start: 192652.24s, 	Step: 551209, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.144424170255661, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.0437524318695068, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.823291301727295, 'test/num_examples': 10000, 'score': 186212.7490684986, 'total_duration': 192652.24084949493, 'accumulated_submission_time': 186212.7490684986, 'accumulated_eval_time': 6390.202693462372, 'accumulated_logging_time': 30.328657150268555}
I0302 11:04:17.280515 140522396051200 logging_writer.py:48] [551209] accumulated_eval_time=6390.202693, accumulated_logging_time=30.328657, accumulated_submission_time=186212.749068, global_step=551209, preemption_count=0, score=186212.749068, test/accuracy=0.631900, test/loss=1.823291, test/num_examples=10000, total_duration=192652.240849, train/accuracy=0.960818, train/loss=0.144424, validation/accuracy=0.755320, validation/loss=1.043752, validation/num_examples=50000
I0302 11:04:48.274873 140522404443904 logging_writer.py:48] [551300] global_step=551300, grad_norm=4.310854434967041, loss=0.5991352796554565
I0302 11:05:22.026337 140522396051200 logging_writer.py:48] [551400] global_step=551400, grad_norm=4.8437418937683105, loss=0.6630720496177673
I0302 11:05:55.786750 140522404443904 logging_writer.py:48] [551500] global_step=551500, grad_norm=4.285548686981201, loss=0.6285384297370911
I0302 11:06:29.525830 140522396051200 logging_writer.py:48] [551600] global_step=551600, grad_norm=4.398076057434082, loss=0.5631470084190369
I0302 11:07:03.319645 140522404443904 logging_writer.py:48] [551700] global_step=551700, grad_norm=4.470836639404297, loss=0.6828881502151489
I0302 11:07:37.109031 140522396051200 logging_writer.py:48] [551800] global_step=551800, grad_norm=4.7238664627075195, loss=0.5990339517593384
I0302 11:08:10.922061 140522404443904 logging_writer.py:48] [551900] global_step=551900, grad_norm=4.151755332946777, loss=0.5884426832199097
I0302 11:08:44.696313 140522396051200 logging_writer.py:48] [552000] global_step=552000, grad_norm=4.862148761749268, loss=0.6092366576194763
I0302 11:09:18.458921 140522404443904 logging_writer.py:48] [552100] global_step=552100, grad_norm=4.9162774085998535, loss=0.6359803080558777
I0302 11:09:52.309005 140522396051200 logging_writer.py:48] [552200] global_step=552200, grad_norm=4.4983696937561035, loss=0.5922428369522095
I0302 11:10:26.138256 140522404443904 logging_writer.py:48] [552300] global_step=552300, grad_norm=4.333548069000244, loss=0.6225787997245789
I0302 11:10:59.921820 140522396051200 logging_writer.py:48] [552400] global_step=552400, grad_norm=4.871983528137207, loss=0.5970786213874817
I0302 11:11:33.732022 140522404443904 logging_writer.py:48] [552500] global_step=552500, grad_norm=4.09126615524292, loss=0.6080590486526489
I0302 11:12:07.501509 140522396051200 logging_writer.py:48] [552600] global_step=552600, grad_norm=4.413490295410156, loss=0.6915076375007629
I0302 11:12:41.315994 140522404443904 logging_writer.py:48] [552700] global_step=552700, grad_norm=4.430405139923096, loss=0.5997553467750549
I0302 11:12:47.195645 140688601454400 spec.py:321] Evaluating on the training split.
I0302 11:12:53.421515 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 11:13:01.906151 140688601454400 spec.py:349] Evaluating on the test split.
I0302 11:13:04.254451 140688601454400 submission_runner.py:411] Time since start: 193179.33s, 	Step: 552719, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14806300401687622, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0442960262298584, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8228309154510498, 'test/num_examples': 10000, 'score': 186722.60253715515, 'total_duration': 193179.32529592514, 'accumulated_submission_time': 186722.60253715515, 'accumulated_eval_time': 6407.26146531105, 'accumulated_logging_time': 30.44870686531067}
I0302 11:13:04.364848 140525264946944 logging_writer.py:48] [552719] accumulated_eval_time=6407.261465, accumulated_logging_time=30.448707, accumulated_submission_time=186722.602537, global_step=552719, preemption_count=0, score=186722.602537, test/accuracy=0.630600, test/loss=1.822831, test/num_examples=10000, total_duration=193179.325296, train/accuracy=0.960977, train/loss=0.148063, validation/accuracy=0.755000, validation/loss=1.044296, validation/num_examples=50000
I0302 11:13:32.034695 140525273339648 logging_writer.py:48] [552800] global_step=552800, grad_norm=4.417169570922852, loss=0.606412410736084
I0302 11:14:05.790217 140525264946944 logging_writer.py:48] [552900] global_step=552900, grad_norm=4.321370601654053, loss=0.6098513007164001
I0302 11:14:39.562911 140525273339648 logging_writer.py:48] [553000] global_step=553000, grad_norm=4.656623840332031, loss=0.6330941915512085
I0302 11:15:13.349070 140525264946944 logging_writer.py:48] [553100] global_step=553100, grad_norm=4.2870612144470215, loss=0.6189090609550476
I0302 11:15:47.218528 140525273339648 logging_writer.py:48] [553200] global_step=553200, grad_norm=4.303844928741455, loss=0.5502896904945374
I0302 11:16:21.022179 140525264946944 logging_writer.py:48] [553300] global_step=553300, grad_norm=4.745962142944336, loss=0.669853925704956
I0302 11:16:54.783204 140525273339648 logging_writer.py:48] [553400] global_step=553400, grad_norm=4.507775783538818, loss=0.669330358505249
I0302 11:17:28.561766 140525264946944 logging_writer.py:48] [553500] global_step=553500, grad_norm=4.254233360290527, loss=0.5471448302268982
I0302 11:18:02.331354 140525273339648 logging_writer.py:48] [553600] global_step=553600, grad_norm=4.472580432891846, loss=0.6316345930099487
I0302 11:18:36.115971 140525264946944 logging_writer.py:48] [553700] global_step=553700, grad_norm=4.731915473937988, loss=0.7104291915893555
I0302 11:19:09.881546 140525273339648 logging_writer.py:48] [553800] global_step=553800, grad_norm=4.933739185333252, loss=0.5928114056587219
I0302 11:19:43.659641 140525264946944 logging_writer.py:48] [553900] global_step=553900, grad_norm=4.189310550689697, loss=0.6040059328079224
I0302 11:20:17.466166 140525273339648 logging_writer.py:48] [554000] global_step=554000, grad_norm=4.642489910125732, loss=0.5706652998924255
I0302 11:20:51.230714 140525264946944 logging_writer.py:48] [554100] global_step=554100, grad_norm=4.477551460266113, loss=0.6130784749984741
I0302 11:21:24.984606 140525273339648 logging_writer.py:48] [554200] global_step=554200, grad_norm=4.410250663757324, loss=0.6430320739746094
I0302 11:21:34.265513 140688601454400 spec.py:321] Evaluating on the training split.
I0302 11:21:40.237189 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 11:21:48.746787 140688601454400 spec.py:349] Evaluating on the test split.
I0302 11:21:51.083546 140688601454400 submission_runner.py:411] Time since start: 193706.15s, 	Step: 554229, 	{'train/accuracy': 0.9595423936843872, 'train/loss': 0.1502232849597931, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0441011190414429, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8223854303359985, 'test/num_examples': 10000, 'score': 187232.44098687172, 'total_duration': 193706.154392004, 'accumulated_submission_time': 187232.44098687172, 'accumulated_eval_time': 6424.07945227623, 'accumulated_logging_time': 30.569879293441772}
I0302 11:21:51.195281 140522404443904 logging_writer.py:48] [554229] accumulated_eval_time=6424.079452, accumulated_logging_time=30.569879, accumulated_submission_time=187232.440987, global_step=554229, preemption_count=0, score=187232.440987, test/accuracy=0.630200, test/loss=1.822385, test/num_examples=10000, total_duration=193706.154392, train/accuracy=0.959542, train/loss=0.150223, validation/accuracy=0.754980, validation/loss=1.044101, validation/num_examples=50000
I0302 11:22:15.522616 140522554390272 logging_writer.py:48] [554300] global_step=554300, grad_norm=4.308436870574951, loss=0.5686606168746948
I0302 11:22:49.195009 140522404443904 logging_writer.py:48] [554400] global_step=554400, grad_norm=4.468685150146484, loss=0.654457151889801
I0302 11:23:22.949811 140522554390272 logging_writer.py:48] [554500] global_step=554500, grad_norm=4.355767250061035, loss=0.6110824346542358
I0302 11:23:56.723887 140522404443904 logging_writer.py:48] [554600] global_step=554600, grad_norm=4.500843048095703, loss=0.5930163860321045
I0302 11:24:30.491291 140522554390272 logging_writer.py:48] [554700] global_step=554700, grad_norm=4.374265670776367, loss=0.5772315263748169
I0302 11:25:04.297192 140522404443904 logging_writer.py:48] [554800] global_step=554800, grad_norm=4.6089253425598145, loss=0.6206239461898804
I0302 11:25:38.062499 140522554390272 logging_writer.py:48] [554900] global_step=554900, grad_norm=4.733727931976318, loss=0.6051596403121948
I0302 11:26:11.852329 140522404443904 logging_writer.py:48] [555000] global_step=555000, grad_norm=4.600805282592773, loss=0.6392722129821777
I0302 11:26:45.619394 140522554390272 logging_writer.py:48] [555100] global_step=555100, grad_norm=4.418620586395264, loss=0.5767940282821655
I0302 11:27:19.387009 140522404443904 logging_writer.py:48] [555200] global_step=555200, grad_norm=4.970551013946533, loss=0.645538330078125
I0302 11:27:53.161034 140522554390272 logging_writer.py:48] [555300] global_step=555300, grad_norm=5.135437965393066, loss=0.6719330549240112
I0302 11:28:27.022253 140522404443904 logging_writer.py:48] [555400] global_step=555400, grad_norm=4.272007942199707, loss=0.5894612073898315
I0302 11:29:00.803896 140522554390272 logging_writer.py:48] [555500] global_step=555500, grad_norm=4.575135231018066, loss=0.6693966388702393
I0302 11:29:34.522808 140522404443904 logging_writer.py:48] [555600] global_step=555600, grad_norm=4.213107585906982, loss=0.6185876131057739
I0302 11:30:08.288376 140522554390272 logging_writer.py:48] [555700] global_step=555700, grad_norm=4.18372917175293, loss=0.6364038586616516
I0302 11:30:21.278065 140688601454400 spec.py:321] Evaluating on the training split.
I0302 11:30:27.326189 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 11:30:35.784724 140688601454400 spec.py:349] Evaluating on the test split.
I0302 11:30:38.037920 140688601454400 submission_runner.py:411] Time since start: 194233.11s, 	Step: 555740, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14735746383666992, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0433720350265503, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8212271928787231, 'test/num_examples': 10000, 'score': 187742.46044564247, 'total_duration': 194233.10874533653, 'accumulated_submission_time': 187742.46044564247, 'accumulated_eval_time': 6440.839239120483, 'accumulated_logging_time': 30.693721771240234}
I0302 11:30:38.146839 140525264946944 logging_writer.py:48] [555740] accumulated_eval_time=6440.839239, accumulated_logging_time=30.693722, accumulated_submission_time=187742.460446, global_step=555740, preemption_count=0, score=187742.460446, test/accuracy=0.631100, test/loss=1.821227, test/num_examples=10000, total_duration=194233.108745, train/accuracy=0.961217, train/loss=0.147357, validation/accuracy=0.755360, validation/loss=1.043372, validation/num_examples=50000
I0302 11:30:58.742544 140525273339648 logging_writer.py:48] [555800] global_step=555800, grad_norm=4.523127555847168, loss=0.6274389624595642
I0302 11:31:32.490046 140525264946944 logging_writer.py:48] [555900] global_step=555900, grad_norm=4.932854175567627, loss=0.6588336825370789
I0302 11:32:06.249738 140525273339648 logging_writer.py:48] [556000] global_step=556000, grad_norm=4.448929786682129, loss=0.5926146507263184
I0302 11:32:40.016808 140525264946944 logging_writer.py:48] [556100] global_step=556100, grad_norm=4.843297481536865, loss=0.6397958397865295
I0302 11:33:13.786105 140525273339648 logging_writer.py:48] [556200] global_step=556200, grad_norm=4.428829193115234, loss=0.598408043384552
I0302 11:33:47.565840 140525264946944 logging_writer.py:48] [556300] global_step=556300, grad_norm=4.430196285247803, loss=0.6595443487167358
I0302 11:34:21.520434 140525273339648 logging_writer.py:48] [556400] global_step=556400, grad_norm=4.541682720184326, loss=0.6217203736305237
I0302 11:34:55.269157 140525264946944 logging_writer.py:48] [556500] global_step=556500, grad_norm=4.553215980529785, loss=0.6449681520462036
I0302 11:35:29.052235 140525273339648 logging_writer.py:48] [556600] global_step=556600, grad_norm=4.461766719818115, loss=0.6812350749969482
I0302 11:36:02.839662 140525264946944 logging_writer.py:48] [556700] global_step=556700, grad_norm=4.691582679748535, loss=0.67414391040802
I0302 11:36:36.638308 140525273339648 logging_writer.py:48] [556800] global_step=556800, grad_norm=4.264453411102295, loss=0.5788480043411255
I0302 11:37:10.409261 140525264946944 logging_writer.py:48] [556900] global_step=556900, grad_norm=4.7441840171813965, loss=0.6145703792572021
I0302 11:37:44.182731 140525273339648 logging_writer.py:48] [557000] global_step=557000, grad_norm=4.930217266082764, loss=0.6075108051300049
I0302 11:38:17.974462 140525264946944 logging_writer.py:48] [557100] global_step=557100, grad_norm=4.487297058105469, loss=0.5911588668823242
I0302 11:38:51.743368 140525273339648 logging_writer.py:48] [557200] global_step=557200, grad_norm=4.4950642585754395, loss=0.7008686661720276
I0302 11:39:08.087507 140688601454400 spec.py:321] Evaluating on the training split.
I0302 11:39:14.110020 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 11:39:22.704319 140688601454400 spec.py:349] Evaluating on the test split.
I0302 11:39:24.970258 140688601454400 submission_runner.py:411] Time since start: 194760.04s, 	Step: 557250, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.15059484541416168, 'validation/accuracy': 0.755299985408783, 'validation/loss': 1.0440723896026611, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8224512338638306, 'test/num_examples': 10000, 'score': 188252.3400375843, 'total_duration': 194760.04110717773, 'accumulated_submission_time': 188252.3400375843, 'accumulated_eval_time': 6457.721947193146, 'accumulated_logging_time': 30.81244969367981}
I0302 11:39:25.081593 140522404443904 logging_writer.py:48] [557250] accumulated_eval_time=6457.721947, accumulated_logging_time=30.812450, accumulated_submission_time=188252.340038, global_step=557250, preemption_count=0, score=188252.340038, test/accuracy=0.631200, test/loss=1.822451, test/num_examples=10000, total_duration=194760.041107, train/accuracy=0.959901, train/loss=0.150595, validation/accuracy=0.755300, validation/loss=1.044072, validation/num_examples=50000
I0302 11:39:42.329945 140522554390272 logging_writer.py:48] [557300] global_step=557300, grad_norm=4.331146240234375, loss=0.6288481950759888
I0302 11:40:16.011898 140522404443904 logging_writer.py:48] [557400] global_step=557400, grad_norm=4.358328819274902, loss=0.5911141037940979
I0302 11:40:49.918859 140522554390272 logging_writer.py:48] [557500] global_step=557500, grad_norm=4.501460552215576, loss=0.6022624969482422
I0302 11:41:23.707055 140522404443904 logging_writer.py:48] [557600] global_step=557600, grad_norm=4.499728679656982, loss=0.5755295753479004
I0302 11:41:57.432558 140522554390272 logging_writer.py:48] [557700] global_step=557700, grad_norm=4.398586750030518, loss=0.6372330188751221
I0302 11:42:31.216647 140522404443904 logging_writer.py:48] [557800] global_step=557800, grad_norm=4.242748737335205, loss=0.6499345898628235
I0302 11:43:05.012883 140522554390272 logging_writer.py:48] [557900] global_step=557900, grad_norm=4.455419063568115, loss=0.6852443218231201
I0302 11:43:38.799783 140522404443904 logging_writer.py:48] [558000] global_step=558000, grad_norm=4.620554447174072, loss=0.6474652290344238
I0302 11:44:12.629938 140522554390272 logging_writer.py:48] [558100] global_step=558100, grad_norm=4.1052165031433105, loss=0.5759109258651733
I0302 11:44:46.412071 140522404443904 logging_writer.py:48] [558200] global_step=558200, grad_norm=4.242577075958252, loss=0.600837230682373
I0302 11:45:20.166644 140522554390272 logging_writer.py:48] [558300] global_step=558300, grad_norm=4.299840450286865, loss=0.6179587244987488
I0302 11:45:53.949258 140522404443904 logging_writer.py:48] [558400] global_step=558400, grad_norm=4.04229211807251, loss=0.6597950458526611
I0302 11:46:27.703042 140522554390272 logging_writer.py:48] [558500] global_step=558500, grad_norm=4.4263787269592285, loss=0.6274430751800537
I0302 11:47:01.584794 140522404443904 logging_writer.py:48] [558600] global_step=558600, grad_norm=4.3732380867004395, loss=0.5457704663276672
I0302 11:47:35.357534 140522554390272 logging_writer.py:48] [558700] global_step=558700, grad_norm=4.826638221740723, loss=0.6621221899986267
I0302 11:47:55.105568 140688601454400 spec.py:321] Evaluating on the training split.
I0302 11:48:01.203571 140688601454400 spec.py:333] Evaluating on the validation split.
I0302 11:48:09.644295 140688601454400 spec.py:349] Evaluating on the test split.
I0302 11:48:11.982843 140688601454400 submission_runner.py:411] Time since start: 195287.05s, 	Step: 558760, 	{'train/accuracy': 0.95804762840271, 'train/loss': 0.1525946706533432, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.043939232826233, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.822641134262085, 'test/num_examples': 10000, 'score': 188762.30083870888, 'total_duration': 195287.05368733406, 'accumulated_submission_time': 188762.30083870888, 'accumulated_eval_time': 6474.599181890488, 'accumulated_logging_time': 30.935455560684204}
I0302 11:48:12.093550 140522396051200 logging_writer.py:48] [558760] accumulated_eval_time=6474.599182, accumulated_logging_time=30.935456, accumulated_submission_time=188762.300839, global_step=558760, preemption_count=0, score=188762.300839, test/accuracy=0.630800, test/loss=1.822641, test/num_examples=10000, total_duration=195287.053687, train/accuracy=0.958048, train/loss=0.152595, validation/accuracy=0.754920, validation/loss=1.043939, validation/num_examples=50000
I0302 11:48:25.960972 140522404443904 logging_writer.py:48] [558800] global_step=558800, grad_norm=4.439610481262207, loss=0.6152316331863403
I0302 11:48:59.686081 140522396051200 logging_writer.py:48] [558900] global_step=558900, grad_norm=4.579306602478027, loss=0.635583758354187
I0302 11:49:33.459690 140522404443904 logging_writer.py:48] [559000] global_step=559000, grad_norm=4.819129467010498, loss=0.6271713376045227
I0302 11:50:07.239876 140522396051200 logging_writer.py:48] [559100] global_step=559100, grad_norm=4.154456615447998, loss=0.566011369228363
I0302 11:50:41.032796 140522404443904 logging_writer.py:48] [559200] global_step=559200, grad_norm=4.434042453765869, loss=0.5920621752738953
I0302 11:51:14.819361 140522396051200 logging_writer.py:48] [559300] global_step=559300, grad_norm=4.540704250335693, loss=0.6098957657814026
I0302 11:51:48.569845 140522404443904 logging_writer.py:48] [559400] global_step=559400, grad_norm=4.414008617401123, loss=0.6313202977180481
I0302 11:52:22.371255 140522396051200 logging_writer.py:48] [559500] global_step=559500, grad_norm=4.8882269859313965, loss=0.7174549102783203
I0302 11:52:34.101912 140522404443904 logging_writer.py:48] [559536] global_step=559536, preemption_count=0, score=189024.185817
I0302 11:52:34.588555 140688601454400 checkpoints.py:490] Saving checkpoint at step: 559536
I0302 11:52:36.263422 140688601454400 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax/trial_1/checkpoint_559536
I0302 11:52:36.288234 140688601454400 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_1/imagenet_resnet_jax/trial_1/checkpoint_559536.
I0302 11:52:37.147409 140688601454400 submission_runner.py:676] Final imagenet_resnet score: 189024.18581700325
