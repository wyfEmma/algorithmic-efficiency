python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_2 --overwrite=true --save_checkpoints=false --rng_seed=3630973847 --max_global_steps=399999 --tuning_ruleset=self 2>&1 | tee -a /logs/wmt_jax_03-12-2024-04-14-45.log
I0312 04:15:07.986014 140343717943104 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax because --overwrite was set.
I0312 04:15:07.988962 140343717943104 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax.
I0312 04:15:08.944143 140343717943104 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0312 04:15:08.944884 140343717943104 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0312 04:15:08.945017 140343717943104 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0312 04:15:09.869872 140343717943104 submission_runner.py:612] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax/trial_1.
I0312 04:15:10.068475 140343717943104 submission_runner.py:209] Initializing dataset.
I0312 04:15:10.080770 140343717943104 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:15:10.087477 140343717943104 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:15:10.241647 140343717943104 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:15:12.214544 140343717943104 submission_runner.py:220] Initializing model.
I0312 04:15:21.160763 140343717943104 submission_runner.py:262] Initializing optimizer.
I0312 04:15:22.156633 140343717943104 submission_runner.py:269] Initializing metrics bundle.
I0312 04:15:22.156822 140343717943104 submission_runner.py:287] Initializing checkpoint and logger.
I0312 04:15:22.157586 140343717943104 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax/trial_1 with prefix checkpoint_
I0312 04:15:22.157733 140343717943104 submission_runner.py:307] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax/trial_1/meta_data_0.json.
I0312 04:15:22.157933 140343717943104 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0312 04:15:22.157992 140343717943104 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0312 04:15:22.442727 140343717943104 logger_utils.py:220] Unable to record git information. Continuing without it.
I0312 04:15:22.708857 140343717943104 submission_runner.py:311] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax/trial_1/flags_0.json.
I0312 04:15:22.719012 140343717943104 submission_runner.py:321] Starting training loop.
I0312 04:16:01.028608 140178689611520 logging_writer.py:48] [0] global_step=0, grad_norm=4.971317291259766, loss=11.11501693725586
I0312 04:16:01.044714 140343717943104 spec.py:321] Evaluating on the training split.
I0312 04:16:01.048411 140343717943104 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:16:01.051251 140343717943104 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:16:01.090265 140343717943104 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:16:09.109571 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 04:21:06.248442 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 04:21:06.252264 140343717943104 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:21:06.259016 140343717943104 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:21:06.294180 140343717943104 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:21:13.156321 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 04:26:02.089699 140343717943104 spec.py:349] Evaluating on the test split.
I0312 04:26:02.092788 140343717943104 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:26:02.096559 140343717943104 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:26:02.143518 140343717943104 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:26:05.035049 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 04:30:53.624378 140343717943104 submission_runner.py:420] Time since start: 930.91s, 	Step: 1, 	{'train/accuracy': 0.0006190601852722466, 'train/loss': 11.111475944519043, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.119153022766113, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.12126636505127, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 38.32567739486694, 'total_duration': 930.9052810668945, 'accumulated_submission_time': 38.32567739486694, 'accumulated_eval_time': 892.5795657634735, 'accumulated_logging_time': 0}
I0312 04:30:53.644535 140174032873216 logging_writer.py:48] [1] accumulated_eval_time=892.579566, accumulated_logging_time=0, accumulated_submission_time=38.325677, global_step=1, preemption_count=0, score=38.325677, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.121266, test/num_examples=3003, total_duration=930.905281, train/accuracy=0.000619, train/bleu=0.000000, train/loss=11.111476, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.119153, validation/num_examples=3000
I0312 04:31:29.644834 140174024480512 logging_writer.py:48] [100] global_step=100, grad_norm=0.17473924160003662, loss=8.255207061767578
I0312 04:32:05.624936 140174032873216 logging_writer.py:48] [200] global_step=200, grad_norm=0.8806710839271545, loss=7.493631362915039
I0312 04:32:41.710265 140174024480512 logging_writer.py:48] [300] global_step=300, grad_norm=0.3187587261199951, loss=6.870786190032959
I0312 04:33:17.841609 140174032873216 logging_writer.py:48] [400] global_step=400, grad_norm=0.3404204249382019, loss=6.261580944061279
I0312 04:33:53.899036 140174024480512 logging_writer.py:48] [500] global_step=500, grad_norm=0.31365835666656494, loss=5.827999591827393
I0312 04:34:29.966428 140174032873216 logging_writer.py:48] [600] global_step=600, grad_norm=0.40159177780151367, loss=5.576822280883789
I0312 04:35:06.048508 140174024480512 logging_writer.py:48] [700] global_step=700, grad_norm=0.38505277037620544, loss=5.288134574890137
I0312 04:35:42.134708 140174032873216 logging_writer.py:48] [800] global_step=800, grad_norm=0.5153511166572571, loss=5.066572189331055
I0312 04:36:18.207996 140174024480512 logging_writer.py:48] [900] global_step=900, grad_norm=0.4545444846153259, loss=4.823561668395996
I0312 04:36:54.316874 140174032873216 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5963039398193359, loss=4.577527046203613
I0312 04:37:30.427968 140174024480512 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4669090509414673, loss=4.272845268249512
I0312 04:38:06.510589 140174032873216 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.48743531107902527, loss=3.987823486328125
I0312 04:38:42.567214 140174024480512 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5474598407745361, loss=4.049308776855469
I0312 04:39:18.659222 140174032873216 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.4928600490093231, loss=3.7997725009918213
I0312 04:39:54.741257 140174024480512 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5363072752952576, loss=3.579259157180786
I0312 04:40:30.819985 140174032873216 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5204704403877258, loss=3.5046682357788086
I0312 04:41:06.942715 140174024480512 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5932168960571289, loss=3.4855153560638428
I0312 04:41:43.023016 140174032873216 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5544502139091492, loss=3.3314762115478516
I0312 04:42:19.070063 140174024480512 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4438033103942871, loss=3.2393393516540527
I0312 04:42:55.127328 140174032873216 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.42903876304626465, loss=3.194066286087036
I0312 04:43:31.232069 140174024480512 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.36664989590644836, loss=3.0677881240844727
I0312 04:44:07.355768 140174032873216 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.540774941444397, loss=3.1059391498565674
I0312 04:44:43.429918 140174024480512 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.3610672056674957, loss=3.032392978668213
I0312 04:44:53.968487 140343717943104 spec.py:321] Evaluating on the training split.
I0312 04:44:56.993418 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 04:47:30.748077 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 04:47:33.470581 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 04:50:10.870065 140343717943104 spec.py:349] Evaluating on the test split.
I0312 04:50:13.606147 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 04:52:38.835132 140343717943104 submission_runner.py:420] Time since start: 2236.12s, 	Step: 2331, 	{'train/accuracy': 0.5102968215942383, 'train/loss': 2.8733136653900146, 'train/bleu': 21.65943524518221, 'validation/accuracy': 0.5102850198745728, 'validation/loss': 2.8761160373687744, 'validation/bleu': 17.68833473762671, 'validation/num_examples': 3000, 'test/accuracy': 0.5068154335021973, 'test/loss': 2.946174144744873, 'test/bleu': 16.345875563990155, 'test/num_examples': 3003, 'score': 878.5678820610046, 'total_duration': 2236.116005897522, 'accumulated_submission_time': 878.5678820610046, 'accumulated_eval_time': 1357.4461088180542, 'accumulated_logging_time': 0.029885053634643555}
I0312 04:52:38.852732 140174032873216 logging_writer.py:48] [2331] accumulated_eval_time=1357.446109, accumulated_logging_time=0.029885, accumulated_submission_time=878.567882, global_step=2331, preemption_count=0, score=878.567882, test/accuracy=0.506815, test/bleu=16.345876, test/loss=2.946174, test/num_examples=3003, total_duration=2236.116006, train/accuracy=0.510297, train/bleu=21.659435, train/loss=2.873314, validation/accuracy=0.510285, validation/bleu=17.688335, validation/loss=2.876116, validation/num_examples=3000
I0312 04:53:04.029147 140174024480512 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.34228384494781494, loss=2.9053328037261963
I0312 04:53:39.969263 140174032873216 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.42004361748695374, loss=2.899550199508667
I0312 04:54:15.991410 140174024480512 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.3739925026893616, loss=2.815847158432007
I0312 04:54:52.045496 140174032873216 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3542821407318115, loss=2.8294425010681152
I0312 04:55:28.152170 140174024480512 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.2945716679096222, loss=2.693711280822754
I0312 04:56:04.300152 140174032873216 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2411241978406906, loss=2.743170738220215
I0312 04:56:40.388434 140174024480512 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2983797788619995, loss=2.7556989192962646
I0312 04:57:16.449480 140174032873216 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.260001540184021, loss=2.6501033306121826
I0312 04:57:52.522886 140174024480512 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.23959846794605255, loss=2.6373040676116943
I0312 04:58:28.557101 140174032873216 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5042885541915894, loss=2.6760947704315186
I0312 04:59:04.631579 140174024480512 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.22397828102111816, loss=2.608280897140503
I0312 04:59:40.678142 140174032873216 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2231975793838501, loss=2.502680540084839
I0312 05:00:16.747791 140174024480512 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.21842165291309357, loss=2.6162452697753906
I0312 05:00:52.810825 140174032873216 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.19884370267391205, loss=2.4896111488342285
I0312 05:01:28.876403 140174024480512 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.22006623446941376, loss=2.472445249557495
I0312 05:02:04.945120 140174032873216 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.21548983454704285, loss=2.4288337230682373
I0312 05:02:40.969678 140174024480512 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.18380863964557648, loss=2.5786964893341064
I0312 05:03:17.016584 140174032873216 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.1850951462984085, loss=2.4093735218048096
I0312 05:03:53.080536 140174024480512 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.19215357303619385, loss=2.462735414505005
I0312 05:04:29.155119 140174032873216 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.19404597580432892, loss=2.4369611740112305
I0312 05:05:05.224169 140174024480512 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.18191270530223846, loss=2.407900333404541
I0312 05:05:41.306277 140174032873216 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.1795128583908081, loss=2.2511720657348633
I0312 05:06:17.390059 140174024480512 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.17488627135753632, loss=2.318326234817505
I0312 05:06:39.124281 140343717943104 spec.py:321] Evaluating on the training split.
I0312 05:06:42.142813 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:09:20.432331 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 05:09:23.146802 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:11:50.656248 140343717943104 spec.py:349] Evaluating on the test split.
I0312 05:11:53.378606 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:14:05.424872 140343717943104 submission_runner.py:420] Time since start: 3522.71s, 	Step: 4662, 	{'train/accuracy': 0.5730522871017456, 'train/loss': 2.2688934803009033, 'train/bleu': 27.330756374155918, 'validation/accuracy': 0.5891309380531311, 'validation/loss': 2.1536061763763428, 'validation/bleu': 23.586874302922432, 'validation/num_examples': 3000, 'test/accuracy': 0.5927139520645142, 'test/loss': 2.1307547092437744, 'test/bleu': 22.215112739406536, 'test/num_examples': 3003, 'score': 1718.7574768066406, 'total_duration': 3522.705801486969, 'accumulated_submission_time': 1718.7574768066406, 'accumulated_eval_time': 1803.7466485500336, 'accumulated_logging_time': 0.05802798271179199}
I0312 05:14:05.440290 140174032873216 logging_writer.py:48] [4662] accumulated_eval_time=1803.746649, accumulated_logging_time=0.058028, accumulated_submission_time=1718.757477, global_step=4662, preemption_count=0, score=1718.757477, test/accuracy=0.592714, test/bleu=22.215113, test/loss=2.130755, test/num_examples=3003, total_duration=3522.705801, train/accuracy=0.573052, train/bleu=27.330756, train/loss=2.268893, validation/accuracy=0.589131, validation/bleu=23.586874, validation/loss=2.153606, validation/num_examples=3000
I0312 05:14:19.463159 140174024480512 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1704208105802536, loss=2.3270905017852783
I0312 05:14:55.389591 140174032873216 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1786026656627655, loss=2.3114023208618164
I0312 05:15:31.372811 140174024480512 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.16864995658397675, loss=2.29374098777771
I0312 05:16:07.464060 140174032873216 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.16054949164390564, loss=2.294846296310425
I0312 05:16:43.497310 140174024480512 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.21337653696537018, loss=2.3537700176239014
I0312 05:17:19.545614 140174032873216 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.16260512173175812, loss=2.226245164871216
I0312 05:17:55.576626 140174024480512 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.14996391534805298, loss=2.289274215698242
I0312 05:18:31.652831 140174032873216 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1573239117860794, loss=2.2836387157440186
I0312 05:19:07.707310 140174024480512 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.15543892979621887, loss=2.2200419902801514
I0312 05:19:43.753357 140174032873216 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.21213094890117645, loss=2.2747297286987305
I0312 05:20:19.814181 140174024480512 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.15153878927230835, loss=2.235819101333618
I0312 05:20:55.903894 140174032873216 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.17221999168395996, loss=2.237994432449341
I0312 05:21:31.951861 140174024480512 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.15531985461711884, loss=2.2114155292510986
I0312 05:22:08.019238 140174032873216 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.14976972341537476, loss=2.2021937370300293
I0312 05:22:44.057801 140174024480512 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.183253675699234, loss=2.216001033782959
I0312 05:23:20.112851 140174032873216 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.177773118019104, loss=2.2276089191436768
I0312 05:23:56.205624 140174024480512 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.15728767216205597, loss=2.149390935897827
I0312 05:24:32.282317 140174032873216 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.13939669728279114, loss=2.2183499336242676
I0312 05:25:08.311492 140174024480512 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.15935975313186646, loss=2.216059684753418
I0312 05:25:44.344613 140174032873216 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.16024014353752136, loss=2.283047676086426
I0312 05:26:20.429214 140174024480512 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.16052064299583435, loss=2.1406052112579346
I0312 05:26:56.497040 140174032873216 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1521300971508026, loss=2.165273666381836
I0312 05:27:32.549363 140174024480512 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.14633965492248535, loss=2.145885705947876
I0312 05:28:05.439796 140343717943104 spec.py:321] Evaluating on the training split.
I0312 05:28:08.455357 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:30:46.204424 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 05:30:48.926303 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:33:16.273594 140343717943104 spec.py:349] Evaluating on the test split.
I0312 05:33:18.982910 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:35:36.051356 140343717943104 submission_runner.py:420] Time since start: 4813.33s, 	Step: 6993, 	{'train/accuracy': 0.606548547744751, 'train/loss': 1.9968069791793823, 'train/bleu': 29.43806395732164, 'validation/accuracy': 0.6165453791618347, 'validation/loss': 1.9260854721069336, 'validation/bleu': 25.35945736848895, 'validation/num_examples': 3000, 'test/accuracy': 0.6202428936958313, 'test/loss': 1.8929826021194458, 'test/bleu': 23.74940006571612, 'test/num_examples': 3003, 'score': 2558.678544282913, 'total_duration': 4813.3322784900665, 'accumulated_submission_time': 2558.678544282913, 'accumulated_eval_time': 2254.3581516742706, 'accumulated_logging_time': 0.08297038078308105}
I0312 05:35:36.065708 140174032873216 logging_writer.py:48] [6993] accumulated_eval_time=2254.358152, accumulated_logging_time=0.082970, accumulated_submission_time=2558.678544, global_step=6993, preemption_count=0, score=2558.678544, test/accuracy=0.620243, test/bleu=23.749400, test/loss=1.892983, test/num_examples=3003, total_duration=4813.332278, train/accuracy=0.606549, train/bleu=29.438064, train/loss=1.996807, validation/accuracy=0.616545, validation/bleu=25.359457, validation/loss=1.926085, validation/num_examples=3000
I0312 05:35:38.950001 140174024480512 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.14471635222434998, loss=2.0966882705688477
I0312 05:36:14.845132 140174032873216 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.17276631295681, loss=2.1198835372924805
I0312 05:36:50.787979 140174024480512 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.15143252909183502, loss=2.180427074432373
I0312 05:37:26.771117 140174032873216 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.3148694932460785, loss=2.0454776287078857
I0312 05:38:02.778142 140174024480512 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.16660337150096893, loss=2.0636861324310303
I0312 05:38:38.846006 140174032873216 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.16457700729370117, loss=2.0036239624023438
I0312 05:39:14.917309 140174024480512 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.2385954111814499, loss=2.1124446392059326
I0312 05:39:50.933003 140174032873216 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.15180601179599762, loss=2.180150270462036
I0312 05:40:27.044412 140174024480512 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.19357365369796753, loss=2.0441720485687256
I0312 05:41:03.120793 140174032873216 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.2540154457092285, loss=2.067610502243042
I0312 05:41:39.132492 140174024480512 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.1607658565044403, loss=2.0332484245300293
I0312 05:42:15.154323 140174032873216 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.16683132946491241, loss=2.1173324584960938
I0312 05:42:51.195746 140174024480512 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17695367336273193, loss=2.061000347137451
I0312 05:43:27.228673 140174032873216 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.19382180273532867, loss=2.0577452182769775
I0312 05:44:03.286185 140174024480512 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.17446106672286987, loss=2.125330924987793
I0312 05:44:39.332716 140174032873216 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.15022395551204681, loss=2.080540418624878
I0312 05:45:15.384735 140174024480512 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.170714870095253, loss=2.067807197570801
I0312 05:45:51.451061 140174032873216 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.17201286554336548, loss=1.9627208709716797
I0312 05:46:27.458448 140174024480512 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.16169309616088867, loss=1.9637395143508911
I0312 05:47:03.515034 140174032873216 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.15943530201911926, loss=2.0428926944732666
I0312 05:47:39.584512 140174024480512 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15826420485973358, loss=2.040125608444214
I0312 05:48:15.599599 140174032873216 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.28326863050460815, loss=2.0732388496398926
I0312 05:48:51.616882 140174024480512 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.24306133389472961, loss=2.084115982055664
I0312 05:49:27.631608 140174032873216 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.2098219394683838, loss=2.020941972732544
I0312 05:49:36.348297 140343717943104 spec.py:321] Evaluating on the training split.
I0312 05:49:39.361235 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:52:25.550647 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 05:52:28.262251 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:54:52.606587 140343717943104 spec.py:349] Evaluating on the test split.
I0312 05:54:55.321548 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 05:57:11.229500 140343717943104 submission_runner.py:420] Time since start: 6108.51s, 	Step: 9326, 	{'train/accuracy': 0.6131796836853027, 'train/loss': 1.9451521635055542, 'train/bleu': 29.644693604374318, 'validation/accuracy': 0.6309779286384583, 'validation/loss': 1.8251482248306274, 'validation/bleu': 26.356215868632788, 'validation/num_examples': 3000, 'test/accuracy': 0.6364766955375671, 'test/loss': 1.7680728435516357, 'test/bleu': 24.992818880390896, 'test/num_examples': 3003, 'score': 3398.8785541057587, 'total_duration': 6108.510417699814, 'accumulated_submission_time': 3398.8785541057587, 'accumulated_eval_time': 2709.2392904758453, 'accumulated_logging_time': 0.10672211647033691}
I0312 05:57:11.244264 140174024480512 logging_writer.py:48] [9326] accumulated_eval_time=2709.239290, accumulated_logging_time=0.106722, accumulated_submission_time=3398.878554, global_step=9326, preemption_count=0, score=3398.878554, test/accuracy=0.636477, test/bleu=24.992819, test/loss=1.768073, test/num_examples=3003, total_duration=6108.510418, train/accuracy=0.613180, train/bleu=29.644694, train/loss=1.945152, validation/accuracy=0.630978, validation/bleu=26.356216, validation/loss=1.825148, validation/num_examples=3000
I0312 05:57:38.135529 140174032873216 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.17275899648666382, loss=2.0091793537139893
I0312 05:58:14.036305 140174024480512 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.15586133301258087, loss=2.0213687419891357
I0312 05:58:50.015523 140174032873216 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.16982784867286682, loss=2.019777774810791
I0312 05:59:26.060625 140174024480512 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15531961619853973, loss=1.9895656108856201
I0312 06:00:02.160794 140174032873216 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.17986558377742767, loss=2.0409698486328125
I0312 06:00:38.171653 140174024480512 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.20827651023864746, loss=1.9868844747543335
I0312 06:01:14.210656 140174032873216 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.17502284049987793, loss=2.0528147220611572
I0312 06:01:50.228785 140174024480512 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.981083869934082, loss=3.964524984359741
I0312 06:02:26.245931 140174032873216 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.24335871636867523, loss=2.034851312637329
I0312 06:03:02.265858 140174024480512 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.15544477105140686, loss=2.050798177719116
I0312 06:03:38.268507 140174032873216 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.16959801316261292, loss=1.9611726999282837
I0312 06:04:14.298797 140174024480512 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.15140452980995178, loss=2.0697708129882812
I0312 06:04:50.332466 140174032873216 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.16318044066429138, loss=1.9581210613250732
I0312 06:05:26.372424 140174024480512 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.15956515073776245, loss=2.024702787399292
I0312 06:06:02.401767 140174032873216 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1664910614490509, loss=2.067329168319702
I0312 06:06:38.409352 140174024480512 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17335331439971924, loss=1.962673306465149
I0312 06:07:14.434530 140174032873216 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.18436485528945923, loss=1.9955806732177734
I0312 06:07:50.480444 140174024480512 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.17096415162086487, loss=1.9544175863265991
I0312 06:08:26.513285 140174032873216 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.176912322640419, loss=1.938102126121521
I0312 06:09:02.511928 140174024480512 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1983007788658142, loss=2.0314724445343018
I0312 06:09:38.514986 140174032873216 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1732325255870819, loss=1.9289681911468506
I0312 06:10:14.549653 140174024480512 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1924913376569748, loss=2.0210695266723633
I0312 06:10:50.623312 140174032873216 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1619088053703308, loss=1.925803542137146
I0312 06:11:11.253319 140343717943104 spec.py:321] Evaluating on the training split.
I0312 06:11:14.257980 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:14:08.749154 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 06:14:11.461668 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:16:36.337504 140343717943104 spec.py:349] Evaluating on the test split.
I0312 06:16:39.054691 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:18:47.689918 140343717943104 submission_runner.py:420] Time since start: 7404.97s, 	Step: 11659, 	{'train/accuracy': 0.616400420665741, 'train/loss': 1.916278600692749, 'train/bleu': 29.702261241348175, 'validation/accuracy': 0.6364831328392029, 'validation/loss': 1.7651243209838867, 'validation/bleu': 26.524825522377707, 'validation/num_examples': 3000, 'test/accuracy': 0.6424728631973267, 'test/loss': 1.711891770362854, 'test/bleu': 25.21233941447844, 'test/num_examples': 3003, 'score': 4238.8062624931335, 'total_duration': 7404.970848798752, 'accumulated_submission_time': 4238.8062624931335, 'accumulated_eval_time': 3165.6758439540863, 'accumulated_logging_time': 0.13177037239074707}
I0312 06:18:47.706748 140174024480512 logging_writer.py:48] [11659] accumulated_eval_time=3165.675844, accumulated_logging_time=0.131770, accumulated_submission_time=4238.806262, global_step=11659, preemption_count=0, score=4238.806262, test/accuracy=0.642473, test/bleu=25.212339, test/loss=1.711892, test/num_examples=3003, total_duration=7404.970849, train/accuracy=0.616400, train/bleu=29.702261, train/loss=1.916279, validation/accuracy=0.636483, validation/bleu=26.524826, validation/loss=1.765124, validation/num_examples=3000
I0312 06:19:02.800624 140174032873216 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.15551821887493134, loss=2.017218589782715
I0312 06:19:38.732023 140174024480512 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1503525823354721, loss=1.9220945835113525
I0312 06:20:14.690961 140174032873216 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.19101475179195404, loss=1.976414442062378
I0312 06:20:50.720318 140174024480512 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.19950906932353973, loss=2.0021190643310547
I0312 06:21:26.736217 140174032873216 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.18404987454414368, loss=1.8656104803085327
I0312 06:22:02.719208 140174024480512 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.20200957357883453, loss=1.9983819723129272
I0312 06:22:38.756560 140174032873216 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1560540646314621, loss=1.9384572505950928
I0312 06:23:14.802021 140174024480512 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2225750982761383, loss=1.8988553285598755
I0312 06:23:50.818579 140174032873216 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.28939273953437805, loss=2.0021958351135254
I0312 06:24:26.862506 140174024480512 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.18149228394031525, loss=1.9986844062805176
I0312 06:25:02.937170 140174032873216 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.23209112882614136, loss=1.9137418270111084
I0312 06:25:39.016373 140174024480512 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1645023375749588, loss=1.9688109159469604
I0312 06:26:15.056303 140174032873216 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.1986365169286728, loss=2.015049457550049
I0312 06:26:51.086646 140174024480512 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.16807889938354492, loss=1.948562741279602
I0312 06:27:27.149941 140174032873216 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1704930067062378, loss=1.915921688079834
I0312 06:28:03.157833 140174024480512 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.17794887721538544, loss=1.9764200448989868
I0312 06:28:39.155362 140174032873216 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.23389273881912231, loss=1.9369066953659058
I0312 06:29:15.143135 140174024480512 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.300861656665802, loss=1.9596134424209595
I0312 06:29:51.147640 140174032873216 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16770301759243011, loss=1.9618492126464844
I0312 06:30:27.230715 140174024480512 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2000802755355835, loss=1.8864142894744873
I0312 06:31:03.275155 140174032873216 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.19207948446273804, loss=1.9161821603775024
I0312 06:31:39.308042 140174024480512 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.18045958876609802, loss=2.011087656021118
I0312 06:32:15.319475 140174032873216 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.19927452504634857, loss=1.9252923727035522
I0312 06:32:47.810944 140343717943104 spec.py:321] Evaluating on the training split.
I0312 06:32:50.821096 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:35:52.160586 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 06:35:54.880586 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:38:31.023103 140343717943104 spec.py:349] Evaluating on the test split.
I0312 06:38:33.732620 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:41:03.014663 140343717943104 submission_runner.py:420] Time since start: 8740.30s, 	Step: 13992, 	{'train/accuracy': 0.6251095533370972, 'train/loss': 1.8352748155593872, 'train/bleu': 29.719058614662316, 'validation/accuracy': 0.6442325711250305, 'validation/loss': 1.7129203081130981, 'validation/bleu': 27.30846081869481, 'validation/num_examples': 3000, 'test/accuracy': 0.6517343521118164, 'test/loss': 1.6489189863204956, 'test/bleu': 25.849126741883687, 'test/num_examples': 3003, 'score': 5078.8299243450165, 'total_duration': 8740.295587062836, 'accumulated_submission_time': 5078.8299243450165, 'accumulated_eval_time': 3660.879511117935, 'accumulated_logging_time': 0.15905261039733887}
I0312 06:41:03.030112 140174024480512 logging_writer.py:48] [13992] accumulated_eval_time=3660.879511, accumulated_logging_time=0.159053, accumulated_submission_time=5078.829924, global_step=13992, preemption_count=0, score=5078.829924, test/accuracy=0.651734, test/bleu=25.849127, test/loss=1.648919, test/num_examples=3003, total_duration=8740.295587, train/accuracy=0.625110, train/bleu=29.719059, train/loss=1.835275, validation/accuracy=0.644233, validation/bleu=27.308461, validation/loss=1.712920, validation/num_examples=3000
I0312 06:41:06.269089 140174032873216 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1969333440065384, loss=1.831770420074463
I0312 06:41:42.148857 140174024480512 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.25680288672447205, loss=1.9532959461212158
I0312 06:42:18.116409 140174032873216 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2086784690618515, loss=2.0119669437408447
I0312 06:42:54.112344 140174024480512 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.16443926095962524, loss=1.9504131078720093
I0312 06:43:30.128437 140174032873216 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.24800501763820648, loss=1.8499006032943726
I0312 06:44:06.168353 140174024480512 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.18601861596107483, loss=1.8599497079849243
I0312 06:44:42.324922 140174032873216 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.1952708214521408, loss=1.8775204420089722
I0312 06:45:18.507999 140174024480512 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.21222595870494843, loss=1.9041310548782349
I0312 06:45:54.512620 140174032873216 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2077767550945282, loss=2.0337893962860107
I0312 06:46:30.523956 140174024480512 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.19153904914855957, loss=1.9299520254135132
I0312 06:47:06.530707 140174032873216 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.1784730702638626, loss=1.98300039768219
I0312 06:47:42.554489 140174024480512 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.18983453512191772, loss=1.9188326597213745
I0312 06:48:18.581946 140174032873216 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.193741574883461, loss=1.923961877822876
I0312 06:48:54.626049 140174024480512 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.1828833371400833, loss=1.8778839111328125
I0312 06:49:30.668604 140174032873216 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.22373174130916595, loss=1.8775619268417358
I0312 06:50:06.709792 140174024480512 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.24434205889701843, loss=1.8744697570800781
I0312 06:50:42.746503 140174032873216 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.20686805248260498, loss=1.793493390083313
I0312 06:51:18.789203 140174024480512 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1875944286584854, loss=1.8985028266906738
I0312 06:51:54.855180 140174032873216 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2623019516468048, loss=1.806244969367981
I0312 06:52:30.885395 140174024480512 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.20532262325286865, loss=1.9705746173858643
I0312 06:53:06.939279 140174032873216 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.20743465423583984, loss=1.9262620210647583
I0312 06:53:42.975057 140174024480512 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.19935519993305206, loss=1.906488060951233
I0312 06:54:19.015729 140174032873216 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.18888434767723083, loss=1.8348402976989746
I0312 06:54:55.044948 140174024480512 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.22438329458236694, loss=1.932952880859375
I0312 06:55:03.059604 140343717943104 spec.py:321] Evaluating on the training split.
I0312 06:55:06.075793 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 06:58:03.988713 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 06:58:06.698827 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:00:35.484914 140343717943104 spec.py:349] Evaluating on the test split.
I0312 07:00:38.201567 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:03:04.749911 140343717943104 submission_runner.py:420] Time since start: 10062.03s, 	Step: 16324, 	{'train/accuracy': 0.6289481520652771, 'train/loss': 1.8222870826721191, 'train/bleu': 30.144586752731794, 'validation/accuracy': 0.6479398608207703, 'validation/loss': 1.6811655759811401, 'validation/bleu': 27.3637250719459, 'validation/num_examples': 3000, 'test/accuracy': 0.6576956510543823, 'test/loss': 1.614374041557312, 'test/bleu': 26.30162304760944, 'test/num_examples': 3003, 'score': 5918.779824733734, 'total_duration': 10062.03084230423, 'accumulated_submission_time': 5918.779824733734, 'accumulated_eval_time': 4142.569768667221, 'accumulated_logging_time': 0.1852412223815918}
I0312 07:03:04.766352 140174032873216 logging_writer.py:48] [16324] accumulated_eval_time=4142.569769, accumulated_logging_time=0.185241, accumulated_submission_time=5918.779825, global_step=16324, preemption_count=0, score=5918.779825, test/accuracy=0.657696, test/bleu=26.301623, test/loss=1.614374, test/num_examples=3003, total_duration=10062.030842, train/accuracy=0.628948, train/bleu=30.144587, train/loss=1.822287, validation/accuracy=0.647940, validation/bleu=27.363725, validation/loss=1.681166, validation/num_examples=3000
I0312 07:03:32.405040 140174024480512 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.25757327675819397, loss=1.9323071241378784
I0312 07:04:08.365478 140174032873216 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.17982935905456543, loss=1.8971621990203857
I0312 07:04:44.378071 140174024480512 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.16904404759407043, loss=1.8361574411392212
I0312 07:05:20.412914 140174032873216 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.18432314693927765, loss=1.925376296043396
I0312 07:05:56.420528 140174024480512 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.21076689660549164, loss=1.9255270957946777
I0312 07:06:32.431328 140174032873216 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2110159546136856, loss=1.9487608671188354
I0312 07:07:08.475296 140174024480512 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.1938343644142151, loss=1.8927586078643799
I0312 07:07:44.487944 140174032873216 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.19650863111019135, loss=1.9962692260742188
I0312 07:08:20.531958 140174024480512 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.19330379366874695, loss=1.858691930770874
I0312 07:08:56.562700 140174032873216 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.18127962946891785, loss=1.8810750246047974
I0312 07:09:32.602254 140174024480512 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.18531560897827148, loss=1.92533540725708
I0312 07:10:08.658984 140174032873216 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.18550832569599152, loss=1.8516833782196045
I0312 07:10:44.695954 140174024480512 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.7489784955978394, loss=1.997820496559143
I0312 07:11:20.758123 140174032873216 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2061815857887268, loss=1.8747588396072388
I0312 07:11:56.784010 140174024480512 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.17613570392131805, loss=1.8989137411117554
I0312 07:12:32.810763 140174032873216 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.19284534454345703, loss=1.9276870489120483
I0312 07:13:08.852504 140174024480512 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.21118685603141785, loss=1.9733245372772217
I0312 07:13:44.909630 140174032873216 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.1894332766532898, loss=1.8489501476287842
I0312 07:14:20.939974 140174024480512 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7105435132980347, loss=1.9229342937469482
I0312 07:14:56.973200 140174032873216 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.18089227378368378, loss=1.8650063276290894
I0312 07:15:33.052868 140174024480512 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.19099578261375427, loss=1.9607583284378052
I0312 07:16:09.085007 140174032873216 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2017296999692917, loss=1.8884280920028687
I0312 07:16:45.108951 140174024480512 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.22619368135929108, loss=1.8479206562042236
I0312 07:17:05.005957 140343717943104 spec.py:321] Evaluating on the training split.
I0312 07:17:08.017564 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:20:38.361497 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 07:20:41.075962 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:22:55.721607 140343717943104 spec.py:349] Evaluating on the test split.
I0312 07:22:58.439587 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:25:19.857679 140343717943104 submission_runner.py:420] Time since start: 11397.14s, 	Step: 18657, 	{'train/accuracy': 0.6277276873588562, 'train/loss': 1.8295557498931885, 'train/bleu': 30.65094670772645, 'validation/accuracy': 0.6513124108314514, 'validation/loss': 1.6622154712677002, 'validation/bleu': 27.49430518378227, 'validation/num_examples': 3000, 'test/accuracy': 0.660449743270874, 'test/loss': 1.6002322435379028, 'test/bleu': 26.631281905157557, 'test/num_examples': 3003, 'score': 6758.940435886383, 'total_duration': 11397.138581991196, 'accumulated_submission_time': 6758.940435886383, 'accumulated_eval_time': 4637.421412944794, 'accumulated_logging_time': 0.21120810508728027}
I0312 07:25:19.877236 140174032873216 logging_writer.py:48] [18657] accumulated_eval_time=4637.421413, accumulated_logging_time=0.211208, accumulated_submission_time=6758.940436, global_step=18657, preemption_count=0, score=6758.940436, test/accuracy=0.660450, test/bleu=26.631282, test/loss=1.600232, test/num_examples=3003, total_duration=11397.138582, train/accuracy=0.627728, train/bleu=30.650947, train/loss=1.829556, validation/accuracy=0.651312, validation/bleu=27.494305, validation/loss=1.662215, validation/num_examples=3000
I0312 07:25:35.709442 140174024480512 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.18722859025001526, loss=1.8153722286224365
I0312 07:26:11.632565 140174032873216 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.19304277002811432, loss=1.907247543334961
I0312 07:26:47.615165 140174024480512 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.18872807919979095, loss=1.8391640186309814
I0312 07:27:23.577097 140174032873216 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.16861452162265778, loss=1.8323835134506226
I0312 07:27:59.593975 140174024480512 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.22462768852710724, loss=1.885810375213623
I0312 07:28:35.619642 140174032873216 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.17812702059745789, loss=1.8458576202392578
I0312 07:29:11.650166 140174024480512 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.18946573138237, loss=1.8158440589904785
I0312 07:29:47.687119 140174032873216 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.24319827556610107, loss=1.8536978960037231
I0312 07:30:23.707402 140174024480512 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2195425182580948, loss=1.865050196647644
I0312 07:30:59.761290 140174032873216 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2030474841594696, loss=1.8227241039276123
I0312 07:31:35.800301 140174024480512 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.18854309618473053, loss=1.8427674770355225
I0312 07:32:11.863874 140174032873216 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.21480801701545715, loss=1.7751421928405762
I0312 07:32:47.910888 140174024480512 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.2744835317134857, loss=1.842241883277893
I0312 07:33:23.972516 140174032873216 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.17144381999969482, loss=1.8597712516784668
I0312 07:33:59.983844 140174024480512 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.21100373566150665, loss=1.871552586555481
I0312 07:34:36.064940 140174032873216 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.2144833505153656, loss=1.8968955278396606
I0312 07:35:12.098766 140174024480512 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.2091303914785385, loss=1.8289964199066162
I0312 07:35:48.143433 140174032873216 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.17574554681777954, loss=1.903995156288147
I0312 07:36:24.190031 140174024480512 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.191559836268425, loss=1.8373429775238037
I0312 07:37:00.225128 140174032873216 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.17176489531993866, loss=1.8607747554779053
I0312 07:37:36.321148 140174024480512 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.21466940641403198, loss=1.8639510869979858
I0312 07:38:12.363296 140174032873216 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.2721157670021057, loss=1.857270359992981
I0312 07:38:48.393369 140174024480512 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.22497858107089996, loss=1.8717637062072754
I0312 07:39:20.191655 140343717943104 spec.py:321] Evaluating on the training split.
I0312 07:39:23.215949 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:42:06.138943 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 07:42:08.844467 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:44:43.871740 140343717943104 spec.py:349] Evaluating on the test split.
I0312 07:44:46.588170 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 07:47:12.209458 140343717943104 submission_runner.py:420] Time since start: 12709.49s, 	Step: 20990, 	{'train/accuracy': 0.6352289915084839, 'train/loss': 1.7688525915145874, 'train/bleu': 30.791125978727298, 'validation/accuracy': 0.6497625708580017, 'validation/loss': 1.6578360795974731, 'validation/bleu': 27.406212220377967, 'validation/num_examples': 3000, 'test/accuracy': 0.6596363186836243, 'test/loss': 1.5901591777801514, 'test/bleu': 26.523374816812826, 'test/num_examples': 3003, 'score': 7599.17161488533, 'total_duration': 12709.490337610245, 'accumulated_submission_time': 7599.17161488533, 'accumulated_eval_time': 5109.4391322135925, 'accumulated_logging_time': 0.24138903617858887}
I0312 07:47:12.230159 140174032873216 logging_writer.py:48] [20990] accumulated_eval_time=5109.439132, accumulated_logging_time=0.241389, accumulated_submission_time=7599.171615, global_step=20990, preemption_count=0, score=7599.171615, test/accuracy=0.659636, test/bleu=26.523375, test/loss=1.590159, test/num_examples=3003, total_duration=12709.490338, train/accuracy=0.635229, train/bleu=30.791126, train/loss=1.768853, validation/accuracy=0.649763, validation/bleu=27.406212, validation/loss=1.657836, validation/num_examples=3000
I0312 07:47:16.189467 140174024480512 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.31140807271003723, loss=1.8961323499679565
I0312 07:47:52.058045 140174032873216 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.27674147486686707, loss=1.8132002353668213
I0312 07:48:28.000787 140174024480512 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.2088385373353958, loss=1.9155759811401367
I0312 07:49:03.971781 140174032873216 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.23532047867774963, loss=1.8775198459625244
I0312 07:49:40.003742 140174024480512 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.24034284055233002, loss=1.8529726266860962
I0312 07:50:16.058947 140174032873216 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.17399881780147552, loss=1.9147576093673706
I0312 07:50:52.056172 140174024480512 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.22028277814388275, loss=1.9059070348739624
I0312 07:51:28.079305 140174032873216 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.21811889111995697, loss=1.8280978202819824
I0312 07:52:04.114802 140174024480512 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.1913231909275055, loss=1.7924894094467163
I0312 07:52:40.193000 140174032873216 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.1833808720111847, loss=1.879492998123169
I0312 07:53:16.280796 140174024480512 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2957231104373932, loss=2.024989366531372
I0312 07:53:52.356888 140174032873216 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1956443190574646, loss=1.8629696369171143
I0312 07:54:28.398749 140174024480512 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.18070432543754578, loss=1.8616799116134644
I0312 07:55:04.427752 140174032873216 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18838083744049072, loss=1.8527138233184814
I0312 07:55:40.445443 140174024480512 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.20149680972099304, loss=1.9144564867019653
I0312 07:56:16.478890 140174032873216 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.18607120215892792, loss=1.897862434387207
I0312 07:56:52.553404 140174024480512 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.20886781811714172, loss=1.815643072128296
I0312 07:57:28.549244 140174032873216 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.19421637058258057, loss=1.9117318391799927
I0312 07:58:04.565920 140174024480512 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.22258463501930237, loss=1.8250460624694824
I0312 07:58:40.594110 140174032873216 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.20304836332798004, loss=1.939510703086853
I0312 07:59:16.643802 140174024480512 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.20535200834274292, loss=1.9172600507736206
I0312 07:59:52.667061 140174032873216 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2286224067211151, loss=1.7829385995864868
I0312 08:00:28.705659 140174024480512 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.1975976675748825, loss=1.8663705587387085
I0312 08:01:04.734097 140174032873216 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3025003671646118, loss=1.8570719957351685
I0312 08:01:12.378391 140343717943104 spec.py:321] Evaluating on the training split.
I0312 08:01:15.385694 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:04:20.079649 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 08:04:22.779200 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:06:50.596307 140343717943104 spec.py:349] Evaluating on the test split.
I0312 08:06:53.303763 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:09:34.858690 140343717943104 submission_runner.py:420] Time since start: 14052.14s, 	Step: 23323, 	{'train/accuracy': 0.6351427435874939, 'train/loss': 1.7711073160171509, 'train/bleu': 31.04816736219118, 'validation/accuracy': 0.6542758345603943, 'validation/loss': 1.6312884092330933, 'validation/bleu': 28.171750929722943, 'validation/num_examples': 3000, 'test/accuracy': 0.6660391688346863, 'test/loss': 1.55837881565094, 'test/bleu': 27.10138290676962, 'test/num_examples': 3003, 'score': 8439.236397266388, 'total_duration': 14052.139616012573, 'accumulated_submission_time': 8439.236397266388, 'accumulated_eval_time': 5611.919378757477, 'accumulated_logging_time': 0.2727680206298828}
I0312 08:09:34.876018 140174024480512 logging_writer.py:48] [23323] accumulated_eval_time=5611.919379, accumulated_logging_time=0.272768, accumulated_submission_time=8439.236397, global_step=23323, preemption_count=0, score=8439.236397, test/accuracy=0.666039, test/bleu=27.101383, test/loss=1.558379, test/num_examples=3003, total_duration=14052.139616, train/accuracy=0.635143, train/bleu=31.048167, train/loss=1.771107, validation/accuracy=0.654276, validation/bleu=28.171751, validation/loss=1.631288, validation/num_examples=3000
I0312 08:10:02.896902 140174032873216 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.21556410193443298, loss=1.918919563293457
I0312 08:10:38.814198 140174024480512 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.1838798224925995, loss=1.8524341583251953
I0312 08:11:14.787264 140174032873216 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.22626128792762756, loss=1.9236092567443848
I0312 08:11:50.756134 140174024480512 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.18059970438480377, loss=1.8480364084243774
I0312 08:12:26.766361 140174032873216 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.1887123584747314, loss=1.777966022491455
I0312 08:13:02.757490 140174024480512 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.2450738102197647, loss=1.8226900100708008
I0312 08:13:38.772994 140174032873216 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2343079298734665, loss=1.9573556184768677
I0312 08:14:14.793768 140174024480512 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.1857747584581375, loss=1.8264321088790894
I0312 08:14:50.818515 140174032873216 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.19428874552249908, loss=1.856131672859192
I0312 08:15:26.840483 140174024480512 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.24587400257587433, loss=1.8312839269638062
I0312 08:16:02.840204 140174032873216 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.3049014210700989, loss=1.8141140937805176
I0312 08:16:38.848906 140174024480512 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.2033604085445404, loss=1.8272515535354614
I0312 08:17:14.852724 140174032873216 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.16889271140098572, loss=1.8589426279067993
I0312 08:17:50.832414 140174024480512 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.19956371188163757, loss=1.8096792697906494
I0312 08:18:26.865030 140174032873216 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.18758410215377808, loss=1.8167051076889038
I0312 08:19:02.855960 140174024480512 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.2164950966835022, loss=1.82974374294281
I0312 08:19:38.885230 140174032873216 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.24455077946186066, loss=1.8187674283981323
I0312 08:20:14.919483 140174024480512 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2635657489299774, loss=1.814401626586914
I0312 08:20:50.933534 140174032873216 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.27643951773643494, loss=1.8584264516830444
I0312 08:21:26.959350 140174024480512 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.19039542973041534, loss=1.9073097705841064
I0312 08:22:02.985021 140174032873216 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.23863719403743744, loss=1.8310383558273315
I0312 08:22:39.076561 140174024480512 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.24257904291152954, loss=1.7111610174179077
I0312 08:23:15.092029 140174032873216 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.18481408059597015, loss=1.8162484169006348
I0312 08:23:34.980874 140343717943104 spec.py:321] Evaluating on the training split.
I0312 08:23:37.987828 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:26:54.216611 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 08:26:56.935372 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:29:28.021676 140343717943104 spec.py:349] Evaluating on the test split.
I0312 08:29:30.737862 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:31:55.679341 140343717943104 submission_runner.py:420] Time since start: 15392.96s, 	Step: 25657, 	{'train/accuracy': 0.6489943861961365, 'train/loss': 1.6708744764328003, 'train/bleu': 31.861470460471523, 'validation/accuracy': 0.6577103734016418, 'validation/loss': 1.6123778820037842, 'validation/bleu': 28.386727715138804, 'validation/num_examples': 3000, 'test/accuracy': 0.6674452424049377, 'test/loss': 1.5467638969421387, 'test/bleu': 27.24795073109318, 'test/num_examples': 3003, 'score': 9279.262301683426, 'total_duration': 15392.960274457932, 'accumulated_submission_time': 9279.262301683426, 'accumulated_eval_time': 6112.617798089981, 'accumulated_logging_time': 0.30026745796203613}
I0312 08:31:55.696518 140174024480512 logging_writer.py:48] [25657] accumulated_eval_time=6112.617798, accumulated_logging_time=0.300267, accumulated_submission_time=9279.262302, global_step=25657, preemption_count=0, score=9279.262302, test/accuracy=0.667445, test/bleu=27.247951, test/loss=1.546764, test/num_examples=3003, total_duration=15392.960274, train/accuracy=0.648994, train/bleu=31.861470, train/loss=1.670874, validation/accuracy=0.657710, validation/bleu=28.386728, validation/loss=1.612378, validation/num_examples=3000
I0312 08:32:11.509259 140174032873216 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.20355680584907532, loss=1.825502872467041
I0312 08:32:47.459312 140174024480512 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.1989109069108963, loss=1.8996798992156982
I0312 08:33:23.422629 140174032873216 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.18261922895908356, loss=1.7907938957214355
I0312 08:33:59.410919 140174024480512 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.17054884135723114, loss=1.8406658172607422
I0312 08:34:35.450828 140174032873216 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.19124126434326172, loss=1.8778504133224487
I0312 08:35:11.519806 140174024480512 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.212894007563591, loss=1.878949522972107
I0312 08:35:47.617420 140174032873216 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.22046680748462677, loss=1.7879067659378052
I0312 08:36:23.617931 140174024480512 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.19104625284671783, loss=1.8210347890853882
I0312 08:36:59.640956 140174032873216 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2590200901031494, loss=1.763014316558838
I0312 08:37:35.671207 140174024480512 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.18422313034534454, loss=1.8061538934707642
I0312 08:38:11.688047 140174032873216 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.17770756781101227, loss=1.824467420578003
I0312 08:38:47.703868 140174024480512 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.24572020769119263, loss=1.8995249271392822
I0312 08:39:23.703664 140174032873216 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2265997678041458, loss=1.8153742551803589
I0312 08:39:59.713159 140174024480512 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.226805180311203, loss=1.8587348461151123
I0312 08:40:35.706807 140174032873216 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.21587547659873962, loss=1.808181643486023
I0312 08:41:11.728022 140174024480512 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.1868739128112793, loss=1.84632408618927
I0312 08:41:47.750886 140174032873216 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.19547449052333832, loss=1.8353813886642456
I0312 08:42:23.757863 140174024480512 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.1817169338464737, loss=1.7648154497146606
I0312 08:42:59.810428 140174032873216 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19299158453941345, loss=1.9240812063217163
I0312 08:43:35.857013 140174024480512 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.1983962208032608, loss=1.7867612838745117
I0312 08:44:11.949561 140174032873216 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.194862961769104, loss=1.8039072751998901
I0312 08:44:47.991603 140174024480512 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.20381546020507812, loss=1.8281859159469604
I0312 08:45:24.039827 140174032873216 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.17523883283138275, loss=1.8389333486557007
I0312 08:45:55.850174 140343717943104 spec.py:321] Evaluating on the training split.
I0312 08:45:58.880599 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:50:13.153967 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 08:50:15.879593 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:53:38.517338 140343717943104 spec.py:349] Evaluating on the test split.
I0312 08:53:41.254663 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 08:56:50.117135 140343717943104 submission_runner.py:420] Time since start: 16887.40s, 	Step: 27990, 	{'train/accuracy': 0.6425632238388062, 'train/loss': 1.7168598175048828, 'train/bleu': 31.24810712567143, 'validation/accuracy': 0.658665120601654, 'validation/loss': 1.6051126718521118, 'validation/bleu': 28.33344174410828, 'validation/num_examples': 3000, 'test/accuracy': 0.6686421632766724, 'test/loss': 1.5359697341918945, 'test/bleu': 27.659953390141943, 'test/num_examples': 3003, 'score': 10119.33438873291, 'total_duration': 16887.398057222366, 'accumulated_submission_time': 10119.33438873291, 'accumulated_eval_time': 6766.884717226028, 'accumulated_logging_time': 0.3277285099029541}
I0312 08:56:50.135612 140174024480512 logging_writer.py:48] [27990] accumulated_eval_time=6766.884717, accumulated_logging_time=0.327729, accumulated_submission_time=10119.334389, global_step=27990, preemption_count=0, score=10119.334389, test/accuracy=0.668642, test/bleu=27.659953, test/loss=1.535970, test/num_examples=3003, total_duration=16887.398057, train/accuracy=0.642563, train/bleu=31.248107, train/loss=1.716860, validation/accuracy=0.658665, validation/bleu=28.333442, validation/loss=1.605113, validation/num_examples=3000
I0312 08:56:54.093030 140174032873216 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.19459477066993713, loss=1.8768247365951538
I0312 08:57:29.960522 140174024480512 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.2502632737159729, loss=1.7764726877212524
I0312 08:58:05.902150 140174032873216 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.19850148260593414, loss=1.8940056562423706
I0312 08:58:41.916816 140174024480512 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.28127872943878174, loss=1.7997112274169922
I0312 08:59:17.934379 140174032873216 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.1846253126859665, loss=1.810733675956726
I0312 08:59:53.945217 140174024480512 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.19414424896240234, loss=1.8119473457336426
I0312 09:00:29.996895 140174032873216 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18643376231193542, loss=1.8101258277893066
I0312 09:01:06.014285 140174024480512 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.2329963892698288, loss=1.7980098724365234
I0312 09:01:42.022061 140174032873216 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.2705210745334625, loss=1.8975944519042969
I0312 09:02:18.058997 140174024480512 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.20043502748012543, loss=1.7986403703689575
I0312 09:02:54.118856 140174032873216 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2091454267501831, loss=1.8436301946640015
I0312 09:03:30.137315 140174024480512 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.18819457292556763, loss=1.7860409021377563
I0312 09:04:06.158798 140174032873216 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.22114208340644836, loss=1.7944746017456055
I0312 09:04:42.171642 140174024480512 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.2087506800889969, loss=1.8607127666473389
I0312 09:05:18.183297 140174032873216 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.2547897696495056, loss=1.8642449378967285
I0312 09:05:54.186311 140174024480512 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.19597168266773224, loss=1.8615933656692505
I0312 09:06:30.220350 140174032873216 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19326283037662506, loss=1.7378934621810913
I0312 09:07:06.269227 140174024480512 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.17873993515968323, loss=1.8022034168243408
I0312 09:07:42.290973 140174032873216 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.18972282111644745, loss=1.843088984489441
I0312 09:08:18.333364 140174024480512 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.2514968514442444, loss=1.8216725587844849
I0312 09:08:54.344448 140174032873216 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.18486784398555756, loss=1.8316246271133423
I0312 09:09:30.341817 140174024480512 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19883309304714203, loss=1.8506503105163574
I0312 09:10:06.340496 140174032873216 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.21417060494422913, loss=1.8341209888458252
I0312 09:10:42.382265 140174024480512 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.1979520320892334, loss=1.871640682220459
I0312 09:10:50.368280 140343717943104 spec.py:321] Evaluating on the training split.
I0312 09:10:53.398706 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:13:49.650101 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 09:13:52.374528 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:16:11.273413 140343717943104 spec.py:349] Evaluating on the test split.
I0312 09:16:13.993057 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:18:28.394938 140343717943104 submission_runner.py:420] Time since start: 18185.68s, 	Step: 30324, 	{'train/accuracy': 0.6387715935707092, 'train/loss': 1.7513316869735718, 'train/bleu': 31.180361760347587, 'validation/accuracy': 0.6582807302474976, 'validation/loss': 1.5954887866973877, 'validation/bleu': 28.016983880669876, 'validation/num_examples': 3000, 'test/accuracy': 0.6704317331314087, 'test/loss': 1.5235344171524048, 'test/bleu': 27.539265690003113, 'test/num_examples': 3003, 'score': 10959.488750457764, 'total_duration': 18185.67586708069, 'accumulated_submission_time': 10959.488750457764, 'accumulated_eval_time': 7224.911325931549, 'accumulated_logging_time': 0.3553922176361084}
I0312 09:18:28.412472 140174032873216 logging_writer.py:48] [30324] accumulated_eval_time=7224.911326, accumulated_logging_time=0.355392, accumulated_submission_time=10959.488750, global_step=30324, preemption_count=0, score=10959.488750, test/accuracy=0.670432, test/bleu=27.539266, test/loss=1.523534, test/num_examples=3003, total_duration=18185.675867, train/accuracy=0.638772, train/bleu=31.180362, train/loss=1.751332, validation/accuracy=0.658281, validation/bleu=28.016984, validation/loss=1.595489, validation/num_examples=3000
I0312 09:18:56.067839 140174024480512 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.20793330669403076, loss=1.813071370124817
I0312 09:19:32.008933 140174032873216 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.17918802797794342, loss=1.7579237222671509
I0312 09:20:08.016103 140174024480512 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.1834181547164917, loss=1.7784621715545654
I0312 09:20:44.008265 140174032873216 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.20426775515079498, loss=1.7684470415115356
I0312 09:21:20.021155 140174024480512 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.20012333989143372, loss=1.8205207586288452
I0312 09:21:56.026742 140174032873216 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.20458945631980896, loss=1.8491183519363403
I0312 09:22:32.042572 140174024480512 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.21743659675121307, loss=1.773028016090393
I0312 09:23:08.083057 140174032873216 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.19990648329257965, loss=1.8939322233200073
I0312 09:23:44.108247 140174024480512 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2104998528957367, loss=1.809382677078247
I0312 09:24:20.214657 140174032873216 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.2093149721622467, loss=1.8366299867630005
I0312 09:24:56.326080 140174024480512 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2520977556705475, loss=1.8836008310317993
I0312 09:25:32.350756 140174032873216 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.2309444397687912, loss=1.7224823236465454
I0312 09:26:08.343837 140174024480512 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1817168891429901, loss=1.8102604150772095
I0312 09:26:44.355088 140174032873216 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.21632921695709229, loss=1.8803882598876953
I0312 09:27:20.359290 140174024480512 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.1919838935136795, loss=1.7872694730758667
I0312 09:27:56.364587 140174032873216 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.22700050473213196, loss=1.818977952003479
I0312 09:28:32.358189 140174024480512 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.17696736752986908, loss=1.8322166204452515
I0312 09:29:08.383031 140174032873216 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2605443000793457, loss=1.799409031867981
I0312 09:29:44.433260 140174024480512 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.1937708854675293, loss=1.7086156606674194
I0312 09:30:20.434392 140174032873216 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.19875743985176086, loss=1.8055460453033447
I0312 09:30:56.472699 140174024480512 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.19208654761314392, loss=1.816505789756775
I0312 09:31:32.471225 140174032873216 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.22211867570877075, loss=1.7964931726455688
I0312 09:32:08.504320 140174024480512 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.18842697143554688, loss=1.848463535308838
I0312 09:32:28.400308 140343717943104 spec.py:321] Evaluating on the training split.
I0312 09:32:31.405196 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:35:39.890688 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 09:35:42.605341 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:38:10.267060 140343717943104 spec.py:349] Evaluating on the test split.
I0312 09:38:12.979604 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:40:38.554287 140343717943104 submission_runner.py:420] Time since start: 19515.84s, 	Step: 32657, 	{'train/accuracy': 0.6421418190002441, 'train/loss': 1.7130132913589478, 'train/bleu': 31.688443196843643, 'validation/accuracy': 0.6594462394714355, 'validation/loss': 1.5872998237609863, 'validation/bleu': 28.124145456230487, 'validation/num_examples': 3000, 'test/accuracy': 0.6709546446800232, 'test/loss': 1.5159692764282227, 'test/bleu': 27.460592661060517, 'test/num_examples': 3003, 'score': 11799.395884513855, 'total_duration': 19515.8352162838, 'accumulated_submission_time': 11799.395884513855, 'accumulated_eval_time': 7715.065255880356, 'accumulated_logging_time': 0.38286900520324707}
I0312 09:40:38.572638 140174032873216 logging_writer.py:48] [32657] accumulated_eval_time=7715.065256, accumulated_logging_time=0.382869, accumulated_submission_time=11799.395885, global_step=32657, preemption_count=0, score=11799.395885, test/accuracy=0.670955, test/bleu=27.460593, test/loss=1.515969, test/num_examples=3003, total_duration=19515.835216, train/accuracy=0.642142, train/bleu=31.688443, train/loss=1.713013, validation/accuracy=0.659446, validation/bleu=28.124145, validation/loss=1.587300, validation/num_examples=3000
I0312 09:40:54.346090 140174024480512 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.1934712529182434, loss=1.810160517692566
I0312 09:41:30.224955 140174032873216 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.23035390675067902, loss=1.845801830291748
I0312 09:42:06.210641 140174024480512 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.2098655104637146, loss=1.81073796749115
I0312 09:42:42.188611 140174032873216 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.1880703866481781, loss=1.8130278587341309
I0312 09:43:18.177345 140174024480512 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.22270499169826508, loss=1.824509859085083
I0312 09:43:54.190581 140174032873216 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.20414455235004425, loss=1.8565454483032227
I0312 09:44:30.279614 140174024480512 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.19222551584243774, loss=1.7929600477218628
I0312 09:45:06.325487 140174032873216 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.27047690749168396, loss=1.8556501865386963
I0312 09:45:42.327139 140174024480512 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.18852168321609497, loss=1.7591142654418945
I0312 09:46:18.334368 140174032873216 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.1876702457666397, loss=1.8212298154830933
I0312 09:46:54.376285 140174024480512 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.19545823335647583, loss=1.8577206134796143
I0312 09:47:30.451620 140174032873216 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.19129537045955658, loss=1.7071082592010498
I0312 09:48:06.447102 140174024480512 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.216534823179245, loss=1.8717899322509766
I0312 09:48:42.508485 140174032873216 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.21051128208637238, loss=1.804349660873413
I0312 09:49:18.572260 140174024480512 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.21475593745708466, loss=1.838852047920227
I0312 09:49:54.620373 140174032873216 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.1942155361175537, loss=1.7435414791107178
I0312 09:50:30.625866 140174024480512 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.22085072100162506, loss=1.803080439567566
I0312 09:51:06.679161 140174032873216 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.19596318900585175, loss=1.7467330694198608
I0312 09:51:42.700507 140174024480512 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.2081974297761917, loss=1.720119595527649
I0312 09:52:18.696615 140174032873216 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.21319986879825592, loss=1.8609660863876343
I0312 09:52:54.719530 140174024480512 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.18701857328414917, loss=1.8207049369812012
I0312 09:53:30.737968 140174032873216 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.21256284415721893, loss=1.7809133529663086
I0312 09:54:06.773973 140174024480512 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.19954070448875427, loss=1.8281538486480713
I0312 09:54:38.905418 140343717943104 spec.py:321] Evaluating on the training split.
I0312 09:54:41.915877 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 09:58:04.924370 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 09:58:07.621166 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:00:38.228885 140343717943104 spec.py:349] Evaluating on the test split.
I0312 10:00:40.936716 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:03:05.380003 140343717943104 submission_runner.py:420] Time since start: 20862.66s, 	Step: 34991, 	{'train/accuracy': 0.6410354971885681, 'train/loss': 1.7259587049484253, 'train/bleu': 31.29086602755576, 'validation/accuracy': 0.6630048155784607, 'validation/loss': 1.5775558948516846, 'validation/bleu': 28.3995127800219, 'validation/num_examples': 3000, 'test/accuracy': 0.672105073928833, 'test/loss': 1.5036916732788086, 'test/bleu': 27.63131568906135, 'test/num_examples': 3003, 'score': 12639.648458957672, 'total_duration': 20862.66091775894, 'accumulated_submission_time': 12639.648458957672, 'accumulated_eval_time': 8221.539778709412, 'accumulated_logging_time': 0.41158080101013184}
I0312 10:03:05.398777 140174032873216 logging_writer.py:48] [34991] accumulated_eval_time=8221.539779, accumulated_logging_time=0.411581, accumulated_submission_time=12639.648459, global_step=34991, preemption_count=0, score=12639.648459, test/accuracy=0.672105, test/bleu=27.631316, test/loss=1.503692, test/num_examples=3003, total_duration=20862.660918, train/accuracy=0.641035, train/bleu=31.290866, train/loss=1.725959, validation/accuracy=0.663005, validation/bleu=28.399513, validation/loss=1.577556, validation/num_examples=3000
I0312 10:03:09.005923 140174024480512 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.1776244044303894, loss=1.7316052913665771
I0312 10:03:44.872111 140174032873216 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2173387110233307, loss=1.8552227020263672
I0312 10:04:20.856760 140174024480512 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.23370809853076935, loss=1.8787922859191895
I0312 10:04:56.850322 140174032873216 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.23659555613994598, loss=1.7844724655151367
I0312 10:05:32.828039 140174024480512 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.21215815842151642, loss=1.789231777191162
I0312 10:06:08.837232 140174032873216 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.21353890001773834, loss=1.8181604146957397
I0312 10:06:44.864232 140174024480512 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.23310773074626923, loss=1.833473563194275
I0312 10:07:20.896547 140174032873216 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.18114162981510162, loss=1.7237179279327393
I0312 10:07:56.937150 140174024480512 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.18640360236167908, loss=1.7045022249221802
I0312 10:08:32.947528 140174032873216 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1906745731830597, loss=1.805751919746399
I0312 10:09:08.964668 140174024480512 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.2333848625421524, loss=1.7941339015960693
I0312 10:09:44.996658 140174032873216 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2098059356212616, loss=1.769087553024292
I0312 10:10:21.025018 140174024480512 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.2032846361398697, loss=1.8128538131713867
I0312 10:10:57.071239 140174032873216 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.22500082850456238, loss=1.8042376041412354
I0312 10:11:33.143244 140174024480512 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.2005917727947235, loss=1.7770013809204102
I0312 10:12:09.174505 140174032873216 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.1880629062652588, loss=1.7791082859039307
I0312 10:12:45.178111 140174024480512 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.226089209318161, loss=1.8174896240234375
I0312 10:13:21.185295 140174032873216 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.20359520614147186, loss=1.6798183917999268
I0312 10:13:57.190570 140174024480512 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.20450516045093536, loss=1.790615439414978
I0312 10:14:33.207989 140174032873216 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.18461930751800537, loss=1.803760290145874
I0312 10:15:09.290456 140174024480512 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.19203704595565796, loss=1.7963277101516724
I0312 10:15:45.287529 140174032873216 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.24309685826301575, loss=1.9218984842300415
I0312 10:16:21.307151 140174024480512 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2967223525047302, loss=1.8452379703521729
I0312 10:16:57.359712 140174032873216 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.28985869884490967, loss=1.8142958879470825
I0312 10:17:05.724573 140343717943104 spec.py:321] Evaluating on the training split.
I0312 10:17:08.736962 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:21:12.298373 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 10:21:15.013776 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:23:59.800707 140343717943104 spec.py:349] Evaluating on the test split.
I0312 10:24:02.529080 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:26:37.471736 140343717943104 submission_runner.py:420] Time since start: 22274.75s, 	Step: 37325, 	{'train/accuracy': 0.6429249048233032, 'train/loss': 1.7336983680725098, 'train/bleu': 31.46545863659349, 'validation/accuracy': 0.663240373134613, 'validation/loss': 1.577974796295166, 'validation/bleu': 28.164983907427093, 'validation/num_examples': 3000, 'test/accuracy': 0.6757538914680481, 'test/loss': 1.5002532005310059, 'test/bleu': 27.793051245605607, 'test/num_examples': 3003, 'score': 13479.89543390274, 'total_duration': 22274.75265264511, 'accumulated_submission_time': 13479.89543390274, 'accumulated_eval_time': 8793.286875247955, 'accumulated_logging_time': 0.4394073486328125}
I0312 10:26:37.490307 140174024480512 logging_writer.py:48] [37325] accumulated_eval_time=8793.286875, accumulated_logging_time=0.439407, accumulated_submission_time=13479.895434, global_step=37325, preemption_count=0, score=13479.895434, test/accuracy=0.675754, test/bleu=27.793051, test/loss=1.500253, test/num_examples=3003, total_duration=22274.752653, train/accuracy=0.642925, train/bleu=31.465459, train/loss=1.733698, validation/accuracy=0.663240, validation/bleu=28.164984, validation/loss=1.577975, validation/num_examples=3000
I0312 10:27:04.805967 140174032873216 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.20379126071929932, loss=1.8529750108718872
I0312 10:27:40.754661 140174024480512 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.23631013929843903, loss=1.738181233406067
I0312 10:28:16.736121 140174032873216 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2145213633775711, loss=1.7982118129730225
I0312 10:28:52.766661 140174024480512 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.2049095183610916, loss=1.7447596788406372
I0312 10:29:28.779091 140174032873216 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.18414831161499023, loss=1.7487090826034546
I0312 10:30:04.775470 140174024480512 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.20000602304935455, loss=1.7166407108306885
I0312 10:30:40.818788 140174032873216 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.1875685155391693, loss=1.7220386266708374
I0312 10:31:16.837959 140174024480512 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.19800016283988953, loss=1.7580043077468872
I0312 10:31:52.855585 140174032873216 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.24085715413093567, loss=1.7379109859466553
I0312 10:32:28.874603 140174024480512 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.21707867085933685, loss=1.7429448366165161
I0312 10:33:04.889590 140174032873216 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.20108377933502197, loss=1.8470951318740845
I0312 10:33:40.884564 140174024480512 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2428945153951645, loss=1.7823638916015625
I0312 10:34:16.893283 140174032873216 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.22028523683547974, loss=1.8020975589752197
I0312 10:34:52.904853 140174024480512 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.18852578103542328, loss=1.7998472452163696
I0312 10:35:28.912959 140174032873216 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.1997487097978592, loss=1.7902028560638428
I0312 10:36:04.918521 140174024480512 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.20721910893917084, loss=1.8405545949935913
I0312 10:36:40.942937 140174032873216 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.18648196756839752, loss=1.7223808765411377
I0312 10:37:16.987640 140174024480512 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5988109111785889, loss=1.7926850318908691
I0312 10:37:53.015375 140174032873216 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.26912054419517517, loss=1.7401480674743652
I0312 10:38:29.045847 140174024480512 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.2270391285419464, loss=1.822386622428894
I0312 10:39:05.073915 140174032873216 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.21919602155685425, loss=1.7095285654067993
I0312 10:39:41.067075 140174024480512 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.19679799675941467, loss=1.8151520490646362
I0312 10:40:17.081936 140174032873216 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.18015781044960022, loss=1.7433258295059204
I0312 10:40:37.690829 140343717943104 spec.py:321] Evaluating on the training split.
I0312 10:40:40.702385 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:44:31.767308 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 10:44:34.474627 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:48:11.918411 140343717943104 spec.py:349] Evaluating on the test split.
I0312 10:48:14.631084 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 10:51:36.121717 140343717943104 submission_runner.py:420] Time since start: 23773.40s, 	Step: 39659, 	{'train/accuracy': 0.6447225213050842, 'train/loss': 1.689799427986145, 'train/bleu': 31.25804832940152, 'validation/accuracy': 0.6650258302688599, 'validation/loss': 1.5642013549804688, 'validation/bleu': 28.47577944514998, 'validation/num_examples': 3000, 'test/accuracy': 0.6762884259223938, 'test/loss': 1.4868884086608887, 'test/bleu': 28.095992516428442, 'test/num_examples': 3003, 'score': 14320.017933368683, 'total_duration': 23773.402626037598, 'accumulated_submission_time': 14320.017933368683, 'accumulated_eval_time': 9451.717700958252, 'accumulated_logging_time': 0.46711254119873047}
I0312 10:51:36.141161 140174024480512 logging_writer.py:48] [39659] accumulated_eval_time=9451.717701, accumulated_logging_time=0.467113, accumulated_submission_time=14320.017933, global_step=39659, preemption_count=0, score=14320.017933, test/accuracy=0.676288, test/bleu=28.095993, test/loss=1.486888, test/num_examples=3003, total_duration=23773.402626, train/accuracy=0.644723, train/bleu=31.258048, train/loss=1.689799, validation/accuracy=0.665026, validation/bleu=28.475779, validation/loss=1.564201, validation/num_examples=3000
I0312 10:51:51.249522 140174032873216 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.20212560892105103, loss=1.7972458600997925
I0312 10:52:27.164025 140174024480512 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.2353145182132721, loss=1.7024810314178467
I0312 10:53:03.106418 140174032873216 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.21114760637283325, loss=1.770519733428955
I0312 10:53:39.075807 140174024480512 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.21763530373573303, loss=1.7500026226043701
I0312 10:54:15.097617 140174032873216 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.22401782870292664, loss=1.795914888381958
I0312 10:54:51.138069 140174024480512 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.17293238639831543, loss=1.704323172569275
I0312 10:55:27.220420 140174032873216 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2029636651277542, loss=1.79974365234375
I0312 10:56:03.296110 140174024480512 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.18560875952243805, loss=1.7464708089828491
I0312 10:56:39.329035 140174032873216 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.20196890830993652, loss=1.8570787906646729
I0312 10:57:15.323226 140174024480512 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20635037124156952, loss=1.758183479309082
I0312 10:57:51.336901 140174032873216 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.20008985698223114, loss=1.7453364133834839
I0312 10:58:27.316061 140174024480512 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.2032395303249359, loss=1.722409963607788
I0312 10:59:03.330778 140174032873216 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.21328489482402802, loss=1.7298755645751953
I0312 10:59:39.345828 140174024480512 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.19501085579395294, loss=1.7854663133621216
I0312 11:00:15.360138 140174032873216 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1834688037633896, loss=1.7042750120162964
I0312 11:00:51.373778 140174024480512 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.20958241820335388, loss=1.7471132278442383
I0312 11:01:27.410204 140174032873216 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.22093439102172852, loss=1.7454454898834229
I0312 11:02:03.442894 140174024480512 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.19898714125156403, loss=1.7937275171279907
I0312 11:02:39.483088 140174032873216 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.25655585527420044, loss=1.766623616218567
I0312 11:03:15.479442 140174024480512 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.19333648681640625, loss=1.759034514427185
I0312 11:03:51.468683 140174032873216 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2073328047990799, loss=1.8310602903366089
I0312 11:04:27.497181 140174024480512 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.18592005968093872, loss=1.7401949167251587
I0312 11:05:03.498391 140174032873216 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.19466818869113922, loss=1.727726936340332
I0312 11:05:36.351054 140343717943104 spec.py:321] Evaluating on the training split.
I0312 11:05:39.366587 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:09:50.306040 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 11:09:53.012963 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:13:19.173100 140343717943104 spec.py:349] Evaluating on the test split.
I0312 11:13:21.898837 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:16:08.554929 140343717943104 submission_runner.py:420] Time since start: 25245.84s, 	Step: 41993, 	{'train/accuracy': 0.6418246626853943, 'train/loss': 1.7203962802886963, 'train/bleu': 31.54752667866807, 'validation/accuracy': 0.6655341982841492, 'validation/loss': 1.562987208366394, 'validation/bleu': 28.69866005713504, 'validation/num_examples': 3000, 'test/accuracy': 0.6767416596412659, 'test/loss': 1.482648491859436, 'test/bleu': 28.159225092401268, 'test/num_examples': 3003, 'score': 15160.147383213043, 'total_duration': 25245.83581018448, 'accumulated_submission_time': 15160.147383213043, 'accumulated_eval_time': 10083.921479225159, 'accumulated_logging_time': 0.4971275329589844}
I0312 11:16:08.578979 140174024480512 logging_writer.py:48] [41993] accumulated_eval_time=10083.921479, accumulated_logging_time=0.497128, accumulated_submission_time=15160.147383, global_step=41993, preemption_count=0, score=15160.147383, test/accuracy=0.676742, test/bleu=28.159225, test/loss=1.482648, test/num_examples=3003, total_duration=25245.835810, train/accuracy=0.641825, train/bleu=31.547527, train/loss=1.720396, validation/accuracy=0.665534, validation/bleu=28.698660, validation/loss=1.562987, validation/num_examples=3000
I0312 11:16:11.462411 140174032873216 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.19381274282932281, loss=1.750389575958252
I0312 11:16:47.376549 140174024480512 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.19362720847129822, loss=1.7062140703201294
I0312 11:17:23.329302 140174032873216 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.40338441729545593, loss=1.8048745393753052
I0312 11:17:59.311578 140174024480512 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.19179634749889374, loss=1.7397668361663818
I0312 11:18:35.340316 140174032873216 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.19182653725147247, loss=1.7747021913528442
I0312 11:19:11.381539 140174024480512 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.19539637863636017, loss=1.8696625232696533
I0312 11:19:47.399021 140174032873216 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2060105949640274, loss=1.8102879524230957
I0312 11:20:23.405329 140174024480512 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.17525503039360046, loss=1.6914135217666626
I0312 11:20:59.439271 140174032873216 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.1999439299106598, loss=1.7153904438018799
I0312 11:21:35.428885 140174024480512 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3166980743408203, loss=1.6834192276000977
I0312 11:22:11.479603 140174032873216 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.21075685322284698, loss=1.7490479946136475
I0312 11:22:47.552004 140174024480512 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.20944726467132568, loss=1.7799781560897827
I0312 11:23:23.548324 140174032873216 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.19300031661987305, loss=1.7821764945983887
I0312 11:23:59.539864 140174024480512 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.20802153646945953, loss=1.7744245529174805
I0312 11:24:35.569314 140174032873216 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.25765684247016907, loss=1.7102824449539185
I0312 11:25:11.579137 140174024480512 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.19467081129550934, loss=1.6679677963256836
I0312 11:25:47.581526 140174032873216 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.22950045764446259, loss=1.7440052032470703
I0312 11:26:23.605091 140174024480512 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.19146454334259033, loss=1.7296769618988037
I0312 11:26:59.628044 140174032873216 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2035556435585022, loss=1.774201512336731
I0312 11:27:35.697113 140174024480512 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.18823860585689545, loss=1.8098435401916504
I0312 11:28:11.707246 140174032873216 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2400827258825302, loss=1.8377238512039185
I0312 11:28:47.791257 140174024480512 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.20135076344013214, loss=1.7998956441879272
I0312 11:29:23.796157 140174032873216 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1952148675918579, loss=1.8118151426315308
I0312 11:29:59.838431 140174024480512 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2208026945590973, loss=1.784387230873108
I0312 11:30:08.564142 140343717943104 spec.py:321] Evaluating on the training split.
I0312 11:30:11.573713 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:33:40.068740 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 11:33:42.794895 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:36:28.743966 140343717943104 spec.py:349] Evaluating on the test split.
I0312 11:36:31.470959 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:39:04.210971 140343717943104 submission_runner.py:420] Time since start: 26621.49s, 	Step: 44326, 	{'train/accuracy': 0.6613112092018127, 'train/loss': 1.5802650451660156, 'train/bleu': 32.61597261217031, 'validation/accuracy': 0.6657201647758484, 'validation/loss': 1.5555511713027954, 'validation/bleu': 28.666405029152983, 'validation/num_examples': 3000, 'test/accuracy': 0.6781128644943237, 'test/loss': 1.4767955541610718, 'test/bleu': 27.77103557395157, 'test/num_examples': 3003, 'score': 16000.051938295364, 'total_duration': 26621.491897821426, 'accumulated_submission_time': 16000.051938295364, 'accumulated_eval_time': 10619.568261384964, 'accumulated_logging_time': 0.5311579704284668}
I0312 11:39:04.230884 140174032873216 logging_writer.py:48] [44326] accumulated_eval_time=10619.568261, accumulated_logging_time=0.531158, accumulated_submission_time=16000.051938, global_step=44326, preemption_count=0, score=16000.051938, test/accuracy=0.678113, test/bleu=27.771036, test/loss=1.476796, test/num_examples=3003, total_duration=26621.491898, train/accuracy=0.661311, train/bleu=32.615973, train/loss=1.580265, validation/accuracy=0.665720, validation/bleu=28.666405, validation/loss=1.555551, validation/num_examples=3000
I0312 11:39:31.123431 140174024480512 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.19831614196300507, loss=1.6758910417556763
I0312 11:40:07.042450 140174032873216 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.22293469309806824, loss=1.7722609043121338
I0312 11:40:43.003125 140174024480512 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1990039199590683, loss=1.7697769403457642
I0312 11:41:19.019989 140174032873216 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.19193685054779053, loss=1.7532398700714111
I0312 11:41:54.998576 140174024480512 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.2536553144454956, loss=1.7387309074401855
I0312 11:42:31.004635 140174032873216 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.19802069664001465, loss=1.7975395917892456
I0312 11:43:07.026858 140174024480512 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.18713589012622833, loss=1.7156285047531128
I0312 11:43:43.039353 140174032873216 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.19846193492412567, loss=1.733221411705017
I0312 11:44:19.076365 140174024480512 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19866818189620972, loss=1.7663381099700928
I0312 11:44:55.085334 140174032873216 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.19110672175884247, loss=1.6816545724868774
I0312 11:45:31.132211 140174024480512 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.20462733507156372, loss=1.735561490058899
I0312 11:46:07.162906 140174032873216 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.21307042241096497, loss=1.7936701774597168
I0312 11:46:43.164008 140174024480512 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.21102136373519897, loss=1.7711831331253052
I0312 11:47:19.143333 140174032873216 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.24607761204242706, loss=1.7252753973007202
I0312 11:47:55.156640 140174024480512 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.22401872277259827, loss=1.733165979385376
I0312 11:48:31.159107 140174032873216 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.19118554890155792, loss=1.8071318864822388
I0312 11:49:07.160397 140174024480512 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2082880437374115, loss=1.8132963180541992
I0312 11:49:43.176267 140174032873216 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2035953253507614, loss=1.8382760286331177
I0312 11:50:19.195476 140174024480512 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.20068852603435516, loss=1.7317183017730713
I0312 11:50:55.181297 140174032873216 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.20194630324840546, loss=1.8098342418670654
I0312 11:51:31.215178 140174024480512 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.2519630491733551, loss=1.8524649143218994
I0312 11:52:07.238625 140174032873216 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.24895352125167847, loss=1.7386467456817627
I0312 11:52:43.262640 140174024480512 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.19533410668373108, loss=1.7898920774459839
I0312 11:53:04.238401 140343717943104 spec.py:321] Evaluating on the training split.
I0312 11:53:07.254572 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:56:57.606977 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 11:57:00.318630 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 11:59:52.523097 140343717943104 spec.py:349] Evaluating on the test split.
I0312 11:59:55.231921 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:02:43.635756 140343717943104 submission_runner.py:420] Time since start: 28040.92s, 	Step: 46660, 	{'train/accuracy': 0.6497166752815247, 'train/loss': 1.6577461957931519, 'train/bleu': 31.555297809185713, 'validation/accuracy': 0.6670964956283569, 'validation/loss': 1.5500373840332031, 'validation/bleu': 28.635666652429748, 'validation/num_examples': 3000, 'test/accuracy': 0.6805066466331482, 'test/loss': 1.4682908058166504, 'test/bleu': 28.084528100338435, 'test/num_examples': 3003, 'score': 16839.980953216553, 'total_duration': 28040.916669368744, 'accumulated_submission_time': 16839.980953216553, 'accumulated_eval_time': 11198.965550661087, 'accumulated_logging_time': 0.561424970626831}
I0312 12:02:43.656285 140174032873216 logging_writer.py:48] [46660] accumulated_eval_time=11198.965551, accumulated_logging_time=0.561425, accumulated_submission_time=16839.980953, global_step=46660, preemption_count=0, score=16839.980953, test/accuracy=0.680507, test/bleu=28.084528, test/loss=1.468291, test/num_examples=3003, total_duration=28040.916669, train/accuracy=0.649717, train/bleu=31.555298, train/loss=1.657746, validation/accuracy=0.667096, validation/bleu=28.635667, validation/loss=1.550037, validation/num_examples=3000
I0312 12:02:58.357585 140174024480512 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2165776640176773, loss=1.7551367282867432
I0312 12:03:34.242937 140174032873216 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2003597617149353, loss=1.746996283531189
I0312 12:04:10.182627 140174024480512 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2083948403596878, loss=1.6835217475891113
I0312 12:04:46.182573 140174032873216 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.17554351687431335, loss=1.7111471891403198
I0312 12:05:22.190300 140174024480512 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.20257872343063354, loss=1.8000925779342651
I0312 12:05:58.247690 140174032873216 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2178182303905487, loss=1.7561005353927612
I0312 12:06:34.270182 140174024480512 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.20067831873893738, loss=1.7488685846328735
I0312 12:07:10.262839 140174032873216 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.19302649796009064, loss=1.6663856506347656
I0312 12:07:46.243792 140174024480512 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2082526981830597, loss=1.8037786483764648
I0312 12:08:22.250204 140174032873216 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.19505344331264496, loss=1.7561112642288208
I0312 12:08:58.239910 140174024480512 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.2104676216840744, loss=1.765618920326233
I0312 12:09:34.245406 140174032873216 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.24713391065597534, loss=1.7696337699890137
I0312 12:10:10.258146 140174024480512 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.20303405821323395, loss=1.7555599212646484
I0312 12:10:46.298149 140174032873216 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2479606568813324, loss=1.7624728679656982
I0312 12:11:22.322725 140174024480512 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3457180857658386, loss=1.6550242900848389
I0312 12:11:58.388632 140174032873216 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.20059926807880402, loss=1.763240933418274
I0312 12:12:34.397314 140174024480512 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2547532916069031, loss=1.7929837703704834
I0312 12:13:10.388229 140174032873216 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.19821031391620636, loss=1.809617280960083
I0312 12:13:46.409002 140174024480512 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.259262353181839, loss=1.7114050388336182
I0312 12:14:22.418940 140174032873216 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.24538974463939667, loss=1.7089015245437622
I0312 12:14:58.468928 140174024480512 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.19419433176517487, loss=1.7348932027816772
I0312 12:15:34.492154 140174032873216 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19400669634342194, loss=1.737427830696106
I0312 12:16:10.499046 140174024480512 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2144114226102829, loss=1.7465808391571045
I0312 12:16:43.698202 140343717943104 spec.py:321] Evaluating on the training split.
I0312 12:16:46.719740 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:20:53.422642 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 12:20:56.156890 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:23:47.888510 140343717943104 spec.py:349] Evaluating on the test split.
I0312 12:23:50.610951 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:26:03.525807 140343717943104 submission_runner.py:420] Time since start: 29440.81s, 	Step: 48994, 	{'train/accuracy': 0.6467783451080322, 'train/loss': 1.6856553554534912, 'train/bleu': 31.408422055872666, 'validation/accuracy': 0.6677412390708923, 'validation/loss': 1.5390740633010864, 'validation/bleu': 28.786372928465706, 'validation/num_examples': 3000, 'test/accuracy': 0.6798210740089417, 'test/loss': 1.4563076496124268, 'test/bleu': 28.033193247154397, 'test/num_examples': 3003, 'score': 17679.945336341858, 'total_duration': 29440.80671453476, 'accumulated_submission_time': 17679.945336341858, 'accumulated_eval_time': 11758.793085098267, 'accumulated_logging_time': 0.5909426212310791}
I0312 12:26:03.545548 140174032873216 logging_writer.py:48] [48994] accumulated_eval_time=11758.793085, accumulated_logging_time=0.590943, accumulated_submission_time=17679.945336, global_step=48994, preemption_count=0, score=17679.945336, test/accuracy=0.679821, test/bleu=28.033193, test/loss=1.456308, test/num_examples=3003, total_duration=29440.806715, train/accuracy=0.646778, train/bleu=31.408422, train/loss=1.685655, validation/accuracy=0.667741, validation/bleu=28.786373, validation/loss=1.539074, validation/num_examples=3000
I0312 12:26:06.069346 140174024480512 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.22686311602592468, loss=1.8160182237625122
I0312 12:26:41.974478 140174032873216 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.20732104778289795, loss=1.6945819854736328
I0312 12:27:17.929565 140174024480512 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.1937621831893921, loss=1.7902966737747192
I0312 12:27:53.914693 140174032873216 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.20480658113956451, loss=1.710395336151123
I0312 12:28:29.916081 140174024480512 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.18792057037353516, loss=1.763749122619629
I0312 12:29:05.966809 140174032873216 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2040604203939438, loss=1.7028546333312988
I0312 12:29:41.968582 140174024480512 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.19274869561195374, loss=1.7673468589782715
I0312 12:30:17.965306 140174032873216 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3336854577064514, loss=1.7886600494384766
I0312 12:30:54.009811 140174024480512 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.2071334421634674, loss=1.7195008993148804
I0312 12:31:30.040006 140174032873216 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1802031546831131, loss=1.6713398694992065
I0312 12:32:06.039719 140174024480512 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.20332956314086914, loss=1.7879667282104492
I0312 12:32:42.046377 140174032873216 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.19375963509082794, loss=1.6847800016403198
I0312 12:33:18.041962 140174024480512 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.20315797626972198, loss=1.741344690322876
I0312 12:33:54.046737 140174032873216 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.22082194685935974, loss=1.741174578666687
I0312 12:34:30.107075 140174024480512 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.2121928334236145, loss=1.862399935722351
I0312 12:35:06.170562 140174032873216 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.22966475784778595, loss=1.7925388813018799
I0312 12:35:42.230594 140174024480512 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.19083209335803986, loss=1.7302597761154175
I0312 12:36:18.272573 140174032873216 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.1991806924343109, loss=1.7135651111602783
I0312 12:36:54.293591 140174024480512 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.20684564113616943, loss=1.7008744478225708
I0312 12:37:30.296226 140174032873216 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.46234598755836487, loss=1.7751233577728271
I0312 12:38:06.288247 140174024480512 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.1923258900642395, loss=1.6876529455184937
I0312 12:38:42.286769 140174032873216 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2328154593706131, loss=1.6723308563232422
I0312 12:39:18.313774 140174024480512 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1910022497177124, loss=1.7567572593688965
I0312 12:39:54.298120 140174032873216 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2114567905664444, loss=1.7795403003692627
I0312 12:40:03.738230 140343717943104 spec.py:321] Evaluating on the training split.
I0312 12:40:06.749220 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:43:37.316514 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 12:43:40.021412 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:46:30.100655 140343717943104 spec.py:349] Evaluating on the test split.
I0312 12:46:32.821845 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 12:49:14.474251 140343717943104 submission_runner.py:420] Time since start: 30831.76s, 	Step: 51328, 	{'train/accuracy': 0.6549112796783447, 'train/loss': 1.6199612617492676, 'train/bleu': 31.915483617633974, 'validation/accuracy': 0.6689687371253967, 'validation/loss': 1.5352182388305664, 'validation/bleu': 28.929565175389858, 'validation/num_examples': 3000, 'test/accuracy': 0.6797513365745544, 'test/loss': 1.4512735605239868, 'test/bleu': 28.20761255202798, 'test/num_examples': 3003, 'score': 18520.05733203888, 'total_duration': 30831.755182504654, 'accumulated_submission_time': 18520.05733203888, 'accumulated_eval_time': 12309.529057741165, 'accumulated_logging_time': 0.6208062171936035}
I0312 12:49:14.494769 140174024480512 logging_writer.py:48] [51328] accumulated_eval_time=12309.529058, accumulated_logging_time=0.620806, accumulated_submission_time=18520.057332, global_step=51328, preemption_count=0, score=18520.057332, test/accuracy=0.679751, test/bleu=28.207613, test/loss=1.451274, test/num_examples=3003, total_duration=30831.755183, train/accuracy=0.654911, train/bleu=31.915484, train/loss=1.619961, validation/accuracy=0.668969, validation/bleu=28.929565, validation/loss=1.535218, validation/num_examples=3000
I0312 12:49:40.692582 140174032873216 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.21911849081516266, loss=1.727630853652954
I0312 12:50:16.572299 140174024480512 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2768156826496124, loss=1.7571265697479248
I0312 12:50:52.548885 140174032873216 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.2008255124092102, loss=1.7322396039962769
I0312 12:51:28.549348 140174024480512 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2144446074962616, loss=1.7643280029296875
I0312 12:52:04.597005 140174032873216 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.18916836380958557, loss=1.7326115369796753
I0312 12:52:40.598900 140174024480512 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2016703486442566, loss=1.7880825996398926
I0312 12:53:16.576790 140174032873216 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.22693651914596558, loss=1.837905764579773
I0312 12:53:52.585698 140174024480512 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.18559624254703522, loss=1.739522099494934
I0312 12:54:28.626888 140174032873216 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20929144322872162, loss=1.6514618396759033
I0312 12:55:04.671273 140174024480512 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.2391226589679718, loss=1.695276141166687
I0312 12:55:40.687215 140174032873216 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.236812025308609, loss=1.7734235525131226
I0312 12:56:16.687700 140174024480512 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.21789909899234772, loss=1.7122468948364258
I0312 12:56:52.705167 140174032873216 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.19694975018501282, loss=1.7260620594024658
I0312 12:57:28.740228 140174024480512 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.23791132867336273, loss=1.7230342626571655
I0312 12:58:04.744415 140174032873216 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.20994256436824799, loss=1.7866382598876953
I0312 12:58:40.805062 140174024480512 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2039608359336853, loss=1.7279599905014038
I0312 12:59:16.798875 140174032873216 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.231694296002388, loss=1.7187700271606445
I0312 12:59:52.837100 140174024480512 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2638724744319916, loss=1.7335823774337769
I0312 13:00:28.858121 140174032873216 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.21070638298988342, loss=1.8265727758407593
I0312 13:01:04.882268 140174024480512 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.23122093081474304, loss=1.7183936834335327
I0312 13:01:40.949682 140174032873216 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1990717202425003, loss=1.7582951784133911
I0312 13:02:16.987482 140174024480512 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.22269552946090698, loss=1.765028953552246
I0312 13:02:53.019492 140174032873216 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.198186993598938, loss=1.7264158725738525
I0312 13:03:14.695590 140343717943104 spec.py:321] Evaluating on the training split.
I0312 13:03:17.706248 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:06:50.213042 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 13:06:52.924500 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:09:40.762935 140343717943104 spec.py:349] Evaluating on the test split.
I0312 13:09:43.468298 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:12:22.373020 140343717943104 submission_runner.py:420] Time since start: 32219.65s, 	Step: 53662, 	{'train/accuracy': 0.6495839357376099, 'train/loss': 1.6710714101791382, 'train/bleu': 31.521800955182695, 'validation/accuracy': 0.6688695549964905, 'validation/loss': 1.532265305519104, 'validation/bleu': 28.870555564318444, 'validation/num_examples': 3000, 'test/accuracy': 0.681552529335022, 'test/loss': 1.4458444118499756, 'test/bleu': 28.29054019765348, 'test/num_examples': 3003, 'score': 19360.178364753723, 'total_duration': 32219.653939008713, 'accumulated_submission_time': 19360.178364753723, 'accumulated_eval_time': 12857.206429243088, 'accumulated_logging_time': 0.6506195068359375}
I0312 13:12:22.393851 140174024480512 logging_writer.py:48] [53662] accumulated_eval_time=12857.206429, accumulated_logging_time=0.650620, accumulated_submission_time=19360.178365, global_step=53662, preemption_count=0, score=19360.178365, test/accuracy=0.681553, test/bleu=28.290540, test/loss=1.445844, test/num_examples=3003, total_duration=32219.653939, train/accuracy=0.649584, train/bleu=31.521801, train/loss=1.671071, validation/accuracy=0.668870, validation/bleu=28.870556, validation/loss=1.532265, validation/num_examples=3000
I0312 13:12:36.400503 140174032873216 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1916307806968689, loss=1.6537789106369019
I0312 13:13:12.342431 140174024480512 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.19032885134220123, loss=1.7446024417877197
I0312 13:13:48.275996 140174032873216 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.24298113584518433, loss=1.7106213569641113
I0312 13:14:24.275816 140174024480512 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.20092807710170746, loss=1.7181051969528198
I0312 13:15:00.261485 140174032873216 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2057422250509262, loss=1.718904733657837
I0312 13:15:36.244513 140174024480512 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.19250436127185822, loss=1.7040952444076538
I0312 13:16:12.254302 140174032873216 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2140575498342514, loss=1.8076409101486206
I0312 13:16:48.278418 140174024480512 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.2083137482404709, loss=1.6459970474243164
I0312 13:17:24.286603 140174032873216 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.20170052349567413, loss=1.6812583208084106
I0312 13:18:00.264394 140174024480512 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.24132628738880157, loss=1.815437912940979
I0312 13:18:36.280492 140174032873216 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.20934636890888214, loss=1.8292392492294312
I0312 13:19:12.352709 140174024480512 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.19823066890239716, loss=1.7318092584609985
I0312 13:19:48.363199 140174032873216 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2034641057252884, loss=1.7525228261947632
I0312 13:20:24.386334 140174024480512 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.19388844072818756, loss=1.7358962297439575
I0312 13:21:00.427320 140174032873216 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.24565888941287994, loss=1.6857253313064575
I0312 13:21:36.435269 140174024480512 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.20572254061698914, loss=1.7173993587493896
I0312 13:22:12.446354 140174032873216 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.18146257102489471, loss=1.6389106512069702
I0312 13:22:48.452328 140174024480512 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.19752447307109833, loss=1.6603349447250366
I0312 13:23:24.446840 140174032873216 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2503911852836609, loss=1.780166506767273
I0312 13:24:00.476984 140174024480512 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2182677984237671, loss=1.6659462451934814
I0312 13:24:36.513123 140174032873216 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.22639228403568268, loss=1.7377183437347412
I0312 13:25:12.539796 140174024480512 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.21645493805408478, loss=1.728273868560791
I0312 13:25:48.563847 140174032873216 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.21701548993587494, loss=1.7293035984039307
I0312 13:26:22.476414 140343717943104 spec.py:321] Evaluating on the training split.
I0312 13:26:25.484805 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:29:51.632526 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 13:29:54.346877 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:32:35.973238 140343717943104 spec.py:349] Evaluating on the test split.
I0312 13:32:38.694894 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:35:09.354198 140343717943104 submission_runner.py:420] Time since start: 33586.64s, 	Step: 55996, 	{'train/accuracy': 0.6558064818382263, 'train/loss': 1.641991376876831, 'train/bleu': 32.056675923308504, 'validation/accuracy': 0.6720685362815857, 'validation/loss': 1.5172662734985352, 'validation/bleu': 29.189644142973954, 'validation/num_examples': 3000, 'test/accuracy': 0.6832491159439087, 'test/loss': 1.438015103340149, 'test/bleu': 28.576496861826488, 'test/num_examples': 3003, 'score': 20200.181646585464, 'total_duration': 33586.63511300087, 'accumulated_submission_time': 20200.181646585464, 'accumulated_eval_time': 13384.084159612656, 'accumulated_logging_time': 0.6815006732940674}
I0312 13:35:09.376039 140174024480512 logging_writer.py:48] [55996] accumulated_eval_time=13384.084160, accumulated_logging_time=0.681501, accumulated_submission_time=20200.181647, global_step=55996, preemption_count=0, score=20200.181647, test/accuracy=0.683249, test/bleu=28.576497, test/loss=1.438015, test/num_examples=3003, total_duration=33586.635113, train/accuracy=0.655806, train/bleu=32.056676, train/loss=1.641991, validation/accuracy=0.672069, validation/bleu=29.189644, validation/loss=1.517266, validation/num_examples=3000
I0312 13:35:11.194648 140174032873216 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.21650807559490204, loss=1.6895767450332642
I0312 13:35:47.134994 140174024480512 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.1906467229127884, loss=1.6576473712921143
I0312 13:36:23.114568 140174032873216 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19278904795646667, loss=1.7525897026062012
I0312 13:36:59.108389 140174024480512 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.21510757505893707, loss=1.7325881719589233
I0312 13:37:35.134133 140174032873216 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.20088286697864532, loss=1.7467114925384521
I0312 13:38:11.141454 140174024480512 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.19387085735797882, loss=1.7010300159454346
I0312 13:38:47.153003 140174032873216 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2497074156999588, loss=1.856977105140686
I0312 13:39:23.189276 140174024480512 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.19886691868305206, loss=1.7138830423355103
I0312 13:39:59.247982 140174032873216 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.20450453460216522, loss=1.7103923559188843
I0312 13:40:35.284181 140174024480512 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2071383148431778, loss=1.7495756149291992
I0312 13:41:11.336465 140174032873216 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.19796441495418549, loss=1.6718343496322632
I0312 13:41:47.328488 140174024480512 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.20769572257995605, loss=1.643737554550171
I0312 13:42:23.317562 140174032873216 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.19630445539951324, loss=1.693243384361267
I0312 13:42:59.293411 140174024480512 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20141811668872833, loss=1.7655082941055298
I0312 13:43:35.307406 140174032873216 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.20089784264564514, loss=1.7653535604476929
I0312 13:44:11.334141 140174024480512 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.21963466703891754, loss=1.7711338996887207
I0312 13:44:47.352158 140174032873216 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2564994990825653, loss=1.7322264909744263
I0312 13:45:23.374996 140174024480512 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2023845762014389, loss=1.7184487581253052
I0312 13:45:59.370798 140174032873216 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.19845062494277954, loss=1.7829383611679077
I0312 13:46:35.378004 140174024480512 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2077251523733139, loss=1.7086819410324097
I0312 13:47:11.425761 140174032873216 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.1999415159225464, loss=1.6609416007995605
I0312 13:47:47.471566 140174024480512 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.23493944108486176, loss=1.7869763374328613
I0312 13:48:23.515956 140174032873216 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19358563423156738, loss=1.7552934885025024
I0312 13:48:59.561535 140174024480512 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.19667580723762512, loss=1.6426479816436768
I0312 13:49:09.367198 140343717943104 spec.py:321] Evaluating on the training split.
I0312 13:49:12.389776 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:53:44.987034 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 13:53:47.715748 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 13:57:31.381635 140343717943104 spec.py:349] Evaluating on the test split.
I0312 13:57:34.094744 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:00:41.165239 140343717943104 submission_runner.py:420] Time since start: 35118.45s, 	Step: 58329, 	{'train/accuracy': 0.657656729221344, 'train/loss': 1.6101428270339966, 'train/bleu': 31.937285743635364, 'validation/accuracy': 0.6714237928390503, 'validation/loss': 1.5166760683059692, 'validation/bleu': 29.03327787917762, 'validation/num_examples': 3000, 'test/accuracy': 0.6833652853965759, 'test/loss': 1.4308712482452393, 'test/bleu': 28.48284404893331, 'test/num_examples': 3003, 'score': 21040.09161424637, 'total_duration': 35118.44615530968, 'accumulated_submission_time': 21040.09161424637, 'accumulated_eval_time': 14075.882135391235, 'accumulated_logging_time': 0.7136006355285645}
I0312 14:00:41.186794 140174032873216 logging_writer.py:48] [58329] accumulated_eval_time=14075.882135, accumulated_logging_time=0.713601, accumulated_submission_time=21040.091614, global_step=58329, preemption_count=0, score=21040.091614, test/accuracy=0.683365, test/bleu=28.482844, test/loss=1.430871, test/num_examples=3003, total_duration=35118.446155, train/accuracy=0.657657, train/bleu=31.937286, train/loss=1.610143, validation/accuracy=0.671424, validation/bleu=29.033278, validation/loss=1.516676, validation/num_examples=3000
I0312 14:01:07.018543 140174024480512 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.20538000762462616, loss=1.7452691793441772
I0312 14:01:42.925330 140174032873216 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.18415190279483795, loss=1.654618263244629
I0312 14:02:18.908111 140174024480512 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.20411549508571625, loss=1.7190183401107788
I0312 14:02:54.941140 140174032873216 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.21702252328395844, loss=1.7096441984176636
I0312 14:03:31.050779 140174024480512 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2021862119436264, loss=1.7552770376205444
I0312 14:04:07.078506 140174032873216 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.2091418355703354, loss=1.686973214149475
I0312 14:04:43.056071 140174024480512 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.21465517580509186, loss=1.6949492692947388
I0312 14:05:19.059727 140174032873216 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19815926253795624, loss=1.7388471364974976
I0312 14:05:55.068533 140174024480512 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.18773092329502106, loss=1.6973824501037598
I0312 14:06:31.063557 140174032873216 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.19827526807785034, loss=1.612416386604309
I0312 14:07:07.114389 140174024480512 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.23463110625743866, loss=1.704175591468811
I0312 14:07:43.094329 140174032873216 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.21570338308811188, loss=1.706441879272461
I0312 14:08:19.106538 140174024480512 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.19447165727615356, loss=1.751845121383667
I0312 14:08:55.091619 140174032873216 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.21349941194057465, loss=1.7182719707489014
I0312 14:09:31.095701 140174024480512 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.20319554209709167, loss=1.6727690696716309
I0312 14:10:07.085026 140174032873216 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.20422211289405823, loss=1.6570944786071777
I0312 14:10:43.167404 140174024480512 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.19290754199028015, loss=1.7280864715576172
I0312 14:11:19.258242 140174032873216 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.19725558161735535, loss=1.7060688734054565
I0312 14:11:55.300222 140174024480512 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.1925814300775528, loss=1.6492927074432373
I0312 14:12:31.334192 140174032873216 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.1935623437166214, loss=1.6449284553527832
I0312 14:13:07.404675 140174024480512 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2142982929944992, loss=1.8019871711730957
I0312 14:13:43.421006 140174032873216 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2157396823167801, loss=1.7558037042617798
I0312 14:14:19.423893 140174024480512 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.24844251573085785, loss=1.691099762916565
I0312 14:14:41.491649 140343717943104 spec.py:321] Evaluating on the training split.
I0312 14:14:44.504196 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:18:51.260471 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 14:18:53.970526 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:22:15.906817 140343717943104 spec.py:349] Evaluating on the test split.
I0312 14:22:18.603897 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:25:09.298366 140343717943104 submission_runner.py:420] Time since start: 36586.58s, 	Step: 60663, 	{'train/accuracy': 0.6537819504737854, 'train/loss': 1.6467565298080444, 'train/bleu': 32.499781876189076, 'validation/accuracy': 0.671584963798523, 'validation/loss': 1.510255217552185, 'validation/bleu': 28.973790934716995, 'validation/num_examples': 3000, 'test/accuracy': 0.6852594614028931, 'test/loss': 1.4206849336624146, 'test/bleu': 28.83500179481324, 'test/num_examples': 3003, 'score': 21880.315083026886, 'total_duration': 36586.57926249504, 'accumulated_submission_time': 21880.315083026886, 'accumulated_eval_time': 14703.68877029419, 'accumulated_logging_time': 0.7454655170440674}
I0312 14:25:09.320689 140174032873216 logging_writer.py:48] [60663] accumulated_eval_time=14703.688770, accumulated_logging_time=0.745466, accumulated_submission_time=21880.315083, global_step=60663, preemption_count=0, score=21880.315083, test/accuracy=0.685259, test/bleu=28.835002, test/loss=1.420685, test/num_examples=3003, total_duration=36586.579262, train/accuracy=0.653782, train/bleu=32.499782, train/loss=1.646757, validation/accuracy=0.671585, validation/bleu=28.973791, validation/loss=1.510255, validation/num_examples=3000
I0312 14:25:22.956474 140174024480512 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.19390013813972473, loss=1.654681921005249
I0312 14:25:58.835238 140174032873216 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.19170939922332764, loss=1.6721036434173584
I0312 14:26:34.785133 140174024480512 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.4858384132385254, loss=1.7257972955703735
I0312 14:27:10.781712 140174032873216 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.23248562216758728, loss=1.7321536540985107
I0312 14:27:46.797331 140174024480512 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.21269693970680237, loss=1.7812613248825073
I0312 14:28:22.794276 140174032873216 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.203413724899292, loss=1.7201533317565918
I0312 14:28:58.787402 140174024480512 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.20305240154266357, loss=1.6878328323364258
I0312 14:29:34.822143 140174032873216 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.20582892000675201, loss=1.7080856561660767
I0312 14:30:10.842653 140174024480512 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19003862142562866, loss=1.7506476640701294
I0312 14:30:46.865709 140174032873216 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.22326140105724335, loss=1.7539033889770508
I0312 14:31:22.888154 140174024480512 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.22337745130062103, loss=1.7458775043487549
I0312 14:31:58.970323 140174032873216 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.20151585340499878, loss=1.6939324140548706
I0312 14:32:35.004638 140174024480512 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.19370687007904053, loss=1.6156537532806396
I0312 14:33:11.027354 140174032873216 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.20262837409973145, loss=1.7256383895874023
I0312 14:33:47.027633 140174024480512 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.19298522174358368, loss=1.7745552062988281
I0312 14:34:23.029510 140174032873216 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.1923333704471588, loss=1.713468074798584
I0312 14:34:59.022095 140174024480512 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.1861998736858368, loss=1.6678121089935303
I0312 14:35:35.045093 140174032873216 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2529917061328888, loss=1.6042604446411133
I0312 14:36:11.059609 140174024480512 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.29118311405181885, loss=1.6685748100280762
I0312 14:36:47.093560 140174032873216 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2121642529964447, loss=1.7068274021148682
I0312 14:37:23.104496 140174024480512 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.24439147114753723, loss=1.7004822492599487
I0312 14:37:59.143676 140174032873216 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.1961531639099121, loss=1.641578197479248
I0312 14:38:35.148424 140174024480512 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.20183667540550232, loss=1.7277581691741943
I0312 14:39:09.437137 140343717943104 spec.py:321] Evaluating on the training split.
I0312 14:39:12.455826 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:43:14.754430 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 14:43:17.476938 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:46:21.014187 140343717943104 spec.py:349] Evaluating on the test split.
I0312 14:46:23.749911 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 14:49:21.350426 140343717943104 submission_runner.py:420] Time since start: 38038.63s, 	Step: 62997, 	{'train/accuracy': 0.6693201661109924, 'train/loss': 1.52785062789917, 'train/bleu': 33.4739314800073, 'validation/accuracy': 0.6742631793022156, 'validation/loss': 1.5014219284057617, 'validation/bleu': 29.266614300038707, 'validation/num_examples': 3000, 'test/accuracy': 0.686967670917511, 'test/loss': 1.4123915433883667, 'test/bleu': 28.856663573822804, 'test/num_examples': 3003, 'score': 22720.353177785873, 'total_duration': 38038.63135719299, 'accumulated_submission_time': 22720.353177785873, 'accumulated_eval_time': 15315.602014303207, 'accumulated_logging_time': 0.77724289894104}
I0312 14:49:21.372704 140174032873216 logging_writer.py:48] [62997] accumulated_eval_time=15315.602014, accumulated_logging_time=0.777243, accumulated_submission_time=22720.353178, global_step=62997, preemption_count=0, score=22720.353178, test/accuracy=0.686968, test/bleu=28.856664, test/loss=1.412392, test/num_examples=3003, total_duration=38038.631357, train/accuracy=0.669320, train/bleu=33.473931, train/loss=1.527851, validation/accuracy=0.674263, validation/bleu=29.266614, validation/loss=1.501422, validation/num_examples=3000
I0312 14:49:22.823575 140174024480512 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.1968582719564438, loss=1.6832245588302612
I0312 14:49:58.694310 140174032873216 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.19750122725963593, loss=1.704664945602417
I0312 14:50:34.637321 140174024480512 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.22932295501232147, loss=1.7642560005187988
I0312 14:51:10.624204 140174032873216 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.1944098323583603, loss=1.622627854347229
I0312 14:51:46.622936 140174024480512 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2168770134449005, loss=1.7197117805480957
I0312 14:52:22.614571 140174032873216 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.22867214679718018, loss=1.7170042991638184
I0312 14:52:58.615101 140174024480512 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.1950724571943283, loss=1.5944790840148926
I0312 14:53:34.632335 140174032873216 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20130234956741333, loss=1.6579333543777466
I0312 14:54:10.630832 140174024480512 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.20489472150802612, loss=1.7110283374786377
I0312 14:54:46.653462 140174032873216 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.24199643731117249, loss=1.6390986442565918
I0312 14:55:22.669437 140174024480512 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2201056331396103, loss=1.7327829599380493
I0312 14:55:58.673683 140174032873216 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.20917244255542755, loss=1.7530158758163452
I0312 14:56:34.749226 140174024480512 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2184024155139923, loss=1.714733600616455
I0312 14:57:10.788112 140174032873216 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19128857553005219, loss=1.733081579208374
I0312 14:57:46.803026 140174024480512 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.20975379645824432, loss=1.7307285070419312
I0312 14:58:22.809724 140174032873216 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2084168791770935, loss=1.7037644386291504
I0312 14:58:58.814818 140174024480512 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.18882372975349426, loss=1.6698598861694336
I0312 14:59:34.834779 140174032873216 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.20545941591262817, loss=1.7427458763122559
I0312 15:00:10.861205 140174024480512 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2060854434967041, loss=1.6831773519515991
I0312 15:00:46.913421 140174032873216 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.21123558282852173, loss=1.6841233968734741
I0312 15:01:22.957208 140174024480512 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.23065464198589325, loss=1.6777492761611938
I0312 15:01:58.972115 140174032873216 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.20706593990325928, loss=1.7133655548095703
I0312 15:02:35.028892 140174024480512 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.19565357267856598, loss=1.6612820625305176
I0312 15:03:11.054593 140174032873216 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.24377210438251495, loss=1.578741431236267
I0312 15:03:21.583475 140343717943104 spec.py:321] Evaluating on the training split.
I0312 15:03:24.589985 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:07:56.231754 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 15:07:58.949682 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:11:30.640003 140343717943104 spec.py:349] Evaluating on the test split.
I0312 15:11:33.345340 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:15:06.514987 140343717943104 submission_runner.py:420] Time since start: 39583.80s, 	Step: 65331, 	{'train/accuracy': 0.6595335006713867, 'train/loss': 1.6017496585845947, 'train/bleu': 32.48676522191736, 'validation/accuracy': 0.6758750677108765, 'validation/loss': 1.486812710762024, 'validation/bleu': 29.294479007176676, 'validation/num_examples': 3000, 'test/accuracy': 0.6884666681289673, 'test/loss': 1.4014354944229126, 'test/bleu': 28.838852312984258, 'test/num_examples': 3003, 'score': 23560.48533654213, 'total_duration': 39583.79589796066, 'accumulated_submission_time': 23560.48533654213, 'accumulated_eval_time': 16020.533456087112, 'accumulated_logging_time': 0.8095309734344482}
I0312 15:15:06.537127 140174024480512 logging_writer.py:48] [65331] accumulated_eval_time=16020.533456, accumulated_logging_time=0.809531, accumulated_submission_time=23560.485337, global_step=65331, preemption_count=0, score=23560.485337, test/accuracy=0.688467, test/bleu=28.838852, test/loss=1.401435, test/num_examples=3003, total_duration=39583.795898, train/accuracy=0.659534, train/bleu=32.486765, train/loss=1.601750, validation/accuracy=0.675875, validation/bleu=29.294479, validation/loss=1.486813, validation/num_examples=3000
I0312 15:15:31.647132 140174032873216 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.19227075576782227, loss=1.684311866760254
I0312 15:16:07.518033 140174024480512 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2021298110485077, loss=1.7161941528320312
I0312 15:16:43.556377 140174032873216 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.20098662376403809, loss=1.6745762825012207
I0312 15:17:19.560502 140174024480512 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20256689190864563, loss=1.7721940279006958
I0312 15:17:55.619595 140174032873216 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.20799535512924194, loss=1.6684160232543945
I0312 15:18:31.624795 140174024480512 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.18814866244792938, loss=1.6437430381774902
I0312 15:19:07.629985 140174032873216 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19714118540287018, loss=1.7467321157455444
I0312 15:19:43.611092 140174024480512 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.27896544337272644, loss=1.7286702394485474
I0312 15:20:19.594923 140174032873216 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.19565460085868835, loss=1.710534930229187
I0312 15:20:55.634258 140174024480512 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.20359663665294647, loss=1.7347737550735474
I0312 15:21:31.709888 140174032873216 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.6461173295974731, loss=1.6967500448226929
I0312 15:22:07.736948 140174024480512 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.18962208926677704, loss=1.6865226030349731
I0312 15:22:43.772198 140174032873216 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2073085606098175, loss=1.659375786781311
I0312 15:23:19.835713 140174024480512 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.21224093437194824, loss=1.625802755355835
I0312 15:23:55.873274 140174032873216 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.20179250836372375, loss=1.637215256690979
I0312 15:24:31.874990 140174024480512 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.19857028126716614, loss=1.7027559280395508
I0312 15:25:07.872873 140174032873216 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.18741077184677124, loss=1.6464406251907349
I0312 15:25:43.860347 140174024480512 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.1913451999425888, loss=1.7383018732070923
I0312 15:26:19.844240 140174032873216 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.20019041001796722, loss=1.6643970012664795
I0312 15:26:55.890636 140174024480512 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.22798094153404236, loss=1.713135838508606
I0312 15:27:31.896915 140174032873216 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.21134509146213531, loss=1.7209845781326294
I0312 15:28:07.905871 140174024480512 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.19242632389068604, loss=1.6083263158798218
I0312 15:28:43.898090 140174032873216 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2029532492160797, loss=1.6075135469436646
I0312 15:29:06.649205 140343717943104 spec.py:321] Evaluating on the training split.
I0312 15:29:09.678558 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:33:34.901515 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 15:33:37.617714 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:36:41.860404 140343717943104 spec.py:349] Evaluating on the test split.
I0312 15:36:44.583491 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:39:03.055847 140343717943104 submission_runner.py:420] Time since start: 41020.34s, 	Step: 67665, 	{'train/accuracy': 0.660723090171814, 'train/loss': 1.6009280681610107, 'train/bleu': 32.36851512432437, 'validation/accuracy': 0.6756022572517395, 'validation/loss': 1.4917354583740234, 'validation/bleu': 29.302940848683274, 'validation/num_examples': 3000, 'test/accuracy': 0.6875835657119751, 'test/loss': 1.4012601375579834, 'test/bleu': 28.650583418270205, 'test/num_examples': 3003, 'score': 24400.5163128376, 'total_duration': 41020.33675789833, 'accumulated_submission_time': 24400.5163128376, 'accumulated_eval_time': 16616.940055847168, 'accumulated_logging_time': 0.8407487869262695}
I0312 15:39:03.078406 140174024480512 logging_writer.py:48] [67665] accumulated_eval_time=16616.940056, accumulated_logging_time=0.840749, accumulated_submission_time=24400.516313, global_step=67665, preemption_count=0, score=24400.516313, test/accuracy=0.687584, test/bleu=28.650583, test/loss=1.401260, test/num_examples=3003, total_duration=41020.336758, train/accuracy=0.660723, train/bleu=32.368515, train/loss=1.600928, validation/accuracy=0.675602, validation/bleu=29.302941, validation/loss=1.491735, validation/num_examples=3000
I0312 15:39:16.079441 140174032873216 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.22945138812065125, loss=1.730492353439331
I0312 15:39:52.008689 140174024480512 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2142399400472641, loss=1.722151517868042
I0312 15:40:27.969662 140174032873216 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.19176527857780457, loss=1.7222434282302856
I0312 15:41:03.967561 140174024480512 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.21902412176132202, loss=1.6841413974761963
I0312 15:41:39.971990 140174032873216 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.21574898064136505, loss=1.6477444171905518
I0312 15:42:15.971145 140174024480512 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.19673433899879456, loss=1.6639662981033325
I0312 15:42:51.978791 140174032873216 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.19573058187961578, loss=1.6363121271133423
I0312 15:43:27.975551 140174024480512 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2014431655406952, loss=1.5859285593032837
I0312 15:44:03.984767 140174032873216 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.18379338085651398, loss=1.6646085977554321
I0312 15:44:39.991958 140174024480512 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.19920890033245087, loss=1.6367827653884888
I0312 15:45:15.992825 140174032873216 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.24362142384052277, loss=1.608972430229187
I0312 15:45:52.009377 140174024480512 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.1994997262954712, loss=1.6813472509384155
I0312 15:46:28.019155 140174032873216 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.1812499314546585, loss=1.677129864692688
I0312 15:47:04.033282 140174024480512 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.1905083954334259, loss=1.6497074365615845
I0312 15:47:40.031008 140174032873216 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.20648743212223053, loss=1.7201710939407349
I0312 15:48:16.009685 140174024480512 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.19596099853515625, loss=1.6546119451522827
I0312 15:48:51.999643 140174032873216 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.20179419219493866, loss=1.711997389793396
I0312 15:49:28.044438 140174024480512 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.18928687274456024, loss=1.6338387727737427
I0312 15:50:04.018976 140174032873216 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.23729737102985382, loss=1.6191949844360352
I0312 15:50:40.015623 140174024480512 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.20561100542545319, loss=1.6062114238739014
I0312 15:51:16.050773 140174032873216 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2086237668991089, loss=1.6824959516525269
I0312 15:51:52.071979 140174024480512 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.17657779157161713, loss=1.6467598676681519
I0312 15:52:28.067796 140174032873216 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.19079405069351196, loss=1.6471766233444214
I0312 15:53:03.413552 140343717943104 spec.py:321] Evaluating on the training split.
I0312 15:53:06.423416 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 15:57:37.772601 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 15:57:40.481573 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:01:31.433129 140343717943104 spec.py:349] Evaluating on the test split.
I0312 16:01:34.149636 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:05:19.257617 140343717943104 submission_runner.py:420] Time since start: 42596.54s, 	Step: 70000, 	{'train/accuracy': 0.6662484407424927, 'train/loss': 1.547742247581482, 'train/bleu': 32.96686890315351, 'validation/accuracy': 0.678317666053772, 'validation/loss': 1.4796814918518066, 'validation/bleu': 29.641713014224447, 'validation/num_examples': 3000, 'test/accuracy': 0.6928824782371521, 'test/loss': 1.385662317276001, 'test/bleu': 29.57294526056814, 'test/num_examples': 3003, 'score': 25240.725229740143, 'total_duration': 42596.53853440285, 'accumulated_submission_time': 25240.725229740143, 'accumulated_eval_time': 17352.784069776535, 'accumulated_logging_time': 0.9213240146636963}
I0312 16:05:19.281304 140174024480512 logging_writer.py:48] [70000] accumulated_eval_time=17352.784070, accumulated_logging_time=0.921324, accumulated_submission_time=25240.725230, global_step=70000, preemption_count=0, score=25240.725230, test/accuracy=0.692882, test/bleu=29.572945, test/loss=1.385662, test/num_examples=3003, total_duration=42596.538534, train/accuracy=0.666248, train/bleu=32.966869, train/loss=1.547742, validation/accuracy=0.678318, validation/bleu=29.641713, validation/loss=1.479681, validation/num_examples=3000
I0312 16:05:19.653025 140174032873216 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.21556824445724487, loss=1.5549458265304565
I0312 16:05:55.541163 140174024480512 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2174670398235321, loss=1.7389602661132812
I0312 16:06:31.440858 140174032873216 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.23071075975894928, loss=1.717624545097351
I0312 16:07:07.397680 140174024480512 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.20965927839279175, loss=1.7071479558944702
I0312 16:07:43.385999 140174032873216 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.1861177533864975, loss=1.7416914701461792
I0312 16:08:19.410098 140174024480512 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.20359060168266296, loss=1.6249727010726929
I0312 16:08:55.423646 140174032873216 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.20151029527187347, loss=1.6240562200546265
I0312 16:09:31.474967 140174024480512 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.20850569009780884, loss=1.6644706726074219
I0312 16:10:07.508412 140174032873216 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.20247489213943481, loss=1.7629281282424927
I0312 16:10:43.544700 140174024480512 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.19507178664207458, loss=1.6352717876434326
I0312 16:11:19.584699 140174032873216 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.22807417809963226, loss=1.692598581314087
I0312 16:11:55.607952 140174024480512 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.21540740132331848, loss=1.6184189319610596
I0312 16:12:31.653212 140174032873216 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.19775496423244476, loss=1.6203871965408325
I0312 16:13:07.677495 140174024480512 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.22051697969436646, loss=1.7174128293991089
I0312 16:13:43.717777 140174032873216 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.20955988764762878, loss=1.6524919271469116
I0312 16:14:19.738222 140174024480512 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.23116953670978546, loss=1.6527066230773926
I0312 16:14:55.792363 140174032873216 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.1918284147977829, loss=1.6540536880493164
I0312 16:15:31.847998 140174024480512 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.1891372799873352, loss=1.604779839515686
I0312 16:16:07.907644 140174032873216 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.20924271643161774, loss=1.7330906391143799
I0312 16:16:43.907611 140174024480512 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2569217085838318, loss=1.681727647781372
I0312 16:17:19.960289 140174032873216 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.18794693052768707, loss=1.575347900390625
I0312 16:17:55.968089 140174024480512 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.2566707730293274, loss=1.6459050178527832
I0312 16:18:32.001435 140174032873216 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.21831980347633362, loss=1.7090178728103638
I0312 16:19:08.008470 140174024480512 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.19319023191928864, loss=1.6233793497085571
I0312 16:19:19.615126 140343717943104 spec.py:321] Evaluating on the training split.
I0312 16:19:22.619033 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:23:45.921092 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 16:23:48.628862 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:27:41.290507 140343717943104 spec.py:349] Evaluating on the test split.
I0312 16:27:44.014018 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:31:21.282675 140343717943104 submission_runner.py:420] Time since start: 44158.56s, 	Step: 72334, 	{'train/accuracy': 0.658568263053894, 'train/loss': 1.5997875928878784, 'train/bleu': 32.689614051614335, 'validation/accuracy': 0.6778836846351624, 'validation/loss': 1.4754083156585693, 'validation/bleu': 29.733602133158186, 'validation/num_examples': 3000, 'test/accuracy': 0.6930567622184753, 'test/loss': 1.3839657306671143, 'test/bleu': 29.155925471336744, 'test/num_examples': 3003, 'score': 26080.97846698761, 'total_duration': 44158.56359124184, 'accumulated_submission_time': 26080.97846698761, 'accumulated_eval_time': 18074.45157289505, 'accumulated_logging_time': 0.9541702270507812}
I0312 16:31:21.307011 140174032873216 logging_writer.py:48] [72334] accumulated_eval_time=18074.451573, accumulated_logging_time=0.954170, accumulated_submission_time=26080.978467, global_step=72334, preemption_count=0, score=26080.978467, test/accuracy=0.693057, test/bleu=29.155925, test/loss=1.383966, test/num_examples=3003, total_duration=44158.563591, train/accuracy=0.658568, train/bleu=32.689614, train/loss=1.599788, validation/accuracy=0.677884, validation/bleu=29.733602, validation/loss=1.475408, validation/num_examples=3000
I0312 16:31:45.347484 140174024480512 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.20938368141651154, loss=1.6990711688995361
I0312 16:32:21.242722 140174032873216 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.20580187439918518, loss=1.6975481510162354
I0312 16:32:57.193251 140174024480512 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2126738429069519, loss=1.5856266021728516
I0312 16:33:33.194427 140174032873216 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.20832476019859314, loss=1.6809556484222412
I0312 16:34:09.199383 140174024480512 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3782181739807129, loss=1.6860061883926392
I0312 16:34:45.197602 140174032873216 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2110913246870041, loss=1.5730758905410767
I0312 16:35:21.195607 140174024480512 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.22471384704113007, loss=1.677333950996399
I0312 16:35:57.212117 140174032873216 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.20211313664913177, loss=1.6503461599349976
I0312 16:36:33.215901 140174024480512 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.227487802505493, loss=1.652496337890625
I0312 16:37:09.266192 140174032873216 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.21886199712753296, loss=1.6132683753967285
I0312 16:37:45.266128 140174024480512 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.1886788010597229, loss=1.674897313117981
I0312 16:38:21.273664 140174032873216 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.1971619725227356, loss=1.5829493999481201
I0312 16:38:57.258582 140174024480512 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.2210463434457779, loss=1.6842622756958008
I0312 16:39:33.232677 140174032873216 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.21066632866859436, loss=1.6568316221237183
I0312 16:40:09.264811 140174024480512 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.188800647854805, loss=1.636858344078064
I0312 16:40:45.270866 140174032873216 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.21412652730941772, loss=1.649383544921875
I0312 16:41:21.286005 140174024480512 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.22228416800498962, loss=1.606622338294983
I0312 16:41:57.325277 140174032873216 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20054736733436584, loss=1.6660293340682983
I0312 16:42:33.358036 140174024480512 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.23227237164974213, loss=1.6402010917663574
I0312 16:43:09.374236 140174032873216 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.20482894778251648, loss=1.7031137943267822
I0312 16:43:45.359067 140174024480512 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.21646612882614136, loss=1.6949079036712646
I0312 16:44:21.356194 140174032873216 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.19075702130794525, loss=1.6505520343780518
I0312 16:44:57.369410 140174024480512 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.18760472536087036, loss=1.6162723302841187
I0312 16:45:21.598957 140343717943104 spec.py:321] Evaluating on the training split.
I0312 16:45:24.608932 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:49:31.184404 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 16:49:33.896097 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:52:19.433919 140343717943104 spec.py:349] Evaluating on the test split.
I0312 16:52:22.142729 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 16:54:56.624679 140343717943104 submission_runner.py:420] Time since start: 45573.91s, 	Step: 74669, 	{'train/accuracy': 0.6603477597236633, 'train/loss': 1.6007717847824097, 'train/bleu': 32.826282796088854, 'validation/accuracy': 0.6780697107315063, 'validation/loss': 1.4693963527679443, 'validation/bleu': 29.297419152581835, 'validation/num_examples': 3000, 'test/accuracy': 0.6938470005989075, 'test/loss': 1.376531720161438, 'test/bleu': 29.201825259701923, 'test/num_examples': 3003, 'score': 26921.192920207977, 'total_duration': 45573.90559220314, 'accumulated_submission_time': 26921.192920207977, 'accumulated_eval_time': 18649.47722673416, 'accumulated_logging_time': 0.9877212047576904}
I0312 16:54:56.648826 140174032873216 logging_writer.py:48] [74669] accumulated_eval_time=18649.477227, accumulated_logging_time=0.987721, accumulated_submission_time=26921.192920, global_step=74669, preemption_count=0, score=26921.192920, test/accuracy=0.693847, test/bleu=29.201825, test/loss=1.376532, test/num_examples=3003, total_duration=45573.905592, train/accuracy=0.660348, train/bleu=32.826283, train/loss=1.600772, validation/accuracy=0.678070, validation/bleu=29.297419, validation/loss=1.469396, validation/num_examples=3000
I0312 16:55:08.139120 140174024480512 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.21503543853759766, loss=1.681229829788208
I0312 16:55:44.078977 140174032873216 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.20784015953540802, loss=1.6373499631881714
I0312 16:56:20.027256 140174024480512 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.20694537460803986, loss=1.7013802528381348
I0312 16:56:56.012967 140174032873216 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.24042733013629913, loss=1.665228247642517
I0312 16:57:32.021898 140174024480512 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2157820612192154, loss=1.6601274013519287
I0312 16:58:08.020353 140174032873216 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.203389510512352, loss=1.7328358888626099
I0312 16:58:44.018678 140174024480512 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.20991171896457672, loss=1.6431506872177124
I0312 16:59:20.018575 140174032873216 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.2008143663406372, loss=1.6204582452774048
I0312 16:59:56.095978 140174024480512 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.20836573839187622, loss=1.5787006616592407
I0312 17:00:32.115864 140174032873216 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2072569578886032, loss=1.6658133268356323
I0312 17:01:08.124799 140174024480512 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.22471651434898376, loss=1.6642969846725464
I0312 17:01:44.100112 140174032873216 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.20611156523227692, loss=1.7503937482833862
I0312 17:02:20.080778 140174024480512 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.20308949053287506, loss=1.6674607992172241
I0312 17:02:56.136220 140174032873216 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20890416204929352, loss=1.6896520853042603
I0312 17:03:32.166595 140174024480512 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.20197437703609467, loss=1.584172010421753
I0312 17:04:08.196787 140174032873216 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.1916249394416809, loss=1.6110272407531738
I0312 17:04:44.236483 140174024480512 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.6520226001739502, loss=1.6599637269973755
I0312 17:05:20.260342 140174032873216 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.20842158794403076, loss=1.7179789543151855
I0312 17:05:56.241658 140174024480512 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.21332108974456787, loss=1.6082515716552734
I0312 17:06:32.294714 140174032873216 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.20435716211795807, loss=1.5553029775619507
I0312 17:07:08.328062 140174024480512 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2079392373561859, loss=1.6809519529342651
I0312 17:07:44.316656 140174032873216 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.20043832063674927, loss=1.6644126176834106
I0312 17:08:20.358546 140174024480512 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.20630040764808655, loss=1.5830743312835693
I0312 17:08:56.366242 140174032873216 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2002141922712326, loss=1.6079556941986084
I0312 17:08:56.806431 140343717943104 spec.py:321] Evaluating on the training split.
I0312 17:08:59.814279 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 17:13:17.931279 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 17:13:20.652647 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 17:17:14.351764 140343717943104 spec.py:349] Evaluating on the test split.
I0312 17:17:17.068716 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 17:21:02.382505 140343717943104 submission_runner.py:420] Time since start: 47139.66s, 	Step: 77003, 	{'train/accuracy': 0.6674932241439819, 'train/loss': 1.5427851676940918, 'train/bleu': 33.33707098666702, 'validation/accuracy': 0.6805866956710815, 'validation/loss': 1.4604946374893188, 'validation/bleu': 29.745982941785442, 'validation/num_examples': 3000, 'test/accuracy': 0.6957759857177734, 'test/loss': 1.3691195249557495, 'test/bleu': 29.476644371476635, 'test/num_examples': 3003, 'score': 27761.27295565605, 'total_duration': 47139.663432359695, 'accumulated_submission_time': 27761.27295565605, 'accumulated_eval_time': 19375.053251981735, 'accumulated_logging_time': 1.0212042331695557}
I0312 17:21:02.406764 140174024480512 logging_writer.py:48] [77003] accumulated_eval_time=19375.053252, accumulated_logging_time=1.021204, accumulated_submission_time=27761.272956, global_step=77003, preemption_count=0, score=27761.272956, test/accuracy=0.695776, test/bleu=29.476644, test/loss=1.369120, test/num_examples=3003, total_duration=47139.663432, train/accuracy=0.667493, train/bleu=33.337071, train/loss=1.542785, validation/accuracy=0.680587, validation/bleu=29.745983, validation/loss=1.460495, validation/num_examples=3000
I0312 17:21:37.590214 140174032873216 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.20632438361644745, loss=1.6602312326431274
I0312 17:22:13.555382 140174024480512 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.19978858530521393, loss=1.6306366920471191
I0312 17:22:49.570217 140174032873216 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21407316625118256, loss=1.7225744724273682
I0312 17:23:25.561534 140174024480512 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.22500407695770264, loss=1.7163437604904175
I0312 17:24:01.607029 140174032873216 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.1916610449552536, loss=1.5980011224746704
I0312 17:24:37.692153 140174024480512 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.20645654201507568, loss=1.6554642915725708
I0312 17:25:13.745787 140174032873216 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.21241576969623566, loss=1.6240406036376953
I0312 17:25:49.778276 140174024480512 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.22218096256256104, loss=1.6895828247070312
I0312 17:26:25.772920 140174032873216 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.19944235682487488, loss=1.6781187057495117
I0312 17:27:01.789561 140174024480512 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.22930803894996643, loss=1.7709063291549683
I0312 17:27:37.809843 140174032873216 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.18784020841121674, loss=1.5952744483947754
I0312 17:28:13.837236 140174024480512 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.19770799577236176, loss=1.5759711265563965
I0312 17:28:49.833641 140174032873216 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2115752249956131, loss=1.6016725301742554
I0312 17:29:25.853024 140174024480512 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.19640007615089417, loss=1.5857995748519897
I0312 17:30:01.878404 140174032873216 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.20172980427742004, loss=1.6571855545043945
I0312 17:30:37.867989 140174024480512 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.20870685577392578, loss=1.6315912008285522
I0312 17:31:13.895671 140174032873216 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2098255753517151, loss=1.5927338600158691
I0312 17:31:49.960747 140174024480512 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2142336219549179, loss=1.6344430446624756
I0312 17:32:25.972659 140174032873216 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.20455588400363922, loss=1.5661660432815552
I0312 17:33:01.974301 140174024480512 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21098899841308594, loss=1.6162538528442383
I0312 17:33:38.043605 140174032873216 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.20474928617477417, loss=1.5946052074432373
I0312 17:34:14.078190 140174024480512 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.20547856390476227, loss=1.6429873704910278
I0312 17:34:50.076009 140174032873216 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.1948576718568802, loss=1.6061497926712036
I0312 17:35:02.394452 140343717943104 spec.py:321] Evaluating on the training split.
I0312 17:35:05.402022 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 17:39:27.064696 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 17:39:29.773711 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 17:42:19.907366 140343717943104 spec.py:349] Evaluating on the test split.
I0312 17:42:22.624338 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 17:45:14.442018 140343717943104 submission_runner.py:420] Time since start: 48591.72s, 	Step: 79336, 	{'train/accuracy': 0.662295937538147, 'train/loss': 1.577651858329773, 'train/bleu': 33.02561788436131, 'validation/accuracy': 0.6813926696777344, 'validation/loss': 1.4510258436203003, 'validation/bleu': 29.73820499849484, 'validation/num_examples': 3000, 'test/accuracy': 0.6976584792137146, 'test/loss': 1.358900547027588, 'test/bleu': 29.926070662587613, 'test/num_examples': 3003, 'score': 28601.181200027466, 'total_duration': 48591.722954034805, 'accumulated_submission_time': 28601.181200027466, 'accumulated_eval_time': 19987.100786447525, 'accumulated_logging_time': 1.0545799732208252}
I0312 17:45:14.465888 140174024480512 logging_writer.py:48] [79336] accumulated_eval_time=19987.100786, accumulated_logging_time=1.054580, accumulated_submission_time=28601.181200, global_step=79336, preemption_count=0, score=28601.181200, test/accuracy=0.697658, test/bleu=29.926071, test/loss=1.358901, test/num_examples=3003, total_duration=48591.722954, train/accuracy=0.662296, train/bleu=33.025618, train/loss=1.577652, validation/accuracy=0.681393, validation/bleu=29.738205, validation/loss=1.451026, validation/num_examples=3000
I0312 17:45:37.784602 140174032873216 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.2035536766052246, loss=1.6221442222595215
I0312 17:46:13.692951 140174024480512 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2038794606924057, loss=1.740120768547058
I0312 17:46:49.685265 140174032873216 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.20910657942295074, loss=1.6815171241760254
I0312 17:47:25.675971 140174024480512 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.20343521237373352, loss=1.6101067066192627
I0312 17:48:01.664657 140174032873216 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.1995023638010025, loss=1.6483869552612305
I0312 17:48:37.650326 140174024480512 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2146719992160797, loss=1.609748363494873
I0312 17:49:13.676484 140174032873216 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.21787194907665253, loss=1.6157835721969604
I0312 17:49:49.711433 140174024480512 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.23219069838523865, loss=1.6703097820281982
I0312 17:50:25.713951 140174032873216 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.21054396033287048, loss=1.5731987953186035
I0312 17:51:01.738906 140174024480512 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.20888227224349976, loss=1.6927231550216675
I0312 17:51:37.802771 140174032873216 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20270730555057526, loss=1.673905372619629
I0312 17:52:13.885332 140174024480512 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.19643045961856842, loss=1.6445869207382202
I0312 17:52:49.939944 140174032873216 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.18992352485656738, loss=1.5709302425384521
I0312 17:53:26.026927 140174024480512 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20613394677639008, loss=1.5975449085235596
I0312 17:54:02.019974 140174032873216 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.2028539627790451, loss=1.7257866859436035
I0312 17:54:38.054954 140174024480512 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20460841059684753, loss=1.6356676816940308
I0312 17:55:14.057002 140174032873216 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.2130812555551529, loss=1.6467094421386719
I0312 17:55:50.057011 140174024480512 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.22504279017448425, loss=1.7052887678146362
I0312 17:56:26.061571 140174032873216 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.19572679698467255, loss=1.6331050395965576
I0312 17:57:02.083423 140174024480512 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.19674378633499146, loss=1.6229016780853271
I0312 17:57:38.129882 140174032873216 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.2011524736881256, loss=1.612131953239441
I0312 17:58:14.171736 140174024480512 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.19221599400043488, loss=1.5028579235076904
I0312 17:58:50.183732 140174032873216 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.21021705865859985, loss=1.659703016281128
I0312 17:59:14.756533 140343717943104 spec.py:321] Evaluating on the training split.
I0312 17:59:17.768558 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:03:08.634983 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 18:03:11.339585 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:06:07.840530 140343717943104 spec.py:349] Evaluating on the test split.
I0312 18:06:10.546061 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:08:36.764738 140343717943104 submission_runner.py:420] Time since start: 49994.05s, 	Step: 81670, 	{'train/accuracy': 0.6846832633018494, 'train/loss': 1.4385541677474976, 'train/bleu': 34.21251206089079, 'validation/accuracy': 0.682719349861145, 'validation/loss': 1.4486734867095947, 'validation/bleu': 29.936081717855267, 'validation/num_examples': 3000, 'test/accuracy': 0.6980535984039307, 'test/loss': 1.3542295694351196, 'test/bleu': 29.6608945179934, 'test/num_examples': 3003, 'score': 29441.392888069153, 'total_duration': 49994.04566836357, 'accumulated_submission_time': 29441.392888069153, 'accumulated_eval_time': 20549.10893559456, 'accumulated_logging_time': 1.0872220993041992}
I0312 18:08:36.789748 140174024480512 logging_writer.py:48] [81670] accumulated_eval_time=20549.108936, accumulated_logging_time=1.087222, accumulated_submission_time=29441.392888, global_step=81670, preemption_count=0, score=29441.392888, test/accuracy=0.698054, test/bleu=29.660895, test/loss=1.354230, test/num_examples=3003, total_duration=49994.045668, train/accuracy=0.684683, train/bleu=34.212512, train/loss=1.438554, validation/accuracy=0.682719, validation/bleu=29.936082, validation/loss=1.448673, validation/num_examples=3000
I0312 18:08:47.901357 140174032873216 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.1925848424434662, loss=1.5782458782196045
I0312 18:09:23.815332 140174024480512 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.22137059271335602, loss=1.59369695186615
I0312 18:09:59.756399 140174032873216 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.20561282336711884, loss=1.7405338287353516
I0312 18:10:35.725453 140174024480512 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.21068599820137024, loss=1.5723259449005127
I0312 18:11:11.700919 140174032873216 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.20095303654670715, loss=1.6068522930145264
I0312 18:11:47.684960 140174024480512 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.19418352842330933, loss=1.6225109100341797
I0312 18:12:23.744430 140174032873216 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.2087399810552597, loss=1.6811388731002808
I0312 18:12:59.781053 140174024480512 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20766426622867584, loss=1.6719635725021362
I0312 18:13:35.792050 140174032873216 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20507100224494934, loss=1.6150171756744385
I0312 18:14:11.824338 140174024480512 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.20402419567108154, loss=1.6177703142166138
I0312 18:14:47.849451 140174032873216 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.19983406364917755, loss=1.6338611841201782
I0312 18:15:23.852594 140174024480512 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.22665368020534515, loss=1.671341061592102
I0312 18:15:59.859936 140174032873216 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.20576989650726318, loss=1.625136137008667
I0312 18:16:35.876976 140174024480512 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.19332116842269897, loss=1.590156078338623
I0312 18:17:11.897168 140174032873216 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.1939334124326706, loss=1.5886896848678589
I0312 18:17:47.929618 140174024480512 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.20636996626853943, loss=1.621247410774231
I0312 18:18:23.997313 140174032873216 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.20819959044456482, loss=1.6306952238082886
I0312 18:19:00.024166 140174024480512 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.200174942612648, loss=1.6137616634368896
I0312 18:19:36.039933 140174032873216 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.21168231964111328, loss=1.653071403503418
I0312 18:20:12.048705 140174024480512 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.22248773276805878, loss=1.5864512920379639
I0312 18:20:48.106110 140174032873216 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.1955469250679016, loss=1.5926169157028198
I0312 18:21:24.139591 140174024480512 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.20106644928455353, loss=1.5719457864761353
I0312 18:22:00.158421 140174032873216 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.206620454788208, loss=1.6336290836334229
I0312 18:22:36.188343 140174024480512 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.20187269151210785, loss=1.5839779376983643
I0312 18:22:36.991650 140343717943104 spec.py:321] Evaluating on the training split.
I0312 18:22:40.015398 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:26:40.407440 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 18:26:43.119874 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:30:48.673872 140343717943104 spec.py:349] Evaluating on the test split.
I0312 18:30:51.400958 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:34:53.557882 140343717943104 submission_runner.py:420] Time since start: 51570.84s, 	Step: 84004, 	{'train/accuracy': 0.6690401434898376, 'train/loss': 1.5280447006225586, 'train/bleu': 33.571556752165826, 'validation/accuracy': 0.6831657290458679, 'validation/loss': 1.441229224205017, 'validation/bleu': 29.646118721990334, 'validation/num_examples': 3000, 'test/accuracy': 0.6986462473869324, 'test/loss': 1.3453952074050903, 'test/bleu': 30.022813565275428, 'test/num_examples': 3003, 'score': 30281.515100955963, 'total_duration': 51570.838802576065, 'accumulated_submission_time': 30281.515100955963, 'accumulated_eval_time': 21285.675103902817, 'accumulated_logging_time': 1.1223838329315186}
I0312 18:34:53.582837 140174032873216 logging_writer.py:48] [84004] accumulated_eval_time=21285.675104, accumulated_logging_time=1.122384, accumulated_submission_time=30281.515101, global_step=84004, preemption_count=0, score=30281.515101, test/accuracy=0.698646, test/bleu=30.022814, test/loss=1.345395, test/num_examples=3003, total_duration=51570.838803, train/accuracy=0.669040, train/bleu=33.571557, train/loss=1.528045, validation/accuracy=0.683166, validation/bleu=29.646119, validation/loss=1.441229, validation/num_examples=3000
I0312 18:35:28.397608 140174024480512 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.21925131976604462, loss=1.6636689901351929
I0312 18:36:04.354629 140174032873216 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.21381711959838867, loss=1.6257867813110352
I0312 18:36:40.341306 140174024480512 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.20086045563220978, loss=1.7067482471466064
I0312 18:37:16.343559 140174032873216 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.20461544394493103, loss=1.7054849863052368
I0312 18:37:52.349176 140174024480512 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.19991976022720337, loss=1.6210709810256958
I0312 18:38:28.362318 140174032873216 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.2053815722465515, loss=1.5713953971862793
I0312 18:39:04.353169 140174024480512 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.19656288623809814, loss=1.5526763200759888
I0312 18:39:40.397891 140174032873216 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.21977998316287994, loss=1.5872653722763062
I0312 18:40:16.482558 140174024480512 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.1963694840669632, loss=1.5890377759933472
I0312 18:40:52.565802 140174032873216 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.20033475756645203, loss=1.5164283514022827
I0312 18:41:28.621858 140174024480512 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.20947891473770142, loss=1.6267064809799194
I0312 18:42:04.621485 140174032873216 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.18987523019313812, loss=1.579551100730896
I0312 18:42:40.621541 140174024480512 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2136881947517395, loss=1.6507128477096558
I0312 18:43:16.608757 140174032873216 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.21336394548416138, loss=1.5197017192840576
I0312 18:43:52.624463 140174024480512 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.20048557221889496, loss=1.6094508171081543
I0312 18:44:28.646464 140174032873216 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2085510641336441, loss=1.6047364473342896
I0312 18:45:04.680663 140174024480512 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.20685303211212158, loss=1.5847188234329224
I0312 18:45:40.707256 140174032873216 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.21031597256660461, loss=1.6984771490097046
I0312 18:46:16.735190 140174024480512 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.23680107295513153, loss=1.57608962059021
I0312 18:46:52.753498 140174032873216 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.209880068898201, loss=1.6478837728500366
I0312 18:47:28.806138 140174024480512 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.22394399344921112, loss=1.650902271270752
I0312 18:48:04.861707 140174032873216 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.19936637580394745, loss=1.5296801328659058
I0312 18:48:40.876610 140174024480512 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.23136281967163086, loss=1.7152082920074463
I0312 18:48:53.558742 140343717943104 spec.py:321] Evaluating on the training split.
I0312 18:48:56.571077 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:53:17.882418 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 18:53:20.610751 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:55:46.439145 140343717943104 spec.py:349] Evaluating on the test split.
I0312 18:55:49.153377 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 18:58:16.957123 140343717943104 submission_runner.py:420] Time since start: 52974.24s, 	Step: 86337, 	{'train/accuracy': 0.672156810760498, 'train/loss': 1.5186889171600342, 'train/bleu': 33.86852339264609, 'validation/accuracy': 0.6863647103309631, 'validation/loss': 1.4359986782073975, 'validation/bleu': 30.026562760041603, 'validation/num_examples': 3000, 'test/accuracy': 0.7010284066200256, 'test/loss': 1.3384041786193848, 'test/bleu': 29.734592836322268, 'test/num_examples': 3003, 'score': 31121.409340381622, 'total_duration': 52974.23804974556, 'accumulated_submission_time': 31121.409340381622, 'accumulated_eval_time': 21849.073429584503, 'accumulated_logging_time': 1.1575572490692139}
I0312 18:58:16.983114 140174032873216 logging_writer.py:48] [86337] accumulated_eval_time=21849.073430, accumulated_logging_time=1.157557, accumulated_submission_time=31121.409340, global_step=86337, preemption_count=0, score=31121.409340, test/accuracy=0.701028, test/bleu=29.734593, test/loss=1.338404, test/num_examples=3003, total_duration=52974.238050, train/accuracy=0.672157, train/bleu=33.868523, train/loss=1.518689, validation/accuracy=0.686365, validation/bleu=30.026563, validation/loss=1.435999, validation/num_examples=3000
I0312 18:58:39.968109 140174024480512 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.20580892264842987, loss=1.5207223892211914
I0312 18:59:15.860261 140174032873216 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.21083450317382812, loss=1.62065589427948
I0312 18:59:51.834987 140174024480512 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.2015797197818756, loss=1.5813838243484497
I0312 19:00:27.842141 140174032873216 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.20253130793571472, loss=1.5829139947891235
I0312 19:01:03.844058 140174024480512 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.20987793803215027, loss=1.5686403512954712
I0312 19:01:39.847666 140174032873216 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2282160371541977, loss=1.6760282516479492
I0312 19:02:15.845901 140174024480512 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2203487604856491, loss=1.6468846797943115
I0312 19:02:51.863165 140174032873216 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.22134451568126678, loss=1.5901321172714233
I0312 19:03:27.842678 140174024480512 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.21067960560321808, loss=1.5933544635772705
I0312 19:04:03.863997 140174032873216 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.21056291460990906, loss=1.549421787261963
I0312 19:04:39.866829 140174024480512 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.23926396667957306, loss=1.6698118448257446
I0312 19:05:15.871378 140174032873216 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.1983257383108139, loss=1.5837223529815674
I0312 19:05:51.879737 140174024480512 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.20949602127075195, loss=1.6151944398880005
I0312 19:06:27.935760 140174032873216 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.20888036489486694, loss=1.628933310508728
I0312 19:07:03.999315 140174024480512 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.2228110134601593, loss=1.5622706413269043
I0312 19:07:39.994390 140174032873216 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.21431006491184235, loss=1.635298728942871
I0312 19:08:15.996485 140174024480512 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.2143854945898056, loss=1.6168012619018555
I0312 19:08:52.035742 140174032873216 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.20546627044677734, loss=1.5518991947174072
I0312 19:09:28.094515 140174024480512 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.2100139707326889, loss=1.6191136837005615
I0312 19:10:04.177057 140174032873216 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.22850486636161804, loss=1.597796082496643
I0312 19:10:40.209466 140174024480512 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.22732320427894592, loss=1.633658766746521
I0312 19:11:16.236708 140174032873216 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21014171838760376, loss=1.6928411722183228
I0312 19:11:52.249874 140174024480512 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.2249767780303955, loss=1.5840519666671753
I0312 19:12:17.196553 140343717943104 spec.py:321] Evaluating on the training split.
I0312 19:12:20.211859 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 19:16:32.771877 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 19:16:35.486322 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 19:19:16.839799 140343717943104 spec.py:349] Evaluating on the test split.
I0312 19:19:19.556897 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 19:21:57.075601 140343717943104 submission_runner.py:420] Time since start: 54394.36s, 	Step: 88671, 	{'train/accuracy': 0.6767730116844177, 'train/loss': 1.4741313457489014, 'train/bleu': 33.87990809492461, 'validation/accuracy': 0.6857447624206543, 'validation/loss': 1.4289525747299194, 'validation/bleu': 30.211913446848925, 'validation/num_examples': 3000, 'test/accuracy': 0.7016443014144897, 'test/loss': 1.3317005634307861, 'test/bleu': 30.041883007413933, 'test/num_examples': 3003, 'score': 31961.54389023781, 'total_duration': 54394.35652112961, 'accumulated_submission_time': 31961.54389023781, 'accumulated_eval_time': 22428.9524166584, 'accumulated_logging_time': 1.192662000656128}
I0312 19:21:57.101660 140174032873216 logging_writer.py:48] [88671] accumulated_eval_time=22428.952417, accumulated_logging_time=1.192662, accumulated_submission_time=31961.543890, global_step=88671, preemption_count=0, score=31961.543890, test/accuracy=0.701644, test/bleu=30.041883, test/loss=1.331701, test/num_examples=3003, total_duration=54394.356521, train/accuracy=0.676773, train/bleu=33.879908, train/loss=1.474131, validation/accuracy=0.685745, validation/bleu=30.211913, validation/loss=1.428953, validation/num_examples=3000
I0312 19:22:07.876521 140174024480512 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.22137628495693207, loss=1.6156227588653564
I0312 19:22:43.734064 140174032873216 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.1969480961561203, loss=1.559890627861023
I0312 19:23:19.690164 140174024480512 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.20834723114967346, loss=1.5853843688964844
I0312 19:23:55.666120 140174032873216 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21177509427070618, loss=1.5711880922317505
I0312 19:24:31.681317 140174024480512 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.20999234914779663, loss=1.5711309909820557
I0312 19:25:07.738503 140174032873216 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2032255381345749, loss=1.587825059890747
I0312 19:25:43.754563 140174024480512 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.2149590402841568, loss=1.5914649963378906
I0312 19:26:19.771176 140174032873216 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.21875768899917603, loss=1.5342035293579102
I0312 19:26:55.825660 140174024480512 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.20909103751182556, loss=1.590781569480896
I0312 19:27:31.844769 140174032873216 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.2157691866159439, loss=1.56484055519104
I0312 19:28:07.879333 140174024480512 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.21351169049739838, loss=1.6231799125671387
I0312 19:28:43.888520 140174032873216 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.1999167501926422, loss=1.534833312034607
I0312 19:29:19.934761 140174024480512 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.23630627989768982, loss=1.553539514541626
I0312 19:29:55.960126 140174032873216 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.21041661500930786, loss=1.6081026792526245
I0312 19:30:32.032402 140174024480512 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.22391235828399658, loss=1.5917822122573853
I0312 19:31:08.091795 140174032873216 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2012636959552765, loss=1.576734185218811
I0312 19:31:44.147673 140174024480512 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.21131470799446106, loss=1.6092195510864258
I0312 19:32:20.160395 140174032873216 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.21934789419174194, loss=1.6319451332092285
I0312 19:32:56.207590 140174024480512 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.20597271621227264, loss=1.5530756711959839
I0312 19:33:32.228182 140174032873216 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.22016069293022156, loss=1.5821796655654907
I0312 19:34:08.254827 140174024480512 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20896247029304504, loss=1.56355619430542
I0312 19:34:44.359144 140174032873216 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.200355663895607, loss=1.628991961479187
I0312 19:35:20.429341 140174024480512 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.22043365240097046, loss=1.5760992765426636
I0312 19:35:56.483844 140174032873216 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.20110318064689636, loss=1.5142751932144165
I0312 19:35:57.283344 140343717943104 spec.py:321] Evaluating on the training split.
I0312 19:36:00.287700 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 19:40:18.283341 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 19:40:20.994821 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 19:43:00.893656 140343717943104 spec.py:349] Evaluating on the test split.
I0312 19:43:03.623361 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 19:45:50.464843 140343717943104 submission_runner.py:420] Time since start: 55827.75s, 	Step: 91004, 	{'train/accuracy': 0.6733483672142029, 'train/loss': 1.5103007555007935, 'train/bleu': 33.843482795286214, 'validation/accuracy': 0.6872202157974243, 'validation/loss': 1.4207197427749634, 'validation/bleu': 30.112663837010107, 'validation/num_examples': 3000, 'test/accuracy': 0.7026785612106323, 'test/loss': 1.3232601881027222, 'test/bleu': 30.104204020163536, 'test/num_examples': 3003, 'score': 32801.646438360214, 'total_duration': 55827.74574255943, 'accumulated_submission_time': 32801.646438360214, 'accumulated_eval_time': 23022.13383102417, 'accumulated_logging_time': 1.2275655269622803}
I0312 19:45:50.496188 140174024480512 logging_writer.py:48] [91004] accumulated_eval_time=23022.133831, accumulated_logging_time=1.227566, accumulated_submission_time=32801.646438, global_step=91004, preemption_count=0, score=32801.646438, test/accuracy=0.702679, test/bleu=30.104204, test/loss=1.323260, test/num_examples=3003, total_duration=55827.745743, train/accuracy=0.673348, train/bleu=33.843483, train/loss=1.510301, validation/accuracy=0.687220, validation/bleu=30.112664, validation/loss=1.420720, validation/num_examples=3000
I0312 19:46:25.286665 140174032873216 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.21799015998840332, loss=1.6215406656265259
I0312 19:47:01.215609 140174024480512 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.22042830288410187, loss=1.6204596757888794
I0312 19:47:37.306493 140174032873216 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.19767455756664276, loss=1.5449668169021606
I0312 19:48:13.327826 140174024480512 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2201234996318817, loss=1.5111873149871826
I0312 19:48:49.322674 140174032873216 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.210478276014328, loss=1.5723360776901245
I0312 19:49:25.335541 140174024480512 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.21816468238830566, loss=1.592706322669983
I0312 19:50:01.368507 140174032873216 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.22170743346214294, loss=1.6158522367477417
I0312 19:50:37.460695 140174024480512 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.22668439149856567, loss=1.6538820266723633
I0312 19:51:13.547466 140174032873216 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.20314215123653412, loss=1.5421761274337769
I0312 19:51:49.570771 140174024480512 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.20878228545188904, loss=1.5904182195663452
I0312 19:52:25.600492 140174032873216 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.2142104208469391, loss=1.5471822023391724
I0312 19:53:01.618437 140174024480512 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.22022506594657898, loss=1.6554845571517944
I0312 19:53:37.625087 140174032873216 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21261848509311676, loss=1.6474469900131226
I0312 19:54:13.678456 140174024480512 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.21609419584274292, loss=1.5523173809051514
I0312 19:54:49.700955 140174032873216 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.21127930283546448, loss=1.5783040523529053
I0312 19:55:25.711146 140174024480512 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.22527356445789337, loss=1.5623217821121216
I0312 19:56:01.719418 140174032873216 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.2081896960735321, loss=1.6330307722091675
I0312 19:56:37.753867 140174024480512 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21527644991874695, loss=1.6201539039611816
I0312 19:57:13.762457 140174032873216 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2180696278810501, loss=1.6229255199432373
I0312 19:57:49.774743 140174024480512 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.22760437428951263, loss=1.6727043390274048
I0312 19:58:25.792944 140174032873216 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21782638132572174, loss=1.5683798789978027
I0312 19:59:01.810482 140174024480512 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.21389305591583252, loss=1.577759861946106
I0312 19:59:37.829862 140174032873216 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.2118576616048813, loss=1.5472397804260254
I0312 19:59:50.506563 140343717943104 spec.py:321] Evaluating on the training split.
I0312 19:59:53.523529 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:03:48.514260 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 20:03:51.213040 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:06:50.128780 140343717943104 spec.py:349] Evaluating on the test split.
I0312 20:06:52.850466 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:09:51.973578 140343717943104 submission_runner.py:420] Time since start: 57269.25s, 	Step: 93337, 	{'train/accuracy': 0.6737951040267944, 'train/loss': 1.5091018676757812, 'train/bleu': 33.77046943780147, 'validation/accuracy': 0.6864390969276428, 'validation/loss': 1.415480136871338, 'validation/bleu': 30.335093032326945, 'validation/num_examples': 3000, 'test/accuracy': 0.7031898498535156, 'test/loss': 1.3187764883041382, 'test/bleu': 30.082529087863982, 'test/num_examples': 3003, 'score': 33641.57714486122, 'total_duration': 57269.25446343422, 'accumulated_submission_time': 33641.57714486122, 'accumulated_eval_time': 23623.600753068924, 'accumulated_logging_time': 1.2687926292419434}
I0312 20:09:52.004759 140174024480512 logging_writer.py:48] [93337] accumulated_eval_time=23623.600753, accumulated_logging_time=1.268793, accumulated_submission_time=33641.577145, global_step=93337, preemption_count=0, score=33641.577145, test/accuracy=0.703190, test/bleu=30.082529, test/loss=1.318776, test/num_examples=3003, total_duration=57269.254463, train/accuracy=0.673795, train/bleu=33.770469, train/loss=1.509102, validation/accuracy=0.686439, validation/bleu=30.335093, validation/loss=1.415480, validation/num_examples=3000
I0312 20:10:14.971646 140174032873216 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21654577553272247, loss=1.5715994834899902
I0312 20:10:50.862695 140174024480512 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.2190774381160736, loss=1.6110824346542358
I0312 20:11:26.836006 140174032873216 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.21737061440944672, loss=1.5726354122161865
I0312 20:12:02.835367 140174024480512 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.2103355973958969, loss=1.5274869203567505
I0312 20:12:38.856218 140174032873216 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2188543975353241, loss=1.5539592504501343
I0312 20:13:14.851886 140174024480512 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.2034541368484497, loss=1.617956280708313
I0312 20:13:50.860702 140174032873216 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21100613474845886, loss=1.5911110639572144
I0312 20:14:26.866950 140174024480512 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.20930445194244385, loss=1.50994873046875
I0312 20:15:02.888573 140174032873216 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.24663054943084717, loss=1.6730657815933228
I0312 20:15:38.957672 140174024480512 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.22717949748039246, loss=1.6051397323608398
I0312 20:16:14.974249 140174032873216 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.22367317974567413, loss=1.5070010423660278
I0312 20:16:51.022223 140174024480512 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.20434224605560303, loss=1.5275460481643677
I0312 20:17:27.106354 140174032873216 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.20934641361236572, loss=1.573417067527771
I0312 20:18:03.194869 140174024480512 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.21638037264347076, loss=1.6014374494552612
I0312 20:18:39.206644 140174032873216 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.21990421414375305, loss=1.5681257247924805
I0312 20:19:15.258975 140174024480512 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.2113051414489746, loss=1.5528478622436523
I0312 20:19:51.252006 140174032873216 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.2192215472459793, loss=1.5712918043136597
I0312 20:20:27.284047 140174024480512 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22663210332393646, loss=1.5423624515533447
I0312 20:21:03.306059 140174032873216 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.22074341773986816, loss=1.575784683227539
I0312 20:21:39.334964 140174024480512 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.22003775835037231, loss=1.5189368724822998
I0312 20:22:15.355821 140174032873216 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.20270946621894836, loss=1.47527277469635
I0312 20:22:51.367823 140174024480512 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.20819440484046936, loss=1.5820726156234741
I0312 20:23:27.399972 140174032873216 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22342433035373688, loss=1.5340971946716309
I0312 20:23:52.322015 140343717943104 spec.py:321] Evaluating on the training split.
I0312 20:23:55.326457 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:28:23.646281 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 20:28:26.371172 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:31:51.718893 140343717943104 spec.py:349] Evaluating on the test split.
I0312 20:31:54.435771 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:35:39.316411 140343717943104 submission_runner.py:420] Time since start: 58816.60s, 	Step: 95671, 	{'train/accuracy': 0.6822972297668457, 'train/loss': 1.4458683729171753, 'train/bleu': 34.47859926606245, 'validation/accuracy': 0.689315676689148, 'validation/loss': 1.4054597616195679, 'validation/bleu': 30.457518693412105, 'validation/num_examples': 3000, 'test/accuracy': 0.7049445509910583, 'test/loss': 1.3141802549362183, 'test/bleu': 30.182513178175295, 'test/num_examples': 3003, 'score': 34481.814543008804, 'total_duration': 58816.59733939171, 'accumulated_submission_time': 34481.814543008804, 'accumulated_eval_time': 24330.59509205818, 'accumulated_logging_time': 1.3097121715545654}
I0312 20:35:39.343811 140174024480512 logging_writer.py:48] [95671] accumulated_eval_time=24330.595092, accumulated_logging_time=1.309712, accumulated_submission_time=34481.814543, global_step=95671, preemption_count=0, score=34481.814543, test/accuracy=0.704945, test/bleu=30.182513, test/loss=1.314180, test/num_examples=3003, total_duration=58816.597339, train/accuracy=0.682297, train/bleu=34.478599, train/loss=1.445868, validation/accuracy=0.689316, validation/bleu=30.457519, validation/loss=1.405460, validation/num_examples=3000
I0312 20:35:50.120707 140174032873216 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.21957632899284363, loss=1.7152787446975708
I0312 20:36:25.998942 140174024480512 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.24289968609809875, loss=1.6216983795166016
I0312 20:37:02.057446 140174032873216 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.20320548117160797, loss=1.5292737483978271
I0312 20:37:38.027122 140174024480512 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.2214241325855255, loss=1.5136786699295044
I0312 20:38:14.029794 140174032873216 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.2097156047821045, loss=1.5341070890426636
I0312 20:38:50.043662 140174024480512 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.21947936713695526, loss=1.5681049823760986
I0312 20:39:26.039117 140174032873216 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.214020237326622, loss=1.5732598304748535
I0312 20:40:02.099273 140174024480512 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.2082490175962448, loss=1.5123844146728516
I0312 20:40:38.132451 140174032873216 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.21267642080783844, loss=1.5890846252441406
I0312 20:41:14.129530 140174024480512 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.2200435996055603, loss=1.5329831838607788
I0312 20:41:50.183051 140174032873216 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.21397331357002258, loss=1.5884901285171509
I0312 20:42:26.265117 140174024480512 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.2113763689994812, loss=1.6197211742401123
I0312 20:43:02.292629 140174032873216 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.21590089797973633, loss=1.6854816675186157
I0312 20:43:38.338284 140174024480512 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.21360361576080322, loss=1.5240917205810547
I0312 20:44:14.353893 140174032873216 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.2194579392671585, loss=1.5143166780471802
I0312 20:44:50.370913 140174024480512 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.20427797734737396, loss=1.5473393201828003
I0312 20:45:26.442988 140174032873216 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.24198013544082642, loss=1.5963972806930542
I0312 20:46:02.472312 140174024480512 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.22131676971912384, loss=1.5461376905441284
I0312 20:46:38.494845 140174032873216 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.20284894108772278, loss=1.4698045253753662
I0312 20:47:14.550404 140174024480512 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21722248196601868, loss=1.6071594953536987
I0312 20:47:50.579946 140174032873216 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.2282579243183136, loss=1.5231295824050903
I0312 20:48:26.618762 140174024480512 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21426273882389069, loss=1.5955368280410767
I0312 20:49:02.670081 140174032873216 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.2115793526172638, loss=1.6088804006576538
I0312 20:49:38.718317 140174024480512 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.22270743548870087, loss=1.575574278831482
I0312 20:49:39.523408 140343717943104 spec.py:321] Evaluating on the training split.
I0312 20:49:42.537291 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:53:26.880504 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 20:53:29.591180 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:56:31.539277 140343717943104 spec.py:349] Evaluating on the test split.
I0312 20:56:34.272876 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 20:59:30.741554 140343717943104 submission_runner.py:420] Time since start: 60248.02s, 	Step: 98004, 	{'train/accuracy': 0.678022563457489, 'train/loss': 1.476861596107483, 'train/bleu': 34.01514409048839, 'validation/accuracy': 0.6899852156639099, 'validation/loss': 1.4042116403579712, 'validation/bleu': 30.621396187092557, 'validation/num_examples': 3000, 'test/accuracy': 0.7058508992195129, 'test/loss': 1.3002111911773682, 'test/bleu': 30.45435340686677, 'test/num_examples': 3003, 'score': 35321.9128882885, 'total_duration': 60248.02245783806, 'accumulated_submission_time': 35321.9128882885, 'accumulated_eval_time': 24921.81315469742, 'accumulated_logging_time': 1.3472654819488525}
I0312 20:59:30.770330 140174032873216 logging_writer.py:48] [98004] accumulated_eval_time=24921.813155, accumulated_logging_time=1.347265, accumulated_submission_time=35321.912888, global_step=98004, preemption_count=0, score=35321.912888, test/accuracy=0.705851, test/bleu=30.454353, test/loss=1.300211, test/num_examples=3003, total_duration=60248.022458, train/accuracy=0.678023, train/bleu=34.015144, train/loss=1.476862, validation/accuracy=0.689985, validation/bleu=30.621396, validation/loss=1.404212, validation/num_examples=3000
I0312 21:00:05.587197 140174024480512 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.22478853166103363, loss=1.553307294845581
I0312 21:00:41.561486 140174032873216 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2138276845216751, loss=1.5792863368988037
I0312 21:01:17.576596 140174024480512 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.21221910417079926, loss=1.5916417837142944
I0312 21:01:53.550263 140174032873216 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21255622804164886, loss=1.486800193786621
I0312 21:02:29.568262 140174024480512 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.22329311072826385, loss=1.55193030834198
I0312 21:03:05.590882 140174032873216 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.22820046544075012, loss=1.594465970993042
I0312 21:03:41.620181 140174024480512 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.20884786546230316, loss=1.5368582010269165
I0312 21:04:17.690497 140174032873216 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.23606564104557037, loss=1.5617649555206299
I0312 21:04:53.696943 140174024480512 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.2164563685655594, loss=1.5108132362365723
I0312 21:05:29.726340 140174032873216 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2163751870393753, loss=1.5651347637176514
I0312 21:06:05.743153 140174024480512 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21471691131591797, loss=1.5403372049331665
I0312 21:06:41.744951 140174032873216 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.21254727244377136, loss=1.5286113023757935
I0312 21:07:17.787751 140174024480512 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21676187217235565, loss=1.5048924684524536
I0312 21:07:53.824918 140174032873216 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.2163066416978836, loss=1.5595414638519287
I0312 21:08:29.836291 140174024480512 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.20656956732273102, loss=1.4716908931732178
I0312 21:09:05.832777 140174032873216 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.23000533878803253, loss=1.6068918704986572
I0312 21:09:41.833957 140174024480512 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.20398391783237457, loss=1.506401538848877
I0312 21:10:17.844834 140174032873216 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.22445817291736603, loss=1.5300832986831665
I0312 21:10:53.902979 140174024480512 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.21731996536254883, loss=1.435992956161499
I0312 21:11:29.893159 140174032873216 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22255636751651764, loss=1.5208712816238403
I0312 21:12:05.896713 140174024480512 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.23138318955898285, loss=1.547034502029419
I0312 21:12:41.947008 140174032873216 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.22806088626384735, loss=1.5785200595855713
I0312 21:13:17.975438 140174024480512 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.22107857465744019, loss=1.5265066623687744
I0312 21:13:30.998165 140343717943104 spec.py:321] Evaluating on the training split.
I0312 21:13:34.010221 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 21:17:27.092610 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 21:17:29.807354 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 21:20:41.361441 140343717943104 spec.py:349] Evaluating on the test split.
I0312 21:20:44.090390 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 21:23:52.509746 140343717943104 submission_runner.py:420] Time since start: 61709.79s, 	Step: 100338, 	{'train/accuracy': 0.6938577890396118, 'train/loss': 1.3822377920150757, 'train/bleu': 34.990644862138524, 'validation/accuracy': 0.6900968551635742, 'validation/loss': 1.3969777822494507, 'validation/bleu': 30.46087354581043, 'validation/num_examples': 3000, 'test/accuracy': 0.7059787511825562, 'test/loss': 1.2977460622787476, 'test/bleu': 30.23059396378059, 'test/num_examples': 3003, 'score': 36162.06133246422, 'total_duration': 61709.79067540169, 'accumulated_submission_time': 36162.06133246422, 'accumulated_eval_time': 25543.324679851532, 'accumulated_logging_time': 1.3858873844146729}
I0312 21:23:52.536841 140174032873216 logging_writer.py:48] [100338] accumulated_eval_time=25543.324680, accumulated_logging_time=1.385887, accumulated_submission_time=36162.061332, global_step=100338, preemption_count=0, score=36162.061332, test/accuracy=0.705979, test/bleu=30.230594, test/loss=1.297746, test/num_examples=3003, total_duration=61709.790675, train/accuracy=0.693858, train/bleu=34.990645, train/loss=1.382238, validation/accuracy=0.690097, validation/bleu=30.460874, validation/loss=1.396978, validation/num_examples=3000
I0312 21:24:15.164598 140174024480512 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.21795795857906342, loss=1.4675933122634888
I0312 21:24:51.109876 140174032873216 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.23578442633152008, loss=1.5104849338531494
I0312 21:25:27.106660 140174024480512 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.22670233249664307, loss=1.5122379064559937
I0312 21:26:03.084321 140174032873216 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.2162580043077469, loss=1.5212962627410889
I0312 21:26:39.116342 140174024480512 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.23745223879814148, loss=1.536510705947876
I0312 21:27:15.149570 140174032873216 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.2252613604068756, loss=1.538528323173523
I0312 21:27:51.163846 140174024480512 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22058676183223724, loss=1.578063726425171
I0312 21:28:27.189013 140174032873216 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.22957180440425873, loss=1.4505199193954468
I0312 21:29:03.226946 140174024480512 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.2215234935283661, loss=1.5682179927825928
I0312 21:29:39.241855 140174032873216 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.21050849556922913, loss=1.5475202798843384
I0312 21:30:15.294626 140174024480512 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.22693794965744019, loss=1.5760165452957153
I0312 21:30:51.301195 140174032873216 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.2578059136867523, loss=1.4931408166885376
I0312 21:31:27.319671 140174024480512 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.2163156419992447, loss=1.5410908460617065
I0312 21:32:03.323681 140174032873216 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.21159876883029938, loss=1.4505740404129028
I0312 21:32:39.387167 140174024480512 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.21416819095611572, loss=1.5836942195892334
I0312 21:33:15.439246 140174032873216 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.24723361432552338, loss=1.5910080671310425
I0312 21:33:51.490065 140174024480512 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.21386995911598206, loss=1.4990839958190918
I0312 21:34:27.542663 140174032873216 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.21920844912528992, loss=1.5037095546722412
I0312 21:35:03.562445 140174024480512 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2333919256925583, loss=1.6429245471954346
I0312 21:35:39.606909 140174032873216 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22595852613449097, loss=1.53676176071167
I0312 21:36:15.614102 140174024480512 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21180786192417145, loss=1.5254929065704346
I0312 21:36:51.650752 140174032873216 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21758060157299042, loss=1.4802027940750122
I0312 21:37:27.707020 140174024480512 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.22225093841552734, loss=1.4504120349884033
I0312 21:37:52.676341 140343717943104 spec.py:321] Evaluating on the training split.
I0312 21:37:55.698851 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 21:41:57.783131 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 21:42:00.504540 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 21:45:39.304781 140343717943104 spec.py:349] Evaluating on the test split.
I0312 21:45:42.030742 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 21:49:19.428369 140343717943104 submission_runner.py:420] Time since start: 63236.71s, 	Step: 102671, 	{'train/accuracy': 0.6866313219070435, 'train/loss': 1.4270374774932861, 'train/bleu': 34.47752292320814, 'validation/accuracy': 0.6905555725097656, 'validation/loss': 1.3927421569824219, 'validation/bleu': 30.45557605030123, 'validation/num_examples': 3000, 'test/accuracy': 0.7083958387374878, 'test/loss': 1.2918590307235718, 'test/bleu': 30.438368768962746, 'test/num_examples': 3003, 'score': 37002.121591091156, 'total_duration': 63236.709287166595, 'accumulated_submission_time': 37002.121591091156, 'accumulated_eval_time': 26230.07666492462, 'accumulated_logging_time': 1.4218950271606445}
I0312 21:49:19.456692 140174032873216 logging_writer.py:48] [102671] accumulated_eval_time=26230.076665, accumulated_logging_time=1.421895, accumulated_submission_time=37002.121591, global_step=102671, preemption_count=0, score=37002.121591, test/accuracy=0.708396, test/bleu=30.438369, test/loss=1.291859, test/num_examples=3003, total_duration=63236.709287, train/accuracy=0.686631, train/bleu=34.477523, train/loss=1.427037, validation/accuracy=0.690556, validation/bleu=30.455576, validation/loss=1.392742, validation/num_examples=3000
I0312 21:49:30.209673 140174024480512 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23925772309303284, loss=1.5310920476913452
I0312 21:50:06.132156 140174032873216 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.21983325481414795, loss=1.4653555154800415
I0312 21:50:42.118117 140174024480512 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.2216813862323761, loss=1.503886342048645
I0312 21:51:18.129371 140174032873216 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2133461982011795, loss=1.5155389308929443
I0312 21:51:54.163652 140174024480512 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.2188015580177307, loss=1.4990837574005127
I0312 21:52:30.180970 140174032873216 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.21459965407848358, loss=1.4876188039779663
I0312 21:53:06.219917 140174024480512 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22545775771141052, loss=1.5621323585510254
I0312 21:53:42.211868 140174032873216 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.24815699458122253, loss=1.610216736793518
I0312 21:54:18.254192 140174024480512 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.21812784671783447, loss=1.4622406959533691
I0312 21:54:54.311659 140174032873216 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21135108172893524, loss=1.4761112928390503
I0312 21:55:30.314932 140174024480512 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.24685458838939667, loss=1.5568366050720215
I0312 21:56:06.340431 140174032873216 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.23043137788772583, loss=1.6189690828323364
I0312 21:56:42.372902 140174024480512 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.2235351800918579, loss=1.507175326347351
I0312 21:57:18.454727 140174032873216 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.22285693883895874, loss=1.5114997625350952
I0312 21:57:54.460448 140174024480512 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.22967299818992615, loss=1.5226325988769531
I0312 21:58:30.479553 140174032873216 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.23288674652576447, loss=1.6148704290390015
I0312 21:59:06.505128 140174024480512 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.23261676728725433, loss=1.5527513027191162
I0312 21:59:42.551044 140174032873216 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.2124379575252533, loss=1.5035920143127441
I0312 22:00:18.592085 140174024480512 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.22410768270492554, loss=1.5143811702728271
I0312 22:00:54.631299 140174032873216 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22795791923999786, loss=1.4590014219284058
I0312 22:01:30.652863 140174024480512 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.21074208617210388, loss=1.4661554098129272
I0312 22:02:06.690553 140174032873216 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.22019560635089874, loss=1.474358320236206
I0312 22:02:42.731477 140174024480512 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.22316668927669525, loss=1.5817012786865234
I0312 22:03:18.731580 140174032873216 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.23962339758872986, loss=1.4991631507873535
I0312 22:03:19.536302 140343717943104 spec.py:321] Evaluating on the training split.
I0312 22:03:22.554467 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 22:07:45.080993 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 22:07:47.784894 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 22:10:45.954613 140343717943104 spec.py:349] Evaluating on the test split.
I0312 22:10:48.671367 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 22:13:14.351654 140343717943104 submission_runner.py:420] Time since start: 64671.63s, 	Step: 105004, 	{'train/accuracy': 0.6834654808044434, 'train/loss': 1.4519506692886353, 'train/bleu': 34.622091299478036, 'validation/accuracy': 0.691969096660614, 'validation/loss': 1.38778555393219, 'validation/bleu': 30.795252705708116, 'validation/num_examples': 3000, 'test/accuracy': 0.7087908983230591, 'test/loss': 1.2866441011428833, 'test/bleu': 30.58542091418264, 'test/num_examples': 3003, 'score': 37842.12282657623, 'total_duration': 64671.632581949234, 'accumulated_submission_time': 37842.12282657623, 'accumulated_eval_time': 26824.891982793808, 'accumulated_logging_time': 1.4596679210662842}
I0312 22:13:14.379378 140174024480512 logging_writer.py:48] [105004] accumulated_eval_time=26824.891983, accumulated_logging_time=1.459668, accumulated_submission_time=37842.122827, global_step=105004, preemption_count=0, score=37842.122827, test/accuracy=0.708791, test/bleu=30.585421, test/loss=1.286644, test/num_examples=3003, total_duration=64671.632582, train/accuracy=0.683465, train/bleu=34.622091, train/loss=1.451951, validation/accuracy=0.691969, validation/bleu=30.795253, validation/loss=1.387786, validation/num_examples=3000
I0312 22:13:49.201957 140174032873216 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.2238909900188446, loss=1.5167771577835083
I0312 22:14:25.159055 140174024480512 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.22514137625694275, loss=1.5826709270477295
I0312 22:15:01.168560 140174032873216 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.23493623733520508, loss=1.536995768547058
I0312 22:15:37.153681 140174024480512 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.2273184061050415, loss=1.4381214380264282
I0312 22:16:13.178963 140174032873216 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.2258368879556656, loss=1.4771099090576172
I0312 22:16:49.210035 140174024480512 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.21591581404209137, loss=1.4800727367401123
I0312 22:17:25.280256 140174032873216 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.2166573852300644, loss=1.5501066446304321
I0312 22:18:01.307398 140174024480512 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.2263440638780594, loss=1.559348464012146
I0312 22:18:37.332571 140174032873216 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.23133698105812073, loss=1.5502727031707764
I0312 22:19:13.354010 140174024480512 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.23946794867515564, loss=1.5755082368850708
I0312 22:19:49.417557 140174032873216 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.23736295104026794, loss=1.5918866395950317
I0312 22:20:25.472338 140174024480512 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.22661179304122925, loss=1.567407488822937
I0312 22:21:01.538218 140174032873216 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.21921204030513763, loss=1.463425874710083
I0312 22:21:37.646752 140174024480512 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.22903698682785034, loss=1.5322043895721436
I0312 22:22:13.691834 140174032873216 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.20813092589378357, loss=1.4397352933883667
I0312 22:22:49.724646 140174024480512 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22489073872566223, loss=1.492358684539795
I0312 22:23:25.732290 140174032873216 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.21966196596622467, loss=1.522834062576294
I0312 22:24:01.762710 140174024480512 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.21886348724365234, loss=1.5008314847946167
I0312 22:24:37.804748 140174032873216 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.23543214797973633, loss=1.5066401958465576
I0312 22:25:13.859386 140174024480512 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.23585189878940582, loss=1.5223156213760376
I0312 22:25:49.859271 140174032873216 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.21757888793945312, loss=1.5303014516830444
I0312 22:26:25.868659 140174024480512 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.23468323051929474, loss=1.53440260887146
I0312 22:27:01.898136 140174032873216 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.23040950298309326, loss=1.5121151208877563
I0312 22:27:14.604148 140343717943104 spec.py:321] Evaluating on the training split.
I0312 22:27:17.621474 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 22:31:29.224429 140343717943104 spec.py:333] Evaluating on the validation split.
I0312 22:31:31.951468 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 22:34:30.740809 140343717943104 spec.py:349] Evaluating on the test split.
I0312 22:34:33.456072 140343717943104 workload.py:181] Translating evaluation dataset.
I0312 22:37:32.907114 140343717943104 submission_runner.py:420] Time since start: 66130.19s, 	Step: 107337, 	{'train/accuracy': 0.690151035785675, 'train/loss': 1.4052026271820068, 'train/bleu': 35.204680267477826, 'validation/accuracy': 0.6935065984725952, 'validation/loss': 1.38334059715271, 'validation/bleu': 30.966569494395618, 'validation/num_examples': 3000, 'test/accuracy': 0.7097437977790833, 'test/loss': 1.2780189514160156, 'test/bleu': 30.80200748225378, 'test/num_examples': 3003, 'score': 38682.26754260063, 'total_duration': 66130.1880209446, 'accumulated_submission_time': 38682.26754260063, 'accumulated_eval_time': 27443.19491481781, 'accumulated_logging_time': 1.4974017143249512}
I0312 22:37:32.935381 140174024480512 logging_writer.py:48] [107337] accumulated_eval_time=27443.194915, accumulated_logging_time=1.497402, accumulated_submission_time=38682.267543, global_step=107337, preemption_count=0, score=38682.267543, test/accuracy=0.709744, test/bleu=30.802007, test/loss=1.278019, test/num_examples=3003, total_duration=66130.188021, train/accuracy=0.690151, train/bleu=35.204680, train/loss=1.405203, validation/accuracy=0.693507, validation/bleu=30.966569, validation/loss=1.383341, validation/num_examples=3000
I0312 22:37:32.962008 140174032873216 logging_writer.py:48] [107337] global_step=107337, preemption_count=0, score=38682.267543
I0312 22:37:34.071657 140343717943104 checkpoints.py:490] Saving checkpoint at step: 107337
I0312 22:37:37.827971 140343717943104 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax/trial_1/checkpoint_107337
I0312 22:37:37.832442 140343717943104 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_2/wmt_jax/trial_1/checkpoint_107337.
I0312 22:37:37.869750 140343717943104 submission_runner.py:683] Final wmt score: 38682.26754260063
