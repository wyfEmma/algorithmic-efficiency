python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_2 --overwrite=true --save_checkpoints=false --rng_seed=2539760414 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_resnet_jax_02-29-2024-05-13-39.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0229 05:14:02.786832 140252611495744 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax.
I0229 05:14:03.842217 140252611495744 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0229 05:14:03.843353 140252611495744 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0229 05:14:03.843501 140252611495744 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0229 05:14:04.788082 140252611495744 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax/trial_1.
I0229 05:14:04.989213 140252611495744 submission_runner.py:206] Initializing dataset.
I0229 05:14:05.005366 140252611495744 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:14:05.015812 140252611495744 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:14:05.389250 140252611495744 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:14:06.584688 140252611495744 submission_runner.py:213] Initializing model.
I0229 05:14:16.786664 140252611495744 submission_runner.py:255] Initializing optimizer.
I0229 05:14:18.502997 140252611495744 submission_runner.py:262] Initializing metrics bundle.
I0229 05:14:18.503187 140252611495744 submission_runner.py:280] Initializing checkpoint and logger.
I0229 05:14:18.504036 140252611495744 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0229 05:14:18.504170 140252611495744 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0229 05:14:18.822137 140252611495744 logger_utils.py:220] Unable to record git information. Continuing without it.
I0229 05:14:19.110332 140252611495744 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax/trial_1/flags_0.json.
I0229 05:14:19.119860 140252611495744 submission_runner.py:314] Starting training loop.
I0229 05:15:10.514869 140084879812352 logging_writer.py:48] [0] global_step=0, grad_norm=0.6651390194892883, loss=6.923315525054932
I0229 05:15:10.531782 140252611495744 spec.py:321] Evaluating on the training split.
I0229 05:15:11.656787 140252611495744 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:11.666288 140252611495744 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:15:11.749056 140252611495744 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:24.980816 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 05:15:26.757837 140252611495744 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:26.767764 140252611495744 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:15:26.811564 140252611495744 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:15:42.262233 140252611495744 spec.py:349] Evaluating on the test split.
I0229 05:15:43.100014 140252611495744 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:15:43.108741 140252611495744 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0229 05:15:43.154883 140252611495744 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:15:47.248838 140252611495744 submission_runner.py:411] Time since start: 88.13s, 	Step: 1, 	{'train/accuracy': 0.0008769132546149194, 'train/loss': 6.912364482879639, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.912585258483887, 'validation/num_examples': 50000, 'test/accuracy': 0.0017000001389533281, 'test/loss': 6.912716865539551, 'test/num_examples': 10000, 'score': 51.41184377670288, 'total_duration': 88.12890410423279, 'accumulated_submission_time': 51.41184377670288, 'accumulated_eval_time': 36.71697783470154, 'accumulated_logging_time': 0}
I0229 05:15:47.266683 140067758642944 logging_writer.py:48] [1] accumulated_eval_time=36.716978, accumulated_logging_time=0, accumulated_submission_time=51.411844, global_step=1, preemption_count=0, score=51.411844, test/accuracy=0.001700, test/loss=6.912717, test/num_examples=10000, total_duration=88.128904, train/accuracy=0.000877, train/loss=6.912364, validation/accuracy=0.001140, validation/loss=6.912585, validation/num_examples=50000
I0229 05:16:20.812207 140067674781440 logging_writer.py:48] [100] global_step=100, grad_norm=0.6590657830238342, loss=6.822249412536621
I0229 05:16:54.415264 140067758642944 logging_writer.py:48] [200] global_step=200, grad_norm=0.7855575680732727, loss=6.57682991027832
I0229 05:17:28.056850 140067674781440 logging_writer.py:48] [300] global_step=300, grad_norm=0.925727367401123, loss=6.281175136566162
I0229 05:18:01.686851 140067758642944 logging_writer.py:48] [400] global_step=400, grad_norm=1.6252986192703247, loss=5.977209091186523
I0229 05:18:35.299276 140067674781440 logging_writer.py:48] [500] global_step=500, grad_norm=5.455197811126709, loss=5.806455612182617
I0229 05:19:08.954315 140067758642944 logging_writer.py:48] [600] global_step=600, grad_norm=2.791689157485962, loss=5.646673202514648
I0229 05:19:42.569339 140067674781440 logging_writer.py:48] [700] global_step=700, grad_norm=4.07612419128418, loss=5.462644577026367
I0229 05:20:16.186784 140067758642944 logging_writer.py:48] [800] global_step=800, grad_norm=3.04201078414917, loss=5.357880115509033
I0229 05:20:49.866612 140067674781440 logging_writer.py:48] [900] global_step=900, grad_norm=4.411283016204834, loss=5.161393165588379
I0229 05:21:23.515261 140067758642944 logging_writer.py:48] [1000] global_step=1000, grad_norm=5.4629130363464355, loss=5.098167896270752
I0229 05:21:57.132964 140067674781440 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.8197712898254395, loss=4.885373592376709
I0229 05:22:30.697265 140067758642944 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.15078067779541, loss=4.801756858825684
I0229 05:23:04.268170 140067674781440 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.6745691299438477, loss=4.706890106201172
I0229 05:23:37.843178 140067758642944 logging_writer.py:48] [1400] global_step=1400, grad_norm=7.369884490966797, loss=4.630002498626709
I0229 05:24:11.483911 140067674781440 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.3774402141571045, loss=4.510443210601807
I0229 05:24:17.301784 140252611495744 spec.py:321] Evaluating on the training split.
I0229 05:24:24.499351 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 05:24:32.541966 140252611495744 spec.py:349] Evaluating on the test split.
I0229 05:24:34.827839 140252611495744 submission_runner.py:411] Time since start: 615.71s, 	Step: 1519, 	{'train/accuracy': 0.16137196123600006, 'train/loss': 4.343627452850342, 'validation/accuracy': 0.1440799981355667, 'validation/loss': 4.503424167633057, 'validation/num_examples': 50000, 'test/accuracy': 0.10940000414848328, 'test/loss': 4.919663429260254, 'test/num_examples': 10000, 'score': 561.3822112083435, 'total_duration': 615.7079174518585, 'accumulated_submission_time': 561.3822112083435, 'accumulated_eval_time': 54.24302554130554, 'accumulated_logging_time': 0.027426719665527344}
I0229 05:24:34.844795 140067767035648 logging_writer.py:48] [1519] accumulated_eval_time=54.243026, accumulated_logging_time=0.027427, accumulated_submission_time=561.382211, global_step=1519, preemption_count=0, score=561.382211, test/accuracy=0.109400, test/loss=4.919663, test/num_examples=10000, total_duration=615.707917, train/accuracy=0.161372, train/loss=4.343627, validation/accuracy=0.144080, validation/loss=4.503424, validation/num_examples=50000
I0229 05:25:02.415338 140067775428352 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.5852601528167725, loss=4.374115467071533
I0229 05:25:36.079992 140067767035648 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.7626094818115234, loss=4.30474853515625
I0229 05:26:09.723563 140067775428352 logging_writer.py:48] [1800] global_step=1800, grad_norm=4.390382289886475, loss=4.292877674102783
I0229 05:26:43.284706 140067767035648 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.250272750854492, loss=4.14080286026001
I0229 05:27:16.959413 140067775428352 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.888949394226074, loss=4.026297569274902
I0229 05:27:50.630701 140067767035648 logging_writer.py:48] [2100] global_step=2100, grad_norm=6.911850452423096, loss=3.9091718196868896
I0229 05:28:24.284734 140067775428352 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.5044028759002686, loss=3.8772764205932617
I0229 05:28:57.892732 140067767035648 logging_writer.py:48] [2300] global_step=2300, grad_norm=6.272110939025879, loss=3.851928949356079
I0229 05:29:31.544696 140067775428352 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.365164756774902, loss=3.8055670261383057
I0229 05:30:05.123962 140067767035648 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.92733097076416, loss=3.8045172691345215
I0229 05:30:38.779653 140067775428352 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.335610866546631, loss=3.608466625213623
I0229 05:31:12.300759 140067767035648 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.798499822616577, loss=3.4511921405792236
I0229 05:31:45.820826 140067775428352 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.549655914306641, loss=3.6312813758850098
I0229 05:32:19.383225 140067767035648 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.9052622318267822, loss=3.565258026123047
I0229 05:32:53.011813 140067775428352 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.8333330154418945, loss=3.3753366470336914
I0229 05:33:04.989173 140252611495744 spec.py:321] Evaluating on the training split.
I0229 05:33:12.067699 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 05:33:20.331620 140252611495744 spec.py:349] Evaluating on the test split.
I0229 05:33:22.533247 140252611495744 submission_runner.py:411] Time since start: 1143.41s, 	Step: 3037, 	{'train/accuracy': 0.3479352593421936, 'train/loss': 3.004847288131714, 'validation/accuracy': 0.32349997758865356, 'validation/loss': 3.174034833908081, 'validation/num_examples': 50000, 'test/accuracy': 0.23920001089572906, 'test/loss': 3.840391159057617, 'test/num_examples': 10000, 'score': 1071.4606931209564, 'total_duration': 1143.4133098125458, 'accumulated_submission_time': 1071.4606931209564, 'accumulated_eval_time': 71.78705096244812, 'accumulated_logging_time': 0.05564451217651367}
I0229 05:33:22.550113 140086557517568 logging_writer.py:48] [3037] accumulated_eval_time=71.787051, accumulated_logging_time=0.055645, accumulated_submission_time=1071.460693, global_step=3037, preemption_count=0, score=1071.460693, test/accuracy=0.239200, test/loss=3.840391, test/num_examples=10000, total_duration=1143.413310, train/accuracy=0.347935, train/loss=3.004847, validation/accuracy=0.323500, validation/loss=3.174035, validation/num_examples=50000
I0229 05:33:44.105322 140086599481088 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5848448276519775, loss=3.5435938835144043
I0229 05:34:17.704957 140086557517568 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.513606548309326, loss=3.2353572845458984
I0229 05:34:51.231767 140086599481088 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.236246585845947, loss=3.263357162475586
I0229 05:35:24.772073 140086557517568 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.400383234024048, loss=3.2175490856170654
I0229 05:35:58.376047 140086599481088 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.9312238693237305, loss=3.1602396965026855
I0229 05:36:31.958101 140086557517568 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.2102437019348145, loss=3.0605149269104004
I0229 05:37:05.446698 140086599481088 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2607545852661133, loss=3.1797611713409424
I0229 05:37:38.989334 140086557517568 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.4166486263275146, loss=3.051098108291626
I0229 05:38:12.493909 140086599481088 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.1666083335876465, loss=3.105201244354248
I0229 05:38:46.061277 140086557517568 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.70129132270813, loss=2.836501359939575
I0229 05:39:19.696401 140086599481088 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.372875928878784, loss=2.977055549621582
I0229 05:39:53.259217 140086557517568 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.0920729637145996, loss=3.019943952560425
I0229 05:40:26.797466 140086599481088 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.66491961479187, loss=2.7817764282226562
I0229 05:41:00.325395 140086557517568 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.9235870838165283, loss=2.815176486968994
I0229 05:41:33.951578 140086599481088 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.9796189069747925, loss=2.761794328689575
I0229 05:41:52.863083 140252611495744 spec.py:321] Evaluating on the training split.
I0229 05:41:59.951586 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 05:42:08.330098 140252611495744 spec.py:349] Evaluating on the test split.
I0229 05:42:10.609438 140252611495744 submission_runner.py:411] Time since start: 1671.49s, 	Step: 4558, 	{'train/accuracy': 0.46697622537612915, 'train/loss': 2.329172372817993, 'validation/accuracy': 0.4377799928188324, 'validation/loss': 2.5032687187194824, 'validation/num_examples': 50000, 'test/accuracy': 0.32360002398490906, 'test/loss': 3.25213885307312, 'test/num_examples': 10000, 'score': 1581.70814204216, 'total_duration': 1671.4895164966583, 'accumulated_submission_time': 1581.70814204216, 'accumulated_eval_time': 89.53337836265564, 'accumulated_logging_time': 0.08395624160766602}
I0229 05:42:10.625868 140086574302976 logging_writer.py:48] [4558] accumulated_eval_time=89.533378, accumulated_logging_time=0.083956, accumulated_submission_time=1581.708142, global_step=4558, preemption_count=0, score=1581.708142, test/accuracy=0.323600, test/loss=3.252139, test/num_examples=10000, total_duration=1671.489516, train/accuracy=0.466976, train/loss=2.329172, validation/accuracy=0.437780, validation/loss=2.503269, validation/num_examples=50000
I0229 05:42:25.071505 140086582695680 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.235407590866089, loss=2.849905252456665
I0229 05:42:58.674278 140086574302976 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.1595470905303955, loss=2.788796901702881
I0229 05:43:32.234664 140086582695680 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.619371175765991, loss=2.7980728149414062
I0229 05:44:05.728595 140086574302976 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.304267644882202, loss=2.6735363006591797
I0229 05:44:39.368252 140086582695680 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.1350390911102295, loss=2.6540305614471436
I0229 05:45:13.031324 140086574302976 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.2964017391204834, loss=2.675016403198242
I0229 05:45:46.664681 140086582695680 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.6254703998565674, loss=2.709320545196533
I0229 05:46:20.268652 140086574302976 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.6932616233825684, loss=2.636634588241577
I0229 05:46:53.760035 140086582695680 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.620035409927368, loss=2.6463961601257324
I0229 05:47:27.302872 140086574302976 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.045764684677124, loss=2.5517592430114746
I0229 05:48:00.796522 140086582695680 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.9290945529937744, loss=2.373342514038086
I0229 05:48:34.396477 140086574302976 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.285790205001831, loss=2.342287540435791
I0229 05:49:07.881008 140086582695680 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.1958303451538086, loss=2.4761955738067627
I0229 05:49:41.449614 140086574302976 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.0451455116271973, loss=2.4355664253234863
I0229 05:50:14.947508 140086582695680 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.9684289693832397, loss=2.5001978874206543
I0229 05:50:40.850747 140252611495744 spec.py:321] Evaluating on the training split.
I0229 05:50:47.917789 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 05:50:56.168492 140252611495744 spec.py:349] Evaluating on the test split.
I0229 05:50:58.380690 140252611495744 submission_runner.py:411] Time since start: 2199.26s, 	Step: 6079, 	{'train/accuracy': 0.5306122303009033, 'train/loss': 2.0072100162506104, 'validation/accuracy': 0.4969799816608429, 'validation/loss': 2.2160658836364746, 'validation/num_examples': 50000, 'test/accuracy': 0.3765000104904175, 'test/loss': 2.998728036880493, 'test/num_examples': 10000, 'score': 2091.871442079544, 'total_duration': 2199.260755300522, 'accumulated_submission_time': 2091.871442079544, 'accumulated_eval_time': 107.06326746940613, 'accumulated_logging_time': 0.10955691337585449}
I0229 05:50:58.397917 140087220213504 logging_writer.py:48] [6079] accumulated_eval_time=107.063267, accumulated_logging_time=0.109557, accumulated_submission_time=2091.871442, global_step=6079, preemption_count=0, score=2091.871442, test/accuracy=0.376500, test/loss=2.998728, test/num_examples=10000, total_duration=2199.260755, train/accuracy=0.530612, train/loss=2.007210, validation/accuracy=0.496980, validation/loss=2.216066, validation/num_examples=50000
I0229 05:51:05.775737 140087228606208 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.4810914993286133, loss=2.3928680419921875
I0229 05:51:39.338845 140087220213504 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.2609739303588867, loss=2.4552669525146484
I0229 05:52:12.859586 140087228606208 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.8102134466171265, loss=2.4760701656341553
I0229 05:52:46.443245 140087220213504 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.4458649158477783, loss=2.3212027549743652
I0229 05:53:19.961591 140087228606208 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.810932993888855, loss=2.3876922130584717
I0229 05:53:53.505000 140087220213504 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.3991007804870605, loss=2.459146499633789
I0229 05:54:27.026130 140087228606208 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.1342992782592773, loss=2.4588096141815186
I0229 05:55:00.580885 140087220213504 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.9989898204803467, loss=2.2618446350097656
I0229 05:55:34.101249 140087228606208 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.7286511659622192, loss=2.4045844078063965
I0229 05:56:07.636096 140087220213504 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.8733954429626465, loss=2.3233742713928223
I0229 05:56:41.153752 140087228606208 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.2520875930786133, loss=2.317934989929199
I0229 05:57:14.710468 140087220213504 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.084925889968872, loss=2.404911518096924
I0229 05:57:48.257811 140087228606208 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.1842939853668213, loss=2.218564987182617
I0229 05:58:21.764819 140087220213504 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.821149468421936, loss=2.2256662845611572
I0229 05:58:55.274270 140087228606208 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2137041091918945, loss=2.241825819015503
I0229 05:59:28.826158 140087220213504 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.183464527130127, loss=2.189056873321533
I0229 05:59:28.834679 140252611495744 spec.py:321] Evaluating on the training split.
I0229 05:59:35.998664 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 05:59:44.135567 140252611495744 spec.py:349] Evaluating on the test split.
I0229 05:59:46.400372 140252611495744 submission_runner.py:411] Time since start: 2727.28s, 	Step: 7601, 	{'train/accuracy': 0.5672432780265808, 'train/loss': 1.8172202110290527, 'validation/accuracy': 0.5317000150680542, 'validation/loss': 2.017817258834839, 'validation/num_examples': 50000, 'test/accuracy': 0.41760000586509705, 'test/loss': 2.7481794357299805, 'test/num_examples': 10000, 'score': 2602.244828939438, 'total_duration': 2727.28044962883, 'accumulated_submission_time': 2602.244828939438, 'accumulated_eval_time': 124.62890672683716, 'accumulated_logging_time': 0.13683772087097168}
I0229 05:59:46.417464 140087052457728 logging_writer.py:48] [7601] accumulated_eval_time=124.628907, accumulated_logging_time=0.136838, accumulated_submission_time=2602.244829, global_step=7601, preemption_count=0, score=2602.244829, test/accuracy=0.417600, test/loss=2.748179, test/num_examples=10000, total_duration=2727.280450, train/accuracy=0.567243, train/loss=1.817220, validation/accuracy=0.531700, validation/loss=2.017817, validation/num_examples=50000
I0229 06:00:19.918279 140087299913472 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.7672452926635742, loss=2.231236219406128
I0229 06:00:53.476303 140087052457728 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.768784999847412, loss=2.1522324085235596
I0229 06:01:26.985684 140087299913472 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.5349395275115967, loss=2.159505844116211
I0229 06:02:00.576120 140087052457728 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.618276834487915, loss=2.249558448791504
I0229 06:02:34.062473 140087299913472 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.9892078638076782, loss=2.169372081756592
I0229 06:03:07.646001 140087052457728 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.173586368560791, loss=2.215134382247925
I0229 06:03:41.189065 140087299913472 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.281851887702942, loss=2.0845766067504883
I0229 06:04:14.708361 140087052457728 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.0007388591766357, loss=2.1208090782165527
I0229 06:04:48.225938 140087299913472 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.6040903329849243, loss=2.176740884780884
I0229 06:05:21.779632 140087052457728 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.470590353012085, loss=2.0390625
I0229 06:05:55.304520 140087299913472 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.7349836826324463, loss=2.1566309928894043
I0229 06:06:28.890851 140087052457728 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.7756242752075195, loss=2.119288682937622
I0229 06:07:02.368949 140087299913472 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.52931547164917, loss=2.1408021450042725
I0229 06:07:35.939647 140087052457728 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.9981586933135986, loss=2.220524787902832
I0229 06:08:09.439823 140087299913472 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.4134770631790161, loss=2.0747852325439453
I0229 06:08:16.591940 140252611495744 spec.py:321] Evaluating on the training split.
I0229 06:08:23.840265 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 06:08:32.067001 140252611495744 spec.py:349] Evaluating on the test split.
I0229 06:08:34.349306 140252611495744 submission_runner.py:411] Time since start: 3255.23s, 	Step: 9123, 	{'train/accuracy': 0.6525231003761292, 'train/loss': 1.4047654867172241, 'validation/accuracy': 0.5640599727630615, 'validation/loss': 1.8469562530517578, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.59698748588562, 'test/num_examples': 10000, 'score': 3112.356943130493, 'total_duration': 3255.2293422222137, 'accumulated_submission_time': 3112.356943130493, 'accumulated_eval_time': 142.38618516921997, 'accumulated_logging_time': 0.1638340950012207}
I0229 06:08:34.367125 140087320815360 logging_writer.py:48] [9123] accumulated_eval_time=142.386185, accumulated_logging_time=0.163834, accumulated_submission_time=3112.356943, global_step=9123, preemption_count=0, score=3112.356943, test/accuracy=0.439700, test/loss=2.596987, test/num_examples=10000, total_duration=3255.229342, train/accuracy=0.652523, train/loss=1.404765, validation/accuracy=0.564060, validation/loss=1.846956, validation/num_examples=50000
I0229 06:09:00.514325 140087362778880 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.9854564666748047, loss=2.1381261348724365
I0229 06:09:34.035375 140087320815360 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.7829711437225342, loss=2.3099803924560547
I0229 06:10:07.554367 140087362778880 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.059434175491333, loss=1.9884001016616821
I0229 06:10:41.143485 140087320815360 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.4637621641159058, loss=2.1965932846069336
I0229 06:11:14.647028 140087362778880 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8583135604858398, loss=2.1454527378082275
I0229 06:11:48.201482 140087320815360 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.6223798990249634, loss=2.205392360687256
I0229 06:12:21.705513 140087362778880 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.548146367073059, loss=2.1016974449157715
I0229 06:12:55.276867 140087320815360 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.7440674304962158, loss=2.140529155731201
I0229 06:13:28.776438 140087362778880 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8628002405166626, loss=2.1662344932556152
I0229 06:14:02.341845 140087320815360 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.6575541496276855, loss=2.1706931591033936
I0229 06:14:35.859423 140087362778880 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.553413987159729, loss=2.125209331512451
I0229 06:15:09.446221 140087320815360 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.599233865737915, loss=2.1590077877044678
I0229 06:15:42.988830 140087362778880 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.5174006223678589, loss=2.162177324295044
I0229 06:16:16.533805 140087320815360 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4626331329345703, loss=1.8755565881729126
I0229 06:16:50.027804 140087362778880 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.6510634422302246, loss=2.1372454166412354
I0229 06:17:04.591262 140252611495744 spec.py:321] Evaluating on the training split.
I0229 06:17:12.079251 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 06:17:20.746057 140252611495744 spec.py:349] Evaluating on the test split.
I0229 06:17:22.988620 140252611495744 submission_runner.py:411] Time since start: 3783.87s, 	Step: 10645, 	{'train/accuracy': 0.6332509517669678, 'train/loss': 1.4783037900924683, 'validation/accuracy': 0.5731399655342102, 'validation/loss': 1.8122446537017822, 'validation/num_examples': 50000, 'test/accuracy': 0.4448000192642212, 'test/loss': 2.552358627319336, 'test/num_examples': 10000, 'score': 3622.518341064453, 'total_duration': 3783.8686969280243, 'accumulated_submission_time': 3622.518341064453, 'accumulated_eval_time': 160.78351044654846, 'accumulated_logging_time': 0.1913130283355713}
I0229 06:17:23.012580 140087337600768 logging_writer.py:48] [10645] accumulated_eval_time=160.783510, accumulated_logging_time=0.191313, accumulated_submission_time=3622.518341, global_step=10645, preemption_count=0, score=3622.518341, test/accuracy=0.444800, test/loss=2.552359, test/num_examples=10000, total_duration=3783.868697, train/accuracy=0.633251, train/loss=1.478304, validation/accuracy=0.573140, validation/loss=1.812245, validation/num_examples=50000
I0229 06:17:41.782677 140087345993472 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.8061232566833496, loss=2.004608631134033
I0229 06:18:15.387898 140087337600768 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.7186907529830933, loss=1.9887734651565552
I0229 06:18:48.923446 140087345993472 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.7869563102722168, loss=2.054155111312866
I0229 06:19:22.469428 140087337600768 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9129873514175415, loss=2.073218822479248
I0229 06:19:56.048774 140087345993472 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.7939177751541138, loss=2.0483431816101074
I0229 06:20:29.549354 140087337600768 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.6470375061035156, loss=2.0467281341552734
I0229 06:21:03.087170 140087345993472 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.909667730331421, loss=1.945342779159546
I0229 06:21:36.638064 140087337600768 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.362447738647461, loss=1.9681484699249268
I0229 06:22:10.202043 140087345993472 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.3127851486206055, loss=1.9806839227676392
I0229 06:22:43.702521 140087337600768 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.6134496927261353, loss=2.083394765853882
I0229 06:23:17.278797 140087345993472 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.7963148355484009, loss=2.083336591720581
I0229 06:23:50.794682 140087337600768 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.7162669897079468, loss=2.0834648609161377
I0229 06:24:24.366187 140087345993472 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.366700530052185, loss=1.9745309352874756
I0229 06:24:57.860300 140087337600768 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.6749459505081177, loss=1.8665955066680908
I0229 06:25:31.398952 140087345993472 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.0617334842681885, loss=1.974113941192627
I0229 06:25:53.274739 140252611495744 spec.py:321] Evaluating on the training split.
I0229 06:26:00.483086 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 06:26:08.889828 140252611495744 spec.py:349] Evaluating on the test split.
I0229 06:26:11.138645 140252611495744 submission_runner.py:411] Time since start: 4312.02s, 	Step: 12167, 	{'train/accuracy': 0.6190608739852905, 'train/loss': 1.5576162338256836, 'validation/accuracy': 0.5658199787139893, 'validation/loss': 1.834885835647583, 'validation/num_examples': 50000, 'test/accuracy': 0.43650001287460327, 'test/loss': 2.630974054336548, 'test/num_examples': 10000, 'score': 4132.719137430191, 'total_duration': 4312.018725633621, 'accumulated_submission_time': 4132.719137430191, 'accumulated_eval_time': 178.64738535881042, 'accumulated_logging_time': 0.22454261779785156}
I0229 06:26:11.156821 140087379564288 logging_writer.py:48] [12167] accumulated_eval_time=178.647385, accumulated_logging_time=0.224543, accumulated_submission_time=4132.719137, global_step=12167, preemption_count=0, score=4132.719137, test/accuracy=0.436500, test/loss=2.630974, test/num_examples=10000, total_duration=4312.018726, train/accuracy=0.619061, train/loss=1.557616, validation/accuracy=0.565820, validation/loss=1.834886, validation/num_examples=50000
I0229 06:26:22.585556 140087387956992 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.3177789449691772, loss=1.9102386236190796
I0229 06:26:56.103229 140087379564288 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.6681382656097412, loss=2.0567033290863037
I0229 06:27:29.663400 140087387956992 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.6568856239318848, loss=2.036893844604492
I0229 06:28:03.201006 140087379564288 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.670182704925537, loss=1.875765323638916
I0229 06:28:36.709737 140087387956992 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.4720420837402344, loss=1.9470596313476562
I0229 06:29:10.181251 140087379564288 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.341119408607483, loss=1.893499732017517
I0229 06:29:43.693815 140087387956992 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.730683445930481, loss=1.9452908039093018
I0229 06:30:17.194966 140087379564288 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.4574856758117676, loss=1.9019412994384766
I0229 06:30:50.791473 140087387956992 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.5968356132507324, loss=1.947038173675537
I0229 06:31:24.284636 140087379564288 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.021397590637207, loss=1.91896390914917
I0229 06:31:57.839258 140087387956992 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.7031339406967163, loss=1.9062620401382446
I0229 06:32:31.328636 140087379564288 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.8746131658554077, loss=1.7812750339508057
I0229 06:33:04.894780 140087387956992 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5838844776153564, loss=1.9241806268692017
I0229 06:33:38.392147 140087379564288 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.5680238008499146, loss=1.9574066400527954
I0229 06:34:11.933888 140087387956992 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.4405615329742432, loss=1.8263252973556519
I0229 06:34:41.179833 140252611495744 spec.py:321] Evaluating on the training split.
I0229 06:34:48.504544 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 06:34:56.664814 140252611495744 spec.py:349] Evaluating on the test split.
I0229 06:34:58.932971 140252611495744 submission_runner.py:411] Time since start: 4839.81s, 	Step: 13689, 	{'train/accuracy': 0.6477000713348389, 'train/loss': 1.4246323108673096, 'validation/accuracy': 0.5928399562835693, 'validation/loss': 1.7087825536727905, 'validation/num_examples': 50000, 'test/accuracy': 0.47700002789497375, 'test/loss': 2.4406228065490723, 'test/num_examples': 10000, 'score': 4642.680076122284, 'total_duration': 4839.813012838364, 'accumulated_submission_time': 4642.680076122284, 'accumulated_eval_time': 196.40045833587646, 'accumulated_logging_time': 0.2521340847015381}
I0229 06:34:58.957750 140088411420416 logging_writer.py:48] [13689] accumulated_eval_time=196.400458, accumulated_logging_time=0.252134, accumulated_submission_time=4642.680076, global_step=13689, preemption_count=0, score=4642.680076, test/accuracy=0.477000, test/loss=2.440623, test/num_examples=10000, total_duration=4839.813013, train/accuracy=0.647700, train/loss=1.424632, validation/accuracy=0.592840, validation/loss=1.708783, validation/num_examples=50000
I0229 06:35:02.987033 140088495281920 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.7133736610412598, loss=1.8249146938323975
I0229 06:35:36.472193 140088411420416 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.5975427627563477, loss=1.906488299369812
I0229 06:36:09.927080 140088495281920 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4287701845169067, loss=2.01133394241333
I0229 06:36:43.465926 140088411420416 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.571990966796875, loss=1.9783594608306885
I0229 06:37:17.014065 140088495281920 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5663625001907349, loss=2.0072317123413086
I0229 06:37:50.559594 140088411420416 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.4764933586120605, loss=1.87398362159729
I0229 06:38:24.048207 140088495281920 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.625282645225525, loss=1.9214208126068115
I0229 06:38:57.542665 140088411420416 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.034491777420044, loss=1.961064338684082
I0229 06:39:31.111138 140088495281920 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6121795177459717, loss=1.8636001348495483
I0229 06:40:04.706210 140088411420416 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.454647183418274, loss=2.0112621784210205
I0229 06:40:38.170303 140088495281920 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.5390452146530151, loss=1.8879921436309814
I0229 06:41:11.702318 140088411420416 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.701295256614685, loss=1.9386738538742065
I0229 06:41:45.223287 140088495281920 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.8303391933441162, loss=1.971622109413147
I0229 06:42:18.760989 140088411420416 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.0096848011016846, loss=2.07802414894104
I0229 06:42:52.310530 140088495281920 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.6852281093597412, loss=1.7712504863739014
I0229 06:43:25.852847 140088411420416 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.4257736206054688, loss=1.9367141723632812
I0229 06:43:28.963967 140252611495744 spec.py:321] Evaluating on the training split.
I0229 06:43:36.944290 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 06:43:45.161180 140252611495744 spec.py:349] Evaluating on the test split.
I0229 06:43:47.406137 140252611495744 submission_runner.py:411] Time since start: 5368.29s, 	Step: 15211, 	{'train/accuracy': 0.6474409699440002, 'train/loss': 1.4198980331420898, 'validation/accuracy': 0.5953800082206726, 'validation/loss': 1.6889138221740723, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.4248533248901367, 'test/num_examples': 10000, 'score': 5152.619967222214, 'total_duration': 5368.286201000214, 'accumulated_submission_time': 5152.619967222214, 'accumulated_eval_time': 214.84256839752197, 'accumulated_logging_time': 0.289806604385376}
I0229 06:43:47.437841 140088403027712 logging_writer.py:48] [15211] accumulated_eval_time=214.842568, accumulated_logging_time=0.289807, accumulated_submission_time=5152.619967, global_step=15211, preemption_count=0, score=5152.619967, test/accuracy=0.472600, test/loss=2.424853, test/num_examples=10000, total_duration=5368.286201, train/accuracy=0.647441, train/loss=1.419898, validation/accuracy=0.595380, validation/loss=1.688914, validation/num_examples=50000
I0229 06:44:17.548445 140088411420416 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.6349074840545654, loss=1.9324207305908203
I0229 06:44:51.027837 140088403027712 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.1581172943115234, loss=1.8992865085601807
I0229 06:45:24.577848 140088411420416 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.6249721050262451, loss=1.8890572786331177
I0229 06:45:58.091516 140088403027712 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.752845048904419, loss=1.8851141929626465
I0229 06:46:31.608882 140088411420416 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.492087483406067, loss=1.7865393161773682
I0229 06:47:05.101202 140088403027712 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.8581041097640991, loss=1.9522621631622314
I0229 06:47:38.670204 140088411420416 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.8216395378112793, loss=1.8446146249771118
I0229 06:48:12.157254 140088403027712 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.7485597133636475, loss=1.8587819337844849
I0229 06:48:45.728864 140088411420416 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.3955742120742798, loss=1.84818434715271
I0229 06:49:19.208103 140088403027712 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.587443470954895, loss=1.8699064254760742
I0229 06:49:52.788657 140088411420416 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.5327433347702026, loss=1.8736721277236938
I0229 06:50:26.274864 140088403027712 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.6182258129119873, loss=1.9652252197265625
I0229 06:50:59.842099 140088411420416 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.604727029800415, loss=1.8785278797149658
I0229 06:51:33.329064 140088403027712 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.4265872240066528, loss=1.8820436000823975
I0229 06:52:06.902408 140088411420416 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.7203843593597412, loss=1.8451422452926636
I0229 06:52:17.712767 140252611495744 spec.py:321] Evaluating on the training split.
I0229 06:52:25.265585 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 06:52:33.589007 140252611495744 spec.py:349] Evaluating on the test split.
I0229 06:52:35.877225 140252611495744 submission_runner.py:411] Time since start: 5896.76s, 	Step: 16734, 	{'train/accuracy': 0.6482182741165161, 'train/loss': 1.4272924661636353, 'validation/accuracy': 0.6008999943733215, 'validation/loss': 1.6777706146240234, 'validation/num_examples': 50000, 'test/accuracy': 0.468500018119812, 'test/loss': 2.439729690551758, 'test/num_examples': 10000, 'score': 5662.831291437149, 'total_duration': 5896.7572984695435, 'accumulated_submission_time': 5662.831291437149, 'accumulated_eval_time': 233.00697684288025, 'accumulated_logging_time': 0.3311009407043457}
I0229 06:52:35.903805 140089820641024 logging_writer.py:48] [16734] accumulated_eval_time=233.006977, accumulated_logging_time=0.331101, accumulated_submission_time=5662.831291, global_step=16734, preemption_count=0, score=5662.831291, test/accuracy=0.468500, test/loss=2.439730, test/num_examples=10000, total_duration=5896.757298, train/accuracy=0.648218, train/loss=1.427292, validation/accuracy=0.600900, validation/loss=1.677771, validation/num_examples=50000
I0229 06:52:58.352572 140089845819136 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.569945216178894, loss=1.8490207195281982
I0229 06:53:31.855742 140089820641024 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.4669015407562256, loss=1.873547077178955
I0229 06:54:05.385922 140089845819136 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.5789154767990112, loss=1.8818267583847046
I0229 06:54:38.918361 140089820641024 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4799162149429321, loss=1.8564480543136597
I0229 06:55:12.410062 140089845819136 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.530189871788025, loss=1.7776784896850586
I0229 06:55:45.868642 140089820641024 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.2825134992599487, loss=1.8799645900726318
I0229 06:56:19.326830 140089845819136 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.808820366859436, loss=1.9041073322296143
I0229 06:56:52.832638 140089820641024 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6247445344924927, loss=1.9822866916656494
I0229 06:57:26.329030 140089845819136 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6237406730651855, loss=1.8502099514007568
I0229 06:57:59.894841 140089820641024 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8673819303512573, loss=1.8215519189834595
I0229 06:58:33.398589 140089845819136 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.6348824501037598, loss=1.9150142669677734
I0229 06:59:06.899007 140089820641024 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.638989806175232, loss=1.8583884239196777
I0229 06:59:40.441051 140089845819136 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.6547824144363403, loss=1.8491566181182861
I0229 07:00:13.892065 140089820641024 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.9096356630325317, loss=1.7242505550384521
I0229 07:00:47.342627 140089845819136 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.590811014175415, loss=1.8184865713119507
I0229 07:01:06.211731 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:01:13.805104 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:01:22.146475 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:01:24.403807 140252611495744 submission_runner.py:411] Time since start: 6425.28s, 	Step: 18258, 	{'train/accuracy': 0.6813018321990967, 'train/loss': 1.2537661790847778, 'validation/accuracy': 0.6010000109672546, 'validation/loss': 1.6689846515655518, 'validation/num_examples': 50000, 'test/accuracy': 0.4749000370502472, 'test/loss': 2.4517030715942383, 'test/num_examples': 10000, 'score': 6173.075484752655, 'total_duration': 6425.283870458603, 'accumulated_submission_time': 6173.075484752655, 'accumulated_eval_time': 251.19899654388428, 'accumulated_logging_time': 0.3682253360748291}
I0229 07:01:24.435533 140089870997248 logging_writer.py:48] [18258] accumulated_eval_time=251.198997, accumulated_logging_time=0.368225, accumulated_submission_time=6173.075485, global_step=18258, preemption_count=0, score=6173.075485, test/accuracy=0.474900, test/loss=2.451703, test/num_examples=10000, total_duration=6425.283870, train/accuracy=0.681302, train/loss=1.253766, validation/accuracy=0.601000, validation/loss=1.668985, validation/num_examples=50000
I0229 07:01:38.832763 140089879389952 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.695557713508606, loss=1.8376253843307495
I0229 07:02:12.293605 140089870997248 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.6847920417785645, loss=1.9226224422454834
I0229 07:02:45.793652 140089879389952 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7156202793121338, loss=1.8690073490142822
I0229 07:03:19.303301 140089870997248 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.635928988456726, loss=1.8500384092330933
I0229 07:03:52.795373 140089879389952 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.740034818649292, loss=1.860987901687622
I0229 07:04:26.261658 140089870997248 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.7122584581375122, loss=1.8806127309799194
I0229 07:04:59.718832 140089879389952 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.4608500003814697, loss=1.8614798784255981
I0229 07:05:33.235186 140089870997248 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.7187871932983398, loss=1.8591753244400024
I0229 07:06:06.743599 140089879389952 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5184862613677979, loss=1.7799209356307983
I0229 07:06:40.317771 140089870997248 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.5032988786697388, loss=1.7630119323730469
I0229 07:07:13.798044 140089879389952 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.6740049123764038, loss=1.9623873233795166
I0229 07:07:47.359068 140089870997248 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.8747310638427734, loss=1.940972924232483
I0229 07:08:20.840603 140089879389952 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.5955851078033447, loss=1.8453822135925293
I0229 07:08:54.385284 140089870997248 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.8858880996704102, loss=1.7034926414489746
I0229 07:09:27.871713 140089879389952 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.5767793655395508, loss=1.809997320175171
I0229 07:09:54.477076 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:10:02.700174 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:10:11.118801 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:10:13.744959 140252611495744 submission_runner.py:411] Time since start: 6954.63s, 	Step: 19781, 	{'train/accuracy': 0.6559510231018066, 'train/loss': 1.3652417659759521, 'validation/accuracy': 0.5920000076293945, 'validation/loss': 1.7166610956192017, 'validation/num_examples': 50000, 'test/accuracy': 0.46790000796318054, 'test/loss': 2.4602901935577393, 'test/num_examples': 10000, 'score': 6683.053819656372, 'total_duration': 6954.625028371811, 'accumulated_submission_time': 6683.053819656372, 'accumulated_eval_time': 270.4668297767639, 'accumulated_logging_time': 0.40930700302124023}
I0229 07:10:13.776835 140089812248320 logging_writer.py:48] [19781] accumulated_eval_time=270.466830, accumulated_logging_time=0.409307, accumulated_submission_time=6683.053820, global_step=19781, preemption_count=0, score=6683.053820, test/accuracy=0.467900, test/loss=2.460290, test/num_examples=10000, total_duration=6954.625028, train/accuracy=0.655951, train/loss=1.365242, validation/accuracy=0.592000, validation/loss=1.716661, validation/num_examples=50000
I0229 07:10:20.488181 140089820641024 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.8245594501495361, loss=1.8174362182617188
I0229 07:10:54.008622 140089812248320 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.4479459524154663, loss=1.7822128534317017
I0229 07:11:27.510677 140089820641024 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.574101448059082, loss=1.7867196798324585
I0229 07:12:01.052896 140089812248320 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.695278525352478, loss=1.7611675262451172
I0229 07:12:34.582438 140089820641024 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.598883032798767, loss=1.802175760269165
I0229 07:13:08.088927 140089812248320 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6990180015563965, loss=1.9070100784301758
I0229 07:13:41.627258 140089820641024 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.7848376035690308, loss=1.9077537059783936
I0229 07:14:15.124721 140089812248320 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.7177423238754272, loss=1.877374291419983
I0229 07:14:48.597546 140089820641024 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.0096375942230225, loss=1.8229767084121704
I0229 07:15:22.054553 140089812248320 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.4643245935440063, loss=1.7748124599456787
I0229 07:15:55.545337 140089820641024 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.980762004852295, loss=1.874596357345581
I0229 07:16:29.085421 140089812248320 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.6354680061340332, loss=1.8304874897003174
I0229 07:17:02.607147 140089820641024 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.7598572969436646, loss=1.857597827911377
I0229 07:17:36.099558 140089812248320 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.6779073476791382, loss=1.9410905838012695
I0229 07:18:09.589449 140089820641024 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.797899603843689, loss=1.8111352920532227
I0229 07:18:43.069128 140089812248320 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.4809931516647339, loss=1.863159418106079
I0229 07:18:43.839501 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:18:51.130203 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:19:04.003694 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:19:06.138458 140252611495744 submission_runner.py:411] Time since start: 7487.02s, 	Step: 21304, 	{'train/accuracy': 0.6759207248687744, 'train/loss': 1.2931432723999023, 'validation/accuracy': 0.6177799701690674, 'validation/loss': 1.5928702354431152, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.318434953689575, 'test/num_examples': 10000, 'score': 7193.051939487457, 'total_duration': 7487.018528699875, 'accumulated_submission_time': 7193.051939487457, 'accumulated_eval_time': 292.76573491096497, 'accumulated_logging_time': 0.4507136344909668}
I0229 07:19:06.156594 140089870997248 logging_writer.py:48] [21304] accumulated_eval_time=292.765735, accumulated_logging_time=0.450714, accumulated_submission_time=7193.051939, global_step=21304, preemption_count=0, score=7193.051939, test/accuracy=0.493400, test/loss=2.318435, test/num_examples=10000, total_duration=7487.018529, train/accuracy=0.675921, train/loss=1.293143, validation/accuracy=0.617780, validation/loss=1.592870, validation/num_examples=50000
I0229 07:19:38.610294 140089879389952 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.5647839307785034, loss=1.8260626792907715
I0229 07:20:12.137379 140089870997248 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.5743439197540283, loss=1.823548436164856
I0229 07:20:45.659118 140089879389952 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.7080535888671875, loss=1.8152432441711426
I0229 07:21:19.239176 140089870997248 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.5721246004104614, loss=1.6506372690200806
I0229 07:21:52.710132 140089879389952 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.605610728263855, loss=1.7365491390228271
I0229 07:22:26.248067 140089870997248 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.6881656646728516, loss=1.760306477546692
I0229 07:22:59.747061 140089879389952 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.6397196054458618, loss=1.855164647102356
I0229 07:23:33.206457 140089870997248 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3603869676589966, loss=1.7335357666015625
I0229 07:24:06.654036 140089879389952 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.6203646659851074, loss=1.826588749885559
I0229 07:24:40.186995 140089870997248 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.9340633153915405, loss=1.8156092166900635
I0229 07:25:13.675366 140089879389952 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.634236216545105, loss=1.757575273513794
I0229 07:25:47.210698 140089870997248 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.4943280220031738, loss=1.606651782989502
I0229 07:26:20.728991 140089879389952 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.9667688608169556, loss=1.845524549484253
I0229 07:26:54.270899 140089870997248 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.6378583908081055, loss=1.7251662015914917
I0229 07:27:27.781079 140089879389952 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.9408167600631714, loss=1.7946759462356567
I0229 07:27:36.262439 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:27:43.643558 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:27:58.019948 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:28:00.156432 140252611495744 submission_runner.py:411] Time since start: 8021.04s, 	Step: 22827, 	{'train/accuracy': 0.6716158986091614, 'train/loss': 1.3155529499053955, 'validation/accuracy': 0.6108999848365784, 'validation/loss': 1.6090680360794067, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.3439013957977295, 'test/num_examples': 10000, 'score': 7703.096212387085, 'total_duration': 8021.0365245342255, 'accumulated_submission_time': 7703.096212387085, 'accumulated_eval_time': 316.6596989631653, 'accumulated_logging_time': 0.4778110980987549}
I0229 07:28:00.173605 140089820641024 logging_writer.py:48] [22827] accumulated_eval_time=316.659699, accumulated_logging_time=0.477811, accumulated_submission_time=7703.096212, global_step=22827, preemption_count=0, score=7703.096212, test/accuracy=0.490200, test/loss=2.343901, test/num_examples=10000, total_duration=8021.036525, train/accuracy=0.671616, train/loss=1.315553, validation/accuracy=0.610900, validation/loss=1.609068, validation/num_examples=50000
I0229 07:28:24.940585 140089829033728 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.7171753644943237, loss=1.6670734882354736
I0229 07:28:58.470185 140089820641024 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.5752168893814087, loss=1.7022840976715088
I0229 07:29:31.953484 140089829033728 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.6275516748428345, loss=1.7220674753189087
I0229 07:30:05.483892 140089820641024 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.574068307876587, loss=1.6625721454620361
I0229 07:30:38.974216 140089829033728 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.858856201171875, loss=1.8669015169143677
I0229 07:31:12.488482 140089820641024 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.6196269989013672, loss=1.8520644903182983
I0229 07:31:45.931269 140089829033728 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.0177457332611084, loss=1.8558372259140015
I0229 07:32:19.410234 140089820641024 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.6690839529037476, loss=1.7182281017303467
I0229 07:32:52.897103 140089829033728 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.772262692451477, loss=1.6985018253326416
I0229 07:33:26.421656 140089820641024 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.7031899690628052, loss=1.6795973777770996
I0229 07:33:59.895596 140089829033728 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.7411342859268188, loss=1.8233280181884766
I0229 07:34:33.347077 140089820641024 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.8247880935668945, loss=1.6739850044250488
I0229 07:35:06.845152 140089829033728 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.6864930391311646, loss=1.6976892948150635
I0229 07:35:40.332397 140089820641024 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.898303747177124, loss=1.837117314338684
I0229 07:36:13.829946 140089829033728 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.311842203140259, loss=1.7301139831542969
I0229 07:36:30.387037 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:36:38.242223 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:36:50.607378 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:36:52.722927 140252611495744 submission_runner.py:411] Time since start: 8553.60s, 	Step: 24351, 	{'train/accuracy': 0.6676697731018066, 'train/loss': 1.3202012777328491, 'validation/accuracy': 0.6128999590873718, 'validation/loss': 1.6204156875610352, 'validation/num_examples': 50000, 'test/accuracy': 0.4942000210285187, 'test/loss': 2.327371120452881, 'test/num_examples': 10000, 'score': 8213.247112512589, 'total_duration': 8553.60300731659, 'accumulated_submission_time': 8213.247112512589, 'accumulated_eval_time': 338.99554920196533, 'accumulated_logging_time': 0.5040531158447266}
I0229 07:36:52.741027 140089820641024 logging_writer.py:48] [24351] accumulated_eval_time=338.995549, accumulated_logging_time=0.504053, accumulated_submission_time=8213.247113, global_step=24351, preemption_count=0, score=8213.247113, test/accuracy=0.494200, test/loss=2.327371, test/num_examples=10000, total_duration=8553.603007, train/accuracy=0.667670, train/loss=1.320201, validation/accuracy=0.612900, validation/loss=1.620416, validation/num_examples=50000
I0229 07:37:09.465411 140089829033728 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.6217269897460938, loss=1.8432226181030273
I0229 07:37:43.012533 140089820641024 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.630568504333496, loss=1.7607613801956177
I0229 07:38:16.468074 140089829033728 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.0702080726623535, loss=1.875966191291809
I0229 07:38:49.903255 140089820641024 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.814807653427124, loss=1.8195184469223022
I0229 07:39:23.413632 140089829033728 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.6508935689926147, loss=1.7881356477737427
I0229 07:39:56.916137 140089820641024 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.9155412912368774, loss=1.7835021018981934
I0229 07:40:30.464012 140089829033728 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.6596444845199585, loss=1.6403441429138184
I0229 07:41:03.973235 140089820641024 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.009643316268921, loss=1.7676794528961182
I0229 07:41:37.472671 140089829033728 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.8129534721374512, loss=1.8298137187957764
I0229 07:42:10.952446 140089820641024 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.4542346000671387, loss=1.6520476341247559
I0229 07:42:44.407204 140089829033728 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.6061170101165771, loss=1.7542521953582764
I0229 07:43:17.884864 140089820641024 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.4898183345794678, loss=1.7489405870437622
I0229 07:43:51.399328 140089829033728 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.7847692966461182, loss=1.824825644493103
I0229 07:44:24.921899 140089820641024 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.7846953868865967, loss=1.7928993701934814
I0229 07:44:58.433828 140089829033728 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.719299077987671, loss=1.7155836820602417
I0229 07:45:22.994184 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:45:30.835272 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:45:44.873257 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:45:47.002198 140252611495744 submission_runner.py:411] Time since start: 9087.88s, 	Step: 25875, 	{'train/accuracy': 0.71097731590271, 'train/loss': 1.1400072574615479, 'validation/accuracy': 0.6279399991035461, 'validation/loss': 1.5446945428848267, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.293175458908081, 'test/num_examples': 10000, 'score': 8723.437615156174, 'total_duration': 9087.882278203964, 'accumulated_submission_time': 8723.437615156174, 'accumulated_eval_time': 363.0035355091095, 'accumulated_logging_time': 0.53145432472229}
I0229 07:45:47.024846 140089870997248 logging_writer.py:48] [25875] accumulated_eval_time=363.003536, accumulated_logging_time=0.531454, accumulated_submission_time=8723.437615, global_step=25875, preemption_count=0, score=8723.437615, test/accuracy=0.500900, test/loss=2.293175, test/num_examples=10000, total_duration=9087.882278, train/accuracy=0.710977, train/loss=1.140007, validation/accuracy=0.627940, validation/loss=1.544695, validation/num_examples=50000
I0229 07:45:55.737400 140089879389952 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.622165322303772, loss=1.7652482986450195
I0229 07:46:29.231396 140089870997248 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.7807631492614746, loss=1.7221487760543823
I0229 07:47:02.723176 140089879389952 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.9305447340011597, loss=1.865871548652649
I0229 07:47:36.267099 140089870997248 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.7038042545318604, loss=1.8197566270828247
I0229 07:48:09.800069 140089879389952 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.917013168334961, loss=1.7724132537841797
I0229 07:48:43.297271 140089870997248 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.665788173675537, loss=1.8357360363006592
I0229 07:49:16.750716 140089879389952 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.8108028173446655, loss=1.7057700157165527
I0229 07:49:50.212337 140089870997248 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6130412817001343, loss=1.7046363353729248
I0229 07:50:23.695753 140089879389952 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8972938060760498, loss=1.6028261184692383
I0229 07:50:57.193535 140089870997248 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.054945707321167, loss=1.740508794784546
I0229 07:51:30.647873 140089879389952 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.5404449701309204, loss=1.821638584136963
I0229 07:52:04.110950 140089870997248 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.7437993288040161, loss=1.9085910320281982
I0229 07:52:37.597487 140089879389952 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.6555004119873047, loss=1.7253122329711914
I0229 07:53:11.161116 140089870997248 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.965436339378357, loss=1.820762038230896
I0229 07:53:44.872766 140089879389952 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.810817003250122, loss=1.7223751544952393
I0229 07:54:17.091538 140252611495744 spec.py:321] Evaluating on the training split.
I0229 07:54:25.102563 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 07:54:39.402089 140252611495744 spec.py:349] Evaluating on the test split.
I0229 07:54:41.540445 140252611495744 submission_runner.py:411] Time since start: 9622.42s, 	Step: 27398, 	{'train/accuracy': 0.697265625, 'train/loss': 1.1765410900115967, 'validation/accuracy': 0.623479962348938, 'validation/loss': 1.5636550188064575, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.3293328285217285, 'test/num_examples': 10000, 'score': 9233.441111087799, 'total_duration': 9622.420518398285, 'accumulated_submission_time': 9233.441111087799, 'accumulated_eval_time': 387.45239639282227, 'accumulated_logging_time': 0.5643725395202637}
I0229 07:54:41.563638 140089820641024 logging_writer.py:48] [27398] accumulated_eval_time=387.452396, accumulated_logging_time=0.564373, accumulated_submission_time=9233.441111, global_step=27398, preemption_count=0, score=9233.441111, test/accuracy=0.493300, test/loss=2.329333, test/num_examples=10000, total_duration=9622.420518, train/accuracy=0.697266, train/loss=1.176541, validation/accuracy=0.623480, validation/loss=1.563655, validation/num_examples=50000
I0229 07:54:42.574016 140089829033728 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.6504576206207275, loss=1.7278096675872803
I0229 07:55:16.005604 140089820641024 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.6504528522491455, loss=1.709445834159851
I0229 07:55:49.476337 140089829033728 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.8402138948440552, loss=1.8837976455688477
I0229 07:56:22.955722 140089820641024 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.6452295780181885, loss=1.8393840789794922
I0229 07:56:56.525856 140089829033728 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.6568009853363037, loss=1.7793495655059814
I0229 07:57:30.000934 140089820641024 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.104135513305664, loss=1.8353265523910522
I0229 07:58:03.448377 140089829033728 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.8112138509750366, loss=1.778584599494934
I0229 07:58:36.917057 140089820641024 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.160047769546509, loss=1.7723063230514526
I0229 07:59:10.442327 140089829033728 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.8540549278259277, loss=1.7313846349716187
I0229 07:59:43.957193 140089820641024 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.7649734020233154, loss=1.7607654333114624
I0229 08:00:17.437835 140089829033728 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.756532073020935, loss=1.7791788578033447
I0229 08:00:50.854864 140089820641024 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.8413336277008057, loss=1.809086561203003
I0229 08:01:24.337546 140089829033728 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.703687071800232, loss=1.5959023237228394
I0229 08:01:57.824742 140089820641024 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.9238262176513672, loss=1.7499293088912964
I0229 08:02:31.362796 140089829033728 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.5992399454116821, loss=1.7564303874969482
I0229 08:03:04.842731 140089820641024 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6435315608978271, loss=1.6771833896636963
I0229 08:03:11.627985 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:03:19.366298 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:03:33.393637 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:03:35.519230 140252611495744 submission_runner.py:411] Time since start: 10156.40s, 	Step: 28922, 	{'train/accuracy': 0.6786311864852905, 'train/loss': 1.2781400680541992, 'validation/accuracy': 0.6108599901199341, 'validation/loss': 1.6249542236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.4853000342845917, 'test/loss': 2.3837692737579346, 'test/num_examples': 10000, 'score': 9743.443282604218, 'total_duration': 10156.39930844307, 'accumulated_submission_time': 9743.443282604218, 'accumulated_eval_time': 411.34359431266785, 'accumulated_logging_time': 0.5972199440002441}
I0229 08:03:35.542075 140089820641024 logging_writer.py:48] [28922] accumulated_eval_time=411.343594, accumulated_logging_time=0.597220, accumulated_submission_time=9743.443283, global_step=28922, preemption_count=0, score=9743.443283, test/accuracy=0.485300, test/loss=2.383769, test/num_examples=10000, total_duration=10156.399308, train/accuracy=0.678631, train/loss=1.278140, validation/accuracy=0.610860, validation/loss=1.624954, validation/num_examples=50000
I0229 08:04:01.985453 140089829033728 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6736459732055664, loss=1.7267117500305176
I0229 08:04:35.485800 140089820641024 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.261178970336914, loss=1.874751329421997
I0229 08:05:08.931422 140089829033728 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.7620246410369873, loss=1.7486180067062378
I0229 08:05:42.429817 140089820641024 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.7365785837173462, loss=1.716655969619751
I0229 08:06:15.891610 140089829033728 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.7282721996307373, loss=1.6408132314682007
I0229 08:06:49.391527 140089820641024 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.8832142353057861, loss=1.737339735031128
I0229 08:07:22.876040 140089829033728 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.7925325632095337, loss=1.681655764579773
I0229 08:07:56.446627 140089820641024 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.000519275665283, loss=1.8490238189697266
I0229 08:08:29.939346 140089829033728 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.761061191558838, loss=1.7118664979934692
I0229 08:09:03.461225 140089820641024 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.7685023546218872, loss=1.7264649868011475
I0229 08:09:36.917417 140089829033728 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.749218463897705, loss=1.6148786544799805
I0229 08:10:10.369052 140089820641024 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.8815487623214722, loss=1.7490023374557495
I0229 08:10:43.862778 140089829033728 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.7785444259643555, loss=1.7643043994903564
I0229 08:11:17.398056 140089820641024 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.687990427017212, loss=1.6470015048980713
I0229 08:11:50.942958 140089829033728 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.8084781169891357, loss=1.7404990196228027
I0229 08:12:05.759529 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:12:13.728983 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:12:28.363043 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:12:30.538137 140252611495744 submission_runner.py:411] Time since start: 10691.42s, 	Step: 30446, 	{'train/accuracy': 0.6844507455825806, 'train/loss': 1.2520849704742432, 'validation/accuracy': 0.6162399649620056, 'validation/loss': 1.5863946676254272, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.291940689086914, 'test/num_examples': 10000, 'score': 10253.598109006882, 'total_duration': 10691.41821050644, 'accumulated_submission_time': 10253.598109006882, 'accumulated_eval_time': 436.1221706867218, 'accumulated_logging_time': 0.6296067237854004}
I0229 08:12:30.562655 140089837426432 logging_writer.py:48] [30446] accumulated_eval_time=436.122171, accumulated_logging_time=0.629607, accumulated_submission_time=10253.598109, global_step=30446, preemption_count=0, score=10253.598109, test/accuracy=0.490500, test/loss=2.291941, test/num_examples=10000, total_duration=10691.418211, train/accuracy=0.684451, train/loss=1.252085, validation/accuracy=0.616240, validation/loss=1.586395, validation/num_examples=50000
I0229 08:12:48.951088 140089845819136 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.765108346939087, loss=1.7745964527130127
I0229 08:13:22.397757 140089837426432 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.8701106309890747, loss=1.7078752517700195
I0229 08:13:55.907942 140089845819136 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.815110445022583, loss=1.6875101327896118
I0229 08:14:29.370385 140089837426432 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7732607126235962, loss=1.7497957944869995
I0229 08:15:02.823703 140089845819136 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.5790584087371826, loss=1.7130485773086548
I0229 08:15:36.348048 140089837426432 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.7156963348388672, loss=1.636251449584961
I0229 08:16:09.905874 140089845819136 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.7677724361419678, loss=1.7903965711593628
I0229 08:16:43.361428 140089837426432 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.7419534921646118, loss=1.5731807947158813
I0229 08:17:16.830718 140089845819136 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.7673794031143188, loss=1.7136231660842896
I0229 08:17:50.313396 140089837426432 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.7114315032958984, loss=1.6108802556991577
I0229 08:18:23.780860 140089845819136 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.6717548370361328, loss=1.7447582483291626
I0229 08:18:57.245359 140089837426432 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.8261054754257202, loss=1.6668506860733032
I0229 08:19:30.718162 140089845819136 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.6858136653900146, loss=1.7234748601913452
I0229 08:20:04.189263 140089837426432 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.643060564994812, loss=1.6117002964019775
I0229 08:20:37.741069 140089845819136 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.8352444171905518, loss=1.7618588209152222
I0229 08:21:00.578936 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:21:08.674286 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:21:22.024948 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:21:24.256630 140252611495744 submission_runner.py:411] Time since start: 11225.14s, 	Step: 31970, 	{'train/accuracy': 0.6871213316917419, 'train/loss': 1.239983081817627, 'validation/accuracy': 0.6253799796104431, 'validation/loss': 1.5454217195510864, 'validation/num_examples': 50000, 'test/accuracy': 0.5026000142097473, 'test/loss': 2.285898447036743, 'test/num_examples': 10000, 'score': 10763.550921201706, 'total_duration': 11225.136703014374, 'accumulated_submission_time': 10763.550921201706, 'accumulated_eval_time': 459.7998206615448, 'accumulated_logging_time': 0.6647074222564697}
I0229 08:21:24.279065 140089350944512 logging_writer.py:48] [31970] accumulated_eval_time=459.799821, accumulated_logging_time=0.664707, accumulated_submission_time=10763.550921, global_step=31970, preemption_count=0, score=10763.550921, test/accuracy=0.502600, test/loss=2.285898, test/num_examples=10000, total_duration=11225.136703, train/accuracy=0.687121, train/loss=1.239983, validation/accuracy=0.625380, validation/loss=1.545422, validation/num_examples=50000
I0229 08:21:34.654035 140089770305280 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.8790128231048584, loss=1.7236398458480835
I0229 08:22:08.116307 140089350944512 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.7497478723526, loss=1.6818289756774902
I0229 08:22:41.683822 140089770305280 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6620906591415405, loss=1.665672779083252
I0229 08:23:15.170114 140089350944512 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.769168734550476, loss=1.6925195455551147
I0229 08:23:48.669941 140089770305280 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6590083837509155, loss=1.769026756286621
I0229 08:24:22.210727 140089350944512 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.664186954498291, loss=1.664330005645752
I0229 08:24:55.717646 140089770305280 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.956566572189331, loss=1.7107547521591187
I0229 08:25:29.193410 140089350944512 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.7956719398498535, loss=1.6677230596542358
I0229 08:26:02.646562 140089770305280 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.8192800283432007, loss=1.7158679962158203
I0229 08:26:36.134014 140089350944512 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.9400100708007812, loss=1.7346651554107666
I0229 08:27:09.609673 140089770305280 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.7166118621826172, loss=1.644795536994934
I0229 08:27:43.073620 140089350944512 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.9623439311981201, loss=1.8190858364105225
I0229 08:28:16.560439 140089770305280 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.6269415616989136, loss=1.7361016273498535
I0229 08:28:50.013297 140089350944512 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.9235708713531494, loss=1.6880570650100708
I0229 08:29:23.507396 140089770305280 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.7284060716629028, loss=1.7267544269561768
I0229 08:29:54.405203 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:30:02.458817 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:30:16.260893 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:30:18.692328 140252611495744 submission_runner.py:411] Time since start: 11759.57s, 	Step: 33494, 	{'train/accuracy': 0.66796875, 'train/loss': 1.3174986839294434, 'validation/accuracy': 0.6146999597549438, 'validation/loss': 1.6063886880874634, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.363152027130127, 'test/num_examples': 10000, 'score': 11273.611344337463, 'total_duration': 11759.572400569916, 'accumulated_submission_time': 11273.611344337463, 'accumulated_eval_time': 484.0868966579437, 'accumulated_logging_time': 0.7003006935119629}
I0229 08:30:18.714589 140089770305280 logging_writer.py:48] [33494] accumulated_eval_time=484.086897, accumulated_logging_time=0.700301, accumulated_submission_time=11273.611344, global_step=33494, preemption_count=0, score=11273.611344, test/accuracy=0.486900, test/loss=2.363152, test/num_examples=10000, total_duration=11759.572401, train/accuracy=0.667969, train/loss=1.317499, validation/accuracy=0.614700, validation/loss=1.606389, validation/num_examples=50000
I0229 08:30:21.056078 140089845819136 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6507152318954468, loss=1.6798579692840576
I0229 08:30:54.503902 140089770305280 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.628204345703125, loss=1.7097499370574951
I0229 08:31:27.979664 140089845819136 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.674407958984375, loss=1.7413312196731567
I0229 08:32:01.435667 140089770305280 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.7445127964019775, loss=1.7190251350402832
I0229 08:32:34.912240 140089845819136 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.9392247200012207, loss=1.7764439582824707
I0229 08:33:08.415311 140089770305280 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.8451390266418457, loss=1.8107329607009888
I0229 08:33:41.904936 140089845819136 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.757856845855713, loss=1.589949369430542
I0229 08:34:15.351789 140089770305280 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.9164814949035645, loss=1.7044734954833984
I0229 08:34:48.828274 140089845819136 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.757598876953125, loss=1.6456775665283203
I0229 08:35:22.354836 140089770305280 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7075189352035522, loss=1.7849164009094238
I0229 08:35:55.864532 140089845819136 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.015446901321411, loss=1.783530592918396
I0229 08:36:29.389488 140089770305280 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.8330470323562622, loss=1.767961859703064
I0229 08:37:02.927818 140089845819136 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.0095348358154297, loss=1.7467759847640991
I0229 08:37:36.397077 140089770305280 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.8810923099517822, loss=1.6678249835968018
I0229 08:38:09.873114 140089845819136 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.8336071968078613, loss=1.712986946105957
I0229 08:38:43.346892 140089770305280 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.6736724376678467, loss=1.615981101989746
I0229 08:38:48.798243 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:38:56.713598 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:39:11.275838 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:39:13.423519 140252611495744 submission_runner.py:411] Time since start: 12294.30s, 	Step: 35018, 	{'train/accuracy': 0.7241310477256775, 'train/loss': 1.0600242614746094, 'validation/accuracy': 0.6274799704551697, 'validation/loss': 1.5452181100845337, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.2676334381103516, 'test/num_examples': 10000, 'score': 11783.628342866898, 'total_duration': 12294.303589820862, 'accumulated_submission_time': 11783.628342866898, 'accumulated_eval_time': 508.71211862564087, 'accumulated_logging_time': 0.7353689670562744}
I0229 08:39:13.444233 140089350944512 logging_writer.py:48] [35018] accumulated_eval_time=508.712119, accumulated_logging_time=0.735369, accumulated_submission_time=11783.628343, global_step=35018, preemption_count=0, score=11783.628343, test/accuracy=0.506400, test/loss=2.267633, test/num_examples=10000, total_duration=12294.303590, train/accuracy=0.724131, train/loss=1.060024, validation/accuracy=0.627480, validation/loss=1.545218, validation/num_examples=50000
I0229 08:39:41.209667 140089770305280 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.810219645500183, loss=1.8181445598602295
I0229 08:40:14.664243 140089350944512 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7993700504302979, loss=1.6965514421463013
I0229 08:40:48.145158 140089770305280 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.6028538942337036, loss=1.743799090385437
I0229 08:41:21.610698 140089350944512 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.4117257595062256, loss=1.7447675466537476
I0229 08:41:55.100182 140089770305280 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.8803319931030273, loss=1.6751668453216553
I0229 08:42:28.614233 140089350944512 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.8523170948028564, loss=1.6949169635772705
I0229 08:43:02.146433 140089770305280 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.724570870399475, loss=1.691776156425476
I0229 08:43:35.630046 140089350944512 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.827347993850708, loss=1.704851508140564
I0229 08:44:09.181163 140089770305280 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.6675734519958496, loss=1.6684201955795288
I0229 08:44:42.686043 140089350944512 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.7477682828903198, loss=1.6772940158843994
I0229 08:45:16.213875 140089770305280 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7132987976074219, loss=1.5932637453079224
I0229 08:45:49.738365 140089350944512 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.7168177366256714, loss=1.7380403280258179
I0229 08:46:23.191789 140089770305280 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.7944915294647217, loss=1.6608805656433105
I0229 08:46:56.701771 140089350944512 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.8593915700912476, loss=1.6871495246887207
I0229 08:47:30.179501 140089770305280 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.8318272829055786, loss=1.6922866106033325
I0229 08:47:43.658073 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:47:51.649357 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:48:06.661113 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:48:08.852352 140252611495744 submission_runner.py:411] Time since start: 12829.73s, 	Step: 36542, 	{'train/accuracy': 0.7159797549247742, 'train/loss': 1.0924153327941895, 'validation/accuracy': 0.6389200091362, 'validation/loss': 1.4935661554336548, 'validation/num_examples': 50000, 'test/accuracy': 0.5040000081062317, 'test/loss': 2.234344959259033, 'test/num_examples': 10000, 'score': 12293.778350830078, 'total_duration': 12829.732397556305, 'accumulated_submission_time': 12293.778350830078, 'accumulated_eval_time': 533.906332731247, 'accumulated_logging_time': 0.7664787769317627}
I0229 08:48:08.875174 140089870997248 logging_writer.py:48] [36542] accumulated_eval_time=533.906333, accumulated_logging_time=0.766479, accumulated_submission_time=12293.778351, global_step=36542, preemption_count=0, score=12293.778351, test/accuracy=0.504000, test/loss=2.234345, test/num_examples=10000, total_duration=12829.732398, train/accuracy=0.715980, train/loss=1.092415, validation/accuracy=0.638920, validation/loss=1.493566, validation/num_examples=50000
I0229 08:48:28.627836 140089879389952 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.0864810943603516, loss=1.720329999923706
I0229 08:49:02.128297 140089870997248 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.7922900915145874, loss=1.7276629209518433
I0229 08:49:35.653604 140089879389952 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.8727328777313232, loss=1.8203524351119995
I0229 08:50:09.133531 140089870997248 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.726555347442627, loss=1.6628987789154053
I0229 08:50:42.654731 140089879389952 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.7361716032028198, loss=1.6705029010772705
I0229 08:51:16.130075 140089870997248 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7526133060455322, loss=1.6279393434524536
I0229 08:51:49.632617 140089879389952 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.7876924276351929, loss=1.7420836687088013
I0229 08:52:23.088663 140089870997248 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.663736343383789, loss=1.749157428741455
I0229 08:52:56.548351 140089879389952 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.8277748823165894, loss=1.7281522750854492
I0229 08:53:30.027657 140089870997248 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.847334861755371, loss=1.7309017181396484
I0229 08:54:03.530828 140089879389952 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.677221417427063, loss=1.7466025352478027
I0229 08:54:36.991814 140089870997248 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.086164712905884, loss=1.6696579456329346
I0229 08:55:10.480073 140089879389952 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.6269115209579468, loss=1.6250897645950317
I0229 08:55:43.956755 140089870997248 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5913342237472534, loss=1.6883608102798462
I0229 08:56:17.497977 140089879389952 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.6045571565628052, loss=1.741689682006836
I0229 08:56:39.016603 140252611495744 spec.py:321] Evaluating on the training split.
I0229 08:56:46.082509 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 08:56:59.648369 140252611495744 spec.py:349] Evaluating on the test split.
I0229 08:57:01.759745 140252611495744 submission_runner.py:411] Time since start: 13362.64s, 	Step: 38066, 	{'train/accuracy': 0.693757951259613, 'train/loss': 1.1842937469482422, 'validation/accuracy': 0.6302199959754944, 'validation/loss': 1.5333516597747803, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.274075508117676, 'test/num_examples': 10000, 'score': 12803.850924253464, 'total_duration': 13362.639755010605, 'accumulated_submission_time': 12803.850924253464, 'accumulated_eval_time': 556.6493656635284, 'accumulated_logging_time': 0.8046107292175293}
I0229 08:57:01.779538 140089837426432 logging_writer.py:48] [38066] accumulated_eval_time=556.649366, accumulated_logging_time=0.804611, accumulated_submission_time=12803.850924, global_step=38066, preemption_count=0, score=12803.850924, test/accuracy=0.497700, test/loss=2.274076, test/num_examples=10000, total_duration=13362.639755, train/accuracy=0.693758, train/loss=1.184294, validation/accuracy=0.630220, validation/loss=1.533352, validation/num_examples=50000
I0229 08:57:13.470266 140089845819136 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8432559967041016, loss=1.6134259700775146
I0229 08:57:46.943295 140089837426432 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.7583385705947876, loss=1.691831350326538
I0229 08:58:20.491344 140089845819136 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.797254204750061, loss=1.6538175344467163
I0229 08:58:53.989105 140089837426432 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7670562267303467, loss=1.6806565523147583
I0229 08:59:27.509634 140089845819136 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.820888876914978, loss=1.654592752456665
I0229 09:00:00.993930 140089837426432 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.7446869611740112, loss=1.6415082216262817
I0229 09:00:34.487129 140089845819136 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.5992140769958496, loss=1.6076829433441162
I0229 09:01:08.008215 140089837426432 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.7674468755722046, loss=1.6237494945526123
I0229 09:01:41.468416 140089845819136 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8479878902435303, loss=1.6706278324127197
I0229 09:02:14.942054 140089837426432 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.9162895679473877, loss=1.6671912670135498
I0229 09:02:48.410942 140089845819136 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.6650199890136719, loss=1.6548677682876587
I0229 09:03:21.886161 140089837426432 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.7819912433624268, loss=1.6637094020843506
I0229 09:03:55.368319 140089845819136 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.8311668634414673, loss=1.670046091079712
I0229 09:04:28.859585 140089837426432 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.9639376401901245, loss=1.727263331413269
I0229 09:05:02.356555 140089845819136 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.9392887353897095, loss=1.6427092552185059
I0229 09:05:31.914335 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:05:38.880357 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:05:51.299019 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:05:53.474329 140252611495744 submission_runner.py:411] Time since start: 13894.35s, 	Step: 39590, 	{'train/accuracy': 0.6944355964660645, 'train/loss': 1.1813580989837646, 'validation/accuracy': 0.6322799921035767, 'validation/loss': 1.5052293539047241, 'validation/num_examples': 50000, 'test/accuracy': 0.5046000480651855, 'test/loss': 2.22383189201355, 'test/num_examples': 10000, 'score': 13313.921353816986, 'total_duration': 13894.354410886765, 'accumulated_submission_time': 13313.921353816986, 'accumulated_eval_time': 578.2093193531036, 'accumulated_logging_time': 0.8341331481933594}
I0229 09:05:53.494068 140089837426432 logging_writer.py:48] [39590] accumulated_eval_time=578.209319, accumulated_logging_time=0.834133, accumulated_submission_time=13313.921354, global_step=39590, preemption_count=0, score=13313.921354, test/accuracy=0.504600, test/loss=2.223832, test/num_examples=10000, total_duration=13894.354411, train/accuracy=0.694436, train/loss=1.181358, validation/accuracy=0.632280, validation/loss=1.505229, validation/num_examples=50000
I0229 09:05:57.176316 140089862604544 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7505671977996826, loss=1.667463779449463
I0229 09:06:30.621261 140089837426432 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.9613876342773438, loss=1.727225422859192
I0229 09:07:04.115813 140089862604544 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.80126953125, loss=1.7123448848724365
I0229 09:07:37.646041 140089837426432 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.9554295539855957, loss=1.8077309131622314
I0229 09:08:11.115490 140089862604544 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.781502604484558, loss=1.6761895418167114
I0229 09:08:44.566260 140089837426432 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.8864235877990723, loss=1.633375644683838
I0229 09:09:17.994565 140089862604544 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.9030343294143677, loss=1.7895699739456177
I0229 09:09:51.529056 140089837426432 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.9010162353515625, loss=1.6095634698867798
I0229 09:10:25.023679 140089862604544 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.2517879009246826, loss=1.8061648607254028
I0229 09:10:58.503253 140089837426432 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.7912826538085938, loss=1.8202615976333618
I0229 09:11:31.972966 140089862604544 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.703519582748413, loss=1.605389952659607
I0229 09:12:05.475426 140089837426432 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.8206520080566406, loss=1.7427611351013184
I0229 09:12:38.941809 140089862604544 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7602674961090088, loss=1.7376766204833984
I0229 09:13:12.452671 140089837426432 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.78915274143219, loss=1.6351511478424072
I0229 09:13:45.954744 140089862604544 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.9819397926330566, loss=1.5907330513000488
I0229 09:14:19.399106 140089837426432 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7803730964660645, loss=1.801184892654419
I0229 09:14:23.525457 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:14:30.323665 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:14:43.861969 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:14:45.991803 140252611495744 submission_runner.py:411] Time since start: 14426.87s, 	Step: 41114, 	{'train/accuracy': 0.6988998651504517, 'train/loss': 1.1760797500610352, 'validation/accuracy': 0.6380599737167358, 'validation/loss': 1.4915393590927124, 'validation/num_examples': 50000, 'test/accuracy': 0.5070000290870667, 'test/loss': 2.210167407989502, 'test/num_examples': 10000, 'score': 13823.889991521835, 'total_duration': 14426.871874332428, 'accumulated_submission_time': 13823.889991521835, 'accumulated_eval_time': 600.675609588623, 'accumulated_logging_time': 0.8630940914154053}
I0229 09:14:46.015546 140089854211840 logging_writer.py:48] [41114] accumulated_eval_time=600.675610, accumulated_logging_time=0.863094, accumulated_submission_time=13823.889992, global_step=41114, preemption_count=0, score=13823.889992, test/accuracy=0.507000, test/loss=2.210167, test/num_examples=10000, total_duration=14426.871874, train/accuracy=0.698900, train/loss=1.176080, validation/accuracy=0.638060, validation/loss=1.491539, validation/num_examples=50000
I0229 09:15:15.146324 140089870997248 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.8331007957458496, loss=1.779201626777649
I0229 09:15:48.609433 140089854211840 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.787678599357605, loss=1.6693580150604248
I0229 09:16:22.089413 140089870997248 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.6561223268508911, loss=1.504066824913025
I0229 09:16:55.535318 140089854211840 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.7776435613632202, loss=1.6016831398010254
I0229 09:17:29.005019 140089870997248 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.8661537170410156, loss=1.5930583477020264
I0229 09:18:02.431878 140089854211840 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.7838821411132812, loss=1.5921242237091064
I0229 09:18:35.898731 140089870997248 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.7138311862945557, loss=1.6277259588241577
I0229 09:19:09.396605 140089854211840 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.929451584815979, loss=1.730366587638855
I0229 09:19:42.960881 140089870997248 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9937539100646973, loss=1.6758320331573486
I0229 09:20:16.428858 140089854211840 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.7169963121414185, loss=1.5437467098236084
I0229 09:20:49.879197 140089870997248 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.9768006801605225, loss=1.5863778591156006
I0229 09:21:23.362900 140089854211840 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.8289731740951538, loss=1.66824209690094
I0229 09:21:56.851494 140089870997248 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.9772061109542847, loss=1.684948205947876
I0229 09:22:30.286108 140089854211840 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.9184671640396118, loss=1.6793020963668823
I0229 09:23:03.773402 140089870997248 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6791938543319702, loss=1.6096675395965576
I0229 09:23:16.246702 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:23:22.922637 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:23:36.893984 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:23:39.053622 140252611495744 submission_runner.py:411] Time since start: 14959.93s, 	Step: 42639, 	{'train/accuracy': 0.6919044852256775, 'train/loss': 1.2171635627746582, 'validation/accuracy': 0.632319986820221, 'validation/loss': 1.5132615566253662, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.2714481353759766, 'test/num_examples': 10000, 'score': 14334.058574199677, 'total_duration': 14959.933693647385, 'accumulated_submission_time': 14334.058574199677, 'accumulated_eval_time': 623.4824783802032, 'accumulated_logging_time': 0.8966398239135742}
I0229 09:23:39.076022 140089350944512 logging_writer.py:48] [42639] accumulated_eval_time=623.482478, accumulated_logging_time=0.896640, accumulated_submission_time=14334.058574, global_step=42639, preemption_count=0, score=14334.058574, test/accuracy=0.499600, test/loss=2.271448, test/num_examples=10000, total_duration=14959.933694, train/accuracy=0.691904, train/loss=1.217164, validation/accuracy=0.632320, validation/loss=1.513262, validation/num_examples=50000
I0229 09:23:59.795684 140089770305280 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.8950364589691162, loss=1.7489690780639648
I0229 09:24:33.263292 140089350944512 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.7969632148742676, loss=1.6132409572601318
I0229 09:25:06.776814 140089770305280 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.0907058715820312, loss=1.7047719955444336
I0229 09:25:40.324630 140089350944512 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.9421244859695435, loss=1.6977012157440186
I0229 09:26:13.832063 140089770305280 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.7918833494186401, loss=1.6021692752838135
I0229 09:26:47.278386 140089350944512 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.6037770509719849, loss=1.4897631406784058
I0229 09:27:20.731368 140089770305280 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.948644757270813, loss=1.7472233772277832
I0229 09:27:54.226483 140089350944512 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.870030164718628, loss=1.66780424118042
I0229 09:28:27.702472 140089770305280 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.844227910041809, loss=1.7541395425796509
I0229 09:29:01.177871 140089350944512 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.8258752822875977, loss=1.6474941968917847
I0229 09:29:34.652189 140089770305280 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.936417579650879, loss=1.68479585647583
I0229 09:30:08.136744 140089350944512 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.782052993774414, loss=1.6303037405014038
I0229 09:30:41.671769 140089770305280 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.9433709383010864, loss=1.7082014083862305
I0229 09:31:15.112151 140089350944512 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7687010765075684, loss=1.6190621852874756
I0229 09:31:48.597731 140089770305280 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.8719584941864014, loss=1.6116143465042114
I0229 09:32:09.126052 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:32:15.763584 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:32:29.184707 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:32:31.348937 140252611495744 submission_runner.py:411] Time since start: 15492.23s, 	Step: 44163, 	{'train/accuracy': 0.7145447731018066, 'train/loss': 1.0940722227096558, 'validation/accuracy': 0.633359968662262, 'validation/loss': 1.494249939918518, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.2277207374572754, 'test/num_examples': 10000, 'score': 14844.04565525055, 'total_duration': 15492.228991985321, 'accumulated_submission_time': 14844.04565525055, 'accumulated_eval_time': 645.7052969932556, 'accumulated_logging_time': 0.9284241199493408}
I0229 09:32:31.377915 140089854211840 logging_writer.py:48] [44163] accumulated_eval_time=645.705297, accumulated_logging_time=0.928424, accumulated_submission_time=14844.045655, global_step=44163, preemption_count=0, score=14844.045655, test/accuracy=0.505100, test/loss=2.227721, test/num_examples=10000, total_duration=15492.228992, train/accuracy=0.714545, train/loss=1.094072, validation/accuracy=0.633360, validation/loss=1.494250, validation/num_examples=50000
I0229 09:32:44.095514 140089862604544 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.9479209184646606, loss=1.7567931413650513
I0229 09:33:17.554801 140089854211840 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.0151944160461426, loss=1.7543699741363525
I0229 09:33:51.016456 140089862604544 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.722617268562317, loss=1.6563220024108887
I0229 09:34:24.492967 140089854211840 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.8283472061157227, loss=1.6030317544937134
I0229 09:34:57.940556 140089862604544 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8636478185653687, loss=1.5328699350357056
I0229 09:35:31.493103 140089854211840 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.7010382413864136, loss=1.6614108085632324
I0229 09:36:04.989179 140089862604544 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.823180913925171, loss=1.5864964723587036
I0229 09:36:38.500082 140089854211840 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.931941270828247, loss=1.615556240081787
I0229 09:37:12.006579 140089862604544 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.9190984964370728, loss=1.530896544456482
I0229 09:37:45.502530 140089854211840 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.9012385606765747, loss=1.511817455291748
I0229 09:38:19.013487 140089862604544 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.8336297273635864, loss=1.5818501710891724
I0229 09:38:52.461982 140089854211840 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8676620721817017, loss=1.6918258666992188
I0229 09:39:25.938487 140089862604544 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.712478518486023, loss=1.6127760410308838
I0229 09:39:59.416047 140089854211840 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7584236860275269, loss=1.6623972654342651
I0229 09:40:32.899761 140089862604544 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.8227403163909912, loss=1.7050564289093018
I0229 09:41:01.464775 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:41:08.095500 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:41:20.990421 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:41:23.228815 140252611495744 submission_runner.py:411] Time since start: 16024.11s, 	Step: 45687, 	{'train/accuracy': 0.7106783986091614, 'train/loss': 1.1211135387420654, 'validation/accuracy': 0.6406999826431274, 'validation/loss': 1.4792100191116333, 'validation/num_examples': 50000, 'test/accuracy': 0.5095000267028809, 'test/loss': 2.23911714553833, 'test/num_examples': 10000, 'score': 15354.069417715073, 'total_duration': 16024.108890295029, 'accumulated_submission_time': 15354.069417715073, 'accumulated_eval_time': 667.4693231582642, 'accumulated_logging_time': 0.9678726196289062}
I0229 09:41:23.253836 140089770305280 logging_writer.py:48] [45687] accumulated_eval_time=667.469323, accumulated_logging_time=0.967873, accumulated_submission_time=15354.069418, global_step=45687, preemption_count=0, score=15354.069418, test/accuracy=0.509500, test/loss=2.239117, test/num_examples=10000, total_duration=16024.108890, train/accuracy=0.710678, train/loss=1.121114, validation/accuracy=0.640700, validation/loss=1.479210, validation/num_examples=50000
I0229 09:41:27.931621 140089778697984 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.6909579038619995, loss=1.6111226081848145
I0229 09:42:01.404555 140089770305280 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.8290106058120728, loss=1.6110284328460693
I0229 09:42:34.865415 140089778697984 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7581534385681152, loss=1.6463485956192017
I0229 09:43:08.346223 140089770305280 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7001960277557373, loss=1.6923372745513916
I0229 09:43:41.841619 140089778697984 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.6946293115615845, loss=1.5865799188613892
I0229 09:44:15.414126 140089770305280 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7659095525741577, loss=1.5232493877410889
I0229 09:44:48.901223 140089778697984 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.8358311653137207, loss=1.6336753368377686
I0229 09:45:22.362927 140089770305280 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.9378516674041748, loss=1.687703013420105
I0229 09:45:55.860023 140089778697984 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.9523242712020874, loss=1.6517146825790405
I0229 09:46:29.382341 140089770305280 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.8890588283538818, loss=1.503517508506775
I0229 09:47:02.855266 140089778697984 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7590330839157104, loss=1.591097116470337
I0229 09:47:36.290787 140089770305280 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.9886674880981445, loss=1.6513490676879883
I0229 09:48:09.762283 140089778697984 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8549824953079224, loss=1.6044073104858398
I0229 09:48:43.295703 140089770305280 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.9281928539276123, loss=1.5432268381118774
I0229 09:49:16.786835 140089778697984 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.8313179016113281, loss=1.6762751340866089
I0229 09:49:50.270850 140089770305280 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.9278216361999512, loss=1.7341523170471191
I0229 09:49:53.375156 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:49:59.867219 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:50:11.520681 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:50:13.844130 140252611495744 submission_runner.py:411] Time since start: 16554.72s, 	Step: 47211, 	{'train/accuracy': 0.6990792155265808, 'train/loss': 1.1850415468215942, 'validation/accuracy': 0.6307199597358704, 'validation/loss': 1.5144232511520386, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.2181320190429688, 'test/num_examples': 10000, 'score': 15864.12791633606, 'total_duration': 16554.72419142723, 'accumulated_submission_time': 15864.12791633606, 'accumulated_eval_time': 687.938235282898, 'accumulated_logging_time': 1.0020246505737305}
I0229 09:50:13.872932 140089870997248 logging_writer.py:48] [47211] accumulated_eval_time=687.938235, accumulated_logging_time=1.002025, accumulated_submission_time=15864.127916, global_step=47211, preemption_count=0, score=15864.127916, test/accuracy=0.514300, test/loss=2.218132, test/num_examples=10000, total_duration=16554.724191, train/accuracy=0.699079, train/loss=1.185042, validation/accuracy=0.630720, validation/loss=1.514423, validation/num_examples=50000
I0229 09:50:44.013094 140089879389952 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.696933388710022, loss=1.5566362142562866
I0229 09:51:17.493305 140089870997248 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.8450672626495361, loss=1.7011827230453491
I0229 09:51:50.956990 140089879389952 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.9834082126617432, loss=1.6890983581542969
I0229 09:52:24.487108 140089870997248 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.928752064704895, loss=1.6885302066802979
I0229 09:52:58.014776 140089879389952 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.8505781888961792, loss=1.6560568809509277
I0229 09:53:31.527493 140089870997248 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.8874382972717285, loss=1.6801952123641968
I0229 09:54:05.064638 140089879389952 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.8865805864334106, loss=1.6061540842056274
I0229 09:54:38.534701 140089870997248 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.9639935493469238, loss=1.6040319204330444
I0229 09:55:11.990505 140089879389952 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.7384668588638306, loss=1.5921785831451416
I0229 09:55:45.521678 140089870997248 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.0555498600006104, loss=1.6915326118469238
I0229 09:56:19.048272 140089879389952 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.6600062847137451, loss=1.6036293506622314
I0229 09:56:52.594823 140089870997248 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.9639567136764526, loss=1.6273983716964722
I0229 09:57:26.099021 140089879389952 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.9610836505889893, loss=1.5881304740905762
I0229 09:57:59.608280 140089870997248 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.6881811618804932, loss=1.61411714553833
I0229 09:58:33.093219 140089879389952 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.8900469541549683, loss=1.6261495351791382
I0229 09:58:43.949205 140252611495744 spec.py:321] Evaluating on the training split.
I0229 09:58:50.309803 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 09:58:59.772118 140252611495744 spec.py:349] Evaluating on the test split.
I0229 09:59:02.099243 140252611495744 submission_runner.py:411] Time since start: 17082.98s, 	Step: 48734, 	{'train/accuracy': 0.71097731590271, 'train/loss': 1.1214135885238647, 'validation/accuracy': 0.6475399732589722, 'validation/loss': 1.454314947128296, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.1623549461364746, 'test/num_examples': 10000, 'score': 16374.140513420105, 'total_duration': 17082.979299545288, 'accumulated_submission_time': 16374.140513420105, 'accumulated_eval_time': 706.0882074832916, 'accumulated_logging_time': 1.0412023067474365}
I0229 09:59:02.127733 140089778697984 logging_writer.py:48] [48734] accumulated_eval_time=706.088207, accumulated_logging_time=1.041202, accumulated_submission_time=16374.140513, global_step=48734, preemption_count=0, score=16374.140513, test/accuracy=0.517300, test/loss=2.162355, test/num_examples=10000, total_duration=17082.979300, train/accuracy=0.710977, train/loss=1.121414, validation/accuracy=0.647540, validation/loss=1.454315, validation/num_examples=50000
I0229 09:59:24.526978 140089837426432 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.7530595064163208, loss=1.5406132936477661
I0229 09:59:58.042195 140089778697984 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.6498875617980957, loss=1.5857120752334595
I0229 10:00:31.540764 140089837426432 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.727608323097229, loss=1.6376655101776123
I0229 10:01:05.114917 140089778697984 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.9515817165374756, loss=1.65920889377594
I0229 10:01:38.608608 140089837426432 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.9122259616851807, loss=1.7200024127960205
I0229 10:02:12.246442 140089778697984 logging_writer.py:48] [49300] global_step=49300, grad_norm=2.001638650894165, loss=1.6420835256576538
I0229 10:02:45.715069 140089837426432 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.9515020847320557, loss=1.5428249835968018
I0229 10:03:19.217375 140089778697984 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.768192172050476, loss=1.5954676866531372
I0229 10:03:52.728054 140089837426432 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.9616506099700928, loss=1.5917187929153442
I0229 10:04:26.258389 140089778697984 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.7057058811187744, loss=1.5208290815353394
I0229 10:04:59.766792 140089837426432 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8105523586273193, loss=1.6909047365188599
I0229 10:05:33.297656 140089778697984 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.065439224243164, loss=1.6366429328918457
I0229 10:06:06.790845 140089837426432 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.8654251098632812, loss=1.629927635192871
I0229 10:06:40.322014 140089778697984 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.792636752128601, loss=1.524092674255371
I0229 10:07:13.817798 140089837426432 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.242098331451416, loss=1.6085578203201294
I0229 10:07:32.394756 140252611495744 spec.py:321] Evaluating on the training split.
I0229 10:07:38.773997 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 10:07:49.224318 140252611495744 spec.py:349] Evaluating on the test split.
I0229 10:07:51.772319 140252611495744 submission_runner.py:411] Time since start: 17612.65s, 	Step: 50257, 	{'train/accuracy': 0.7004743218421936, 'train/loss': 1.1613342761993408, 'validation/accuracy': 0.6417399644851685, 'validation/loss': 1.4614994525909424, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.2078135013580322, 'test/num_examples': 10000, 'score': 16884.34206557274, 'total_duration': 17612.652390241623, 'accumulated_submission_time': 16884.34206557274, 'accumulated_eval_time': 725.4657227993011, 'accumulated_logging_time': 1.08018159866333}
I0229 10:07:51.794893 140089862604544 logging_writer.py:48] [50257] accumulated_eval_time=725.465723, accumulated_logging_time=1.080182, accumulated_submission_time=16884.342066, global_step=50257, preemption_count=0, score=16884.342066, test/accuracy=0.513700, test/loss=2.207814, test/num_examples=10000, total_duration=17612.652390, train/accuracy=0.700474, train/loss=1.161334, validation/accuracy=0.641740, validation/loss=1.461499, validation/num_examples=50000
I0229 10:08:06.522333 140089870997248 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.6348013877868652, loss=1.5882084369659424
I0229 10:08:40.052619 140089862604544 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.8671283721923828, loss=1.5183578729629517
I0229 10:09:13.534939 140089870997248 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.758011817932129, loss=1.5614110231399536
I0229 10:09:47.016827 140089862604544 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.882090449333191, loss=1.5727717876434326
I0229 10:10:20.466095 140089870997248 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.7228397130966187, loss=1.5542247295379639
I0229 10:10:53.914660 140089862604544 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.9634002447128296, loss=1.687188982963562
I0229 10:11:27.411291 140089870997248 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.76753568649292, loss=1.6049444675445557
I0229 10:12:00.939684 140089862604544 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8655645847320557, loss=1.6334422826766968
I0229 10:12:34.482923 140089870997248 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7952927350997925, loss=1.5546000003814697
I0229 10:13:07.961532 140089862604544 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7504609823226929, loss=1.5270437002182007
I0229 10:13:41.423527 140089870997248 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9735333919525146, loss=1.5840154886245728
I0229 10:14:14.888566 140089862604544 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.775697946548462, loss=1.5498417615890503
I0229 10:14:48.456516 140089870997248 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.7382011413574219, loss=1.6726651191711426
I0229 10:15:21.927738 140089862604544 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.017582654953003, loss=1.6099567413330078
I0229 10:15:55.381055 140089870997248 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.196377992630005, loss=1.5054471492767334
I0229 10:16:21.905220 140252611495744 spec.py:321] Evaluating on the training split.
I0229 10:16:28.151527 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 10:16:38.437343 140252611495744 spec.py:349] Evaluating on the test split.
I0229 10:16:40.697358 140252611495744 submission_runner.py:411] Time since start: 18141.58s, 	Step: 51781, 	{'train/accuracy': 0.7308274507522583, 'train/loss': 1.039522409439087, 'validation/accuracy': 0.6245399713516235, 'validation/loss': 1.5539544820785522, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.28792405128479, 'test/num_examples': 10000, 'score': 17394.389335393906, 'total_duration': 18141.577426195145, 'accumulated_submission_time': 17394.389335393906, 'accumulated_eval_time': 744.2578091621399, 'accumulated_logging_time': 1.1118073463439941}
I0229 10:16:40.729677 140089778697984 logging_writer.py:48] [51781] accumulated_eval_time=744.257809, accumulated_logging_time=1.111807, accumulated_submission_time=17394.389335, global_step=51781, preemption_count=0, score=17394.389335, test/accuracy=0.503500, test/loss=2.287924, test/num_examples=10000, total_duration=18141.577426, train/accuracy=0.730827, train/loss=1.039522, validation/accuracy=0.624540, validation/loss=1.553954, validation/num_examples=50000
I0229 10:16:47.439243 140089837426432 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8045002222061157, loss=1.6696174144744873
I0229 10:17:20.916781 140089778697984 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.8587965965270996, loss=1.5165132284164429
I0229 10:17:54.400442 140089837426432 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.7261145114898682, loss=1.5404348373413086
I0229 10:18:27.836298 140089778697984 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8552896976470947, loss=1.5448650121688843
I0229 10:19:01.310881 140089837426432 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8687533140182495, loss=1.6292604207992554
I0229 10:19:34.765965 140089778697984 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.8390148878097534, loss=1.5224180221557617
I0229 10:20:08.219674 140089837426432 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.9251291751861572, loss=1.6198347806930542
I0229 10:20:41.748475 140089778697984 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8642851114273071, loss=1.588212251663208
I0229 10:21:15.227305 140089837426432 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9979569911956787, loss=1.6502882242202759
I0229 10:21:48.707470 140089778697984 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8651219606399536, loss=1.563565969467163
I0229 10:22:22.178319 140089837426432 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8782322406768799, loss=1.6492598056793213
I0229 10:22:55.629025 140089778697984 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8842041492462158, loss=1.670078158378601
I0229 10:23:29.114443 140089837426432 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.0358612537384033, loss=1.5844465494155884
I0229 10:24:02.600004 140089778697984 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.8028638362884521, loss=1.4943044185638428
I0229 10:24:36.058928 140089837426432 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.9400612115859985, loss=1.64568030834198
I0229 10:25:09.508743 140089778697984 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.875195026397705, loss=1.6515475511550903
I0229 10:25:10.985055 140252611495744 spec.py:321] Evaluating on the training split.
I0229 10:25:17.134545 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 10:25:26.711100 140252611495744 spec.py:349] Evaluating on the test split.
I0229 10:25:28.990116 140252611495744 submission_runner.py:411] Time since start: 18669.87s, 	Step: 53306, 	{'train/accuracy': 0.7253667116165161, 'train/loss': 1.0528578758239746, 'validation/accuracy': 0.6495199799537659, 'validation/loss': 1.4327813386917114, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.1753416061401367, 'test/num_examples': 10000, 'score': 17904.58046245575, 'total_duration': 18669.87014555931, 'accumulated_submission_time': 17904.58046245575, 'accumulated_eval_time': 762.2627856731415, 'accumulated_logging_time': 1.1545979976654053}
I0229 10:25:29.019364 140089862604544 logging_writer.py:48] [53306] accumulated_eval_time=762.262786, accumulated_logging_time=1.154598, accumulated_submission_time=17904.580462, global_step=53306, preemption_count=0, score=17904.580462, test/accuracy=0.510900, test/loss=2.175342, test/num_examples=10000, total_duration=18669.870146, train/accuracy=0.725367, train/loss=1.052858, validation/accuracy=0.649520, validation/loss=1.432781, validation/num_examples=50000
I0229 10:26:00.820536 140089870997248 logging_writer.py:48] [53400] global_step=53400, grad_norm=2.082354784011841, loss=1.5205254554748535
I0229 10:26:34.350455 140089862604544 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.775296926498413, loss=1.634777545928955
I0229 10:27:07.819354 140089870997248 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.7867182493209839, loss=1.6011643409729004
I0229 10:27:41.324996 140089862604544 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.95720374584198, loss=1.731784462928772
I0229 10:28:14.756158 140089870997248 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8778384923934937, loss=1.5410982370376587
I0229 10:28:48.222855 140089862604544 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.7361230850219727, loss=1.4710326194763184
I0229 10:29:21.724364 140089870997248 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.7930824756622314, loss=1.6118017435073853
I0229 10:29:55.217060 140089862604544 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.9433362483978271, loss=1.5098679065704346
I0229 10:30:28.676452 140089870997248 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.0796010494232178, loss=1.6437644958496094
I0229 10:31:02.119638 140089862604544 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7792669534683228, loss=1.5314667224884033
I0229 10:31:35.598213 140089870997248 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.7512835264205933, loss=1.483540415763855
I0229 10:32:09.056991 140089862604544 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.969373345375061, loss=1.5597854852676392
I0229 10:32:42.679057 140089870997248 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.9617186784744263, loss=1.5943474769592285
I0229 10:33:16.165172 140089862604544 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.1643710136413574, loss=1.5828756093978882
I0229 10:33:49.618534 140089870997248 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8139973878860474, loss=1.546663761138916
I0229 10:33:59.093223 140252611495744 spec.py:321] Evaluating on the training split.
I0229 10:34:05.423286 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 10:34:14.195229 140252611495744 spec.py:349] Evaluating on the test split.
I0229 10:34:16.428804 140252611495744 submission_runner.py:411] Time since start: 19197.31s, 	Step: 54830, 	{'train/accuracy': 0.7131297588348389, 'train/loss': 1.118245244026184, 'validation/accuracy': 0.6433199644088745, 'validation/loss': 1.4688860177993774, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.2052316665649414, 'test/num_examples': 10000, 'score': 18414.588896036148, 'total_duration': 19197.308834314346, 'accumulated_submission_time': 18414.588896036148, 'accumulated_eval_time': 779.59827876091, 'accumulated_logging_time': 1.1950109004974365}
I0229 10:34:16.456900 140089770305280 logging_writer.py:48] [54830] accumulated_eval_time=779.598279, accumulated_logging_time=1.195011, accumulated_submission_time=18414.588896, global_step=54830, preemption_count=0, score=18414.588896, test/accuracy=0.520900, test/loss=2.205232, test/num_examples=10000, total_duration=19197.308834, train/accuracy=0.713130, train/loss=1.118245, validation/accuracy=0.643320, validation/loss=1.468886, validation/num_examples=50000
I0229 10:34:40.232937 140089778697984 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.0028836727142334, loss=1.6466251611709595
I0229 10:35:13.711314 140089770305280 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.84079110622406, loss=1.6160695552825928
I0229 10:35:47.180635 140089778697984 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.7930738925933838, loss=1.5595037937164307
I0229 10:36:20.619362 140089770305280 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.0484492778778076, loss=1.62051522731781
I0229 10:36:54.107100 140089778697984 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.755666732788086, loss=1.5978295803070068
I0229 10:37:27.587203 140089770305280 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.9208614826202393, loss=1.5144966840744019
I0229 10:38:01.021705 140089778697984 logging_writer.py:48] [55500] global_step=55500, grad_norm=2.0286662578582764, loss=1.6599410772323608
I0229 10:38:34.483099 140089770305280 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.8634287118911743, loss=1.5947843790054321
I0229 10:39:08.057026 140089778697984 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.8652180433273315, loss=1.6366854906082153
I0229 10:39:41.525951 140089770305280 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.9206088781356812, loss=1.615535020828247
I0229 10:40:15.064112 140089778697984 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.7934539318084717, loss=1.474867343902588
I0229 10:40:48.540706 140089770305280 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7768102884292603, loss=1.4551753997802734
I0229 10:41:21.982474 140089778697984 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8770211935043335, loss=1.6106321811676025
I0229 10:41:55.436053 140089770305280 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8082895278930664, loss=1.6615997552871704
I0229 10:42:28.930334 140089778697984 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8269532918930054, loss=1.6840567588806152
I0229 10:42:46.440967 140252611495744 spec.py:321] Evaluating on the training split.
I0229 10:42:52.564404 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 10:43:02.725303 140252611495744 spec.py:349] Evaluating on the test split.
I0229 10:43:05.007606 140252611495744 submission_runner.py:411] Time since start: 19725.89s, 	Step: 56354, 	{'train/accuracy': 0.6937978267669678, 'train/loss': 1.2075111865997314, 'validation/accuracy': 0.6305800080299377, 'validation/loss': 1.5379019975662231, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.3033556938171387, 'test/num_examples': 10000, 'score': 18924.505990982056, 'total_duration': 19725.887673854828, 'accumulated_submission_time': 18924.505990982056, 'accumulated_eval_time': 798.1648647785187, 'accumulated_logging_time': 1.2350389957427979}
I0229 10:43:05.033770 140089552271104 logging_writer.py:48] [56354] accumulated_eval_time=798.164865, accumulated_logging_time=1.235039, accumulated_submission_time=18924.505991, global_step=56354, preemption_count=0, score=18924.505991, test/accuracy=0.498900, test/loss=2.303356, test/num_examples=10000, total_duration=19725.887674, train/accuracy=0.693798, train/loss=1.207511, validation/accuracy=0.630580, validation/loss=1.537902, validation/num_examples=50000
I0229 10:43:20.750500 140089770305280 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.819481372833252, loss=1.4889103174209595
I0229 10:43:54.220014 140089552271104 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.1110618114471436, loss=1.5254900455474854
I0229 10:44:27.669156 140089770305280 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9484338760375977, loss=1.6351243257522583
I0229 10:45:01.208436 140089552271104 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.8433390855789185, loss=1.5275307893753052
I0229 10:45:34.641915 140089770305280 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9370222091674805, loss=1.621743083000183
I0229 10:46:08.130194 140089552271104 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8750518560409546, loss=1.584489345550537
I0229 10:46:41.640848 140089770305280 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9056202173233032, loss=1.607421875
I0229 10:47:15.071853 140089552271104 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.9354361295700073, loss=1.5328670740127563
I0229 10:47:48.520661 140089770305280 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.9783313274383545, loss=1.6393239498138428
I0229 10:48:21.972146 140089552271104 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.8281724452972412, loss=1.5492181777954102
I0229 10:48:55.440009 140089770305280 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.8829494714736938, loss=1.5140469074249268
I0229 10:49:28.865479 140089552271104 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.7238519191741943, loss=1.611932396888733
I0229 10:50:02.353515 140089770305280 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.8522204160690308, loss=1.6337497234344482
I0229 10:50:35.794229 140089552271104 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.886790156364441, loss=1.5976200103759766
I0229 10:51:09.314939 140089770305280 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.9545447826385498, loss=1.6317541599273682
I0229 10:51:35.172278 140252611495744 spec.py:321] Evaluating on the training split.
I0229 10:51:41.224720 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 10:51:49.626084 140252611495744 spec.py:349] Evaluating on the test split.
I0229 10:51:51.873304 140252611495744 submission_runner.py:411] Time since start: 20252.75s, 	Step: 57879, 	{'train/accuracy': 0.7224768400192261, 'train/loss': 1.0849560499191284, 'validation/accuracy': 0.6535800099372864, 'validation/loss': 1.4212536811828613, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.125169515609741, 'test/num_examples': 10000, 'score': 19434.580425977707, 'total_duration': 20252.75336956978, 'accumulated_submission_time': 19434.580425977707, 'accumulated_eval_time': 814.8658380508423, 'accumulated_logging_time': 1.271176815032959}
I0229 10:51:51.902873 140089862604544 logging_writer.py:48] [57879] accumulated_eval_time=814.865838, accumulated_logging_time=1.271177, accumulated_submission_time=19434.580426, global_step=57879, preemption_count=0, score=19434.580426, test/accuracy=0.526600, test/loss=2.125170, test/num_examples=10000, total_duration=20252.753370, train/accuracy=0.722477, train/loss=1.084956, validation/accuracy=0.653580, validation/loss=1.421254, validation/num_examples=50000
I0229 10:51:59.271932 140089870997248 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.822481393814087, loss=1.4721375703811646
I0229 10:52:32.757157 140089862604544 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.8745708465576172, loss=1.3914024829864502
I0229 10:53:06.193495 140089870997248 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.431448459625244, loss=1.5397552251815796
I0229 10:53:39.662222 140089862604544 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.0437638759613037, loss=1.5834920406341553
I0229 10:54:13.132665 140089870997248 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.9820259809494019, loss=1.5170787572860718
I0229 10:54:46.567271 140089862604544 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.8756821155548096, loss=1.557418942451477
I0229 10:55:20.027979 140089870997248 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.448732852935791, loss=1.6471788883209229
I0229 10:55:53.497376 140089862604544 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9459359645843506, loss=1.5895894765853882
I0229 10:56:26.945903 140089870997248 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.9453275203704834, loss=1.6244210004806519
I0229 10:57:00.515157 140089862604544 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8720369338989258, loss=1.4833074808120728
I0229 10:57:33.962096 140089870997248 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8944106101989746, loss=1.590437412261963
I0229 10:58:07.512242 140089862604544 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.8818305730819702, loss=1.5060009956359863
I0229 10:58:40.982964 140089870997248 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.0090837478637695, loss=1.670631766319275
I0229 10:59:14.492109 140089862604544 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9442284107208252, loss=1.6967343091964722
I0229 10:59:47.948267 140089870997248 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.9047662019729614, loss=1.507071614265442
I0229 11:00:21.395137 140089862604544 logging_writer.py:48] [59400] global_step=59400, grad_norm=2.1178150177001953, loss=1.5436044931411743
I0229 11:00:22.193863 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:00:28.314785 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:00:36.922882 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:00:39.203599 140252611495744 submission_runner.py:411] Time since start: 20780.08s, 	Step: 59404, 	{'train/accuracy': 0.7204440236091614, 'train/loss': 1.080566167831421, 'validation/accuracy': 0.6579599976539612, 'validation/loss': 1.4034535884857178, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.1135621070861816, 'test/num_examples': 10000, 'score': 19944.806575775146, 'total_duration': 20780.083662986755, 'accumulated_submission_time': 19944.806575775146, 'accumulated_eval_time': 831.8755283355713, 'accumulated_logging_time': 1.3114573955535889}
I0229 11:00:39.234005 140089770305280 logging_writer.py:48] [59404] accumulated_eval_time=831.875528, accumulated_logging_time=1.311457, accumulated_submission_time=19944.806576, global_step=59404, preemption_count=0, score=19944.806576, test/accuracy=0.529700, test/loss=2.113562, test/num_examples=10000, total_duration=20780.083663, train/accuracy=0.720444, train/loss=1.080566, validation/accuracy=0.657960, validation/loss=1.403454, validation/num_examples=50000
I0229 11:01:11.690671 140089778697984 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.8114829063415527, loss=1.4817004203796387
I0229 11:01:45.164505 140089770305280 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.812001347541809, loss=1.6201014518737793
I0229 11:02:18.628654 140089778697984 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9217182397842407, loss=1.636725902557373
I0229 11:02:52.080680 140089770305280 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.9585829973220825, loss=1.5074669122695923
I0229 11:03:25.714011 140089778697984 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.951784610748291, loss=1.5128732919692993
I0229 11:03:59.208062 140089770305280 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.11789608001709, loss=1.5742065906524658
I0229 11:04:32.756827 140089778697984 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.9644113779067993, loss=1.573470115661621
I0229 11:05:06.295544 140089770305280 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.9094339609146118, loss=1.4513942003250122
I0229 11:05:39.787280 140089778697984 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.1833391189575195, loss=1.651554822921753
I0229 11:06:13.285560 140089770305280 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.034473180770874, loss=1.47770357131958
I0229 11:06:46.806507 140089778697984 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.9177206754684448, loss=1.4708762168884277
I0229 11:07:20.253041 140089770305280 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.032899856567383, loss=1.6683762073516846
I0229 11:07:53.717585 140089778697984 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.94417142868042, loss=1.5453673601150513
I0229 11:08:27.237467 140089770305280 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.103787660598755, loss=1.6519510746002197
I0229 11:09:00.719997 140089778697984 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8524417877197266, loss=1.5478832721710205
I0229 11:09:09.303657 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:09:15.414551 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:09:23.942390 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:09:26.159063 140252611495744 submission_runner.py:411] Time since start: 21307.04s, 	Step: 60927, 	{'train/accuracy': 0.750398576259613, 'train/loss': 0.9552063941955566, 'validation/accuracy': 0.6536399722099304, 'validation/loss': 1.428490400314331, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.1520943641662598, 'test/num_examples': 10000, 'score': 20454.81146836281, 'total_duration': 21307.039116859436, 'accumulated_submission_time': 20454.81146836281, 'accumulated_eval_time': 848.7308654785156, 'accumulated_logging_time': 1.3519139289855957}
I0229 11:09:26.189767 140089862604544 logging_writer.py:48] [60927] accumulated_eval_time=848.730865, accumulated_logging_time=1.351914, accumulated_submission_time=20454.811468, global_step=60927, preemption_count=0, score=20454.811468, test/accuracy=0.530400, test/loss=2.152094, test/num_examples=10000, total_duration=21307.039117, train/accuracy=0.750399, train/loss=0.955206, validation/accuracy=0.653640, validation/loss=1.428490, validation/num_examples=50000
I0229 11:09:50.979195 140089870997248 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.0859439373016357, loss=1.5776994228363037
I0229 11:10:24.420334 140089862604544 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.0935556888580322, loss=1.4934091567993164
I0229 11:10:57.892590 140089870997248 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.081150770187378, loss=1.5992004871368408
I0229 11:11:31.341110 140089862604544 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.783974051475525, loss=1.5316710472106934
I0229 11:12:04.798190 140089870997248 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.868507981300354, loss=1.591848611831665
I0229 11:12:38.281653 140089862604544 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8560274839401245, loss=1.641011357307434
I0229 11:13:11.761465 140089870997248 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.8785232305526733, loss=1.5946359634399414
I0229 11:13:45.305944 140089862604544 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9012751579284668, loss=1.6850472688674927
I0229 11:14:18.767011 140089870997248 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9285798072814941, loss=1.4436808824539185
I0229 11:14:52.194055 140089862604544 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.8497949838638306, loss=1.4836704730987549
I0229 11:15:25.733200 140089870997248 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9567184448242188, loss=1.5384610891342163
I0229 11:15:59.176828 140089862604544 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.0111286640167236, loss=1.464607834815979
I0229 11:16:32.650344 140089870997248 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.8767679929733276, loss=1.469343900680542
I0229 11:17:06.144840 140089862604544 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.9212875366210938, loss=1.5095058679580688
I0229 11:17:39.596317 140089870997248 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9484847784042358, loss=1.5549960136413574
I0229 11:17:56.446731 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:18:02.687978 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:18:11.045527 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:18:13.326950 140252611495744 submission_runner.py:411] Time since start: 21834.21s, 	Step: 62452, 	{'train/accuracy': 0.7303292155265808, 'train/loss': 1.0258498191833496, 'validation/accuracy': 0.6525799632072449, 'validation/loss': 1.4282927513122559, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.1652936935424805, 'test/num_examples': 10000, 'score': 20965.004019737244, 'total_duration': 21834.207016468048, 'accumulated_submission_time': 20965.004019737244, 'accumulated_eval_time': 865.6110301017761, 'accumulated_logging_time': 1.3921701908111572}
I0229 11:18:13.358110 140089837426432 logging_writer.py:48] [62452] accumulated_eval_time=865.611030, accumulated_logging_time=1.392170, accumulated_submission_time=20965.004020, global_step=62452, preemption_count=0, score=20965.004020, test/accuracy=0.521500, test/loss=2.165294, test/num_examples=10000, total_duration=21834.207016, train/accuracy=0.730329, train/loss=1.025850, validation/accuracy=0.652580, validation/loss=1.428293, validation/num_examples=50000
I0229 11:18:29.743620 140089845819136 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.1825809478759766, loss=1.7046847343444824
I0229 11:19:03.247416 140089837426432 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.069657325744629, loss=1.6043732166290283
I0229 11:19:36.697064 140089845819136 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.830562710762024, loss=1.5138057470321655
I0229 11:20:10.172095 140089837426432 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.949544072151184, loss=1.568725824356079
I0229 11:20:43.612071 140089845819136 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.9919828176498413, loss=1.489147663116455
I0229 11:21:17.174963 140089837426432 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.1406989097595215, loss=1.5112218856811523
I0229 11:21:50.602447 140089845819136 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.1922922134399414, loss=1.6848392486572266
I0229 11:22:24.045541 140089837426432 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.144757032394409, loss=1.598056435585022
I0229 11:22:57.526620 140089845819136 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.8659101724624634, loss=1.5487169027328491
I0229 11:23:30.997590 140089837426432 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.7922265529632568, loss=1.4177701473236084
I0229 11:24:04.443212 140089845819136 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.8889336585998535, loss=1.5290191173553467
I0229 11:24:37.920267 140089837426432 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9701262712478638, loss=1.6738805770874023
I0229 11:25:11.362616 140089845819136 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.9316370487213135, loss=1.592717170715332
I0229 11:25:44.799319 140089837426432 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.0220909118652344, loss=1.4995347261428833
I0229 11:26:18.259731 140089845819136 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.020366907119751, loss=1.6200172901153564
I0229 11:26:43.496397 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:26:49.569378 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:26:58.003544 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:27:00.281290 140252611495744 submission_runner.py:411] Time since start: 22361.16s, 	Step: 63977, 	{'train/accuracy': 0.7126116156578064, 'train/loss': 1.0953108072280884, 'validation/accuracy': 0.6471799612045288, 'validation/loss': 1.4452688694000244, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.201335906982422, 'test/num_examples': 10000, 'score': 21475.078028202057, 'total_duration': 22361.16133069992, 'accumulated_submission_time': 21475.078028202057, 'accumulated_eval_time': 882.3958539962769, 'accumulated_logging_time': 1.4340605735778809}
I0229 11:27:00.320623 140089778697984 logging_writer.py:48] [63977] accumulated_eval_time=882.395854, accumulated_logging_time=1.434061, accumulated_submission_time=21475.078028, global_step=63977, preemption_count=0, score=21475.078028, test/accuracy=0.516100, test/loss=2.201336, test/num_examples=10000, total_duration=22361.161331, train/accuracy=0.712612, train/loss=1.095311, validation/accuracy=0.647180, validation/loss=1.445269, validation/num_examples=50000
I0229 11:27:08.335104 140089837426432 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.9887615442276, loss=1.5405817031860352
I0229 11:27:41.821577 140089778697984 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.8614201545715332, loss=1.4158352613449097
I0229 11:28:15.262643 140089837426432 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.7272971868515015, loss=1.538401484489441
I0229 11:28:48.698304 140089778697984 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9922301769256592, loss=1.5124852657318115
I0229 11:29:22.122153 140089837426432 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8739427328109741, loss=1.451716423034668
I0229 11:29:55.580230 140089778697984 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.039133071899414, loss=1.5224626064300537
I0229 11:30:29.008197 140089837426432 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8535346984863281, loss=1.5347347259521484
I0229 11:31:02.467499 140089778697984 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.046721935272217, loss=1.637701153755188
I0229 11:31:35.893732 140089837426432 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0695316791534424, loss=1.5243805646896362
I0229 11:32:09.372715 140089778697984 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.8823579549789429, loss=1.6468195915222168
I0229 11:32:42.841969 140089837426432 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.8086451292037964, loss=1.6803033351898193
I0229 11:33:16.360108 140089778697984 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.921086072921753, loss=1.5856311321258545
I0229 11:33:49.785259 140089837426432 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.83577561378479, loss=1.3933402299880981
I0229 11:34:23.238086 140089778697984 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.2809057235717773, loss=1.5180013179779053
I0229 11:34:56.678920 140089837426432 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9486467838287354, loss=1.4104065895080566
I0229 11:35:30.113113 140089778697984 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.0523502826690674, loss=1.6377325057983398
I0229 11:35:30.590674 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:35:36.639480 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:35:45.050744 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:35:47.326467 140252611495744 submission_runner.py:411] Time since start: 22888.21s, 	Step: 65503, 	{'train/accuracy': 0.7235730290412903, 'train/loss': 1.0580490827560425, 'validation/accuracy': 0.6538400053977966, 'validation/loss': 1.4230231046676636, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.1661126613616943, 'test/num_examples': 10000, 'score': 21985.282883882523, 'total_duration': 22888.206524848938, 'accumulated_submission_time': 21985.282883882523, 'accumulated_eval_time': 899.1315841674805, 'accumulated_logging_time': 1.4839389324188232}
I0229 11:35:47.354510 140089845819136 logging_writer.py:48] [65503] accumulated_eval_time=899.131584, accumulated_logging_time=1.483939, accumulated_submission_time=21985.282884, global_step=65503, preemption_count=0, score=21985.282884, test/accuracy=0.528200, test/loss=2.166113, test/num_examples=10000, total_duration=22888.206525, train/accuracy=0.723573, train/loss=1.058049, validation/accuracy=0.653840, validation/loss=1.423023, validation/num_examples=50000
I0229 11:36:20.146174 140089854211840 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.8503426313400269, loss=1.5697126388549805
I0229 11:36:53.578444 140089845819136 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.9710795879364014, loss=1.443436861038208
I0229 11:37:27.023624 140089854211840 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.152841567993164, loss=1.616350531578064
I0229 11:38:00.456612 140089845819136 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.120008707046509, loss=1.5428102016448975
I0229 11:38:33.890298 140089854211840 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.005467176437378, loss=1.5278688669204712
I0229 11:39:07.334456 140089845819136 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9609907865524292, loss=1.6016826629638672
I0229 11:39:40.874071 140089854211840 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.9596270322799683, loss=1.5590295791625977
I0229 11:40:14.321641 140089845819136 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.1880815029144287, loss=1.6190154552459717
I0229 11:40:47.802362 140089854211840 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.032959222793579, loss=1.5515004396438599
I0229 11:41:21.260680 140089845819136 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.0292441844940186, loss=1.5482029914855957
I0229 11:41:54.707683 140089854211840 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.044933319091797, loss=1.631386399269104
I0229 11:42:28.169484 140089845819136 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.0967912673950195, loss=1.5002076625823975
I0229 11:43:01.611618 140089854211840 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.079072952270508, loss=1.5860028266906738
I0229 11:43:35.068626 140089845819136 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.0293116569519043, loss=1.5362308025360107
I0229 11:44:08.503133 140089854211840 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.778511643409729, loss=1.457470178604126
I0229 11:44:17.345817 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:44:23.472327 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:44:31.821728 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:44:34.101897 140252611495744 submission_runner.py:411] Time since start: 23414.98s, 	Step: 67028, 	{'train/accuracy': 0.7344347834587097, 'train/loss': 1.0239489078521729, 'validation/accuracy': 0.6676200032234192, 'validation/loss': 1.360720157623291, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.0818004608154297, 'test/num_examples': 10000, 'score': 22495.20993900299, 'total_duration': 23414.981965065002, 'accumulated_submission_time': 22495.20993900299, 'accumulated_eval_time': 915.8876085281372, 'accumulated_logging_time': 1.521956205368042}
I0229 11:44:34.132786 140089770305280 logging_writer.py:48] [67028] accumulated_eval_time=915.887609, accumulated_logging_time=1.521956, accumulated_submission_time=22495.209939, global_step=67028, preemption_count=0, score=22495.209939, test/accuracy=0.539900, test/loss=2.081800, test/num_examples=10000, total_duration=23414.981965, train/accuracy=0.734435, train/loss=1.023949, validation/accuracy=0.667620, validation/loss=1.360720, validation/num_examples=50000
I0229 11:44:58.576062 140089778697984 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.998854160308838, loss=1.539715051651001
I0229 11:45:32.168884 140089770305280 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.011742115020752, loss=1.5409283638000488
I0229 11:46:05.616925 140089778697984 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9366787672042847, loss=1.4423668384552002
I0229 11:46:39.050257 140089770305280 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.22810697555542, loss=1.5281026363372803
I0229 11:47:12.503257 140089778697984 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.1077523231506348, loss=1.5673404932022095
I0229 11:47:45.929650 140089770305280 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.147948980331421, loss=1.5131891965866089
I0229 11:48:19.382400 140089778697984 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.8673263788223267, loss=1.4414243698120117
I0229 11:48:52.817197 140089770305280 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.8381149768829346, loss=1.523085594177246
I0229 11:49:26.263823 140089778697984 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.1386642456054688, loss=1.4575791358947754
I0229 11:49:59.740659 140089770305280 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.914076328277588, loss=1.531639575958252
I0229 11:50:33.195603 140089778697984 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.8199719190597534, loss=1.5349560976028442
I0229 11:51:06.644036 140089770305280 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.059220790863037, loss=1.6237883567810059
I0229 11:51:40.199292 140089778697984 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.1713311672210693, loss=1.4642820358276367
I0229 11:52:13.609235 140089770305280 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0703184604644775, loss=1.4738075733184814
I0229 11:52:47.081231 140089778697984 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.3316495418548584, loss=1.5843863487243652
I0229 11:53:04.266301 140252611495744 spec.py:321] Evaluating on the training split.
I0229 11:53:10.366059 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 11:53:18.633009 140252611495744 spec.py:349] Evaluating on the test split.
I0229 11:53:20.890329 140252611495744 submission_runner.py:411] Time since start: 23941.77s, 	Step: 68553, 	{'train/accuracy': 0.7496412396430969, 'train/loss': 0.9645988941192627, 'validation/accuracy': 0.641319990158081, 'validation/loss': 1.464681625366211, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.1813247203826904, 'test/num_examples': 10000, 'score': 23005.27852010727, 'total_duration': 23941.77029824257, 'accumulated_submission_time': 23005.27852010727, 'accumulated_eval_time': 932.5114860534668, 'accumulated_logging_time': 1.5625011920928955}
I0229 11:53:20.922245 140089854211840 logging_writer.py:48] [68553] accumulated_eval_time=932.511486, accumulated_logging_time=1.562501, accumulated_submission_time=23005.278520, global_step=68553, preemption_count=0, score=23005.278520, test/accuracy=0.512100, test/loss=2.181325, test/num_examples=10000, total_duration=23941.770298, train/accuracy=0.749641, train/loss=0.964599, validation/accuracy=0.641320, validation/loss=1.464682, validation/num_examples=50000
I0229 11:53:36.985623 140089870997248 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.9490426778793335, loss=1.4289164543151855
I0229 11:54:10.422396 140089854211840 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.1550168991088867, loss=1.5469269752502441
I0229 11:54:43.888825 140089870997248 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.093471050262451, loss=1.5426160097122192
I0229 11:55:17.335832 140089854211840 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0735530853271484, loss=1.5994398593902588
I0229 11:55:50.810960 140089870997248 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8425748348236084, loss=1.4369271993637085
I0229 11:56:24.241114 140089854211840 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.069911241531372, loss=1.518868088722229
I0229 11:56:57.702707 140089870997248 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.2102720737457275, loss=1.6196136474609375
I0229 11:57:31.214122 140089854211840 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.915333867073059, loss=1.4629957675933838
I0229 11:58:04.664010 140089870997248 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.8957149982452393, loss=1.501887321472168
I0229 11:58:38.126725 140089854211840 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.1001546382904053, loss=1.449840784072876
I0229 11:59:11.548452 140089870997248 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.0762617588043213, loss=1.5198456048965454
I0229 11:59:44.991039 140089854211840 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.9750487804412842, loss=1.4752737283706665
I0229 12:00:18.444281 140089870997248 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.901131272315979, loss=1.513524055480957
I0229 12:00:51.910121 140089854211840 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.869997262954712, loss=1.5171788930892944
I0229 12:01:25.376869 140089870997248 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.8969310522079468, loss=1.4398410320281982
I0229 12:01:50.915584 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:01:57.041829 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:02:05.466585 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:02:07.794819 140252611495744 submission_runner.py:411] Time since start: 24468.67s, 	Step: 70078, 	{'train/accuracy': 0.7494021058082581, 'train/loss': 0.9496089220046997, 'validation/accuracy': 0.6649399995803833, 'validation/loss': 1.3731956481933594, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.1011109352111816, 'test/num_examples': 10000, 'score': 23515.206540346146, 'total_duration': 24468.674884796143, 'accumulated_submission_time': 23515.206540346146, 'accumulated_eval_time': 949.3906710147858, 'accumulated_logging_time': 1.6044843196868896}
I0229 12:02:07.827127 140089770305280 logging_writer.py:48] [70078] accumulated_eval_time=949.390671, accumulated_logging_time=1.604484, accumulated_submission_time=23515.206540, global_step=70078, preemption_count=0, score=23515.206540, test/accuracy=0.530100, test/loss=2.101111, test/num_examples=10000, total_duration=24468.674885, train/accuracy=0.749402, train/loss=0.949609, validation/accuracy=0.664940, validation/loss=1.373196, validation/num_examples=50000
I0229 12:02:15.525209 140089778697984 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0914716720581055, loss=1.5496675968170166
I0229 12:02:48.965667 140089770305280 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.8766354322433472, loss=1.493914246559143
I0229 12:03:22.423473 140089778697984 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.0780792236328125, loss=1.555737018585205
I0229 12:03:55.947797 140089770305280 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.2624573707580566, loss=1.5460635423660278
I0229 12:04:29.437778 140089778697984 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.9805235862731934, loss=1.446549892425537
I0229 12:05:02.938829 140089770305280 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.296647071838379, loss=1.532890796661377
I0229 12:05:36.410412 140089778697984 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.0035324096679688, loss=1.572171926498413
I0229 12:06:09.860406 140089770305280 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0909509658813477, loss=1.518761396408081
I0229 12:06:43.361047 140089778697984 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.117239236831665, loss=1.491788387298584
I0229 12:07:16.827900 140089770305280 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.2293155193328857, loss=1.4950578212738037
I0229 12:07:50.252686 140089778697984 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.9177188873291016, loss=1.4009451866149902
I0229 12:08:23.687310 140089770305280 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.9937934875488281, loss=1.4840543270111084
I0229 12:08:57.146919 140089778697984 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.9292755126953125, loss=1.5330415964126587
I0229 12:09:30.664908 140089770305280 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.020906448364258, loss=1.45253586769104
I0229 12:10:04.102437 140089778697984 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.9079972505569458, loss=1.3906824588775635
I0229 12:10:37.578091 140089770305280 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.144584894180298, loss=1.5218862295150757
I0229 12:10:38.054924 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:10:44.200865 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:10:52.518016 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:10:54.780320 140252611495744 submission_runner.py:411] Time since start: 24995.66s, 	Step: 71603, 	{'train/accuracy': 0.7323421239852905, 'train/loss': 1.0162076950073242, 'validation/accuracy': 0.6559000015258789, 'validation/loss': 1.420893669128418, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.1404755115509033, 'test/num_examples': 10000, 'score': 24025.367934703827, 'total_duration': 24995.660386800766, 'accumulated_submission_time': 24025.367934703827, 'accumulated_eval_time': 966.1160097122192, 'accumulated_logging_time': 1.6491477489471436}
I0229 12:10:54.810667 140089854211840 logging_writer.py:48] [71603] accumulated_eval_time=966.116010, accumulated_logging_time=1.649148, accumulated_submission_time=24025.367935, global_step=71603, preemption_count=0, score=24025.367935, test/accuracy=0.529400, test/loss=2.140476, test/num_examples=10000, total_duration=24995.660387, train/accuracy=0.732342, train/loss=1.016208, validation/accuracy=0.655900, validation/loss=1.420894, validation/num_examples=50000
I0229 12:11:27.582543 140089862604544 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.1099069118499756, loss=1.5196913480758667
I0229 12:12:01.034755 140089854211840 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.8816815614700317, loss=1.471706748008728
I0229 12:12:34.499178 140089862604544 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.271909236907959, loss=1.5331536531448364
I0229 12:13:07.951650 140089854211840 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.1236586570739746, loss=1.4749159812927246
I0229 12:13:41.415272 140089862604544 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.0472352504730225, loss=1.4500060081481934
I0229 12:14:14.866336 140089854211840 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.1613378524780273, loss=1.529794454574585
I0229 12:14:48.311256 140089862604544 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.0630130767822266, loss=1.4939109086990356
I0229 12:15:21.766334 140089854211840 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9991644620895386, loss=1.356390357017517
I0229 12:15:55.286096 140089862604544 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.9373772144317627, loss=1.4961676597595215
I0229 12:16:28.757789 140089854211840 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9199706315994263, loss=1.404750108718872
I0229 12:17:02.208565 140089862604544 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.9465843439102173, loss=1.5199965238571167
I0229 12:17:35.665482 140089854211840 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.2591488361358643, loss=1.481404185295105
I0229 12:18:09.125928 140089862604544 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.0127062797546387, loss=1.4099584817886353
I0229 12:18:42.565347 140089854211840 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.9465900659561157, loss=1.4208197593688965
I0229 12:19:16.014826 140089862604544 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.932407259941101, loss=1.478439450263977
I0229 12:19:24.865837 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:19:30.956731 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:19:39.216596 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:19:41.454846 140252611495744 submission_runner.py:411] Time since start: 25522.33s, 	Step: 73128, 	{'train/accuracy': 0.7429647445678711, 'train/loss': 0.9706432223320007, 'validation/accuracy': 0.6710000038146973, 'validation/loss': 1.3437200784683228, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.0839552879333496, 'test/num_examples': 10000, 'score': 24535.35845017433, 'total_duration': 25522.33491587639, 'accumulated_submission_time': 24535.35845017433, 'accumulated_eval_time': 982.704963684082, 'accumulated_logging_time': 1.689319372177124}
I0229 12:19:41.490252 140089778697984 logging_writer.py:48] [73128] accumulated_eval_time=982.704964, accumulated_logging_time=1.689319, accumulated_submission_time=24535.358450, global_step=73128, preemption_count=0, score=24535.358450, test/accuracy=0.542000, test/loss=2.083955, test/num_examples=10000, total_duration=25522.334916, train/accuracy=0.742965, train/loss=0.970643, validation/accuracy=0.671000, validation/loss=1.343720, validation/num_examples=50000
I0229 12:20:05.911228 140089837426432 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.7314714193344116, loss=1.4483412504196167
I0229 12:20:39.341890 140089778697984 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.8795604705810547, loss=1.4055063724517822
I0229 12:21:12.818085 140089837426432 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.111687183380127, loss=1.4754865169525146
I0229 12:21:46.390976 140089778697984 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.9969583749771118, loss=1.5036989450454712
I0229 12:22:19.872736 140089837426432 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.0441529750823975, loss=1.4331283569335938
I0229 12:22:53.362380 140089778697984 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0795767307281494, loss=1.5700582265853882
I0229 12:23:26.822699 140089837426432 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.9233107566833496, loss=1.4899404048919678
I0229 12:24:00.260870 140089778697984 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.154925584793091, loss=1.4583103656768799
I0229 12:24:33.700205 140089837426432 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9169713258743286, loss=1.4510114192962646
I0229 12:25:07.129142 140089778697984 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.9955195188522339, loss=1.5448477268218994
I0229 12:25:40.610634 140089837426432 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.1632652282714844, loss=1.558078408241272
I0229 12:26:14.101006 140089778697984 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.0170180797576904, loss=1.3399734497070312
I0229 12:26:47.565017 140089837426432 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.1418724060058594, loss=1.529640793800354
I0229 12:27:20.999652 140089778697984 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.085279703140259, loss=1.4731025695800781
I0229 12:27:54.572750 140089837426432 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.3431601524353027, loss=1.4477282762527466
I0229 12:28:11.753486 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:28:17.948315 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:28:26.112135 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:28:28.385838 140252611495744 submission_runner.py:411] Time since start: 26049.27s, 	Step: 74653, 	{'train/accuracy': 0.7179328799247742, 'train/loss': 1.0882242918014526, 'validation/accuracy': 0.6489799618721008, 'validation/loss': 1.4370622634887695, 'validation/num_examples': 50000, 'test/accuracy': 0.51500004529953, 'test/loss': 2.1855597496032715, 'test/num_examples': 10000, 'score': 25045.55570292473, 'total_duration': 26049.26572537422, 'accumulated_submission_time': 25045.55570292473, 'accumulated_eval_time': 999.3370826244354, 'accumulated_logging_time': 1.7352948188781738}
I0229 12:28:28.414531 140089862604544 logging_writer.py:48] [74653] accumulated_eval_time=999.337083, accumulated_logging_time=1.735295, accumulated_submission_time=25045.555703, global_step=74653, preemption_count=0, score=25045.555703, test/accuracy=0.515000, test/loss=2.185560, test/num_examples=10000, total_duration=26049.265725, train/accuracy=0.717933, train/loss=1.088224, validation/accuracy=0.648980, validation/loss=1.437062, validation/num_examples=50000
I0229 12:28:44.472904 140089870997248 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.8567745685577393, loss=1.412445068359375
I0229 12:29:17.931686 140089862604544 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.393035411834717, loss=1.4239732027053833
I0229 12:29:51.399106 140089870997248 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.046834707260132, loss=1.4665206670761108
I0229 12:30:24.838437 140089862604544 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.2636260986328125, loss=1.4177157878875732
I0229 12:30:58.293167 140089870997248 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.9114887714385986, loss=1.466118335723877
I0229 12:31:31.717362 140089862604544 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.0123350620269775, loss=1.4951868057250977
I0229 12:32:05.210575 140089870997248 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.2181291580200195, loss=1.5444703102111816
I0229 12:32:38.693917 140089862604544 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.1354053020477295, loss=1.485037088394165
I0229 12:33:12.240531 140089870997248 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.1511619091033936, loss=1.4792282581329346
I0229 12:33:45.696145 140089862604544 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.067044734954834, loss=1.4992598295211792
I0229 12:34:19.259156 140089870997248 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.19686222076416, loss=1.5465303659439087
I0229 12:34:52.702339 140089862604544 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9728221893310547, loss=1.4455339908599854
I0229 12:35:26.152068 140089870997248 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.175171375274658, loss=1.5487029552459717
I0229 12:35:59.608782 140089862604544 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0931830406188965, loss=1.5988974571228027
I0229 12:36:33.059134 140089870997248 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.028477430343628, loss=1.4074305295944214
I0229 12:36:58.639572 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:37:05.524761 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:37:13.749650 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:37:16.004366 140252611495744 submission_runner.py:411] Time since start: 26576.88s, 	Step: 76178, 	{'train/accuracy': 0.73246169090271, 'train/loss': 1.026774287223816, 'validation/accuracy': 0.6608399748802185, 'validation/loss': 1.3760240077972412, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.1334023475646973, 'test/num_examples': 10000, 'score': 25555.715767621994, 'total_duration': 26576.88431286812, 'accumulated_submission_time': 25555.715767621994, 'accumulated_eval_time': 1016.7017018795013, 'accumulated_logging_time': 1.7748703956604004}
I0229 12:37:16.039553 140089770305280 logging_writer.py:48] [76178] accumulated_eval_time=1016.701702, accumulated_logging_time=1.774870, accumulated_submission_time=25555.715768, global_step=76178, preemption_count=0, score=25555.715768, test/accuracy=0.530000, test/loss=2.133402, test/num_examples=10000, total_duration=26576.884313, train/accuracy=0.732462, train/loss=1.026774, validation/accuracy=0.660840, validation/loss=1.376024, validation/num_examples=50000
I0229 12:37:23.741605 140089778697984 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.2041521072387695, loss=1.4871021509170532
I0229 12:37:57.179831 140089770305280 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1373867988586426, loss=1.475200891494751
I0229 12:38:30.638791 140089778697984 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.915502667427063, loss=1.3469715118408203
I0229 12:39:04.114807 140089770305280 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.1010754108428955, loss=1.457234501838684
I0229 12:39:37.579855 140089778697984 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.0453877449035645, loss=1.5117720365524292
I0229 12:40:11.121875 140089770305280 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.1199965476989746, loss=1.4344390630722046
I0229 12:40:44.575307 140089778697984 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.9740452766418457, loss=1.4876680374145508
I0229 12:41:18.033915 140089770305280 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.1512291431427, loss=1.5079725980758667
I0229 12:41:51.474305 140089778697984 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.9858680963516235, loss=1.4601205587387085
I0229 12:42:24.931076 140089770305280 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.161893606185913, loss=1.4893457889556885
I0229 12:42:58.362511 140089778697984 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.141103744506836, loss=1.492783546447754
I0229 12:43:31.831724 140089770305280 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.0604491233825684, loss=1.4524133205413818
I0229 12:44:05.274331 140089778697984 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1415109634399414, loss=1.383360743522644
I0229 12:44:38.957468 140089770305280 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.099083185195923, loss=1.462233066558838
I0229 12:45:12.419097 140089778697984 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.1168229579925537, loss=1.5332841873168945
I0229 12:45:45.914459 140089770305280 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.1275157928466797, loss=1.4155793190002441
I0229 12:45:46.060221 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:45:52.161114 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:46:00.644052 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:46:02.936216 140252611495744 submission_runner.py:411] Time since start: 27103.82s, 	Step: 77702, 	{'train/accuracy': 0.7727798223495483, 'train/loss': 0.8478021025657654, 'validation/accuracy': 0.670199990272522, 'validation/loss': 1.3518798351287842, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.1114277839660645, 'test/num_examples': 10000, 'score': 26065.671036958694, 'total_duration': 27103.81628870964, 'accumulated_submission_time': 26065.671036958694, 'accumulated_eval_time': 1033.5776641368866, 'accumulated_logging_time': 1.8206498622894287}
I0229 12:46:02.965783 140089854211840 logging_writer.py:48] [77702] accumulated_eval_time=1033.577664, accumulated_logging_time=1.820650, accumulated_submission_time=26065.671037, global_step=77702, preemption_count=0, score=26065.671037, test/accuracy=0.536800, test/loss=2.111428, test/num_examples=10000, total_duration=27103.816289, train/accuracy=0.772780, train/loss=0.847802, validation/accuracy=0.670200, validation/loss=1.351880, validation/num_examples=50000
I0229 12:46:36.077450 140089862604544 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.9211094379425049, loss=1.4702335596084595
I0229 12:47:09.563786 140089854211840 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.1556406021118164, loss=1.5290042161941528
I0229 12:47:43.029195 140089862604544 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.1651618480682373, loss=1.4791834354400635
I0229 12:48:16.468094 140089854211840 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.955418348312378, loss=1.394164800643921
I0229 12:48:49.937826 140089862604544 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.0998375415802, loss=1.5550768375396729
I0229 12:49:23.390762 140089854211840 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1928248405456543, loss=1.4190367460250854
I0229 12:49:56.845397 140089862604544 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.1174843311309814, loss=1.3992154598236084
I0229 12:50:30.307711 140089854211840 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.129952907562256, loss=1.4815914630889893
I0229 12:51:03.777416 140089862604544 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.2258095741271973, loss=1.4091287851333618
I0229 12:51:37.235647 140089854211840 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.9520012140274048, loss=1.5354702472686768
I0229 12:52:10.805332 140089862604544 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.2753477096557617, loss=1.3976706266403198
I0229 12:52:44.262042 140089854211840 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.0621583461761475, loss=1.370236873626709
I0229 12:53:17.753293 140089862604544 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.3217613697052, loss=1.5709686279296875
I0229 12:53:51.201065 140089854211840 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.0569334030151367, loss=1.495597243309021
I0229 12:54:24.661337 140089862604544 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.112950086593628, loss=1.400230050086975
I0229 12:54:33.198795 140252611495744 spec.py:321] Evaluating on the training split.
I0229 12:54:39.325151 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 12:54:47.684296 140252611495744 spec.py:349] Evaluating on the test split.
I0229 12:54:49.953490 140252611495744 submission_runner.py:411] Time since start: 27630.83s, 	Step: 79227, 	{'train/accuracy': 0.7599848508834839, 'train/loss': 0.9066538214683533, 'validation/accuracy': 0.6744799613952637, 'validation/loss': 1.3212937116622925, 'validation/num_examples': 50000, 'test/accuracy': 0.5470000505447388, 'test/loss': 2.0392796993255615, 'test/num_examples': 10000, 'score': 26575.838837385178, 'total_duration': 27630.83352947235, 'accumulated_submission_time': 26575.838837385178, 'accumulated_eval_time': 1050.3322749137878, 'accumulated_logging_time': 1.8604519367218018}
I0229 12:54:50.002600 140089845819136 logging_writer.py:48] [79227] accumulated_eval_time=1050.332275, accumulated_logging_time=1.860452, accumulated_submission_time=26575.838837, global_step=79227, preemption_count=0, score=26575.838837, test/accuracy=0.547000, test/loss=2.039280, test/num_examples=10000, total_duration=27630.833529, train/accuracy=0.759985, train/loss=0.906654, validation/accuracy=0.674480, validation/loss=1.321294, validation/num_examples=50000
I0229 12:55:14.730357 140089879389952 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.515477418899536, loss=1.5013118982315063
I0229 12:55:48.195731 140089845819136 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.1884052753448486, loss=1.499594807624817
I0229 12:56:21.660941 140089879389952 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.9541078805923462, loss=1.3986077308654785
I0229 12:56:55.130778 140089845819136 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.1038506031036377, loss=1.429823875427246
I0229 12:57:28.637904 140089879389952 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.099256753921509, loss=1.4827080965042114
I0229 12:58:02.094105 140089845819136 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.162961721420288, loss=1.4643197059631348
I0229 12:58:35.664712 140089879389952 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.799916386604309, loss=1.3440968990325928
I0229 12:59:09.130250 140089845819136 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.513598918914795, loss=1.5356860160827637
I0229 12:59:42.576000 140089879389952 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.1906630992889404, loss=1.4915492534637451
I0229 13:00:16.056112 140089845819136 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.2019293308258057, loss=1.4707356691360474
I0229 13:00:49.505454 140089879389952 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.109398126602173, loss=1.548764944076538
I0229 13:01:22.990015 140089845819136 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.268268346786499, loss=1.5180708169937134
I0229 13:01:56.492163 140089879389952 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.2206852436065674, loss=1.538881540298462
I0229 13:02:29.961678 140089845819136 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.0324931144714355, loss=1.4095706939697266
I0229 13:03:03.402488 140089879389952 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.8879117965698242, loss=1.362522006034851
I0229 13:03:19.957899 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:03:26.052969 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:03:34.329807 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:03:36.597115 140252611495744 submission_runner.py:411] Time since start: 28157.48s, 	Step: 80751, 	{'train/accuracy': 0.7531289458274841, 'train/loss': 0.9358610510826111, 'validation/accuracy': 0.6725999712944031, 'validation/loss': 1.338200569152832, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.07574200630188, 'test/num_examples': 10000, 'score': 27085.72404384613, 'total_duration': 28157.477063655853, 'accumulated_submission_time': 27085.72404384613, 'accumulated_eval_time': 1066.9713323116302, 'accumulated_logging_time': 1.9235823154449463}
I0229 13:03:36.627851 140089837426432 logging_writer.py:48] [80751] accumulated_eval_time=1066.971332, accumulated_logging_time=1.923582, accumulated_submission_time=27085.724044, global_step=80751, preemption_count=0, score=27085.724044, test/accuracy=0.538400, test/loss=2.075742, test/num_examples=10000, total_duration=28157.477064, train/accuracy=0.753129, train/loss=0.935861, validation/accuracy=0.672600, validation/loss=1.338201, validation/num_examples=50000
I0229 13:03:53.335690 140089854211840 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.2811410427093506, loss=1.4029730558395386
I0229 13:04:26.852445 140089837426432 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1594014167785645, loss=1.5290348529815674
I0229 13:05:00.316274 140089854211840 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.0279085636138916, loss=1.5216165781021118
I0229 13:05:33.781748 140089837426432 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1867241859436035, loss=1.506088376045227
I0229 13:06:07.211162 140089854211840 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.1417715549468994, loss=1.3972898721694946
I0229 13:06:40.675974 140089837426432 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.1075868606567383, loss=1.4243823289871216
I0229 13:07:14.135794 140089854211840 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.1841702461242676, loss=1.4730892181396484
I0229 13:07:47.571549 140089837426432 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.153961420059204, loss=1.5033411979675293
I0229 13:08:21.042919 140089854211840 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.0225770473480225, loss=1.2393157482147217
I0229 13:08:54.500091 140089837426432 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.022343397140503, loss=1.4270881414413452
I0229 13:09:27.956214 140089854211840 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.215909242630005, loss=1.4315694570541382
I0229 13:10:01.428959 140089837426432 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.059035301208496, loss=1.5333189964294434
I0229 13:10:34.979964 140089854211840 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1340131759643555, loss=1.4857676029205322
I0229 13:11:08.410092 140089837426432 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.088984727859497, loss=1.4632574319839478
I0229 13:11:41.900083 140089854211840 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.129937171936035, loss=1.456140398979187
I0229 13:12:06.783378 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:12:12.931362 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:12:21.330875 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:12:23.556077 140252611495744 submission_runner.py:411] Time since start: 28684.44s, 	Step: 82276, 	{'train/accuracy': 0.7472097873687744, 'train/loss': 0.9645992517471313, 'validation/accuracy': 0.6704999804496765, 'validation/loss': 1.3342227935791016, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0555646419525146, 'test/num_examples': 10000, 'score': 27595.813599586487, 'total_duration': 28684.43613266945, 'accumulated_submission_time': 27595.813599586487, 'accumulated_eval_time': 1083.7439770698547, 'accumulated_logging_time': 1.9646568298339844}
I0229 13:12:23.590205 140089845819136 logging_writer.py:48] [82276] accumulated_eval_time=1083.743977, accumulated_logging_time=1.964657, accumulated_submission_time=27595.813600, global_step=82276, preemption_count=0, score=27595.813600, test/accuracy=0.541500, test/loss=2.055565, test/num_examples=10000, total_duration=28684.436133, train/accuracy=0.747210, train/loss=0.964599, validation/accuracy=0.670500, validation/loss=1.334223, validation/num_examples=50000
I0229 13:12:31.950496 140089870997248 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.031585931777954, loss=1.3654255867004395
I0229 13:13:05.388379 140089845819136 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.202822685241699, loss=1.4703006744384766
I0229 13:13:38.816211 140089870997248 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2111477851867676, loss=1.4683349132537842
I0229 13:14:12.239748 140089845819136 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.013439893722534, loss=1.4571239948272705
I0229 13:14:45.680748 140089870997248 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.1993744373321533, loss=1.4272617101669312
I0229 13:15:19.094263 140089845819136 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.9596225023269653, loss=1.4450606107711792
I0229 13:15:52.553546 140089870997248 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.0539743900299072, loss=1.4604606628417969
I0229 13:16:26.089258 140089845819136 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.9504857063293457, loss=1.3520989418029785
I0229 13:16:59.546512 140089870997248 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1831886768341064, loss=1.4031637907028198
I0229 13:17:32.949010 140089845819136 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.366237163543701, loss=1.5149667263031006
I0229 13:18:06.412807 140089870997248 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.9845284223556519, loss=1.4284164905548096
I0229 13:18:39.853679 140089845819136 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.3392510414123535, loss=1.3520011901855469
I0229 13:19:13.331045 140089870997248 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.242489814758301, loss=1.4172070026397705
I0229 13:19:46.787608 140089845819136 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.0616657733917236, loss=1.2950201034545898
I0229 13:20:20.220757 140089870997248 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.461423397064209, loss=1.5574840307235718
I0229 13:20:53.684594 140089845819136 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.152322769165039, loss=1.50619375705719
I0229 13:20:53.691305 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:20:59.817904 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:21:08.202064 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:21:10.450335 140252611495744 submission_runner.py:411] Time since start: 29211.33s, 	Step: 83801, 	{'train/accuracy': 0.7385004758834839, 'train/loss': 0.9893120527267456, 'validation/accuracy': 0.664900004863739, 'validation/loss': 1.3621970415115356, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.107455253601074, 'test/num_examples': 10000, 'score': 28105.848766088486, 'total_duration': 29211.330406665802, 'accumulated_submission_time': 28105.848766088486, 'accumulated_eval_time': 1100.5029304027557, 'accumulated_logging_time': 2.0091564655303955}
I0229 13:21:10.480577 140089770305280 logging_writer.py:48] [83801] accumulated_eval_time=1100.502930, accumulated_logging_time=2.009156, accumulated_submission_time=28105.848766, global_step=83801, preemption_count=0, score=28105.848766, test/accuracy=0.538300, test/loss=2.107455, test/num_examples=10000, total_duration=29211.330407, train/accuracy=0.738500, train/loss=0.989312, validation/accuracy=0.664900, validation/loss=1.362197, validation/num_examples=50000
I0229 13:21:43.938879 140089778697984 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.2429823875427246, loss=1.4913699626922607
I0229 13:22:17.376466 140089770305280 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.27316951751709, loss=1.4615638256072998
I0229 13:22:50.880702 140089778697984 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.149509906768799, loss=1.4324597120285034
I0229 13:23:24.332089 140089770305280 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.326723337173462, loss=1.5504708290100098
I0229 13:23:57.756508 140089778697984 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.0825796127319336, loss=1.5255835056304932
I0229 13:24:31.208190 140089770305280 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.0919227600097656, loss=1.4555119276046753
I0229 13:25:04.642959 140089778697984 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.1421432495117188, loss=1.3760662078857422
I0229 13:25:38.098760 140089770305280 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.199742555618286, loss=1.3805718421936035
I0229 13:26:11.527733 140089778697984 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.1985747814178467, loss=1.402296543121338
I0229 13:26:44.990480 140089770305280 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.2535789012908936, loss=1.4651283025741577
I0229 13:27:18.413307 140089778697984 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.1008524894714355, loss=1.4843823909759521
I0229 13:27:51.853308 140089770305280 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.641544818878174, loss=1.4033139944076538
I0229 13:28:25.383657 140089778697984 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.1420016288757324, loss=1.429648995399475
I0229 13:28:58.853949 140089770305280 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.337393045425415, loss=1.3535082340240479
I0229 13:29:32.268479 140089778697984 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.409750461578369, loss=1.4478129148483276
I0229 13:29:40.764233 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:29:46.932562 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:29:55.121217 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:29:57.426732 140252611495744 submission_runner.py:411] Time since start: 29738.31s, 	Step: 85327, 	{'train/accuracy': 0.7606425285339355, 'train/loss': 0.9143653512001038, 'validation/accuracy': 0.6753199696540833, 'validation/loss': 1.3267306089401245, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.0425949096679688, 'test/num_examples': 10000, 'score': 28616.067785024643, 'total_duration': 29738.30677652359, 'accumulated_submission_time': 28616.067785024643, 'accumulated_eval_time': 1117.165348291397, 'accumulated_logging_time': 2.049769401550293}
I0229 13:29:57.462157 140089862604544 logging_writer.py:48] [85327] accumulated_eval_time=1117.165348, accumulated_logging_time=2.049769, accumulated_submission_time=28616.067785, global_step=85327, preemption_count=0, score=28616.067785, test/accuracy=0.538500, test/loss=2.042595, test/num_examples=10000, total_duration=29738.306777, train/accuracy=0.760643, train/loss=0.914365, validation/accuracy=0.675320, validation/loss=1.326731, validation/num_examples=50000
I0229 13:30:22.199440 140089870997248 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.200274705886841, loss=1.3166587352752686
I0229 13:30:55.663740 140089862604544 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.0826189517974854, loss=1.3963572978973389
I0229 13:31:29.122406 140089870997248 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.044705629348755, loss=1.387025237083435
I0229 13:32:02.544912 140089862604544 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.12910532951355, loss=1.4611191749572754
I0229 13:32:36.054375 140089870997248 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.3215436935424805, loss=1.402384638786316
I0229 13:33:09.511036 140089862604544 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.2905025482177734, loss=1.396743655204773
I0229 13:33:42.933142 140089870997248 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.1794016361236572, loss=1.5122877359390259
I0229 13:34:16.394964 140089862604544 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.1701247692108154, loss=1.4277327060699463
I0229 13:34:49.975954 140089870997248 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.215498447418213, loss=1.4575878381729126
I0229 13:35:23.416509 140089862604544 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.3099629878997803, loss=1.4286075830459595
I0229 13:35:56.862885 140089870997248 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.148582935333252, loss=1.3793282508850098
I0229 13:36:30.309369 140089862604544 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.472588539123535, loss=1.4326149225234985
I0229 13:37:03.735118 140089870997248 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.4425508975982666, loss=1.4370183944702148
I0229 13:37:37.193416 140089862604544 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.263798713684082, loss=1.334943413734436
I0229 13:38:10.634046 140089870997248 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.1126983165740967, loss=1.4282305240631104
I0229 13:38:27.496802 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:38:33.604027 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:38:41.865524 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:38:44.143581 140252611495744 submission_runner.py:411] Time since start: 30265.02s, 	Step: 86852, 	{'train/accuracy': 0.7731385231018066, 'train/loss': 0.8398574590682983, 'validation/accuracy': 0.6742599606513977, 'validation/loss': 1.3247716426849365, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.076277732849121, 'test/num_examples': 10000, 'score': 29126.03837108612, 'total_duration': 30265.023649930954, 'accumulated_submission_time': 29126.03837108612, 'accumulated_eval_time': 1133.8120720386505, 'accumulated_logging_time': 2.0952630043029785}
I0229 13:38:44.175867 140089770305280 logging_writer.py:48] [86852] accumulated_eval_time=1133.812072, accumulated_logging_time=2.095263, accumulated_submission_time=29126.038371, global_step=86852, preemption_count=0, score=29126.038371, test/accuracy=0.539700, test/loss=2.076278, test/num_examples=10000, total_duration=30265.023650, train/accuracy=0.773139, train/loss=0.839857, validation/accuracy=0.674260, validation/loss=1.324772, validation/num_examples=50000
I0229 13:39:00.556942 140089778697984 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.548082113265991, loss=1.5288180112838745
I0229 13:39:34.017783 140089770305280 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.128178596496582, loss=1.4618555307388306
I0229 13:40:07.524801 140089778697984 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.2543864250183105, loss=1.468942642211914
I0229 13:40:41.142940 140089770305280 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.163799524307251, loss=1.398166537284851
I0229 13:41:14.566537 140089778697984 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2128543853759766, loss=1.4021707773208618
I0229 13:41:48.052289 140089770305280 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.3514153957366943, loss=1.4135459661483765
I0229 13:42:21.548405 140089778697984 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.2739109992980957, loss=1.4705199003219604
I0229 13:42:55.008069 140089770305280 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.276034116744995, loss=1.4998664855957031
I0229 13:43:28.461678 140089778697984 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.059903621673584, loss=1.3835275173187256
I0229 13:44:01.922739 140089770305280 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.236567497253418, loss=1.4511911869049072
I0229 13:44:35.430850 140089778697984 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.35274076461792, loss=1.4083620309829712
I0229 13:45:08.892117 140089770305280 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.226166248321533, loss=1.4726203680038452
I0229 13:45:42.374686 140089778697984 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.0552175045013428, loss=1.3818309307098389
I0229 13:46:15.829978 140089770305280 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3943166732788086, loss=1.4804327487945557
I0229 13:46:49.331117 140089778697984 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.138509750366211, loss=1.3528497219085693
I0229 13:47:14.238638 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:47:20.325529 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:47:28.499370 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:47:30.724855 140252611495744 submission_runner.py:411] Time since start: 30791.60s, 	Step: 88376, 	{'train/accuracy': 0.7651267647743225, 'train/loss': 0.8875182867050171, 'validation/accuracy': 0.6764000058174133, 'validation/loss': 1.306697130203247, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.0423593521118164, 'test/num_examples': 10000, 'score': 29636.034289360046, 'total_duration': 30791.604912519455, 'accumulated_submission_time': 29636.034289360046, 'accumulated_eval_time': 1150.2982242107391, 'accumulated_logging_time': 2.140028476715088}
I0229 13:47:30.758071 140089854211840 logging_writer.py:48] [88376] accumulated_eval_time=1150.298224, accumulated_logging_time=2.140028, accumulated_submission_time=29636.034289, global_step=88376, preemption_count=0, score=29636.034289, test/accuracy=0.545500, test/loss=2.042359, test/num_examples=10000, total_duration=30791.604913, train/accuracy=0.765127, train/loss=0.887518, validation/accuracy=0.676400, validation/loss=1.306697, validation/num_examples=50000
I0229 13:47:39.139954 140089862604544 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.299865961074829, loss=1.4193251132965088
I0229 13:48:12.564799 140089854211840 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.1984200477600098, loss=1.415808081626892
I0229 13:48:46.047571 140089862604544 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.2972044944763184, loss=1.4962718486785889
I0229 13:49:19.514186 140089854211840 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.110565423965454, loss=1.3101882934570312
I0229 13:49:52.964165 140089862604544 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.280954360961914, loss=1.5122129917144775
I0229 13:50:26.466724 140089854211840 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.4283459186553955, loss=1.3312938213348389
I0229 13:50:59.968699 140089862604544 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.1989247798919678, loss=1.3878164291381836
I0229 13:51:33.503631 140089854211840 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.263840913772583, loss=1.4811623096466064
I0229 13:52:06.974391 140089862604544 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.3044798374176025, loss=1.4611836671829224
I0229 13:52:40.422844 140089854211840 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.3095786571502686, loss=1.3959981203079224
I0229 13:53:13.976766 140089862604544 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.373469591140747, loss=1.4732972383499146
I0229 13:53:47.417150 140089854211840 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.2674527168273926, loss=1.4222863912582397
I0229 13:54:20.897871 140089862604544 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2183804512023926, loss=1.4074288606643677
I0229 13:54:54.332183 140089854211840 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.2594361305236816, loss=1.472602128982544
I0229 13:55:27.810711 140089862604544 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.2481377124786377, loss=1.4210869073867798
I0229 13:56:00.739182 140252611495744 spec.py:321] Evaluating on the training split.
I0229 13:56:07.050488 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 13:56:15.312949 140252611495744 spec.py:349] Evaluating on the test split.
I0229 13:56:17.594073 140252611495744 submission_runner.py:411] Time since start: 31318.47s, 	Step: 89900, 	{'train/accuracy': 0.7606425285339355, 'train/loss': 0.8991544842720032, 'validation/accuracy': 0.6783599853515625, 'validation/loss': 1.304556965827942, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.004889488220215, 'test/num_examples': 10000, 'score': 30145.949870347977, 'total_duration': 31318.474117040634, 'accumulated_submission_time': 30145.949870347977, 'accumulated_eval_time': 1167.15305352211, 'accumulated_logging_time': 2.1838526725769043}
I0229 13:56:17.634543 140089770305280 logging_writer.py:48] [89900] accumulated_eval_time=1167.153054, accumulated_logging_time=2.183853, accumulated_submission_time=30145.949870, global_step=89900, preemption_count=0, score=30145.949870, test/accuracy=0.551500, test/loss=2.004889, test/num_examples=10000, total_duration=31318.474117, train/accuracy=0.760643, train/loss=0.899154, validation/accuracy=0.678360, validation/loss=1.304557, validation/num_examples=50000
I0229 13:56:17.982597 140089778697984 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.2449495792388916, loss=1.392587423324585
I0229 13:56:51.444457 140089770305280 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.3115949630737305, loss=1.4001493453979492
I0229 13:57:24.900982 140089778697984 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.2478346824645996, loss=1.5015418529510498
I0229 13:57:58.341601 140089770305280 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.3302953243255615, loss=1.4083170890808105
I0229 13:58:31.830719 140089778697984 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.4321062564849854, loss=1.367686152458191
I0229 13:59:05.396613 140089770305280 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.183054208755493, loss=1.4301977157592773
I0229 13:59:38.887906 140089778697984 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.3521909713745117, loss=1.4501062631607056
I0229 14:00:12.330525 140089770305280 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3771610260009766, loss=1.3977110385894775
I0229 14:00:45.802722 140089778697984 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.2600526809692383, loss=1.41984224319458
I0229 14:01:19.283492 140089770305280 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.248368501663208, loss=1.3835185766220093
I0229 14:01:52.744413 140089778697984 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.197115421295166, loss=1.384249210357666
I0229 14:02:26.224688 140089770305280 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.5846946239471436, loss=1.4061591625213623
I0229 14:02:59.707694 140089778697984 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.1494874954223633, loss=1.4459038972854614
I0229 14:03:33.185887 140089770305280 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.176147699356079, loss=1.3693642616271973
I0229 14:04:06.646013 140089778697984 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.2478232383728027, loss=1.3542884588241577
I0229 14:04:40.143272 140089770305280 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.27492356300354, loss=1.413115382194519
I0229 14:04:47.660652 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:04:54.230654 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:05:02.535961 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:05:04.844378 140252611495744 submission_runner.py:411] Time since start: 31845.72s, 	Step: 91424, 	{'train/accuracy': 0.7586694955825806, 'train/loss': 0.9094310998916626, 'validation/accuracy': 0.6788399815559387, 'validation/loss': 1.301895022392273, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.0211398601531982, 'test/num_examples': 10000, 'score': 30655.910806179047, 'total_duration': 31845.72444677353, 'accumulated_submission_time': 30655.910806179047, 'accumulated_eval_time': 1184.3367433547974, 'accumulated_logging_time': 2.234565019607544}
I0229 14:05:04.877135 140089778697984 logging_writer.py:48] [91424] accumulated_eval_time=1184.336743, accumulated_logging_time=2.234565, accumulated_submission_time=30655.910806, global_step=91424, preemption_count=0, score=30655.910806, test/accuracy=0.554000, test/loss=2.021140, test/num_examples=10000, total_duration=31845.724447, train/accuracy=0.758669, train/loss=0.909431, validation/accuracy=0.678840, validation/loss=1.301895, validation/num_examples=50000
I0229 14:05:30.639669 140089854211840 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.312145948410034, loss=1.3398964405059814
I0229 14:06:04.130019 140089778697984 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.348377227783203, loss=1.457708477973938
I0229 14:06:37.620573 140089854211840 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.341707944869995, loss=1.5337839126586914
I0229 14:07:11.075273 140089778697984 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.2089972496032715, loss=1.309462547302246
I0229 14:07:44.537612 140089854211840 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.2437801361083984, loss=1.390181064605713
I0229 14:08:18.009536 140089778697984 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.479907274246216, loss=1.4376096725463867
I0229 14:08:51.456308 140089854211840 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.361994981765747, loss=1.364124059677124
I0229 14:09:24.938986 140089778697984 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.5013725757598877, loss=1.3942909240722656
I0229 14:09:58.381493 140089854211840 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.4234440326690674, loss=1.5281023979187012
I0229 14:10:31.858687 140089778697984 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.2043442726135254, loss=1.3249284029006958
I0229 14:11:05.435616 140089854211840 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.442079544067383, loss=1.346084475517273
I0229 14:11:38.909337 140089778697984 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.068877935409546, loss=1.3341970443725586
I0229 14:12:12.404573 140089854211840 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.665588855743408, loss=1.341803789138794
I0229 14:12:45.885458 140089778697984 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.371736764907837, loss=1.3830087184906006
I0229 14:13:19.323852 140089854211840 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.685403347015381, loss=1.306910514831543
I0229 14:13:34.847997 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:13:41.081988 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:13:49.155465 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:13:51.517344 140252611495744 submission_runner.py:411] Time since start: 32372.40s, 	Step: 92948, 	{'train/accuracy': 0.7504783272743225, 'train/loss': 0.9368448853492737, 'validation/accuracy': 0.6730200052261353, 'validation/loss': 1.3249033689498901, 'validation/num_examples': 50000, 'test/accuracy': 0.5422000288963318, 'test/loss': 2.0770034790039062, 'test/num_examples': 10000, 'score': 31165.815375089645, 'total_duration': 32372.397426128387, 'accumulated_submission_time': 31165.815375089645, 'accumulated_eval_time': 1201.0060527324677, 'accumulated_logging_time': 2.280010223388672}
I0229 14:13:51.547367 140089845819136 logging_writer.py:48] [92948] accumulated_eval_time=1201.006053, accumulated_logging_time=2.280010, accumulated_submission_time=31165.815375, global_step=92948, preemption_count=0, score=31165.815375, test/accuracy=0.542200, test/loss=2.077003, test/num_examples=10000, total_duration=32372.397426, train/accuracy=0.750478, train/loss=0.936845, validation/accuracy=0.673020, validation/loss=1.324903, validation/num_examples=50000
I0229 14:14:09.270039 140089879389952 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.4722342491149902, loss=1.343415379524231
I0229 14:14:42.734332 140089845819136 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.389120578765869, loss=1.3203554153442383
I0229 14:15:16.202013 140089879389952 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.3066861629486084, loss=1.3493521213531494
I0229 14:15:49.729750 140089845819136 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.0837674140930176, loss=1.2391326427459717
I0229 14:16:23.161847 140089879389952 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.5176351070404053, loss=1.3298413753509521
I0229 14:16:56.607303 140089845819136 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.3859798908233643, loss=1.4433403015136719
I0229 14:17:30.173960 140089879389952 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.3543660640716553, loss=1.3955235481262207
I0229 14:18:03.627188 140089845819136 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.375671148300171, loss=1.3201590776443481
I0229 14:18:37.151729 140089879389952 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.460245370864868, loss=1.4029066562652588
I0229 14:19:10.641905 140089845819136 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.5184576511383057, loss=1.301390528678894
I0229 14:19:44.106822 140089879389952 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.4620249271392822, loss=1.4638152122497559
I0229 14:20:17.576402 140089845819136 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.324270486831665, loss=1.4036238193511963
I0229 14:20:51.045564 140089879389952 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.4350950717926025, loss=1.4074914455413818
I0229 14:21:24.528300 140089845819136 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.561265230178833, loss=1.4144008159637451
I0229 14:21:58.006144 140089879389952 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.250725269317627, loss=1.3296397924423218
I0229 14:22:21.575803 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:22:27.739201 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:22:35.992506 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:22:38.235324 140252611495744 submission_runner.py:411] Time since start: 32899.12s, 	Step: 94472, 	{'train/accuracy': 0.802754282951355, 'train/loss': 0.7244368195533752, 'validation/accuracy': 0.6849600076675415, 'validation/loss': 1.2787599563598633, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.0316712856292725, 'test/num_examples': 10000, 'score': 31675.780433416367, 'total_duration': 32899.11539578438, 'accumulated_submission_time': 31675.780433416367, 'accumulated_eval_time': 1217.6655213832855, 'accumulated_logging_time': 2.319309711456299}
I0229 14:22:38.270088 140089778697984 logging_writer.py:48] [94472] accumulated_eval_time=1217.665521, accumulated_logging_time=2.319310, accumulated_submission_time=31675.780433, global_step=94472, preemption_count=0, score=31675.780433, test/accuracy=0.551200, test/loss=2.031671, test/num_examples=10000, total_duration=32899.115396, train/accuracy=0.802754, train/loss=0.724437, validation/accuracy=0.684960, validation/loss=1.278760, validation/num_examples=50000
I0229 14:22:47.972568 140089837426432 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.4200634956359863, loss=1.3437273502349854
I0229 14:23:21.557574 140089778697984 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.2726428508758545, loss=1.285687804222107
I0229 14:23:55.126669 140089837426432 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.4628398418426514, loss=1.4063611030578613
I0229 14:24:28.613479 140089778697984 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.277909278869629, loss=1.3577544689178467
I0229 14:25:02.144348 140089837426432 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.3232734203338623, loss=1.4598158597946167
I0229 14:25:35.634956 140089778697984 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.423757791519165, loss=1.3841415643692017
I0229 14:26:09.170849 140089837426432 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.275040626525879, loss=1.3838385343551636
I0229 14:26:42.615181 140089778697984 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.3671693801879883, loss=1.380569338798523
I0229 14:27:16.078264 140089837426432 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.7183475494384766, loss=1.4379072189331055
I0229 14:27:49.561381 140089778697984 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.2168428897857666, loss=1.3535317182540894
I0229 14:28:23.043395 140089837426432 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.2617175579071045, loss=1.5149818658828735
I0229 14:28:56.498147 140089778697984 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.6311538219451904, loss=1.3702586889266968
I0229 14:29:30.037427 140089837426432 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.413661241531372, loss=1.3596279621124268
I0229 14:30:03.517117 140089778697984 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.6226203441619873, loss=1.4265515804290771
I0229 14:30:36.991210 140089837426432 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.303386688232422, loss=1.2952735424041748
I0229 14:31:08.562478 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:31:14.612275 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:31:22.898472 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:31:25.161589 140252611495744 submission_runner.py:411] Time since start: 33426.04s, 	Step: 95996, 	{'train/accuracy': 0.7570551633834839, 'train/loss': 0.9185478091239929, 'validation/accuracy': 0.6616399884223938, 'validation/loss': 1.392592191696167, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.1422789096832275, 'test/num_examples': 10000, 'score': 32186.006425857544, 'total_duration': 33426.04164767265, 'accumulated_submission_time': 32186.006425857544, 'accumulated_eval_time': 1234.2645723819733, 'accumulated_logging_time': 2.3646020889282227}
I0229 14:31:25.210078 140089552271104 logging_writer.py:48] [95996] accumulated_eval_time=1234.264572, accumulated_logging_time=2.364602, accumulated_submission_time=32186.006426, global_step=95996, preemption_count=0, score=32186.006426, test/accuracy=0.530500, test/loss=2.142279, test/num_examples=10000, total_duration=33426.041648, train/accuracy=0.757055, train/loss=0.918548, validation/accuracy=0.661640, validation/loss=1.392592, validation/num_examples=50000
I0229 14:31:26.897952 140089770305280 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.7047407627105713, loss=1.3603882789611816
I0229 14:32:00.354609 140089552271104 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.3585710525512695, loss=1.3503884077072144
I0229 14:32:33.814491 140089770305280 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.576258897781372, loss=1.3199007511138916
I0229 14:33:07.293196 140089552271104 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.6152572631835938, loss=1.375880479812622
I0229 14:33:40.738403 140089770305280 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.2972753047943115, loss=1.3964674472808838
I0229 14:34:14.238897 140089552271104 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.7380313873291016, loss=1.3756991624832153
I0229 14:34:47.748831 140089770305280 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.2254421710968018, loss=1.293269157409668
I0229 14:35:21.338333 140089552271104 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.4240453243255615, loss=1.4251617193222046
I0229 14:35:54.812605 140089770305280 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.2784745693206787, loss=1.3496605157852173
I0229 14:36:28.272805 140089552271104 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.503392457962036, loss=1.3319729566574097
I0229 14:37:01.725682 140089770305280 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.2359797954559326, loss=1.3509305715560913
I0229 14:37:35.229337 140089552271104 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3849375247955322, loss=1.3281251192092896
I0229 14:38:08.705847 140089770305280 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.6185529232025146, loss=1.455104947090149
I0229 14:38:42.178832 140089552271104 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.4980194568634033, loss=1.3357537984848022
I0229 14:39:15.644452 140089770305280 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.5637731552124023, loss=1.3661911487579346
I0229 14:39:49.133289 140089552271104 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.513373374938965, loss=1.4105042219161987
I0229 14:39:55.299524 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:40:01.479138 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:40:09.883790 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:40:12.154174 140252611495744 submission_runner.py:411] Time since start: 33953.03s, 	Step: 97520, 	{'train/accuracy': 0.7798349857330322, 'train/loss': 0.8255358338356018, 'validation/accuracy': 0.6879799962043762, 'validation/loss': 1.2594505548477173, 'validation/num_examples': 50000, 'test/accuracy': 0.5635000467300415, 'test/loss': 1.9730379581451416, 'test/num_examples': 10000, 'score': 32696.02758693695, 'total_duration': 33953.034247636795, 'accumulated_submission_time': 32696.02758693695, 'accumulated_eval_time': 1251.1191718578339, 'accumulated_logging_time': 2.4251294136047363}
I0229 14:40:12.189608 140089862604544 logging_writer.py:48] [97520] accumulated_eval_time=1251.119172, accumulated_logging_time=2.425129, accumulated_submission_time=32696.027587, global_step=97520, preemption_count=0, score=32696.027587, test/accuracy=0.563500, test/loss=1.973038, test/num_examples=10000, total_duration=33953.034248, train/accuracy=0.779835, train/loss=0.825536, validation/accuracy=0.687980, validation/loss=1.259451, validation/num_examples=50000
I0229 14:40:39.288992 140089870997248 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.346482992172241, loss=1.4144439697265625
I0229 14:41:12.743355 140089862604544 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.450268268585205, loss=1.351311445236206
I0229 14:41:46.284159 140089870997248 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.309173345565796, loss=1.303192377090454
I0229 14:42:19.760558 140089862604544 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.296367883682251, loss=1.3431509733200073
I0229 14:42:53.241661 140089870997248 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.4529001712799072, loss=1.2341221570968628
I0229 14:43:26.665344 140089862604544 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.8364694118499756, loss=1.4397668838500977
I0229 14:44:00.142336 140089870997248 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.466456651687622, loss=1.4074562788009644
I0229 14:44:33.610574 140089862604544 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.4557137489318848, loss=1.3893071413040161
I0229 14:45:07.069465 140089870997248 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.317491054534912, loss=1.385970950126648
I0229 14:45:40.523818 140089862604544 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.4172818660736084, loss=1.3328288793563843
I0229 14:46:14.014111 140089870997248 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.5152859687805176, loss=1.3644087314605713
I0229 14:46:47.462299 140089862604544 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.5601608753204346, loss=1.350826382637024
I0229 14:47:20.911978 140089870997248 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.3892362117767334, loss=1.3929625749588013
I0229 14:47:54.450067 140089862604544 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.377952814102173, loss=1.3833848237991333
I0229 14:48:27.916791 140089870997248 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.6764044761657715, loss=1.4238327741622925
I0229 14:48:42.439626 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:48:48.561681 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:48:56.807689 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:48:59.066503 140252611495744 submission_runner.py:411] Time since start: 34479.95s, 	Step: 99045, 	{'train/accuracy': 0.7669602632522583, 'train/loss': 0.872205913066864, 'validation/accuracy': 0.6832599639892578, 'validation/loss': 1.2858682870864868, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.031160354614258, 'test/num_examples': 10000, 'score': 33206.21092581749, 'total_duration': 34479.94657087326, 'accumulated_submission_time': 33206.21092581749, 'accumulated_eval_time': 1267.7459924221039, 'accumulated_logging_time': 2.4731388092041016}
I0229 14:48:59.100064 140089837426432 logging_writer.py:48] [99045] accumulated_eval_time=1267.745992, accumulated_logging_time=2.473139, accumulated_submission_time=33206.210926, global_step=99045, preemption_count=0, score=33206.210926, test/accuracy=0.549000, test/loss=2.031160, test/num_examples=10000, total_duration=34479.946571, train/accuracy=0.766960, train/loss=0.872206, validation/accuracy=0.683260, validation/loss=1.285868, validation/num_examples=50000
I0229 14:49:17.865539 140089845819136 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.6183300018310547, loss=1.3520104885101318
I0229 14:49:51.372502 140089837426432 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.5198071002960205, loss=1.296339750289917
I0229 14:50:24.845159 140089845819136 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.4853219985961914, loss=1.3535778522491455
I0229 14:50:58.303301 140089837426432 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.358635187149048, loss=1.3814512491226196
I0229 14:51:31.764394 140089845819136 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.4688565731048584, loss=1.4105746746063232
I0229 14:52:05.211716 140089837426432 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.2079286575317383, loss=1.3073385953903198
I0229 14:52:38.651413 140089845819136 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.418243408203125, loss=1.2659820318222046
I0229 14:53:12.158122 140089837426432 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.74938702583313, loss=1.3226134777069092
I0229 14:53:45.656484 140089845819136 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.7476024627685547, loss=1.3800400495529175
I0229 14:54:19.163470 140089837426432 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.4549944400787354, loss=1.3769499063491821
I0229 14:54:52.583957 140089845819136 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.2627782821655273, loss=1.2514395713806152
I0229 14:55:26.056123 140089837426432 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.2892532348632812, loss=1.2953884601593018
I0229 14:55:59.577464 140089845819136 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.5240490436553955, loss=1.31428861618042
I0229 14:56:33.019271 140089837426432 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4633052349090576, loss=1.433132529258728
I0229 14:57:06.480960 140089845819136 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.452427387237549, loss=1.3009377717971802
I0229 14:57:29.086742 140252611495744 spec.py:321] Evaluating on the training split.
I0229 14:57:35.152470 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 14:57:43.418417 140252611495744 spec.py:349] Evaluating on the test split.
I0229 14:57:45.686992 140252611495744 submission_runner.py:411] Time since start: 35006.57s, 	Step: 100569, 	{'train/accuracy': 0.7652662396430969, 'train/loss': 0.8769233822822571, 'validation/accuracy': 0.6861599683761597, 'validation/loss': 1.2804961204528809, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.010899305343628, 'test/num_examples': 10000, 'score': 33716.131663799286, 'total_duration': 35006.567068099976, 'accumulated_submission_time': 33716.131663799286, 'accumulated_eval_time': 1284.3461983203888, 'accumulated_logging_time': 2.5171451568603516}
I0229 14:57:45.725868 140089778697984 logging_writer.py:48] [100569] accumulated_eval_time=1284.346198, accumulated_logging_time=2.517145, accumulated_submission_time=33716.131664, global_step=100569, preemption_count=0, score=33716.131664, test/accuracy=0.561800, test/loss=2.010899, test/num_examples=10000, total_duration=35006.567068, train/accuracy=0.765266, train/loss=0.876923, validation/accuracy=0.686160, validation/loss=1.280496, validation/num_examples=50000
I0229 14:57:56.430985 140089837426432 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.484189510345459, loss=1.3138154745101929
I0229 14:58:29.908139 140089778697984 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.4204537868499756, loss=1.2933318614959717
I0229 14:59:03.388182 140089837426432 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.619624376296997, loss=1.2887029647827148
I0229 14:59:36.939425 140089778697984 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.752721071243286, loss=1.3528780937194824
I0229 15:00:10.376761 140089837426432 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.6838760375976562, loss=1.4478418827056885
I0229 15:00:43.820768 140089778697984 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.8173563480377197, loss=1.3418558835983276
I0229 15:01:17.274034 140089837426432 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.435828685760498, loss=1.246237874031067
I0229 15:01:50.704444 140089778697984 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.3714191913604736, loss=1.3562564849853516
I0229 15:02:24.170950 140089837426432 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.6164817810058594, loss=1.3838088512420654
I0229 15:02:57.651474 140089778697984 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.7359211444854736, loss=1.402346134185791
I0229 15:03:31.088603 140089837426432 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.6411917209625244, loss=1.3513157367706299
I0229 15:04:04.577336 140089778697984 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.5488853454589844, loss=1.3398244380950928
I0229 15:04:38.008806 140089837426432 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.5624709129333496, loss=1.3926706314086914
I0229 15:05:11.460982 140089778697984 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.512133836746216, loss=1.4006223678588867
I0229 15:05:44.989664 140089837426432 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.2102978229522705, loss=1.360600233078003
I0229 15:06:15.911696 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:06:22.113881 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:06:30.364684 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:06:32.627022 140252611495744 submission_runner.py:411] Time since start: 35533.51s, 	Step: 102094, 	{'train/accuracy': 0.7729990482330322, 'train/loss': 0.8372607827186584, 'validation/accuracy': 0.6893799901008606, 'validation/loss': 1.2580443620681763, 'validation/num_examples': 50000, 'test/accuracy': 0.5538000464439392, 'test/loss': 1.9944852590560913, 'test/num_examples': 10000, 'score': 34226.252163648605, 'total_duration': 35533.50708389282, 'accumulated_submission_time': 34226.252163648605, 'accumulated_eval_time': 1301.0614657402039, 'accumulated_logging_time': 2.5662214756011963}
I0229 15:06:32.667057 140089845819136 logging_writer.py:48] [102094] accumulated_eval_time=1301.061466, accumulated_logging_time=2.566221, accumulated_submission_time=34226.252164, global_step=102094, preemption_count=0, score=34226.252164, test/accuracy=0.553800, test/loss=1.994485, test/num_examples=10000, total_duration=35533.507084, train/accuracy=0.772999, train/loss=0.837261, validation/accuracy=0.689380, validation/loss=1.258044, validation/num_examples=50000
I0229 15:06:35.016564 140089854211840 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.550586223602295, loss=1.3086135387420654
I0229 15:07:08.486305 140089845819136 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.425067901611328, loss=1.3617100715637207
I0229 15:07:41.989459 140089854211840 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.543795585632324, loss=1.2561390399932861
I0229 15:08:15.415741 140089845819136 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.3888893127441406, loss=1.2744524478912354
I0229 15:08:48.891247 140089854211840 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.438934087753296, loss=1.177799940109253
I0229 15:09:22.369833 140089845819136 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.3313148021698, loss=1.334080457687378
I0229 15:09:55.814642 140089854211840 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.6642918586730957, loss=1.4836125373840332
I0229 15:10:29.282508 140089845819136 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.5755419731140137, loss=1.423812747001648
I0229 15:11:02.716712 140089854211840 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.41740345954895, loss=1.4432592391967773
I0229 15:11:36.265182 140089845819136 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.689821720123291, loss=1.3624008893966675
I0229 15:12:09.728722 140089854211840 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.694438934326172, loss=1.3795738220214844
I0229 15:12:43.207369 140089845819136 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.508279323577881, loss=1.3130100965499878
I0229 15:13:16.649374 140089854211840 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.4295620918273926, loss=1.3068280220031738
I0229 15:13:50.113365 140089845819136 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.50966739654541, loss=1.3424656391143799
I0229 15:14:23.572014 140089854211840 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.570951223373413, loss=1.2640568017959595
I0229 15:14:57.012055 140089845819136 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.718698740005493, loss=1.3542065620422363
I0229 15:15:02.848051 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:15:08.971348 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:15:17.161509 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:15:19.463625 140252611495744 submission_runner.py:411] Time since start: 36060.34s, 	Step: 103619, 	{'train/accuracy': 0.7914739847183228, 'train/loss': 0.7607530951499939, 'validation/accuracy': 0.6838200092315674, 'validation/loss': 1.2859368324279785, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.011260747909546, 'test/num_examples': 10000, 'score': 34736.3680062294, 'total_duration': 36060.34365081787, 'accumulated_submission_time': 34736.3680062294, 'accumulated_eval_time': 1317.6769466400146, 'accumulated_logging_time': 2.6166603565216064}
I0229 15:15:19.497943 140089870997248 logging_writer.py:48] [103619] accumulated_eval_time=1317.676947, accumulated_logging_time=2.616660, accumulated_submission_time=34736.368006, global_step=103619, preemption_count=0, score=34736.368006, test/accuracy=0.558200, test/loss=2.011261, test/num_examples=10000, total_duration=36060.343651, train/accuracy=0.791474, train/loss=0.760753, validation/accuracy=0.683820, validation/loss=1.285937, validation/num_examples=50000
I0229 15:15:46.991559 140089879389952 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.5587282180786133, loss=1.3032212257385254
I0229 15:16:20.452702 140089870997248 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.631920576095581, loss=1.4552841186523438
I0229 15:16:53.897068 140089879389952 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.6749353408813477, loss=1.3027490377426147
I0229 15:17:27.331592 140089870997248 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.618276834487915, loss=1.3359923362731934
I0229 15:18:00.872606 140089879389952 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.7286219596862793, loss=1.3604036569595337
I0229 15:18:34.356116 140089870997248 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.4837188720703125, loss=1.2409371137619019
I0229 15:19:07.807261 140089879389952 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.601109027862549, loss=1.3262615203857422
I0229 15:19:41.230186 140089870997248 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.6052019596099854, loss=1.4392372369766235
I0229 15:20:14.690293 140089879389952 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.630298376083374, loss=1.4289497137069702
I0229 15:20:48.171701 140089870997248 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.4367761611938477, loss=1.318718671798706
I0229 15:21:21.630433 140089879389952 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.4699037075042725, loss=1.3052597045898438
I0229 15:21:55.084562 140089870997248 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.626378297805786, loss=1.290348768234253
I0229 15:22:28.540764 140089879389952 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.6950230598449707, loss=1.281594157218933
I0229 15:23:01.992585 140089870997248 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.501366138458252, loss=1.3256421089172363
I0229 15:23:35.456482 140089879389952 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.422154188156128, loss=1.3006647825241089
I0229 15:23:49.727213 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:23:55.842485 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:24:04.147850 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:24:06.432071 140252611495744 submission_runner.py:411] Time since start: 36587.31s, 	Step: 105144, 	{'train/accuracy': 0.7905572056770325, 'train/loss': 0.7790660858154297, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.2633452415466309, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 1.953182339668274, 'test/num_examples': 10000, 'score': 35246.5287566185, 'total_duration': 36587.31213951111, 'accumulated_submission_time': 35246.5287566185, 'accumulated_eval_time': 1334.381745815277, 'accumulated_logging_time': 2.665559768676758}
I0229 15:24:06.469423 140089552271104 logging_writer.py:48] [105144] accumulated_eval_time=1334.381746, accumulated_logging_time=2.665560, accumulated_submission_time=35246.528757, global_step=105144, preemption_count=0, score=35246.528757, test/accuracy=0.565000, test/loss=1.953182, test/num_examples=10000, total_duration=36587.312140, train/accuracy=0.790557, train/loss=0.779066, validation/accuracy=0.690420, validation/loss=1.263345, validation/num_examples=50000
I0229 15:24:25.577927 140089770305280 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.5473837852478027, loss=1.3006300926208496
I0229 15:24:59.042583 140089552271104 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.6752657890319824, loss=1.2374706268310547
I0229 15:25:32.503297 140089770305280 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.936234474182129, loss=1.2840378284454346
I0229 15:26:05.941860 140089552271104 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.537630319595337, loss=1.246580958366394
I0229 15:26:39.382777 140089770305280 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.4351489543914795, loss=1.3685052394866943
I0229 15:27:12.829667 140089552271104 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.4672656059265137, loss=1.2914190292358398
I0229 15:27:46.274114 140089770305280 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.551831007003784, loss=1.3166383504867554
I0229 15:28:19.710836 140089552271104 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.4357693195343018, loss=1.264467716217041
I0229 15:28:53.155559 140089770305280 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.6078054904937744, loss=1.2648464441299438
I0229 15:29:26.594389 140089552271104 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.4387009143829346, loss=1.3796964883804321
I0229 15:30:00.229103 140089770305280 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.4682083129882812, loss=1.284914255142212
I0229 15:30:33.665505 140089552271104 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.5840182304382324, loss=1.2349449396133423
I0229 15:31:07.133515 140089770305280 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.817444324493408, loss=1.2812068462371826
I0229 15:31:40.587603 140089552271104 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.4887876510620117, loss=1.189316749572754
I0229 15:32:14.041765 140089770305280 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.4825375080108643, loss=1.2836105823516846
I0229 15:32:36.594184 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:32:42.738426 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:32:51.034717 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:32:53.331687 140252611495744 submission_runner.py:411] Time since start: 37114.21s, 	Step: 106669, 	{'train/accuracy': 0.7826650142669678, 'train/loss': 0.8016412258148193, 'validation/accuracy': 0.6915199756622314, 'validation/loss': 1.2654014825820923, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.0167007446289062, 'test/num_examples': 10000, 'score': 35756.58667945862, 'total_duration': 37114.21175909042, 'accumulated_submission_time': 35756.58667945862, 'accumulated_eval_time': 1351.1192100048065, 'accumulated_logging_time': 2.7153921127319336}
I0229 15:32:53.369510 140089770305280 logging_writer.py:48] [106669] accumulated_eval_time=1351.119210, accumulated_logging_time=2.715392, accumulated_submission_time=35756.586679, global_step=106669, preemption_count=0, score=35756.586679, test/accuracy=0.559900, test/loss=2.016701, test/num_examples=10000, total_duration=37114.211759, train/accuracy=0.782665, train/loss=0.801641, validation/accuracy=0.691520, validation/loss=1.265401, validation/num_examples=50000
I0229 15:33:04.111573 140089845819136 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.6680679321289062, loss=1.288732886314392
I0229 15:33:37.547841 140089770305280 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.579934597015381, loss=1.185776948928833
I0229 15:34:10.987489 140089845819136 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.673964023590088, loss=1.275782585144043
I0229 15:34:44.447937 140089770305280 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.524467945098877, loss=1.2957357168197632
I0229 15:35:17.955559 140089845819136 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6932156085968018, loss=1.3846505880355835
I0229 15:35:51.411051 140089770305280 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.9688737392425537, loss=1.340918779373169
I0229 15:36:24.957640 140089845819136 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.7658238410949707, loss=1.4112191200256348
I0229 15:36:58.427471 140089770305280 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.8124637603759766, loss=1.2168642282485962
I0229 15:37:31.873786 140089845819136 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.5463383197784424, loss=1.3424768447875977
I0229 15:38:05.342910 140089770305280 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.3050482273101807, loss=1.2177766561508179
I0229 15:38:38.858109 140089845819136 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.987417697906494, loss=1.3309340476989746
I0229 15:39:12.288748 140089770305280 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.701812982559204, loss=1.308144211769104
I0229 15:39:45.730687 140089845819136 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.7267842292785645, loss=1.2408015727996826
I0229 15:40:19.185621 140089770305280 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.668956995010376, loss=1.328680157661438
I0229 15:40:52.679213 140089845819136 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.533820629119873, loss=1.367365837097168
I0229 15:41:23.587106 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:41:29.749638 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:41:37.982270 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:41:40.246449 140252611495744 submission_runner.py:411] Time since start: 37641.13s, 	Step: 108194, 	{'train/accuracy': 0.7798748016357422, 'train/loss': 0.8180897831916809, 'validation/accuracy': 0.6882199645042419, 'validation/loss': 1.2640862464904785, 'validation/num_examples': 50000, 'test/accuracy': 0.5564000010490417, 'test/loss': 2.0124053955078125, 'test/num_examples': 10000, 'score': 36266.73639631271, 'total_duration': 37641.12651062012, 'accumulated_submission_time': 36266.73639631271, 'accumulated_eval_time': 1367.7784917354584, 'accumulated_logging_time': 2.7660505771636963}
I0229 15:41:40.286903 140089837426432 logging_writer.py:48] [108194] accumulated_eval_time=1367.778492, accumulated_logging_time=2.766051, accumulated_submission_time=36266.736396, global_step=108194, preemption_count=0, score=36266.736396, test/accuracy=0.556400, test/loss=2.012405, test/num_examples=10000, total_duration=37641.126511, train/accuracy=0.779875, train/loss=0.818090, validation/accuracy=0.688220, validation/loss=1.264086, validation/num_examples=50000
I0229 15:41:42.644298 140089870997248 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.6148416996002197, loss=1.3619906902313232
I0229 15:42:16.215459 140089837426432 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.55967116355896, loss=1.3095483779907227
I0229 15:42:49.689846 140089870997248 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.473844289779663, loss=1.252547264099121
I0229 15:43:23.137101 140089837426432 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.611690044403076, loss=1.2302923202514648
I0229 15:43:56.591484 140089870997248 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.6755759716033936, loss=1.3267581462860107
I0229 15:44:30.058264 140089837426432 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.456310510635376, loss=1.2345014810562134
I0229 15:45:03.531749 140089870997248 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.7809176445007324, loss=1.3582439422607422
I0229 15:45:36.968753 140089837426432 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.6739704608917236, loss=1.2798776626586914
I0229 15:46:10.403121 140089870997248 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.659179210662842, loss=1.2236039638519287
I0229 15:46:43.882017 140089837426432 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.728269338607788, loss=1.3477321863174438
I0229 15:47:17.337628 140089870997248 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6418540477752686, loss=1.1666951179504395
I0229 15:47:50.781205 140089837426432 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.8823864459991455, loss=1.2669774293899536
I0229 15:48:24.310130 140089870997248 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.645940065383911, loss=1.2819252014160156
I0229 15:48:57.796620 140089837426432 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.705589532852173, loss=1.2507171630859375
I0229 15:49:31.236216 140089870997248 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.6869494915008545, loss=1.3121812343597412
I0229 15:50:04.694331 140089837426432 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.8237287998199463, loss=1.2582495212554932
I0229 15:50:10.521846 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:50:16.615972 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:50:24.760890 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:50:27.064920 140252611495744 submission_runner.py:411] Time since start: 38167.94s, 	Step: 109719, 	{'train/accuracy': 0.786531388759613, 'train/loss': 0.7900282740592957, 'validation/accuracy': 0.6940799951553345, 'validation/loss': 1.2394052743911743, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 1.9760229587554932, 'test/num_examples': 10000, 'score': 36776.9069879055, 'total_duration': 38167.94488573074, 'accumulated_submission_time': 36776.9069879055, 'accumulated_eval_time': 1384.3214082717896, 'accumulated_logging_time': 2.8171417713165283}
I0229 15:50:27.099735 140089845819136 logging_writer.py:48] [109719] accumulated_eval_time=1384.321408, accumulated_logging_time=2.817142, accumulated_submission_time=36776.906988, global_step=109719, preemption_count=0, score=36776.906988, test/accuracy=0.562200, test/loss=1.976023, test/num_examples=10000, total_duration=38167.944886, train/accuracy=0.786531, train/loss=0.790028, validation/accuracy=0.694080, validation/loss=1.239405, validation/num_examples=50000
I0229 15:50:54.528063 140089854211840 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.78143310546875, loss=1.2774407863616943
I0229 15:51:27.971396 140089845819136 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.718804359436035, loss=1.323683738708496
I0229 15:52:01.413214 140089854211840 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.6314380168914795, loss=1.2895348072052002
I0229 15:52:34.874197 140089845819136 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.502591371536255, loss=1.1800150871276855
I0229 15:53:08.365517 140089854211840 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.755021572113037, loss=1.2320044040679932
I0229 15:53:41.819367 140089845819136 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.617337942123413, loss=1.268488883972168
I0229 15:54:15.412708 140089854211840 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.6203365325927734, loss=1.291140079498291
I0229 15:54:48.844013 140089845819136 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.710765838623047, loss=1.2590854167938232
I0229 15:55:22.298705 140089854211840 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.5797598361968994, loss=1.27191162109375
I0229 15:55:55.737156 140089845819136 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.812248706817627, loss=1.2689158916473389
I0229 15:56:29.191588 140089854211840 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.7305550575256348, loss=1.2492830753326416
I0229 15:57:02.671658 140089845819136 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.6214189529418945, loss=1.2495746612548828
I0229 15:57:36.132273 140089854211840 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.875999927520752, loss=1.3184257745742798
I0229 15:58:09.595610 140089845819136 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.956787109375, loss=1.3575068712234497
I0229 15:58:43.076078 140089854211840 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.692253589630127, loss=1.1969398260116577
I0229 15:58:57.289917 140252611495744 spec.py:321] Evaluating on the training split.
I0229 15:59:03.582777 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 15:59:11.817775 140252611495744 spec.py:349] Evaluating on the test split.
I0229 15:59:14.141188 140252611495744 submission_runner.py:411] Time since start: 38695.02s, 	Step: 111244, 	{'train/accuracy': 0.8427534699440002, 'train/loss': 0.5710492134094238, 'validation/accuracy': 0.6987000107765198, 'validation/loss': 1.2164056301116943, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9428856372833252, 'test/num_examples': 10000, 'score': 37287.03126382828, 'total_duration': 38695.021256923676, 'accumulated_submission_time': 37287.03126382828, 'accumulated_eval_time': 1401.1726398468018, 'accumulated_logging_time': 2.862706422805786}
I0229 15:59:14.179686 140089778697984 logging_writer.py:48] [111244] accumulated_eval_time=1401.172640, accumulated_logging_time=2.862706, accumulated_submission_time=37287.031264, global_step=111244, preemption_count=0, score=37287.031264, test/accuracy=0.574900, test/loss=1.942886, test/num_examples=10000, total_duration=38695.021257, train/accuracy=0.842753, train/loss=0.571049, validation/accuracy=0.698700, validation/loss=1.216406, validation/num_examples=50000
I0229 15:59:33.280982 140089837426432 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.61578631401062, loss=1.1233539581298828
I0229 16:00:06.714162 140089778697984 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.6426544189453125, loss=1.2774971723556519
I0229 16:00:40.283339 140089837426432 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.9142367839813232, loss=1.295201301574707
I0229 16:01:13.764912 140089778697984 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.724013566970825, loss=1.3447598218917847
I0229 16:01:47.225935 140089837426432 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.6396663188934326, loss=1.1783608198165894
I0229 16:02:20.697557 140089778697984 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.6884617805480957, loss=1.2690255641937256
I0229 16:02:54.147784 140089837426432 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.554429769515991, loss=1.247800350189209
I0229 16:03:27.622575 140089778697984 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.3845863342285156, loss=1.283794641494751
I0229 16:04:01.064108 140089837426432 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.491729736328125, loss=1.291471004486084
I0229 16:04:34.539716 140089778697984 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.5391952991485596, loss=1.2451868057250977
I0229 16:05:07.982945 140089837426432 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.9843106269836426, loss=1.265503168106079
I0229 16:05:41.440841 140089778697984 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.8884522914886475, loss=1.3226110935211182
I0229 16:06:14.932169 140089837426432 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.7337605953216553, loss=1.2775282859802246
I0229 16:06:48.507020 140089778697984 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.6557748317718506, loss=1.2334444522857666
I0229 16:07:21.979406 140089837426432 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.774240732192993, loss=1.227007269859314
I0229 16:07:44.241667 140252611495744 spec.py:321] Evaluating on the training split.
I0229 16:07:50.334044 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 16:07:58.601625 140252611495744 spec.py:349] Evaluating on the test split.
I0229 16:08:00.897043 140252611495744 submission_runner.py:411] Time since start: 39221.78s, 	Step: 112768, 	{'train/accuracy': 0.812898576259613, 'train/loss': 0.6848605871200562, 'validation/accuracy': 0.698419988155365, 'validation/loss': 1.2241036891937256, 'validation/num_examples': 50000, 'test/accuracy': 0.5710000395774841, 'test/loss': 1.9627013206481934, 'test/num_examples': 10000, 'score': 37797.02880716324, 'total_duration': 39221.77711343765, 'accumulated_submission_time': 37797.02880716324, 'accumulated_eval_time': 1417.827962398529, 'accumulated_logging_time': 2.9112823009490967}
I0229 16:08:00.934813 140089845819136 logging_writer.py:48] [112768] accumulated_eval_time=1417.827962, accumulated_logging_time=2.911282, accumulated_submission_time=37797.028807, global_step=112768, preemption_count=0, score=37797.028807, test/accuracy=0.571000, test/loss=1.962701, test/num_examples=10000, total_duration=39221.777113, train/accuracy=0.812899, train/loss=0.684861, validation/accuracy=0.698420, validation/loss=1.224104, validation/num_examples=50000
I0229 16:08:11.982129 140089854211840 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.916571617126465, loss=1.2010560035705566
I0229 16:08:45.448301 140089845819136 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.7433948516845703, loss=1.2897189855575562
I0229 16:09:18.899636 140089854211840 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.696532964706421, loss=1.1892406940460205
I0229 16:09:52.365405 140089845819136 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.7887370586395264, loss=1.2296910285949707
I0229 16:10:25.909302 140089854211840 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.6861064434051514, loss=1.2559832334518433
I0229 16:10:59.391791 140089845819136 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.6506242752075195, loss=1.2407811880111694
I0229 16:11:32.813576 140089854211840 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.833324670791626, loss=1.2746942043304443
I0229 16:12:06.277133 140089845819136 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.8559179306030273, loss=1.304240107536316
I0229 16:12:39.869530 140089854211840 logging_writer.py:48] [113600] global_step=113600, grad_norm=3.1761505603790283, loss=1.2566193342208862
I0229 16:13:13.315830 140089845819136 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.6678717136383057, loss=1.2745909690856934
I0229 16:13:46.794483 140089854211840 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.7472808361053467, loss=1.1878551244735718
I0229 16:14:20.210963 140089845819136 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.8214967250823975, loss=1.238128423690796
I0229 16:14:53.731541 140089854211840 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.564436435699463, loss=1.2778184413909912
I0229 16:15:27.203590 140089845819136 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.773310899734497, loss=1.2205383777618408
I0229 16:16:00.632933 140089854211840 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.7927956581115723, loss=1.202221155166626
I0229 16:16:31.199499 140252611495744 spec.py:321] Evaluating on the training split.
I0229 16:16:37.286167 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 16:16:45.451822 140252611495744 spec.py:349] Evaluating on the test split.
I0229 16:16:47.748935 140252611495744 submission_runner.py:411] Time since start: 39748.63s, 	Step: 114293, 	{'train/accuracy': 0.8019969463348389, 'train/loss': 0.7204362154006958, 'validation/accuracy': 0.7006999850273132, 'validation/loss': 1.2203973531723022, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.921513319015503, 'test/num_examples': 10000, 'score': 38307.227852106094, 'total_duration': 39748.628996133804, 'accumulated_submission_time': 38307.227852106094, 'accumulated_eval_time': 1434.3773369789124, 'accumulated_logging_time': 2.959364175796509}
I0229 16:16:47.787856 140089778697984 logging_writer.py:48] [114293] accumulated_eval_time=1434.377337, accumulated_logging_time=2.959364, accumulated_submission_time=38307.227852, global_step=114293, preemption_count=0, score=38307.227852, test/accuracy=0.574900, test/loss=1.921513, test/num_examples=10000, total_duration=39748.628996, train/accuracy=0.801997, train/loss=0.720436, validation/accuracy=0.700700, validation/loss=1.220397, validation/num_examples=50000
I0229 16:16:50.495593 140089837426432 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.7219583988189697, loss=1.1743828058242798
I0229 16:17:23.909770 140089778697984 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.893357515335083, loss=1.30635404586792
I0229 16:17:57.381860 140089837426432 logging_writer.py:48] [114500] global_step=114500, grad_norm=3.021603584289551, loss=1.2659573554992676
I0229 16:18:30.971577 140089778697984 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.751325845718384, loss=1.2871770858764648
I0229 16:19:04.417203 140089837426432 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.680575132369995, loss=1.2087147235870361
I0229 16:19:37.881874 140089778697984 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.83945369720459, loss=1.2276564836502075
I0229 16:20:11.306974 140089837426432 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.7900352478027344, loss=1.2263338565826416
I0229 16:20:44.767940 140089778697984 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.653792142868042, loss=1.2874445915222168
I0229 16:21:18.260612 140089837426432 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.822665214538574, loss=1.2455534934997559
I0229 16:21:51.675639 140089778697984 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.6061153411865234, loss=1.1432514190673828
I0229 16:22:25.155560 140089837426432 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.6517984867095947, loss=1.2310094833374023
I0229 16:22:58.612932 140089778697984 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.880077600479126, loss=1.2191847562789917
I0229 16:23:32.075970 140089837426432 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.8685004711151123, loss=1.2531129121780396
I0229 16:24:05.511956 140089778697984 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.9794647693634033, loss=1.12898588180542
I0229 16:24:39.104317 140089837426432 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.941676139831543, loss=1.2563538551330566
I0229 16:25:12.538092 140089778697984 logging_writer.py:48] [115800] global_step=115800, grad_norm=3.1361920833587646, loss=1.2894724607467651
I0229 16:25:18.035684 140252611495744 spec.py:321] Evaluating on the training split.
I0229 16:25:24.193751 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 16:25:32.319019 140252611495744 spec.py:349] Evaluating on the test split.
I0229 16:25:34.613986 140252611495744 submission_runner.py:411] Time since start: 40275.49s, 	Step: 115818, 	{'train/accuracy': 0.8029735088348389, 'train/loss': 0.7088333964347839, 'validation/accuracy': 0.7026199698448181, 'validation/loss': 1.2160744667053223, 'validation/num_examples': 50000, 'test/accuracy': 0.5778000354766846, 'test/loss': 1.939614176750183, 'test/num_examples': 10000, 'score': 38817.41033864021, 'total_duration': 40275.49406194687, 'accumulated_submission_time': 38817.41033864021, 'accumulated_eval_time': 1450.955587387085, 'accumulated_logging_time': 3.0082755088806152}
I0229 16:25:34.652038 140089770305280 logging_writer.py:48] [115818] accumulated_eval_time=1450.955587, accumulated_logging_time=3.008276, accumulated_submission_time=38817.410339, global_step=115818, preemption_count=0, score=38817.410339, test/accuracy=0.577800, test/loss=1.939614, test/num_examples=10000, total_duration=40275.494062, train/accuracy=0.802974, train/loss=0.708833, validation/accuracy=0.702620, validation/loss=1.216074, validation/num_examples=50000
I0229 16:26:02.427666 140089778697984 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.888949394226074, loss=1.239790916442871
I0229 16:26:35.952853 140089770305280 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.7782950401306152, loss=1.238366961479187
I0229 16:27:09.381401 140089778697984 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.6647698879241943, loss=1.086025595664978
I0229 16:27:42.843316 140089770305280 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.807542085647583, loss=1.280300259590149
I0229 16:28:16.283870 140089778697984 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.819132089614868, loss=1.1936237812042236
I0229 16:28:49.725756 140089770305280 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.7724459171295166, loss=1.2225995063781738
I0229 16:29:23.189198 140089778697984 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.9520063400268555, loss=1.2726423740386963
I0229 16:29:56.625949 140089770305280 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.8252739906311035, loss=1.198124647140503
I0229 16:30:30.107969 140089778697984 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.862431526184082, loss=1.3086912631988525
I0229 16:31:03.635521 140089770305280 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.879209518432617, loss=1.1988686323165894
I0229 16:31:37.104558 140089778697984 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.8682777881622314, loss=1.1968415975570679
I0229 16:32:10.570035 140089770305280 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9096717834472656, loss=1.2540310621261597
I0229 16:32:44.014927 140089778697984 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.9479501247406006, loss=1.163856863975525
I0229 16:33:17.481579 140089770305280 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.013674259185791, loss=1.1425344944000244
I0229 16:33:50.914006 140089778697984 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.850245475769043, loss=1.2135465145111084
I0229 16:34:04.785795 140252611495744 spec.py:321] Evaluating on the training split.
I0229 16:34:10.836006 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 16:34:19.006080 140252611495744 spec.py:349] Evaluating on the test split.
I0229 16:34:21.289981 140252611495744 submission_runner.py:411] Time since start: 40802.17s, 	Step: 117343, 	{'train/accuracy': 0.7983697056770325, 'train/loss': 0.7344706654548645, 'validation/accuracy': 0.7015999555587769, 'validation/loss': 1.2197657823562622, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.9619872570037842, 'test/num_examples': 10000, 'score': 39327.479441165924, 'total_duration': 40802.17005300522, 'accumulated_submission_time': 39327.479441165924, 'accumulated_eval_time': 1467.459722518921, 'accumulated_logging_time': 3.0562262535095215}
I0229 16:34:21.328228 140089854211840 logging_writer.py:48] [117343] accumulated_eval_time=1467.459723, accumulated_logging_time=3.056226, accumulated_submission_time=39327.479441, global_step=117343, preemption_count=0, score=39327.479441, test/accuracy=0.572500, test/loss=1.961987, test/num_examples=10000, total_duration=40802.170053, train/accuracy=0.798370, train/loss=0.734471, validation/accuracy=0.701600, validation/loss=1.219766, validation/num_examples=50000
I0229 16:34:40.741587 140089870997248 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.964625358581543, loss=1.2150580883026123
I0229 16:35:14.158283 140089854211840 logging_writer.py:48] [117500] global_step=117500, grad_norm=3.0244321823120117, loss=1.2563858032226562
I0229 16:35:47.613789 140089870997248 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.9878909587860107, loss=1.3049782514572144
I0229 16:36:21.045143 140089854211840 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.793198347091675, loss=1.2225122451782227
I0229 16:36:54.543585 140089870997248 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.996631622314453, loss=1.309831976890564
I0229 16:37:27.975702 140089854211840 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.8628337383270264, loss=1.2926952838897705
I0229 16:38:01.439776 140089870997248 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.848206043243408, loss=1.175216555595398
I0229 16:38:34.887746 140089854211840 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.8591420650482178, loss=1.1934701204299927
I0229 16:39:08.343368 140089870997248 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.7623391151428223, loss=1.166430950164795
I0229 16:39:41.785862 140089854211840 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.764453887939453, loss=1.1830549240112305
I0229 16:40:15.229468 140089870997248 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.813596725463867, loss=1.2419999837875366
I0229 16:40:48.676797 140089854211840 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.0370452404022217, loss=1.2149624824523926
I0229 16:41:22.097238 140089870997248 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.526736259460449, loss=1.2051548957824707
I0229 16:41:55.566892 140089854211840 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9688010215759277, loss=1.1722838878631592
I0229 16:42:29.008921 140089870997248 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.0796563625335693, loss=1.2367936372756958
I0229 16:42:51.616708 140252611495744 spec.py:321] Evaluating on the training split.
I0229 16:42:57.831923 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 16:43:06.050980 140252611495744 spec.py:349] Evaluating on the test split.
I0229 16:43:08.322673 140252611495744 submission_runner.py:411] Time since start: 41329.20s, 	Step: 118869, 	{'train/accuracy': 0.8006417155265808, 'train/loss': 0.7289294600486755, 'validation/accuracy': 0.700439989566803, 'validation/loss': 1.2137749195098877, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 1.956821322441101, 'test/num_examples': 10000, 'score': 39837.70289897919, 'total_duration': 41329.202719688416, 'accumulated_submission_time': 39837.70289897919, 'accumulated_eval_time': 1484.165627002716, 'accumulated_logging_time': 3.1047351360321045}
I0229 16:43:08.362748 140089770305280 logging_writer.py:48] [118869] accumulated_eval_time=1484.165627, accumulated_logging_time=3.104735, accumulated_submission_time=39837.702899, global_step=118869, preemption_count=0, score=39837.702899, test/accuracy=0.570500, test/loss=1.956821, test/num_examples=10000, total_duration=41329.202720, train/accuracy=0.800642, train/loss=0.728929, validation/accuracy=0.700440, validation/loss=1.213775, validation/num_examples=50000
I0229 16:43:19.062839 140089778697984 logging_writer.py:48] [118900] global_step=118900, grad_norm=3.300873279571533, loss=1.201926350593567
I0229 16:43:52.550931 140089770305280 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.98635196685791, loss=1.172006368637085
I0229 16:44:26.013608 140089778697984 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.1324260234832764, loss=1.2291795015335083
I0229 16:44:59.435398 140089770305280 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.922729253768921, loss=1.2334446907043457
I0229 16:45:32.879756 140089778697984 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.9321720600128174, loss=1.173365592956543
I0229 16:46:06.315118 140089770305280 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.030775547027588, loss=1.1040791273117065
I0229 16:46:39.746205 140089778697984 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.7870702743530273, loss=1.0963342189788818
I0229 16:47:13.222787 140089770305280 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.15584397315979, loss=1.24997878074646
I0229 16:47:46.634627 140089778697984 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.7991299629211426, loss=1.19150972366333
I0229 16:48:20.112528 140089770305280 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.917553663253784, loss=1.2787107229232788
I0229 16:48:53.719969 140089778697984 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.068455934524536, loss=1.1740965843200684
I0229 16:49:27.168584 140089770305280 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.022857904434204, loss=1.1931601762771606
I0229 16:50:00.605097 140089778697984 logging_writer.py:48] [120100] global_step=120100, grad_norm=3.063603162765503, loss=1.2715357542037964
I0229 16:50:34.048636 140089770305280 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9252262115478516, loss=1.1952389478683472
I0229 16:51:07.484264 140089778697984 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.8525044918060303, loss=1.1606866121292114
I0229 16:51:38.388041 140252611495744 spec.py:321] Evaluating on the training split.
I0229 16:51:44.627812 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 16:51:52.636107 140252611495744 spec.py:349] Evaluating on the test split.
I0229 16:51:54.897460 140252611495744 submission_runner.py:411] Time since start: 41855.78s, 	Step: 120394, 	{'train/accuracy': 0.8328284025192261, 'train/loss': 0.6055227518081665, 'validation/accuracy': 0.7037999629974365, 'validation/loss': 1.199912428855896, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.9066051244735718, 'test/num_examples': 10000, 'score': 40347.663089990616, 'total_duration': 41855.777509212494, 'accumulated_submission_time': 40347.663089990616, 'accumulated_eval_time': 1500.6749844551086, 'accumulated_logging_time': 3.1550703048706055}
I0229 16:51:54.936323 140089770305280 logging_writer.py:48] [120394] accumulated_eval_time=1500.674984, accumulated_logging_time=3.155070, accumulated_submission_time=40347.663090, global_step=120394, preemption_count=0, score=40347.663090, test/accuracy=0.579800, test/loss=1.906605, test/num_examples=10000, total_duration=41855.777509, train/accuracy=0.832828, train/loss=0.605523, validation/accuracy=0.703800, validation/loss=1.199912, validation/num_examples=50000
I0229 16:51:57.287220 140089845819136 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.8937294483184814, loss=1.1817243099212646
I0229 16:52:30.725632 140089770305280 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.780935287475586, loss=1.2203986644744873
I0229 16:53:04.174914 140089845819136 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.009227752685547, loss=1.197762131690979
I0229 16:53:37.609808 140089770305280 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.0957295894622803, loss=1.1470155715942383
I0229 16:54:11.055242 140089845819136 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.0660502910614014, loss=1.1799352169036865
I0229 16:54:44.524973 140089770305280 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.770843505859375, loss=1.1022733449935913
I0229 16:55:18.085800 140089845819136 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.0583975315093994, loss=1.2041336297988892
I0229 16:55:51.527372 140089770305280 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.0440475940704346, loss=1.1665031909942627
I0229 16:56:24.970321 140089845819136 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.8895533084869385, loss=1.1298134326934814
I0229 16:56:58.439651 140089770305280 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.9516611099243164, loss=1.1646203994750977
I0229 16:57:31.883149 140089845819136 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.125999689102173, loss=1.1993898153305054
I0229 16:58:05.323358 140089770305280 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.8698174953460693, loss=1.2080347537994385
I0229 16:58:38.774841 140089845819136 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.9969379901885986, loss=1.2273621559143066
I0229 16:59:12.243474 140089770305280 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.195248603820801, loss=1.2182667255401611
I0229 16:59:45.652009 140089845819136 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.9215354919433594, loss=1.229069709777832
I0229 17:00:19.102953 140089770305280 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.113182306289673, loss=1.2329041957855225
I0229 17:00:24.927732 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:00:31.109287 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:00:39.135102 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:00:41.412029 140252611495744 submission_runner.py:411] Time since start: 42382.29s, 	Step: 121919, 	{'train/accuracy': 0.82425856590271, 'train/loss': 0.62799072265625, 'validation/accuracy': 0.7076799869537354, 'validation/loss': 1.1814346313476562, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.9057252407073975, 'test/num_examples': 10000, 'score': 40857.58970141411, 'total_duration': 42382.29209589958, 'accumulated_submission_time': 40857.58970141411, 'accumulated_eval_time': 1517.159220457077, 'accumulated_logging_time': 3.204002857208252}
I0229 17:00:41.450903 140089854211840 logging_writer.py:48] [121919] accumulated_eval_time=1517.159220, accumulated_logging_time=3.204003, accumulated_submission_time=40857.589701, global_step=121919, preemption_count=0, score=40857.589701, test/accuracy=0.580700, test/loss=1.905725, test/num_examples=10000, total_duration=42382.292096, train/accuracy=0.824259, train/loss=0.627991, validation/accuracy=0.707680, validation/loss=1.181435, validation/num_examples=50000
I0229 17:01:08.917842 140089862604544 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.875032663345337, loss=1.1170217990875244
I0229 17:01:42.354986 140089854211840 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.0792109966278076, loss=1.1197950839996338
I0229 17:02:15.776140 140089862604544 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.835834264755249, loss=1.163299560546875
I0229 17:02:49.244089 140089854211840 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.022606134414673, loss=1.2578942775726318
I0229 17:03:22.684285 140089862604544 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.0720598697662354, loss=1.1639482975006104
I0229 17:03:56.144391 140089854211840 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.933610439300537, loss=1.1394078731536865
I0229 17:04:29.605031 140089862604544 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.881380081176758, loss=1.1359045505523682
I0229 17:05:03.047207 140089854211840 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.9982755184173584, loss=1.1084297895431519
I0229 17:05:36.493117 140089862604544 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.2254583835601807, loss=1.1921207904815674
I0229 17:06:09.939375 140089854211840 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.9365010261535645, loss=1.1265335083007812
I0229 17:06:43.341032 140089862604544 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.2973084449768066, loss=1.2271559238433838
I0229 17:07:16.977959 140089854211840 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.9619672298431396, loss=1.2197259664535522
I0229 17:07:50.390881 140089862604544 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.050189733505249, loss=1.1326663494110107
I0229 17:08:23.881536 140089854211840 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.830714464187622, loss=1.0486400127410889
I0229 17:08:57.359667 140089862604544 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.872591733932495, loss=1.1160519123077393
I0229 17:09:11.577287 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:09:17.747040 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:09:25.969237 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:09:28.254337 140252611495744 submission_runner.py:411] Time since start: 42909.13s, 	Step: 123444, 	{'train/accuracy': 0.8248166441917419, 'train/loss': 0.625325620174408, 'validation/accuracy': 0.7119799852371216, 'validation/loss': 1.1708735227584839, 'validation/num_examples': 50000, 'test/accuracy': 0.5823000073432922, 'test/loss': 1.8903378248214722, 'test/num_examples': 10000, 'score': 41367.651956796646, 'total_duration': 42909.13440656662, 'accumulated_submission_time': 41367.651956796646, 'accumulated_eval_time': 1533.8362264633179, 'accumulated_logging_time': 3.2530641555786133}
I0229 17:09:28.295557 140089778697984 logging_writer.py:48] [123444] accumulated_eval_time=1533.836226, accumulated_logging_time=3.253064, accumulated_submission_time=41367.651957, global_step=123444, preemption_count=0, score=41367.651957, test/accuracy=0.582300, test/loss=1.890338, test/num_examples=10000, total_duration=42909.134407, train/accuracy=0.824817, train/loss=0.625326, validation/accuracy=0.711980, validation/loss=1.170874, validation/num_examples=50000
I0229 17:09:47.376267 140089837426432 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.079744338989258, loss=1.2171003818511963
I0229 17:10:20.836921 140089778697984 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.0558054447174072, loss=1.119642734527588
I0229 17:10:54.254594 140089837426432 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.262392044067383, loss=1.1531394720077515
I0229 17:11:27.701109 140089778697984 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.8759188652038574, loss=1.0685733556747437
I0229 17:12:01.130974 140089837426432 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.024136781692505, loss=1.213079571723938
I0229 17:12:34.576110 140089778697984 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.9177043437957764, loss=1.1373286247253418
I0229 17:13:08.160665 140089837426432 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.0143609046936035, loss=1.1553806066513062
I0229 17:13:41.617810 140089778697984 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.080946445465088, loss=1.1564764976501465
I0229 17:14:15.104578 140089837426432 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.1754841804504395, loss=1.1961514949798584
I0229 17:14:48.582822 140089778697984 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.094327449798584, loss=1.0520967245101929
I0229 17:15:22.034067 140089837426432 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.900261640548706, loss=1.1032710075378418
I0229 17:15:55.514458 140089778697984 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.9365053176879883, loss=1.1216026544570923
I0229 17:16:28.948991 140089837426432 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.9405698776245117, loss=1.0977863073349
I0229 17:17:02.365987 140089778697984 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.9957070350646973, loss=1.163356065750122
I0229 17:17:35.837380 140089837426432 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.166001081466675, loss=1.1738229990005493
I0229 17:17:58.422183 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:18:04.731939 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:18:12.890710 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:18:15.147332 140252611495744 submission_runner.py:411] Time since start: 43436.03s, 	Step: 124969, 	{'train/accuracy': 0.8237603306770325, 'train/loss': 0.6367748379707336, 'validation/accuracy': 0.7152199745178223, 'validation/loss': 1.161817193031311, 'validation/num_examples': 50000, 'test/accuracy': 0.5931000113487244, 'test/loss': 1.8835399150848389, 'test/num_examples': 10000, 'score': 41877.71378183365, 'total_duration': 43436.02739739418, 'accumulated_submission_time': 41877.71378183365, 'accumulated_eval_time': 1550.5613188743591, 'accumulated_logging_time': 3.3043570518493652}
I0229 17:18:15.185767 140089778697984 logging_writer.py:48] [124969] accumulated_eval_time=1550.561319, accumulated_logging_time=3.304357, accumulated_submission_time=41877.713782, global_step=124969, preemption_count=0, score=41877.713782, test/accuracy=0.593100, test/loss=1.883540, test/num_examples=10000, total_duration=43436.027397, train/accuracy=0.823760, train/loss=0.636775, validation/accuracy=0.715220, validation/loss=1.161817, validation/num_examples=50000
I0229 17:18:25.893308 140089854211840 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.747115135192871, loss=1.0938727855682373
I0229 17:18:59.318728 140089778697984 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.260612964630127, loss=1.1633654832839966
I0229 17:19:32.932801 140089854211840 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.0304362773895264, loss=1.1213533878326416
I0229 17:20:06.352290 140089778697984 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.9833548069000244, loss=1.1804265975952148
I0229 17:20:39.849674 140089854211840 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.122727155685425, loss=1.1940011978149414
I0229 17:21:13.291225 140089778697984 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.2178263664245605, loss=1.1644097566604614
I0229 17:21:46.719956 140089854211840 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.976901054382324, loss=1.193894624710083
I0229 17:22:20.183867 140089778697984 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.1542768478393555, loss=1.0757942199707031
I0229 17:22:53.615158 140089854211840 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.977494239807129, loss=1.0718520879745483
I0229 17:23:27.068533 140089778697984 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.1591672897338867, loss=1.1565473079681396
I0229 17:24:00.516002 140089854211840 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.1072497367858887, loss=1.185777187347412
I0229 17:24:33.945467 140089778697984 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.9761838912963867, loss=1.1167089939117432
I0229 17:25:07.607211 140089854211840 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.121962070465088, loss=1.1266402006149292
I0229 17:25:41.016769 140089778697984 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.1321287155151367, loss=1.0938377380371094
I0229 17:26:14.481274 140089854211840 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.9563844203948975, loss=1.040963888168335
I0229 17:26:45.373482 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:26:51.539311 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:26:59.678974 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:27:02.028350 140252611495744 submission_runner.py:411] Time since start: 43962.91s, 	Step: 126494, 	{'train/accuracy': 0.8254543542861938, 'train/loss': 0.6237724423408508, 'validation/accuracy': 0.7156199812889099, 'validation/loss': 1.1618980169296265, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 1.8814029693603516, 'test/num_examples': 10000, 'score': 42387.83588695526, 'total_duration': 43962.908398628235, 'accumulated_submission_time': 42387.83588695526, 'accumulated_eval_time': 1567.2161169052124, 'accumulated_logging_time': 3.3536806106567383}
I0229 17:27:02.065132 140089770305280 logging_writer.py:48] [126494] accumulated_eval_time=1567.216117, accumulated_logging_time=3.353681, accumulated_submission_time=42387.835887, global_step=126494, preemption_count=0, score=42387.835887, test/accuracy=0.584100, test/loss=1.881403, test/num_examples=10000, total_duration=43962.908399, train/accuracy=0.825454, train/loss=0.623772, validation/accuracy=0.715620, validation/loss=1.161898, validation/num_examples=50000
I0229 17:27:04.407461 140089778697984 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.2481985092163086, loss=1.0828262567520142
I0229 17:27:37.855009 140089770305280 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.263719320297241, loss=1.114078402519226
I0229 17:28:11.305844 140089778697984 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.358368396759033, loss=1.0802934169769287
I0229 17:28:44.782517 140089770305280 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.2176673412323, loss=1.0304899215698242
I0229 17:29:18.239792 140089778697984 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.2097325325012207, loss=1.0973329544067383
I0229 17:29:51.670187 140089770305280 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.785801649093628, loss=1.07279634475708
I0229 17:30:25.122329 140089778697984 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.0569143295288086, loss=1.0960414409637451
I0229 17:30:58.550865 140089770305280 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.286303758621216, loss=1.1494503021240234
I0229 17:31:32.110750 140089778697984 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.165574312210083, loss=1.1807985305786133
I0229 17:32:05.555638 140089770305280 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.3257594108581543, loss=1.0163971185684204
I0229 17:32:39.006262 140089778697984 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.2659671306610107, loss=1.2487099170684814
I0229 17:33:12.429207 140089770305280 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.017943859100342, loss=1.060903787612915
I0229 17:33:46.125898 140089778697984 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.310478925704956, loss=1.1687438488006592
I0229 17:34:19.564509 140089770305280 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.083359479904175, loss=1.0755101442337036
I0229 17:34:53.003834 140089778697984 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.1454992294311523, loss=1.1452528238296509
I0229 17:35:26.435106 140089770305280 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.2953169345855713, loss=1.0838160514831543
I0229 17:35:32.267673 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:35:38.384404 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:35:46.688065 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:35:48.924295 140252611495744 submission_runner.py:411] Time since start: 44489.80s, 	Step: 128019, 	{'train/accuracy': 0.8600525856018066, 'train/loss': 0.49769002199172974, 'validation/accuracy': 0.7145199775695801, 'validation/loss': 1.1730865240097046, 'validation/num_examples': 50000, 'test/accuracy': 0.5861000418663025, 'test/loss': 1.9103562831878662, 'test/num_examples': 10000, 'score': 42897.97383713722, 'total_duration': 44489.804361343384, 'accumulated_submission_time': 42897.97383713722, 'accumulated_eval_time': 1583.872680425644, 'accumulated_logging_time': 3.401171922683716}
I0229 17:35:48.973195 140089552271104 logging_writer.py:48] [128019] accumulated_eval_time=1583.872680, accumulated_logging_time=3.401172, accumulated_submission_time=42897.973837, global_step=128019, preemption_count=0, score=42897.973837, test/accuracy=0.586100, test/loss=1.910356, test/num_examples=10000, total_duration=44489.804361, train/accuracy=0.860053, train/loss=0.497690, validation/accuracy=0.714520, validation/loss=1.173087, validation/num_examples=50000
I0229 17:36:16.404937 140089770305280 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.052639961242676, loss=1.0537571907043457
I0229 17:36:49.837267 140089552271104 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.239269971847534, loss=1.1311368942260742
I0229 17:37:23.367871 140089770305280 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.1419365406036377, loss=1.0741664171218872
I0229 17:37:56.812231 140089552271104 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.567979097366333, loss=1.1504515409469604
I0229 17:38:30.263658 140089770305280 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.2103466987609863, loss=1.2076504230499268
I0229 17:39:03.723486 140089552271104 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.377823829650879, loss=1.0436769723892212
I0229 17:39:37.205374 140089770305280 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.2944328784942627, loss=1.1136773824691772
I0229 17:40:10.637426 140089552271104 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.974074363708496, loss=0.9913111329078674
I0229 17:40:44.074001 140089770305280 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.237123489379883, loss=1.137782335281372
I0229 17:41:17.512307 140089552271104 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.1826772689819336, loss=1.1877059936523438
I0229 17:41:50.958102 140089770305280 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.4036221504211426, loss=1.116927146911621
I0229 17:42:24.388065 140089552271104 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.308955669403076, loss=1.0996432304382324
I0229 17:42:57.847941 140089770305280 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.0410897731781006, loss=1.051642656326294
I0229 17:43:31.361324 140089552271104 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.3606417179107666, loss=1.2001763582229614
I0229 17:44:04.855606 140089770305280 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.550434112548828, loss=1.1473991870880127
I0229 17:44:19.024023 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:44:25.246070 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:44:33.439533 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:44:35.722933 140252611495744 submission_runner.py:411] Time since start: 45016.60s, 	Step: 129544, 	{'train/accuracy': 0.8498086333274841, 'train/loss': 0.5317939519882202, 'validation/accuracy': 0.7183399796485901, 'validation/loss': 1.1576786041259766, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.882236361503601, 'test/num_examples': 10000, 'score': 43407.957873106, 'total_duration': 45016.60299420357, 'accumulated_submission_time': 43407.957873106, 'accumulated_eval_time': 1600.5715289115906, 'accumulated_logging_time': 3.4611611366271973}
I0229 17:44:35.763330 140089552271104 logging_writer.py:48] [129544] accumulated_eval_time=1600.571529, accumulated_logging_time=3.461161, accumulated_submission_time=43407.957873, global_step=129544, preemption_count=0, score=43407.957873, test/accuracy=0.587900, test/loss=1.882236, test/num_examples=10000, total_duration=45016.602994, train/accuracy=0.849809, train/loss=0.531794, validation/accuracy=0.718340, validation/loss=1.157679, validation/num_examples=50000
I0229 17:44:54.812226 140089837426432 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.297227382659912, loss=1.0614228248596191
I0229 17:45:28.296253 140089552271104 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.536553144454956, loss=1.1498136520385742
I0229 17:46:01.710601 140089837426432 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.1744344234466553, loss=1.0616098642349243
I0229 17:46:35.233964 140089552271104 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.288741111755371, loss=1.0816175937652588
I0229 17:47:08.694190 140089837426432 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.301408052444458, loss=1.0584110021591187
I0229 17:47:42.133542 140089552271104 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.0168349742889404, loss=1.0757876634597778
I0229 17:48:15.597244 140089837426432 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.2152559757232666, loss=1.2274688482284546
I0229 17:48:49.028226 140089552271104 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.422959566116333, loss=1.0950820446014404
I0229 17:49:22.459670 140089837426432 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.4220380783081055, loss=1.1200259923934937
I0229 17:49:55.996057 140089552271104 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.118760347366333, loss=1.0762877464294434
I0229 17:50:29.489631 140089837426432 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.116424083709717, loss=1.0794382095336914
I0229 17:51:02.940829 140089552271104 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.5164523124694824, loss=1.095211386680603
I0229 17:51:36.399339 140089837426432 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.2601747512817383, loss=1.063831090927124
I0229 17:52:09.844260 140089552271104 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.3519670963287354, loss=1.0635420083999634
I0229 17:52:43.291248 140089837426432 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.3958652019500732, loss=1.0858433246612549
I0229 17:53:05.886221 140252611495744 spec.py:321] Evaluating on the training split.
I0229 17:53:12.057767 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 17:53:20.259153 140252611495744 spec.py:349] Evaluating on the test split.
I0229 17:53:22.538252 140252611495744 submission_runner.py:411] Time since start: 45543.42s, 	Step: 131069, 	{'train/accuracy': 0.8396045565605164, 'train/loss': 0.5685706734657288, 'validation/accuracy': 0.7088800072669983, 'validation/loss': 1.184848427772522, 'validation/num_examples': 50000, 'test/accuracy': 0.5843000411987305, 'test/loss': 1.9149274826049805, 'test/num_examples': 10000, 'score': 43918.01558470726, 'total_duration': 45543.41831231117, 'accumulated_submission_time': 43918.01558470726, 'accumulated_eval_time': 1617.2234988212585, 'accumulated_logging_time': 3.512183427810669}
I0229 17:53:22.579480 140089862604544 logging_writer.py:48] [131069] accumulated_eval_time=1617.223499, accumulated_logging_time=3.512183, accumulated_submission_time=43918.015585, global_step=131069, preemption_count=0, score=43918.015585, test/accuracy=0.584300, test/loss=1.914927, test/num_examples=10000, total_duration=45543.418312, train/accuracy=0.839605, train/loss=0.568571, validation/accuracy=0.708880, validation/loss=1.184848, validation/num_examples=50000
I0229 17:53:33.279381 140089870997248 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.176238536834717, loss=1.0952485799789429
I0229 17:54:06.720471 140089862604544 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.268913984298706, loss=1.155192494392395
I0229 17:54:40.189083 140089870997248 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.2461862564086914, loss=1.0287399291992188
I0229 17:55:13.624834 140089862604544 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.0933597087860107, loss=1.0815894603729248
I0229 17:55:47.181931 140089870997248 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.329324245452881, loss=1.1163302659988403
I0229 17:56:20.623729 140089862604544 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.077721118927002, loss=1.1123422384262085
I0229 17:56:54.092162 140089870997248 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.3338048458099365, loss=1.1221257448196411
I0229 17:57:27.579060 140089862604544 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.2508606910705566, loss=1.0694804191589355
I0229 17:58:01.015588 140089870997248 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.424363374710083, loss=1.12544846534729
I0229 17:58:34.477162 140089862604544 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.294553518295288, loss=1.0073562860488892
I0229 17:59:07.925546 140089870997248 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.248387336730957, loss=1.1289513111114502
I0229 17:59:41.394592 140089862604544 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.232811212539673, loss=1.0227735042572021
I0229 18:00:14.822680 140089870997248 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.085609197616577, loss=1.150649070739746
I0229 18:00:48.279466 140089862604544 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.0260300636291504, loss=0.9472218155860901
I0229 18:01:21.715598 140089870997248 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.7300541400909424, loss=1.0684789419174194
I0229 18:01:52.737084 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:01:58.899086 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:02:07.166947 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:02:09.433883 140252611495744 submission_runner.py:411] Time since start: 46070.31s, 	Step: 132594, 	{'train/accuracy': 0.8439094424247742, 'train/loss': 0.5533644556999207, 'validation/accuracy': 0.7190600037574768, 'validation/loss': 1.15404212474823, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 1.8933364152908325, 'test/num_examples': 10000, 'score': 44428.10785365105, 'total_duration': 46070.31389427185, 'accumulated_submission_time': 44428.10785365105, 'accumulated_eval_time': 1633.9201967716217, 'accumulated_logging_time': 3.5640017986297607}
I0229 18:02:09.477555 140089778697984 logging_writer.py:48] [132594] accumulated_eval_time=1633.920197, accumulated_logging_time=3.564002, accumulated_submission_time=44428.107854, global_step=132594, preemption_count=0, score=44428.107854, test/accuracy=0.593900, test/loss=1.893336, test/num_examples=10000, total_duration=46070.313894, train/accuracy=0.843909, train/loss=0.553364, validation/accuracy=0.719060, validation/loss=1.154042, validation/num_examples=50000
I0229 18:02:11.834847 140089837426432 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.2825164794921875, loss=1.0383341312408447
I0229 18:02:45.265400 140089778697984 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.372408151626587, loss=1.0343858003616333
I0229 18:03:18.688427 140089837426432 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.707881212234497, loss=1.1149271726608276
I0229 18:03:52.116712 140089778697984 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.5274226665496826, loss=1.1353251934051514
I0229 18:04:25.549674 140089837426432 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.0554680824279785, loss=1.075742244720459
I0229 18:04:58.985044 140089778697984 logging_writer.py:48] [133100] global_step=133100, grad_norm=4.126556396484375, loss=1.1080940961837769
I0229 18:05:32.412535 140089837426432 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.5751166343688965, loss=1.076025128364563
I0229 18:06:05.869930 140089778697984 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.266792058944702, loss=1.036150336265564
I0229 18:06:39.298052 140089837426432 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.2311959266662598, loss=1.0526922941207886
I0229 18:07:12.725107 140089778697984 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.5315725803375244, loss=0.9792922735214233
I0229 18:07:46.212868 140089837426432 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.273754119873047, loss=1.0343092679977417
I0229 18:08:19.634077 140089778697984 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.265113353729248, loss=1.0869569778442383
I0229 18:08:53.125443 140089837426432 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.378596305847168, loss=1.0411913394927979
I0229 18:09:26.596774 140089778697984 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.5398600101470947, loss=0.9887755513191223
I0229 18:10:00.028436 140089837426432 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.40677547454834, loss=1.0951677560806274
I0229 18:10:33.480035 140089778697984 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.2041409015655518, loss=0.9439184069633484
I0229 18:10:39.649820 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:10:45.763228 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:10:54.006050 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:10:56.283218 140252611495744 submission_runner.py:411] Time since start: 46597.16s, 	Step: 134120, 	{'train/accuracy': 0.8478555083274841, 'train/loss': 0.5405254364013672, 'validation/accuracy': 0.7234199643135071, 'validation/loss': 1.133036494255066, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8870649337768555, 'test/num_examples': 10000, 'score': 44938.210675001144, 'total_duration': 46597.1632874012, 'accumulated_submission_time': 44938.210675001144, 'accumulated_eval_time': 1650.5535411834717, 'accumulated_logging_time': 3.6218745708465576}
I0229 18:10:56.325398 140089862604544 logging_writer.py:48] [134120] accumulated_eval_time=1650.553541, accumulated_logging_time=3.621875, accumulated_submission_time=44938.210675, global_step=134120, preemption_count=0, score=44938.210675, test/accuracy=0.589600, test/loss=1.887065, test/num_examples=10000, total_duration=46597.163287, train/accuracy=0.847856, train/loss=0.540525, validation/accuracy=0.723420, validation/loss=1.133036, validation/num_examples=50000
I0229 18:11:23.396234 140089870997248 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.4096405506134033, loss=1.1069318056106567
I0229 18:11:56.845776 140089862604544 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.230833053588867, loss=1.0259900093078613
I0229 18:12:30.295696 140089870997248 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.1753830909729004, loss=0.9963483810424805
I0229 18:13:03.726307 140089862604544 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.6840546131134033, loss=1.1484708786010742
I0229 18:13:37.166876 140089870997248 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.19150972366333, loss=1.047682523727417
I0229 18:14:10.677595 140089862604544 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.6447157859802246, loss=1.1236083507537842
I0229 18:14:44.119514 140089870997248 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.5173287391662598, loss=1.0497989654541016
I0229 18:15:17.550560 140089862604544 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.4873268604278564, loss=0.9344560503959656
I0229 18:15:50.992717 140089870997248 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.3574492931365967, loss=1.0695048570632935
I0229 18:16:24.426569 140089862604544 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.6171271800994873, loss=0.9966268539428711
I0229 18:16:57.877519 140089870997248 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.3015823364257812, loss=0.9399014711380005
I0229 18:17:31.329006 140089862604544 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.3180010318756104, loss=1.0135321617126465
I0229 18:18:04.766262 140089870997248 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.78009033203125, loss=1.136214256286621
I0229 18:18:38.230987 140089862604544 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.4356117248535156, loss=1.0343354940414429
I0229 18:19:11.647868 140089870997248 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.3889403343200684, loss=1.0081331729888916
I0229 18:19:26.494643 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:19:32.567816 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:19:40.758927 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:19:43.134629 140252611495744 submission_runner.py:411] Time since start: 47124.01s, 	Step: 135646, 	{'train/accuracy': 0.8484534025192261, 'train/loss': 0.5400087833404541, 'validation/accuracy': 0.7185199856758118, 'validation/loss': 1.1664830446243286, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.909881353378296, 'test/num_examples': 10000, 'score': 45448.31420826912, 'total_duration': 47124.01470160484, 'accumulated_submission_time': 45448.31420826912, 'accumulated_eval_time': 1667.193475484848, 'accumulated_logging_time': 3.674959659576416}
I0229 18:19:43.174186 140089837426432 logging_writer.py:48] [135646] accumulated_eval_time=1667.193475, accumulated_logging_time=3.674960, accumulated_submission_time=45448.314208, global_step=135646, preemption_count=0, score=45448.314208, test/accuracy=0.593000, test/loss=1.909881, test/num_examples=10000, total_duration=47124.014702, train/accuracy=0.848453, train/loss=0.540009, validation/accuracy=0.718520, validation/loss=1.166483, validation/num_examples=50000
I0229 18:20:01.661224 140089845819136 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.3876519203186035, loss=0.9663273096084595
I0229 18:20:35.094859 140089837426432 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.5409669876098633, loss=1.015819787979126
I0229 18:21:08.538663 140089845819136 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.3714864253997803, loss=1.1214978694915771
I0229 18:21:41.959096 140089837426432 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.605647563934326, loss=1.0022605657577515
I0229 18:22:15.388869 140089845819136 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.219635009765625, loss=0.9163435697555542
I0229 18:22:48.817785 140089837426432 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.4037532806396484, loss=1.0437743663787842
I0229 18:23:22.262995 140089845819136 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.2339541912078857, loss=0.8915929794311523
I0229 18:23:55.694292 140089837426432 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.352494478225708, loss=1.0688015222549438
I0229 18:24:29.127085 140089845819136 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.1704485416412354, loss=0.9497631192207336
I0229 18:25:02.547442 140089837426432 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.7027997970581055, loss=0.9858987927436829
I0229 18:25:35.982519 140089845819136 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.690080404281616, loss=0.983890950679779
I0229 18:26:09.502388 140089837426432 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.7744693756103516, loss=0.903354823589325
I0229 18:26:42.951204 140089845819136 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.5757851600646973, loss=1.0285823345184326
I0229 18:27:16.390017 140089837426432 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.684741973876953, loss=1.128761887550354
I0229 18:27:49.814908 140089845819136 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.1546099185943604, loss=1.0680091381072998
I0229 18:28:13.368339 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:28:19.549865 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:28:27.778185 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:28:30.054944 140252611495744 submission_runner.py:411] Time since start: 47650.93s, 	Step: 137172, 	{'train/accuracy': 0.8808394074440002, 'train/loss': 0.4171130955219269, 'validation/accuracy': 0.7249799966812134, 'validation/loss': 1.1274539232254028, 'validation/num_examples': 50000, 'test/accuracy': 0.5922000408172607, 'test/loss': 1.8785035610198975, 'test/num_examples': 10000, 'score': 45958.44335961342, 'total_duration': 47650.93498468399, 'accumulated_submission_time': 45958.44335961342, 'accumulated_eval_time': 1683.8800098896027, 'accumulated_logging_time': 3.7243666648864746}
I0229 18:28:30.100502 140089770305280 logging_writer.py:48] [137172] accumulated_eval_time=1683.880010, accumulated_logging_time=3.724367, accumulated_submission_time=45958.443360, global_step=137172, preemption_count=0, score=45958.443360, test/accuracy=0.592200, test/loss=1.878504, test/num_examples=10000, total_duration=47650.934985, train/accuracy=0.880839, train/loss=0.417113, validation/accuracy=0.724980, validation/loss=1.127454, validation/num_examples=50000
I0229 18:28:39.786976 140089778697984 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.4232380390167236, loss=1.019466519355774
I0229 18:29:13.213071 140089770305280 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.286651849746704, loss=0.9758387207984924
I0229 18:29:46.640760 140089778697984 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.9855926036834717, loss=1.0275993347167969
I0229 18:30:20.077024 140089770305280 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.903689384460449, loss=1.0130174160003662
I0229 18:30:53.492714 140089778697984 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.3723623752593994, loss=0.9841448068618774
I0229 18:31:26.935421 140089770305280 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.5920867919921875, loss=1.056814432144165
I0229 18:32:00.389839 140089778697984 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.805872678756714, loss=1.084323525428772
I0229 18:32:33.827139 140089770305280 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.287916421890259, loss=1.0507633686065674
I0229 18:33:07.260753 140089778697984 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.5700113773345947, loss=0.9010065197944641
I0229 18:33:40.686676 140089770305280 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.391777992248535, loss=0.9062250852584839
I0229 18:34:14.140208 140089778697984 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.67311429977417, loss=1.0556389093399048
I0229 18:34:47.551787 140089770305280 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.7084951400756836, loss=1.1170189380645752
I0229 18:35:21.041751 140089778697984 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.4821372032165527, loss=1.0585970878601074
I0229 18:35:54.465575 140089770305280 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.493605136871338, loss=0.9664406776428223
I0229 18:36:27.914608 140089778697984 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.3551979064941406, loss=0.9206780195236206
I0229 18:37:00.154276 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:37:06.395378 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:37:14.576888 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:37:16.866606 140252611495744 submission_runner.py:411] Time since start: 48177.75s, 	Step: 138698, 	{'train/accuracy': 0.8705556392669678, 'train/loss': 0.44832268357276917, 'validation/accuracy': 0.7280600070953369, 'validation/loss': 1.13063383102417, 'validation/num_examples': 50000, 'test/accuracy': 0.6017000079154968, 'test/loss': 1.8933743238449097, 'test/num_examples': 10000, 'score': 46468.43311071396, 'total_duration': 48177.746673583984, 'accumulated_submission_time': 46468.43311071396, 'accumulated_eval_time': 1700.59228515625, 'accumulated_logging_time': 3.780108690261841}
I0229 18:37:16.906069 140089837426432 logging_writer.py:48] [138698] accumulated_eval_time=1700.592285, accumulated_logging_time=3.780109, accumulated_submission_time=46468.433111, global_step=138698, preemption_count=0, score=46468.433111, test/accuracy=0.601700, test/loss=1.893374, test/num_examples=10000, total_duration=48177.746674, train/accuracy=0.870556, train/loss=0.448323, validation/accuracy=0.728060, validation/loss=1.130634, validation/num_examples=50000
I0229 18:37:17.928769 140089845819136 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.3723995685577393, loss=0.9742319583892822
I0229 18:37:51.393915 140089837426432 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.724695920944214, loss=0.9508846998214722
I0229 18:38:25.005102 140089845819136 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.462010145187378, loss=0.9908096790313721
I0229 18:38:58.440544 140089837426432 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.474900484085083, loss=0.9909231662750244
I0229 18:39:31.910353 140089845819136 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.4518988132476807, loss=0.9502009749412537
I0229 18:40:05.367112 140089837426432 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.7256131172180176, loss=0.9244621992111206
I0229 18:40:38.788780 140089845819136 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.781661033630371, loss=1.0450026988983154
I0229 18:41:12.247980 140089837426432 logging_writer.py:48] [139400] global_step=139400, grad_norm=4.181591510772705, loss=1.1492280960083008
I0229 18:41:45.659934 140089845819136 logging_writer.py:48] [139500] global_step=139500, grad_norm=4.148693084716797, loss=1.064286470413208
I0229 18:42:19.127401 140089837426432 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.608168363571167, loss=0.9915893077850342
I0229 18:42:52.585997 140089845819136 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.608747959136963, loss=1.0393341779708862
I0229 18:43:26.021007 140089837426432 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.357823371887207, loss=1.0049853324890137
I0229 18:43:59.572533 140089845819136 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.5151638984680176, loss=0.9637465476989746
I0229 18:44:33.005221 140089837426432 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.6889569759368896, loss=1.0616986751556396
I0229 18:45:06.451193 140089845819136 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.67626953125, loss=0.9507212042808533
I0229 18:45:39.902586 140089837426432 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.765737295150757, loss=1.0092356204986572
I0229 18:45:47.057836 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:45:53.123590 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:46:01.351096 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:46:03.801980 140252611495744 submission_runner.py:411] Time since start: 48704.68s, 	Step: 140223, 	{'train/accuracy': 0.8729272484779358, 'train/loss': 0.4394337832927704, 'validation/accuracy': 0.7323200106620789, 'validation/loss': 1.1090658903121948, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.8609994649887085, 'test/num_examples': 10000, 'score': 46978.51990914345, 'total_duration': 48704.682052373886, 'accumulated_submission_time': 46978.51990914345, 'accumulated_eval_time': 1717.3363778591156, 'accumulated_logging_time': 3.8292012214660645}
I0229 18:46:03.841421 140089770305280 logging_writer.py:48] [140223] accumulated_eval_time=1717.336378, accumulated_logging_time=3.829201, accumulated_submission_time=46978.519909, global_step=140223, preemption_count=0, score=46978.519909, test/accuracy=0.601100, test/loss=1.860999, test/num_examples=10000, total_duration=48704.682052, train/accuracy=0.872927, train/loss=0.439434, validation/accuracy=0.732320, validation/loss=1.109066, validation/num_examples=50000
I0229 18:46:29.932218 140089778697984 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.5921146869659424, loss=1.0283786058425903
I0229 18:47:03.377942 140089770305280 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.793750286102295, loss=1.0131572484970093
I0229 18:47:36.819386 140089778697984 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.5522232055664062, loss=0.9663417339324951
I0229 18:48:10.244534 140089770305280 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.3325719833374023, loss=0.9158172607421875
I0229 18:48:43.688553 140089778697984 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.8129992485046387, loss=0.9893953800201416
I0229 18:49:17.127753 140089770305280 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.5597972869873047, loss=0.9918163418769836
I0229 18:49:50.571263 140089778697984 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.5916872024536133, loss=0.9214811325073242
I0229 18:50:24.095715 140089770305280 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.5990681648254395, loss=0.9254663586616516
I0229 18:50:57.538468 140089778697984 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.7085952758789062, loss=0.9626219868659973
I0229 18:51:30.995898 140089770305280 logging_writer.py:48] [141200] global_step=141200, grad_norm=4.006949424743652, loss=0.9616377353668213
I0229 18:52:04.444686 140089778697984 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.531903028488159, loss=0.9327865839004517
I0229 18:52:37.878368 140089770305280 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.7510151863098145, loss=1.0154330730438232
I0229 18:53:11.322257 140089778697984 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.4503891468048096, loss=0.9088616967201233
I0229 18:53:44.738728 140089770305280 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.4091172218322754, loss=0.8807806968688965
I0229 18:54:18.190994 140089778697984 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.9444680213928223, loss=0.9373675584793091
I0229 18:54:34.048449 140252611495744 spec.py:321] Evaluating on the training split.
I0229 18:54:40.192494 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 18:54:48.348672 140252611495744 spec.py:349] Evaluating on the test split.
I0229 18:54:50.691685 140252611495744 submission_runner.py:411] Time since start: 49231.57s, 	Step: 141749, 	{'train/accuracy': 0.8720304369926453, 'train/loss': 0.4448748230934143, 'validation/accuracy': 0.7297999858856201, 'validation/loss': 1.107924222946167, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.851275086402893, 'test/num_examples': 10000, 'score': 47488.66160154343, 'total_duration': 49231.571748018265, 'accumulated_submission_time': 47488.66160154343, 'accumulated_eval_time': 1733.9795546531677, 'accumulated_logging_time': 3.879467010498047}
I0229 18:54:50.735281 140089862604544 logging_writer.py:48] [141749] accumulated_eval_time=1733.979555, accumulated_logging_time=3.879467, accumulated_submission_time=47488.661602, global_step=141749, preemption_count=0, score=47488.661602, test/accuracy=0.602400, test/loss=1.851275, test/num_examples=10000, total_duration=49231.571748, train/accuracy=0.872030, train/loss=0.444875, validation/accuracy=0.729800, validation/loss=1.107924, validation/num_examples=50000
I0229 18:55:08.121119 140089870997248 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.7543089389801025, loss=0.9351745247840881
I0229 18:55:41.541301 140089862604544 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.6226320266723633, loss=0.9931458830833435
I0229 18:56:15.060240 140089870997248 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.690541982650757, loss=0.9750836491584778
I0229 18:56:48.509068 140089862604544 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.44484281539917, loss=0.922397255897522
I0229 18:57:21.926342 140089870997248 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.7700085639953613, loss=0.9409729838371277
I0229 18:57:55.376793 140089862604544 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.372809886932373, loss=0.9307103157043457
I0229 18:58:28.780333 140089870997248 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.621636152267456, loss=0.877278745174408
I0229 18:59:02.230467 140089862604544 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.388075113296509, loss=0.8890981674194336
I0229 18:59:35.637061 140089870997248 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6735150814056396, loss=0.9590499401092529
I0229 19:00:09.091211 140089862604544 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.6225199699401855, loss=0.8629909157752991
I0229 19:00:42.505990 140089870997248 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.8671231269836426, loss=0.9732969403266907
I0229 19:01:15.947162 140089862604544 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.5630884170532227, loss=0.9635530710220337
I0229 19:01:49.417491 140089870997248 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.759521961212158, loss=0.9885638356208801
I0229 19:02:22.924938 140089862604544 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.6335480213165283, loss=0.9498006105422974
I0229 19:02:56.324591 140089870997248 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.690769672393799, loss=0.9291605949401855
I0229 19:03:20.911865 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:03:26.985517 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:03:35.210983 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:03:37.482712 140252611495744 submission_runner.py:411] Time since start: 49758.36s, 	Step: 143275, 	{'train/accuracy': 0.8688217401504517, 'train/loss': 0.4498252868652344, 'validation/accuracy': 0.7297799587249756, 'validation/loss': 1.1145541667938232, 'validation/num_examples': 50000, 'test/accuracy': 0.6026000380516052, 'test/loss': 1.8679395914077759, 'test/num_examples': 10000, 'score': 47998.768881082535, 'total_duration': 49758.36278486252, 'accumulated_submission_time': 47998.768881082535, 'accumulated_eval_time': 1750.5503687858582, 'accumulated_logging_time': 3.9371063709259033}
I0229 19:03:37.528037 140089552271104 logging_writer.py:48] [143275] accumulated_eval_time=1750.550369, accumulated_logging_time=3.937106, accumulated_submission_time=47998.768881, global_step=143275, preemption_count=0, score=47998.768881, test/accuracy=0.602600, test/loss=1.867940, test/num_examples=10000, total_duration=49758.362785, train/accuracy=0.868822, train/loss=0.449825, validation/accuracy=0.729780, validation/loss=1.114554, validation/num_examples=50000
I0229 19:03:46.214200 140089770305280 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.5289268493652344, loss=0.9216885566711426
I0229 19:04:19.637946 140089552271104 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.733339309692383, loss=0.9885202646255493
I0229 19:04:53.106901 140089770305280 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.724248170852661, loss=0.9317117929458618
I0229 19:05:26.568757 140089552271104 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.7772409915924072, loss=0.9404418468475342
I0229 19:06:00.052795 140089770305280 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.59670090675354, loss=0.9446575045585632
I0229 19:06:33.517186 140089552271104 logging_writer.py:48] [143800] global_step=143800, grad_norm=4.141885280609131, loss=0.994599461555481
I0229 19:07:06.953967 140089770305280 logging_writer.py:48] [143900] global_step=143900, grad_norm=4.038818836212158, loss=1.0073776245117188
I0229 19:07:40.397177 140089552271104 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.759788990020752, loss=0.9371134042739868
I0229 19:08:13.963795 140089770305280 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.4481358528137207, loss=0.8652292490005493
I0229 19:08:47.436912 140089552271104 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.8249409198760986, loss=0.910799503326416
I0229 19:09:20.924646 140089770305280 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.623060703277588, loss=0.8875592350959778
I0229 19:09:54.342066 140089552271104 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.8593249320983887, loss=0.9242382645606995
I0229 19:10:27.822629 140089770305280 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.7691524028778076, loss=0.8754552006721497
I0229 19:11:01.297344 140089552271104 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.3530540466308594, loss=0.8953104615211487
I0229 19:11:34.760739 140089770305280 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.907754898071289, loss=0.8584452271461487
I0229 19:12:07.692082 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:12:13.880694 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:12:22.000347 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:12:24.263946 140252611495744 submission_runner.py:411] Time since start: 50285.14s, 	Step: 144800, 	{'train/accuracy': 0.89164137840271, 'train/loss': 0.37929603457450867, 'validation/accuracy': 0.7303000092506409, 'validation/loss': 1.1208324432373047, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.8936916589736938, 'test/num_examples': 10000, 'score': 48508.867156744, 'total_duration': 50285.14401555061, 'accumulated_submission_time': 48508.867156744, 'accumulated_eval_time': 1767.1221826076508, 'accumulated_logging_time': 3.994133949279785}
I0229 19:12:24.304894 140089862604544 logging_writer.py:48] [144800] accumulated_eval_time=1767.122183, accumulated_logging_time=3.994134, accumulated_submission_time=48508.867157, global_step=144800, preemption_count=0, score=48508.867157, test/accuracy=0.603200, test/loss=1.893692, test/num_examples=10000, total_duration=50285.144016, train/accuracy=0.891641, train/loss=0.379296, validation/accuracy=0.730300, validation/loss=1.120832, validation/num_examples=50000
I0229 19:12:24.645293 140089870997248 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.9594156742095947, loss=0.9393618106842041
I0229 19:12:58.069590 140089862604544 logging_writer.py:48] [144900] global_step=144900, grad_norm=4.216879367828369, loss=1.037602186203003
I0229 19:13:31.555880 140089870997248 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.6427314281463623, loss=0.9292389154434204
I0229 19:14:05.047138 140089862604544 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.8572170734405518, loss=0.9930155873298645
I0229 19:14:38.595354 140089870997248 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.9266021251678467, loss=0.9367784261703491
I0229 19:15:12.060741 140089862604544 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.789881467819214, loss=0.9358006715774536
I0229 19:15:45.495959 140089870997248 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.9979794025421143, loss=0.9273204803466797
I0229 19:16:18.959450 140089862604544 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.987856149673462, loss=0.9443153142929077
I0229 19:16:52.404911 140089870997248 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.996493101119995, loss=1.0515005588531494
I0229 19:17:25.860678 140089862604544 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.933788776397705, loss=0.9924442172050476
I0229 19:17:59.344235 140089870997248 logging_writer.py:48] [145800] global_step=145800, grad_norm=4.13003396987915, loss=0.9442920684814453
I0229 19:18:32.790815 140089862604544 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.7585978507995605, loss=0.8919498324394226
I0229 19:19:06.284116 140089870997248 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.750408411026001, loss=0.8777761459350586
I0229 19:19:39.735910 140089862604544 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.6448569297790527, loss=0.9848318099975586
I0229 19:20:13.162165 140089870997248 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.9174726009368896, loss=0.9198554158210754
I0229 19:20:46.706194 140089862604544 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.487248182296753, loss=0.8808648586273193
I0229 19:20:54.533360 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:21:00.700142 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:21:09.065826 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:21:11.345144 140252611495744 submission_runner.py:411] Time since start: 50812.23s, 	Step: 146325, 	{'train/accuracy': 0.8965840339660645, 'train/loss': 0.36079141497612, 'validation/accuracy': 0.7327799797058105, 'validation/loss': 1.1133227348327637, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.8500038385391235, 'test/num_examples': 10000, 'score': 49019.031086206436, 'total_duration': 50812.22521138191, 'accumulated_submission_time': 49019.031086206436, 'accumulated_eval_time': 1783.9339122772217, 'accumulated_logging_time': 4.045078992843628}
I0229 19:21:11.385793 140089552271104 logging_writer.py:48] [146325] accumulated_eval_time=1783.933912, accumulated_logging_time=4.045079, accumulated_submission_time=49019.031086, global_step=146325, preemption_count=0, score=49019.031086, test/accuracy=0.606500, test/loss=1.850004, test/num_examples=10000, total_duration=50812.225211, train/accuracy=0.896584, train/loss=0.360791, validation/accuracy=0.732780, validation/loss=1.113323, validation/num_examples=50000
I0229 19:21:36.807284 140089770305280 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.330577373504639, loss=0.999171257019043
I0229 19:22:10.267380 140089552271104 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.9683618545532227, loss=0.9637865424156189
I0229 19:22:43.687064 140089770305280 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.434783935546875, loss=0.8083239793777466
I0229 19:23:17.142911 140089552271104 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.9790918827056885, loss=0.9613628387451172
I0229 19:23:50.579519 140089770305280 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.968911647796631, loss=0.878210186958313
I0229 19:24:24.014227 140089552271104 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.6882030963897705, loss=1.0090910196304321
I0229 19:24:57.447226 140089770305280 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.9174726009368896, loss=0.946039617061615
I0229 19:25:30.899510 140089552271104 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.012478351593018, loss=0.9624110460281372
I0229 19:26:04.328124 140089770305280 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.724102020263672, loss=0.8775785565376282
I0229 19:26:37.842185 140089552271104 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.8318188190460205, loss=0.9431453347206116
I0229 19:27:11.292130 140089770305280 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.690521001815796, loss=0.8867164850234985
I0229 19:27:44.763049 140089552271104 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.907731771469116, loss=0.9237124919891357
I0229 19:28:18.177650 140089770305280 logging_writer.py:48] [147600] global_step=147600, grad_norm=4.171967029571533, loss=0.9243442416191101
I0229 19:28:51.635975 140089552271104 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.8523082733154297, loss=0.878364086151123
I0229 19:29:25.063360 140089770305280 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.159187316894531, loss=0.867263674736023
I0229 19:29:41.595742 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:29:47.702548 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:29:55.807659 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:29:58.134364 140252611495744 submission_runner.py:411] Time since start: 51339.01s, 	Step: 147851, 	{'train/accuracy': 0.8950892686843872, 'train/loss': 0.3633810877799988, 'validation/accuracy': 0.7359399795532227, 'validation/loss': 1.0984907150268555, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.8604868650436401, 'test/num_examples': 10000, 'score': 49529.17516756058, 'total_duration': 51339.01440405846, 'accumulated_submission_time': 49529.17516756058, 'accumulated_eval_time': 1800.4724493026733, 'accumulated_logging_time': 4.096417427062988}
I0229 19:29:58.175661 140089770305280 logging_writer.py:48] [147851] accumulated_eval_time=1800.472449, accumulated_logging_time=4.096417, accumulated_submission_time=49529.175168, global_step=147851, preemption_count=0, score=49529.175168, test/accuracy=0.606700, test/loss=1.860487, test/num_examples=10000, total_duration=51339.014404, train/accuracy=0.895089, train/loss=0.363381, validation/accuracy=0.735940, validation/loss=1.098491, validation/num_examples=50000
I0229 19:30:14.932391 140089778697984 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.042553424835205, loss=0.862550675868988
I0229 19:30:48.388283 140089770305280 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.582535743713379, loss=0.8475841879844666
I0229 19:31:21.807137 140089778697984 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.9613754749298096, loss=0.8276984691619873
I0229 19:31:55.294712 140089770305280 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.7072417736053467, loss=0.8850411772727966
I0229 19:32:28.775275 140089778697984 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.8663442134857178, loss=0.8791357278823853
I0229 19:33:02.344430 140089770305280 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.947514295578003, loss=0.898938775062561
I0229 19:33:35.851265 140089778697984 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.68963885307312, loss=0.8064377903938293
I0229 19:34:09.285472 140089770305280 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.251275539398193, loss=0.8132791519165039
I0229 19:34:42.738996 140089778697984 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.6976795196533203, loss=0.8353139758110046
I0229 19:35:16.175926 140089770305280 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.35286808013916, loss=0.871384859085083
I0229 19:35:49.608237 140089778697984 logging_writer.py:48] [148900] global_step=148900, grad_norm=4.277066230773926, loss=0.9487040638923645
I0229 19:36:23.076527 140089770305280 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.7953879833221436, loss=0.8314152956008911
I0229 19:36:56.540308 140089778697984 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.253573894500732, loss=0.9281123876571655
I0229 19:37:29.997053 140089770305280 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.795698642730713, loss=0.9527039527893066
I0229 19:38:03.476353 140089778697984 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.865817070007324, loss=0.9568019509315491
I0229 19:38:28.351079 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:38:34.737570 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:38:42.863744 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:38:45.211812 140252611495744 submission_runner.py:411] Time since start: 51866.09s, 	Step: 149376, 	{'train/accuracy': 0.8971220850944519, 'train/loss': 0.3583529591560364, 'validation/accuracy': 0.7389199733734131, 'validation/loss': 1.0933120250701904, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.858315110206604, 'test/num_examples': 10000, 'score': 50039.28535270691, 'total_duration': 51866.091879844666, 'accumulated_submission_time': 50039.28535270691, 'accumulated_eval_time': 1817.3331348896027, 'accumulated_logging_time': 4.147741079330444}
I0229 19:38:45.253459 140089770305280 logging_writer.py:48] [149376] accumulated_eval_time=1817.333135, accumulated_logging_time=4.147741, accumulated_submission_time=50039.285353, global_step=149376, preemption_count=0, score=50039.285353, test/accuracy=0.607700, test/loss=1.858315, test/num_examples=10000, total_duration=51866.091880, train/accuracy=0.897122, train/loss=0.358353, validation/accuracy=0.738920, validation/loss=1.093312, validation/num_examples=50000
I0229 19:38:53.623273 140089837426432 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.7843220233917236, loss=0.8481184244155884
I0229 19:39:27.081505 140089770305280 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.87136173248291, loss=0.889277994632721
I0229 19:40:00.538960 140089837426432 logging_writer.py:48] [149600] global_step=149600, grad_norm=4.110464572906494, loss=0.848743200302124
I0229 19:40:33.968885 140089770305280 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.8754894733428955, loss=0.8754538297653198
I0229 19:41:07.438399 140089837426432 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.455315113067627, loss=0.8852707147598267
I0229 19:41:40.897604 140089770305280 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.206686496734619, loss=0.8778904676437378
I0229 19:42:14.331759 140089837426432 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.9375579357147217, loss=0.851932168006897
I0229 19:42:47.819541 140089770305280 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.060028076171875, loss=0.9292486906051636
I0229 19:43:21.287299 140089837426432 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.6479713916778564, loss=0.8281155824661255
I0229 19:43:54.720357 140089770305280 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.071697235107422, loss=0.812321126461029
I0229 19:44:28.188694 140089837426432 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.9018540382385254, loss=0.8405728340148926
I0229 19:45:01.715631 140089770305280 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.903717041015625, loss=0.8699849843978882
I0229 19:45:35.127758 140089837426432 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.1840009689331055, loss=0.8718631863594055
I0229 19:46:08.558266 140089770305280 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.407435417175293, loss=0.8860675692558289
I0229 19:46:41.984937 140089837426432 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.955111265182495, loss=0.8085881471633911
I0229 19:47:15.449336 140089770305280 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.646467685699463, loss=0.8884155750274658
I0229 19:47:15.456139 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:47:21.522943 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:47:29.740709 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:47:32.101744 140252611495744 submission_runner.py:411] Time since start: 52392.98s, 	Step: 150901, 	{'train/accuracy': 0.8970025181770325, 'train/loss': 0.34878623485565186, 'validation/accuracy': 0.740339994430542, 'validation/loss': 1.1000778675079346, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.8836166858673096, 'test/num_examples': 10000, 'score': 50549.420686244965, 'total_duration': 52392.9816904068, 'accumulated_submission_time': 50549.420686244965, 'accumulated_eval_time': 1833.978541135788, 'accumulated_logging_time': 4.200202941894531}
I0229 19:47:32.147226 140089552271104 logging_writer.py:48] [150901] accumulated_eval_time=1833.978541, accumulated_logging_time=4.200203, accumulated_submission_time=50549.420686, global_step=150901, preemption_count=0, score=50549.420686, test/accuracy=0.607300, test/loss=1.883617, test/num_examples=10000, total_duration=52392.981690, train/accuracy=0.897003, train/loss=0.348786, validation/accuracy=0.740340, validation/loss=1.100078, validation/num_examples=50000
I0229 19:48:05.620140 140089770305280 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.075803756713867, loss=0.8884156346321106
I0229 19:48:39.074489 140089552271104 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.021389007568359, loss=0.7826602458953857
I0229 19:49:12.538007 140089770305280 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.911721706390381, loss=0.8376513123512268
I0229 19:49:45.978893 140089552271104 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.093053817749023, loss=0.8836634159088135
I0229 19:50:19.432391 140089770305280 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.6611220836639404, loss=0.7097198963165283
I0229 19:50:52.980778 140089552271104 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.8615338802337646, loss=0.8707016706466675
I0229 19:51:26.414499 140089770305280 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.30642557144165, loss=0.9285552501678467
I0229 19:51:59.855569 140089552271104 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.02296257019043, loss=0.870423436164856
I0229 19:52:33.290284 140089770305280 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.21043062210083, loss=0.7968629598617554
I0229 19:53:06.761856 140089552271104 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.313770771026611, loss=0.8864030838012695
I0229 19:53:40.240730 140089770305280 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.232472896575928, loss=0.8874164819717407
I0229 19:54:13.678641 140089552271104 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.0745720863342285, loss=0.812104344367981
I0229 19:54:47.167389 140089770305280 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.249450206756592, loss=0.9072585701942444
I0229 19:55:20.615140 140089552271104 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.710829734802246, loss=0.8605237603187561
I0229 19:55:54.072868 140089770305280 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.169076919555664, loss=0.8019716143608093
I0229 19:56:02.235757 140252611495744 spec.py:321] Evaluating on the training split.
I0229 19:56:08.315676 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 19:56:16.590496 140252611495744 spec.py:349] Evaluating on the test split.
I0229 19:56:18.908954 140252611495744 submission_runner.py:411] Time since start: 52919.79s, 	Step: 152426, 	{'train/accuracy': 0.9053730964660645, 'train/loss': 0.33586665987968445, 'validation/accuracy': 0.738319993019104, 'validation/loss': 1.0839452743530273, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.8294333219528198, 'test/num_examples': 10000, 'score': 51059.445302248, 'total_duration': 52919.78895497322, 'accumulated_submission_time': 51059.445302248, 'accumulated_eval_time': 1850.6516172885895, 'accumulated_logging_time': 4.255874156951904}
I0229 19:56:18.953249 140089770305280 logging_writer.py:48] [152426] accumulated_eval_time=1850.651617, accumulated_logging_time=4.255874, accumulated_submission_time=51059.445302, global_step=152426, preemption_count=0, score=51059.445302, test/accuracy=0.608900, test/loss=1.829433, test/num_examples=10000, total_duration=52919.788955, train/accuracy=0.905373, train/loss=0.335867, validation/accuracy=0.738320, validation/loss=1.083945, validation/num_examples=50000
I0229 19:56:44.036794 140089778697984 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.148616790771484, loss=0.8190180659294128
I0229 19:57:17.655417 140089770305280 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.174142837524414, loss=0.8475724458694458
I0229 19:57:51.066339 140089778697984 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.178292751312256, loss=0.8229067325592041
I0229 19:58:24.525446 140089770305280 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.193158149719238, loss=0.8297246694564819
I0229 19:58:57.937202 140089778697984 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.209256172180176, loss=0.8471187353134155
I0229 19:59:31.412029 140089770305280 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.179840564727783, loss=0.7932194471359253
I0229 20:00:04.812855 140089778697984 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.078962802886963, loss=0.7822647094726562
I0229 20:00:38.304374 140089770305280 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.402345657348633, loss=0.8190866112709045
I0229 20:01:11.760284 140089778697984 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.710026264190674, loss=0.7610934376716614
I0229 20:01:45.187114 140089770305280 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.59665584564209, loss=0.9449383616447449
I0229 20:02:18.643279 140089778697984 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.6466262340545654, loss=0.779674232006073
I0229 20:02:52.236239 140089770305280 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.208677768707275, loss=0.8335553407669067
I0229 20:03:25.673842 140089778697984 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.948828935623169, loss=0.7593696713447571
I0229 20:03:59.109320 140089770305280 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.3262834548950195, loss=0.899118959903717
I0229 20:04:32.563972 140089778697984 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.045355319976807, loss=0.8181865215301514
I0229 20:04:49.101868 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:04:55.842018 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:05:04.178200 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:05:06.443886 140252611495744 submission_runner.py:411] Time since start: 53447.32s, 	Step: 153951, 	{'train/accuracy': 0.9297871589660645, 'train/loss': 0.24879996478557587, 'validation/accuracy': 0.7396799921989441, 'validation/loss': 1.0863995552062988, 'validation/num_examples': 50000, 'test/accuracy': 0.6115000247955322, 'test/loss': 1.8779557943344116, 'test/num_examples': 10000, 'score': 51569.52954649925, 'total_duration': 53447.32394480705, 'accumulated_submission_time': 51569.52954649925, 'accumulated_eval_time': 1867.9935710430145, 'accumulated_logging_time': 4.310412168502808}
I0229 20:05:06.488375 140089845819136 logging_writer.py:48] [153951] accumulated_eval_time=1867.993571, accumulated_logging_time=4.310412, accumulated_submission_time=51569.529546, global_step=153951, preemption_count=0, score=51569.529546, test/accuracy=0.611500, test/loss=1.877956, test/num_examples=10000, total_duration=53447.323945, train/accuracy=0.929787, train/loss=0.248800, validation/accuracy=0.739680, validation/loss=1.086400, validation/num_examples=50000
I0229 20:05:23.228939 140089862604544 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.0209527015686035, loss=0.7719691395759583
I0229 20:05:56.687511 140089845819136 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.102741718292236, loss=0.8057093620300293
I0229 20:06:30.122774 140089862604544 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.147744655609131, loss=0.8256688117980957
I0229 20:07:03.548665 140089845819136 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.902423143386841, loss=0.856238603591919
I0229 20:07:36.987618 140089862604544 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.854032278060913, loss=0.7298250794410706
I0229 20:08:10.404325 140089845819136 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.005654335021973, loss=0.8303781151771545
I0229 20:08:43.867744 140089862604544 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.453282833099365, loss=0.9121131896972656
I0229 20:09:17.456647 140089845819136 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.60513973236084, loss=0.8337522149085999
I0229 20:09:50.907526 140089862604544 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.193294048309326, loss=0.8095835447311401
I0229 20:10:24.399580 140089845819136 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.321094512939453, loss=0.7331453561782837
I0229 20:10:57.832699 140089862604544 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.045220375061035, loss=0.7929685711860657
I0229 20:11:31.274888 140089845819136 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.925606727600098, loss=0.8988862633705139
I0229 20:12:04.743113 140089862604544 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.117122650146484, loss=0.7902128100395203
I0229 20:12:38.167183 140089845819136 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.9700798988342285, loss=0.8187052607536316
I0229 20:13:11.613219 140089862604544 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.086363792419434, loss=0.8379639983177185
I0229 20:13:36.527075 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:13:42.678987 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:13:50.890955 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:13:53.128444 140252611495744 submission_runner.py:411] Time since start: 53974.01s, 	Step: 155476, 	{'train/accuracy': 0.919343888759613, 'train/loss': 0.27055612206459045, 'validation/accuracy': 0.743939995765686, 'validation/loss': 1.0910815000534058, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.8548239469528198, 'test/num_examples': 10000, 'score': 52079.503801584244, 'total_duration': 53974.00850534439, 'accumulated_submission_time': 52079.503801584244, 'accumulated_eval_time': 1884.5948798656464, 'accumulated_logging_time': 4.365032911300659}
I0229 20:13:53.171005 140089854211840 logging_writer.py:48] [155476] accumulated_eval_time=1884.594880, accumulated_logging_time=4.365033, accumulated_submission_time=52079.503802, global_step=155476, preemption_count=0, score=52079.503802, test/accuracy=0.617400, test/loss=1.854824, test/num_examples=10000, total_duration=53974.008505, train/accuracy=0.919344, train/loss=0.270556, validation/accuracy=0.743940, validation/loss=1.091082, validation/num_examples=50000
I0229 20:14:01.528049 140089870997248 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.074110984802246, loss=0.7592512965202332
I0229 20:14:34.966891 140089854211840 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.394044399261475, loss=0.8996132016181946
I0229 20:15:08.493248 140089870997248 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.896524429321289, loss=0.874306857585907
I0229 20:15:41.928467 140089854211840 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.346131801605225, loss=0.805648922920227
I0229 20:16:15.415984 140089870997248 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.8815107345581055, loss=0.7752419710159302
I0229 20:16:48.859143 140089854211840 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.9777767658233643, loss=0.765877902507782
I0229 20:17:22.313846 140089870997248 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.111319065093994, loss=0.7686346173286438
I0229 20:17:55.749115 140089854211840 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.772732257843018, loss=0.8859716057777405
I0229 20:18:29.233146 140089870997248 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.100116729736328, loss=0.8007605075836182
I0229 20:19:02.715849 140089854211840 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.101613521575928, loss=0.8072129487991333
I0229 20:19:36.149633 140089870997248 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.0686516761779785, loss=0.6967368721961975
I0229 20:20:09.612201 140089854211840 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.313261032104492, loss=0.7975839376449585
I0229 20:20:43.090437 140089870997248 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.174301624298096, loss=0.8033778667449951
I0229 20:21:16.705319 140089854211840 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.119027137756348, loss=0.7763305902481079
I0229 20:21:50.180393 140089870997248 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.135930061340332, loss=0.8638172745704651
I0229 20:22:23.599515 140089854211840 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.968607187271118, loss=0.7223305702209473
I0229 20:22:23.606571 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:22:29.677406 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:22:37.828504 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:22:40.092391 140252611495744 submission_runner.py:411] Time since start: 54500.97s, 	Step: 157001, 	{'train/accuracy': 0.9226123690605164, 'train/loss': 0.2707573473453522, 'validation/accuracy': 0.7443199753761292, 'validation/loss': 1.0791981220245361, 'validation/num_examples': 50000, 'test/accuracy': 0.6187000274658203, 'test/loss': 1.8395804166793823, 'test/num_examples': 10000, 'score': 52589.87325882912, 'total_duration': 54500.97246050835, 'accumulated_submission_time': 52589.87325882912, 'accumulated_eval_time': 1901.0806233882904, 'accumulated_logging_time': 4.418842077255249}
I0229 20:22:40.139064 140089552271104 logging_writer.py:48] [157001] accumulated_eval_time=1901.080623, accumulated_logging_time=4.418842, accumulated_submission_time=52589.873259, global_step=157001, preemption_count=0, score=52589.873259, test/accuracy=0.618700, test/loss=1.839580, test/num_examples=10000, total_duration=54500.972461, train/accuracy=0.922612, train/loss=0.270757, validation/accuracy=0.744320, validation/loss=1.079198, validation/num_examples=50000
I0229 20:23:13.639919 140089770305280 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.447863578796387, loss=0.8417837619781494
I0229 20:23:47.089885 140089552271104 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.2459917068481445, loss=0.7921435236930847
I0229 20:24:20.550171 140089770305280 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.238511562347412, loss=0.8212910890579224
I0229 20:24:54.025506 140089552271104 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.093105316162109, loss=0.7505393624305725
I0229 20:25:27.464398 140089770305280 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.293215751647949, loss=0.7731466293334961
I0229 20:26:00.915012 140089552271104 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.262903213500977, loss=0.7816310524940491
I0229 20:26:34.412943 140089770305280 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.385807991027832, loss=0.8167734742164612
I0229 20:27:08.119367 140089552271104 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.642258167266846, loss=0.8147130012512207
I0229 20:27:41.571780 140089770305280 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.4930548667907715, loss=0.7874158024787903
I0229 20:28:15.027088 140089552271104 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.43983793258667, loss=0.8666003942489624
I0229 20:28:48.517618 140089770305280 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.589964389801025, loss=0.8772825598716736
I0229 20:29:21.995429 140089552271104 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.208106994628906, loss=0.8188647031784058
I0229 20:29:55.448372 140089770305280 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.576313018798828, loss=0.8443790674209595
I0229 20:30:28.907159 140089552271104 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.984880208969116, loss=0.7760369777679443
I0229 20:31:02.365350 140089770305280 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.452826499938965, loss=0.7630590796470642
I0229 20:31:10.195439 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:31:16.270179 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:31:24.441540 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:31:26.740948 140252611495744 submission_runner.py:411] Time since start: 55027.62s, 	Step: 158525, 	{'train/accuracy': 0.9240074753761292, 'train/loss': 0.26375100016593933, 'validation/accuracy': 0.7441999912261963, 'validation/loss': 1.0730669498443604, 'validation/num_examples': 50000, 'test/accuracy': 0.6181000471115112, 'test/loss': 1.8403584957122803, 'test/num_examples': 10000, 'score': 53099.864657878876, 'total_duration': 55027.62099266052, 'accumulated_submission_time': 53099.864657878876, 'accumulated_eval_time': 1917.6260554790497, 'accumulated_logging_time': 4.475511312484741}
I0229 20:31:26.783802 140089770305280 logging_writer.py:48] [158525] accumulated_eval_time=1917.626055, accumulated_logging_time=4.475511, accumulated_submission_time=53099.864658, global_step=158525, preemption_count=0, score=53099.864658, test/accuracy=0.618100, test/loss=1.840358, test/num_examples=10000, total_duration=55027.620993, train/accuracy=0.924007, train/loss=0.263751, validation/accuracy=0.744200, validation/loss=1.073067, validation/num_examples=50000
I0229 20:31:52.194887 140089778697984 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.048344135284424, loss=0.7381616830825806
I0229 20:32:25.643880 140089770305280 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.272660255432129, loss=0.7408602237701416
I0229 20:32:59.175041 140089778697984 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.664089202880859, loss=0.8086326718330383
I0229 20:33:32.712071 140089770305280 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.2000732421875, loss=0.7544326782226562
I0229 20:34:06.177520 140089778697984 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.518956661224365, loss=0.7784990072250366
I0229 20:34:39.627775 140089770305280 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.162380218505859, loss=0.7359000444412231
I0229 20:35:13.067429 140089778697984 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.294600009918213, loss=0.8099307417869568
I0229 20:35:46.542838 140089770305280 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.157605171203613, loss=0.826105535030365
I0229 20:36:20.012807 140089778697984 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.8186275959014893, loss=0.6714374423027039
I0229 20:36:53.452914 140089770305280 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.067312717437744, loss=0.6833958029747009
I0229 20:37:26.937802 140089778697984 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.589474201202393, loss=0.834920346736908
I0229 20:38:00.413589 140089770305280 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.481008529663086, loss=0.7586626410484314
I0229 20:38:33.848466 140089778697984 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.063233375549316, loss=0.7295109033584595
I0229 20:39:07.299642 140089770305280 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.497939586639404, loss=0.7582182884216309
I0229 20:39:40.843780 140089778697984 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.3491435050964355, loss=0.8409914970397949
I0229 20:39:57.019809 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:40:03.322000 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:40:11.502583 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:40:13.790148 140252611495744 submission_runner.py:411] Time since start: 55554.67s, 	Step: 160050, 	{'train/accuracy': 0.9275350570678711, 'train/loss': 0.25113847851753235, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.0759245157241821, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.8441084623336792, 'test/num_examples': 10000, 'score': 53610.03590750694, 'total_duration': 55554.67021775246, 'accumulated_submission_time': 53610.03590750694, 'accumulated_eval_time': 1934.3963434696198, 'accumulated_logging_time': 4.528732776641846}
I0229 20:40:13.843253 140089854211840 logging_writer.py:48] [160050] accumulated_eval_time=1934.396343, accumulated_logging_time=4.528733, accumulated_submission_time=53610.035908, global_step=160050, preemption_count=0, score=53610.035908, test/accuracy=0.621300, test/loss=1.844108, test/num_examples=10000, total_duration=55554.670218, train/accuracy=0.927535, train/loss=0.251138, validation/accuracy=0.745700, validation/loss=1.075925, validation/num_examples=50000
I0229 20:40:30.935420 140089862604544 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.224380970001221, loss=0.6938828825950623
I0229 20:41:04.372045 140089854211840 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.617408275604248, loss=0.7417502403259277
I0229 20:41:37.815639 140089862604544 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.68377685546875, loss=0.7793061137199402
I0229 20:42:11.312781 140089854211840 logging_writer.py:48] [160400] global_step=160400, grad_norm=5.292831897735596, loss=0.743642270565033
I0229 20:42:44.771247 140089862604544 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.674627304077148, loss=0.7100100517272949
I0229 20:43:18.240370 140089854211840 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.5107903480529785, loss=0.7555932402610779
I0229 20:43:51.700701 140089862604544 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.300135135650635, loss=0.7858442068099976
I0229 20:44:25.135488 140089854211840 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.358860492706299, loss=0.7007419466972351
I0229 20:44:58.599733 140089862604544 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.284050941467285, loss=0.7427710890769958
I0229 20:45:32.164784 140089854211840 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.3902788162231445, loss=0.8108409643173218
I0229 20:46:05.642175 140089862604544 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.490230083465576, loss=0.7443039417266846
I0229 20:46:39.077888 140089854211840 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.297149181365967, loss=0.7648211121559143
I0229 20:47:12.522083 140089862604544 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.143503189086914, loss=0.7171800136566162
I0229 20:47:45.982811 140089854211840 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.417979717254639, loss=0.7133662104606628
I0229 20:48:19.412700 140089862604544 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.522886276245117, loss=0.8124831318855286
I0229 20:48:44.019947 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:48:50.105988 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:48:58.256079 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:49:00.519301 140252611495744 submission_runner.py:411] Time since start: 56081.40s, 	Step: 161575, 	{'train/accuracy': 0.9306042790412903, 'train/loss': 0.2416285127401352, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.0739290714263916, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8437731266021729, 'test/num_examples': 10000, 'score': 54120.14770627022, 'total_duration': 56081.3993742466, 'accumulated_submission_time': 54120.14770627022, 'accumulated_eval_time': 1950.8956489562988, 'accumulated_logging_time': 4.591880559921265}
I0229 20:49:00.562442 140089778697984 logging_writer.py:48] [161575] accumulated_eval_time=1950.895649, accumulated_logging_time=4.591881, accumulated_submission_time=54120.147706, global_step=161575, preemption_count=0, score=54120.147706, test/accuracy=0.618400, test/loss=1.843773, test/num_examples=10000, total_duration=56081.399374, train/accuracy=0.930604, train/loss=0.241629, validation/accuracy=0.747240, validation/loss=1.073929, validation/num_examples=50000
I0229 20:49:09.288187 140089837426432 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.114900588989258, loss=0.7557564377784729
I0229 20:49:42.739780 140089778697984 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.472882270812988, loss=0.7900187373161316
I0229 20:50:16.210867 140089837426432 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.381402492523193, loss=0.7643085718154907
I0229 20:50:49.710869 140089778697984 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.238785743713379, loss=0.6734601855278015
I0229 20:51:23.165021 140089837426432 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.516899108886719, loss=0.7757990956306458
I0229 20:51:56.703047 140089778697984 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.426669120788574, loss=0.7081776857376099
I0229 20:52:30.149288 140089837426432 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.067922592163086, loss=0.6547677516937256
I0229 20:53:03.607609 140089778697984 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.523077011108398, loss=0.7875919342041016
I0229 20:53:37.093173 140089837426432 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.063435077667236, loss=0.7175807356834412
I0229 20:54:10.629652 140089778697984 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.493101596832275, loss=0.791903018951416
I0229 20:54:44.069675 140089837426432 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.220044136047363, loss=0.6924277544021606
I0229 20:55:17.513849 140089778697984 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.386593818664551, loss=0.7355968952178955
I0229 20:55:50.958601 140089837426432 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.117838382720947, loss=0.7206670045852661
I0229 20:56:24.405757 140089778697984 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.356222629547119, loss=0.7442612648010254
I0229 20:56:57.894676 140089837426432 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.762251377105713, loss=0.8481284976005554
I0229 20:57:30.570629 140252611495744 spec.py:321] Evaluating on the training split.
I0229 20:57:36.648860 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 20:57:44.837179 140252611495744 spec.py:349] Evaluating on the test split.
I0229 20:57:47.138741 140252611495744 submission_runner.py:411] Time since start: 56608.02s, 	Step: 163099, 	{'train/accuracy': 0.9433194994926453, 'train/loss': 0.2045804262161255, 'validation/accuracy': 0.7457999587059021, 'validation/loss': 1.0675607919692993, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.8366868495941162, 'test/num_examples': 10000, 'score': 54630.08940792084, 'total_duration': 56608.01881337166, 'accumulated_submission_time': 54630.08940792084, 'accumulated_eval_time': 1967.4637095928192, 'accumulated_logging_time': 4.646895170211792}
I0229 20:57:47.184423 140089854211840 logging_writer.py:48] [163099] accumulated_eval_time=1967.463710, accumulated_logging_time=4.646895, accumulated_submission_time=54630.089408, global_step=163099, preemption_count=0, score=54630.089408, test/accuracy=0.621300, test/loss=1.836687, test/num_examples=10000, total_duration=56608.018813, train/accuracy=0.943319, train/loss=0.204580, validation/accuracy=0.745800, validation/loss=1.067561, validation/num_examples=50000
I0229 20:57:47.865260 140089862604544 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.468844413757324, loss=0.7656993865966797
I0229 20:58:21.333044 140089854211840 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.288268566131592, loss=0.7751877903938293
I0229 20:58:54.787006 140089862604544 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.324903964996338, loss=0.6861729025840759
I0229 20:59:28.233497 140089854211840 logging_writer.py:48] [163400] global_step=163400, grad_norm=5.00160551071167, loss=0.6941673755645752
I0229 21:00:01.696021 140089862604544 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.310586452484131, loss=0.6623488664627075
I0229 21:00:35.184234 140089854211840 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.057382583618164, loss=0.7139703631401062
I0229 21:01:08.626672 140089862604544 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.528592586517334, loss=0.7915053367614746
I0229 21:01:42.093683 140089854211840 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.827763080596924, loss=0.7706568837165833
I0229 21:02:15.530108 140089862604544 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.8275678157806396, loss=0.6595311760902405
I0229 21:02:49.003338 140089854211840 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.766689777374268, loss=0.7642450332641602
I0229 21:03:22.426759 140089862604544 logging_writer.py:48] [164100] global_step=164100, grad_norm=5.2771172523498535, loss=0.7480380535125732
I0229 21:03:55.990343 140089854211840 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.120124340057373, loss=0.7093921303749084
I0229 21:04:29.453113 140089862604544 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.551490306854248, loss=0.7019439935684204
I0229 21:05:02.919641 140089854211840 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.294674873352051, loss=0.7774551510810852
I0229 21:05:36.363985 140089862604544 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.741966724395752, loss=0.7434787750244141
I0229 21:06:09.846748 140089854211840 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.5744123458862305, loss=0.7371968030929565
I0229 21:06:17.365340 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:06:23.583112 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:06:31.747146 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:06:34.001168 140252611495744 submission_runner.py:411] Time since start: 57134.88s, 	Step: 164624, 	{'train/accuracy': 0.9432597160339355, 'train/loss': 0.200706347823143, 'validation/accuracy': 0.7478399872779846, 'validation/loss': 1.0719140768051147, 'validation/num_examples': 50000, 'test/accuracy': 0.6182000041007996, 'test/loss': 1.8608922958374023, 'test/num_examples': 10000, 'score': 55140.205631017685, 'total_duration': 57134.881234407425, 'accumulated_submission_time': 55140.205631017685, 'accumulated_eval_time': 1984.099481344223, 'accumulated_logging_time': 4.70342230796814}
I0229 21:06:34.049545 140089837426432 logging_writer.py:48] [164624] accumulated_eval_time=1984.099481, accumulated_logging_time=4.703422, accumulated_submission_time=55140.205631, global_step=164624, preemption_count=0, score=55140.205631, test/accuracy=0.618200, test/loss=1.860892, test/num_examples=10000, total_duration=57134.881234, train/accuracy=0.943260, train/loss=0.200706, validation/accuracy=0.747840, validation/loss=1.071914, validation/num_examples=50000
I0229 21:06:59.824154 140089845819136 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.755965709686279, loss=0.8537552356719971
I0229 21:07:33.314115 140089837426432 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.518355846405029, loss=0.7292847633361816
I0229 21:08:06.751061 140089845819136 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.329751014709473, loss=0.6641927361488342
I0229 21:08:40.192563 140089837426432 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.090475559234619, loss=0.6483335494995117
I0229 21:09:13.650710 140089845819136 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.396143436431885, loss=0.7140724062919617
I0229 21:09:47.216621 140089837426432 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.680450439453125, loss=0.7053679823875427
I0229 21:10:20.655960 140089845819136 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.553439140319824, loss=0.7244149446487427
I0229 21:10:54.079488 140089837426432 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.6085991859436035, loss=0.722812294960022
I0229 21:11:27.558400 140089845819136 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.219408988952637, loss=0.6905978918075562
I0229 21:12:01.023351 140089837426432 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.660261631011963, loss=0.704676628112793
I0229 21:12:34.474981 140089845819136 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.457911014556885, loss=0.6616605520248413
I0229 21:13:07.984382 140089837426432 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.243679523468018, loss=0.6194632649421692
I0229 21:13:41.429658 140089845819136 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.06640625, loss=0.6231058835983276
I0229 21:14:14.865168 140089837426432 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.643509387969971, loss=0.7435482740402222
I0229 21:14:48.330831 140089845819136 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.361334323883057, loss=0.7551543712615967
I0229 21:15:04.241034 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:15:10.408912 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:15:18.594510 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:15:20.858763 140252611495744 submission_runner.py:411] Time since start: 57661.74s, 	Step: 166149, 	{'train/accuracy': 0.9441764950752258, 'train/loss': 0.19637244939804077, 'validation/accuracy': 0.7493000030517578, 'validation/loss': 1.0691767930984497, 'validation/num_examples': 50000, 'test/accuracy': 0.6215000152587891, 'test/loss': 1.8464528322219849, 'test/num_examples': 10000, 'score': 55650.33197426796, 'total_duration': 57661.73883676529, 'accumulated_submission_time': 55650.33197426796, 'accumulated_eval_time': 2000.7171757221222, 'accumulated_logging_time': 4.762126207351685}
I0229 21:15:20.908010 140089778697984 logging_writer.py:48] [166149] accumulated_eval_time=2000.717176, accumulated_logging_time=4.762126, accumulated_submission_time=55650.331974, global_step=166149, preemption_count=0, score=55650.331974, test/accuracy=0.621500, test/loss=1.846453, test/num_examples=10000, total_duration=57661.738837, train/accuracy=0.944176, train/loss=0.196372, validation/accuracy=0.749300, validation/loss=1.069177, validation/num_examples=50000
I0229 21:15:38.311118 140089854211840 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.526275157928467, loss=0.6643130779266357
I0229 21:16:11.911111 140089778697984 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.7372236251831055, loss=0.763235330581665
I0229 21:16:45.352153 140089854211840 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.444131851196289, loss=0.755962610244751
I0229 21:17:18.827443 140089778697984 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.37316370010376, loss=0.6966893672943115
I0229 21:17:52.295934 140089854211840 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.832740306854248, loss=0.6796274185180664
I0229 21:18:25.748299 140089778697984 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.1455488204956055, loss=0.6539461016654968
I0229 21:18:59.219549 140089854211840 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.463133335113525, loss=0.6957783699035645
I0229 21:19:32.673061 140089778697984 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.1311564445495605, loss=0.6618224382400513
I0229 21:20:06.158606 140089854211840 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.8331732749938965, loss=0.6704236268997192
I0229 21:20:39.637511 140089778697984 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.556855201721191, loss=0.7243375778198242
I0229 21:21:13.063225 140089854211840 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.469728469848633, loss=0.7323749661445618
I0229 21:21:46.627029 140089778697984 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.81444787979126, loss=0.7983683347702026
I0229 21:22:20.090354 140089854211840 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.44934606552124, loss=0.7504275441169739
I0229 21:22:53.568252 140089778697984 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.793395519256592, loss=0.626625657081604
I0229 21:23:27.099920 140089854211840 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.336264133453369, loss=0.6519433856010437
I0229 21:23:50.974830 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:23:57.040235 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:24:05.285095 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:24:07.567234 140252611495744 submission_runner.py:411] Time since start: 58188.45s, 	Step: 167673, 	{'train/accuracy': 0.9439173936843872, 'train/loss': 0.19878268241882324, 'validation/accuracy': 0.7494999766349792, 'validation/loss': 1.0682849884033203, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.8440488576889038, 'test/num_examples': 10000, 'score': 56160.3334069252, 'total_duration': 58188.44730424881, 'accumulated_submission_time': 56160.3334069252, 'accumulated_eval_time': 2017.3095281124115, 'accumulated_logging_time': 4.821733713150024}
I0229 21:24:07.612033 140089837426432 logging_writer.py:48] [167673] accumulated_eval_time=2017.309528, accumulated_logging_time=4.821734, accumulated_submission_time=56160.333407, global_step=167673, preemption_count=0, score=56160.333407, test/accuracy=0.624300, test/loss=1.844049, test/num_examples=10000, total_duration=58188.447304, train/accuracy=0.943917, train/loss=0.198783, validation/accuracy=0.749500, validation/loss=1.068285, validation/num_examples=50000
I0229 21:24:16.981743 140089845819136 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.141369819641113, loss=0.6254500150680542
I0229 21:24:50.430663 140089837426432 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.722713470458984, loss=0.6586065292358398
I0229 21:25:23.886863 140089845819136 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.568424701690674, loss=0.7031958699226379
I0229 21:25:57.316427 140089837426432 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.751837730407715, loss=0.6368008255958557
I0229 21:26:30.779688 140089845819136 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.324492454528809, loss=0.7067338824272156
I0229 21:27:04.236517 140089837426432 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.616556167602539, loss=0.6918784379959106
I0229 21:27:37.667805 140089845819136 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.992764949798584, loss=0.6706894040107727
I0229 21:28:11.237650 140089837426432 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.6779680252075195, loss=0.6625549793243408
I0229 21:28:44.685609 140089845819136 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.816455841064453, loss=0.6789331436157227
I0229 21:29:18.143081 140089837426432 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.543951988220215, loss=0.6869003772735596
I0229 21:29:51.575327 140089845819136 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.332265853881836, loss=0.6792194843292236
I0229 21:30:25.079030 140089837426432 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.547097682952881, loss=0.6226678490638733
I0229 21:30:58.534988 140089845819136 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.2968621253967285, loss=0.6276914477348328
I0229 21:31:31.981301 140089837426432 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.625564098358154, loss=0.7164233922958374
I0229 21:32:05.515524 140089845819136 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.453701496124268, loss=0.6467994451522827
I0229 21:32:37.748410 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:32:44.002538 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:32:52.075694 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:32:54.351161 140252611495744 submission_runner.py:411] Time since start: 58715.23s, 	Step: 169198, 	{'train/accuracy': 0.9467872977256775, 'train/loss': 0.18832539021968842, 'validation/accuracy': 0.7502999901771545, 'validation/loss': 1.0689047574996948, 'validation/num_examples': 50000, 'test/accuracy': 0.6219000220298767, 'test/loss': 1.847320795059204, 'test/num_examples': 10000, 'score': 56670.40325450897, 'total_duration': 58715.23123264313, 'accumulated_submission_time': 56670.40325450897, 'accumulated_eval_time': 2033.9122295379639, 'accumulated_logging_time': 4.877910375595093}
I0229 21:32:54.400223 140089770305280 logging_writer.py:48] [169198] accumulated_eval_time=2033.912230, accumulated_logging_time=4.877910, accumulated_submission_time=56670.403255, global_step=169198, preemption_count=0, score=56670.403255, test/accuracy=0.621900, test/loss=1.847321, test/num_examples=10000, total_duration=58715.231233, train/accuracy=0.946787, train/loss=0.188325, validation/accuracy=0.750300, validation/loss=1.068905, validation/num_examples=50000
I0229 21:32:55.412868 140089778697984 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.593505859375, loss=0.7174638509750366
I0229 21:33:28.869852 140089770305280 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.337219715118408, loss=0.6891013979911804
I0229 21:34:02.408748 140089778697984 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.810765266418457, loss=0.6924338936805725
I0229 21:34:35.856973 140089770305280 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.550513744354248, loss=0.6584158539772034
I0229 21:35:09.316193 140089778697984 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.47532320022583, loss=0.6909502148628235
I0229 21:35:42.793416 140089770305280 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.295121669769287, loss=0.6556291580200195
I0229 21:36:16.221396 140089778697984 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.611145496368408, loss=0.6625843048095703
I0229 21:36:49.689299 140089770305280 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.373802661895752, loss=0.6827162504196167
I0229 21:37:23.110440 140089778697984 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.3179168701171875, loss=0.6279687285423279
I0229 21:37:56.590653 140089770305280 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.3614325523376465, loss=0.7048875093460083
I0229 21:38:30.063096 140089778697984 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.9080071449279785, loss=0.7001839876174927
I0229 21:39:03.488137 140089770305280 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.455706596374512, loss=0.654854953289032
I0229 21:39:36.954313 140089778697984 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.970059871673584, loss=0.6649740934371948
I0229 21:40:10.561884 140089770305280 logging_writer.py:48] [170500] global_step=170500, grad_norm=5.124007225036621, loss=0.6796444654464722
I0229 21:40:44.021467 140089778697984 logging_writer.py:48] [170600] global_step=170600, grad_norm=5.029290676116943, loss=0.6622744202613831
I0229 21:41:17.484879 140089770305280 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.63287878036499, loss=0.6292605996131897
I0229 21:41:24.652537 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:41:30.740868 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:41:38.938116 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:41:41.198861 140252611495744 submission_runner.py:411] Time since start: 59242.08s, 	Step: 170723, 	{'train/accuracy': 0.9580875039100647, 'train/loss': 0.15853305160999298, 'validation/accuracy': 0.7505199909210205, 'validation/loss': 1.0651742219924927, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.8428583145141602, 'test/num_examples': 10000, 'score': 57180.59017777443, 'total_duration': 59242.078933000565, 'accumulated_submission_time': 57180.59017777443, 'accumulated_eval_time': 2050.458500146866, 'accumulated_logging_time': 4.937411308288574}
I0229 21:41:41.247523 140089778697984 logging_writer.py:48] [170723] accumulated_eval_time=2050.458500, accumulated_logging_time=4.937411, accumulated_submission_time=57180.590178, global_step=170723, preemption_count=0, score=57180.590178, test/accuracy=0.626000, test/loss=1.842858, test/num_examples=10000, total_duration=59242.078933, train/accuracy=0.958088, train/loss=0.158533, validation/accuracy=0.750520, validation/loss=1.065174, validation/num_examples=50000
I0229 21:42:07.326783 140089854211840 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.655912399291992, loss=0.6358342170715332
I0229 21:42:40.814520 140089778697984 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.209200382232666, loss=0.6882115006446838
I0229 21:43:14.299559 140089854211840 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.6575140953063965, loss=0.7295130491256714
I0229 21:43:47.749401 140089778697984 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.304370403289795, loss=0.7058806419372559
I0229 21:44:21.178374 140089854211840 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.747503757476807, loss=0.7555103898048401
I0229 21:44:54.653204 140089778697984 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.681130886077881, loss=0.6457575559616089
I0229 21:45:28.147298 140089854211840 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.840088367462158, loss=0.606370747089386
I0229 21:46:01.736937 140089778697984 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.234013557434082, loss=0.6478943824768066
I0229 21:46:35.196878 140089854211840 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.6057281494140625, loss=0.7551388144493103
I0229 21:47:08.642513 140089778697984 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.266443252563477, loss=0.6026162505149841
I0229 21:47:42.104796 140089854211840 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.970091819763184, loss=0.668677031993866
I0229 21:48:15.549758 140089778697984 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.480967044830322, loss=0.6319276690483093
I0229 21:48:48.982441 140089854211840 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.717367172241211, loss=0.7137001752853394
I0229 21:49:22.464789 140089778697984 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.537001132965088, loss=0.6841831207275391
I0229 21:49:55.993207 140089854211840 logging_writer.py:48] [172200] global_step=172200, grad_norm=5.028459548950195, loss=0.6616991758346558
I0229 21:50:11.502241 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:50:17.608049 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:50:25.696783 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:50:28.004457 140252611495744 submission_runner.py:411] Time since start: 59768.88s, 	Step: 172248, 	{'train/accuracy': 0.9566724896430969, 'train/loss': 0.1599704623222351, 'validation/accuracy': 0.7523999810218811, 'validation/loss': 1.0659375190734863, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8572520017623901, 'test/num_examples': 10000, 'score': 57690.77718710899, 'total_duration': 59768.884503126144, 'accumulated_submission_time': 57690.77718710899, 'accumulated_eval_time': 2066.9606392383575, 'accumulated_logging_time': 4.997782468795776}
I0229 21:50:28.055132 140089837426432 logging_writer.py:48] [172248] accumulated_eval_time=2066.960639, accumulated_logging_time=4.997782, accumulated_submission_time=57690.777187, global_step=172248, preemption_count=0, score=57690.777187, test/accuracy=0.627300, test/loss=1.857252, test/num_examples=10000, total_duration=59768.884503, train/accuracy=0.956672, train/loss=0.159970, validation/accuracy=0.752400, validation/loss=1.065938, validation/num_examples=50000
I0229 21:50:45.779508 140089845819136 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.515047550201416, loss=0.720793604850769
I0229 21:51:19.289617 140089837426432 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.821021556854248, loss=0.6849881410598755
I0229 21:51:52.769349 140089845819136 logging_writer.py:48] [172500] global_step=172500, grad_norm=5.045756816864014, loss=0.6733304262161255
I0229 21:52:26.503813 140089837426432 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.3416852951049805, loss=0.6643060445785522
I0229 21:52:59.984524 140089845819136 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.4422430992126465, loss=0.6154606342315674
I0229 21:53:33.431978 140089837426432 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.17517614364624, loss=0.6857218146324158
I0229 21:54:06.880178 140089845819136 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.2877678871154785, loss=0.6652425527572632
I0229 21:54:40.358707 140089837426432 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.627389430999756, loss=0.6109250783920288
I0229 21:55:13.805634 140089845819136 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.469483375549316, loss=0.7019357681274414
I0229 21:55:47.249599 140089837426432 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.20329475402832, loss=0.6227761507034302
I0229 21:56:20.726176 140089845819136 logging_writer.py:48] [173300] global_step=173300, grad_norm=5.034358978271484, loss=0.7080878019332886
I0229 21:56:54.169970 140089837426432 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.029257774353027, loss=0.6458737850189209
I0229 21:57:27.640982 140089845819136 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.646945476531982, loss=0.6672141551971436
I0229 21:58:01.098811 140089837426432 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.302215576171875, loss=0.6868352293968201
I0229 21:58:34.652594 140089845819136 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.004288196563721, loss=0.7130429744720459
I0229 21:58:58.218067 140252611495744 spec.py:321] Evaluating on the training split.
I0229 21:59:04.476017 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 21:59:12.604507 140252611495744 spec.py:349] Evaluating on the test split.
I0229 21:59:14.895770 140252611495744 submission_runner.py:411] Time since start: 60295.78s, 	Step: 173772, 	{'train/accuracy': 0.9553371667861938, 'train/loss': 0.1635049283504486, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 1.0613198280334473, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8496977090835571, 'test/num_examples': 10000, 'score': 58200.87525296211, 'total_duration': 60295.775837898254, 'accumulated_submission_time': 58200.87525296211, 'accumulated_eval_time': 2083.6383051872253, 'accumulated_logging_time': 5.0588531494140625}
I0229 21:59:14.942708 140089770305280 logging_writer.py:48] [173772] accumulated_eval_time=2083.638305, accumulated_logging_time=5.058853, accumulated_submission_time=58200.875253, global_step=173772, preemption_count=0, score=58200.875253, test/accuracy=0.628000, test/loss=1.849698, test/num_examples=10000, total_duration=60295.775838, train/accuracy=0.955337, train/loss=0.163505, validation/accuracy=0.753900, validation/loss=1.061320, validation/num_examples=50000
I0229 21:59:24.668958 140089778697984 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.503837585449219, loss=0.6771217584609985
I0229 21:59:58.147625 140089770305280 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.321381568908691, loss=0.6176567077636719
I0229 22:00:31.632537 140089778697984 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.825275897979736, loss=0.6321170926094055
I0229 22:01:05.086525 140089770305280 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.203861236572266, loss=0.6391809582710266
I0229 22:01:38.605804 140089778697984 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.611767768859863, loss=0.6932209134101868
I0229 22:02:12.071003 140089770305280 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.125937461853027, loss=0.6312384009361267
I0229 22:02:45.518047 140089778697984 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.156852722167969, loss=0.6735955476760864
I0229 22:03:18.956763 140089770305280 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.660581588745117, loss=0.657717227935791
I0229 22:03:52.426671 140089778697984 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.767475605010986, loss=0.7552221417427063
I0229 22:04:25.992392 140089770305280 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.586663722991943, loss=0.6431564092636108
I0229 22:04:59.430264 140089778697984 logging_writer.py:48] [174800] global_step=174800, grad_norm=5.062665939331055, loss=0.6417430639266968
I0229 22:05:32.893834 140089770305280 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.85037899017334, loss=0.6137755513191223
I0229 22:06:06.331539 140089778697984 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.964420318603516, loss=0.6610252261161804
I0229 22:06:39.807870 140089770305280 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.22111701965332, loss=0.620301365852356
I0229 22:07:13.260686 140089778697984 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.733240604400635, loss=0.672935426235199
I0229 22:07:45.168208 140252611495744 spec.py:321] Evaluating on the training split.
I0229 22:07:51.274280 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 22:07:59.451516 140252611495744 spec.py:349] Evaluating on the test split.
I0229 22:08:01.770439 140252611495744 submission_runner.py:411] Time since start: 60822.65s, 	Step: 175297, 	{'train/accuracy': 0.9546197056770325, 'train/loss': 0.16151142120361328, 'validation/accuracy': 0.7534799575805664, 'validation/loss': 1.060207486152649, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.851643443107605, 'test/num_examples': 10000, 'score': 58711.03318500519, 'total_duration': 60822.650486946106, 'accumulated_submission_time': 58711.03318500519, 'accumulated_eval_time': 2100.2404623031616, 'accumulated_logging_time': 5.118907690048218}
I0229 22:08:01.831621 140089862604544 logging_writer.py:48] [175297] accumulated_eval_time=2100.240462, accumulated_logging_time=5.118908, accumulated_submission_time=58711.033185, global_step=175297, preemption_count=0, score=58711.033185, test/accuracy=0.628000, test/loss=1.851643, test/num_examples=10000, total_duration=60822.650487, train/accuracy=0.954620, train/loss=0.161511, validation/accuracy=0.753480, validation/loss=1.060207, validation/num_examples=50000
I0229 22:08:03.188508 140089870997248 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.333252906799316, loss=0.5969477295875549
I0229 22:08:36.636444 140089862604544 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.544145584106445, loss=0.6690797209739685
I0229 22:09:10.153387 140089870997248 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.859565258026123, loss=0.617173433303833
I0229 22:09:43.596746 140089862604544 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.736058235168457, loss=0.6547613143920898
I0229 22:10:17.197216 140089870997248 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.739285945892334, loss=0.7424473166465759
I0229 22:10:50.672388 140089862604544 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.725636005401611, loss=0.7897442579269409
I0229 22:11:24.129544 140089870997248 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.646583557128906, loss=0.6838890910148621
I0229 22:11:57.595689 140089862604544 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.70683479309082, loss=0.7002586722373962
I0229 22:12:31.056204 140089870997248 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.225991725921631, loss=0.615357518196106
I0229 22:13:04.499164 140089862604544 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.391585350036621, loss=0.6708733439445496
I0229 22:13:37.990279 140089870997248 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.476112365722656, loss=0.6766537427902222
I0229 22:14:11.424313 140089862604544 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.541898727416992, loss=0.5859631896018982
I0229 22:14:44.897616 140089870997248 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.617876052856445, loss=0.6106389164924622
I0229 22:15:18.323223 140089862604544 logging_writer.py:48] [176600] global_step=176600, grad_norm=5.112998008728027, loss=0.6588255763053894
I0229 22:15:51.764639 140089870997248 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.7240424156188965, loss=0.6461899280548096
I0229 22:16:25.328083 140089862604544 logging_writer.py:48] [176800] global_step=176800, grad_norm=5.046757221221924, loss=0.6596186757087708
I0229 22:16:31.821689 140252611495744 spec.py:321] Evaluating on the training split.
I0229 22:16:37.880941 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 22:16:46.003138 140252611495744 spec.py:349] Evaluating on the test split.
I0229 22:16:48.292299 140252611495744 submission_runner.py:411] Time since start: 61349.17s, 	Step: 176821, 	{'train/accuracy': 0.9572106003761292, 'train/loss': 0.15862207114696503, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0589282512664795, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.847219467163086, 'test/num_examples': 10000, 'score': 59220.95730185509, 'total_duration': 61349.172367334366, 'accumulated_submission_time': 59220.95730185509, 'accumulated_eval_time': 2116.7110154628754, 'accumulated_logging_time': 5.191303253173828}
I0229 22:16:48.338971 140089552271104 logging_writer.py:48] [176821] accumulated_eval_time=2116.711015, accumulated_logging_time=5.191303, accumulated_submission_time=59220.957302, global_step=176821, preemption_count=0, score=59220.957302, test/accuracy=0.626100, test/loss=1.847219, test/num_examples=10000, total_duration=61349.172367, train/accuracy=0.957211, train/loss=0.158622, validation/accuracy=0.754060, validation/loss=1.058928, validation/num_examples=50000
I0229 22:17:15.105055 140089770305280 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.445378303527832, loss=0.6487573385238647
I0229 22:17:48.578268 140089552271104 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.276119709014893, loss=0.6184665560722351
I0229 22:18:22.027018 140089770305280 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.396247863769531, loss=0.5912840366363525
I0229 22:18:55.456571 140089552271104 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.274762153625488, loss=0.5832386016845703
I0229 22:19:28.924640 140089770305280 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.472170352935791, loss=0.6583417654037476
I0229 22:20:02.363101 140089552271104 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.023632049560547, loss=0.621593177318573
I0229 22:20:35.843947 140089770305280 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.581212043762207, loss=0.6845816969871521
I0229 22:21:09.326413 140089552271104 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.336203098297119, loss=0.6520276665687561
I0229 22:21:42.777840 140089770305280 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.514925479888916, loss=0.6330410838127136
I0229 22:22:16.516769 140089552271104 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.925358772277832, loss=0.6856430768966675
I0229 22:22:50.006710 140089770305280 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.666662693023682, loss=0.638629138469696
I0229 22:23:23.441199 140089552271104 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.171452045440674, loss=0.5782809257507324
I0229 22:23:56.915073 140089770305280 logging_writer.py:48] [178100] global_step=178100, grad_norm=5.022851467132568, loss=0.6353903412818909
I0229 22:24:30.349340 140089552271104 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.2130513191223145, loss=0.6025310754776001
I0229 22:25:03.816488 140089770305280 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.579268932342529, loss=0.629353404045105
I0229 22:25:18.337168 140252611495744 spec.py:321] Evaluating on the training split.
I0229 22:25:24.554176 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 22:25:32.743216 140252611495744 spec.py:349] Evaluating on the test split.
I0229 22:25:35.044082 140252611495744 submission_runner.py:411] Time since start: 61875.92s, 	Step: 178345, 	{'train/accuracy': 0.95804762840271, 'train/loss': 0.153394877910614, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0549086332321167, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.8444610834121704, 'test/num_examples': 10000, 'score': 59730.89150452614, 'total_duration': 61875.924149274826, 'accumulated_submission_time': 59730.89150452614, 'accumulated_eval_time': 2133.4178862571716, 'accumulated_logging_time': 5.247831106185913}
I0229 22:25:35.090481 140089552271104 logging_writer.py:48] [178345] accumulated_eval_time=2133.417886, accumulated_logging_time=5.247831, accumulated_submission_time=59730.891505, global_step=178345, preemption_count=0, score=59730.891505, test/accuracy=0.625800, test/loss=1.844461, test/num_examples=10000, total_duration=61875.924149, train/accuracy=0.958048, train/loss=0.153395, validation/accuracy=0.755120, validation/loss=1.054909, validation/num_examples=50000
I0229 22:25:53.821342 140089770305280 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.794643878936768, loss=0.5985533595085144
I0229 22:26:27.279674 140089552271104 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.871878623962402, loss=0.6022412776947021
I0229 22:27:00.784098 140089770305280 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.366829872131348, loss=0.5945734977722168
I0229 22:27:34.241193 140089552271104 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.320681571960449, loss=0.5974490642547607
I0229 22:28:07.743960 140089770305280 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.835784912109375, loss=0.6684761643409729
I0229 22:28:41.283463 140089552271104 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.181366920471191, loss=0.5980504751205444
I0229 22:29:14.758251 140089770305280 logging_writer.py:48] [179000] global_step=179000, grad_norm=5.390310287475586, loss=0.6236308813095093
I0229 22:29:48.204735 140089552271104 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.461440563201904, loss=0.6343086957931519
I0229 22:30:21.712762 140089770305280 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.657703876495361, loss=0.6076262593269348
I0229 22:30:55.212712 140089552271104 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.3096604347229, loss=0.5882866978645325
I0229 22:31:28.697843 140089770305280 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.706169128417969, loss=0.5850270986557007
I0229 22:32:02.135088 140089552271104 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.725593090057373, loss=0.6538589596748352
I0229 22:32:35.603347 140089770305280 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.358574390411377, loss=0.6562314033508301
I0229 22:33:09.036056 140089552271104 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.209598541259766, loss=0.6509731411933899
I0229 22:33:42.477840 140089770305280 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.512711048126221, loss=0.6145918965339661
I0229 22:34:05.082029 140252611495744 spec.py:321] Evaluating on the training split.
I0229 22:34:11.275904 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 22:34:19.328138 140252611495744 spec.py:349] Evaluating on the test split.
I0229 22:34:21.606166 140252611495744 submission_runner.py:411] Time since start: 62402.49s, 	Step: 179869, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14461882412433624, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 1.056932806968689, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8462977409362793, 'test/num_examples': 10000, 'score': 60240.81658220291, 'total_duration': 62402.48623228073, 'accumulated_submission_time': 60240.81658220291, 'accumulated_eval_time': 2149.941981315613, 'accumulated_logging_time': 5.304441213607788}
I0229 22:34:21.657760 140089845819136 logging_writer.py:48] [179869] accumulated_eval_time=2149.941981, accumulated_logging_time=5.304441, accumulated_submission_time=60240.816582, global_step=179869, preemption_count=0, score=60240.816582, test/accuracy=0.627900, test/loss=1.846298, test/num_examples=10000, total_duration=62402.486232, train/accuracy=0.961595, train/loss=0.144619, validation/accuracy=0.755400, validation/loss=1.056933, validation/num_examples=50000
I0229 22:34:32.350946 140089862604544 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.571391582489014, loss=0.6109445691108704
I0229 22:35:05.958499 140089845819136 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.773595333099365, loss=0.5871771574020386
I0229 22:35:39.428153 140089862604544 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.671064376831055, loss=0.6003859639167786
I0229 22:36:12.879054 140089845819136 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.406329154968262, loss=0.6069289445877075
I0229 22:36:46.335974 140089862604544 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.456258296966553, loss=0.5798139572143555
I0229 22:37:19.771716 140089845819136 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.529541015625, loss=0.6736007928848267
I0229 22:37:53.242621 140089862604544 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.77435302734375, loss=0.7152056097984314
I0229 22:38:26.663013 140089845819136 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.595582008361816, loss=0.6639454364776611
I0229 22:39:00.132236 140089862604544 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.534726619720459, loss=0.6225613355636597
I0229 22:39:33.574635 140089845819136 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.446038246154785, loss=0.6373656988143921
I0229 22:40:07.017636 140089862604544 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.816585063934326, loss=0.6604862809181213
I0229 22:40:40.516388 140089845819136 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.556765556335449, loss=0.7249903678894043
I0229 22:41:14.054202 140089862604544 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.470937728881836, loss=0.5664079785346985
I0229 22:41:47.583106 140089845819136 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.764737129211426, loss=0.7016922235488892
I0229 22:42:21.055930 140089862604544 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.581567764282227, loss=0.6182609796524048
I0229 22:42:51.611624 140252611495744 spec.py:321] Evaluating on the training split.
I0229 22:42:57.736221 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 22:43:06.162891 140252611495744 spec.py:349] Evaluating on the test split.
I0229 22:43:08.434793 140252611495744 submission_runner.py:411] Time since start: 62929.31s, 	Step: 181393, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.1480083018541336, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0549815893173218, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8437385559082031, 'test/num_examples': 10000, 'score': 60750.70178294182, 'total_duration': 62929.314858675, 'accumulated_submission_time': 60750.70178294182, 'accumulated_eval_time': 2166.7651064395905, 'accumulated_logging_time': 5.369521379470825}
I0229 22:43:08.485509 140089770305280 logging_writer.py:48] [181393] accumulated_eval_time=2166.765106, accumulated_logging_time=5.369521, accumulated_submission_time=60750.701783, global_step=181393, preemption_count=0, score=60750.701783, test/accuracy=0.627900, test/loss=1.843739, test/num_examples=10000, total_duration=62929.314859, train/accuracy=0.960001, train/loss=0.148008, validation/accuracy=0.756100, validation/loss=1.054982, validation/num_examples=50000
I0229 22:43:11.173816 140089778697984 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.727734088897705, loss=0.6042969226837158
I0229 22:43:44.688063 140089770305280 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.225910186767578, loss=0.588778555393219
I0229 22:44:18.131632 140089778697984 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.252051830291748, loss=0.5471298694610596
I0229 22:44:51.572532 140089770305280 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.6592583656311035, loss=0.6222677826881409
I0229 22:45:25.040794 140089778697984 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.152880668640137, loss=0.557242751121521
I0229 22:45:58.472171 140089770305280 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.529043674468994, loss=0.6166195869445801
I0229 22:46:31.924006 140089778697984 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.438753604888916, loss=0.5880646705627441
I0229 22:47:05.485259 140089770305280 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.257503986358643, loss=0.6436800956726074
I0229 22:47:38.914234 140089778697984 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.81505012512207, loss=0.5740825533866882
I0229 22:48:12.360837 140089770305280 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.54709529876709, loss=0.6285445690155029
I0229 22:48:45.835715 140089778697984 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.181973457336426, loss=0.6056003570556641
I0229 22:49:19.269489 140089770305280 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.821156024932861, loss=0.6097712516784668
I0229 22:49:52.760923 140089778697984 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.9691691398620605, loss=0.6456751823425293
I0229 22:50:26.224408 140089770305280 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.233780384063721, loss=0.5574949979782104
I0229 22:50:59.652438 140089778697984 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.108710765838623, loss=0.7160347104072571
I0229 22:51:33.106410 140089770305280 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.52457332611084, loss=0.6758719086647034
I0229 22:51:38.592918 140252611495744 spec.py:321] Evaluating on the training split.
I0229 22:51:44.780400 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 22:51:52.863806 140252611495744 spec.py:349] Evaluating on the test split.
I0229 22:51:55.142440 140252611495744 submission_runner.py:411] Time since start: 63456.02s, 	Step: 182918, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.1445799022912979, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0538650751113892, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8420039415359497, 'test/num_examples': 10000, 'score': 61260.74364876747, 'total_duration': 63456.022471666336, 'accumulated_submission_time': 61260.74364876747, 'accumulated_eval_time': 2183.3145368099213, 'accumulated_logging_time': 5.4302978515625}
I0229 22:51:55.194287 140089854211840 logging_writer.py:48] [182918] accumulated_eval_time=2183.314537, accumulated_logging_time=5.430298, accumulated_submission_time=61260.743649, global_step=182918, preemption_count=0, score=61260.743649, test/accuracy=0.627400, test/loss=1.842004, test/num_examples=10000, total_duration=63456.022472, train/accuracy=0.961416, train/loss=0.144580, validation/accuracy=0.755140, validation/loss=1.053865, validation/num_examples=50000
I0229 22:52:22.954816 140089862604544 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.528668403625488, loss=0.569395899772644
I0229 22:52:56.497202 140089854211840 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.246317386627197, loss=0.567562460899353
I0229 22:53:29.961558 140089862604544 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.465297222137451, loss=0.584376335144043
I0229 22:54:03.454801 140089854211840 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.54327392578125, loss=0.633126437664032
I0229 22:54:36.963451 140089862604544 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.993707180023193, loss=0.6845129728317261
I0229 22:55:10.429294 140089854211840 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.963859558105469, loss=0.6343464851379395
I0229 22:55:43.851304 140089862604544 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.691370964050293, loss=0.6072857975959778
I0229 22:56:17.346090 140089854211840 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.7222981452941895, loss=0.67598557472229
I0229 22:56:50.805247 140089862604544 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.3058953285217285, loss=0.5833839774131775
I0229 22:57:24.246585 140089854211840 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.2087721824646, loss=0.5936471223831177
I0229 22:57:57.724569 140089862604544 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.748786449432373, loss=0.6636740565299988
I0229 22:58:31.150300 140089854211840 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.4800519943237305, loss=0.5727584362030029
I0229 22:59:04.775996 140089862604544 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.662003040313721, loss=0.5765272378921509
I0229 22:59:38.297094 140089854211840 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.827263355255127, loss=0.7087578177452087
I0229 23:00:11.777482 140089862604544 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.136803150177002, loss=0.591049313545227
I0229 23:00:25.283823 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:00:31.398649 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:00:39.555693 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:00:41.824693 140252611495744 submission_runner.py:411] Time since start: 63982.70s, 	Step: 184442, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14817655086517334, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.054193139076233, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8438823223114014, 'test/num_examples': 10000, 'score': 61770.76652598381, 'total_duration': 63982.70475888252, 'accumulated_submission_time': 61770.76652598381, 'accumulated_eval_time': 2199.8553504943848, 'accumulated_logging_time': 5.492738246917725}
I0229 23:00:41.875328 140089770305280 logging_writer.py:48] [184442] accumulated_eval_time=2199.855350, accumulated_logging_time=5.492738, accumulated_submission_time=61770.766526, global_step=184442, preemption_count=0, score=61770.766526, test/accuracy=0.626700, test/loss=1.843882, test/num_examples=10000, total_duration=63982.704759, train/accuracy=0.960599, train/loss=0.148177, validation/accuracy=0.755240, validation/loss=1.054193, validation/num_examples=50000
I0229 23:01:01.609195 140089778697984 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.566766262054443, loss=0.6799366474151611
I0229 23:01:35.106709 140089770305280 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.40049934387207, loss=0.6650563478469849
I0229 23:02:08.575107 140089778697984 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.488286018371582, loss=0.6500673890113831
I0229 23:02:42.089841 140089770305280 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.688794136047363, loss=0.6328015923500061
I0229 23:03:15.546829 140089778697984 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.622005462646484, loss=0.6014724969863892
I0229 23:03:48.994520 140089770305280 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.6119465827941895, loss=0.5961825251579285
I0229 23:04:22.425801 140089778697984 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.822695255279541, loss=0.6044953465461731
I0229 23:04:55.982076 140089770305280 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.5717620849609375, loss=0.6734634041786194
I0229 23:05:29.412128 140089778697984 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.9487433433532715, loss=0.6753925085067749
I0229 23:06:02.854057 140089770305280 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.728235244750977, loss=0.6603842973709106
I0229 23:06:36.295698 140089778697984 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.592197418212891, loss=0.6410833597183228
I0229 23:07:09.758355 140089770305280 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.205970287322998, loss=0.5820856690406799
I0229 23:07:43.201746 140089778697984 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.261911392211914, loss=0.6123478412628174
I0229 23:08:16.646239 140089770305280 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.450390338897705, loss=0.6216726303100586
I0229 23:08:50.078356 140089778697984 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.3503804206848145, loss=0.6279029846191406
I0229 23:09:11.930185 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:09:18.174297 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:09:26.252000 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:09:28.534771 140252611495744 submission_runner.py:411] Time since start: 64509.41s, 	Step: 185967, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14852996170520782, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0535261631011963, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8431257009506226, 'test/num_examples': 10000, 'score': 62280.755940914154, 'total_duration': 64509.414836645126, 'accumulated_submission_time': 62280.755940914154, 'accumulated_eval_time': 2216.459883451462, 'accumulated_logging_time': 5.554203510284424}
I0229 23:09:28.587327 140089854211840 logging_writer.py:48] [185967] accumulated_eval_time=2216.459883, accumulated_logging_time=5.554204, accumulated_submission_time=62280.755941, global_step=185967, preemption_count=0, score=62280.755941, test/accuracy=0.627100, test/loss=1.843126, test/num_examples=10000, total_duration=64509.414837, train/accuracy=0.960698, train/loss=0.148530, validation/accuracy=0.755860, validation/loss=1.053526, validation/num_examples=50000
I0229 23:09:39.965404 140089862604544 logging_writer.py:48] [186000] global_step=186000, grad_norm=4.778067111968994, loss=0.6037134528160095
I0229 23:10:13.436700 140089854211840 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.9773077964782715, loss=0.635775089263916
I0229 23:10:46.866967 140089862604544 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.571348667144775, loss=0.6109764575958252
I0229 23:11:20.423565 140089854211840 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.5180511474609375, loss=0.6202999353408813
I0229 23:11:53.874001 140089862604544 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.901252269744873, loss=0.613706111907959
I0229 23:12:27.333944 140089854211840 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.318490982055664, loss=0.6283706426620483
I0229 23:13:00.836949 140089862604544 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.661849021911621, loss=0.5677598118782043
I0229 23:13:34.256567 140089854211840 logging_writer.py:48] [186700] global_step=186700, grad_norm=4.329949855804443, loss=0.6013956069946289
I0229 23:14:07.722631 140089862604544 logging_writer.py:48] [186800] global_step=186800, grad_norm=4.371547222137451, loss=0.5364479422569275
I0229 23:14:41.190526 140089854211840 logging_writer.py:48] [186900] global_step=186900, grad_norm=5.031790733337402, loss=0.6856659650802612
I0229 23:15:14.669424 140089862604544 logging_writer.py:48] [187000] global_step=187000, grad_norm=4.303043365478516, loss=0.5940548777580261
I0229 23:15:48.084037 140089854211840 logging_writer.py:48] [187100] global_step=187100, grad_norm=4.8434062004089355, loss=0.6112141013145447
I0229 23:16:21.596443 140089862604544 logging_writer.py:48] [187200] global_step=187200, grad_norm=4.7812180519104, loss=0.6129646897315979
I0229 23:16:55.035948 140089854211840 logging_writer.py:48] [187300] global_step=187300, grad_norm=4.358046054840088, loss=0.5834399461746216
I0229 23:17:28.550843 140089862604544 logging_writer.py:48] [187400] global_step=187400, grad_norm=4.460229396820068, loss=0.638982892036438
I0229 23:17:58.787518 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:18:05.071992 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:18:13.303383 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:18:15.519633 140252611495744 submission_runner.py:411] Time since start: 65036.40s, 	Step: 187492, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14638006687164307, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0537035465240479, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8425025939941406, 'test/num_examples': 10000, 'score': 62790.8897960186, 'total_duration': 65036.399695158005, 'accumulated_submission_time': 62790.8897960186, 'accumulated_eval_time': 2233.191954135895, 'accumulated_logging_time': 5.617657661437988}
I0229 23:18:15.568536 140089778697984 logging_writer.py:48] [187492] accumulated_eval_time=2233.191954, accumulated_logging_time=5.617658, accumulated_submission_time=62790.889796, global_step=187492, preemption_count=0, score=62790.889796, test/accuracy=0.626800, test/loss=1.842503, test/num_examples=10000, total_duration=65036.399695, train/accuracy=0.960579, train/loss=0.146380, validation/accuracy=0.755800, validation/loss=1.053704, validation/num_examples=50000
I0229 23:18:18.594645 140089837426432 logging_writer.py:48] [187500] global_step=187500, grad_norm=5.0489935874938965, loss=0.5794029235839844
I0229 23:18:52.053350 140089778697984 logging_writer.py:48] [187600] global_step=187600, grad_norm=4.750178813934326, loss=0.6231351494789124
I0229 23:19:25.534439 140089837426432 logging_writer.py:48] [187700] global_step=187700, grad_norm=4.794527053833008, loss=0.6913766860961914
I0229 23:19:58.998046 140089778697984 logging_writer.py:48] [187800] global_step=187800, grad_norm=4.772191524505615, loss=0.6252954006195068
I0229 23:20:32.436872 140089837426432 logging_writer.py:48] [187900] global_step=187900, grad_norm=4.6116180419921875, loss=0.6206044554710388
I0229 23:21:05.883152 140089778697984 logging_writer.py:48] [188000] global_step=188000, grad_norm=4.748085975646973, loss=0.6244266629219055
I0229 23:21:39.318171 140089837426432 logging_writer.py:48] [188100] global_step=188100, grad_norm=4.677742958068848, loss=0.6389875411987305
I0229 23:22:12.777656 140089778697984 logging_writer.py:48] [188200] global_step=188200, grad_norm=4.624058246612549, loss=0.5899408459663391
I0229 23:22:46.235458 140089837426432 logging_writer.py:48] [188300] global_step=188300, grad_norm=4.254281044006348, loss=0.6365950703620911
I0229 23:23:19.750881 140089778697984 logging_writer.py:48] [188400] global_step=188400, grad_norm=5.359372615814209, loss=0.6568260788917542
I0229 23:23:53.197931 140089837426432 logging_writer.py:48] [188500] global_step=188500, grad_norm=4.295638084411621, loss=0.5919374227523804
I0229 23:24:26.663643 140089778697984 logging_writer.py:48] [188600] global_step=188600, grad_norm=4.916491985321045, loss=0.6572626233100891
I0229 23:25:00.101335 140089837426432 logging_writer.py:48] [188700] global_step=188700, grad_norm=4.056114196777344, loss=0.6489019393920898
I0229 23:25:33.559427 140089778697984 logging_writer.py:48] [188800] global_step=188800, grad_norm=4.1631622314453125, loss=0.5376083254814148
I0229 23:26:06.998691 140089837426432 logging_writer.py:48] [188900] global_step=188900, grad_norm=4.680171012878418, loss=0.5777938961982727
I0229 23:26:40.462556 140089778697984 logging_writer.py:48] [189000] global_step=189000, grad_norm=4.337271690368652, loss=0.5953992605209351
I0229 23:26:45.621608 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:26:51.719389 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:26:59.816767 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:27:02.139868 140252611495744 submission_runner.py:411] Time since start: 65563.02s, 	Step: 189017, 	{'train/accuracy': 0.9619937539100647, 'train/loss': 0.14264976978302002, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0539004802703857, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8438462018966675, 'test/num_examples': 10000, 'score': 63300.8772277832, 'total_duration': 65563.01993250847, 'accumulated_submission_time': 63300.8772277832, 'accumulated_eval_time': 2249.7101545333862, 'accumulated_logging_time': 5.676936149597168}
I0229 23:27:02.189495 140089552271104 logging_writer.py:48] [189017] accumulated_eval_time=2249.710155, accumulated_logging_time=5.676936, accumulated_submission_time=63300.877228, global_step=189017, preemption_count=0, score=63300.877228, test/accuracy=0.628100, test/loss=1.843846, test/num_examples=10000, total_duration=65563.019933, train/accuracy=0.961994, train/loss=0.142650, validation/accuracy=0.755860, validation/loss=1.053900, validation/num_examples=50000
I0229 23:27:30.276975 140089770305280 logging_writer.py:48] [189100] global_step=189100, grad_norm=4.6390700340271, loss=0.6244133710861206
I0229 23:28:03.758243 140089552271104 logging_writer.py:48] [189200] global_step=189200, grad_norm=4.32006311416626, loss=0.576927661895752
I0229 23:28:37.187970 140089770305280 logging_writer.py:48] [189300] global_step=189300, grad_norm=4.4445977210998535, loss=0.6124894618988037
I0229 23:29:10.636269 140089552271104 logging_writer.py:48] [189400] global_step=189400, grad_norm=4.743135929107666, loss=0.6376226544380188
I0229 23:29:44.188531 140089770305280 logging_writer.py:48] [189500] global_step=189500, grad_norm=5.2750725746154785, loss=0.690841555595398
I0229 23:30:17.619755 140089552271104 logging_writer.py:48] [189600] global_step=189600, grad_norm=4.622487545013428, loss=0.5974081754684448
I0229 23:30:51.074518 140089770305280 logging_writer.py:48] [189700] global_step=189700, grad_norm=4.97785758972168, loss=0.578009843826294
I0229 23:31:24.559153 140089552271104 logging_writer.py:48] [189800] global_step=189800, grad_norm=4.681689262390137, loss=0.6275600790977478
I0229 23:31:57.981632 140089770305280 logging_writer.py:48] [189900] global_step=189900, grad_norm=4.328115463256836, loss=0.613699197769165
I0229 23:32:31.449745 140089552271104 logging_writer.py:48] [190000] global_step=190000, grad_norm=4.915765285491943, loss=0.6806485652923584
I0229 23:33:04.872303 140089770305280 logging_writer.py:48] [190100] global_step=190100, grad_norm=4.7221174240112305, loss=0.6134179830551147
I0229 23:33:38.322710 140089552271104 logging_writer.py:48] [190200] global_step=190200, grad_norm=5.246295928955078, loss=0.6368657350540161
I0229 23:34:11.753312 140089770305280 logging_writer.py:48] [190300] global_step=190300, grad_norm=5.16494607925415, loss=0.5477307438850403
I0229 23:34:45.214996 140089552271104 logging_writer.py:48] [190400] global_step=190400, grad_norm=4.168940544128418, loss=0.6114864349365234
I0229 23:35:18.883012 140089770305280 logging_writer.py:48] [190500] global_step=190500, grad_norm=4.669248104095459, loss=0.5665631890296936
I0229 23:35:32.383177 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:35:38.521905 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:35:46.720247 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:35:49.009466 140252611495744 submission_runner.py:411] Time since start: 66089.89s, 	Step: 190542, 	{'train/accuracy': 0.9632294178009033, 'train/loss': 0.14245198667049408, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0543040037155151, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8426756858825684, 'test/num_examples': 10000, 'score': 63811.004403591156, 'total_duration': 66089.88953256607, 'accumulated_submission_time': 63811.004403591156, 'accumulated_eval_time': 2266.336406469345, 'accumulated_logging_time': 5.737735748291016}
I0229 23:35:49.061331 140089837426432 logging_writer.py:48] [190542] accumulated_eval_time=2266.336406, accumulated_logging_time=5.737736, accumulated_submission_time=63811.004404, global_step=190542, preemption_count=0, score=63811.004404, test/accuracy=0.626500, test/loss=1.842676, test/num_examples=10000, total_duration=66089.889533, train/accuracy=0.963229, train/loss=0.142452, validation/accuracy=0.755880, validation/loss=1.054304, validation/num_examples=50000
I0229 23:36:08.800633 140089845819136 logging_writer.py:48] [190600] global_step=190600, grad_norm=4.144671440124512, loss=0.5761788487434387
I0229 23:36:42.274623 140089837426432 logging_writer.py:48] [190700] global_step=190700, grad_norm=4.6900248527526855, loss=0.7422927618026733
I0229 23:37:15.703297 140089845819136 logging_writer.py:48] [190800] global_step=190800, grad_norm=4.949509620666504, loss=0.6267536282539368
I0229 23:37:49.178117 140089837426432 logging_writer.py:48] [190900] global_step=190900, grad_norm=4.815210342407227, loss=0.6953001022338867
I0229 23:38:22.606063 140089845819136 logging_writer.py:48] [191000] global_step=191000, grad_norm=4.596707820892334, loss=0.6358609199523926
I0229 23:38:56.078929 140089837426432 logging_writer.py:48] [191100] global_step=191100, grad_norm=5.150791645050049, loss=0.6264835000038147
I0229 23:39:29.514439 140089845819136 logging_writer.py:48] [191200] global_step=191200, grad_norm=3.9694056510925293, loss=0.5730001926422119
I0229 23:40:02.956731 140089837426432 logging_writer.py:48] [191300] global_step=191300, grad_norm=4.560117244720459, loss=0.6114701628684998
I0229 23:40:36.424563 140089845819136 logging_writer.py:48] [191400] global_step=191400, grad_norm=4.728490352630615, loss=0.5854713320732117
I0229 23:41:09.868144 140089837426432 logging_writer.py:48] [191500] global_step=191500, grad_norm=4.182063579559326, loss=0.6365660429000854
I0229 23:41:43.488106 140089845819136 logging_writer.py:48] [191600] global_step=191600, grad_norm=4.656746864318848, loss=0.6160179376602173
I0229 23:42:16.974061 140089837426432 logging_writer.py:48] [191700] global_step=191700, grad_norm=5.000046730041504, loss=0.6773675680160522
I0229 23:42:50.468910 140089845819136 logging_writer.py:48] [191800] global_step=191800, grad_norm=5.237472057342529, loss=0.6135149002075195
I0229 23:43:23.890386 140089837426432 logging_writer.py:48] [191900] global_step=191900, grad_norm=5.038933753967285, loss=0.5812123417854309
I0229 23:43:57.330621 140089845819136 logging_writer.py:48] [192000] global_step=192000, grad_norm=4.199538230895996, loss=0.6303091645240784
I0229 23:44:19.213826 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:44:25.332315 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:44:33.442425 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:44:35.697448 140252611495744 submission_runner.py:411] Time since start: 66616.58s, 	Step: 192067, 	{'train/accuracy': 0.9621531963348389, 'train/loss': 0.1447095423936844, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0540560483932495, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8448634147644043, 'test/num_examples': 10000, 'score': 64321.091645240784, 'total_duration': 66616.57750272751, 'accumulated_submission_time': 64321.091645240784, 'accumulated_eval_time': 2282.8199610710144, 'accumulated_logging_time': 5.799633979797363}
I0229 23:44:35.749147 140089854211840 logging_writer.py:48] [192067] accumulated_eval_time=2282.819961, accumulated_logging_time=5.799634, accumulated_submission_time=64321.091645, global_step=192067, preemption_count=0, score=64321.091645, test/accuracy=0.627000, test/loss=1.844863, test/num_examples=10000, total_duration=66616.577503, train/accuracy=0.962153, train/loss=0.144710, validation/accuracy=0.755640, validation/loss=1.054056, validation/num_examples=50000
I0229 23:44:47.118772 140089862604544 logging_writer.py:48] [192100] global_step=192100, grad_norm=4.79742956161499, loss=0.6213484406471252
I0229 23:45:20.552477 140089854211840 logging_writer.py:48] [192200] global_step=192200, grad_norm=5.076029300689697, loss=0.6013467907905579
I0229 23:45:53.983098 140089862604544 logging_writer.py:48] [192300] global_step=192300, grad_norm=4.805531024932861, loss=0.6136583685874939
I0229 23:46:27.474786 140089854211840 logging_writer.py:48] [192400] global_step=192400, grad_norm=4.585102558135986, loss=0.6799083352088928
I0229 23:47:00.900185 140089862604544 logging_writer.py:48] [192500] global_step=192500, grad_norm=4.172595024108887, loss=0.5849543809890747
I0229 23:47:34.413658 140089854211840 logging_writer.py:48] [192600] global_step=192600, grad_norm=4.616902828216553, loss=0.557188868522644
I0229 23:48:07.851582 140089862604544 logging_writer.py:48] [192700] global_step=192700, grad_norm=5.135476112365723, loss=0.6190189719200134
I0229 23:48:41.328362 140089854211840 logging_writer.py:48] [192800] global_step=192800, grad_norm=4.676131725311279, loss=0.6528737545013428
I0229 23:49:14.737414 140089862604544 logging_writer.py:48] [192900] global_step=192900, grad_norm=4.936822414398193, loss=0.6257324814796448
I0229 23:49:48.198259 140089854211840 logging_writer.py:48] [193000] global_step=193000, grad_norm=4.3523454666137695, loss=0.5610511898994446
I0229 23:50:21.631298 140089862604544 logging_writer.py:48] [193100] global_step=193100, grad_norm=4.711306571960449, loss=0.6178947687149048
I0229 23:50:55.075139 140089854211840 logging_writer.py:48] [193200] global_step=193200, grad_norm=4.619185924530029, loss=0.699661135673523
I0229 23:51:28.543437 140089862604544 logging_writer.py:48] [193300] global_step=193300, grad_norm=4.368329048156738, loss=0.5896497368812561
I0229 23:52:01.962290 140089854211840 logging_writer.py:48] [193400] global_step=193400, grad_norm=4.337966442108154, loss=0.5724737048149109
I0229 23:52:35.434303 140089862604544 logging_writer.py:48] [193500] global_step=193500, grad_norm=4.530272960662842, loss=0.6107884645462036
I0229 23:53:06.030071 140252611495744 spec.py:321] Evaluating on the training split.
I0229 23:53:12.931092 140252611495744 spec.py:333] Evaluating on the validation split.
I0229 23:53:21.087710 140252611495744 spec.py:349] Evaluating on the test split.
I0229 23:53:23.332886 140252611495744 submission_runner.py:411] Time since start: 67144.21s, 	Step: 193593, 	{'train/accuracy': 0.9602199792861938, 'train/loss': 0.14605779945850372, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0536680221557617, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.841867446899414, 'test/num_examples': 10000, 'score': 64831.305763959885, 'total_duration': 67144.21295118332, 'accumulated_submission_time': 64831.305763959885, 'accumulated_eval_time': 2300.1227231025696, 'accumulated_logging_time': 5.86241602897644}
I0229 23:53:23.385949 140089845819136 logging_writer.py:48] [193593] accumulated_eval_time=2300.122723, accumulated_logging_time=5.862416, accumulated_submission_time=64831.305764, global_step=193593, preemption_count=0, score=64831.305764, test/accuracy=0.627200, test/loss=1.841867, test/num_examples=10000, total_duration=67144.212951, train/accuracy=0.960220, train/loss=0.146058, validation/accuracy=0.756020, validation/loss=1.053668, validation/num_examples=50000
I0229 23:53:26.059409 140089879389952 logging_writer.py:48] [193600] global_step=193600, grad_norm=4.580533504486084, loss=0.6174793243408203
I0229 23:53:59.571574 140089845819136 logging_writer.py:48] [193700] global_step=193700, grad_norm=4.732730388641357, loss=0.6165632605552673
I0229 23:54:33.014544 140089879389952 logging_writer.py:48] [193800] global_step=193800, grad_norm=4.788757801055908, loss=0.5849664807319641
I0229 23:55:06.438144 140089845819136 logging_writer.py:48] [193900] global_step=193900, grad_norm=4.267153739929199, loss=0.6340693235397339
I0229 23:55:39.904597 140089879389952 logging_writer.py:48] [194000] global_step=194000, grad_norm=4.209312438964844, loss=0.602385938167572
I0229 23:56:13.327106 140089845819136 logging_writer.py:48] [194100] global_step=194100, grad_norm=5.369801998138428, loss=0.7152971625328064
I0229 23:56:46.813391 140089879389952 logging_writer.py:48] [194200] global_step=194200, grad_norm=4.731440544128418, loss=0.5814650058746338
I0229 23:57:20.236228 140089845819136 logging_writer.py:48] [194300] global_step=194300, grad_norm=4.43317985534668, loss=0.5668911933898926
I0229 23:57:53.680812 140089879389952 logging_writer.py:48] [194400] global_step=194400, grad_norm=4.401595592498779, loss=0.6184987425804138
I0229 23:58:27.082585 140089845819136 logging_writer.py:48] [194500] global_step=194500, grad_norm=5.108607292175293, loss=0.674432098865509
I0229 23:59:00.571396 140089879389952 logging_writer.py:48] [194600] global_step=194600, grad_norm=5.058213710784912, loss=0.6388465166091919
I0229 23:59:34.099806 140089845819136 logging_writer.py:48] [194700] global_step=194700, grad_norm=4.262746334075928, loss=0.5825456380844116
I0301 00:00:07.551416 140089879389952 logging_writer.py:48] [194800] global_step=194800, grad_norm=4.597192764282227, loss=0.6675540804862976
I0301 00:00:40.968142 140089845819136 logging_writer.py:48] [194900] global_step=194900, grad_norm=4.32720422744751, loss=0.5345559120178223
I0301 00:01:14.438573 140089879389952 logging_writer.py:48] [195000] global_step=195000, grad_norm=4.465542316436768, loss=0.5886763334274292
I0301 00:01:47.862934 140089845819136 logging_writer.py:48] [195100] global_step=195100, grad_norm=4.18737268447876, loss=0.5609428286552429
I0301 00:01:53.349084 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:01:59.416636 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:02:07.777159 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:02:10.053449 140252611495744 submission_runner.py:411] Time since start: 67670.93s, 	Step: 195118, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14718115329742432, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.053143858909607, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.84319007396698, 'test/num_examples': 10000, 'score': 65341.203255176544, 'total_duration': 67670.93338441849, 'accumulated_submission_time': 65341.203255176544, 'accumulated_eval_time': 2316.826899766922, 'accumulated_logging_time': 5.926252603530884}
I0301 00:02:10.104475 140089770305280 logging_writer.py:48] [195118] accumulated_eval_time=2316.826900, accumulated_logging_time=5.926253, accumulated_submission_time=65341.203255, global_step=195118, preemption_count=0, score=65341.203255, test/accuracy=0.626600, test/loss=1.843190, test/num_examples=10000, total_duration=67670.933384, train/accuracy=0.960459, train/loss=0.147181, validation/accuracy=0.756040, validation/loss=1.053144, validation/num_examples=50000
I0301 00:02:37.863728 140089778697984 logging_writer.py:48] [195200] global_step=195200, grad_norm=4.749941825866699, loss=0.6706477999687195
I0301 00:03:11.313262 140089770305280 logging_writer.py:48] [195300] global_step=195300, grad_norm=4.726958751678467, loss=0.6408460736274719
I0301 00:03:44.772800 140089778697984 logging_writer.py:48] [195400] global_step=195400, grad_norm=4.577794551849365, loss=0.6538598537445068
I0301 00:04:18.244539 140089770305280 logging_writer.py:48] [195500] global_step=195500, grad_norm=4.411534309387207, loss=0.6429651379585266
I0301 00:04:51.665085 140089778697984 logging_writer.py:48] [195600] global_step=195600, grad_norm=4.660114288330078, loss=0.6332434415817261
I0301 00:05:25.133648 140089770305280 logging_writer.py:48] [195700] global_step=195700, grad_norm=4.356061935424805, loss=0.6178953051567078
I0301 00:05:58.644598 140089778697984 logging_writer.py:48] [195800] global_step=195800, grad_norm=4.244968414306641, loss=0.5927771329879761
I0301 00:06:32.088758 140089770305280 logging_writer.py:48] [195900] global_step=195900, grad_norm=4.812993049621582, loss=0.6676947474479675
I0301 00:07:05.549711 140089778697984 logging_writer.py:48] [196000] global_step=196000, grad_norm=4.307487487792969, loss=0.6050922870635986
I0301 00:07:39.067327 140089770305280 logging_writer.py:48] [196100] global_step=196100, grad_norm=4.279395580291748, loss=0.6389826536178589
I0301 00:08:12.514433 140089778697984 logging_writer.py:48] [196200] global_step=196200, grad_norm=5.041910171508789, loss=0.611621618270874
I0301 00:08:45.956411 140089770305280 logging_writer.py:48] [196300] global_step=196300, grad_norm=4.965705394744873, loss=0.6105937361717224
I0301 00:09:19.371408 140089778697984 logging_writer.py:48] [196400] global_step=196400, grad_norm=4.540592670440674, loss=0.6334930658340454
I0301 00:09:52.833832 140089770305280 logging_writer.py:48] [196500] global_step=196500, grad_norm=4.901871204376221, loss=0.6302955150604248
I0301 00:10:26.271463 140089778697984 logging_writer.py:48] [196600] global_step=196600, grad_norm=4.494480133056641, loss=0.5774644017219543
I0301 00:10:40.106675 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:10:46.267894 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:10:54.262382 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:10:56.544434 140252611495744 submission_runner.py:411] Time since start: 68197.42s, 	Step: 196643, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14293892681598663, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0525805950164795, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8403793573379517, 'test/num_examples': 10000, 'score': 65851.13903975487, 'total_duration': 68197.4244966507, 'accumulated_submission_time': 65851.13903975487, 'accumulated_eval_time': 2333.264596939087, 'accumulated_logging_time': 5.989895343780518}
I0301 00:10:56.593624 140089854211840 logging_writer.py:48] [196643] accumulated_eval_time=2333.264597, accumulated_logging_time=5.989895, accumulated_submission_time=65851.139040, global_step=196643, preemption_count=0, score=65851.139040, test/accuracy=0.627700, test/loss=1.840379, test/num_examples=10000, total_duration=68197.424497, train/accuracy=0.961934, train/loss=0.142939, validation/accuracy=0.756120, validation/loss=1.052581, validation/num_examples=50000
I0301 00:11:15.988376 140089862604544 logging_writer.py:48] [196700] global_step=196700, grad_norm=3.9386367797851562, loss=0.5696662664413452
I0301 00:11:49.522139 140089854211840 logging_writer.py:48] [196800] global_step=196800, grad_norm=4.217210292816162, loss=0.5919820070266724
I0301 00:12:22.953436 140089862604544 logging_writer.py:48] [196900] global_step=196900, grad_norm=4.020962238311768, loss=0.5641839504241943
I0301 00:12:56.387573 140089854211840 logging_writer.py:48] [197000] global_step=197000, grad_norm=4.584681034088135, loss=0.6052733063697815
I0301 00:13:29.894000 140089862604544 logging_writer.py:48] [197100] global_step=197100, grad_norm=4.331197738647461, loss=0.6186495423316956
I0301 00:14:03.341166 140089854211840 logging_writer.py:48] [197200] global_step=197200, grad_norm=4.183091640472412, loss=0.607062041759491
I0301 00:14:36.767663 140089862604544 logging_writer.py:48] [197300] global_step=197300, grad_norm=5.162972450256348, loss=0.5896217823028564
I0301 00:15:10.238737 140089854211840 logging_writer.py:48] [197400] global_step=197400, grad_norm=4.356447219848633, loss=0.61558997631073
I0301 00:15:43.719799 140089862604544 logging_writer.py:48] [197500] global_step=197500, grad_norm=4.646154880523682, loss=0.6178145408630371
I0301 00:16:17.144660 140089854211840 logging_writer.py:48] [197600] global_step=197600, grad_norm=4.87886381149292, loss=0.6350398063659668
I0301 00:16:50.646011 140089862604544 logging_writer.py:48] [197700] global_step=197700, grad_norm=4.183365821838379, loss=0.5908925533294678
I0301 00:17:24.098628 140089854211840 logging_writer.py:48] [197800] global_step=197800, grad_norm=4.780614852905273, loss=0.5809126496315002
I0301 00:17:57.600498 140089862604544 logging_writer.py:48] [197900] global_step=197900, grad_norm=4.437747478485107, loss=0.6474062204360962
I0301 00:18:31.044893 140089854211840 logging_writer.py:48] [198000] global_step=198000, grad_norm=4.262312412261963, loss=0.6122463345527649
I0301 00:19:04.511807 140089862604544 logging_writer.py:48] [198100] global_step=198100, grad_norm=4.424649238586426, loss=0.5820939540863037
I0301 00:19:26.726471 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:19:32.795529 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:19:40.987131 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:19:43.292546 140252611495744 submission_runner.py:411] Time since start: 68724.17s, 	Step: 198168, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.14341798424720764, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.052737832069397, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.841537594795227, 'test/num_examples': 10000, 'score': 66361.20763134956, 'total_duration': 68724.17261481285, 'accumulated_submission_time': 66361.20763134956, 'accumulated_eval_time': 2349.8306188583374, 'accumulated_logging_time': 6.049070358276367}
I0301 00:19:43.344753 140089778697984 logging_writer.py:48] [198168] accumulated_eval_time=2349.830619, accumulated_logging_time=6.049070, accumulated_submission_time=66361.207631, global_step=198168, preemption_count=0, score=66361.207631, test/accuracy=0.627500, test/loss=1.841538, test/num_examples=10000, total_duration=68724.172615, train/accuracy=0.961854, train/loss=0.143418, validation/accuracy=0.755920, validation/loss=1.052738, validation/num_examples=50000
I0301 00:19:54.377419 140089837426432 logging_writer.py:48] [198200] global_step=198200, grad_norm=4.869114398956299, loss=0.6514561772346497
I0301 00:20:27.826673 140089778697984 logging_writer.py:48] [198300] global_step=198300, grad_norm=4.221611022949219, loss=0.5723713040351868
I0301 00:21:01.308347 140089837426432 logging_writer.py:48] [198400] global_step=198400, grad_norm=4.567625999450684, loss=0.6152266263961792
I0301 00:21:34.745357 140089778697984 logging_writer.py:48] [198500] global_step=198500, grad_norm=4.396690368652344, loss=0.6044663786888123
I0301 00:22:08.221532 140089837426432 logging_writer.py:48] [198600] global_step=198600, grad_norm=4.4192585945129395, loss=0.547254204750061
I0301 00:22:41.653416 140089778697984 logging_writer.py:48] [198700] global_step=198700, grad_norm=4.8752264976501465, loss=0.6555702090263367
I0301 00:23:15.103528 140089837426432 logging_writer.py:48] [198800] global_step=198800, grad_norm=4.773193359375, loss=0.5896880030632019
I0301 00:23:48.557523 140089778697984 logging_writer.py:48] [198900] global_step=198900, grad_norm=4.779181957244873, loss=0.5391842126846313
I0301 00:24:22.146754 140089837426432 logging_writer.py:48] [199000] global_step=199000, grad_norm=4.592876434326172, loss=0.6758505702018738
I0301 00:24:55.633423 140089778697984 logging_writer.py:48] [199100] global_step=199100, grad_norm=4.345834732055664, loss=0.6301648616790771
I0301 00:25:29.128173 140089837426432 logging_writer.py:48] [199200] global_step=199200, grad_norm=4.322171211242676, loss=0.5988470315933228
I0301 00:26:02.586890 140089778697984 logging_writer.py:48] [199300] global_step=199300, grad_norm=4.159204483032227, loss=0.5874042510986328
I0301 00:26:36.040281 140089837426432 logging_writer.py:48] [199400] global_step=199400, grad_norm=4.561333656311035, loss=0.5631384253501892
I0301 00:27:09.476413 140089778697984 logging_writer.py:48] [199500] global_step=199500, grad_norm=5.002875804901123, loss=0.6862938404083252
I0301 00:27:42.954348 140089837426432 logging_writer.py:48] [199600] global_step=199600, grad_norm=4.394320011138916, loss=0.5890689492225647
I0301 00:28:13.576369 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:28:19.779654 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:28:27.870068 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:28:30.161407 140252611495744 submission_runner.py:411] Time since start: 69251.04s, 	Step: 199693, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14405032992362976, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0533536672592163, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8423495292663574, 'test/num_examples': 10000, 'score': 66871.37352252007, 'total_duration': 69251.04145288467, 'accumulated_submission_time': 66871.37352252007, 'accumulated_eval_time': 2366.4155824184418, 'accumulated_logging_time': 6.112751245498657}
I0301 00:28:30.213612 140089770305280 logging_writer.py:48] [199693] accumulated_eval_time=2366.415582, accumulated_logging_time=6.112751, accumulated_submission_time=66871.373523, global_step=199693, preemption_count=0, score=66871.373523, test/accuracy=0.626800, test/loss=1.842350, test/num_examples=10000, total_duration=69251.041453, train/accuracy=0.961635, train/loss=0.144050, validation/accuracy=0.755800, validation/loss=1.053354, validation/num_examples=50000
I0301 00:28:32.893565 140089778697984 logging_writer.py:48] [199700] global_step=199700, grad_norm=4.656457901000977, loss=0.5784262418746948
I0301 00:29:06.347348 140089770305280 logging_writer.py:48] [199800] global_step=199800, grad_norm=4.303843975067139, loss=0.5907970070838928
I0301 00:29:39.769527 140089778697984 logging_writer.py:48] [199900] global_step=199900, grad_norm=4.961230278015137, loss=0.6321973204612732
I0301 00:30:13.287275 140089770305280 logging_writer.py:48] [200000] global_step=200000, grad_norm=4.887850761413574, loss=0.6325579285621643
I0301 00:30:46.755460 140089778697984 logging_writer.py:48] [200100] global_step=200100, grad_norm=4.372105598449707, loss=0.5495210289955139
I0301 00:31:20.236034 140089770305280 logging_writer.py:48] [200200] global_step=200200, grad_norm=4.853952407836914, loss=0.7175000905990601
I0301 00:31:53.722549 140089778697984 logging_writer.py:48] [200300] global_step=200300, grad_norm=4.643678188323975, loss=0.630240261554718
I0301 00:32:27.178488 140089770305280 logging_writer.py:48] [200400] global_step=200400, grad_norm=4.545689582824707, loss=0.666689932346344
I0301 00:33:00.624078 140089778697984 logging_writer.py:48] [200500] global_step=200500, grad_norm=4.764520645141602, loss=0.6364315748214722
I0301 00:33:34.090734 140089770305280 logging_writer.py:48] [200600] global_step=200600, grad_norm=4.8625617027282715, loss=0.6412732601165771
I0301 00:34:07.534138 140089778697984 logging_writer.py:48] [200700] global_step=200700, grad_norm=4.330499172210693, loss=0.6750838756561279
I0301 00:34:41.002918 140089770305280 logging_writer.py:48] [200800] global_step=200800, grad_norm=5.204173564910889, loss=0.6423998475074768
I0301 00:35:14.468793 140089778697984 logging_writer.py:48] [200900] global_step=200900, grad_norm=4.750401973724365, loss=0.6224259734153748
I0301 00:35:47.901321 140089770305280 logging_writer.py:48] [201000] global_step=201000, grad_norm=4.725929260253906, loss=0.5884366035461426
I0301 00:36:21.549468 140089778697984 logging_writer.py:48] [201100] global_step=201100, grad_norm=4.819881916046143, loss=0.7132729291915894
I0301 00:36:55.029905 140089770305280 logging_writer.py:48] [201200] global_step=201200, grad_norm=4.620511531829834, loss=0.5499236583709717
I0301 00:37:00.187375 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:37:06.437754 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:37:14.669005 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:37:16.976897 140252611495744 submission_runner.py:411] Time since start: 69777.86s, 	Step: 201217, 	{'train/accuracy': 0.9601601958274841, 'train/loss': 0.1471266746520996, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0531275272369385, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8406308889389038, 'test/num_examples': 10000, 'score': 67381.27967381477, 'total_duration': 69777.85696268082, 'accumulated_submission_time': 67381.27967381477, 'accumulated_eval_time': 2383.2050466537476, 'accumulated_logging_time': 6.177959680557251}
I0301 00:37:17.032999 140089770305280 logging_writer.py:48] [201217] accumulated_eval_time=2383.205047, accumulated_logging_time=6.177960, accumulated_submission_time=67381.279674, global_step=201217, preemption_count=0, score=67381.279674, test/accuracy=0.627100, test/loss=1.840631, test/num_examples=10000, total_duration=69777.856963, train/accuracy=0.960160, train/loss=0.147127, validation/accuracy=0.756160, validation/loss=1.053128, validation/num_examples=50000
I0301 00:37:45.198093 140089837426432 logging_writer.py:48] [201300] global_step=201300, grad_norm=4.63124418258667, loss=0.5907686948776245
I0301 00:38:18.642603 140089770305280 logging_writer.py:48] [201400] global_step=201400, grad_norm=4.41954231262207, loss=0.5805301070213318
I0301 00:38:52.102711 140089837426432 logging_writer.py:48] [201500] global_step=201500, grad_norm=5.822747230529785, loss=0.6314507722854614
I0301 00:39:25.557513 140089770305280 logging_writer.py:48] [201600] global_step=201600, grad_norm=4.615281105041504, loss=0.5676572918891907
I0301 00:39:59.044816 140089837426432 logging_writer.py:48] [201700] global_step=201700, grad_norm=4.577590465545654, loss=0.6613408923149109
I0301 00:40:32.486204 140089770305280 logging_writer.py:48] [201800] global_step=201800, grad_norm=4.552316665649414, loss=0.6097501516342163
I0301 00:41:05.920607 140089837426432 logging_writer.py:48] [201900] global_step=201900, grad_norm=4.715949535369873, loss=0.5974388122558594
I0301 00:41:39.402218 140089770305280 logging_writer.py:48] [202000] global_step=202000, grad_norm=4.349386692047119, loss=0.637574315071106
I0301 00:42:12.958802 140089837426432 logging_writer.py:48] [202100] global_step=202100, grad_norm=4.489991664886475, loss=0.6122080683708191
I0301 00:42:46.477557 140089770305280 logging_writer.py:48] [202200] global_step=202200, grad_norm=4.613103866577148, loss=0.6268142461776733
I0301 00:43:19.911808 140089837426432 logging_writer.py:48] [202300] global_step=202300, grad_norm=4.732911109924316, loss=0.687765896320343
I0301 00:43:53.367046 140089770305280 logging_writer.py:48] [202400] global_step=202400, grad_norm=4.943324089050293, loss=0.6382202506065369
I0301 00:44:26.844342 140089837426432 logging_writer.py:48] [202500] global_step=202500, grad_norm=4.3960161209106445, loss=0.6744407415390015
I0301 00:45:00.283849 140089770305280 logging_writer.py:48] [202600] global_step=202600, grad_norm=4.493252754211426, loss=0.6657329797744751
I0301 00:45:33.733033 140089837426432 logging_writer.py:48] [202700] global_step=202700, grad_norm=4.78388786315918, loss=0.6015620827674866
I0301 00:45:47.242614 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:45:53.310456 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:46:01.491361 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:46:03.979819 140252611495744 submission_runner.py:411] Time since start: 70304.86s, 	Step: 202742, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14618711173534393, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0531686544418335, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8418047428131104, 'test/num_examples': 10000, 'score': 67891.42406487465, 'total_duration': 70304.85989117622, 'accumulated_submission_time': 67891.42406487465, 'accumulated_eval_time': 2399.942197084427, 'accumulated_logging_time': 6.244342565536499}
I0301 00:46:04.030033 140089770305280 logging_writer.py:48] [202742] accumulated_eval_time=2399.942197, accumulated_logging_time=6.244343, accumulated_submission_time=67891.424065, global_step=202742, preemption_count=0, score=67891.424065, test/accuracy=0.627800, test/loss=1.841805, test/num_examples=10000, total_duration=70304.859891, train/accuracy=0.960459, train/loss=0.146187, validation/accuracy=0.755620, validation/loss=1.053169, validation/num_examples=50000
I0301 00:46:23.779153 140089778697984 logging_writer.py:48] [202800] global_step=202800, grad_norm=4.277576923370361, loss=0.5916048884391785
I0301 00:46:57.211899 140089770305280 logging_writer.py:48] [202900] global_step=202900, grad_norm=4.564604759216309, loss=0.6529649496078491
I0301 00:47:30.670780 140089778697984 logging_writer.py:48] [203000] global_step=203000, grad_norm=4.8659515380859375, loss=0.5997405052185059
I0301 00:48:04.091723 140089770305280 logging_writer.py:48] [203100] global_step=203100, grad_norm=5.336437702178955, loss=0.6939597725868225
I0301 00:48:37.630501 140089778697984 logging_writer.py:48] [203200] global_step=203200, grad_norm=4.493921756744385, loss=0.6529459953308105
I0301 00:49:11.068253 140089770305280 logging_writer.py:48] [203300] global_step=203300, grad_norm=4.870317459106445, loss=0.5706502199172974
I0301 00:49:44.475227 140089778697984 logging_writer.py:48] [203400] global_step=203400, grad_norm=4.198248386383057, loss=0.5698376893997192
I0301 00:50:17.921222 140089770305280 logging_writer.py:48] [203500] global_step=203500, grad_norm=4.339787483215332, loss=0.6095550656318665
I0301 00:50:51.337194 140089778697984 logging_writer.py:48] [203600] global_step=203600, grad_norm=4.228259086608887, loss=0.6467199325561523
I0301 00:51:24.781315 140089770305280 logging_writer.py:48] [203700] global_step=203700, grad_norm=4.5485615730285645, loss=0.5991835594177246
I0301 00:51:58.204405 140089778697984 logging_writer.py:48] [203800] global_step=203800, grad_norm=4.8349738121032715, loss=0.6070120334625244
I0301 00:52:31.637762 140089770305280 logging_writer.py:48] [203900] global_step=203900, grad_norm=4.996655464172363, loss=0.5821847915649414
I0301 00:53:05.112884 140089778697984 logging_writer.py:48] [204000] global_step=204000, grad_norm=4.428022384643555, loss=0.6155128479003906
I0301 00:53:38.528977 140089770305280 logging_writer.py:48] [204100] global_step=204100, grad_norm=4.580681800842285, loss=0.6448595523834229
I0301 00:54:12.035197 140089778697984 logging_writer.py:48] [204200] global_step=204200, grad_norm=4.621574401855469, loss=0.6032282710075378
I0301 00:54:34.253323 140252611495744 spec.py:321] Evaluating on the training split.
I0301 00:54:40.362499 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 00:54:48.496224 140252611495744 spec.py:349] Evaluating on the test split.
I0301 00:54:50.761090 140252611495744 submission_runner.py:411] Time since start: 70831.64s, 	Step: 204268, 	{'train/accuracy': 0.9620735049247742, 'train/loss': 0.14243349432945251, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0534887313842773, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8423887491226196, 'test/num_examples': 10000, 'score': 68401.58019638062, 'total_duration': 70831.64112019539, 'accumulated_submission_time': 68401.58019638062, 'accumulated_eval_time': 2416.4498698711395, 'accumulated_logging_time': 6.305903196334839}
I0301 00:54:50.817821 140089770305280 logging_writer.py:48] [204268] accumulated_eval_time=2416.449870, accumulated_logging_time=6.305903, accumulated_submission_time=68401.580196, global_step=204268, preemption_count=0, score=68401.580196, test/accuracy=0.627300, test/loss=1.842389, test/num_examples=10000, total_duration=70831.641120, train/accuracy=0.962074, train/loss=0.142433, validation/accuracy=0.756140, validation/loss=1.053489, validation/num_examples=50000
I0301 00:55:01.871255 140089845819136 logging_writer.py:48] [204300] global_step=204300, grad_norm=4.1808342933654785, loss=0.6194086670875549
I0301 00:55:35.297404 140089770305280 logging_writer.py:48] [204400] global_step=204400, grad_norm=4.8193182945251465, loss=0.6327851414680481
I0301 00:56:08.747755 140089845819136 logging_writer.py:48] [204500] global_step=204500, grad_norm=4.654716968536377, loss=0.6533070802688599
I0301 00:56:42.187607 140089770305280 logging_writer.py:48] [204600] global_step=204600, grad_norm=4.401614665985107, loss=0.6027741432189941
I0301 00:57:15.646067 140089845819136 logging_writer.py:48] [204700] global_step=204700, grad_norm=4.8691558837890625, loss=0.5676904320716858
I0301 00:57:49.112759 140089770305280 logging_writer.py:48] [204800] global_step=204800, grad_norm=4.373248100280762, loss=0.6221737861633301
I0301 00:58:22.535298 140089845819136 logging_writer.py:48] [204900] global_step=204900, grad_norm=4.63368034362793, loss=0.6708681583404541
I0301 00:58:56.014575 140089770305280 logging_writer.py:48] [205000] global_step=205000, grad_norm=4.434230804443359, loss=0.5892618298530579
I0301 00:59:29.454486 140089845819136 logging_writer.py:48] [205100] global_step=205100, grad_norm=4.984544277191162, loss=0.6624553799629211
I0301 01:00:02.867894 140089770305280 logging_writer.py:48] [205200] global_step=205200, grad_norm=4.673436164855957, loss=0.5902698040008545
I0301 01:00:36.450711 140089845819136 logging_writer.py:48] [205300] global_step=205300, grad_norm=4.290334701538086, loss=0.6430586576461792
I0301 01:01:09.889811 140089770305280 logging_writer.py:48] [205400] global_step=205400, grad_norm=4.623503684997559, loss=0.6012746095657349
I0301 01:01:43.323428 140089845819136 logging_writer.py:48] [205500] global_step=205500, grad_norm=4.513620376586914, loss=0.6112528443336487
I0301 01:02:16.767134 140089770305280 logging_writer.py:48] [205600] global_step=205600, grad_norm=4.671977519989014, loss=0.6812883615493774
I0301 01:02:50.207689 140089845819136 logging_writer.py:48] [205700] global_step=205700, grad_norm=4.798749923706055, loss=0.6523272395133972
I0301 01:03:20.786495 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:03:26.916794 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:03:35.065831 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:03:37.298908 140252611495744 submission_runner.py:411] Time since start: 71358.18s, 	Step: 205793, 	{'train/accuracy': 0.9616150856018066, 'train/loss': 0.14590393006801605, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0521328449249268, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8403428792953491, 'test/num_examples': 10000, 'score': 68911.47825837135, 'total_duration': 71358.17896771431, 'accumulated_submission_time': 68911.47825837135, 'accumulated_eval_time': 2432.9622209072113, 'accumulated_logging_time': 6.379040241241455}
I0301 01:03:37.355994 140089770305280 logging_writer.py:48] [205793] accumulated_eval_time=2432.962221, accumulated_logging_time=6.379040, accumulated_submission_time=68911.478258, global_step=205793, preemption_count=0, score=68911.478258, test/accuracy=0.627900, test/loss=1.840343, test/num_examples=10000, total_duration=71358.178968, train/accuracy=0.961615, train/loss=0.145904, validation/accuracy=0.755840, validation/loss=1.052133, validation/num_examples=50000
I0301 01:03:40.044225 140089778697984 logging_writer.py:48] [205800] global_step=205800, grad_norm=4.697550296783447, loss=0.6863871216773987
I0301 01:04:13.489856 140089770305280 logging_writer.py:48] [205900] global_step=205900, grad_norm=5.071937561035156, loss=0.631643533706665
I0301 01:04:46.939461 140089778697984 logging_writer.py:48] [206000] global_step=206000, grad_norm=4.43121862411499, loss=0.6218224763870239
I0301 01:05:20.388536 140089770305280 logging_writer.py:48] [206100] global_step=206100, grad_norm=4.998283386230469, loss=0.6334208846092224
I0301 01:05:53.829764 140089778697984 logging_writer.py:48] [206200] global_step=206200, grad_norm=4.604788780212402, loss=0.676266074180603
I0301 01:06:27.447792 140089770305280 logging_writer.py:48] [206300] global_step=206300, grad_norm=4.449262619018555, loss=0.5946099162101746
I0301 01:07:00.903706 140089778697984 logging_writer.py:48] [206400] global_step=206400, grad_norm=4.5951738357543945, loss=0.6171985268592834
I0301 01:07:34.356671 140089770305280 logging_writer.py:48] [206500] global_step=206500, grad_norm=4.235679626464844, loss=0.586245596408844
I0301 01:08:07.819740 140089778697984 logging_writer.py:48] [206600] global_step=206600, grad_norm=4.991721153259277, loss=0.6653282046318054
I0301 01:08:41.239686 140089770305280 logging_writer.py:48] [206700] global_step=206700, grad_norm=4.2261176109313965, loss=0.6143138408660889
I0301 01:09:14.691077 140089778697984 logging_writer.py:48] [206800] global_step=206800, grad_norm=5.295431613922119, loss=0.6557500958442688
I0301 01:09:48.138930 140089770305280 logging_writer.py:48] [206900] global_step=206900, grad_norm=4.242266654968262, loss=0.5779882669448853
I0301 01:10:21.604644 140089778697984 logging_writer.py:48] [207000] global_step=207000, grad_norm=4.481080055236816, loss=0.6044503450393677
I0301 01:10:55.014872 140089770305280 logging_writer.py:48] [207100] global_step=207100, grad_norm=4.581619739532471, loss=0.6169809699058533
I0301 01:11:28.470370 140089778697984 logging_writer.py:48] [207200] global_step=207200, grad_norm=4.563442707061768, loss=0.6786705851554871
I0301 01:12:01.886626 140089770305280 logging_writer.py:48] [207300] global_step=207300, grad_norm=4.752500534057617, loss=0.639267086982727
I0301 01:12:07.397036 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:12:13.508590 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:12:21.800394 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:12:24.091149 140252611495744 submission_runner.py:411] Time since start: 71884.97s, 	Step: 207318, 	{'train/accuracy': 0.9592832922935486, 'train/loss': 0.1478613168001175, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0538548231124878, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8419071435928345, 'test/num_examples': 10000, 'score': 69421.45237731934, 'total_duration': 71884.97119355202, 'accumulated_submission_time': 69421.45237731934, 'accumulated_eval_time': 2449.6562552452087, 'accumulated_logging_time': 6.4470906257629395}
I0301 01:12:24.153638 140089770305280 logging_writer.py:48] [207318] accumulated_eval_time=2449.656255, accumulated_logging_time=6.447091, accumulated_submission_time=69421.452377, global_step=207318, preemption_count=0, score=69421.452377, test/accuracy=0.627200, test/loss=1.841907, test/num_examples=10000, total_duration=71884.971194, train/accuracy=0.959283, train/loss=0.147861, validation/accuracy=0.756040, validation/loss=1.053855, validation/num_examples=50000
I0301 01:12:51.951547 140089862604544 logging_writer.py:48] [207400] global_step=207400, grad_norm=4.692963123321533, loss=0.6924692988395691
I0301 01:13:25.414166 140089770305280 logging_writer.py:48] [207500] global_step=207500, grad_norm=4.791979789733887, loss=0.6583795547485352
I0301 01:13:58.841444 140089862604544 logging_writer.py:48] [207600] global_step=207600, grad_norm=4.764326095581055, loss=0.5583875775337219
I0301 01:14:32.318414 140089770305280 logging_writer.py:48] [207700] global_step=207700, grad_norm=4.553632736206055, loss=0.5883812308311462
I0301 01:15:05.748034 140089862604544 logging_writer.py:48] [207800] global_step=207800, grad_norm=4.463265895843506, loss=0.6412239074707031
I0301 01:15:39.183357 140089770305280 logging_writer.py:48] [207900] global_step=207900, grad_norm=4.065364837646484, loss=0.5710458159446716
I0301 01:16:12.725588 140089862604544 logging_writer.py:48] [208000] global_step=208000, grad_norm=4.641368865966797, loss=0.6316426396369934
I0301 01:16:46.165345 140089770305280 logging_writer.py:48] [208100] global_step=208100, grad_norm=5.320874214172363, loss=0.6730495691299438
I0301 01:17:19.609469 140089862604544 logging_writer.py:48] [208200] global_step=208200, grad_norm=4.611995220184326, loss=0.6033456325531006
I0301 01:17:53.077846 140089770305280 logging_writer.py:48] [208300] global_step=208300, grad_norm=4.464502811431885, loss=0.5323914885520935
I0301 01:18:26.648833 140089862604544 logging_writer.py:48] [208400] global_step=208400, grad_norm=4.523418426513672, loss=0.6751806735992432
I0301 01:19:00.115492 140089770305280 logging_writer.py:48] [208500] global_step=208500, grad_norm=4.653196811676025, loss=0.6134876012802124
I0301 01:19:33.552722 140089862604544 logging_writer.py:48] [208600] global_step=208600, grad_norm=4.6682586669921875, loss=0.6070131063461304
I0301 01:20:07.027366 140089770305280 logging_writer.py:48] [208700] global_step=208700, grad_norm=4.5876030921936035, loss=0.6296314001083374
I0301 01:20:40.457453 140089862604544 logging_writer.py:48] [208800] global_step=208800, grad_norm=4.67962646484375, loss=0.6017197966575623
I0301 01:20:54.329854 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:21:00.449229 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:21:08.655246 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:21:10.913938 140252611495744 submission_runner.py:411] Time since start: 72411.79s, 	Step: 208843, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14563435316085815, 'validation/accuracy': 0.7562199831008911, 'validation/loss': 1.0526604652404785, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8434849977493286, 'test/num_examples': 10000, 'score': 69931.56385087967, 'total_duration': 72411.7939863205, 'accumulated_submission_time': 69931.56385087967, 'accumulated_eval_time': 2466.2402641773224, 'accumulated_logging_time': 6.519393682479858}
I0301 01:21:10.965404 140089837426432 logging_writer.py:48] [208843] accumulated_eval_time=2466.240264, accumulated_logging_time=6.519394, accumulated_submission_time=69931.563851, global_step=208843, preemption_count=0, score=69931.563851, test/accuracy=0.627400, test/loss=1.843485, test/num_examples=10000, total_duration=72411.793986, train/accuracy=0.961476, train/loss=0.145634, validation/accuracy=0.756220, validation/loss=1.052660, validation/num_examples=50000
I0301 01:21:30.394725 140089845819136 logging_writer.py:48] [208900] global_step=208900, grad_norm=4.644374370574951, loss=0.5846162438392639
I0301 01:22:03.828029 140089837426432 logging_writer.py:48] [209000] global_step=209000, grad_norm=4.69120454788208, loss=0.612483024597168
I0301 01:22:37.266921 140089845819136 logging_writer.py:48] [209100] global_step=209100, grad_norm=4.917914390563965, loss=0.604187548160553
I0301 01:23:10.716538 140089837426432 logging_writer.py:48] [209200] global_step=209200, grad_norm=4.168638229370117, loss=0.6202398538589478
I0301 01:23:44.143991 140089845819136 logging_writer.py:48] [209300] global_step=209300, grad_norm=4.574558258056641, loss=0.620749294757843
I0301 01:24:17.605975 140089837426432 logging_writer.py:48] [209400] global_step=209400, grad_norm=4.579481601715088, loss=0.6559015512466431
I0301 01:24:51.164076 140089845819136 logging_writer.py:48] [209500] global_step=209500, grad_norm=4.13394832611084, loss=0.5952171087265015
I0301 01:25:24.597283 140089837426432 logging_writer.py:48] [209600] global_step=209600, grad_norm=4.279033660888672, loss=0.5815099477767944
I0301 01:25:58.044929 140089845819136 logging_writer.py:48] [209700] global_step=209700, grad_norm=4.755843162536621, loss=0.6603085994720459
I0301 01:26:31.467516 140089837426432 logging_writer.py:48] [209800] global_step=209800, grad_norm=4.378566741943359, loss=0.6227606534957886
I0301 01:27:04.912398 140089845819136 logging_writer.py:48] [209900] global_step=209900, grad_norm=4.3375396728515625, loss=0.5951950550079346
I0301 01:27:38.371705 140089837426432 logging_writer.py:48] [210000] global_step=210000, grad_norm=4.314953327178955, loss=0.6316618323326111
I0301 01:28:11.793704 140089845819136 logging_writer.py:48] [210100] global_step=210100, grad_norm=5.716091156005859, loss=0.607304036617279
I0301 01:28:45.261974 140089837426432 logging_writer.py:48] [210200] global_step=210200, grad_norm=4.48373556137085, loss=0.5482885837554932
I0301 01:29:18.725047 140089845819136 logging_writer.py:48] [210300] global_step=210300, grad_norm=4.755493640899658, loss=0.6698825359344482
I0301 01:29:40.953169 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:29:47.113476 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:29:55.293301 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:29:57.607448 140252611495744 submission_runner.py:411] Time since start: 72938.49s, 	Step: 210368, 	{'train/accuracy': 0.9622129797935486, 'train/loss': 0.14515914022922516, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0537360906600952, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.842454433441162, 'test/num_examples': 10000, 'score': 70441.48584985733, 'total_duration': 72938.48751044273, 'accumulated_submission_time': 70441.48584985733, 'accumulated_eval_time': 2482.894499540329, 'accumulated_logging_time': 6.58108377456665}
I0301 01:29:57.663137 140089552271104 logging_writer.py:48] [210368] accumulated_eval_time=2482.894500, accumulated_logging_time=6.581084, accumulated_submission_time=70441.485850, global_step=210368, preemption_count=0, score=70441.485850, test/accuracy=0.628300, test/loss=1.842454, test/num_examples=10000, total_duration=72938.487510, train/accuracy=0.962213, train/loss=0.145159, validation/accuracy=0.756080, validation/loss=1.053736, validation/num_examples=50000
I0301 01:30:08.703394 140089770305280 logging_writer.py:48] [210400] global_step=210400, grad_norm=4.402925491333008, loss=0.6734768152236938
I0301 01:30:42.187677 140089552271104 logging_writer.py:48] [210500] global_step=210500, grad_norm=4.7206292152404785, loss=0.6442919969558716
I0301 01:31:15.644013 140089770305280 logging_writer.py:48] [210600] global_step=210600, grad_norm=4.516228675842285, loss=0.5956804156303406
I0301 01:31:49.144789 140089552271104 logging_writer.py:48] [210700] global_step=210700, grad_norm=4.835652828216553, loss=0.6809576749801636
I0301 01:32:22.600817 140089770305280 logging_writer.py:48] [210800] global_step=210800, grad_norm=4.859679698944092, loss=0.6195995211601257
I0301 01:32:56.079019 140089552271104 logging_writer.py:48] [210900] global_step=210900, grad_norm=4.570009231567383, loss=0.6368341445922852
I0301 01:33:29.491179 140089770305280 logging_writer.py:48] [211000] global_step=211000, grad_norm=4.630521774291992, loss=0.648857831954956
I0301 01:34:02.936193 140089552271104 logging_writer.py:48] [211100] global_step=211100, grad_norm=4.646681308746338, loss=0.5787554979324341
I0301 01:34:36.356667 140089770305280 logging_writer.py:48] [211200] global_step=211200, grad_norm=4.99830961227417, loss=0.6296696066856384
I0301 01:35:09.819150 140089552271104 logging_writer.py:48] [211300] global_step=211300, grad_norm=4.600572109222412, loss=0.6175081133842468
I0301 01:35:43.237345 140089770305280 logging_writer.py:48] [211400] global_step=211400, grad_norm=4.822223663330078, loss=0.6274011135101318
I0301 01:36:16.721034 140089552271104 logging_writer.py:48] [211500] global_step=211500, grad_norm=4.525018692016602, loss=0.6355222463607788
I0301 01:36:50.365906 140089770305280 logging_writer.py:48] [211600] global_step=211600, grad_norm=4.388216495513916, loss=0.6488271355628967
I0301 01:37:23.845870 140089552271104 logging_writer.py:48] [211700] global_step=211700, grad_norm=4.710887432098389, loss=0.5968639254570007
I0301 01:37:57.278233 140089770305280 logging_writer.py:48] [211800] global_step=211800, grad_norm=4.601919651031494, loss=0.668341338634491
I0301 01:38:27.887178 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:38:34.081280 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:38:42.110135 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:38:44.417249 140252611495744 submission_runner.py:411] Time since start: 73465.30s, 	Step: 211893, 	{'train/accuracy': 0.962312638759613, 'train/loss': 0.14246387779712677, 'validation/accuracy': 0.7555199861526489, 'validation/loss': 1.0546331405639648, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8440243005752563, 'test/num_examples': 10000, 'score': 70951.64380955696, 'total_duration': 73465.29731369019, 'accumulated_submission_time': 70951.64380955696, 'accumulated_eval_time': 2499.4245150089264, 'accumulated_logging_time': 6.648257732391357}
I0301 01:38:44.471362 140089552271104 logging_writer.py:48] [211893] accumulated_eval_time=2499.424515, accumulated_logging_time=6.648258, accumulated_submission_time=70951.643810, global_step=211893, preemption_count=0, score=70951.643810, test/accuracy=0.627100, test/loss=1.844024, test/num_examples=10000, total_duration=73465.297314, train/accuracy=0.962313, train/loss=0.142464, validation/accuracy=0.755520, validation/loss=1.054633, validation/num_examples=50000
I0301 01:38:47.144090 140089770305280 logging_writer.py:48] [211900] global_step=211900, grad_norm=4.114728927612305, loss=0.5492199063301086
I0301 01:39:20.570602 140089552271104 logging_writer.py:48] [212000] global_step=212000, grad_norm=4.632537364959717, loss=0.6417677402496338
I0301 01:39:54.032733 140089770305280 logging_writer.py:48] [212100] global_step=212100, grad_norm=4.545258045196533, loss=0.6486003994941711
I0301 01:40:27.502413 140089552271104 logging_writer.py:48] [212200] global_step=212200, grad_norm=4.765871524810791, loss=0.5972746014595032
I0301 01:41:00.931942 140089770305280 logging_writer.py:48] [212300] global_step=212300, grad_norm=4.166557788848877, loss=0.5868859887123108
I0301 01:41:34.419720 140089552271104 logging_writer.py:48] [212400] global_step=212400, grad_norm=4.483714580535889, loss=0.6065579652786255
I0301 01:42:07.842528 140089770305280 logging_writer.py:48] [212500] global_step=212500, grad_norm=4.9138898849487305, loss=0.6282117366790771
I0301 01:42:41.393359 140089552271104 logging_writer.py:48] [212600] global_step=212600, grad_norm=4.834135055541992, loss=0.6344704627990723
I0301 01:43:14.851078 140089770305280 logging_writer.py:48] [212700] global_step=212700, grad_norm=4.386221885681152, loss=0.5897433757781982
I0301 01:43:48.311902 140089552271104 logging_writer.py:48] [212800] global_step=212800, grad_norm=4.931925296783447, loss=0.6494665145874023
I0301 01:44:21.763576 140089770305280 logging_writer.py:48] [212900] global_step=212900, grad_norm=4.227001667022705, loss=0.6283699870109558
I0301 01:44:55.173655 140089552271104 logging_writer.py:48] [213000] global_step=213000, grad_norm=4.547633171081543, loss=0.6483298540115356
I0301 01:45:28.634653 140089770305280 logging_writer.py:48] [213100] global_step=213100, grad_norm=5.26528263092041, loss=0.6238072514533997
I0301 01:46:02.062112 140089552271104 logging_writer.py:48] [213200] global_step=213200, grad_norm=4.31921911239624, loss=0.6254428029060364
I0301 01:46:35.538781 140089770305280 logging_writer.py:48] [213300] global_step=213300, grad_norm=4.770367622375488, loss=0.5880533456802368
I0301 01:47:08.965419 140089552271104 logging_writer.py:48] [213400] global_step=213400, grad_norm=4.585981845855713, loss=0.5673612952232361
I0301 01:47:14.457226 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:47:20.549967 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:47:28.785005 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:47:31.064744 140252611495744 submission_runner.py:411] Time since start: 73991.94s, 	Step: 213418, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.1486295908689499, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0539764165878296, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8437877893447876, 'test/num_examples': 10000, 'score': 71461.56416463852, 'total_duration': 73991.9448082447, 'accumulated_submission_time': 71461.56416463852, 'accumulated_eval_time': 2516.0319879055023, 'accumulated_logging_time': 6.712458610534668}
I0301 01:47:31.117732 140089845819136 logging_writer.py:48] [213418] accumulated_eval_time=2516.031988, accumulated_logging_time=6.712459, accumulated_submission_time=71461.564165, global_step=213418, preemption_count=0, score=71461.564165, test/accuracy=0.627800, test/loss=1.843788, test/num_examples=10000, total_duration=73991.944808, train/accuracy=0.960300, train/loss=0.148630, validation/accuracy=0.756200, validation/loss=1.053976, validation/num_examples=50000
I0301 01:47:58.869765 140089870997248 logging_writer.py:48] [213500] global_step=213500, grad_norm=4.400928497314453, loss=0.6505699753761292
I0301 01:48:32.361805 140089845819136 logging_writer.py:48] [213600] global_step=213600, grad_norm=4.471161365509033, loss=0.6635929346084595
I0301 01:49:05.892356 140089870997248 logging_writer.py:48] [213700] global_step=213700, grad_norm=4.245041847229004, loss=0.5582308173179626
I0301 01:49:39.377964 140089845819136 logging_writer.py:48] [213800] global_step=213800, grad_norm=5.850784778594971, loss=0.6021389365196228
I0301 01:50:12.822845 140089870997248 logging_writer.py:48] [213900] global_step=213900, grad_norm=4.465217590332031, loss=0.6356415152549744
I0301 01:50:46.266019 140089845819136 logging_writer.py:48] [214000] global_step=214000, grad_norm=4.449742794036865, loss=0.5707800984382629
I0301 01:51:19.742055 140089870997248 logging_writer.py:48] [214100] global_step=214100, grad_norm=4.096431255340576, loss=0.5250257253646851
I0301 01:51:53.175456 140089845819136 logging_writer.py:48] [214200] global_step=214200, grad_norm=4.550327301025391, loss=0.6340691447257996
I0301 01:52:26.627486 140089870997248 logging_writer.py:48] [214300] global_step=214300, grad_norm=4.248073101043701, loss=0.5345736742019653
I0301 01:53:00.141837 140089845819136 logging_writer.py:48] [214400] global_step=214400, grad_norm=4.313965797424316, loss=0.5639684200286865
I0301 01:53:33.597983 140089870997248 logging_writer.py:48] [214500] global_step=214500, grad_norm=4.353030681610107, loss=0.5976506471633911
I0301 01:54:07.021901 140089845819136 logging_writer.py:48] [214600] global_step=214600, grad_norm=4.795652389526367, loss=0.6149828433990479
I0301 01:54:40.483111 140089870997248 logging_writer.py:48] [214700] global_step=214700, grad_norm=5.124633312225342, loss=0.6372053027153015
I0301 01:55:14.030579 140089845819136 logging_writer.py:48] [214800] global_step=214800, grad_norm=5.175625324249268, loss=0.6059520244598389
I0301 01:55:47.477566 140089870997248 logging_writer.py:48] [214900] global_step=214900, grad_norm=4.810006618499756, loss=0.6504768133163452
I0301 01:56:01.320963 140252611495744 spec.py:321] Evaluating on the training split.
I0301 01:56:07.699270 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 01:56:15.878600 140252611495744 spec.py:349] Evaluating on the test split.
I0301 01:56:18.127753 140252611495744 submission_runner.py:411] Time since start: 74519.01s, 	Step: 214943, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.14881549775600433, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0519776344299316, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8391730785369873, 'test/num_examples': 10000, 'score': 71971.70177531242, 'total_duration': 74519.00781488419, 'accumulated_submission_time': 71971.70177531242, 'accumulated_eval_time': 2532.8387157917023, 'accumulated_logging_time': 6.777054309844971}
I0301 01:56:18.184804 140089862604544 logging_writer.py:48] [214943] accumulated_eval_time=2532.838716, accumulated_logging_time=6.777054, accumulated_submission_time=71971.701775, global_step=214943, preemption_count=0, score=71971.701775, test/accuracy=0.626800, test/loss=1.839173, test/num_examples=10000, total_duration=74519.007815, train/accuracy=0.959263, train/loss=0.148815, validation/accuracy=0.755460, validation/loss=1.051978, validation/num_examples=50000
I0301 01:56:37.591945 140089870997248 logging_writer.py:48] [215000] global_step=215000, grad_norm=4.516607284545898, loss=0.6384378671646118
I0301 01:57:11.038730 140089862604544 logging_writer.py:48] [215100] global_step=215100, grad_norm=4.672736167907715, loss=0.6541105508804321
I0301 01:57:44.505669 140089870997248 logging_writer.py:48] [215200] global_step=215200, grad_norm=4.313976764678955, loss=0.5956600904464722
I0301 01:58:17.976823 140089862604544 logging_writer.py:48] [215300] global_step=215300, grad_norm=4.20300817489624, loss=0.530339777469635
I0301 01:58:51.435126 140089870997248 logging_writer.py:48] [215400] global_step=215400, grad_norm=4.985086441040039, loss=0.6502285003662109
I0301 01:59:24.902333 140089862604544 logging_writer.py:48] [215500] global_step=215500, grad_norm=4.6583251953125, loss=0.6338236331939697
I0301 01:59:58.339489 140089870997248 logging_writer.py:48] [215600] global_step=215600, grad_norm=4.647406101226807, loss=0.6426346302032471
I0301 02:00:31.808441 140089862604544 logging_writer.py:48] [215700] global_step=215700, grad_norm=4.691535472869873, loss=0.6482440233230591
I0301 02:01:05.313295 140089870997248 logging_writer.py:48] [215800] global_step=215800, grad_norm=4.906662940979004, loss=0.7592657804489136
I0301 02:01:38.762224 140089862604544 logging_writer.py:48] [215900] global_step=215900, grad_norm=4.943084239959717, loss=0.6803779006004333
I0301 02:02:12.212001 140089870997248 logging_writer.py:48] [216000] global_step=216000, grad_norm=4.567770957946777, loss=0.6177635192871094
I0301 02:02:45.690908 140089862604544 logging_writer.py:48] [216100] global_step=216100, grad_norm=4.631772041320801, loss=0.6430699825286865
I0301 02:03:19.146379 140089870997248 logging_writer.py:48] [216200] global_step=216200, grad_norm=4.380275249481201, loss=0.6911117434501648
I0301 02:03:52.590558 140089862604544 logging_writer.py:48] [216300] global_step=216300, grad_norm=4.63781213760376, loss=0.6531897783279419
I0301 02:04:26.039047 140089870997248 logging_writer.py:48] [216400] global_step=216400, grad_norm=5.025362968444824, loss=0.6140897870063782
I0301 02:04:48.261131 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:04:54.312753 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:05:02.562515 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:05:04.865038 140252611495744 submission_runner.py:411] Time since start: 75045.75s, 	Step: 216468, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.14529596269130707, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0536319017410278, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.8440710306167603, 'test/num_examples': 10000, 'score': 72481.71206188202, 'total_duration': 75045.74508166313, 'accumulated_submission_time': 72481.71206188202, 'accumulated_eval_time': 2549.4425597190857, 'accumulated_logging_time': 6.845051527023315}
I0301 02:05:04.919898 140089552271104 logging_writer.py:48] [216468] accumulated_eval_time=2549.442560, accumulated_logging_time=6.845052, accumulated_submission_time=72481.712062, global_step=216468, preemption_count=0, score=72481.712062, test/accuracy=0.626900, test/loss=1.844071, test/num_examples=10000, total_duration=75045.745082, train/accuracy=0.961336, train/loss=0.145296, validation/accuracy=0.756060, validation/loss=1.053632, validation/num_examples=50000
I0301 02:05:15.957796 140089770305280 logging_writer.py:48] [216500] global_step=216500, grad_norm=4.649538516998291, loss=0.5628494620323181
I0301 02:05:49.451224 140089552271104 logging_writer.py:48] [216600] global_step=216600, grad_norm=4.520072937011719, loss=0.5975642204284668
I0301 02:06:22.888179 140089770305280 logging_writer.py:48] [216700] global_step=216700, grad_norm=4.484546661376953, loss=0.6317318081855774
I0301 02:06:56.447104 140089552271104 logging_writer.py:48] [216800] global_step=216800, grad_norm=4.38524866104126, loss=0.6080730557441711
I0301 02:07:29.928878 140089770305280 logging_writer.py:48] [216900] global_step=216900, grad_norm=4.616993427276611, loss=0.6140763163566589
I0301 02:08:03.396826 140089552271104 logging_writer.py:48] [217000] global_step=217000, grad_norm=4.634731292724609, loss=0.6110675930976868
I0301 02:08:36.924703 140089770305280 logging_writer.py:48] [217100] global_step=217100, grad_norm=4.593121528625488, loss=0.6058143377304077
I0301 02:09:10.389727 140089552271104 logging_writer.py:48] [217200] global_step=217200, grad_norm=4.732360363006592, loss=0.5722888112068176
I0301 02:09:43.818357 140089770305280 logging_writer.py:48] [217300] global_step=217300, grad_norm=4.7531304359436035, loss=0.6420028209686279
I0301 02:10:17.284806 140089552271104 logging_writer.py:48] [217400] global_step=217400, grad_norm=4.749495506286621, loss=0.6825088262557983
I0301 02:10:50.748111 140089770305280 logging_writer.py:48] [217500] global_step=217500, grad_norm=4.752823352813721, loss=0.585835337638855
I0301 02:11:24.180122 140089552271104 logging_writer.py:48] [217600] global_step=217600, grad_norm=4.960269451141357, loss=0.6644584536552429
I0301 02:11:57.686973 140089770305280 logging_writer.py:48] [217700] global_step=217700, grad_norm=4.594170570373535, loss=0.5668582320213318
I0301 02:12:31.176082 140089552271104 logging_writer.py:48] [217800] global_step=217800, grad_norm=4.674105167388916, loss=0.6534362435340881
I0301 02:13:04.733439 140089770305280 logging_writer.py:48] [217900] global_step=217900, grad_norm=4.719837665557861, loss=0.6331350803375244
I0301 02:13:35.001780 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:13:41.122670 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:13:49.220827 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:13:51.463003 140252611495744 submission_runner.py:411] Time since start: 75572.34s, 	Step: 217992, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14565877616405487, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0531891584396362, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.841581106185913, 'test/num_examples': 10000, 'score': 72991.72962641716, 'total_duration': 75572.34304332733, 'accumulated_submission_time': 72991.72962641716, 'accumulated_eval_time': 2565.9037024974823, 'accumulated_logging_time': 6.909708738327026}
I0301 02:13:51.537741 140089862604544 logging_writer.py:48] [217992] accumulated_eval_time=2565.903702, accumulated_logging_time=6.909709, accumulated_submission_time=72991.729626, global_step=217992, preemption_count=0, score=72991.729626, test/accuracy=0.626400, test/loss=1.841581, test/num_examples=10000, total_duration=75572.343043, train/accuracy=0.961635, train/loss=0.145659, validation/accuracy=0.755900, validation/loss=1.053189, validation/num_examples=50000
I0301 02:13:54.576274 140089870997248 logging_writer.py:48] [218000] global_step=218000, grad_norm=6.332373142242432, loss=0.7441875338554382
I0301 02:14:28.011779 140089862604544 logging_writer.py:48] [218100] global_step=218100, grad_norm=4.22669792175293, loss=0.5993576049804688
I0301 02:15:01.522755 140089870997248 logging_writer.py:48] [218200] global_step=218200, grad_norm=4.783188819885254, loss=0.6592574715614319
I0301 02:15:35.012403 140089862604544 logging_writer.py:48] [218300] global_step=218300, grad_norm=4.538776397705078, loss=0.6585044860839844
I0301 02:16:08.464802 140089870997248 logging_writer.py:48] [218400] global_step=218400, grad_norm=4.409053325653076, loss=0.6528314352035522
I0301 02:16:41.939387 140089862604544 logging_writer.py:48] [218500] global_step=218500, grad_norm=3.969780445098877, loss=0.5077383518218994
I0301 02:17:15.398611 140089870997248 logging_writer.py:48] [218600] global_step=218600, grad_norm=5.231098175048828, loss=0.6476696729660034
I0301 02:17:48.832768 140089862604544 logging_writer.py:48] [218700] global_step=218700, grad_norm=4.561588287353516, loss=0.6239213943481445
I0301 02:18:22.283008 140089870997248 logging_writer.py:48] [218800] global_step=218800, grad_norm=4.915067195892334, loss=0.613645076751709
I0301 02:18:55.716847 140089862604544 logging_writer.py:48] [218900] global_step=218900, grad_norm=4.157439231872559, loss=0.5819262862205505
I0301 02:19:29.297420 140089870997248 logging_writer.py:48] [219000] global_step=219000, grad_norm=4.710813522338867, loss=0.6597141027450562
I0301 02:20:02.743524 140089862604544 logging_writer.py:48] [219100] global_step=219100, grad_norm=4.883922100067139, loss=0.6889219284057617
I0301 02:20:36.214295 140089870997248 logging_writer.py:48] [219200] global_step=219200, grad_norm=4.363555431365967, loss=0.6160839796066284
I0301 02:21:09.646991 140089862604544 logging_writer.py:48] [219300] global_step=219300, grad_norm=4.1868767738342285, loss=0.5836036205291748
I0301 02:21:43.113735 140089870997248 logging_writer.py:48] [219400] global_step=219400, grad_norm=4.908642292022705, loss=0.6199129819869995
I0301 02:22:16.556490 140089862604544 logging_writer.py:48] [219500] global_step=219500, grad_norm=4.102336883544922, loss=0.5641805529594421
I0301 02:22:21.737538 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:22:27.940940 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:22:35.986379 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:22:38.264510 140252611495744 submission_runner.py:411] Time since start: 76099.14s, 	Step: 219517, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.1475105881690979, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.052554965019226, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8403915166854858, 'test/num_examples': 10000, 'score': 73501.86399936676, 'total_duration': 76099.1445813179, 'accumulated_submission_time': 73501.86399936676, 'accumulated_eval_time': 2582.430620908737, 'accumulated_logging_time': 6.99518346786499}
I0301 02:22:38.316923 140089778697984 logging_writer.py:48] [219517] accumulated_eval_time=2582.430621, accumulated_logging_time=6.995183, accumulated_submission_time=73501.863999, global_step=219517, preemption_count=0, score=73501.863999, test/accuracy=0.627900, test/loss=1.840392, test/num_examples=10000, total_duration=76099.144581, train/accuracy=0.959961, train/loss=0.147511, validation/accuracy=0.755800, validation/loss=1.052555, validation/num_examples=50000
I0301 02:23:06.462913 140089837426432 logging_writer.py:48] [219600] global_step=219600, grad_norm=4.618686199188232, loss=0.5712750554084778
I0301 02:23:39.937979 140089778697984 logging_writer.py:48] [219700] global_step=219700, grad_norm=4.750812530517578, loss=0.6470004320144653
I0301 02:24:13.399277 140089837426432 logging_writer.py:48] [219800] global_step=219800, grad_norm=4.4023756980896, loss=0.6084845066070557
I0301 02:24:46.859710 140089778697984 logging_writer.py:48] [219900] global_step=219900, grad_norm=4.231189250946045, loss=0.6272022128105164
I0301 02:25:20.415578 140089837426432 logging_writer.py:48] [220000] global_step=220000, grad_norm=4.115530967712402, loss=0.5870405435562134
I0301 02:25:53.899288 140089778697984 logging_writer.py:48] [220100] global_step=220100, grad_norm=4.805436611175537, loss=0.6122269630432129
I0301 02:26:27.404447 140089837426432 logging_writer.py:48] [220200] global_step=220200, grad_norm=5.053231239318848, loss=0.6801986694335938
I0301 02:27:00.836561 140089778697984 logging_writer.py:48] [220300] global_step=220300, grad_norm=4.528774738311768, loss=0.6587005853652954
I0301 02:27:34.286209 140089837426432 logging_writer.py:48] [220400] global_step=220400, grad_norm=4.21305513381958, loss=0.5497510433197021
I0301 02:28:07.757113 140089778697984 logging_writer.py:48] [220500] global_step=220500, grad_norm=4.449879169464111, loss=0.6251201629638672
I0301 02:28:41.239914 140089837426432 logging_writer.py:48] [220600] global_step=220600, grad_norm=4.670487403869629, loss=0.5631055235862732
I0301 02:29:14.674596 140089778697984 logging_writer.py:48] [220700] global_step=220700, grad_norm=4.407319068908691, loss=0.6023920774459839
I0301 02:29:48.176869 140089837426432 logging_writer.py:48] [220800] global_step=220800, grad_norm=4.214423179626465, loss=0.5765220522880554
I0301 02:30:21.668913 140089778697984 logging_writer.py:48] [220900] global_step=220900, grad_norm=4.335378170013428, loss=0.6292566061019897
I0301 02:30:55.130673 140089837426432 logging_writer.py:48] [221000] global_step=221000, grad_norm=4.558969497680664, loss=0.660919189453125
I0301 02:31:08.339015 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:31:14.727064 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:31:22.805397 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:31:25.093057 140252611495744 submission_runner.py:411] Time since start: 76625.97s, 	Step: 221041, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.1453995704650879, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0543261766433716, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8431285619735718, 'test/num_examples': 10000, 'score': 74011.82157802582, 'total_duration': 76625.97312402725, 'accumulated_submission_time': 74011.82157802582, 'accumulated_eval_time': 2599.1846072673798, 'accumulated_logging_time': 7.0580079555511475}
I0301 02:31:25.146068 140089862604544 logging_writer.py:48] [221041] accumulated_eval_time=2599.184607, accumulated_logging_time=7.058008, accumulated_submission_time=74011.821578, global_step=221041, preemption_count=0, score=74011.821578, test/accuracy=0.627200, test/loss=1.843129, test/num_examples=10000, total_duration=76625.973124, train/accuracy=0.960320, train/loss=0.145400, validation/accuracy=0.755680, validation/loss=1.054326, validation/num_examples=50000
I0301 02:31:45.193463 140089870997248 logging_writer.py:48] [221100] global_step=221100, grad_norm=5.139342308044434, loss=0.7302278280258179
I0301 02:32:18.653401 140089862604544 logging_writer.py:48] [221200] global_step=221200, grad_norm=4.709648132324219, loss=0.6458571553230286
I0301 02:32:52.073420 140089870997248 logging_writer.py:48] [221300] global_step=221300, grad_norm=4.652583599090576, loss=0.6515411138534546
I0301 02:33:25.490413 140089862604544 logging_writer.py:48] [221400] global_step=221400, grad_norm=4.041146278381348, loss=0.5170657634735107
I0301 02:33:58.921159 140089870997248 logging_writer.py:48] [221500] global_step=221500, grad_norm=4.920148849487305, loss=0.603960394859314
I0301 02:34:32.421364 140089862604544 logging_writer.py:48] [221600] global_step=221600, grad_norm=4.328347206115723, loss=0.6155690550804138
I0301 02:35:05.864253 140089870997248 logging_writer.py:48] [221700] global_step=221700, grad_norm=4.550869941711426, loss=0.6060947179794312
I0301 02:35:39.293501 140089862604544 logging_writer.py:48] [221800] global_step=221800, grad_norm=4.47564697265625, loss=0.6294736266136169
I0301 02:36:12.720234 140089870997248 logging_writer.py:48] [221900] global_step=221900, grad_norm=4.243982791900635, loss=0.5822994112968445
I0301 02:36:46.155930 140089862604544 logging_writer.py:48] [222000] global_step=222000, grad_norm=4.403657913208008, loss=0.5809367895126343
I0301 02:37:19.688140 140089870997248 logging_writer.py:48] [222100] global_step=222100, grad_norm=4.572480201721191, loss=0.6564250588417053
I0301 02:37:53.124469 140089862604544 logging_writer.py:48] [222200] global_step=222200, grad_norm=4.458846569061279, loss=0.6388000249862671
I0301 02:38:26.544088 140089870997248 logging_writer.py:48] [222300] global_step=222300, grad_norm=4.376144886016846, loss=0.6690387725830078
I0301 02:38:59.996880 140089862604544 logging_writer.py:48] [222400] global_step=222400, grad_norm=4.566076755523682, loss=0.6859960556030273
I0301 02:39:33.411406 140089870997248 logging_writer.py:48] [222500] global_step=222500, grad_norm=5.765942096710205, loss=0.6588327288627625
I0301 02:39:55.284397 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:40:01.393020 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:40:09.666702 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:40:11.939911 140252611495744 submission_runner.py:411] Time since start: 77152.82s, 	Step: 222567, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14474906027317047, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.052621603012085, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.84185791015625, 'test/num_examples': 10000, 'score': 74521.89327073097, 'total_duration': 77152.81997966766, 'accumulated_submission_time': 74521.89327073097, 'accumulated_eval_time': 2615.8400671482086, 'accumulated_logging_time': 7.1233837604522705}
I0301 02:40:11.995180 140089837426432 logging_writer.py:48] [222567] accumulated_eval_time=2615.840067, accumulated_logging_time=7.123384, accumulated_submission_time=74521.893271, global_step=222567, preemption_count=0, score=74521.893271, test/accuracy=0.627900, test/loss=1.841858, test/num_examples=10000, total_duration=77152.819980, train/accuracy=0.961356, train/loss=0.144749, validation/accuracy=0.755640, validation/loss=1.052622, validation/num_examples=50000
I0301 02:40:23.366988 140089845819136 logging_writer.py:48] [222600] global_step=222600, grad_norm=4.596818447113037, loss=0.610504686832428
I0301 02:40:56.819643 140089837426432 logging_writer.py:48] [222700] global_step=222700, grad_norm=4.5803704261779785, loss=0.6024987697601318
I0301 02:41:30.275144 140089845819136 logging_writer.py:48] [222800] global_step=222800, grad_norm=4.65988826751709, loss=0.6015135645866394
I0301 02:42:03.697431 140089837426432 logging_writer.py:48] [222900] global_step=222900, grad_norm=4.538620471954346, loss=0.5426026582717896
I0301 02:42:37.152621 140089845819136 logging_writer.py:48] [223000] global_step=223000, grad_norm=5.520429611206055, loss=0.6920563578605652
I0301 02:43:10.617764 140089837426432 logging_writer.py:48] [223100] global_step=223100, grad_norm=5.044398784637451, loss=0.6044014692306519
I0301 02:43:44.133091 140089845819136 logging_writer.py:48] [223200] global_step=223200, grad_norm=4.724261283874512, loss=0.6535729169845581
I0301 02:44:17.574389 140089837426432 logging_writer.py:48] [223300] global_step=223300, grad_norm=4.310482978820801, loss=0.635849118232727
I0301 02:44:51.040261 140089845819136 logging_writer.py:48] [223400] global_step=223400, grad_norm=4.5396246910095215, loss=0.5132554173469543
I0301 02:45:24.474316 140089837426432 logging_writer.py:48] [223500] global_step=223500, grad_norm=4.45076322555542, loss=0.6752223968505859
I0301 02:45:57.955394 140089845819136 logging_writer.py:48] [223600] global_step=223600, grad_norm=4.7936553955078125, loss=0.6580756306648254
I0301 02:46:31.397394 140089837426432 logging_writer.py:48] [223700] global_step=223700, grad_norm=4.604355335235596, loss=0.6726136803627014
I0301 02:47:04.882443 140089845819136 logging_writer.py:48] [223800] global_step=223800, grad_norm=4.3286919593811035, loss=0.6588747501373291
I0301 02:47:38.372996 140089837426432 logging_writer.py:48] [223900] global_step=223900, grad_norm=5.322882175445557, loss=0.774291455745697
I0301 02:48:11.796956 140089845819136 logging_writer.py:48] [224000] global_step=224000, grad_norm=4.242794513702393, loss=0.5676034688949585
I0301 02:48:42.047258 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:48:48.287993 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:48:56.359204 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:48:58.643003 140252611495744 submission_runner.py:411] Time since start: 77679.52s, 	Step: 224092, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.1450876146554947, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0538315773010254, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8424419164657593, 'test/num_examples': 10000, 'score': 75031.88020396233, 'total_duration': 77679.52306723595, 'accumulated_submission_time': 75031.88020396233, 'accumulated_eval_time': 2632.435756444931, 'accumulated_logging_time': 7.188803672790527}
I0301 02:48:58.699395 140089770305280 logging_writer.py:48] [224092] accumulated_eval_time=2632.435756, accumulated_logging_time=7.188804, accumulated_submission_time=75031.880204, global_step=224092, preemption_count=0, score=75031.880204, test/accuracy=0.626500, test/loss=1.842442, test/num_examples=10000, total_duration=77679.523067, train/accuracy=0.961834, train/loss=0.145088, validation/accuracy=0.755780, validation/loss=1.053832, validation/num_examples=50000
I0301 02:49:01.735642 140089778697984 logging_writer.py:48] [224100] global_step=224100, grad_norm=4.748082160949707, loss=0.6846970319747925
I0301 02:49:35.258411 140089770305280 logging_writer.py:48] [224200] global_step=224200, grad_norm=4.769686222076416, loss=0.6677333116531372
I0301 02:50:08.739102 140089778697984 logging_writer.py:48] [224300] global_step=224300, grad_norm=4.555494785308838, loss=0.5586740374565125
I0301 02:50:42.213560 140089770305280 logging_writer.py:48] [224400] global_step=224400, grad_norm=4.241482257843018, loss=0.5932630896568298
I0301 02:51:15.646357 140089778697984 logging_writer.py:48] [224500] global_step=224500, grad_norm=4.357548713684082, loss=0.5912690162658691
I0301 02:51:49.078026 140089770305280 logging_writer.py:48] [224600] global_step=224600, grad_norm=4.466117858886719, loss=0.6388118863105774
I0301 02:52:22.526010 140089778697984 logging_writer.py:48] [224700] global_step=224700, grad_norm=4.191916465759277, loss=0.5892870426177979
I0301 02:52:55.955032 140089770305280 logging_writer.py:48] [224800] global_step=224800, grad_norm=4.268874168395996, loss=0.5555040836334229
I0301 02:53:29.408289 140089778697984 logging_writer.py:48] [224900] global_step=224900, grad_norm=4.692342281341553, loss=0.6640554666519165
I0301 02:54:02.819182 140089770305280 logging_writer.py:48] [225000] global_step=225000, grad_norm=4.681553840637207, loss=0.6606358289718628
I0301 02:54:36.279081 140089778697984 logging_writer.py:48] [225100] global_step=225100, grad_norm=4.482715129852295, loss=0.6288408041000366
I0301 02:55:09.687444 140089770305280 logging_writer.py:48] [225200] global_step=225200, grad_norm=4.384554862976074, loss=0.6845148205757141
I0301 02:55:43.212541 140089778697984 logging_writer.py:48] [225300] global_step=225300, grad_norm=4.221151828765869, loss=0.5662845373153687
I0301 02:56:16.666864 140089770305280 logging_writer.py:48] [225400] global_step=225400, grad_norm=4.638729095458984, loss=0.6046898365020752
I0301 02:56:50.111801 140089778697984 logging_writer.py:48] [225500] global_step=225500, grad_norm=4.903330326080322, loss=0.6885273456573486
I0301 02:57:23.534507 140089770305280 logging_writer.py:48] [225600] global_step=225600, grad_norm=4.633721828460693, loss=0.6414955258369446
I0301 02:57:28.706760 140252611495744 spec.py:321] Evaluating on the training split.
I0301 02:57:34.867788 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 02:57:43.098640 140252611495744 spec.py:349] Evaluating on the test split.
I0301 02:57:45.367286 140252611495744 submission_runner.py:411] Time since start: 78206.25s, 	Step: 225617, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14972545206546783, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.053784728050232, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8433343172073364, 'test/num_examples': 10000, 'score': 75541.82284140587, 'total_duration': 78206.24735283852, 'accumulated_submission_time': 75541.82284140587, 'accumulated_eval_time': 2649.096224308014, 'accumulated_logging_time': 7.255248308181763}
I0301 02:57:45.422851 140089770305280 logging_writer.py:48] [225617] accumulated_eval_time=2649.096224, accumulated_logging_time=7.255248, accumulated_submission_time=75541.822841, global_step=225617, preemption_count=0, score=75541.822841, test/accuracy=0.628200, test/loss=1.843334, test/num_examples=10000, total_duration=78206.247353, train/accuracy=0.960639, train/loss=0.149725, validation/accuracy=0.755880, validation/loss=1.053785, validation/num_examples=50000
I0301 02:58:13.512635 140089778697984 logging_writer.py:48] [225700] global_step=225700, grad_norm=4.767435550689697, loss=0.5870051980018616
I0301 02:58:46.955759 140089770305280 logging_writer.py:48] [225800] global_step=225800, grad_norm=4.3100152015686035, loss=0.5975070595741272
I0301 02:59:20.396806 140089778697984 logging_writer.py:48] [225900] global_step=225900, grad_norm=4.399377346038818, loss=0.6866112351417542
I0301 02:59:53.827385 140089770305280 logging_writer.py:48] [226000] global_step=226000, grad_norm=4.730621814727783, loss=0.6023807525634766
I0301 03:00:27.266047 140089778697984 logging_writer.py:48] [226100] global_step=226100, grad_norm=4.515761375427246, loss=0.6739578247070312
I0301 03:01:00.731219 140089770305280 logging_writer.py:48] [226200] global_step=226200, grad_norm=4.583622932434082, loss=0.585335373878479
I0301 03:01:34.238049 140089778697984 logging_writer.py:48] [226300] global_step=226300, grad_norm=4.238281726837158, loss=0.5741208791732788
I0301 03:02:07.679498 140089770305280 logging_writer.py:48] [226400] global_step=226400, grad_norm=4.896437644958496, loss=0.552540123462677
I0301 03:02:41.153477 140089778697984 logging_writer.py:48] [226500] global_step=226500, grad_norm=4.860553741455078, loss=0.6759101152420044
I0301 03:03:14.580798 140089770305280 logging_writer.py:48] [226600] global_step=226600, grad_norm=4.516014575958252, loss=0.685124933719635
I0301 03:03:48.042948 140089778697984 logging_writer.py:48] [226700] global_step=226700, grad_norm=5.044347763061523, loss=0.6058378219604492
I0301 03:04:21.465349 140089770305280 logging_writer.py:48] [226800] global_step=226800, grad_norm=4.553722858428955, loss=0.6062325835227966
I0301 03:04:54.905672 140089778697984 logging_writer.py:48] [226900] global_step=226900, grad_norm=4.059245586395264, loss=0.5854741930961609
I0301 03:05:28.394069 140089770305280 logging_writer.py:48] [227000] global_step=227000, grad_norm=4.874324798583984, loss=0.6343554854393005
I0301 03:06:01.813159 140089778697984 logging_writer.py:48] [227100] global_step=227100, grad_norm=4.499650001525879, loss=0.6278320550918579
I0301 03:06:15.666371 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:06:21.898444 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:06:30.054046 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:06:32.326567 140252611495744 submission_runner.py:411] Time since start: 78733.21s, 	Step: 227143, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.1467025727033615, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0523626804351807, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8423489332199097, 'test/num_examples': 10000, 'score': 76052.00051903725, 'total_duration': 78733.20663499832, 'accumulated_submission_time': 76052.00051903725, 'accumulated_eval_time': 2665.7563643455505, 'accumulated_logging_time': 7.321617126464844}
I0301 03:06:32.391552 140089552271104 logging_writer.py:48] [227143] accumulated_eval_time=2665.756364, accumulated_logging_time=7.321617, accumulated_submission_time=76052.000519, global_step=227143, preemption_count=0, score=76052.000519, test/accuracy=0.627400, test/loss=1.842349, test/num_examples=10000, total_duration=78733.206635, train/accuracy=0.960858, train/loss=0.146703, validation/accuracy=0.755960, validation/loss=1.052363, validation/num_examples=50000
I0301 03:06:51.783774 140089770305280 logging_writer.py:48] [227200] global_step=227200, grad_norm=4.5545430183410645, loss=0.61008620262146
I0301 03:07:25.225163 140089552271104 logging_writer.py:48] [227300] global_step=227300, grad_norm=4.074431896209717, loss=0.5436452031135559
I0301 03:07:58.805634 140089770305280 logging_writer.py:48] [227400] global_step=227400, grad_norm=4.571905136108398, loss=0.5534626245498657
I0301 03:08:32.267989 140089552271104 logging_writer.py:48] [227500] global_step=227500, grad_norm=4.357079029083252, loss=0.6201373934745789
I0301 03:09:05.713597 140089770305280 logging_writer.py:48] [227600] global_step=227600, grad_norm=4.35991096496582, loss=0.5756250619888306
I0301 03:09:39.139485 140089552271104 logging_writer.py:48] [227700] global_step=227700, grad_norm=5.254115104675293, loss=0.6305380463600159
I0301 03:10:12.587812 140089770305280 logging_writer.py:48] [227800] global_step=227800, grad_norm=5.062458515167236, loss=0.6090130805969238
I0301 03:10:46.017336 140089552271104 logging_writer.py:48] [227900] global_step=227900, grad_norm=4.784565448760986, loss=0.6205041408538818
I0301 03:11:19.713254 140089770305280 logging_writer.py:48] [228000] global_step=228000, grad_norm=4.470592498779297, loss=0.6244242191314697
I0301 03:11:53.187565 140089552271104 logging_writer.py:48] [228100] global_step=228100, grad_norm=4.76349401473999, loss=0.6538400650024414
I0301 03:12:26.617976 140089770305280 logging_writer.py:48] [228200] global_step=228200, grad_norm=4.265477657318115, loss=0.610460638999939
I0301 03:13:00.085073 140089552271104 logging_writer.py:48] [228300] global_step=228300, grad_norm=4.593439102172852, loss=0.6632588505744934
I0301 03:13:33.514065 140089770305280 logging_writer.py:48] [228400] global_step=228400, grad_norm=4.05040979385376, loss=0.5563814043998718
I0301 03:14:07.133636 140089552271104 logging_writer.py:48] [228500] global_step=228500, grad_norm=4.30849552154541, loss=0.5462786555290222
I0301 03:14:40.591309 140089770305280 logging_writer.py:48] [228600] global_step=228600, grad_norm=4.976977825164795, loss=0.6996846795082092
I0301 03:15:02.443096 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:15:08.546651 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:15:16.679698 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:15:18.969578 140252611495744 submission_runner.py:411] Time since start: 79259.85s, 	Step: 228667, 	{'train/accuracy': 0.962890625, 'train/loss': 0.1419253796339035, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0544745922088623, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8436956405639648, 'test/num_examples': 10000, 'score': 76561.98649954796, 'total_duration': 79259.84963774681, 'accumulated_submission_time': 76561.98649954796, 'accumulated_eval_time': 2682.2827892303467, 'accumulated_logging_time': 7.397505044937134}
I0301 03:15:19.027908 140089854211840 logging_writer.py:48] [228667] accumulated_eval_time=2682.282789, accumulated_logging_time=7.397505, accumulated_submission_time=76561.986500, global_step=228667, preemption_count=0, score=76561.986500, test/accuracy=0.627200, test/loss=1.843696, test/num_examples=10000, total_duration=79259.849638, train/accuracy=0.962891, train/loss=0.141925, validation/accuracy=0.756020, validation/loss=1.054475, validation/num_examples=50000
I0301 03:15:30.423864 140089879389952 logging_writer.py:48] [228700] global_step=228700, grad_norm=5.411428451538086, loss=0.6337555050849915
I0301 03:16:03.861271 140089854211840 logging_writer.py:48] [228800] global_step=228800, grad_norm=4.325077056884766, loss=0.6021600365638733
I0301 03:16:37.285100 140089879389952 logging_writer.py:48] [228900] global_step=228900, grad_norm=4.564232349395752, loss=0.6306732892990112
I0301 03:17:10.764874 140089854211840 logging_writer.py:48] [229000] global_step=229000, grad_norm=4.12183141708374, loss=0.5579988360404968
I0301 03:17:44.238566 140089879389952 logging_writer.py:48] [229100] global_step=229100, grad_norm=4.945369720458984, loss=0.6604013442993164
I0301 03:18:17.665407 140089854211840 logging_writer.py:48] [229200] global_step=229200, grad_norm=4.55692195892334, loss=0.6223543882369995
I0301 03:18:51.136456 140089879389952 logging_writer.py:48] [229300] global_step=229300, grad_norm=4.479496479034424, loss=0.6162319779396057
I0301 03:19:24.576200 140089854211840 logging_writer.py:48] [229400] global_step=229400, grad_norm=4.153385639190674, loss=0.5516706109046936
I0301 03:19:58.169188 140089879389952 logging_writer.py:48] [229500] global_step=229500, grad_norm=4.489864349365234, loss=0.6718495488166809
I0301 03:20:31.619019 140089854211840 logging_writer.py:48] [229600] global_step=229600, grad_norm=4.227680683135986, loss=0.6575822234153748
I0301 03:21:05.055383 140089879389952 logging_writer.py:48] [229700] global_step=229700, grad_norm=4.821732997894287, loss=0.586524486541748
I0301 03:21:38.512828 140089854211840 logging_writer.py:48] [229800] global_step=229800, grad_norm=4.490629196166992, loss=0.5455759167671204
I0301 03:22:11.958011 140089879389952 logging_writer.py:48] [229900] global_step=229900, grad_norm=4.277910232543945, loss=0.6022486686706543
I0301 03:22:45.398339 140089854211840 logging_writer.py:48] [230000] global_step=230000, grad_norm=4.4702229499816895, loss=0.5849310159683228
I0301 03:23:18.833965 140089879389952 logging_writer.py:48] [230100] global_step=230100, grad_norm=4.887095928192139, loss=0.7104355692863464
I0301 03:23:49.093245 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:23:55.377907 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:24:03.566798 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:24:05.897912 140252611495744 submission_runner.py:411] Time since start: 79786.78s, 	Step: 230192, 	{'train/accuracy': 0.9621332883834839, 'train/loss': 0.14512936770915985, 'validation/accuracy': 0.7562199831008911, 'validation/loss': 1.053865909576416, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8432239294052124, 'test/num_examples': 10000, 'score': 77071.98637270927, 'total_duration': 79786.77798008919, 'accumulated_submission_time': 77071.98637270927, 'accumulated_eval_time': 2699.087404489517, 'accumulated_logging_time': 7.466514587402344}
I0301 03:24:05.957631 140089770305280 logging_writer.py:48] [230192] accumulated_eval_time=2699.087404, accumulated_logging_time=7.466515, accumulated_submission_time=77071.986373, global_step=230192, preemption_count=0, score=77071.986373, test/accuracy=0.627500, test/loss=1.843224, test/num_examples=10000, total_duration=79786.777980, train/accuracy=0.962133, train/loss=0.145129, validation/accuracy=0.756220, validation/loss=1.053866, validation/num_examples=50000
I0301 03:24:08.968555 140089778697984 logging_writer.py:48] [230200] global_step=230200, grad_norm=5.206189155578613, loss=0.6766154170036316
I0301 03:24:42.450114 140089770305280 logging_writer.py:48] [230300] global_step=230300, grad_norm=4.477675914764404, loss=0.588646411895752
I0301 03:25:15.890164 140089778697984 logging_writer.py:48] [230400] global_step=230400, grad_norm=4.512090682983398, loss=0.6667163372039795
I0301 03:25:49.327482 140089770305280 logging_writer.py:48] [230500] global_step=230500, grad_norm=4.302963733673096, loss=0.6547380685806274
I0301 03:26:22.962327 140089778697984 logging_writer.py:48] [230600] global_step=230600, grad_norm=4.586442947387695, loss=0.5881831645965576
I0301 03:26:56.388772 140089770305280 logging_writer.py:48] [230700] global_step=230700, grad_norm=4.937550067901611, loss=0.6310895085334778
I0301 03:27:29.856653 140089778697984 logging_writer.py:48] [230800] global_step=230800, grad_norm=4.475752353668213, loss=0.6295059323310852
I0301 03:28:03.287811 140089770305280 logging_writer.py:48] [230900] global_step=230900, grad_norm=4.326602935791016, loss=0.6309413313865662
I0301 03:28:36.765662 140089778697984 logging_writer.py:48] [231000] global_step=231000, grad_norm=4.735996246337891, loss=0.64202880859375
I0301 03:29:10.196525 140089770305280 logging_writer.py:48] [231100] global_step=231100, grad_norm=4.007980823516846, loss=0.5254583358764648
I0301 03:29:43.674310 140089778697984 logging_writer.py:48] [231200] global_step=231200, grad_norm=5.139501571655273, loss=0.7122742533683777
I0301 03:30:17.136047 140089770305280 logging_writer.py:48] [231300] global_step=231300, grad_norm=4.482360363006592, loss=0.619784951210022
I0301 03:30:50.588380 140089778697984 logging_writer.py:48] [231400] global_step=231400, grad_norm=4.578143119812012, loss=0.6927129030227661
I0301 03:31:24.058041 140089770305280 logging_writer.py:48] [231500] global_step=231500, grad_norm=4.425815582275391, loss=0.6294010877609253
I0301 03:31:57.622821 140089778697984 logging_writer.py:48] [231600] global_step=231600, grad_norm=4.528421878814697, loss=0.6638123989105225
I0301 03:32:31.087859 140089770305280 logging_writer.py:48] [231700] global_step=231700, grad_norm=4.810657501220703, loss=0.6404533982276917
I0301 03:32:35.906551 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:32:42.736476 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:32:50.816225 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:32:53.676325 140252611495744 submission_runner.py:411] Time since start: 80314.56s, 	Step: 231716, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14448919892311096, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.053759217262268, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8422167301177979, 'test/num_examples': 10000, 'score': 77581.86835837364, 'total_duration': 80314.55636286736, 'accumulated_submission_time': 77581.86835837364, 'accumulated_eval_time': 2716.8570907115936, 'accumulated_logging_time': 7.537179708480835}
I0301 03:32:53.724730 140089870997248 logging_writer.py:48] [231716] accumulated_eval_time=2716.857091, accumulated_logging_time=7.537180, accumulated_submission_time=77581.868358, global_step=231716, preemption_count=0, score=77581.868358, test/accuracy=0.627300, test/loss=1.842217, test/num_examples=10000, total_duration=80314.556363, train/accuracy=0.960957, train/loss=0.144489, validation/accuracy=0.756080, validation/loss=1.053759, validation/num_examples=50000
I0301 03:33:22.150559 140089879389952 logging_writer.py:48] [231800] global_step=231800, grad_norm=4.9286909103393555, loss=0.7008832097053528
I0301 03:33:55.629663 140089870997248 logging_writer.py:48] [231900] global_step=231900, grad_norm=4.562514781951904, loss=0.6417434811592102
I0301 03:34:29.082411 140089879389952 logging_writer.py:48] [232000] global_step=232000, grad_norm=4.374069690704346, loss=0.5475471019744873
I0301 03:35:02.531324 140089870997248 logging_writer.py:48] [232100] global_step=232100, grad_norm=4.5709614753723145, loss=0.621096670627594
I0301 03:35:36.002687 140089879389952 logging_writer.py:48] [232200] global_step=232200, grad_norm=4.580721855163574, loss=0.66114342212677
I0301 03:36:09.443185 140089870997248 logging_writer.py:48] [232300] global_step=232300, grad_norm=4.665266990661621, loss=0.7142189741134644
I0301 03:36:42.882702 140089879389952 logging_writer.py:48] [232400] global_step=232400, grad_norm=4.707469463348389, loss=0.5774520039558411
I0301 03:37:16.335394 140089870997248 logging_writer.py:48] [232500] global_step=232500, grad_norm=4.219201564788818, loss=0.5781034827232361
I0301 03:37:49.765003 140089879389952 logging_writer.py:48] [232600] global_step=232600, grad_norm=4.340318202972412, loss=0.6486960649490356
I0301 03:38:23.338264 140089870997248 logging_writer.py:48] [232700] global_step=232700, grad_norm=4.75416374206543, loss=0.6185765266418457
I0301 03:38:56.785353 140089879389952 logging_writer.py:48] [232800] global_step=232800, grad_norm=4.648956298828125, loss=0.6446878910064697
I0301 03:39:30.229609 140089870997248 logging_writer.py:48] [232900] global_step=232900, grad_norm=4.030467987060547, loss=0.575727105140686
I0301 03:40:03.664136 140089879389952 logging_writer.py:48] [233000] global_step=233000, grad_norm=4.315762042999268, loss=0.6091388463973999
I0301 03:40:37.180322 140089870997248 logging_writer.py:48] [233100] global_step=233100, grad_norm=4.9778733253479, loss=0.5851483941078186
I0301 03:41:10.631422 140089879389952 logging_writer.py:48] [233200] global_step=233200, grad_norm=4.455233097076416, loss=0.6214115023612976
I0301 03:41:23.857244 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:41:29.957666 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:41:38.098711 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:41:40.364945 140252611495744 submission_runner.py:411] Time since start: 80841.25s, 	Step: 233241, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14682520925998688, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0530420541763306, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8426487445831299, 'test/num_examples': 10000, 'score': 78091.93625879288, 'total_duration': 80841.24501657486, 'accumulated_submission_time': 78091.93625879288, 'accumulated_eval_time': 2733.3647408485413, 'accumulated_logging_time': 7.594546318054199}
I0301 03:41:40.422755 140089778697984 logging_writer.py:48] [233241] accumulated_eval_time=2733.364741, accumulated_logging_time=7.594546, accumulated_submission_time=78091.936259, global_step=233241, preemption_count=0, score=78091.936259, test/accuracy=0.627100, test/loss=1.842649, test/num_examples=10000, total_duration=80841.245017, train/accuracy=0.960200, train/loss=0.146825, validation/accuracy=0.755680, validation/loss=1.053042, validation/num_examples=50000
I0301 03:42:00.500215 140089837426432 logging_writer.py:48] [233300] global_step=233300, grad_norm=4.316929817199707, loss=0.5793712139129639
I0301 03:42:33.946940 140089778697984 logging_writer.py:48] [233400] global_step=233400, grad_norm=4.589156627655029, loss=0.6065791249275208
I0301 03:43:07.418119 140089837426432 logging_writer.py:48] [233500] global_step=233500, grad_norm=5.3935956954956055, loss=0.6568714380264282
I0301 03:43:40.869053 140089778697984 logging_writer.py:48] [233600] global_step=233600, grad_norm=4.516613960266113, loss=0.5314634442329407
I0301 03:44:14.393717 140089837426432 logging_writer.py:48] [233700] global_step=233700, grad_norm=4.567445755004883, loss=0.6593330502510071
I0301 03:44:47.854225 140089778697984 logging_writer.py:48] [233800] global_step=233800, grad_norm=4.3365397453308105, loss=0.6457162499427795
I0301 03:45:21.333204 140089837426432 logging_writer.py:48] [233900] global_step=233900, grad_norm=4.187381744384766, loss=0.5074325799942017
I0301 03:45:54.772193 140089778697984 logging_writer.py:48] [234000] global_step=234000, grad_norm=4.783894062042236, loss=0.5725987553596497
I0301 03:46:28.221625 140089837426432 logging_writer.py:48] [234100] global_step=234100, grad_norm=4.267868995666504, loss=0.5041272044181824
I0301 03:47:01.708772 140089778697984 logging_writer.py:48] [234200] global_step=234200, grad_norm=4.800155162811279, loss=0.545754611492157
I0301 03:47:35.135572 140089837426432 logging_writer.py:48] [234300] global_step=234300, grad_norm=4.2341694831848145, loss=0.6160978078842163
I0301 03:48:08.604428 140089778697984 logging_writer.py:48] [234400] global_step=234400, grad_norm=4.706140995025635, loss=0.5972544550895691
I0301 03:48:42.029633 140089837426432 logging_writer.py:48] [234500] global_step=234500, grad_norm=4.808832168579102, loss=0.6304104924201965
I0301 03:49:15.490310 140089778697984 logging_writer.py:48] [234600] global_step=234600, grad_norm=4.702714920043945, loss=0.6089984178543091
I0301 03:49:48.931325 140089837426432 logging_writer.py:48] [234700] global_step=234700, grad_norm=4.677229881286621, loss=0.6642570495605469
I0301 03:50:10.465124 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:50:16.839218 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:50:24.939444 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:50:27.251514 140252611495744 submission_runner.py:411] Time since start: 81368.13s, 	Step: 234766, 	{'train/accuracy': 0.9616948366165161, 'train/loss': 0.14411841332912445, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.0534793138504028, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8427488803863525, 'test/num_examples': 10000, 'score': 78601.9127805233, 'total_duration': 81368.13152265549, 'accumulated_submission_time': 78601.9127805233, 'accumulated_eval_time': 2750.1510181427, 'accumulated_logging_time': 7.663392543792725}
I0301 03:50:27.318140 140089862604544 logging_writer.py:48] [234766] accumulated_eval_time=2750.151018, accumulated_logging_time=7.663393, accumulated_submission_time=78601.912781, global_step=234766, preemption_count=0, score=78601.912781, test/accuracy=0.627600, test/loss=1.842749, test/num_examples=10000, total_duration=81368.131523, train/accuracy=0.961695, train/loss=0.144118, validation/accuracy=0.756260, validation/loss=1.053479, validation/num_examples=50000
I0301 03:50:39.029349 140089870997248 logging_writer.py:48] [234800] global_step=234800, grad_norm=4.311532974243164, loss=0.6342004537582397
I0301 03:51:12.475751 140089862604544 logging_writer.py:48] [234900] global_step=234900, grad_norm=5.208341598510742, loss=0.7029837369918823
I0301 03:51:45.948388 140089870997248 logging_writer.py:48] [235000] global_step=235000, grad_norm=4.845181941986084, loss=0.6177695393562317
I0301 03:52:19.376541 140089862604544 logging_writer.py:48] [235100] global_step=235100, grad_norm=5.685459136962891, loss=0.6567767262458801
I0301 03:52:52.834347 140089870997248 logging_writer.py:48] [235200] global_step=235200, grad_norm=4.295130252838135, loss=0.5897203683853149
I0301 03:53:26.309885 140089862604544 logging_writer.py:48] [235300] global_step=235300, grad_norm=4.792186737060547, loss=0.6289018392562866
I0301 03:53:59.787222 140089870997248 logging_writer.py:48] [235400] global_step=235400, grad_norm=5.0032639503479, loss=0.6423351764678955
I0301 03:54:33.244472 140089862604544 logging_writer.py:48] [235500] global_step=235500, grad_norm=4.587875843048096, loss=0.6098977327346802
I0301 03:55:06.688036 140089870997248 logging_writer.py:48] [235600] global_step=235600, grad_norm=4.511375427246094, loss=0.6406089067459106
I0301 03:55:40.136844 140089862604544 logging_writer.py:48] [235700] global_step=235700, grad_norm=4.813362121582031, loss=0.6447365283966064
I0301 03:56:13.603015 140089870997248 logging_writer.py:48] [235800] global_step=235800, grad_norm=4.717925071716309, loss=0.6129064559936523
I0301 03:56:47.167351 140089862604544 logging_writer.py:48] [235900] global_step=235900, grad_norm=4.424774646759033, loss=0.586135983467102
I0301 03:57:20.651676 140089870997248 logging_writer.py:48] [236000] global_step=236000, grad_norm=4.453549861907959, loss=0.6247788071632385
I0301 03:57:54.110327 140089862604544 logging_writer.py:48] [236100] global_step=236100, grad_norm=4.4126057624816895, loss=0.6216598749160767
I0301 03:58:27.573400 140089870997248 logging_writer.py:48] [236200] global_step=236200, grad_norm=4.249220371246338, loss=0.5874536037445068
I0301 03:58:57.500602 140252611495744 spec.py:321] Evaluating on the training split.
I0301 03:59:03.792664 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 03:59:11.874114 140252611495744 spec.py:349] Evaluating on the test split.
I0301 03:59:14.158332 140252611495744 submission_runner.py:411] Time since start: 81895.04s, 	Step: 236291, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14358919858932495, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0528885126113892, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8410484790802002, 'test/num_examples': 10000, 'score': 79112.02974677086, 'total_duration': 81895.03840327263, 'accumulated_submission_time': 79112.02974677086, 'accumulated_eval_time': 2766.8086998462677, 'accumulated_logging_time': 7.740317106246948}
I0301 03:59:14.215130 140089552271104 logging_writer.py:48] [236291] accumulated_eval_time=2766.808700, accumulated_logging_time=7.740317, accumulated_submission_time=79112.029747, global_step=236291, preemption_count=0, score=79112.029747, test/accuracy=0.627700, test/loss=1.841048, test/num_examples=10000, total_duration=81895.038403, train/accuracy=0.961356, train/loss=0.143589, validation/accuracy=0.756080, validation/loss=1.052889, validation/num_examples=50000
I0301 03:59:17.553287 140089778697984 logging_writer.py:48] [236300] global_step=236300, grad_norm=4.561129093170166, loss=0.6230021715164185
I0301 03:59:50.994042 140089552271104 logging_writer.py:48] [236400] global_step=236400, grad_norm=4.4063873291015625, loss=0.597620964050293
I0301 04:00:24.448017 140089778697984 logging_writer.py:48] [236500] global_step=236500, grad_norm=4.455272197723389, loss=0.6648704409599304
I0301 04:00:57.876174 140089552271104 logging_writer.py:48] [236600] global_step=236600, grad_norm=4.553277969360352, loss=0.6768767237663269
I0301 04:01:31.348357 140089778697984 logging_writer.py:48] [236700] global_step=236700, grad_norm=4.750866413116455, loss=0.6473467946052551
I0301 04:02:04.801123 140089552271104 logging_writer.py:48] [236800] global_step=236800, grad_norm=4.505403995513916, loss=0.6384254097938538
I0301 04:02:38.310715 140089778697984 logging_writer.py:48] [236900] global_step=236900, grad_norm=4.052892684936523, loss=0.5462957620620728
I0301 04:03:11.737766 140089552271104 logging_writer.py:48] [237000] global_step=237000, grad_norm=4.4336018562316895, loss=0.6593645811080933
I0301 04:03:45.217385 140089778697984 logging_writer.py:48] [237100] global_step=237100, grad_norm=4.076115131378174, loss=0.6375850439071655
I0301 04:04:18.639786 140089552271104 logging_writer.py:48] [237200] global_step=237200, grad_norm=4.630143642425537, loss=0.7234103679656982
I0301 04:04:52.100703 140089778697984 logging_writer.py:48] [237300] global_step=237300, grad_norm=4.332231044769287, loss=0.5742052793502808
I0301 04:05:25.521008 140089552271104 logging_writer.py:48] [237400] global_step=237400, grad_norm=5.268266201019287, loss=0.6476864814758301
I0301 04:05:58.987492 140089778697984 logging_writer.py:48] [237500] global_step=237500, grad_norm=4.720273017883301, loss=0.6800946593284607
I0301 04:06:32.410947 140089552271104 logging_writer.py:48] [237600] global_step=237600, grad_norm=4.58796501159668, loss=0.6856659054756165
I0301 04:07:05.877354 140089778697984 logging_writer.py:48] [237700] global_step=237700, grad_norm=4.842214584350586, loss=0.6548517942428589
I0301 04:07:39.344737 140089552271104 logging_writer.py:48] [237800] global_step=237800, grad_norm=4.68665075302124, loss=0.6874426007270813
I0301 04:07:44.187819 140252611495744 spec.py:321] Evaluating on the training split.
I0301 04:07:50.271882 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 04:07:58.494703 140252611495744 spec.py:349] Evaluating on the test split.
I0301 04:08:00.725709 140252611495744 submission_runner.py:411] Time since start: 82421.61s, 	Step: 237816, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14329083263874054, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0543791055679321, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8438612222671509, 'test/num_examples': 10000, 'score': 79621.93251681328, 'total_duration': 82421.6057536602, 'accumulated_submission_time': 79621.93251681328, 'accumulated_eval_time': 2783.3465077877045, 'accumulated_logging_time': 7.8104212284088135}
I0301 04:08:00.782389 140089870997248 logging_writer.py:48] [237816] accumulated_eval_time=2783.346508, accumulated_logging_time=7.810421, accumulated_submission_time=79621.932517, global_step=237816, preemption_count=0, score=79621.932517, test/accuracy=0.627900, test/loss=1.843861, test/num_examples=10000, total_duration=82421.605754, train/accuracy=0.961735, train/loss=0.143291, validation/accuracy=0.756100, validation/loss=1.054379, validation/num_examples=50000
I0301 04:08:29.194248 140089879389952 logging_writer.py:48] [237900] global_step=237900, grad_norm=4.5602707862854, loss=0.5877423882484436
I0301 04:09:02.727133 140089870997248 logging_writer.py:48] [238000] global_step=238000, grad_norm=4.164583683013916, loss=0.6159554719924927
I0301 04:09:36.185328 140089879389952 logging_writer.py:48] [238100] global_step=238100, grad_norm=4.5744500160217285, loss=0.6227627396583557
I0301 04:10:09.657262 140089870997248 logging_writer.py:48] [238200] global_step=238200, grad_norm=4.534466743469238, loss=0.5449572205543518
I0301 04:10:43.139957 140089879389952 logging_writer.py:48] [238300] global_step=238300, grad_norm=4.228515625, loss=0.5103837847709656
I0301 04:11:16.560955 140089870997248 logging_writer.py:48] [238400] global_step=238400, grad_norm=4.214178085327148, loss=0.5921188592910767
I0301 04:11:50.058677 140089879389952 logging_writer.py:48] [238500] global_step=238500, grad_norm=4.498870849609375, loss=0.6348915696144104
I0301 04:12:23.527290 140089870997248 logging_writer.py:48] [238600] global_step=238600, grad_norm=4.51248836517334, loss=0.6789722442626953
I0301 04:12:56.955611 140089879389952 logging_writer.py:48] [238700] global_step=238700, grad_norm=4.774290084838867, loss=0.6055564880371094
I0301 04:13:30.405213 140089870997248 logging_writer.py:48] [238800] global_step=238800, grad_norm=5.027692794799805, loss=0.6003463268280029
I0301 04:14:03.849045 140089879389952 logging_writer.py:48] [238900] global_step=238900, grad_norm=4.3172760009765625, loss=0.6405912637710571
I0301 04:14:37.362046 140089870997248 logging_writer.py:48] [239000] global_step=239000, grad_norm=4.629632472991943, loss=0.6771107316017151
I0301 04:15:10.837551 140089879389952 logging_writer.py:48] [239100] global_step=239100, grad_norm=4.856864929199219, loss=0.6482879519462585
I0301 04:15:44.316535 140089870997248 logging_writer.py:48] [239200] global_step=239200, grad_norm=4.442300796508789, loss=0.6382612586021423
I0301 04:16:17.767650 140089879389952 logging_writer.py:48] [239300] global_step=239300, grad_norm=4.296552658081055, loss=0.5779584050178528
I0301 04:16:30.943881 140252611495744 spec.py:321] Evaluating on the training split.
I0301 04:16:37.107678 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 04:16:45.204687 140252611495744 spec.py:349] Evaluating on the test split.
I0301 04:16:47.492429 140252611495744 submission_runner.py:411] Time since start: 82948.37s, 	Step: 239341, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14744789898395538, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0538862943649292, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8431426286697388, 'test/num_examples': 10000, 'score': 80132.0266327858, 'total_duration': 82948.37228608131, 'accumulated_submission_time': 80132.0266327858, 'accumulated_eval_time': 2799.8947973251343, 'accumulated_logging_time': 7.878611326217651}
I0301 04:16:47.552139 140089837426432 logging_writer.py:48] [239341] accumulated_eval_time=2799.894797, accumulated_logging_time=7.878611, accumulated_submission_time=80132.026633, global_step=239341, preemption_count=0, score=80132.026633, test/accuracy=0.627100, test/loss=1.843143, test/num_examples=10000, total_duration=82948.372286, train/accuracy=0.960778, train/loss=0.147448, validation/accuracy=0.755600, validation/loss=1.053886, validation/num_examples=50000
I0301 04:17:07.634088 140089845819136 logging_writer.py:48] [239400] global_step=239400, grad_norm=4.519123077392578, loss=0.6077756881713867
I0301 04:17:41.061621 140089837426432 logging_writer.py:48] [239500] global_step=239500, grad_norm=4.317131042480469, loss=0.616284191608429
I0301 04:18:14.499235 140089845819136 logging_writer.py:48] [239600] global_step=239600, grad_norm=4.508172035217285, loss=0.6752520799636841
I0301 04:18:47.931852 140089837426432 logging_writer.py:48] [239700] global_step=239700, grad_norm=5.014122486114502, loss=0.6462634205818176
I0301 04:19:21.372451 140089845819136 logging_writer.py:48] [239800] global_step=239800, grad_norm=4.490553855895996, loss=0.585155725479126
I0301 04:19:54.801323 140089837426432 logging_writer.py:48] [239900] global_step=239900, grad_norm=4.418236255645752, loss=0.6036336421966553
I0301 04:20:28.253226 140089845819136 logging_writer.py:48] [240000] global_step=240000, grad_norm=4.633788585662842, loss=0.6777456998825073
I0301 04:21:01.792816 140089837426432 logging_writer.py:48] [240100] global_step=240100, grad_norm=4.047714710235596, loss=0.5211762189865112
I0301 04:21:35.231772 140089845819136 logging_writer.py:48] [240200] global_step=240200, grad_norm=5.4213714599609375, loss=0.690271258354187
I0301 04:22:08.669555 140089837426432 logging_writer.py:48] [240300] global_step=240300, grad_norm=4.196235656738281, loss=0.5372661352157593
I0301 04:22:42.132231 140089845819136 logging_writer.py:48] [240400] global_step=240400, grad_norm=4.763868808746338, loss=0.6242108345031738
I0301 04:23:15.564504 140089837426432 logging_writer.py:48] [240500] global_step=240500, grad_norm=4.589292049407959, loss=0.6537026762962341
I0301 04:23:49.030552 140089845819136 logging_writer.py:48] [240600] global_step=240600, grad_norm=4.03112268447876, loss=0.5253806114196777
I0301 04:24:22.469437 140089837426432 logging_writer.py:48] [240700] global_step=240700, grad_norm=4.766543865203857, loss=0.6752990484237671
I0301 04:24:55.928488 140089845819136 logging_writer.py:48] [240800] global_step=240800, grad_norm=4.564889907836914, loss=0.6164153814315796
I0301 04:25:17.793876 140252611495744 spec.py:321] Evaluating on the training split.
I0301 04:25:23.931730 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 04:25:32.039363 140252611495744 spec.py:349] Evaluating on the test split.
I0301 04:25:34.313940 140252611495744 submission_runner.py:411] Time since start: 83475.19s, 	Step: 240867, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.1430841088294983, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0552139282226562, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8439744710922241, 'test/num_examples': 10000, 'score': 80642.20257234573, 'total_duration': 83475.19399499893, 'accumulated_submission_time': 80642.20257234573, 'accumulated_eval_time': 2816.4147934913635, 'accumulated_logging_time': 7.948309659957886}
I0301 04:25:34.373765 140089552271104 logging_writer.py:48] [240867] accumulated_eval_time=2816.414793, accumulated_logging_time=7.948310, accumulated_submission_time=80642.202572, global_step=240867, preemption_count=0, score=80642.202572, test/accuracy=0.627900, test/loss=1.843974, test/num_examples=10000, total_duration=83475.193995, train/accuracy=0.961316, train/loss=0.143084, validation/accuracy=0.755740, validation/loss=1.055214, validation/num_examples=50000
I0301 04:25:45.748430 140089778697984 logging_writer.py:48] [240900] global_step=240900, grad_norm=4.461857318878174, loss=0.6093195080757141
I0301 04:26:19.185851 140089552271104 logging_writer.py:48] [241000] global_step=241000, grad_norm=4.239387512207031, loss=0.5712544918060303
I0301 04:26:52.769064 140089778697984 logging_writer.py:48] [241100] global_step=241100, grad_norm=4.914525032043457, loss=0.6140869855880737
I0301 04:27:26.219086 140089552271104 logging_writer.py:48] [241200] global_step=241200, grad_norm=5.30556583404541, loss=0.6419168710708618
I0301 04:27:59.638368 140089778697984 logging_writer.py:48] [241300] global_step=241300, grad_norm=4.3915276527404785, loss=0.5792843699455261
I0301 04:28:33.085952 140089552271104 logging_writer.py:48] [241400] global_step=241400, grad_norm=4.343364238739014, loss=0.6221343278884888
I0301 04:29:06.521222 140089778697984 logging_writer.py:48] [241500] global_step=241500, grad_norm=4.767142295837402, loss=0.6113235950469971
I0301 04:29:39.966185 140089552271104 logging_writer.py:48] [241600] global_step=241600, grad_norm=4.383199214935303, loss=0.626218318939209
I0301 04:30:13.421470 140089778697984 logging_writer.py:48] [241700] global_step=241700, grad_norm=4.701838493347168, loss=0.6319054961204529
I0301 04:30:46.882189 140089552271104 logging_writer.py:48] [241800] global_step=241800, grad_norm=4.4537739753723145, loss=0.6098140478134155
I0301 04:31:20.329749 140089778697984 logging_writer.py:48] [241900] global_step=241900, grad_norm=4.743865489959717, loss=0.6659842133522034
I0301 04:31:53.773337 140089552271104 logging_writer.py:48] [242000] global_step=242000, grad_norm=4.689532279968262, loss=0.5895145535469055
I0301 04:32:27.233327 140089778697984 logging_writer.py:48] [242100] global_step=242100, grad_norm=4.735660076141357, loss=0.533872127532959
I0301 04:33:00.774343 140089552271104 logging_writer.py:48] [242200] global_step=242200, grad_norm=4.650125980377197, loss=0.6863194704055786
I0301 04:33:34.185574 140089778697984 logging_writer.py:48] [242300] global_step=242300, grad_norm=4.349796295166016, loss=0.6236076354980469
I0301 04:34:04.432702 140252611495744 spec.py:321] Evaluating on the training split.
I0301 04:34:10.532992 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 04:34:18.636584 140252611495744 spec.py:349] Evaluating on the test split.
I0301 04:34:20.973634 140252611495744 submission_runner.py:411] Time since start: 84001.85s, 	Step: 242392, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14512062072753906, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.054344654083252, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8432506322860718, 'test/num_examples': 10000, 'score': 81152.19447517395, 'total_duration': 84001.85370469093, 'accumulated_submission_time': 81152.19447517395, 'accumulated_eval_time': 2832.955675125122, 'accumulated_logging_time': 8.019760608673096}
I0301 04:34:21.034581 140089845819136 logging_writer.py:48] [242392] accumulated_eval_time=2832.955675, accumulated_logging_time=8.019761, accumulated_submission_time=81152.194475, global_step=242392, preemption_count=0, score=81152.194475, test/accuracy=0.628100, test/loss=1.843251, test/num_examples=10000, total_duration=84001.853705, train/accuracy=0.961535, train/loss=0.145121, validation/accuracy=0.756140, validation/loss=1.054345, validation/num_examples=50000
I0301 04:34:24.049924 140089870997248 logging_writer.py:48] [242400] global_step=242400, grad_norm=4.291670322418213, loss=0.5625854134559631
I0301 04:34:57.481789 140089845819136 logging_writer.py:48] [242500] global_step=242500, grad_norm=4.995604038238525, loss=0.644318163394928
I0301 04:35:30.939136 140089870997248 logging_writer.py:48] [242600] global_step=242600, grad_norm=4.233730792999268, loss=0.5917307138442993
I0301 04:36:04.373268 140089845819136 logging_writer.py:48] [242700] global_step=242700, grad_norm=4.563957691192627, loss=0.6432896852493286
I0301 04:36:37.845099 140089870997248 logging_writer.py:48] [242800] global_step=242800, grad_norm=4.762828826904297, loss=0.6487053036689758
I0301 04:37:11.286048 140089845819136 logging_writer.py:48] [242900] global_step=242900, grad_norm=4.646289825439453, loss=0.6383128762245178
I0301 04:37:44.719171 140089870997248 logging_writer.py:48] [243000] global_step=243000, grad_norm=4.649956703186035, loss=0.6400498747825623
I0301 04:38:18.155536 140089845819136 logging_writer.py:48] [243100] global_step=243100, grad_norm=4.433269023895264, loss=0.5500028729438782
I0301 04:38:51.699386 140089870997248 logging_writer.py:48] [243200] global_step=243200, grad_norm=4.629419326782227, loss=0.6341188549995422
I0301 04:39:25.144407 140089845819136 logging_writer.py:48] [243300] global_step=243300, grad_norm=4.42294454574585, loss=0.5667155385017395
I0301 04:39:58.571498 140089870997248 logging_writer.py:48] [243400] global_step=243400, grad_norm=4.339428901672363, loss=0.6043840646743774
I0301 04:40:32.005154 140089845819136 logging_writer.py:48] [243500] global_step=243500, grad_norm=4.382504940032959, loss=0.6169247031211853
I0301 04:41:05.441188 140089870997248 logging_writer.py:48] [243600] global_step=243600, grad_norm=4.259820938110352, loss=0.6249678134918213
I0301 04:41:38.893730 140089845819136 logging_writer.py:48] [243700] global_step=243700, grad_norm=4.784806251525879, loss=0.678104043006897
I0301 04:42:12.343550 140089870997248 logging_writer.py:48] [243800] global_step=243800, grad_norm=4.812136173248291, loss=0.6537008285522461
I0301 04:42:45.775979 140089845819136 logging_writer.py:48] [243900] global_step=243900, grad_norm=4.32453727722168, loss=0.5432938933372498
I0301 04:42:51.267158 140252611495744 spec.py:321] Evaluating on the training split.
I0301 04:42:57.414424 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 04:43:05.719281 140252611495744 spec.py:349] Evaluating on the test split.
I0301 04:43:08.012389 140252611495744 submission_runner.py:411] Time since start: 84528.89s, 	Step: 243918, 	{'train/accuracy': 0.9616748690605164, 'train/loss': 0.1430436074733734, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.0540276765823364, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8438608646392822, 'test/num_examples': 10000, 'score': 81662.36057519913, 'total_duration': 84528.89245724678, 'accumulated_submission_time': 81662.36057519913, 'accumulated_eval_time': 2849.7008497714996, 'accumulated_logging_time': 8.091844081878662}
I0301 04:43:08.069308 140089837426432 logging_writer.py:48] [243918] accumulated_eval_time=2849.700850, accumulated_logging_time=8.091844, accumulated_submission_time=81662.360575, global_step=243918, preemption_count=0, score=81662.360575, test/accuracy=0.627300, test/loss=1.843861, test/num_examples=10000, total_duration=84528.892457, train/accuracy=0.961675, train/loss=0.143044, validation/accuracy=0.756180, validation/loss=1.054028, validation/num_examples=50000
I0301 04:43:35.833165 140089845819136 logging_writer.py:48] [244000] global_step=244000, grad_norm=4.468274116516113, loss=0.6905896663665771
I0301 04:44:09.301927 140089837426432 logging_writer.py:48] [244100] global_step=244100, grad_norm=4.297966003417969, loss=0.6025112867355347
I0301 04:44:42.725458 140089845819136 logging_writer.py:48] [244200] global_step=244200, grad_norm=4.813726902008057, loss=0.6073422431945801
I0301 04:45:16.257908 140089837426432 logging_writer.py:48] [244300] global_step=244300, grad_norm=4.38964319229126, loss=0.5734940767288208
I0301 04:45:49.721193 140089845819136 logging_writer.py:48] [244400] global_step=244400, grad_norm=4.678243160247803, loss=0.6772509813308716
I0301 04:46:23.183449 140089837426432 logging_writer.py:48] [244500] global_step=244500, grad_norm=4.8334760665893555, loss=0.5733551979064941
I0301 04:46:56.603718 140089845819136 logging_writer.py:48] [244600] global_step=244600, grad_norm=4.285180568695068, loss=0.6272423267364502
I0301 04:47:30.063755 140089837426432 logging_writer.py:48] [244700] global_step=244700, grad_norm=4.148632526397705, loss=0.630339503288269
I0301 04:48:03.498383 140089845819136 logging_writer.py:48] [244800] global_step=244800, grad_norm=4.7103705406188965, loss=0.6882134675979614
I0301 04:48:36.974873 140089837426432 logging_writer.py:48] [244900] global_step=244900, grad_norm=4.741955757141113, loss=0.6528952717781067
I0301 04:49:10.408631 140089845819136 logging_writer.py:48] [245000] global_step=245000, grad_norm=4.472424507141113, loss=0.6848822236061096
I0301 04:49:43.854009 140089837426432 logging_writer.py:48] [245100] global_step=245100, grad_norm=4.602787494659424, loss=0.6108880639076233
I0301 04:50:17.324185 140089845819136 logging_writer.py:48] [245200] global_step=245200, grad_norm=4.371920108795166, loss=0.6125410795211792
I0301 04:50:50.749637 140089837426432 logging_writer.py:48] [245300] global_step=245300, grad_norm=4.731192111968994, loss=0.605879008769989
I0301 04:51:24.260850 140089845819136 logging_writer.py:48] [245400] global_step=245400, grad_norm=4.387360572814941, loss=0.5732646584510803
I0301 04:51:38.101110 140252611495744 spec.py:321] Evaluating on the training split.
I0301 04:51:44.354315 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 04:51:52.479866 140252611495744 spec.py:349] Evaluating on the test split.
I0301 04:51:54.784815 140252611495744 submission_runner.py:411] Time since start: 85055.66s, 	Step: 245443, 	{'train/accuracy': 0.9590441584587097, 'train/loss': 0.1470848023891449, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0540157556533813, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8433830738067627, 'test/num_examples': 10000, 'score': 82172.32745957375, 'total_duration': 85055.66488361359, 'accumulated_submission_time': 82172.32745957375, 'accumulated_eval_time': 2866.384510755539, 'accumulated_logging_time': 8.159352779388428}
I0301 04:51:54.844632 140089778697984 logging_writer.py:48] [245443] accumulated_eval_time=2866.384511, accumulated_logging_time=8.159353, accumulated_submission_time=82172.327460, global_step=245443, preemption_count=0, score=82172.327460, test/accuracy=0.627600, test/loss=1.843383, test/num_examples=10000, total_duration=85055.664884, train/accuracy=0.959044, train/loss=0.147085, validation/accuracy=0.755900, validation/loss=1.054016, validation/num_examples=50000
I0301 04:52:14.219071 140089837426432 logging_writer.py:48] [245500] global_step=245500, grad_norm=4.717665672302246, loss=0.6227059960365295
I0301 04:52:47.665753 140089778697984 logging_writer.py:48] [245600] global_step=245600, grad_norm=4.659875392913818, loss=0.6314025521278381
I0301 04:53:21.135768 140089837426432 logging_writer.py:48] [245700] global_step=245700, grad_norm=4.787137508392334, loss=0.6253786683082581
I0301 04:53:54.570380 140089778697984 logging_writer.py:48] [245800] global_step=245800, grad_norm=4.669308185577393, loss=0.6404827237129211
I0301 04:54:28.064501 140089837426432 logging_writer.py:48] [245900] global_step=245900, grad_norm=4.192937850952148, loss=0.6252457499504089
I0301 04:55:01.562190 140089778697984 logging_writer.py:48] [246000] global_step=246000, grad_norm=4.675858020782471, loss=0.6213865280151367
I0301 04:55:35.002055 140089837426432 logging_writer.py:48] [246100] global_step=246100, grad_norm=4.449721813201904, loss=0.5599918365478516
I0301 04:56:08.433985 140089778697984 logging_writer.py:48] [246200] global_step=246200, grad_norm=4.38740348815918, loss=0.5372493863105774
I0301 04:56:41.951707 140089837426432 logging_writer.py:48] [246300] global_step=246300, grad_norm=5.38930082321167, loss=0.6725757122039795
I0301 04:57:15.496340 140089778697984 logging_writer.py:48] [246400] global_step=246400, grad_norm=5.707680702209473, loss=0.60187828540802
I0301 04:57:48.973619 140089837426432 logging_writer.py:48] [246500] global_step=246500, grad_norm=4.292515277862549, loss=0.5743616223335266
I0301 04:58:22.425419 140089778697984 logging_writer.py:48] [246600] global_step=246600, grad_norm=4.3153581619262695, loss=0.5684337615966797
I0301 04:58:55.886199 140089837426432 logging_writer.py:48] [246700] global_step=246700, grad_norm=4.5782060623168945, loss=0.6155109405517578
I0301 04:59:29.346743 140089778697984 logging_writer.py:48] [246800] global_step=246800, grad_norm=4.092447280883789, loss=0.5985606908798218
I0301 05:00:02.793823 140089837426432 logging_writer.py:48] [246900] global_step=246900, grad_norm=4.378364086151123, loss=0.6108043193817139
I0301 05:00:25.011963 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:00:31.216390 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:00:39.314502 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:00:41.601190 140252611495744 submission_runner.py:411] Time since start: 85582.48s, 	Step: 246968, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1470487266778946, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.053966760635376, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.843010663986206, 'test/num_examples': 10000, 'score': 82682.42796611786, 'total_duration': 85582.4812579155, 'accumulated_submission_time': 82682.42796611786, 'accumulated_eval_time': 2882.9736964702606, 'accumulated_logging_time': 8.230422973632812}
I0301 05:00:41.661820 140089845819136 logging_writer.py:48] [246968] accumulated_eval_time=2882.973696, accumulated_logging_time=8.230423, accumulated_submission_time=82682.427966, global_step=246968, preemption_count=0, score=82682.427966, test/accuracy=0.627000, test/loss=1.843011, test/num_examples=10000, total_duration=85582.481258, train/accuracy=0.960479, train/loss=0.147049, validation/accuracy=0.756060, validation/loss=1.053967, validation/num_examples=50000
I0301 05:00:52.716155 140089854211840 logging_writer.py:48] [247000] global_step=247000, grad_norm=4.639815330505371, loss=0.6629964113235474
I0301 05:01:26.178816 140089845819136 logging_writer.py:48] [247100] global_step=247100, grad_norm=4.227229595184326, loss=0.6219788789749146
I0301 05:01:59.634279 140089854211840 logging_writer.py:48] [247200] global_step=247200, grad_norm=4.302164077758789, loss=0.6391885280609131
I0301 05:02:33.119458 140089845819136 logging_writer.py:48] [247300] global_step=247300, grad_norm=4.179650783538818, loss=0.624326229095459
I0301 05:03:06.753654 140089854211840 logging_writer.py:48] [247400] global_step=247400, grad_norm=4.355087757110596, loss=0.6436467170715332
I0301 05:03:40.174721 140089845819136 logging_writer.py:48] [247500] global_step=247500, grad_norm=4.412695407867432, loss=0.5983673334121704
I0301 05:04:13.649371 140089854211840 logging_writer.py:48] [247600] global_step=247600, grad_norm=4.573875904083252, loss=0.5966097712516785
I0301 05:04:47.093176 140089845819136 logging_writer.py:48] [247700] global_step=247700, grad_norm=4.0607991218566895, loss=0.5652076005935669
I0301 05:05:20.555082 140089854211840 logging_writer.py:48] [247800] global_step=247800, grad_norm=4.34813928604126, loss=0.5872327089309692
I0301 05:05:54.034264 140089845819136 logging_writer.py:48] [247900] global_step=247900, grad_norm=4.6123881340026855, loss=0.6282780170440674
I0301 05:06:27.496484 140089854211840 logging_writer.py:48] [248000] global_step=248000, grad_norm=4.510287761688232, loss=0.6077296733856201
I0301 05:07:00.945196 140089845819136 logging_writer.py:48] [248100] global_step=248100, grad_norm=4.426468849182129, loss=0.5983655452728271
I0301 05:07:34.436538 140089854211840 logging_writer.py:48] [248200] global_step=248200, grad_norm=4.414047718048096, loss=0.6318969130516052
I0301 05:08:07.866200 140089845819136 logging_writer.py:48] [248300] global_step=248300, grad_norm=4.851970195770264, loss=0.6864544749259949
I0301 05:08:41.326192 140089854211840 logging_writer.py:48] [248400] global_step=248400, grad_norm=4.212606430053711, loss=0.5495396852493286
I0301 05:09:11.658936 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:09:17.728407 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:09:25.881998 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:09:28.134567 140252611495744 submission_runner.py:411] Time since start: 86109.01s, 	Step: 248492, 	{'train/accuracy': 0.9622329473495483, 'train/loss': 0.14388325810432434, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0548738241195679, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8438180685043335, 'test/num_examples': 10000, 'score': 83192.36039113998, 'total_duration': 86109.01463675499, 'accumulated_submission_time': 83192.36039113998, 'accumulated_eval_time': 2899.4492712020874, 'accumulated_logging_time': 8.30149245262146}
I0301 05:09:28.197644 140089770305280 logging_writer.py:48] [248492] accumulated_eval_time=2899.449271, accumulated_logging_time=8.301492, accumulated_submission_time=83192.360391, global_step=248492, preemption_count=0, score=83192.360391, test/accuracy=0.627700, test/loss=1.843818, test/num_examples=10000, total_duration=86109.014637, train/accuracy=0.962233, train/loss=0.143883, validation/accuracy=0.755680, validation/loss=1.054874, validation/num_examples=50000
I0301 05:09:31.207162 140089778697984 logging_writer.py:48] [248500] global_step=248500, grad_norm=4.595107078552246, loss=0.668765664100647
I0301 05:10:04.646597 140089770305280 logging_writer.py:48] [248600] global_step=248600, grad_norm=4.545738220214844, loss=0.6666975617408752
I0301 05:10:38.107155 140089778697984 logging_writer.py:48] [248700] global_step=248700, grad_norm=4.892996311187744, loss=0.5844264626502991
I0301 05:11:11.543896 140089770305280 logging_writer.py:48] [248800] global_step=248800, grad_norm=4.467103004455566, loss=0.6180850863456726
I0301 05:11:44.996635 140089778697984 logging_writer.py:48] [248900] global_step=248900, grad_norm=4.321588516235352, loss=0.5817189812660217
I0301 05:12:18.425972 140089770305280 logging_writer.py:48] [249000] global_step=249000, grad_norm=5.059454917907715, loss=0.6390097737312317
I0301 05:12:51.884983 140089778697984 logging_writer.py:48] [249100] global_step=249100, grad_norm=4.770821571350098, loss=0.634290874004364
I0301 05:13:25.322463 140089770305280 logging_writer.py:48] [249200] global_step=249200, grad_norm=4.249172210693359, loss=0.6064015626907349
I0301 05:13:58.746453 140089778697984 logging_writer.py:48] [249300] global_step=249300, grad_norm=4.5210280418396, loss=0.619708776473999
I0301 05:14:32.177577 140089770305280 logging_writer.py:48] [249400] global_step=249400, grad_norm=4.612269878387451, loss=0.6410406231880188
I0301 05:15:05.722302 140089778697984 logging_writer.py:48] [249500] global_step=249500, grad_norm=4.133993148803711, loss=0.5517880320549011
I0301 05:15:39.170996 140089770305280 logging_writer.py:48] [249600] global_step=249600, grad_norm=4.89656400680542, loss=0.646701991558075
I0301 05:16:12.654356 140089778697984 logging_writer.py:48] [249700] global_step=249700, grad_norm=4.622910022735596, loss=0.6573200225830078
I0301 05:16:46.096995 140089770305280 logging_writer.py:48] [249800] global_step=249800, grad_norm=4.593780040740967, loss=0.6830329895019531
I0301 05:17:19.540000 140089778697984 logging_writer.py:48] [249900] global_step=249900, grad_norm=5.052596092224121, loss=0.6017124056816101
I0301 05:17:52.974049 140089770305280 logging_writer.py:48] [250000] global_step=250000, grad_norm=4.350203990936279, loss=0.6053522229194641
I0301 05:17:58.143544 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:18:04.509619 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:18:12.522302 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:18:14.815271 140252611495744 submission_runner.py:411] Time since start: 86635.70s, 	Step: 250017, 	{'train/accuracy': 0.9619140625, 'train/loss': 0.14314717054367065, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0536892414093018, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8433725833892822, 'test/num_examples': 10000, 'score': 83702.24043631554, 'total_duration': 86635.69530034065, 'accumulated_submission_time': 83702.24043631554, 'accumulated_eval_time': 2916.1209042072296, 'accumulated_logging_time': 8.375693798065186}
I0301 05:18:14.875496 140089770305280 logging_writer.py:48] [250017] accumulated_eval_time=2916.120904, accumulated_logging_time=8.375694, accumulated_submission_time=83702.240436, global_step=250017, preemption_count=0, score=83702.240436, test/accuracy=0.627400, test/loss=1.843373, test/num_examples=10000, total_duration=86635.695300, train/accuracy=0.961914, train/loss=0.143147, validation/accuracy=0.756060, validation/loss=1.053689, validation/num_examples=50000
I0301 05:18:42.987823 140089862604544 logging_writer.py:48] [250100] global_step=250100, grad_norm=4.677309989929199, loss=0.6691211462020874
I0301 05:19:16.463563 140089770305280 logging_writer.py:48] [250200] global_step=250200, grad_norm=4.850959777832031, loss=0.6914492249488831
I0301 05:19:49.954523 140089862604544 logging_writer.py:48] [250300] global_step=250300, grad_norm=4.676981449127197, loss=0.6218276619911194
I0301 05:20:23.362478 140089770305280 logging_writer.py:48] [250400] global_step=250400, grad_norm=5.161322593688965, loss=0.7682034373283386
I0301 05:20:56.836773 140089862604544 logging_writer.py:48] [250500] global_step=250500, grad_norm=4.583392143249512, loss=0.6757736206054688
I0301 05:21:30.356485 140089770305280 logging_writer.py:48] [250600] global_step=250600, grad_norm=4.148593425750732, loss=0.587228000164032
I0301 05:22:03.826701 140089862604544 logging_writer.py:48] [250700] global_step=250700, grad_norm=4.341269493103027, loss=0.6530157327651978
I0301 05:22:37.244786 140089770305280 logging_writer.py:48] [250800] global_step=250800, grad_norm=4.822073936462402, loss=0.6072915196418762
I0301 05:23:10.721764 140089862604544 logging_writer.py:48] [250900] global_step=250900, grad_norm=4.401766300201416, loss=0.7156610488891602
I0301 05:23:44.178756 140089770305280 logging_writer.py:48] [251000] global_step=251000, grad_norm=4.534702777862549, loss=0.6109407544136047
I0301 05:24:17.629618 140089862604544 logging_writer.py:48] [251100] global_step=251100, grad_norm=4.667694568634033, loss=0.5961642265319824
I0301 05:24:51.089224 140089770305280 logging_writer.py:48] [251200] global_step=251200, grad_norm=4.557042598724365, loss=0.6788337826728821
I0301 05:25:24.529649 140089862604544 logging_writer.py:48] [251300] global_step=251300, grad_norm=4.326821804046631, loss=0.6366982460021973
I0301 05:25:57.994206 140089770305280 logging_writer.py:48] [251400] global_step=251400, grad_norm=4.098300457000732, loss=0.5668089985847473
I0301 05:26:31.442534 140089862604544 logging_writer.py:48] [251500] global_step=251500, grad_norm=4.951910972595215, loss=0.717246949672699
I0301 05:26:44.970935 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:26:51.122795 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:26:59.263433 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:27:01.554099 140252611495744 submission_runner.py:411] Time since start: 87162.43s, 	Step: 251542, 	{'train/accuracy': 0.9607182741165161, 'train/loss': 0.1478823870420456, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0535964965820312, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8413687944412231, 'test/num_examples': 10000, 'score': 84212.27120828629, 'total_duration': 87162.43416666985, 'accumulated_submission_time': 84212.27120828629, 'accumulated_eval_time': 2932.704018354416, 'accumulated_logging_time': 8.445951461791992}
I0301 05:27:01.642074 140089552271104 logging_writer.py:48] [251542] accumulated_eval_time=2932.704018, accumulated_logging_time=8.445951, accumulated_submission_time=84212.271208, global_step=251542, preemption_count=0, score=84212.271208, test/accuracy=0.628100, test/loss=1.841369, test/num_examples=10000, total_duration=87162.434167, train/accuracy=0.960718, train/loss=0.147882, validation/accuracy=0.755920, validation/loss=1.053596, validation/num_examples=50000
I0301 05:27:21.470840 140089837426432 logging_writer.py:48] [251600] global_step=251600, grad_norm=4.648866653442383, loss=0.6181233525276184
I0301 05:27:54.930131 140089552271104 logging_writer.py:48] [251700] global_step=251700, grad_norm=4.794132232666016, loss=0.6106972694396973
I0301 05:28:28.362686 140089837426432 logging_writer.py:48] [251800] global_step=251800, grad_norm=4.286207675933838, loss=0.585007905960083
I0301 05:29:01.820199 140089552271104 logging_writer.py:48] [251900] global_step=251900, grad_norm=4.638601779937744, loss=0.6843267679214478
I0301 05:29:35.266577 140089837426432 logging_writer.py:48] [252000] global_step=252000, grad_norm=4.63426399230957, loss=0.6614260673522949
I0301 05:30:08.721504 140089552271104 logging_writer.py:48] [252100] global_step=252100, grad_norm=4.812514305114746, loss=0.5675539374351501
I0301 05:30:42.156241 140089837426432 logging_writer.py:48] [252200] global_step=252200, grad_norm=4.279262542724609, loss=0.5911170840263367
I0301 05:31:15.602452 140089552271104 logging_writer.py:48] [252300] global_step=252300, grad_norm=4.193281173706055, loss=0.6331487894058228
I0301 05:31:49.059661 140089837426432 logging_writer.py:48] [252400] global_step=252400, grad_norm=4.806395053863525, loss=0.659173309803009
I0301 05:32:22.499973 140089552271104 logging_writer.py:48] [252500] global_step=252500, grad_norm=4.3399553298950195, loss=0.6186026930809021
I0301 05:32:55.923805 140089837426432 logging_writer.py:48] [252600] global_step=252600, grad_norm=4.6787285804748535, loss=0.6698257923126221
I0301 05:33:29.478948 140089552271104 logging_writer.py:48] [252700] global_step=252700, grad_norm=4.851438999176025, loss=0.6876217722892761
I0301 05:34:02.938919 140089837426432 logging_writer.py:48] [252800] global_step=252800, grad_norm=4.496291637420654, loss=0.6269800066947937
I0301 05:34:36.374530 140089552271104 logging_writer.py:48] [252900] global_step=252900, grad_norm=4.4063825607299805, loss=0.6244127154350281
I0301 05:35:09.822222 140089837426432 logging_writer.py:48] [253000] global_step=253000, grad_norm=4.129119396209717, loss=0.5452157258987427
I0301 05:35:31.711085 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:35:37.837502 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:35:45.950958 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:35:48.249157 140252611495744 submission_runner.py:411] Time since start: 87689.13s, 	Step: 253067, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.14749857783317566, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0543384552001953, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.843252420425415, 'test/num_examples': 10000, 'score': 84722.27003097534, 'total_duration': 87689.12922596931, 'accumulated_submission_time': 84722.27003097534, 'accumulated_eval_time': 2949.2420732975006, 'accumulated_logging_time': 8.548237800598145}
I0301 05:35:48.306824 140089837426432 logging_writer.py:48] [253067] accumulated_eval_time=2949.242073, accumulated_logging_time=8.548238, accumulated_submission_time=84722.270031, global_step=253067, preemption_count=0, score=84722.270031, test/accuracy=0.626700, test/loss=1.843252, test/num_examples=10000, total_duration=87689.129226, train/accuracy=0.959901, train/loss=0.147499, validation/accuracy=0.755660, validation/loss=1.054338, validation/num_examples=50000
I0301 05:35:59.701270 140089862604544 logging_writer.py:48] [253100] global_step=253100, grad_norm=5.146722793579102, loss=0.6998376250267029
I0301 05:36:33.133041 140089837426432 logging_writer.py:48] [253200] global_step=253200, grad_norm=4.6908440589904785, loss=0.6647651195526123
I0301 05:37:06.616727 140089862604544 logging_writer.py:48] [253300] global_step=253300, grad_norm=5.181614875793457, loss=0.6793432235717773
I0301 05:37:40.064557 140089837426432 logging_writer.py:48] [253400] global_step=253400, grad_norm=4.82492208480835, loss=0.6135913729667664
I0301 05:38:13.516654 140089862604544 logging_writer.py:48] [253500] global_step=253500, grad_norm=4.386058330535889, loss=0.6131132245063782
I0301 05:38:47.022591 140089837426432 logging_writer.py:48] [253600] global_step=253600, grad_norm=4.315649509429932, loss=0.5788524150848389
I0301 05:39:20.558603 140089862604544 logging_writer.py:48] [253700] global_step=253700, grad_norm=4.406954765319824, loss=0.6253116130828857
I0301 05:39:54.074055 140089837426432 logging_writer.py:48] [253800] global_step=253800, grad_norm=4.520232677459717, loss=0.6461950540542603
I0301 05:40:27.541499 140089862604544 logging_writer.py:48] [253900] global_step=253900, grad_norm=4.295170307159424, loss=0.6108188629150391
I0301 05:41:01.043147 140089837426432 logging_writer.py:48] [254000] global_step=254000, grad_norm=4.55456018447876, loss=0.5702727437019348
I0301 05:41:34.459976 140089862604544 logging_writer.py:48] [254100] global_step=254100, grad_norm=4.725276947021484, loss=0.5574451088905334
I0301 05:42:07.907778 140089837426432 logging_writer.py:48] [254200] global_step=254200, grad_norm=4.55352258682251, loss=0.6182871460914612
I0301 05:42:41.362964 140089862604544 logging_writer.py:48] [254300] global_step=254300, grad_norm=4.2486701011657715, loss=0.5783615708351135
I0301 05:43:14.787003 140089837426432 logging_writer.py:48] [254400] global_step=254400, grad_norm=4.584846496582031, loss=0.6150983572006226
I0301 05:43:48.246918 140089862604544 logging_writer.py:48] [254500] global_step=254500, grad_norm=4.573273658752441, loss=0.567359983921051
I0301 05:44:18.520494 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:44:24.729155 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:44:32.754344 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:44:35.127715 140252611495744 submission_runner.py:411] Time since start: 88216.01s, 	Step: 254592, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14667247235774994, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0540852546691895, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8423309326171875, 'test/num_examples': 10000, 'score': 85232.41868042946, 'total_duration': 88216.00778889656, 'accumulated_submission_time': 85232.41868042946, 'accumulated_eval_time': 2965.849251270294, 'accumulated_logging_time': 8.616014003753662}
I0301 05:44:35.186503 140089770305280 logging_writer.py:48] [254592] accumulated_eval_time=2965.849251, accumulated_logging_time=8.616014, accumulated_submission_time=85232.418680, global_step=254592, preemption_count=0, score=85232.418680, test/accuracy=0.627500, test/loss=1.842331, test/num_examples=10000, total_duration=88216.007789, train/accuracy=0.960320, train/loss=0.146672, validation/accuracy=0.755960, validation/loss=1.054085, validation/num_examples=50000
I0301 05:44:38.197013 140089845819136 logging_writer.py:48] [254600] global_step=254600, grad_norm=4.53519344329834, loss=0.6132352352142334
I0301 05:45:11.660505 140089770305280 logging_writer.py:48] [254700] global_step=254700, grad_norm=4.8528618812561035, loss=0.760570764541626
I0301 05:45:45.160080 140089845819136 logging_writer.py:48] [254800] global_step=254800, grad_norm=4.598171710968018, loss=0.6132605075836182
I0301 05:46:18.601533 140089770305280 logging_writer.py:48] [254900] global_step=254900, grad_norm=3.8999361991882324, loss=0.5178189277648926
I0301 05:46:52.064391 140089845819136 logging_writer.py:48] [255000] global_step=255000, grad_norm=4.5406317710876465, loss=0.6337215304374695
I0301 05:47:25.546348 140089770305280 logging_writer.py:48] [255100] global_step=255100, grad_norm=4.373575210571289, loss=0.655665397644043
I0301 05:47:58.991453 140089845819136 logging_writer.py:48] [255200] global_step=255200, grad_norm=4.808417320251465, loss=0.6761451363563538
I0301 05:48:32.445736 140089770305280 logging_writer.py:48] [255300] global_step=255300, grad_norm=4.992587566375732, loss=0.6299564838409424
I0301 05:49:05.877039 140089845819136 logging_writer.py:48] [255400] global_step=255400, grad_norm=4.745208263397217, loss=0.6577286124229431
I0301 05:49:39.341314 140089770305280 logging_writer.py:48] [255500] global_step=255500, grad_norm=4.916206359863281, loss=0.7044063806533813
I0301 05:50:12.772196 140089845819136 logging_writer.py:48] [255600] global_step=255600, grad_norm=4.428699016571045, loss=0.5819108486175537
I0301 05:50:46.222999 140089770305280 logging_writer.py:48] [255700] global_step=255700, grad_norm=4.6652512550354, loss=0.683782696723938
I0301 05:51:19.658454 140089845819136 logging_writer.py:48] [255800] global_step=255800, grad_norm=4.781039237976074, loss=0.6084865927696228
I0301 05:51:53.170283 140089770305280 logging_writer.py:48] [255900] global_step=255900, grad_norm=4.595754623413086, loss=0.6273932456970215
I0301 05:52:26.625813 140089845819136 logging_writer.py:48] [256000] global_step=256000, grad_norm=4.516228675842285, loss=0.6630572080612183
I0301 05:53:00.045230 140089770305280 logging_writer.py:48] [256100] global_step=256100, grad_norm=4.8754754066467285, loss=0.6792006492614746
I0301 05:53:05.197503 140252611495744 spec.py:321] Evaluating on the training split.
I0301 05:53:11.277375 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 05:53:19.466578 140252611495744 spec.py:349] Evaluating on the test split.
I0301 05:53:21.783551 140252611495744 submission_runner.py:411] Time since start: 88742.66s, 	Step: 256117, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.1451389640569687, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0543256998062134, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8429205417633057, 'test/num_examples': 10000, 'score': 85742.36588978767, 'total_duration': 88742.66361165047, 'accumulated_submission_time': 85742.36588978767, 'accumulated_eval_time': 2982.4352350234985, 'accumulated_logging_time': 8.684788465499878}
I0301 05:53:21.839606 140089770305280 logging_writer.py:48] [256117] accumulated_eval_time=2982.435235, accumulated_logging_time=8.684788, accumulated_submission_time=85742.365890, global_step=256117, preemption_count=0, score=85742.365890, test/accuracy=0.627300, test/loss=1.842921, test/num_examples=10000, total_duration=88742.663612, train/accuracy=0.961057, train/loss=0.145139, validation/accuracy=0.756080, validation/loss=1.054326, validation/num_examples=50000
I0301 05:53:49.957888 140089778697984 logging_writer.py:48] [256200] global_step=256200, grad_norm=4.7373480796813965, loss=0.5809171199798584
I0301 05:54:23.451830 140089770305280 logging_writer.py:48] [256300] global_step=256300, grad_norm=4.409929275512695, loss=0.6449691653251648
I0301 05:54:56.899519 140089778697984 logging_writer.py:48] [256400] global_step=256400, grad_norm=4.048384189605713, loss=0.5863363742828369
I0301 05:55:30.330268 140089770305280 logging_writer.py:48] [256500] global_step=256500, grad_norm=5.535199165344238, loss=0.6343272924423218
I0301 05:56:03.805276 140089778697984 logging_writer.py:48] [256600] global_step=256600, grad_norm=4.270618438720703, loss=0.5756607055664062
I0301 05:56:37.250702 140089770305280 logging_writer.py:48] [256700] global_step=256700, grad_norm=4.694575786590576, loss=0.6814278960227966
I0301 05:57:10.699770 140089778697984 logging_writer.py:48] [256800] global_step=256800, grad_norm=4.881394386291504, loss=0.7155439853668213
I0301 05:57:44.259249 140089770305280 logging_writer.py:48] [256900] global_step=256900, grad_norm=4.359059810638428, loss=0.6674951314926147
I0301 05:58:17.720314 140089778697984 logging_writer.py:48] [257000] global_step=257000, grad_norm=4.597543239593506, loss=0.615532636642456
I0301 05:58:51.167484 140089770305280 logging_writer.py:48] [257100] global_step=257100, grad_norm=4.475930690765381, loss=0.6277398467063904
I0301 05:59:24.623275 140089778697984 logging_writer.py:48] [257200] global_step=257200, grad_norm=4.565339088439941, loss=0.6012657880783081
I0301 05:59:58.092468 140089770305280 logging_writer.py:48] [257300] global_step=257300, grad_norm=4.577997207641602, loss=0.6751992106437683
I0301 06:00:31.516378 140089778697984 logging_writer.py:48] [257400] global_step=257400, grad_norm=4.68052864074707, loss=0.6420847177505493
I0301 06:01:04.974174 140089770305280 logging_writer.py:48] [257500] global_step=257500, grad_norm=4.312691688537598, loss=0.6247260570526123
I0301 06:01:38.425786 140089778697984 logging_writer.py:48] [257600] global_step=257600, grad_norm=5.115940093994141, loss=0.644607663154602
I0301 06:01:51.932805 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:01:58.064093 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:02:06.346806 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:02:08.618518 140252611495744 submission_runner.py:411] Time since start: 89269.50s, 	Step: 257642, 	{'train/accuracy': 0.9624322056770325, 'train/loss': 0.14316606521606445, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0529989004135132, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8423770666122437, 'test/num_examples': 10000, 'score': 86252.392973423, 'total_duration': 89269.49858641624, 'accumulated_submission_time': 86252.392973423, 'accumulated_eval_time': 2999.1208930015564, 'accumulated_logging_time': 8.751543760299683}
I0301 06:02:08.681543 140089845819136 logging_writer.py:48] [257642] accumulated_eval_time=2999.120893, accumulated_logging_time=8.751544, accumulated_submission_time=86252.392973, global_step=257642, preemption_count=0, score=86252.392973, test/accuracy=0.626800, test/loss=1.842377, test/num_examples=10000, total_duration=89269.498586, train/accuracy=0.962432, train/loss=0.143166, validation/accuracy=0.755960, validation/loss=1.052999, validation/num_examples=50000
I0301 06:02:28.435364 140089854211840 logging_writer.py:48] [257700] global_step=257700, grad_norm=4.328707695007324, loss=0.6204251050949097
I0301 06:03:01.868106 140089845819136 logging_writer.py:48] [257800] global_step=257800, grad_norm=4.851945400238037, loss=0.6553568243980408
I0301 06:03:35.323374 140089854211840 logging_writer.py:48] [257900] global_step=257900, grad_norm=3.9716527462005615, loss=0.555087685585022
I0301 06:04:08.962592 140089845819136 logging_writer.py:48] [258000] global_step=258000, grad_norm=4.827024936676025, loss=0.6024304628372192
I0301 06:04:42.436507 140089854211840 logging_writer.py:48] [258100] global_step=258100, grad_norm=4.248442649841309, loss=0.5523914694786072
I0301 06:05:15.864279 140089845819136 logging_writer.py:48] [258200] global_step=258200, grad_norm=4.658870697021484, loss=0.6569184064865112
I0301 06:05:49.338994 140089854211840 logging_writer.py:48] [258300] global_step=258300, grad_norm=4.75720739364624, loss=0.5940659046173096
I0301 06:06:22.758646 140089845819136 logging_writer.py:48] [258400] global_step=258400, grad_norm=4.6960930824279785, loss=0.5713176131248474
I0301 06:06:56.258175 140089854211840 logging_writer.py:48] [258500] global_step=258500, grad_norm=4.297969818115234, loss=0.6257722973823547
I0301 06:07:29.740917 140089845819136 logging_writer.py:48] [258600] global_step=258600, grad_norm=4.607335090637207, loss=0.6705811619758606
I0301 06:08:03.192780 140089854211840 logging_writer.py:48] [258700] global_step=258700, grad_norm=5.702674388885498, loss=0.6166026592254639
I0301 06:08:36.638168 140089845819136 logging_writer.py:48] [258800] global_step=258800, grad_norm=5.121572494506836, loss=0.6593610048294067
I0301 06:09:10.091377 140089854211840 logging_writer.py:48] [258900] global_step=258900, grad_norm=4.997791290283203, loss=0.6475361585617065
I0301 06:09:43.700183 140089845819136 logging_writer.py:48] [259000] global_step=259000, grad_norm=4.553768157958984, loss=0.6331416964530945
I0301 06:10:17.220410 140089854211840 logging_writer.py:48] [259100] global_step=259100, grad_norm=5.01369047164917, loss=0.6648268699645996
I0301 06:10:38.763559 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:10:44.866784 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:10:52.993338 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:10:55.300527 140252611495744 submission_runner.py:411] Time since start: 89796.18s, 	Step: 259166, 	{'train/accuracy': 0.959402859210968, 'train/loss': 0.14947785437107086, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0539062023162842, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8413981199264526, 'test/num_examples': 10000, 'score': 86762.40819621086, 'total_duration': 89796.18056607246, 'accumulated_submission_time': 86762.40819621086, 'accumulated_eval_time': 3015.657775402069, 'accumulated_logging_time': 8.82495379447937}
I0301 06:10:55.360790 140089770305280 logging_writer.py:48] [259166] accumulated_eval_time=3015.657775, accumulated_logging_time=8.824954, accumulated_submission_time=86762.408196, global_step=259166, preemption_count=0, score=86762.408196, test/accuracy=0.626700, test/loss=1.841398, test/num_examples=10000, total_duration=89796.180566, train/accuracy=0.959403, train/loss=0.149478, validation/accuracy=0.755740, validation/loss=1.053906, validation/num_examples=50000
I0301 06:11:07.059965 140089837426432 logging_writer.py:48] [259200] global_step=259200, grad_norm=4.696762561798096, loss=0.6130214929580688
I0301 06:11:40.506616 140089770305280 logging_writer.py:48] [259300] global_step=259300, grad_norm=4.636855125427246, loss=0.6524681448936462
I0301 06:12:13.971627 140089837426432 logging_writer.py:48] [259400] global_step=259400, grad_norm=4.84918212890625, loss=0.7081995606422424
I0301 06:12:47.467662 140089770305280 logging_writer.py:48] [259500] global_step=259500, grad_norm=4.658605098724365, loss=0.6841762065887451
I0301 06:13:20.927044 140089837426432 logging_writer.py:48] [259600] global_step=259600, grad_norm=4.830275535583496, loss=0.7020896077156067
I0301 06:13:54.374768 140089770305280 logging_writer.py:48] [259700] global_step=259700, grad_norm=4.636103630065918, loss=0.6335726976394653
I0301 06:14:27.880186 140089837426432 logging_writer.py:48] [259800] global_step=259800, grad_norm=4.4885406494140625, loss=0.5512107610702515
I0301 06:15:01.330689 140089770305280 logging_writer.py:48] [259900] global_step=259900, grad_norm=4.150878429412842, loss=0.568788468837738
I0301 06:15:34.772175 140089837426432 logging_writer.py:48] [260000] global_step=260000, grad_norm=4.9252214431762695, loss=0.5971216559410095
I0301 06:16:08.363054 140089770305280 logging_writer.py:48] [260100] global_step=260100, grad_norm=4.3432087898254395, loss=0.6014323830604553
I0301 06:16:41.806213 140089837426432 logging_writer.py:48] [260200] global_step=260200, grad_norm=4.1859822273254395, loss=0.5402125120162964
I0301 06:17:15.302839 140089770305280 logging_writer.py:48] [260300] global_step=260300, grad_norm=4.6361613273620605, loss=0.5883280038833618
I0301 06:17:48.768557 140089837426432 logging_writer.py:48] [260400] global_step=260400, grad_norm=4.520956516265869, loss=0.5795658826828003
I0301 06:18:22.207377 140089770305280 logging_writer.py:48] [260500] global_step=260500, grad_norm=4.775784969329834, loss=0.6268812417984009
I0301 06:18:55.678926 140089837426432 logging_writer.py:48] [260600] global_step=260600, grad_norm=4.881979465484619, loss=0.637832760810852
I0301 06:19:25.605649 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:19:31.759248 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:19:40.010474 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:19:42.318585 140252611495744 submission_runner.py:411] Time since start: 90323.20s, 	Step: 260691, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14541095495224, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0525975227355957, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8419866561889648, 'test/num_examples': 10000, 'score': 87272.5882089138, 'total_duration': 90323.19861745834, 'accumulated_submission_time': 87272.5882089138, 'accumulated_eval_time': 3032.370623588562, 'accumulated_logging_time': 8.895149946212769}
I0301 06:19:42.384298 140089778697984 logging_writer.py:48] [260691] accumulated_eval_time=3032.370624, accumulated_logging_time=8.895150, accumulated_submission_time=87272.588209, global_step=260691, preemption_count=0, score=87272.588209, test/accuracy=0.627200, test/loss=1.841987, test/num_examples=10000, total_duration=90323.198617, train/accuracy=0.961276, train/loss=0.145411, validation/accuracy=0.755900, validation/loss=1.052598, validation/num_examples=50000
I0301 06:19:45.745127 140089845819136 logging_writer.py:48] [260700] global_step=260700, grad_norm=4.374734878540039, loss=0.5947080254554749
I0301 06:20:19.160261 140089778697984 logging_writer.py:48] [260800] global_step=260800, grad_norm=4.3874077796936035, loss=0.5704020857810974
I0301 06:20:52.632486 140089845819136 logging_writer.py:48] [260900] global_step=260900, grad_norm=4.362648963928223, loss=0.6005902886390686
I0301 06:21:26.141444 140089778697984 logging_writer.py:48] [261000] global_step=261000, grad_norm=4.75984525680542, loss=0.618427574634552
I0301 06:21:59.650157 140089845819136 logging_writer.py:48] [261100] global_step=261100, grad_norm=4.471502304077148, loss=0.6290310621261597
I0301 06:22:33.137405 140089778697984 logging_writer.py:48] [261200] global_step=261200, grad_norm=5.0188117027282715, loss=0.6237446069717407
I0301 06:23:06.564164 140089845819136 logging_writer.py:48] [261300] global_step=261300, grad_norm=4.933685302734375, loss=0.6460399627685547
I0301 06:23:40.064221 140089778697984 logging_writer.py:48] [261400] global_step=261400, grad_norm=4.319524765014648, loss=0.5783606767654419
I0301 06:24:13.540211 140089845819136 logging_writer.py:48] [261500] global_step=261500, grad_norm=4.573075294494629, loss=0.651624858379364
I0301 06:24:47.010126 140089778697984 logging_writer.py:48] [261600] global_step=261600, grad_norm=5.009364604949951, loss=0.6200587153434753
I0301 06:25:20.453891 140089845819136 logging_writer.py:48] [261700] global_step=261700, grad_norm=4.496903896331787, loss=0.6681187748908997
I0301 06:25:53.932652 140089778697984 logging_writer.py:48] [261800] global_step=261800, grad_norm=4.579639434814453, loss=0.6240504384040833
I0301 06:26:27.360226 140089845819136 logging_writer.py:48] [261900] global_step=261900, grad_norm=4.595139503479004, loss=0.7020761370658875
I0301 06:27:00.844642 140089778697984 logging_writer.py:48] [262000] global_step=262000, grad_norm=4.622803211212158, loss=0.5964271426200867
I0301 06:27:34.328348 140089845819136 logging_writer.py:48] [262100] global_step=262100, grad_norm=4.1834635734558105, loss=0.6160683631896973
I0301 06:28:07.912536 140089778697984 logging_writer.py:48] [262200] global_step=262200, grad_norm=4.511313438415527, loss=0.6166847348213196
I0301 06:28:12.403367 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:28:18.620323 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:28:26.662849 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:28:28.967780 140252611495744 submission_runner.py:411] Time since start: 90849.85s, 	Step: 262215, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14662615954875946, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0539066791534424, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.8433120250701904, 'test/num_examples': 10000, 'score': 87782.54205465317, 'total_duration': 90849.84784388542, 'accumulated_submission_time': 87782.54205465317, 'accumulated_eval_time': 3048.934982776642, 'accumulated_logging_time': 8.971031188964844}
I0301 06:28:29.035680 140089837426432 logging_writer.py:48] [262215] accumulated_eval_time=3048.934983, accumulated_logging_time=8.971031, accumulated_submission_time=87782.542055, global_step=262215, preemption_count=0, score=87782.542055, test/accuracy=0.626900, test/loss=1.843312, test/num_examples=10000, total_duration=90849.847844, train/accuracy=0.961137, train/loss=0.146626, validation/accuracy=0.755940, validation/loss=1.053907, validation/num_examples=50000
I0301 06:28:57.789187 140089862604544 logging_writer.py:48] [262300] global_step=262300, grad_norm=5.029445171356201, loss=0.6868281960487366
I0301 06:29:31.258237 140089837426432 logging_writer.py:48] [262400] global_step=262400, grad_norm=4.858937740325928, loss=0.6392681002616882
I0301 06:30:04.715479 140089862604544 logging_writer.py:48] [262500] global_step=262500, grad_norm=5.093132972717285, loss=0.6702114343643188
I0301 06:30:38.183680 140089837426432 logging_writer.py:48] [262600] global_step=262600, grad_norm=4.495449542999268, loss=0.607939600944519
I0301 06:31:11.684821 140089862604544 logging_writer.py:48] [262700] global_step=262700, grad_norm=4.646636962890625, loss=0.6361865401268005
I0301 06:31:45.140051 140089837426432 logging_writer.py:48] [262800] global_step=262800, grad_norm=4.908451557159424, loss=0.6905468702316284
I0301 06:32:18.587021 140089862604544 logging_writer.py:48] [262900] global_step=262900, grad_norm=4.676977634429932, loss=0.5801197290420532
I0301 06:32:52.040933 140089837426432 logging_writer.py:48] [263000] global_step=263000, grad_norm=4.249321937561035, loss=0.5737511515617371
I0301 06:33:25.498191 140089862604544 logging_writer.py:48] [263100] global_step=263100, grad_norm=4.6497626304626465, loss=0.6068191528320312
I0301 06:33:59.068540 140089837426432 logging_writer.py:48] [263200] global_step=263200, grad_norm=5.04474401473999, loss=0.649448037147522
I0301 06:34:32.522991 140089862604544 logging_writer.py:48] [263300] global_step=263300, grad_norm=5.017054557800293, loss=0.6148402094841003
I0301 06:35:05.995746 140089837426432 logging_writer.py:48] [263400] global_step=263400, grad_norm=4.814772605895996, loss=0.6389142274856567
I0301 06:35:39.432427 140089862604544 logging_writer.py:48] [263500] global_step=263500, grad_norm=5.098242282867432, loss=0.6101561188697815
I0301 06:36:12.924135 140089837426432 logging_writer.py:48] [263600] global_step=263600, grad_norm=4.613958835601807, loss=0.6155887842178345
I0301 06:36:46.407134 140089862604544 logging_writer.py:48] [263700] global_step=263700, grad_norm=4.86186408996582, loss=0.5981323719024658
I0301 06:36:59.262643 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:37:05.694661 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:37:13.702447 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:37:15.984976 140252611495744 submission_runner.py:411] Time since start: 91376.86s, 	Step: 263740, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14679908752441406, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0533528327941895, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.843176245689392, 'test/num_examples': 10000, 'score': 88292.70301318169, 'total_duration': 91376.86492967606, 'accumulated_submission_time': 88292.70301318169, 'accumulated_eval_time': 3065.657157897949, 'accumulated_logging_time': 9.049619197845459}
I0301 06:37:16.044178 140089770305280 logging_writer.py:48] [263740] accumulated_eval_time=3065.657158, accumulated_logging_time=9.049619, accumulated_submission_time=88292.703013, global_step=263740, preemption_count=0, score=88292.703013, test/accuracy=0.627900, test/loss=1.843176, test/num_examples=10000, total_duration=91376.864930, train/accuracy=0.961515, train/loss=0.146799, validation/accuracy=0.755700, validation/loss=1.053353, validation/num_examples=50000
I0301 06:37:36.481029 140089778697984 logging_writer.py:48] [263800] global_step=263800, grad_norm=4.637753486633301, loss=0.5870787501335144
I0301 06:38:09.902285 140089770305280 logging_writer.py:48] [263900] global_step=263900, grad_norm=4.465353488922119, loss=0.6038122177124023
I0301 06:38:43.353286 140089778697984 logging_writer.py:48] [264000] global_step=264000, grad_norm=4.597858428955078, loss=0.6387857794761658
I0301 06:39:16.794722 140089770305280 logging_writer.py:48] [264100] global_step=264100, grad_norm=4.2922868728637695, loss=0.6299180388450623
I0301 06:39:50.244532 140089778697984 logging_writer.py:48] [264200] global_step=264200, grad_norm=4.648652076721191, loss=0.5792296528816223
I0301 06:40:23.790835 140089770305280 logging_writer.py:48] [264300] global_step=264300, grad_norm=4.518424034118652, loss=0.60811448097229
I0301 06:40:57.257934 140089778697984 logging_writer.py:48] [264400] global_step=264400, grad_norm=5.016361713409424, loss=0.636527955532074
I0301 06:41:30.702924 140089770305280 logging_writer.py:48] [264500] global_step=264500, grad_norm=4.604945182800293, loss=0.6779667139053345
I0301 06:42:04.144748 140089778697984 logging_writer.py:48] [264600] global_step=264600, grad_norm=4.829129695892334, loss=0.6316325664520264
I0301 06:42:37.615303 140089770305280 logging_writer.py:48] [264700] global_step=264700, grad_norm=4.535290718078613, loss=0.6109945178031921
I0301 06:43:11.048031 140089778697984 logging_writer.py:48] [264800] global_step=264800, grad_norm=4.715083599090576, loss=0.628722071647644
I0301 06:43:44.502458 140089770305280 logging_writer.py:48] [264900] global_step=264900, grad_norm=4.842270851135254, loss=0.6760926246643066
I0301 06:44:17.972036 140089778697984 logging_writer.py:48] [265000] global_step=265000, grad_norm=4.303180694580078, loss=0.5938588380813599
I0301 06:44:51.406265 140089770305280 logging_writer.py:48] [265100] global_step=265100, grad_norm=4.335013389587402, loss=0.6087961792945862
I0301 06:45:24.850961 140089778697984 logging_writer.py:48] [265200] global_step=265200, grad_norm=4.595827102661133, loss=0.6160715818405151
I0301 06:45:46.087072 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:45:52.230939 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:46:00.428302 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:46:02.724643 140252611495744 submission_runner.py:411] Time since start: 91903.60s, 	Step: 265265, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14750458300113678, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0530436038970947, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8413385152816772, 'test/num_examples': 10000, 'score': 88802.68064022064, 'total_duration': 91903.60470747948, 'accumulated_submission_time': 88802.68064022064, 'accumulated_eval_time': 3082.2946906089783, 'accumulated_logging_time': 9.11883282661438}
I0301 06:46:02.785908 140089862604544 logging_writer.py:48] [265265] accumulated_eval_time=3082.294691, accumulated_logging_time=9.118833, accumulated_submission_time=88802.680640, global_step=265265, preemption_count=0, score=88802.680640, test/accuracy=0.626800, test/loss=1.841339, test/num_examples=10000, total_duration=91903.604707, train/accuracy=0.960419, train/loss=0.147505, validation/accuracy=0.755840, validation/loss=1.053044, validation/num_examples=50000
I0301 06:46:14.913113 140089870997248 logging_writer.py:48] [265300] global_step=265300, grad_norm=4.856592178344727, loss=0.705316960811615
I0301 06:46:48.342794 140089862604544 logging_writer.py:48] [265400] global_step=265400, grad_norm=4.466981887817383, loss=0.553548276424408
I0301 06:47:21.787055 140089870997248 logging_writer.py:48] [265500] global_step=265500, grad_norm=4.571263313293457, loss=0.6124399900436401
I0301 06:47:55.207263 140089862604544 logging_writer.py:48] [265600] global_step=265600, grad_norm=4.454578876495361, loss=0.6335954070091248
I0301 06:48:28.639881 140089870997248 logging_writer.py:48] [265700] global_step=265700, grad_norm=4.63922643661499, loss=0.6154289245605469
I0301 06:49:02.081470 140089862604544 logging_writer.py:48] [265800] global_step=265800, grad_norm=4.823537349700928, loss=0.6174608469009399
I0301 06:49:35.523956 140089870997248 logging_writer.py:48] [265900] global_step=265900, grad_norm=4.513230323791504, loss=0.6262629628181458
I0301 06:50:08.965023 140089862604544 logging_writer.py:48] [266000] global_step=266000, grad_norm=4.447393417358398, loss=0.6733356714248657
I0301 06:50:42.401305 140089870997248 logging_writer.py:48] [266100] global_step=266100, grad_norm=4.1756815910339355, loss=0.6596649885177612
I0301 06:51:15.843396 140089862604544 logging_writer.py:48] [266200] global_step=266200, grad_norm=4.7685699462890625, loss=0.6041596531867981
I0301 06:51:49.269378 140089870997248 logging_writer.py:48] [266300] global_step=266300, grad_norm=4.95982027053833, loss=0.6651952862739563
I0301 06:52:22.785429 140089862604544 logging_writer.py:48] [266400] global_step=266400, grad_norm=5.10626220703125, loss=0.6377450823783875
I0301 06:52:56.224708 140089870997248 logging_writer.py:48] [266500] global_step=266500, grad_norm=4.690887928009033, loss=0.6492738127708435
I0301 06:53:29.660756 140089862604544 logging_writer.py:48] [266600] global_step=266600, grad_norm=4.408823490142822, loss=0.638822078704834
I0301 06:54:03.112977 140089870997248 logging_writer.py:48] [266700] global_step=266700, grad_norm=4.416319370269775, loss=0.5928323268890381
I0301 06:54:33.040742 140252611495744 spec.py:321] Evaluating on the training split.
I0301 06:54:39.171289 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 06:54:47.264821 140252611495744 spec.py:349] Evaluating on the test split.
I0301 06:54:49.540759 140252611495744 submission_runner.py:411] Time since start: 92430.42s, 	Step: 266791, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14366863667964935, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.053915023803711, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8437012434005737, 'test/num_examples': 10000, 'score': 89312.86989212036, 'total_duration': 92430.42082619667, 'accumulated_submission_time': 89312.86989212036, 'accumulated_eval_time': 3098.7946536540985, 'accumulated_logging_time': 9.190809488296509}
I0301 06:54:49.601278 140089770305280 logging_writer.py:48] [266791] accumulated_eval_time=3098.794654, accumulated_logging_time=9.190809, accumulated_submission_time=89312.869892, global_step=266791, preemption_count=0, score=89312.869892, test/accuracy=0.627100, test/loss=1.843701, test/num_examples=10000, total_duration=92430.420826, train/accuracy=0.961655, train/loss=0.143669, validation/accuracy=0.755600, validation/loss=1.053915, validation/num_examples=50000
I0301 06:54:52.951986 140089778697984 logging_writer.py:48] [266800] global_step=266800, grad_norm=4.2416205406188965, loss=0.6935220956802368
I0301 06:55:26.399773 140089770305280 logging_writer.py:48] [266900] global_step=266900, grad_norm=4.503047466278076, loss=0.6396267414093018
I0301 06:55:59.815900 140089778697984 logging_writer.py:48] [267000] global_step=267000, grad_norm=5.045853614807129, loss=0.6405588388442993
I0301 06:56:33.283724 140089770305280 logging_writer.py:48] [267100] global_step=267100, grad_norm=4.598094940185547, loss=0.6356340646743774
I0301 06:57:06.757660 140089778697984 logging_writer.py:48] [267200] global_step=267200, grad_norm=4.799362659454346, loss=0.6658857464790344
I0301 06:57:40.178763 140089770305280 logging_writer.py:48] [267300] global_step=267300, grad_norm=4.668628215789795, loss=0.5971295833587646
I0301 06:58:13.653318 140089778697984 logging_writer.py:48] [267400] global_step=267400, grad_norm=5.122609615325928, loss=0.5885473489761353
I0301 06:58:47.230988 140089770305280 logging_writer.py:48] [267500] global_step=267500, grad_norm=4.201323986053467, loss=0.6073397994041443
I0301 06:59:20.678557 140089778697984 logging_writer.py:48] [267600] global_step=267600, grad_norm=4.482912063598633, loss=0.5960901975631714
I0301 06:59:54.140901 140089770305280 logging_writer.py:48] [267700] global_step=267700, grad_norm=4.534901142120361, loss=0.6134405136108398
I0301 07:00:27.545370 140089778697984 logging_writer.py:48] [267800] global_step=267800, grad_norm=4.559458255767822, loss=0.5875434875488281
I0301 07:01:01.565220 140089770305280 logging_writer.py:48] [267900] global_step=267900, grad_norm=4.489826679229736, loss=0.693247377872467
I0301 07:01:35.003165 140089778697984 logging_writer.py:48] [268000] global_step=268000, grad_norm=4.703263759613037, loss=0.6559855341911316
I0301 07:02:08.424309 140089770305280 logging_writer.py:48] [268100] global_step=268100, grad_norm=4.9166460037231445, loss=0.6317693591117859
I0301 07:02:41.879260 140089778697984 logging_writer.py:48] [268200] global_step=268200, grad_norm=4.450624942779541, loss=0.5335462093353271
I0301 07:03:15.329006 140089770305280 logging_writer.py:48] [268300] global_step=268300, grad_norm=4.5040059089660645, loss=0.66274493932724
I0301 07:03:19.810942 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:03:25.903578 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:03:34.008693 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:03:36.296506 140252611495744 submission_runner.py:411] Time since start: 92957.18s, 	Step: 268315, 	{'train/accuracy': 0.9625318646430969, 'train/loss': 0.14640094339847565, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0538593530654907, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.842480182647705, 'test/num_examples': 10000, 'score': 89823.01347899437, 'total_duration': 92957.17655205727, 'accumulated_submission_time': 89823.01347899437, 'accumulated_eval_time': 3115.280136346817, 'accumulated_logging_time': 9.261037111282349}
I0301 07:03:36.380661 140089770305280 logging_writer.py:48] [268315] accumulated_eval_time=3115.280136, accumulated_logging_time=9.261037, accumulated_submission_time=89823.013479, global_step=268315, preemption_count=0, score=89823.013479, test/accuracy=0.628400, test/loss=1.842480, test/num_examples=10000, total_duration=92957.176552, train/accuracy=0.962532, train/loss=0.146401, validation/accuracy=0.755720, validation/loss=1.053859, validation/num_examples=50000
I0301 07:04:05.144987 140089854211840 logging_writer.py:48] [268400] global_step=268400, grad_norm=4.301080703735352, loss=0.5837239623069763
I0301 07:04:38.711406 140089770305280 logging_writer.py:48] [268500] global_step=268500, grad_norm=5.023233413696289, loss=0.6406843066215515
I0301 07:05:12.147313 140089854211840 logging_writer.py:48] [268600] global_step=268600, grad_norm=4.5071210861206055, loss=0.6354714632034302
I0301 07:05:45.594920 140089770305280 logging_writer.py:48] [268700] global_step=268700, grad_norm=4.348437786102295, loss=0.5742645859718323
I0301 07:06:19.043614 140089854211840 logging_writer.py:48] [268800] global_step=268800, grad_norm=4.898679256439209, loss=0.6136981248855591
I0301 07:06:52.486277 140089770305280 logging_writer.py:48] [268900] global_step=268900, grad_norm=4.4620842933654785, loss=0.6320288181304932
I0301 07:07:25.944794 140089854211840 logging_writer.py:48] [269000] global_step=269000, grad_norm=4.24644660949707, loss=0.6482027769088745
I0301 07:07:59.375766 140089770305280 logging_writer.py:48] [269100] global_step=269100, grad_norm=4.523979187011719, loss=0.6670839190483093
I0301 07:08:32.804769 140089854211840 logging_writer.py:48] [269200] global_step=269200, grad_norm=4.179233551025391, loss=0.6465274095535278
I0301 07:09:06.241795 140089770305280 logging_writer.py:48] [269300] global_step=269300, grad_norm=4.701980113983154, loss=0.6632152795791626
I0301 07:09:39.756547 140089854211840 logging_writer.py:48] [269400] global_step=269400, grad_norm=4.416545867919922, loss=0.584159255027771
I0301 07:10:13.173280 140089770305280 logging_writer.py:48] [269500] global_step=269500, grad_norm=4.03631067276001, loss=0.5415103435516357
I0301 07:10:46.701204 140089854211840 logging_writer.py:48] [269600] global_step=269600, grad_norm=4.827672004699707, loss=0.6162126064300537
I0301 07:11:20.165390 140089770305280 logging_writer.py:48] [269700] global_step=269700, grad_norm=4.438333988189697, loss=0.6079122424125671
I0301 07:11:53.612659 140089854211840 logging_writer.py:48] [269800] global_step=269800, grad_norm=4.554370403289795, loss=0.6493111848831177
I0301 07:12:06.469288 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:12:12.562119 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:12:20.734987 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:12:23.015941 140252611495744 submission_runner.py:411] Time since start: 93483.90s, 	Step: 269840, 	{'train/accuracy': 0.9630300998687744, 'train/loss': 0.1411333978176117, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0552451610565186, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8441766500473022, 'test/num_examples': 10000, 'score': 90333.03701162338, 'total_duration': 93483.89601063728, 'accumulated_submission_time': 90333.03701162338, 'accumulated_eval_time': 3131.8267362117767, 'accumulated_logging_time': 9.355196475982666}
I0301 07:12:23.080972 140089862604544 logging_writer.py:48] [269840] accumulated_eval_time=3131.826736, accumulated_logging_time=9.355196, accumulated_submission_time=90333.037012, global_step=269840, preemption_count=0, score=90333.037012, test/accuracy=0.627600, test/loss=1.844177, test/num_examples=10000, total_duration=93483.896011, train/accuracy=0.963030, train/loss=0.141133, validation/accuracy=0.755260, validation/loss=1.055245, validation/num_examples=50000
I0301 07:12:43.487347 140089870997248 logging_writer.py:48] [269900] global_step=269900, grad_norm=4.51786470413208, loss=0.5902343988418579
I0301 07:13:16.965170 140089862604544 logging_writer.py:48] [270000] global_step=270000, grad_norm=4.340419769287109, loss=0.576025664806366
I0301 07:13:50.410573 140089870997248 logging_writer.py:48] [270100] global_step=270100, grad_norm=4.670970916748047, loss=0.6907602548599243
I0301 07:14:23.856471 140089862604544 logging_writer.py:48] [270200] global_step=270200, grad_norm=4.2496771812438965, loss=0.5868785381317139
I0301 07:14:57.273643 140089870997248 logging_writer.py:48] [270300] global_step=270300, grad_norm=4.4901580810546875, loss=0.6073243021965027
I0301 07:15:30.739355 140089862604544 logging_writer.py:48] [270400] global_step=270400, grad_norm=5.157252311706543, loss=0.7105920314788818
I0301 07:16:04.170317 140089870997248 logging_writer.py:48] [270500] global_step=270500, grad_norm=4.484278202056885, loss=0.6206588745117188
I0301 07:16:37.708274 140089862604544 logging_writer.py:48] [270600] global_step=270600, grad_norm=5.024670124053955, loss=0.6458339095115662
I0301 07:17:11.150868 140089870997248 logging_writer.py:48] [270700] global_step=270700, grad_norm=4.995770454406738, loss=0.7168228030204773
I0301 07:17:44.564683 140089862604544 logging_writer.py:48] [270800] global_step=270800, grad_norm=4.711216926574707, loss=0.6232865452766418
I0301 07:18:18.004083 140089870997248 logging_writer.py:48] [270900] global_step=270900, grad_norm=4.452775955200195, loss=0.650743842124939
I0301 07:18:51.429614 140089862604544 logging_writer.py:48] [271000] global_step=271000, grad_norm=4.172337532043457, loss=0.5547651648521423
I0301 07:19:24.876206 140089870997248 logging_writer.py:48] [271100] global_step=271100, grad_norm=4.513071060180664, loss=0.6248825192451477
I0301 07:19:58.303537 140089862604544 logging_writer.py:48] [271200] global_step=271200, grad_norm=4.774861812591553, loss=0.663373589515686
I0301 07:20:31.747113 140089870997248 logging_writer.py:48] [271300] global_step=271300, grad_norm=5.004649639129639, loss=0.5815839767456055
I0301 07:20:53.280439 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:21:00.030276 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:21:08.368192 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:21:10.683063 140252611495744 submission_runner.py:411] Time since start: 94011.56s, 	Step: 271366, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14524425566196442, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.054051399230957, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.844045639038086, 'test/num_examples': 10000, 'score': 90843.17112255096, 'total_duration': 94011.56312346458, 'accumulated_submission_time': 90843.17112255096, 'accumulated_eval_time': 3149.2292981147766, 'accumulated_logging_time': 9.430007934570312}
I0301 07:21:10.751846 140089778697984 logging_writer.py:48] [271366] accumulated_eval_time=3149.229298, accumulated_logging_time=9.430008, accumulated_submission_time=90843.171123, global_step=271366, preemption_count=0, score=90843.171123, test/accuracy=0.626800, test/loss=1.844046, test/num_examples=10000, total_duration=94011.563123, train/accuracy=0.960180, train/loss=0.145244, validation/accuracy=0.756040, validation/loss=1.054051, validation/num_examples=50000
I0301 07:21:22.449392 140089837426432 logging_writer.py:48] [271400] global_step=271400, grad_norm=4.522914409637451, loss=0.633764922618866
I0301 07:21:55.885177 140089778697984 logging_writer.py:48] [271500] global_step=271500, grad_norm=4.540823459625244, loss=0.6312763690948486
I0301 07:22:29.299879 140089837426432 logging_writer.py:48] [271600] global_step=271600, grad_norm=4.62011194229126, loss=0.6223152875900269
I0301 07:23:02.808844 140089778697984 logging_writer.py:48] [271700] global_step=271700, grad_norm=4.450394630432129, loss=0.6227284669876099
I0301 07:23:36.247193 140089837426432 logging_writer.py:48] [271800] global_step=271800, grad_norm=4.77232027053833, loss=0.6177788376808167
I0301 07:24:09.685328 140089778697984 logging_writer.py:48] [271900] global_step=271900, grad_norm=4.832984447479248, loss=0.6276570558547974
I0301 07:24:43.130993 140089837426432 logging_writer.py:48] [272000] global_step=272000, grad_norm=4.699276447296143, loss=0.6286864280700684
I0301 07:25:16.579842 140089778697984 logging_writer.py:48] [272100] global_step=272100, grad_norm=4.0970001220703125, loss=0.581898033618927
I0301 07:25:50.039592 140089837426432 logging_writer.py:48] [272200] global_step=272200, grad_norm=4.536816596984863, loss=0.618666410446167
I0301 07:26:23.502082 140089778697984 logging_writer.py:48] [272300] global_step=272300, grad_norm=5.3670854568481445, loss=0.6914526224136353
I0301 07:26:56.945339 140089837426432 logging_writer.py:48] [272400] global_step=272400, grad_norm=4.982012748718262, loss=0.621006965637207
I0301 07:27:30.375861 140089778697984 logging_writer.py:48] [272500] global_step=272500, grad_norm=4.448211193084717, loss=0.6407877802848816
I0301 07:28:03.805135 140089837426432 logging_writer.py:48] [272600] global_step=272600, grad_norm=4.9181928634643555, loss=0.5811414122581482
I0301 07:28:37.354975 140089778697984 logging_writer.py:48] [272700] global_step=272700, grad_norm=4.499555587768555, loss=0.6832591891288757
I0301 07:29:10.759920 140089837426432 logging_writer.py:48] [272800] global_step=272800, grad_norm=4.392798900604248, loss=0.6880695819854736
I0301 07:29:40.999594 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:29:47.118751 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:29:55.247777 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:29:57.577588 140252611495744 submission_runner.py:411] Time since start: 94538.46s, 	Step: 272892, 	{'train/accuracy': 0.9593630433082581, 'train/loss': 0.14949363470077515, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0542312860488892, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8424327373504639, 'test/num_examples': 10000, 'score': 91353.35414123535, 'total_duration': 94538.45765447617, 'accumulated_submission_time': 91353.35414123535, 'accumulated_eval_time': 3165.8072340488434, 'accumulated_logging_time': 9.508738994598389}
I0301 07:29:57.644391 140089778697984 logging_writer.py:48] [272892] accumulated_eval_time=3165.807234, accumulated_logging_time=9.508739, accumulated_submission_time=91353.354141, global_step=272892, preemption_count=0, score=91353.354141, test/accuracy=0.627800, test/loss=1.842433, test/num_examples=10000, total_duration=94538.457654, train/accuracy=0.959363, train/loss=0.149494, validation/accuracy=0.755800, validation/loss=1.054231, validation/num_examples=50000
I0301 07:30:00.673063 140089862604544 logging_writer.py:48] [272900] global_step=272900, grad_norm=4.3183159828186035, loss=0.5816181898117065
I0301 07:30:34.155095 140089778697984 logging_writer.py:48] [273000] global_step=273000, grad_norm=5.061232089996338, loss=0.7076811194419861
I0301 07:31:07.586698 140089862604544 logging_writer.py:48] [273100] global_step=273100, grad_norm=4.794085502624512, loss=0.6053428649902344
I0301 07:31:41.024570 140089778697984 logging_writer.py:48] [273200] global_step=273200, grad_norm=4.249954700469971, loss=0.6081514358520508
I0301 07:32:14.475028 140089862604544 logging_writer.py:48] [273300] global_step=273300, grad_norm=4.320559501647949, loss=0.5781424045562744
I0301 07:32:47.923144 140089778697984 logging_writer.py:48] [273400] global_step=273400, grad_norm=4.641358852386475, loss=0.6367551684379578
I0301 07:33:21.352677 140089862604544 logging_writer.py:48] [273500] global_step=273500, grad_norm=4.347838401794434, loss=0.5761800408363342
I0301 07:33:54.790595 140089778697984 logging_writer.py:48] [273600] global_step=273600, grad_norm=4.271378993988037, loss=0.6041756272315979
I0301 07:34:28.222178 140089862604544 logging_writer.py:48] [273700] global_step=273700, grad_norm=5.050910949707031, loss=0.6812677979469299
I0301 07:35:01.725830 140089778697984 logging_writer.py:48] [273800] global_step=273800, grad_norm=4.72119665145874, loss=0.6302618384361267
I0301 07:35:35.185872 140089862604544 logging_writer.py:48] [273900] global_step=273900, grad_norm=4.654698848724365, loss=0.6279577612876892
I0301 07:36:08.665181 140089778697984 logging_writer.py:48] [274000] global_step=274000, grad_norm=4.653449535369873, loss=0.6829652190208435
I0301 07:36:42.086506 140089862604544 logging_writer.py:48] [274100] global_step=274100, grad_norm=4.71265983581543, loss=0.6575151681900024
I0301 07:37:15.541255 140089778697984 logging_writer.py:48] [274200] global_step=274200, grad_norm=4.573229789733887, loss=0.5835375189781189
I0301 07:37:48.983722 140089862604544 logging_writer.py:48] [274300] global_step=274300, grad_norm=4.670750617980957, loss=0.6296671628952026
I0301 07:38:22.416653 140089778697984 logging_writer.py:48] [274400] global_step=274400, grad_norm=4.221118927001953, loss=0.6311662197113037
I0301 07:38:27.906004 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:38:34.074108 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:38:42.186839 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:38:44.475500 140252611495744 submission_runner.py:411] Time since start: 95065.36s, 	Step: 274418, 	{'train/accuracy': 0.962312638759613, 'train/loss': 0.14119455218315125, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.053430438041687, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.8420876264572144, 'test/num_examples': 10000, 'score': 91863.55009460449, 'total_duration': 95065.35556936264, 'accumulated_submission_time': 91863.55009460449, 'accumulated_eval_time': 3182.3766729831696, 'accumulated_logging_time': 9.585581302642822}
I0301 07:38:44.542150 140089837426432 logging_writer.py:48] [274418] accumulated_eval_time=3182.376673, accumulated_logging_time=9.585581, accumulated_submission_time=91863.550095, global_step=274418, preemption_count=0, score=91863.550095, test/accuracy=0.626200, test/loss=1.842088, test/num_examples=10000, total_duration=95065.355569, train/accuracy=0.962313, train/loss=0.141195, validation/accuracy=0.755700, validation/loss=1.053430, validation/num_examples=50000
I0301 07:39:12.302264 140089845819136 logging_writer.py:48] [274500] global_step=274500, grad_norm=4.374876022338867, loss=0.6597177982330322
I0301 07:39:45.766441 140089837426432 logging_writer.py:48] [274600] global_step=274600, grad_norm=4.533325672149658, loss=0.6507046222686768
I0301 07:40:19.198686 140089845819136 logging_writer.py:48] [274700] global_step=274700, grad_norm=4.496820449829102, loss=0.5877999067306519
I0301 07:40:52.785356 140089837426432 logging_writer.py:48] [274800] global_step=274800, grad_norm=4.558701992034912, loss=0.6092636585235596
I0301 07:41:26.241091 140089845819136 logging_writer.py:48] [274900] global_step=274900, grad_norm=3.9128916263580322, loss=0.5244758129119873
I0301 07:41:59.749544 140089837426432 logging_writer.py:48] [275000] global_step=275000, grad_norm=4.2384867668151855, loss=0.5631741285324097
I0301 07:42:33.218371 140089845819136 logging_writer.py:48] [275100] global_step=275100, grad_norm=4.260791301727295, loss=0.5697718262672424
I0301 07:43:06.666768 140089837426432 logging_writer.py:48] [275200] global_step=275200, grad_norm=4.502502918243408, loss=0.5968595743179321
I0301 07:43:40.141134 140089845819136 logging_writer.py:48] [275300] global_step=275300, grad_norm=5.466413974761963, loss=0.7060226202011108
I0301 07:44:13.581347 140089837426432 logging_writer.py:48] [275400] global_step=275400, grad_norm=4.319066047668457, loss=0.5536965131759644
I0301 07:44:47.048733 140089845819136 logging_writer.py:48] [275500] global_step=275500, grad_norm=4.41266393661499, loss=0.6252657175064087
I0301 07:45:20.535661 140089837426432 logging_writer.py:48] [275600] global_step=275600, grad_norm=4.421685218811035, loss=0.628808319568634
I0301 07:45:53.962179 140089845819136 logging_writer.py:48] [275700] global_step=275700, grad_norm=4.187360763549805, loss=0.5894381403923035
I0301 07:46:27.435767 140089837426432 logging_writer.py:48] [275800] global_step=275800, grad_norm=4.429525375366211, loss=0.6121605634689331
I0301 07:47:00.992493 140089845819136 logging_writer.py:48] [275900] global_step=275900, grad_norm=4.86792516708374, loss=0.6685642600059509
I0301 07:47:14.529297 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:47:20.684198 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:47:28.770009 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:47:31.072108 140252611495744 submission_runner.py:411] Time since start: 95591.95s, 	Step: 275942, 	{'train/accuracy': 0.9617745280265808, 'train/loss': 0.14347991347312927, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.0534061193466187, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.8437176942825317, 'test/num_examples': 10000, 'score': 92373.46947193146, 'total_duration': 95591.95217609406, 'accumulated_submission_time': 92373.46947193146, 'accumulated_eval_time': 3198.9194293022156, 'accumulated_logging_time': 9.665200233459473}
I0301 07:47:31.135414 140089770305280 logging_writer.py:48] [275942] accumulated_eval_time=3198.919429, accumulated_logging_time=9.665200, accumulated_submission_time=92373.469472, global_step=275942, preemption_count=0, score=92373.469472, test/accuracy=0.626900, test/loss=1.843718, test/num_examples=10000, total_duration=95591.952176, train/accuracy=0.961775, train/loss=0.143480, validation/accuracy=0.755620, validation/loss=1.053406, validation/num_examples=50000
I0301 07:47:50.860056 140089778697984 logging_writer.py:48] [276000] global_step=276000, grad_norm=4.635538578033447, loss=0.6731014251708984
I0301 07:48:24.320255 140089770305280 logging_writer.py:48] [276100] global_step=276100, grad_norm=4.76269006729126, loss=0.6688319444656372
I0301 07:48:57.818507 140089778697984 logging_writer.py:48] [276200] global_step=276200, grad_norm=4.360813140869141, loss=0.5978492498397827
I0301 07:49:31.306979 140089770305280 logging_writer.py:48] [276300] global_step=276300, grad_norm=4.141225337982178, loss=0.574573814868927
I0301 07:50:04.776472 140089778697984 logging_writer.py:48] [276400] global_step=276400, grad_norm=4.5231523513793945, loss=0.5735787749290466
I0301 07:50:38.310997 140089770305280 logging_writer.py:48] [276500] global_step=276500, grad_norm=4.911972999572754, loss=0.6081810593605042
I0301 07:51:11.750596 140089778697984 logging_writer.py:48] [276600] global_step=276600, grad_norm=4.5167555809021, loss=0.6058934926986694
I0301 07:51:45.218713 140089770305280 logging_writer.py:48] [276700] global_step=276700, grad_norm=5.147902011871338, loss=0.642767608165741
I0301 07:52:18.661842 140089778697984 logging_writer.py:48] [276800] global_step=276800, grad_norm=4.399049758911133, loss=0.6281996965408325
I0301 07:52:52.095252 140089770305280 logging_writer.py:48] [276900] global_step=276900, grad_norm=5.332185745239258, loss=0.6290315389633179
I0301 07:53:25.748927 140089778697984 logging_writer.py:48] [277000] global_step=277000, grad_norm=4.8698039054870605, loss=0.7155187726020813
I0301 07:53:59.260545 140089770305280 logging_writer.py:48] [277100] global_step=277100, grad_norm=4.598864555358887, loss=0.5960589051246643
I0301 07:54:32.804130 140089778697984 logging_writer.py:48] [277200] global_step=277200, grad_norm=4.502392292022705, loss=0.7024741172790527
I0301 07:55:06.272611 140089770305280 logging_writer.py:48] [277300] global_step=277300, grad_norm=4.377042770385742, loss=0.5979240536689758
I0301 07:55:39.726985 140089778697984 logging_writer.py:48] [277400] global_step=277400, grad_norm=4.340221405029297, loss=0.5979610681533813
I0301 07:56:01.279316 140252611495744 spec.py:321] Evaluating on the training split.
I0301 07:56:07.659615 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 07:56:15.695882 140252611495744 spec.py:349] Evaluating on the test split.
I0301 07:56:18.017593 140252611495744 submission_runner.py:411] Time since start: 96118.90s, 	Step: 277466, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14486335217952728, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0545258522033691, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8438247442245483, 'test/num_examples': 10000, 'score': 92883.54826879501, 'total_duration': 96118.89766335487, 'accumulated_submission_time': 92883.54826879501, 'accumulated_eval_time': 3215.657660007477, 'accumulated_logging_time': 9.739282369613647}
I0301 07:56:18.084573 140089770305280 logging_writer.py:48] [277466] accumulated_eval_time=3215.657660, accumulated_logging_time=9.739282, accumulated_submission_time=92883.548269, global_step=277466, preemption_count=0, score=92883.548269, test/accuracy=0.627300, test/loss=1.843825, test/num_examples=10000, total_duration=96118.897663, train/accuracy=0.961934, train/loss=0.144863, validation/accuracy=0.756080, validation/loss=1.054526, validation/num_examples=50000
I0301 07:56:29.809298 140089837426432 logging_writer.py:48] [277500] global_step=277500, grad_norm=4.594496726989746, loss=0.6462448835372925
I0301 07:57:03.298182 140089770305280 logging_writer.py:48] [277600] global_step=277600, grad_norm=4.711406230926514, loss=0.6575813293457031
I0301 07:57:36.758880 140089837426432 logging_writer.py:48] [277700] global_step=277700, grad_norm=4.623167037963867, loss=0.624316394329071
I0301 07:58:10.200345 140089770305280 logging_writer.py:48] [277800] global_step=277800, grad_norm=4.439425468444824, loss=0.6216122508049011
I0301 07:58:43.757889 140089837426432 logging_writer.py:48] [277900] global_step=277900, grad_norm=4.204050540924072, loss=0.540702223777771
I0301 07:59:17.320506 140089770305280 logging_writer.py:48] [278000] global_step=278000, grad_norm=4.489657878875732, loss=0.6071246266365051
I0301 07:59:50.814913 140089837426432 logging_writer.py:48] [278100] global_step=278100, grad_norm=4.331253528594971, loss=0.5309782028198242
I0301 08:00:24.536103 140089770305280 logging_writer.py:48] [278200] global_step=278200, grad_norm=4.671268463134766, loss=0.65499347448349
I0301 08:00:57.983297 140089837426432 logging_writer.py:48] [278300] global_step=278300, grad_norm=4.499597072601318, loss=0.6432130336761475
I0301 08:01:31.479298 140089770305280 logging_writer.py:48] [278400] global_step=278400, grad_norm=4.629425048828125, loss=0.6334156394004822
I0301 08:02:05.017435 140089837426432 logging_writer.py:48] [278500] global_step=278500, grad_norm=4.3223958015441895, loss=0.6004925966262817
I0301 08:02:38.501556 140089770305280 logging_writer.py:48] [278600] global_step=278600, grad_norm=4.524858474731445, loss=0.5390569567680359
I0301 08:03:11.959615 140089837426432 logging_writer.py:48] [278700] global_step=278700, grad_norm=4.765097141265869, loss=0.6013467907905579
I0301 08:03:45.418763 140089770305280 logging_writer.py:48] [278800] global_step=278800, grad_norm=4.369495868682861, loss=0.6346734762191772
I0301 08:04:18.940008 140089837426432 logging_writer.py:48] [278900] global_step=278900, grad_norm=4.50481653213501, loss=0.7129174470901489
I0301 08:04:48.196574 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:04:54.311434 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:05:02.614927 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:05:04.883351 140252611495744 submission_runner.py:411] Time since start: 96645.76s, 	Step: 278989, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14711593091487885, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0548198223114014, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.8432543277740479, 'test/num_examples': 10000, 'score': 93393.59517908096, 'total_duration': 96645.7634203434, 'accumulated_submission_time': 93393.59517908096, 'accumulated_eval_time': 3232.344386816025, 'accumulated_logging_time': 9.817296981811523}
I0301 08:05:04.949945 140089770305280 logging_writer.py:48] [278989] accumulated_eval_time=3232.344387, accumulated_logging_time=9.817297, accumulated_submission_time=93393.595179, global_step=278989, preemption_count=0, score=93393.595179, test/accuracy=0.626900, test/loss=1.843254, test/num_examples=10000, total_duration=96645.763420, train/accuracy=0.960379, train/loss=0.147116, validation/accuracy=0.755800, validation/loss=1.054820, validation/num_examples=50000
I0301 08:05:09.108502 140089862604544 logging_writer.py:48] [279000] global_step=279000, grad_norm=5.239069938659668, loss=0.6088583469390869
I0301 08:05:42.574377 140089770305280 logging_writer.py:48] [279100] global_step=279100, grad_norm=4.482818603515625, loss=0.668982207775116
I0301 08:06:16.109897 140089862604544 logging_writer.py:48] [279200] global_step=279200, grad_norm=4.0039286613464355, loss=0.5185772180557251
I0301 08:06:49.622936 140089770305280 logging_writer.py:48] [279300] global_step=279300, grad_norm=4.900326251983643, loss=0.6434202194213867
I0301 08:07:23.154577 140089862604544 logging_writer.py:48] [279400] global_step=279400, grad_norm=4.577628135681152, loss=0.6486161351203918
I0301 08:07:56.696939 140089770305280 logging_writer.py:48] [279500] global_step=279500, grad_norm=4.5505475997924805, loss=0.6287358403205872
I0301 08:08:30.191314 140089862604544 logging_writer.py:48] [279600] global_step=279600, grad_norm=4.187447547912598, loss=0.5970309972763062
I0301 08:09:03.708091 140089770305280 logging_writer.py:48] [279700] global_step=279700, grad_norm=5.1968488693237305, loss=0.6841411590576172
I0301 08:09:37.226992 140089862604544 logging_writer.py:48] [279800] global_step=279800, grad_norm=4.2966718673706055, loss=0.5894656777381897
I0301 08:10:10.686429 140089770305280 logging_writer.py:48] [279900] global_step=279900, grad_norm=4.758572578430176, loss=0.640323281288147
I0301 08:10:44.132734 140089862604544 logging_writer.py:48] [280000] global_step=280000, grad_norm=4.706943035125732, loss=0.6268221735954285
I0301 08:11:17.750858 140089770305280 logging_writer.py:48] [280100] global_step=280100, grad_norm=4.570343494415283, loss=0.5890758037567139
I0301 08:11:51.196311 140089862604544 logging_writer.py:48] [280200] global_step=280200, grad_norm=4.355831146240234, loss=0.5600162148475647
I0301 08:12:24.686297 140089770305280 logging_writer.py:48] [280300] global_step=280300, grad_norm=4.861774921417236, loss=0.6611340045928955
I0301 08:12:58.183275 140089862604544 logging_writer.py:48] [280400] global_step=280400, grad_norm=4.40702486038208, loss=0.6272264719009399
I0301 08:13:31.766690 140089770305280 logging_writer.py:48] [280500] global_step=280500, grad_norm=4.130057334899902, loss=0.6389898657798767
I0301 08:13:34.919953 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:13:40.988072 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:13:49.125118 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:13:51.430172 140252611495744 submission_runner.py:411] Time since start: 97172.31s, 	Step: 280511, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14689503610134125, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0539928674697876, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.842954397201538, 'test/num_examples': 10000, 'score': 93903.50009322166, 'total_duration': 97172.31008005142, 'accumulated_submission_time': 93903.50009322166, 'accumulated_eval_time': 3248.8543951511383, 'accumulated_logging_time': 9.893784284591675}
I0301 08:13:51.533212 140089837426432 logging_writer.py:48] [280511] accumulated_eval_time=3248.854395, accumulated_logging_time=9.893784, accumulated_submission_time=93903.500093, global_step=280511, preemption_count=0, score=93903.500093, test/accuracy=0.627200, test/loss=1.842954, test/num_examples=10000, total_duration=97172.310080, train/accuracy=0.960320, train/loss=0.146895, validation/accuracy=0.755840, validation/loss=1.053993, validation/num_examples=50000
I0301 08:14:21.687781 140089845819136 logging_writer.py:48] [280600] global_step=280600, grad_norm=4.818512439727783, loss=0.6838178634643555
I0301 08:14:55.180976 140089837426432 logging_writer.py:48] [280700] global_step=280700, grad_norm=4.477458953857422, loss=0.580054521560669
I0301 08:15:28.689328 140089845819136 logging_writer.py:48] [280800] global_step=280800, grad_norm=4.484512805938721, loss=0.6617921590805054
I0301 08:16:02.104312 140089837426432 logging_writer.py:48] [280900] global_step=280900, grad_norm=4.285031318664551, loss=0.5984236598014832
I0301 08:16:35.592382 140089845819136 logging_writer.py:48] [281000] global_step=281000, grad_norm=4.871011257171631, loss=0.6207898259162903
I0301 08:17:09.071306 140089837426432 logging_writer.py:48] [281100] global_step=281100, grad_norm=4.745604991912842, loss=0.6532172560691833
I0301 08:17:42.792767 140089845819136 logging_writer.py:48] [281200] global_step=281200, grad_norm=4.411531925201416, loss=0.5884886980056763
I0301 08:18:16.255245 140089837426432 logging_writer.py:48] [281300] global_step=281300, grad_norm=4.622533798217773, loss=0.7273749709129333
I0301 08:18:49.685364 140089845819136 logging_writer.py:48] [281400] global_step=281400, grad_norm=4.8195414543151855, loss=0.5921920537948608
I0301 08:19:23.169633 140089837426432 logging_writer.py:48] [281500] global_step=281500, grad_norm=4.524960517883301, loss=0.6454803943634033
I0301 08:19:56.685737 140089845819136 logging_writer.py:48] [281600] global_step=281600, grad_norm=4.908253192901611, loss=0.6882851719856262
I0301 08:20:30.184313 140089837426432 logging_writer.py:48] [281700] global_step=281700, grad_norm=4.720685005187988, loss=0.6125397682189941
I0301 08:21:03.652970 140089845819136 logging_writer.py:48] [281800] global_step=281800, grad_norm=4.7784647941589355, loss=0.6771710515022278
I0301 08:21:37.102410 140089837426432 logging_writer.py:48] [281900] global_step=281900, grad_norm=4.281705856323242, loss=0.579247236251831
I0301 08:22:10.582216 140089845819136 logging_writer.py:48] [282000] global_step=282000, grad_norm=4.52877950668335, loss=0.6261256337165833
I0301 08:22:21.755596 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:22:27.842917 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:22:35.952454 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:22:38.264800 140252611495744 submission_runner.py:411] Time since start: 97699.14s, 	Step: 282035, 	{'train/accuracy': 0.9624720811843872, 'train/loss': 0.1407591849565506, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.053431510925293, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.8427528142929077, 'test/num_examples': 10000, 'score': 94413.65333604813, 'total_duration': 97699.14486837387, 'accumulated_submission_time': 94413.65333604813, 'accumulated_eval_time': 3265.3635454177856, 'accumulated_logging_time': 10.01119351387024}
I0301 08:22:38.331702 140089778697984 logging_writer.py:48] [282035] accumulated_eval_time=3265.363545, accumulated_logging_time=10.011194, accumulated_submission_time=94413.653336, global_step=282035, preemption_count=0, score=94413.653336, test/accuracy=0.626200, test/loss=1.842753, test/num_examples=10000, total_duration=97699.144868, train/accuracy=0.962472, train/loss=0.140759, validation/accuracy=0.755940, validation/loss=1.053432, validation/num_examples=50000
I0301 08:23:00.394511 140089854211840 logging_writer.py:48] [282100] global_step=282100, grad_norm=4.708756923675537, loss=0.6164996027946472
I0301 08:23:33.966000 140089778697984 logging_writer.py:48] [282200] global_step=282200, grad_norm=4.635396480560303, loss=0.6470255851745605
I0301 08:24:07.424140 140089854211840 logging_writer.py:48] [282300] global_step=282300, grad_norm=4.761958122253418, loss=0.6189844608306885
I0301 08:24:40.937611 140089778697984 logging_writer.py:48] [282400] global_step=282400, grad_norm=4.257838249206543, loss=0.5421371459960938
I0301 08:25:14.440989 140089854211840 logging_writer.py:48] [282500] global_step=282500, grad_norm=4.465925693511963, loss=0.6254645586013794
I0301 08:25:47.972152 140089778697984 logging_writer.py:48] [282600] global_step=282600, grad_norm=4.767085552215576, loss=0.5844319462776184
I0301 08:26:21.489041 140089854211840 logging_writer.py:48] [282700] global_step=282700, grad_norm=4.48956823348999, loss=0.6024512648582458
I0301 08:26:54.937627 140089778697984 logging_writer.py:48] [282800] global_step=282800, grad_norm=5.1965203285217285, loss=0.6103352308273315
I0301 08:27:28.422456 140089854211840 logging_writer.py:48] [282900] global_step=282900, grad_norm=4.718620300292969, loss=0.5790843367576599
I0301 08:28:01.995594 140089778697984 logging_writer.py:48] [283000] global_step=283000, grad_norm=4.34693717956543, loss=0.6083391904830933
I0301 08:28:35.447825 140089854211840 logging_writer.py:48] [283100] global_step=283100, grad_norm=4.612461566925049, loss=0.6649526357650757
I0301 08:29:08.911958 140089778697984 logging_writer.py:48] [283200] global_step=283200, grad_norm=4.94635534286499, loss=0.6991822719573975
I0301 08:29:42.453489 140089854211840 logging_writer.py:48] [283300] global_step=283300, grad_norm=4.5930938720703125, loss=0.5817352533340454
I0301 08:30:15.991866 140089778697984 logging_writer.py:48] [283400] global_step=283400, grad_norm=4.174962043762207, loss=0.5514656901359558
I0301 08:30:49.481838 140089854211840 logging_writer.py:48] [283500] global_step=283500, grad_norm=4.120153427124023, loss=0.5868521332740784
I0301 08:31:08.403588 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:31:14.527281 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:31:22.564630 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:31:24.865733 140252611495744 submission_runner.py:411] Time since start: 98225.75s, 	Step: 283558, 	{'train/accuracy': 0.9611965417861938, 'train/loss': 0.14650842547416687, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0535814762115479, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8434096574783325, 'test/num_examples': 10000, 'score': 94923.66025543213, 'total_duration': 98225.74566698074, 'accumulated_submission_time': 94923.66025543213, 'accumulated_eval_time': 3281.825499534607, 'accumulated_logging_time': 10.088145017623901}
I0301 08:31:24.933208 140089845819136 logging_writer.py:48] [283558] accumulated_eval_time=3281.825500, accumulated_logging_time=10.088145, accumulated_submission_time=94923.660255, global_step=283558, preemption_count=0, score=94923.660255, test/accuracy=0.627800, test/loss=1.843410, test/num_examples=10000, total_duration=98225.745667, train/accuracy=0.961197, train/loss=0.146508, validation/accuracy=0.756060, validation/loss=1.053581, validation/num_examples=50000
I0301 08:31:39.337222 140089879389952 logging_writer.py:48] [283600] global_step=283600, grad_norm=4.436383247375488, loss=0.7048810720443726
I0301 08:32:12.797337 140089845819136 logging_writer.py:48] [283700] global_step=283700, grad_norm=4.556038856506348, loss=0.6185595393180847
I0301 08:32:46.295958 140089879389952 logging_writer.py:48] [283800] global_step=283800, grad_norm=4.407905578613281, loss=0.6311674118041992
I0301 08:33:19.828111 140089845819136 logging_writer.py:48] [283900] global_step=283900, grad_norm=4.141773700714111, loss=0.5584312081336975
I0301 08:33:53.330986 140089879389952 logging_writer.py:48] [284000] global_step=284000, grad_norm=4.2547783851623535, loss=0.5533608198165894
I0301 08:34:26.846187 140089845819136 logging_writer.py:48] [284100] global_step=284100, grad_norm=4.4031500816345215, loss=0.5616310238838196
I0301 08:35:00.292074 140089879389952 logging_writer.py:48] [284200] global_step=284200, grad_norm=4.725260257720947, loss=0.5931463241577148
I0301 08:35:33.816613 140089845819136 logging_writer.py:48] [284300] global_step=284300, grad_norm=4.66661262512207, loss=0.5956984758377075
I0301 08:36:07.278591 140089879389952 logging_writer.py:48] [284400] global_step=284400, grad_norm=5.25341796875, loss=0.7341809272766113
I0301 08:36:40.754605 140089845819136 logging_writer.py:48] [284500] global_step=284500, grad_norm=4.598012447357178, loss=0.6297315359115601
I0301 08:37:14.217414 140089879389952 logging_writer.py:48] [284600] global_step=284600, grad_norm=4.664675235748291, loss=0.6460350155830383
I0301 08:37:47.690136 140089845819136 logging_writer.py:48] [284700] global_step=284700, grad_norm=4.843252182006836, loss=0.6240208745002747
I0301 08:38:21.150442 140089879389952 logging_writer.py:48] [284800] global_step=284800, grad_norm=5.413491725921631, loss=0.6183202266693115
I0301 08:38:54.624458 140089845819136 logging_writer.py:48] [284900] global_step=284900, grad_norm=4.548364639282227, loss=0.615305483341217
I0301 08:39:28.109650 140089879389952 logging_writer.py:48] [285000] global_step=285000, grad_norm=4.570008754730225, loss=0.5476832389831543
I0301 08:39:55.050746 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:40:01.245448 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:40:09.510940 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:40:11.811545 140252611495744 submission_runner.py:411] Time since start: 98752.69s, 	Step: 285082, 	{'train/accuracy': 0.9600207209587097, 'train/loss': 0.1455494463443756, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0529658794403076, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8420534133911133, 'test/num_examples': 10000, 'score': 95433.71224570274, 'total_duration': 98752.69161009789, 'accumulated_submission_time': 95433.71224570274, 'accumulated_eval_time': 3298.5862398147583, 'accumulated_logging_time': 10.166528940200806}
I0301 08:40:11.878168 140089552271104 logging_writer.py:48] [285082] accumulated_eval_time=3298.586240, accumulated_logging_time=10.166529, accumulated_submission_time=95433.712246, global_step=285082, preemption_count=0, score=95433.712246, test/accuracy=0.627700, test/loss=1.842053, test/num_examples=10000, total_duration=98752.691610, train/accuracy=0.960021, train/loss=0.145549, validation/accuracy=0.756020, validation/loss=1.052966, validation/num_examples=50000
I0301 08:40:18.246777 140089770305280 logging_writer.py:48] [285100] global_step=285100, grad_norm=4.050741195678711, loss=0.5752123594284058
I0301 08:40:51.757933 140089552271104 logging_writer.py:48] [285200] global_step=285200, grad_norm=4.001465797424316, loss=0.5988444089889526
I0301 08:41:25.290330 140089770305280 logging_writer.py:48] [285300] global_step=285300, grad_norm=4.93442964553833, loss=0.6338869333267212
I0301 08:41:58.778996 140089552271104 logging_writer.py:48] [285400] global_step=285400, grad_norm=4.407838344573975, loss=0.5708187818527222
I0301 08:42:32.230812 140089770305280 logging_writer.py:48] [285500] global_step=285500, grad_norm=4.931127071380615, loss=0.7633670568466187
I0301 08:43:05.716171 140089552271104 logging_writer.py:48] [285600] global_step=285600, grad_norm=4.3721113204956055, loss=0.5382273197174072
I0301 08:43:39.267503 140089770305280 logging_writer.py:48] [285700] global_step=285700, grad_norm=4.463512420654297, loss=0.6337476968765259
I0301 08:44:12.770382 140089552271104 logging_writer.py:48] [285800] global_step=285800, grad_norm=4.712175369262695, loss=0.6215968132019043
I0301 08:44:46.258532 140089770305280 logging_writer.py:48] [285900] global_step=285900, grad_norm=5.177056312561035, loss=0.6057422757148743
I0301 08:45:19.746774 140089552271104 logging_writer.py:48] [286000] global_step=286000, grad_norm=4.899759769439697, loss=0.5982008576393127
I0301 08:45:53.207836 140089770305280 logging_writer.py:48] [286100] global_step=286100, grad_norm=4.527585506439209, loss=0.6186385750770569
I0301 08:46:26.680337 140089552271104 logging_writer.py:48] [286200] global_step=286200, grad_norm=4.520269870758057, loss=0.5673502087593079
I0301 08:47:00.194879 140089770305280 logging_writer.py:48] [286300] global_step=286300, grad_norm=4.2555437088012695, loss=0.6535139679908752
I0301 08:47:33.785365 140089552271104 logging_writer.py:48] [286400] global_step=286400, grad_norm=4.541202068328857, loss=0.6025474071502686
I0301 08:48:07.324139 140089770305280 logging_writer.py:48] [286500] global_step=286500, grad_norm=4.6344122886657715, loss=0.6607534289360046
I0301 08:48:40.792840 140089552271104 logging_writer.py:48] [286600] global_step=286600, grad_norm=4.1254682540893555, loss=0.5529025793075562
I0301 08:48:41.937580 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:48:48.026883 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:48:56.122889 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:48:58.417538 140252611495744 submission_runner.py:411] Time since start: 99279.30s, 	Step: 286605, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14590491354465485, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.054235577583313, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.8440227508544922, 'test/num_examples': 10000, 'score': 95943.70705962181, 'total_duration': 99279.29757618904, 'accumulated_submission_time': 95943.70705962181, 'accumulated_eval_time': 3315.0661194324493, 'accumulated_logging_time': 10.243273496627808}
I0301 08:48:58.486234 140089552271104 logging_writer.py:48] [286605] accumulated_eval_time=3315.066119, accumulated_logging_time=10.243273, accumulated_submission_time=95943.707060, global_step=286605, preemption_count=0, score=95943.707060, test/accuracy=0.626900, test/loss=1.844023, test/num_examples=10000, total_duration=99279.297576, train/accuracy=0.961256, train/loss=0.145905, validation/accuracy=0.755980, validation/loss=1.054236, validation/num_examples=50000
I0301 08:49:30.617048 140089770305280 logging_writer.py:48] [286700] global_step=286700, grad_norm=4.453070640563965, loss=0.6512863636016846
I0301 08:50:04.105204 140089552271104 logging_writer.py:48] [286800] global_step=286800, grad_norm=4.454782009124756, loss=0.698139488697052
I0301 08:50:37.602473 140089770305280 logging_writer.py:48] [286900] global_step=286900, grad_norm=5.010633945465088, loss=0.6003062129020691
I0301 08:51:11.107947 140089552271104 logging_writer.py:48] [287000] global_step=287000, grad_norm=4.129536151885986, loss=0.6141185164451599
I0301 08:51:44.551235 140089770305280 logging_writer.py:48] [287100] global_step=287100, grad_norm=4.2633442878723145, loss=0.6432432532310486
I0301 08:52:18.015025 140089552271104 logging_writer.py:48] [287200] global_step=287200, grad_norm=4.457170009613037, loss=0.680568516254425
I0301 08:52:51.524969 140089770305280 logging_writer.py:48] [287300] global_step=287300, grad_norm=5.069561004638672, loss=0.6848857998847961
I0301 08:53:25.135635 140089552271104 logging_writer.py:48] [287400] global_step=287400, grad_norm=4.1529974937438965, loss=0.5654276609420776
I0301 08:53:58.602061 140089770305280 logging_writer.py:48] [287500] global_step=287500, grad_norm=5.108669281005859, loss=0.6680930852890015
I0301 08:54:32.077360 140089552271104 logging_writer.py:48] [287600] global_step=287600, grad_norm=4.439695835113525, loss=0.6676747798919678
I0301 08:55:05.561234 140089770305280 logging_writer.py:48] [287700] global_step=287700, grad_norm=4.2628679275512695, loss=0.5854682326316833
I0301 08:55:39.056739 140089552271104 logging_writer.py:48] [287800] global_step=287800, grad_norm=4.676843166351318, loss=0.6745991706848145
I0301 08:56:12.509323 140089770305280 logging_writer.py:48] [287900] global_step=287900, grad_norm=4.687344074249268, loss=0.6488603949546814
I0301 08:56:46.006328 140089552271104 logging_writer.py:48] [288000] global_step=288000, grad_norm=4.93804931640625, loss=0.6406590342521667
I0301 08:57:19.459706 140089770305280 logging_writer.py:48] [288100] global_step=288100, grad_norm=4.532339572906494, loss=0.5957790017127991
I0301 08:57:28.628444 140252611495744 spec.py:321] Evaluating on the training split.
I0301 08:57:34.750204 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 08:57:42.924181 140252611495744 spec.py:349] Evaluating on the test split.
I0301 08:57:45.186607 140252611495744 submission_runner.py:411] Time since start: 99806.07s, 	Step: 288129, 	{'train/accuracy': 0.9620535373687744, 'train/loss': 0.14622661471366882, 'validation/accuracy': 0.7553799748420715, 'validation/loss': 1.0544589757919312, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8442836999893188, 'test/num_examples': 10000, 'score': 96453.7833352089, 'total_duration': 99806.06667613983, 'accumulated_submission_time': 96453.7833352089, 'accumulated_eval_time': 3331.6242294311523, 'accumulated_logging_time': 10.32362985610962}
I0301 08:57:45.255190 140089778697984 logging_writer.py:48] [288129] accumulated_eval_time=3331.624229, accumulated_logging_time=10.323630, accumulated_submission_time=96453.783335, global_step=288129, preemption_count=0, score=96453.783335, test/accuracy=0.627000, test/loss=1.844284, test/num_examples=10000, total_duration=99806.066676, train/accuracy=0.962054, train/loss=0.146227, validation/accuracy=0.755380, validation/loss=1.054459, validation/num_examples=50000
I0301 08:58:09.346737 140089845819136 logging_writer.py:48] [288200] global_step=288200, grad_norm=4.590095043182373, loss=0.6795979142189026
I0301 08:58:42.809420 140089778697984 logging_writer.py:48] [288300] global_step=288300, grad_norm=4.6718363761901855, loss=0.6141223311424255
I0301 08:59:16.302464 140089845819136 logging_writer.py:48] [288400] global_step=288400, grad_norm=4.77669620513916, loss=0.7296997308731079
I0301 08:59:49.841341 140089778697984 logging_writer.py:48] [288500] global_step=288500, grad_norm=4.386845111846924, loss=0.5949565172195435
I0301 09:00:23.381803 140089845819136 logging_writer.py:48] [288600] global_step=288600, grad_norm=4.681433200836182, loss=0.6766756772994995
I0301 09:00:56.874234 140089778697984 logging_writer.py:48] [288700] global_step=288700, grad_norm=4.7256245613098145, loss=0.6308638453483582
I0301 09:01:30.355474 140089845819136 logging_writer.py:48] [288800] global_step=288800, grad_norm=4.872753143310547, loss=0.5521409511566162
I0301 09:02:03.795997 140089778697984 logging_writer.py:48] [288900] global_step=288900, grad_norm=4.29619026184082, loss=0.6433982849121094
I0301 09:02:37.294195 140089845819136 logging_writer.py:48] [289000] global_step=289000, grad_norm=4.136960029602051, loss=0.5565147399902344
I0301 09:03:10.768635 140089778697984 logging_writer.py:48] [289100] global_step=289100, grad_norm=4.7088141441345215, loss=0.7184869050979614
I0301 09:03:44.209215 140089845819136 logging_writer.py:48] [289200] global_step=289200, grad_norm=4.951099872589111, loss=0.625328004360199
I0301 09:04:17.690637 140089778697984 logging_writer.py:48] [289300] global_step=289300, grad_norm=4.370758533477783, loss=0.6316648125648499
I0301 09:04:51.182983 140089845819136 logging_writer.py:48] [289400] global_step=289400, grad_norm=4.909926414489746, loss=0.6501340270042419
I0301 09:05:24.704412 140089778697984 logging_writer.py:48] [289500] global_step=289500, grad_norm=4.548454284667969, loss=0.6561781167984009
I0301 09:05:58.296451 140089845819136 logging_writer.py:48] [289600] global_step=289600, grad_norm=4.5841522216796875, loss=0.667182981967926
I0301 09:06:15.509358 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:06:21.707724 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:06:29.925767 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:06:32.196035 140252611495744 submission_runner.py:411] Time since start: 100333.08s, 	Step: 289653, 	{'train/accuracy': 0.9625717401504517, 'train/loss': 0.14200511574745178, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.053595781326294, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8435249328613281, 'test/num_examples': 10000, 'score': 96963.97358345985, 'total_duration': 100333.07609534264, 'accumulated_submission_time': 96963.97358345985, 'accumulated_eval_time': 3348.3108434677124, 'accumulated_logging_time': 10.402316093444824}
I0301 09:06:32.266485 140089854211840 logging_writer.py:48] [289653] accumulated_eval_time=3348.310843, accumulated_logging_time=10.402316, accumulated_submission_time=96963.973583, global_step=289653, preemption_count=0, score=96963.973583, test/accuracy=0.627000, test/loss=1.843525, test/num_examples=10000, total_duration=100333.076095, train/accuracy=0.962572, train/loss=0.142005, validation/accuracy=0.756200, validation/loss=1.053596, validation/num_examples=50000
I0301 09:06:48.358232 140089870997248 logging_writer.py:48] [289700] global_step=289700, grad_norm=4.091067314147949, loss=0.6245396733283997
I0301 09:07:21.805183 140089854211840 logging_writer.py:48] [289800] global_step=289800, grad_norm=4.393036842346191, loss=0.6312445402145386
I0301 09:07:55.243281 140089870997248 logging_writer.py:48] [289900] global_step=289900, grad_norm=4.868551254272461, loss=0.6016013026237488
I0301 09:08:28.727571 140089854211840 logging_writer.py:48] [290000] global_step=290000, grad_norm=4.948643207550049, loss=0.6092463731765747
I0301 09:09:02.296901 140089870997248 logging_writer.py:48] [290100] global_step=290100, grad_norm=4.422530651092529, loss=0.5653173327445984
I0301 09:09:35.809736 140089854211840 logging_writer.py:48] [290200] global_step=290200, grad_norm=4.820552825927734, loss=0.6668139696121216
I0301 09:10:09.289901 140089870997248 logging_writer.py:48] [290300] global_step=290300, grad_norm=4.23144006729126, loss=0.6100167036056519
I0301 09:10:42.733892 140089854211840 logging_writer.py:48] [290400] global_step=290400, grad_norm=4.554234981536865, loss=0.6237356662750244
I0301 09:11:16.236595 140089870997248 logging_writer.py:48] [290500] global_step=290500, grad_norm=4.595615863800049, loss=0.5736985802650452
I0301 09:11:49.794932 140089854211840 logging_writer.py:48] [290600] global_step=290600, grad_norm=5.384066581726074, loss=0.6382864117622375
I0301 09:12:23.294495 140089870997248 logging_writer.py:48] [290700] global_step=290700, grad_norm=4.479892730712891, loss=0.6520900726318359
I0301 09:12:56.810910 140089854211840 logging_writer.py:48] [290800] global_step=290800, grad_norm=4.4605817794799805, loss=0.6654937863349915
I0301 09:13:30.328719 140089870997248 logging_writer.py:48] [290900] global_step=290900, grad_norm=4.173159599304199, loss=0.5791789293289185
I0301 09:14:03.802699 140089854211840 logging_writer.py:48] [291000] global_step=291000, grad_norm=4.467194080352783, loss=0.6382537484169006
I0301 09:14:37.303255 140089870997248 logging_writer.py:48] [291100] global_step=291100, grad_norm=4.737363815307617, loss=0.6943321228027344
I0301 09:15:02.208847 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:15:08.354455 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:15:16.514768 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:15:18.815887 140252611495744 submission_runner.py:411] Time since start: 100859.70s, 	Step: 291176, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.14845694601535797, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0529533624649048, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8418251276016235, 'test/num_examples': 10000, 'score': 97473.8518486023, 'total_duration': 100859.69595813751, 'accumulated_submission_time': 97473.8518486023, 'accumulated_eval_time': 3364.91783618927, 'accumulated_logging_time': 10.482990264892578}
I0301 09:15:18.880095 140089778697984 logging_writer.py:48] [291176] accumulated_eval_time=3364.917836, accumulated_logging_time=10.482990, accumulated_submission_time=97473.851849, global_step=291176, preemption_count=0, score=97473.851849, test/accuracy=0.628200, test/loss=1.841825, test/num_examples=10000, total_duration=100859.695958, train/accuracy=0.959263, train/loss=0.148457, validation/accuracy=0.756160, validation/loss=1.052953, validation/num_examples=50000
I0301 09:15:27.254997 140089837426432 logging_writer.py:48] [291200] global_step=291200, grad_norm=4.784255027770996, loss=0.6591174006462097
I0301 09:16:00.710537 140089778697984 logging_writer.py:48] [291300] global_step=291300, grad_norm=4.947895526885986, loss=0.6184526085853577
I0301 09:16:34.188792 140089837426432 logging_writer.py:48] [291400] global_step=291400, grad_norm=4.417919635772705, loss=0.6181026697158813
I0301 09:17:07.684466 140089778697984 logging_writer.py:48] [291500] global_step=291500, grad_norm=5.2564287185668945, loss=0.6013455390930176
I0301 09:17:41.189530 140089837426432 logging_writer.py:48] [291600] global_step=291600, grad_norm=5.100582122802734, loss=0.6838042736053467
I0301 09:18:14.774028 140089778697984 logging_writer.py:48] [291700] global_step=291700, grad_norm=4.55579137802124, loss=0.6132269501686096
I0301 09:18:48.272384 140089837426432 logging_writer.py:48] [291800] global_step=291800, grad_norm=4.404965877532959, loss=0.6160486936569214
I0301 09:19:21.824890 140089778697984 logging_writer.py:48] [291900] global_step=291900, grad_norm=4.459107875823975, loss=0.6587972640991211
I0301 09:19:55.297970 140089837426432 logging_writer.py:48] [292000] global_step=292000, grad_norm=4.587175369262695, loss=0.6157361268997192
I0301 09:20:28.874205 140089778697984 logging_writer.py:48] [292100] global_step=292100, grad_norm=4.531639575958252, loss=0.6502881646156311
I0301 09:21:02.357751 140089837426432 logging_writer.py:48] [292200] global_step=292200, grad_norm=4.383113861083984, loss=0.5743294358253479
I0301 09:21:35.884802 140089778697984 logging_writer.py:48] [292300] global_step=292300, grad_norm=4.600541591644287, loss=0.609745442867279
I0301 09:22:09.313886 140089837426432 logging_writer.py:48] [292400] global_step=292400, grad_norm=4.4864325523376465, loss=0.6027142405509949
I0301 09:22:42.814104 140089778697984 logging_writer.py:48] [292500] global_step=292500, grad_norm=5.153730392456055, loss=0.6481489539146423
I0301 09:23:16.287631 140089837426432 logging_writer.py:48] [292600] global_step=292600, grad_norm=5.084286689758301, loss=0.7338273525238037
I0301 09:23:48.921980 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:23:55.303339 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:24:03.532008 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:24:05.800814 140252611495744 submission_runner.py:411] Time since start: 101386.68s, 	Step: 292699, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.1482071578502655, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.052278757095337, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.8424854278564453, 'test/num_examples': 10000, 'score': 97983.82679986954, 'total_duration': 101386.6808810234, 'accumulated_submission_time': 97983.82679986954, 'accumulated_eval_time': 3381.7966146469116, 'accumulated_logging_time': 10.558565378189087}
I0301 09:24:05.865142 140089854211840 logging_writer.py:48] [292699] accumulated_eval_time=3381.796615, accumulated_logging_time=10.558565, accumulated_submission_time=97983.826800, global_step=292699, preemption_count=0, score=97983.826800, test/accuracy=0.626600, test/loss=1.842485, test/num_examples=10000, total_duration=101386.680881, train/accuracy=0.960140, train/loss=0.148207, validation/accuracy=0.756180, validation/loss=1.052279, validation/num_examples=50000
I0301 09:24:06.548713 140089862604544 logging_writer.py:48] [292700] global_step=292700, grad_norm=4.267695903778076, loss=0.5959142446517944
I0301 09:24:40.019729 140089854211840 logging_writer.py:48] [292800] global_step=292800, grad_norm=4.569202423095703, loss=0.6179589629173279
I0301 09:25:13.484929 140089862604544 logging_writer.py:48] [292900] global_step=292900, grad_norm=4.3713555335998535, loss=0.6630430221557617
I0301 09:25:46.997479 140089854211840 logging_writer.py:48] [293000] global_step=293000, grad_norm=4.413253307342529, loss=0.5774295926094055
I0301 09:26:20.507715 140089862604544 logging_writer.py:48] [293100] global_step=293100, grad_norm=4.543944358825684, loss=0.5483499765396118
I0301 09:26:54.034284 140089854211840 logging_writer.py:48] [293200] global_step=293200, grad_norm=4.644827842712402, loss=0.5605321526527405
I0301 09:27:27.512275 140089862604544 logging_writer.py:48] [293300] global_step=293300, grad_norm=3.9883687496185303, loss=0.5820075869560242
I0301 09:28:01.000951 140089854211840 logging_writer.py:48] [293400] global_step=293400, grad_norm=4.937838554382324, loss=0.6099007725715637
I0301 09:28:34.458063 140089862604544 logging_writer.py:48] [293500] global_step=293500, grad_norm=4.214818000793457, loss=0.5501640439033508
I0301 09:29:07.918657 140089854211840 logging_writer.py:48] [293600] global_step=293600, grad_norm=4.586376667022705, loss=0.6890453696250916
I0301 09:29:41.424060 140089862604544 logging_writer.py:48] [293700] global_step=293700, grad_norm=4.485903263092041, loss=0.5956732630729675
I0301 09:30:15.122680 140089854211840 logging_writer.py:48] [293800] global_step=293800, grad_norm=4.905628681182861, loss=0.6830683350563049
I0301 09:30:48.626980 140089862604544 logging_writer.py:48] [293900] global_step=293900, grad_norm=4.992348670959473, loss=0.7341774702072144
I0301 09:31:22.112744 140089854211840 logging_writer.py:48] [294000] global_step=294000, grad_norm=4.26063871383667, loss=0.6109232902526855
I0301 09:31:55.577883 140089862604544 logging_writer.py:48] [294100] global_step=294100, grad_norm=4.9556989669799805, loss=0.6418114304542542
I0301 09:32:29.022827 140089854211840 logging_writer.py:48] [294200] global_step=294200, grad_norm=4.290855407714844, loss=0.6296790242195129
I0301 09:32:35.884877 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:32:42.075262 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:32:50.193776 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:32:52.489310 140252611495744 submission_runner.py:411] Time since start: 101913.37s, 	Step: 294222, 	{'train/accuracy': 0.9607182741165161, 'train/loss': 0.14716771245002747, 'validation/accuracy': 0.7562400102615356, 'validation/loss': 1.0532735586166382, 'validation/num_examples': 50000, 'test/accuracy': 0.626300036907196, 'test/loss': 1.8423364162445068, 'test/num_examples': 10000, 'score': 98493.78168106079, 'total_duration': 101913.36937975883, 'accumulated_submission_time': 98493.78168106079, 'accumulated_eval_time': 3398.401019334793, 'accumulated_logging_time': 10.632897138595581}
I0301 09:32:52.546019 140089837426432 logging_writer.py:48] [294222] accumulated_eval_time=3398.401019, accumulated_logging_time=10.632897, accumulated_submission_time=98493.781681, global_step=294222, preemption_count=0, score=98493.781681, test/accuracy=0.626300, test/loss=1.842336, test/num_examples=10000, total_duration=101913.369380, train/accuracy=0.960718, train/loss=0.147168, validation/accuracy=0.756240, validation/loss=1.053274, validation/num_examples=50000
I0301 09:33:18.964172 140089845819136 logging_writer.py:48] [294300] global_step=294300, grad_norm=4.964784622192383, loss=0.6546992659568787
I0301 09:33:52.534573 140089837426432 logging_writer.py:48] [294400] global_step=294400, grad_norm=4.424222946166992, loss=0.5992075204849243
I0301 09:34:25.992803 140089845819136 logging_writer.py:48] [294500] global_step=294500, grad_norm=4.336600303649902, loss=0.6068509221076965
I0301 09:34:59.518151 140089837426432 logging_writer.py:48] [294600] global_step=294600, grad_norm=4.449831962585449, loss=0.66506427526474
I0301 09:35:33.000857 140089845819136 logging_writer.py:48] [294700] global_step=294700, grad_norm=4.371152400970459, loss=0.6013191342353821
I0301 09:36:06.462398 140089837426432 logging_writer.py:48] [294800] global_step=294800, grad_norm=5.074646949768066, loss=0.6359070539474487
I0301 09:36:40.045195 140089845819136 logging_writer.py:48] [294900] global_step=294900, grad_norm=4.275322437286377, loss=0.63544100522995
I0301 09:37:13.498151 140089837426432 logging_writer.py:48] [295000] global_step=295000, grad_norm=4.639342308044434, loss=0.6338775157928467
I0301 09:37:46.978687 140089845819136 logging_writer.py:48] [295100] global_step=295100, grad_norm=4.442359447479248, loss=0.6369263529777527
I0301 09:38:20.422406 140089837426432 logging_writer.py:48] [295200] global_step=295200, grad_norm=4.685819149017334, loss=0.6632872819900513
I0301 09:38:53.909157 140089845819136 logging_writer.py:48] [295300] global_step=295300, grad_norm=5.11206579208374, loss=0.6091541051864624
I0301 09:39:27.363546 140089837426432 logging_writer.py:48] [295400] global_step=295400, grad_norm=4.446893692016602, loss=0.5938211679458618
I0301 09:40:00.804523 140089845819136 logging_writer.py:48] [295500] global_step=295500, grad_norm=4.745667934417725, loss=0.6718295216560364
I0301 09:40:34.314426 140089837426432 logging_writer.py:48] [295600] global_step=295600, grad_norm=4.746931076049805, loss=0.6399060487747192
I0301 09:41:07.770886 140089845819136 logging_writer.py:48] [295700] global_step=295700, grad_norm=4.416451930999756, loss=0.619615375995636
I0301 09:41:22.606069 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:41:28.726109 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:41:36.871504 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:41:39.148000 140252611495744 submission_runner.py:411] Time since start: 102440.03s, 	Step: 295746, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.14415274560451508, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0529661178588867, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8412050008773804, 'test/num_examples': 10000, 'score': 99003.77694940567, 'total_duration': 102440.02795219421, 'accumulated_submission_time': 99003.77694940567, 'accumulated_eval_time': 3414.94277882576, 'accumulated_logging_time': 10.699851036071777}
I0301 09:41:39.218408 140089778697984 logging_writer.py:48] [295746] accumulated_eval_time=3414.942779, accumulated_logging_time=10.699851, accumulated_submission_time=99003.776949, global_step=295746, preemption_count=0, score=99003.776949, test/accuracy=0.627400, test/loss=1.841205, test/num_examples=10000, total_duration=102440.027952, train/accuracy=0.961854, train/loss=0.144153, validation/accuracy=0.756060, validation/loss=1.052966, validation/num_examples=50000
I0301 09:41:57.640103 140089837426432 logging_writer.py:48] [295800] global_step=295800, grad_norm=4.631592750549316, loss=0.7255379557609558
I0301 09:42:31.253965 140089778697984 logging_writer.py:48] [295900] global_step=295900, grad_norm=4.275142669677734, loss=0.6426599621772766
I0301 09:43:04.691200 140089837426432 logging_writer.py:48] [296000] global_step=296000, grad_norm=4.32794189453125, loss=0.5617002248764038
I0301 09:43:38.160549 140089778697984 logging_writer.py:48] [296100] global_step=296100, grad_norm=4.8048272132873535, loss=0.6998789310455322
I0301 09:44:11.605750 140089837426432 logging_writer.py:48] [296200] global_step=296200, grad_norm=4.685161113739014, loss=0.6163278222084045
I0301 09:44:45.067116 140089778697984 logging_writer.py:48] [296300] global_step=296300, grad_norm=4.799755573272705, loss=0.5316299796104431
I0301 09:45:18.521840 140089837426432 logging_writer.py:48] [296400] global_step=296400, grad_norm=4.259521961212158, loss=0.579693615436554
I0301 09:45:52.023968 140089778697984 logging_writer.py:48] [296500] global_step=296500, grad_norm=4.261299133300781, loss=0.6334274411201477
I0301 09:46:25.474333 140089837426432 logging_writer.py:48] [296600] global_step=296600, grad_norm=4.7892913818359375, loss=0.7248660922050476
I0301 09:46:58.949381 140089778697984 logging_writer.py:48] [296700] global_step=296700, grad_norm=4.603715419769287, loss=0.6345601677894592
I0301 09:47:32.396168 140089837426432 logging_writer.py:48] [296800] global_step=296800, grad_norm=4.396729946136475, loss=0.6054151654243469
I0301 09:48:05.834378 140089778697984 logging_writer.py:48] [296900] global_step=296900, grad_norm=4.905159950256348, loss=0.6043065786361694
I0301 09:48:39.425280 140089837426432 logging_writer.py:48] [297000] global_step=297000, grad_norm=5.369972229003906, loss=0.7402449250221252
I0301 09:49:12.881774 140089778697984 logging_writer.py:48] [297100] global_step=297100, grad_norm=4.693267822265625, loss=0.6660847067832947
I0301 09:49:46.347390 140089837426432 logging_writer.py:48] [297200] global_step=297200, grad_norm=4.6493611335754395, loss=0.620913028717041
I0301 09:50:09.221858 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:50:15.302424 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:50:23.444252 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:50:25.743424 140252611495744 submission_runner.py:411] Time since start: 102966.62s, 	Step: 297270, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14590246975421906, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.05452299118042, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8436675071716309, 'test/num_examples': 10000, 'score': 99513.71591758728, 'total_duration': 102966.62348175049, 'accumulated_submission_time': 99513.71591758728, 'accumulated_eval_time': 3431.464282512665, 'accumulated_logging_time': 10.780103921890259}
I0301 09:50:25.808564 140089770305280 logging_writer.py:48] [297270] accumulated_eval_time=3431.464283, accumulated_logging_time=10.780104, accumulated_submission_time=99513.715918, global_step=297270, preemption_count=0, score=99513.715918, test/accuracy=0.627400, test/loss=1.843668, test/num_examples=10000, total_duration=102966.623482, train/accuracy=0.960419, train/loss=0.145902, validation/accuracy=0.755760, validation/loss=1.054523, validation/num_examples=50000
I0301 09:50:36.193022 140089854211840 logging_writer.py:48] [297300] global_step=297300, grad_norm=5.031946659088135, loss=0.5682758688926697
I0301 09:51:09.673084 140089770305280 logging_writer.py:48] [297400] global_step=297400, grad_norm=4.828853607177734, loss=0.6428986191749573
I0301 09:51:43.115401 140089854211840 logging_writer.py:48] [297500] global_step=297500, grad_norm=4.454829216003418, loss=0.6681964993476868
I0301 09:52:16.585641 140089770305280 logging_writer.py:48] [297600] global_step=297600, grad_norm=4.509407997131348, loss=0.5713112354278564
I0301 09:52:50.115381 140089854211840 logging_writer.py:48] [297700] global_step=297700, grad_norm=4.98833703994751, loss=0.6743453145027161
I0301 09:53:23.594848 140089770305280 logging_writer.py:48] [297800] global_step=297800, grad_norm=4.638036251068115, loss=0.6135789155960083
I0301 09:53:57.084905 140089854211840 logging_writer.py:48] [297900] global_step=297900, grad_norm=4.768624305725098, loss=0.6749674081802368
I0301 09:54:30.635512 140089770305280 logging_writer.py:48] [298000] global_step=298000, grad_norm=4.110910415649414, loss=0.5858770608901978
I0301 09:55:04.111751 140089854211840 logging_writer.py:48] [298100] global_step=298100, grad_norm=5.243349552154541, loss=0.6472928524017334
I0301 09:55:37.564178 140089770305280 logging_writer.py:48] [298200] global_step=298200, grad_norm=4.438729763031006, loss=0.6535494923591614
I0301 09:56:11.037828 140089854211840 logging_writer.py:48] [298300] global_step=298300, grad_norm=4.723494529724121, loss=0.6141204833984375
I0301 09:56:44.532279 140089770305280 logging_writer.py:48] [298400] global_step=298400, grad_norm=4.195497035980225, loss=0.5733953714370728
I0301 09:57:18.008217 140089854211840 logging_writer.py:48] [298500] global_step=298500, grad_norm=5.029825687408447, loss=0.6589646339416504
I0301 09:57:51.482090 140089770305280 logging_writer.py:48] [298600] global_step=298600, grad_norm=4.695550441741943, loss=0.5884377956390381
I0301 09:58:24.942939 140089854211840 logging_writer.py:48] [298700] global_step=298700, grad_norm=4.77029275894165, loss=0.5841923356056213
I0301 09:58:55.878189 140252611495744 spec.py:321] Evaluating on the training split.
I0301 09:59:02.017404 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 09:59:10.295229 140252611495744 spec.py:349] Evaluating on the test split.
I0301 09:59:12.533169 140252611495744 submission_runner.py:411] Time since start: 103493.41s, 	Step: 298794, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14517304301261902, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 1.0528514385223389, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8408688306808472, 'test/num_examples': 10000, 'score': 100023.7195968628, 'total_duration': 103493.4132258892, 'accumulated_submission_time': 100023.7195968628, 'accumulated_eval_time': 3448.119199991226, 'accumulated_logging_time': 10.856154441833496}
I0301 09:59:12.602768 140089778697984 logging_writer.py:48] [298794] accumulated_eval_time=3448.119200, accumulated_logging_time=10.856154, accumulated_submission_time=100023.719597, global_step=298794, preemption_count=0, score=100023.719597, test/accuracy=0.627300, test/loss=1.840869, test/num_examples=10000, total_duration=103493.413226, train/accuracy=0.960938, train/loss=0.145173, validation/accuracy=0.756280, validation/loss=1.052851, validation/num_examples=50000
I0301 09:59:14.948387 140089837426432 logging_writer.py:48] [298800] global_step=298800, grad_norm=4.725098609924316, loss=0.6604372262954712
I0301 09:59:48.394796 140089778697984 logging_writer.py:48] [298900] global_step=298900, grad_norm=4.5505499839782715, loss=0.6447342038154602
I0301 10:00:21.980351 140089837426432 logging_writer.py:48] [299000] global_step=299000, grad_norm=4.414828300476074, loss=0.611744225025177
I0301 10:00:55.435668 140089778697984 logging_writer.py:48] [299100] global_step=299100, grad_norm=5.127660274505615, loss=0.6417177319526672
I0301 10:01:28.894814 140089837426432 logging_writer.py:48] [299200] global_step=299200, grad_norm=5.1764302253723145, loss=0.6170473098754883
I0301 10:02:02.372203 140089778697984 logging_writer.py:48] [299300] global_step=299300, grad_norm=4.739116668701172, loss=0.6918293833732605
I0301 10:02:35.864464 140089837426432 logging_writer.py:48] [299400] global_step=299400, grad_norm=4.71134614944458, loss=0.6616031527519226
I0301 10:03:09.396597 140089778697984 logging_writer.py:48] [299500] global_step=299500, grad_norm=4.963200092315674, loss=0.6165445446968079
I0301 10:03:42.869090 140089837426432 logging_writer.py:48] [299600] global_step=299600, grad_norm=4.197730541229248, loss=0.6071016788482666
I0301 10:04:16.315860 140089778697984 logging_writer.py:48] [299700] global_step=299700, grad_norm=4.849785804748535, loss=0.6184796094894409
I0301 10:04:49.760512 140089837426432 logging_writer.py:48] [299800] global_step=299800, grad_norm=4.754743576049805, loss=0.5678899884223938
I0301 10:05:23.224431 140089778697984 logging_writer.py:48] [299900] global_step=299900, grad_norm=4.46403694152832, loss=0.5815637707710266
I0301 10:05:56.669762 140089837426432 logging_writer.py:48] [300000] global_step=300000, grad_norm=4.549872875213623, loss=0.5886809229850769
I0301 10:06:30.243010 140089778697984 logging_writer.py:48] [300100] global_step=300100, grad_norm=4.178147792816162, loss=0.6069320440292358
I0301 10:07:03.711919 140089837426432 logging_writer.py:48] [300200] global_step=300200, grad_norm=4.735125541687012, loss=0.5666193962097168
I0301 10:07:37.199187 140089778697984 logging_writer.py:48] [300300] global_step=300300, grad_norm=4.407745838165283, loss=0.5653823614120483
I0301 10:07:42.688933 140252611495744 spec.py:321] Evaluating on the training split.
I0301 10:07:48.821149 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 10:07:56.989491 140252611495744 spec.py:349] Evaluating on the test split.
I0301 10:07:59.276930 140252611495744 submission_runner.py:411] Time since start: 104020.16s, 	Step: 300318, 	{'train/accuracy': 0.9599409699440002, 'train/loss': 0.14764149487018585, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.0543302297592163, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.844310998916626, 'test/num_examples': 10000, 'score': 100533.74055194855, 'total_duration': 104020.15699887276, 'accumulated_submission_time': 100533.74055194855, 'accumulated_eval_time': 3464.7071413993835, 'accumulated_logging_time': 10.93600869178772}
I0301 10:07:59.343143 140089854211840 logging_writer.py:48] [300318] accumulated_eval_time=3464.707141, accumulated_logging_time=10.936009, accumulated_submission_time=100533.740552, global_step=300318, preemption_count=0, score=100533.740552, test/accuracy=0.628000, test/loss=1.844311, test/num_examples=10000, total_duration=104020.156999, train/accuracy=0.959941, train/loss=0.147641, validation/accuracy=0.755640, validation/loss=1.054330, validation/num_examples=50000
I0301 10:08:27.123581 140089862604544 logging_writer.py:48] [300400] global_step=300400, grad_norm=4.930019378662109, loss=0.6440038084983826
I0301 10:09:00.548108 140089854211840 logging_writer.py:48] [300500] global_step=300500, grad_norm=4.653802871704102, loss=0.6145392060279846
I0301 10:09:34.018177 140089862604544 logging_writer.py:48] [300600] global_step=300600, grad_norm=5.045473575592041, loss=0.6693734526634216
I0301 10:10:07.480579 140089854211840 logging_writer.py:48] [300700] global_step=300700, grad_norm=4.396735668182373, loss=0.6185503005981445
I0301 10:10:40.928444 140089862604544 logging_writer.py:48] [300800] global_step=300800, grad_norm=4.356980323791504, loss=0.6278671026229858
I0301 10:11:14.372097 140089854211840 logging_writer.py:48] [300900] global_step=300900, grad_norm=4.8780388832092285, loss=0.6821184158325195
I0301 10:11:47.877218 140089862604544 logging_writer.py:48] [301000] global_step=301000, grad_norm=4.200558185577393, loss=0.5171970129013062
I0301 10:12:21.436775 140089854211840 logging_writer.py:48] [301100] global_step=301100, grad_norm=4.483739376068115, loss=0.5820555686950684
I0301 10:12:54.939771 140089862604544 logging_writer.py:48] [301200] global_step=301200, grad_norm=4.408638000488281, loss=0.5952309370040894
I0301 10:13:28.378770 140089854211840 logging_writer.py:48] [301300] global_step=301300, grad_norm=5.315891265869141, loss=0.6256081461906433
I0301 10:14:01.849349 140089862604544 logging_writer.py:48] [301400] global_step=301400, grad_norm=4.848719120025635, loss=0.6707157492637634
I0301 10:14:35.332533 140089854211840 logging_writer.py:48] [301500] global_step=301500, grad_norm=4.1923112869262695, loss=0.571750283241272
I0301 10:15:08.779819 140089862604544 logging_writer.py:48] [301600] global_step=301600, grad_norm=4.5050368309021, loss=0.6719404458999634
I0301 10:15:42.254446 140089854211840 logging_writer.py:48] [301700] global_step=301700, grad_norm=4.649360179901123, loss=0.5960150957107544
I0301 10:16:15.747148 140089862604544 logging_writer.py:48] [301800] global_step=301800, grad_norm=4.408941745758057, loss=0.6122839450836182
I0301 10:16:29.591578 140252611495744 spec.py:321] Evaluating on the training split.
I0301 10:16:35.741034 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 10:16:43.802204 140252611495744 spec.py:349] Evaluating on the test split.
I0301 10:16:46.065323 140252611495744 submission_runner.py:411] Time since start: 104546.95s, 	Step: 301843, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14494802057743073, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.0539348125457764, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8417717218399048, 'test/num_examples': 10000, 'score': 101043.92377829552, 'total_duration': 104546.94538855553, 'accumulated_submission_time': 101043.92377829552, 'accumulated_eval_time': 3481.1808309555054, 'accumulated_logging_time': 11.012544393539429}
I0301 10:16:46.145431 140089552271104 logging_writer.py:48] [301843] accumulated_eval_time=3481.180831, accumulated_logging_time=11.012544, accumulated_submission_time=101043.923778, global_step=301843, preemption_count=0, score=101043.923778, test/accuracy=0.628000, test/loss=1.841772, test/num_examples=10000, total_duration=104546.945389, train/accuracy=0.961515, train/loss=0.144948, validation/accuracy=0.756180, validation/loss=1.053935, validation/num_examples=50000
I0301 10:17:05.538885 140089770305280 logging_writer.py:48] [301900] global_step=301900, grad_norm=4.381178379058838, loss=0.6033714413642883
I0301 10:17:39.047956 140089552271104 logging_writer.py:48] [302000] global_step=302000, grad_norm=4.424770355224609, loss=0.6504895091056824
I0301 10:18:12.520861 140089770305280 logging_writer.py:48] [302100] global_step=302100, grad_norm=4.7074055671691895, loss=0.6684758067131042
I0301 10:18:46.132243 140089552271104 logging_writer.py:48] [302200] global_step=302200, grad_norm=4.54069709777832, loss=0.669745147228241
I0301 10:19:19.611590 140089770305280 logging_writer.py:48] [302300] global_step=302300, grad_norm=4.45777702331543, loss=0.5782709717750549
I0301 10:19:53.081769 140089552271104 logging_writer.py:48] [302400] global_step=302400, grad_norm=4.413662433624268, loss=0.5877566337585449
I0301 10:20:26.523729 140089770305280 logging_writer.py:48] [302500] global_step=302500, grad_norm=4.373415470123291, loss=0.5415616631507874
I0301 10:21:00.020648 140089552271104 logging_writer.py:48] [302600] global_step=302600, grad_norm=4.714846611022949, loss=0.5701934695243835
I0301 10:21:33.493619 140089770305280 logging_writer.py:48] [302700] global_step=302700, grad_norm=4.540966033935547, loss=0.5946241021156311
I0301 10:22:06.964453 140089552271104 logging_writer.py:48] [302800] global_step=302800, grad_norm=4.529996395111084, loss=0.6193966865539551
I0301 10:22:40.456582 140089770305280 logging_writer.py:48] [302900] global_step=302900, grad_norm=4.7537522315979, loss=0.5872272253036499
I0301 10:23:14.020335 140089552271104 logging_writer.py:48] [303000] global_step=303000, grad_norm=4.662184238433838, loss=0.6469792127609253
I0301 10:23:47.514236 140089770305280 logging_writer.py:48] [303100] global_step=303100, grad_norm=4.566357135772705, loss=0.6571595668792725
I0301 10:24:21.074964 140089552271104 logging_writer.py:48] [303200] global_step=303200, grad_norm=4.7684454917907715, loss=0.6150872111320496
I0301 10:24:54.608886 140089770305280 logging_writer.py:48] [303300] global_step=303300, grad_norm=4.325203895568848, loss=0.6341029405593872
I0301 10:25:16.190503 140252611495744 spec.py:321] Evaluating on the training split.
I0301 10:25:22.443441 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 10:25:30.490074 140252611495744 spec.py:349] Evaluating on the test split.
I0301 10:25:32.754806 140252611495744 submission_runner.py:411] Time since start: 105073.63s, 	Step: 303366, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14850832521915436, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.054638147354126, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8432505130767822, 'test/num_examples': 10000, 'score': 101553.90138459206, 'total_duration': 105073.63487243652, 'accumulated_submission_time': 101553.90138459206, 'accumulated_eval_time': 3497.745079278946, 'accumulated_logging_time': 11.105444431304932}
I0301 10:25:32.828812 140089552271104 logging_writer.py:48] [303366] accumulated_eval_time=3497.745079, accumulated_logging_time=11.105444, accumulated_submission_time=101553.901385, global_step=303366, preemption_count=0, score=101553.901385, test/accuracy=0.626700, test/loss=1.843251, test/num_examples=10000, total_duration=105073.634872, train/accuracy=0.960320, train/loss=0.148508, validation/accuracy=0.755340, validation/loss=1.054638, validation/num_examples=50000
I0301 10:25:44.543935 140089770305280 logging_writer.py:48] [303400] global_step=303400, grad_norm=5.010218620300293, loss=0.7443320751190186
I0301 10:26:18.009608 140089552271104 logging_writer.py:48] [303500] global_step=303500, grad_norm=4.697934627532959, loss=0.6067550182342529
I0301 10:26:51.465187 140089770305280 logging_writer.py:48] [303600] global_step=303600, grad_norm=4.0701398849487305, loss=0.5726762413978577
I0301 10:27:24.910240 140089552271104 logging_writer.py:48] [303700] global_step=303700, grad_norm=5.183469295501709, loss=0.6870196461677551
I0301 10:27:58.390209 140089770305280 logging_writer.py:48] [303800] global_step=303800, grad_norm=4.375940799713135, loss=0.6005342602729797
I0301 10:28:31.833524 140089552271104 logging_writer.py:48] [303900] global_step=303900, grad_norm=4.857062816619873, loss=0.6988270878791809
I0301 10:29:05.299902 140089770305280 logging_writer.py:48] [304000] global_step=304000, grad_norm=4.900278091430664, loss=0.5849335193634033
I0301 10:29:38.807585 140089552271104 logging_writer.py:48] [304100] global_step=304100, grad_norm=4.609952926635742, loss=0.5687050819396973
I0301 10:30:12.263220 140089770305280 logging_writer.py:48] [304200] global_step=304200, grad_norm=4.3739142417907715, loss=0.6011266708374023
I0301 10:30:45.769947 140089552271104 logging_writer.py:48] [304300] global_step=304300, grad_norm=4.583516597747803, loss=0.5517978668212891
I0301 10:31:19.211935 140089770305280 logging_writer.py:48] [304400] global_step=304400, grad_norm=4.495348930358887, loss=0.5954537987709045
I0301 10:31:52.709022 140089552271104 logging_writer.py:48] [304500] global_step=304500, grad_norm=4.971292018890381, loss=0.6822471618652344
I0301 10:32:26.216010 140089770305280 logging_writer.py:48] [304600] global_step=304600, grad_norm=4.7024970054626465, loss=0.6261587142944336
I0301 10:32:59.683924 140089552271104 logging_writer.py:48] [304700] global_step=304700, grad_norm=4.892334938049316, loss=0.6975192427635193
I0301 10:33:33.164781 140089770305280 logging_writer.py:48] [304800] global_step=304800, grad_norm=4.998588562011719, loss=0.701500415802002
I0301 10:34:02.755874 140252611495744 spec.py:321] Evaluating on the training split.
I0301 10:34:08.972542 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 10:34:16.974827 140252611495744 spec.py:349] Evaluating on the test split.
I0301 10:34:19.272589 140252611495744 submission_runner.py:411] Time since start: 105600.15s, 	Step: 304890, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14555788040161133, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0540353059768677, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8422266244888306, 'test/num_examples': 10000, 'score': 102063.76321053505, 'total_duration': 105600.15265583992, 'accumulated_submission_time': 102063.76321053505, 'accumulated_eval_time': 3514.2617585659027, 'accumulated_logging_time': 11.190539121627808}
I0301 10:34:19.341862 140089845819136 logging_writer.py:48] [304890] accumulated_eval_time=3514.261759, accumulated_logging_time=11.190539, accumulated_submission_time=102063.763211, global_step=304890, preemption_count=0, score=102063.763211, test/accuracy=0.627000, test/loss=1.842227, test/num_examples=10000, total_duration=105600.152656, train/accuracy=0.961476, train/loss=0.145558, validation/accuracy=0.755660, validation/loss=1.054035, validation/num_examples=50000
I0301 10:34:23.024899 140089862604544 logging_writer.py:48] [304900] global_step=304900, grad_norm=4.690578460693359, loss=0.611200213432312
I0301 10:34:56.509363 140089845819136 logging_writer.py:48] [305000] global_step=305000, grad_norm=4.721370220184326, loss=0.6660887002944946
I0301 10:35:29.961610 140089862604544 logging_writer.py:48] [305100] global_step=305100, grad_norm=4.795094966888428, loss=0.6236299872398376
I0301 10:36:03.440490 140089845819136 logging_writer.py:48] [305200] global_step=305200, grad_norm=4.94084358215332, loss=0.6728689074516296
I0301 10:36:36.921890 140089862604544 logging_writer.py:48] [305300] global_step=305300, grad_norm=4.433143138885498, loss=0.584900975227356
I0301 10:37:10.481130 140089845819136 logging_writer.py:48] [305400] global_step=305400, grad_norm=4.214422702789307, loss=0.5963091254234314
I0301 10:37:43.992478 140089862604544 logging_writer.py:48] [305500] global_step=305500, grad_norm=5.037545204162598, loss=0.6506547927856445
I0301 10:38:17.458425 140089845819136 logging_writer.py:48] [305600] global_step=305600, grad_norm=4.286125659942627, loss=0.5957732796669006
I0301 10:38:50.927005 140089862604544 logging_writer.py:48] [305700] global_step=305700, grad_norm=4.34298849105835, loss=0.6007328033447266
I0301 10:39:24.413659 140089845819136 logging_writer.py:48] [305800] global_step=305800, grad_norm=4.777757167816162, loss=0.6504348516464233
I0301 10:39:57.907185 140089862604544 logging_writer.py:48] [305900] global_step=305900, grad_norm=4.488533973693848, loss=0.5806406140327454
I0301 10:40:31.346647 140089845819136 logging_writer.py:48] [306000] global_step=306000, grad_norm=4.357891082763672, loss=0.5864668488502502
I0301 10:41:04.849551 140089862604544 logging_writer.py:48] [306100] global_step=306100, grad_norm=4.424196243286133, loss=0.608778715133667
I0301 10:41:38.314562 140089845819136 logging_writer.py:48] [306200] global_step=306200, grad_norm=4.760282516479492, loss=0.5954231023788452
I0301 10:42:11.786977 140089862604544 logging_writer.py:48] [306300] global_step=306300, grad_norm=4.323544502258301, loss=0.583483874797821
I0301 10:42:45.372323 140089845819136 logging_writer.py:48] [306400] global_step=306400, grad_norm=4.5102152824401855, loss=0.6721194982528687
I0301 10:42:49.531415 140252611495744 spec.py:321] Evaluating on the training split.
I0301 10:42:55.661824 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 10:43:03.947563 140252611495744 spec.py:349] Evaluating on the test split.
I0301 10:43:06.214522 140252611495744 submission_runner.py:411] Time since start: 106127.09s, 	Step: 306414, 	{'train/accuracy': 0.9628507494926453, 'train/loss': 0.14309334754943848, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0519496202468872, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.841051697731018, 'test/num_examples': 10000, 'score': 102573.88640189171, 'total_duration': 106127.09458732605, 'accumulated_submission_time': 102573.88640189171, 'accumulated_eval_time': 3530.9448194503784, 'accumulated_logging_time': 11.271536588668823}
I0301 10:43:06.285722 140089770305280 logging_writer.py:48] [306414] accumulated_eval_time=3530.944819, accumulated_logging_time=11.271537, accumulated_submission_time=102573.886402, global_step=306414, preemption_count=0, score=102573.886402, test/accuracy=0.626800, test/loss=1.841052, test/num_examples=10000, total_duration=106127.094587, train/accuracy=0.962851, train/loss=0.143093, validation/accuracy=0.755780, validation/loss=1.051950, validation/num_examples=50000
I0301 10:43:35.400686 140089778697984 logging_writer.py:48] [306500] global_step=306500, grad_norm=4.565325736999512, loss=0.670593798160553
I0301 10:44:08.949961 140089770305280 logging_writer.py:48] [306600] global_step=306600, grad_norm=4.861050605773926, loss=0.6398922801017761
I0301 10:44:42.388103 140089778697984 logging_writer.py:48] [306700] global_step=306700, grad_norm=4.549483299255371, loss=0.5917090177536011
I0301 10:45:15.847344 140089770305280 logging_writer.py:48] [306800] global_step=306800, grad_norm=5.368992805480957, loss=0.62943434715271
I0301 10:45:49.364747 140089778697984 logging_writer.py:48] [306900] global_step=306900, grad_norm=4.464684963226318, loss=0.561660647392273
I0301 10:46:22.854413 140089770305280 logging_writer.py:48] [307000] global_step=307000, grad_norm=4.710569858551025, loss=0.6179265379905701
I0301 10:46:56.382647 140089778697984 logging_writer.py:48] [307100] global_step=307100, grad_norm=4.560654163360596, loss=0.5123524069786072
I0301 10:47:29.859359 140089770305280 logging_writer.py:48] [307200] global_step=307200, grad_norm=4.208527565002441, loss=0.5409092307090759
I0301 10:48:03.298916 140089778697984 logging_writer.py:48] [307300] global_step=307300, grad_norm=4.09606409072876, loss=0.6156580448150635
I0301 10:48:36.741185 140089770305280 logging_writer.py:48] [307400] global_step=307400, grad_norm=4.442331790924072, loss=0.576596200466156
I0301 10:49:10.421210 140089778697984 logging_writer.py:48] [307500] global_step=307500, grad_norm=4.972792625427246, loss=0.612484335899353
I0301 10:49:43.916028 140089770305280 logging_writer.py:48] [307600] global_step=307600, grad_norm=4.4074225425720215, loss=0.6491708755493164
I0301 10:50:17.362430 140089778697984 logging_writer.py:48] [307700] global_step=307700, grad_norm=4.392489910125732, loss=0.6279054284095764
I0301 10:50:50.802697 140089770305280 logging_writer.py:48] [307800] global_step=307800, grad_norm=4.948160171508789, loss=0.6796725988388062
I0301 10:51:24.302613 140089778697984 logging_writer.py:48] [307900] global_step=307900, grad_norm=4.832233905792236, loss=0.648582935333252
I0301 10:51:36.510413 140252611495744 spec.py:321] Evaluating on the training split.
I0301 10:51:42.633723 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 10:51:50.732889 140252611495744 spec.py:349] Evaluating on the test split.
I0301 10:51:53.299969 140252611495744 submission_runner.py:411] Time since start: 106654.18s, 	Step: 307938, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.1455347239971161, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.0540136098861694, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.842125654220581, 'test/num_examples': 10000, 'score': 103084.04380178452, 'total_duration': 106654.18004608154, 'accumulated_submission_time': 103084.04380178452, 'accumulated_eval_time': 3547.7343316078186, 'accumulated_logging_time': 11.354299306869507}
I0301 10:51:53.354295 140089862604544 logging_writer.py:48] [307938] accumulated_eval_time=3547.734332, accumulated_logging_time=11.354299, accumulated_submission_time=103084.043802, global_step=307938, preemption_count=0, score=103084.043802, test/accuracy=0.626600, test/loss=1.842126, test/num_examples=10000, total_duration=106654.180046, train/accuracy=0.961854, train/loss=0.145535, validation/accuracy=0.756260, validation/loss=1.054014, validation/num_examples=50000
I0301 10:52:14.441854 140089870997248 logging_writer.py:48] [308000] global_step=308000, grad_norm=4.335395812988281, loss=0.6447347402572632
I0301 10:52:47.955707 140089862604544 logging_writer.py:48] [308100] global_step=308100, grad_norm=4.399455547332764, loss=0.6535061001777649
I0301 10:53:21.411473 140089870997248 logging_writer.py:48] [308200] global_step=308200, grad_norm=4.507837772369385, loss=0.6155778169631958
I0301 10:53:54.873521 140089862604544 logging_writer.py:48] [308300] global_step=308300, grad_norm=4.384885787963867, loss=0.5984193086624146
I0301 10:54:28.356496 140089870997248 logging_writer.py:48] [308400] global_step=308400, grad_norm=4.891445159912109, loss=0.6707673072814941
I0301 10:55:01.871485 140089862604544 logging_writer.py:48] [308500] global_step=308500, grad_norm=4.46910285949707, loss=0.6195412278175354
I0301 10:55:35.438183 140089870997248 logging_writer.py:48] [308600] global_step=308600, grad_norm=4.346346855163574, loss=0.6727273464202881
I0301 10:56:08.901473 140089862604544 logging_writer.py:48] [308700] global_step=308700, grad_norm=4.366719722747803, loss=0.6044877171516418
I0301 10:56:42.387336 140089870997248 logging_writer.py:48] [308800] global_step=308800, grad_norm=4.2704877853393555, loss=0.618911623954773
I0301 10:57:15.888658 140089862604544 logging_writer.py:48] [308900] global_step=308900, grad_norm=4.730152606964111, loss=0.7318490743637085
I0301 10:57:49.348838 140089870997248 logging_writer.py:48] [309000] global_step=309000, grad_norm=4.252220630645752, loss=0.6073298454284668
I0301 10:58:22.845584 140089862604544 logging_writer.py:48] [309100] global_step=309100, grad_norm=4.753105640411377, loss=0.5789436101913452
I0301 10:58:56.324614 140089870997248 logging_writer.py:48] [309200] global_step=309200, grad_norm=4.4920501708984375, loss=0.5937167406082153
I0301 10:59:29.811622 140089862604544 logging_writer.py:48] [309300] global_step=309300, grad_norm=4.999979496002197, loss=0.6418490409851074
I0301 11:00:03.285035 140089870997248 logging_writer.py:48] [309400] global_step=309400, grad_norm=4.983781814575195, loss=0.6826397776603699
I0301 11:00:23.538421 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:00:30.484218 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:00:38.557885 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:00:40.852186 140252611495744 submission_runner.py:411] Time since start: 107181.73s, 	Step: 309462, 	{'train/accuracy': 0.9624919891357422, 'train/loss': 0.1417141854763031, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0533521175384521, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8425387144088745, 'test/num_examples': 10000, 'score': 103594.16372561455, 'total_duration': 107181.73225021362, 'accumulated_submission_time': 103594.16372561455, 'accumulated_eval_time': 3565.0480420589447, 'accumulated_logging_time': 11.417406558990479}
I0301 11:00:40.921070 140089837426432 logging_writer.py:48] [309462] accumulated_eval_time=3565.048042, accumulated_logging_time=11.417407, accumulated_submission_time=103594.163726, global_step=309462, preemption_count=0, score=103594.163726, test/accuracy=0.627200, test/loss=1.842539, test/num_examples=10000, total_duration=107181.732250, train/accuracy=0.962492, train/loss=0.141714, validation/accuracy=0.755740, validation/loss=1.053352, validation/num_examples=50000
I0301 11:00:54.021789 140089845819136 logging_writer.py:48] [309500] global_step=309500, grad_norm=4.541114330291748, loss=0.6682988405227661
I0301 11:01:27.601186 140089837426432 logging_writer.py:48] [309600] global_step=309600, grad_norm=4.828741073608398, loss=0.5715203285217285
I0301 11:02:01.079165 140089845819136 logging_writer.py:48] [309700] global_step=309700, grad_norm=4.3458051681518555, loss=0.5930185317993164
I0301 11:02:34.586102 140089837426432 logging_writer.py:48] [309800] global_step=309800, grad_norm=5.006515979766846, loss=0.6385778188705444
I0301 11:03:08.150679 140089845819136 logging_writer.py:48] [309900] global_step=309900, grad_norm=5.517788887023926, loss=0.6925848722457886
I0301 11:03:41.616652 140089837426432 logging_writer.py:48] [310000] global_step=310000, grad_norm=4.559243679046631, loss=0.6032662391662598
I0301 11:04:15.203304 140089845819136 logging_writer.py:48] [310100] global_step=310100, grad_norm=4.09222936630249, loss=0.6106488108634949
I0301 11:04:48.762096 140089837426432 logging_writer.py:48] [310200] global_step=310200, grad_norm=4.380008697509766, loss=0.6032325625419617
I0301 11:05:22.265607 140089845819136 logging_writer.py:48] [310300] global_step=310300, grad_norm=4.474776744842529, loss=0.6322150826454163
I0301 11:05:55.815105 140089837426432 logging_writer.py:48] [310400] global_step=310400, grad_norm=4.477676868438721, loss=0.6701954007148743
I0301 11:06:29.313025 140089845819136 logging_writer.py:48] [310500] global_step=310500, grad_norm=4.082840919494629, loss=0.6196064352989197
I0301 11:07:02.855280 140089837426432 logging_writer.py:48] [310600] global_step=310600, grad_norm=4.350035190582275, loss=0.6292731761932373
I0301 11:07:36.380321 140089845819136 logging_writer.py:48] [310700] global_step=310700, grad_norm=4.664952754974365, loss=0.6865925788879395
I0301 11:08:09.879256 140089837426432 logging_writer.py:48] [310800] global_step=310800, grad_norm=4.2135114669799805, loss=0.557503342628479
I0301 11:08:43.352410 140089845819136 logging_writer.py:48] [310900] global_step=310900, grad_norm=4.3301167488098145, loss=0.5910961627960205
I0301 11:09:11.001892 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:09:17.225920 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:09:25.329385 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:09:27.659154 140252611495744 submission_runner.py:411] Time since start: 107708.54s, 	Step: 310984, 	{'train/accuracy': 0.9591238498687744, 'train/loss': 0.14968757331371307, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0534484386444092, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.842854619026184, 'test/num_examples': 10000, 'score': 104104.17977261543, 'total_duration': 107708.53921103477, 'accumulated_submission_time': 104104.17977261543, 'accumulated_eval_time': 3581.7052421569824, 'accumulated_logging_time': 11.497244358062744}
I0301 11:09:27.723378 140089770305280 logging_writer.py:48] [310984] accumulated_eval_time=3581.705242, accumulated_logging_time=11.497244, accumulated_submission_time=104104.179773, global_step=310984, preemption_count=0, score=104104.179773, test/accuracy=0.627600, test/loss=1.842855, test/num_examples=10000, total_duration=107708.539211, train/accuracy=0.959124, train/loss=0.149688, validation/accuracy=0.755920, validation/loss=1.053448, validation/num_examples=50000
I0301 11:09:33.421397 140089778697984 logging_writer.py:48] [311000] global_step=311000, grad_norm=4.350615978240967, loss=0.6995975375175476
I0301 11:10:06.866755 140089770305280 logging_writer.py:48] [311100] global_step=311100, grad_norm=4.818698406219482, loss=0.5768711566925049
I0301 11:10:40.323803 140089778697984 logging_writer.py:48] [311200] global_step=311200, grad_norm=4.834100723266602, loss=0.6369562149047852
I0301 11:11:13.822511 140089770305280 logging_writer.py:48] [311300] global_step=311300, grad_norm=4.416625499725342, loss=0.6310604810714722
I0301 11:11:47.321756 140089778697984 logging_writer.py:48] [311400] global_step=311400, grad_norm=4.398015975952148, loss=0.5951159000396729
I0301 11:12:20.898208 140089770305280 logging_writer.py:48] [311500] global_step=311500, grad_norm=4.893080711364746, loss=0.6887593269348145
I0301 11:12:54.400111 140089778697984 logging_writer.py:48] [311600] global_step=311600, grad_norm=4.691169738769531, loss=0.641301691532135
I0301 11:13:27.980462 140089770305280 logging_writer.py:48] [311700] global_step=311700, grad_norm=4.515580177307129, loss=0.5954428315162659
I0301 11:14:01.461236 140089778697984 logging_writer.py:48] [311800] global_step=311800, grad_norm=4.940367221832275, loss=0.6905408501625061
I0301 11:14:34.984426 140089770305280 logging_writer.py:48] [311900] global_step=311900, grad_norm=4.455264568328857, loss=0.5955781936645508
I0301 11:15:08.461143 140089778697984 logging_writer.py:48] [312000] global_step=312000, grad_norm=5.162137508392334, loss=0.6188644766807556
I0301 11:15:41.912511 140089770305280 logging_writer.py:48] [312100] global_step=312100, grad_norm=4.855118274688721, loss=0.6240226030349731
I0301 11:16:15.391344 140089778697984 logging_writer.py:48] [312200] global_step=312200, grad_norm=4.792494297027588, loss=0.7008072137832642
I0301 11:16:48.901000 140089770305280 logging_writer.py:48] [312300] global_step=312300, grad_norm=4.8399248123168945, loss=0.6730186939239502
I0301 11:17:22.400977 140089778697984 logging_writer.py:48] [312400] global_step=312400, grad_norm=4.600677490234375, loss=0.6551966667175293
I0301 11:17:55.885632 140089770305280 logging_writer.py:48] [312500] global_step=312500, grad_norm=4.07193660736084, loss=0.5953741073608398
I0301 11:17:57.703022 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:18:03.953662 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:18:12.106033 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:18:14.467089 140252611495744 submission_runner.py:411] Time since start: 108235.35s, 	Step: 312507, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.1447564959526062, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.053959846496582, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8433014154434204, 'test/num_examples': 10000, 'score': 104614.0937845707, 'total_duration': 108235.34715676308, 'accumulated_submission_time': 104614.0937845707, 'accumulated_eval_time': 3598.4693970680237, 'accumulated_logging_time': 11.572259664535522}
I0301 11:18:14.537921 140089854211840 logging_writer.py:48] [312507] accumulated_eval_time=3598.469397, accumulated_logging_time=11.572260, accumulated_submission_time=104614.093785, global_step=312507, preemption_count=0, score=104614.093785, test/accuracy=0.627900, test/loss=1.843301, test/num_examples=10000, total_duration=108235.347157, train/accuracy=0.961834, train/loss=0.144756, validation/accuracy=0.755880, validation/loss=1.053960, validation/num_examples=50000
I0301 11:18:46.023764 140089870997248 logging_writer.py:48] [312600] global_step=312600, grad_norm=4.289653778076172, loss=0.6007257103919983
I0301 11:19:19.559801 140089854211840 logging_writer.py:48] [312700] global_step=312700, grad_norm=4.775351524353027, loss=0.6292400360107422
I0301 11:19:53.083733 140089870997248 logging_writer.py:48] [312800] global_step=312800, grad_norm=4.353915691375732, loss=0.6293395757675171
I0301 11:20:26.545165 140089854211840 logging_writer.py:48] [312900] global_step=312900, grad_norm=4.2583909034729, loss=0.5960641503334045
I0301 11:21:00.000112 140089870997248 logging_writer.py:48] [313000] global_step=313000, grad_norm=4.342663764953613, loss=0.5512294173240662
I0301 11:21:33.500416 140089854211840 logging_writer.py:48] [313100] global_step=313100, grad_norm=5.063211917877197, loss=0.6534156799316406
I0301 11:22:07.021165 140089870997248 logging_writer.py:48] [313200] global_step=313200, grad_norm=4.553078651428223, loss=0.5809277296066284
I0301 11:22:40.599325 140089854211840 logging_writer.py:48] [313300] global_step=313300, grad_norm=4.880792617797852, loss=0.7374012470245361
I0301 11:23:14.064152 140089870997248 logging_writer.py:48] [313400] global_step=313400, grad_norm=4.553599834442139, loss=0.6087148189544678
I0301 11:23:47.608052 140089854211840 logging_writer.py:48] [313500] global_step=313500, grad_norm=4.399895191192627, loss=0.5985673666000366
I0301 11:24:21.125604 140089870997248 logging_writer.py:48] [313600] global_step=313600, grad_norm=4.857041835784912, loss=0.6251276731491089
I0301 11:24:54.632622 140089854211840 logging_writer.py:48] [313700] global_step=313700, grad_norm=4.356521129608154, loss=0.606198251247406
I0301 11:25:28.214534 140089870997248 logging_writer.py:48] [313800] global_step=313800, grad_norm=4.064583778381348, loss=0.5787135362625122
I0301 11:26:01.700287 140089854211840 logging_writer.py:48] [313900] global_step=313900, grad_norm=4.187789440155029, loss=0.5754881501197815
I0301 11:26:35.244102 140089870997248 logging_writer.py:48] [314000] global_step=314000, grad_norm=4.364266395568848, loss=0.5906649827957153
I0301 11:26:44.468411 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:26:50.683635 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:26:58.758527 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:27:01.085366 140252611495744 submission_runner.py:411] Time since start: 108761.97s, 	Step: 314029, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14271236956119537, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0546585321426392, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8429999351501465, 'test/num_examples': 10000, 'score': 105123.96016263962, 'total_duration': 108761.96542572975, 'accumulated_submission_time': 105123.96016263962, 'accumulated_eval_time': 3615.0863020420074, 'accumulated_logging_time': 11.65283203125}
I0301 11:27:01.154110 140089837426432 logging_writer.py:48] [314029] accumulated_eval_time=3615.086302, accumulated_logging_time=11.652832, accumulated_submission_time=105123.960163, global_step=314029, preemption_count=0, score=105123.960163, test/accuracy=0.627200, test/loss=1.843000, test/num_examples=10000, total_duration=108761.965426, train/accuracy=0.961456, train/loss=0.142712, validation/accuracy=0.755800, validation/loss=1.054659, validation/num_examples=50000
I0301 11:27:25.239348 140089845819136 logging_writer.py:48] [314100] global_step=314100, grad_norm=5.028558254241943, loss=0.6660447120666504
I0301 11:27:58.723023 140089837426432 logging_writer.py:48] [314200] global_step=314200, grad_norm=4.398440361022949, loss=0.6037797927856445
I0301 11:28:32.222051 140089845819136 logging_writer.py:48] [314300] global_step=314300, grad_norm=4.845752239227295, loss=0.6395280361175537
I0301 11:29:05.675302 140089837426432 logging_writer.py:48] [314400] global_step=314400, grad_norm=4.94563102722168, loss=0.6728357076644897
I0301 11:29:39.126528 140089845819136 logging_writer.py:48] [314500] global_step=314500, grad_norm=4.563633918762207, loss=0.631759762763977
I0301 11:30:12.593916 140089837426432 logging_writer.py:48] [314600] global_step=314600, grad_norm=4.97509765625, loss=0.7486346364021301
I0301 11:30:46.062292 140089845819136 logging_writer.py:48] [314700] global_step=314700, grad_norm=4.321847438812256, loss=0.6019057631492615
I0301 11:31:19.531416 140089837426432 logging_writer.py:48] [314800] global_step=314800, grad_norm=5.055662631988525, loss=0.6513389348983765
I0301 11:31:53.218408 140089845819136 logging_writer.py:48] [314900] global_step=314900, grad_norm=4.350867748260498, loss=0.6392043232917786
I0301 11:32:26.660480 140089837426432 logging_writer.py:48] [315000] global_step=315000, grad_norm=5.053450107574463, loss=0.6621019840240479
I0301 11:33:00.108794 140089845819136 logging_writer.py:48] [315100] global_step=315100, grad_norm=4.380703449249268, loss=0.6260228157043457
I0301 11:33:33.610372 140089837426432 logging_writer.py:48] [315200] global_step=315200, grad_norm=4.206508159637451, loss=0.5865609049797058
I0301 11:34:07.062225 140089845819136 logging_writer.py:48] [315300] global_step=315300, grad_norm=4.661346435546875, loss=0.6539202332496643
I0301 11:34:40.521475 140089837426432 logging_writer.py:48] [315400] global_step=315400, grad_norm=4.410764217376709, loss=0.5836852788925171
I0301 11:35:14.002025 140089845819136 logging_writer.py:48] [315500] global_step=315500, grad_norm=5.4900007247924805, loss=0.6851632595062256
I0301 11:35:31.241085 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:35:37.370214 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:35:45.527296 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:35:47.768207 140252611495744 submission_runner.py:411] Time since start: 109288.65s, 	Step: 315553, 	{'train/accuracy': 0.9622528553009033, 'train/loss': 0.1424500048160553, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0530117750167847, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8422795534133911, 'test/num_examples': 10000, 'score': 105633.98216438293, 'total_duration': 109288.64826965332, 'accumulated_submission_time': 105633.98216438293, 'accumulated_eval_time': 3631.61336350441, 'accumulated_logging_time': 11.731301069259644}
I0301 11:35:47.847865 140089770305280 logging_writer.py:48] [315553] accumulated_eval_time=3631.613364, accumulated_logging_time=11.731301, accumulated_submission_time=105633.982164, global_step=315553, preemption_count=0, score=105633.982164, test/accuracy=0.626800, test/loss=1.842280, test/num_examples=10000, total_duration=109288.648270, train/accuracy=0.962253, train/loss=0.142450, validation/accuracy=0.756140, validation/loss=1.053012, validation/num_examples=50000
I0301 11:36:03.909884 140089778697984 logging_writer.py:48] [315600] global_step=315600, grad_norm=4.600593090057373, loss=0.6746215224266052
I0301 11:36:37.397833 140089770305280 logging_writer.py:48] [315700] global_step=315700, grad_norm=4.712123870849609, loss=0.6130856275558472
I0301 11:37:10.863828 140089778697984 logging_writer.py:48] [315800] global_step=315800, grad_norm=4.305678844451904, loss=0.5828407406806946
I0301 11:37:44.409888 140089770305280 logging_writer.py:48] [315900] global_step=315900, grad_norm=4.695526599884033, loss=0.6757398843765259
I0301 11:38:17.888647 140089778697984 logging_writer.py:48] [316000] global_step=316000, grad_norm=4.312469005584717, loss=0.6032154560089111
I0301 11:38:51.370916 140089770305280 logging_writer.py:48] [316100] global_step=316100, grad_norm=4.561586380004883, loss=0.6258096694946289
I0301 11:39:24.834698 140089778697984 logging_writer.py:48] [316200] global_step=316200, grad_norm=4.054934978485107, loss=0.6262888312339783
I0301 11:39:58.293685 140089770305280 logging_writer.py:48] [316300] global_step=316300, grad_norm=4.377284526824951, loss=0.6280025839805603
I0301 11:40:31.772764 140089778697984 logging_writer.py:48] [316400] global_step=316400, grad_norm=5.1508989334106445, loss=0.6135536432266235
I0301 11:41:05.275586 140089770305280 logging_writer.py:48] [316500] global_step=316500, grad_norm=4.749467372894287, loss=0.6366347670555115
I0301 11:41:38.725236 140089778697984 logging_writer.py:48] [316600] global_step=316600, grad_norm=4.50469970703125, loss=0.6686999201774597
I0301 11:42:12.185444 140089770305280 logging_writer.py:48] [316700] global_step=316700, grad_norm=4.49967098236084, loss=0.6220354437828064
I0301 11:42:45.670947 140089778697984 logging_writer.py:48] [316800] global_step=316800, grad_norm=4.973973274230957, loss=0.6919140219688416
I0301 11:43:19.132198 140089770305280 logging_writer.py:48] [316900] global_step=316900, grad_norm=4.716784954071045, loss=0.666874885559082
I0301 11:43:52.696471 140089778697984 logging_writer.py:48] [317000] global_step=317000, grad_norm=4.668118953704834, loss=0.6466129422187805
I0301 11:44:17.900635 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:44:23.979239 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:44:32.148181 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:44:34.452166 140252611495744 submission_runner.py:411] Time since start: 109815.33s, 	Step: 317077, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14580152928829193, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0532641410827637, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8440029621124268, 'test/num_examples': 10000, 'score': 106143.96684598923, 'total_duration': 109815.33223104477, 'accumulated_submission_time': 106143.96684598923, 'accumulated_eval_time': 3648.1648371219635, 'accumulated_logging_time': 11.824159622192383}
I0301 11:44:34.521100 140089870997248 logging_writer.py:48] [317077] accumulated_eval_time=3648.164837, accumulated_logging_time=11.824160, accumulated_submission_time=106143.966846, global_step=317077, preemption_count=0, score=106143.966846, test/accuracy=0.627700, test/loss=1.844003, test/num_examples=10000, total_duration=109815.332231, train/accuracy=0.961276, train/loss=0.145802, validation/accuracy=0.756040, validation/loss=1.053264, validation/num_examples=50000
I0301 11:44:42.571153 140089879389952 logging_writer.py:48] [317100] global_step=317100, grad_norm=4.375871658325195, loss=0.6511392593383789
I0301 11:45:16.033745 140089870997248 logging_writer.py:48] [317200] global_step=317200, grad_norm=4.785661220550537, loss=0.5913160443305969
I0301 11:45:49.613564 140089879389952 logging_writer.py:48] [317300] global_step=317300, grad_norm=4.102146148681641, loss=0.5772565603256226
I0301 11:46:23.097221 140089870997248 logging_writer.py:48] [317400] global_step=317400, grad_norm=4.386749267578125, loss=0.6102418899536133
I0301 11:46:56.604377 140089879389952 logging_writer.py:48] [317500] global_step=317500, grad_norm=4.828652858734131, loss=0.6287350654602051
I0301 11:47:30.039876 140089870997248 logging_writer.py:48] [317600] global_step=317600, grad_norm=4.633460521697998, loss=0.6201692223548889
I0301 11:48:03.498552 140089879389952 logging_writer.py:48] [317700] global_step=317700, grad_norm=4.946901321411133, loss=0.6858464479446411
I0301 11:48:36.936062 140089870997248 logging_writer.py:48] [317800] global_step=317800, grad_norm=4.517637252807617, loss=0.6409980654716492
I0301 11:49:10.404882 140089879389952 logging_writer.py:48] [317900] global_step=317900, grad_norm=4.232626914978027, loss=0.6417590975761414
I0301 11:49:43.943979 140089870997248 logging_writer.py:48] [318000] global_step=318000, grad_norm=4.349199295043945, loss=0.5620633959770203
I0301 11:50:17.470584 140089879389952 logging_writer.py:48] [318100] global_step=318100, grad_norm=4.4460625648498535, loss=0.5960232019424438
I0301 11:50:50.975360 140089870997248 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.119967460632324, loss=0.595546305179596
I0301 11:51:24.481929 140089879389952 logging_writer.py:48] [318300] global_step=318300, grad_norm=4.847421169281006, loss=0.725603461265564
I0301 11:51:57.932713 140089870997248 logging_writer.py:48] [318400] global_step=318400, grad_norm=4.268939971923828, loss=0.5900968313217163
I0301 11:52:31.369582 140089879389952 logging_writer.py:48] [318500] global_step=318500, grad_norm=4.678149223327637, loss=0.6360325813293457
I0301 11:53:04.839371 140089870997248 logging_writer.py:48] [318600] global_step=318600, grad_norm=4.375606536865234, loss=0.6314483284950256
I0301 11:53:04.846212 140252611495744 spec.py:321] Evaluating on the training split.
I0301 11:53:10.966382 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 11:53:19.071660 140252611495744 spec.py:349] Evaluating on the test split.
I0301 11:53:21.337691 140252611495744 submission_runner.py:411] Time since start: 110342.22s, 	Step: 318601, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14599236845970154, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0537469387054443, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8443596363067627, 'test/num_examples': 10000, 'score': 106654.22787618637, 'total_duration': 110342.21775627136, 'accumulated_submission_time': 106654.22787618637, 'accumulated_eval_time': 3664.6562349796295, 'accumulated_logging_time': 11.90298581123352}
I0301 11:53:21.410553 140089770305280 logging_writer.py:48] [318601] accumulated_eval_time=3664.656235, accumulated_logging_time=11.902986, accumulated_submission_time=106654.227876, global_step=318601, preemption_count=0, score=106654.227876, test/accuracy=0.627600, test/loss=1.844360, test/num_examples=10000, total_duration=110342.217756, train/accuracy=0.961017, train/loss=0.145992, validation/accuracy=0.755880, validation/loss=1.053747, validation/num_examples=50000
I0301 11:53:54.899658 140089778697984 logging_writer.py:48] [318700] global_step=318700, grad_norm=5.62026309967041, loss=0.6657693982124329
I0301 11:54:28.323348 140089770305280 logging_writer.py:48] [318800] global_step=318800, grad_norm=4.704751968383789, loss=0.5849332213401794
I0301 11:55:01.846033 140089778697984 logging_writer.py:48] [318900] global_step=318900, grad_norm=4.239089488983154, loss=0.5878793001174927
I0301 11:55:35.289672 140089770305280 logging_writer.py:48] [319000] global_step=319000, grad_norm=4.358485221862793, loss=0.6427773833274841
I0301 11:56:08.856959 140089778697984 logging_writer.py:48] [319100] global_step=319100, grad_norm=4.355742931365967, loss=0.6525633931159973
I0301 11:56:42.324060 140089770305280 logging_writer.py:48] [319200] global_step=319200, grad_norm=4.3743815422058105, loss=0.5992458462715149
I0301 11:57:15.783437 140089778697984 logging_writer.py:48] [319300] global_step=319300, grad_norm=4.448309898376465, loss=0.6034405827522278
I0301 11:57:49.259603 140089770305280 logging_writer.py:48] [319400] global_step=319400, grad_norm=4.413855075836182, loss=0.6328085660934448
I0301 11:58:22.721828 140089778697984 logging_writer.py:48] [319500] global_step=319500, grad_norm=4.252691268920898, loss=0.623484194278717
I0301 11:58:56.157495 140089770305280 logging_writer.py:48] [319600] global_step=319600, grad_norm=4.287504196166992, loss=0.6358484029769897
I0301 11:59:29.636015 140089778697984 logging_writer.py:48] [319700] global_step=319700, grad_norm=4.322901248931885, loss=0.604461133480072
I0301 12:00:03.140444 140089770305280 logging_writer.py:48] [319800] global_step=319800, grad_norm=4.404059410095215, loss=0.6401227116584778
I0301 12:00:36.584386 140089778697984 logging_writer.py:48] [319900] global_step=319900, grad_norm=4.266972064971924, loss=0.6496276259422302
I0301 12:01:10.063323 140089770305280 logging_writer.py:48] [320000] global_step=320000, grad_norm=4.973948001861572, loss=0.6872222423553467
I0301 12:01:43.685119 140089778697984 logging_writer.py:48] [320100] global_step=320100, grad_norm=4.238898277282715, loss=0.5952737331390381
I0301 12:01:51.530761 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:01:57.610197 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:02:05.932493 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:02:08.194380 140252611495744 submission_runner.py:411] Time since start: 110869.07s, 	Step: 320125, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.14677579700946808, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0519914627075195, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.839996337890625, 'test/num_examples': 10000, 'score': 107164.28194069862, 'total_duration': 110869.07444214821, 'accumulated_submission_time': 107164.28194069862, 'accumulated_eval_time': 3681.319794178009, 'accumulated_logging_time': 11.985785961151123}
I0301 12:02:08.264651 140089854211840 logging_writer.py:48] [320125] accumulated_eval_time=3681.319794, accumulated_logging_time=11.985786, accumulated_submission_time=107164.281941, global_step=320125, preemption_count=0, score=107164.281941, test/accuracy=0.627700, test/loss=1.839996, test/num_examples=10000, total_duration=110869.074442, train/accuracy=0.961376, train/loss=0.146776, validation/accuracy=0.756100, validation/loss=1.051991, validation/num_examples=50000
I0301 12:02:33.724783 140089862604544 logging_writer.py:48] [320200] global_step=320200, grad_norm=4.247872829437256, loss=0.5950430631637573
I0301 12:03:07.203027 140089854211840 logging_writer.py:48] [320300] global_step=320300, grad_norm=4.816982746124268, loss=0.6726336479187012
I0301 12:03:40.636836 140089862604544 logging_writer.py:48] [320400] global_step=320400, grad_norm=4.179502964019775, loss=0.5337597727775574
I0301 12:04:14.118643 140089854211840 logging_writer.py:48] [320500] global_step=320500, grad_norm=4.582137584686279, loss=0.6370075941085815
I0301 12:04:47.597773 140089862604544 logging_writer.py:48] [320600] global_step=320600, grad_norm=4.551484107971191, loss=0.5851761102676392
I0301 12:05:21.046497 140089854211840 logging_writer.py:48] [320700] global_step=320700, grad_norm=4.610870838165283, loss=0.6444015502929688
I0301 12:05:54.506908 140089862604544 logging_writer.py:48] [320800] global_step=320800, grad_norm=4.871103763580322, loss=0.5872687101364136
I0301 12:06:27.997949 140089854211840 logging_writer.py:48] [320900] global_step=320900, grad_norm=4.895142555236816, loss=0.6580830216407776
I0301 12:07:01.498451 140089862604544 logging_writer.py:48] [321000] global_step=321000, grad_norm=4.67966890335083, loss=0.6959395408630371
I0301 12:07:34.969619 140089854211840 logging_writer.py:48] [321100] global_step=321100, grad_norm=4.595118999481201, loss=0.6938191652297974
I0301 12:08:08.551244 140089862604544 logging_writer.py:48] [321200] global_step=321200, grad_norm=4.835795879364014, loss=0.6763361692428589
I0301 12:08:41.976697 140089854211840 logging_writer.py:48] [321300] global_step=321300, grad_norm=4.489444732666016, loss=0.652184247970581
I0301 12:09:15.461504 140089862604544 logging_writer.py:48] [321400] global_step=321400, grad_norm=4.419817924499512, loss=0.5944120287895203
I0301 12:09:48.889426 140089854211840 logging_writer.py:48] [321500] global_step=321500, grad_norm=4.494634628295898, loss=0.5977094173431396
I0301 12:10:24.871254 140089862604544 logging_writer.py:48] [321600] global_step=321600, grad_norm=4.634228229522705, loss=0.7414886355400085
I0301 12:10:38.280402 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:10:45.635674 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:10:54.039614 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:10:56.321261 140252611495744 submission_runner.py:411] Time since start: 111397.20s, 	Step: 321629, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14228476583957672, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0536316633224487, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8425594568252563, 'test/num_examples': 10000, 'score': 107674.23223781586, 'total_duration': 111397.20132136345, 'accumulated_submission_time': 107674.23223781586, 'accumulated_eval_time': 3699.360595703125, 'accumulated_logging_time': 12.066577196121216}
I0301 12:10:56.393772 140089837426432 logging_writer.py:48] [321629] accumulated_eval_time=3699.360596, accumulated_logging_time=12.066577, accumulated_submission_time=107674.232238, global_step=321629, preemption_count=0, score=107674.232238, test/accuracy=0.627300, test/loss=1.842559, test/num_examples=10000, total_duration=111397.201321, train/accuracy=0.961276, train/loss=0.142285, validation/accuracy=0.756040, validation/loss=1.053632, validation/num_examples=50000
I0301 12:11:20.449940 140089845819136 logging_writer.py:48] [321700] global_step=321700, grad_norm=4.411343097686768, loss=0.6396777629852295
I0301 12:11:53.977626 140089837426432 logging_writer.py:48] [321800] global_step=321800, grad_norm=4.25678825378418, loss=0.5952423214912415
I0301 12:12:27.497364 140089845819136 logging_writer.py:48] [321900] global_step=321900, grad_norm=4.564049243927002, loss=0.5989592671394348
I0301 12:13:00.995768 140089837426432 logging_writer.py:48] [322000] global_step=322000, grad_norm=4.942857265472412, loss=0.5672131776809692
I0301 12:13:34.520137 140089845819136 logging_writer.py:48] [322100] global_step=322100, grad_norm=4.843582630157471, loss=0.6284306645393372
I0301 12:14:08.122400 140089837426432 logging_writer.py:48] [322200] global_step=322200, grad_norm=4.265341758728027, loss=0.6000193357467651
I0301 12:14:41.589612 140089845819136 logging_writer.py:48] [322300] global_step=322300, grad_norm=4.601493835449219, loss=0.6371851563453674
I0301 12:15:15.045966 140089837426432 logging_writer.py:48] [322400] global_step=322400, grad_norm=4.724452495574951, loss=0.6289086937904358
I0301 12:15:48.499958 140089845819136 logging_writer.py:48] [322500] global_step=322500, grad_norm=4.484135627746582, loss=0.6749120354652405
I0301 12:16:21.988587 140089837426432 logging_writer.py:48] [322600] global_step=322600, grad_norm=4.530808448791504, loss=0.6572775840759277
I0301 12:16:55.472924 140089845819136 logging_writer.py:48] [322700] global_step=322700, grad_norm=4.630702018737793, loss=0.558469295501709
I0301 12:17:28.923916 140089837426432 logging_writer.py:48] [322800] global_step=322800, grad_norm=4.413728713989258, loss=0.5795810222625732
I0301 12:18:02.409895 140089845819136 logging_writer.py:48] [322900] global_step=322900, grad_norm=4.520963191986084, loss=0.645977258682251
I0301 12:18:35.877501 140089837426432 logging_writer.py:48] [323000] global_step=323000, grad_norm=4.694876670837402, loss=0.6269561052322388
I0301 12:19:09.384916 140089845819136 logging_writer.py:48] [323100] global_step=323100, grad_norm=4.5277419090271, loss=0.6275790929794312
I0301 12:19:26.526910 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:19:33.076801 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:19:41.551357 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:19:43.820802 140252611495744 submission_runner.py:411] Time since start: 111924.70s, 	Step: 323153, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.145976260304451, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0547045469284058, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.844212532043457, 'test/num_examples': 10000, 'score': 108184.30120563507, 'total_duration': 111924.70087218285, 'accumulated_submission_time': 108184.30120563507, 'accumulated_eval_time': 3716.6544332504272, 'accumulated_logging_time': 12.149267673492432}
I0301 12:19:43.893535 140089778697984 logging_writer.py:48] [323153] accumulated_eval_time=3716.654433, accumulated_logging_time=12.149268, accumulated_submission_time=108184.301206, global_step=323153, preemption_count=0, score=108184.301206, test/accuracy=0.627100, test/loss=1.844213, test/num_examples=10000, total_duration=111924.700872, train/accuracy=0.961037, train/loss=0.145976, validation/accuracy=0.755600, validation/loss=1.054705, validation/num_examples=50000
I0301 12:19:59.987321 140089854211840 logging_writer.py:48] [323200] global_step=323200, grad_norm=4.594904899597168, loss=0.6094837784767151
I0301 12:20:33.540738 140089778697984 logging_writer.py:48] [323300] global_step=323300, grad_norm=4.920886516571045, loss=0.6182875037193298
I0301 12:21:06.989205 140089854211840 logging_writer.py:48] [323400] global_step=323400, grad_norm=4.296161651611328, loss=0.5619535446166992
I0301 12:21:40.478729 140089778697984 logging_writer.py:48] [323500] global_step=323500, grad_norm=4.726416110992432, loss=0.7058961391448975
I0301 12:22:13.959052 140089854211840 logging_writer.py:48] [323600] global_step=323600, grad_norm=4.793889999389648, loss=0.5662298202514648
I0301 12:22:47.416076 140089778697984 logging_writer.py:48] [323700] global_step=323700, grad_norm=4.609317302703857, loss=0.5660513043403625
I0301 12:23:20.858418 140089854211840 logging_writer.py:48] [323800] global_step=323800, grad_norm=4.437305450439453, loss=0.6353970766067505
I0301 12:23:54.310786 140089778697984 logging_writer.py:48] [323900] global_step=323900, grad_norm=5.021483898162842, loss=0.7198304533958435
I0301 12:24:27.750313 140089854211840 logging_writer.py:48] [324000] global_step=324000, grad_norm=4.3069915771484375, loss=0.6197183132171631
I0301 12:25:01.213278 140089778697984 logging_writer.py:48] [324100] global_step=324100, grad_norm=4.180466651916504, loss=0.5279062390327454
I0301 12:25:34.652988 140089854211840 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.678522109985352, loss=0.6030354499816895
I0301 12:26:08.184992 140089778697984 logging_writer.py:48] [324300] global_step=324300, grad_norm=4.665175437927246, loss=0.6872466802597046
I0301 12:26:41.640459 140089854211840 logging_writer.py:48] [324400] global_step=324400, grad_norm=4.270482540130615, loss=0.5573908686637878
I0301 12:27:15.161058 140089778697984 logging_writer.py:48] [324500] global_step=324500, grad_norm=4.443782806396484, loss=0.5934659242630005
I0301 12:27:48.628028 140089854211840 logging_writer.py:48] [324600] global_step=324600, grad_norm=4.823934555053711, loss=0.6454311609268188
I0301 12:28:13.892342 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:28:20.338380 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:28:28.677275 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:28:30.942001 140252611495744 submission_runner.py:411] Time since start: 112451.82s, 	Step: 324677, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14724412560462952, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0521292686462402, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8409771919250488, 'test/num_examples': 10000, 'score': 108694.235871315, 'total_duration': 112451.8219499588, 'accumulated_submission_time': 108694.235871315, 'accumulated_eval_time': 3733.7039256095886, 'accumulated_logging_time': 12.232320070266724}
I0301 12:28:31.013949 140089770305280 logging_writer.py:48] [324677] accumulated_eval_time=3733.703926, accumulated_logging_time=12.232320, accumulated_submission_time=108694.235871, global_step=324677, preemption_count=0, score=108694.235871, test/accuracy=0.627000, test/loss=1.840977, test/num_examples=10000, total_duration=112451.821950, train/accuracy=0.960041, train/loss=0.147244, validation/accuracy=0.756120, validation/loss=1.052129, validation/num_examples=50000
I0301 12:28:39.041785 140089778697984 logging_writer.py:48] [324700] global_step=324700, grad_norm=5.133731842041016, loss=0.6395155191421509
I0301 12:29:12.506500 140089770305280 logging_writer.py:48] [324800] global_step=324800, grad_norm=5.429816722869873, loss=0.616972029209137
I0301 12:29:45.983351 140089778697984 logging_writer.py:48] [324900] global_step=324900, grad_norm=4.6816935539245605, loss=0.5719382762908936
I0301 12:30:19.468868 140089770305280 logging_writer.py:48] [325000] global_step=325000, grad_norm=4.717875957489014, loss=0.6496891975402832
I0301 12:30:52.917121 140089778697984 logging_writer.py:48] [325100] global_step=325100, grad_norm=4.604073524475098, loss=0.6858171820640564
I0301 12:31:26.353113 140089770305280 logging_writer.py:48] [325200] global_step=325200, grad_norm=4.312839031219482, loss=0.6348516345024109
I0301 12:31:59.808340 140089778697984 logging_writer.py:48] [325300] global_step=325300, grad_norm=4.327734470367432, loss=0.5938771963119507
I0301 12:32:33.375653 140089770305280 logging_writer.py:48] [325400] global_step=325400, grad_norm=4.16133451461792, loss=0.5851365923881531
I0301 12:33:06.816942 140089778697984 logging_writer.py:48] [325500] global_step=325500, grad_norm=4.84208345413208, loss=0.6796771883964539
I0301 12:33:40.267037 140089770305280 logging_writer.py:48] [325600] global_step=325600, grad_norm=4.578922748565674, loss=0.5675761699676514
I0301 12:34:13.765101 140089778697984 logging_writer.py:48] [325700] global_step=325700, grad_norm=4.614023685455322, loss=0.6836451292037964
I0301 12:34:47.245208 140089770305280 logging_writer.py:48] [325800] global_step=325800, grad_norm=4.042059421539307, loss=0.6131707429885864
I0301 12:35:20.693349 140089778697984 logging_writer.py:48] [325900] global_step=325900, grad_norm=4.39087438583374, loss=0.6072189211845398
I0301 12:35:54.180913 140089770305280 logging_writer.py:48] [326000] global_step=326000, grad_norm=4.419717788696289, loss=0.5601298213005066
I0301 12:36:27.638217 140089778697984 logging_writer.py:48] [326100] global_step=326100, grad_norm=4.5157036781311035, loss=0.5846167206764221
I0301 12:37:01.098043 140089770305280 logging_writer.py:48] [326200] global_step=326200, grad_norm=4.444359302520752, loss=0.5959997773170471
I0301 12:37:01.106158 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:37:07.457889 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:37:15.978136 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:37:18.277739 140252611495744 submission_runner.py:411] Time since start: 112979.16s, 	Step: 326201, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14570246636867523, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.053580641746521, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.8415600061416626, 'test/num_examples': 10000, 'score': 109204.26220989227, 'total_duration': 112979.1577911377, 'accumulated_submission_time': 109204.26220989227, 'accumulated_eval_time': 3750.8754110336304, 'accumulated_logging_time': 12.315529584884644}
I0301 12:37:18.350243 140089862604544 logging_writer.py:48] [326201] accumulated_eval_time=3750.875411, accumulated_logging_time=12.315530, accumulated_submission_time=109204.262210, global_step=326201, preemption_count=0, score=109204.262210, test/accuracy=0.628800, test/loss=1.841560, test/num_examples=10000, total_duration=112979.157791, train/accuracy=0.961256, train/loss=0.145702, validation/accuracy=0.755820, validation/loss=1.053581, validation/num_examples=50000
I0301 12:37:51.814307 140089870997248 logging_writer.py:48] [326300] global_step=326300, grad_norm=4.577042102813721, loss=0.6519662141799927
I0301 12:38:25.407549 140089862604544 logging_writer.py:48] [326400] global_step=326400, grad_norm=5.221065521240234, loss=0.6074681282043457
I0301 12:38:58.898556 140089870997248 logging_writer.py:48] [326500] global_step=326500, grad_norm=4.618803977966309, loss=0.5898751616477966
I0301 12:39:32.431354 140089862604544 logging_writer.py:48] [326600] global_step=326600, grad_norm=4.509489059448242, loss=0.6301748156547546
I0301 12:40:05.916571 140089870997248 logging_writer.py:48] [326700] global_step=326700, grad_norm=4.509052276611328, loss=0.5129379034042358
I0301 12:40:39.429923 140089862604544 logging_writer.py:48] [326800] global_step=326800, grad_norm=4.309885501861572, loss=0.6288774013519287
I0301 12:41:12.867007 140089870997248 logging_writer.py:48] [326900] global_step=326900, grad_norm=4.749467372894287, loss=0.6088107228279114
I0301 12:41:46.342840 140089862604544 logging_writer.py:48] [327000] global_step=327000, grad_norm=4.5116400718688965, loss=0.5978379249572754
I0301 12:42:19.820340 140089870997248 logging_writer.py:48] [327100] global_step=327100, grad_norm=4.784511089324951, loss=0.6201077699661255
I0301 12:42:53.350255 140089862604544 logging_writer.py:48] [327200] global_step=327200, grad_norm=5.078935623168945, loss=0.6581312417984009
I0301 12:43:26.779633 140089870997248 logging_writer.py:48] [327300] global_step=327300, grad_norm=4.4404520988464355, loss=0.5847780704498291
I0301 12:44:00.276468 140089862604544 logging_writer.py:48] [327400] global_step=327400, grad_norm=4.392024040222168, loss=0.6091964840888977
I0301 12:44:33.835407 140089870997248 logging_writer.py:48] [327500] global_step=327500, grad_norm=4.4018635749816895, loss=0.6141334772109985
I0301 12:45:07.310574 140089862604544 logging_writer.py:48] [327600] global_step=327600, grad_norm=4.513411045074463, loss=0.5601352453231812
I0301 12:45:40.786913 140089870997248 logging_writer.py:48] [327700] global_step=327700, grad_norm=4.157087802886963, loss=0.557205855846405
I0301 12:45:48.301799 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:45:54.473889 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:46:03.010314 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:46:05.325781 140252611495744 submission_runner.py:411] Time since start: 113506.21s, 	Step: 327724, 	{'train/accuracy': 0.962332546710968, 'train/loss': 0.14259448647499084, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0540307760238647, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.84445321559906, 'test/num_examples': 10000, 'score': 109714.14976787567, 'total_duration': 113506.20581841469, 'accumulated_submission_time': 109714.14976787567, 'accumulated_eval_time': 3767.8993055820465, 'accumulated_logging_time': 12.398417234420776}
I0301 12:46:05.407625 140089778697984 logging_writer.py:48] [327724] accumulated_eval_time=3767.899306, accumulated_logging_time=12.398417, accumulated_submission_time=109714.149768, global_step=327724, preemption_count=0, score=109714.149768, test/accuracy=0.627000, test/loss=1.844453, test/num_examples=10000, total_duration=113506.205818, train/accuracy=0.962333, train/loss=0.142594, validation/accuracy=0.755940, validation/loss=1.054031, validation/num_examples=50000
I0301 12:46:31.161057 140089837426432 logging_writer.py:48] [327800] global_step=327800, grad_norm=4.642550468444824, loss=0.6112751960754395
I0301 12:47:04.603495 140089778697984 logging_writer.py:48] [327900] global_step=327900, grad_norm=4.156904697418213, loss=0.6552156209945679
I0301 12:47:38.070201 140089837426432 logging_writer.py:48] [328000] global_step=328000, grad_norm=4.955567836761475, loss=0.6945162415504456
I0301 12:48:11.546297 140089778697984 logging_writer.py:48] [328100] global_step=328100, grad_norm=4.723596572875977, loss=0.6781241297721863
I0301 12:48:45.015365 140089837426432 logging_writer.py:48] [328200] global_step=328200, grad_norm=4.464625358581543, loss=0.6628302335739136
I0301 12:49:18.734465 140089778697984 logging_writer.py:48] [328300] global_step=328300, grad_norm=4.866150379180908, loss=0.6336719393730164
I0301 12:49:52.210909 140089837426432 logging_writer.py:48] [328400] global_step=328400, grad_norm=4.693678379058838, loss=0.6103439331054688
I0301 12:50:25.718863 140089778697984 logging_writer.py:48] [328500] global_step=328500, grad_norm=4.671815872192383, loss=0.6119422316551208
I0301 12:50:59.190640 140089837426432 logging_writer.py:48] [328600] global_step=328600, grad_norm=5.064628601074219, loss=0.6581188440322876
I0301 12:51:32.664667 140089778697984 logging_writer.py:48] [328700] global_step=328700, grad_norm=4.4910969734191895, loss=0.6485974788665771
I0301 12:52:06.120356 140089837426432 logging_writer.py:48] [328800] global_step=328800, grad_norm=4.133433818817139, loss=0.6110842227935791
I0301 12:52:39.582717 140089778697984 logging_writer.py:48] [328900] global_step=328900, grad_norm=4.045182228088379, loss=0.5902810096740723
I0301 12:53:13.032881 140089837426432 logging_writer.py:48] [329000] global_step=329000, grad_norm=4.587123870849609, loss=0.6512917280197144
I0301 12:53:46.471370 140089778697984 logging_writer.py:48] [329100] global_step=329100, grad_norm=4.469178676605225, loss=0.5715915560722351
I0301 12:54:19.954112 140089837426432 logging_writer.py:48] [329200] global_step=329200, grad_norm=4.9500813484191895, loss=0.6558606624603271
I0301 12:54:35.513247 140252611495744 spec.py:321] Evaluating on the training split.
I0301 12:54:41.711296 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 12:54:50.100318 140252611495744 spec.py:349] Evaluating on the test split.
I0301 12:54:52.406148 140252611495744 submission_runner.py:411] Time since start: 114033.29s, 	Step: 329248, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14657630026340485, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0538091659545898, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.841633915901184, 'test/num_examples': 10000, 'score': 110224.18912863731, 'total_duration': 114033.2861776352, 'accumulated_submission_time': 110224.18912863731, 'accumulated_eval_time': 3784.7921130657196, 'accumulated_logging_time': 12.491395235061646}
I0301 12:54:52.519634 140089879389952 logging_writer.py:48] [329248] accumulated_eval_time=3784.792113, accumulated_logging_time=12.491395, accumulated_submission_time=110224.189129, global_step=329248, preemption_count=0, score=110224.189129, test/accuracy=0.626500, test/loss=1.841634, test/num_examples=10000, total_duration=114033.286178, train/accuracy=0.961496, train/loss=0.146576, validation/accuracy=0.755920, validation/loss=1.053809, validation/num_examples=50000
I0301 12:55:10.283883 140089887782656 logging_writer.py:48] [329300] global_step=329300, grad_norm=4.721327304840088, loss=0.5775911808013916
I0301 12:55:43.769155 140089879389952 logging_writer.py:48] [329400] global_step=329400, grad_norm=3.940523624420166, loss=0.527202844619751
I0301 12:56:17.206590 140089887782656 logging_writer.py:48] [329500] global_step=329500, grad_norm=4.945653915405273, loss=0.6157236099243164
I0301 12:56:50.777775 140089879389952 logging_writer.py:48] [329600] global_step=329600, grad_norm=4.594815731048584, loss=0.6558469533920288
I0301 12:57:24.248259 140089887782656 logging_writer.py:48] [329700] global_step=329700, grad_norm=4.8359904289245605, loss=0.7160318493843079
I0301 12:57:57.736291 140089879389952 logging_writer.py:48] [329800] global_step=329800, grad_norm=4.534517765045166, loss=0.6584571599960327
I0301 12:58:31.246542 140089887782656 logging_writer.py:48] [329900] global_step=329900, grad_norm=4.497348308563232, loss=0.6466430425643921
I0301 12:59:04.673157 140089879389952 logging_writer.py:48] [330000] global_step=330000, grad_norm=4.5312347412109375, loss=0.6692017316818237
I0301 12:59:38.140209 140089887782656 logging_writer.py:48] [330100] global_step=330100, grad_norm=5.066451072692871, loss=0.6567004919052124
I0301 13:00:11.636242 140089879389952 logging_writer.py:48] [330200] global_step=330200, grad_norm=4.457487106323242, loss=0.6217041015625
I0301 13:00:45.092898 140089887782656 logging_writer.py:48] [330300] global_step=330300, grad_norm=4.537911415100098, loss=0.6139519810676575
I0301 13:01:18.549620 140089879389952 logging_writer.py:48] [330400] global_step=330400, grad_norm=4.653259754180908, loss=0.5976690053939819
I0301 13:01:52.035992 140089887782656 logging_writer.py:48] [330500] global_step=330500, grad_norm=4.4224443435668945, loss=0.6024964451789856
I0301 13:02:25.489925 140089879389952 logging_writer.py:48] [330600] global_step=330600, grad_norm=4.341081619262695, loss=0.6448701024055481
I0301 13:02:59.090339 140089887782656 logging_writer.py:48] [330700] global_step=330700, grad_norm=4.458352088928223, loss=0.5967506766319275
I0301 13:03:22.632958 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:03:28.744864 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:03:37.264019 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:03:39.554787 140252611495744 submission_runner.py:411] Time since start: 114560.43s, 	Step: 330772, 	{'train/accuracy': 0.9596420526504517, 'train/loss': 0.14901460707187653, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0536932945251465, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8433705568313599, 'test/num_examples': 10000, 'score': 110734.2365758419, 'total_duration': 114560.43485379219, 'accumulated_submission_time': 110734.2365758419, 'accumulated_eval_time': 3801.7138855457306, 'accumulated_logging_time': 12.616500854492188}
I0301 13:03:39.626547 140089778697984 logging_writer.py:48] [330772] accumulated_eval_time=3801.713886, accumulated_logging_time=12.616501, accumulated_submission_time=110734.236576, global_step=330772, preemption_count=0, score=110734.236576, test/accuracy=0.627900, test/loss=1.843371, test/num_examples=10000, total_duration=114560.434854, train/accuracy=0.959642, train/loss=0.149015, validation/accuracy=0.756140, validation/loss=1.053693, validation/num_examples=50000
I0301 13:03:49.354348 140089837426432 logging_writer.py:48] [330800] global_step=330800, grad_norm=4.743221759796143, loss=0.5832036733627319
I0301 13:04:22.807229 140089778697984 logging_writer.py:48] [330900] global_step=330900, grad_norm=4.5305047035217285, loss=0.6719175577163696
I0301 13:04:56.270272 140089837426432 logging_writer.py:48] [331000] global_step=331000, grad_norm=4.507992267608643, loss=0.6220644116401672
I0301 13:05:29.786206 140089778697984 logging_writer.py:48] [331100] global_step=331100, grad_norm=4.603790283203125, loss=0.6124424934387207
I0301 13:06:03.242111 140089837426432 logging_writer.py:48] [331200] global_step=331200, grad_norm=4.624390125274658, loss=0.6333554983139038
I0301 13:06:36.693975 140089778697984 logging_writer.py:48] [331300] global_step=331300, grad_norm=4.242093086242676, loss=0.6104363203048706
I0301 13:07:10.158981 140089837426432 logging_writer.py:48] [331400] global_step=331400, grad_norm=4.267072677612305, loss=0.5580910444259644
I0301 13:07:43.661934 140089778697984 logging_writer.py:48] [331500] global_step=331500, grad_norm=4.722395420074463, loss=0.6621898412704468
I0301 13:08:17.180897 140089837426432 logging_writer.py:48] [331600] global_step=331600, grad_norm=4.326122760772705, loss=0.6336333751678467
I0301 13:08:50.871622 140089778697984 logging_writer.py:48] [331700] global_step=331700, grad_norm=4.672231674194336, loss=0.6562081575393677
I0301 13:09:24.353039 140089837426432 logging_writer.py:48] [331800] global_step=331800, grad_norm=4.544023036956787, loss=0.6147425174713135
I0301 13:09:57.793024 140089778697984 logging_writer.py:48] [331900] global_step=331900, grad_norm=4.6529154777526855, loss=0.5831391215324402
I0301 13:10:31.257722 140089837426432 logging_writer.py:48] [332000] global_step=332000, grad_norm=4.859570026397705, loss=0.6452600359916687
I0301 13:11:04.741669 140089778697984 logging_writer.py:48] [332100] global_step=332100, grad_norm=4.657986164093018, loss=0.5957002639770508
I0301 13:11:38.220131 140089837426432 logging_writer.py:48] [332200] global_step=332200, grad_norm=4.753077983856201, loss=0.7101530432701111
I0301 13:12:09.797336 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:12:15.936884 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:12:24.454624 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:12:26.732720 140252611495744 submission_runner.py:411] Time since start: 115087.61s, 	Step: 332296, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.1480126678943634, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.053968071937561, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.843773365020752, 'test/num_examples': 10000, 'score': 111244.34099173546, 'total_duration': 115087.61276984215, 'accumulated_submission_time': 111244.34099173546, 'accumulated_eval_time': 3818.649199962616, 'accumulated_logging_time': 12.698453664779663}
I0301 13:12:26.808496 140089770305280 logging_writer.py:48] [332296] accumulated_eval_time=3818.649200, accumulated_logging_time=12.698454, accumulated_submission_time=111244.340992, global_step=332296, preemption_count=0, score=111244.340992, test/accuracy=0.627600, test/loss=1.843773, test/num_examples=10000, total_duration=115087.612770, train/accuracy=0.960120, train/loss=0.148013, validation/accuracy=0.755600, validation/loss=1.053968, validation/num_examples=50000
I0301 13:12:28.489541 140089778697984 logging_writer.py:48] [332300] global_step=332300, grad_norm=4.696842670440674, loss=0.633965015411377
I0301 13:13:01.948482 140089770305280 logging_writer.py:48] [332400] global_step=332400, grad_norm=4.107781410217285, loss=0.5455712080001831
I0301 13:13:35.400333 140089778697984 logging_writer.py:48] [332500] global_step=332500, grad_norm=5.221972942352295, loss=0.6801702976226807
I0301 13:14:08.900796 140089770305280 logging_writer.py:48] [332600] global_step=332600, grad_norm=4.2665605545043945, loss=0.6285820007324219
I0301 13:14:42.349222 140089778697984 logging_writer.py:48] [332700] global_step=332700, grad_norm=4.91515588760376, loss=0.6942594051361084
I0301 13:15:15.846886 140089770305280 logging_writer.py:48] [332800] global_step=332800, grad_norm=5.086794853210449, loss=0.6448001861572266
I0301 13:15:49.313362 140089778697984 logging_writer.py:48] [332900] global_step=332900, grad_norm=4.2079081535339355, loss=0.6002387404441833
I0301 13:16:22.784284 140089770305280 logging_writer.py:48] [333000] global_step=333000, grad_norm=4.442055702209473, loss=0.5751418471336365
I0301 13:16:56.241708 140089778697984 logging_writer.py:48] [333100] global_step=333100, grad_norm=4.14727783203125, loss=0.6189704537391663
I0301 13:17:29.759490 140089770305280 logging_writer.py:48] [333200] global_step=333200, grad_norm=4.6128315925598145, loss=0.5839464664459229
I0301 13:18:03.277872 140089778697984 logging_writer.py:48] [333300] global_step=333300, grad_norm=4.516454219818115, loss=0.6091752052307129
I0301 13:18:36.786414 140089770305280 logging_writer.py:48] [333400] global_step=333400, grad_norm=5.017631530761719, loss=0.6103911399841309
I0301 13:19:10.230581 140089778697984 logging_writer.py:48] [333500] global_step=333500, grad_norm=4.385252475738525, loss=0.5899460315704346
I0301 13:19:43.670765 140089770305280 logging_writer.py:48] [333600] global_step=333600, grad_norm=4.546961784362793, loss=0.6369068622589111
I0301 13:20:17.161631 140089778697984 logging_writer.py:48] [333700] global_step=333700, grad_norm=4.842576026916504, loss=0.6335066556930542
I0301 13:20:50.681462 140089770305280 logging_writer.py:48] [333800] global_step=333800, grad_norm=4.737405300140381, loss=0.6116534471511841
I0301 13:20:56.845493 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:21:03.079710 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:21:11.292091 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:21:13.566570 140252611495744 submission_runner.py:411] Time since start: 115614.45s, 	Step: 333820, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14424332976341248, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0530657768249512, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8418196439743042, 'test/num_examples': 10000, 'score': 111754.31213140488, 'total_duration': 115614.44660925865, 'accumulated_submission_time': 111754.31213140488, 'accumulated_eval_time': 3835.3701899051666, 'accumulated_logging_time': 12.785258054733276}
I0301 13:21:13.646035 140089870997248 logging_writer.py:48] [333820] accumulated_eval_time=3835.370190, accumulated_logging_time=12.785258, accumulated_submission_time=111754.312131, global_step=333820, preemption_count=0, score=111754.312131, test/accuracy=0.627100, test/loss=1.841820, test/num_examples=10000, total_duration=115614.446609, train/accuracy=0.961595, train/loss=0.144243, validation/accuracy=0.756020, validation/loss=1.053066, validation/num_examples=50000
I0301 13:21:40.777294 140089879389952 logging_writer.py:48] [333900] global_step=333900, grad_norm=4.440299034118652, loss=0.6135914325714111
I0301 13:22:14.312617 140089870997248 logging_writer.py:48] [334000] global_step=334000, grad_norm=4.860217094421387, loss=0.69384765625
I0301 13:22:47.790939 140089879389952 logging_writer.py:48] [334100] global_step=334100, grad_norm=4.456585884094238, loss=0.6282485723495483
I0301 13:23:21.249308 140089870997248 logging_writer.py:48] [334200] global_step=334200, grad_norm=4.827636241912842, loss=0.6710164546966553
I0301 13:23:54.755412 140089879389952 logging_writer.py:48] [334300] global_step=334300, grad_norm=4.391693115234375, loss=0.5815157294273376
I0301 13:24:28.255670 140089870997248 logging_writer.py:48] [334400] global_step=334400, grad_norm=5.008869647979736, loss=0.6444442868232727
I0301 13:25:01.695756 140089879389952 logging_writer.py:48] [334500] global_step=334500, grad_norm=4.011589527130127, loss=0.5622814297676086
I0301 13:25:35.178458 140089870997248 logging_writer.py:48] [334600] global_step=334600, grad_norm=4.400357723236084, loss=0.6352921724319458
I0301 13:26:08.676308 140089879389952 logging_writer.py:48] [334700] global_step=334700, grad_norm=4.810286045074463, loss=0.6082577705383301
I0301 13:26:42.188586 140089870997248 logging_writer.py:48] [334800] global_step=334800, grad_norm=4.273022174835205, loss=0.59792160987854
I0301 13:27:15.814010 140089879389952 logging_writer.py:48] [334900] global_step=334900, grad_norm=4.420076847076416, loss=0.6677988767623901
I0301 13:27:49.282422 140089870997248 logging_writer.py:48] [335000] global_step=335000, grad_norm=4.437679767608643, loss=0.6821374893188477
I0301 13:28:22.767362 140089879389952 logging_writer.py:48] [335100] global_step=335100, grad_norm=4.863035202026367, loss=0.669524610042572
I0301 13:28:56.211453 140089870997248 logging_writer.py:48] [335200] global_step=335200, grad_norm=5.38315486907959, loss=0.6840083003044128
I0301 13:29:29.669267 140089879389952 logging_writer.py:48] [335300] global_step=335300, grad_norm=4.313528537750244, loss=0.6341026425361633
I0301 13:29:43.871659 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:29:49.956015 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:29:58.328437 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:30:00.594153 140252611495744 submission_runner.py:411] Time since start: 116141.47s, 	Step: 335344, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14424613118171692, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0534335374832153, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8424676656723022, 'test/num_examples': 10000, 'score': 112264.47166848183, 'total_duration': 116141.47416877747, 'accumulated_submission_time': 112264.47166848183, 'accumulated_eval_time': 3852.092576265335, 'accumulated_logging_time': 12.87584137916565}
I0301 13:30:00.693827 140089778697984 logging_writer.py:48] [335344] accumulated_eval_time=3852.092576, accumulated_logging_time=12.875841, accumulated_submission_time=112264.471668, global_step=335344, preemption_count=0, score=112264.471668, test/accuracy=0.628200, test/loss=1.842468, test/num_examples=10000, total_duration=116141.474169, train/accuracy=0.960639, train/loss=0.144246, validation/accuracy=0.755560, validation/loss=1.053434, validation/num_examples=50000
I0301 13:30:19.768733 140089837426432 logging_writer.py:48] [335400] global_step=335400, grad_norm=5.235562324523926, loss=0.7021054029464722
I0301 13:30:53.253642 140089778697984 logging_writer.py:48] [335500] global_step=335500, grad_norm=4.167891025543213, loss=0.564079761505127
I0301 13:31:26.686285 140089837426432 logging_writer.py:48] [335600] global_step=335600, grad_norm=4.878703594207764, loss=0.6658014059066772
I0301 13:32:00.150476 140089778697984 logging_writer.py:48] [335700] global_step=335700, grad_norm=5.2356438636779785, loss=0.643940269947052
I0301 13:32:33.630179 140089837426432 logging_writer.py:48] [335800] global_step=335800, grad_norm=4.422054767608643, loss=0.6421226859092712
I0301 13:33:07.246700 140089778697984 logging_writer.py:48] [335900] global_step=335900, grad_norm=4.245441913604736, loss=0.6453048586845398
I0301 13:33:40.721988 140089837426432 logging_writer.py:48] [336000] global_step=336000, grad_norm=3.8931498527526855, loss=0.5494794249534607
I0301 13:34:14.210407 140089778697984 logging_writer.py:48] [336100] global_step=336100, grad_norm=4.571807384490967, loss=0.6669413447380066
I0301 13:34:47.643726 140089837426432 logging_writer.py:48] [336200] global_step=336200, grad_norm=4.17036771774292, loss=0.58905029296875
I0301 13:35:21.095005 140089778697984 logging_writer.py:48] [336300] global_step=336300, grad_norm=4.7214155197143555, loss=0.6564648747444153
I0301 13:35:54.549962 140089837426432 logging_writer.py:48] [336400] global_step=336400, grad_norm=4.349726676940918, loss=0.5993163585662842
I0301 13:36:28.002802 140089778697984 logging_writer.py:48] [336500] global_step=336500, grad_norm=4.756231307983398, loss=0.638891339302063
I0301 13:37:01.469363 140089837426432 logging_writer.py:48] [336600] global_step=336600, grad_norm=4.6862287521362305, loss=0.5870538949966431
I0301 13:37:34.935949 140089778697984 logging_writer.py:48] [336700] global_step=336700, grad_norm=4.101423263549805, loss=0.5446739792823792
I0301 13:38:08.366712 140089837426432 logging_writer.py:48] [336800] global_step=336800, grad_norm=4.537148952484131, loss=0.6835207939147949
I0301 13:38:30.623719 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:38:36.709095 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:38:44.999661 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:38:47.279786 140252611495744 submission_runner.py:411] Time since start: 116668.16s, 	Step: 336868, 	{'train/accuracy': 0.9594228267669678, 'train/loss': 0.14783793687820435, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0530071258544922, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8411434888839722, 'test/num_examples': 10000, 'score': 112774.3374478817, 'total_duration': 116668.15985655785, 'accumulated_submission_time': 112774.3374478817, 'accumulated_eval_time': 3868.7485876083374, 'accumulated_logging_time': 12.985454320907593}
I0301 13:38:47.350677 140089854211840 logging_writer.py:48] [336868] accumulated_eval_time=3868.748588, accumulated_logging_time=12.985454, accumulated_submission_time=112774.337448, global_step=336868, preemption_count=0, score=112774.337448, test/accuracy=0.626800, test/loss=1.841143, test/num_examples=10000, total_duration=116668.159857, train/accuracy=0.959423, train/loss=0.147838, validation/accuracy=0.756000, validation/loss=1.053007, validation/num_examples=50000
I0301 13:38:58.368626 140089862604544 logging_writer.py:48] [336900] global_step=336900, grad_norm=5.0479936599731445, loss=0.7199941277503967
I0301 13:39:31.924643 140089854211840 logging_writer.py:48] [337000] global_step=337000, grad_norm=4.959980010986328, loss=0.6822443008422852
I0301 13:40:05.387246 140089862604544 logging_writer.py:48] [337100] global_step=337100, grad_norm=4.428948402404785, loss=0.5698561072349548
I0301 13:40:38.862580 140089854211840 logging_writer.py:48] [337200] global_step=337200, grad_norm=4.7430572509765625, loss=0.6446163058280945
I0301 13:41:12.375237 140089862604544 logging_writer.py:48] [337300] global_step=337300, grad_norm=4.533377170562744, loss=0.5945627689361572
I0301 13:41:45.849150 140089854211840 logging_writer.py:48] [337400] global_step=337400, grad_norm=5.068889141082764, loss=0.7090359926223755
I0301 13:42:19.306577 140089862604544 logging_writer.py:48] [337500] global_step=337500, grad_norm=4.422883987426758, loss=0.5586245656013489
I0301 13:42:52.781264 140089854211840 logging_writer.py:48] [337600] global_step=337600, grad_norm=4.400978088378906, loss=0.6844200491905212
I0301 13:43:26.228852 140089862604544 logging_writer.py:48] [337700] global_step=337700, grad_norm=4.6642045974731445, loss=0.5822144746780396
I0301 13:43:59.695738 140089854211840 logging_writer.py:48] [337800] global_step=337800, grad_norm=4.542819499969482, loss=0.6206492781639099
I0301 13:44:33.133716 140089862604544 logging_writer.py:48] [337900] global_step=337900, grad_norm=4.946660041809082, loss=0.6007382869720459
I0301 13:45:06.586971 140089854211840 logging_writer.py:48] [338000] global_step=338000, grad_norm=5.091572284698486, loss=0.6238722205162048
I0301 13:45:40.109743 140089862604544 logging_writer.py:48] [338100] global_step=338100, grad_norm=5.01509428024292, loss=0.6825083494186401
I0301 13:46:13.563179 140089854211840 logging_writer.py:48] [338200] global_step=338200, grad_norm=4.092784404754639, loss=0.5499086380004883
I0301 13:46:47.004029 140089862604544 logging_writer.py:48] [338300] global_step=338300, grad_norm=4.469308376312256, loss=0.6122921705245972
I0301 13:47:17.550400 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:47:23.729111 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:47:31.880202 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:47:34.237205 140252611495744 submission_runner.py:411] Time since start: 117195.12s, 	Step: 338393, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14575576782226562, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0537904500961304, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.84160315990448, 'test/num_examples': 10000, 'score': 113284.472843647, 'total_duration': 117195.11727333069, 'accumulated_submission_time': 113284.472843647, 'accumulated_eval_time': 3885.4353473186493, 'accumulated_logging_time': 13.066871643066406}
I0301 13:47:34.312757 140089837426432 logging_writer.py:48] [338393] accumulated_eval_time=3885.435347, accumulated_logging_time=13.066872, accumulated_submission_time=113284.472844, global_step=338393, preemption_count=0, score=113284.472844, test/accuracy=0.627800, test/loss=1.841603, test/num_examples=10000, total_duration=117195.117273, train/accuracy=0.960778, train/loss=0.145756, validation/accuracy=0.755920, validation/loss=1.053790, validation/num_examples=50000
I0301 13:47:36.997409 140089845819136 logging_writer.py:48] [338400] global_step=338400, grad_norm=4.675785064697266, loss=0.6848556995391846
I0301 13:48:10.437026 140089837426432 logging_writer.py:48] [338500] global_step=338500, grad_norm=4.962055206298828, loss=0.6918913125991821
I0301 13:48:43.898764 140089845819136 logging_writer.py:48] [338600] global_step=338600, grad_norm=4.185173511505127, loss=0.6150007247924805
I0301 13:49:17.361793 140089837426432 logging_writer.py:48] [338700] global_step=338700, grad_norm=4.398797512054443, loss=0.6482734084129333
I0301 13:49:50.790481 140089845819136 logging_writer.py:48] [338800] global_step=338800, grad_norm=4.946114540100098, loss=0.6349228620529175
I0301 13:50:24.268104 140089837426432 logging_writer.py:48] [338900] global_step=338900, grad_norm=4.715010166168213, loss=0.6328681707382202
I0301 13:50:57.737049 140089845819136 logging_writer.py:48] [339000] global_step=339000, grad_norm=4.4783220291137695, loss=0.5997658967971802
I0301 13:51:31.266201 140089837426432 logging_writer.py:48] [339100] global_step=339100, grad_norm=5.211003303527832, loss=0.7162021994590759
I0301 13:52:04.740903 140089845819136 logging_writer.py:48] [339200] global_step=339200, grad_norm=4.45423698425293, loss=0.6118001937866211
I0301 13:52:38.188845 140089837426432 logging_writer.py:48] [339300] global_step=339300, grad_norm=4.86699104309082, loss=0.6717343330383301
I0301 13:53:11.642902 140089845819136 logging_writer.py:48] [339400] global_step=339400, grad_norm=4.661493301391602, loss=0.6251870393753052
I0301 13:53:45.109416 140089837426432 logging_writer.py:48] [339500] global_step=339500, grad_norm=4.958796977996826, loss=0.5980141758918762
I0301 13:54:18.581617 140089845819136 logging_writer.py:48] [339600] global_step=339600, grad_norm=4.42231559753418, loss=0.5441792607307434
I0301 13:54:52.008708 140089837426432 logging_writer.py:48] [339700] global_step=339700, grad_norm=4.550507545471191, loss=0.5626734495162964
I0301 13:55:25.466405 140089845819136 logging_writer.py:48] [339800] global_step=339800, grad_norm=4.79365873336792, loss=0.6874145865440369
I0301 13:55:58.909156 140089837426432 logging_writer.py:48] [339900] global_step=339900, grad_norm=4.611163139343262, loss=0.6671251058578491
I0301 13:56:04.389010 140252611495744 spec.py:321] Evaluating on the training split.
I0301 13:56:10.488231 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 13:56:18.787501 140252611495744 spec.py:349] Evaluating on the test split.
I0301 13:56:21.106837 140252611495744 submission_runner.py:411] Time since start: 117721.99s, 	Step: 339918, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14373284578323364, 'validation/accuracy': 0.7555599808692932, 'validation/loss': 1.0540083646774292, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.843093991279602, 'test/num_examples': 10000, 'score': 113794.48411178589, 'total_duration': 117721.9868991375, 'accumulated_submission_time': 113794.48411178589, 'accumulated_eval_time': 3902.1531109809875, 'accumulated_logging_time': 13.152750968933105}
I0301 13:56:21.181966 140089778697984 logging_writer.py:48] [339918] accumulated_eval_time=3902.153111, accumulated_logging_time=13.152751, accumulated_submission_time=113794.484112, global_step=339918, preemption_count=0, score=113794.484112, test/accuracy=0.627600, test/loss=1.843094, test/num_examples=10000, total_duration=117721.986899, train/accuracy=0.961416, train/loss=0.143733, validation/accuracy=0.755560, validation/loss=1.054008, validation/num_examples=50000
I0301 13:56:48.971983 140089854211840 logging_writer.py:48] [340000] global_step=340000, grad_norm=5.2988762855529785, loss=0.6618775725364685
I0301 13:57:22.513178 140089778697984 logging_writer.py:48] [340100] global_step=340100, grad_norm=4.646523475646973, loss=0.6088331341743469
I0301 13:57:55.983499 140089854211840 logging_writer.py:48] [340200] global_step=340200, grad_norm=5.138730049133301, loss=0.5847331285476685
I0301 13:58:29.452365 140089778697984 logging_writer.py:48] [340300] global_step=340300, grad_norm=4.412714004516602, loss=0.6015385985374451
I0301 13:59:02.887804 140089854211840 logging_writer.py:48] [340400] global_step=340400, grad_norm=4.787121772766113, loss=0.5781981945037842
I0301 13:59:36.354025 140089778697984 logging_writer.py:48] [340500] global_step=340500, grad_norm=4.743992805480957, loss=0.6365586519241333
I0301 14:00:09.813771 140089854211840 logging_writer.py:48] [340600] global_step=340600, grad_norm=4.635909557342529, loss=0.6234816908836365
I0301 14:00:43.235064 140089778697984 logging_writer.py:48] [340700] global_step=340700, grad_norm=5.067224979400635, loss=0.5797257423400879
I0301 14:01:16.723743 140089854211840 logging_writer.py:48] [340800] global_step=340800, grad_norm=4.464191436767578, loss=0.6160531640052795
I0301 14:01:50.152686 140089778697984 logging_writer.py:48] [340900] global_step=340900, grad_norm=3.962665557861328, loss=0.5063109397888184
I0301 14:02:23.601675 140089854211840 logging_writer.py:48] [341000] global_step=341000, grad_norm=4.880806922912598, loss=0.6556594371795654
I0301 14:02:57.055470 140089778697984 logging_writer.py:48] [341100] global_step=341100, grad_norm=4.914551258087158, loss=0.640662431716919
I0301 14:03:30.554183 140089854211840 logging_writer.py:48] [341200] global_step=341200, grad_norm=4.393249988555908, loss=0.5646509528160095
I0301 14:04:03.997136 140089778697984 logging_writer.py:48] [341300] global_step=341300, grad_norm=4.372186183929443, loss=0.5968086123466492
I0301 14:04:37.458829 140089854211840 logging_writer.py:48] [341400] global_step=341400, grad_norm=4.050519943237305, loss=0.5903695225715637
I0301 14:04:51.319919 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:04:57.639280 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:05:05.785547 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:05:08.079526 140252611495744 submission_runner.py:411] Time since start: 118248.96s, 	Step: 341443, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14729546010494232, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0539937019348145, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.843406319618225, 'test/num_examples': 10000, 'score': 114304.55684185028, 'total_duration': 118248.95959162712, 'accumulated_submission_time': 114304.55684185028, 'accumulated_eval_time': 3918.9126632213593, 'accumulated_logging_time': 13.23846983909607}
I0301 14:05:08.156418 140089770305280 logging_writer.py:48] [341443] accumulated_eval_time=3918.912663, accumulated_logging_time=13.238470, accumulated_submission_time=114304.556842, global_step=341443, preemption_count=0, score=114304.556842, test/accuracy=0.627600, test/loss=1.843406, test/num_examples=10000, total_duration=118248.959592, train/accuracy=0.961834, train/loss=0.147295, validation/accuracy=0.755680, validation/loss=1.053994, validation/num_examples=50000
I0301 14:05:27.556980 140089778697984 logging_writer.py:48] [341500] global_step=341500, grad_norm=4.56489372253418, loss=0.5808953046798706
I0301 14:06:00.982743 140089770305280 logging_writer.py:48] [341600] global_step=341600, grad_norm=4.367955207824707, loss=0.6298344135284424
I0301 14:06:34.439268 140089778697984 logging_writer.py:48] [341700] global_step=341700, grad_norm=4.556977272033691, loss=0.649238646030426
I0301 14:07:07.888486 140089770305280 logging_writer.py:48] [341800] global_step=341800, grad_norm=5.106204509735107, loss=0.6101595163345337
I0301 14:07:41.376190 140089778697984 logging_writer.py:48] [341900] global_step=341900, grad_norm=4.912197589874268, loss=0.6449068784713745
I0301 14:08:14.804700 140089770305280 logging_writer.py:48] [342000] global_step=342000, grad_norm=4.792957782745361, loss=0.6813066005706787
I0301 14:08:48.243920 140089778697984 logging_writer.py:48] [342100] global_step=342100, grad_norm=4.252536773681641, loss=0.6650546789169312
I0301 14:09:21.727446 140089770305280 logging_writer.py:48] [342200] global_step=342200, grad_norm=4.561819553375244, loss=0.624882698059082
I0301 14:09:55.285781 140089778697984 logging_writer.py:48] [342300] global_step=342300, grad_norm=4.466882705688477, loss=0.5019842982292175
I0301 14:10:28.744310 140089770305280 logging_writer.py:48] [342400] global_step=342400, grad_norm=4.699540138244629, loss=0.6794586181640625
I0301 14:11:02.201502 140089778697984 logging_writer.py:48] [342500] global_step=342500, grad_norm=4.207176208496094, loss=0.6240276098251343
I0301 14:11:35.685646 140089770305280 logging_writer.py:48] [342600] global_step=342600, grad_norm=4.791552543640137, loss=0.6073334813117981
I0301 14:12:09.131479 140089778697984 logging_writer.py:48] [342700] global_step=342700, grad_norm=4.079565525054932, loss=0.600799560546875
I0301 14:12:42.585786 140089770305280 logging_writer.py:48] [342800] global_step=342800, grad_norm=4.3986687660217285, loss=0.586211621761322
I0301 14:13:16.038016 140089778697984 logging_writer.py:48] [342900] global_step=342900, grad_norm=4.294719696044922, loss=0.5744006633758545
I0301 14:13:38.263088 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:13:44.360678 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:13:52.598808 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:13:54.930574 140252611495744 submission_runner.py:411] Time since start: 118775.81s, 	Step: 342968, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.14888696372509003, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0539145469665527, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8429653644561768, 'test/num_examples': 10000, 'score': 114814.5960021019, 'total_duration': 118775.81064414978, 'accumulated_submission_time': 114814.5960021019, 'accumulated_eval_time': 3935.580096244812, 'accumulated_logging_time': 13.327780723571777}
I0301 14:13:55.010043 140089854211840 logging_writer.py:48] [342968] accumulated_eval_time=3935.580096, accumulated_logging_time=13.327781, accumulated_submission_time=114814.596002, global_step=342968, preemption_count=0, score=114814.596002, test/accuracy=0.627400, test/loss=1.842965, test/num_examples=10000, total_duration=118775.810644, train/accuracy=0.959682, train/loss=0.148887, validation/accuracy=0.756080, validation/loss=1.053915, validation/num_examples=50000
I0301 14:14:06.056580 140089862604544 logging_writer.py:48] [343000] global_step=343000, grad_norm=4.803675174713135, loss=0.6489501595497131
I0301 14:14:39.550562 140089854211840 logging_writer.py:48] [343100] global_step=343100, grad_norm=4.322965621948242, loss=0.6278235912322998
I0301 14:15:12.982284 140089862604544 logging_writer.py:48] [343200] global_step=343200, grad_norm=4.734392166137695, loss=0.6687160134315491
I0301 14:15:46.523270 140089854211840 logging_writer.py:48] [343300] global_step=343300, grad_norm=5.04689359664917, loss=0.6639297008514404
I0301 14:16:19.963315 140089862604544 logging_writer.py:48] [343400] global_step=343400, grad_norm=4.487049579620361, loss=0.7126829028129578
I0301 14:16:53.415598 140089854211840 logging_writer.py:48] [343500] global_step=343500, grad_norm=4.9502272605896, loss=0.6008233428001404
I0301 14:17:26.865502 140089862604544 logging_writer.py:48] [343600] global_step=343600, grad_norm=4.473813533782959, loss=0.5561412572860718
I0301 14:18:00.308938 140089854211840 logging_writer.py:48] [343700] global_step=343700, grad_norm=4.256066799163818, loss=0.5824509263038635
I0301 14:18:33.738975 140089862604544 logging_writer.py:48] [343800] global_step=343800, grad_norm=4.394167423248291, loss=0.6485964059829712
I0301 14:19:07.206426 140089854211840 logging_writer.py:48] [343900] global_step=343900, grad_norm=4.178354740142822, loss=0.5644685626029968
I0301 14:19:40.641266 140089862604544 logging_writer.py:48] [344000] global_step=344000, grad_norm=5.0255231857299805, loss=0.5638076663017273
I0301 14:20:14.095381 140089854211840 logging_writer.py:48] [344100] global_step=344100, grad_norm=4.192861557006836, loss=0.5626944899559021
I0301 14:20:47.570518 140089862604544 logging_writer.py:48] [344200] global_step=344200, grad_norm=4.818285942077637, loss=0.6238878965377808
I0301 14:21:21.080797 140089854211840 logging_writer.py:48] [344300] global_step=344300, grad_norm=4.41162633895874, loss=0.5799164175987244
I0301 14:21:54.615110 140089862604544 logging_writer.py:48] [344400] global_step=344400, grad_norm=4.144482135772705, loss=0.6063663363456726
I0301 14:22:25.182734 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:22:31.314114 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:22:39.504329 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:22:41.838009 140252611495744 submission_runner.py:411] Time since start: 119302.72s, 	Step: 344493, 	{'train/accuracy': 0.9618741869926453, 'train/loss': 0.14408862590789795, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.052625298500061, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.84093177318573, 'test/num_examples': 10000, 'score': 115324.70213341713, 'total_duration': 119302.71807384491, 'accumulated_submission_time': 115324.70213341713, 'accumulated_eval_time': 3952.2353222370148, 'accumulated_logging_time': 13.418944597244263}
I0301 14:22:41.915503 140089770305280 logging_writer.py:48] [344493] accumulated_eval_time=3952.235322, accumulated_logging_time=13.418945, accumulated_submission_time=115324.702133, global_step=344493, preemption_count=0, score=115324.702133, test/accuracy=0.627500, test/loss=1.840932, test/num_examples=10000, total_duration=119302.718074, train/accuracy=0.961874, train/loss=0.144089, validation/accuracy=0.756180, validation/loss=1.052625, validation/num_examples=50000
I0301 14:22:44.596394 140089778697984 logging_writer.py:48] [344500] global_step=344500, grad_norm=4.382407188415527, loss=0.5923748016357422
I0301 14:23:18.076245 140089770305280 logging_writer.py:48] [344600] global_step=344600, grad_norm=4.583707809448242, loss=0.6535165905952454
I0301 14:23:51.558724 140089778697984 logging_writer.py:48] [344700] global_step=344700, grad_norm=4.059206962585449, loss=0.5024468302726746
I0301 14:24:25.052400 140089770305280 logging_writer.py:48] [344800] global_step=344800, grad_norm=4.645161151885986, loss=0.5348308086395264
I0301 14:24:58.516584 140089778697984 logging_writer.py:48] [344900] global_step=344900, grad_norm=4.879456043243408, loss=0.6500834822654724
I0301 14:25:31.979171 140089770305280 logging_writer.py:48] [345000] global_step=345000, grad_norm=4.370725154876709, loss=0.577426016330719
I0301 14:26:05.459242 140089778697984 logging_writer.py:48] [345100] global_step=345100, grad_norm=4.251685619354248, loss=0.6190635561943054
I0301 14:26:38.996043 140089770305280 logging_writer.py:48] [345200] global_step=345200, grad_norm=4.415151596069336, loss=0.6935524940490723
I0301 14:27:12.412615 140089778697984 logging_writer.py:48] [345300] global_step=345300, grad_norm=4.345344543457031, loss=0.5860958099365234
I0301 14:27:45.970671 140089770305280 logging_writer.py:48] [345400] global_step=345400, grad_norm=4.980058670043945, loss=0.6836099624633789
I0301 14:28:19.441367 140089778697984 logging_writer.py:48] [345500] global_step=345500, grad_norm=4.847256660461426, loss=0.6592247486114502
I0301 14:28:52.897067 140089770305280 logging_writer.py:48] [345600] global_step=345600, grad_norm=4.861146450042725, loss=0.6436880230903625
I0301 14:29:26.331679 140089778697984 logging_writer.py:48] [345700] global_step=345700, grad_norm=4.878129482269287, loss=0.6601449847221375
I0301 14:29:59.766220 140089770305280 logging_writer.py:48] [345800] global_step=345800, grad_norm=4.922506332397461, loss=0.6161298155784607
I0301 14:30:33.257450 140089778697984 logging_writer.py:48] [345900] global_step=345900, grad_norm=5.071548938751221, loss=0.6426507234573364
I0301 14:31:06.749297 140089770305280 logging_writer.py:48] [346000] global_step=346000, grad_norm=4.447174549102783, loss=0.5954698920249939
I0301 14:31:11.914562 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:31:18.097305 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:31:26.141611 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:31:28.419603 140252611495744 submission_runner.py:411] Time since start: 119829.30s, 	Step: 346017, 	{'train/accuracy': 0.9626514315605164, 'train/loss': 0.14406661689281464, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0532101392745972, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8426051139831543, 'test/num_examples': 10000, 'score': 115834.6356317997, 'total_duration': 119829.2996737957, 'accumulated_submission_time': 115834.6356317997, 'accumulated_eval_time': 3968.7403090000153, 'accumulated_logging_time': 13.50695252418518}
I0301 14:31:28.494940 140089862604544 logging_writer.py:48] [346017] accumulated_eval_time=3968.740309, accumulated_logging_time=13.506953, accumulated_submission_time=115834.635632, global_step=346017, preemption_count=0, score=115834.635632, test/accuracy=0.628000, test/loss=1.842605, test/num_examples=10000, total_duration=119829.299674, train/accuracy=0.962651, train/loss=0.144067, validation/accuracy=0.755980, validation/loss=1.053210, validation/num_examples=50000
I0301 14:31:56.587494 140089870997248 logging_writer.py:48] [346100] global_step=346100, grad_norm=4.3530592918396, loss=0.5860161781311035
I0301 14:32:30.049300 140089862604544 logging_writer.py:48] [346200] global_step=346200, grad_norm=4.882332801818848, loss=0.5700168609619141
I0301 14:33:03.476033 140089870997248 logging_writer.py:48] [346300] global_step=346300, grad_norm=4.466404438018799, loss=0.6308637857437134
I0301 14:33:36.933547 140089862604544 logging_writer.py:48] [346400] global_step=346400, grad_norm=4.238648414611816, loss=0.657310962677002
I0301 14:34:10.462743 140089870997248 logging_writer.py:48] [346500] global_step=346500, grad_norm=5.087412357330322, loss=0.7387228012084961
I0301 14:34:43.912369 140089862604544 logging_writer.py:48] [346600] global_step=346600, grad_norm=4.592937469482422, loss=0.6543484926223755
I0301 14:35:17.360798 140089870997248 logging_writer.py:48] [346700] global_step=346700, grad_norm=4.489109992980957, loss=0.6528620719909668
I0301 14:35:50.819859 140089862604544 logging_writer.py:48] [346800] global_step=346800, grad_norm=4.828794002532959, loss=0.5860467553138733
I0301 14:36:24.271739 140089870997248 logging_writer.py:48] [346900] global_step=346900, grad_norm=4.678474426269531, loss=0.6291165351867676
I0301 14:36:57.760333 140089862604544 logging_writer.py:48] [347000] global_step=347000, grad_norm=5.043734073638916, loss=0.6081531047821045
I0301 14:37:31.192381 140089870997248 logging_writer.py:48] [347100] global_step=347100, grad_norm=4.669920444488525, loss=0.6174221634864807
I0301 14:38:04.644824 140089862604544 logging_writer.py:48] [347200] global_step=347200, grad_norm=5.271938800811768, loss=0.7404922842979431
I0301 14:38:38.130173 140089870997248 logging_writer.py:48] [347300] global_step=347300, grad_norm=4.244542598724365, loss=0.5985280275344849
I0301 14:39:11.577563 140089862604544 logging_writer.py:48] [347400] global_step=347400, grad_norm=4.886314868927002, loss=0.6621852517127991
I0301 14:39:45.051342 140089870997248 logging_writer.py:48] [347500] global_step=347500, grad_norm=4.380382061004639, loss=0.6454560160636902
I0301 14:39:58.666389 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:40:04.948776 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:40:13.143633 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:40:15.424091 140252611495744 submission_runner.py:411] Time since start: 120356.30s, 	Step: 347542, 	{'train/accuracy': 0.9629504084587097, 'train/loss': 0.142887681722641, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0534168481826782, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.844247579574585, 'test/num_examples': 10000, 'score': 116344.7417371273, 'total_duration': 120356.30416107178, 'accumulated_submission_time': 116344.7417371273, 'accumulated_eval_time': 3985.4980008602142, 'accumulated_logging_time': 13.592343807220459}
I0301 14:40:15.500965 140089778697984 logging_writer.py:48] [347542] accumulated_eval_time=3985.498001, accumulated_logging_time=13.592344, accumulated_submission_time=116344.741737, global_step=347542, preemption_count=0, score=116344.741737, test/accuracy=0.627500, test/loss=1.844248, test/num_examples=10000, total_duration=120356.304161, train/accuracy=0.962950, train/loss=0.142888, validation/accuracy=0.755960, validation/loss=1.053417, validation/num_examples=50000
I0301 14:40:35.264368 140089837426432 logging_writer.py:48] [347600] global_step=347600, grad_norm=4.398459434509277, loss=0.6231452822685242
I0301 14:41:08.724161 140089778697984 logging_writer.py:48] [347700] global_step=347700, grad_norm=4.6095147132873535, loss=0.5921534895896912
I0301 14:41:42.168433 140089837426432 logging_writer.py:48] [347800] global_step=347800, grad_norm=4.9042768478393555, loss=0.626922607421875
I0301 14:42:15.614662 140089778697984 logging_writer.py:48] [347900] global_step=347900, grad_norm=4.577824115753174, loss=0.6126048564910889
I0301 14:42:49.051453 140089837426432 logging_writer.py:48] [348000] global_step=348000, grad_norm=4.851025104522705, loss=0.6791561841964722
I0301 14:43:22.509839 140089778697984 logging_writer.py:48] [348100] global_step=348100, grad_norm=4.519045352935791, loss=0.5928876996040344
I0301 14:43:55.971487 140089837426432 logging_writer.py:48] [348200] global_step=348200, grad_norm=4.159925937652588, loss=0.5467458367347717
I0301 14:44:29.405328 140089778697984 logging_writer.py:48] [348300] global_step=348300, grad_norm=4.575275421142578, loss=0.6609455347061157
I0301 14:45:02.865218 140089837426432 logging_writer.py:48] [348400] global_step=348400, grad_norm=4.403256893157959, loss=0.6530038118362427
I0301 14:45:36.348558 140089778697984 logging_writer.py:48] [348500] global_step=348500, grad_norm=4.252964973449707, loss=0.5851626992225647
I0301 14:46:09.848419 140089837426432 logging_writer.py:48] [348600] global_step=348600, grad_norm=4.484900951385498, loss=0.6437402963638306
I0301 14:46:43.298329 140089778697984 logging_writer.py:48] [348700] global_step=348700, grad_norm=5.376777648925781, loss=0.7074002027511597
I0301 14:47:16.753879 140089837426432 logging_writer.py:48] [348800] global_step=348800, grad_norm=4.529996395111084, loss=0.5618746280670166
I0301 14:47:50.203739 140089778697984 logging_writer.py:48] [348900] global_step=348900, grad_norm=4.321932792663574, loss=0.612944483757019
I0301 14:48:23.676355 140089837426432 logging_writer.py:48] [349000] global_step=349000, grad_norm=4.163100719451904, loss=0.6004666090011597
I0301 14:48:45.583186 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:48:52.328744 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:49:00.431558 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:49:02.834814 140252611495744 submission_runner.py:411] Time since start: 120883.71s, 	Step: 349067, 	{'train/accuracy': 0.9593430757522583, 'train/loss': 0.14777535200119019, 'validation/accuracy': 0.7563599944114685, 'validation/loss': 1.0534800291061401, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8426477909088135, 'test/num_examples': 10000, 'score': 116854.75827503204, 'total_duration': 120883.71487736702, 'accumulated_submission_time': 116854.75827503204, 'accumulated_eval_time': 4002.7495708465576, 'accumulated_logging_time': 13.679721355438232}
I0301 14:49:02.910385 140089778697984 logging_writer.py:48] [349067] accumulated_eval_time=4002.749571, accumulated_logging_time=13.679721, accumulated_submission_time=116854.758275, global_step=349067, preemption_count=0, score=116854.758275, test/accuracy=0.627500, test/loss=1.842648, test/num_examples=10000, total_duration=120883.714877, train/accuracy=0.959343, train/loss=0.147775, validation/accuracy=0.756360, validation/loss=1.053480, validation/num_examples=50000
I0301 14:49:14.294231 140089854211840 logging_writer.py:48] [349100] global_step=349100, grad_norm=4.720754146575928, loss=0.6123542189598083
I0301 14:49:47.761934 140089778697984 logging_writer.py:48] [349200] global_step=349200, grad_norm=4.752832412719727, loss=0.6151663064956665
I0301 14:50:21.228595 140089854211840 logging_writer.py:48] [349300] global_step=349300, grad_norm=5.448831081390381, loss=0.6890010833740234
I0301 14:50:54.675426 140089778697984 logging_writer.py:48] [349400] global_step=349400, grad_norm=4.224067211151123, loss=0.587748110294342
I0301 14:51:28.178869 140089854211840 logging_writer.py:48] [349500] global_step=349500, grad_norm=4.428458213806152, loss=0.6356099843978882
I0301 14:52:01.623445 140089778697984 logging_writer.py:48] [349600] global_step=349600, grad_norm=4.537114143371582, loss=0.6070879101753235
I0301 14:52:35.237063 140089854211840 logging_writer.py:48] [349700] global_step=349700, grad_norm=4.8758440017700195, loss=0.6486338376998901
I0301 14:53:08.709358 140089778697984 logging_writer.py:48] [349800] global_step=349800, grad_norm=4.170147895812988, loss=0.5976141691207886
I0301 14:53:42.213076 140089854211840 logging_writer.py:48] [349900] global_step=349900, grad_norm=4.498981952667236, loss=0.5590882301330566
I0301 14:54:15.643988 140089778697984 logging_writer.py:48] [350000] global_step=350000, grad_norm=5.1315083503723145, loss=0.6937245726585388
I0301 14:54:49.136711 140089854211840 logging_writer.py:48] [350100] global_step=350100, grad_norm=4.287398815155029, loss=0.5942621231079102
I0301 14:55:22.617085 140089778697984 logging_writer.py:48] [350200] global_step=350200, grad_norm=4.638564109802246, loss=0.5747883319854736
I0301 14:55:56.114395 140089854211840 logging_writer.py:48] [350300] global_step=350300, grad_norm=4.632640838623047, loss=0.5779550671577454
I0301 14:56:29.552112 140089778697984 logging_writer.py:48] [350400] global_step=350400, grad_norm=4.245948314666748, loss=0.591989278793335
I0301 14:57:03.035381 140089854211840 logging_writer.py:48] [350500] global_step=350500, grad_norm=4.702818393707275, loss=0.6830132007598877
I0301 14:57:32.984957 140252611495744 spec.py:321] Evaluating on the training split.
I0301 14:57:39.101035 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 14:57:47.199903 140252611495744 spec.py:349] Evaluating on the test split.
I0301 14:57:49.485527 140252611495744 submission_runner.py:411] Time since start: 121410.37s, 	Step: 350591, 	{'train/accuracy': 0.9602199792861938, 'train/loss': 0.1474127322435379, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0539977550506592, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.8422508239746094, 'test/num_examples': 10000, 'score': 117364.76794171333, 'total_duration': 121410.36559605598, 'accumulated_submission_time': 117364.76794171333, 'accumulated_eval_time': 4019.2500882148743, 'accumulated_logging_time': 13.76552438735962}
I0301 14:57:49.563848 140089845819136 logging_writer.py:48] [350591] accumulated_eval_time=4019.250088, accumulated_logging_time=13.765524, accumulated_submission_time=117364.767942, global_step=350591, preemption_count=0, score=117364.767942, test/accuracy=0.626600, test/loss=1.842251, test/num_examples=10000, total_duration=121410.365596, train/accuracy=0.960220, train/loss=0.147413, validation/accuracy=0.755920, validation/loss=1.053998, validation/num_examples=50000
I0301 14:57:52.907088 140089870997248 logging_writer.py:48] [350600] global_step=350600, grad_norm=4.998286247253418, loss=0.6123718023300171
I0301 14:58:26.429788 140089845819136 logging_writer.py:48] [350700] global_step=350700, grad_norm=4.808105945587158, loss=0.5311043858528137
I0301 14:58:59.880776 140089870997248 logging_writer.py:48] [350800] global_step=350800, grad_norm=4.222891330718994, loss=0.6097906827926636
I0301 14:59:33.352275 140089845819136 logging_writer.py:48] [350900] global_step=350900, grad_norm=4.8349223136901855, loss=0.7141973972320557
I0301 15:00:06.828771 140089870997248 logging_writer.py:48] [351000] global_step=351000, grad_norm=4.596806049346924, loss=0.6287463903427124
I0301 15:00:40.262748 140089845819136 logging_writer.py:48] [351100] global_step=351100, grad_norm=4.676934242248535, loss=0.5760684609413147
I0301 15:01:13.722106 140089870997248 logging_writer.py:48] [351200] global_step=351200, grad_norm=4.546255111694336, loss=0.6313453316688538
I0301 15:01:47.260551 140089845819136 logging_writer.py:48] [351300] global_step=351300, grad_norm=4.349263668060303, loss=0.5998861789703369
I0301 15:02:20.737916 140089870997248 logging_writer.py:48] [351400] global_step=351400, grad_norm=5.020261764526367, loss=0.7371373176574707
I0301 15:02:54.203969 140089845819136 logging_writer.py:48] [351500] global_step=351500, grad_norm=4.536487102508545, loss=0.6453839540481567
I0301 15:03:27.641417 140089870997248 logging_writer.py:48] [351600] global_step=351600, grad_norm=4.68890905380249, loss=0.5831512808799744
I0301 15:04:01.117306 140089845819136 logging_writer.py:48] [351700] global_step=351700, grad_norm=4.167508125305176, loss=0.5421911478042603
I0301 15:04:34.679161 140089870997248 logging_writer.py:48] [351800] global_step=351800, grad_norm=4.532872676849365, loss=0.5742034912109375
I0301 15:05:08.122192 140089845819136 logging_writer.py:48] [351900] global_step=351900, grad_norm=4.3326616287231445, loss=0.6275950074195862
I0301 15:05:41.594054 140089870997248 logging_writer.py:48] [352000] global_step=352000, grad_norm=4.244888782501221, loss=0.5971096158027649
I0301 15:06:15.031481 140089845819136 logging_writer.py:48] [352100] global_step=352100, grad_norm=4.693994998931885, loss=0.6070160269737244
I0301 15:06:19.528280 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:06:25.737263 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:06:33.794781 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:06:36.046173 140252611495744 submission_runner.py:411] Time since start: 121936.93s, 	Step: 352115, 	{'train/accuracy': 0.9626514315605164, 'train/loss': 0.1426747590303421, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.054054856300354, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8433915376663208, 'test/num_examples': 10000, 'score': 117874.66694450378, 'total_duration': 121936.92624044418, 'accumulated_submission_time': 117874.66694450378, 'accumulated_eval_time': 4035.7679233551025, 'accumulated_logging_time': 13.854008674621582}
I0301 15:06:36.125794 140089770305280 logging_writer.py:48] [352115] accumulated_eval_time=4035.767923, accumulated_logging_time=13.854009, accumulated_submission_time=117874.666945, global_step=352115, preemption_count=0, score=117874.666945, test/accuracy=0.628000, test/loss=1.843392, test/num_examples=10000, total_duration=121936.926240, train/accuracy=0.962651, train/loss=0.142675, validation/accuracy=0.755760, validation/loss=1.054055, validation/num_examples=50000
I0301 15:07:04.906492 140089778697984 logging_writer.py:48] [352200] global_step=352200, grad_norm=4.592097759246826, loss=0.6602169871330261
I0301 15:07:38.332640 140089770305280 logging_writer.py:48] [352300] global_step=352300, grad_norm=5.320481777191162, loss=0.6421390771865845
I0301 15:08:11.795610 140089778697984 logging_writer.py:48] [352400] global_step=352400, grad_norm=5.2248616218566895, loss=0.6800363063812256
I0301 15:08:45.279173 140089770305280 logging_writer.py:48] [352500] global_step=352500, grad_norm=4.683184623718262, loss=0.6279019117355347
I0301 15:09:18.777512 140089778697984 logging_writer.py:48] [352600] global_step=352600, grad_norm=4.297869682312012, loss=0.49970629811286926
I0301 15:09:52.233948 140089770305280 logging_writer.py:48] [352700] global_step=352700, grad_norm=4.911219596862793, loss=0.5977182984352112
I0301 15:10:25.756024 140089778697984 logging_writer.py:48] [352800] global_step=352800, grad_norm=4.709214687347412, loss=0.6529324650764465
I0301 15:10:59.251123 140089770305280 logging_writer.py:48] [352900] global_step=352900, grad_norm=4.327799320220947, loss=0.548867404460907
I0301 15:11:32.708818 140089778697984 logging_writer.py:48] [353000] global_step=353000, grad_norm=4.754088878631592, loss=0.6960822343826294
I0301 15:12:06.156522 140089770305280 logging_writer.py:48] [353100] global_step=353100, grad_norm=4.377559661865234, loss=0.5655990839004517
I0301 15:12:39.655193 140089778697984 logging_writer.py:48] [353200] global_step=353200, grad_norm=4.066921234130859, loss=0.5253090262413025
I0301 15:13:13.132116 140089770305280 logging_writer.py:48] [353300] global_step=353300, grad_norm=4.001518726348877, loss=0.5390612483024597
I0301 15:13:46.615424 140089778697984 logging_writer.py:48] [353400] global_step=353400, grad_norm=4.490982532501221, loss=0.5712742805480957
I0301 15:14:20.070254 140089770305280 logging_writer.py:48] [353500] global_step=353500, grad_norm=4.655061721801758, loss=0.6084593534469604
I0301 15:14:53.541538 140089778697984 logging_writer.py:48] [353600] global_step=353600, grad_norm=4.693323612213135, loss=0.5891308784484863
I0301 15:15:06.067305 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:15:12.147285 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:15:20.307123 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:15:22.623336 140252611495744 submission_runner.py:411] Time since start: 122463.50s, 	Step: 353639, 	{'train/accuracy': 0.9620535373687744, 'train/loss': 0.14172083139419556, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0543521642684937, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8451670408248901, 'test/num_examples': 10000, 'score': 118384.54222488403, 'total_duration': 122463.50339007378, 'accumulated_submission_time': 118384.54222488403, 'accumulated_eval_time': 4052.3238999843597, 'accumulated_logging_time': 13.944149494171143}
I0301 15:15:22.700881 140089854211840 logging_writer.py:48] [353639] accumulated_eval_time=4052.323900, accumulated_logging_time=13.944149, accumulated_submission_time=118384.542225, global_step=353639, preemption_count=0, score=118384.542225, test/accuracy=0.627500, test/loss=1.845167, test/num_examples=10000, total_duration=122463.503390, train/accuracy=0.962054, train/loss=0.141721, validation/accuracy=0.755940, validation/loss=1.054352, validation/num_examples=50000
I0301 15:15:43.456768 140089862604544 logging_writer.py:48] [353700] global_step=353700, grad_norm=4.553755283355713, loss=0.6158000230789185
I0301 15:16:16.888098 140089854211840 logging_writer.py:48] [353800] global_step=353800, grad_norm=4.555608749389648, loss=0.6415411829948425
I0301 15:16:50.495287 140089862604544 logging_writer.py:48] [353900] global_step=353900, grad_norm=4.746703147888184, loss=0.6424193382263184
I0301 15:17:23.938399 140089854211840 logging_writer.py:48] [354000] global_step=354000, grad_norm=4.4344305992126465, loss=0.577670156955719
I0301 15:17:57.403701 140089862604544 logging_writer.py:48] [354100] global_step=354100, grad_norm=4.645790100097656, loss=0.6389087438583374
I0301 15:18:30.864975 140089854211840 logging_writer.py:48] [354200] global_step=354200, grad_norm=5.004940032958984, loss=0.67975252866745
I0301 15:19:04.315586 140089862604544 logging_writer.py:48] [354300] global_step=354300, grad_norm=4.6414594650268555, loss=0.6498972177505493
I0301 15:19:37.839533 140089854211840 logging_writer.py:48] [354400] global_step=354400, grad_norm=5.268594264984131, loss=0.6535013318061829
I0301 15:20:11.312683 140089862604544 logging_writer.py:48] [354500] global_step=354500, grad_norm=4.871385097503662, loss=0.6716378927230835
I0301 15:20:44.832813 140089854211840 logging_writer.py:48] [354600] global_step=354600, grad_norm=3.8485524654388428, loss=0.5420273542404175
I0301 15:21:18.290476 140089862604544 logging_writer.py:48] [354700] global_step=354700, grad_norm=4.871304035186768, loss=0.5865367650985718
I0301 15:21:51.764398 140089854211840 logging_writer.py:48] [354800] global_step=354800, grad_norm=4.726465225219727, loss=0.6868244409561157
I0301 15:22:25.327789 140089862604544 logging_writer.py:48] [354900] global_step=354900, grad_norm=5.088848114013672, loss=0.6493018865585327
I0301 15:22:58.817379 140089854211840 logging_writer.py:48] [355000] global_step=355000, grad_norm=4.26563835144043, loss=0.6050823330879211
I0301 15:23:32.296282 140089862604544 logging_writer.py:48] [355100] global_step=355100, grad_norm=4.285511493682861, loss=0.5935807824134827
I0301 15:23:52.859116 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:23:59.252795 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:24:07.488045 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:24:09.792938 140252611495744 submission_runner.py:411] Time since start: 122990.67s, 	Step: 355163, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14469733834266663, 'validation/accuracy': 0.7563599944114685, 'validation/loss': 1.0527466535568237, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8399124145507812, 'test/num_examples': 10000, 'score': 118894.63495087624, 'total_duration': 122990.67299675941, 'accumulated_submission_time': 118894.63495087624, 'accumulated_eval_time': 4069.257670402527, 'accumulated_logging_time': 14.031378030776978}
I0301 15:24:09.867374 140089837426432 logging_writer.py:48] [355163] accumulated_eval_time=4069.257670, accumulated_logging_time=14.031378, accumulated_submission_time=118894.634951, global_step=355163, preemption_count=0, score=118894.634951, test/accuracy=0.626800, test/loss=1.839912, test/num_examples=10000, total_duration=122990.672997, train/accuracy=0.961655, train/loss=0.144697, validation/accuracy=0.756360, validation/loss=1.052747, validation/num_examples=50000
I0301 15:24:22.585571 140089845819136 logging_writer.py:48] [355200] global_step=355200, grad_norm=4.762151718139648, loss=0.5937920808792114
I0301 15:24:56.065983 140089837426432 logging_writer.py:48] [355300] global_step=355300, grad_norm=4.896486282348633, loss=0.6460831761360168
I0301 15:25:29.520797 140089845819136 logging_writer.py:48] [355400] global_step=355400, grad_norm=4.863089084625244, loss=0.5794146060943604
I0301 15:26:02.995377 140089837426432 logging_writer.py:48] [355500] global_step=355500, grad_norm=4.286962509155273, loss=0.6196179389953613
I0301 15:26:36.451637 140089845819136 logging_writer.py:48] [355600] global_step=355600, grad_norm=3.995392084121704, loss=0.5257813334465027
I0301 15:27:09.889017 140089837426432 logging_writer.py:48] [355700] global_step=355700, grad_norm=4.673470973968506, loss=0.6093234419822693
I0301 15:27:43.360831 140089845819136 logging_writer.py:48] [355800] global_step=355800, grad_norm=5.08687162399292, loss=0.643048882484436
I0301 15:28:16.875424 140089837426432 logging_writer.py:48] [355900] global_step=355900, grad_norm=4.511786937713623, loss=0.5656301975250244
I0301 15:28:50.416484 140089845819136 logging_writer.py:48] [356000] global_step=356000, grad_norm=4.171566963195801, loss=0.5871323943138123
I0301 15:29:23.921581 140089837426432 logging_writer.py:48] [356100] global_step=356100, grad_norm=4.471042156219482, loss=0.6089711785316467
I0301 15:29:57.366806 140089845819136 logging_writer.py:48] [356200] global_step=356200, grad_norm=4.247884750366211, loss=0.5766009092330933
I0301 15:30:30.852508 140089837426432 logging_writer.py:48] [356300] global_step=356300, grad_norm=4.621005535125732, loss=0.5949915647506714
I0301 15:31:04.385835 140089845819136 logging_writer.py:48] [356400] global_step=356400, grad_norm=4.988552570343018, loss=0.6547667384147644
I0301 15:31:37.866928 140089837426432 logging_writer.py:48] [356500] global_step=356500, grad_norm=4.152833938598633, loss=0.5579797625541687
I0301 15:32:11.319369 140089845819136 logging_writer.py:48] [356600] global_step=356600, grad_norm=4.497147560119629, loss=0.5132625699043274
I0301 15:32:39.923706 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:32:46.038853 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:32:54.216328 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:32:56.500458 140252611495744 submission_runner.py:411] Time since start: 123517.38s, 	Step: 356687, 	{'train/accuracy': 0.9599210619926453, 'train/loss': 0.14801307022571564, 'validation/accuracy': 0.7562999725341797, 'validation/loss': 1.0537092685699463, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8429639339447021, 'test/num_examples': 10000, 'score': 119404.62611865997, 'total_duration': 123517.38052773476, 'accumulated_submission_time': 119404.62611865997, 'accumulated_eval_time': 4085.834371328354, 'accumulated_logging_time': 14.115801095962524}
I0301 15:32:56.578301 140089778697984 logging_writer.py:48] [356687] accumulated_eval_time=4085.834371, accumulated_logging_time=14.115801, accumulated_submission_time=119404.626119, global_step=356687, preemption_count=0, score=119404.626119, test/accuracy=0.626800, test/loss=1.842964, test/num_examples=10000, total_duration=123517.380528, train/accuracy=0.959921, train/loss=0.148013, validation/accuracy=0.756300, validation/loss=1.053709, validation/num_examples=50000
I0301 15:33:01.258032 140089837426432 logging_writer.py:48] [356700] global_step=356700, grad_norm=4.324127197265625, loss=0.6040835976600647
I0301 15:33:34.690749 140089778697984 logging_writer.py:48] [356800] global_step=356800, grad_norm=4.276443004608154, loss=0.5970985293388367
I0301 15:34:08.164934 140089837426432 logging_writer.py:48] [356900] global_step=356900, grad_norm=4.457599639892578, loss=0.6355350613594055
I0301 15:34:41.730165 140089778697984 logging_writer.py:48] [357000] global_step=357000, grad_norm=5.018612861633301, loss=0.6946983337402344
I0301 15:35:15.167891 140089837426432 logging_writer.py:48] [357100] global_step=357100, grad_norm=4.703395366668701, loss=0.6552682518959045
I0301 15:35:48.620394 140089778697984 logging_writer.py:48] [357200] global_step=357200, grad_norm=4.542481899261475, loss=0.5757790803909302
I0301 15:36:22.129664 140089837426432 logging_writer.py:48] [357300] global_step=357300, grad_norm=4.568016052246094, loss=0.6213826537132263
I0301 15:36:55.549919 140089778697984 logging_writer.py:48] [357400] global_step=357400, grad_norm=4.831604957580566, loss=0.6621498465538025
I0301 15:37:28.999976 140089837426432 logging_writer.py:48] [357500] global_step=357500, grad_norm=4.741724967956543, loss=0.6238434314727783
I0301 15:38:02.434778 140089778697984 logging_writer.py:48] [357600] global_step=357600, grad_norm=4.644374847412109, loss=0.5846078395843506
I0301 15:38:35.920723 140089837426432 logging_writer.py:48] [357700] global_step=357700, grad_norm=5.095616817474365, loss=0.6229540109634399
I0301 15:39:09.357877 140089778697984 logging_writer.py:48] [357800] global_step=357800, grad_norm=4.574927806854248, loss=0.6506393551826477
I0301 15:39:42.833652 140089837426432 logging_writer.py:48] [357900] global_step=357900, grad_norm=4.350752353668213, loss=0.5722590684890747
I0301 15:40:16.310306 140089778697984 logging_writer.py:48] [358000] global_step=358000, grad_norm=4.256233215332031, loss=0.62945556640625
I0301 15:40:49.874862 140089837426432 logging_writer.py:48] [358100] global_step=358100, grad_norm=4.403448581695557, loss=0.5899952054023743
I0301 15:41:23.356878 140089778697984 logging_writer.py:48] [358200] global_step=358200, grad_norm=4.250397205352783, loss=0.5949593782424927
I0301 15:41:26.514292 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:41:32.590250 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:41:40.765064 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:41:43.118476 140252611495744 submission_runner.py:411] Time since start: 124044.00s, 	Step: 358211, 	{'train/accuracy': 0.9610769748687744, 'train/loss': 0.14486491680145264, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.0543800592422485, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8446106910705566, 'test/num_examples': 10000, 'score': 119914.49568676949, 'total_duration': 124043.99854040146, 'accumulated_submission_time': 119914.49568676949, 'accumulated_eval_time': 4102.438501119614, 'accumulated_logging_time': 14.206216812133789}
I0301 15:41:43.197376 140089778697984 logging_writer.py:48] [358211] accumulated_eval_time=4102.438501, accumulated_logging_time=14.206217, accumulated_submission_time=119914.495687, global_step=358211, preemption_count=0, score=119914.495687, test/accuracy=0.627800, test/loss=1.844611, test/num_examples=10000, total_duration=124043.998540, train/accuracy=0.961077, train/loss=0.144865, validation/accuracy=0.755700, validation/loss=1.054380, validation/num_examples=50000
I0301 15:42:13.316846 140089837426432 logging_writer.py:48] [358300] global_step=358300, grad_norm=4.770573616027832, loss=0.5972250699996948
I0301 15:42:46.773071 140089778697984 logging_writer.py:48] [358400] global_step=358400, grad_norm=4.266933917999268, loss=0.5985040664672852
I0301 15:43:20.247134 140089837426432 logging_writer.py:48] [358500] global_step=358500, grad_norm=4.532420635223389, loss=0.689914345741272
I0301 15:43:53.727537 140089778697984 logging_writer.py:48] [358600] global_step=358600, grad_norm=4.366515159606934, loss=0.6431180834770203
I0301 15:44:27.180075 140089837426432 logging_writer.py:48] [358700] global_step=358700, grad_norm=4.349213600158691, loss=0.5788276791572571
I0301 15:45:00.641209 140089778697984 logging_writer.py:48] [358800] global_step=358800, grad_norm=4.894794464111328, loss=0.7208842635154724
I0301 15:45:34.118513 140089837426432 logging_writer.py:48] [358900] global_step=358900, grad_norm=4.240839958190918, loss=0.5248339772224426
I0301 15:46:07.571145 140089778697984 logging_writer.py:48] [359000] global_step=359000, grad_norm=4.620455741882324, loss=0.6382595300674438
I0301 15:46:41.161536 140089837426432 logging_writer.py:48] [359100] global_step=359100, grad_norm=5.1978654861450195, loss=0.6439368724822998
I0301 15:47:14.594717 140089778697984 logging_writer.py:48] [359200] global_step=359200, grad_norm=4.5724663734436035, loss=0.6507539749145508
I0301 15:47:48.110575 140089837426432 logging_writer.py:48] [359300] global_step=359300, grad_norm=4.174823760986328, loss=0.6403942108154297
I0301 15:48:21.596206 140089778697984 logging_writer.py:48] [359400] global_step=359400, grad_norm=4.777977466583252, loss=0.6569539308547974
I0301 15:48:55.044158 140089837426432 logging_writer.py:48] [359500] global_step=359500, grad_norm=4.974236965179443, loss=0.6751872301101685
I0301 15:49:28.507662 140089778697984 logging_writer.py:48] [359600] global_step=359600, grad_norm=4.6828460693359375, loss=0.6613849401473999
I0301 15:50:01.991567 140089837426432 logging_writer.py:48] [359700] global_step=359700, grad_norm=4.361337661743164, loss=0.6751977205276489
I0301 15:50:13.171072 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:50:19.336994 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:50:27.470369 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:50:29.747350 140252611495744 submission_runner.py:411] Time since start: 124570.63s, 	Step: 359735, 	{'train/accuracy': 0.9620934128761292, 'train/loss': 0.14297376573085785, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0538039207458496, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8425129652023315, 'test/num_examples': 10000, 'score': 120424.40429615974, 'total_duration': 124570.62741327286, 'accumulated_submission_time': 120424.40429615974, 'accumulated_eval_time': 4119.014720439911, 'accumulated_logging_time': 14.295626163482666}
I0301 15:50:29.826304 140089870997248 logging_writer.py:48] [359735] accumulated_eval_time=4119.014720, accumulated_logging_time=14.295626, accumulated_submission_time=120424.404296, global_step=359735, preemption_count=0, score=120424.404296, test/accuracy=0.626700, test/loss=1.842513, test/num_examples=10000, total_duration=124570.627413, train/accuracy=0.962093, train/loss=0.142974, validation/accuracy=0.755940, validation/loss=1.053804, validation/num_examples=50000
I0301 15:50:51.915082 140089879389952 logging_writer.py:48] [359800] global_step=359800, grad_norm=4.481720924377441, loss=0.5877479910850525
I0301 15:51:25.393148 140089870997248 logging_writer.py:48] [359900] global_step=359900, grad_norm=4.842878341674805, loss=0.627571165561676
I0301 15:51:58.906889 140089879389952 logging_writer.py:48] [360000] global_step=360000, grad_norm=4.7709784507751465, loss=0.6502143144607544
I0301 15:52:32.376543 140089870997248 logging_writer.py:48] [360100] global_step=360100, grad_norm=4.548736095428467, loss=0.5751439929008484
I0301 15:53:05.925170 140089879389952 logging_writer.py:48] [360200] global_step=360200, grad_norm=4.788381099700928, loss=0.6364977955818176
I0301 15:53:39.416465 140089870997248 logging_writer.py:48] [360300] global_step=360300, grad_norm=4.31802225112915, loss=0.5983803272247314
I0301 15:54:12.860638 140089879389952 logging_writer.py:48] [360400] global_step=360400, grad_norm=4.645254611968994, loss=0.6112074255943298
I0301 15:54:46.341168 140089870997248 logging_writer.py:48] [360500] global_step=360500, grad_norm=4.347896575927734, loss=0.6175717115402222
I0301 15:55:19.800043 140089879389952 logging_writer.py:48] [360600] global_step=360600, grad_norm=4.450770378112793, loss=0.5689912438392639
I0301 15:55:53.253089 140089870997248 logging_writer.py:48] [360700] global_step=360700, grad_norm=4.083418846130371, loss=0.6074368953704834
I0301 15:56:26.703556 140089879389952 logging_writer.py:48] [360800] global_step=360800, grad_norm=4.602015972137451, loss=0.6104084253311157
I0301 15:57:00.161627 140089870997248 logging_writer.py:48] [360900] global_step=360900, grad_norm=4.76215124130249, loss=0.6051905751228333
I0301 15:57:33.649868 140089879389952 logging_writer.py:48] [361000] global_step=361000, grad_norm=4.3097357749938965, loss=0.581267237663269
I0301 15:58:07.134506 140089870997248 logging_writer.py:48] [361100] global_step=361100, grad_norm=4.465329170227051, loss=0.6158145070075989
I0301 15:58:40.560883 140089879389952 logging_writer.py:48] [361200] global_step=361200, grad_norm=4.517333984375, loss=0.6105576157569885
I0301 15:58:59.876377 140252611495744 spec.py:321] Evaluating on the training split.
I0301 15:59:06.209783 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 15:59:14.350222 140252611495744 spec.py:349] Evaluating on the test split.
I0301 15:59:16.652416 140252611495744 submission_runner.py:411] Time since start: 125097.53s, 	Step: 361259, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.1437092125415802, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0533498525619507, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8423242568969727, 'test/num_examples': 10000, 'score': 120934.38921999931, 'total_duration': 125097.53248429298, 'accumulated_submission_time': 120934.38921999931, 'accumulated_eval_time': 4135.7907111644745, 'accumulated_logging_time': 14.385024547576904}
I0301 15:59:16.732433 140089837426432 logging_writer.py:48] [361259] accumulated_eval_time=4135.790711, accumulated_logging_time=14.385025, accumulated_submission_time=120934.389220, global_step=361259, preemption_count=0, score=120934.389220, test/accuracy=0.627100, test/loss=1.842324, test/num_examples=10000, total_duration=125097.532484, train/accuracy=0.960957, train/loss=0.143709, validation/accuracy=0.756160, validation/loss=1.053350, validation/num_examples=50000
I0301 15:59:30.787640 140089845819136 logging_writer.py:48] [361300] global_step=361300, grad_norm=4.629652500152588, loss=0.7173544764518738
I0301 16:00:04.228094 140089837426432 logging_writer.py:48] [361400] global_step=361400, grad_norm=4.852694511413574, loss=0.6065560579299927
I0301 16:00:37.680401 140089845819136 logging_writer.py:48] [361500] global_step=361500, grad_norm=4.334566116333008, loss=0.6197464466094971
I0301 16:01:11.124926 140089837426432 logging_writer.py:48] [361600] global_step=361600, grad_norm=4.368927001953125, loss=0.551651120185852
I0301 16:01:44.577319 140089845819136 logging_writer.py:48] [361700] global_step=361700, grad_norm=4.536017417907715, loss=0.6257946491241455
I0301 16:02:18.031628 140089837426432 logging_writer.py:48] [361800] global_step=361800, grad_norm=4.411801815032959, loss=0.6508209705352783
I0301 16:02:51.490341 140089845819136 logging_writer.py:48] [361900] global_step=361900, grad_norm=4.7764410972595215, loss=0.6153836846351624
I0301 16:03:24.951205 140089837426432 logging_writer.py:48] [362000] global_step=362000, grad_norm=4.205475330352783, loss=0.5942056775093079
I0301 16:03:58.405855 140089845819136 logging_writer.py:48] [362100] global_step=362100, grad_norm=4.329518795013428, loss=0.5214544534683228
I0301 16:04:31.881256 140089837426432 logging_writer.py:48] [362200] global_step=362200, grad_norm=4.271913528442383, loss=0.6086024045944214
I0301 16:05:05.437524 140089845819136 logging_writer.py:48] [362300] global_step=362300, grad_norm=4.951660633087158, loss=0.6117618680000305
I0301 16:05:38.933199 140089837426432 logging_writer.py:48] [362400] global_step=362400, grad_norm=4.339694976806641, loss=0.6434571146965027
I0301 16:06:12.396600 140089845819136 logging_writer.py:48] [362500] global_step=362500, grad_norm=4.867059707641602, loss=0.6111942529678345
I0301 16:06:45.856760 140089837426432 logging_writer.py:48] [362600] global_step=362600, grad_norm=4.383941650390625, loss=0.6172020435333252
I0301 16:07:19.316471 140089845819136 logging_writer.py:48] [362700] global_step=362700, grad_norm=4.703757286071777, loss=0.6776143312454224
I0301 16:07:46.888676 140252611495744 spec.py:321] Evaluating on the training split.
I0301 16:07:53.061877 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 16:08:01.155719 140252611495744 spec.py:349] Evaluating on the test split.
I0301 16:08:03.543858 140252611495744 submission_runner.py:411] Time since start: 125624.42s, 	Step: 362784, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14759178459644318, 'validation/accuracy': 0.7564399838447571, 'validation/loss': 1.053044080734253, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8418200016021729, 'test/num_examples': 10000, 'score': 121444.4788107872, 'total_duration': 125624.42392516136, 'accumulated_submission_time': 121444.4788107872, 'accumulated_eval_time': 4152.445852518082, 'accumulated_logging_time': 14.476088762283325}
I0301 16:08:03.624107 140089770305280 logging_writer.py:48] [362784] accumulated_eval_time=4152.445853, accumulated_logging_time=14.476089, accumulated_submission_time=121444.478811, global_step=362784, preemption_count=0, score=121444.478811, test/accuracy=0.628200, test/loss=1.841820, test/num_examples=10000, total_duration=125624.423925, train/accuracy=0.961057, train/loss=0.147592, validation/accuracy=0.756440, validation/loss=1.053044, validation/num_examples=50000
I0301 16:08:09.316967 140089778697984 logging_writer.py:48] [362800] global_step=362800, grad_norm=4.6454973220825195, loss=0.6871324777603149
I0301 16:08:42.835882 140089770305280 logging_writer.py:48] [362900] global_step=362900, grad_norm=4.217957973480225, loss=0.58054119348526
I0301 16:09:16.259306 140089778697984 logging_writer.py:48] [363000] global_step=363000, grad_norm=4.842221736907959, loss=0.660994827747345
I0301 16:09:49.710245 140089770305280 logging_writer.py:48] [363100] global_step=363100, grad_norm=4.420446872711182, loss=0.602866530418396
I0301 16:10:23.163487 140089778697984 logging_writer.py:48] [363200] global_step=363200, grad_norm=4.620275974273682, loss=0.5702724456787109
I0301 16:10:56.597744 140089770305280 logging_writer.py:48] [363300] global_step=363300, grad_norm=4.903365135192871, loss=0.6761301755905151
I0301 16:11:30.178913 140089778697984 logging_writer.py:48] [363400] global_step=363400, grad_norm=4.613069534301758, loss=0.6483797430992126
I0301 16:12:03.631530 140089770305280 logging_writer.py:48] [363500] global_step=363500, grad_norm=4.414269924163818, loss=0.5574356317520142
I0301 16:12:37.100889 140089778697984 logging_writer.py:48] [363600] global_step=363600, grad_norm=4.737766742706299, loss=0.6742584705352783
I0301 16:13:10.544949 140089770305280 logging_writer.py:48] [363700] global_step=363700, grad_norm=4.49430513381958, loss=0.5683935284614563
I0301 16:13:44.011838 140089778697984 logging_writer.py:48] [363800] global_step=363800, grad_norm=4.355673313140869, loss=0.5988729596138
I0301 16:14:17.473633 140089770305280 logging_writer.py:48] [363900] global_step=363900, grad_norm=4.455784320831299, loss=0.6712022423744202
I0301 16:14:50.917335 140089778697984 logging_writer.py:48] [364000] global_step=364000, grad_norm=5.144180774688721, loss=0.6239896416664124
I0301 16:15:24.385833 140089770305280 logging_writer.py:48] [364100] global_step=364100, grad_norm=4.4711079597473145, loss=0.6036711931228638
I0301 16:15:57.827716 140089778697984 logging_writer.py:48] [364200] global_step=364200, grad_norm=5.110280990600586, loss=0.6265706419944763
I0301 16:16:31.288476 140089770305280 logging_writer.py:48] [364300] global_step=364300, grad_norm=4.389256477355957, loss=0.5833501815795898
I0301 16:16:33.778124 140252611495744 spec.py:321] Evaluating on the training split.
I0301 16:16:39.904997 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 16:16:48.073622 140252611495744 spec.py:349] Evaluating on the test split.
I0301 16:16:50.400636 140252611495744 submission_runner.py:411] Time since start: 126151.28s, 	Step: 364309, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.1446724534034729, 'validation/accuracy': 0.7563199996948242, 'validation/loss': 1.0528602600097656, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.841619849205017, 'test/num_examples': 10000, 'score': 121954.5669374466, 'total_duration': 126151.28058385849, 'accumulated_submission_time': 121954.5669374466, 'accumulated_eval_time': 4169.068185567856, 'accumulated_logging_time': 14.566959142684937}
I0301 16:16:50.489242 140089770305280 logging_writer.py:48] [364309] accumulated_eval_time=4169.068186, accumulated_logging_time=14.566959, accumulated_submission_time=121954.566937, global_step=364309, preemption_count=0, score=121954.566937, test/accuracy=0.627900, test/loss=1.841620, test/num_examples=10000, total_duration=126151.280584, train/accuracy=0.960599, train/loss=0.144672, validation/accuracy=0.756320, validation/loss=1.052860, validation/num_examples=50000
I0301 16:17:21.324274 140089778697984 logging_writer.py:48] [364400] global_step=364400, grad_norm=4.912840366363525, loss=0.6958878636360168
I0301 16:17:54.779913 140089770305280 logging_writer.py:48] [364500] global_step=364500, grad_norm=4.426610946655273, loss=0.6154803037643433
I0301 16:18:28.261191 140089778697984 logging_writer.py:48] [364600] global_step=364600, grad_norm=5.416842937469482, loss=0.6620181798934937
I0301 16:19:01.690363 140089770305280 logging_writer.py:48] [364700] global_step=364700, grad_norm=4.9065446853637695, loss=0.6805388331413269
I0301 16:19:35.146219 140089778697984 logging_writer.py:48] [364800] global_step=364800, grad_norm=4.598291397094727, loss=0.5486193895339966
I0301 16:20:08.586448 140089770305280 logging_writer.py:48] [364900] global_step=364900, grad_norm=4.371596813201904, loss=0.5760294198989868
I0301 16:20:42.020542 140089778697984 logging_writer.py:48] [365000] global_step=365000, grad_norm=4.658942699432373, loss=0.6635457873344421
I0301 16:21:15.496102 140089770305280 logging_writer.py:48] [365100] global_step=365100, grad_norm=4.46658182144165, loss=0.6270907521247864
I0301 16:21:48.957989 140089778697984 logging_writer.py:48] [365200] global_step=365200, grad_norm=4.359466075897217, loss=0.6226532459259033
I0301 16:22:22.430201 140089770305280 logging_writer.py:48] [365300] global_step=365300, grad_norm=4.846637725830078, loss=0.7208201885223389
I0301 16:22:55.921558 140089778697984 logging_writer.py:48] [365400] global_step=365400, grad_norm=5.1785807609558105, loss=0.6660182476043701
I0301 16:23:29.492411 140089770305280 logging_writer.py:48] [365500] global_step=365500, grad_norm=4.3422465324401855, loss=0.6177549958229065
I0301 16:24:02.969301 140089778697984 logging_writer.py:48] [365600] global_step=365600, grad_norm=4.651750087738037, loss=0.6190230846405029
I0301 16:24:36.472263 140089770305280 logging_writer.py:48] [365700] global_step=365700, grad_norm=5.008306980133057, loss=0.5894513726234436
I0301 16:25:09.928566 140089778697984 logging_writer.py:48] [365800] global_step=365800, grad_norm=4.3343987464904785, loss=0.5993870496749878
I0301 16:25:20.426430 140252611495744 spec.py:321] Evaluating on the training split.
I0301 16:25:26.622282 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 16:25:34.557941 140252611495744 spec.py:349] Evaluating on the test split.
I0301 16:25:36.884994 140252611495744 submission_runner.py:411] Time since start: 126677.76s, 	Step: 365833, 	{'train/accuracy': 0.9619140625, 'train/loss': 0.1466599553823471, 'validation/accuracy': 0.7559599876403809, 'validation/loss': 1.0532474517822266, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8416664600372314, 'test/num_examples': 10000, 'score': 122464.43940424919, 'total_duration': 126677.76492261887, 'accumulated_submission_time': 122464.43940424919, 'accumulated_eval_time': 4185.5265572071075, 'accumulated_logging_time': 14.665619611740112}
I0301 16:25:36.964598 140089778697984 logging_writer.py:48] [365833] accumulated_eval_time=4185.526557, accumulated_logging_time=14.665620, accumulated_submission_time=122464.439404, global_step=365833, preemption_count=0, score=122464.439404, test/accuracy=0.626800, test/loss=1.841666, test/num_examples=10000, total_duration=126677.764923, train/accuracy=0.961914, train/loss=0.146660, validation/accuracy=0.755960, validation/loss=1.053247, validation/num_examples=50000
I0301 16:25:59.690504 140089837426432 logging_writer.py:48] [365900] global_step=365900, grad_norm=4.4160847663879395, loss=0.5714051723480225
I0301 16:26:33.141345 140089778697984 logging_writer.py:48] [366000] global_step=366000, grad_norm=4.79951286315918, loss=0.6988641023635864
I0301 16:27:06.586686 140089837426432 logging_writer.py:48] [366100] global_step=366100, grad_norm=4.639421463012695, loss=0.6518521308898926
I0301 16:27:40.033422 140089778697984 logging_writer.py:48] [366200] global_step=366200, grad_norm=4.1407036781311035, loss=0.6132067441940308
I0301 16:28:13.503772 140089837426432 logging_writer.py:48] [366300] global_step=366300, grad_norm=4.631946563720703, loss=0.6128295660018921
I0301 16:28:46.991121 140089778697984 logging_writer.py:48] [366400] global_step=366400, grad_norm=4.4228386878967285, loss=0.5524544715881348
I0301 16:29:20.544159 140089837426432 logging_writer.py:48] [366500] global_step=366500, grad_norm=4.648926258087158, loss=0.6990101337432861
I0301 16:29:54.046283 140089778697984 logging_writer.py:48] [366600] global_step=366600, grad_norm=4.615699768066406, loss=0.5416780114173889
I0301 16:30:27.462758 140089837426432 logging_writer.py:48] [366700] global_step=366700, grad_norm=4.623208999633789, loss=0.6815088987350464
I0301 16:31:00.938477 140089778697984 logging_writer.py:48] [366800] global_step=366800, grad_norm=4.3407793045043945, loss=0.5962990522384644
I0301 16:31:34.348180 140089837426432 logging_writer.py:48] [366900] global_step=366900, grad_norm=5.162810325622559, loss=0.7096777558326721
I0301 16:32:07.798995 140089778697984 logging_writer.py:48] [367000] global_step=367000, grad_norm=4.466498851776123, loss=0.6057228446006775
I0301 16:32:41.260716 140089837426432 logging_writer.py:48] [367100] global_step=367100, grad_norm=4.546741962432861, loss=0.6445058584213257
I0301 16:33:14.738682 140089778697984 logging_writer.py:48] [367200] global_step=367200, grad_norm=4.377045154571533, loss=0.6872740983963013
I0301 16:33:48.171141 140089837426432 logging_writer.py:48] [367300] global_step=367300, grad_norm=4.489026069641113, loss=0.5569616556167603
I0301 16:34:07.052530 140252611495744 spec.py:321] Evaluating on the training split.
I0301 16:34:13.213558 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 16:34:21.245375 140252611495744 spec.py:349] Evaluating on the test split.
I0301 16:34:23.542335 140252611495744 submission_runner.py:411] Time since start: 127204.42s, 	Step: 367358, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.1442684829235077, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 1.0540716648101807, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8430508375167847, 'test/num_examples': 10000, 'score': 122974.46217608452, 'total_duration': 127204.42240476608, 'accumulated_submission_time': 122974.46217608452, 'accumulated_eval_time': 4202.016310453415, 'accumulated_logging_time': 14.755435466766357}
I0301 16:34:23.622105 140089778697984 logging_writer.py:48] [367358] accumulated_eval_time=4202.016310, accumulated_logging_time=14.755435, accumulated_submission_time=122974.462176, global_step=367358, preemption_count=0, score=122974.462176, test/accuracy=0.627000, test/loss=1.843051, test/num_examples=10000, total_duration=127204.422405, train/accuracy=0.961496, train/loss=0.144268, validation/accuracy=0.756280, validation/loss=1.054072, validation/num_examples=50000
I0301 16:34:37.991546 140089837426432 logging_writer.py:48] [367400] global_step=367400, grad_norm=4.825026512145996, loss=0.6970987319946289
I0301 16:35:11.420446 140089778697984 logging_writer.py:48] [367500] global_step=367500, grad_norm=4.869405746459961, loss=0.6981130838394165
I0301 16:35:44.971989 140089837426432 logging_writer.py:48] [367600] global_step=367600, grad_norm=4.71116304397583, loss=0.6743453741073608
I0301 16:36:18.410970 140089778697984 logging_writer.py:48] [367700] global_step=367700, grad_norm=4.407825946807861, loss=0.5493256449699402
I0301 16:36:51.875720 140089837426432 logging_writer.py:48] [367800] global_step=367800, grad_norm=4.309931755065918, loss=0.6333385705947876
I0301 16:37:25.308714 140089778697984 logging_writer.py:48] [367900] global_step=367900, grad_norm=4.888925552368164, loss=0.6797990798950195
I0301 16:37:58.769255 140089837426432 logging_writer.py:48] [368000] global_step=368000, grad_norm=4.785171031951904, loss=0.5461073517799377
I0301 16:38:32.215815 140089778697984 logging_writer.py:48] [368100] global_step=368100, grad_norm=4.804336071014404, loss=0.6240204572677612
I0301 16:39:05.641201 140089837426432 logging_writer.py:48] [368200] global_step=368200, grad_norm=4.2538604736328125, loss=0.6237003803253174
I0301 16:39:39.093525 140089778697984 logging_writer.py:48] [368300] global_step=368300, grad_norm=4.596433162689209, loss=0.601007342338562
I0301 16:40:12.532426 140089837426432 logging_writer.py:48] [368400] global_step=368400, grad_norm=4.413167953491211, loss=0.6091641783714294
I0301 16:40:45.984502 140089778697984 logging_writer.py:48] [368500] global_step=368500, grad_norm=4.725089073181152, loss=0.5761180520057678
I0301 16:41:19.412309 140089837426432 logging_writer.py:48] [368600] global_step=368600, grad_norm=4.66227388381958, loss=0.5545881390571594
I0301 16:41:52.933753 140089778697984 logging_writer.py:48] [368700] global_step=368700, grad_norm=4.900670528411865, loss=0.5982199907302856
I0301 16:42:26.373482 140089837426432 logging_writer.py:48] [368800] global_step=368800, grad_norm=4.448562145233154, loss=0.5347446203231812
I0301 16:42:53.605857 140252611495744 spec.py:321] Evaluating on the training split.
I0301 16:42:59.703568 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 16:43:08.012181 140252611495744 spec.py:349] Evaluating on the test split.
I0301 16:43:10.272800 140252611495744 submission_runner.py:411] Time since start: 127731.15s, 	Step: 368883, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14584901928901672, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 1.0531740188598633, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.842363953590393, 'test/num_examples': 10000, 'score': 123484.38162231445, 'total_duration': 127731.15286946297, 'accumulated_submission_time': 123484.38162231445, 'accumulated_eval_time': 4218.683201313019, 'accumulated_logging_time': 14.845154762268066}
I0301 16:43:10.353391 140089778697984 logging_writer.py:48] [368883] accumulated_eval_time=4218.683201, accumulated_logging_time=14.845155, accumulated_submission_time=123484.381622, global_step=368883, preemption_count=0, score=123484.381622, test/accuracy=0.627300, test/loss=1.842364, test/num_examples=10000, total_duration=127731.152869, train/accuracy=0.960698, train/loss=0.145849, validation/accuracy=0.756280, validation/loss=1.053174, validation/num_examples=50000
I0301 16:43:16.389998 140089862604544 logging_writer.py:48] [368900] global_step=368900, grad_norm=4.2674102783203125, loss=0.6429916620254517
I0301 16:43:49.848145 140089778697984 logging_writer.py:48] [369000] global_step=369000, grad_norm=4.765386581420898, loss=0.58798748254776
I0301 16:44:23.296855 140089862604544 logging_writer.py:48] [369100] global_step=369100, grad_norm=4.761735439300537, loss=0.6517372131347656
I0301 16:44:56.748323 140089778697984 logging_writer.py:48] [369200] global_step=369200, grad_norm=4.576288223266602, loss=0.6450015902519226
I0301 16:45:30.196003 140089862604544 logging_writer.py:48] [369300] global_step=369300, grad_norm=4.487175941467285, loss=0.6272092461585999
I0301 16:46:03.648255 140089778697984 logging_writer.py:48] [369400] global_step=369400, grad_norm=4.506168365478516, loss=0.6277426481246948
I0301 16:46:37.103015 140089862604544 logging_writer.py:48] [369500] global_step=369500, grad_norm=4.525401592254639, loss=0.6460542678833008
I0301 16:47:10.553774 140089778697984 logging_writer.py:48] [369600] global_step=369600, grad_norm=4.225445747375488, loss=0.6508032083511353
I0301 16:47:44.090637 140089862604544 logging_writer.py:48] [369700] global_step=369700, grad_norm=4.793998718261719, loss=0.6494865417480469
I0301 16:48:17.565276 140089778697984 logging_writer.py:48] [369800] global_step=369800, grad_norm=4.641887187957764, loss=0.5654363632202148
I0301 16:48:51.010127 140089862604544 logging_writer.py:48] [369900] global_step=369900, grad_norm=4.310374736785889, loss=0.6490163207054138
I0301 16:49:24.469028 140089778697984 logging_writer.py:48] [370000] global_step=370000, grad_norm=4.55419397354126, loss=0.6030223369598389
I0301 16:49:57.936720 140089862604544 logging_writer.py:48] [370100] global_step=370100, grad_norm=4.965134620666504, loss=0.5867137312889099
I0301 16:50:31.367149 140089778697984 logging_writer.py:48] [370200] global_step=370200, grad_norm=4.359807014465332, loss=0.6136077046394348
I0301 16:51:04.877219 140089862604544 logging_writer.py:48] [370300] global_step=370300, grad_norm=5.193596839904785, loss=0.624430239200592
I0301 16:51:38.334141 140089778697984 logging_writer.py:48] [370400] global_step=370400, grad_norm=5.303249835968018, loss=0.6461682319641113
I0301 16:51:40.490685 140252611495744 spec.py:321] Evaluating on the training split.
I0301 16:51:46.624655 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 16:51:54.769480 140252611495744 spec.py:349] Evaluating on the test split.
I0301 16:51:57.069850 140252611495744 submission_runner.py:411] Time since start: 128257.95s, 	Step: 370408, 	{'train/accuracy': 0.9596619606018066, 'train/loss': 0.14953453838825226, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0537803173065186, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.842165470123291, 'test/num_examples': 10000, 'score': 123994.45357394218, 'total_duration': 128257.9499156475, 'accumulated_submission_time': 123994.45357394218, 'accumulated_eval_time': 4235.262308835983, 'accumulated_logging_time': 14.935875177383423}
I0301 16:51:57.147400 140089854211840 logging_writer.py:48] [370408] accumulated_eval_time=4235.262309, accumulated_logging_time=14.935875, accumulated_submission_time=123994.453574, global_step=370408, preemption_count=0, score=123994.453574, test/accuracy=0.626900, test/loss=1.842165, test/num_examples=10000, total_duration=128257.949916, train/accuracy=0.959662, train/loss=0.149535, validation/accuracy=0.756160, validation/loss=1.053780, validation/num_examples=50000
I0301 16:52:28.257242 140089879389952 logging_writer.py:48] [370500] global_step=370500, grad_norm=4.734704971313477, loss=0.6401527523994446
I0301 16:53:01.677804 140089854211840 logging_writer.py:48] [370600] global_step=370600, grad_norm=4.494412899017334, loss=0.6514990329742432
I0301 16:53:35.126662 140089879389952 logging_writer.py:48] [370700] global_step=370700, grad_norm=4.749324321746826, loss=0.5586883425712585
I0301 16:54:08.672538 140089854211840 logging_writer.py:48] [370800] global_step=370800, grad_norm=4.4619550704956055, loss=0.6098251938819885
I0301 16:54:42.102253 140089879389952 logging_writer.py:48] [370900] global_step=370900, grad_norm=4.325595378875732, loss=0.6417286992073059
I0301 16:55:15.547147 140089854211840 logging_writer.py:48] [371000] global_step=371000, grad_norm=4.709197521209717, loss=0.7142948508262634
I0301 16:55:48.988382 140089879389952 logging_writer.py:48] [371100] global_step=371100, grad_norm=4.934977054595947, loss=0.6729046702384949
I0301 16:56:22.421408 140089854211840 logging_writer.py:48] [371200] global_step=371200, grad_norm=4.490163803100586, loss=0.5482915639877319
I0301 16:56:55.869480 140089879389952 logging_writer.py:48] [371300] global_step=371300, grad_norm=4.918583393096924, loss=0.6513197422027588
I0301 16:57:29.291174 140089854211840 logging_writer.py:48] [371400] global_step=371400, grad_norm=4.7677998542785645, loss=0.6374921798706055
I0301 16:58:02.722951 140089879389952 logging_writer.py:48] [371500] global_step=371500, grad_norm=4.267775058746338, loss=0.5242176651954651
I0301 16:58:36.165429 140089854211840 logging_writer.py:48] [371600] global_step=371600, grad_norm=5.131946563720703, loss=0.6786487102508545
I0301 16:59:09.639566 140089879389952 logging_writer.py:48] [371700] global_step=371700, grad_norm=5.0373945236206055, loss=0.6514752507209778
I0301 16:59:43.135941 140089854211840 logging_writer.py:48] [371800] global_step=371800, grad_norm=4.476254463195801, loss=0.590935230255127
I0301 17:00:16.569443 140089879389952 logging_writer.py:48] [371900] global_step=371900, grad_norm=4.50490665435791, loss=0.6796789169311523
I0301 17:00:27.089298 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:00:33.232892 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:00:41.338933 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:00:43.642259 140252611495744 submission_runner.py:411] Time since start: 128784.52s, 	Step: 371933, 	{'train/accuracy': 0.9612364172935486, 'train/loss': 0.14533200860023499, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0531513690948486, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8418188095092773, 'test/num_examples': 10000, 'score': 124504.32865929604, 'total_duration': 128784.52232336998, 'accumulated_submission_time': 124504.32865929604, 'accumulated_eval_time': 4251.81521987915, 'accumulated_logging_time': 15.023420572280884}
I0301 17:00:43.723313 140089862604544 logging_writer.py:48] [371933] accumulated_eval_time=4251.815220, accumulated_logging_time=15.023421, accumulated_submission_time=124504.328659, global_step=371933, preemption_count=0, score=124504.328659, test/accuracy=0.627400, test/loss=1.841819, test/num_examples=10000, total_duration=128784.522323, train/accuracy=0.961236, train/loss=0.145332, validation/accuracy=0.756060, validation/loss=1.053151, validation/num_examples=50000
I0301 17:01:06.463960 140089879389952 logging_writer.py:48] [372000] global_step=372000, grad_norm=4.405819416046143, loss=0.6180175542831421
I0301 17:01:39.899686 140089862604544 logging_writer.py:48] [372100] global_step=372100, grad_norm=4.233121871948242, loss=0.5500288605690002
I0301 17:02:13.359280 140089879389952 logging_writer.py:48] [372200] global_step=372200, grad_norm=4.64450216293335, loss=0.703630805015564
I0301 17:02:46.826179 140089862604544 logging_writer.py:48] [372300] global_step=372300, grad_norm=4.131905555725098, loss=0.6056923866271973
I0301 17:03:20.284714 140089879389952 logging_writer.py:48] [372400] global_step=372400, grad_norm=4.489579677581787, loss=0.6142159700393677
I0301 17:03:53.735958 140089862604544 logging_writer.py:48] [372500] global_step=372500, grad_norm=5.02621603012085, loss=0.6675354838371277
I0301 17:04:27.189857 140089879389952 logging_writer.py:48] [372600] global_step=372600, grad_norm=4.719440460205078, loss=0.6457093358039856
I0301 17:05:00.636669 140089862604544 logging_writer.py:48] [372700] global_step=372700, grad_norm=4.7676472663879395, loss=0.7159412503242493
I0301 17:05:34.113161 140089879389952 logging_writer.py:48] [372800] global_step=372800, grad_norm=4.726284980773926, loss=0.6435366272926331
I0301 17:06:07.710654 140089862604544 logging_writer.py:48] [372900] global_step=372900, grad_norm=5.379613876342773, loss=0.6297941207885742
I0301 17:06:41.161295 140089879389952 logging_writer.py:48] [373000] global_step=373000, grad_norm=5.043084621429443, loss=0.6347373127937317
I0301 17:07:14.597076 140089862604544 logging_writer.py:48] [373100] global_step=373100, grad_norm=4.519508361816406, loss=0.5994431972503662
I0301 17:07:48.034747 140089879389952 logging_writer.py:48] [373200] global_step=373200, grad_norm=4.507314205169678, loss=0.6092941761016846
I0301 17:08:21.502204 140089862604544 logging_writer.py:48] [373300] global_step=373300, grad_norm=4.208778381347656, loss=0.6091006994247437
I0301 17:08:54.952114 140089879389952 logging_writer.py:48] [373400] global_step=373400, grad_norm=4.375797271728516, loss=0.5112826824188232
I0301 17:09:13.833186 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:09:19.891481 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:09:27.987713 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:09:30.283444 140252611495744 submission_runner.py:411] Time since start: 129311.16s, 	Step: 373458, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14604857563972473, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0542411804199219, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8427166938781738, 'test/num_examples': 10000, 'score': 125014.37292766571, 'total_duration': 129311.16351056099, 'accumulated_submission_time': 125014.37292766571, 'accumulated_eval_time': 4268.265427350998, 'accumulated_logging_time': 15.114747524261475}
I0301 17:09:30.361851 140089770305280 logging_writer.py:48] [373458] accumulated_eval_time=4268.265427, accumulated_logging_time=15.114748, accumulated_submission_time=125014.372928, global_step=373458, preemption_count=0, score=125014.372928, test/accuracy=0.627400, test/loss=1.842717, test/num_examples=10000, total_duration=129311.163511, train/accuracy=0.960320, train/loss=0.146049, validation/accuracy=0.756060, validation/loss=1.054241, validation/num_examples=50000
I0301 17:09:44.730039 140089778697984 logging_writer.py:48] [373500] global_step=373500, grad_norm=4.605144500732422, loss=0.6471776962280273
I0301 17:10:18.225907 140089770305280 logging_writer.py:48] [373600] global_step=373600, grad_norm=5.1513752937316895, loss=0.7037379145622253
I0301 17:10:51.709139 140089778697984 logging_writer.py:48] [373700] global_step=373700, grad_norm=4.870543479919434, loss=0.6358535289764404
I0301 17:11:25.177629 140089770305280 logging_writer.py:48] [373800] global_step=373800, grad_norm=4.367916107177734, loss=0.5943695306777954
I0301 17:11:58.768800 140089778697984 logging_writer.py:48] [373900] global_step=373900, grad_norm=4.62454891204834, loss=0.655867874622345
I0301 17:12:32.220018 140089770305280 logging_writer.py:48] [374000] global_step=374000, grad_norm=5.275877952575684, loss=0.725825309753418
I0301 17:13:05.660286 140089778697984 logging_writer.py:48] [374100] global_step=374100, grad_norm=4.435562610626221, loss=0.6073169112205505
I0301 17:13:39.162448 140089770305280 logging_writer.py:48] [374200] global_step=374200, grad_norm=4.667116641998291, loss=0.597897469997406
I0301 17:14:12.608261 140089778697984 logging_writer.py:48] [374300] global_step=374300, grad_norm=5.202804088592529, loss=0.6652225852012634
I0301 17:14:46.063160 140089770305280 logging_writer.py:48] [374400] global_step=374400, grad_norm=4.550419330596924, loss=0.5913761854171753
I0301 17:15:19.523655 140089778697984 logging_writer.py:48] [374500] global_step=374500, grad_norm=4.614497661590576, loss=0.6995574831962585
I0301 17:15:52.965649 140089770305280 logging_writer.py:48] [374600] global_step=374600, grad_norm=4.929760932922363, loss=0.6164820790290833
I0301 17:16:26.432393 140089778697984 logging_writer.py:48] [374700] global_step=374700, grad_norm=4.536629676818848, loss=0.6523293256759644
I0301 17:16:59.878686 140089770305280 logging_writer.py:48] [374800] global_step=374800, grad_norm=4.224765777587891, loss=0.5925291776657104
I0301 17:17:33.338197 140089778697984 logging_writer.py:48] [374900] global_step=374900, grad_norm=5.443907260894775, loss=0.6995535492897034
I0301 17:18:00.376801 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:18:06.727658 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:18:14.878947 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:18:17.183212 140252611495744 submission_runner.py:411] Time since start: 129838.06s, 	Step: 374982, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14521554112434387, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.052392840385437, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.8403536081314087, 'test/num_examples': 10000, 'score': 125524.3220334053, 'total_duration': 129838.06326699257, 'accumulated_submission_time': 125524.3220334053, 'accumulated_eval_time': 4285.071767568588, 'accumulated_logging_time': 15.203657388687134}
I0301 17:18:17.266527 140089770305280 logging_writer.py:48] [374982] accumulated_eval_time=4285.071768, accumulated_logging_time=15.203657, accumulated_submission_time=125524.322033, global_step=374982, preemption_count=0, score=125524.322033, test/accuracy=0.628400, test/loss=1.840354, test/num_examples=10000, total_duration=129838.063267, train/accuracy=0.961137, train/loss=0.145216, validation/accuracy=0.756180, validation/loss=1.052393, validation/num_examples=50000
I0301 17:18:23.607927 140089778697984 logging_writer.py:48] [375000] global_step=375000, grad_norm=5.336477279663086, loss=0.6388816237449646
I0301 17:18:57.110595 140089770305280 logging_writer.py:48] [375100] global_step=375100, grad_norm=4.465387344360352, loss=0.6303929090499878
I0301 17:19:30.547568 140089778697984 logging_writer.py:48] [375200] global_step=375200, grad_norm=4.52482795715332, loss=0.6586945652961731
I0301 17:20:04.002515 140089770305280 logging_writer.py:48] [375300] global_step=375300, grad_norm=4.26946496963501, loss=0.6582654714584351
I0301 17:20:37.459926 140089778697984 logging_writer.py:48] [375400] global_step=375400, grad_norm=4.774466037750244, loss=0.5978791117668152
I0301 17:21:10.968768 140089770305280 logging_writer.py:48] [375500] global_step=375500, grad_norm=4.484858512878418, loss=0.6891536712646484
I0301 17:21:44.410890 140089778697984 logging_writer.py:48] [375600] global_step=375600, grad_norm=4.937458038330078, loss=0.7176262736320496
I0301 17:22:17.856359 140089770305280 logging_writer.py:48] [375700] global_step=375700, grad_norm=4.591948986053467, loss=0.6038764119148254
I0301 17:22:51.337811 140089778697984 logging_writer.py:48] [375800] global_step=375800, grad_norm=4.458970069885254, loss=0.5914263725280762
I0301 17:23:24.843107 140089770305280 logging_writer.py:48] [375900] global_step=375900, grad_norm=4.569869041442871, loss=0.6501758098602295
I0301 17:23:58.374507 140089778697984 logging_writer.py:48] [376000] global_step=376000, grad_norm=4.472264766693115, loss=0.615261435508728
I0301 17:24:31.891722 140089770305280 logging_writer.py:48] [376100] global_step=376100, grad_norm=4.237666130065918, loss=0.6374668478965759
I0301 17:25:05.335933 140089778697984 logging_writer.py:48] [376200] global_step=376200, grad_norm=5.936933517456055, loss=0.6140187382698059
I0301 17:25:38.781306 140089770305280 logging_writer.py:48] [376300] global_step=376300, grad_norm=4.414468765258789, loss=0.6186107397079468
I0301 17:26:12.260874 140089778697984 logging_writer.py:48] [376400] global_step=376400, grad_norm=4.566247463226318, loss=0.5696333646774292
I0301 17:26:45.748360 140089770305280 logging_writer.py:48] [376500] global_step=376500, grad_norm=4.53883695602417, loss=0.6354241371154785
I0301 17:26:47.227593 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:26:53.319864 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:27:01.578419 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:27:04.049997 140252611495744 submission_runner.py:411] Time since start: 130364.93s, 	Step: 376506, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.14702221751213074, 'validation/accuracy': 0.756339967250824, 'validation/loss': 1.051685094833374, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.839387059211731, 'test/num_examples': 10000, 'score': 126034.21823072433, 'total_duration': 130364.93006849289, 'accumulated_submission_time': 126034.21823072433, 'accumulated_eval_time': 4301.894116163254, 'accumulated_logging_time': 15.297083377838135}
I0301 17:27:04.133798 140089770305280 logging_writer.py:48] [376506] accumulated_eval_time=4301.894116, accumulated_logging_time=15.297083, accumulated_submission_time=126034.218231, global_step=376506, preemption_count=0, score=126034.218231, test/accuracy=0.627900, test/loss=1.839387, test/num_examples=10000, total_duration=130364.930068, train/accuracy=0.960499, train/loss=0.147022, validation/accuracy=0.756340, validation/loss=1.051685, validation/num_examples=50000
I0301 17:27:35.916841 140089837426432 logging_writer.py:48] [376600] global_step=376600, grad_norm=4.963301181793213, loss=0.7027612924575806
I0301 17:28:09.378860 140089770305280 logging_writer.py:48] [376700] global_step=376700, grad_norm=4.271407127380371, loss=0.6012195944786072
I0301 17:28:42.871016 140089837426432 logging_writer.py:48] [376800] global_step=376800, grad_norm=4.537701606750488, loss=0.5718600749969482
I0301 17:29:16.314836 140089770305280 logging_writer.py:48] [376900] global_step=376900, grad_norm=4.437038898468018, loss=0.5744441151618958
I0301 17:29:49.759685 140089837426432 logging_writer.py:48] [377000] global_step=377000, grad_norm=4.53257942199707, loss=0.597628116607666
I0301 17:30:23.339777 140089770305280 logging_writer.py:48] [377100] global_step=377100, grad_norm=4.396024703979492, loss=0.606619656085968
I0301 17:30:56.805599 140089837426432 logging_writer.py:48] [377200] global_step=377200, grad_norm=4.447091579437256, loss=0.5689784288406372
I0301 17:31:30.251479 140089770305280 logging_writer.py:48] [377300] global_step=377300, grad_norm=4.446763038635254, loss=0.6004400849342346
I0301 17:32:03.698144 140089837426432 logging_writer.py:48] [377400] global_step=377400, grad_norm=4.221733570098877, loss=0.5350914001464844
I0301 17:32:37.154539 140089770305280 logging_writer.py:48] [377500] global_step=377500, grad_norm=4.742166519165039, loss=0.6792714595794678
I0301 17:33:10.600073 140089837426432 logging_writer.py:48] [377600] global_step=377600, grad_norm=4.910445690155029, loss=0.615147054195404
I0301 17:33:44.053095 140089770305280 logging_writer.py:48] [377700] global_step=377700, grad_norm=4.181037425994873, loss=0.5497334003448486
I0301 17:34:17.503260 140089837426432 logging_writer.py:48] [377800] global_step=377800, grad_norm=4.694149971008301, loss=0.6151267886161804
I0301 17:34:50.965993 140089770305280 logging_writer.py:48] [377900] global_step=377900, grad_norm=4.745077133178711, loss=0.6044414043426514
I0301 17:35:24.402770 140089837426432 logging_writer.py:48] [378000] global_step=378000, grad_norm=4.862809181213379, loss=0.7078299522399902
I0301 17:35:34.248959 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:35:40.385483 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:35:48.506252 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:35:50.823342 140252611495744 submission_runner.py:411] Time since start: 130891.70s, 	Step: 378031, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14617305994033813, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0533385276794434, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8416091203689575, 'test/num_examples': 10000, 'score': 126544.26707220078, 'total_duration': 130891.7034125328, 'accumulated_submission_time': 126544.26707220078, 'accumulated_eval_time': 4318.468457937241, 'accumulated_logging_time': 15.392176628112793}
I0301 17:35:50.903956 140089770305280 logging_writer.py:48] [378031] accumulated_eval_time=4318.468458, accumulated_logging_time=15.392177, accumulated_submission_time=126544.267072, global_step=378031, preemption_count=0, score=126544.267072, test/accuracy=0.626800, test/loss=1.841609, test/num_examples=10000, total_duration=130891.703413, train/accuracy=0.960439, train/loss=0.146173, validation/accuracy=0.756000, validation/loss=1.053339, validation/num_examples=50000
I0301 17:36:14.417693 140089778697984 logging_writer.py:48] [378100] global_step=378100, grad_norm=5.073490142822266, loss=0.5661773681640625
I0301 17:36:47.866651 140089770305280 logging_writer.py:48] [378200] global_step=378200, grad_norm=4.611590385437012, loss=0.6366990804672241
I0301 17:37:21.313066 140089778697984 logging_writer.py:48] [378300] global_step=378300, grad_norm=4.811156272888184, loss=0.6562048196792603
I0301 17:37:54.750789 140089770305280 logging_writer.py:48] [378400] global_step=378400, grad_norm=4.5952911376953125, loss=0.6615926027297974
I0301 17:38:28.475302 140089778697984 logging_writer.py:48] [378500] global_step=378500, grad_norm=4.276396751403809, loss=0.534140944480896
I0301 17:39:01.959349 140089770305280 logging_writer.py:48] [378600] global_step=378600, grad_norm=4.434536933898926, loss=0.6466232538223267
I0301 17:39:35.394653 140089778697984 logging_writer.py:48] [378700] global_step=378700, grad_norm=4.512282371520996, loss=0.6089349985122681
I0301 17:40:08.832551 140089770305280 logging_writer.py:48] [378800] global_step=378800, grad_norm=4.570555210113525, loss=0.6697777509689331
I0301 17:40:42.279704 140089778697984 logging_writer.py:48] [378900] global_step=378900, grad_norm=5.10252046585083, loss=0.6203991174697876
I0301 17:41:15.709000 140089770305280 logging_writer.py:48] [379000] global_step=379000, grad_norm=4.238088607788086, loss=0.5902483463287354
I0301 17:41:49.182668 140089778697984 logging_writer.py:48] [379100] global_step=379100, grad_norm=3.9928696155548096, loss=0.578269362449646
I0301 17:42:22.731385 140089770305280 logging_writer.py:48] [379200] global_step=379200, grad_norm=4.333582401275635, loss=0.6089845299720764
I0301 17:42:56.172473 140089778697984 logging_writer.py:48] [379300] global_step=379300, grad_norm=4.1048054695129395, loss=0.580405592918396
I0301 17:43:29.630320 140089770305280 logging_writer.py:48] [379400] global_step=379400, grad_norm=4.639694690704346, loss=0.6361917853355408
I0301 17:44:03.072993 140089778697984 logging_writer.py:48] [379500] global_step=379500, grad_norm=4.047762870788574, loss=0.5570130944252014
I0301 17:44:20.945822 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:44:27.203284 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:44:35.174950 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:44:37.484825 140252611495744 submission_runner.py:411] Time since start: 131418.36s, 	Step: 379555, 	{'train/accuracy': 0.9622528553009033, 'train/loss': 0.14611463248729706, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0539250373840332, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8432484865188599, 'test/num_examples': 10000, 'score': 127054.2448322773, 'total_duration': 131418.3648777008, 'accumulated_submission_time': 127054.2448322773, 'accumulated_eval_time': 4335.007391452789, 'accumulated_logging_time': 15.482544660568237}
I0301 17:44:37.570414 140089854211840 logging_writer.py:48] [379555] accumulated_eval_time=4335.007391, accumulated_logging_time=15.482545, accumulated_submission_time=127054.244832, global_step=379555, preemption_count=0, score=127054.244832, test/accuracy=0.626800, test/loss=1.843248, test/num_examples=10000, total_duration=131418.364878, train/accuracy=0.962253, train/loss=0.146115, validation/accuracy=0.755980, validation/loss=1.053925, validation/num_examples=50000
I0301 17:44:52.949814 140089870997248 logging_writer.py:48] [379600] global_step=379600, grad_norm=4.89371395111084, loss=0.6348963975906372
I0301 17:45:26.416220 140089854211840 logging_writer.py:48] [379700] global_step=379700, grad_norm=5.051426410675049, loss=0.7395830750465393
I0301 17:45:59.881752 140089870997248 logging_writer.py:48] [379800] global_step=379800, grad_norm=4.912449836730957, loss=0.7026667594909668
I0301 17:46:33.323148 140089854211840 logging_writer.py:48] [379900] global_step=379900, grad_norm=4.621659755706787, loss=0.6378679871559143
I0301 17:47:06.790713 140089870997248 logging_writer.py:48] [380000] global_step=380000, grad_norm=4.7132344245910645, loss=0.6087352633476257
I0301 17:47:40.223139 140089854211840 logging_writer.py:48] [380100] global_step=380100, grad_norm=4.850449085235596, loss=0.6701117753982544
I0301 17:48:13.718230 140089870997248 logging_writer.py:48] [380200] global_step=380200, grad_norm=4.665011405944824, loss=0.5911781191825867
I0301 17:48:47.262482 140089854211840 logging_writer.py:48] [380300] global_step=380300, grad_norm=4.4497456550598145, loss=0.6709015369415283
I0301 17:49:20.745527 140089870997248 logging_writer.py:48] [380400] global_step=380400, grad_norm=4.663755416870117, loss=0.5952563881874084
I0301 17:49:54.184511 140089854211840 logging_writer.py:48] [380500] global_step=380500, grad_norm=4.275923252105713, loss=0.5641481280326843
I0301 17:50:27.647908 140089870997248 logging_writer.py:48] [380600] global_step=380600, grad_norm=4.179368495941162, loss=0.5974205732345581
I0301 17:51:01.121349 140089854211840 logging_writer.py:48] [380700] global_step=380700, grad_norm=5.655399322509766, loss=0.6797443628311157
I0301 17:51:34.609653 140089870997248 logging_writer.py:48] [380800] global_step=380800, grad_norm=4.129676818847656, loss=0.6312751770019531
I0301 17:52:08.041380 140089854211840 logging_writer.py:48] [380900] global_step=380900, grad_norm=4.165392875671387, loss=0.608482301235199
I0301 17:52:41.508063 140089870997248 logging_writer.py:48] [381000] global_step=381000, grad_norm=5.207988739013672, loss=0.6728557348251343
I0301 17:53:07.731601 140252611495744 spec.py:321] Evaluating on the training split.
I0301 17:53:13.887298 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 17:53:21.953206 140252611495744 spec.py:349] Evaluating on the test split.
I0301 17:53:24.294319 140252611495744 submission_runner.py:411] Time since start: 131945.17s, 	Step: 381080, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14764155447483063, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 1.0527596473693848, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8413126468658447, 'test/num_examples': 10000, 'score': 127564.3393175602, 'total_duration': 131945.17438149452, 'accumulated_submission_time': 127564.3393175602, 'accumulated_eval_time': 4351.570056438446, 'accumulated_logging_time': 15.580342531204224}
I0301 17:53:24.382699 140089837426432 logging_writer.py:48] [381080] accumulated_eval_time=4351.570056, accumulated_logging_time=15.580343, accumulated_submission_time=127564.339318, global_step=381080, preemption_count=0, score=127564.339318, test/accuracy=0.628200, test/loss=1.841313, test/num_examples=10000, total_duration=131945.174381, train/accuracy=0.960519, train/loss=0.147642, validation/accuracy=0.756280, validation/loss=1.052760, validation/num_examples=50000
I0301 17:53:31.418432 140089845819136 logging_writer.py:48] [381100] global_step=381100, grad_norm=4.7553019523620605, loss=0.5647426843643188
I0301 17:54:04.844051 140089837426432 logging_writer.py:48] [381200] global_step=381200, grad_norm=4.831129550933838, loss=0.6614522933959961
I0301 17:54:38.356233 140089845819136 logging_writer.py:48] [381300] global_step=381300, grad_norm=4.686447620391846, loss=0.6244267821311951
I0301 17:55:11.814039 140089837426432 logging_writer.py:48] [381400] global_step=381400, grad_norm=4.672064304351807, loss=0.6906993985176086
I0301 17:55:45.257030 140089845819136 logging_writer.py:48] [381500] global_step=381500, grad_norm=4.921908855438232, loss=0.7038112878799438
I0301 17:56:18.737832 140089837426432 logging_writer.py:48] [381600] global_step=381600, grad_norm=4.5299201011657715, loss=0.6056636571884155
I0301 17:56:52.203351 140089845819136 logging_writer.py:48] [381700] global_step=381700, grad_norm=4.242288589477539, loss=0.6015772819519043
I0301 17:57:25.643021 140089837426432 logging_writer.py:48] [381800] global_step=381800, grad_norm=4.7691850662231445, loss=0.6708353757858276
I0301 17:57:59.111002 140089845819136 logging_writer.py:48] [381900] global_step=381900, grad_norm=4.484877109527588, loss=0.6658800840377808
I0301 17:58:32.555680 140089837426432 logging_writer.py:48] [382000] global_step=382000, grad_norm=4.879469871520996, loss=0.6337728500366211
I0301 17:59:06.010334 140089845819136 logging_writer.py:48] [382100] global_step=382100, grad_norm=4.349023342132568, loss=0.6388314962387085
I0301 17:59:39.464408 140089837426432 logging_writer.py:48] [382200] global_step=382200, grad_norm=4.531820297241211, loss=0.5763677358627319
I0301 18:00:12.906386 140089845819136 logging_writer.py:48] [382300] global_step=382300, grad_norm=4.006895065307617, loss=0.5116334557533264
I0301 18:00:46.479953 140089837426432 logging_writer.py:48] [382400] global_step=382400, grad_norm=4.459325313568115, loss=0.6518216133117676
I0301 18:01:19.935946 140089845819136 logging_writer.py:48] [382500] global_step=382500, grad_norm=4.452557563781738, loss=0.6424577832221985
I0301 18:01:53.374576 140089837426432 logging_writer.py:48] [382600] global_step=382600, grad_norm=4.804610729217529, loss=0.7056386470794678
I0301 18:01:54.523315 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:02:00.698557 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:02:09.000263 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:02:11.293656 140252611495744 submission_runner.py:411] Time since start: 132472.17s, 	Step: 382605, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14708101749420166, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0539182424545288, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8423075675964355, 'test/num_examples': 10000, 'score': 128074.41396832466, 'total_duration': 132472.1737203598, 'accumulated_submission_time': 128074.41396832466, 'accumulated_eval_time': 4368.340341567993, 'accumulated_logging_time': 15.680543184280396}
I0301 18:02:11.374331 140089778697984 logging_writer.py:48] [382605] accumulated_eval_time=4368.340342, accumulated_logging_time=15.680543, accumulated_submission_time=128074.413968, global_step=382605, preemption_count=0, score=128074.413968, test/accuracy=0.628100, test/loss=1.842308, test/num_examples=10000, total_duration=132472.173720, train/accuracy=0.960439, train/loss=0.147081, validation/accuracy=0.756000, validation/loss=1.053918, validation/num_examples=50000
I0301 18:02:43.552473 140089854211840 logging_writer.py:48] [382700] global_step=382700, grad_norm=4.859433174133301, loss=0.653868556022644
I0301 18:03:17.024003 140089778697984 logging_writer.py:48] [382800] global_step=382800, grad_norm=4.613311767578125, loss=0.6893397569656372
I0301 18:03:50.461381 140089854211840 logging_writer.py:48] [382900] global_step=382900, grad_norm=4.6063761711120605, loss=0.6291009187698364
I0301 18:04:23.919816 140089778697984 logging_writer.py:48] [383000] global_step=383000, grad_norm=4.890320777893066, loss=0.6250303983688354
I0301 18:04:57.382668 140089854211840 logging_writer.py:48] [383100] global_step=383100, grad_norm=4.112215518951416, loss=0.6089286208152771
I0301 18:05:30.814975 140089778697984 logging_writer.py:48] [383200] global_step=383200, grad_norm=4.610497951507568, loss=0.6376901865005493
I0301 18:06:04.296844 140089854211840 logging_writer.py:48] [383300] global_step=383300, grad_norm=4.294504165649414, loss=0.6267297267913818
I0301 18:06:37.813354 140089778697984 logging_writer.py:48] [383400] global_step=383400, grad_norm=4.66302490234375, loss=0.6257566213607788
I0301 18:07:11.269766 140089854211840 logging_writer.py:48] [383500] global_step=383500, grad_norm=4.33526611328125, loss=0.6139528155326843
I0301 18:07:44.727716 140089778697984 logging_writer.py:48] [383600] global_step=383600, grad_norm=4.686686038970947, loss=0.6718884110450745
I0301 18:08:18.189172 140089854211840 logging_writer.py:48] [383700] global_step=383700, grad_norm=5.306463241577148, loss=0.6191868782043457
I0301 18:08:51.629287 140089778697984 logging_writer.py:48] [383800] global_step=383800, grad_norm=4.3366923332214355, loss=0.5729033946990967
I0301 18:09:25.124214 140089854211840 logging_writer.py:48] [383900] global_step=383900, grad_norm=4.906020164489746, loss=0.7479352355003357
I0301 18:09:58.595204 140089778697984 logging_writer.py:48] [384000] global_step=384000, grad_norm=4.54372501373291, loss=0.6279003024101257
I0301 18:10:32.033005 140089854211840 logging_writer.py:48] [384100] global_step=384100, grad_norm=4.2949957847595215, loss=0.6361302137374878
I0301 18:10:41.571285 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:10:47.684601 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:10:55.853037 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:10:58.070437 140252611495744 submission_runner.py:411] Time since start: 132998.95s, 	Step: 384130, 	{'train/accuracy': 0.9626514315605164, 'train/loss': 0.14418169856071472, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0531625747680664, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8423826694488525, 'test/num_examples': 10000, 'score': 128584.54533720016, 'total_duration': 132998.9505019188, 'accumulated_submission_time': 128584.54533720016, 'accumulated_eval_time': 4384.839435815811, 'accumulated_logging_time': 15.771271228790283}
I0301 18:10:58.153180 140089778697984 logging_writer.py:48] [384130] accumulated_eval_time=4384.839436, accumulated_logging_time=15.771271, accumulated_submission_time=128584.545337, global_step=384130, preemption_count=0, score=128584.545337, test/accuracy=0.626800, test/loss=1.842383, test/num_examples=10000, total_duration=132998.950502, train/accuracy=0.962651, train/loss=0.144182, validation/accuracy=0.755780, validation/loss=1.053163, validation/num_examples=50000
I0301 18:11:21.939916 140089845819136 logging_writer.py:48] [384200] global_step=384200, grad_norm=4.294457912445068, loss=0.5382048487663269
I0301 18:11:55.402641 140089778697984 logging_writer.py:48] [384300] global_step=384300, grad_norm=4.4001617431640625, loss=0.5909944772720337
I0301 18:12:28.841116 140089845819136 logging_writer.py:48] [384400] global_step=384400, grad_norm=4.869379997253418, loss=0.6382538080215454
I0301 18:13:02.380851 140089778697984 logging_writer.py:48] [384500] global_step=384500, grad_norm=4.485345840454102, loss=0.6188246607780457
I0301 18:13:35.868944 140089845819136 logging_writer.py:48] [384600] global_step=384600, grad_norm=4.2950286865234375, loss=0.5994374752044678
I0301 18:14:09.357244 140089778697984 logging_writer.py:48] [384700] global_step=384700, grad_norm=5.123743534088135, loss=0.7229400873184204
I0301 18:14:42.827977 140089845819136 logging_writer.py:48] [384800] global_step=384800, grad_norm=4.4684648513793945, loss=0.6094871163368225
I0301 18:15:16.286001 140089778697984 logging_writer.py:48] [384900] global_step=384900, grad_norm=4.574726581573486, loss=0.6508379578590393
I0301 18:15:49.776539 140089845819136 logging_writer.py:48] [385000] global_step=385000, grad_norm=4.157121181488037, loss=0.6405848264694214
I0301 18:16:23.211082 140089778697984 logging_writer.py:48] [385100] global_step=385100, grad_norm=4.774448871612549, loss=0.6819928288459778
I0301 18:16:56.657927 140089845819136 logging_writer.py:48] [385200] global_step=385200, grad_norm=4.525742053985596, loss=0.6391913294792175
I0301 18:17:30.160795 140089778697984 logging_writer.py:48] [385300] global_step=385300, grad_norm=5.289425849914551, loss=0.6610925793647766
I0301 18:18:03.622314 140089845819136 logging_writer.py:48] [385400] global_step=385400, grad_norm=4.802059173583984, loss=0.6288741230964661
I0301 18:18:37.148551 140089778697984 logging_writer.py:48] [385500] global_step=385500, grad_norm=4.8962202072143555, loss=0.6293754577636719
I0301 18:19:10.589018 140089845819136 logging_writer.py:48] [385600] global_step=385600, grad_norm=4.527965545654297, loss=0.6019144058227539
I0301 18:19:28.139695 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:19:34.289147 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:19:42.379482 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:19:44.645253 140252611495744 submission_runner.py:411] Time since start: 133525.53s, 	Step: 385654, 	{'train/accuracy': 0.9628706574440002, 'train/loss': 0.14295707643032074, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0541527271270752, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.84333074092865, 'test/num_examples': 10000, 'score': 129094.4659075737, 'total_duration': 133525.52531456947, 'accumulated_submission_time': 129094.4659075737, 'accumulated_eval_time': 4401.344930171967, 'accumulated_logging_time': 15.865436553955078}
I0301 18:19:44.722012 140089778697984 logging_writer.py:48] [385654] accumulated_eval_time=4401.344930, accumulated_logging_time=15.865437, accumulated_submission_time=129094.465908, global_step=385654, preemption_count=0, score=129094.465908, test/accuracy=0.627400, test/loss=1.843331, test/num_examples=10000, total_duration=133525.525315, train/accuracy=0.962871, train/loss=0.142957, validation/accuracy=0.755800, validation/loss=1.054153, validation/num_examples=50000
I0301 18:20:00.445227 140089837426432 logging_writer.py:48] [385700] global_step=385700, grad_norm=5.284790992736816, loss=0.5837804079055786
I0301 18:20:33.878272 140089778697984 logging_writer.py:48] [385800] global_step=385800, grad_norm=5.047281742095947, loss=0.6735974550247192
I0301 18:21:07.353840 140089837426432 logging_writer.py:48] [385900] global_step=385900, grad_norm=4.361886501312256, loss=0.5857278108596802
I0301 18:21:40.792433 140089778697984 logging_writer.py:48] [386000] global_step=386000, grad_norm=4.562010288238525, loss=0.620171308517456
I0301 18:22:14.251571 140089837426432 logging_writer.py:48] [386100] global_step=386100, grad_norm=4.8416218757629395, loss=0.645162045955658
I0301 18:22:47.720727 140089778697984 logging_writer.py:48] [386200] global_step=386200, grad_norm=4.570786952972412, loss=0.6428541541099548
I0301 18:23:21.156318 140089837426432 logging_writer.py:48] [386300] global_step=386300, grad_norm=4.47199010848999, loss=0.6096481680870056
I0301 18:23:54.641566 140089778697984 logging_writer.py:48] [386400] global_step=386400, grad_norm=4.449660778045654, loss=0.5779889225959778
I0301 18:24:28.069498 140089837426432 logging_writer.py:48] [386500] global_step=386500, grad_norm=4.896136283874512, loss=0.6367834806442261
I0301 18:25:01.569879 140089778697984 logging_writer.py:48] [386600] global_step=386600, grad_norm=4.258984088897705, loss=0.5762513279914856
I0301 18:25:35.035659 140089837426432 logging_writer.py:48] [386700] global_step=386700, grad_norm=4.1271185874938965, loss=0.5803536176681519
I0301 18:26:08.486742 140089778697984 logging_writer.py:48] [386800] global_step=386800, grad_norm=4.711424827575684, loss=0.6510763168334961
I0301 18:26:41.934732 140089837426432 logging_writer.py:48] [386900] global_step=386900, grad_norm=4.619235515594482, loss=0.6315367817878723
I0301 18:27:15.410214 140089778697984 logging_writer.py:48] [387000] global_step=387000, grad_norm=4.48691463470459, loss=0.5782626271247864
I0301 18:27:48.828208 140089837426432 logging_writer.py:48] [387100] global_step=387100, grad_norm=4.60529088973999, loss=0.5988589525222778
I0301 18:28:14.726143 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:28:21.479817 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:28:29.690700 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:28:31.986431 140252611495744 submission_runner.py:411] Time since start: 134052.87s, 	Step: 387179, 	{'train/accuracy': 0.9623923301696777, 'train/loss': 0.14351202547550201, 'validation/accuracy': 0.7556999921798706, 'validation/loss': 1.053622841835022, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8408176898956299, 'test/num_examples': 10000, 'score': 129604.40239357948, 'total_duration': 134052.86649990082, 'accumulated_submission_time': 129604.40239357948, 'accumulated_eval_time': 4418.605168104172, 'accumulated_logging_time': 15.95547103881836}
I0301 18:28:32.070721 140089552271104 logging_writer.py:48] [387179] accumulated_eval_time=4418.605168, accumulated_logging_time=15.955471, accumulated_submission_time=129604.402394, global_step=387179, preemption_count=0, score=129604.402394, test/accuracy=0.628000, test/loss=1.840818, test/num_examples=10000, total_duration=134052.866500, train/accuracy=0.962392, train/loss=0.143512, validation/accuracy=0.755700, validation/loss=1.053623, validation/num_examples=50000
I0301 18:28:39.436848 140089770305280 logging_writer.py:48] [387200] global_step=387200, grad_norm=4.163127899169922, loss=0.5586187839508057
I0301 18:29:12.882919 140089552271104 logging_writer.py:48] [387300] global_step=387300, grad_norm=4.446133136749268, loss=0.6211640238761902
I0301 18:29:46.393036 140089770305280 logging_writer.py:48] [387400] global_step=387400, grad_norm=4.166069984436035, loss=0.558804988861084
I0301 18:30:19.845918 140089552271104 logging_writer.py:48] [387500] global_step=387500, grad_norm=4.355813026428223, loss=0.658082902431488
I0301 18:30:53.298512 140089770305280 logging_writer.py:48] [387600] global_step=387600, grad_norm=4.317008972167969, loss=0.5707588791847229
I0301 18:31:26.809524 140089552271104 logging_writer.py:48] [387700] global_step=387700, grad_norm=4.607882976531982, loss=0.610177755355835
I0301 18:32:00.289833 140089770305280 logging_writer.py:48] [387800] global_step=387800, grad_norm=4.396273612976074, loss=0.6600844860076904
I0301 18:32:33.712315 140089552271104 logging_writer.py:48] [387900] global_step=387900, grad_norm=4.5812764167785645, loss=0.5527260303497314
I0301 18:33:07.186775 140089770305280 logging_writer.py:48] [388000] global_step=388000, grad_norm=4.16376256942749, loss=0.6329665780067444
I0301 18:33:40.660433 140089552271104 logging_writer.py:48] [388100] global_step=388100, grad_norm=4.797815799713135, loss=0.6456480026245117
I0301 18:34:14.089967 140089770305280 logging_writer.py:48] [388200] global_step=388200, grad_norm=4.727269172668457, loss=0.6013453602790833
I0301 18:34:47.555767 140089552271104 logging_writer.py:48] [388300] global_step=388300, grad_norm=4.186328887939453, loss=0.6118156313896179
I0301 18:35:20.991816 140089770305280 logging_writer.py:48] [388400] global_step=388400, grad_norm=4.631975173950195, loss=0.5612706542015076
I0301 18:35:54.436629 140089552271104 logging_writer.py:48] [388500] global_step=388500, grad_norm=4.847201824188232, loss=0.6115951538085938
I0301 18:36:27.904279 140089770305280 logging_writer.py:48] [388600] global_step=388600, grad_norm=4.685605049133301, loss=0.5773961544036865
I0301 18:37:01.407641 140089552271104 logging_writer.py:48] [388700] global_step=388700, grad_norm=4.6559295654296875, loss=0.6148033142089844
I0301 18:37:02.238960 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:37:08.469352 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:37:16.566362 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:37:18.834324 140252611495744 submission_runner.py:411] Time since start: 134579.71s, 	Step: 388704, 	{'train/accuracy': 0.9595822691917419, 'train/loss': 0.14606274664402008, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.053932547569275, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8424186706542969, 'test/num_examples': 10000, 'score': 130114.50559592247, 'total_duration': 134579.7143881321, 'accumulated_submission_time': 130114.50559592247, 'accumulated_eval_time': 4435.200484991074, 'accumulated_logging_time': 16.04998469352722}
I0301 18:37:18.920573 140089770305280 logging_writer.py:48] [388704] accumulated_eval_time=4435.200485, accumulated_logging_time=16.049985, accumulated_submission_time=130114.505596, global_step=388704, preemption_count=0, score=130114.505596, test/accuracy=0.626700, test/loss=1.842419, test/num_examples=10000, total_duration=134579.714388, train/accuracy=0.959582, train/loss=0.146063, validation/accuracy=0.756100, validation/loss=1.053933, validation/num_examples=50000
I0301 18:37:51.370665 140089778697984 logging_writer.py:48] [388800] global_step=388800, grad_norm=4.614242076873779, loss=0.6317114233970642
I0301 18:38:24.829131 140089770305280 logging_writer.py:48] [388900] global_step=388900, grad_norm=4.660521984100342, loss=0.616852343082428
I0301 18:38:58.278588 140089778697984 logging_writer.py:48] [389000] global_step=389000, grad_norm=4.229179859161377, loss=0.6007064580917358
I0301 18:39:31.724300 140089770305280 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.336066246032715, loss=0.6611316204071045
I0301 18:40:05.152746 140089778697984 logging_writer.py:48] [389200] global_step=389200, grad_norm=4.490623950958252, loss=0.6432757377624512
I0301 18:40:38.605362 140089770305280 logging_writer.py:48] [389300] global_step=389300, grad_norm=4.420104026794434, loss=0.5588900446891785
I0301 18:41:12.043790 140089778697984 logging_writer.py:48] [389400] global_step=389400, grad_norm=5.076550483703613, loss=0.5922036170959473
I0301 18:41:45.521846 140089770305280 logging_writer.py:48] [389500] global_step=389500, grad_norm=4.479796886444092, loss=0.646518349647522
I0301 18:42:18.941616 140089778697984 logging_writer.py:48] [389600] global_step=389600, grad_norm=4.247594833374023, loss=0.5760437250137329
I0301 18:42:52.420514 140089770305280 logging_writer.py:48] [389700] global_step=389700, grad_norm=4.6379780769348145, loss=0.6254812479019165
I0301 18:43:26.019603 140089778697984 logging_writer.py:48] [389800] global_step=389800, grad_norm=4.740063190460205, loss=0.6503982543945312
I0301 18:43:59.451735 140089770305280 logging_writer.py:48] [389900] global_step=389900, grad_norm=4.323768138885498, loss=0.5718887448310852
I0301 18:44:32.881431 140089778697984 logging_writer.py:48] [390000] global_step=390000, grad_norm=4.70682954788208, loss=0.6670005321502686
I0301 18:45:06.316545 140089770305280 logging_writer.py:48] [390100] global_step=390100, grad_norm=4.315623760223389, loss=0.5717961192131042
I0301 18:45:39.774188 140089778697984 logging_writer.py:48] [390200] global_step=390200, grad_norm=4.836596965789795, loss=0.6989291310310364
I0301 18:45:48.959043 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:45:55.041047 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:46:03.236484 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:46:05.512222 140252611495744 submission_runner.py:411] Time since start: 135106.39s, 	Step: 390229, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14755330979824066, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0535551309585571, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.843072533607483, 'test/num_examples': 10000, 'score': 130624.4790096283, 'total_duration': 135106.39228892326, 'accumulated_submission_time': 130624.4790096283, 'accumulated_eval_time': 4451.753606081009, 'accumulated_logging_time': 16.146687269210815}
I0301 18:46:05.592308 140089770305280 logging_writer.py:48] [390229] accumulated_eval_time=4451.753606, accumulated_logging_time=16.146687, accumulated_submission_time=130624.479010, global_step=390229, preemption_count=0, score=130624.479010, test/accuracy=0.626700, test/loss=1.843073, test/num_examples=10000, total_duration=135106.392289, train/accuracy=0.960938, train/loss=0.147553, validation/accuracy=0.756000, validation/loss=1.053555, validation/num_examples=50000
I0301 18:46:29.690458 140089862604544 logging_writer.py:48] [390300] global_step=390300, grad_norm=4.45827054977417, loss=0.5605898499488831
I0301 18:47:03.100665 140089770305280 logging_writer.py:48] [390400] global_step=390400, grad_norm=4.706482887268066, loss=0.5939152240753174
I0301 18:47:36.556967 140089862604544 logging_writer.py:48] [390500] global_step=390500, grad_norm=4.975193023681641, loss=0.6606972217559814
I0301 18:48:09.986440 140089770305280 logging_writer.py:48] [390600] global_step=390600, grad_norm=4.681216716766357, loss=0.7065346240997314
I0301 18:48:43.422093 140089862604544 logging_writer.py:48] [390700] global_step=390700, grad_norm=4.388336181640625, loss=0.5849317908287048
I0301 18:49:17.009758 140089770305280 logging_writer.py:48] [390800] global_step=390800, grad_norm=4.339348793029785, loss=0.5485289096832275
I0301 18:49:50.466964 140089862604544 logging_writer.py:48] [390900] global_step=390900, grad_norm=4.46218729019165, loss=0.67259681224823
I0301 18:50:23.904837 140089770305280 logging_writer.py:48] [391000] global_step=391000, grad_norm=4.5601348876953125, loss=0.6405511498451233
I0301 18:50:57.362851 140089862604544 logging_writer.py:48] [391100] global_step=391100, grad_norm=4.307185173034668, loss=0.6479758024215698
I0301 18:51:30.839906 140089770305280 logging_writer.py:48] [391200] global_step=391200, grad_norm=5.285957336425781, loss=0.634782075881958
I0301 18:52:04.273119 140089862604544 logging_writer.py:48] [391300] global_step=391300, grad_norm=5.113374710083008, loss=0.6724704504013062
I0301 18:52:37.749866 140089770305280 logging_writer.py:48] [391400] global_step=391400, grad_norm=4.622387409210205, loss=0.6969708204269409
I0301 18:53:11.193308 140089862604544 logging_writer.py:48] [391500] global_step=391500, grad_norm=4.481445789337158, loss=0.6058424711227417
I0301 18:53:44.660185 140089770305280 logging_writer.py:48] [391600] global_step=391600, grad_norm=4.72406530380249, loss=0.611504852771759
I0301 18:54:18.131433 140089862604544 logging_writer.py:48] [391700] global_step=391700, grad_norm=4.321696758270264, loss=0.5921908020973206
I0301 18:54:35.675425 140252611495744 spec.py:321] Evaluating on the training split.
I0301 18:54:41.861182 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 18:54:49.993172 140252611495744 spec.py:349] Evaluating on the test split.
I0301 18:54:52.291713 140252611495744 submission_runner.py:411] Time since start: 135633.17s, 	Step: 391754, 	{'train/accuracy': 0.9625119566917419, 'train/loss': 0.14108845591545105, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0546152591705322, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8442800045013428, 'test/num_examples': 10000, 'score': 131134.4970755577, 'total_duration': 135633.17174863815, 'accumulated_submission_time': 131134.4970755577, 'accumulated_eval_time': 4468.369817733765, 'accumulated_logging_time': 16.23644709587097}
I0301 18:54:52.420509 140089770305280 logging_writer.py:48] [391754] accumulated_eval_time=4468.369818, accumulated_logging_time=16.236447, accumulated_submission_time=131134.497076, global_step=391754, preemption_count=0, score=131134.497076, test/accuracy=0.627100, test/loss=1.844280, test/num_examples=10000, total_duration=135633.171749, train/accuracy=0.962512, train/loss=0.141088, validation/accuracy=0.755780, validation/loss=1.054615, validation/num_examples=50000
I0301 18:55:08.178333 140089778697984 logging_writer.py:48] [391800] global_step=391800, grad_norm=4.352248668670654, loss=0.6630016565322876
I0301 18:55:41.734472 140089770305280 logging_writer.py:48] [391900] global_step=391900, grad_norm=4.772311210632324, loss=0.6422380208969116
I0301 18:56:15.196825 140089778697984 logging_writer.py:48] [392000] global_step=392000, grad_norm=4.78943395614624, loss=0.6927751302719116
I0301 18:56:48.646839 140089770305280 logging_writer.py:48] [392100] global_step=392100, grad_norm=4.344675064086914, loss=0.5742064714431763
I0301 18:57:22.128918 140089778697984 logging_writer.py:48] [392200] global_step=392200, grad_norm=4.311561584472656, loss=0.5191361308097839
I0301 18:57:55.560665 140089770305280 logging_writer.py:48] [392300] global_step=392300, grad_norm=4.561445713043213, loss=0.620106041431427
I0301 18:58:29.006520 140089778697984 logging_writer.py:48] [392400] global_step=392400, grad_norm=4.514964580535889, loss=0.64338618516922
I0301 18:59:02.462049 140089770305280 logging_writer.py:48] [392500] global_step=392500, grad_norm=4.856274127960205, loss=0.6382440328598022
I0301 18:59:35.909733 140089778697984 logging_writer.py:48] [392600] global_step=392600, grad_norm=4.199122428894043, loss=0.5658715963363647
I0301 19:00:09.364594 140089770305280 logging_writer.py:48] [392700] global_step=392700, grad_norm=4.504369258880615, loss=0.5849340558052063
I0301 19:00:42.811904 140089778697984 logging_writer.py:48] [392800] global_step=392800, grad_norm=4.435908317565918, loss=0.6276026964187622
I0301 19:01:16.265934 140089770305280 logging_writer.py:48] [392900] global_step=392900, grad_norm=4.67368745803833, loss=0.6648737192153931
I0301 19:01:49.781925 140089778697984 logging_writer.py:48] [393000] global_step=393000, grad_norm=4.942335605621338, loss=0.7120233178138733
I0301 19:02:23.198119 140089770305280 logging_writer.py:48] [393100] global_step=393100, grad_norm=4.366962432861328, loss=0.6451703906059265
I0301 19:02:56.676366 140089778697984 logging_writer.py:48] [393200] global_step=393200, grad_norm=5.2660932540893555, loss=0.6560161113739014
I0301 19:03:22.552692 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:03:28.750313 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:03:36.772718 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:03:39.056857 140252611495744 submission_runner.py:411] Time since start: 136159.94s, 	Step: 393279, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.145300954580307, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0527784824371338, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.84110689163208, 'test/num_examples': 10000, 'score': 131644.5588364601, 'total_duration': 136159.93689537048, 'accumulated_submission_time': 131644.5588364601, 'accumulated_eval_time': 4484.873900651932, 'accumulated_logging_time': 16.379529237747192}
I0301 19:03:39.151242 140089862604544 logging_writer.py:48] [393279] accumulated_eval_time=4484.873901, accumulated_logging_time=16.379529, accumulated_submission_time=131644.558836, global_step=393279, preemption_count=0, score=131644.558836, test/accuracy=0.626800, test/loss=1.841107, test/num_examples=10000, total_duration=136159.936895, train/accuracy=0.960957, train/loss=0.145301, validation/accuracy=0.755880, validation/loss=1.052778, validation/num_examples=50000
I0301 19:03:46.509154 140089870997248 logging_writer.py:48] [393300] global_step=393300, grad_norm=4.224614143371582, loss=0.6249530911445618
I0301 19:04:19.961595 140089862604544 logging_writer.py:48] [393400] global_step=393400, grad_norm=4.839012145996094, loss=0.6564966440200806
I0301 19:04:53.395454 140089870997248 logging_writer.py:48] [393500] global_step=393500, grad_norm=4.33824348449707, loss=0.6173707842826843
I0301 19:05:26.833999 140089862604544 logging_writer.py:48] [393600] global_step=393600, grad_norm=4.651679515838623, loss=0.6471585035324097
I0301 19:06:00.288011 140089870997248 logging_writer.py:48] [393700] global_step=393700, grad_norm=4.256263732910156, loss=0.5663574934005737
I0301 19:06:33.723680 140089862604544 logging_writer.py:48] [393800] global_step=393800, grad_norm=4.949023246765137, loss=0.5967469215393066
I0301 19:07:07.195352 140089870997248 logging_writer.py:48] [393900] global_step=393900, grad_norm=4.231498718261719, loss=0.5435776710510254
I0301 19:07:40.789523 140089862604544 logging_writer.py:48] [394000] global_step=394000, grad_norm=4.640908718109131, loss=0.6651331782341003
I0301 19:08:14.267954 140089870997248 logging_writer.py:48] [394100] global_step=394100, grad_norm=4.244904041290283, loss=0.5877417325973511
I0301 19:08:47.755671 140089862604544 logging_writer.py:48] [394200] global_step=394200, grad_norm=4.365466594696045, loss=0.6489899754524231
I0301 19:09:21.176855 140089870997248 logging_writer.py:48] [394300] global_step=394300, grad_norm=4.5261993408203125, loss=0.6402139067649841
I0301 19:09:54.646773 140089862604544 logging_writer.py:48] [394400] global_step=394400, grad_norm=4.58894157409668, loss=0.6489229202270508
I0301 19:10:28.070605 140089870997248 logging_writer.py:48] [394500] global_step=394500, grad_norm=4.987116813659668, loss=0.6771895289421082
I0301 19:11:01.538027 140089862604544 logging_writer.py:48] [394600] global_step=394600, grad_norm=4.431868553161621, loss=0.6220154762268066
I0301 19:11:35.021856 140089870997248 logging_writer.py:48] [394700] global_step=394700, grad_norm=4.626547336578369, loss=0.646987795829773
I0301 19:12:08.463790 140089862604544 logging_writer.py:48] [394800] global_step=394800, grad_norm=4.5867204666137695, loss=0.6156324148178101
I0301 19:12:09.284350 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:12:15.394345 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:12:23.533207 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:12:25.824498 140252611495744 submission_runner.py:411] Time since start: 136686.70s, 	Step: 394804, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.1456609070301056, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0530370473861694, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8425297737121582, 'test/num_examples': 10000, 'score': 132154.6266951561, 'total_duration': 136686.70456910133, 'accumulated_submission_time': 132154.6266951561, 'accumulated_eval_time': 4501.413993358612, 'accumulated_logging_time': 16.484485864639282}
I0301 19:12:25.905993 140089770305280 logging_writer.py:48] [394804] accumulated_eval_time=4501.413993, accumulated_logging_time=16.484486, accumulated_submission_time=132154.626695, global_step=394804, preemption_count=0, score=132154.626695, test/accuracy=0.626800, test/loss=1.842530, test/num_examples=10000, total_duration=136686.704569, train/accuracy=0.961296, train/loss=0.145661, validation/accuracy=0.755820, validation/loss=1.053037, validation/num_examples=50000
I0301 19:12:58.355232 140089778697984 logging_writer.py:48] [394900] global_step=394900, grad_norm=4.278775691986084, loss=0.5567665696144104
I0301 19:13:31.860389 140089770305280 logging_writer.py:48] [395000] global_step=395000, grad_norm=4.757595539093018, loss=0.6153340339660645
I0301 19:14:05.385319 140089778697984 logging_writer.py:48] [395100] global_step=395100, grad_norm=4.868369102478027, loss=0.6091816425323486
I0301 19:14:38.882108 140089770305280 logging_writer.py:48] [395200] global_step=395200, grad_norm=5.2698588371276855, loss=0.6302692890167236
I0301 19:15:12.390672 140089778697984 logging_writer.py:48] [395300] global_step=395300, grad_norm=4.820743083953857, loss=0.595923900604248
I0301 19:15:45.839091 140089770305280 logging_writer.py:48] [395400] global_step=395400, grad_norm=4.608847141265869, loss=0.6292451620101929
I0301 19:16:19.306810 140089778697984 logging_writer.py:48] [395500] global_step=395500, grad_norm=4.532922744750977, loss=0.6724151968955994
I0301 19:16:52.778041 140089770305280 logging_writer.py:48] [395600] global_step=395600, grad_norm=4.9403300285339355, loss=0.6718744039535522
I0301 19:17:26.204926 140089778697984 logging_writer.py:48] [395700] global_step=395700, grad_norm=4.5037336349487305, loss=0.5447720289230347
I0301 19:17:59.667232 140089770305280 logging_writer.py:48] [395800] global_step=395800, grad_norm=4.187406063079834, loss=0.6291429400444031
I0301 19:18:33.119950 140089778697984 logging_writer.py:48] [395900] global_step=395900, grad_norm=4.217267990112305, loss=0.5372905731201172
I0301 19:19:06.614313 140089770305280 logging_writer.py:48] [396000] global_step=396000, grad_norm=4.340514659881592, loss=0.5938411355018616
I0301 19:19:40.162003 140089778697984 logging_writer.py:48] [396100] global_step=396100, grad_norm=4.347668647766113, loss=0.6245760917663574
I0301 19:20:13.636917 140089770305280 logging_writer.py:48] [396200] global_step=396200, grad_norm=5.168044567108154, loss=0.6657743453979492
I0301 19:20:47.107383 140089778697984 logging_writer.py:48] [396300] global_step=396300, grad_norm=4.326280117034912, loss=0.6309394836425781
I0301 19:20:55.936423 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:21:02.201196 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:21:10.206049 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:21:12.486699 140252611495744 submission_runner.py:411] Time since start: 137213.37s, 	Step: 396328, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14476333558559418, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0543824434280396, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.8426116704940796, 'test/num_examples': 10000, 'score': 132664.5911166668, 'total_duration': 137213.3666226864, 'accumulated_submission_time': 132664.5911166668, 'accumulated_eval_time': 4517.964068174362, 'accumulated_logging_time': 16.57689118385315}
I0301 19:21:12.570814 140089854211840 logging_writer.py:48] [396328] accumulated_eval_time=4517.964068, accumulated_logging_time=16.576891, accumulated_submission_time=132664.591117, global_step=396328, preemption_count=0, score=132664.591117, test/accuracy=0.626000, test/loss=1.842612, test/num_examples=10000, total_duration=137213.366623, train/accuracy=0.961496, train/loss=0.144763, validation/accuracy=0.755920, validation/loss=1.054382, validation/num_examples=50000
I0301 19:21:36.988391 140089862604544 logging_writer.py:48] [396400] global_step=396400, grad_norm=4.746836185455322, loss=0.6348984241485596
I0301 19:22:10.452754 140089854211840 logging_writer.py:48] [396500] global_step=396500, grad_norm=4.590897083282471, loss=0.5866942405700684
I0301 19:22:43.887004 140089862604544 logging_writer.py:48] [396600] global_step=396600, grad_norm=4.274966716766357, loss=0.6153916120529175
I0301 19:23:17.338879 140089854211840 logging_writer.py:48] [396700] global_step=396700, grad_norm=4.080791473388672, loss=0.5980108976364136
I0301 19:23:50.806491 140089862604544 logging_writer.py:48] [396800] global_step=396800, grad_norm=4.584857940673828, loss=0.5783883929252625
I0301 19:24:24.252009 140089854211840 logging_writer.py:48] [396900] global_step=396900, grad_norm=4.031484127044678, loss=0.5323899984359741
I0301 19:24:57.696991 140089862604544 logging_writer.py:48] [397000] global_step=397000, grad_norm=4.329615592956543, loss=0.6548802852630615
I0301 19:25:31.162327 140089854211840 logging_writer.py:48] [397100] global_step=397100, grad_norm=5.379220485687256, loss=0.659066915512085
I0301 19:26:04.697978 140089862604544 logging_writer.py:48] [397200] global_step=397200, grad_norm=4.588835716247559, loss=0.6485079526901245
I0301 19:26:38.134706 140089854211840 logging_writer.py:48] [397300] global_step=397300, grad_norm=4.551784038543701, loss=0.6299565434455872
I0301 19:27:11.616958 140089862604544 logging_writer.py:48] [397400] global_step=397400, grad_norm=4.710222244262695, loss=0.6457458734512329
I0301 19:27:45.087939 140089854211840 logging_writer.py:48] [397500] global_step=397500, grad_norm=5.046377182006836, loss=0.6964426636695862
I0301 19:28:18.534814 140089862604544 logging_writer.py:48] [397600] global_step=397600, grad_norm=4.263537406921387, loss=0.6126767992973328
I0301 19:28:51.977924 140089854211840 logging_writer.py:48] [397700] global_step=397700, grad_norm=4.433230400085449, loss=0.5894612073898315
I0301 19:29:25.423023 140089862604544 logging_writer.py:48] [397800] global_step=397800, grad_norm=4.300905227661133, loss=0.5825961232185364
I0301 19:29:42.645285 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:29:48.738385 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:29:56.880579 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:29:59.157367 140252611495744 submission_runner.py:411] Time since start: 137740.04s, 	Step: 397853, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14631615579128265, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0536457300186157, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8429162502288818, 'test/num_examples': 10000, 'score': 133174.60023641586, 'total_duration': 137740.03743171692, 'accumulated_submission_time': 133174.60023641586, 'accumulated_eval_time': 4534.476091146469, 'accumulated_logging_time': 16.671013355255127}
I0301 19:29:59.243655 140089770305280 logging_writer.py:48] [397853] accumulated_eval_time=4534.476091, accumulated_logging_time=16.671013, accumulated_submission_time=133174.600236, global_step=397853, preemption_count=0, score=133174.600236, test/accuracy=0.626800, test/loss=1.842916, test/num_examples=10000, total_duration=137740.037432, train/accuracy=0.960559, train/loss=0.146316, validation/accuracy=0.755820, validation/loss=1.053646, validation/num_examples=50000
I0301 19:30:15.272980 140089778697984 logging_writer.py:48] [397900] global_step=397900, grad_norm=4.4314093589782715, loss=0.5833553075790405
I0301 19:30:48.705035 140089770305280 logging_writer.py:48] [398000] global_step=398000, grad_norm=4.678286075592041, loss=0.5851033329963684
I0301 19:31:22.188839 140089778697984 logging_writer.py:48] [398100] global_step=398100, grad_norm=4.634235858917236, loss=0.6683832406997681
I0301 19:31:55.731007 140089770305280 logging_writer.py:48] [398200] global_step=398200, grad_norm=4.858269214630127, loss=0.6602034568786621
I0301 19:32:29.185675 140089778697984 logging_writer.py:48] [398300] global_step=398300, grad_norm=5.216692924499512, loss=0.6422256231307983
I0301 19:33:02.618647 140089770305280 logging_writer.py:48] [398400] global_step=398400, grad_norm=5.04406213760376, loss=0.6617029905319214
I0301 19:33:36.076578 140089778697984 logging_writer.py:48] [398500] global_step=398500, grad_norm=4.412257671356201, loss=0.5908828973770142
I0301 19:34:09.528954 140089770305280 logging_writer.py:48] [398600] global_step=398600, grad_norm=4.330434799194336, loss=0.6529561281204224
I0301 19:34:42.975062 140089778697984 logging_writer.py:48] [398700] global_step=398700, grad_norm=4.677549362182617, loss=0.6232279539108276
I0301 19:35:16.414611 140089770305280 logging_writer.py:48] [398800] global_step=398800, grad_norm=4.228154182434082, loss=0.5929808020591736
I0301 19:35:49.871354 140089778697984 logging_writer.py:48] [398900] global_step=398900, grad_norm=4.717573165893555, loss=0.6309695839881897
I0301 19:36:23.308243 140089770305280 logging_writer.py:48] [399000] global_step=399000, grad_norm=4.679637908935547, loss=0.6509225368499756
I0301 19:36:56.749778 140089778697984 logging_writer.py:48] [399100] global_step=399100, grad_norm=4.619471073150635, loss=0.623083233833313
I0301 19:37:30.149077 140089770305280 logging_writer.py:48] [399200] global_step=399200, grad_norm=4.544108867645264, loss=0.66115802526474
I0301 19:38:03.719633 140089778697984 logging_writer.py:48] [399300] global_step=399300, grad_norm=4.750720977783203, loss=0.6177570819854736
I0301 19:38:29.279735 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:38:35.423528 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:38:43.451179 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:38:45.714139 140252611495744 submission_runner.py:411] Time since start: 138266.59s, 	Step: 399378, 	{'train/accuracy': 0.9625717401504517, 'train/loss': 0.14070643484592438, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.053896427154541, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8440628051757812, 'test/num_examples': 10000, 'score': 133684.56909918785, 'total_duration': 138266.5942044258, 'accumulated_submission_time': 133684.56909918785, 'accumulated_eval_time': 4550.910442829132, 'accumulated_logging_time': 16.769649982452393}
I0301 19:38:45.797372 140089862604544 logging_writer.py:48] [399378] accumulated_eval_time=4550.910443, accumulated_logging_time=16.769650, accumulated_submission_time=133684.569099, global_step=399378, preemption_count=0, score=133684.569099, test/accuracy=0.627500, test/loss=1.844063, test/num_examples=10000, total_duration=138266.594204, train/accuracy=0.962572, train/loss=0.140706, validation/accuracy=0.755860, validation/loss=1.053896, validation/num_examples=50000
I0301 19:38:53.506231 140089870997248 logging_writer.py:48] [399400] global_step=399400, grad_norm=4.756906509399414, loss=0.6480767726898193
I0301 19:39:26.955285 140089862604544 logging_writer.py:48] [399500] global_step=399500, grad_norm=5.26425313949585, loss=0.5848544836044312
I0301 19:40:00.422970 140089870997248 logging_writer.py:48] [399600] global_step=399600, grad_norm=4.8558807373046875, loss=0.653253436088562
I0301 19:40:33.873189 140089862604544 logging_writer.py:48] [399700] global_step=399700, grad_norm=4.684558391571045, loss=0.66148841381073
I0301 19:41:07.375849 140089870997248 logging_writer.py:48] [399800] global_step=399800, grad_norm=4.750209808349609, loss=0.722676157951355
I0301 19:41:40.811838 140089862604544 logging_writer.py:48] [399900] global_step=399900, grad_norm=5.930485248565674, loss=0.6371459364891052
I0301 19:42:14.262295 140089870997248 logging_writer.py:48] [400000] global_step=400000, grad_norm=5.7956743240356445, loss=0.6813026666641235
I0301 19:42:47.747327 140089862604544 logging_writer.py:48] [400100] global_step=400100, grad_norm=4.528326988220215, loss=0.5800483822822571
I0301 19:43:21.213641 140089870997248 logging_writer.py:48] [400200] global_step=400200, grad_norm=4.849611282348633, loss=0.617124080657959
I0301 19:43:54.657766 140089862604544 logging_writer.py:48] [400300] global_step=400300, grad_norm=4.4000091552734375, loss=0.6455010771751404
I0301 19:44:28.268162 140089870997248 logging_writer.py:48] [400400] global_step=400400, grad_norm=4.994781970977783, loss=0.6873102784156799
I0301 19:45:01.719522 140089862604544 logging_writer.py:48] [400500] global_step=400500, grad_norm=4.438249588012695, loss=0.6388511657714844
I0301 19:45:35.189243 140089870997248 logging_writer.py:48] [400600] global_step=400600, grad_norm=4.4817795753479, loss=0.5892385244369507
I0301 19:46:08.623458 140089862604544 logging_writer.py:48] [400700] global_step=400700, grad_norm=4.364994049072266, loss=0.552829384803772
I0301 19:46:42.067608 140089870997248 logging_writer.py:48] [400800] global_step=400800, grad_norm=4.642080307006836, loss=0.6348423361778259
I0301 19:47:15.519832 140089862604544 logging_writer.py:48] [400900] global_step=400900, grad_norm=4.344091415405273, loss=0.5727992057800293
I0301 19:47:15.996361 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:47:22.120054 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:47:30.288853 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:47:32.517828 140252611495744 submission_runner.py:411] Time since start: 138793.40s, 	Step: 400903, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.1468479335308075, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0537818670272827, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8432790040969849, 'test/num_examples': 10000, 'score': 134194.7005777359, 'total_duration': 138793.39784646034, 'accumulated_submission_time': 134194.7005777359, 'accumulated_eval_time': 4567.4318034648895, 'accumulated_logging_time': 16.86515712738037}
I0301 19:47:32.605407 140089845819136 logging_writer.py:48] [400903] accumulated_eval_time=4567.431803, accumulated_logging_time=16.865157, accumulated_submission_time=134194.700578, global_step=400903, preemption_count=0, score=134194.700578, test/accuracy=0.627000, test/loss=1.843279, test/num_examples=10000, total_duration=138793.397846, train/accuracy=0.960240, train/loss=0.146848, validation/accuracy=0.755720, validation/loss=1.053782, validation/num_examples=50000
I0301 19:48:05.378539 140089854211840 logging_writer.py:48] [401000] global_step=401000, grad_norm=4.622766971588135, loss=0.6822196841239929
I0301 19:48:38.851824 140089845819136 logging_writer.py:48] [401100] global_step=401100, grad_norm=4.791579723358154, loss=0.669835090637207
I0301 19:49:12.282900 140089854211840 logging_writer.py:48] [401200] global_step=401200, grad_norm=4.9365644454956055, loss=0.6730304956436157
I0301 19:49:45.735028 140089845819136 logging_writer.py:48] [401300] global_step=401300, grad_norm=4.686367034912109, loss=0.6009607315063477
I0301 19:50:19.311282 140089854211840 logging_writer.py:48] [401400] global_step=401400, grad_norm=4.679378509521484, loss=0.6358323097229004
I0301 19:50:52.785355 140089845819136 logging_writer.py:48] [401500] global_step=401500, grad_norm=5.228360652923584, loss=0.6161854267120361
I0301 19:51:26.211071 140089854211840 logging_writer.py:48] [401600] global_step=401600, grad_norm=4.694047927856445, loss=0.5907925367355347
I0301 19:51:59.671044 140089845819136 logging_writer.py:48] [401700] global_step=401700, grad_norm=4.46915340423584, loss=0.6404218673706055
I0301 19:52:33.124514 140089854211840 logging_writer.py:48] [401800] global_step=401800, grad_norm=4.209846496582031, loss=0.593361496925354
I0301 19:53:06.551750 140089845819136 logging_writer.py:48] [401900] global_step=401900, grad_norm=4.860130786895752, loss=0.6726862192153931
I0301 19:53:40.022559 140089854211840 logging_writer.py:48] [402000] global_step=402000, grad_norm=5.15968132019043, loss=0.5761758089065552
I0301 19:54:13.465496 140089845819136 logging_writer.py:48] [402100] global_step=402100, grad_norm=4.374110698699951, loss=0.6368756294250488
I0301 19:54:46.911689 140089854211840 logging_writer.py:48] [402200] global_step=402200, grad_norm=5.076177597045898, loss=0.6755958795547485
I0301 19:55:20.323658 140089845819136 logging_writer.py:48] [402300] global_step=402300, grad_norm=4.634064674377441, loss=0.5799415707588196
I0301 19:55:53.772732 140089854211840 logging_writer.py:48] [402400] global_step=402400, grad_norm=5.1291584968566895, loss=0.6249568462371826
I0301 19:56:02.601452 140252611495744 spec.py:321] Evaluating on the training split.
I0301 19:56:09.017629 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 19:56:17.290035 140252611495744 spec.py:349] Evaluating on the test split.
I0301 19:56:19.598208 140252611495744 submission_runner.py:411] Time since start: 139320.48s, 	Step: 402428, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14780345559120178, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.053891658782959, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.842927098274231, 'test/num_examples': 10000, 'score': 134704.63186764717, 'total_duration': 139320.47827506065, 'accumulated_submission_time': 134704.63186764717, 'accumulated_eval_time': 4584.428512334824, 'accumulated_logging_time': 16.963099002838135}
I0301 19:56:19.685817 140089778697984 logging_writer.py:48] [402428] accumulated_eval_time=4584.428512, accumulated_logging_time=16.963099, accumulated_submission_time=134704.631868, global_step=402428, preemption_count=0, score=134704.631868, test/accuracy=0.627600, test/loss=1.842927, test/num_examples=10000, total_duration=139320.478275, train/accuracy=0.960439, train/loss=0.147803, validation/accuracy=0.755940, validation/loss=1.053892, validation/num_examples=50000
I0301 19:56:44.100637 140089837426432 logging_writer.py:48] [402500] global_step=402500, grad_norm=4.411903381347656, loss=0.6693549156188965
I0301 19:57:17.565534 140089778697984 logging_writer.py:48] [402600] global_step=402600, grad_norm=4.291567802429199, loss=0.6037020087242126
I0301 19:57:51.017914 140089837426432 logging_writer.py:48] [402700] global_step=402700, grad_norm=5.007810592651367, loss=0.6727707386016846
I0301 19:58:24.455445 140089778697984 logging_writer.py:48] [402800] global_step=402800, grad_norm=4.576240062713623, loss=0.6552581787109375
I0301 19:58:57.952171 140089837426432 logging_writer.py:48] [402900] global_step=402900, grad_norm=4.906312465667725, loss=0.618346631526947
I0301 19:59:31.396122 140089778697984 logging_writer.py:48] [403000] global_step=403000, grad_norm=4.641386032104492, loss=0.7018486261367798
I0301 20:00:04.848534 140089837426432 logging_writer.py:48] [403100] global_step=403100, grad_norm=4.175591468811035, loss=0.6200382709503174
I0301 20:00:38.301679 140089778697984 logging_writer.py:48] [403200] global_step=403200, grad_norm=4.7371826171875, loss=0.5811360478401184
I0301 20:01:11.739019 140089837426432 logging_writer.py:48] [403300] global_step=403300, grad_norm=4.5911664962768555, loss=0.6800363063812256
I0301 20:01:45.219572 140089778697984 logging_writer.py:48] [403400] global_step=403400, grad_norm=4.60521936416626, loss=0.6657050251960754
I0301 20:02:18.753017 140089837426432 logging_writer.py:48] [403500] global_step=403500, grad_norm=4.687187671661377, loss=0.6175933480262756
I0301 20:02:52.176631 140089778697984 logging_writer.py:48] [403600] global_step=403600, grad_norm=4.621628761291504, loss=0.5577508807182312
I0301 20:03:25.616158 140089837426432 logging_writer.py:48] [403700] global_step=403700, grad_norm=4.515742778778076, loss=0.665412962436676
I0301 20:03:59.087813 140089778697984 logging_writer.py:48] [403800] global_step=403800, grad_norm=4.60237979888916, loss=0.6761030554771423
I0301 20:04:32.589736 140089837426432 logging_writer.py:48] [403900] global_step=403900, grad_norm=4.425273418426514, loss=0.5871928930282593
I0301 20:04:49.762802 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:04:55.972454 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:05:04.137508 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:05:06.401350 140252611495744 submission_runner.py:411] Time since start: 139847.28s, 	Step: 403953, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.14615795016288757, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0521721839904785, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8392106294631958, 'test/num_examples': 10000, 'score': 135214.64415454865, 'total_duration': 139847.28141379356, 'accumulated_submission_time': 135214.64415454865, 'accumulated_eval_time': 4601.067002296448, 'accumulated_logging_time': 17.061861038208008}
I0301 20:05:06.488277 140089862604544 logging_writer.py:48] [403953] accumulated_eval_time=4601.067002, accumulated_logging_time=17.061861, accumulated_submission_time=135214.644155, global_step=403953, preemption_count=0, score=135214.644155, test/accuracy=0.627100, test/loss=1.839211, test/num_examples=10000, total_duration=139847.281414, train/accuracy=0.961336, train/loss=0.146158, validation/accuracy=0.756120, validation/loss=1.052172, validation/num_examples=50000
I0301 20:05:22.529978 140089870997248 logging_writer.py:48] [404000] global_step=404000, grad_norm=4.729620933532715, loss=0.6510471105575562
I0301 20:05:56.003621 140089862604544 logging_writer.py:48] [404100] global_step=404100, grad_norm=5.032043933868408, loss=0.6305550932884216
I0301 20:06:29.433638 140089870997248 logging_writer.py:48] [404200] global_step=404200, grad_norm=4.654223918914795, loss=0.6211899518966675
I0301 20:07:02.895155 140089862604544 logging_writer.py:48] [404300] global_step=404300, grad_norm=4.423074722290039, loss=0.6799193024635315
I0301 20:07:36.378697 140089870997248 logging_writer.py:48] [404400] global_step=404400, grad_norm=4.455571174621582, loss=0.6462303400039673
I0301 20:08:09.820983 140089862604544 logging_writer.py:48] [404500] global_step=404500, grad_norm=4.734559535980225, loss=0.6075010299682617
I0301 20:08:43.341650 140089870997248 logging_writer.py:48] [404600] global_step=404600, grad_norm=4.925087928771973, loss=0.6863076090812683
I0301 20:09:16.785245 140089862604544 logging_writer.py:48] [404700] global_step=404700, grad_norm=4.259178638458252, loss=0.5382760763168335
I0301 20:09:50.211929 140089870997248 logging_writer.py:48] [404800] global_step=404800, grad_norm=4.689299583435059, loss=0.6360493898391724
I0301 20:10:23.719693 140089862604544 logging_writer.py:48] [404900] global_step=404900, grad_norm=5.933216094970703, loss=0.6830769777297974
I0301 20:10:57.183084 140089870997248 logging_writer.py:48] [405000] global_step=405000, grad_norm=4.538146018981934, loss=0.6227971911430359
I0301 20:11:30.613986 140089862604544 logging_writer.py:48] [405100] global_step=405100, grad_norm=4.420904636383057, loss=0.6522626876831055
I0301 20:12:04.054410 140089870997248 logging_writer.py:48] [405200] global_step=405200, grad_norm=4.444550037384033, loss=0.5529884696006775
I0301 20:12:37.514863 140089862604544 logging_writer.py:48] [405300] global_step=405300, grad_norm=4.458954811096191, loss=0.6158900856971741
I0301 20:13:10.950816 140089870997248 logging_writer.py:48] [405400] global_step=405400, grad_norm=4.36829137802124, loss=0.596982479095459
I0301 20:13:36.537980 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:13:42.705632 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:13:50.822586 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:13:53.099620 140252611495744 submission_runner.py:411] Time since start: 140373.98s, 	Step: 405478, 	{'train/accuracy': 0.9627909660339355, 'train/loss': 0.14217674732208252, 'validation/accuracy': 0.756119966506958, 'validation/loss': 1.0536324977874756, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.84215247631073, 'test/num_examples': 10000, 'score': 135724.62863755226, 'total_duration': 140373.97967410088, 'accumulated_submission_time': 135724.62863755226, 'accumulated_eval_time': 4617.628590106964, 'accumulated_logging_time': 17.158963441848755}
I0301 20:13:53.192746 140089778697984 logging_writer.py:48] [405478] accumulated_eval_time=4617.628590, accumulated_logging_time=17.158963, accumulated_submission_time=135724.628638, global_step=405478, preemption_count=0, score=135724.628638, test/accuracy=0.627300, test/loss=1.842152, test/num_examples=10000, total_duration=140373.979674, train/accuracy=0.962791, train/loss=0.142177, validation/accuracy=0.756120, validation/loss=1.053632, validation/num_examples=50000
I0301 20:14:00.893917 140089837426432 logging_writer.py:48] [405500] global_step=405500, grad_norm=4.620757102966309, loss=0.6474533677101135
I0301 20:14:34.476850 140089778697984 logging_writer.py:48] [405600] global_step=405600, grad_norm=4.385003566741943, loss=0.6302376389503479
I0301 20:15:07.920759 140089837426432 logging_writer.py:48] [405700] global_step=405700, grad_norm=4.29897403717041, loss=0.5922403335571289
I0301 20:15:41.370364 140089778697984 logging_writer.py:48] [405800] global_step=405800, grad_norm=4.441115856170654, loss=0.5904574394226074
I0301 20:16:14.835724 140089837426432 logging_writer.py:48] [405900] global_step=405900, grad_norm=5.218477249145508, loss=0.5719207525253296
I0301 20:16:48.286432 140089778697984 logging_writer.py:48] [406000] global_step=406000, grad_norm=4.1789021492004395, loss=0.569573163986206
I0301 20:17:21.717000 140089837426432 logging_writer.py:48] [406100] global_step=406100, grad_norm=4.9327073097229, loss=0.5712701082229614
I0301 20:17:55.177734 140089778697984 logging_writer.py:48] [406200] global_step=406200, grad_norm=4.31164026260376, loss=0.6228913068771362
I0301 20:18:28.662876 140089837426432 logging_writer.py:48] [406300] global_step=406300, grad_norm=4.619550704956055, loss=0.6068975925445557
I0301 20:19:02.166431 140089778697984 logging_writer.py:48] [406400] global_step=406400, grad_norm=4.633012771606445, loss=0.6862249970436096
I0301 20:19:35.597904 140089837426432 logging_writer.py:48] [406500] global_step=406500, grad_norm=4.102325916290283, loss=0.5663710832595825
I0301 20:20:09.069520 140089778697984 logging_writer.py:48] [406600] global_step=406600, grad_norm=4.16923189163208, loss=0.6736541986465454
I0301 20:20:42.565608 140089837426432 logging_writer.py:48] [406700] global_step=406700, grad_norm=5.064310550689697, loss=0.6715394854545593
I0301 20:21:16.077088 140089778697984 logging_writer.py:48] [406800] global_step=406800, grad_norm=4.426794528961182, loss=0.656640887260437
I0301 20:21:49.529936 140089837426432 logging_writer.py:48] [406900] global_step=406900, grad_norm=4.39808988571167, loss=0.6206911206245422
I0301 20:22:22.965291 140089778697984 logging_writer.py:48] [407000] global_step=407000, grad_norm=4.595669746398926, loss=0.6144391298294067
I0301 20:22:23.112965 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:22:29.262693 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:22:37.400827 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:22:39.672344 140252611495744 submission_runner.py:411] Time since start: 140900.55s, 	Step: 407002, 	{'train/accuracy': 0.9618144035339355, 'train/loss': 0.14525561034679413, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0538339614868164, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8435428142547607, 'test/num_examples': 10000, 'score': 136234.4849574566, 'total_duration': 140900.5523967743, 'accumulated_submission_time': 136234.4849574566, 'accumulated_eval_time': 4634.187923192978, 'accumulated_logging_time': 17.261436939239502}
I0301 20:22:39.755660 140089770305280 logging_writer.py:48] [407002] accumulated_eval_time=4634.187923, accumulated_logging_time=17.261437, accumulated_submission_time=136234.484957, global_step=407002, preemption_count=0, score=136234.484957, test/accuracy=0.627600, test/loss=1.843543, test/num_examples=10000, total_duration=140900.552397, train/accuracy=0.961814, train/loss=0.145256, validation/accuracy=0.756000, validation/loss=1.053834, validation/num_examples=50000
I0301 20:23:12.885315 140089778697984 logging_writer.py:48] [407100] global_step=407100, grad_norm=4.743765354156494, loss=0.6456232666969299
I0301 20:23:46.331201 140089770305280 logging_writer.py:48] [407200] global_step=407200, grad_norm=4.647051811218262, loss=0.6658291816711426
I0301 20:24:19.779471 140089778697984 logging_writer.py:48] [407300] global_step=407300, grad_norm=4.342223644256592, loss=0.6634901762008667
I0301 20:24:53.272392 140089770305280 logging_writer.py:48] [407400] global_step=407400, grad_norm=4.26125431060791, loss=0.5917661786079407
I0301 20:25:26.692419 140089778697984 logging_writer.py:48] [407500] global_step=407500, grad_norm=4.412552356719971, loss=0.5937475562095642
I0301 20:26:00.152024 140089770305280 logging_writer.py:48] [407600] global_step=407600, grad_norm=4.362570762634277, loss=0.6289418339729309
I0301 20:26:33.600306 140089778697984 logging_writer.py:48] [407700] global_step=407700, grad_norm=4.39108419418335, loss=0.6198830008506775
I0301 20:27:07.094603 140089770305280 logging_writer.py:48] [407800] global_step=407800, grad_norm=4.718493938446045, loss=0.5885616540908813
I0301 20:27:40.548197 140089778697984 logging_writer.py:48] [407900] global_step=407900, grad_norm=4.998073577880859, loss=0.6777085661888123
I0301 20:28:13.998251 140089770305280 logging_writer.py:48] [408000] global_step=408000, grad_norm=5.160985469818115, loss=0.6960184574127197
I0301 20:28:47.450904 140089778697984 logging_writer.py:48] [408100] global_step=408100, grad_norm=4.49858283996582, loss=0.6338179111480713
I0301 20:29:20.940869 140089770305280 logging_writer.py:48] [408200] global_step=408200, grad_norm=4.489147186279297, loss=0.6403169631958008
I0301 20:29:54.369327 140089778697984 logging_writer.py:48] [408300] global_step=408300, grad_norm=4.554711818695068, loss=0.559798002243042
I0301 20:30:27.851323 140089770305280 logging_writer.py:48] [408400] global_step=408400, grad_norm=5.069632053375244, loss=0.5788723826408386
I0301 20:31:01.271152 140089778697984 logging_writer.py:48] [408500] global_step=408500, grad_norm=4.747878074645996, loss=0.6500399708747864
I0301 20:31:09.765940 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:31:15.883941 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:31:24.057498 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:31:26.301369 140252611495744 submission_runner.py:411] Time since start: 141427.18s, 	Step: 408527, 	{'train/accuracy': 0.9584860801696777, 'train/loss': 0.15102314949035645, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0538654327392578, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.842506766319275, 'test/num_examples': 10000, 'score': 136744.42813515663, 'total_duration': 141427.18142938614, 'accumulated_submission_time': 136744.42813515663, 'accumulated_eval_time': 4650.723289728165, 'accumulated_logging_time': 17.356701374053955}
I0301 20:31:26.390643 140089854211840 logging_writer.py:48] [408527] accumulated_eval_time=4650.723290, accumulated_logging_time=17.356701, accumulated_submission_time=136744.428135, global_step=408527, preemption_count=0, score=136744.428135, test/accuracy=0.627200, test/loss=1.842507, test/num_examples=10000, total_duration=141427.181429, train/accuracy=0.958486, train/loss=0.151023, validation/accuracy=0.756040, validation/loss=1.053865, validation/num_examples=50000
I0301 20:31:51.180168 140089862604544 logging_writer.py:48] [408600] global_step=408600, grad_norm=4.470951080322266, loss=0.7213961482048035
I0301 20:32:24.647733 140089854211840 logging_writer.py:48] [408700] global_step=408700, grad_norm=4.507940769195557, loss=0.6655856370925903
I0301 20:32:58.161943 140089862604544 logging_writer.py:48] [408800] global_step=408800, grad_norm=4.294538497924805, loss=0.6682045459747314
I0301 20:33:31.620172 140089854211840 logging_writer.py:48] [408900] global_step=408900, grad_norm=4.354513168334961, loss=0.6009002327919006
I0301 20:34:05.086829 140089862604544 logging_writer.py:48] [409000] global_step=409000, grad_norm=4.910516738891602, loss=0.6348065733909607
I0301 20:34:38.577092 140089854211840 logging_writer.py:48] [409100] global_step=409100, grad_norm=4.6102752685546875, loss=0.659415602684021
I0301 20:35:12.049903 140089862604544 logging_writer.py:48] [409200] global_step=409200, grad_norm=5.1477370262146, loss=0.6005581617355347
I0301 20:35:45.539909 140089854211840 logging_writer.py:48] [409300] global_step=409300, grad_norm=4.6519646644592285, loss=0.6049172282218933
I0301 20:36:18.988725 140089862604544 logging_writer.py:48] [409400] global_step=409400, grad_norm=4.410200595855713, loss=0.6549316644668579
I0301 20:36:52.447329 140089854211840 logging_writer.py:48] [409500] global_step=409500, grad_norm=4.920465469360352, loss=0.6094106435775757
I0301 20:37:25.900358 140089862604544 logging_writer.py:48] [409600] global_step=409600, grad_norm=4.857126712799072, loss=0.6583173274993896
I0301 20:37:59.359575 140089854211840 logging_writer.py:48] [409700] global_step=409700, grad_norm=4.197394371032715, loss=0.5640183687210083
I0301 20:38:32.800964 140089862604544 logging_writer.py:48] [409800] global_step=409800, grad_norm=4.239893913269043, loss=0.551959216594696
I0301 20:39:06.299697 140089854211840 logging_writer.py:48] [409900] global_step=409900, grad_norm=4.7380900382995605, loss=0.7205198407173157
I0301 20:39:39.759518 140089862604544 logging_writer.py:48] [410000] global_step=410000, grad_norm=4.462313175201416, loss=0.5900213718414307
I0301 20:39:56.613910 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:40:02.842282 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:40:10.895834 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:40:13.172666 140252611495744 submission_runner.py:411] Time since start: 141954.05s, 	Step: 410052, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14615143835544586, 'validation/accuracy': 0.7562400102615356, 'validation/loss': 1.053558349609375, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.843324899673462, 'test/num_examples': 10000, 'score': 137254.58539009094, 'total_duration': 141954.05271077156, 'accumulated_submission_time': 137254.58539009094, 'accumulated_eval_time': 4667.281981468201, 'accumulated_logging_time': 17.456937551498413}
I0301 20:40:13.254257 140089552271104 logging_writer.py:48] [410052] accumulated_eval_time=4667.281981, accumulated_logging_time=17.456938, accumulated_submission_time=137254.585390, global_step=410052, preemption_count=0, score=137254.585390, test/accuracy=0.627600, test/loss=1.843325, test/num_examples=10000, total_duration=141954.052711, train/accuracy=0.960639, train/loss=0.146151, validation/accuracy=0.756240, validation/loss=1.053558, validation/num_examples=50000
I0301 20:40:29.682906 140089770305280 logging_writer.py:48] [410100] global_step=410100, grad_norm=4.517074108123779, loss=0.6400080919265747
I0301 20:41:03.144139 140089552271104 logging_writer.py:48] [410200] global_step=410200, grad_norm=5.4335856437683105, loss=0.7441739439964294
I0301 20:41:36.579112 140089770305280 logging_writer.py:48] [410300] global_step=410300, grad_norm=4.37143087387085, loss=0.6535016298294067
I0301 20:42:10.066220 140089552271104 logging_writer.py:48] [410400] global_step=410400, grad_norm=4.691273212432861, loss=0.6486589908599854
I0301 20:42:43.554965 140089770305280 logging_writer.py:48] [410500] global_step=410500, grad_norm=4.221381187438965, loss=0.5784870386123657
I0301 20:43:16.994222 140089552271104 logging_writer.py:48] [410600] global_step=410600, grad_norm=4.50738525390625, loss=0.6987788081169128
I0301 20:43:50.487515 140089770305280 logging_writer.py:48] [410700] global_step=410700, grad_norm=4.230262279510498, loss=0.5960100889205933
I0301 20:44:23.994289 140089552271104 logging_writer.py:48] [410800] global_step=410800, grad_norm=4.472657203674316, loss=0.5730339884757996
I0301 20:44:57.499554 140089770305280 logging_writer.py:48] [410900] global_step=410900, grad_norm=4.603691101074219, loss=0.5725669860839844
I0301 20:45:31.040673 140089552271104 logging_writer.py:48] [411000] global_step=411000, grad_norm=5.227067947387695, loss=0.6867980360984802
I0301 20:46:04.533679 140089770305280 logging_writer.py:48] [411100] global_step=411100, grad_norm=4.539700984954834, loss=0.6088722944259644
I0301 20:46:37.996601 140089552271104 logging_writer.py:48] [411200] global_step=411200, grad_norm=4.418534755706787, loss=0.586442232131958
I0301 20:47:11.439087 140089770305280 logging_writer.py:48] [411300] global_step=411300, grad_norm=4.8864569664001465, loss=0.6920630931854248
I0301 20:47:44.917563 140089552271104 logging_writer.py:48] [411400] global_step=411400, grad_norm=4.6037278175354, loss=0.6599174737930298
I0301 20:48:18.379378 140089770305280 logging_writer.py:48] [411500] global_step=411500, grad_norm=4.75748872756958, loss=0.6072570085525513
I0301 20:48:43.274417 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:48:49.388238 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:48:57.503859 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:48:59.867499 140252611495744 submission_runner.py:411] Time since start: 142480.75s, 	Step: 411576, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.1457064151763916, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0536881685256958, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8429654836654663, 'test/num_examples': 10000, 'score': 137764.53922367096, 'total_duration': 142480.7475554943, 'accumulated_submission_time': 137764.53922367096, 'accumulated_eval_time': 4683.87499833107, 'accumulated_logging_time': 17.549427270889282}
I0301 20:48:59.952269 140089770305280 logging_writer.py:48] [411576] accumulated_eval_time=4683.874998, accumulated_logging_time=17.549427, accumulated_submission_time=137764.539224, global_step=411576, preemption_count=0, score=137764.539224, test/accuracy=0.627700, test/loss=1.842965, test/num_examples=10000, total_duration=142480.747555, train/accuracy=0.961396, train/loss=0.145706, validation/accuracy=0.756140, validation/loss=1.053688, validation/num_examples=50000
I0301 20:49:08.326231 140089845819136 logging_writer.py:48] [411600] global_step=411600, grad_norm=4.376092910766602, loss=0.663019597530365
I0301 20:49:41.776087 140089770305280 logging_writer.py:48] [411700] global_step=411700, grad_norm=4.110745906829834, loss=0.6076139211654663
I0301 20:50:15.234900 140089845819136 logging_writer.py:48] [411800] global_step=411800, grad_norm=5.048290252685547, loss=0.6044079661369324
I0301 20:50:48.719966 140089770305280 logging_writer.py:48] [411900] global_step=411900, grad_norm=4.837336540222168, loss=0.6241160035133362
I0301 20:51:22.280771 140089845819136 logging_writer.py:48] [412000] global_step=412000, grad_norm=4.59659481048584, loss=0.6544603109359741
I0301 20:51:55.737904 140089770305280 logging_writer.py:48] [412100] global_step=412100, grad_norm=4.714539051055908, loss=0.6783198118209839
I0301 20:52:29.168340 140089845819136 logging_writer.py:48] [412200] global_step=412200, grad_norm=4.483939170837402, loss=0.6788510084152222
I0301 20:53:02.609843 140089770305280 logging_writer.py:48] [412300] global_step=412300, grad_norm=4.210735321044922, loss=0.5727604627609253
I0301 20:53:36.054942 140089845819136 logging_writer.py:48] [412400] global_step=412400, grad_norm=4.305084228515625, loss=0.634814977645874
I0301 20:54:09.509342 140089770305280 logging_writer.py:48] [412500] global_step=412500, grad_norm=4.978357791900635, loss=0.6691091656684875
I0301 20:54:42.980580 140089845819136 logging_writer.py:48] [412600] global_step=412600, grad_norm=4.37357234954834, loss=0.6267759203910828
I0301 20:55:16.452014 140089770305280 logging_writer.py:48] [412700] global_step=412700, grad_norm=4.716691493988037, loss=0.6809191703796387
I0301 20:55:49.920970 140089845819136 logging_writer.py:48] [412800] global_step=412800, grad_norm=4.868144989013672, loss=0.6040091514587402
I0301 20:56:23.360844 140089770305280 logging_writer.py:48] [412900] global_step=412900, grad_norm=5.087831020355225, loss=0.6667949557304382
I0301 20:56:56.821817 140089845819136 logging_writer.py:48] [413000] global_step=413000, grad_norm=4.452123641967773, loss=0.5970566868782043
I0301 20:57:29.888124 140252611495744 spec.py:321] Evaluating on the training split.
I0301 20:57:36.096460 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 20:57:44.185809 140252611495744 spec.py:349] Evaluating on the test split.
I0301 20:57:46.539574 140252611495744 submission_runner.py:411] Time since start: 143007.42s, 	Step: 413100, 	{'train/accuracy': 0.9624919891357422, 'train/loss': 0.14171329140663147, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.054227352142334, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.843506932258606, 'test/num_examples': 10000, 'score': 138274.40833592415, 'total_duration': 143007.41964292526, 'accumulated_submission_time': 138274.40833592415, 'accumulated_eval_time': 4700.526404619217, 'accumulated_logging_time': 17.64485478401184}
I0301 20:57:46.622972 140089778697984 logging_writer.py:48] [413100] accumulated_eval_time=4700.526405, accumulated_logging_time=17.644855, accumulated_submission_time=138274.408336, global_step=413100, preemption_count=0, score=138274.408336, test/accuracy=0.627100, test/loss=1.843507, test/num_examples=10000, total_duration=143007.419643, train/accuracy=0.962492, train/loss=0.141713, validation/accuracy=0.755800, validation/loss=1.054227, validation/num_examples=50000
I0301 20:57:46.962896 140089837426432 logging_writer.py:48] [413100] global_step=413100, grad_norm=4.254164218902588, loss=0.5660873651504517
I0301 20:58:20.391344 140089778697984 logging_writer.py:48] [413200] global_step=413200, grad_norm=4.209826469421387, loss=0.6005902886390686
I0301 20:58:53.825490 140089837426432 logging_writer.py:48] [413300] global_step=413300, grad_norm=4.338797569274902, loss=0.6546659469604492
I0301 20:59:27.263903 140089778697984 logging_writer.py:48] [413400] global_step=413400, grad_norm=4.568479061126709, loss=0.6283981800079346
I0301 21:00:00.694723 140089837426432 logging_writer.py:48] [413500] global_step=413500, grad_norm=5.017947196960449, loss=0.7001426815986633
I0301 21:00:34.172829 140089778697984 logging_writer.py:48] [413600] global_step=413600, grad_norm=4.743122100830078, loss=0.6216303110122681
I0301 21:01:07.595499 140089837426432 logging_writer.py:48] [413700] global_step=413700, grad_norm=4.178593158721924, loss=0.6005426645278931
I0301 21:01:41.067469 140089778697984 logging_writer.py:48] [413800] global_step=413800, grad_norm=4.389853477478027, loss=0.6479068398475647
I0301 21:02:14.494372 140089837426432 logging_writer.py:48] [413900] global_step=413900, grad_norm=4.4415178298950195, loss=0.6893931031227112
I0301 21:02:47.957420 140089778697984 logging_writer.py:48] [414000] global_step=414000, grad_norm=4.687839031219482, loss=0.6018270254135132
I0301 21:03:21.527477 140089837426432 logging_writer.py:48] [414100] global_step=414100, grad_norm=4.67412805557251, loss=0.6559815406799316
I0301 21:03:54.980720 140089778697984 logging_writer.py:48] [414200] global_step=414200, grad_norm=4.467487335205078, loss=0.5997713804244995
I0301 21:04:28.432600 140089837426432 logging_writer.py:48] [414300] global_step=414300, grad_norm=4.540652275085449, loss=0.5447959899902344
I0301 21:05:01.881994 140089778697984 logging_writer.py:48] [414400] global_step=414400, grad_norm=5.023528099060059, loss=0.6549601554870605
I0301 21:05:35.375158 140089837426432 logging_writer.py:48] [414500] global_step=414500, grad_norm=4.719621658325195, loss=0.6361506581306458
I0301 21:06:08.828906 140089778697984 logging_writer.py:48] [414600] global_step=414600, grad_norm=4.577661514282227, loss=0.5842225551605225
I0301 21:06:16.671095 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:06:22.925504 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:06:31.093666 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:06:33.348613 140252611495744 submission_runner.py:411] Time since start: 143534.23s, 	Step: 414625, 	{'train/accuracy': 0.9585259556770325, 'train/loss': 0.1491367220878601, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0536048412322998, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8436604738235474, 'test/num_examples': 10000, 'score': 138784.3907828331, 'total_duration': 143534.22866630554, 'accumulated_submission_time': 138784.3907828331, 'accumulated_eval_time': 4717.203856706619, 'accumulated_logging_time': 17.738426208496094}
I0301 21:06:33.437098 140089778697984 logging_writer.py:48] [414625] accumulated_eval_time=4717.203857, accumulated_logging_time=17.738426, accumulated_submission_time=138784.390783, global_step=414625, preemption_count=0, score=138784.390783, test/accuracy=0.627100, test/loss=1.843660, test/num_examples=10000, total_duration=143534.228666, train/accuracy=0.958526, train/loss=0.149137, validation/accuracy=0.755440, validation/loss=1.053605, validation/num_examples=50000
I0301 21:06:58.866876 140089837426432 logging_writer.py:48] [414700] global_step=414700, grad_norm=4.233179569244385, loss=0.6436284184455872
I0301 21:07:32.324687 140089778697984 logging_writer.py:48] [414800] global_step=414800, grad_norm=4.41084098815918, loss=0.6575832366943359
I0301 21:08:05.755395 140089837426432 logging_writer.py:48] [414900] global_step=414900, grad_norm=4.3812761306762695, loss=0.6252768635749817
I0301 21:08:39.214095 140089778697984 logging_writer.py:48] [415000] global_step=415000, grad_norm=4.435048580169678, loss=0.562610387802124
I0301 21:09:12.652228 140089837426432 logging_writer.py:48] [415100] global_step=415100, grad_norm=4.160778045654297, loss=0.6056224703788757
I0301 21:09:46.194776 140089778697984 logging_writer.py:48] [415200] global_step=415200, grad_norm=3.8804984092712402, loss=0.5474038124084473
I0301 21:10:19.656180 140089837426432 logging_writer.py:48] [415300] global_step=415300, grad_norm=4.934890270233154, loss=0.6430699229240417
I0301 21:10:53.108645 140089778697984 logging_writer.py:48] [415400] global_step=415400, grad_norm=4.347513675689697, loss=0.5325742959976196
I0301 21:11:26.534314 140089837426432 logging_writer.py:48] [415500] global_step=415500, grad_norm=4.4549760818481445, loss=0.6018270254135132
I0301 21:12:00.040279 140089778697984 logging_writer.py:48] [415600] global_step=415600, grad_norm=5.654922008514404, loss=0.6567897200584412
I0301 21:12:33.477555 140089837426432 logging_writer.py:48] [415700] global_step=415700, grad_norm=4.55757999420166, loss=0.5570303797721863
I0301 21:13:06.943964 140089778697984 logging_writer.py:48] [415800] global_step=415800, grad_norm=4.418560981750488, loss=0.5745663046836853
I0301 21:13:40.424027 140089837426432 logging_writer.py:48] [415900] global_step=415900, grad_norm=5.0881195068359375, loss=0.6236540675163269
I0301 21:14:13.849917 140089778697984 logging_writer.py:48] [416000] global_step=416000, grad_norm=4.766395568847656, loss=0.6227924227714539
I0301 21:14:47.314728 140089837426432 logging_writer.py:48] [416100] global_step=416100, grad_norm=4.139088153839111, loss=0.5382020473480225
I0301 21:15:03.519211 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:15:09.653623 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:15:17.691520 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:15:19.992737 140252611495744 submission_runner.py:411] Time since start: 144060.87s, 	Step: 416150, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.1463647484779358, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.052968978881836, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.8419307470321655, 'test/num_examples': 10000, 'score': 139294.40902137756, 'total_duration': 144060.8727862835, 'accumulated_submission_time': 139294.40902137756, 'accumulated_eval_time': 4733.677309751511, 'accumulated_logging_time': 17.83676266670227}
I0301 21:15:20.077970 140089552271104 logging_writer.py:48] [416150] accumulated_eval_time=4733.677310, accumulated_logging_time=17.836763, accumulated_submission_time=139294.409021, global_step=416150, preemption_count=0, score=139294.409021, test/accuracy=0.626900, test/loss=1.841931, test/num_examples=10000, total_duration=144060.872786, train/accuracy=0.960778, train/loss=0.146365, validation/accuracy=0.755720, validation/loss=1.052969, validation/num_examples=50000
I0301 21:15:37.290571 140089770305280 logging_writer.py:48] [416200] global_step=416200, grad_norm=4.480930328369141, loss=0.6268795728683472
I0301 21:16:10.754620 140089552271104 logging_writer.py:48] [416300] global_step=416300, grad_norm=4.463036060333252, loss=0.6961376667022705
I0301 21:16:44.202280 140089770305280 logging_writer.py:48] [416400] global_step=416400, grad_norm=4.329256057739258, loss=0.514925479888916
I0301 21:17:17.680704 140089552271104 logging_writer.py:48] [416500] global_step=416500, grad_norm=4.665633201599121, loss=0.6913631558418274
I0301 21:17:51.145624 140089770305280 logging_writer.py:48] [416600] global_step=416600, grad_norm=4.361579895019531, loss=0.6982792615890503
I0301 21:18:24.609047 140089552271104 logging_writer.py:48] [416700] global_step=416700, grad_norm=4.444125175476074, loss=0.5691373348236084
I0301 21:18:58.052758 140089770305280 logging_writer.py:48] [416800] global_step=416800, grad_norm=4.086721420288086, loss=0.5400710701942444
I0301 21:19:31.505740 140089552271104 logging_writer.py:48] [416900] global_step=416900, grad_norm=4.409653186798096, loss=0.6318919062614441
I0301 21:20:04.932827 140089770305280 logging_writer.py:48] [417000] global_step=417000, grad_norm=4.139643669128418, loss=0.5634303092956543
I0301 21:20:38.391267 140089552271104 logging_writer.py:48] [417100] global_step=417100, grad_norm=4.549612522125244, loss=0.6440507769584656
I0301 21:21:11.804094 140089770305280 logging_writer.py:48] [417200] global_step=417200, grad_norm=4.221089839935303, loss=0.6046730875968933
I0301 21:21:45.339543 140089552271104 logging_writer.py:48] [417300] global_step=417300, grad_norm=5.384428024291992, loss=0.6165913939476013
I0301 21:22:18.797592 140089770305280 logging_writer.py:48] [417400] global_step=417400, grad_norm=4.544858932495117, loss=0.6385130882263184
I0301 21:22:52.278577 140089552271104 logging_writer.py:48] [417500] global_step=417500, grad_norm=5.023409366607666, loss=0.6067415475845337
I0301 21:23:25.714392 140089770305280 logging_writer.py:48] [417600] global_step=417600, grad_norm=4.508249282836914, loss=0.6777239441871643
I0301 21:23:50.266790 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:23:56.329612 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:24:04.671022 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:24:06.945656 140252611495744 submission_runner.py:411] Time since start: 144587.83s, 	Step: 417675, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.14534547924995422, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.053350567817688, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8424909114837646, 'test/num_examples': 10000, 'score': 139804.53283405304, 'total_duration': 144587.82571840286, 'accumulated_submission_time': 139804.53283405304, 'accumulated_eval_time': 4750.356118917465, 'accumulated_logging_time': 17.932732820510864}
I0301 21:24:07.037981 140089854211840 logging_writer.py:48] [417675] accumulated_eval_time=4750.356119, accumulated_logging_time=17.932733, accumulated_submission_time=139804.532834, global_step=417675, preemption_count=0, score=139804.532834, test/accuracy=0.627700, test/loss=1.842491, test/num_examples=10000, total_duration=144587.825718, train/accuracy=0.961177, train/loss=0.145345, validation/accuracy=0.756040, validation/loss=1.053351, validation/num_examples=50000
I0301 21:24:15.722353 140089862604544 logging_writer.py:48] [417700] global_step=417700, grad_norm=4.4343037605285645, loss=0.6688932180404663
I0301 21:24:49.157784 140089854211840 logging_writer.py:48] [417800] global_step=417800, grad_norm=4.618514060974121, loss=0.6085498332977295
I0301 21:25:22.641905 140089862604544 logging_writer.py:48] [417900] global_step=417900, grad_norm=4.241510391235352, loss=0.5591084957122803
I0301 21:25:56.100011 140089854211840 logging_writer.py:48] [418000] global_step=418000, grad_norm=5.396664619445801, loss=0.6157758235931396
I0301 21:26:29.563958 140089862604544 logging_writer.py:48] [418100] global_step=418100, grad_norm=4.835492134094238, loss=0.6506850719451904
I0301 21:27:03.032994 140089854211840 logging_writer.py:48] [418200] global_step=418200, grad_norm=4.3821821212768555, loss=0.5749379396438599
I0301 21:27:36.483281 140089862604544 logging_writer.py:48] [418300] global_step=418300, grad_norm=4.351173400878906, loss=0.6089564561843872
I0301 21:28:10.034907 140089854211840 logging_writer.py:48] [418400] global_step=418400, grad_norm=4.69249963760376, loss=0.6690456867218018
I0301 21:28:43.509922 140089862604544 logging_writer.py:48] [418500] global_step=418500, grad_norm=5.1010823249816895, loss=0.6913465261459351
I0301 21:29:16.979840 140089854211840 logging_writer.py:48] [418600] global_step=418600, grad_norm=4.632394790649414, loss=0.5995457172393799
I0301 21:29:50.506390 140089862604544 logging_writer.py:48] [418700] global_step=418700, grad_norm=4.724162578582764, loss=0.5685449838638306
I0301 21:30:23.987737 140089854211840 logging_writer.py:48] [418800] global_step=418800, grad_norm=4.5268635749816895, loss=0.679115891456604
I0301 21:30:57.419171 140089862604544 logging_writer.py:48] [418900] global_step=418900, grad_norm=4.731163024902344, loss=0.6007518768310547
I0301 21:31:30.869458 140089854211840 logging_writer.py:48] [419000] global_step=419000, grad_norm=4.548099040985107, loss=0.5759153962135315
I0301 21:32:04.300106 140089862604544 logging_writer.py:48] [419100] global_step=419100, grad_norm=4.93717622756958, loss=0.6654258966445923
I0301 21:32:37.229497 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:32:43.363494 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:32:51.442043 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:32:53.847005 140252611495744 submission_runner.py:411] Time since start: 145114.73s, 	Step: 419200, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14846687018871307, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.053810477256775, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8426084518432617, 'test/num_examples': 10000, 'score': 140314.65873599052, 'total_duration': 145114.72708678246, 'accumulated_submission_time': 140314.65873599052, 'accumulated_eval_time': 4766.973588705063, 'accumulated_logging_time': 18.03618574142456}
I0301 21:32:53.915426 140089778697984 logging_writer.py:48] [419200] accumulated_eval_time=4766.973589, accumulated_logging_time=18.036186, accumulated_submission_time=140314.658736, global_step=419200, preemption_count=0, score=140314.658736, test/accuracy=0.627700, test/loss=1.842608, test/num_examples=10000, total_duration=145114.727087, train/accuracy=0.961017, train/loss=0.148467, validation/accuracy=0.755680, validation/loss=1.053810, validation/num_examples=50000
I0301 21:32:54.260192 140089837426432 logging_writer.py:48] [419200] global_step=419200, grad_norm=4.5434417724609375, loss=0.6167116165161133
I0301 21:33:27.712041 140089778697984 logging_writer.py:48] [419300] global_step=419300, grad_norm=4.924827575683594, loss=0.6319540143013
I0301 21:34:01.262526 140089837426432 logging_writer.py:48] [419400] global_step=419400, grad_norm=4.47955846786499, loss=0.6762294769287109
I0301 21:34:34.732640 140089778697984 logging_writer.py:48] [419500] global_step=419500, grad_norm=4.809747219085693, loss=0.6333758234977722
I0301 21:35:08.201209 140089837426432 logging_writer.py:48] [419600] global_step=419600, grad_norm=4.213839530944824, loss=0.6081838607788086
I0301 21:35:41.651026 140089778697984 logging_writer.py:48] [419700] global_step=419700, grad_norm=4.292684078216553, loss=0.6011925339698792
I0301 21:36:15.117770 140089837426432 logging_writer.py:48] [419800] global_step=419800, grad_norm=4.583832263946533, loss=0.6428998112678528
I0301 21:36:48.614509 140089778697984 logging_writer.py:48] [419900] global_step=419900, grad_norm=4.648753643035889, loss=0.6515495777130127
I0301 21:37:22.079380 140089837426432 logging_writer.py:48] [420000] global_step=420000, grad_norm=4.579339027404785, loss=0.6232910752296448
I0301 21:37:55.527559 140089778697984 logging_writer.py:48] [420100] global_step=420100, grad_norm=4.6049065589904785, loss=0.6393543481826782
I0301 21:38:28.982925 140089837426432 logging_writer.py:48] [420200] global_step=420200, grad_norm=4.921885967254639, loss=0.6597762703895569
I0301 21:39:02.442058 140089778697984 logging_writer.py:48] [420300] global_step=420300, grad_norm=4.287955284118652, loss=0.590051531791687
I0301 21:39:35.907836 140089837426432 logging_writer.py:48] [420400] global_step=420400, grad_norm=5.035536289215088, loss=0.7091448307037354
I0301 21:40:09.447989 140089778697984 logging_writer.py:48] [420500] global_step=420500, grad_norm=4.8891777992248535, loss=0.5913262367248535
I0301 21:40:42.907064 140089837426432 logging_writer.py:48] [420600] global_step=420600, grad_norm=4.310731887817383, loss=0.6266721487045288
I0301 21:41:16.348956 140089778697984 logging_writer.py:48] [420700] global_step=420700, grad_norm=4.778526306152344, loss=0.6660125851631165
I0301 21:41:23.849840 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:41:30.080524 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:41:38.082350 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:41:40.366904 140252611495744 submission_runner.py:411] Time since start: 145641.25s, 	Step: 420724, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14693209528923035, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0542471408843994, 'validation/num_examples': 50000, 'test/accuracy': 0.626300036907196, 'test/loss': 1.8435076475143433, 'test/num_examples': 10000, 'score': 140824.52908945084, 'total_duration': 145641.24696421623, 'accumulated_submission_time': 140824.52908945084, 'accumulated_eval_time': 4783.490590810776, 'accumulated_logging_time': 18.113587141036987}
I0301 21:41:40.453826 140089778697984 logging_writer.py:48] [420724] accumulated_eval_time=4783.490591, accumulated_logging_time=18.113587, accumulated_submission_time=140824.529089, global_step=420724, preemption_count=0, score=140824.529089, test/accuracy=0.626300, test/loss=1.843508, test/num_examples=10000, total_duration=145641.246964, train/accuracy=0.960658, train/loss=0.146932, validation/accuracy=0.756040, validation/loss=1.054247, validation/num_examples=50000
I0301 21:42:06.225440 140089837426432 logging_writer.py:48] [420800] global_step=420800, grad_norm=4.519266605377197, loss=0.6425535082817078
I0301 21:42:39.662870 140089778697984 logging_writer.py:48] [420900] global_step=420900, grad_norm=4.515925884246826, loss=0.6449353694915771
I0301 21:43:13.123426 140089837426432 logging_writer.py:48] [421000] global_step=421000, grad_norm=4.971899032592773, loss=0.6557172536849976
I0301 21:43:46.598083 140089778697984 logging_writer.py:48] [421100] global_step=421100, grad_norm=4.518950462341309, loss=0.6056070923805237
I0301 21:44:20.037789 140089837426432 logging_writer.py:48] [421200] global_step=421200, grad_norm=3.960303544998169, loss=0.5614914298057556
I0301 21:44:53.521779 140089778697984 logging_writer.py:48] [421300] global_step=421300, grad_norm=4.413005828857422, loss=0.6184278726577759
I0301 21:45:26.993468 140089837426432 logging_writer.py:48] [421400] global_step=421400, grad_norm=4.531271457672119, loss=0.5874444246292114
I0301 21:46:00.499223 140089778697984 logging_writer.py:48] [421500] global_step=421500, grad_norm=4.5826311111450195, loss=0.5945538282394409
I0301 21:46:33.941621 140089837426432 logging_writer.py:48] [421600] global_step=421600, grad_norm=4.333893775939941, loss=0.5773502588272095
I0301 21:47:07.424313 140089778697984 logging_writer.py:48] [421700] global_step=421700, grad_norm=4.4696807861328125, loss=0.6144266724586487
I0301 21:47:40.884543 140089837426432 logging_writer.py:48] [421800] global_step=421800, grad_norm=4.592807292938232, loss=0.6339378952980042
I0301 21:48:14.325685 140089778697984 logging_writer.py:48] [421900] global_step=421900, grad_norm=4.8649420738220215, loss=0.6493830680847168
I0301 21:48:47.798068 140089837426432 logging_writer.py:48] [422000] global_step=422000, grad_norm=4.846174716949463, loss=0.618867814540863
I0301 21:49:21.231766 140089778697984 logging_writer.py:48] [422100] global_step=422100, grad_norm=5.116276741027832, loss=0.7263045907020569
I0301 21:49:54.712389 140089837426432 logging_writer.py:48] [422200] global_step=422200, grad_norm=4.26198148727417, loss=0.6359210014343262
I0301 21:50:10.582447 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:50:16.835324 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:50:24.958835 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:50:27.256257 140252611495744 submission_runner.py:411] Time since start: 146168.14s, 	Step: 422249, 	{'train/accuracy': 0.9617944359779358, 'train/loss': 0.14492955803871155, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0536588430404663, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8419170379638672, 'test/num_examples': 10000, 'score': 141334.59196543694, 'total_duration': 146168.13632249832, 'accumulated_submission_time': 141334.59196543694, 'accumulated_eval_time': 4800.164356708527, 'accumulated_logging_time': 18.211303234100342}
I0301 21:50:27.345351 140089770305280 logging_writer.py:48] [422249] accumulated_eval_time=4800.164357, accumulated_logging_time=18.211303, accumulated_submission_time=141334.591965, global_step=422249, preemption_count=0, score=141334.591965, test/accuracy=0.627900, test/loss=1.841917, test/num_examples=10000, total_duration=146168.136322, train/accuracy=0.961794, train/loss=0.144930, validation/accuracy=0.755800, validation/loss=1.053659, validation/num_examples=50000
I0301 21:50:44.748556 140089778697984 logging_writer.py:48] [422300] global_step=422300, grad_norm=4.569168567657471, loss=0.6132636070251465
I0301 21:51:18.184930 140089770305280 logging_writer.py:48] [422400] global_step=422400, grad_norm=4.774272918701172, loss=0.6666523814201355
I0301 21:51:51.610788 140089778697984 logging_writer.py:48] [422500] global_step=422500, grad_norm=4.6051249504089355, loss=0.5778459310531616
I0301 21:52:25.130274 140089770305280 logging_writer.py:48] [422600] global_step=422600, grad_norm=4.417307376861572, loss=0.5576592683792114
I0301 21:52:58.576361 140089778697984 logging_writer.py:48] [422700] global_step=422700, grad_norm=4.670620918273926, loss=0.6207553744316101
I0301 21:53:32.024797 140089770305280 logging_writer.py:48] [422800] global_step=422800, grad_norm=4.082230091094971, loss=0.5902490615844727
I0301 21:54:05.451494 140089778697984 logging_writer.py:48] [422900] global_step=422900, grad_norm=4.413762092590332, loss=0.5947778820991516
I0301 21:54:38.931030 140089770305280 logging_writer.py:48] [423000] global_step=423000, grad_norm=4.555089473724365, loss=0.6297260522842407
I0301 21:55:12.390182 140089778697984 logging_writer.py:48] [423100] global_step=423100, grad_norm=4.807081699371338, loss=0.6168282628059387
I0301 21:55:45.827428 140089770305280 logging_writer.py:48] [423200] global_step=423200, grad_norm=4.565701961517334, loss=0.7201558947563171
I0301 21:56:19.286741 140089778697984 logging_writer.py:48] [423300] global_step=423300, grad_norm=4.673571586608887, loss=0.6312187314033508
I0301 21:56:52.725341 140089770305280 logging_writer.py:48] [423400] global_step=423400, grad_norm=4.432917594909668, loss=0.6040180325508118
I0301 21:57:26.180220 140089778697984 logging_writer.py:48] [423500] global_step=423500, grad_norm=4.823018550872803, loss=0.6624049544334412
I0301 21:57:59.628027 140089770305280 logging_writer.py:48] [423600] global_step=423600, grad_norm=4.879500389099121, loss=0.7743120193481445
I0301 21:58:33.156564 140089778697984 logging_writer.py:48] [423700] global_step=423700, grad_norm=4.778135299682617, loss=0.6362082958221436
I0301 21:58:57.389796 140252611495744 spec.py:321] Evaluating on the training split.
I0301 21:59:03.803296 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 21:59:11.806503 140252611495744 spec.py:349] Evaluating on the test split.
I0301 21:59:14.100726 140252611495744 submission_runner.py:411] Time since start: 146694.98s, 	Step: 423774, 	{'train/accuracy': 0.9623525142669678, 'train/loss': 0.14280004799365997, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0537769794464111, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8425239324569702, 'test/num_examples': 10000, 'score': 141844.57013821602, 'total_duration': 146694.98079276085, 'accumulated_submission_time': 141844.57013821602, 'accumulated_eval_time': 4816.875243186951, 'accumulated_logging_time': 18.31092357635498}
I0301 21:59:14.191479 140089854211840 logging_writer.py:48] [423774] accumulated_eval_time=4816.875243, accumulated_logging_time=18.310924, accumulated_submission_time=141844.570138, global_step=423774, preemption_count=0, score=141844.570138, test/accuracy=0.628000, test/loss=1.842524, test/num_examples=10000, total_duration=146694.980793, train/accuracy=0.962353, train/loss=0.142800, validation/accuracy=0.755760, validation/loss=1.053777, validation/num_examples=50000
I0301 21:59:23.227871 140089862604544 logging_writer.py:48] [423800] global_step=423800, grad_norm=4.357247829437256, loss=0.6293610334396362
I0301 21:59:56.716917 140089854211840 logging_writer.py:48] [423900] global_step=423900, grad_norm=4.501498699188232, loss=0.6348668336868286
I0301 22:00:30.132380 140089862604544 logging_writer.py:48] [424000] global_step=424000, grad_norm=4.678393840789795, loss=0.6164177060127258
I0301 22:01:03.598459 140089854211840 logging_writer.py:48] [424100] global_step=424100, grad_norm=4.509310722351074, loss=0.6317676901817322
I0301 22:01:37.024197 140089862604544 logging_writer.py:48] [424200] global_step=424200, grad_norm=4.518098831176758, loss=0.6363465785980225
I0301 22:02:10.485637 140089854211840 logging_writer.py:48] [424300] global_step=424300, grad_norm=4.347303867340088, loss=0.6097832322120667
I0301 22:02:43.949157 140089862604544 logging_writer.py:48] [424400] global_step=424400, grad_norm=4.687145233154297, loss=0.6155889630317688
I0301 22:03:17.490796 140089854211840 logging_writer.py:48] [424500] global_step=424500, grad_norm=4.69244909286499, loss=0.6002392768859863
I0301 22:03:50.930192 140089862604544 logging_writer.py:48] [424600] global_step=424600, grad_norm=4.497910499572754, loss=0.623760461807251
I0301 22:04:24.421786 140089854211840 logging_writer.py:48] [424700] global_step=424700, grad_norm=4.208146572113037, loss=0.6372811198234558
I0301 22:04:57.889567 140089862604544 logging_writer.py:48] [424800] global_step=424800, grad_norm=4.418575763702393, loss=0.7111988663673401
I0301 22:05:31.367063 140089854211840 logging_writer.py:48] [424900] global_step=424900, grad_norm=4.744466781616211, loss=0.5708199143409729
I0301 22:06:04.811066 140089862604544 logging_writer.py:48] [425000] global_step=425000, grad_norm=4.687252044677734, loss=0.6613601446151733
I0301 22:06:38.240181 140089854211840 logging_writer.py:48] [425100] global_step=425100, grad_norm=4.383710861206055, loss=0.6220290660858154
I0301 22:07:11.717935 140089862604544 logging_writer.py:48] [425200] global_step=425200, grad_norm=4.4448018074035645, loss=0.6164442300796509
I0301 22:07:44.335450 140252611495744 spec.py:321] Evaluating on the training split.
I0301 22:07:50.461884 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 22:07:58.599464 140252611495744 spec.py:349] Evaluating on the test split.
I0301 22:08:00.906769 140252611495744 submission_runner.py:411] Time since start: 147221.79s, 	Step: 425299, 	{'train/accuracy': 0.9629504084587097, 'train/loss': 0.14256997406482697, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.053001046180725, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8420350551605225, 'test/num_examples': 10000, 'score': 142354.64644885063, 'total_duration': 147221.78683519363, 'accumulated_submission_time': 142354.64644885063, 'accumulated_eval_time': 4833.446505069733, 'accumulated_logging_time': 18.412598848342896}
I0301 22:08:00.997255 140089837426432 logging_writer.py:48] [425299] accumulated_eval_time=4833.446505, accumulated_logging_time=18.412599, accumulated_submission_time=142354.646449, global_step=425299, preemption_count=0, score=142354.646449, test/accuracy=0.627100, test/loss=1.842035, test/num_examples=10000, total_duration=147221.786835, train/accuracy=0.962950, train/loss=0.142570, validation/accuracy=0.756080, validation/loss=1.053001, validation/num_examples=50000
I0301 22:08:01.671852 140089845819136 logging_writer.py:48] [425300] global_step=425300, grad_norm=4.368305683135986, loss=0.6631397008895874
I0301 22:08:35.157930 140089837426432 logging_writer.py:48] [425400] global_step=425400, grad_norm=4.0018415451049805, loss=0.5838670134544373
I0301 22:09:08.588827 140089845819136 logging_writer.py:48] [425500] global_step=425500, grad_norm=5.209071636199951, loss=0.6382113099098206
I0301 22:09:42.040636 140089837426432 logging_writer.py:48] [425600] global_step=425600, grad_norm=4.549322605133057, loss=0.6930461525917053
I0301 22:10:15.468290 140089845819136 logging_writer.py:48] [425700] global_step=425700, grad_norm=4.832230567932129, loss=0.6930228471755981
I0301 22:10:49.023385 140089837426432 logging_writer.py:48] [425800] global_step=425800, grad_norm=4.7430219650268555, loss=0.6273410320281982
I0301 22:11:22.486509 140089845819136 logging_writer.py:48] [425900] global_step=425900, grad_norm=4.551563262939453, loss=0.635793149471283
I0301 22:11:55.927684 140089837426432 logging_writer.py:48] [426000] global_step=426000, grad_norm=4.747054576873779, loss=0.5995245575904846
I0301 22:12:29.402110 140089845819136 logging_writer.py:48] [426100] global_step=426100, grad_norm=4.363053321838379, loss=0.5665105581283569
I0301 22:13:02.866505 140089837426432 logging_writer.py:48] [426200] global_step=426200, grad_norm=4.716281414031982, loss=0.6792013049125671
I0301 22:13:36.301431 140089845819136 logging_writer.py:48] [426300] global_step=426300, grad_norm=5.018750190734863, loss=0.6739332675933838
I0301 22:14:09.763211 140089837426432 logging_writer.py:48] [426400] global_step=426400, grad_norm=4.510941505432129, loss=0.596642255783081
I0301 22:14:43.190033 140089845819136 logging_writer.py:48] [426500] global_step=426500, grad_norm=4.633460521697998, loss=0.6731119155883789
I0301 22:15:16.637902 140089837426432 logging_writer.py:48] [426600] global_step=426600, grad_norm=4.612124919891357, loss=0.7157285809516907
I0301 22:15:50.088730 140089845819136 logging_writer.py:48] [426700] global_step=426700, grad_norm=4.662994384765625, loss=0.6586204171180725
I0301 22:16:23.600029 140089837426432 logging_writer.py:48] [426800] global_step=426800, grad_norm=4.696696758270264, loss=0.6090911030769348
I0301 22:16:31.082577 140252611495744 spec.py:321] Evaluating on the training split.
I0301 22:16:37.960382 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 22:16:46.146962 140252611495744 spec.py:349] Evaluating on the test split.
I0301 22:16:48.433983 140252611495744 submission_runner.py:411] Time since start: 147749.31s, 	Step: 426824, 	{'train/accuracy': 0.9593231678009033, 'train/loss': 0.1488415002822876, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0532394647598267, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.840661644935608, 'test/num_examples': 10000, 'score': 142864.66627311707, 'total_duration': 147749.3140487671, 'accumulated_submission_time': 142864.66627311707, 'accumulated_eval_time': 4850.79785990715, 'accumulated_logging_time': 18.513120651245117}
I0301 22:16:48.523971 140089854211840 logging_writer.py:48] [426824] accumulated_eval_time=4850.797860, accumulated_logging_time=18.513121, accumulated_submission_time=142864.666273, global_step=426824, preemption_count=0, score=142864.666273, test/accuracy=0.627500, test/loss=1.840662, test/num_examples=10000, total_duration=147749.314049, train/accuracy=0.959323, train/loss=0.148842, validation/accuracy=0.755880, validation/loss=1.053239, validation/num_examples=50000
I0301 22:17:14.268769 140089862604544 logging_writer.py:48] [426900] global_step=426900, grad_norm=4.772314548492432, loss=0.5666015148162842
I0301 22:17:47.755205 140089854211840 logging_writer.py:48] [427000] global_step=427000, grad_norm=4.4531683921813965, loss=0.581778883934021
I0301 22:18:21.187130 140089862604544 logging_writer.py:48] [427100] global_step=427100, grad_norm=4.43573522567749, loss=0.5831469297409058
I0301 22:18:54.664506 140089854211840 logging_writer.py:48] [427200] global_step=427200, grad_norm=4.262736797332764, loss=0.6072076559066772
I0301 22:19:28.124968 140089862604544 logging_writer.py:48] [427300] global_step=427300, grad_norm=4.598685264587402, loss=0.6500027179718018
I0301 22:20:01.575584 140089854211840 logging_writer.py:48] [427400] global_step=427400, grad_norm=4.160074710845947, loss=0.6089811325073242
I0301 22:20:35.022904 140089862604544 logging_writer.py:48] [427500] global_step=427500, grad_norm=4.753181457519531, loss=0.6145894527435303
I0301 22:21:08.529225 140089854211840 logging_writer.py:48] [427600] global_step=427600, grad_norm=4.5075459480285645, loss=0.5829843282699585
I0301 22:21:41.955938 140089862604544 logging_writer.py:48] [427700] global_step=427700, grad_norm=5.356960773468018, loss=0.6395338773727417
I0301 22:22:15.407901 140089854211840 logging_writer.py:48] [427800] global_step=427800, grad_norm=4.73988676071167, loss=0.6574441194534302
I0301 22:22:48.946096 140089862604544 logging_writer.py:48] [427900] global_step=427900, grad_norm=4.362417697906494, loss=0.6824818849563599
I0301 22:23:22.384596 140089854211840 logging_writer.py:48] [428000] global_step=428000, grad_norm=4.799338340759277, loss=0.6964421272277832
I0301 22:23:55.834295 140089862604544 logging_writer.py:48] [428100] global_step=428100, grad_norm=4.704367160797119, loss=0.6095327138900757
I0301 22:24:29.288297 140089854211840 logging_writer.py:48] [428200] global_step=428200, grad_norm=4.441246509552002, loss=0.530828058719635
I0301 22:25:02.712219 140089862604544 logging_writer.py:48] [428300] global_step=428300, grad_norm=5.1349945068359375, loss=0.6259112358093262
I0301 22:25:18.583610 140252611495744 spec.py:321] Evaluating on the training split.
I0301 22:25:24.943977 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 22:25:32.941182 140252611495744 spec.py:349] Evaluating on the test split.
I0301 22:25:35.232596 140252611495744 submission_runner.py:411] Time since start: 148276.11s, 	Step: 428349, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14543013274669647, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0528854131698608, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8414006233215332, 'test/num_examples': 10000, 'score': 143374.65701889992, 'total_duration': 148276.11263155937, 'accumulated_submission_time': 143374.65701889992, 'accumulated_eval_time': 4867.446770191193, 'accumulated_logging_time': 18.61627769470215}
I0301 22:25:35.321935 140089770305280 logging_writer.py:48] [428349] accumulated_eval_time=4867.446770, accumulated_logging_time=18.616278, accumulated_submission_time=143374.657019, global_step=428349, preemption_count=0, score=143374.657019, test/accuracy=0.627400, test/loss=1.841401, test/num_examples=10000, total_duration=148276.112632, train/accuracy=0.960678, train/loss=0.145430, validation/accuracy=0.755740, validation/loss=1.052885, validation/num_examples=50000
I0301 22:25:52.713126 140089778697984 logging_writer.py:48] [428400] global_step=428400, grad_norm=4.464855670928955, loss=0.6559587121009827
I0301 22:26:26.174856 140089770305280 logging_writer.py:48] [428500] global_step=428500, grad_norm=4.803275108337402, loss=0.7112047672271729
I0301 22:26:59.632597 140089778697984 logging_writer.py:48] [428600] global_step=428600, grad_norm=4.1895270347595215, loss=0.6248583793640137
I0301 22:27:33.382126 140089770305280 logging_writer.py:48] [428700] global_step=428700, grad_norm=4.59564208984375, loss=0.6884554028511047
I0301 22:28:06.852063 140089778697984 logging_writer.py:48] [428800] global_step=428800, grad_norm=4.575704097747803, loss=0.6319764256477356
I0301 22:28:40.362751 140089770305280 logging_writer.py:48] [428900] global_step=428900, grad_norm=4.967796802520752, loss=0.6633686423301697
I0301 22:29:13.796759 140089778697984 logging_writer.py:48] [429000] global_step=429000, grad_norm=4.736837387084961, loss=0.6038978099822998
I0301 22:29:47.246115 140089770305280 logging_writer.py:48] [429100] global_step=429100, grad_norm=4.597161769866943, loss=0.6233296394348145
I0301 22:30:20.748564 140089778697984 logging_writer.py:48] [429200] global_step=429200, grad_norm=4.767740726470947, loss=0.675184428691864
I0301 22:30:54.224108 140089770305280 logging_writer.py:48] [429300] global_step=429300, grad_norm=4.78867769241333, loss=0.6108819842338562
I0301 22:31:27.667373 140089778697984 logging_writer.py:48] [429400] global_step=429400, grad_norm=4.666061878204346, loss=0.6623174548149109
I0301 22:32:01.132886 140089770305280 logging_writer.py:48] [429500] global_step=429500, grad_norm=4.907286167144775, loss=0.711963415145874
I0301 22:32:34.586738 140089778697984 logging_writer.py:48] [429600] global_step=429600, grad_norm=4.555274963378906, loss=0.6321974992752075
I0301 22:33:08.036851 140089770305280 logging_writer.py:48] [429700] global_step=429700, grad_norm=4.2357258796691895, loss=0.6050426363945007
I0301 22:33:41.487637 140089778697984 logging_writer.py:48] [429800] global_step=429800, grad_norm=4.668816566467285, loss=0.6541659235954285
I0301 22:34:05.388618 140252611495744 spec.py:321] Evaluating on the training split.
I0301 22:34:11.567806 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 22:34:19.661610 140252611495744 spec.py:349] Evaluating on the test split.
I0301 22:34:21.939090 140252611495744 submission_runner.py:411] Time since start: 148802.82s, 	Step: 429873, 	{'train/accuracy': 0.9620535373687744, 'train/loss': 0.14405019581317902, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.053205966949463, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8428633213043213, 'test/num_examples': 10000, 'score': 143884.65881085396, 'total_duration': 148802.81915712357, 'accumulated_submission_time': 143884.65881085396, 'accumulated_eval_time': 4883.9972012043, 'accumulated_logging_time': 18.715560913085938}
I0301 22:34:22.023422 140089845819136 logging_writer.py:48] [429873] accumulated_eval_time=4883.997201, accumulated_logging_time=18.715561, accumulated_submission_time=143884.658811, global_step=429873, preemption_count=0, score=143884.658811, test/accuracy=0.627200, test/loss=1.842863, test/num_examples=10000, total_duration=148802.819157, train/accuracy=0.962054, train/loss=0.144050, validation/accuracy=0.756100, validation/loss=1.053206, validation/num_examples=50000
I0301 22:34:31.396901 140089854211840 logging_writer.py:48] [429900] global_step=429900, grad_norm=5.458669185638428, loss=0.7295656204223633
I0301 22:35:04.929267 140089845819136 logging_writer.py:48] [430000] global_step=430000, grad_norm=4.908866882324219, loss=0.6628286838531494
I0301 22:35:38.367497 140089854211840 logging_writer.py:48] [430100] global_step=430100, grad_norm=4.527543067932129, loss=0.5525715351104736
I0301 22:36:11.845265 140089845819136 logging_writer.py:48] [430200] global_step=430200, grad_norm=4.5826520919799805, loss=0.692441463470459
I0301 22:36:45.285410 140089854211840 logging_writer.py:48] [430300] global_step=430300, grad_norm=4.643392562866211, loss=0.6575416326522827
I0301 22:37:18.743545 140089845819136 logging_writer.py:48] [430400] global_step=430400, grad_norm=4.288617134094238, loss=0.6041891574859619
I0301 22:37:52.181871 140089854211840 logging_writer.py:48] [430500] global_step=430500, grad_norm=4.406376838684082, loss=0.5560291409492493
I0301 22:38:25.643751 140089845819136 logging_writer.py:48] [430600] global_step=430600, grad_norm=5.020113945007324, loss=0.6449331045150757
I0301 22:38:59.091123 140089854211840 logging_writer.py:48] [430700] global_step=430700, grad_norm=4.485681056976318, loss=0.589367151260376
I0301 22:39:32.542263 140089845819136 logging_writer.py:48] [430800] global_step=430800, grad_norm=4.3208513259887695, loss=0.5816892385482788
I0301 22:40:05.997757 140089854211840 logging_writer.py:48] [430900] global_step=430900, grad_norm=4.722883701324463, loss=0.6513155102729797
I0301 22:40:39.441581 140089845819136 logging_writer.py:48] [431000] global_step=431000, grad_norm=4.614528656005859, loss=0.6070890426635742
I0301 22:41:12.964991 140089854211840 logging_writer.py:48] [431100] global_step=431100, grad_norm=4.416632175445557, loss=0.598259687423706
I0301 22:41:46.428878 140089845819136 logging_writer.py:48] [431200] global_step=431200, grad_norm=4.799036979675293, loss=0.6008911728858948
I0301 22:42:19.884267 140089854211840 logging_writer.py:48] [431300] global_step=431300, grad_norm=5.112901210784912, loss=0.6647548079490662
I0301 22:42:52.129344 140252611495744 spec.py:321] Evaluating on the training split.
I0301 22:42:58.196748 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 22:43:06.482120 140252611495744 spec.py:349] Evaluating on the test split.
I0301 22:43:08.750860 140252611495744 submission_runner.py:411] Time since start: 149329.63s, 	Step: 431398, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.142555832862854, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0534591674804688, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8429311513900757, 'test/num_examples': 10000, 'score': 144394.69930911064, 'total_duration': 149329.63092327118, 'accumulated_submission_time': 144394.69930911064, 'accumulated_eval_time': 4900.618678569794, 'accumulated_logging_time': 18.810232400894165}
I0301 22:43:08.840610 140089500890880 logging_writer.py:48] [431398] accumulated_eval_time=4900.618679, accumulated_logging_time=18.810232, accumulated_submission_time=144394.699309, global_step=431398, preemption_count=0, score=144394.699309, test/accuracy=0.627300, test/loss=1.842931, test/num_examples=10000, total_duration=149329.630923, train/accuracy=0.961934, train/loss=0.142556, validation/accuracy=0.756140, validation/loss=1.053459, validation/num_examples=50000
I0301 22:43:09.857076 140089770305280 logging_writer.py:48] [431400] global_step=431400, grad_norm=4.052428722381592, loss=0.5624218583106995
I0301 22:43:43.313092 140089500890880 logging_writer.py:48] [431500] global_step=431500, grad_norm=4.554420471191406, loss=0.6634652614593506
I0301 22:44:16.768125 140089770305280 logging_writer.py:48] [431600] global_step=431600, grad_norm=4.177720546722412, loss=0.6095137000083923
I0301 22:44:50.211296 140089500890880 logging_writer.py:48] [431700] global_step=431700, grad_norm=4.295172214508057, loss=0.6048869490623474
I0301 22:45:23.672133 140089770305280 logging_writer.py:48] [431800] global_step=431800, grad_norm=4.371417045593262, loss=0.6056513786315918
I0301 22:45:57.116387 140089500890880 logging_writer.py:48] [431900] global_step=431900, grad_norm=5.258426666259766, loss=0.6188603639602661
I0301 22:46:30.558048 140089770305280 logging_writer.py:48] [432000] global_step=432000, grad_norm=4.262074947357178, loss=0.5550573468208313
I0301 22:47:04.113248 140089500890880 logging_writer.py:48] [432100] global_step=432100, grad_norm=4.4931206703186035, loss=0.6601651906967163
I0301 22:47:37.580862 140089770305280 logging_writer.py:48] [432200] global_step=432200, grad_norm=5.299440383911133, loss=0.6678972244262695
I0301 22:48:11.073108 140089500890880 logging_writer.py:48] [432300] global_step=432300, grad_norm=4.227120876312256, loss=0.6025595664978027
I0301 22:48:44.504829 140089770305280 logging_writer.py:48] [432400] global_step=432400, grad_norm=5.016288757324219, loss=0.6717731952667236
I0301 22:49:17.985862 140089500890880 logging_writer.py:48] [432500] global_step=432500, grad_norm=4.915953636169434, loss=0.6744389533996582
I0301 22:49:51.434335 140089770305280 logging_writer.py:48] [432600] global_step=432600, grad_norm=4.660285472869873, loss=0.800512969493866
I0301 22:50:24.881596 140089500890880 logging_writer.py:48] [432700] global_step=432700, grad_norm=5.094257831573486, loss=0.6935960650444031
I0301 22:50:58.329436 140089770305280 logging_writer.py:48] [432800] global_step=432800, grad_norm=4.708499431610107, loss=0.6090778112411499
I0301 22:51:31.790042 140089500890880 logging_writer.py:48] [432900] global_step=432900, grad_norm=4.694357872009277, loss=0.6871396899223328
I0301 22:51:38.973118 140252611495744 spec.py:321] Evaluating on the training split.
I0301 22:51:45.099040 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 22:51:53.177810 140252611495744 spec.py:349] Evaluating on the test split.
I0301 22:51:55.439107 140252611495744 submission_runner.py:411] Time since start: 149856.32s, 	Step: 432923, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14346066117286682, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0536073446273804, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.842706322669983, 'test/num_examples': 10000, 'score': 144904.76746439934, 'total_duration': 149856.3191523552, 'accumulated_submission_time': 144904.76746439934, 'accumulated_eval_time': 4917.084590911865, 'accumulated_logging_time': 18.910045385360718}
I0301 22:51:55.527891 140089854211840 logging_writer.py:48] [432923] accumulated_eval_time=4917.084591, accumulated_logging_time=18.910045, accumulated_submission_time=144904.767464, global_step=432923, preemption_count=0, score=144904.767464, test/accuracy=0.627200, test/loss=1.842706, test/num_examples=10000, total_duration=149856.319152, train/accuracy=0.961356, train/loss=0.143461, validation/accuracy=0.755900, validation/loss=1.053607, validation/num_examples=50000
I0301 22:52:21.621122 140089862604544 logging_writer.py:48] [433000] global_step=433000, grad_norm=4.890657424926758, loss=0.6039786338806152
I0301 22:52:55.058190 140089854211840 logging_writer.py:48] [433100] global_step=433100, grad_norm=5.049357891082764, loss=0.733369767665863
I0301 22:53:28.588502 140089862604544 logging_writer.py:48] [433200] global_step=433200, grad_norm=4.561739921569824, loss=0.6519762277603149
I0301 22:54:02.050257 140089854211840 logging_writer.py:48] [433300] global_step=433300, grad_norm=4.61823844909668, loss=0.6592960357666016
I0301 22:54:35.509806 140089862604544 logging_writer.py:48] [433400] global_step=433400, grad_norm=4.625616073608398, loss=0.6290546655654907
I0301 22:55:08.999861 140089854211840 logging_writer.py:48] [433500] global_step=433500, grad_norm=4.881139278411865, loss=0.6636810302734375
I0301 22:55:42.511883 140089862604544 logging_writer.py:48] [433600] global_step=433600, grad_norm=4.437219619750977, loss=0.6109736561775208
I0301 22:56:15.945991 140089854211840 logging_writer.py:48] [433700] global_step=433700, grad_norm=4.796176433563232, loss=0.6833524107933044
I0301 22:56:49.419214 140089862604544 logging_writer.py:48] [433800] global_step=433800, grad_norm=4.743832588195801, loss=0.6595912575721741
I0301 22:57:22.885756 140089854211840 logging_writer.py:48] [433900] global_step=433900, grad_norm=5.035618305206299, loss=0.6124138832092285
I0301 22:57:56.334518 140089862604544 logging_writer.py:48] [434000] global_step=434000, grad_norm=4.483564853668213, loss=0.5850696563720703
I0301 22:58:29.800624 140089854211840 logging_writer.py:48] [434100] global_step=434100, grad_norm=4.451104164123535, loss=0.6454319953918457
I0301 22:59:03.256750 140089862604544 logging_writer.py:48] [434200] global_step=434200, grad_norm=5.054593563079834, loss=0.627714991569519
I0301 22:59:36.816255 140089854211840 logging_writer.py:48] [434300] global_step=434300, grad_norm=4.783230304718018, loss=0.6356328725814819
I0301 23:00:10.338162 140089862604544 logging_writer.py:48] [434400] global_step=434400, grad_norm=4.323439598083496, loss=0.581229031085968
I0301 23:00:25.517671 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:00:31.689129 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:00:39.869367 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:00:42.108869 140252611495744 submission_runner.py:411] Time since start: 150382.99s, 	Step: 434447, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14721320569515228, 'validation/accuracy': 0.7562199831008911, 'validation/loss': 1.0545414686203003, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8429332971572876, 'test/num_examples': 10000, 'score': 145414.69251775742, 'total_duration': 150382.98892378807, 'accumulated_submission_time': 145414.69251775742, 'accumulated_eval_time': 4933.675719738007, 'accumulated_logging_time': 19.008899688720703}
I0301 23:00:42.200188 140089837426432 logging_writer.py:48] [434447] accumulated_eval_time=4933.675720, accumulated_logging_time=19.008900, accumulated_submission_time=145414.692518, global_step=434447, preemption_count=0, score=145414.692518, test/accuracy=0.627100, test/loss=1.842933, test/num_examples=10000, total_duration=150382.988924, train/accuracy=0.961097, train/loss=0.147213, validation/accuracy=0.756220, validation/loss=1.054541, validation/num_examples=50000
I0301 23:01:00.279516 140089845819136 logging_writer.py:48] [434500] global_step=434500, grad_norm=4.458122730255127, loss=0.5822287201881409
I0301 23:01:33.732869 140089837426432 logging_writer.py:48] [434600] global_step=434600, grad_norm=4.835874557495117, loss=0.6743879914283752
I0301 23:02:07.173180 140089845819136 logging_writer.py:48] [434700] global_step=434700, grad_norm=4.387824058532715, loss=0.5667411088943481
I0301 23:02:40.623740 140089837426432 logging_writer.py:48] [434800] global_step=434800, grad_norm=4.236139297485352, loss=0.5529745817184448
I0301 23:03:14.068912 140089845819136 logging_writer.py:48] [434900] global_step=434900, grad_norm=4.874669075012207, loss=0.6518957614898682
I0301 23:03:47.525659 140089837426432 logging_writer.py:48] [435000] global_step=435000, grad_norm=4.660739898681641, loss=0.5987502336502075
I0301 23:04:20.954125 140089845819136 logging_writer.py:48] [435100] global_step=435100, grad_norm=4.493224143981934, loss=0.5415147542953491
I0301 23:04:54.417409 140089837426432 logging_writer.py:48] [435200] global_step=435200, grad_norm=4.771661281585693, loss=0.582027792930603
I0301 23:05:27.954640 140089845819136 logging_writer.py:48] [435300] global_step=435300, grad_norm=4.513713836669922, loss=0.6274629235267639
I0301 23:06:01.410521 140089837426432 logging_writer.py:48] [435400] global_step=435400, grad_norm=5.147727966308594, loss=0.6361610889434814
I0301 23:06:34.853488 140089845819136 logging_writer.py:48] [435500] global_step=435500, grad_norm=4.670563697814941, loss=0.683383047580719
I0301 23:07:08.355549 140089837426432 logging_writer.py:48] [435600] global_step=435600, grad_norm=4.747830867767334, loss=0.6523061990737915
I0301 23:07:41.825643 140089845819136 logging_writer.py:48] [435700] global_step=435700, grad_norm=5.49041748046875, loss=0.5836694836616516
I0301 23:08:15.281858 140089837426432 logging_writer.py:48] [435800] global_step=435800, grad_norm=4.760045528411865, loss=0.6395621299743652
I0301 23:08:48.753708 140089845819136 logging_writer.py:48] [435900] global_step=435900, grad_norm=3.9728171825408936, loss=0.5466380715370178
I0301 23:09:12.368679 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:09:18.482185 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:09:26.672286 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:09:28.972731 140252611495744 submission_runner.py:411] Time since start: 150909.85s, 	Step: 435972, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14549028873443604, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.053587555885315, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8414922952651978, 'test/num_examples': 10000, 'score': 145924.79586172104, 'total_duration': 150909.85272932053, 'accumulated_submission_time': 145924.79586172104, 'accumulated_eval_time': 4950.279651641846, 'accumulated_logging_time': 19.110421895980835}
I0301 23:09:29.060097 140089854211840 logging_writer.py:48] [435972] accumulated_eval_time=4950.279652, accumulated_logging_time=19.110422, accumulated_submission_time=145924.795862, global_step=435972, preemption_count=0, score=145924.795862, test/accuracy=0.627500, test/loss=1.841492, test/num_examples=10000, total_duration=150909.852729, train/accuracy=0.960778, train/loss=0.145490, validation/accuracy=0.755840, validation/loss=1.053588, validation/num_examples=50000
I0301 23:09:38.760994 140089862604544 logging_writer.py:48] [436000] global_step=436000, grad_norm=4.899216651916504, loss=0.6906624436378479
I0301 23:10:12.215849 140089854211840 logging_writer.py:48] [436100] global_step=436100, grad_norm=4.635051250457764, loss=0.5870863795280457
I0301 23:10:45.644718 140089862604544 logging_writer.py:48] [436200] global_step=436200, grad_norm=5.036425590515137, loss=0.6402713060379028
I0301 23:11:19.088017 140089854211840 logging_writer.py:48] [436300] global_step=436300, grad_norm=4.161745548248291, loss=0.6037123799324036
I0301 23:11:52.637814 140089862604544 logging_writer.py:48] [436400] global_step=436400, grad_norm=4.741468906402588, loss=0.6051453351974487
I0301 23:12:26.084691 140089854211840 logging_writer.py:48] [436500] global_step=436500, grad_norm=4.215292930603027, loss=0.5589724183082581
I0301 23:12:59.539350 140089862604544 logging_writer.py:48] [436600] global_step=436600, grad_norm=4.5063862800598145, loss=0.5909722447395325
I0301 23:13:33.005759 140089854211840 logging_writer.py:48] [436700] global_step=436700, grad_norm=4.300680637359619, loss=0.6258894205093384
I0301 23:14:06.454087 140089862604544 logging_writer.py:48] [436800] global_step=436800, grad_norm=4.335789203643799, loss=0.5529976487159729
I0301 23:14:39.925798 140089854211840 logging_writer.py:48] [436900] global_step=436900, grad_norm=4.58860969543457, loss=0.6346558332443237
I0301 23:15:13.366267 140089862604544 logging_writer.py:48] [437000] global_step=437000, grad_norm=4.525514602661133, loss=0.6315884590148926
I0301 23:15:46.838858 140089854211840 logging_writer.py:48] [437100] global_step=437100, grad_norm=4.573469161987305, loss=0.6686581373214722
I0301 23:16:20.296853 140089862604544 logging_writer.py:48] [437200] global_step=437200, grad_norm=4.735235691070557, loss=0.6941291689872742
I0301 23:16:53.749985 140089854211840 logging_writer.py:48] [437300] global_step=437300, grad_norm=4.505201816558838, loss=0.5864047408103943
I0301 23:17:27.313258 140089862604544 logging_writer.py:48] [437400] global_step=437400, grad_norm=4.458677768707275, loss=0.5839488506317139
I0301 23:17:59.218755 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:18:05.498544 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:18:13.543677 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:18:15.858604 140252611495744 submission_runner.py:411] Time since start: 151436.74s, 	Step: 437497, 	{'train/accuracy': 0.9619937539100647, 'train/loss': 0.14326541125774384, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.053731918334961, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8432046175003052, 'test/num_examples': 10000, 'score': 146434.88862538338, 'total_duration': 151436.73866152763, 'accumulated_submission_time': 146434.88862538338, 'accumulated_eval_time': 4966.919440507889, 'accumulated_logging_time': 19.20820665359497}
I0301 23:18:15.951339 140089500890880 logging_writer.py:48] [437497] accumulated_eval_time=4966.919441, accumulated_logging_time=19.208207, accumulated_submission_time=146434.888625, global_step=437497, preemption_count=0, score=146434.888625, test/accuracy=0.627100, test/loss=1.843205, test/num_examples=10000, total_duration=151436.738662, train/accuracy=0.961994, train/loss=0.143265, validation/accuracy=0.756100, validation/loss=1.053732, validation/num_examples=50000
I0301 23:18:17.297513 140089770305280 logging_writer.py:48] [437500] global_step=437500, grad_norm=4.566316604614258, loss=0.5867048501968384
I0301 23:18:50.738226 140089500890880 logging_writer.py:48] [437600] global_step=437600, grad_norm=4.31115198135376, loss=0.6101133227348328
I0301 23:19:24.212180 140089770305280 logging_writer.py:48] [437700] global_step=437700, grad_norm=4.5425124168396, loss=0.6351498365402222
I0301 23:19:57.628869 140089500890880 logging_writer.py:48] [437800] global_step=437800, grad_norm=4.960583686828613, loss=0.6517592668533325
I0301 23:20:31.101816 140089770305280 logging_writer.py:48] [437900] global_step=437900, grad_norm=4.628011703491211, loss=0.630220890045166
I0301 23:21:04.585679 140089500890880 logging_writer.py:48] [438000] global_step=438000, grad_norm=4.584024906158447, loss=0.6043739318847656
I0301 23:21:38.014302 140089770305280 logging_writer.py:48] [438100] global_step=438100, grad_norm=4.820209503173828, loss=0.6894733905792236
I0301 23:22:11.480122 140089500890880 logging_writer.py:48] [438200] global_step=438200, grad_norm=4.739920139312744, loss=0.7020794153213501
I0301 23:22:44.953134 140089770305280 logging_writer.py:48] [438300] global_step=438300, grad_norm=4.52938985824585, loss=0.5663449168205261
I0301 23:23:18.380816 140089500890880 logging_writer.py:48] [438400] global_step=438400, grad_norm=4.498138904571533, loss=0.6048443913459778
I0301 23:23:51.907007 140089770305280 logging_writer.py:48] [438500] global_step=438500, grad_norm=4.631394386291504, loss=0.701956570148468
I0301 23:24:25.376702 140089500890880 logging_writer.py:48] [438600] global_step=438600, grad_norm=4.38944149017334, loss=0.6253023743629456
I0301 23:24:58.853702 140089770305280 logging_writer.py:48] [438700] global_step=438700, grad_norm=5.123816013336182, loss=0.6250225305557251
I0301 23:25:32.291810 140089500890880 logging_writer.py:48] [438800] global_step=438800, grad_norm=4.556077480316162, loss=0.6320363283157349
I0301 23:26:05.737138 140089770305280 logging_writer.py:48] [438900] global_step=438900, grad_norm=4.45685338973999, loss=0.6123825907707214
I0301 23:26:39.166072 140089500890880 logging_writer.py:48] [439000] global_step=439000, grad_norm=4.404314041137695, loss=0.6031297445297241
I0301 23:26:45.992445 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:26:52.105779 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:27:00.215495 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:27:02.534131 140252611495744 submission_runner.py:411] Time since start: 151963.41s, 	Step: 439022, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14424113929271698, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.053155779838562, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.8422584533691406, 'test/num_examples': 10000, 'score': 146944.86338567734, 'total_duration': 151963.4141998291, 'accumulated_submission_time': 146944.86338567734, 'accumulated_eval_time': 4983.461073637009, 'accumulated_logging_time': 19.31168293952942}
I0301 23:27:02.629711 140089862604544 logging_writer.py:48] [439022] accumulated_eval_time=4983.461074, accumulated_logging_time=19.311683, accumulated_submission_time=146944.863386, global_step=439022, preemption_count=0, score=146944.863386, test/accuracy=0.626600, test/loss=1.842258, test/num_examples=10000, total_duration=151963.414200, train/accuracy=0.961117, train/loss=0.144241, validation/accuracy=0.755860, validation/loss=1.053156, validation/num_examples=50000
I0301 23:27:29.075448 140089870997248 logging_writer.py:48] [439100] global_step=439100, grad_norm=4.65861701965332, loss=0.6321350932121277
I0301 23:28:02.537677 140089862604544 logging_writer.py:48] [439200] global_step=439200, grad_norm=4.42050313949585, loss=0.6407467722892761
I0301 23:28:36.001079 140089870997248 logging_writer.py:48] [439300] global_step=439300, grad_norm=4.340669631958008, loss=0.5895116925239563
I0301 23:29:09.502371 140089862604544 logging_writer.py:48] [439400] global_step=439400, grad_norm=4.511516571044922, loss=0.6337674260139465
I0301 23:29:43.005649 140089870997248 logging_writer.py:48] [439500] global_step=439500, grad_norm=4.10946798324585, loss=0.576636016368866
I0301 23:30:16.451085 140089862604544 logging_writer.py:48] [439600] global_step=439600, grad_norm=4.288457870483398, loss=0.5250102281570435
I0301 23:30:49.917532 140089870997248 logging_writer.py:48] [439700] global_step=439700, grad_norm=4.5771803855896, loss=0.6364454030990601
I0301 23:31:23.360226 140089862604544 logging_writer.py:48] [439800] global_step=439800, grad_norm=4.5649309158325195, loss=0.5954468250274658
I0301 23:31:56.806348 140089870997248 logging_writer.py:48] [439900] global_step=439900, grad_norm=4.503785610198975, loss=0.6058904528617859
I0301 23:32:30.272608 140089862604544 logging_writer.py:48] [440000] global_step=440000, grad_norm=4.69189977645874, loss=0.6126720309257507
I0301 23:33:03.716485 140089870997248 logging_writer.py:48] [440100] global_step=440100, grad_norm=4.890162467956543, loss=0.5719696283340454
I0301 23:33:37.177949 140089862604544 logging_writer.py:48] [440200] global_step=440200, grad_norm=4.09500789642334, loss=0.6010662317276001
I0301 23:34:10.627099 140089870997248 logging_writer.py:48] [440300] global_step=440300, grad_norm=4.257973670959473, loss=0.6363375782966614
I0301 23:34:44.080415 140089862604544 logging_writer.py:48] [440400] global_step=440400, grad_norm=5.11388635635376, loss=0.653931200504303
I0301 23:35:17.524028 140089870997248 logging_writer.py:48] [440500] global_step=440500, grad_norm=4.786348342895508, loss=0.6361535787582397
I0301 23:35:32.693631 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:35:39.192485 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:35:47.195178 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:35:49.470546 140252611495744 submission_runner.py:411] Time since start: 152490.35s, 	Step: 440547, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.14789773523807526, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.053455114364624, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8422164916992188, 'test/num_examples': 10000, 'score': 147454.86088490486, 'total_duration': 152490.3506128788, 'accumulated_submission_time': 147454.86088490486, 'accumulated_eval_time': 5000.237932682037, 'accumulated_logging_time': 19.418365716934204}
I0301 23:35:49.569851 140089770305280 logging_writer.py:48] [440547] accumulated_eval_time=5000.237933, accumulated_logging_time=19.418366, accumulated_submission_time=147454.860885, global_step=440547, preemption_count=0, score=147454.860885, test/accuracy=0.627100, test/loss=1.842216, test/num_examples=10000, total_duration=152490.350613, train/accuracy=0.959682, train/loss=0.147898, validation/accuracy=0.756080, validation/loss=1.053455, validation/num_examples=50000
I0301 23:36:07.617238 140089778697984 logging_writer.py:48] [440600] global_step=440600, grad_norm=4.836325645446777, loss=0.6127593517303467
I0301 23:36:41.084834 140089770305280 logging_writer.py:48] [440700] global_step=440700, grad_norm=4.358481407165527, loss=0.576045036315918
I0301 23:37:14.535743 140089778697984 logging_writer.py:48] [440800] global_step=440800, grad_norm=4.496216773986816, loss=0.624653697013855
I0301 23:37:47.974839 140089770305280 logging_writer.py:48] [440900] global_step=440900, grad_norm=4.394118309020996, loss=0.6199231743812561
I0301 23:38:21.451227 140089778697984 logging_writer.py:48] [441000] global_step=441000, grad_norm=4.199409484863281, loss=0.6060328483581543
I0301 23:38:54.871847 140089770305280 logging_writer.py:48] [441100] global_step=441100, grad_norm=4.199620246887207, loss=0.4846625030040741
I0301 23:39:28.321985 140089778697984 logging_writer.py:48] [441200] global_step=441200, grad_norm=4.677919387817383, loss=0.6489413976669312
I0301 23:40:01.726419 140089770305280 logging_writer.py:48] [441300] global_step=441300, grad_norm=4.312240123748779, loss=0.5844295024871826
I0301 23:40:35.186620 140089778697984 logging_writer.py:48] [441400] global_step=441400, grad_norm=4.576310157775879, loss=0.6145340204238892
I0301 23:41:08.608013 140089770305280 logging_writer.py:48] [441500] global_step=441500, grad_norm=4.816915988922119, loss=0.6310276985168457
I0301 23:41:42.042049 140089778697984 logging_writer.py:48] [441600] global_step=441600, grad_norm=4.273928165435791, loss=0.6055593490600586
I0301 23:42:15.572716 140089770305280 logging_writer.py:48] [441700] global_step=441700, grad_norm=4.7012128829956055, loss=0.5655369758605957
I0301 23:42:49.008934 140089778697984 logging_writer.py:48] [441800] global_step=441800, grad_norm=5.026739597320557, loss=0.6369803547859192
I0301 23:43:22.456658 140089770305280 logging_writer.py:48] [441900] global_step=441900, grad_norm=4.249093532562256, loss=0.6557342410087585
I0301 23:43:55.870704 140089778697984 logging_writer.py:48] [442000] global_step=442000, grad_norm=4.303952217102051, loss=0.6336525082588196
I0301 23:44:19.754048 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:44:25.847451 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:44:33.957209 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:44:36.230627 140252611495744 submission_runner.py:411] Time since start: 153017.11s, 	Step: 442073, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14587366580963135, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0526623725891113, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8417083024978638, 'test/num_examples': 10000, 'score': 147964.97935509682, 'total_duration': 153017.11069369316, 'accumulated_submission_time': 147964.97935509682, 'accumulated_eval_time': 5016.71445608139, 'accumulated_logging_time': 19.52862310409546}
I0301 23:44:36.321927 140089770305280 logging_writer.py:48] [442073] accumulated_eval_time=5016.714456, accumulated_logging_time=19.528623, accumulated_submission_time=147964.979355, global_step=442073, preemption_count=0, score=147964.979355, test/accuracy=0.627700, test/loss=1.841708, test/num_examples=10000, total_duration=153017.110694, train/accuracy=0.961017, train/loss=0.145874, validation/accuracy=0.755660, validation/loss=1.052662, validation/num_examples=50000
I0301 23:44:45.694255 140089778697984 logging_writer.py:48] [442100] global_step=442100, grad_norm=4.9945502281188965, loss=0.6430895328521729
I0301 23:45:19.125952 140089770305280 logging_writer.py:48] [442200] global_step=442200, grad_norm=4.507665634155273, loss=0.6562446355819702
I0301 23:45:52.562911 140089778697984 logging_writer.py:48] [442300] global_step=442300, grad_norm=4.451197147369385, loss=0.6255223751068115
I0301 23:46:25.996494 140089770305280 logging_writer.py:48] [442400] global_step=442400, grad_norm=4.902543067932129, loss=0.6402773857116699
I0301 23:46:59.428297 140089778697984 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.470300674438477, loss=0.6245520114898682
I0301 23:47:32.872792 140089770305280 logging_writer.py:48] [442600] global_step=442600, grad_norm=5.0846943855285645, loss=0.7056716084480286
I0301 23:48:06.367252 140089778697984 logging_writer.py:48] [442700] global_step=442700, grad_norm=4.712431907653809, loss=0.5930199027061462
I0301 23:48:39.830368 140089770305280 logging_writer.py:48] [442800] global_step=442800, grad_norm=4.637057304382324, loss=0.5927106738090515
I0301 23:49:13.290556 140089778697984 logging_writer.py:48] [442900] global_step=442900, grad_norm=4.288329601287842, loss=0.6341741681098938
I0301 23:49:46.726371 140089770305280 logging_writer.py:48] [443000] global_step=443000, grad_norm=4.601197242736816, loss=0.6511328220367432
I0301 23:50:20.174676 140089778697984 logging_writer.py:48] [443100] global_step=443100, grad_norm=4.41576623916626, loss=0.6044576168060303
I0301 23:50:53.597097 140089770305280 logging_writer.py:48] [443200] global_step=443200, grad_norm=4.351397514343262, loss=0.6305395364761353
I0301 23:51:27.056334 140089778697984 logging_writer.py:48] [443300] global_step=443300, grad_norm=4.8196587562561035, loss=0.6429097056388855
I0301 23:52:00.493403 140089770305280 logging_writer.py:48] [443400] global_step=443400, grad_norm=5.016655921936035, loss=0.658054769039154
I0301 23:52:33.926228 140089778697984 logging_writer.py:48] [443500] global_step=443500, grad_norm=4.531489849090576, loss=0.5905359387397766
I0301 23:53:06.499359 140252611495744 spec.py:321] Evaluating on the training split.
I0301 23:53:12.546757 140252611495744 spec.py:333] Evaluating on the validation split.
I0301 23:53:20.740083 140252611495744 spec.py:349] Evaluating on the test split.
I0301 23:53:23.057917 140252611495744 submission_runner.py:411] Time since start: 153543.94s, 	Step: 443599, 	{'train/accuracy': 0.962332546710968, 'train/loss': 0.1446838676929474, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.053775429725647, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8434412479400635, 'test/num_examples': 10000, 'score': 148475.09104943275, 'total_duration': 153543.9378838539, 'accumulated_submission_time': 148475.09104943275, 'accumulated_eval_time': 5033.272862672806, 'accumulated_logging_time': 19.630709886550903}
I0301 23:53:23.150890 140089854211840 logging_writer.py:48] [443599] accumulated_eval_time=5033.272863, accumulated_logging_time=19.630710, accumulated_submission_time=148475.091049, global_step=443599, preemption_count=0, score=148475.091049, test/accuracy=0.627200, test/loss=1.843441, test/num_examples=10000, total_duration=153543.937884, train/accuracy=0.962333, train/loss=0.144684, validation/accuracy=0.755780, validation/loss=1.053775, validation/num_examples=50000
I0301 23:53:23.830224 140089870997248 logging_writer.py:48] [443600] global_step=443600, grad_norm=5.009987831115723, loss=0.5746323466300964
I0301 23:53:57.373888 140089854211840 logging_writer.py:48] [443700] global_step=443700, grad_norm=4.961880207061768, loss=0.6028493642807007
I0301 23:54:30.838244 140089870997248 logging_writer.py:48] [443800] global_step=443800, grad_norm=4.431791305541992, loss=0.6238795518875122
I0301 23:55:04.268311 140089854211840 logging_writer.py:48] [443900] global_step=443900, grad_norm=4.467960834503174, loss=0.6233463287353516
I0301 23:55:37.738696 140089870997248 logging_writer.py:48] [444000] global_step=444000, grad_norm=4.962141990661621, loss=0.6411374807357788
I0301 23:56:11.198674 140089854211840 logging_writer.py:48] [444100] global_step=444100, grad_norm=4.348455429077148, loss=0.6324065327644348
I0301 23:56:44.652889 140089870997248 logging_writer.py:48] [444200] global_step=444200, grad_norm=4.475801467895508, loss=0.6099886894226074
I0301 23:57:18.092262 140089854211840 logging_writer.py:48] [444300] global_step=444300, grad_norm=5.049751281738281, loss=0.5286821126937866
I0301 23:57:51.531648 140089870997248 logging_writer.py:48] [444400] global_step=444400, grad_norm=4.5394134521484375, loss=0.6409876346588135
I0301 23:58:24.963794 140089854211840 logging_writer.py:48] [444500] global_step=444500, grad_norm=4.283498287200928, loss=0.5938803553581238
I0301 23:58:58.405042 140089870997248 logging_writer.py:48] [444600] global_step=444600, grad_norm=4.61803674697876, loss=0.6069537401199341
I0301 23:59:31.872955 140089854211840 logging_writer.py:48] [444700] global_step=444700, grad_norm=4.33976936340332, loss=0.5961517095565796
I0302 00:00:05.416200 140089870997248 logging_writer.py:48] [444800] global_step=444800, grad_norm=5.302619934082031, loss=0.678659200668335
I0302 00:00:38.850258 140089854211840 logging_writer.py:48] [444900] global_step=444900, grad_norm=4.125837802886963, loss=0.5784478187561035
I0302 00:01:12.328905 140089870997248 logging_writer.py:48] [445000] global_step=445000, grad_norm=4.842118740081787, loss=0.6138245463371277
I0302 00:01:45.756636 140089854211840 logging_writer.py:48] [445100] global_step=445100, grad_norm=4.634968280792236, loss=0.6571229696273804
I0302 00:01:53.271003 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:01:59.406229 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:02:07.612678 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:02:09.898498 140252611495744 submission_runner.py:411] Time since start: 154070.78s, 	Step: 445124, 	{'train/accuracy': 0.9621731042861938, 'train/loss': 0.14341571927070618, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.0535000562667847, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.84416663646698, 'test/num_examples': 10000, 'score': 148985.1442747116, 'total_duration': 154070.77855920792, 'accumulated_submission_time': 148985.1442747116, 'accumulated_eval_time': 5049.900293827057, 'accumulated_logging_time': 19.734724521636963}
I0302 00:02:09.984487 140089778697984 logging_writer.py:48] [445124] accumulated_eval_time=5049.900294, accumulated_logging_time=19.734725, accumulated_submission_time=148985.144275, global_step=445124, preemption_count=0, score=148985.144275, test/accuracy=0.626700, test/loss=1.844167, test/num_examples=10000, total_duration=154070.778559, train/accuracy=0.962173, train/loss=0.143416, validation/accuracy=0.756260, validation/loss=1.053500, validation/num_examples=50000
I0302 00:02:35.715442 140089837426432 logging_writer.py:48] [445200] global_step=445200, grad_norm=4.782098293304443, loss=0.5994906425476074
I0302 00:03:09.183260 140089778697984 logging_writer.py:48] [445300] global_step=445300, grad_norm=4.272616863250732, loss=0.6051335334777832
I0302 00:03:42.623005 140089837426432 logging_writer.py:48] [445400] global_step=445400, grad_norm=4.747055530548096, loss=0.6421799659729004
I0302 00:04:16.087900 140089778697984 logging_writer.py:48] [445500] global_step=445500, grad_norm=4.418449401855469, loss=0.630854606628418
I0302 00:04:49.537773 140089837426432 logging_writer.py:48] [445600] global_step=445600, grad_norm=4.150444507598877, loss=0.6437487602233887
I0302 00:05:22.993793 140089778697984 logging_writer.py:48] [445700] global_step=445700, grad_norm=4.345268249511719, loss=0.62152498960495
I0302 00:05:56.479270 140089837426432 logging_writer.py:48] [445800] global_step=445800, grad_norm=4.785755634307861, loss=0.6883983016014099
I0302 00:06:30.051835 140089778697984 logging_writer.py:48] [445900] global_step=445900, grad_norm=5.486298561096191, loss=0.6493385434150696
I0302 00:07:03.523165 140089837426432 logging_writer.py:48] [446000] global_step=446000, grad_norm=4.802461624145508, loss=0.6892485022544861
I0302 00:07:37.025638 140089778697984 logging_writer.py:48] [446100] global_step=446100, grad_norm=4.217276096343994, loss=0.5955947637557983
I0302 00:08:10.520107 140089837426432 logging_writer.py:48] [446200] global_step=446200, grad_norm=4.335389137268066, loss=0.6050362586975098
I0302 00:08:43.952091 140089778697984 logging_writer.py:48] [446300] global_step=446300, grad_norm=4.648524284362793, loss=0.7125093340873718
I0302 00:09:17.413206 140089837426432 logging_writer.py:48] [446400] global_step=446400, grad_norm=4.391452789306641, loss=0.6402149796485901
I0302 00:09:50.840706 140089778697984 logging_writer.py:48] [446500] global_step=446500, grad_norm=5.335606575012207, loss=0.6290960907936096
I0302 00:10:24.295518 140089837426432 logging_writer.py:48] [446600] global_step=446600, grad_norm=4.396144866943359, loss=0.5687832832336426
I0302 00:10:40.178541 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:10:46.267284 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:10:54.482545 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:10:56.787290 140252611495744 submission_runner.py:411] Time since start: 154597.67s, 	Step: 446649, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.1488574594259262, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0542877912521362, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8439942598342896, 'test/num_examples': 10000, 'score': 149495.27058815956, 'total_duration': 154597.66735219955, 'accumulated_submission_time': 149495.27058815956, 'accumulated_eval_time': 5066.508980512619, 'accumulated_logging_time': 19.83415412902832}
I0302 00:10:56.879278 140089770305280 logging_writer.py:48] [446649] accumulated_eval_time=5066.508981, accumulated_logging_time=19.834154, accumulated_submission_time=149495.270588, global_step=446649, preemption_count=0, score=149495.270588, test/accuracy=0.627300, test/loss=1.843994, test/num_examples=10000, total_duration=154597.667352, train/accuracy=0.959861, train/loss=0.148857, validation/accuracy=0.756160, validation/loss=1.054288, validation/num_examples=50000
I0302 00:11:14.266417 140089778697984 logging_writer.py:48] [446700] global_step=446700, grad_norm=4.7532877922058105, loss=0.6766119003295898
I0302 00:11:47.762909 140089770305280 logging_writer.py:48] [446800] global_step=446800, grad_norm=4.812866687774658, loss=0.5943052768707275
I0302 00:12:21.288110 140089778697984 logging_writer.py:48] [446900] global_step=446900, grad_norm=4.3605451583862305, loss=0.5948774218559265
I0302 00:12:54.730252 140089770305280 logging_writer.py:48] [447000] global_step=447000, grad_norm=4.600616931915283, loss=0.6693845987319946
I0302 00:13:28.177468 140089778697984 logging_writer.py:48] [447100] global_step=447100, grad_norm=4.247311115264893, loss=0.6464374661445618
I0302 00:14:01.628878 140089770305280 logging_writer.py:48] [447200] global_step=447200, grad_norm=4.226284980773926, loss=0.6301360130310059
I0302 00:14:35.069885 140089778697984 logging_writer.py:48] [447300] global_step=447300, grad_norm=5.111658573150635, loss=0.6258372664451599
I0302 00:15:08.537194 140089770305280 logging_writer.py:48] [447400] global_step=447400, grad_norm=4.536942481994629, loss=0.6105983257293701
I0302 00:15:41.980362 140089778697984 logging_writer.py:48] [447500] global_step=447500, grad_norm=4.454960823059082, loss=0.6009462475776672
I0302 00:16:15.425353 140089770305280 logging_writer.py:48] [447600] global_step=447600, grad_norm=4.26665735244751, loss=0.6302703619003296
I0302 00:16:48.866249 140089778697984 logging_writer.py:48] [447700] global_step=447700, grad_norm=5.787543296813965, loss=0.6525558829307556
I0302 00:17:22.323663 140089770305280 logging_writer.py:48] [447800] global_step=447800, grad_norm=4.849116802215576, loss=0.6323639750480652
I0302 00:17:55.809751 140089778697984 logging_writer.py:48] [447900] global_step=447900, grad_norm=4.723942756652832, loss=0.6336069107055664
I0302 00:18:29.377323 140089770305280 logging_writer.py:48] [448000] global_step=448000, grad_norm=4.726173400878906, loss=0.6622723340988159
I0302 00:19:02.847268 140089778697984 logging_writer.py:48] [448100] global_step=448100, grad_norm=4.562901496887207, loss=0.5999236702919006
I0302 00:19:27.056521 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:19:33.200276 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:19:41.228754 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:19:43.487702 140252611495744 submission_runner.py:411] Time since start: 155124.37s, 	Step: 448174, 	{'train/accuracy': 0.9597815275192261, 'train/loss': 0.1462220698595047, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.0554804801940918, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8451577425003052, 'test/num_examples': 10000, 'score': 150005.38194799423, 'total_duration': 155124.3677699566, 'accumulated_submission_time': 150005.38194799423, 'accumulated_eval_time': 5082.940105676651, 'accumulated_logging_time': 19.93660569190979}
I0302 00:19:43.581562 140089862604544 logging_writer.py:48] [448174] accumulated_eval_time=5082.940106, accumulated_logging_time=19.936606, accumulated_submission_time=150005.381948, global_step=448174, preemption_count=0, score=150005.381948, test/accuracy=0.627500, test/loss=1.845158, test/num_examples=10000, total_duration=155124.367770, train/accuracy=0.959782, train/loss=0.146222, validation/accuracy=0.755480, validation/loss=1.055480, validation/num_examples=50000
I0302 00:19:52.619811 140089870997248 logging_writer.py:48] [448200] global_step=448200, grad_norm=4.299988269805908, loss=0.5568098425865173
I0302 00:20:26.046428 140089862604544 logging_writer.py:48] [448300] global_step=448300, grad_norm=4.521458148956299, loss=0.553053617477417
I0302 00:20:59.507695 140089870997248 logging_writer.py:48] [448400] global_step=448400, grad_norm=4.539106369018555, loss=0.6620866656303406
I0302 00:21:32.972903 140089862604544 logging_writer.py:48] [448500] global_step=448500, grad_norm=4.783819675445557, loss=0.6702109575271606
I0302 00:22:06.406117 140089870997248 logging_writer.py:48] [448600] global_step=448600, grad_norm=4.157073497772217, loss=0.5777647495269775
I0302 00:22:39.843732 140089862604544 logging_writer.py:48] [448700] global_step=448700, grad_norm=4.4385271072387695, loss=0.5985490083694458
I0302 00:23:13.281585 140089870997248 logging_writer.py:48] [448800] global_step=448800, grad_norm=4.911450386047363, loss=0.6826807260513306
I0302 00:23:46.731587 140089862604544 logging_writer.py:48] [448900] global_step=448900, grad_norm=4.938761234283447, loss=0.6470428705215454
I0302 00:24:20.207364 140089870997248 logging_writer.py:48] [449000] global_step=449000, grad_norm=4.565526008605957, loss=0.5912401080131531
I0302 00:24:53.749753 140089862604544 logging_writer.py:48] [449100] global_step=449100, grad_norm=4.548634052276611, loss=0.6728922128677368
I0302 00:25:27.194122 140089870997248 logging_writer.py:48] [449200] global_step=449200, grad_norm=4.549890041351318, loss=0.617559552192688
I0302 00:26:00.639023 140089862604544 logging_writer.py:48] [449300] global_step=449300, grad_norm=4.424905300140381, loss=0.6109097599983215
I0302 00:26:34.114743 140089870997248 logging_writer.py:48] [449400] global_step=449400, grad_norm=4.491168975830078, loss=0.6218584775924683
I0302 00:27:07.546747 140089862604544 logging_writer.py:48] [449500] global_step=449500, grad_norm=4.869219779968262, loss=0.6394539475440979
I0302 00:27:40.992744 140089870997248 logging_writer.py:48] [449600] global_step=449600, grad_norm=3.923659324645996, loss=0.5158897638320923
I0302 00:28:13.563096 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:28:19.765647 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:28:27.787991 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:28:30.067490 140252611495744 submission_runner.py:411] Time since start: 155650.95s, 	Step: 449699, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.1465611755847931, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0540775060653687, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8444410562515259, 'test/num_examples': 10000, 'score': 150515.29887747765, 'total_duration': 155650.94755601883, 'accumulated_submission_time': 150515.29887747765, 'accumulated_eval_time': 5099.444447517395, 'accumulated_logging_time': 20.040841579437256}
I0302 00:28:30.169226 140089845819136 logging_writer.py:48] [449699] accumulated_eval_time=5099.444448, accumulated_logging_time=20.040842, accumulated_submission_time=150515.298877, global_step=449699, preemption_count=0, score=150515.298877, test/accuracy=0.627000, test/loss=1.844441, test/num_examples=10000, total_duration=155650.947556, train/accuracy=0.961117, train/loss=0.146561, validation/accuracy=0.755740, validation/loss=1.054078, validation/num_examples=50000
I0302 00:28:30.850393 140089854211840 logging_writer.py:48] [449700] global_step=449700, grad_norm=4.681254863739014, loss=0.5606502294540405
I0302 00:29:04.304733 140089845819136 logging_writer.py:48] [449800] global_step=449800, grad_norm=4.203196048736572, loss=0.6050440073013306
I0302 00:29:37.783723 140089854211840 logging_writer.py:48] [449900] global_step=449900, grad_norm=4.325382709503174, loss=0.5580428242683411
I0302 00:30:11.228168 140089845819136 logging_writer.py:48] [450000] global_step=450000, grad_norm=4.675260543823242, loss=0.588517963886261
I0302 00:30:44.768046 140089854211840 logging_writer.py:48] [450100] global_step=450100, grad_norm=4.6903462409973145, loss=0.6543484926223755
I0302 00:31:18.213619 140089845819136 logging_writer.py:48] [450200] global_step=450200, grad_norm=4.291775703430176, loss=0.6063721179962158
I0302 00:31:51.645625 140089854211840 logging_writer.py:48] [450300] global_step=450300, grad_norm=4.748243808746338, loss=0.6699682474136353
I0302 00:32:25.081275 140089845819136 logging_writer.py:48] [450400] global_step=450400, grad_norm=4.400501251220703, loss=0.5862409472465515
I0302 00:32:58.536003 140089854211840 logging_writer.py:48] [450500] global_step=450500, grad_norm=5.352464199066162, loss=0.6488463282585144
I0302 00:33:31.957913 140089845819136 logging_writer.py:48] [450600] global_step=450600, grad_norm=5.19650936126709, loss=0.5827293395996094
I0302 00:34:05.393416 140089854211840 logging_writer.py:48] [450700] global_step=450700, grad_norm=4.330874443054199, loss=0.6002837419509888
I0302 00:34:38.877584 140089845819136 logging_writer.py:48] [450800] global_step=450800, grad_norm=4.725055694580078, loss=0.6456688642501831
I0302 00:35:12.331117 140089854211840 logging_writer.py:48] [450900] global_step=450900, grad_norm=4.010183811187744, loss=0.5693131685256958
I0302 00:35:45.784873 140089845819136 logging_writer.py:48] [451000] global_step=451000, grad_norm=4.803100109100342, loss=0.7178018689155579
I0302 00:36:19.252676 140089854211840 logging_writer.py:48] [451100] global_step=451100, grad_norm=4.654403209686279, loss=0.6853244304656982
I0302 00:36:52.806151 140089845819136 logging_writer.py:48] [451200] global_step=451200, grad_norm=4.861880302429199, loss=0.6525000929832458
I0302 00:37:00.317932 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:37:06.753039 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:37:14.725720 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:37:16.992636 140252611495744 submission_runner.py:411] Time since start: 156177.87s, 	Step: 451224, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14594563841819763, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.054438829421997, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8438587188720703, 'test/num_examples': 10000, 'score': 151025.3807592392, 'total_duration': 156177.87268805504, 'accumulated_submission_time': 151025.3807592392, 'accumulated_eval_time': 5116.119092226028, 'accumulated_logging_time': 20.154107332229614}
I0302 00:37:17.082420 140089837426432 logging_writer.py:48] [451224] accumulated_eval_time=5116.119092, accumulated_logging_time=20.154107, accumulated_submission_time=151025.380759, global_step=451224, preemption_count=0, score=151025.380759, test/accuracy=0.627500, test/loss=1.843859, test/num_examples=10000, total_duration=156177.872688, train/accuracy=0.960798, train/loss=0.145946, validation/accuracy=0.755260, validation/loss=1.054439, validation/num_examples=50000
I0302 00:37:42.823523 140089862604544 logging_writer.py:48] [451300] global_step=451300, grad_norm=4.78122615814209, loss=0.6354559063911438
I0302 00:38:16.284265 140089837426432 logging_writer.py:48] [451400] global_step=451400, grad_norm=4.6112470626831055, loss=0.6530770063400269
I0302 00:38:49.751967 140089862604544 logging_writer.py:48] [451500] global_step=451500, grad_norm=4.332769393920898, loss=0.5634651184082031
I0302 00:39:23.182681 140089837426432 logging_writer.py:48] [451600] global_step=451600, grad_norm=4.601210117340088, loss=0.7000935673713684
I0302 00:39:56.661288 140089862604544 logging_writer.py:48] [451700] global_step=451700, grad_norm=4.1808037757873535, loss=0.5582005381584167
I0302 00:40:30.090736 140089837426432 logging_writer.py:48] [451800] global_step=451800, grad_norm=4.716607093811035, loss=0.7169797420501709
I0302 00:41:03.557842 140089862604544 logging_writer.py:48] [451900] global_step=451900, grad_norm=4.107987403869629, loss=0.5700496435165405
I0302 00:41:37.039070 140089837426432 logging_writer.py:48] [452000] global_step=452000, grad_norm=4.731393337249756, loss=0.5823661684989929
I0302 00:42:10.507624 140089862604544 logging_writer.py:48] [452100] global_step=452100, grad_norm=4.085941791534424, loss=0.583990216255188
I0302 00:42:44.057330 140089837426432 logging_writer.py:48] [452200] global_step=452200, grad_norm=4.755445957183838, loss=0.6459746360778809
I0302 00:43:17.498531 140089862604544 logging_writer.py:48] [452300] global_step=452300, grad_norm=4.583611965179443, loss=0.5619542598724365
I0302 00:43:50.947044 140089837426432 logging_writer.py:48] [452400] global_step=452400, grad_norm=4.644277572631836, loss=0.6366134881973267
I0302 00:44:24.420557 140089862604544 logging_writer.py:48] [452500] global_step=452500, grad_norm=5.1620917320251465, loss=0.6952844262123108
I0302 00:44:57.869987 140089837426432 logging_writer.py:48] [452600] global_step=452600, grad_norm=4.469287872314453, loss=0.600949764251709
I0302 00:45:31.315519 140089862604544 logging_writer.py:48] [452700] global_step=452700, grad_norm=4.28464937210083, loss=0.6325398683547974
I0302 00:45:47.158030 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:45:53.284111 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:46:01.316845 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:46:03.760889 140252611495744 submission_runner.py:411] Time since start: 156704.64s, 	Step: 452749, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14417435228824615, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.052942156791687, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8419733047485352, 'test/num_examples': 10000, 'score': 151535.39058494568, 'total_duration': 156704.6409471035, 'accumulated_submission_time': 151535.39058494568, 'accumulated_eval_time': 5132.721889019012, 'accumulated_logging_time': 20.254146337509155}
I0302 00:46:03.856929 140089778697984 logging_writer.py:48] [452749] accumulated_eval_time=5132.721889, accumulated_logging_time=20.254146, accumulated_submission_time=151535.390585, global_step=452749, preemption_count=0, score=151535.390585, test/accuracy=0.627400, test/loss=1.841973, test/num_examples=10000, total_duration=156704.640947, train/accuracy=0.961217, train/loss=0.144174, validation/accuracy=0.755840, validation/loss=1.052942, validation/num_examples=50000
I0302 00:46:21.253604 140089837426432 logging_writer.py:48] [452800] global_step=452800, grad_norm=4.796109199523926, loss=0.63318932056427
I0302 00:46:54.686396 140089778697984 logging_writer.py:48] [452900] global_step=452900, grad_norm=4.452761650085449, loss=0.6586588621139526
I0302 00:47:28.117360 140089837426432 logging_writer.py:48] [453000] global_step=453000, grad_norm=5.077256679534912, loss=0.6063901782035828
I0302 00:48:01.555362 140089778697984 logging_writer.py:48] [453100] global_step=453100, grad_norm=4.561971664428711, loss=0.60578453540802
I0302 00:48:34.995476 140089837426432 logging_writer.py:48] [453200] global_step=453200, grad_norm=4.891880989074707, loss=0.6787745356559753
I0302 00:49:08.504170 140089778697984 logging_writer.py:48] [453300] global_step=453300, grad_norm=4.77236795425415, loss=0.6390637755393982
I0302 00:49:41.959844 140089837426432 logging_writer.py:48] [453400] global_step=453400, grad_norm=4.224536895751953, loss=0.6787585616111755
I0302 00:50:15.396353 140089778697984 logging_writer.py:48] [453500] global_step=453500, grad_norm=4.113152503967285, loss=0.5785583257675171
I0302 00:50:48.833476 140089837426432 logging_writer.py:48] [453600] global_step=453600, grad_norm=4.399979591369629, loss=0.6135228276252747
I0302 00:51:22.276654 140089778697984 logging_writer.py:48] [453700] global_step=453700, grad_norm=5.2436981201171875, loss=0.5861003994941711
I0302 00:51:55.711383 140089837426432 logging_writer.py:48] [453800] global_step=453800, grad_norm=4.663302898406982, loss=0.6561159491539001
I0302 00:52:29.141556 140089778697984 logging_writer.py:48] [453900] global_step=453900, grad_norm=4.3538618087768555, loss=0.6001310348510742
I0302 00:53:02.590847 140089837426432 logging_writer.py:48] [454000] global_step=454000, grad_norm=4.868582248687744, loss=0.6319612860679626
I0302 00:53:36.039628 140089778697984 logging_writer.py:48] [454100] global_step=454100, grad_norm=4.351211071014404, loss=0.5715119242668152
I0302 00:54:09.493822 140089837426432 logging_writer.py:48] [454200] global_step=454200, grad_norm=4.986472129821777, loss=0.7198485136032104
I0302 00:54:34.049469 140252611495744 spec.py:321] Evaluating on the training split.
I0302 00:54:40.189479 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 00:54:48.213891 140252611495744 spec.py:349] Evaluating on the test split.
I0302 00:54:50.827570 140252611495744 submission_runner.py:411] Time since start: 157231.71s, 	Step: 454275, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.14575372636318207, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0537270307540894, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8424499034881592, 'test/num_examples': 10000, 'score': 152045.5179359913, 'total_duration': 157231.70755052567, 'accumulated_submission_time': 152045.5179359913, 'accumulated_eval_time': 5149.499848604202, 'accumulated_logging_time': 20.360689640045166}
I0302 00:54:50.919424 140089770305280 logging_writer.py:48] [454275] accumulated_eval_time=5149.499849, accumulated_logging_time=20.360690, accumulated_submission_time=152045.517936, global_step=454275, preemption_count=0, score=152045.517936, test/accuracy=0.627900, test/loss=1.842450, test/num_examples=10000, total_duration=157231.707551, train/accuracy=0.960360, train/loss=0.145754, validation/accuracy=0.756060, validation/loss=1.053727, validation/num_examples=50000
I0302 00:54:59.639191 140089778697984 logging_writer.py:48] [454300] global_step=454300, grad_norm=4.991424560546875, loss=0.6026701331138611
I0302 00:55:33.081561 140089770305280 logging_writer.py:48] [454400] global_step=454400, grad_norm=4.209263801574707, loss=0.5828368663787842
I0302 00:56:06.520558 140089778697984 logging_writer.py:48] [454500] global_step=454500, grad_norm=3.9468798637390137, loss=0.5813238024711609
I0302 00:56:40.008043 140089770305280 logging_writer.py:48] [454600] global_step=454600, grad_norm=4.275014400482178, loss=0.6394414901733398
I0302 00:57:13.416007 140089778697984 logging_writer.py:48] [454700] global_step=454700, grad_norm=5.350737571716309, loss=0.6812971830368042
I0302 00:57:46.878991 140089770305280 logging_writer.py:48] [454800] global_step=454800, grad_norm=4.348940372467041, loss=0.5505627989768982
I0302 00:58:20.289883 140089778697984 logging_writer.py:48] [454900] global_step=454900, grad_norm=5.278149604797363, loss=0.6080107688903809
I0302 00:58:53.739958 140089770305280 logging_writer.py:48] [455000] global_step=455000, grad_norm=4.431879043579102, loss=0.608888566493988
I0302 00:59:27.212288 140089778697984 logging_writer.py:48] [455100] global_step=455100, grad_norm=4.3603835105896, loss=0.5854515433311462
I0302 01:00:00.635413 140089770305280 logging_writer.py:48] [455200] global_step=455200, grad_norm=4.159270286560059, loss=0.5704386830329895
I0302 01:00:34.101420 140089778697984 logging_writer.py:48] [455300] global_step=455300, grad_norm=4.285032749176025, loss=0.5956007242202759
I0302 01:01:07.670735 140089770305280 logging_writer.py:48] [455400] global_step=455400, grad_norm=4.6677775382995605, loss=0.6376169323921204
I0302 01:01:41.147354 140089778697984 logging_writer.py:48] [455500] global_step=455500, grad_norm=4.503995895385742, loss=0.607978105545044
I0302 01:02:14.586604 140089770305280 logging_writer.py:48] [455600] global_step=455600, grad_norm=4.311458110809326, loss=0.679410457611084
I0302 01:02:48.046711 140089778697984 logging_writer.py:48] [455700] global_step=455700, grad_norm=4.78079891204834, loss=0.5811022520065308
I0302 01:03:21.002168 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:03:27.153087 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:03:35.137127 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:03:37.393055 140252611495744 submission_runner.py:411] Time since start: 157758.27s, 	Step: 455800, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.1473921835422516, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0532865524291992, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8414360284805298, 'test/num_examples': 10000, 'score': 152555.53424096107, 'total_duration': 157758.27311992645, 'accumulated_submission_time': 152555.53424096107, 'accumulated_eval_time': 5165.890701532364, 'accumulated_logging_time': 20.463539123535156}
I0302 01:03:37.489956 140089778697984 logging_writer.py:48] [455800] accumulated_eval_time=5165.890702, accumulated_logging_time=20.463539, accumulated_submission_time=152555.534241, global_step=455800, preemption_count=0, score=152555.534241, test/accuracy=0.626800, test/loss=1.841436, test/num_examples=10000, total_duration=157758.273120, train/accuracy=0.960419, train/loss=0.147392, validation/accuracy=0.755760, validation/loss=1.053287, validation/num_examples=50000
I0302 01:03:37.829815 140089837426432 logging_writer.py:48] [455800] global_step=455800, grad_norm=4.423581123352051, loss=0.6137056946754456
I0302 01:04:11.301420 140089778697984 logging_writer.py:48] [455900] global_step=455900, grad_norm=4.351661205291748, loss=0.6355581879615784
I0302 01:04:44.753626 140089837426432 logging_writer.py:48] [456000] global_step=456000, grad_norm=4.124263763427734, loss=0.5711584091186523
I0302 01:05:18.210289 140089778697984 logging_writer.py:48] [456100] global_step=456100, grad_norm=4.494295120239258, loss=0.6593704223632812
I0302 01:05:51.640261 140089837426432 logging_writer.py:48] [456200] global_step=456200, grad_norm=4.034319877624512, loss=0.5857724547386169
I0302 01:06:25.092648 140089778697984 logging_writer.py:48] [456300] global_step=456300, grad_norm=4.480036735534668, loss=0.6597266793251038
I0302 01:06:58.647155 140089837426432 logging_writer.py:48] [456400] global_step=456400, grad_norm=4.764894485473633, loss=0.5916770696640015
I0302 01:07:32.135757 140089778697984 logging_writer.py:48] [456500] global_step=456500, grad_norm=4.767723083496094, loss=0.700884222984314
I0302 01:08:05.557157 140089837426432 logging_writer.py:48] [456600] global_step=456600, grad_norm=4.091940879821777, loss=0.6083599328994751
I0302 01:08:39.014448 140089778697984 logging_writer.py:48] [456700] global_step=456700, grad_norm=4.551182746887207, loss=0.6659201383590698
I0302 01:09:12.457295 140089837426432 logging_writer.py:48] [456800] global_step=456800, grad_norm=5.294681549072266, loss=0.6706852316856384
I0302 01:09:45.902180 140089778697984 logging_writer.py:48] [456900] global_step=456900, grad_norm=4.669515132904053, loss=0.6179061532020569
I0302 01:10:19.378216 140089837426432 logging_writer.py:48] [457000] global_step=457000, grad_norm=4.335451602935791, loss=0.5925881266593933
I0302 01:10:52.810014 140089778697984 logging_writer.py:48] [457100] global_step=457100, grad_norm=4.305539131164551, loss=0.5970204472541809
I0302 01:11:26.255504 140089837426432 logging_writer.py:48] [457200] global_step=457200, grad_norm=4.532235145568848, loss=0.6450722813606262
I0302 01:11:59.681007 140089778697984 logging_writer.py:48] [457300] global_step=457300, grad_norm=4.681273937225342, loss=0.6290774345397949
I0302 01:12:07.504565 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:12:13.616986 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:12:21.655109 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:12:23.936732 140252611495744 submission_runner.py:411] Time since start: 158284.82s, 	Step: 457325, 	{'train/accuracy': 0.9620735049247742, 'train/loss': 0.1456650346517563, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0540398359298706, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8419164419174194, 'test/num_examples': 10000, 'score': 153065.48213148117, 'total_duration': 158284.81679821014, 'accumulated_submission_time': 153065.48213148117, 'accumulated_eval_time': 5182.322814702988, 'accumulated_logging_time': 20.57215189933777}
I0302 01:12:24.029518 140089854211840 logging_writer.py:48] [457325] accumulated_eval_time=5182.322815, accumulated_logging_time=20.572152, accumulated_submission_time=153065.482131, global_step=457325, preemption_count=0, score=153065.482131, test/accuracy=0.627100, test/loss=1.841916, test/num_examples=10000, total_duration=158284.816798, train/accuracy=0.962074, train/loss=0.145665, validation/accuracy=0.755580, validation/loss=1.054040, validation/num_examples=50000
I0302 01:12:49.481709 140089862604544 logging_writer.py:48] [457400] global_step=457400, grad_norm=4.285460948944092, loss=0.610980749130249
I0302 01:13:22.982716 140089854211840 logging_writer.py:48] [457500] global_step=457500, grad_norm=4.639543533325195, loss=0.6130372881889343
I0302 01:13:56.439090 140089862604544 logging_writer.py:48] [457600] global_step=457600, grad_norm=4.763302803039551, loss=0.6813014149665833
I0302 01:14:29.870826 140089854211840 logging_writer.py:48] [457700] global_step=457700, grad_norm=4.519190788269043, loss=0.6074450016021729
I0302 01:15:03.303837 140089862604544 logging_writer.py:48] [457800] global_step=457800, grad_norm=4.724604606628418, loss=0.6591534614562988
I0302 01:15:36.764997 140089854211840 logging_writer.py:48] [457900] global_step=457900, grad_norm=4.594470977783203, loss=0.6313905715942383
I0302 01:16:10.193490 140089862604544 logging_writer.py:48] [458000] global_step=458000, grad_norm=4.7005414962768555, loss=0.5932212471961975
I0302 01:16:43.632958 140089854211840 logging_writer.py:48] [458100] global_step=458100, grad_norm=4.959586143493652, loss=0.6576122641563416
I0302 01:17:17.040262 140089862604544 logging_writer.py:48] [458200] global_step=458200, grad_norm=4.0263285636901855, loss=0.5082257986068726
I0302 01:17:50.473962 140089854211840 logging_writer.py:48] [458300] global_step=458300, grad_norm=4.423946380615234, loss=0.6345545053482056
I0302 01:18:23.894939 140089862604544 logging_writer.py:48] [458400] global_step=458400, grad_norm=4.151598930358887, loss=0.5509304404258728
I0302 01:18:57.355076 140089854211840 logging_writer.py:48] [458500] global_step=458500, grad_norm=4.458865642547607, loss=0.5628855228424072
I0302 01:19:30.890243 140089862604544 logging_writer.py:48] [458600] global_step=458600, grad_norm=4.3150129318237305, loss=0.5713526606559753
I0302 01:20:04.338957 140089854211840 logging_writer.py:48] [458700] global_step=458700, grad_norm=4.441720962524414, loss=0.5678766965866089
I0302 01:20:37.769135 140089862604544 logging_writer.py:48] [458800] global_step=458800, grad_norm=3.8125007152557373, loss=0.5269465446472168
I0302 01:20:53.991234 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:21:00.186307 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:21:08.354521 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:21:10.704908 140252611495744 submission_runner.py:411] Time since start: 158811.58s, 	Step: 458850, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.1474863737821579, 'validation/accuracy': 0.7563999891281128, 'validation/loss': 1.0531036853790283, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8431851863861084, 'test/num_examples': 10000, 'score': 153575.37730908394, 'total_duration': 158811.58496284485, 'accumulated_submission_time': 153575.37730908394, 'accumulated_eval_time': 5199.0364236831665, 'accumulated_logging_time': 20.67511820793152}
I0302 01:21:10.798061 140089845819136 logging_writer.py:48] [458850] accumulated_eval_time=5199.036424, accumulated_logging_time=20.675118, accumulated_submission_time=153575.377309, global_step=458850, preemption_count=0, score=153575.377309, test/accuracy=0.627400, test/loss=1.843185, test/num_examples=10000, total_duration=158811.584963, train/accuracy=0.960559, train/loss=0.147486, validation/accuracy=0.756400, validation/loss=1.053104, validation/num_examples=50000
I0302 01:21:27.836016 140089862604544 logging_writer.py:48] [458900] global_step=458900, grad_norm=4.505648136138916, loss=0.5812395811080933
I0302 01:22:01.273004 140089845819136 logging_writer.py:48] [459000] global_step=459000, grad_norm=4.710488319396973, loss=0.5747601389884949
I0302 01:22:34.720763 140089862604544 logging_writer.py:48] [459100] global_step=459100, grad_norm=4.257397174835205, loss=0.6304293870925903
I0302 01:23:08.183177 140089845819136 logging_writer.py:48] [459200] global_step=459200, grad_norm=4.0480804443359375, loss=0.5163780450820923
I0302 01:23:41.627879 140089862604544 logging_writer.py:48] [459300] global_step=459300, grad_norm=4.848288536071777, loss=0.6529308557510376
I0302 01:24:15.097981 140089845819136 logging_writer.py:48] [459400] global_step=459400, grad_norm=4.390669345855713, loss=0.59884113073349
I0302 01:24:48.533937 140089862604544 logging_writer.py:48] [459500] global_step=459500, grad_norm=4.454483509063721, loss=0.6680415272712708
I0302 01:25:22.027484 140089845819136 logging_writer.py:48] [459600] global_step=459600, grad_norm=4.8840436935424805, loss=0.6485546827316284
I0302 01:25:55.458552 140089862604544 logging_writer.py:48] [459700] global_step=459700, grad_norm=4.750864028930664, loss=0.6711596250534058
I0302 01:26:28.879242 140089845819136 logging_writer.py:48] [459800] global_step=459800, grad_norm=4.3860321044921875, loss=0.5906186699867249
I0302 01:27:02.358321 140089862604544 logging_writer.py:48] [459900] global_step=459900, grad_norm=4.731731414794922, loss=0.7002388834953308
I0302 01:27:35.820055 140089845819136 logging_writer.py:48] [460000] global_step=460000, grad_norm=4.564553260803223, loss=0.5687559843063354
I0302 01:28:09.249817 140089862604544 logging_writer.py:48] [460100] global_step=460100, grad_norm=4.241878509521484, loss=0.5735993385314941
I0302 01:28:42.684981 140089845819136 logging_writer.py:48] [460200] global_step=460200, grad_norm=4.1627326011657715, loss=0.5954147577285767
I0302 01:29:16.134621 140089862604544 logging_writer.py:48] [460300] global_step=460300, grad_norm=4.627932071685791, loss=0.7055506110191345
I0302 01:29:40.993643 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:29:47.072694 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:29:55.138394 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:29:57.430520 140252611495744 submission_runner.py:411] Time since start: 159338.31s, 	Step: 460376, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14776617288589478, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0535650253295898, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8425542116165161, 'test/num_examples': 10000, 'score': 154085.50613236427, 'total_duration': 159338.31057286263, 'accumulated_submission_time': 154085.50613236427, 'accumulated_eval_time': 5215.4732303619385, 'accumulated_logging_time': 20.78009033203125}
I0302 01:29:57.526447 140089837426432 logging_writer.py:48] [460376] accumulated_eval_time=5215.473230, accumulated_logging_time=20.780090, accumulated_submission_time=154085.506132, global_step=460376, preemption_count=0, score=154085.506132, test/accuracy=0.627600, test/loss=1.842554, test/num_examples=10000, total_duration=159338.310573, train/accuracy=0.960559, train/loss=0.147766, validation/accuracy=0.756040, validation/loss=1.053565, validation/num_examples=50000
I0302 01:30:05.877249 140089854211840 logging_writer.py:48] [460400] global_step=460400, grad_norm=4.846161842346191, loss=0.6065312027931213
I0302 01:30:39.307605 140089837426432 logging_writer.py:48] [460500] global_step=460500, grad_norm=4.322945594787598, loss=0.5636467337608337
I0302 01:31:12.757250 140089854211840 logging_writer.py:48] [460600] global_step=460600, grad_norm=4.321166038513184, loss=0.6353154182434082
I0302 01:31:46.359705 140089837426432 logging_writer.py:48] [460700] global_step=460700, grad_norm=4.488587856292725, loss=0.6254676580429077
I0302 01:32:19.805727 140089854211840 logging_writer.py:48] [460800] global_step=460800, grad_norm=4.677035331726074, loss=0.5973952412605286
I0302 01:32:53.275537 140089837426432 logging_writer.py:48] [460900] global_step=460900, grad_norm=5.253738880157471, loss=0.5581163167953491
I0302 01:33:26.739396 140089854211840 logging_writer.py:48] [461000] global_step=461000, grad_norm=4.208267688751221, loss=0.5721684694290161
I0302 01:34:00.181346 140089837426432 logging_writer.py:48] [461100] global_step=461100, grad_norm=4.648835182189941, loss=0.5214214324951172
I0302 01:34:33.640774 140089854211840 logging_writer.py:48] [461200] global_step=461200, grad_norm=4.527954578399658, loss=0.6240010261535645
I0302 01:35:07.071448 140089837426432 logging_writer.py:48] [461300] global_step=461300, grad_norm=4.530605792999268, loss=0.6074017286300659
I0302 01:35:40.504507 140089854211840 logging_writer.py:48] [461400] global_step=461400, grad_norm=4.9421706199646, loss=0.6313946843147278
I0302 01:36:13.943806 140089837426432 logging_writer.py:48] [461500] global_step=461500, grad_norm=4.521591663360596, loss=0.6670736074447632
I0302 01:36:47.364471 140089854211840 logging_writer.py:48] [461600] global_step=461600, grad_norm=4.751604080200195, loss=0.6894029974937439
I0302 01:37:20.823123 140089837426432 logging_writer.py:48] [461700] global_step=461700, grad_norm=4.465810298919678, loss=0.6076976656913757
I0302 01:37:54.371645 140089854211840 logging_writer.py:48] [461800] global_step=461800, grad_norm=4.47976016998291, loss=0.5805310010910034
I0302 01:38:27.835590 140089837426432 logging_writer.py:48] [461900] global_step=461900, grad_norm=4.746513843536377, loss=0.6393147110939026
I0302 01:38:27.844759 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:38:34.011854 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:38:42.104742 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:38:44.464671 140252611495744 submission_runner.py:411] Time since start: 159865.34s, 	Step: 461901, 	{'train/accuracy': 0.9631098508834839, 'train/loss': 0.14079298079013824, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0532169342041016, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8411426544189453, 'test/num_examples': 10000, 'score': 154595.75695943832, 'total_duration': 159865.34474110603, 'accumulated_submission_time': 154595.75695943832, 'accumulated_eval_time': 5232.09307384491, 'accumulated_logging_time': 20.888579607009888}
I0302 01:38:44.557966 140089845819136 logging_writer.py:48] [461901] accumulated_eval_time=5232.093074, accumulated_logging_time=20.888580, accumulated_submission_time=154595.756959, global_step=461901, preemption_count=0, score=154595.756959, test/accuracy=0.628000, test/loss=1.841143, test/num_examples=10000, total_duration=159865.344741, train/accuracy=0.963110, train/loss=0.140793, validation/accuracy=0.755740, validation/loss=1.053217, validation/num_examples=50000
I0302 01:39:18.012440 140089862604544 logging_writer.py:48] [462000] global_step=462000, grad_norm=4.74063777923584, loss=0.6580156683921814
I0302 01:39:51.483363 140089845819136 logging_writer.py:48] [462100] global_step=462100, grad_norm=4.9655232429504395, loss=0.6586064100265503
I0302 01:40:24.930283 140089862604544 logging_writer.py:48] [462200] global_step=462200, grad_norm=4.516250133514404, loss=0.6251605749130249
I0302 01:40:58.391561 140089845819136 logging_writer.py:48] [462300] global_step=462300, grad_norm=4.516127586364746, loss=0.6310440301895142
I0302 01:41:31.867295 140089862604544 logging_writer.py:48] [462400] global_step=462400, grad_norm=4.056142807006836, loss=0.568663477897644
I0302 01:42:05.295458 140089845819136 logging_writer.py:48] [462500] global_step=462500, grad_norm=4.18218994140625, loss=0.6049531102180481
I0302 01:42:38.797569 140089862604544 logging_writer.py:48] [462600] global_step=462600, grad_norm=5.028351783752441, loss=0.6791828274726868
I0302 01:43:12.257941 140089845819136 logging_writer.py:48] [462700] global_step=462700, grad_norm=4.485493183135986, loss=0.637463390827179
I0302 01:43:45.812431 140089862604544 logging_writer.py:48] [462800] global_step=462800, grad_norm=4.23860502243042, loss=0.537529706954956
I0302 01:44:19.271563 140089845819136 logging_writer.py:48] [462900] global_step=462900, grad_norm=4.784902095794678, loss=0.6903774738311768
I0302 01:44:52.724371 140089862604544 logging_writer.py:48] [463000] global_step=463000, grad_norm=4.667266368865967, loss=0.6195290088653564
I0302 01:45:26.206696 140089845819136 logging_writer.py:48] [463100] global_step=463100, grad_norm=4.671474456787109, loss=0.5672817826271057
I0302 01:45:59.668593 140089862604544 logging_writer.py:48] [463200] global_step=463200, grad_norm=4.292450428009033, loss=0.6191476583480835
I0302 01:46:33.109478 140089845819136 logging_writer.py:48] [463300] global_step=463300, grad_norm=4.293817520141602, loss=0.58001309633255
I0302 01:47:06.595273 140089862604544 logging_writer.py:48] [463400] global_step=463400, grad_norm=4.675321578979492, loss=0.6005860567092896
I0302 01:47:14.753774 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:47:20.885852 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:47:29.034467 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:47:31.288214 140252611495744 submission_runner.py:411] Time since start: 160392.17s, 	Step: 463426, 	{'train/accuracy': 0.9622329473495483, 'train/loss': 0.14507044851779938, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0533766746520996, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8413161039352417, 'test/num_examples': 10000, 'score': 155105.88724708557, 'total_duration': 160392.16827869415, 'accumulated_submission_time': 155105.88724708557, 'accumulated_eval_time': 5248.6274580955505, 'accumulated_logging_time': 20.992839574813843}
I0302 01:47:31.382720 140089870997248 logging_writer.py:48] [463426] accumulated_eval_time=5248.627458, accumulated_logging_time=20.992840, accumulated_submission_time=155105.887247, global_step=463426, preemption_count=0, score=155105.887247, test/accuracy=0.627500, test/loss=1.841316, test/num_examples=10000, total_duration=160392.168279, train/accuracy=0.962233, train/loss=0.145070, validation/accuracy=0.755980, validation/loss=1.053377, validation/num_examples=50000
I0302 01:47:56.484946 140089879389952 logging_writer.py:48] [463500] global_step=463500, grad_norm=4.505936622619629, loss=0.6697266697883606
I0302 01:48:29.947248 140089870997248 logging_writer.py:48] [463600] global_step=463600, grad_norm=4.391022205352783, loss=0.5978740453720093
I0302 01:49:03.444863 140089879389952 logging_writer.py:48] [463700] global_step=463700, grad_norm=4.917038440704346, loss=0.6532251238822937
I0302 01:49:36.884762 140089870997248 logging_writer.py:48] [463800] global_step=463800, grad_norm=4.378106117248535, loss=0.5857082009315491
I0302 01:50:10.443958 140089879389952 logging_writer.py:48] [463900] global_step=463900, grad_norm=5.238116264343262, loss=0.7188734412193298
I0302 01:50:43.881746 140089870997248 logging_writer.py:48] [464000] global_step=464000, grad_norm=4.666297435760498, loss=0.658595085144043
I0302 01:51:17.349493 140089879389952 logging_writer.py:48] [464100] global_step=464100, grad_norm=4.4071044921875, loss=0.5959744453430176
I0302 01:51:50.801335 140089870997248 logging_writer.py:48] [464200] global_step=464200, grad_norm=4.283332824707031, loss=0.5718520879745483
I0302 01:52:24.258161 140089879389952 logging_writer.py:48] [464300] global_step=464300, grad_norm=4.496006011962891, loss=0.6196210980415344
I0302 01:52:57.755356 140089870997248 logging_writer.py:48] [464400] global_step=464400, grad_norm=4.314159870147705, loss=0.5755761861801147
I0302 01:53:31.233526 140089879389952 logging_writer.py:48] [464500] global_step=464500, grad_norm=4.621384620666504, loss=0.6394518613815308
I0302 01:54:04.663235 140089870997248 logging_writer.py:48] [464600] global_step=464600, grad_norm=4.435225963592529, loss=0.6626006364822388
I0302 01:54:38.110394 140089879389952 logging_writer.py:48] [464700] global_step=464700, grad_norm=4.686136722564697, loss=0.5893945693969727
I0302 01:55:11.579672 140089870997248 logging_writer.py:48] [464800] global_step=464800, grad_norm=4.6733880043029785, loss=0.587828516960144
I0302 01:55:45.169427 140089879389952 logging_writer.py:48] [464900] global_step=464900, grad_norm=4.366633892059326, loss=0.6360574960708618
I0302 01:56:01.405001 140252611495744 spec.py:321] Evaluating on the training split.
I0302 01:56:07.551990 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 01:56:16.220696 140252611495744 spec.py:349] Evaluating on the test split.
I0302 01:56:18.466027 140252611495744 submission_runner.py:411] Time since start: 160919.35s, 	Step: 464950, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.1451112926006317, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.054116129875183, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8424440622329712, 'test/num_examples': 10000, 'score': 155615.8431122303, 'total_duration': 160919.34607696533, 'accumulated_submission_time': 155615.8431122303, 'accumulated_eval_time': 5265.688413143158, 'accumulated_logging_time': 21.098219394683838}
I0302 01:56:18.561069 140089770305280 logging_writer.py:48] [464950] accumulated_eval_time=5265.688413, accumulated_logging_time=21.098219, accumulated_submission_time=155615.843112, global_step=464950, preemption_count=0, score=155615.843112, test/accuracy=0.626700, test/loss=1.842444, test/num_examples=10000, total_duration=160919.346077, train/accuracy=0.961834, train/loss=0.145111, validation/accuracy=0.755640, validation/loss=1.054116, validation/num_examples=50000
I0302 01:56:35.645050 140089778697984 logging_writer.py:48] [465000] global_step=465000, grad_norm=4.78952693939209, loss=0.5499840974807739
I0302 01:57:09.095544 140089770305280 logging_writer.py:48] [465100] global_step=465100, grad_norm=4.512942314147949, loss=0.5851132273674011
I0302 01:57:42.557867 140089778697984 logging_writer.py:48] [465200] global_step=465200, grad_norm=4.324616432189941, loss=0.6279023885726929
I0302 01:58:16.011319 140089770305280 logging_writer.py:48] [465300] global_step=465300, grad_norm=4.727867603302002, loss=0.6200338006019592
I0302 01:58:49.438320 140089778697984 logging_writer.py:48] [465400] global_step=465400, grad_norm=4.324819564819336, loss=0.6485739350318909
I0302 01:59:22.916059 140089770305280 logging_writer.py:48] [465500] global_step=465500, grad_norm=5.42868709564209, loss=0.7273007035255432
I0302 01:59:56.336943 140089778697984 logging_writer.py:48] [465600] global_step=465600, grad_norm=4.69102668762207, loss=0.6757398247718811
I0302 02:00:29.794528 140089770305280 logging_writer.py:48] [465700] global_step=465700, grad_norm=5.476552963256836, loss=0.6395472884178162
I0302 02:01:03.230712 140089778697984 logging_writer.py:48] [465800] global_step=465800, grad_norm=4.5299811363220215, loss=0.6123694777488708
I0302 02:01:36.689065 140089770305280 logging_writer.py:48] [465900] global_step=465900, grad_norm=5.473353862762451, loss=0.7007209658622742
I0302 02:02:10.223183 140089778697984 logging_writer.py:48] [466000] global_step=466000, grad_norm=4.165846824645996, loss=0.5526977777481079
I0302 02:02:43.737830 140089770305280 logging_writer.py:48] [466100] global_step=466100, grad_norm=4.209530353546143, loss=0.582323431968689
I0302 02:03:17.205512 140089778697984 logging_writer.py:48] [466200] global_step=466200, grad_norm=4.3665618896484375, loss=0.5785578489303589
I0302 02:03:50.671483 140089770305280 logging_writer.py:48] [466300] global_step=466300, grad_norm=4.441930294036865, loss=0.5523476004600525
I0302 02:04:24.093634 140089778697984 logging_writer.py:48] [466400] global_step=466400, grad_norm=4.652741432189941, loss=0.5462064146995544
I0302 02:04:48.668603 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:04:54.863573 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:05:03.042970 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:05:05.321248 140252611495744 submission_runner.py:411] Time since start: 161446.20s, 	Step: 466475, 	{'train/accuracy': 0.9597217440605164, 'train/loss': 0.14561453461647034, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.052058219909668, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.840462565422058, 'test/num_examples': 10000, 'score': 156125.88497161865, 'total_duration': 161446.20129728317, 'accumulated_submission_time': 156125.88497161865, 'accumulated_eval_time': 5282.34098482132, 'accumulated_logging_time': 21.203664302825928}
I0302 02:05:05.411042 140089500890880 logging_writer.py:48] [466475] accumulated_eval_time=5282.340985, accumulated_logging_time=21.203664, accumulated_submission_time=156125.884972, global_step=466475, preemption_count=0, score=156125.884972, test/accuracy=0.627900, test/loss=1.840463, test/num_examples=10000, total_duration=161446.201297, train/accuracy=0.959722, train/loss=0.145615, validation/accuracy=0.756260, validation/loss=1.052058, validation/num_examples=50000
I0302 02:05:14.126284 140089770305280 logging_writer.py:48] [466500] global_step=466500, grad_norm=4.191966533660889, loss=0.5394563674926758
I0302 02:05:47.551407 140089500890880 logging_writer.py:48] [466600] global_step=466600, grad_norm=4.339738845825195, loss=0.6032370924949646
I0302 02:06:21.013158 140089770305280 logging_writer.py:48] [466700] global_step=466700, grad_norm=4.361605167388916, loss=0.5857727527618408
I0302 02:06:54.447622 140089500890880 logging_writer.py:48] [466800] global_step=466800, grad_norm=4.265592098236084, loss=0.6194318532943726
I0302 02:07:27.902282 140089770305280 logging_writer.py:48] [466900] global_step=466900, grad_norm=4.578591823577881, loss=0.6697492003440857
I0302 02:08:01.417209 140089500890880 logging_writer.py:48] [467000] global_step=467000, grad_norm=4.797104835510254, loss=0.6450032591819763
I0302 02:08:34.850223 140089770305280 logging_writer.py:48] [467100] global_step=467100, grad_norm=4.420647621154785, loss=0.6032986640930176
I0302 02:09:08.319449 140089500890880 logging_writer.py:48] [467200] global_step=467200, grad_norm=4.02714729309082, loss=0.5555852651596069
I0302 02:09:41.754657 140089770305280 logging_writer.py:48] [467300] global_step=467300, grad_norm=4.469738960266113, loss=0.6063689589500427
I0302 02:10:15.203596 140089500890880 logging_writer.py:48] [467400] global_step=467400, grad_norm=4.279595375061035, loss=0.6119756698608398
I0302 02:10:48.683175 140089770305280 logging_writer.py:48] [467500] global_step=467500, grad_norm=4.686001777648926, loss=0.6752864718437195
I0302 02:11:22.126035 140089500890880 logging_writer.py:48] [467600] global_step=467600, grad_norm=4.616176128387451, loss=0.7101924419403076
I0302 02:11:55.569237 140089770305280 logging_writer.py:48] [467700] global_step=467700, grad_norm=4.291232109069824, loss=0.6084422469139099
I0302 02:12:28.995858 140089500890880 logging_writer.py:48] [467800] global_step=467800, grad_norm=4.416436195373535, loss=0.6080046892166138
I0302 02:13:02.422950 140089770305280 logging_writer.py:48] [467900] global_step=467900, grad_norm=5.279143810272217, loss=0.6285330653190613
I0302 02:13:35.338857 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:13:41.431883 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:13:49.485419 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:13:51.778017 140252611495744 submission_runner.py:411] Time since start: 161972.66s, 	Step: 468000, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14747408032417297, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0522536039352417, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8407483100891113, 'test/num_examples': 10000, 'score': 156635.74769496918, 'total_duration': 161972.6580798626, 'accumulated_submission_time': 156635.74769496918, 'accumulated_eval_time': 5298.78008556366, 'accumulated_logging_time': 21.30401349067688}
I0302 02:13:51.871913 140089500890880 logging_writer.py:48] [468000] accumulated_eval_time=5298.780086, accumulated_logging_time=21.304013, accumulated_submission_time=156635.747695, global_step=468000, preemption_count=0, score=156635.747695, test/accuracy=0.627800, test/loss=1.840748, test/num_examples=10000, total_duration=161972.658080, train/accuracy=0.960419, train/loss=0.147474, validation/accuracy=0.755820, validation/loss=1.052254, validation/num_examples=50000
I0302 02:13:52.233842 140089770305280 logging_writer.py:48] [468000] global_step=468000, grad_norm=4.732604503631592, loss=0.5823144912719727
I0302 02:14:25.805026 140089500890880 logging_writer.py:48] [468100] global_step=468100, grad_norm=4.4909234046936035, loss=0.5545165538787842
I0302 02:14:59.272267 140089770305280 logging_writer.py:48] [468200] global_step=468200, grad_norm=4.532935619354248, loss=0.6191929578781128
I0302 02:15:32.719183 140089500890880 logging_writer.py:48] [468300] global_step=468300, grad_norm=4.486330032348633, loss=0.6597431898117065
I0302 02:16:06.167967 140089770305280 logging_writer.py:48] [468400] global_step=468400, grad_norm=4.671759605407715, loss=0.6520734429359436
I0302 02:16:39.619599 140089500890880 logging_writer.py:48] [468500] global_step=468500, grad_norm=4.411238670349121, loss=0.596019446849823
I0302 02:17:13.083873 140089770305280 logging_writer.py:48] [468600] global_step=468600, grad_norm=4.50742244720459, loss=0.5729475617408752
I0302 02:17:46.507332 140089500890880 logging_writer.py:48] [468700] global_step=468700, grad_norm=4.114225387573242, loss=0.5896535515785217
I0302 02:18:19.966790 140089770305280 logging_writer.py:48] [468800] global_step=468800, grad_norm=4.539186954498291, loss=0.6560014486312866
I0302 02:18:53.408561 140089500890880 logging_writer.py:48] [468900] global_step=468900, grad_norm=4.939082145690918, loss=0.6185413002967834
I0302 02:19:26.863730 140089770305280 logging_writer.py:48] [469000] global_step=469000, grad_norm=4.417056083679199, loss=0.563043475151062
I0302 02:20:00.331338 140089500890880 logging_writer.py:48] [469100] global_step=469100, grad_norm=5.1989569664001465, loss=0.6332033276557922
I0302 02:20:33.902053 140089770305280 logging_writer.py:48] [469200] global_step=469200, grad_norm=4.890387535095215, loss=0.6176552772521973
I0302 02:21:07.348083 140089500890880 logging_writer.py:48] [469300] global_step=469300, grad_norm=4.883161544799805, loss=0.7100532650947571
I0302 02:21:40.796797 140089770305280 logging_writer.py:48] [469400] global_step=469400, grad_norm=4.467641830444336, loss=0.5746277570724487
I0302 02:22:14.265532 140089500890880 logging_writer.py:48] [469500] global_step=469500, grad_norm=4.63516092300415, loss=0.6361098885536194
I0302 02:22:21.784893 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:22:27.865795 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:22:35.988582 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:22:38.319380 140252611495744 submission_runner.py:411] Time since start: 162499.20s, 	Step: 469524, 	{'train/accuracy': 0.9623923301696777, 'train/loss': 0.14375124871730804, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0532188415527344, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8413631916046143, 'test/num_examples': 10000, 'score': 157145.59330034256, 'total_duration': 162499.1994020939, 'accumulated_submission_time': 157145.59330034256, 'accumulated_eval_time': 5315.314467906952, 'accumulated_logging_time': 21.41022562980652}
I0302 02:22:38.420119 140089770305280 logging_writer.py:48] [469524] accumulated_eval_time=5315.314468, accumulated_logging_time=21.410226, accumulated_submission_time=157145.593300, global_step=469524, preemption_count=0, score=157145.593300, test/accuracy=0.627200, test/loss=1.841363, test/num_examples=10000, total_duration=162499.199402, train/accuracy=0.962392, train/loss=0.143751, validation/accuracy=0.755920, validation/loss=1.053219, validation/num_examples=50000
I0302 02:23:04.160485 140089778697984 logging_writer.py:48] [469600] global_step=469600, grad_norm=4.613445281982422, loss=0.6571112871170044
I0302 02:23:37.603880 140089770305280 logging_writer.py:48] [469700] global_step=469700, grad_norm=4.442294597625732, loss=0.636488139629364
I0302 02:24:11.073381 140089778697984 logging_writer.py:48] [469800] global_step=469800, grad_norm=4.491513252258301, loss=0.6752884387969971
I0302 02:24:44.515159 140089770305280 logging_writer.py:48] [469900] global_step=469900, grad_norm=4.347870349884033, loss=0.5731030702590942
I0302 02:25:17.944778 140089778697984 logging_writer.py:48] [470000] global_step=470000, grad_norm=4.143251895904541, loss=0.5876251459121704
I0302 02:25:51.399450 140089770305280 logging_writer.py:48] [470100] global_step=470100, grad_norm=4.417696952819824, loss=0.6335533857345581
I0302 02:26:24.929062 140089778697984 logging_writer.py:48] [470200] global_step=470200, grad_norm=4.713768482208252, loss=0.5946601629257202
I0302 02:26:58.352844 140089770305280 logging_writer.py:48] [470300] global_step=470300, grad_norm=4.603637218475342, loss=0.6627230644226074
I0302 02:27:31.818644 140089778697984 logging_writer.py:48] [470400] global_step=470400, grad_norm=4.398748397827148, loss=0.5894206762313843
I0302 02:28:05.254847 140089770305280 logging_writer.py:48] [470500] global_step=470500, grad_norm=4.773719787597656, loss=0.6483615636825562
I0302 02:28:38.722677 140089778697984 logging_writer.py:48] [470600] global_step=470600, grad_norm=4.669475078582764, loss=0.577737033367157
I0302 02:29:12.187745 140089770305280 logging_writer.py:48] [470700] global_step=470700, grad_norm=4.553305149078369, loss=0.6451445817947388
I0302 02:29:45.655850 140089778697984 logging_writer.py:48] [470800] global_step=470800, grad_norm=4.55416202545166, loss=0.6609609127044678
I0302 02:30:19.115015 140089770305280 logging_writer.py:48] [470900] global_step=470900, grad_norm=4.581080436706543, loss=0.558727502822876
I0302 02:30:52.547441 140089778697984 logging_writer.py:48] [471000] global_step=471000, grad_norm=4.1795125007629395, loss=0.5979570150375366
I0302 02:31:08.382540 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:31:14.488275 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:31:22.591315 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:31:24.815093 140252611495744 submission_runner.py:411] Time since start: 163025.70s, 	Step: 471049, 	{'train/accuracy': 0.9620934128761292, 'train/loss': 0.14252321422100067, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0534279346466064, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8422869443893433, 'test/num_examples': 10000, 'score': 157655.4903459549, 'total_duration': 163025.69514727592, 'accumulated_submission_time': 157655.4903459549, 'accumulated_eval_time': 5331.7469527721405, 'accumulated_logging_time': 21.521337509155273}
I0302 02:31:24.903435 140089862604544 logging_writer.py:48] [471049] accumulated_eval_time=5331.746953, accumulated_logging_time=21.521338, accumulated_submission_time=157655.490346, global_step=471049, preemption_count=0, score=157655.490346, test/accuracy=0.627600, test/loss=1.842287, test/num_examples=10000, total_duration=163025.695147, train/accuracy=0.962093, train/loss=0.142523, validation/accuracy=0.755740, validation/loss=1.053428, validation/num_examples=50000
I0302 02:31:42.272281 140089870997248 logging_writer.py:48] [471100] global_step=471100, grad_norm=4.6359477043151855, loss=0.7011051774024963
I0302 02:32:15.723839 140089862604544 logging_writer.py:48] [471200] global_step=471200, grad_norm=4.4065141677856445, loss=0.6030462384223938
I0302 02:32:49.340166 140089870997248 logging_writer.py:48] [471300] global_step=471300, grad_norm=4.237603187561035, loss=0.5455518364906311
I0302 02:33:22.786733 140089862604544 logging_writer.py:48] [471400] global_step=471400, grad_norm=4.480926036834717, loss=0.5807482004165649
I0302 02:33:56.243630 140089870997248 logging_writer.py:48] [471500] global_step=471500, grad_norm=4.772382736206055, loss=0.6045465469360352
I0302 02:34:29.658021 140089862604544 logging_writer.py:48] [471600] global_step=471600, grad_norm=4.945784568786621, loss=0.677668571472168
I0302 02:35:03.161633 140089870997248 logging_writer.py:48] [471700] global_step=471700, grad_norm=4.291858196258545, loss=0.5941752195358276
I0302 02:35:36.630249 140089862604544 logging_writer.py:48] [471800] global_step=471800, grad_norm=4.790320873260498, loss=0.6648250818252563
I0302 02:36:10.042297 140089870997248 logging_writer.py:48] [471900] global_step=471900, grad_norm=5.148594379425049, loss=0.5960042476654053
I0302 02:36:43.509756 140089862604544 logging_writer.py:48] [472000] global_step=472000, grad_norm=4.066617488861084, loss=0.596110463142395
I0302 02:37:16.931250 140089870997248 logging_writer.py:48] [472100] global_step=472100, grad_norm=4.971308708190918, loss=0.6172749400138855
I0302 02:37:50.385626 140089862604544 logging_writer.py:48] [472200] global_step=472200, grad_norm=5.686345100402832, loss=0.6052495837211609
I0302 02:38:23.935605 140089870997248 logging_writer.py:48] [472300] global_step=472300, grad_norm=4.744055271148682, loss=0.6236433386802673
I0302 02:38:57.345272 140089862604544 logging_writer.py:48] [472400] global_step=472400, grad_norm=4.4824628829956055, loss=0.5767115354537964
I0302 02:39:30.781245 140089870997248 logging_writer.py:48] [472500] global_step=472500, grad_norm=4.59298849105835, loss=0.5712928771972656
I0302 02:39:54.981529 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:40:01.225875 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:40:09.328187 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:40:11.599873 140252611495744 submission_runner.py:411] Time since start: 163552.48s, 	Step: 472574, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14650176465511322, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.054178237915039, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.8426686525344849, 'test/num_examples': 10000, 'score': 158165.50344014168, 'total_duration': 163552.47993898392, 'accumulated_submission_time': 158165.50344014168, 'accumulated_eval_time': 5348.365239858627, 'accumulated_logging_time': 21.620718955993652}
I0302 02:40:11.695827 140089770305280 logging_writer.py:48] [472574] accumulated_eval_time=5348.365240, accumulated_logging_time=21.620719, accumulated_submission_time=158165.503440, global_step=472574, preemption_count=0, score=158165.503440, test/accuracy=0.626600, test/loss=1.842669, test/num_examples=10000, total_duration=163552.479939, train/accuracy=0.961476, train/loss=0.146502, validation/accuracy=0.755920, validation/loss=1.054178, validation/num_examples=50000
I0302 02:40:20.738576 140089778697984 logging_writer.py:48] [472600] global_step=472600, grad_norm=4.249711036682129, loss=0.5652744174003601
I0302 02:40:54.184055 140089770305280 logging_writer.py:48] [472700] global_step=472700, grad_norm=4.535914897918701, loss=0.626618504524231
I0302 02:41:27.619907 140089778697984 logging_writer.py:48] [472800] global_step=472800, grad_norm=4.555213451385498, loss=0.6507633924484253
I0302 02:42:01.056930 140089770305280 logging_writer.py:48] [472900] global_step=472900, grad_norm=4.723311424255371, loss=0.6484627723693848
I0302 02:42:34.480822 140089778697984 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.540245056152344, loss=0.6141595840454102
I0302 02:43:07.925511 140089770305280 logging_writer.py:48] [473100] global_step=473100, grad_norm=4.600940704345703, loss=0.5976197123527527
I0302 02:43:41.345943 140089778697984 logging_writer.py:48] [473200] global_step=473200, grad_norm=4.698855876922607, loss=0.6665801405906677
I0302 02:44:14.844108 140089770305280 logging_writer.py:48] [473300] global_step=473300, grad_norm=4.338219165802002, loss=0.6364315748214722
I0302 02:44:48.383399 140089778697984 logging_writer.py:48] [473400] global_step=473400, grad_norm=4.551577568054199, loss=0.6852659583091736
I0302 02:45:21.809429 140089770305280 logging_writer.py:48] [473500] global_step=473500, grad_norm=4.023421764373779, loss=0.5540260672569275
I0302 02:45:55.249585 140089778697984 logging_writer.py:48] [473600] global_step=473600, grad_norm=4.8610920906066895, loss=0.6334350109100342
I0302 02:46:28.686652 140089770305280 logging_writer.py:48] [473700] global_step=473700, grad_norm=5.154351234436035, loss=0.6572825312614441
I0302 02:47:02.118015 140089778697984 logging_writer.py:48] [473800] global_step=473800, grad_norm=4.306107997894287, loss=0.5668279528617859
I0302 02:47:35.608155 140089770305280 logging_writer.py:48] [473900] global_step=473900, grad_norm=4.755309581756592, loss=0.6576799750328064
I0302 02:48:09.044243 140089778697984 logging_writer.py:48] [474000] global_step=474000, grad_norm=4.883881568908691, loss=0.6446908712387085
I0302 02:48:41.653030 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:48:47.730363 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:48:55.778618 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:48:58.113932 140252611495744 submission_runner.py:411] Time since start: 164078.99s, 	Step: 474099, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14438466727733612, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.0545673370361328, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.8445450067520142, 'test/num_examples': 10000, 'score': 158675.39512062073, 'total_duration': 164078.99399518967, 'accumulated_submission_time': 158675.39512062073, 'accumulated_eval_time': 5364.82608294487, 'accumulated_logging_time': 21.726683616638184}
I0302 02:48:58.210259 140089870997248 logging_writer.py:48] [474099] accumulated_eval_time=5364.826083, accumulated_logging_time=21.726684, accumulated_submission_time=158675.395121, global_step=474099, preemption_count=0, score=158675.395121, test/accuracy=0.626000, test/loss=1.844545, test/num_examples=10000, total_duration=164078.993995, train/accuracy=0.960599, train/loss=0.144385, validation/accuracy=0.756180, validation/loss=1.054567, validation/num_examples=50000
I0302 02:48:58.896078 140089879389952 logging_writer.py:48] [474100] global_step=474100, grad_norm=5.156649589538574, loss=0.704456090927124
I0302 02:49:32.311813 140089870997248 logging_writer.py:48] [474200] global_step=474200, grad_norm=4.602357864379883, loss=0.6430379748344421
I0302 02:50:05.767060 140089879389952 logging_writer.py:48] [474300] global_step=474300, grad_norm=5.2178168296813965, loss=0.6282569169998169
I0302 02:50:39.351260 140089870997248 logging_writer.py:48] [474400] global_step=474400, grad_norm=4.441882610321045, loss=0.5728402733802795
I0302 02:51:12.814137 140089879389952 logging_writer.py:48] [474500] global_step=474500, grad_norm=4.679932594299316, loss=0.6148781776428223
I0302 02:51:46.261266 140089870997248 logging_writer.py:48] [474600] global_step=474600, grad_norm=5.172928810119629, loss=0.6430781483650208
I0302 02:52:19.698640 140089879389952 logging_writer.py:48] [474700] global_step=474700, grad_norm=5.0965118408203125, loss=0.6214455366134644
I0302 02:52:53.145750 140089870997248 logging_writer.py:48] [474800] global_step=474800, grad_norm=4.1203484535217285, loss=0.5834630131721497
I0302 02:53:26.589745 140089879389952 logging_writer.py:48] [474900] global_step=474900, grad_norm=4.770991325378418, loss=0.6472944021224976
I0302 02:54:00.036260 140089870997248 logging_writer.py:48] [475000] global_step=475000, grad_norm=4.468679904937744, loss=0.6450190544128418
I0302 02:54:33.465908 140089879389952 logging_writer.py:48] [475100] global_step=475100, grad_norm=4.357307434082031, loss=0.5809594392776489
I0302 02:55:06.919138 140089870997248 logging_writer.py:48] [475200] global_step=475200, grad_norm=4.942885398864746, loss=0.6509994268417358
I0302 02:55:40.329516 140089879389952 logging_writer.py:48] [475300] global_step=475300, grad_norm=4.184756278991699, loss=0.6589055061340332
I0302 02:56:13.776558 140089870997248 logging_writer.py:48] [475400] global_step=475400, grad_norm=4.506003379821777, loss=0.6819950342178345
I0302 02:56:47.320224 140089879389952 logging_writer.py:48] [475500] global_step=475500, grad_norm=4.552230358123779, loss=0.598364531993866
I0302 02:57:20.752836 140089870997248 logging_writer.py:48] [475600] global_step=475600, grad_norm=4.442284107208252, loss=0.6289027333259583
I0302 02:57:28.245221 140252611495744 spec.py:321] Evaluating on the training split.
I0302 02:57:34.438174 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 02:57:42.515242 140252611495744 spec.py:349] Evaluating on the test split.
I0302 02:57:44.712789 140252611495744 submission_runner.py:411] Time since start: 164605.59s, 	Step: 475624, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14537644386291504, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.0535876750946045, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8428512811660767, 'test/num_examples': 10000, 'score': 159185.36111426353, 'total_duration': 164605.59284853935, 'accumulated_submission_time': 159185.36111426353, 'accumulated_eval_time': 5381.293584108353, 'accumulated_logging_time': 21.83575701713562}
I0302 02:57:44.803142 140089778697984 logging_writer.py:48] [475624] accumulated_eval_time=5381.293584, accumulated_logging_time=21.835757, accumulated_submission_time=159185.361114, global_step=475624, preemption_count=0, score=159185.361114, test/accuracy=0.627600, test/loss=1.842851, test/num_examples=10000, total_duration=164605.592849, train/accuracy=0.961735, train/loss=0.145376, validation/accuracy=0.756100, validation/loss=1.053588, validation/num_examples=50000
I0302 02:58:10.568658 140089837426432 logging_writer.py:48] [475700] global_step=475700, grad_norm=4.481766700744629, loss=0.6612792015075684
I0302 02:58:44.023165 140089778697984 logging_writer.py:48] [475800] global_step=475800, grad_norm=4.341200828552246, loss=0.6009702086448669
I0302 02:59:17.441899 140089837426432 logging_writer.py:48] [475900] global_step=475900, grad_norm=5.3606061935424805, loss=0.6218043565750122
I0302 02:59:50.910238 140089778697984 logging_writer.py:48] [476000] global_step=476000, grad_norm=4.410635471343994, loss=0.6375896334648132
I0302 03:00:24.321596 140089837426432 logging_writer.py:48] [476100] global_step=476100, grad_norm=4.406060218811035, loss=0.5907555222511292
I0302 03:00:57.797524 140089778697984 logging_writer.py:48] [476200] global_step=476200, grad_norm=4.919465065002441, loss=0.704003095626831
I0302 03:01:31.247511 140089837426432 logging_writer.py:48] [476300] global_step=476300, grad_norm=4.296731472015381, loss=0.5266770124435425
I0302 03:02:04.682314 140089778697984 logging_writer.py:48] [476400] global_step=476400, grad_norm=5.275103569030762, loss=0.6987466812133789
I0302 03:02:38.285415 140089837426432 logging_writer.py:48] [476500] global_step=476500, grad_norm=4.682382583618164, loss=0.5931073427200317
I0302 03:03:11.718479 140089778697984 logging_writer.py:48] [476600] global_step=476600, grad_norm=4.416539669036865, loss=0.6420961618423462
I0302 03:03:45.190180 140089837426432 logging_writer.py:48] [476700] global_step=476700, grad_norm=4.317212104797363, loss=0.5409175157546997
I0302 03:04:18.648440 140089778697984 logging_writer.py:48] [476800] global_step=476800, grad_norm=4.707351207733154, loss=0.5669739842414856
I0302 03:04:52.090859 140089837426432 logging_writer.py:48] [476900] global_step=476900, grad_norm=4.352132320404053, loss=0.650280773639679
I0302 03:05:25.555101 140089778697984 logging_writer.py:48] [477000] global_step=477000, grad_norm=5.006165504455566, loss=0.6174038648605347
I0302 03:05:58.990818 140089837426432 logging_writer.py:48] [477100] global_step=477100, grad_norm=4.482156753540039, loss=0.6108023524284363
I0302 03:06:14.878713 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:06:21.035853 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:06:29.270422 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:06:31.572543 140252611495744 submission_runner.py:411] Time since start: 165132.45s, 	Step: 477149, 	{'train/accuracy': 0.9620137214660645, 'train/loss': 0.14198219776153564, 'validation/accuracy': 0.756339967250824, 'validation/loss': 1.0527808666229248, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8418272733688354, 'test/num_examples': 10000, 'score': 159695.37145113945, 'total_duration': 165132.4526028633, 'accumulated_submission_time': 159695.37145113945, 'accumulated_eval_time': 5397.987356424332, 'accumulated_logging_time': 21.93658208847046}
I0302 03:06:31.663723 140089500890880 logging_writer.py:48] [477149] accumulated_eval_time=5397.987356, accumulated_logging_time=21.936582, accumulated_submission_time=159695.371451, global_step=477149, preemption_count=0, score=159695.371451, test/accuracy=0.627200, test/loss=1.841827, test/num_examples=10000, total_duration=165132.452603, train/accuracy=0.962014, train/loss=0.141982, validation/accuracy=0.756340, validation/loss=1.052781, validation/num_examples=50000
I0302 03:06:49.052898 140089770305280 logging_writer.py:48] [477200] global_step=477200, grad_norm=4.553152561187744, loss=0.6592815518379211
I0302 03:07:22.500541 140089500890880 logging_writer.py:48] [477300] global_step=477300, grad_norm=4.794081211090088, loss=0.6798486709594727
I0302 03:07:55.969193 140089770305280 logging_writer.py:48] [477400] global_step=477400, grad_norm=4.641725540161133, loss=0.7179852724075317
I0302 03:08:29.399678 140089500890880 logging_writer.py:48] [477500] global_step=477500, grad_norm=4.625769138336182, loss=0.5771540999412537
I0302 03:09:02.933910 140089770305280 logging_writer.py:48] [477600] global_step=477600, grad_norm=4.813400745391846, loss=0.6883118152618408
I0302 03:09:36.367273 140089500890880 logging_writer.py:48] [477700] global_step=477700, grad_norm=4.536417007446289, loss=0.5747119188308716
I0302 03:10:09.815166 140089770305280 logging_writer.py:48] [477800] global_step=477800, grad_norm=4.598731517791748, loss=0.6366216540336609
I0302 03:10:43.245184 140089500890880 logging_writer.py:48] [477900] global_step=477900, grad_norm=4.351029396057129, loss=0.6163124442100525
I0302 03:11:16.720559 140089770305280 logging_writer.py:48] [478000] global_step=478000, grad_norm=4.1369099617004395, loss=0.6049594879150391
I0302 03:11:50.139212 140089500890880 logging_writer.py:48] [478100] global_step=478100, grad_norm=4.860081195831299, loss=0.6266094446182251
I0302 03:12:23.599761 140089770305280 logging_writer.py:48] [478200] global_step=478200, grad_norm=4.604630947113037, loss=0.6415016055107117
I0302 03:12:57.045106 140089500890880 logging_writer.py:48] [478300] global_step=478300, grad_norm=4.261404514312744, loss=0.6221091151237488
I0302 03:13:30.469941 140089770305280 logging_writer.py:48] [478400] global_step=478400, grad_norm=4.873384475708008, loss=0.6598206758499146
I0302 03:14:03.912324 140089500890880 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.403467178344727, loss=0.5391965508460999
I0302 03:14:37.356308 140089770305280 logging_writer.py:48] [478600] global_step=478600, grad_norm=4.753547668457031, loss=0.6579382419586182
I0302 03:15:01.638736 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:15:07.945888 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:15:15.879699 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:15:18.149309 140252611495744 submission_runner.py:411] Time since start: 165659.03s, 	Step: 478674, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14745572209358215, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.052963137626648, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8423027992248535, 'test/num_examples': 10000, 'score': 160205.27926325798, 'total_duration': 165659.02926802635, 'accumulated_submission_time': 160205.27926325798, 'accumulated_eval_time': 5414.497764825821, 'accumulated_logging_time': 22.03964066505432}
I0302 03:15:18.247663 140089770305280 logging_writer.py:48] [478674] accumulated_eval_time=5414.497765, accumulated_logging_time=22.039641, accumulated_submission_time=160205.279263, global_step=478674, preemption_count=0, score=160205.279263, test/accuracy=0.627900, test/loss=1.842303, test/num_examples=10000, total_duration=165659.029268, train/accuracy=0.960001, train/loss=0.147456, validation/accuracy=0.756180, validation/loss=1.052963, validation/num_examples=50000
I0302 03:15:27.283458 140089845819136 logging_writer.py:48] [478700] global_step=478700, grad_norm=4.385972023010254, loss=0.5980126261711121
I0302 03:16:00.998947 140089770305280 logging_writer.py:48] [478800] global_step=478800, grad_norm=4.325803279876709, loss=0.5846556425094604
I0302 03:16:34.481637 140089845819136 logging_writer.py:48] [478900] global_step=478900, grad_norm=4.653346061706543, loss=0.658161997795105
I0302 03:17:07.955154 140089770305280 logging_writer.py:48] [479000] global_step=479000, grad_norm=4.693952560424805, loss=0.6241458654403687
I0302 03:17:41.372987 140089845819136 logging_writer.py:48] [479100] global_step=479100, grad_norm=4.201583385467529, loss=0.5801030993461609
I0302 03:18:14.848008 140089770305280 logging_writer.py:48] [479200] global_step=479200, grad_norm=4.707737445831299, loss=0.6104337573051453
I0302 03:18:48.268851 140089845819136 logging_writer.py:48] [479300] global_step=479300, grad_norm=4.62013578414917, loss=0.6723325848579407
I0302 03:19:21.730829 140089770305280 logging_writer.py:48] [479400] global_step=479400, grad_norm=4.827011585235596, loss=0.639616847038269
I0302 03:19:55.185939 140089845819136 logging_writer.py:48] [479500] global_step=479500, grad_norm=4.482436656951904, loss=0.6573847532272339
I0302 03:20:28.607286 140089770305280 logging_writer.py:48] [479600] global_step=479600, grad_norm=4.523420810699463, loss=0.6270629167556763
I0302 03:21:02.170960 140089845819136 logging_writer.py:48] [479700] global_step=479700, grad_norm=4.654969692230225, loss=0.6008899807929993
I0302 03:21:35.627281 140089770305280 logging_writer.py:48] [479800] global_step=479800, grad_norm=4.8226704597473145, loss=0.6609899997711182
I0302 03:22:09.091634 140089845819136 logging_writer.py:48] [479900] global_step=479900, grad_norm=4.328130722045898, loss=0.626693844795227
I0302 03:22:42.526260 140089770305280 logging_writer.py:48] [480000] global_step=480000, grad_norm=4.6186747550964355, loss=0.6011903285980225
I0302 03:23:15.971891 140089845819136 logging_writer.py:48] [480100] global_step=480100, grad_norm=5.004117965698242, loss=0.6488529443740845
I0302 03:23:48.209813 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:23:54.311295 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:24:02.516346 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:24:04.828211 140252611495744 submission_runner.py:411] Time since start: 166185.71s, 	Step: 480198, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.14765025675296783, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.053896188735962, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8432202339172363, 'test/num_examples': 10000, 'score': 160715.17672729492, 'total_duration': 166185.70827054977, 'accumulated_submission_time': 160715.17672729492, 'accumulated_eval_time': 5431.116099834442, 'accumulated_logging_time': 22.14845061302185}
I0302 03:24:04.918001 140089770305280 logging_writer.py:48] [480198] accumulated_eval_time=5431.116100, accumulated_logging_time=22.148451, accumulated_submission_time=160715.176727, global_step=480198, preemption_count=0, score=160715.176727, test/accuracy=0.627700, test/loss=1.843220, test/num_examples=10000, total_duration=166185.708271, train/accuracy=0.960499, train/loss=0.147650, validation/accuracy=0.756200, validation/loss=1.053896, validation/num_examples=50000
I0302 03:24:05.935923 140089778697984 logging_writer.py:48] [480200] global_step=480200, grad_norm=4.326776027679443, loss=0.6261751055717468
I0302 03:24:39.386763 140089770305280 logging_writer.py:48] [480300] global_step=480300, grad_norm=4.200583457946777, loss=0.6236304044723511
I0302 03:25:12.852101 140089778697984 logging_writer.py:48] [480400] global_step=480400, grad_norm=4.537783145904541, loss=0.5784264206886292
I0302 03:25:46.277337 140089770305280 logging_writer.py:48] [480500] global_step=480500, grad_norm=4.564732551574707, loss=0.6041374206542969
I0302 03:26:19.716542 140089778697984 logging_writer.py:48] [480600] global_step=480600, grad_norm=4.54239559173584, loss=0.613304615020752
I0302 03:26:53.155003 140089770305280 logging_writer.py:48] [480700] global_step=480700, grad_norm=4.555822372436523, loss=0.6391679644584656
I0302 03:27:26.659872 140089778697984 logging_writer.py:48] [480800] global_step=480800, grad_norm=4.647111892700195, loss=0.6188717484474182
I0302 03:28:00.102374 140089770305280 logging_writer.py:48] [480900] global_step=480900, grad_norm=4.347183704376221, loss=0.6037657856941223
I0302 03:28:33.548445 140089778697984 logging_writer.py:48] [481000] global_step=481000, grad_norm=4.436166286468506, loss=0.6709162592887878
I0302 03:29:06.993459 140089770305280 logging_writer.py:48] [481100] global_step=481100, grad_norm=4.610709190368652, loss=0.6242263317108154
I0302 03:29:40.428756 140089778697984 logging_writer.py:48] [481200] global_step=481200, grad_norm=4.810281753540039, loss=0.7132997512817383
I0302 03:30:13.867788 140089770305280 logging_writer.py:48] [481300] global_step=481300, grad_norm=4.431840419769287, loss=0.6277058124542236
I0302 03:30:47.293822 140089778697984 logging_writer.py:48] [481400] global_step=481400, grad_norm=4.4909257888793945, loss=0.6779635548591614
I0302 03:31:20.729823 140089770305280 logging_writer.py:48] [481500] global_step=481500, grad_norm=4.514211177825928, loss=0.627020537853241
I0302 03:31:54.164047 140089778697984 logging_writer.py:48] [481600] global_step=481600, grad_norm=4.885186672210693, loss=0.6792299747467041
I0302 03:32:27.615133 140089770305280 logging_writer.py:48] [481700] global_step=481700, grad_norm=4.407822608947754, loss=0.5812305808067322
I0302 03:32:35.129238 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:32:41.305590 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:32:49.394808 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:32:51.710228 140252611495744 submission_runner.py:411] Time since start: 166712.59s, 	Step: 481724, 	{'train/accuracy': 0.9612364172935486, 'train/loss': 0.14400172233581543, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0537447929382324, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8416016101837158, 'test/num_examples': 10000, 'score': 161225.3224759102, 'total_duration': 166712.59029626846, 'accumulated_submission_time': 161225.3224759102, 'accumulated_eval_time': 5447.69705748558, 'accumulated_logging_time': 22.248322248458862}
I0302 03:32:51.808253 140089845819136 logging_writer.py:48] [481724] accumulated_eval_time=5447.697057, accumulated_logging_time=22.248322, accumulated_submission_time=161225.322476, global_step=481724, preemption_count=0, score=161225.322476, test/accuracy=0.627100, test/loss=1.841602, test/num_examples=10000, total_duration=166712.590296, train/accuracy=0.961236, train/loss=0.144002, validation/accuracy=0.755860, validation/loss=1.053745, validation/num_examples=50000
I0302 03:33:17.691123 140089862604544 logging_writer.py:48] [481800] global_step=481800, grad_norm=4.764272689819336, loss=0.6987426280975342
I0302 03:33:51.106567 140089845819136 logging_writer.py:48] [481900] global_step=481900, grad_norm=5.054962635040283, loss=0.5818503499031067
I0302 03:34:24.540183 140089862604544 logging_writer.py:48] [482000] global_step=482000, grad_norm=4.382037162780762, loss=0.649630069732666
I0302 03:34:57.981388 140089845819136 logging_writer.py:48] [482100] global_step=482100, grad_norm=4.581539154052734, loss=0.6347486972808838
I0302 03:35:31.420193 140089862604544 logging_writer.py:48] [482200] global_step=482200, grad_norm=4.106544494628906, loss=0.5831294059753418
I0302 03:36:04.874669 140089845819136 logging_writer.py:48] [482300] global_step=482300, grad_norm=4.652368545532227, loss=0.6055155992507935
I0302 03:36:38.301923 140089862604544 logging_writer.py:48] [482400] global_step=482400, grad_norm=4.3220720291137695, loss=0.5625336766242981
I0302 03:37:11.757682 140089845819136 logging_writer.py:48] [482500] global_step=482500, grad_norm=4.448873996734619, loss=0.6197338104248047
I0302 03:37:45.183764 140089862604544 logging_writer.py:48] [482600] global_step=482600, grad_norm=5.067746162414551, loss=0.6728646755218506
I0302 03:38:18.645870 140089845819136 logging_writer.py:48] [482700] global_step=482700, grad_norm=4.498079299926758, loss=0.6632516384124756
I0302 03:38:52.062527 140089862604544 logging_writer.py:48] [482800] global_step=482800, grad_norm=4.599931716918945, loss=0.6663070917129517
I0302 03:39:25.585666 140089845819136 logging_writer.py:48] [482900] global_step=482900, grad_norm=4.748002529144287, loss=0.7111116051673889
I0302 03:39:59.029882 140089862604544 logging_writer.py:48] [483000] global_step=483000, grad_norm=4.402041435241699, loss=0.6155368089675903
I0302 03:40:32.441315 140089845819136 logging_writer.py:48] [483100] global_step=483100, grad_norm=4.3389177322387695, loss=0.576324999332428
I0302 03:41:05.898973 140089862604544 logging_writer.py:48] [483200] global_step=483200, grad_norm=4.796207427978516, loss=0.6706292629241943
I0302 03:41:21.731647 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:41:27.915855 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:41:35.954294 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:41:38.216444 140252611495744 submission_runner.py:411] Time since start: 167239.10s, 	Step: 483249, 	{'train/accuracy': 0.9628507494926453, 'train/loss': 0.14296504855155945, 'validation/accuracy': 0.7554599642753601, 'validation/loss': 1.0544278621673584, 'validation/num_examples': 50000, 'test/accuracy': 0.626300036907196, 'test/loss': 1.843042254447937, 'test/num_examples': 10000, 'score': 161735.1811146736, 'total_duration': 167239.09651231766, 'accumulated_submission_time': 161735.1811146736, 'accumulated_eval_time': 5464.181796789169, 'accumulated_logging_time': 22.356643676757812}
I0302 03:41:38.319491 140089770305280 logging_writer.py:48] [483249] accumulated_eval_time=5464.181797, accumulated_logging_time=22.356644, accumulated_submission_time=161735.181115, global_step=483249, preemption_count=0, score=161735.181115, test/accuracy=0.626300, test/loss=1.843042, test/num_examples=10000, total_duration=167239.096512, train/accuracy=0.962851, train/loss=0.142965, validation/accuracy=0.755460, validation/loss=1.054428, validation/num_examples=50000
I0302 03:41:55.693158 140089837426432 logging_writer.py:48] [483300] global_step=483300, grad_norm=4.8175883293151855, loss=0.6129125356674194
I0302 03:42:29.130443 140089770305280 logging_writer.py:48] [483400] global_step=483400, grad_norm=4.475192070007324, loss=0.6794339418411255
I0302 03:43:02.557859 140089837426432 logging_writer.py:48] [483500] global_step=483500, grad_norm=4.4731245040893555, loss=0.5965075492858887
I0302 03:43:36.010298 140089770305280 logging_writer.py:48] [483600] global_step=483600, grad_norm=4.121832847595215, loss=0.6185705661773682
I0302 03:44:09.453074 140089837426432 logging_writer.py:48] [483700] global_step=483700, grad_norm=4.558166027069092, loss=0.6209872961044312
I0302 03:44:42.912934 140089770305280 logging_writer.py:48] [483800] global_step=483800, grad_norm=4.488070487976074, loss=0.6012470722198486
I0302 03:45:16.371074 140089837426432 logging_writer.py:48] [483900] global_step=483900, grad_norm=4.865152835845947, loss=0.6321232914924622
I0302 03:45:49.904489 140089770305280 logging_writer.py:48] [484000] global_step=484000, grad_norm=4.713775157928467, loss=0.6417858004570007
I0302 03:46:23.332290 140089837426432 logging_writer.py:48] [484100] global_step=484100, grad_norm=4.396047115325928, loss=0.6512894630432129
I0302 03:46:56.774598 140089770305280 logging_writer.py:48] [484200] global_step=484200, grad_norm=4.566868305206299, loss=0.679693341255188
I0302 03:47:30.226138 140089837426432 logging_writer.py:48] [484300] global_step=484300, grad_norm=4.660645008087158, loss=0.5394303798675537
I0302 03:48:03.693476 140089770305280 logging_writer.py:48] [484400] global_step=484400, grad_norm=5.285971164703369, loss=0.7011999487876892
I0302 03:48:37.164108 140089837426432 logging_writer.py:48] [484500] global_step=484500, grad_norm=5.091409206390381, loss=0.6468225717544556
I0302 03:49:10.624816 140089770305280 logging_writer.py:48] [484600] global_step=484600, grad_norm=5.018603801727295, loss=0.6427838206291199
I0302 03:49:44.101049 140089837426432 logging_writer.py:48] [484700] global_step=484700, grad_norm=4.506955146789551, loss=0.5953276753425598
I0302 03:50:08.294569 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:50:14.383221 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:50:22.436555 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:50:24.706454 140252611495744 submission_runner.py:411] Time since start: 167765.59s, 	Step: 484774, 	{'train/accuracy': 0.9619140625, 'train/loss': 0.14586769044399261, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0542570352554321, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8424336910247803, 'test/num_examples': 10000, 'score': 162245.09081196785, 'total_duration': 167765.5865190029, 'accumulated_submission_time': 162245.09081196785, 'accumulated_eval_time': 5480.593630313873, 'accumulated_logging_time': 22.470579624176025}
I0302 03:50:24.806273 140089770305280 logging_writer.py:48] [484774] accumulated_eval_time=5480.593630, accumulated_logging_time=22.470580, accumulated_submission_time=162245.090812, global_step=484774, preemption_count=0, score=162245.090812, test/accuracy=0.628000, test/loss=1.842434, test/num_examples=10000, total_duration=167765.586519, train/accuracy=0.961914, train/loss=0.145868, validation/accuracy=0.755900, validation/loss=1.054257, validation/num_examples=50000
I0302 03:50:33.843975 140089845819136 logging_writer.py:48] [484800] global_step=484800, grad_norm=5.137113571166992, loss=0.5861384868621826
I0302 03:51:07.266800 140089770305280 logging_writer.py:48] [484900] global_step=484900, grad_norm=4.518746376037598, loss=0.6540455222129822
I0302 03:51:40.798195 140089845819136 logging_writer.py:48] [485000] global_step=485000, grad_norm=4.524062633514404, loss=0.6834068298339844
I0302 03:52:14.236199 140089770305280 logging_writer.py:48] [485100] global_step=485100, grad_norm=4.344922065734863, loss=0.5597814321517944
I0302 03:52:47.667130 140089845819136 logging_writer.py:48] [485200] global_step=485200, grad_norm=4.971452236175537, loss=0.6578689217567444
I0302 03:53:21.120377 140089770305280 logging_writer.py:48] [485300] global_step=485300, grad_norm=4.719561576843262, loss=0.6753786206245422
I0302 03:53:54.528898 140089845819136 logging_writer.py:48] [485400] global_step=485400, grad_norm=4.476355075836182, loss=0.5994179248809814
I0302 03:54:27.988478 140089770305280 logging_writer.py:48] [485500] global_step=485500, grad_norm=4.412686347961426, loss=0.5506821870803833
I0302 03:55:01.436036 140089845819136 logging_writer.py:48] [485600] global_step=485600, grad_norm=4.811783790588379, loss=0.6801248788833618
I0302 03:55:34.854452 140089770305280 logging_writer.py:48] [485700] global_step=485700, grad_norm=4.338679790496826, loss=0.6522454023361206
I0302 03:56:08.319655 140089845819136 logging_writer.py:48] [485800] global_step=485800, grad_norm=4.60542631149292, loss=0.6489171981811523
I0302 03:56:41.763091 140089770305280 logging_writer.py:48] [485900] global_step=485900, grad_norm=4.886907577514648, loss=0.612464964389801
I0302 03:57:15.210132 140089845819136 logging_writer.py:48] [486000] global_step=486000, grad_norm=5.184148788452148, loss=0.7427889704704285
I0302 03:57:48.758028 140089770305280 logging_writer.py:48] [486100] global_step=486100, grad_norm=4.4085001945495605, loss=0.6154285073280334
I0302 03:58:22.209006 140089845819136 logging_writer.py:48] [486200] global_step=486200, grad_norm=4.990835189819336, loss=0.6420630216598511
I0302 03:58:54.751476 140252611495744 spec.py:321] Evaluating on the training split.
I0302 03:59:00.899451 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 03:59:09.131878 140252611495744 spec.py:349] Evaluating on the test split.
I0302 03:59:11.423719 140252611495744 submission_runner.py:411] Time since start: 168292.30s, 	Step: 486299, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.1485474854707718, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0532896518707275, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8425171375274658, 'test/num_examples': 10000, 'score': 162754.97107815742, 'total_duration': 168292.3037648201, 'accumulated_submission_time': 162754.97107815742, 'accumulated_eval_time': 5497.265798330307, 'accumulated_logging_time': 22.580674409866333}
I0302 03:59:11.523331 140089770305280 logging_writer.py:48] [486299] accumulated_eval_time=5497.265798, accumulated_logging_time=22.580674, accumulated_submission_time=162754.971078, global_step=486299, preemption_count=0, score=162754.971078, test/accuracy=0.627400, test/loss=1.842517, test/num_examples=10000, total_duration=168292.303765, train/accuracy=0.959762, train/loss=0.148547, validation/accuracy=0.755920, validation/loss=1.053290, validation/num_examples=50000
I0302 03:59:12.209065 140089778697984 logging_writer.py:48] [486300] global_step=486300, grad_norm=4.784488677978516, loss=0.6304186582565308
I0302 03:59:45.618294 140089770305280 logging_writer.py:48] [486400] global_step=486400, grad_norm=5.15336275100708, loss=0.6280092000961304
I0302 04:00:19.062767 140089778697984 logging_writer.py:48] [486500] global_step=486500, grad_norm=4.784456253051758, loss=0.6475598812103271
I0302 04:00:52.511119 140089770305280 logging_writer.py:48] [486600] global_step=486600, grad_norm=4.624556541442871, loss=0.7188043594360352
I0302 04:01:25.943624 140089778697984 logging_writer.py:48] [486700] global_step=486700, grad_norm=4.7032647132873535, loss=0.6347091197967529
I0302 04:01:59.373516 140089770305280 logging_writer.py:48] [486800] global_step=486800, grad_norm=4.172755241394043, loss=0.5660377740859985
I0302 04:02:32.801918 140089778697984 logging_writer.py:48] [486900] global_step=486900, grad_norm=4.580259323120117, loss=0.6463850736618042
I0302 04:03:06.234651 140089770305280 logging_writer.py:48] [487000] global_step=487000, grad_norm=4.579747676849365, loss=0.6170706748962402
I0302 04:03:39.652092 140089778697984 logging_writer.py:48] [487100] global_step=487100, grad_norm=4.451083183288574, loss=0.5433897376060486
I0302 04:04:13.180380 140089770305280 logging_writer.py:48] [487200] global_step=487200, grad_norm=4.7787957191467285, loss=0.6881755590438843
I0302 04:04:46.604434 140089778697984 logging_writer.py:48] [487300] global_step=487300, grad_norm=4.4080023765563965, loss=0.5806461572647095
I0302 04:05:20.046388 140089770305280 logging_writer.py:48] [487400] global_step=487400, grad_norm=4.605834007263184, loss=0.6496751308441162
I0302 04:05:53.474002 140089778697984 logging_writer.py:48] [487500] global_step=487500, grad_norm=4.5593719482421875, loss=0.6737340092658997
I0302 04:06:26.906807 140089770305280 logging_writer.py:48] [487600] global_step=487600, grad_norm=4.998461723327637, loss=0.6085038185119629
I0302 04:07:00.323659 140089778697984 logging_writer.py:48] [487700] global_step=487700, grad_norm=4.425297737121582, loss=0.612244725227356
I0302 04:07:33.730976 140089770305280 logging_writer.py:48] [487800] global_step=487800, grad_norm=4.530332565307617, loss=0.6744950413703918
I0302 04:07:41.559405 140252611495744 spec.py:321] Evaluating on the training split.
I0302 04:07:47.672842 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 04:07:55.767254 140252611495744 spec.py:349] Evaluating on the test split.
I0302 04:07:58.059645 140252611495744 submission_runner.py:411] Time since start: 168818.94s, 	Step: 487825, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14682205021381378, 'validation/accuracy': 0.7554999589920044, 'validation/loss': 1.0537186861038208, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8440321683883667, 'test/num_examples': 10000, 'score': 163264.93875932693, 'total_duration': 168818.93969726562, 'accumulated_submission_time': 163264.93875932693, 'accumulated_eval_time': 5513.765965938568, 'accumulated_logging_time': 22.693618059158325}
I0302 04:07:58.157636 140089770305280 logging_writer.py:48] [487825] accumulated_eval_time=5513.765966, accumulated_logging_time=22.693618, accumulated_submission_time=163264.938759, global_step=487825, preemption_count=0, score=163264.938759, test/accuracy=0.627100, test/loss=1.844032, test/num_examples=10000, total_duration=168818.939697, train/accuracy=0.959801, train/loss=0.146822, validation/accuracy=0.755500, validation/loss=1.053719, validation/num_examples=50000
I0302 04:08:23.575918 140089862604544 logging_writer.py:48] [487900] global_step=487900, grad_norm=4.424835681915283, loss=0.5650690197944641
I0302 04:08:57.035500 140089770305280 logging_writer.py:48] [488000] global_step=488000, grad_norm=5.002386093139648, loss=0.6325163245201111
I0302 04:09:30.478080 140089862604544 logging_writer.py:48] [488100] global_step=488100, grad_norm=4.798676490783691, loss=0.6578494310379028
I0302 04:10:04.015922 140089770305280 logging_writer.py:48] [488200] global_step=488200, grad_norm=4.5579423904418945, loss=0.6117449402809143
I0302 04:10:37.462818 140089862604544 logging_writer.py:48] [488300] global_step=488300, grad_norm=5.009601593017578, loss=0.6072334051132202
I0302 04:11:10.901397 140089770305280 logging_writer.py:48] [488400] global_step=488400, grad_norm=4.3198933601379395, loss=0.5842835307121277
I0302 04:11:44.348527 140089862604544 logging_writer.py:48] [488500] global_step=488500, grad_norm=4.490044116973877, loss=0.6580755710601807
I0302 04:12:17.780349 140089770305280 logging_writer.py:48] [488600] global_step=488600, grad_norm=4.6085896492004395, loss=0.6425648927688599
I0302 04:12:51.227669 140089862604544 logging_writer.py:48] [488700] global_step=488700, grad_norm=4.861237525939941, loss=0.6569897532463074
I0302 04:13:24.678568 140089770305280 logging_writer.py:48] [488800] global_step=488800, grad_norm=4.667007923126221, loss=0.7408770322799683
I0302 04:13:58.120075 140089862604544 logging_writer.py:48] [488900] global_step=488900, grad_norm=4.814974784851074, loss=0.6157671213150024
I0302 04:14:31.558164 140089770305280 logging_writer.py:48] [489000] global_step=489000, grad_norm=4.767084121704102, loss=0.6511238813400269
I0302 04:15:04.996484 140089862604544 logging_writer.py:48] [489100] global_step=489100, grad_norm=4.3679423332214355, loss=0.5849635004997253
I0302 04:15:38.465055 140089770305280 logging_writer.py:48] [489200] global_step=489200, grad_norm=4.442047595977783, loss=0.5741381645202637
I0302 04:16:11.989855 140089862604544 logging_writer.py:48] [489300] global_step=489300, grad_norm=4.670323848724365, loss=0.643573522567749
I0302 04:16:28.211086 140252611495744 spec.py:321] Evaluating on the training split.
I0302 04:16:34.339715 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 04:16:42.522039 140252611495744 spec.py:349] Evaluating on the test split.
I0302 04:16:44.794437 140252611495744 submission_runner.py:411] Time since start: 169345.67s, 	Step: 489350, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.1468583345413208, 'validation/accuracy': 0.7562400102615356, 'validation/loss': 1.0533497333526611, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8408963680267334, 'test/num_examples': 10000, 'score': 163774.9257595539, 'total_duration': 169345.6744902134, 'accumulated_submission_time': 163774.9257595539, 'accumulated_eval_time': 5530.3492596149445, 'accumulated_logging_time': 22.80390238761902}
I0302 04:16:44.893793 140089770305280 logging_writer.py:48] [489350] accumulated_eval_time=5530.349260, accumulated_logging_time=22.803902, accumulated_submission_time=163774.925760, global_step=489350, preemption_count=0, score=163774.925760, test/accuracy=0.626800, test/loss=1.840896, test/num_examples=10000, total_duration=169345.674490, train/accuracy=0.961177, train/loss=0.146858, validation/accuracy=0.756240, validation/loss=1.053350, validation/num_examples=50000
I0302 04:17:01.981506 140089837426432 logging_writer.py:48] [489400] global_step=489400, grad_norm=4.568675518035889, loss=0.5439143180847168
I0302 04:17:35.416823 140089770305280 logging_writer.py:48] [489500] global_step=489500, grad_norm=4.770382404327393, loss=0.5501580834388733
I0302 04:18:08.841756 140089837426432 logging_writer.py:48] [489600] global_step=489600, grad_norm=4.748448848724365, loss=0.71498042345047
I0302 04:18:42.248390 140089770305280 logging_writer.py:48] [489700] global_step=489700, grad_norm=4.30820369720459, loss=0.5983196496963501
I0302 04:19:15.701637 140089837426432 logging_writer.py:48] [489800] global_step=489800, grad_norm=4.292295932769775, loss=0.5626622438430786
I0302 04:19:49.098966 140089770305280 logging_writer.py:48] [489900] global_step=489900, grad_norm=4.490712642669678, loss=0.5681886672973633
I0302 04:20:22.536903 140089837426432 logging_writer.py:48] [490000] global_step=490000, grad_norm=4.447171688079834, loss=0.6020442843437195
I0302 04:20:55.954282 140089770305280 logging_writer.py:48] [490100] global_step=490100, grad_norm=4.577413558959961, loss=0.5419069528579712
I0302 04:21:29.381112 140089837426432 logging_writer.py:48] [490200] global_step=490200, grad_norm=4.351487636566162, loss=0.61947101354599
I0302 04:22:02.866641 140089770305280 logging_writer.py:48] [490300] global_step=490300, grad_norm=5.185995578765869, loss=0.5723519921302795
I0302 04:22:36.300934 140089837426432 logging_writer.py:48] [490400] global_step=490400, grad_norm=4.764101982116699, loss=0.6455204486846924
I0302 04:23:09.721466 140089770305280 logging_writer.py:48] [490500] global_step=490500, grad_norm=4.93014669418335, loss=0.7544695734977722
I0302 04:23:43.140964 140089837426432 logging_writer.py:48] [490600] global_step=490600, grad_norm=4.659738540649414, loss=0.5747497081756592
I0302 04:24:16.578138 140089770305280 logging_writer.py:48] [490700] global_step=490700, grad_norm=4.858741283416748, loss=0.7586398124694824
I0302 04:24:50.011281 140089837426432 logging_writer.py:48] [490800] global_step=490800, grad_norm=4.4954633712768555, loss=0.6128635406494141
I0302 04:25:14.893661 140252611495744 spec.py:321] Evaluating on the training split.
I0302 04:25:21.033605 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 04:25:29.104179 140252611495744 spec.py:349] Evaluating on the test split.
I0302 04:25:31.384670 140252611495744 submission_runner.py:411] Time since start: 169872.26s, 	Step: 490876, 	{'train/accuracy': 0.9616748690605164, 'train/loss': 0.14340081810951233, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0531055927276611, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8428215980529785, 'test/num_examples': 10000, 'score': 164284.85869836807, 'total_duration': 169872.2647330761, 'accumulated_submission_time': 164284.85869836807, 'accumulated_eval_time': 5546.840213775635, 'accumulated_logging_time': 22.913722276687622}
I0302 04:25:31.490856 140089500890880 logging_writer.py:48] [490876] accumulated_eval_time=5546.840214, accumulated_logging_time=22.913722, accumulated_submission_time=164284.858698, global_step=490876, preemption_count=0, score=164284.858698, test/accuracy=0.627400, test/loss=1.842822, test/num_examples=10000, total_duration=169872.264733, train/accuracy=0.961675, train/loss=0.143401, validation/accuracy=0.755940, validation/loss=1.053106, validation/num_examples=50000
I0302 04:25:39.840640 140089770305280 logging_writer.py:48] [490900] global_step=490900, grad_norm=4.126919746398926, loss=0.5264600515365601
I0302 04:26:13.289283 140089500890880 logging_writer.py:48] [491000] global_step=491000, grad_norm=5.014538288116455, loss=0.6143560409545898
I0302 04:26:46.750280 140089770305280 logging_writer.py:48] [491100] global_step=491100, grad_norm=4.694111347198486, loss=0.6160658001899719
I0302 04:27:20.184066 140089500890880 logging_writer.py:48] [491200] global_step=491200, grad_norm=4.750326156616211, loss=0.6830719113349915
I0302 04:27:53.620752 140089770305280 logging_writer.py:48] [491300] global_step=491300, grad_norm=4.70971155166626, loss=0.6402562856674194
I0302 04:28:27.149174 140089500890880 logging_writer.py:48] [491400] global_step=491400, grad_norm=4.4945173263549805, loss=0.6360830068588257
I0302 04:29:00.583770 140089770305280 logging_writer.py:48] [491500] global_step=491500, grad_norm=4.463217735290527, loss=0.6405046582221985
I0302 04:29:34.037170 140089500890880 logging_writer.py:48] [491600] global_step=491600, grad_norm=4.619196891784668, loss=0.6137625575065613
I0302 04:30:07.481722 140089770305280 logging_writer.py:48] [491700] global_step=491700, grad_norm=4.721035957336426, loss=0.6704756021499634
I0302 04:30:40.928246 140089500890880 logging_writer.py:48] [491800] global_step=491800, grad_norm=4.8337602615356445, loss=0.5771225690841675
I0302 04:31:14.374579 140089770305280 logging_writer.py:48] [491900] global_step=491900, grad_norm=5.246262550354004, loss=0.6759408712387085
I0302 04:31:47.805587 140089500890880 logging_writer.py:48] [492000] global_step=492000, grad_norm=4.696444511413574, loss=0.6142690181732178
I0302 04:32:21.261810 140089770305280 logging_writer.py:48] [492100] global_step=492100, grad_norm=4.758366584777832, loss=0.5657930970191956
I0302 04:32:54.693387 140089500890880 logging_writer.py:48] [492200] global_step=492200, grad_norm=4.330109119415283, loss=0.6266326308250427
I0302 04:33:28.130682 140089770305280 logging_writer.py:48] [492300] global_step=492300, grad_norm=4.460830211639404, loss=0.5858747363090515
I0302 04:34:01.547362 140089500890880 logging_writer.py:48] [492400] global_step=492400, grad_norm=4.323233604431152, loss=0.5658040046691895
I0302 04:34:01.555745 140252611495744 spec.py:321] Evaluating on the training split.
I0302 04:34:07.818899 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 04:34:15.769790 140252611495744 spec.py:349] Evaluating on the test split.
I0302 04:34:18.381159 140252611495744 submission_runner.py:411] Time since start: 170399.26s, 	Step: 492401, 	{'train/accuracy': 0.95902419090271, 'train/loss': 0.14910300076007843, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0529998540878296, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8415027856826782, 'test/num_examples': 10000, 'score': 164794.85840845108, 'total_duration': 170399.26122307777, 'accumulated_submission_time': 164794.85840845108, 'accumulated_eval_time': 5563.665554523468, 'accumulated_logging_time': 23.030481100082397}
I0302 04:34:18.479830 140089845819136 logging_writer.py:48] [492401] accumulated_eval_time=5563.665555, accumulated_logging_time=23.030481, accumulated_submission_time=164794.858408, global_step=492401, preemption_count=0, score=164794.858408, test/accuracy=0.628000, test/loss=1.841503, test/num_examples=10000, total_duration=170399.261223, train/accuracy=0.959024, train/loss=0.149103, validation/accuracy=0.756000, validation/loss=1.053000, validation/num_examples=50000
I0302 04:34:51.921675 140089854211840 logging_writer.py:48] [492500] global_step=492500, grad_norm=4.432610511779785, loss=0.553800642490387
I0302 04:35:25.332977 140089845819136 logging_writer.py:48] [492600] global_step=492600, grad_norm=4.932293891906738, loss=0.6815878748893738
I0302 04:35:58.801729 140089854211840 logging_writer.py:48] [492700] global_step=492700, grad_norm=4.376987457275391, loss=0.6353742480278015
I0302 04:36:32.225451 140089845819136 logging_writer.py:48] [492800] global_step=492800, grad_norm=4.860991954803467, loss=0.6717031598091125
I0302 04:37:05.676275 140089854211840 logging_writer.py:48] [492900] global_step=492900, grad_norm=4.872268199920654, loss=0.6829661130905151
I0302 04:37:39.117643 140089845819136 logging_writer.py:48] [493000] global_step=493000, grad_norm=4.97714376449585, loss=0.6540473699569702
I0302 04:38:12.553739 140089854211840 logging_writer.py:48] [493100] global_step=493100, grad_norm=4.17698335647583, loss=0.5560092329978943
I0302 04:38:45.957341 140089845819136 logging_writer.py:48] [493200] global_step=493200, grad_norm=4.304904937744141, loss=0.5434632301330566
I0302 04:39:19.412541 140089854211840 logging_writer.py:48] [493300] global_step=493300, grad_norm=4.529541015625, loss=0.6407397985458374
I0302 04:39:52.863525 140089845819136 logging_writer.py:48] [493400] global_step=493400, grad_norm=4.24452018737793, loss=0.5347158908843994
I0302 04:40:26.358375 140089854211840 logging_writer.py:48] [493500] global_step=493500, grad_norm=4.752655506134033, loss=0.6936478614807129
I0302 04:40:59.773451 140089845819136 logging_writer.py:48] [493600] global_step=493600, grad_norm=4.736639976501465, loss=0.6684640645980835
I0302 04:41:33.215429 140089854211840 logging_writer.py:48] [493700] global_step=493700, grad_norm=4.6356282234191895, loss=0.6564826965332031
I0302 04:42:06.645187 140089845819136 logging_writer.py:48] [493800] global_step=493800, grad_norm=4.598158836364746, loss=0.7118434309959412
I0302 04:42:40.085499 140089854211840 logging_writer.py:48] [493900] global_step=493900, grad_norm=4.615569591522217, loss=0.6414639949798584
I0302 04:42:48.579960 140252611495744 spec.py:321] Evaluating on the training split.
I0302 04:42:54.684242 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 04:43:02.872560 140252611495744 spec.py:349] Evaluating on the test split.
I0302 04:43:05.177328 140252611495744 submission_runner.py:411] Time since start: 170926.06s, 	Step: 493927, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14402621984481812, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0536928176879883, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.842665672302246, 'test/num_examples': 10000, 'score': 165304.89320635796, 'total_duration': 170926.0573911667, 'accumulated_submission_time': 165304.89320635796, 'accumulated_eval_time': 5580.262861967087, 'accumulated_logging_time': 23.13922953605652}
I0302 04:43:05.277325 140089778697984 logging_writer.py:48] [493927] accumulated_eval_time=5580.262862, accumulated_logging_time=23.139230, accumulated_submission_time=165304.893206, global_step=493927, preemption_count=0, score=165304.893206, test/accuracy=0.627000, test/loss=1.842666, test/num_examples=10000, total_duration=170926.057391, train/accuracy=0.961535, train/loss=0.144026, validation/accuracy=0.755760, validation/loss=1.053693, validation/num_examples=50000
I0302 04:43:30.028079 140089837426432 logging_writer.py:48] [494000] global_step=494000, grad_norm=4.351068019866943, loss=0.6146973967552185
I0302 04:44:03.452615 140089778697984 logging_writer.py:48] [494100] global_step=494100, grad_norm=4.224913597106934, loss=0.5666126012802124
I0302 04:44:36.878926 140089837426432 logging_writer.py:48] [494200] global_step=494200, grad_norm=4.205770015716553, loss=0.6279916167259216
I0302 04:45:10.333727 140089778697984 logging_writer.py:48] [494300] global_step=494300, grad_norm=4.556978225708008, loss=0.6040621995925903
I0302 04:45:43.806259 140089837426432 logging_writer.py:48] [494400] global_step=494400, grad_norm=4.334091663360596, loss=0.6271928548812866
I0302 04:46:17.232944 140089778697984 logging_writer.py:48] [494500] global_step=494500, grad_norm=3.9947502613067627, loss=0.5726069808006287
I0302 04:46:50.767193 140089837426432 logging_writer.py:48] [494600] global_step=494600, grad_norm=4.376977443695068, loss=0.6276307702064514
I0302 04:47:24.211426 140089778697984 logging_writer.py:48] [494700] global_step=494700, grad_norm=4.09351110458374, loss=0.5723448395729065
I0302 04:47:57.674009 140089837426432 logging_writer.py:48] [494800] global_step=494800, grad_norm=4.6374006271362305, loss=0.634393572807312
I0302 04:48:31.107863 140089778697984 logging_writer.py:48] [494900] global_step=494900, grad_norm=5.1472039222717285, loss=0.6622697114944458
I0302 04:49:04.554257 140089837426432 logging_writer.py:48] [495000] global_step=495000, grad_norm=4.512298107147217, loss=0.5788531303405762
I0302 04:49:38.012100 140089778697984 logging_writer.py:48] [495100] global_step=495100, grad_norm=4.600504398345947, loss=0.6124520301818848
I0302 04:50:11.480721 140089837426432 logging_writer.py:48] [495200] global_step=495200, grad_norm=5.239795684814453, loss=0.7131282091140747
I0302 04:50:44.908852 140089778697984 logging_writer.py:48] [495300] global_step=495300, grad_norm=4.498318195343018, loss=0.6050428152084351
I0302 04:51:18.357759 140089837426432 logging_writer.py:48] [495400] global_step=495400, grad_norm=4.506859302520752, loss=0.5852612257003784
I0302 04:51:35.192683 140252611495744 spec.py:321] Evaluating on the training split.
I0302 04:51:41.345184 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 04:51:49.448361 140252611495744 spec.py:349] Evaluating on the test split.
I0302 04:51:51.705703 140252611495744 submission_runner.py:411] Time since start: 171452.59s, 	Step: 495452, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.146340012550354, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.053356647491455, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.8420727252960205, 'test/num_examples': 10000, 'score': 165814.74216461182, 'total_duration': 171452.58564066887, 'accumulated_submission_time': 165814.74216461182, 'accumulated_eval_time': 5596.775694847107, 'accumulated_logging_time': 23.250207901000977}
I0302 04:51:51.803603 140089854211840 logging_writer.py:48] [495452] accumulated_eval_time=5596.775695, accumulated_logging_time=23.250208, accumulated_submission_time=165814.742165, global_step=495452, preemption_count=0, score=165814.742165, test/accuracy=0.625700, test/loss=1.842073, test/num_examples=10000, total_duration=171452.585641, train/accuracy=0.960798, train/loss=0.146340, validation/accuracy=0.755880, validation/loss=1.053357, validation/num_examples=50000
I0302 04:52:08.163912 140089862604544 logging_writer.py:48] [495500] global_step=495500, grad_norm=4.655078887939453, loss=0.5777208805084229
I0302 04:52:41.704093 140089854211840 logging_writer.py:48] [495600] global_step=495600, grad_norm=4.550659656524658, loss=0.6387627720832825
I0302 04:53:15.138168 140089862604544 logging_writer.py:48] [495700] global_step=495700, grad_norm=4.1242146492004395, loss=0.5467886924743652
I0302 04:53:48.580374 140089854211840 logging_writer.py:48] [495800] global_step=495800, grad_norm=4.197694301605225, loss=0.660091757774353
I0302 04:54:22.028023 140089862604544 logging_writer.py:48] [495900] global_step=495900, grad_norm=4.365766525268555, loss=0.6153647303581238
I0302 04:54:55.462496 140089854211840 logging_writer.py:48] [496000] global_step=496000, grad_norm=4.697632312774658, loss=0.6666387915611267
I0302 04:55:28.936348 140089862604544 logging_writer.py:48] [496100] global_step=496100, grad_norm=4.404119968414307, loss=0.5885494947433472
I0302 04:56:02.378156 140089854211840 logging_writer.py:48] [496200] global_step=496200, grad_norm=5.015588760375977, loss=0.7158312797546387
I0302 04:56:35.828296 140089862604544 logging_writer.py:48] [496300] global_step=496300, grad_norm=4.020798206329346, loss=0.594713032245636
I0302 04:57:09.276280 140089854211840 logging_writer.py:48] [496400] global_step=496400, grad_norm=4.569906234741211, loss=0.6246079206466675
I0302 04:57:42.714669 140089862604544 logging_writer.py:48] [496500] global_step=496500, grad_norm=4.018622875213623, loss=0.5695916414260864
I0302 04:58:16.146175 140089854211840 logging_writer.py:48] [496600] global_step=496600, grad_norm=4.800583839416504, loss=0.6287447214126587
I0302 04:58:49.699309 140089862604544 logging_writer.py:48] [496700] global_step=496700, grad_norm=4.601401329040527, loss=0.6846333742141724
I0302 04:59:23.124298 140089854211840 logging_writer.py:48] [496800] global_step=496800, grad_norm=4.767087459564209, loss=0.617063581943512
I0302 04:59:56.602144 140089862604544 logging_writer.py:48] [496900] global_step=496900, grad_norm=4.2759246826171875, loss=0.6054545640945435
I0302 05:00:21.811109 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:00:27.958258 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:00:35.966987 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:00:38.253458 140252611495744 submission_runner.py:411] Time since start: 171979.13s, 	Step: 496977, 	{'train/accuracy': 0.9622329473495483, 'train/loss': 0.14735938608646393, 'validation/accuracy': 0.7556599974632263, 'validation/loss': 1.0541554689407349, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.841199278831482, 'test/num_examples': 10000, 'score': 166324.6839056015, 'total_duration': 171979.1335196495, 'accumulated_submission_time': 166324.6839056015, 'accumulated_eval_time': 5613.217988491058, 'accumulated_logging_time': 23.358347415924072}
I0302 05:00:38.354223 140089500890880 logging_writer.py:48] [496977] accumulated_eval_time=5613.217988, accumulated_logging_time=23.358347, accumulated_submission_time=166324.683906, global_step=496977, preemption_count=0, score=166324.683906, test/accuracy=0.627300, test/loss=1.841199, test/num_examples=10000, total_duration=171979.133520, train/accuracy=0.962233, train/loss=0.147359, validation/accuracy=0.755660, validation/loss=1.054155, validation/num_examples=50000
I0302 05:00:46.381724 140089770305280 logging_writer.py:48] [497000] global_step=497000, grad_norm=5.057633399963379, loss=0.8318154811859131
I0302 05:01:19.863447 140089500890880 logging_writer.py:48] [497100] global_step=497100, grad_norm=5.123898983001709, loss=0.7167518734931946
I0302 05:01:53.312035 140089770305280 logging_writer.py:48] [497200] global_step=497200, grad_norm=4.83347749710083, loss=0.6475390195846558
I0302 05:02:26.788929 140089500890880 logging_writer.py:48] [497300] global_step=497300, grad_norm=4.744828701019287, loss=0.6034790277481079
I0302 05:03:00.239103 140089770305280 logging_writer.py:48] [497400] global_step=497400, grad_norm=4.301150798797607, loss=0.5510428547859192
I0302 05:03:33.667122 140089500890880 logging_writer.py:48] [497500] global_step=497500, grad_norm=4.679430961608887, loss=0.5856071710586548
I0302 05:04:07.105791 140089770305280 logging_writer.py:48] [497600] global_step=497600, grad_norm=4.074317455291748, loss=0.5582122802734375
I0302 05:04:40.621908 140089500890880 logging_writer.py:48] [497700] global_step=497700, grad_norm=5.335230350494385, loss=0.7197027206420898
I0302 05:05:14.060835 140089770305280 logging_writer.py:48] [497800] global_step=497800, grad_norm=4.662534713745117, loss=0.6777164340019226
I0302 05:05:47.510734 140089500890880 logging_writer.py:48] [497900] global_step=497900, grad_norm=4.727438926696777, loss=0.5441408157348633
I0302 05:06:20.964003 140089770305280 logging_writer.py:48] [498000] global_step=498000, grad_norm=4.626553535461426, loss=0.7455048561096191
I0302 05:06:54.429788 140089500890880 logging_writer.py:48] [498100] global_step=498100, grad_norm=4.459883689880371, loss=0.6292039752006531
I0302 05:07:27.869983 140089770305280 logging_writer.py:48] [498200] global_step=498200, grad_norm=4.738768100738525, loss=0.6567798852920532
I0302 05:08:01.327364 140089500890880 logging_writer.py:48] [498300] global_step=498300, grad_norm=4.588685989379883, loss=0.6256043314933777
I0302 05:08:34.789351 140089770305280 logging_writer.py:48] [498400] global_step=498400, grad_norm=4.709461688995361, loss=0.6647144556045532
I0302 05:09:08.218354 140089500890880 logging_writer.py:48] [498500] global_step=498500, grad_norm=4.59095573425293, loss=0.5720118880271912
I0302 05:09:08.371262 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:09:14.572448 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:09:22.604474 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:09:24.870228 140252611495744 submission_runner.py:411] Time since start: 172505.75s, 	Step: 498502, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.1480731964111328, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.0539394617080688, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8436610698699951, 'test/num_examples': 10000, 'score': 166834.6341896057, 'total_duration': 172505.75029206276, 'accumulated_submission_time': 166834.6341896057, 'accumulated_eval_time': 5629.716914653778, 'accumulated_logging_time': 23.470913410186768}
I0302 05:09:24.969274 140089845819136 logging_writer.py:48] [498502] accumulated_eval_time=5629.716915, accumulated_logging_time=23.470913, accumulated_submission_time=166834.634190, global_step=498502, preemption_count=0, score=166834.634190, test/accuracy=0.627000, test/loss=1.843661, test/num_examples=10000, total_duration=172505.750292, train/accuracy=0.960081, train/loss=0.148073, validation/accuracy=0.755820, validation/loss=1.053939, validation/num_examples=50000
I0302 05:09:58.110240 140089854211840 logging_writer.py:48] [498600] global_step=498600, grad_norm=4.616035461425781, loss=0.6238266229629517
I0302 05:10:31.535067 140089845819136 logging_writer.py:48] [498700] global_step=498700, grad_norm=4.301344871520996, loss=0.6144722700119019
I0302 05:11:05.073427 140089854211840 logging_writer.py:48] [498800] global_step=498800, grad_norm=4.738710880279541, loss=0.6652129292488098
I0302 05:11:38.506640 140089845819136 logging_writer.py:48] [498900] global_step=498900, grad_norm=4.777602672576904, loss=0.6134021282196045
I0302 05:12:11.932991 140089854211840 logging_writer.py:48] [499000] global_step=499000, grad_norm=5.049605369567871, loss=0.6498896479606628
I0302 05:12:45.352492 140089845819136 logging_writer.py:48] [499100] global_step=499100, grad_norm=4.203108787536621, loss=0.5627841949462891
I0302 05:13:18.793461 140089854211840 logging_writer.py:48] [499200] global_step=499200, grad_norm=4.296350002288818, loss=0.5729672312736511
I0302 05:13:52.256198 140089845819136 logging_writer.py:48] [499300] global_step=499300, grad_norm=4.768570899963379, loss=0.6373781561851501
I0302 05:14:25.673491 140089854211840 logging_writer.py:48] [499400] global_step=499400, grad_norm=4.552242279052734, loss=0.5906945466995239
I0302 05:14:59.129011 140089845819136 logging_writer.py:48] [499500] global_step=499500, grad_norm=4.6464924812316895, loss=0.6485593318939209
I0302 05:15:32.515153 140089854211840 logging_writer.py:48] [499600] global_step=499600, grad_norm=4.629451274871826, loss=0.5962374806404114
I0302 05:16:05.990417 140089845819136 logging_writer.py:48] [499700] global_step=499700, grad_norm=4.480544567108154, loss=0.6503971219062805
I0302 05:16:39.420860 140089854211840 logging_writer.py:48] [499800] global_step=499800, grad_norm=4.462923526763916, loss=0.636652410030365
I0302 05:17:12.892046 140089845819136 logging_writer.py:48] [499900] global_step=499900, grad_norm=4.38287353515625, loss=0.6567415595054626
I0302 05:17:46.340848 140089854211840 logging_writer.py:48] [500000] global_step=500000, grad_norm=4.730711460113525, loss=0.6846230030059814
I0302 05:17:55.190606 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:18:01.265915 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:18:09.494303 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:18:11.811122 140252611495744 submission_runner.py:411] Time since start: 173032.69s, 	Step: 500028, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14351986348628998, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.05421781539917, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8444896936416626, 'test/num_examples': 10000, 'score': 167344.79048371315, 'total_duration': 173032.69118475914, 'accumulated_submission_time': 167344.79048371315, 'accumulated_eval_time': 5646.337370872498, 'accumulated_logging_time': 23.58010768890381}
I0302 05:18:11.911288 140089837426432 logging_writer.py:48] [500028] accumulated_eval_time=5646.337371, accumulated_logging_time=23.580108, accumulated_submission_time=167344.790484, global_step=500028, preemption_count=0, score=167344.790484, test/accuracy=0.627900, test/loss=1.844490, test/num_examples=10000, total_duration=173032.691185, train/accuracy=0.961934, train/loss=0.143520, validation/accuracy=0.755900, validation/loss=1.054218, validation/num_examples=50000
I0302 05:18:36.335235 140089870997248 logging_writer.py:48] [500100] global_step=500100, grad_norm=4.170328617095947, loss=0.6031622290611267
I0302 05:19:09.768594 140089837426432 logging_writer.py:48] [500200] global_step=500200, grad_norm=4.5071282386779785, loss=0.6657397150993347
I0302 05:19:43.221577 140089870997248 logging_writer.py:48] [500300] global_step=500300, grad_norm=4.739717483520508, loss=0.665997326374054
I0302 05:20:16.683102 140089837426432 logging_writer.py:48] [500400] global_step=500400, grad_norm=4.713408470153809, loss=0.6051602363586426
I0302 05:20:50.116637 140089870997248 logging_writer.py:48] [500500] global_step=500500, grad_norm=4.6857781410217285, loss=0.6527719497680664
I0302 05:21:23.565089 140089837426432 logging_writer.py:48] [500600] global_step=500600, grad_norm=4.437585353851318, loss=0.6381572484970093
I0302 05:21:56.999387 140089870997248 logging_writer.py:48] [500700] global_step=500700, grad_norm=4.410192966461182, loss=0.6538911461830139
I0302 05:22:30.448937 140089837426432 logging_writer.py:48] [500800] global_step=500800, grad_norm=4.619570255279541, loss=0.6212770342826843
I0302 05:23:03.944468 140089870997248 logging_writer.py:48] [500900] global_step=500900, grad_norm=4.2464599609375, loss=0.579341471195221
I0302 05:23:37.378460 140089837426432 logging_writer.py:48] [501000] global_step=501000, grad_norm=4.383309841156006, loss=0.5818758010864258
I0302 05:24:10.825953 140089870997248 logging_writer.py:48] [501100] global_step=501100, grad_norm=4.123699188232422, loss=0.5521247386932373
I0302 05:24:44.249682 140089837426432 logging_writer.py:48] [501200] global_step=501200, grad_norm=4.748866558074951, loss=0.6497924327850342
I0302 05:25:17.695909 140089870997248 logging_writer.py:48] [501300] global_step=501300, grad_norm=4.118501663208008, loss=0.6078199744224548
I0302 05:25:51.134526 140089837426432 logging_writer.py:48] [501400] global_step=501400, grad_norm=4.32070779800415, loss=0.598932147026062
I0302 05:26:24.596658 140089870997248 logging_writer.py:48] [501500] global_step=501500, grad_norm=4.776303291320801, loss=0.7694621682167053
I0302 05:26:42.137492 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:26:48.282873 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:26:56.458805 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:26:58.790038 140252611495744 submission_runner.py:411] Time since start: 173559.67s, 	Step: 501554, 	{'train/accuracy': 0.9622528553009033, 'train/loss': 0.14418639242649078, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0531775951385498, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8417606353759766, 'test/num_examples': 10000, 'score': 167854.95328998566, 'total_duration': 173559.6701028347, 'accumulated_submission_time': 167854.95328998566, 'accumulated_eval_time': 5662.989856958389, 'accumulated_logging_time': 23.689915418624878}
I0302 05:26:58.890933 140089854211840 logging_writer.py:48] [501554] accumulated_eval_time=5662.989857, accumulated_logging_time=23.689915, accumulated_submission_time=167854.953290, global_step=501554, preemption_count=0, score=167854.953290, test/accuracy=0.627200, test/loss=1.841761, test/num_examples=10000, total_duration=173559.670103, train/accuracy=0.962253, train/loss=0.144186, validation/accuracy=0.756200, validation/loss=1.053178, validation/num_examples=50000
I0302 05:27:14.605744 140089862604544 logging_writer.py:48] [501600] global_step=501600, grad_norm=4.739412307739258, loss=0.6477066874504089
I0302 05:27:48.066827 140089854211840 logging_writer.py:48] [501700] global_step=501700, grad_norm=4.474442958831787, loss=0.6033011078834534
I0302 05:28:26.296884 140089862604544 logging_writer.py:48] [501800] global_step=501800, grad_norm=4.289148330688477, loss=0.5674514174461365
I0302 05:29:06.439077 140089854211840 logging_writer.py:48] [501900] global_step=501900, grad_norm=4.6172871589660645, loss=0.7018042206764221
I0302 05:29:39.980889 140089862604544 logging_writer.py:48] [502000] global_step=502000, grad_norm=4.46034049987793, loss=0.6298683881759644
I0302 05:30:13.433303 140089854211840 logging_writer.py:48] [502100] global_step=502100, grad_norm=4.340714454650879, loss=0.6310132741928101
I0302 05:30:46.915595 140089862604544 logging_writer.py:48] [502200] global_step=502200, grad_norm=4.6419548988342285, loss=0.6086564064025879
I0302 05:31:20.364482 140089854211840 logging_writer.py:48] [502300] global_step=502300, grad_norm=4.398685455322266, loss=0.5613953471183777
I0302 05:31:53.824718 140089862604544 logging_writer.py:48] [502400] global_step=502400, grad_norm=4.056003093719482, loss=0.5492616891860962
I0302 05:32:27.294264 140089854211840 logging_writer.py:48] [502500] global_step=502500, grad_norm=4.355137825012207, loss=0.5823429822921753
I0302 05:33:00.727586 140089862604544 logging_writer.py:48] [502600] global_step=502600, grad_norm=4.575845718383789, loss=0.622907817363739
I0302 05:33:34.181324 140089854211840 logging_writer.py:48] [502700] global_step=502700, grad_norm=4.5300726890563965, loss=0.6055391430854797
I0302 05:34:07.655027 140089862604544 logging_writer.py:48] [502800] global_step=502800, grad_norm=4.107407093048096, loss=0.5352565050125122
I0302 05:34:41.078397 140089854211840 logging_writer.py:48] [502900] global_step=502900, grad_norm=4.660281181335449, loss=0.6266607046127319
I0302 05:35:14.651293 140089862604544 logging_writer.py:48] [503000] global_step=503000, grad_norm=4.770133972167969, loss=0.6894275546073914
I0302 05:35:28.843049 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:35:34.971832 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:35:42.979023 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:35:45.261666 140252611495744 submission_runner.py:411] Time since start: 174086.14s, 	Step: 503044, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.1459151953458786, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.053280234336853, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.84154212474823, 'test/num_examples': 10000, 'score': 168364.83893346786, 'total_duration': 174086.14171028137, 'accumulated_submission_time': 168364.83893346786, 'accumulated_eval_time': 5679.408395290375, 'accumulated_logging_time': 23.802663803100586}
I0302 05:35:45.360870 140089770305280 logging_writer.py:48] [503044] accumulated_eval_time=5679.408395, accumulated_logging_time=23.802664, accumulated_submission_time=168364.838933, global_step=503044, preemption_count=0, score=168364.838933, test/accuracy=0.626800, test/loss=1.841542, test/num_examples=10000, total_duration=174086.141710, train/accuracy=0.961934, train/loss=0.145915, validation/accuracy=0.755880, validation/loss=1.053280, validation/num_examples=50000
I0302 05:36:04.503979 140089778697984 logging_writer.py:48] [503100] global_step=503100, grad_norm=4.487930774688721, loss=0.6136558651924133
I0302 05:36:37.962356 140089770305280 logging_writer.py:48] [503200] global_step=503200, grad_norm=4.171625137329102, loss=0.5702226161956787
I0302 05:37:11.393840 140089778697984 logging_writer.py:48] [503300] global_step=503300, grad_norm=5.475418567657471, loss=0.724254846572876
I0302 05:37:44.896102 140089770305280 logging_writer.py:48] [503400] global_step=503400, grad_norm=4.215952396392822, loss=0.5606966614723206
I0302 05:38:18.372681 140089778697984 logging_writer.py:48] [503500] global_step=503500, grad_norm=4.657732009887695, loss=0.6528465747833252
I0302 05:38:51.840547 140089770305280 logging_writer.py:48] [503600] global_step=503600, grad_norm=4.448825359344482, loss=0.6448202133178711
I0302 05:39:25.312712 140089778697984 logging_writer.py:48] [503700] global_step=503700, grad_norm=4.55293607711792, loss=0.6493231654167175
I0302 05:39:58.763883 140089770305280 logging_writer.py:48] [503800] global_step=503800, grad_norm=4.588841438293457, loss=0.6570570468902588
I0302 05:40:32.204090 140089778697984 logging_writer.py:48] [503900] global_step=503900, grad_norm=4.378349781036377, loss=0.6010487079620361
I0302 05:41:05.672022 140089770305280 logging_writer.py:48] [504000] global_step=504000, grad_norm=4.57853364944458, loss=0.6023140549659729
I0302 05:41:39.247672 140089778697984 logging_writer.py:48] [504100] global_step=504100, grad_norm=4.780668258666992, loss=0.6580609083175659
I0302 05:42:12.729451 140089770305280 logging_writer.py:48] [504200] global_step=504200, grad_norm=4.847267150878906, loss=0.5886574983596802
I0302 05:42:46.218740 140089778697984 logging_writer.py:48] [504300] global_step=504300, grad_norm=4.749243259429932, loss=0.5798226594924927
I0302 05:43:19.677339 140089770305280 logging_writer.py:48] [504400] global_step=504400, grad_norm=4.640085220336914, loss=0.677358865737915
I0302 05:43:53.105939 140089778697984 logging_writer.py:48] [504500] global_step=504500, grad_norm=4.375024795532227, loss=0.6641911268234253
I0302 05:44:15.378581 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:44:22.348497 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:44:30.357134 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:44:32.622719 140252611495744 submission_runner.py:411] Time since start: 174613.50s, 	Step: 504568, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14532075822353363, 'validation/accuracy': 0.7561799883842468, 'validation/loss': 1.0537110567092896, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.8432775735855103, 'test/num_examples': 10000, 'score': 168874.79024291039, 'total_duration': 174613.502784729, 'accumulated_submission_time': 168874.79024291039, 'accumulated_eval_time': 5696.6524930000305, 'accumulated_logging_time': 23.912421226501465}
I0302 05:44:32.721716 140089770305280 logging_writer.py:48] [504568] accumulated_eval_time=5696.652493, accumulated_logging_time=23.912421, accumulated_submission_time=168874.790243, global_step=504568, preemption_count=0, score=168874.790243, test/accuracy=0.626700, test/loss=1.843278, test/num_examples=10000, total_duration=174613.502785, train/accuracy=0.960519, train/loss=0.145321, validation/accuracy=0.756180, validation/loss=1.053711, validation/num_examples=50000
I0302 05:44:43.768775 140089778697984 logging_writer.py:48] [504600] global_step=504600, grad_norm=4.417261123657227, loss=0.6351119875907898
I0302 05:45:17.218124 140089770305280 logging_writer.py:48] [504700] global_step=504700, grad_norm=4.606009483337402, loss=0.5855178236961365
I0302 05:45:50.651266 140089778697984 logging_writer.py:48] [504800] global_step=504800, grad_norm=4.468835830688477, loss=0.6067505478858948
I0302 05:46:24.112255 140089770305280 logging_writer.py:48] [504900] global_step=504900, grad_norm=4.629612445831299, loss=0.6576758027076721
I0302 05:46:57.592926 140089778697984 logging_writer.py:48] [505000] global_step=505000, grad_norm=4.390711307525635, loss=0.6119272112846375
I0302 05:47:31.033061 140089770305280 logging_writer.py:48] [505100] global_step=505100, grad_norm=4.426662445068359, loss=0.5973869562149048
I0302 05:48:04.531752 140089778697984 logging_writer.py:48] [505200] global_step=505200, grad_norm=4.814716815948486, loss=0.7005936503410339
I0302 05:48:37.991859 140089770305280 logging_writer.py:48] [505300] global_step=505300, grad_norm=4.7734856605529785, loss=0.6216325759887695
I0302 05:49:11.424297 140089778697984 logging_writer.py:48] [505400] global_step=505400, grad_norm=4.793711185455322, loss=0.6021547913551331
I0302 05:49:44.898859 140089770305280 logging_writer.py:48] [505500] global_step=505500, grad_norm=4.747079849243164, loss=0.6901301145553589
I0302 05:50:18.379976 140089778697984 logging_writer.py:48] [505600] global_step=505600, grad_norm=4.740809917449951, loss=0.6183401942253113
I0302 05:50:51.812936 140089770305280 logging_writer.py:48] [505700] global_step=505700, grad_norm=4.37246561050415, loss=0.6181814670562744
I0302 05:51:25.282221 140089778697984 logging_writer.py:48] [505800] global_step=505800, grad_norm=4.123739719390869, loss=0.5475383996963501
I0302 05:51:58.727370 140089770305280 logging_writer.py:48] [505900] global_step=505900, grad_norm=4.8330979347229, loss=0.6041573882102966
I0302 05:52:32.160324 140089778697984 logging_writer.py:48] [506000] global_step=506000, grad_norm=4.569167137145996, loss=0.640495777130127
I0302 05:53:02.763896 140252611495744 spec.py:321] Evaluating on the training split.
I0302 05:53:08.861629 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 05:53:16.999528 140252611495744 spec.py:349] Evaluating on the test split.
I0302 05:53:19.265057 140252611495744 submission_runner.py:411] Time since start: 175140.15s, 	Step: 506093, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14533057808876038, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0525985956192017, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8410217761993408, 'test/num_examples': 10000, 'score': 169384.76676797867, 'total_duration': 175140.1451165676, 'accumulated_submission_time': 169384.76676797867, 'accumulated_eval_time': 5713.153592586517, 'accumulated_logging_time': 24.021644353866577}
I0302 05:53:19.366198 140089500890880 logging_writer.py:48] [506093] accumulated_eval_time=5713.153593, accumulated_logging_time=24.021644, accumulated_submission_time=169384.766768, global_step=506093, preemption_count=0, score=169384.766768, test/accuracy=0.627800, test/loss=1.841022, test/num_examples=10000, total_duration=175140.145117, train/accuracy=0.960619, train/loss=0.145331, validation/accuracy=0.756060, validation/loss=1.052599, validation/num_examples=50000
I0302 05:53:22.043196 140089770305280 logging_writer.py:48] [506100] global_step=506100, grad_norm=4.588872909545898, loss=0.553338348865509
I0302 05:53:55.559515 140089500890880 logging_writer.py:48] [506200] global_step=506200, grad_norm=4.867552757263184, loss=0.5967714190483093
I0302 05:54:29.018841 140089770305280 logging_writer.py:48] [506300] global_step=506300, grad_norm=4.489989757537842, loss=0.6001330018043518
I0302 05:55:02.474696 140089500890880 logging_writer.py:48] [506400] global_step=506400, grad_norm=4.347247123718262, loss=0.6997034549713135
I0302 05:55:35.901890 140089770305280 logging_writer.py:48] [506500] global_step=506500, grad_norm=4.141883850097656, loss=0.5518114566802979
I0302 05:56:09.321772 140089500890880 logging_writer.py:48] [506600] global_step=506600, grad_norm=4.408046245574951, loss=0.6696467995643616
I0302 05:56:42.767645 140089770305280 logging_writer.py:48] [506700] global_step=506700, grad_norm=4.751102924346924, loss=0.6192420721054077
I0302 05:57:16.209941 140089500890880 logging_writer.py:48] [506800] global_step=506800, grad_norm=4.236886024475098, loss=0.5943120121955872
I0302 05:57:49.669864 140089770305280 logging_writer.py:48] [506900] global_step=506900, grad_norm=4.6762919425964355, loss=0.6250963807106018
I0302 05:58:23.130062 140089500890880 logging_writer.py:48] [507000] global_step=507000, grad_norm=4.363230228424072, loss=0.5678941011428833
I0302 05:58:56.566272 140089770305280 logging_writer.py:48] [507100] global_step=507100, grad_norm=4.3267436027526855, loss=0.5726656913757324
I0302 05:59:29.991057 140089500890880 logging_writer.py:48] [507200] global_step=507200, grad_norm=4.869409084320068, loss=0.6196148991584778
I0302 06:00:03.596760 140089770305280 logging_writer.py:48] [507300] global_step=507300, grad_norm=4.89844274520874, loss=0.578494668006897
I0302 06:00:37.026121 140089500890880 logging_writer.py:48] [507400] global_step=507400, grad_norm=5.170105934143066, loss=0.6932842135429382
I0302 06:01:10.483153 140089770305280 logging_writer.py:48] [507500] global_step=507500, grad_norm=4.039261817932129, loss=0.5733672976493835
I0302 06:01:43.936552 140089500890880 logging_writer.py:48] [507600] global_step=507600, grad_norm=4.450491428375244, loss=0.6213869452476501
I0302 06:01:49.427184 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:01:55.510569 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:02:03.806742 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:02:06.051192 140252611495744 submission_runner.py:411] Time since start: 175666.93s, 	Step: 507618, 	{'train/accuracy': 0.9618144035339355, 'train/loss': 0.1438751220703125, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.0549389123916626, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8442169427871704, 'test/num_examples': 10000, 'score': 169894.76099991798, 'total_duration': 175666.93125104904, 'accumulated_submission_time': 169894.76099991798, 'accumulated_eval_time': 5729.777534723282, 'accumulated_logging_time': 24.134642362594604}
I0302 06:02:06.153188 140089854211840 logging_writer.py:48] [507618] accumulated_eval_time=5729.777535, accumulated_logging_time=24.134642, accumulated_submission_time=169894.761000, global_step=507618, preemption_count=0, score=169894.761000, test/accuracy=0.628200, test/loss=1.844217, test/num_examples=10000, total_duration=175666.931251, train/accuracy=0.961814, train/loss=0.143875, validation/accuracy=0.755920, validation/loss=1.054939, validation/num_examples=50000
I0302 06:02:33.902150 140089870997248 logging_writer.py:48] [507700] global_step=507700, grad_norm=4.621561527252197, loss=0.6208935379981995
I0302 06:03:07.339646 140089854211840 logging_writer.py:48] [507800] global_step=507800, grad_norm=4.528426170349121, loss=0.6490631699562073
I0302 06:03:40.796853 140089870997248 logging_writer.py:48] [507900] global_step=507900, grad_norm=4.561791896820068, loss=0.6178357601165771
I0302 06:04:14.265513 140089854211840 logging_writer.py:48] [508000] global_step=508000, grad_norm=4.707276344299316, loss=0.5809513926506042
I0302 06:04:47.700459 140089870997248 logging_writer.py:48] [508100] global_step=508100, grad_norm=4.254691123962402, loss=0.6470643281936646
I0302 06:05:21.179940 140089854211840 logging_writer.py:48] [508200] global_step=508200, grad_norm=4.679492950439453, loss=0.5963892340660095
I0302 06:05:54.727555 140089870997248 logging_writer.py:48] [508300] global_step=508300, grad_norm=4.563769817352295, loss=0.6560874581336975
I0302 06:06:28.180458 140089854211840 logging_writer.py:48] [508400] global_step=508400, grad_norm=4.913215637207031, loss=0.6377109885215759
I0302 06:07:01.612880 140089870997248 logging_writer.py:48] [508500] global_step=508500, grad_norm=5.383347988128662, loss=0.6886469721794128
I0302 06:07:35.072447 140089854211840 logging_writer.py:48] [508600] global_step=508600, grad_norm=4.466283798217773, loss=0.7230179905891418
I0302 06:08:08.513853 140089870997248 logging_writer.py:48] [508700] global_step=508700, grad_norm=4.687310218811035, loss=0.64536052942276
I0302 06:08:41.952548 140089854211840 logging_writer.py:48] [508800] global_step=508800, grad_norm=4.379806041717529, loss=0.5924808979034424
I0302 06:09:15.401633 140089870997248 logging_writer.py:48] [508900] global_step=508900, grad_norm=4.949202060699463, loss=0.630325973033905
I0302 06:09:48.846908 140089854211840 logging_writer.py:48] [509000] global_step=509000, grad_norm=4.6922221183776855, loss=0.6183515191078186
I0302 06:10:22.319500 140089870997248 logging_writer.py:48] [509100] global_step=509100, grad_norm=4.851224422454834, loss=0.6640951037406921
I0302 06:10:36.172021 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:10:42.366559 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:10:50.436668 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:10:52.730297 140252611495744 submission_runner.py:411] Time since start: 176193.61s, 	Step: 509143, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.14291082322597504, 'validation/accuracy': 0.756339967250824, 'validation/loss': 1.0527304410934448, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8414567708969116, 'test/num_examples': 10000, 'score': 170404.71442842484, 'total_duration': 176193.6103644371, 'accumulated_submission_time': 170404.71442842484, 'accumulated_eval_time': 5746.335768461227, 'accumulated_logging_time': 24.246805906295776}
I0302 06:10:52.830246 140089500890880 logging_writer.py:48] [509143] accumulated_eval_time=5746.335768, accumulated_logging_time=24.246806, accumulated_submission_time=170404.714428, global_step=509143, preemption_count=0, score=170404.714428, test/accuracy=0.627600, test/loss=1.841457, test/num_examples=10000, total_duration=176193.610364, train/accuracy=0.961854, train/loss=0.142911, validation/accuracy=0.756340, validation/loss=1.052730, validation/num_examples=50000
I0302 06:11:12.213794 140089770305280 logging_writer.py:48] [509200] global_step=509200, grad_norm=4.325314044952393, loss=0.5672668218612671
I0302 06:11:45.638535 140089500890880 logging_writer.py:48] [509300] global_step=509300, grad_norm=4.878802299499512, loss=0.6637169122695923
I0302 06:12:19.173959 140089770305280 logging_writer.py:48] [509400] global_step=509400, grad_norm=4.8008928298950195, loss=0.6816922426223755
I0302 06:12:52.639077 140089500890880 logging_writer.py:48] [509500] global_step=509500, grad_norm=4.697275638580322, loss=0.5974701046943665
I0302 06:13:26.072244 140089770305280 logging_writer.py:48] [509600] global_step=509600, grad_norm=4.7023210525512695, loss=0.6458219885826111
I0302 06:13:59.536775 140089500890880 logging_writer.py:48] [509700] global_step=509700, grad_norm=4.813088417053223, loss=0.690890908241272
I0302 06:14:32.979995 140089770305280 logging_writer.py:48] [509800] global_step=509800, grad_norm=4.3246660232543945, loss=0.6037988066673279
I0302 06:15:06.419970 140089500890880 logging_writer.py:48] [509900] global_step=509900, grad_norm=4.664464950561523, loss=0.6640162467956543
I0302 06:15:39.853635 140089770305280 logging_writer.py:48] [510000] global_step=510000, grad_norm=4.2742018699646, loss=0.5809212327003479
I0302 06:16:13.322835 140089500890880 logging_writer.py:48] [510100] global_step=510100, grad_norm=4.417353630065918, loss=0.5789602398872375
I0302 06:16:46.755085 140089770305280 logging_writer.py:48] [510200] global_step=510200, grad_norm=4.5892791748046875, loss=0.6509609222412109
I0302 06:17:20.216311 140089500890880 logging_writer.py:48] [510300] global_step=510300, grad_norm=4.646756649017334, loss=0.6359983682632446
I0302 06:17:53.658616 140089770305280 logging_writer.py:48] [510400] global_step=510400, grad_norm=4.536256313323975, loss=0.6454930305480957
I0302 06:18:27.209878 140089500890880 logging_writer.py:48] [510500] global_step=510500, grad_norm=5.044270992279053, loss=0.7174341678619385
I0302 06:19:00.662077 140089770305280 logging_writer.py:48] [510600] global_step=510600, grad_norm=4.330037593841553, loss=0.6147835850715637
I0302 06:19:22.894194 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:19:29.031332 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:19:37.067963 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:19:39.334394 140252611495744 submission_runner.py:411] Time since start: 176720.21s, 	Step: 510668, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14370694756507874, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0545107126235962, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.842883825302124, 'test/num_examples': 10000, 'score': 170914.71456050873, 'total_duration': 176720.21445131302, 'accumulated_submission_time': 170914.71456050873, 'accumulated_eval_time': 5762.775901317596, 'accumulated_logging_time': 24.35623812675476}
I0302 06:19:39.436785 140089770305280 logging_writer.py:48] [510668] accumulated_eval_time=5762.775901, accumulated_logging_time=24.356238, accumulated_submission_time=170914.714561, global_step=510668, preemption_count=0, score=170914.714561, test/accuracy=0.628400, test/loss=1.842884, test/num_examples=10000, total_duration=176720.214451, train/accuracy=0.961356, train/loss=0.143707, validation/accuracy=0.756160, validation/loss=1.054511, validation/num_examples=50000
I0302 06:19:50.481451 140089854211840 logging_writer.py:48] [510700] global_step=510700, grad_norm=4.465798854827881, loss=0.5840577483177185
I0302 06:20:23.932963 140089770305280 logging_writer.py:48] [510800] global_step=510800, grad_norm=4.060626983642578, loss=0.5511999130249023
I0302 06:20:57.371825 140089854211840 logging_writer.py:48] [510900] global_step=510900, grad_norm=4.098230838775635, loss=0.5891009569168091
I0302 06:21:30.848893 140089770305280 logging_writer.py:48] [511000] global_step=511000, grad_norm=5.333400726318359, loss=0.6856693029403687
I0302 06:22:04.329988 140089854211840 logging_writer.py:48] [511100] global_step=511100, grad_norm=4.834435939788818, loss=0.6895703673362732
I0302 06:22:37.758194 140089770305280 logging_writer.py:48] [511200] global_step=511200, grad_norm=5.185296535491943, loss=0.6522071957588196
I0302 06:23:11.226795 140089854211840 logging_writer.py:48] [511300] global_step=511300, grad_norm=4.368874549865723, loss=0.572472095489502
I0302 06:23:44.671659 140089770305280 logging_writer.py:48] [511400] global_step=511400, grad_norm=5.00551176071167, loss=0.6231560707092285
I0302 06:24:18.184173 140089854211840 logging_writer.py:48] [511500] global_step=511500, grad_norm=5.097877502441406, loss=0.5817892551422119
I0302 06:24:51.658254 140089770305280 logging_writer.py:48] [511600] global_step=511600, grad_norm=4.471340656280518, loss=0.6176639199256897
I0302 06:25:25.162713 140089854211840 logging_writer.py:48] [511700] global_step=511700, grad_norm=4.719640731811523, loss=0.6009891629219055
I0302 06:25:58.589743 140089770305280 logging_writer.py:48] [511800] global_step=511800, grad_norm=4.48492956161499, loss=0.6479451060295105
I0302 06:26:32.096908 140089854211840 logging_writer.py:48] [511900] global_step=511900, grad_norm=4.231700897216797, loss=0.6011596918106079
I0302 06:27:05.568107 140089770305280 logging_writer.py:48] [512000] global_step=512000, grad_norm=4.8409647941589355, loss=0.6035086512565613
I0302 06:27:38.989305 140089854211840 logging_writer.py:48] [512100] global_step=512100, grad_norm=4.289440155029297, loss=0.6207932233810425
I0302 06:28:09.563665 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:28:15.714446 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:28:23.768812 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:28:26.045426 140252611495744 submission_runner.py:411] Time since start: 177246.93s, 	Step: 512193, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.1483767330646515, 'validation/accuracy': 0.7562400102615356, 'validation/loss': 1.0535874366760254, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8420346975326538, 'test/num_examples': 10000, 'score': 171424.77597403526, 'total_duration': 177246.92549419403, 'accumulated_submission_time': 171424.77597403526, 'accumulated_eval_time': 5779.257610321045, 'accumulated_logging_time': 24.469741344451904}
I0302 06:28:26.146878 140089837426432 logging_writer.py:48] [512193] accumulated_eval_time=5779.257610, accumulated_logging_time=24.469741, accumulated_submission_time=171424.775974, global_step=512193, preemption_count=0, score=171424.775974, test/accuracy=0.627900, test/loss=1.842035, test/num_examples=10000, total_duration=177246.925494, train/accuracy=0.960340, train/loss=0.148377, validation/accuracy=0.756240, validation/loss=1.053587, validation/num_examples=50000
I0302 06:28:28.830635 140089845819136 logging_writer.py:48] [512200] global_step=512200, grad_norm=5.234527111053467, loss=0.635715901851654
I0302 06:29:02.260396 140089837426432 logging_writer.py:48] [512300] global_step=512300, grad_norm=4.815877914428711, loss=0.597644567489624
I0302 06:29:35.688397 140089845819136 logging_writer.py:48] [512400] global_step=512400, grad_norm=4.433192729949951, loss=0.6253323554992676
I0302 06:30:09.158479 140089837426432 logging_writer.py:48] [512500] global_step=512500, grad_norm=4.677941799163818, loss=0.5742823481559753
I0302 06:30:42.707867 140089845819136 logging_writer.py:48] [512600] global_step=512600, grad_norm=4.559441089630127, loss=0.5833830237388611
I0302 06:31:16.178996 140089837426432 logging_writer.py:48] [512700] global_step=512700, grad_norm=4.5289788246154785, loss=0.6018347144126892
I0302 06:31:49.602996 140089845819136 logging_writer.py:48] [512800] global_step=512800, grad_norm=4.653560161590576, loss=0.6415424346923828
I0302 06:32:23.082460 140089837426432 logging_writer.py:48] [512900] global_step=512900, grad_norm=4.43305778503418, loss=0.6252643465995789
I0302 06:32:56.514284 140089845819136 logging_writer.py:48] [513000] global_step=513000, grad_norm=4.119939804077148, loss=0.6121773719787598
I0302 06:33:30.003519 140089837426432 logging_writer.py:48] [513100] global_step=513100, grad_norm=4.7050065994262695, loss=0.6834307312965393
I0302 06:34:03.468037 140089845819136 logging_writer.py:48] [513200] global_step=513200, grad_norm=4.4681396484375, loss=0.6574478149414062
I0302 06:34:36.899954 140089837426432 logging_writer.py:48] [513300] global_step=513300, grad_norm=4.870690822601318, loss=0.578914999961853
I0302 06:35:10.341173 140089845819136 logging_writer.py:48] [513400] global_step=513400, grad_norm=4.694272518157959, loss=0.6156820058822632
I0302 06:35:43.813121 140089837426432 logging_writer.py:48] [513500] global_step=513500, grad_norm=4.9612040519714355, loss=0.6152663230895996
I0302 06:36:17.378876 140089845819136 logging_writer.py:48] [513600] global_step=513600, grad_norm=4.8594069480896, loss=0.6752631664276123
I0302 06:36:50.818281 140089837426432 logging_writer.py:48] [513700] global_step=513700, grad_norm=4.953802108764648, loss=0.6785297393798828
I0302 06:36:56.310502 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:37:02.607189 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:37:10.680099 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:37:12.927538 140252611495744 submission_runner.py:411] Time since start: 177773.81s, 	Step: 513718, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14431409537792206, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.052869439125061, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8419123888015747, 'test/num_examples': 10000, 'score': 171934.87462615967, 'total_duration': 177773.80760240555, 'accumulated_submission_time': 171934.87462615967, 'accumulated_eval_time': 5795.874584913254, 'accumulated_logging_time': 24.5813729763031}
I0302 06:37:13.029214 140089837426432 logging_writer.py:48] [513718] accumulated_eval_time=5795.874585, accumulated_logging_time=24.581373, accumulated_submission_time=171934.874626, global_step=513718, preemption_count=0, score=171934.874626, test/accuracy=0.626800, test/loss=1.841912, test/num_examples=10000, total_duration=177773.807602, train/accuracy=0.961037, train/loss=0.144314, validation/accuracy=0.756040, validation/loss=1.052869, validation/num_examples=50000
I0302 06:37:40.826931 140089845819136 logging_writer.py:48] [513800] global_step=513800, grad_norm=4.547637939453125, loss=0.5671797394752502
I0302 06:38:14.326340 140089837426432 logging_writer.py:48] [513900] global_step=513900, grad_norm=4.2810211181640625, loss=0.6073223948478699
I0302 06:38:47.853662 140089845819136 logging_writer.py:48] [514000] global_step=514000, grad_norm=4.7776312828063965, loss=0.6247445940971375
I0302 06:39:21.350687 140089837426432 logging_writer.py:48] [514100] global_step=514100, grad_norm=4.394294738769531, loss=0.6433899402618408
I0302 06:39:54.831541 140089845819136 logging_writer.py:48] [514200] global_step=514200, grad_norm=4.377640247344971, loss=0.6500264406204224
I0302 06:40:28.286329 140089837426432 logging_writer.py:48] [514300] global_step=514300, grad_norm=5.090743064880371, loss=0.5996111035346985
I0302 06:41:01.759240 140089845819136 logging_writer.py:48] [514400] global_step=514400, grad_norm=5.012391567230225, loss=0.6562393307685852
I0302 06:41:35.280827 140089837426432 logging_writer.py:48] [514500] global_step=514500, grad_norm=4.402134895324707, loss=0.5779224038124084
I0302 06:42:08.750540 140089845819136 logging_writer.py:48] [514600] global_step=514600, grad_norm=4.427572727203369, loss=0.5923964977264404
I0302 06:42:42.285135 140089837426432 logging_writer.py:48] [514700] global_step=514700, grad_norm=4.275313854217529, loss=0.6345764994621277
I0302 06:43:15.777113 140089845819136 logging_writer.py:48] [514800] global_step=514800, grad_norm=4.387220859527588, loss=0.6535382270812988
I0302 06:43:49.205526 140089837426432 logging_writer.py:48] [514900] global_step=514900, grad_norm=3.9908664226531982, loss=0.525277316570282
I0302 06:44:22.686774 140089845819136 logging_writer.py:48] [515000] global_step=515000, grad_norm=4.4356303215026855, loss=0.5515444874763489
I0302 06:44:56.136293 140089837426432 logging_writer.py:48] [515100] global_step=515100, grad_norm=4.516122817993164, loss=0.617936909198761
I0302 06:45:29.573506 140089845819136 logging_writer.py:48] [515200] global_step=515200, grad_norm=4.80581521987915, loss=0.624298632144928
I0302 06:45:43.110013 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:45:49.249497 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:45:57.264833 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:45:59.562489 140252611495744 submission_runner.py:411] Time since start: 178300.44s, 	Step: 515242, 	{'train/accuracy': 0.9625318646430969, 'train/loss': 0.14274661242961884, 'validation/accuracy': 0.755620002746582, 'validation/loss': 1.054432988166809, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8430670499801636, 'test/num_examples': 10000, 'score': 172444.89008188248, 'total_duration': 178300.44254755974, 'accumulated_submission_time': 172444.89008188248, 'accumulated_eval_time': 5812.326997756958, 'accumulated_logging_time': 24.693369388580322}
I0302 06:45:59.665102 140089778697984 logging_writer.py:48] [515242] accumulated_eval_time=5812.326998, accumulated_logging_time=24.693369, accumulated_submission_time=172444.890082, global_step=515242, preemption_count=0, score=172444.890082, test/accuracy=0.626800, test/loss=1.843067, test/num_examples=10000, total_duration=178300.442548, train/accuracy=0.962532, train/loss=0.142747, validation/accuracy=0.755620, validation/loss=1.054433, validation/num_examples=50000
I0302 06:46:19.371196 140089862604544 logging_writer.py:48] [515300] global_step=515300, grad_norm=4.491514682769775, loss=0.6020745635032654
I0302 06:46:52.861680 140089778697984 logging_writer.py:48] [515400] global_step=515400, grad_norm=4.51969051361084, loss=0.6432149410247803
I0302 06:47:26.352629 140089862604544 logging_writer.py:48] [515500] global_step=515500, grad_norm=4.4981889724731445, loss=0.6228649020195007
I0302 06:47:59.781379 140089778697984 logging_writer.py:48] [515600] global_step=515600, grad_norm=4.596025466918945, loss=0.6067956686019897
I0302 06:48:33.307757 140089862604544 logging_writer.py:48] [515700] global_step=515700, grad_norm=4.385719299316406, loss=0.6307130455970764
I0302 06:49:06.774996 140089778697984 logging_writer.py:48] [515800] global_step=515800, grad_norm=4.736046314239502, loss=0.5999446511268616
I0302 06:49:40.253486 140089862604544 logging_writer.py:48] [515900] global_step=515900, grad_norm=4.235221862792969, loss=0.5432450175285339
I0302 06:50:13.682034 140089778697984 logging_writer.py:48] [516000] global_step=516000, grad_norm=4.921644687652588, loss=0.7045394778251648
I0302 06:50:47.146650 140089862604544 logging_writer.py:48] [516100] global_step=516100, grad_norm=4.4697089195251465, loss=0.6361565589904785
I0302 06:51:20.602306 140089778697984 logging_writer.py:48] [516200] global_step=516200, grad_norm=4.276393890380859, loss=0.5688789486885071
I0302 06:51:54.045966 140089862604544 logging_writer.py:48] [516300] global_step=516300, grad_norm=4.237146377563477, loss=0.5912485122680664
I0302 06:52:27.496131 140089778697984 logging_writer.py:48] [516400] global_step=516400, grad_norm=4.732136249542236, loss=0.6471418142318726
I0302 06:53:00.937790 140089862604544 logging_writer.py:48] [516500] global_step=516500, grad_norm=4.946646690368652, loss=0.6153960227966309
I0302 06:53:34.404521 140089778697984 logging_writer.py:48] [516600] global_step=516600, grad_norm=4.992016792297363, loss=0.634485125541687
I0302 06:54:07.837454 140089862604544 logging_writer.py:48] [516700] global_step=516700, grad_norm=4.6865715980529785, loss=0.604373037815094
I0302 06:54:29.809001 140252611495744 spec.py:321] Evaluating on the training split.
I0302 06:54:35.996416 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 06:54:43.988566 140252611495744 spec.py:349] Evaluating on the test split.
I0302 06:54:46.296350 140252611495744 submission_runner.py:411] Time since start: 178827.18s, 	Step: 516767, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14627687633037567, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0523682832717896, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8397737741470337, 'test/num_examples': 10000, 'score': 172954.96911978722, 'total_duration': 178827.1764163971, 'accumulated_submission_time': 172954.96911978722, 'accumulated_eval_time': 5828.814296245575, 'accumulated_logging_time': 24.806583642959595}
I0302 06:54:46.399556 140089770305280 logging_writer.py:48] [516767] accumulated_eval_time=5828.814296, accumulated_logging_time=24.806584, accumulated_submission_time=172954.969120, global_step=516767, preemption_count=0, score=172954.969120, test/accuracy=0.627000, test/loss=1.839774, test/num_examples=10000, total_duration=178827.176416, train/accuracy=0.960619, train/loss=0.146277, validation/accuracy=0.755880, validation/loss=1.052368, validation/num_examples=50000
I0302 06:54:57.796661 140089845819136 logging_writer.py:48] [516800] global_step=516800, grad_norm=4.423110485076904, loss=0.5695745944976807
I0302 06:55:31.270006 140089770305280 logging_writer.py:48] [516900] global_step=516900, grad_norm=4.9418110847473145, loss=0.6106640100479126
I0302 06:56:04.751608 140089845819136 logging_writer.py:48] [517000] global_step=517000, grad_norm=4.722665309906006, loss=0.6169534921646118
I0302 06:56:38.308793 140089770305280 logging_writer.py:48] [517100] global_step=517100, grad_norm=4.489121913909912, loss=0.5989575386047363
I0302 06:57:11.767522 140089845819136 logging_writer.py:48] [517200] global_step=517200, grad_norm=4.663789749145508, loss=0.6800976991653442
I0302 06:57:45.197194 140089770305280 logging_writer.py:48] [517300] global_step=517300, grad_norm=4.44527006149292, loss=0.6403218507766724
I0302 06:58:18.708501 140089845819136 logging_writer.py:48] [517400] global_step=517400, grad_norm=4.584763050079346, loss=0.6123536229133606
I0302 06:58:52.207865 140089770305280 logging_writer.py:48] [517500] global_step=517500, grad_norm=4.536417007446289, loss=0.6490562558174133
I0302 06:59:25.678223 140089845819136 logging_writer.py:48] [517600] global_step=517600, grad_norm=4.8935418128967285, loss=0.6511480808258057
I0302 06:59:59.134377 140089770305280 logging_writer.py:48] [517700] global_step=517700, grad_norm=4.3066606521606445, loss=0.5424929857254028
I0302 07:00:32.672746 140089845819136 logging_writer.py:48] [517800] global_step=517800, grad_norm=4.967899322509766, loss=0.6396669745445251
I0302 07:01:06.155822 140089770305280 logging_writer.py:48] [517900] global_step=517900, grad_norm=4.767142295837402, loss=0.6951071619987488
I0302 07:01:39.594691 140089845819136 logging_writer.py:48] [518000] global_step=518000, grad_norm=4.496794700622559, loss=0.6369055509567261
I0302 07:02:13.043069 140089770305280 logging_writer.py:48] [518100] global_step=518100, grad_norm=5.149195194244385, loss=0.5925410389900208
I0302 07:02:46.501165 140089845819136 logging_writer.py:48] [518200] global_step=518200, grad_norm=4.286204814910889, loss=0.5204128623008728
I0302 07:03:16.399938 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:03:22.511590 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:03:30.521109 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:03:32.808740 140252611495744 submission_runner.py:411] Time since start: 179353.69s, 	Step: 518291, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.14684636890888214, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0545798540115356, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8436776399612427, 'test/num_examples': 10000, 'score': 173464.90362358093, 'total_duration': 179353.68880295753, 'accumulated_submission_time': 173464.90362358093, 'accumulated_eval_time': 5845.223039627075, 'accumulated_logging_time': 24.920645713806152}
I0302 07:03:32.910737 140089837426432 logging_writer.py:48] [518291] accumulated_eval_time=5845.223040, accumulated_logging_time=24.920646, accumulated_submission_time=173464.903624, global_step=518291, preemption_count=0, score=173464.903624, test/accuracy=0.627100, test/loss=1.843678, test/num_examples=10000, total_duration=179353.688803, train/accuracy=0.960479, train/loss=0.146846, validation/accuracy=0.755900, validation/loss=1.054580, validation/num_examples=50000
I0302 07:03:36.260815 140089862604544 logging_writer.py:48] [518300] global_step=518300, grad_norm=4.007167816162109, loss=0.6238732933998108
I0302 07:04:09.689949 140089837426432 logging_writer.py:48] [518400] global_step=518400, grad_norm=4.602260589599609, loss=0.6052878499031067
I0302 07:04:43.177302 140089862604544 logging_writer.py:48] [518500] global_step=518500, grad_norm=4.423563480377197, loss=0.6338849067687988
I0302 07:05:16.634394 140089837426432 logging_writer.py:48] [518600] global_step=518600, grad_norm=4.587383270263672, loss=0.6028841733932495
I0302 07:05:50.087773 140089862604544 logging_writer.py:48] [518700] global_step=518700, grad_norm=4.250443458557129, loss=0.5735902786254883
I0302 07:06:23.543742 140089837426432 logging_writer.py:48] [518800] global_step=518800, grad_norm=4.408902168273926, loss=0.5975211262702942
I0302 07:06:57.040103 140089862604544 logging_writer.py:48] [518900] global_step=518900, grad_norm=4.527134895324707, loss=0.6132919788360596
I0302 07:07:30.471532 140089837426432 logging_writer.py:48] [519000] global_step=519000, grad_norm=4.391264915466309, loss=0.66521155834198
I0302 07:08:03.921403 140089862604544 logging_writer.py:48] [519100] global_step=519100, grad_norm=4.799893379211426, loss=0.693284809589386
I0302 07:08:37.363944 140089837426432 logging_writer.py:48] [519200] global_step=519200, grad_norm=4.366513729095459, loss=0.6664357781410217
I0302 07:09:10.836943 140089862604544 logging_writer.py:48] [519300] global_step=519300, grad_norm=4.2842488288879395, loss=0.6288806200027466
I0302 07:09:44.256266 140089837426432 logging_writer.py:48] [519400] global_step=519400, grad_norm=4.509069442749023, loss=0.6592475175857544
I0302 07:10:17.704533 140089862604544 logging_writer.py:48] [519500] global_step=519500, grad_norm=4.972683429718018, loss=0.6117477416992188
I0302 07:10:51.120739 140089837426432 logging_writer.py:48] [519600] global_step=519600, grad_norm=4.5101237297058105, loss=0.5892917513847351
I0302 07:11:24.589187 140089862604544 logging_writer.py:48] [519700] global_step=519700, grad_norm=4.949897289276123, loss=0.6400882601737976
I0302 07:11:58.050918 140089837426432 logging_writer.py:48] [519800] global_step=519800, grad_norm=4.334149360656738, loss=0.6393042206764221
I0302 07:12:02.891852 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:12:09.118695 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:12:17.218469 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:12:19.528457 140252611495744 submission_runner.py:411] Time since start: 179880.41s, 	Step: 519816, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.14565972983837128, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0524157285690308, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8394850492477417, 'test/num_examples': 10000, 'score': 173974.819116354, 'total_duration': 179880.40839219093, 'accumulated_submission_time': 173974.819116354, 'accumulated_eval_time': 5861.859467506409, 'accumulated_logging_time': 25.033472061157227}
I0302 07:12:19.632901 140089770305280 logging_writer.py:48] [519816] accumulated_eval_time=5861.859468, accumulated_logging_time=25.033472, accumulated_submission_time=173974.819116, global_step=519816, preemption_count=0, score=173974.819116, test/accuracy=0.627800, test/loss=1.839485, test/num_examples=10000, total_duration=179880.408392, train/accuracy=0.961296, train/loss=0.145660, validation/accuracy=0.755600, validation/loss=1.052416, validation/num_examples=50000
I0302 07:12:48.152481 140089778697984 logging_writer.py:48] [519900] global_step=519900, grad_norm=4.222630500793457, loss=0.5844637155532837
I0302 07:13:21.615499 140089770305280 logging_writer.py:48] [520000] global_step=520000, grad_norm=4.267605781555176, loss=0.5230712890625
I0302 07:13:55.071337 140089778697984 logging_writer.py:48] [520100] global_step=520100, grad_norm=4.779983043670654, loss=0.6420926451683044
I0302 07:14:28.525264 140089770305280 logging_writer.py:48] [520200] global_step=520200, grad_norm=4.483860015869141, loss=0.6875945329666138
I0302 07:15:01.953994 140089778697984 logging_writer.py:48] [520300] global_step=520300, grad_norm=5.2225847244262695, loss=0.7155247330665588
I0302 07:15:35.395162 140089770305280 logging_writer.py:48] [520400] global_step=520400, grad_norm=4.478704929351807, loss=0.5913094282150269
I0302 07:16:08.825726 140089778697984 logging_writer.py:48] [520500] global_step=520500, grad_norm=5.073781490325928, loss=0.6853730082511902
I0302 07:16:42.277628 140089770305280 logging_writer.py:48] [520600] global_step=520600, grad_norm=4.4025468826293945, loss=0.4926988482475281
I0302 07:17:15.725770 140089778697984 logging_writer.py:48] [520700] global_step=520700, grad_norm=5.033729553222656, loss=0.6843885183334351
I0302 07:17:49.151819 140089770305280 logging_writer.py:48] [520800] global_step=520800, grad_norm=4.58397912979126, loss=0.5835464000701904
I0302 07:18:22.586594 140089778697984 logging_writer.py:48] [520900] global_step=520900, grad_norm=4.856125354766846, loss=0.5919979214668274
I0302 07:18:56.087644 140089770305280 logging_writer.py:48] [521000] global_step=521000, grad_norm=4.549121856689453, loss=0.6197139620780945
I0302 07:19:29.526528 140089778697984 logging_writer.py:48] [521100] global_step=521100, grad_norm=4.36517858505249, loss=0.541513979434967
I0302 07:20:02.980360 140089770305280 logging_writer.py:48] [521200] global_step=521200, grad_norm=4.990673065185547, loss=0.6294947266578674
I0302 07:20:36.422448 140089778697984 logging_writer.py:48] [521300] global_step=521300, grad_norm=4.35919713973999, loss=0.622614860534668
I0302 07:20:49.607954 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:20:55.689396 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:21:03.988786 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:21:06.279222 140252611495744 submission_runner.py:411] Time since start: 180407.16s, 	Step: 521341, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14454030990600586, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.05410897731781, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8433527946472168, 'test/num_examples': 10000, 'score': 174484.72979211807, 'total_duration': 180407.15928792953, 'accumulated_submission_time': 174484.72979211807, 'accumulated_eval_time': 5878.530686855316, 'accumulated_logging_time': 25.147634506225586}
I0302 07:21:06.383757 140089770305280 logging_writer.py:48] [521341] accumulated_eval_time=5878.530687, accumulated_logging_time=25.147635, accumulated_submission_time=174484.729792, global_step=521341, preemption_count=0, score=174484.729792, test/accuracy=0.627300, test/loss=1.843353, test/num_examples=10000, total_duration=180407.159288, train/accuracy=0.961834, train/loss=0.144540, validation/accuracy=0.756000, validation/loss=1.054109, validation/num_examples=50000
I0302 07:21:26.478993 140089862604544 logging_writer.py:48] [521400] global_step=521400, grad_norm=4.8197922706604, loss=0.5871495604515076
I0302 07:21:59.957812 140089770305280 logging_writer.py:48] [521500] global_step=521500, grad_norm=5.0887579917907715, loss=0.7096748352050781
I0302 07:22:33.411952 140089862604544 logging_writer.py:48] [521600] global_step=521600, grad_norm=4.538136005401611, loss=0.5713005065917969
I0302 07:23:06.852706 140089770305280 logging_writer.py:48] [521700] global_step=521700, grad_norm=4.377389430999756, loss=0.624423086643219
I0302 07:23:40.296058 140089862604544 logging_writer.py:48] [521800] global_step=521800, grad_norm=4.825192928314209, loss=0.5903002619743347
I0302 07:24:13.719159 140089770305280 logging_writer.py:48] [521900] global_step=521900, grad_norm=4.519647121429443, loss=0.6049236059188843
I0302 07:24:47.203113 140089862604544 logging_writer.py:48] [522000] global_step=522000, grad_norm=4.826883316040039, loss=0.6822147369384766
I0302 07:25:20.830649 140089770305280 logging_writer.py:48] [522100] global_step=522100, grad_norm=4.428617477416992, loss=0.5873071551322937
I0302 07:25:54.256716 140089862604544 logging_writer.py:48] [522200] global_step=522200, grad_norm=4.610958099365234, loss=0.6413654088973999
I0302 07:26:27.699522 140089770305280 logging_writer.py:48] [522300] global_step=522300, grad_norm=4.614851951599121, loss=0.6546088457107544
I0302 07:27:01.144383 140089862604544 logging_writer.py:48] [522400] global_step=522400, grad_norm=4.138092041015625, loss=0.5875793695449829
I0302 07:27:34.624682 140089770305280 logging_writer.py:48] [522500] global_step=522500, grad_norm=4.644108295440674, loss=0.6193122863769531
I0302 07:28:08.077934 140089862604544 logging_writer.py:48] [522600] global_step=522600, grad_norm=4.932289123535156, loss=0.6538848876953125
I0302 07:28:41.538607 140089770305280 logging_writer.py:48] [522700] global_step=522700, grad_norm=4.662078857421875, loss=0.6073448657989502
I0302 07:29:15.032274 140089862604544 logging_writer.py:48] [522800] global_step=522800, grad_norm=4.557357311248779, loss=0.654629647731781
I0302 07:29:36.583945 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:29:42.684787 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:29:50.816951 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:29:53.138795 140252611495744 submission_runner.py:411] Time since start: 180934.02s, 	Step: 522866, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.1441616415977478, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0527174472808838, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.842559814453125, 'test/num_examples': 10000, 'score': 174994.86545610428, 'total_duration': 180934.01882386208, 'accumulated_submission_time': 174994.86545610428, 'accumulated_eval_time': 5895.08545088768, 'accumulated_logging_time': 25.262465476989746}
I0302 07:29:53.300981 140089778697984 logging_writer.py:48] [522866] accumulated_eval_time=5895.085451, accumulated_logging_time=25.262465, accumulated_submission_time=174994.865456, global_step=522866, preemption_count=0, score=174994.865456, test/accuracy=0.626700, test/loss=1.842560, test/num_examples=10000, total_duration=180934.018824, train/accuracy=0.961934, train/loss=0.144162, validation/accuracy=0.755760, validation/loss=1.052717, validation/num_examples=50000
I0302 07:30:05.020114 140089837426432 logging_writer.py:48] [522900] global_step=522900, grad_norm=4.695127010345459, loss=0.6528909802436829
I0302 07:30:38.480736 140089778697984 logging_writer.py:48] [523000] global_step=523000, grad_norm=4.474781036376953, loss=0.5630831718444824
I0302 07:31:12.038251 140089837426432 logging_writer.py:48] [523100] global_step=523100, grad_norm=4.196892738342285, loss=0.6195189952850342
I0302 07:31:45.492909 140089778697984 logging_writer.py:48] [523200] global_step=523200, grad_norm=4.589285373687744, loss=0.6121873259544373
I0302 07:32:18.958497 140089837426432 logging_writer.py:48] [523300] global_step=523300, grad_norm=4.475265979766846, loss=0.636233925819397
I0302 07:32:52.441503 140089778697984 logging_writer.py:48] [523400] global_step=523400, grad_norm=4.771721363067627, loss=0.6138315796852112
I0302 07:33:25.890440 140089837426432 logging_writer.py:48] [523500] global_step=523500, grad_norm=5.4140305519104, loss=0.6059091687202454
I0302 07:33:59.333293 140089778697984 logging_writer.py:48] [523600] global_step=523600, grad_norm=4.803499698638916, loss=0.6234238743782043
I0302 07:34:32.782776 140089837426432 logging_writer.py:48] [523700] global_step=523700, grad_norm=4.505289077758789, loss=0.6313665509223938
I0302 07:35:06.225597 140089778697984 logging_writer.py:48] [523800] global_step=523800, grad_norm=4.411555767059326, loss=0.5931481122970581
I0302 07:35:39.674355 140089837426432 logging_writer.py:48] [523900] global_step=523900, grad_norm=4.704763412475586, loss=0.711260974407196
I0302 07:36:13.193037 140089778697984 logging_writer.py:48] [524000] global_step=524000, grad_norm=4.568349361419678, loss=0.7020595073699951
I0302 07:36:46.621633 140089837426432 logging_writer.py:48] [524100] global_step=524100, grad_norm=4.64838171005249, loss=0.6902297735214233
I0302 07:37:20.135607 140089778697984 logging_writer.py:48] [524200] global_step=524200, grad_norm=4.478004455566406, loss=0.620450496673584
I0302 07:37:53.602030 140089837426432 logging_writer.py:48] [524300] global_step=524300, grad_norm=4.131612300872803, loss=0.5569993853569031
I0302 07:38:23.206228 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:38:29.320868 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:38:37.415376 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:38:39.683466 140252611495744 submission_runner.py:411] Time since start: 181460.56s, 	Step: 524390, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.1470319777727127, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.053484320640564, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8422954082489014, 'test/num_examples': 10000, 'score': 175504.70123004913, 'total_duration': 181460.5635290146, 'accumulated_submission_time': 175504.70123004913, 'accumulated_eval_time': 5911.562630414963, 'accumulated_logging_time': 25.43892741203308}
I0302 07:38:39.785431 140089778697984 logging_writer.py:48] [524390] accumulated_eval_time=5911.562630, accumulated_logging_time=25.438927, accumulated_submission_time=175504.701230, global_step=524390, preemption_count=0, score=175504.701230, test/accuracy=0.627600, test/loss=1.842295, test/num_examples=10000, total_duration=181460.563529, train/accuracy=0.960778, train/loss=0.147032, validation/accuracy=0.755760, validation/loss=1.053484, validation/num_examples=50000
I0302 07:38:43.475069 140089862604544 logging_writer.py:48] [524400] global_step=524400, grad_norm=4.475329875946045, loss=0.5750861167907715
I0302 07:39:16.923164 140089778697984 logging_writer.py:48] [524500] global_step=524500, grad_norm=4.697627544403076, loss=0.5854153633117676
I0302 07:39:50.395538 140089862604544 logging_writer.py:48] [524600] global_step=524600, grad_norm=4.540473937988281, loss=0.603830099105835
I0302 07:40:23.860480 140089778697984 logging_writer.py:48] [524700] global_step=524700, grad_norm=4.992747783660889, loss=0.6051353216171265
I0302 07:40:57.328128 140089862604544 logging_writer.py:48] [524800] global_step=524800, grad_norm=4.249571800231934, loss=0.5792537331581116
I0302 07:41:30.786587 140089778697984 logging_writer.py:48] [524900] global_step=524900, grad_norm=4.605363845825195, loss=0.6309927105903625
I0302 07:42:04.229706 140089862604544 logging_writer.py:48] [525000] global_step=525000, grad_norm=4.400016784667969, loss=0.5794605016708374
I0302 07:42:37.717545 140089778697984 logging_writer.py:48] [525100] global_step=525100, grad_norm=4.567640781402588, loss=0.6705933809280396
I0302 07:43:11.377607 140089862604544 logging_writer.py:48] [525200] global_step=525200, grad_norm=6.322304725646973, loss=0.6656358242034912
I0302 07:43:44.818136 140089778697984 logging_writer.py:48] [525300] global_step=525300, grad_norm=4.580685138702393, loss=0.6859169602394104
I0302 07:44:18.299825 140089862604544 logging_writer.py:48] [525400] global_step=525400, grad_norm=4.279343605041504, loss=0.5965566039085388
I0302 07:44:51.789874 140089778697984 logging_writer.py:48] [525500] global_step=525500, grad_norm=4.42316198348999, loss=0.5915891528129578
I0302 07:45:25.217616 140089862604544 logging_writer.py:48] [525600] global_step=525600, grad_norm=4.061052322387695, loss=0.5523366332054138
I0302 07:45:58.688041 140089778697984 logging_writer.py:48] [525700] global_step=525700, grad_norm=5.96829891204834, loss=0.7156156301498413
I0302 07:46:32.167134 140089862604544 logging_writer.py:48] [525800] global_step=525800, grad_norm=4.627531051635742, loss=0.6423121690750122
I0302 07:47:05.603456 140089778697984 logging_writer.py:48] [525900] global_step=525900, grad_norm=4.416846752166748, loss=0.6503611207008362
I0302 07:47:09.766977 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:47:15.840654 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:47:23.960652 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:47:26.237207 140252611495744 submission_runner.py:411] Time since start: 181987.12s, 	Step: 525914, 	{'train/accuracy': 0.958984375, 'train/loss': 0.1497214436531067, 'validation/accuracy': 0.7561599612236023, 'validation/loss': 1.0536900758743286, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8431615829467773, 'test/num_examples': 10000, 'score': 176014.61433887482, 'total_duration': 181987.11727404594, 'accumulated_submission_time': 176014.61433887482, 'accumulated_eval_time': 5928.032803058624, 'accumulated_logging_time': 25.552964210510254}
I0302 07:47:26.344902 140089837426432 logging_writer.py:48] [525914] accumulated_eval_time=5928.032803, accumulated_logging_time=25.552964, accumulated_submission_time=176014.614339, global_step=525914, preemption_count=0, score=176014.614339, test/accuracy=0.627000, test/loss=1.843162, test/num_examples=10000, total_duration=181987.117274, train/accuracy=0.958984, train/loss=0.149721, validation/accuracy=0.756160, validation/loss=1.053690, validation/num_examples=50000
I0302 07:47:55.488261 140089845819136 logging_writer.py:48] [526000] global_step=526000, grad_norm=4.7765212059021, loss=0.6761398315429688
I0302 07:48:28.925079 140089837426432 logging_writer.py:48] [526100] global_step=526100, grad_norm=4.643795967102051, loss=0.595731794834137
I0302 07:49:02.382428 140089845819136 logging_writer.py:48] [526200] global_step=526200, grad_norm=4.561966896057129, loss=0.6334095001220703
I0302 07:49:36.015455 140089837426432 logging_writer.py:48] [526300] global_step=526300, grad_norm=4.780048370361328, loss=0.5902704000473022
I0302 07:50:09.446519 140089845819136 logging_writer.py:48] [526400] global_step=526400, grad_norm=4.607888698577881, loss=0.5845026969909668
I0302 07:50:42.916998 140089837426432 logging_writer.py:48] [526500] global_step=526500, grad_norm=5.210252285003662, loss=0.7031006813049316
I0302 07:51:16.423917 140089845819136 logging_writer.py:48] [526600] global_step=526600, grad_norm=4.7689080238342285, loss=0.6131947040557861
I0302 07:51:49.867199 140089837426432 logging_writer.py:48] [526700] global_step=526700, grad_norm=5.064720630645752, loss=0.5975106954574585
I0302 07:52:23.326008 140089845819136 logging_writer.py:48] [526800] global_step=526800, grad_norm=4.976682186126709, loss=0.654761016368866
I0302 07:52:56.782231 140089837426432 logging_writer.py:48] [526900] global_step=526900, grad_norm=4.2261223793029785, loss=0.5320519208908081
I0302 07:53:30.232360 140089845819136 logging_writer.py:48] [527000] global_step=527000, grad_norm=4.344735145568848, loss=0.5704705119132996
I0302 07:54:03.722003 140089837426432 logging_writer.py:48] [527100] global_step=527100, grad_norm=4.575338363647461, loss=0.6248564720153809
I0302 07:54:37.196902 140089845819136 logging_writer.py:48] [527200] global_step=527200, grad_norm=4.52400016784668, loss=0.6477423906326294
I0302 07:55:10.632061 140089837426432 logging_writer.py:48] [527300] global_step=527300, grad_norm=5.076940059661865, loss=0.6131855249404907
I0302 07:55:44.213599 140089845819136 logging_writer.py:48] [527400] global_step=527400, grad_norm=4.207258224487305, loss=0.5921311378479004
I0302 07:55:56.390641 140252611495744 spec.py:321] Evaluating on the training split.
I0302 07:56:02.831421 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 07:56:10.811025 140252611495744 spec.py:349] Evaluating on the test split.
I0302 07:56:13.236269 140252611495744 submission_runner.py:411] Time since start: 182514.12s, 	Step: 527438, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14564043283462524, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0531024932861328, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8414415121078491, 'test/num_examples': 10000, 'score': 176524.59498858452, 'total_duration': 182514.11633515358, 'accumulated_submission_time': 176524.59498858452, 'accumulated_eval_time': 5944.878372192383, 'accumulated_logging_time': 25.67080807685852}
I0302 07:56:13.342941 140089862604544 logging_writer.py:48] [527438] accumulated_eval_time=5944.878372, accumulated_logging_time=25.670808, accumulated_submission_time=176524.594989, global_step=527438, preemption_count=0, score=176524.594989, test/accuracy=0.627800, test/loss=1.841442, test/num_examples=10000, total_duration=182514.116335, train/accuracy=0.961635, train/loss=0.145640, validation/accuracy=0.756060, validation/loss=1.053102, validation/num_examples=50000
I0302 07:56:34.405443 140089870997248 logging_writer.py:48] [527500] global_step=527500, grad_norm=4.128246307373047, loss=0.5951259732246399
I0302 07:57:07.836939 140089862604544 logging_writer.py:48] [527600] global_step=527600, grad_norm=4.523892402648926, loss=0.6611868143081665
I0302 07:57:41.302297 140089870997248 logging_writer.py:48] [527700] global_step=527700, grad_norm=4.568972110748291, loss=0.6279244422912598
I0302 07:58:14.742055 140089862604544 logging_writer.py:48] [527800] global_step=527800, grad_norm=4.467440128326416, loss=0.6824952363967896
I0302 07:58:48.215661 140089870997248 logging_writer.py:48] [527900] global_step=527900, grad_norm=4.72041654586792, loss=0.7169945240020752
I0302 07:59:21.655103 140089862604544 logging_writer.py:48] [528000] global_step=528000, grad_norm=4.307323932647705, loss=0.603875994682312
I0302 07:59:55.102061 140089870997248 logging_writer.py:48] [528100] global_step=528100, grad_norm=4.514718055725098, loss=0.6821987628936768
I0302 08:00:28.621844 140089862604544 logging_writer.py:48] [528200] global_step=528200, grad_norm=4.905316352844238, loss=0.6817671060562134
I0302 08:01:02.066897 140089870997248 logging_writer.py:48] [528300] global_step=528300, grad_norm=4.556980609893799, loss=0.6266531348228455
I0302 08:01:35.663980 140089862604544 logging_writer.py:48] [528400] global_step=528400, grad_norm=4.058305740356445, loss=0.5278452634811401
I0302 08:02:09.134411 140089870997248 logging_writer.py:48] [528500] global_step=528500, grad_norm=4.823129653930664, loss=0.6158608198165894
I0302 08:02:42.596364 140089862604544 logging_writer.py:48] [528600] global_step=528600, grad_norm=4.736064434051514, loss=0.585566520690918
I0302 08:03:16.035710 140089870997248 logging_writer.py:48] [528700] global_step=528700, grad_norm=4.529271602630615, loss=0.6581434011459351
I0302 08:03:49.550064 140089862604544 logging_writer.py:48] [528800] global_step=528800, grad_norm=4.839661598205566, loss=0.6563848257064819
I0302 08:04:23.039455 140089870997248 logging_writer.py:48] [528900] global_step=528900, grad_norm=5.066937446594238, loss=0.5781817436218262
I0302 08:04:43.310587 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:04:49.519010 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:04:57.453022 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:04:59.731806 140252611495744 submission_runner.py:411] Time since start: 183040.61s, 	Step: 528962, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14529374241828918, 'validation/accuracy': 0.7559199929237366, 'validation/loss': 1.05355703830719, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8419240713119507, 'test/num_examples': 10000, 'score': 177034.4949579239, 'total_duration': 183040.61176109314, 'accumulated_submission_time': 177034.4949579239, 'accumulated_eval_time': 5961.299426794052, 'accumulated_logging_time': 25.78989005088806}
I0302 08:04:59.836691 140089778697984 logging_writer.py:48] [528962] accumulated_eval_time=5961.299427, accumulated_logging_time=25.789890, accumulated_submission_time=177034.494958, global_step=528962, preemption_count=0, score=177034.494958, test/accuracy=0.627500, test/loss=1.841924, test/num_examples=10000, total_duration=183040.611761, train/accuracy=0.960619, train/loss=0.145294, validation/accuracy=0.755920, validation/loss=1.053557, validation/num_examples=50000
I0302 08:05:13.750464 140089837426432 logging_writer.py:48] [529000] global_step=529000, grad_norm=5.299375534057617, loss=0.6761711835861206
I0302 08:05:47.227298 140089778697984 logging_writer.py:48] [529100] global_step=529100, grad_norm=4.137908935546875, loss=0.6206072568893433
I0302 08:06:20.668013 140089837426432 logging_writer.py:48] [529200] global_step=529200, grad_norm=4.805147171020508, loss=0.6039499640464783
I0302 08:06:54.166791 140089778697984 logging_writer.py:48] [529300] global_step=529300, grad_norm=4.9543304443359375, loss=0.5923970341682434
I0302 08:07:27.605624 140089837426432 logging_writer.py:48] [529400] global_step=529400, grad_norm=4.338099956512451, loss=0.579083263874054
I0302 08:08:01.177360 140089778697984 logging_writer.py:48] [529500] global_step=529500, grad_norm=4.309897422790527, loss=0.6121147871017456
I0302 08:08:34.635832 140089837426432 logging_writer.py:48] [529600] global_step=529600, grad_norm=3.91988205909729, loss=0.573607861995697
I0302 08:09:08.109423 140089778697984 logging_writer.py:48] [529700] global_step=529700, grad_norm=4.441744327545166, loss=0.6668274402618408
I0302 08:09:41.543055 140089837426432 logging_writer.py:48] [529800] global_step=529800, grad_norm=4.550862789154053, loss=0.606928288936615
I0302 08:10:14.996322 140089778697984 logging_writer.py:48] [529900] global_step=529900, grad_norm=4.5710768699646, loss=0.6123239994049072
I0302 08:10:48.437062 140089837426432 logging_writer.py:48] [530000] global_step=530000, grad_norm=4.574629783630371, loss=0.6355525255203247
I0302 08:11:21.881160 140089778697984 logging_writer.py:48] [530100] global_step=530100, grad_norm=4.456437110900879, loss=0.6105127334594727
I0302 08:11:55.355967 140089837426432 logging_writer.py:48] [530200] global_step=530200, grad_norm=3.9090583324432373, loss=0.533388614654541
I0302 08:12:28.791139 140089778697984 logging_writer.py:48] [530300] global_step=530300, grad_norm=4.785789489746094, loss=0.6638587713241577
I0302 08:13:02.286844 140089837426432 logging_writer.py:48] [530400] global_step=530400, grad_norm=4.652070045471191, loss=0.6500160098075867
I0302 08:13:29.875225 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:13:36.330832 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:13:44.398106 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:13:46.736711 140252611495744 submission_runner.py:411] Time since start: 183567.62s, 	Step: 530484, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14444096386432648, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.053533673286438, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.84251868724823, 'test/num_examples': 10000, 'score': 177543.64189291, 'total_duration': 183567.61677789688, 'accumulated_submission_time': 177543.64189291, 'accumulated_eval_time': 5978.160856962204, 'accumulated_logging_time': 26.731829404830933}
I0302 08:13:46.842713 140089770305280 logging_writer.py:48] [530484] accumulated_eval_time=5978.160857, accumulated_logging_time=26.731829, accumulated_submission_time=177543.641893, global_step=530484, preemption_count=0, score=177543.641893, test/accuracy=0.626600, test/loss=1.842519, test/num_examples=10000, total_duration=183567.616778, train/accuracy=0.960957, train/loss=0.144441, validation/accuracy=0.755940, validation/loss=1.053534, validation/num_examples=50000
I0302 08:13:52.549673 140089778697984 logging_writer.py:48] [530500] global_step=530500, grad_norm=4.673425674438477, loss=0.616746723651886
I0302 08:14:26.008917 140089770305280 logging_writer.py:48] [530600] global_step=530600, grad_norm=4.266403675079346, loss=0.5633529424667358
I0302 08:14:59.451013 140089778697984 logging_writer.py:48] [530700] global_step=530700, grad_norm=4.644363880157471, loss=0.6520174145698547
I0302 08:15:32.922386 140089770305280 logging_writer.py:48] [530800] global_step=530800, grad_norm=4.634085655212402, loss=0.6478282809257507
I0302 08:16:06.402413 140089778697984 logging_writer.py:48] [530900] global_step=530900, grad_norm=4.816487789154053, loss=0.6794665455818176
I0302 08:16:39.824963 140089770305280 logging_writer.py:48] [531000] global_step=531000, grad_norm=4.8858256340026855, loss=0.6860358715057373
I0302 08:17:13.266800 140089778697984 logging_writer.py:48] [531100] global_step=531100, grad_norm=4.2386474609375, loss=0.6116958856582642
I0302 08:17:46.692243 140089770305280 logging_writer.py:48] [531200] global_step=531200, grad_norm=4.336875915527344, loss=0.6060748100280762
I0302 08:18:20.134163 140089778697984 logging_writer.py:48] [531300] global_step=531300, grad_norm=4.740810871124268, loss=0.6978020668029785
I0302 08:18:53.579697 140089770305280 logging_writer.py:48] [531400] global_step=531400, grad_norm=4.731095790863037, loss=0.5920732021331787
I0302 08:19:27.005747 140089778697984 logging_writer.py:48] [531500] global_step=531500, grad_norm=4.378273963928223, loss=0.6214010715484619
I0302 08:20:00.568136 140089770305280 logging_writer.py:48] [531600] global_step=531600, grad_norm=4.707623481750488, loss=0.6360511779785156
I0302 08:20:34.016401 140089778697984 logging_writer.py:48] [531700] global_step=531700, grad_norm=4.55055570602417, loss=0.6137471199035645
I0302 08:21:07.440140 140089770305280 logging_writer.py:48] [531800] global_step=531800, grad_norm=4.351109504699707, loss=0.5689989328384399
I0302 08:21:40.907474 140089778697984 logging_writer.py:48] [531900] global_step=531900, grad_norm=4.64508581161499, loss=0.645560622215271
I0302 08:22:14.330434 140089770305280 logging_writer.py:48] [532000] global_step=532000, grad_norm=4.541382789611816, loss=0.6426327228546143
I0302 08:22:16.807594 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:22:23.003103 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:22:31.055992 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:22:33.325399 140252611495744 submission_runner.py:411] Time since start: 184094.21s, 	Step: 532009, 	{'train/accuracy': 0.9603993892669678, 'train/loss': 0.14790327847003937, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0539860725402832, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.8430086374282837, 'test/num_examples': 10000, 'score': 178053.5409553051, 'total_duration': 184094.20546674728, 'accumulated_submission_time': 178053.5409553051, 'accumulated_eval_time': 5994.678615808487, 'accumulated_logging_time': 26.848421096801758}
I0302 08:22:33.433019 140089770305280 logging_writer.py:48] [532009] accumulated_eval_time=5994.678616, accumulated_logging_time=26.848421, accumulated_submission_time=178053.540955, global_step=532009, preemption_count=0, score=178053.540955, test/accuracy=0.627000, test/loss=1.843009, test/num_examples=10000, total_duration=184094.205467, train/accuracy=0.960399, train/loss=0.147903, validation/accuracy=0.755880, validation/loss=1.053986, validation/num_examples=50000
I0302 08:23:04.217068 140089845819136 logging_writer.py:48] [532100] global_step=532100, grad_norm=4.632339000701904, loss=0.628142774105072
I0302 08:23:37.663909 140089770305280 logging_writer.py:48] [532200] global_step=532200, grad_norm=4.441016674041748, loss=0.5647637844085693
I0302 08:24:11.123866 140089845819136 logging_writer.py:48] [532300] global_step=532300, grad_norm=4.613924503326416, loss=0.6660494208335876
I0302 08:24:44.622533 140089770305280 logging_writer.py:48] [532400] global_step=532400, grad_norm=4.52609395980835, loss=0.622298002243042
I0302 08:25:18.139227 140089845819136 logging_writer.py:48] [532500] global_step=532500, grad_norm=5.189746856689453, loss=0.6359618902206421
I0302 08:25:51.671423 140089770305280 logging_writer.py:48] [532600] global_step=532600, grad_norm=4.819828033447266, loss=0.6684291362762451
I0302 08:26:25.184595 140089845819136 logging_writer.py:48] [532700] global_step=532700, grad_norm=4.351941108703613, loss=0.5991864204406738
I0302 08:26:58.685169 140089770305280 logging_writer.py:48] [532800] global_step=532800, grad_norm=4.503008842468262, loss=0.6231509447097778
I0302 08:27:32.137170 140089845819136 logging_writer.py:48] [532900] global_step=532900, grad_norm=4.504167556762695, loss=0.6884548664093018
I0302 08:28:05.563450 140089770305280 logging_writer.py:48] [533000] global_step=533000, grad_norm=4.560028553009033, loss=0.6526836156845093
I0302 08:28:39.025128 140089845819136 logging_writer.py:48] [533100] global_step=533100, grad_norm=4.6615753173828125, loss=0.6650566458702087
I0302 08:29:12.472815 140089770305280 logging_writer.py:48] [533200] global_step=533200, grad_norm=4.446820259094238, loss=0.6212860941886902
I0302 08:29:45.963300 140089845819136 logging_writer.py:48] [533300] global_step=533300, grad_norm=4.72132682800293, loss=0.6075699329376221
I0302 08:30:19.433084 140089770305280 logging_writer.py:48] [533400] global_step=533400, grad_norm=4.565113544464111, loss=0.6682777404785156
I0302 08:30:52.865108 140089845819136 logging_writer.py:48] [533500] global_step=533500, grad_norm=4.753377914428711, loss=0.5892642736434937
I0302 08:31:03.388525 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:31:09.515482 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:31:17.539232 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:31:19.814979 140252611495744 submission_runner.py:411] Time since start: 184620.70s, 	Step: 533533, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.14448429644107819, 'validation/accuracy': 0.7559799551963806, 'validation/loss': 1.0537562370300293, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8435173034667969, 'test/num_examples': 10000, 'score': 178563.4308130741, 'total_duration': 184620.69504141808, 'accumulated_submission_time': 178563.4308130741, 'accumulated_eval_time': 6011.105018854141, 'accumulated_logging_time': 26.96674156188965}
I0302 08:31:19.922127 140089837426432 logging_writer.py:48] [533533] accumulated_eval_time=6011.105019, accumulated_logging_time=26.966742, accumulated_submission_time=178563.430813, global_step=533533, preemption_count=0, score=178563.430813, test/accuracy=0.627300, test/loss=1.843517, test/num_examples=10000, total_duration=184620.695041, train/accuracy=0.961336, train/loss=0.144484, validation/accuracy=0.755980, validation/loss=1.053756, validation/num_examples=50000
I0302 08:31:42.692884 140089862604544 logging_writer.py:48] [533600] global_step=533600, grad_norm=4.469115257263184, loss=0.5611922740936279
I0302 08:32:16.193619 140089837426432 logging_writer.py:48] [533700] global_step=533700, grad_norm=4.582347869873047, loss=0.613061785697937
I0302 08:32:49.618931 140089862604544 logging_writer.py:48] [533800] global_step=533800, grad_norm=4.609491348266602, loss=0.5703558325767517
I0302 08:33:23.087763 140089837426432 logging_writer.py:48] [533900] global_step=533900, grad_norm=4.595313549041748, loss=0.6157236099243164
I0302 08:33:56.510139 140089862604544 logging_writer.py:48] [534000] global_step=534000, grad_norm=5.181253910064697, loss=0.6576974391937256
I0302 08:34:29.946667 140089837426432 logging_writer.py:48] [534100] global_step=534100, grad_norm=4.517270088195801, loss=0.6795443892478943
I0302 08:35:03.386731 140089862604544 logging_writer.py:48] [534200] global_step=534200, grad_norm=4.150234222412109, loss=0.6191423535346985
I0302 08:35:36.854075 140089837426432 logging_writer.py:48] [534300] global_step=534300, grad_norm=4.580542087554932, loss=0.6549895405769348
I0302 08:36:10.340882 140089862604544 logging_writer.py:48] [534400] global_step=534400, grad_norm=4.515756607055664, loss=0.6022146940231323
I0302 08:36:43.766456 140089837426432 logging_writer.py:48] [534500] global_step=534500, grad_norm=4.5153679847717285, loss=0.6090155839920044
I0302 08:37:17.241742 140089862604544 logging_writer.py:48] [534600] global_step=534600, grad_norm=4.323979377746582, loss=0.5823293924331665
I0302 08:37:50.678802 140089837426432 logging_writer.py:48] [534700] global_step=534700, grad_norm=4.223320484161377, loss=0.6134968400001526
I0302 08:38:24.208072 140089862604544 logging_writer.py:48] [534800] global_step=534800, grad_norm=4.2610321044921875, loss=0.5432858467102051
I0302 08:38:57.687286 140089837426432 logging_writer.py:48] [534900] global_step=534900, grad_norm=4.529917240142822, loss=0.63791823387146
I0302 08:39:31.175123 140089862604544 logging_writer.py:48] [535000] global_step=535000, grad_norm=4.458573818206787, loss=0.6094297170639038
I0302 08:39:50.022806 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:39:56.173245 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:40:04.314594 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:40:06.586499 140252611495744 submission_runner.py:411] Time since start: 185147.47s, 	Step: 535058, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14714959263801575, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.0533289909362793, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.842696189880371, 'test/num_examples': 10000, 'score': 179073.46681928635, 'total_duration': 185147.46656560898, 'accumulated_submission_time': 179073.46681928635, 'accumulated_eval_time': 6027.668654680252, 'accumulated_logging_time': 27.084050178527832}
I0302 08:40:06.693200 140089845819136 logging_writer.py:48] [535058] accumulated_eval_time=6027.668655, accumulated_logging_time=27.084050, accumulated_submission_time=179073.466819, global_step=535058, preemption_count=0, score=179073.466819, test/accuracy=0.628100, test/loss=1.842696, test/num_examples=10000, total_duration=185147.466566, train/accuracy=0.960977, train/loss=0.147150, validation/accuracy=0.755800, validation/loss=1.053329, validation/num_examples=50000
I0302 08:40:21.101626 140089854211840 logging_writer.py:48] [535100] global_step=535100, grad_norm=4.7650017738342285, loss=0.6469120979309082
I0302 08:40:54.541235 140089845819136 logging_writer.py:48] [535200] global_step=535200, grad_norm=4.273480415344238, loss=0.5309758186340332
I0302 08:41:46.441318 140089854211840 logging_writer.py:48] [535300] global_step=535300, grad_norm=4.203868389129639, loss=0.6022990345954895
I0302 08:42:27.530229 140089845819136 logging_writer.py:48] [535400] global_step=535400, grad_norm=4.429083347320557, loss=0.6662001013755798
I0302 08:43:01.082643 140089854211840 logging_writer.py:48] [535500] global_step=535500, grad_norm=4.384209632873535, loss=0.5647638440132141
I0302 08:43:34.512717 140089845819136 logging_writer.py:48] [535600] global_step=535600, grad_norm=4.5577216148376465, loss=0.6189643144607544
I0302 08:44:07.969464 140089854211840 logging_writer.py:48] [535700] global_step=535700, grad_norm=4.561962127685547, loss=0.5873645544052124
I0302 08:44:41.527580 140089845819136 logging_writer.py:48] [535800] global_step=535800, grad_norm=4.751821517944336, loss=0.6462134718894958
I0302 08:45:14.994137 140089854211840 logging_writer.py:48] [535900] global_step=535900, grad_norm=4.699509143829346, loss=0.5929977893829346
I0302 08:45:48.429762 140089845819136 logging_writer.py:48] [536000] global_step=536000, grad_norm=4.865715980529785, loss=0.6472780108451843
I0302 08:46:21.897823 140089854211840 logging_writer.py:48] [536100] global_step=536100, grad_norm=4.38115930557251, loss=0.632480263710022
I0302 08:46:55.362685 140089845819136 logging_writer.py:48] [536200] global_step=536200, grad_norm=4.260075092315674, loss=0.567094087600708
I0302 08:47:28.794450 140089854211840 logging_writer.py:48] [536300] global_step=536300, grad_norm=4.514418125152588, loss=0.6238811016082764
I0302 08:48:02.262582 140089845819136 logging_writer.py:48] [536400] global_step=536400, grad_norm=4.1969313621521, loss=0.6489680409431458
I0302 08:48:35.720297 140089854211840 logging_writer.py:48] [536500] global_step=536500, grad_norm=4.784717559814453, loss=0.5790344476699829
I0302 08:48:36.868008 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:48:42.997125 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:48:51.159092 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:48:53.443183 140252611495744 submission_runner.py:411] Time since start: 185674.32s, 	Step: 536505, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.1462796926498413, 'validation/accuracy': 0.7561399936676025, 'validation/loss': 1.0543361902236938, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.84367835521698, 'test/num_examples': 10000, 'score': 179583.57815003395, 'total_duration': 185674.32324552536, 'accumulated_submission_time': 179583.57815003395, 'accumulated_eval_time': 6044.2437698841095, 'accumulated_logging_time': 27.201574563980103}
I0302 08:48:53.554887 140089837426432 logging_writer.py:48] [536505] accumulated_eval_time=6044.243770, accumulated_logging_time=27.201575, accumulated_submission_time=179583.578150, global_step=536505, preemption_count=0, score=179583.578150, test/accuracy=0.627100, test/loss=1.843678, test/num_examples=10000, total_duration=185674.323246, train/accuracy=0.961296, train/loss=0.146280, validation/accuracy=0.756140, validation/loss=1.054336, validation/num_examples=50000
I0302 08:49:25.731790 140089845819136 logging_writer.py:48] [536600] global_step=536600, grad_norm=4.533487319946289, loss=0.6378122568130493
I0302 08:49:59.162622 140089837426432 logging_writer.py:48] [536700] global_step=536700, grad_norm=4.471604347229004, loss=0.5937949419021606
I0302 08:50:32.629999 140089845819136 logging_writer.py:48] [536800] global_step=536800, grad_norm=4.922995567321777, loss=0.7315493822097778
I0302 08:51:06.180610 140089837426432 logging_writer.py:48] [536900] global_step=536900, grad_norm=4.501197338104248, loss=0.6468653082847595
I0302 08:51:39.635269 140089845819136 logging_writer.py:48] [537000] global_step=537000, grad_norm=4.155378341674805, loss=0.5796433091163635
I0302 08:52:13.094511 140089837426432 logging_writer.py:48] [537100] global_step=537100, grad_norm=4.230592250823975, loss=0.5886687636375427
I0302 08:52:46.565234 140089845819136 logging_writer.py:48] [537200] global_step=537200, grad_norm=4.733815670013428, loss=0.5545238852500916
I0302 08:53:20.058735 140089837426432 logging_writer.py:48] [537300] global_step=537300, grad_norm=4.446260929107666, loss=0.6672071218490601
I0302 08:53:53.479916 140089845819136 logging_writer.py:48] [537400] global_step=537400, grad_norm=4.408456325531006, loss=0.6218941807746887
I0302 08:54:26.963221 140089837426432 logging_writer.py:48] [537500] global_step=537500, grad_norm=4.234738826751709, loss=0.5449070334434509
I0302 08:55:00.429133 140089845819136 logging_writer.py:48] [537600] global_step=537600, grad_norm=4.52690315246582, loss=0.6337438225746155
I0302 08:55:33.864315 140089837426432 logging_writer.py:48] [537700] global_step=537700, grad_norm=5.133843898773193, loss=0.6595318913459778
I0302 08:56:07.316422 140089845819136 logging_writer.py:48] [537800] global_step=537800, grad_norm=4.747081756591797, loss=0.6501853466033936
I0302 08:56:40.748558 140089837426432 logging_writer.py:48] [537900] global_step=537900, grad_norm=4.487581253051758, loss=0.6595112085342407
I0302 08:57:14.300100 140089845819136 logging_writer.py:48] [538000] global_step=538000, grad_norm=4.876141548156738, loss=0.7273535132408142
I0302 08:57:23.462097 140252611495744 spec.py:321] Evaluating on the training split.
I0302 08:57:29.513124 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 08:57:37.647595 140252611495744 spec.py:349] Evaluating on the test split.
I0302 08:57:39.920696 140252611495744 submission_runner.py:411] Time since start: 186200.80s, 	Step: 538029, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.14841654896736145, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0530855655670166, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8410930633544922, 'test/num_examples': 10000, 'score': 180093.41668057442, 'total_duration': 186200.8007659912, 'accumulated_submission_time': 180093.41668057442, 'accumulated_eval_time': 6060.702314376831, 'accumulated_logging_time': 27.327200889587402}
I0302 08:57:40.026605 140089862604544 logging_writer.py:48] [538029] accumulated_eval_time=6060.702314, accumulated_logging_time=27.327201, accumulated_submission_time=180093.416681, global_step=538029, preemption_count=0, score=180093.416681, test/accuracy=0.627600, test/loss=1.841093, test/num_examples=10000, total_duration=186200.800766, train/accuracy=0.960479, train/loss=0.148417, validation/accuracy=0.755580, validation/loss=1.053086, validation/num_examples=50000
I0302 08:58:04.113052 140089870997248 logging_writer.py:48] [538100] global_step=538100, grad_norm=4.35756254196167, loss=0.613662600517273
I0302 08:58:37.562936 140089862604544 logging_writer.py:48] [538200] global_step=538200, grad_norm=4.484294414520264, loss=0.61959308385849
I0302 08:59:11.023000 140089870997248 logging_writer.py:48] [538300] global_step=538300, grad_norm=4.49326229095459, loss=0.6447457075119019
I0302 08:59:44.473562 140089862604544 logging_writer.py:48] [538400] global_step=538400, grad_norm=4.397457599639893, loss=0.5274882912635803
I0302 09:00:17.920015 140089870997248 logging_writer.py:48] [538500] global_step=538500, grad_norm=4.433351039886475, loss=0.5847381949424744
I0302 09:00:51.401932 140089862604544 logging_writer.py:48] [538600] global_step=538600, grad_norm=4.346185207366943, loss=0.6359593272209167
I0302 09:01:24.860638 140089870997248 logging_writer.py:48] [538700] global_step=538700, grad_norm=4.116692543029785, loss=0.5498864650726318
I0302 09:01:58.289736 140089862604544 logging_writer.py:48] [538800] global_step=538800, grad_norm=4.574645042419434, loss=0.6429968476295471
I0302 09:02:31.715944 140089870997248 logging_writer.py:48] [538900] global_step=538900, grad_norm=4.793429374694824, loss=0.6342472434043884
I0302 09:03:05.334394 140089862604544 logging_writer.py:48] [539000] global_step=539000, grad_norm=4.572112560272217, loss=0.6232057213783264
I0302 09:03:38.766932 140089870997248 logging_writer.py:48] [539100] global_step=539100, grad_norm=4.195144176483154, loss=0.5924354791641235
I0302 09:04:12.205275 140089862604544 logging_writer.py:48] [539200] global_step=539200, grad_norm=4.504336833953857, loss=0.6003420352935791
I0302 09:04:45.636400 140089870997248 logging_writer.py:48] [539300] global_step=539300, grad_norm=4.818033695220947, loss=0.6146891117095947
I0302 09:05:19.081606 140089862604544 logging_writer.py:48] [539400] global_step=539400, grad_norm=4.241726875305176, loss=0.6085330843925476
I0302 09:05:52.560317 140089870997248 logging_writer.py:48] [539500] global_step=539500, grad_norm=4.3245625495910645, loss=0.6180577874183655
I0302 09:06:10.131035 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:06:16.119126 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:06:24.412754 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:06:26.687875 140252611495744 submission_runner.py:411] Time since start: 186727.57s, 	Step: 539554, 	{'train/accuracy': 0.9626315236091614, 'train/loss': 0.14214278757572174, 'validation/accuracy': 0.7562999725341797, 'validation/loss': 1.053134799003601, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.8427072763442993, 'test/num_examples': 10000, 'score': 180603.4564025402, 'total_duration': 186727.56794548035, 'accumulated_submission_time': 180603.4564025402, 'accumulated_eval_time': 6077.2591071128845, 'accumulated_logging_time': 27.44335126876831}
I0302 09:06:26.791159 140089770305280 logging_writer.py:48] [539554] accumulated_eval_time=6077.259107, accumulated_logging_time=27.443351, accumulated_submission_time=180603.456403, global_step=539554, preemption_count=0, score=180603.456403, test/accuracy=0.627300, test/loss=1.842707, test/num_examples=10000, total_duration=186727.567945, train/accuracy=0.962632, train/loss=0.142143, validation/accuracy=0.756300, validation/loss=1.053135, validation/num_examples=50000
I0302 09:06:42.495344 140089778697984 logging_writer.py:48] [539600] global_step=539600, grad_norm=4.74070405960083, loss=0.5598724484443665
I0302 09:07:15.922137 140089770305280 logging_writer.py:48] [539700] global_step=539700, grad_norm=4.1570143699646, loss=0.5612969994544983
I0302 09:07:49.441585 140089778697984 logging_writer.py:48] [539800] global_step=539800, grad_norm=4.588584899902344, loss=0.6143200993537903
I0302 09:08:22.908315 140089770305280 logging_writer.py:48] [539900] global_step=539900, grad_norm=5.082259654998779, loss=0.673109769821167
I0302 09:08:56.452127 140089778697984 logging_writer.py:48] [540000] global_step=540000, grad_norm=4.588242530822754, loss=0.6473323106765747
I0302 09:09:29.945586 140089770305280 logging_writer.py:48] [540100] global_step=540100, grad_norm=4.756972789764404, loss=0.6316428184509277
I0302 09:10:03.419781 140089778697984 logging_writer.py:48] [540200] global_step=540200, grad_norm=4.851756572723389, loss=0.6351491212844849
I0302 09:10:36.859871 140089770305280 logging_writer.py:48] [540300] global_step=540300, grad_norm=4.163488388061523, loss=0.6035180687904358
I0302 09:11:10.364528 140089778697984 logging_writer.py:48] [540400] global_step=540400, grad_norm=4.94917631149292, loss=0.6064053773880005
I0302 09:11:43.847589 140089770305280 logging_writer.py:48] [540500] global_step=540500, grad_norm=5.516983509063721, loss=0.7079400420188904
I0302 09:12:17.271442 140089778697984 logging_writer.py:48] [540600] global_step=540600, grad_norm=4.007879734039307, loss=0.5588367581367493
I0302 09:12:50.737561 140089770305280 logging_writer.py:48] [540700] global_step=540700, grad_norm=4.806643486022949, loss=0.6848349571228027
I0302 09:13:24.218985 140089778697984 logging_writer.py:48] [540800] global_step=540800, grad_norm=4.792689323425293, loss=0.6477580070495605
I0302 09:13:57.675228 140089770305280 logging_writer.py:48] [540900] global_step=540900, grad_norm=4.485291957855225, loss=0.694171130657196
I0302 09:14:31.149540 140089778697984 logging_writer.py:48] [541000] global_step=541000, grad_norm=4.308918476104736, loss=0.6058628559112549
I0302 09:14:56.788017 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:15:02.987950 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:15:11.229117 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:15:13.502394 140252611495744 submission_runner.py:411] Time since start: 187254.38s, 	Step: 541078, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14626702666282654, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0533305406570435, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8408427238464355, 'test/num_examples': 10000, 'score': 181113.3877146244, 'total_duration': 187254.3824417591, 'accumulated_submission_time': 181113.3877146244, 'accumulated_eval_time': 6093.973418951035, 'accumulated_logging_time': 27.557010173797607}
I0302 09:15:13.609178 140089870997248 logging_writer.py:48] [541078] accumulated_eval_time=6093.973419, accumulated_logging_time=27.557010, accumulated_submission_time=181113.387715, global_step=541078, preemption_count=0, score=181113.387715, test/accuracy=0.627500, test/loss=1.840843, test/num_examples=10000, total_duration=187254.382442, train/accuracy=0.961934, train/loss=0.146267, validation/accuracy=0.755760, validation/loss=1.053331, validation/num_examples=50000
I0302 09:15:21.285412 140089879389952 logging_writer.py:48] [541100] global_step=541100, grad_norm=4.44240665435791, loss=0.5836691856384277
I0302 09:15:54.760357 140089870997248 logging_writer.py:48] [541200] global_step=541200, grad_norm=4.371560096740723, loss=0.5400391817092896
I0302 09:16:28.226297 140089879389952 logging_writer.py:48] [541300] global_step=541300, grad_norm=4.4390034675598145, loss=0.6018192768096924
I0302 09:17:01.670772 140089870997248 logging_writer.py:48] [541400] global_step=541400, grad_norm=4.487025260925293, loss=0.688582718372345
I0302 09:17:35.134715 140089879389952 logging_writer.py:48] [541500] global_step=541500, grad_norm=4.378455638885498, loss=0.6077316403388977
I0302 09:18:08.581740 140089870997248 logging_writer.py:48] [541600] global_step=541600, grad_norm=4.77711820602417, loss=0.6026365756988525
I0302 09:18:42.088716 140089879389952 logging_writer.py:48] [541700] global_step=541700, grad_norm=4.946023464202881, loss=0.6867730617523193
I0302 09:19:15.536676 140089870997248 logging_writer.py:48] [541800] global_step=541800, grad_norm=4.788318634033203, loss=0.6264185905456543
I0302 09:19:48.991912 140089879389952 logging_writer.py:48] [541900] global_step=541900, grad_norm=4.171209812164307, loss=0.5958662033081055
I0302 09:20:22.415431 140089870997248 logging_writer.py:48] [542000] global_step=542000, grad_norm=4.699129104614258, loss=0.637860894203186
I0302 09:20:55.880671 140089879389952 logging_writer.py:48] [542100] global_step=542100, grad_norm=4.078681468963623, loss=0.559644341468811
I0302 09:21:29.567156 140089870997248 logging_writer.py:48] [542200] global_step=542200, grad_norm=4.231990337371826, loss=0.5872939229011536
I0302 09:22:03.070606 140089879389952 logging_writer.py:48] [542300] global_step=542300, grad_norm=5.106812477111816, loss=0.6082740426063538
I0302 09:22:36.541065 140089870997248 logging_writer.py:48] [542400] global_step=542400, grad_norm=4.532333850860596, loss=0.6598501801490784
I0302 09:23:09.992278 140089879389952 logging_writer.py:48] [542500] global_step=542500, grad_norm=4.345351696014404, loss=0.6044586300849915
I0302 09:23:43.443257 140089870997248 logging_writer.py:48] [542600] global_step=542600, grad_norm=4.760432243347168, loss=0.6272645592689514
I0302 09:23:43.604699 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:23:49.563204 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:23:58.182893 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:24:00.428350 140252611495744 submission_runner.py:411] Time since start: 187781.31s, 	Step: 542602, 	{'train/accuracy': 0.962890625, 'train/loss': 0.14197196066379547, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.0550309419631958, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8440637588500977, 'test/num_examples': 10000, 'score': 181623.31734347343, 'total_duration': 187781.30841875076, 'accumulated_submission_time': 181623.31734347343, 'accumulated_eval_time': 6110.797034263611, 'accumulated_logging_time': 27.675252437591553}
I0302 09:24:00.535014 140089500890880 logging_writer.py:48] [542602] accumulated_eval_time=6110.797034, accumulated_logging_time=27.675252, accumulated_submission_time=181623.317343, global_step=542602, preemption_count=0, score=181623.317343, test/accuracy=0.626500, test/loss=1.844064, test/num_examples=10000, total_duration=187781.308419, train/accuracy=0.962891, train/loss=0.141972, validation/accuracy=0.755740, validation/loss=1.055031, validation/num_examples=50000
I0302 09:24:33.675451 140089770305280 logging_writer.py:48] [542700] global_step=542700, grad_norm=4.548583507537842, loss=0.6260136961936951
I0302 09:25:07.104324 140089500890880 logging_writer.py:48] [542800] global_step=542800, grad_norm=4.460745811462402, loss=0.584685206413269
I0302 09:25:40.566734 140089770305280 logging_writer.py:48] [542900] global_step=542900, grad_norm=4.412120819091797, loss=0.579864501953125
I0302 09:26:14.000943 140089500890880 logging_writer.py:48] [543000] global_step=543000, grad_norm=4.281069755554199, loss=0.5900954008102417
I0302 09:26:47.512435 140089770305280 logging_writer.py:48] [543100] global_step=543100, grad_norm=4.182478904724121, loss=0.5988735556602478
I0302 09:27:21.059738 140089500890880 logging_writer.py:48] [543200] global_step=543200, grad_norm=4.5365309715271, loss=0.6473115086555481
I0302 09:27:54.490949 140089770305280 logging_writer.py:48] [543300] global_step=543300, grad_norm=4.823786735534668, loss=0.6290068030357361
I0302 09:28:27.922699 140089500890880 logging_writer.py:48] [543400] global_step=543400, grad_norm=4.420923709869385, loss=0.5764206647872925
I0302 09:29:01.362534 140089770305280 logging_writer.py:48] [543500] global_step=543500, grad_norm=5.321451187133789, loss=0.574234664440155
I0302 09:29:34.821153 140089500890880 logging_writer.py:48] [543600] global_step=543600, grad_norm=4.289183139801025, loss=0.6117722988128662
I0302 09:30:08.274682 140089770305280 logging_writer.py:48] [543700] global_step=543700, grad_norm=4.631780624389648, loss=0.6538395881652832
I0302 09:30:41.707961 140089500890880 logging_writer.py:48] [543800] global_step=543800, grad_norm=4.544804096221924, loss=0.6150684356689453
I0302 09:31:15.169916 140089770305280 logging_writer.py:48] [543900] global_step=543900, grad_norm=4.7199249267578125, loss=0.6180327534675598
I0302 09:31:48.612309 140089500890880 logging_writer.py:48] [544000] global_step=544000, grad_norm=4.684595108032227, loss=0.6366584897041321
I0302 09:32:22.083286 140089770305280 logging_writer.py:48] [544100] global_step=544100, grad_norm=5.049154758453369, loss=0.6444664001464844
I0302 09:32:30.581170 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:32:36.634264 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:32:44.825413 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:32:47.170753 140252611495744 submission_runner.py:411] Time since start: 188308.05s, 	Step: 544127, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.1471571922302246, 'validation/accuracy': 0.755899965763092, 'validation/loss': 1.0522381067276, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8415024280548096, 'test/num_examples': 10000, 'score': 182133.297362566, 'total_duration': 188308.05081892014, 'accumulated_submission_time': 182133.297362566, 'accumulated_eval_time': 6127.386559247971, 'accumulated_logging_time': 27.793840646743774}
I0302 09:32:47.277353 140089845819136 logging_writer.py:48] [544127] accumulated_eval_time=6127.386559, accumulated_logging_time=27.793841, accumulated_submission_time=182133.297363, global_step=544127, preemption_count=0, score=182133.297363, test/accuracy=0.627900, test/loss=1.841502, test/num_examples=10000, total_duration=188308.050819, train/accuracy=0.959682, train/loss=0.147157, validation/accuracy=0.755900, validation/loss=1.052238, validation/num_examples=50000
I0302 09:33:12.030179 140089854211840 logging_writer.py:48] [544200] global_step=544200, grad_norm=4.701120853424072, loss=0.652137041091919
I0302 09:33:45.591548 140089845819136 logging_writer.py:48] [544300] global_step=544300, grad_norm=4.637772560119629, loss=0.6005421876907349
I0302 09:34:19.040271 140089854211840 logging_writer.py:48] [544400] global_step=544400, grad_norm=4.367140293121338, loss=0.6562504768371582
I0302 09:34:52.480303 140089845819136 logging_writer.py:48] [544500] global_step=544500, grad_norm=3.9343605041503906, loss=0.5699520111083984
I0302 09:35:25.913642 140089854211840 logging_writer.py:48] [544600] global_step=544600, grad_norm=4.657501697540283, loss=0.6330105662345886
I0302 09:35:59.377343 140089845819136 logging_writer.py:48] [544700] global_step=544700, grad_norm=4.470658302307129, loss=0.6300749182701111
I0302 09:36:32.813645 140089854211840 logging_writer.py:48] [544800] global_step=544800, grad_norm=5.861496925354004, loss=0.6144641041755676
I0302 09:37:06.283701 140089845819136 logging_writer.py:48] [544900] global_step=544900, grad_norm=5.339839458465576, loss=0.6516019701957703
I0302 09:37:39.807578 140089854211840 logging_writer.py:48] [545000] global_step=545000, grad_norm=4.609921455383301, loss=0.6597896814346313
I0302 09:38:13.268378 140089845819136 logging_writer.py:48] [545100] global_step=545100, grad_norm=4.227875232696533, loss=0.5945038795471191
I0302 09:38:46.693814 140089854211840 logging_writer.py:48] [545200] global_step=545200, grad_norm=5.385225296020508, loss=0.7068887948989868
I0302 09:39:20.159653 140089845819136 logging_writer.py:48] [545300] global_step=545300, grad_norm=4.4458723068237305, loss=0.5636584758758545
I0302 09:39:53.708370 140089854211840 logging_writer.py:48] [545400] global_step=545400, grad_norm=5.017998695373535, loss=0.7299281358718872
I0302 09:40:27.156081 140089845819136 logging_writer.py:48] [545500] global_step=545500, grad_norm=4.679396629333496, loss=0.6371910572052002
I0302 09:41:00.617070 140089854211840 logging_writer.py:48] [545600] global_step=545600, grad_norm=4.57545280456543, loss=0.6092018485069275
I0302 09:41:17.461507 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:41:23.382946 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:41:31.544815 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:41:33.803445 140252611495744 submission_runner.py:411] Time since start: 188834.68s, 	Step: 545652, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14573781192302704, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.053802251815796, 'validation/num_examples': 50000, 'test/accuracy': 0.6266000270843506, 'test/loss': 1.8414411544799805, 'test/num_examples': 10000, 'score': 182643.4147348404, 'total_duration': 188834.6835012436, 'accumulated_submission_time': 182643.4147348404, 'accumulated_eval_time': 6143.728442192078, 'accumulated_logging_time': 27.912110328674316}
I0302 09:41:33.908541 140089862604544 logging_writer.py:48] [545652] accumulated_eval_time=6143.728442, accumulated_logging_time=27.912110, accumulated_submission_time=182643.414735, global_step=545652, preemption_count=0, score=182643.414735, test/accuracy=0.626600, test/loss=1.841441, test/num_examples=10000, total_duration=188834.683501, train/accuracy=0.960579, train/loss=0.145738, validation/accuracy=0.756100, validation/loss=1.053802, validation/num_examples=50000
I0302 09:41:50.305358 140089870997248 logging_writer.py:48] [545700] global_step=545700, grad_norm=4.714333534240723, loss=0.6877274513244629
I0302 09:42:23.745509 140089862604544 logging_writer.py:48] [545800] global_step=545800, grad_norm=4.329836845397949, loss=0.6025177836418152
I0302 09:42:57.221174 140089870997248 logging_writer.py:48] [545900] global_step=545900, grad_norm=4.974121570587158, loss=0.5347480773925781
I0302 09:43:30.683168 140089862604544 logging_writer.py:48] [546000] global_step=546000, grad_norm=4.554978847503662, loss=0.6294477581977844
I0302 09:44:04.121990 140089870997248 logging_writer.py:48] [546100] global_step=546100, grad_norm=4.865915775299072, loss=0.6231570243835449
I0302 09:44:37.609964 140089862604544 logging_writer.py:48] [546200] global_step=546200, grad_norm=4.308937072753906, loss=0.5723616480827332
I0302 09:45:11.035860 140089870997248 logging_writer.py:48] [546300] global_step=546300, grad_norm=4.2647480964660645, loss=0.6328747272491455
I0302 09:45:44.663412 140089862604544 logging_writer.py:48] [546400] global_step=546400, grad_norm=4.940335750579834, loss=0.6980650424957275
I0302 09:46:18.085913 140089870997248 logging_writer.py:48] [546500] global_step=546500, grad_norm=4.676148414611816, loss=0.690205991268158
I0302 09:46:51.550672 140089862604544 logging_writer.py:48] [546600] global_step=546600, grad_norm=4.644951820373535, loss=0.6944932341575623
I0302 09:47:25.047302 140089870997248 logging_writer.py:48] [546700] global_step=546700, grad_norm=4.760360240936279, loss=0.601186215877533
I0302 09:47:58.540942 140089862604544 logging_writer.py:48] [546800] global_step=546800, grad_norm=4.264508247375488, loss=0.5693355798721313
I0302 09:48:32.035700 140089870997248 logging_writer.py:48] [546900] global_step=546900, grad_norm=5.399350643157959, loss=0.6151900291442871
I0302 09:49:05.492243 140089862604544 logging_writer.py:48] [547000] global_step=547000, grad_norm=4.342053413391113, loss=0.5654417872428894
I0302 09:49:38.933324 140089870997248 logging_writer.py:48] [547100] global_step=547100, grad_norm=5.175440311431885, loss=0.6323074102401733
I0302 09:50:03.887101 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:50:09.866555 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:50:18.061739 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:50:20.327694 140252611495744 submission_runner.py:411] Time since start: 189361.21s, 	Step: 547176, 	{'train/accuracy': 0.9627709984779358, 'train/loss': 0.1430668830871582, 'validation/accuracy': 0.7557599544525146, 'validation/loss': 1.0532115697860718, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8418182134628296, 'test/num_examples': 10000, 'score': 183153.32903194427, 'total_duration': 189361.20776104927, 'accumulated_submission_time': 183153.32903194427, 'accumulated_eval_time': 6160.168985366821, 'accumulated_logging_time': 28.02666687965393}
I0302 09:50:20.435173 140089770305280 logging_writer.py:48] [547176] accumulated_eval_time=6160.168985, accumulated_logging_time=28.026667, accumulated_submission_time=183153.329032, global_step=547176, preemption_count=0, score=183153.329032, test/accuracy=0.627600, test/loss=1.841818, test/num_examples=10000, total_duration=189361.207761, train/accuracy=0.962771, train/loss=0.143067, validation/accuracy=0.755760, validation/loss=1.053212, validation/num_examples=50000
I0302 09:50:28.797382 140089778697984 logging_writer.py:48] [547200] global_step=547200, grad_norm=4.5764594078063965, loss=0.6626647710800171
I0302 09:51:02.226879 140089770305280 logging_writer.py:48] [547300] global_step=547300, grad_norm=4.496666431427002, loss=0.5833379626274109
I0302 09:51:35.823409 140089778697984 logging_writer.py:48] [547400] global_step=547400, grad_norm=4.407718658447266, loss=0.6270459890365601
I0302 09:52:09.265482 140089770305280 logging_writer.py:48] [547500] global_step=547500, grad_norm=4.351195812225342, loss=0.6574563980102539
I0302 09:52:42.739892 140089778697984 logging_writer.py:48] [547600] global_step=547600, grad_norm=4.319637298583984, loss=0.6290141344070435
I0302 09:53:16.220126 140089770305280 logging_writer.py:48] [547700] global_step=547700, grad_norm=6.11461877822876, loss=0.6507358551025391
I0302 09:53:49.683158 140089778697984 logging_writer.py:48] [547800] global_step=547800, grad_norm=4.201593399047852, loss=0.6168034076690674
I0302 09:54:23.132014 140089770305280 logging_writer.py:48] [547900] global_step=547900, grad_norm=4.55428409576416, loss=0.6013192534446716
I0302 09:54:56.608188 140089778697984 logging_writer.py:48] [548000] global_step=548000, grad_norm=5.305531978607178, loss=0.6729773283004761
I0302 09:55:30.089409 140089770305280 logging_writer.py:48] [548100] global_step=548100, grad_norm=4.336109638214111, loss=0.5512657165527344
I0302 09:56:03.542540 140089778697984 logging_writer.py:48] [548200] global_step=548200, grad_norm=4.4535908699035645, loss=0.5877073407173157
I0302 09:56:36.995300 140089770305280 logging_writer.py:48] [548300] global_step=548300, grad_norm=4.571666717529297, loss=0.6359854936599731
I0302 09:57:10.464558 140089778697984 logging_writer.py:48] [548400] global_step=548400, grad_norm=4.403198719024658, loss=0.6790807247161865
I0302 09:57:44.037027 140089770305280 logging_writer.py:48] [548500] global_step=548500, grad_norm=4.466519355773926, loss=0.6217830777168274
I0302 09:58:17.528781 140089778697984 logging_writer.py:48] [548600] global_step=548600, grad_norm=4.351948261260986, loss=0.6167866587638855
I0302 09:58:50.469206 140252611495744 spec.py:321] Evaluating on the training split.
I0302 09:58:56.480377 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 09:59:04.787040 140252611495744 spec.py:349] Evaluating on the test split.
I0302 09:59:07.077530 140252611495744 submission_runner.py:411] Time since start: 189887.96s, 	Step: 548700, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14525896310806274, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0536679029464722, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.84215247631073, 'test/num_examples': 10000, 'score': 183663.29820084572, 'total_duration': 189887.9575984478, 'accumulated_submission_time': 183663.29820084572, 'accumulated_eval_time': 6176.777256727219, 'accumulated_logging_time': 28.143900156021118}
I0302 09:59:07.192283 140089870997248 logging_writer.py:48] [548700] accumulated_eval_time=6176.777257, accumulated_logging_time=28.143900, accumulated_submission_time=183663.298201, global_step=548700, preemption_count=0, score=183663.298201, test/accuracy=0.627000, test/loss=1.842152, test/num_examples=10000, total_duration=189887.957598, train/accuracy=0.961037, train/loss=0.145259, validation/accuracy=0.755780, validation/loss=1.053668, validation/num_examples=50000
I0302 09:59:07.550156 140089879389952 logging_writer.py:48] [548700] global_step=548700, grad_norm=5.024587631225586, loss=0.6365233659744263
I0302 09:59:40.997273 140089870997248 logging_writer.py:48] [548800] global_step=548800, grad_norm=4.224862575531006, loss=0.5974211096763611
I0302 10:00:14.448857 140089879389952 logging_writer.py:48] [548900] global_step=548900, grad_norm=4.674380302429199, loss=0.6478538513183594
I0302 10:00:47.895522 140089870997248 logging_writer.py:48] [549000] global_step=549000, grad_norm=5.367785930633545, loss=0.6230106353759766
I0302 10:01:21.380107 140089879389952 logging_writer.py:48] [549100] global_step=549100, grad_norm=4.31539249420166, loss=0.5505032539367676
I0302 10:01:54.802686 140089870997248 logging_writer.py:48] [549200] global_step=549200, grad_norm=4.436970233917236, loss=0.6538337469100952
I0302 10:02:28.248726 140089879389952 logging_writer.py:48] [549300] global_step=549300, grad_norm=4.265665054321289, loss=0.619509756565094
I0302 10:03:01.670802 140089870997248 logging_writer.py:48] [549400] global_step=549400, grad_norm=4.570468425750732, loss=0.6017229557037354
I0302 10:03:35.121178 140089879389952 logging_writer.py:48] [549500] global_step=549500, grad_norm=4.983436584472656, loss=0.6294065713882446
I0302 10:04:08.652441 140089870997248 logging_writer.py:48] [549600] global_step=549600, grad_norm=4.152565956115723, loss=0.5761920213699341
I0302 10:04:42.125740 140089879389952 logging_writer.py:48] [549700] global_step=549700, grad_norm=4.543679237365723, loss=0.6136726140975952
I0302 10:05:15.565449 140089870997248 logging_writer.py:48] [549800] global_step=549800, grad_norm=4.238494396209717, loss=0.599036455154419
I0302 10:05:49.009073 140089879389952 logging_writer.py:48] [549900] global_step=549900, grad_norm=4.257888317108154, loss=0.6546815037727356
I0302 10:06:22.429790 140089870997248 logging_writer.py:48] [550000] global_step=550000, grad_norm=5.052794456481934, loss=0.6602382659912109
I0302 10:06:55.881916 140089879389952 logging_writer.py:48] [550100] global_step=550100, grad_norm=4.4543023109436035, loss=0.6516066789627075
I0302 10:07:29.343220 140089870997248 logging_writer.py:48] [550200] global_step=550200, grad_norm=4.860455513000488, loss=0.6158688068389893
I0302 10:07:37.180748 140252611495744 spec.py:321] Evaluating on the training split.
I0302 10:07:43.168452 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 10:07:51.391673 140252611495744 spec.py:349] Evaluating on the test split.
I0302 10:07:53.694713 140252611495744 submission_runner.py:411] Time since start: 190414.57s, 	Step: 550225, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14361946284770966, 'validation/accuracy': 0.756060004234314, 'validation/loss': 1.0538721084594727, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8426450490951538, 'test/num_examples': 10000, 'score': 184173.2213394642, 'total_duration': 190414.57474303246, 'accumulated_submission_time': 184173.2213394642, 'accumulated_eval_time': 6193.291128158569, 'accumulated_logging_time': 28.26856780052185}
I0302 10:07:53.862942 140089770305280 logging_writer.py:48] [550225] accumulated_eval_time=6193.291128, accumulated_logging_time=28.268568, accumulated_submission_time=184173.221339, global_step=550225, preemption_count=0, score=184173.221339, test/accuracy=0.627700, test/loss=1.842645, test/num_examples=10000, total_duration=190414.574743, train/accuracy=0.961097, train/loss=0.143619, validation/accuracy=0.756060, validation/loss=1.053872, validation/num_examples=50000
I0302 10:08:19.317097 140089778697984 logging_writer.py:48] [550300] global_step=550300, grad_norm=4.6314592361450195, loss=0.5906563401222229
I0302 10:08:52.749928 140089770305280 logging_writer.py:48] [550400] global_step=550400, grad_norm=4.5177812576293945, loss=0.6128503084182739
I0302 10:09:26.240073 140089778697984 logging_writer.py:48] [550500] global_step=550500, grad_norm=4.266630172729492, loss=0.6254037618637085
I0302 10:09:59.776755 140089770305280 logging_writer.py:48] [550600] global_step=550600, grad_norm=4.565074920654297, loss=0.6285277009010315
I0302 10:10:33.275566 140089778697984 logging_writer.py:48] [550700] global_step=550700, grad_norm=4.760237693786621, loss=0.6461604237556458
I0302 10:11:06.740218 140089770305280 logging_writer.py:48] [550800] global_step=550800, grad_norm=4.645562171936035, loss=0.6541556119918823
I0302 10:11:40.189821 140089778697984 logging_writer.py:48] [550900] global_step=550900, grad_norm=4.225107192993164, loss=0.5906206965446472
I0302 10:12:13.655135 140089770305280 logging_writer.py:48] [551000] global_step=551000, grad_norm=4.747890472412109, loss=0.6181554198265076
I0302 10:12:47.130705 140089778697984 logging_writer.py:48] [551100] global_step=551100, grad_norm=4.938553810119629, loss=0.6277505159378052
I0302 10:13:20.559471 140089770305280 logging_writer.py:48] [551200] global_step=551200, grad_norm=4.624536514282227, loss=0.620906412601471
I0302 10:13:54.068460 140089778697984 logging_writer.py:48] [551300] global_step=551300, grad_norm=4.295058727264404, loss=0.6168441772460938
I0302 10:14:27.551716 140089770305280 logging_writer.py:48] [551400] global_step=551400, grad_norm=4.634040355682373, loss=0.6247662901878357
I0302 10:15:00.998215 140089778697984 logging_writer.py:48] [551500] global_step=551500, grad_norm=4.547006130218506, loss=0.6366779804229736
I0302 10:15:34.449197 140089770305280 logging_writer.py:48] [551600] global_step=551600, grad_norm=5.004559516906738, loss=0.677844226360321
I0302 10:16:08.023345 140089778697984 logging_writer.py:48] [551700] global_step=551700, grad_norm=4.417936325073242, loss=0.6190817952156067
I0302 10:16:23.878995 140252611495744 spec.py:321] Evaluating on the training split.
I0302 10:16:29.857580 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 10:16:38.059508 140252611495744 spec.py:349] Evaluating on the test split.
I0302 10:16:40.324672 140252611495744 submission_runner.py:411] Time since start: 190941.20s, 	Step: 551749, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14761671423912048, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.0538707971572876, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8423097133636475, 'test/num_examples': 10000, 'score': 184683.1692752838, 'total_duration': 190941.20473623276, 'accumulated_submission_time': 184683.1692752838, 'accumulated_eval_time': 6209.7367441654205, 'accumulated_logging_time': 28.450235843658447}
I0302 10:16:40.436196 140089500890880 logging_writer.py:48] [551749] accumulated_eval_time=6209.736744, accumulated_logging_time=28.450236, accumulated_submission_time=184683.169275, global_step=551749, preemption_count=0, score=184683.169275, test/accuracy=0.626500, test/loss=1.842310, test/num_examples=10000, total_duration=190941.204736, train/accuracy=0.961037, train/loss=0.147617, validation/accuracy=0.755600, validation/loss=1.053871, validation/num_examples=50000
I0302 10:16:57.801997 140089770305280 logging_writer.py:48] [551800] global_step=551800, grad_norm=4.250715255737305, loss=0.5779772400856018
I0302 10:17:31.259293 140089500890880 logging_writer.py:48] [551900] global_step=551900, grad_norm=4.431867599487305, loss=0.5485876202583313
I0302 10:18:04.719270 140089770305280 logging_writer.py:48] [552000] global_step=552000, grad_norm=4.749039173126221, loss=0.7497355937957764
I0302 10:18:38.171772 140089500890880 logging_writer.py:48] [552100] global_step=552100, grad_norm=4.55450963973999, loss=0.5665038228034973
I0302 10:19:11.650205 140089770305280 logging_writer.py:48] [552200] global_step=552200, grad_norm=4.77141809463501, loss=0.6452921628952026
I0302 10:19:45.081106 140089500890880 logging_writer.py:48] [552300] global_step=552300, grad_norm=4.501895427703857, loss=0.5903525352478027
I0302 10:20:18.580682 140089770305280 logging_writer.py:48] [552400] global_step=552400, grad_norm=5.2252326011657715, loss=0.6888698935508728
I0302 10:20:52.055492 140089500890880 logging_writer.py:48] [552500] global_step=552500, grad_norm=4.138339519500732, loss=0.5223126411437988
I0302 10:21:25.534381 140089770305280 logging_writer.py:48] [552600] global_step=552600, grad_norm=5.569090366363525, loss=0.5626356601715088
I0302 10:21:59.008071 140089500890880 logging_writer.py:48] [552700] global_step=552700, grad_norm=4.717895984649658, loss=0.7070052027702332
I0302 10:22:32.508720 140089770305280 logging_writer.py:48] [552800] global_step=552800, grad_norm=4.569714546203613, loss=0.6839463710784912
I0302 10:23:05.992347 140089500890880 logging_writer.py:48] [552900] global_step=552900, grad_norm=4.913358688354492, loss=0.6915183663368225
I0302 10:23:39.461364 140089770305280 logging_writer.py:48] [553000] global_step=553000, grad_norm=4.604475975036621, loss=0.546783983707428
I0302 10:24:12.913680 140089500890880 logging_writer.py:48] [553100] global_step=553100, grad_norm=4.270247459411621, loss=0.6488394737243652
I0302 10:24:46.359363 140089770305280 logging_writer.py:48] [553200] global_step=553200, grad_norm=4.433193206787109, loss=0.5990400314331055
I0302 10:25:10.599331 140252611495744 spec.py:321] Evaluating on the training split.
I0302 10:25:16.664258 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 10:25:24.826181 140252611495744 spec.py:349] Evaluating on the test split.
I0302 10:25:27.085894 140252611495744 submission_runner.py:411] Time since start: 191467.97s, 	Step: 553274, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14577318727970123, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 1.0535832643508911, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.841801643371582, 'test/num_examples': 10000, 'score': 185193.2670288086, 'total_duration': 191467.96596193314, 'accumulated_submission_time': 185193.2670288086, 'accumulated_eval_time': 6226.223253250122, 'accumulated_logging_time': 28.5725200176239}
I0302 10:25:27.193075 140089854211840 logging_writer.py:48] [553274] accumulated_eval_time=6226.223253, accumulated_logging_time=28.572520, accumulated_submission_time=185193.267029, global_step=553274, preemption_count=0, score=185193.267029, test/accuracy=0.626900, test/loss=1.841802, test/num_examples=10000, total_duration=191467.965962, train/accuracy=0.961137, train/loss=0.145773, validation/accuracy=0.755860, validation/loss=1.053583, validation/num_examples=50000
I0302 10:25:36.225725 140089870997248 logging_writer.py:48] [553300] global_step=553300, grad_norm=4.565115451812744, loss=0.6579961180686951
I0302 10:26:09.678684 140089854211840 logging_writer.py:48] [553400] global_step=553400, grad_norm=5.055851459503174, loss=0.7111761569976807
I0302 10:26:43.166913 140089870997248 logging_writer.py:48] [553500] global_step=553500, grad_norm=3.8647115230560303, loss=0.5468051433563232
I0302 10:27:16.657084 140089854211840 logging_writer.py:48] [553600] global_step=553600, grad_norm=4.746587753295898, loss=0.6495925784111023
I0302 10:27:50.086050 140089870997248 logging_writer.py:48] [553700] global_step=553700, grad_norm=4.547414302825928, loss=0.6119635105133057
I0302 10:28:23.633338 140089854211840 logging_writer.py:48] [553800] global_step=553800, grad_norm=4.524197101593018, loss=0.6170744895935059
I0302 10:28:57.101173 140089870997248 logging_writer.py:48] [553900] global_step=553900, grad_norm=4.280471324920654, loss=0.5860621333122253
I0302 10:29:30.559238 140089854211840 logging_writer.py:48] [554000] global_step=554000, grad_norm=4.721416473388672, loss=0.6256134510040283
I0302 10:30:04.007884 140089870997248 logging_writer.py:48] [554100] global_step=554100, grad_norm=4.835413932800293, loss=0.6581831574440002
I0302 10:30:37.479246 140089854211840 logging_writer.py:48] [554200] global_step=554200, grad_norm=4.477626800537109, loss=0.673863410949707
I0302 10:31:10.905040 140089870997248 logging_writer.py:48] [554300] global_step=554300, grad_norm=4.192526817321777, loss=0.5716201663017273
I0302 10:31:44.386809 140089854211840 logging_writer.py:48] [554400] global_step=554400, grad_norm=4.871196746826172, loss=0.6908881068229675
I0302 10:32:17.838385 140089870997248 logging_writer.py:48] [554500] global_step=554500, grad_norm=4.251855373382568, loss=0.6029221415519714
I0302 10:32:51.295991 140089854211840 logging_writer.py:48] [554600] global_step=554600, grad_norm=4.390100002288818, loss=0.6336104273796082
I0302 10:33:24.779841 140089870997248 logging_writer.py:48] [554700] global_step=554700, grad_norm=4.209084510803223, loss=0.6064706444740295
I0302 10:33:57.353938 140252611495744 spec.py:321] Evaluating on the training split.
I0302 10:34:03.526594 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 10:34:11.670569 140252611495744 spec.py:349] Evaluating on the test split.
I0302 10:34:13.965952 140252611495744 submission_runner.py:411] Time since start: 191994.85s, 	Step: 554799, 	{'train/accuracy': 0.9621731042861938, 'train/loss': 0.14207059144973755, 'validation/accuracy': 0.7558199763298035, 'validation/loss': 1.052920937538147, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.842273473739624, 'test/num_examples': 10000, 'score': 185703.3607866764, 'total_duration': 191994.8460161686, 'accumulated_submission_time': 185703.3607866764, 'accumulated_eval_time': 6242.835218429565, 'accumulated_logging_time': 28.690925359725952}
I0302 10:34:14.076391 140089862604544 logging_writer.py:48] [554799] accumulated_eval_time=6242.835218, accumulated_logging_time=28.690925, accumulated_submission_time=185703.360787, global_step=554799, preemption_count=0, score=185703.360787, test/accuracy=0.626700, test/loss=1.842273, test/num_examples=10000, total_duration=191994.846016, train/accuracy=0.962173, train/loss=0.142071, validation/accuracy=0.755820, validation/loss=1.052921, validation/num_examples=50000
I0302 10:34:14.759656 140089887782656 logging_writer.py:48] [554800] global_step=554800, grad_norm=4.517431259155273, loss=0.570122480392456
I0302 10:34:48.297005 140089862604544 logging_writer.py:48] [554900] global_step=554900, grad_norm=4.708713531494141, loss=0.6118784546852112
I0302 10:35:21.756769 140089887782656 logging_writer.py:48] [555000] global_step=555000, grad_norm=4.516647815704346, loss=0.6305645108222961
I0302 10:35:55.236015 140089862604544 logging_writer.py:48] [555100] global_step=555100, grad_norm=4.460747241973877, loss=0.5865739583969116
I0302 10:36:28.688138 140089887782656 logging_writer.py:48] [555200] global_step=555200, grad_norm=5.07474946975708, loss=0.5734750628471375
I0302 10:37:02.166558 140089862604544 logging_writer.py:48] [555300] global_step=555300, grad_norm=4.504954814910889, loss=0.6219677925109863
I0302 10:37:35.608002 140089887782656 logging_writer.py:48] [555400] global_step=555400, grad_norm=4.741788387298584, loss=0.674124002456665
I0302 10:38:09.062745 140089862604544 logging_writer.py:48] [555500] global_step=555500, grad_norm=4.732375144958496, loss=0.6739351749420166
I0302 10:38:42.494850 140089887782656 logging_writer.py:48] [555600] global_step=555600, grad_norm=4.6890387535095215, loss=0.6318478584289551
I0302 10:39:15.939002 140089862604544 logging_writer.py:48] [555700] global_step=555700, grad_norm=4.472230911254883, loss=0.6148557662963867
I0302 10:39:49.364204 140089887782656 logging_writer.py:48] [555800] global_step=555800, grad_norm=4.743807792663574, loss=0.5994712114334106
I0302 10:40:22.801471 140089862604544 logging_writer.py:48] [555900] global_step=555900, grad_norm=4.519266128540039, loss=0.6163953542709351
I0302 10:40:56.349242 140089887782656 logging_writer.py:48] [556000] global_step=556000, grad_norm=4.476113319396973, loss=0.6029072999954224
I0302 10:41:29.802526 140089862604544 logging_writer.py:48] [556100] global_step=556100, grad_norm=4.566154956817627, loss=0.5472227931022644
I0302 10:42:03.232998 140089887782656 logging_writer.py:48] [556200] global_step=556200, grad_norm=4.845162868499756, loss=0.6636696457862854
I0302 10:42:36.694280 140089862604544 logging_writer.py:48] [556300] global_step=556300, grad_norm=4.1953630447387695, loss=0.5272296071052551
I0302 10:42:44.186950 140252611495744 spec.py:321] Evaluating on the training split.
I0302 10:42:50.114670 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 10:42:58.345134 140252611495744 spec.py:349] Evaluating on the test split.
I0302 10:43:00.627144 140252611495744 submission_runner.py:411] Time since start: 192521.51s, 	Step: 556324, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14606821537017822, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.053757905960083, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.8436654806137085, 'test/num_examples': 10000, 'score': 186213.40687561035, 'total_duration': 192521.5072004795, 'accumulated_submission_time': 186213.40687561035, 'accumulated_eval_time': 6259.27534365654, 'accumulated_logging_time': 28.811352014541626}
I0302 10:43:00.735807 140089778697984 logging_writer.py:48] [556324] accumulated_eval_time=6259.275344, accumulated_logging_time=28.811352, accumulated_submission_time=186213.406876, global_step=556324, preemption_count=0, score=186213.406876, test/accuracy=0.627600, test/loss=1.843665, test/num_examples=10000, total_duration=192521.507200, train/accuracy=0.960798, train/loss=0.146068, validation/accuracy=0.755720, validation/loss=1.053758, validation/num_examples=50000
I0302 10:43:26.486658 140089837426432 logging_writer.py:48] [556400] global_step=556400, grad_norm=4.2246479988098145, loss=0.5745229721069336
I0302 10:43:59.938738 140089778697984 logging_writer.py:48] [556500] global_step=556500, grad_norm=4.737374305725098, loss=0.6363529562950134
I0302 10:44:33.374643 140089837426432 logging_writer.py:48] [556600] global_step=556600, grad_norm=4.585903167724609, loss=0.6192188858985901
I0302 10:45:06.838706 140089778697984 logging_writer.py:48] [556700] global_step=556700, grad_norm=4.142157554626465, loss=0.5838644504547119
I0302 10:45:40.266297 140089837426432 logging_writer.py:48] [556800] global_step=556800, grad_norm=4.879758834838867, loss=0.6499976515769958
I0302 10:46:13.746270 140089778697984 logging_writer.py:48] [556900] global_step=556900, grad_norm=4.7204670906066895, loss=0.589200496673584
I0302 10:46:47.310518 140089837426432 logging_writer.py:48] [557000] global_step=557000, grad_norm=4.605097770690918, loss=0.7060686945915222
I0302 10:47:20.764425 140089778697984 logging_writer.py:48] [557100] global_step=557100, grad_norm=4.108129024505615, loss=0.5773322582244873
I0302 10:47:54.243451 140089837426432 logging_writer.py:48] [557200] global_step=557200, grad_norm=4.466888904571533, loss=0.5997936129570007
I0302 10:48:27.695619 140089778697984 logging_writer.py:48] [557300] global_step=557300, grad_norm=4.259212017059326, loss=0.5779651999473572
I0302 10:49:01.156412 140089837426432 logging_writer.py:48] [557400] global_step=557400, grad_norm=4.653815746307373, loss=0.6687451601028442
I0302 10:49:34.605921 140089778697984 logging_writer.py:48] [557500] global_step=557500, grad_norm=4.550745487213135, loss=0.5774162411689758
I0302 10:50:08.074147 140089837426432 logging_writer.py:48] [557600] global_step=557600, grad_norm=4.481760025024414, loss=0.6277804374694824
I0302 10:50:41.520387 140089778697984 logging_writer.py:48] [557700] global_step=557700, grad_norm=4.881882190704346, loss=0.640817403793335
I0302 10:51:14.947577 140089837426432 logging_writer.py:48] [557800] global_step=557800, grad_norm=4.822842121124268, loss=0.6346900463104248
I0302 10:51:30.835926 140252611495744 spec.py:321] Evaluating on the training split.
I0302 10:51:36.824437 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 10:51:45.073018 140252611495744 spec.py:349] Evaluating on the test split.
I0302 10:51:47.362387 140252611495744 submission_runner.py:411] Time since start: 193048.24s, 	Step: 557849, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14640016853809357, 'validation/accuracy': 0.7560200095176697, 'validation/loss': 1.0539249181747437, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.843936562538147, 'test/num_examples': 10000, 'score': 186723.44114279747, 'total_duration': 193048.2424557209, 'accumulated_submission_time': 186723.44114279747, 'accumulated_eval_time': 6275.80174779892, 'accumulated_logging_time': 28.930872678756714}
I0302 10:51:47.471955 140089854211840 logging_writer.py:48] [557849] accumulated_eval_time=6275.801748, accumulated_logging_time=28.930873, accumulated_submission_time=186723.441143, global_step=557849, preemption_count=0, score=186723.441143, test/accuracy=0.627000, test/loss=1.843937, test/num_examples=10000, total_duration=193048.242456, train/accuracy=0.960579, train/loss=0.146400, validation/accuracy=0.756020, validation/loss=1.053925, validation/num_examples=50000
I0302 10:52:04.850546 140089862604544 logging_writer.py:48] [557900] global_step=557900, grad_norm=4.601437568664551, loss=0.6470610499382019
I0302 10:52:38.375821 140089854211840 logging_writer.py:48] [558000] global_step=558000, grad_norm=4.083141326904297, loss=0.5889343023300171
I0302 10:53:11.822429 140089862604544 logging_writer.py:48] [558100] global_step=558100, grad_norm=4.42212438583374, loss=0.6099956035614014
I0302 10:53:45.268660 140089854211840 logging_writer.py:48] [558200] global_step=558200, grad_norm=4.300793170928955, loss=0.6057701110839844
I0302 10:54:18.711605 140089862604544 logging_writer.py:48] [558300] global_step=558300, grad_norm=4.392671585083008, loss=0.6563816666603088
I0302 10:54:52.158801 140089854211840 logging_writer.py:48] [558400] global_step=558400, grad_norm=4.503751754760742, loss=0.596630871295929
I0302 10:55:25.603348 140089862604544 logging_writer.py:48] [558500] global_step=558500, grad_norm=4.922428131103516, loss=0.6152169704437256
I0302 10:55:59.036697 140089854211840 logging_writer.py:48] [558600] global_step=558600, grad_norm=4.455594062805176, loss=0.6047058701515198
I0302 10:56:32.481117 140089862604544 logging_writer.py:48] [558700] global_step=558700, grad_norm=4.673192024230957, loss=0.6226800084114075
I0302 10:57:05.902143 140089854211840 logging_writer.py:48] [558800] global_step=558800, grad_norm=4.561650276184082, loss=0.6321123838424683
I0302 10:57:39.358152 140089862604544 logging_writer.py:48] [558900] global_step=558900, grad_norm=4.226509094238281, loss=0.5890921354293823
I0302 10:58:12.799801 140089854211840 logging_writer.py:48] [559000] global_step=559000, grad_norm=4.51961088180542, loss=0.629991888999939
I0302 10:58:46.364257 140089862604544 logging_writer.py:48] [559100] global_step=559100, grad_norm=4.895406723022461, loss=0.6054378747940063
I0302 10:59:19.796796 140089854211840 logging_writer.py:48] [559200] global_step=559200, grad_norm=4.847355365753174, loss=0.6737082004547119
I0302 10:59:53.262570 140089862604544 logging_writer.py:48] [559300] global_step=559300, grad_norm=4.8596510887146, loss=0.6167861819267273
I0302 11:00:17.500087 140252611495744 spec.py:321] Evaluating on the training split.
I0302 11:00:23.447159 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 11:00:31.609585 140252611495744 spec.py:349] Evaluating on the test split.
I0302 11:00:33.940056 140252611495744 submission_runner.py:411] Time since start: 193574.82s, 	Step: 559374, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14686231315135956, 'validation/accuracy': 0.7558000087738037, 'validation/loss': 1.052876591682434, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.8411613702774048, 'test/num_examples': 10000, 'score': 187233.40432572365, 'total_duration': 193574.8201098442, 'accumulated_submission_time': 187233.40432572365, 'accumulated_eval_time': 6292.241653680801, 'accumulated_logging_time': 29.049833059310913}
I0302 11:00:34.049252 140089837426432 logging_writer.py:48] [559374] accumulated_eval_time=6292.241654, accumulated_logging_time=29.049833, accumulated_submission_time=187233.404326, global_step=559374, preemption_count=0, score=187233.404326, test/accuracy=0.627900, test/loss=1.841161, test/num_examples=10000, total_duration=193574.820110, train/accuracy=0.961017, train/loss=0.146862, validation/accuracy=0.755800, validation/loss=1.052877, validation/num_examples=50000
I0302 11:00:43.096565 140089845819136 logging_writer.py:48] [559400] global_step=559400, grad_norm=4.637805938720703, loss=0.67424476146698
I0302 11:01:16.567879 140089837426432 logging_writer.py:48] [559500] global_step=559500, grad_norm=4.632068634033203, loss=0.5998234748840332
I0302 11:01:49.996982 140089845819136 logging_writer.py:48] [559600] global_step=559600, grad_norm=5.078797340393066, loss=0.5977038741111755
I0302 11:02:23.436573 140089837426432 logging_writer.py:48] [559700] global_step=559700, grad_norm=4.3449482917785645, loss=0.6168429851531982
I0302 11:02:56.892371 140089845819136 logging_writer.py:48] [559800] global_step=559800, grad_norm=5.4928059577941895, loss=0.6768432855606079
I0302 11:03:30.318627 140089837426432 logging_writer.py:48] [559900] global_step=559900, grad_norm=4.125832557678223, loss=0.6288353204727173
I0302 11:04:02.613042 140252611495744 spec.py:321] Evaluating on the training split.
I0302 11:04:08.580792 140252611495744 spec.py:333] Evaluating on the validation split.
I0302 11:04:16.803355 140252611495744 spec.py:349] Evaluating on the test split.
I0302 11:04:19.009639 140252611495744 submission_runner.py:411] Time since start: 193799.89s, 	Step: 559998, 	{'train/accuracy': 0.9618144035339355, 'train/loss': 0.1436149626970291, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.0530809164047241, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.8419371843338013, 'test/num_examples': 10000, 'score': 187441.93405771255, 'total_duration': 193799.88971567154, 'accumulated_submission_time': 187441.93405771255, 'accumulated_eval_time': 6308.638211250305, 'accumulated_logging_time': 29.170271158218384}
I0302 11:04:19.112792 140089778697984 logging_writer.py:48] [559998] accumulated_eval_time=6308.638211, accumulated_logging_time=29.170271, accumulated_submission_time=187441.934058, global_step=559998, preemption_count=0, score=187441.934058, test/accuracy=0.626500, test/loss=1.841937, test/num_examples=10000, total_duration=193799.889716, train/accuracy=0.961814, train/loss=0.143615, validation/accuracy=0.755780, validation/loss=1.053081, validation/num_examples=50000
I0302 11:04:19.204531 140089837426432 logging_writer.py:48] [559998] global_step=559998, preemption_count=0, score=187441.934058
I0302 11:04:19.551811 140252611495744 checkpoints.py:490] Saving checkpoint at step: 559998
I0302 11:04:20.763555 140252611495744 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax/trial_1/checkpoint_559998
I0302 11:04:20.789664 140252611495744 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_2/imagenet_resnet_jax/trial_1/checkpoint_559998.
I0302 11:04:21.623342 140252611495744 submission_runner.py:676] Final imagenet_resnet score: 187441.93405771255
