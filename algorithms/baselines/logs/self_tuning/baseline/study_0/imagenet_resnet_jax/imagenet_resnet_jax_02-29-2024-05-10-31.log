python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_0 --overwrite=true --save_checkpoints=false --rng_seed=565442107 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_resnet_jax_02-29-2024-05-10-31.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0229 05:10:52.164252 139753105983296 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax because --overwrite was set.
I0229 05:10:52.166686 139753105983296 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax.
I0229 05:10:53.241118 139753105983296 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0229 05:10:53.241860 139753105983296 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0229 05:10:53.242111 139753105983296 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0229 05:10:54.070054 139753105983296 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax/trial_1.
I0229 05:10:54.276514 139753105983296 submission_runner.py:206] Initializing dataset.
I0229 05:10:54.291900 139753105983296 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:10:54.302891 139753105983296 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:10:54.680785 139753105983296 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:10:55.898710 139753105983296 submission_runner.py:213] Initializing model.
I0229 05:11:06.610570 139753105983296 submission_runner.py:255] Initializing optimizer.
I0229 05:11:08.362056 139753105983296 submission_runner.py:262] Initializing metrics bundle.
I0229 05:11:08.362255 139753105983296 submission_runner.py:280] Initializing checkpoint and logger.
I0229 05:11:08.363138 139753105983296 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0229 05:11:08.363290 139753105983296 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0229 05:11:08.688282 139753105983296 logger_utils.py:220] Unable to record git information. Continuing without it.
I0229 05:11:08.987023 139753105983296 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax/trial_1/flags_0.json.
I0229 05:11:08.996699 139753105983296 submission_runner.py:314] Starting training loop.
I0229 05:12:01.254784 139587007543040 logging_writer.py:48] [0] global_step=0, grad_norm=0.6710035800933838, loss=6.930171012878418
I0229 05:12:01.273517 139753105983296 spec.py:321] Evaluating on the training split.
I0229 05:12:02.564244 139753105983296 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:12:02.573559 139753105983296 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:12:02.662630 139753105983296 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:12:16.215935 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 05:12:17.878109 139753105983296 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:12:17.887236 139753105983296 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0229 05:12:17.929735 139753105983296 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0229 05:12:33.490501 139753105983296 spec.py:349] Evaluating on the test split.
I0229 05:12:34.299209 139753105983296 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:12:34.311910 139753105983296 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0229 05:12:34.350902 139753105983296 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0229 05:12:38.592653 139753105983296 submission_runner.py:411] Time since start: 89.60s, 	Step: 1, 	{'train/accuracy': 0.0011160713620483875, 'train/loss': 6.914276599884033, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.913983345031738, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.914037704467773, 'test/num_examples': 10000, 'score': 52.276708364486694, 'total_duration': 89.5958993434906, 'accumulated_submission_time': 52.276708364486694, 'accumulated_eval_time': 37.31908655166626, 'accumulated_logging_time': 0}
I0229 05:12:38.610147 139567747282688 logging_writer.py:48] [1] accumulated_eval_time=37.319087, accumulated_logging_time=0, accumulated_submission_time=52.276708, global_step=1, preemption_count=0, score=52.276708, test/accuracy=0.000800, test/loss=6.914038, test/num_examples=10000, total_duration=89.595899, train/accuracy=0.001116, train/loss=6.914277, validation/accuracy=0.001120, validation/loss=6.913983, validation/num_examples=50000
I0229 05:13:12.489821 139567738889984 logging_writer.py:48] [100] global_step=100, grad_norm=0.6764416694641113, loss=6.828644275665283
I0229 05:13:46.484287 139567747282688 logging_writer.py:48] [200] global_step=200, grad_norm=0.8360884785652161, loss=6.554777145385742
I0229 05:14:20.529217 139567738889984 logging_writer.py:48] [300] global_step=300, grad_norm=1.0310133695602417, loss=6.271829605102539
I0229 05:14:54.564033 139567747282688 logging_writer.py:48] [400] global_step=400, grad_norm=1.6244663000106812, loss=6.0208539962768555
I0229 05:15:28.660962 139567738889984 logging_writer.py:48] [500] global_step=500, grad_norm=2.2105302810668945, loss=5.792984962463379
I0229 05:16:02.763763 139567747282688 logging_writer.py:48] [600] global_step=600, grad_norm=2.3131444454193115, loss=5.549654960632324
I0229 05:16:36.859032 139567738889984 logging_writer.py:48] [700] global_step=700, grad_norm=4.975005149841309, loss=5.461348533630371
I0229 05:17:10.974386 139567747282688 logging_writer.py:48] [800] global_step=800, grad_norm=2.8844127655029297, loss=5.359619617462158
I0229 05:17:45.079961 139567738889984 logging_writer.py:48] [900] global_step=900, grad_norm=3.9193992614746094, loss=5.203693866729736
I0229 05:18:19.195913 139567747282688 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.7747039794921875, loss=5.10858678817749
I0229 05:18:53.287776 139567738889984 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.71632981300354, loss=4.882287979125977
I0229 05:19:27.393773 139567747282688 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.2530574798583984, loss=4.812053680419922
I0229 05:20:01.510345 139567738889984 logging_writer.py:48] [1300] global_step=1300, grad_norm=6.976914882659912, loss=4.763275623321533
I0229 05:20:35.614705 139567747282688 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.44474458694458, loss=4.607274055480957
I0229 05:21:08.822685 139753105983296 spec.py:321] Evaluating on the training split.
I0229 05:21:16.162331 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 05:21:24.396780 139753105983296 spec.py:349] Evaluating on the test split.
I0229 05:21:26.842693 139753105983296 submission_runner.py:411] Time since start: 617.85s, 	Step: 1499, 	{'train/accuracy': 0.16585618257522583, 'train/loss': 4.275528907775879, 'validation/accuracy': 0.14723999798297882, 'validation/loss': 4.434837818145752, 'validation/num_examples': 50000, 'test/accuracy': 0.11180000752210617, 'test/loss': 4.884735107421875, 'test/num_examples': 10000, 'score': 562.4275517463684, 'total_duration': 617.8459360599518, 'accumulated_submission_time': 562.4275517463684, 'accumulated_eval_time': 55.3390634059906, 'accumulated_logging_time': 0.02741074562072754}
I0229 05:21:26.864034 139567755675392 logging_writer.py:48] [1499] accumulated_eval_time=55.339063, accumulated_logging_time=0.027411, accumulated_submission_time=562.427552, global_step=1499, preemption_count=0, score=562.427552, test/accuracy=0.111800, test/loss=4.884735, test/num_examples=10000, total_duration=617.845936, train/accuracy=0.165856, train/loss=4.275529, validation/accuracy=0.147240, validation/loss=4.434838, validation/num_examples=50000
I0229 05:21:27.577439 139567764068096 logging_writer.py:48] [1500] global_step=1500, grad_norm=7.560323715209961, loss=4.420153617858887
I0229 05:22:01.650813 139567755675392 logging_writer.py:48] [1600] global_step=1600, grad_norm=8.246813774108887, loss=4.417690753936768
I0229 05:22:35.744779 139567764068096 logging_writer.py:48] [1700] global_step=1700, grad_norm=7.938628196716309, loss=4.30731201171875
I0229 05:23:09.833573 139567755675392 logging_writer.py:48] [1800] global_step=1800, grad_norm=6.419836044311523, loss=4.201689720153809
I0229 05:23:43.939292 139567764068096 logging_writer.py:48] [1900] global_step=1900, grad_norm=5.55511999130249, loss=4.093661308288574
I0229 05:24:18.095476 139567755675392 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.125288963317871, loss=4.046833515167236
I0229 05:24:52.211801 139567764068096 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.6903204917907715, loss=4.017977714538574
I0229 05:25:26.329150 139567755675392 logging_writer.py:48] [2200] global_step=2200, grad_norm=7.041365623474121, loss=3.8343911170959473
I0229 05:26:00.416998 139567764068096 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.05821418762207, loss=3.8307032585144043
I0229 05:26:34.491725 139567755675392 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.781704902648926, loss=3.799973726272583
I0229 05:27:08.587953 139567764068096 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.207143306732178, loss=3.651160717010498
I0229 05:27:42.699469 139567755675392 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.560330867767334, loss=3.546229124069214
I0229 05:28:16.789672 139567764068096 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.6768686771392822, loss=3.6231019496917725
I0229 05:28:50.886898 139567755675392 logging_writer.py:48] [2800] global_step=2800, grad_norm=6.327297687530518, loss=3.4743905067443848
I0229 05:29:24.966316 139567764068096 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.152909278869629, loss=3.480642795562744
I0229 05:29:57.170267 139753105983296 spec.py:321] Evaluating on the training split.
I0229 05:30:04.708485 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 05:30:12.751347 139753105983296 spec.py:349] Evaluating on the test split.
I0229 05:30:15.038335 139753105983296 submission_runner.py:411] Time since start: 1146.04s, 	Step: 2996, 	{'train/accuracy': 0.34502550959587097, 'train/loss': 3.0155081748962402, 'validation/accuracy': 0.31943997740745544, 'validation/loss': 3.1867895126342773, 'validation/num_examples': 50000, 'test/accuracy': 0.23650000989437103, 'test/loss': 3.8011603355407715, 'test/num_examples': 10000, 'score': 1072.6695590019226, 'total_duration': 1146.0415749549866, 'accumulated_submission_time': 1072.6695590019226, 'accumulated_eval_time': 73.20708441734314, 'accumulated_logging_time': 0.060552120208740234}
I0229 05:30:15.055147 139585724053248 logging_writer.py:48] [2996] accumulated_eval_time=73.207084, accumulated_logging_time=0.060552, accumulated_submission_time=1072.669559, global_step=2996, preemption_count=0, score=1072.669559, test/accuracy=0.236500, test/loss=3.801160, test/num_examples=10000, total_duration=1146.041575, train/accuracy=0.345026, train/loss=3.015508, validation/accuracy=0.319440, validation/loss=3.186790, validation/num_examples=50000
I0229 05:30:16.765260 139585732445952 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.1625263690948486, loss=3.447021245956421
I0229 05:30:50.838959 139585724053248 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.126509666442871, loss=3.33377742767334
I0229 05:31:24.919939 139585732445952 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.496218204498291, loss=3.3042335510253906
I0229 05:31:58.991100 139585724053248 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.196705341339111, loss=3.2536773681640625
I0229 05:32:33.064663 139585732445952 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.307151794433594, loss=3.104827880859375
I0229 05:33:07.140891 139585724053248 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.1331028938293457, loss=3.1372785568237305
I0229 05:33:41.240211 139585732445952 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.4328975677490234, loss=3.145905017852783
I0229 05:34:15.329676 139585724053248 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.935460090637207, loss=3.2430672645568848
I0229 05:34:49.407298 139585732445952 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.0134124755859375, loss=3.0870776176452637
I0229 05:35:23.471113 139585724053248 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.512362003326416, loss=3.100590229034424
I0229 05:35:57.536147 139585732445952 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.6439521312713623, loss=3.0103330612182617
I0229 05:36:31.604293 139585724053248 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.4590532779693604, loss=2.8753485679626465
I0229 05:37:05.684103 139585732445952 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.080833911895752, loss=2.934910297393799
I0229 05:37:39.721868 139585724053248 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.559065580368042, loss=2.7316484451293945
I0229 05:38:13.789255 139585732445952 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.8354562520980835, loss=2.736363649368286
I0229 05:38:45.209273 139753105983296 spec.py:321] Evaluating on the training split.
I0229 05:38:52.406545 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 05:39:00.723372 139753105983296 spec.py:349] Evaluating on the test split.
I0229 05:39:03.006353 139753105983296 submission_runner.py:411] Time since start: 1674.01s, 	Step: 4494, 	{'train/accuracy': 0.4690887928009033, 'train/loss': 2.3160347938537598, 'validation/accuracy': 0.4360799789428711, 'validation/loss': 2.511484146118164, 'validation/num_examples': 50000, 'test/accuracy': 0.32760000228881836, 'test/loss': 3.2471020221710205, 'test/num_examples': 10000, 'score': 1582.7634320259094, 'total_duration': 1674.0095942020416, 'accumulated_submission_time': 1582.7634320259094, 'accumulated_eval_time': 91.00414776802063, 'accumulated_logging_time': 0.08711767196655273}
I0229 05:39:03.023875 139590161643264 logging_writer.py:48] [4494] accumulated_eval_time=91.004148, accumulated_logging_time=0.087118, accumulated_submission_time=1582.763432, global_step=4494, preemption_count=0, score=1582.763432, test/accuracy=0.327600, test/loss=3.247102, test/num_examples=10000, total_duration=1674.009594, train/accuracy=0.469089, train/loss=2.316035, validation/accuracy=0.436080, validation/loss=2.511484, validation/num_examples=50000
I0229 05:39:05.423667 139590170035968 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.7637009620666504, loss=2.837547779083252
I0229 05:39:39.447551 139590161643264 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.8811535835266113, loss=2.7359352111816406
I0229 05:40:13.481652 139590170035968 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.096026659011841, loss=2.7635498046875
I0229 05:40:47.505479 139590161643264 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.3201587200164795, loss=2.683641195297241
I0229 05:41:21.543773 139590170035968 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.5054354667663574, loss=2.657334089279175
I0229 05:41:55.594360 139590161643264 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.0093448162078857, loss=2.6669278144836426
I0229 05:42:29.676898 139590170035968 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.1995491981506348, loss=2.7433974742889404
I0229 05:43:03.708231 139590161643264 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.3713316917419434, loss=2.6591954231262207
I0229 05:43:37.712504 139590170035968 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.0012433528900146, loss=2.6149933338165283
I0229 05:44:11.700172 139590161643264 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.5798068046569824, loss=2.6207094192504883
I0229 05:44:45.718847 139590170035968 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.8507227897644043, loss=2.598161220550537
I0229 05:45:19.740101 139590161643264 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.7759754657745361, loss=2.4073095321655273
I0229 05:45:53.756834 139590170035968 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.638759732246399, loss=2.5247244834899902
I0229 05:46:27.793143 139590161643264 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.988709568977356, loss=2.549685478210449
I0229 05:47:01.801905 139590170035968 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.5664565563201904, loss=2.483290433883667
I0229 05:47:33.187682 139753105983296 spec.py:321] Evaluating on the training split.
I0229 05:47:40.329765 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 05:47:48.560185 139753105983296 spec.py:349] Evaluating on the test split.
I0229 05:47:50.827547 139753105983296 submission_runner.py:411] Time since start: 2201.83s, 	Step: 5994, 	{'train/accuracy': 0.5284797549247742, 'train/loss': 2.0127384662628174, 'validation/accuracy': 0.49601998925209045, 'validation/loss': 2.2188453674316406, 'validation/num_examples': 50000, 'test/accuracy': 0.37560001015663147, 'test/loss': 2.9999661445617676, 'test/num_examples': 10000, 'score': 2092.8671317100525, 'total_duration': 2201.830789089203, 'accumulated_submission_time': 2092.8671317100525, 'accumulated_eval_time': 108.64397525787354, 'accumulated_logging_time': 0.11474180221557617}
I0229 05:47:50.845099 139589859669760 logging_writer.py:48] [5994] accumulated_eval_time=108.643975, accumulated_logging_time=0.114742, accumulated_submission_time=2092.867132, global_step=5994, preemption_count=0, score=2092.867132, test/accuracy=0.375600, test/loss=2.999966, test/num_examples=10000, total_duration=2201.830789, train/accuracy=0.528480, train/loss=2.012738, validation/accuracy=0.496020, validation/loss=2.218845, validation/num_examples=50000
I0229 05:47:53.229370 139590144857856 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.0086183547973633, loss=2.5733025074005127
I0229 05:48:27.196873 139589859669760 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.9996674060821533, loss=2.320606231689453
I0229 05:49:01.261742 139590144857856 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.1164708137512207, loss=2.4045896530151367
I0229 05:49:35.277940 139589859669760 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.6206204891204834, loss=2.5174102783203125
I0229 05:50:09.316167 139590144857856 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.040332078933716, loss=2.360836982727051
I0229 05:50:43.337592 139589859669760 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.7380754947662354, loss=2.424809217453003
I0229 05:51:17.351354 139590144857856 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.8755743503570557, loss=2.3368911743164062
I0229 05:51:51.389885 139589859669760 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.5400288105010986, loss=2.307124137878418
I0229 05:52:25.401429 139590144857856 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.1557669639587402, loss=2.308842182159424
I0229 05:52:59.411377 139589859669760 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.2555251121520996, loss=2.344377279281616
I0229 05:53:33.405131 139590144857856 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.0228981971740723, loss=2.3466691970825195
I0229 05:54:07.397267 139589859669760 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.720328450202942, loss=2.2715837955474854
I0229 05:54:41.434908 139590144857856 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.1086831092834473, loss=2.3484952449798584
I0229 05:55:15.446652 139589859669760 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.9776129722595215, loss=2.2812116146087646
I0229 05:55:49.463164 139590144857856 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.7826666831970215, loss=2.2479801177978516
I0229 05:56:20.836671 139753105983296 spec.py:321] Evaluating on the training split.
I0229 05:56:28.060861 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 05:56:36.335313 139753105983296 spec.py:349] Evaluating on the test split.
I0229 05:56:38.756393 139753105983296 submission_runner.py:411] Time since start: 2729.76s, 	Step: 7494, 	{'train/accuracy': 0.5736607313156128, 'train/loss': 1.7896974086761475, 'validation/accuracy': 0.5335999727249146, 'validation/loss': 1.997921347618103, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.7276718616485596, 'test/num_examples': 10000, 'score': 2602.7957921028137, 'total_duration': 2729.759635448456, 'accumulated_submission_time': 2602.7957921028137, 'accumulated_eval_time': 126.56366062164307, 'accumulated_logging_time': 0.14190220832824707}
I0229 05:56:38.774095 139589717059328 logging_writer.py:48] [7494] accumulated_eval_time=126.563661, accumulated_logging_time=0.141902, accumulated_submission_time=2602.795792, global_step=7494, preemption_count=0, score=2602.795792, test/accuracy=0.416400, test/loss=2.727672, test/num_examples=10000, total_duration=2729.759635, train/accuracy=0.573661, train/loss=1.789697, validation/accuracy=0.533600, validation/loss=1.997921, validation/num_examples=50000
I0229 05:56:41.153999 139590170035968 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.5158134698867798, loss=2.2072718143463135
I0229 05:57:15.093873 139589717059328 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.0285816192626953, loss=2.2496321201324463
I0229 05:57:49.088763 139590170035968 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.8624162673950195, loss=2.2142539024353027
I0229 05:58:23.080246 139589717059328 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.907039999961853, loss=2.276109218597412
I0229 05:58:57.080650 139590170035968 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.7192795276641846, loss=2.147850751876831
I0229 05:59:31.101755 139589717059328 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.491903781890869, loss=2.0963635444641113
I0229 06:00:05.081899 139590170035968 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.6032308340072632, loss=2.3691256046295166
I0229 06:00:39.095212 139589717059328 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.3171422481536865, loss=2.3021790981292725
I0229 06:01:13.155587 139590170035968 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.4452719688415527, loss=2.2211105823516846
I0229 06:01:47.172310 139589717059328 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.086836814880371, loss=2.146702527999878
I0229 06:02:21.164299 139590170035968 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.251113176345825, loss=2.139167547225952
I0229 06:02:55.190053 139589717059328 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.7142879962921143, loss=2.0743870735168457
I0229 06:03:29.204190 139590170035968 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.245659828186035, loss=2.1870639324188232
I0229 06:04:03.203832 139589717059328 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.2021484375, loss=2.140660524368286
I0229 06:04:37.195101 139590170035968 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.2345770597457886, loss=2.2432217597961426
I0229 06:05:08.928331 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:05:16.219015 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:05:24.720262 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:05:27.002804 139753105983296 submission_runner.py:411] Time since start: 3258.01s, 	Step: 8995, 	{'train/accuracy': 0.5866350531578064, 'train/loss': 1.7268341779708862, 'validation/accuracy': 0.5454999804496765, 'validation/loss': 1.9390524625778198, 'validation/num_examples': 50000, 'test/accuracy': 0.4229000210762024, 'test/loss': 2.710902690887451, 'test/num_examples': 10000, 'score': 3112.8875806331635, 'total_duration': 3258.0060436725616, 'accumulated_submission_time': 3112.8875806331635, 'accumulated_eval_time': 144.63809752464294, 'accumulated_logging_time': 0.1693737506866455}
I0229 06:05:27.020575 139589717059328 logging_writer.py:48] [8995] accumulated_eval_time=144.638098, accumulated_logging_time=0.169374, accumulated_submission_time=3112.887581, global_step=8995, preemption_count=0, score=3112.887581, test/accuracy=0.422900, test/loss=2.710903, test/num_examples=10000, total_duration=3258.006044, train/accuracy=0.586635, train/loss=1.726834, validation/accuracy=0.545500, validation/loss=1.939052, validation/num_examples=50000
I0229 06:05:29.070072 139589725452032 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4442721605300903, loss=2.0494282245635986
I0229 06:06:03.067583 139589717059328 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.807264804840088, loss=2.049213409423828
I0229 06:06:37.055212 139589725452032 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.6473935842514038, loss=2.1464884281158447
I0229 06:07:11.119015 139589717059328 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.4925363063812256, loss=2.246401786804199
I0229 06:07:45.146590 139589725452032 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.267128825187683, loss=2.149049997329712
I0229 06:08:19.162512 139589717059328 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.4114573001861572, loss=2.0745978355407715
I0229 06:08:53.157961 139589725452032 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.0534474849700928, loss=2.1842124462127686
I0229 06:09:27.152972 139589717059328 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.9526077508926392, loss=1.9424422979354858
I0229 06:10:01.177997 139589725452032 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.015899181365967, loss=2.2201626300811768
I0229 06:10:35.228435 139589717059328 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6045821905136108, loss=2.153097152709961
I0229 06:11:09.259705 139589725452032 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.6148958206176758, loss=2.2771544456481934
I0229 06:11:43.267416 139589717059328 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.6287169456481934, loss=2.0918545722961426
I0229 06:12:17.278164 139589725452032 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.983191967010498, loss=2.054206609725952
I0229 06:12:51.326033 139589717059328 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.4386193752288818, loss=2.1148521900177
I0229 06:13:25.360109 139589725452032 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.4300477504730225, loss=2.0018656253814697
I0229 06:13:57.104058 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:14:04.662220 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:14:13.056349 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:14:15.354714 139753105983296 submission_runner.py:411] Time since start: 3786.36s, 	Step: 10495, 	{'train/accuracy': 0.6527024507522583, 'train/loss': 1.4016082286834717, 'validation/accuracy': 0.5786399841308594, 'validation/loss': 1.790817141532898, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.5373878479003906, 'test/num_examples': 10000, 'score': 3622.9106509685516, 'total_duration': 3786.3579564094543, 'accumulated_submission_time': 3622.9106509685516, 'accumulated_eval_time': 162.88873028755188, 'accumulated_logging_time': 0.19646191596984863}
I0229 06:14:15.376961 139589708666624 logging_writer.py:48] [10495] accumulated_eval_time=162.888730, accumulated_logging_time=0.196462, accumulated_submission_time=3622.910651, global_step=10495, preemption_count=0, score=3622.910651, test/accuracy=0.452100, test/loss=2.537388, test/num_examples=10000, total_duration=3786.357956, train/accuracy=0.652702, train/loss=1.401608, validation/accuracy=0.578640, validation/loss=1.790817, validation/num_examples=50000
I0229 06:14:17.414962 139590161643264 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.9180351495742798, loss=2.1176111698150635
I0229 06:14:51.354969 139589708666624 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.5707693099975586, loss=2.108543872833252
I0229 06:15:25.335021 139590161643264 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.4998252391815186, loss=2.050086736679077
I0229 06:15:59.336343 139589708666624 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.5561243295669556, loss=2.0681874752044678
I0229 06:16:33.332400 139590161643264 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.4456827640533447, loss=2.07660174369812
I0229 06:17:07.334084 139589708666624 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.8756660223007202, loss=2.24929141998291
I0229 06:17:41.316337 139590161643264 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.6404156684875488, loss=2.0733401775360107
I0229 06:18:15.292474 139589708666624 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.7367639541625977, loss=2.1477110385894775
I0229 06:18:49.295030 139590161643264 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.068664073944092, loss=1.9557253122329712
I0229 06:19:23.315155 139589708666624 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.4833705425262451, loss=1.9965847730636597
I0229 06:19:57.292925 139590161643264 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.6147500276565552, loss=2.096787929534912
I0229 06:20:31.290008 139589708666624 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.680922508239746, loss=1.965506911277771
I0229 06:21:05.252269 139590161643264 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.9720566272735596, loss=2.0062098503112793
I0229 06:21:39.257162 139589708666624 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.6482211351394653, loss=1.9764872789382935
I0229 06:22:13.237019 139590161643264 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4268816709518433, loss=1.9322805404663086
I0229 06:22:45.618912 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:22:53.040566 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:23:01.530390 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:23:03.891485 139753105983296 submission_runner.py:411] Time since start: 4314.89s, 	Step: 11997, 	{'train/accuracy': 0.6460459232330322, 'train/loss': 1.433350682258606, 'validation/accuracy': 0.5874199867248535, 'validation/loss': 1.7437560558319092, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.4923598766326904, 'test/num_examples': 10000, 'score': 4133.089784383774, 'total_duration': 4314.894702672958, 'accumulated_submission_time': 4133.089784383774, 'accumulated_eval_time': 181.16124773025513, 'accumulated_logging_time': 0.23114228248596191}
I0229 06:23:03.915464 139590170035968 logging_writer.py:48] [11997] accumulated_eval_time=181.161248, accumulated_logging_time=0.231142, accumulated_submission_time=4133.089784, global_step=11997, preemption_count=0, score=4133.089784, test/accuracy=0.459900, test/loss=2.492360, test/num_examples=10000, total_duration=4314.894703, train/accuracy=0.646046, train/loss=1.433351, validation/accuracy=0.587420, validation/loss=1.743756, validation/num_examples=50000
I0229 06:23:05.290365 139590178428672 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.3593878746032715, loss=1.8669451475143433
I0229 06:23:39.244658 139590170035968 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.2633023262023926, loss=1.9184941053390503
I0229 06:24:13.225339 139590178428672 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6688657999038696, loss=1.9738290309906006
I0229 06:24:47.217850 139590170035968 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.7152315378189087, loss=2.011277437210083
I0229 06:25:21.202202 139590178428672 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.4362685680389404, loss=2.0019373893737793
I0229 06:25:55.246301 139590170035968 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.3948137760162354, loss=1.9980807304382324
I0229 06:26:29.259829 139590178428672 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.5013840198516846, loss=1.950232982635498
I0229 06:27:03.270588 139590170035968 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4691765308380127, loss=2.014455795288086
I0229 06:27:37.281663 139590178428672 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.6823049783706665, loss=2.0535905361175537
I0229 06:28:11.259359 139590170035968 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.5905306339263916, loss=1.9483063220977783
I0229 06:28:45.270296 139590178428672 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.3499609231948853, loss=1.9603424072265625
I0229 06:29:19.253900 139590170035968 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.3961931467056274, loss=1.9254305362701416
I0229 06:29:53.258538 139590178428672 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4957926273345947, loss=1.8434091806411743
I0229 06:30:27.279852 139590170035968 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.6418616771697998, loss=1.9909850358963013
I0229 06:31:01.290056 139590178428672 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.7756081819534302, loss=1.9647879600524902
I0229 06:31:34.052907 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:31:41.920728 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:31:50.338333 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:31:52.645723 139753105983296 submission_runner.py:411] Time since start: 4843.65s, 	Step: 13498, 	{'train/accuracy': 0.6513671875, 'train/loss': 1.4058401584625244, 'validation/accuracy': 0.5913599729537964, 'validation/loss': 1.706515908241272, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.4181039333343506, 'test/num_examples': 10000, 'score': 4643.165565252304, 'total_duration': 4843.648952245712, 'accumulated_submission_time': 4643.165565252304, 'accumulated_eval_time': 199.75401329994202, 'accumulated_logging_time': 0.26534509658813477}
I0229 06:31:52.671200 139590144857856 logging_writer.py:48] [13498] accumulated_eval_time=199.754013, accumulated_logging_time=0.265345, accumulated_submission_time=4643.165565, global_step=13498, preemption_count=0, score=4643.165565, test/accuracy=0.473100, test/loss=2.418104, test/num_examples=10000, total_duration=4843.648952, train/accuracy=0.651367, train/loss=1.405840, validation/accuracy=0.591360, validation/loss=1.706516, validation/num_examples=50000
I0229 06:31:53.695551 139590153250560 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.7034002542495728, loss=2.022664785385132
I0229 06:32:27.626713 139590144857856 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6529033184051514, loss=1.945988655090332
I0229 06:33:01.560245 139590153250560 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.582625389099121, loss=1.9461770057678223
I0229 06:33:35.546664 139590144857856 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.5581375360488892, loss=1.982901692390442
I0229 06:34:09.513293 139590153250560 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9470982551574707, loss=1.916000485420227
I0229 06:34:43.523078 139590144857856 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.7112717628479004, loss=1.9479860067367554
I0229 06:35:17.504244 139590153250560 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.9796741008758545, loss=1.8430739641189575
I0229 06:35:51.459357 139590144857856 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.3934848308563232, loss=1.8854185342788696
I0229 06:36:25.433712 139590153250560 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.5575953722000122, loss=1.9323668479919434
I0229 06:36:59.399609 139590144857856 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.651368498802185, loss=1.9560662508010864
I0229 06:37:33.380349 139590153250560 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6252024173736572, loss=1.8313217163085938
I0229 06:38:07.375704 139590144857856 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.5362621545791626, loss=1.904963731765747
I0229 06:38:41.322481 139590153250560 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.609490156173706, loss=1.9351041316986084
I0229 06:39:15.320027 139590144857856 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.4981056451797485, loss=2.078381061553955
I0229 06:39:49.292875 139590153250560 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.4550631046295166, loss=1.9141666889190674
I0229 06:40:22.661899 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:40:30.122591 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:40:38.516245 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:40:40.812335 139753105983296 submission_runner.py:411] Time since start: 5371.82s, 	Step: 15000, 	{'train/accuracy': 0.6477000713348389, 'train/loss': 1.419167160987854, 'validation/accuracy': 0.592960000038147, 'validation/loss': 1.7010241746902466, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.420872211456299, 'test/num_examples': 10000, 'score': 5153.095174074173, 'total_duration': 5371.81556725502, 'accumulated_submission_time': 5153.095174074173, 'accumulated_eval_time': 217.90440320968628, 'accumulated_logging_time': 0.3007020950317383}
I0229 06:40:40.836913 139589717059328 logging_writer.py:48] [15000] accumulated_eval_time=217.904403, accumulated_logging_time=0.300702, accumulated_submission_time=5153.095174, global_step=15000, preemption_count=0, score=5153.095174, test/accuracy=0.472600, test/loss=2.420872, test/num_examples=10000, total_duration=5371.815567, train/accuracy=0.647700, train/loss=1.419167, validation/accuracy=0.592960, validation/loss=1.701024, validation/num_examples=50000
I0229 06:40:41.206109 139590170035968 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.898801565170288, loss=1.9437007904052734
I0229 06:41:15.158127 139589717059328 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.4118151664733887, loss=1.9442497491836548
I0229 06:41:49.107663 139590170035968 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.604915738105774, loss=1.9282653331756592
I0229 06:42:23.113507 139589717059328 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4618910551071167, loss=1.83439040184021
I0229 06:42:57.111076 139590170035968 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5595494508743286, loss=1.7860859632492065
I0229 06:43:31.106127 139589717059328 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.3618371486663818, loss=1.8265706300735474
I0229 06:44:05.125838 139590170035968 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.761558175086975, loss=1.964176058769226
I0229 06:44:39.089239 139589717059328 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.7769769430160522, loss=1.8961763381958008
I0229 06:45:13.080114 139590170035968 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.6886647939682007, loss=1.7869881391525269
I0229 06:45:47.108178 139589717059328 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.376883864402771, loss=1.9501854181289673
I0229 06:46:21.043187 139590170035968 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.4248440265655518, loss=1.9016913175582886
I0229 06:46:55.060762 139589717059328 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.5163400173187256, loss=1.903651237487793
I0229 06:47:29.043974 139590170035968 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.4988024234771729, loss=1.8679472208023071
I0229 06:48:03.036055 139589717059328 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.490147352218628, loss=1.927765965461731
I0229 06:48:37.022464 139590170035968 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.4042448997497559, loss=1.9326046705245972
I0229 06:49:10.992558 139589717059328 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.4323670864105225, loss=1.8363133668899536
I0229 06:49:11.001071 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:49:18.484400 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:49:26.905549 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:49:29.230651 139753105983296 submission_runner.py:411] Time since start: 5900.23s, 	Step: 16501, 	{'train/accuracy': 0.6524234414100647, 'train/loss': 1.3966569900512695, 'validation/accuracy': 0.5991599559783936, 'validation/loss': 1.6813833713531494, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.4155619144439697, 'test/num_examples': 10000, 'score': 5663.19630074501, 'total_duration': 5900.233821630478, 'accumulated_submission_time': 5663.19630074501, 'accumulated_eval_time': 236.13387608528137, 'accumulated_logging_time': 0.3381190299987793}
I0229 06:49:29.251544 139590144857856 logging_writer.py:48] [16501] accumulated_eval_time=236.133876, accumulated_logging_time=0.338119, accumulated_submission_time=5663.196301, global_step=16501, preemption_count=0, score=5663.196301, test/accuracy=0.470800, test/loss=2.415562, test/num_examples=10000, total_duration=5900.233822, train/accuracy=0.652423, train/loss=1.396657, validation/accuracy=0.599160, validation/loss=1.681383, validation/num_examples=50000
I0229 06:50:03.185347 139590153250560 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.6638821363449097, loss=1.8838661909103394
I0229 06:50:37.197720 139590144857856 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.7685356140136719, loss=1.949094295501709
I0229 06:51:11.207659 139590153250560 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4264274835586548, loss=1.957148790359497
I0229 06:51:45.193417 139590144857856 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.6885948181152344, loss=1.9087600708007812
I0229 06:52:19.174484 139590153250560 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.5366002321243286, loss=2.0151426792144775
I0229 06:52:53.129989 139590144857856 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.658389925956726, loss=1.8750022649765015
I0229 06:53:27.113973 139590153250560 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4909727573394775, loss=1.8299846649169922
I0229 06:54:01.099248 139590144857856 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.565402865409851, loss=1.6998491287231445
I0229 06:54:35.073346 139590153250560 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.654670000076294, loss=2.047632932662964
I0229 06:55:09.050788 139590144857856 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.5324567556381226, loss=1.8254585266113281
I0229 06:55:43.023006 139590153250560 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.495483160018921, loss=1.8965704441070557
I0229 06:56:17.032496 139590144857856 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8866857290267944, loss=1.8857709169387817
I0229 06:56:50.998433 139590153250560 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.548718810081482, loss=1.9036059379577637
I0229 06:57:24.976952 139590144857856 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5805182456970215, loss=1.8658456802368164
I0229 06:57:58.951190 139590153250560 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.442582368850708, loss=1.8349862098693848
I0229 06:57:59.387830 139753105983296 spec.py:321] Evaluating on the training split.
I0229 06:58:07.659509 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 06:58:16.256256 139753105983296 spec.py:349] Evaluating on the test split.
I0229 06:58:18.522986 139753105983296 submission_runner.py:411] Time since start: 6429.53s, 	Step: 18003, 	{'train/accuracy': 0.6489955186843872, 'train/loss': 1.4116501808166504, 'validation/accuracy': 0.5984399914741516, 'validation/loss': 1.6733524799346924, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.3799636363983154, 'test/num_examples': 10000, 'score': 6173.271780967712, 'total_duration': 6429.526218175888, 'accumulated_submission_time': 6173.271780967712, 'accumulated_eval_time': 255.26897883415222, 'accumulated_logging_time': 0.36900877952575684}
I0229 06:58:18.552712 139589708666624 logging_writer.py:48] [18003] accumulated_eval_time=255.268979, accumulated_logging_time=0.369009, accumulated_submission_time=6173.271781, global_step=18003, preemption_count=0, score=6173.271781, test/accuracy=0.475900, test/loss=2.379964, test/num_examples=10000, total_duration=6429.526218, train/accuracy=0.648996, train/loss=1.411650, validation/accuracy=0.598440, validation/loss=1.673352, validation/num_examples=50000
I0229 06:58:51.772124 139589717059328 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.4370993375778198, loss=1.781043291091919
I0229 06:59:25.721263 139589708666624 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.646649956703186, loss=1.8042172193527222
I0229 06:59:59.689017 139589717059328 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.5389450788497925, loss=1.8649053573608398
I0229 07:00:33.682569 139589708666624 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.5719561576843262, loss=1.879242181777954
I0229 07:01:07.664647 139589717059328 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7506266832351685, loss=1.8283085823059082
I0229 07:01:41.645011 139589708666624 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.6335482597351074, loss=1.848795771598816
I0229 07:02:15.597325 139589717059328 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.5680118799209595, loss=1.7843716144561768
I0229 07:02:49.606674 139589708666624 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.5956186056137085, loss=1.7547982931137085
I0229 07:03:23.560932 139589717059328 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.6145389080047607, loss=1.7288804054260254
I0229 07:03:57.544641 139589708666624 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5174070596694946, loss=1.772680640220642
I0229 07:04:31.519993 139589717059328 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5333830118179321, loss=1.7729921340942383
I0229 07:05:05.495143 139589708666624 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.0082619190216064, loss=1.7084968090057373
I0229 07:05:39.481241 139589717059328 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.089829683303833, loss=1.9279186725616455
I0229 07:06:13.467974 139589708666624 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.4845772981643677, loss=1.7969646453857422
I0229 07:06:47.432006 139589717059328 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.5417094230651855, loss=1.9246430397033691
I0229 07:06:48.554111 139753105983296 spec.py:321] Evaluating on the training split.
I0229 07:06:56.448548 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 07:07:05.156152 139753105983296 spec.py:349] Evaluating on the test split.
I0229 07:07:07.467733 139753105983296 submission_runner.py:411] Time since start: 6958.47s, 	Step: 19505, 	{'train/accuracy': 0.7088249325752258, 'train/loss': 1.1469347476959229, 'validation/accuracy': 0.6089000105857849, 'validation/loss': 1.630070447921753, 'validation/num_examples': 50000, 'test/accuracy': 0.48680001497268677, 'test/loss': 2.3481414318084717, 'test/num_examples': 10000, 'score': 6683.211872577667, 'total_duration': 6958.470961809158, 'accumulated_submission_time': 6683.211872577667, 'accumulated_eval_time': 274.1825485229492, 'accumulated_logging_time': 0.4079773426055908}
I0229 07:07:07.490518 139590170035968 logging_writer.py:48] [19505] accumulated_eval_time=274.182549, accumulated_logging_time=0.407977, accumulated_submission_time=6683.211873, global_step=19505, preemption_count=0, score=6683.211873, test/accuracy=0.486800, test/loss=2.348141, test/num_examples=10000, total_duration=6958.470962, train/accuracy=0.708825, train/loss=1.146935, validation/accuracy=0.608900, validation/loss=1.630070, validation/num_examples=50000
I0229 07:07:40.045035 139590178428672 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.601836085319519, loss=1.7636209726333618
I0229 07:08:13.999737 139590170035968 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.6849513053894043, loss=1.8635939359664917
I0229 07:08:48.039851 139590178428672 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6513934135437012, loss=1.7722773551940918
I0229 07:09:22.022753 139590170035968 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.586878776550293, loss=1.698307991027832
I0229 07:09:56.001462 139590178428672 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.7473750114440918, loss=1.810741901397705
I0229 07:10:29.975224 139590170035968 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6442184448242188, loss=1.7739839553833008
I0229 07:11:03.957211 139590178428672 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.833052396774292, loss=1.8820273876190186
I0229 07:11:37.928587 139590170035968 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.624820351600647, loss=1.7682493925094604
I0229 07:12:11.903693 139590178428672 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.689426064491272, loss=1.7374297380447388
I0229 07:12:45.878012 139590170035968 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.626416802406311, loss=1.7317488193511963
I0229 07:13:19.848383 139590178428672 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.3832988739013672, loss=1.688284993171692
I0229 07:13:53.823548 139590170035968 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.9769880771636963, loss=1.75447678565979
I0229 07:14:27.824975 139590178428672 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7152857780456543, loss=1.714086651802063
I0229 07:15:01.863197 139590170035968 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.9706963300704956, loss=1.9265145063400269
I0229 07:15:35.829168 139590178428672 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.6901402473449707, loss=1.851926565170288
I0229 07:15:37.627952 139753105983296 spec.py:321] Evaluating on the training split.
I0229 07:15:45.183811 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 07:15:53.771882 139753105983296 spec.py:349] Evaluating on the test split.
I0229 07:15:56.045463 139753105983296 submission_runner.py:411] Time since start: 7487.05s, 	Step: 21007, 	{'train/accuracy': 0.6806440949440002, 'train/loss': 1.2608718872070312, 'validation/accuracy': 0.6103000044822693, 'validation/loss': 1.6341451406478882, 'validation/num_examples': 50000, 'test/accuracy': 0.48280003666877747, 'test/loss': 2.377382516860962, 'test/num_examples': 10000, 'score': 7193.288778066635, 'total_duration': 7487.048682928085, 'accumulated_submission_time': 7193.288778066635, 'accumulated_eval_time': 292.59999918937683, 'accumulated_logging_time': 0.44109582901000977}
I0229 07:15:56.080787 139589717059328 logging_writer.py:48] [21007] accumulated_eval_time=292.599999, accumulated_logging_time=0.441096, accumulated_submission_time=7193.288778, global_step=21007, preemption_count=0, score=7193.288778, test/accuracy=0.482800, test/loss=2.377383, test/num_examples=10000, total_duration=7487.048683, train/accuracy=0.680644, train/loss=1.260872, validation/accuracy=0.610300, validation/loss=1.634145, validation/num_examples=50000
I0229 07:16:27.984176 139589725452032 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.5844711065292358, loss=1.8852074146270752
I0229 07:17:01.928413 139589717059328 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.6019809246063232, loss=1.8370211124420166
I0229 07:17:35.902285 139589725452032 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.819973111152649, loss=1.7870391607284546
I0229 07:18:09.895703 139589717059328 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6803983449935913, loss=1.8606139421463013
I0229 07:18:43.878377 139589725452032 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.7245562076568604, loss=1.829748511314392
I0229 07:19:17.897369 139589717059328 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6838047504425049, loss=1.7521589994430542
I0229 07:19:51.848470 139589725452032 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.9063905477523804, loss=1.9966126680374146
I0229 07:20:25.861682 139589717059328 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.0459651947021484, loss=1.8382891416549683
I0229 07:20:59.889353 139589725452032 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.8661377429962158, loss=1.7656465768814087
I0229 07:21:33.933176 139589717059328 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.7688567638397217, loss=1.9306659698486328
I0229 07:22:07.908309 139589725452032 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.7612720727920532, loss=1.9170399904251099
I0229 07:22:41.923690 139589717059328 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.7182855606079102, loss=1.78653883934021
I0229 07:23:15.923875 139589725452032 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.8514307737350464, loss=1.818353533744812
I0229 07:23:49.909065 139589717059328 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.4205487966537476, loss=1.7409027814865112
I0229 07:24:23.927627 139589725452032 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.8912912607192993, loss=1.8251214027404785
I0229 07:24:26.071893 139753105983296 spec.py:321] Evaluating on the training split.
I0229 07:24:33.681421 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 07:24:42.268774 139753105983296 spec.py:349] Evaluating on the test split.
I0229 07:24:44.519351 139753105983296 submission_runner.py:411] Time since start: 8015.52s, 	Step: 22508, 	{'train/accuracy': 0.6704998016357422, 'train/loss': 1.3082401752471924, 'validation/accuracy': 0.6072799563407898, 'validation/loss': 1.6411656141281128, 'validation/num_examples': 50000, 'test/accuracy': 0.48280003666877747, 'test/loss': 2.3659961223602295, 'test/num_examples': 10000, 'score': 7703.217472076416, 'total_duration': 8015.52258348465, 'accumulated_submission_time': 7703.217472076416, 'accumulated_eval_time': 311.04740691185, 'accumulated_logging_time': 0.4867265224456787}
I0229 07:24:44.546638 139589708666624 logging_writer.py:48] [22508] accumulated_eval_time=311.047407, accumulated_logging_time=0.486727, accumulated_submission_time=7703.217472, global_step=22508, preemption_count=0, score=7703.217472, test/accuracy=0.482800, test/loss=2.365996, test/num_examples=10000, total_duration=8015.522583, train/accuracy=0.670500, train/loss=1.308240, validation/accuracy=0.607280, validation/loss=1.641166, validation/num_examples=50000
I0229 07:25:16.096490 139589717059328 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.572145938873291, loss=1.8417651653289795
I0229 07:25:50.044465 139589708666624 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.8778904676437378, loss=1.8581857681274414
I0229 07:26:24.017717 139589717059328 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.8247904777526855, loss=1.9425204992294312
I0229 07:26:57.995660 139589708666624 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.5632606744766235, loss=1.8247535228729248
I0229 07:27:32.023954 139589717059328 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.4442124366760254, loss=1.7042474746704102
I0229 07:28:06.018821 139589708666624 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7430977821350098, loss=1.7742184400558472
I0229 07:28:39.991483 139589717059328 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.8083701133728027, loss=1.7196877002716064
I0229 07:29:13.985244 139589708666624 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.73188054561615, loss=1.8445849418640137
I0229 07:29:47.951946 139589717059328 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.4794188737869263, loss=1.7520949840545654
I0229 07:30:21.948260 139589708666624 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.6791508197784424, loss=1.7466470003128052
I0229 07:30:55.920851 139589717059328 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.9425581693649292, loss=1.7705180644989014
I0229 07:31:29.902733 139589708666624 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.6738234758377075, loss=1.8750038146972656
I0229 07:32:03.877640 139589717059328 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.8530923128128052, loss=1.7644872665405273
I0229 07:32:37.858311 139589708666624 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.7678391933441162, loss=1.8357059955596924
I0229 07:33:11.827177 139589717059328 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.637105107307434, loss=1.7173371315002441
I0229 07:33:14.663485 139753105983296 spec.py:321] Evaluating on the training split.
I0229 07:33:22.586118 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 07:33:33.583528 139753105983296 spec.py:349] Evaluating on the test split.
I0229 07:33:35.883756 139753105983296 submission_runner.py:411] Time since start: 8546.89s, 	Step: 24010, 	{'train/accuracy': 0.6690449714660645, 'train/loss': 1.3177435398101807, 'validation/accuracy': 0.608460009098053, 'validation/loss': 1.6242786645889282, 'validation/num_examples': 50000, 'test/accuracy': 0.48180001974105835, 'test/loss': 2.3678853511810303, 'test/num_examples': 10000, 'score': 8213.273429870605, 'total_duration': 8546.886992692947, 'accumulated_submission_time': 8213.273429870605, 'accumulated_eval_time': 332.26763224601746, 'accumulated_logging_time': 0.5245239734649658}
I0229 07:33:35.904078 139590161643264 logging_writer.py:48] [24010] accumulated_eval_time=332.267632, accumulated_logging_time=0.524524, accumulated_submission_time=8213.273430, global_step=24010, preemption_count=0, score=8213.273430, test/accuracy=0.481800, test/loss=2.367885, test/num_examples=10000, total_duration=8546.886993, train/accuracy=0.669045, train/loss=1.317744, validation/accuracy=0.608460, validation/loss=1.624279, validation/num_examples=50000
I0229 07:34:06.780803 139590186821376 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.5900516510009766, loss=1.757210373878479
I0229 07:34:40.770585 139590161643264 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.8385469913482666, loss=1.8208733797073364
I0229 07:35:14.743051 139590186821376 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.7630057334899902, loss=1.7082409858703613
I0229 07:35:48.726708 139590161643264 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.6433770656585693, loss=1.8927873373031616
I0229 07:36:22.715721 139590186821376 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.7182270288467407, loss=1.7793039083480835
I0229 07:36:56.713973 139590161643264 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.8975160121917725, loss=1.6928452253341675
I0229 07:37:30.706219 139590186821376 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.7952828407287598, loss=1.7699427604675293
I0229 07:38:04.683628 139590161643264 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.2374560832977295, loss=1.8358533382415771
I0229 07:38:38.633396 139590186821376 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.7834824323654175, loss=1.800995945930481
I0229 07:39:12.634484 139590161643264 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.741786003112793, loss=1.8572423458099365
I0229 07:39:46.663100 139590186821376 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.8555973768234253, loss=1.8216798305511475
I0229 07:40:20.651089 139590161643264 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.5951143503189087, loss=1.7617955207824707
I0229 07:40:54.645446 139590186821376 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.8637964725494385, loss=1.708951473236084
I0229 07:41:28.647562 139590161643264 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8630989789962769, loss=1.7020756006240845
I0229 07:42:02.625688 139590186821376 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.4356619119644165, loss=1.5601898431777954
I0229 07:42:06.139727 139753105983296 spec.py:321] Evaluating on the training split.
I0229 07:42:13.704633 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 07:42:27.199459 139753105983296 spec.py:349] Evaluating on the test split.
I0229 07:42:29.334685 139753105983296 submission_runner.py:411] Time since start: 9080.34s, 	Step: 25512, 	{'train/accuracy': 0.6869817972183228, 'train/loss': 1.2383699417114258, 'validation/accuracy': 0.627079963684082, 'validation/loss': 1.5450199842453003, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.262160301208496, 'test/num_examples': 10000, 'score': 8723.448077917099, 'total_duration': 9080.337821245193, 'accumulated_submission_time': 8723.448077917099, 'accumulated_eval_time': 355.46244287490845, 'accumulated_logging_time': 0.5544643402099609}
I0229 07:42:29.352184 139590144857856 logging_writer.py:48] [25512] accumulated_eval_time=355.462443, accumulated_logging_time=0.554464, accumulated_submission_time=8723.448078, global_step=25512, preemption_count=0, score=8723.448078, test/accuracy=0.499500, test/loss=2.262160, test/num_examples=10000, total_duration=9080.337821, train/accuracy=0.686982, train/loss=1.238370, validation/accuracy=0.627080, validation/loss=1.545020, validation/num_examples=50000
I0229 07:42:59.534240 139590153250560 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.662327527999878, loss=1.6811399459838867
I0229 07:43:33.431525 139590144857856 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.90312922000885, loss=1.8187848329544067
I0229 07:44:07.360352 139590153250560 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.5461522340774536, loss=1.7972643375396729
I0229 07:44:41.326415 139590144857856 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.7505285739898682, loss=1.7213890552520752
I0229 07:45:15.321306 139590153250560 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.6619495153427124, loss=1.7904597520828247
I0229 07:45:49.347734 139590144857856 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.6310548782348633, loss=1.8514958620071411
I0229 07:46:23.311019 139590153250560 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6686408519744873, loss=1.7423758506774902
I0229 07:46:57.278341 139590144857856 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6688339710235596, loss=1.696158766746521
I0229 07:47:31.242853 139590153250560 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.663098931312561, loss=1.744035243988037
I0229 07:48:05.227984 139590144857856 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.718874216079712, loss=1.7568871974945068
I0229 07:48:39.186804 139590153250560 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.8234676122665405, loss=1.7412574291229248
I0229 07:49:13.152128 139590144857856 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.5329681634902954, loss=1.7081398963928223
I0229 07:49:47.094532 139590153250560 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.9746989011764526, loss=1.7497503757476807
I0229 07:50:21.056608 139590144857856 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.6752371788024902, loss=1.7094932794570923
I0229 07:50:55.049472 139590153250560 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.7879319190979004, loss=1.8131515979766846
I0229 07:50:59.596689 139753105983296 spec.py:321] Evaluating on the training split.
I0229 07:51:07.173527 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 07:51:21.014993 139753105983296 spec.py:349] Evaluating on the test split.
I0229 07:51:23.220437 139753105983296 submission_runner.py:411] Time since start: 9614.22s, 	Step: 27015, 	{'train/accuracy': 0.6685666441917419, 'train/loss': 1.3166530132293701, 'validation/accuracy': 0.6162599921226501, 'validation/loss': 1.593363881111145, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.3350415229797363, 'test/num_examples': 10000, 'score': 9233.63310432434, 'total_duration': 9614.2236931324, 'accumulated_submission_time': 9233.63310432434, 'accumulated_eval_time': 379.08618807792664, 'accumulated_logging_time': 0.5806448459625244}
I0229 07:51:23.244079 139590161643264 logging_writer.py:48] [27015] accumulated_eval_time=379.086188, accumulated_logging_time=0.580645, accumulated_submission_time=9233.633104, global_step=27015, preemption_count=0, score=9233.633104, test/accuracy=0.488300, test/loss=2.335042, test/num_examples=10000, total_duration=9614.223693, train/accuracy=0.668567, train/loss=1.316653, validation/accuracy=0.616260, validation/loss=1.593364, validation/num_examples=50000
I0229 07:51:52.623585 139590178428672 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.8245443105697632, loss=1.8012863397598267
I0229 07:52:26.607816 139590161643264 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.4814260005950928, loss=1.6311099529266357
I0229 07:53:00.578860 139590178428672 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6824005842208862, loss=1.745303988456726
I0229 07:53:34.575826 139590161643264 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.6653879880905151, loss=1.6529887914657593
I0229 07:54:08.565957 139590178428672 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.855985164642334, loss=1.7203203439712524
I0229 07:54:42.564840 139590161643264 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.6556586027145386, loss=1.768296480178833
I0229 07:55:16.551614 139590178428672 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.8798487186431885, loss=1.8018856048583984
I0229 07:55:50.541522 139590161643264 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.7117335796356201, loss=1.7978153228759766
I0229 07:56:24.507234 139590178428672 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.9803225994110107, loss=1.7244083881378174
I0229 07:56:58.492402 139590161643264 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.6921923160552979, loss=1.804251790046692
I0229 07:57:32.449169 139590178428672 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.7994660139083862, loss=1.8271452188491821
I0229 07:58:06.422511 139590161643264 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.8278601169586182, loss=1.860659122467041
I0229 07:58:40.445213 139590178428672 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.8399903774261475, loss=1.788086175918579
I0229 07:59:14.432083 139590161643264 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.6693270206451416, loss=1.688856601715088
I0229 07:59:48.444434 139590178428672 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.167670249938965, loss=1.8042731285095215
I0229 07:59:53.315673 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:00:00.981753 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:00:15.033024 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:00:17.183294 139753105983296 submission_runner.py:411] Time since start: 10148.19s, 	Step: 28516, 	{'train/accuracy': 0.7271006107330322, 'train/loss': 1.0419692993164062, 'validation/accuracy': 0.6188399791717529, 'validation/loss': 1.5885690450668335, 'validation/num_examples': 50000, 'test/accuracy': 0.48980003595352173, 'test/loss': 2.3473498821258545, 'test/num_examples': 10000, 'score': 9743.41288614273, 'total_duration': 10148.186551094055, 'accumulated_submission_time': 9743.41288614273, 'accumulated_eval_time': 402.9537811279297, 'accumulated_logging_time': 0.8449652194976807}
I0229 08:00:17.202835 139590144857856 logging_writer.py:48] [28516] accumulated_eval_time=402.953781, accumulated_logging_time=0.844965, accumulated_submission_time=9743.412886, global_step=28516, preemption_count=0, score=9743.412886, test/accuracy=0.489800, test/loss=2.347350, test/num_examples=10000, total_duration=10148.186551, train/accuracy=0.727101, train/loss=1.041969, validation/accuracy=0.618840, validation/loss=1.588569, validation/num_examples=50000
I0229 08:00:46.002254 139590153250560 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.7777554988861084, loss=1.7616119384765625
I0229 08:01:19.969473 139590144857856 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6866010427474976, loss=1.8221728801727295
I0229 08:01:53.925872 139590153250560 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.666935682296753, loss=1.7028262615203857
I0229 08:02:27.906095 139590144857856 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.145432710647583, loss=1.6932075023651123
I0229 08:03:01.860790 139590153250560 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6966995000839233, loss=1.7132996320724487
I0229 08:03:35.846390 139590144857856 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.769596815109253, loss=1.6992340087890625
I0229 08:04:09.794904 139590153250560 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.689428448677063, loss=1.825517177581787
I0229 08:04:43.815322 139590144857856 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.8887485265731812, loss=1.6465697288513184
I0229 08:05:17.782826 139590153250560 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.7968924045562744, loss=1.7377084493637085
I0229 08:05:51.791063 139590144857856 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.9369558095932007, loss=1.7329697608947754
I0229 08:06:25.774493 139590153250560 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.7017278671264648, loss=1.7273117303848267
I0229 08:06:59.742022 139590144857856 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.7185715436935425, loss=1.6569032669067383
I0229 08:07:33.715176 139590153250560 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7074689865112305, loss=1.7719327211380005
I0229 08:08:07.721770 139590144857856 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.764867901802063, loss=1.7305123805999756
I0229 08:08:41.686321 139590153250560 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.701436161994934, loss=1.7042832374572754
I0229 08:08:47.232465 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:08:55.004702 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:09:09.978108 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:09:12.132334 139753105983296 submission_runner.py:411] Time since start: 10683.14s, 	Step: 30018, 	{'train/accuracy': 0.70804762840271, 'train/loss': 1.1357526779174805, 'validation/accuracy': 0.6301800012588501, 'validation/loss': 1.5323799848556519, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2726728916168213, 'test/num_examples': 10000, 'score': 10253.381639242172, 'total_duration': 10683.135558843613, 'accumulated_submission_time': 10253.381639242172, 'accumulated_eval_time': 427.8536124229431, 'accumulated_logging_time': 0.8739020824432373}
I0229 08:09:12.152188 139589708666624 logging_writer.py:48] [30018] accumulated_eval_time=427.853612, accumulated_logging_time=0.873902, accumulated_submission_time=10253.381639, global_step=30018, preemption_count=0, score=10253.381639, test/accuracy=0.505400, test/loss=2.272673, test/num_examples=10000, total_duration=10683.135559, train/accuracy=0.708048, train/loss=1.135753, validation/accuracy=0.630180, validation/loss=1.532380, validation/num_examples=50000
I0229 08:09:40.286993 139589725452032 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.704708218574524, loss=1.6652904748916626
I0229 08:10:14.243158 139589708666624 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.6359201669692993, loss=1.7066551446914673
I0229 08:10:48.244912 139589725452032 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.572879433631897, loss=1.7039411067962646
I0229 08:11:22.244158 139589708666624 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.7381223440170288, loss=1.8341021537780762
I0229 08:11:56.226789 139589725452032 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.9214539527893066, loss=1.695025086402893
I0229 08:12:30.223240 139589708666624 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.7359375953674316, loss=1.574796199798584
I0229 08:13:04.229924 139589725452032 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.5123037099838257, loss=1.8234752416610718
I0229 08:13:38.197770 139589708666624 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.8928534984588623, loss=1.7325888872146606
I0229 08:14:12.191143 139589725452032 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.9194979667663574, loss=1.6472535133361816
I0229 08:14:46.150638 139589708666624 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.728659749031067, loss=1.8332750797271729
I0229 08:15:20.134496 139589725452032 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.5259329080581665, loss=1.570508360862732
I0229 08:15:54.094527 139589708666624 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.7874780893325806, loss=1.6765689849853516
I0229 08:16:28.092052 139589725452032 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.8833026885986328, loss=1.709444284439087
I0229 08:17:02.106399 139589708666624 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.949480414390564, loss=1.7799839973449707
I0229 08:17:36.115357 139589725452032 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7312284708023071, loss=1.6409008502960205
I0229 08:17:42.352502 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:17:50.030406 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:18:05.201673 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:18:07.334959 139753105983296 submission_runner.py:411] Time since start: 11218.34s, 	Step: 31520, 	{'train/accuracy': 0.6942163705825806, 'train/loss': 1.2090507745742798, 'validation/accuracy': 0.6239399909973145, 'validation/loss': 1.5508848428726196, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.2808399200439453, 'test/num_examples': 10000, 'score': 10763.521374940872, 'total_duration': 11218.338215351105, 'accumulated_submission_time': 10763.521374940872, 'accumulated_eval_time': 452.8360719680786, 'accumulated_logging_time': 0.9039633274078369}
I0229 08:18:07.358752 139590153250560 logging_writer.py:48] [31520] accumulated_eval_time=452.836072, accumulated_logging_time=0.903963, accumulated_submission_time=10763.521375, global_step=31520, preemption_count=0, score=10763.521375, test/accuracy=0.495100, test/loss=2.280840, test/num_examples=10000, total_duration=11218.338215, train/accuracy=0.694216, train/loss=1.209051, validation/accuracy=0.623940, validation/loss=1.550885, validation/num_examples=50000
I0229 08:18:34.824549 139590161643264 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.7111003398895264, loss=1.7188998460769653
I0229 08:19:08.749373 139590153250560 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.702114462852478, loss=1.677895426750183
I0229 08:19:42.700975 139590161643264 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.031059980392456, loss=1.732059359550476
I0229 08:20:16.678807 139590153250560 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.7717357873916626, loss=1.6304388046264648
I0229 08:20:50.646786 139590161643264 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.8967695236206055, loss=1.6824696063995361
I0229 08:21:24.632153 139590153250560 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.709862470626831, loss=1.6363801956176758
I0229 08:21:58.608890 139590161643264 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6801496744155884, loss=1.6961262226104736
I0229 08:22:32.589246 139590153250560 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.0145015716552734, loss=1.6775639057159424
I0229 08:23:06.589055 139590161643264 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.0151336193084717, loss=1.735498309135437
I0229 08:23:40.555855 139590153250560 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.716446876525879, loss=1.7274658679962158
I0229 08:24:14.556331 139590161643264 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.8055052757263184, loss=1.6579062938690186
I0229 08:24:48.529812 139590153250560 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.6404547691345215, loss=1.531186580657959
I0229 08:25:22.500962 139590161643264 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.772482991218567, loss=1.8130977153778076
I0229 08:25:56.496827 139590153250560 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.9195494651794434, loss=1.7688487768173218
I0229 08:26:30.489202 139590161643264 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.7204443216323853, loss=1.7090542316436768
I0229 08:26:37.411402 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:26:45.612272 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:26:59.626132 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:27:01.774440 139753105983296 submission_runner.py:411] Time since start: 11752.78s, 	Step: 33022, 	{'train/accuracy': 0.6808235049247742, 'train/loss': 1.2375199794769287, 'validation/accuracy': 0.6197400093078613, 'validation/loss': 1.5743621587753296, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.32073712348938, 'test/num_examples': 10000, 'score': 11273.513464927673, 'total_duration': 11752.77766919136, 'accumulated_submission_time': 11273.513464927673, 'accumulated_eval_time': 477.19905734062195, 'accumulated_logging_time': 0.9372315406799316}
I0229 08:27:01.796841 139589717059328 logging_writer.py:48] [33022] accumulated_eval_time=477.199057, accumulated_logging_time=0.937232, accumulated_submission_time=11273.513465, global_step=33022, preemption_count=0, score=11273.513465, test/accuracy=0.494400, test/loss=2.320737, test/num_examples=10000, total_duration=11752.777669, train/accuracy=0.680824, train/loss=1.237520, validation/accuracy=0.619740, validation/loss=1.574362, validation/num_examples=50000
I0229 08:27:28.579591 139589725452032 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.6680963039398193, loss=1.6004831790924072
I0229 08:28:02.536740 139589717059328 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.7889717817306519, loss=1.6188853979110718
I0229 08:28:36.496676 139589725452032 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8867979049682617, loss=1.842777967453003
I0229 08:29:10.484283 139589717059328 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.7708755731582642, loss=1.7623915672302246
I0229 08:29:44.576213 139589725452032 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.637462854385376, loss=1.715145468711853
I0229 08:30:18.535512 139589717059328 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.588459849357605, loss=1.6508219242095947
I0229 08:30:52.514607 139589725452032 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.6887609958648682, loss=1.7234692573547363
I0229 08:31:26.481173 139589717059328 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.737441897392273, loss=1.6691014766693115
I0229 08:32:00.463586 139589725452032 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.898943543434143, loss=1.7359209060668945
I0229 08:32:34.444849 139589717059328 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.0003507137298584, loss=1.738857388496399
I0229 08:33:08.414352 139589725452032 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.608110785484314, loss=1.590256690979004
I0229 08:33:42.402812 139589717059328 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.7180354595184326, loss=1.6770548820495605
I0229 08:34:16.356518 139589725452032 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8176674842834473, loss=1.6113862991333008
I0229 08:34:50.339941 139589717059328 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7378343343734741, loss=1.6415077447891235
I0229 08:35:24.310796 139589725452032 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.996044635772705, loss=1.5870338678359985
I0229 08:35:32.227376 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:35:40.561691 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:35:56.636228 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:35:58.806343 139753105983296 submission_runner.py:411] Time since start: 12289.81s, 	Step: 34524, 	{'train/accuracy': 0.6873405575752258, 'train/loss': 1.2315131425857544, 'validation/accuracy': 0.626479983329773, 'validation/loss': 1.5449154376983643, 'validation/num_examples': 50000, 'test/accuracy': 0.49650001525878906, 'test/loss': 2.297478675842285, 'test/num_examples': 10000, 'score': 11783.884510755539, 'total_duration': 12289.809584379196, 'accumulated_submission_time': 11783.884510755539, 'accumulated_eval_time': 503.7779715061188, 'accumulated_logging_time': 0.9684596061706543}
I0229 08:35:58.829434 139589708666624 logging_writer.py:48] [34524] accumulated_eval_time=503.777972, accumulated_logging_time=0.968460, accumulated_submission_time=11783.884511, global_step=34524, preemption_count=0, score=11783.884511, test/accuracy=0.496500, test/loss=2.297479, test/num_examples=10000, total_duration=12289.809584, train/accuracy=0.687341, train/loss=1.231513, validation/accuracy=0.626480, validation/loss=1.544915, validation/num_examples=50000
I0229 08:36:24.939256 139589717059328 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.7420969009399414, loss=1.6437742710113525
I0229 08:36:58.832582 139589708666624 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6776418685913086, loss=1.5231804847717285
I0229 08:37:32.789746 139589717059328 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.9095076322555542, loss=1.5654938220977783
I0229 08:38:06.764768 139589708666624 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.5854008197784424, loss=1.6813517808914185
I0229 08:38:40.726516 139589717059328 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.9135329723358154, loss=1.6868512630462646
I0229 08:39:14.710832 139589708666624 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.036107063293457, loss=1.6425434350967407
I0229 08:39:48.659327 139589717059328 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.894679069519043, loss=1.6026864051818848
I0229 08:40:22.615308 139589708666624 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.2063074111938477, loss=1.7476388216018677
I0229 08:40:56.573849 139589717059328 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.902966022491455, loss=1.7390729188919067
I0229 08:41:30.529009 139589708666624 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.9118720293045044, loss=1.7361351251602173
I0229 08:42:04.521602 139589717059328 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.9377206563949585, loss=1.7180509567260742
I0229 08:42:38.512224 139589708666624 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.9755216836929321, loss=1.7783280611038208
I0229 08:43:12.500918 139589717059328 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.8900588750839233, loss=1.703315019607544
I0229 08:43:46.480134 139589708666624 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.8588712215423584, loss=1.7005658149719238
I0229 08:44:20.456927 139589717059328 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6935548782348633, loss=1.6752703189849854
I0229 08:44:29.035644 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:44:37.150122 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:44:51.805952 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:44:53.939712 139753105983296 submission_runner.py:411] Time since start: 12824.94s, 	Step: 36027, 	{'train/accuracy': 0.6824377775192261, 'train/loss': 1.2468916177749634, 'validation/accuracy': 0.6263799667358398, 'validation/loss': 1.5462514162063599, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.299901008605957, 'test/num_examples': 10000, 'score': 12294.029720783234, 'total_duration': 12824.942950963974, 'accumulated_submission_time': 12294.029720783234, 'accumulated_eval_time': 528.6820592880249, 'accumulated_logging_time': 1.0011744499206543}
I0229 08:44:53.959928 139590153250560 logging_writer.py:48] [36027] accumulated_eval_time=528.682059, accumulated_logging_time=1.001174, accumulated_submission_time=12294.029721, global_step=36027, preemption_count=0, score=12294.029721, test/accuracy=0.501200, test/loss=2.299901, test/num_examples=10000, total_duration=12824.942951, train/accuracy=0.682438, train/loss=1.246892, validation/accuracy=0.626380, validation/loss=1.546251, validation/num_examples=50000
I0229 08:45:19.033699 139590161643264 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.878259301185608, loss=1.7305935621261597
I0229 08:45:52.985418 139590153250560 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.7632359266281128, loss=1.670974850654602
I0229 08:46:26.922094 139590161643264 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.81955087184906, loss=1.71541428565979
I0229 08:47:00.872648 139590153250560 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.178943157196045, loss=1.7832802534103394
I0229 08:47:34.866387 139590161643264 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.102628469467163, loss=1.736577033996582
I0229 08:48:08.852008 139590153250560 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.7106618881225586, loss=1.648486614227295
I0229 08:48:42.886481 139590161643264 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.9790016412734985, loss=1.7132699489593506
I0229 08:49:16.879616 139590153250560 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.7867162227630615, loss=1.7677991390228271
I0229 08:49:50.854611 139590161643264 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7903482913970947, loss=1.725064754486084
I0229 08:50:24.835074 139590153250560 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.8157175779342651, loss=1.6596927642822266
I0229 08:50:58.833419 139590161643264 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.6984184980392456, loss=1.6426348686218262
I0229 08:51:32.812589 139590153250560 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.021244764328003, loss=1.6277424097061157
I0229 08:52:06.795050 139590161643264 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6971856355667114, loss=1.6350973844528198
I0229 08:52:40.755566 139590153250560 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.049731731414795, loss=1.6328442096710205
I0229 08:53:14.729132 139590161643264 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.2595157623291016, loss=1.8021029233932495
I0229 08:53:24.028290 139753105983296 spec.py:321] Evaluating on the training split.
I0229 08:53:31.390831 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 08:53:44.620572 139753105983296 spec.py:349] Evaluating on the test split.
I0229 08:53:46.852482 139753105983296 submission_runner.py:411] Time since start: 13357.86s, 	Step: 37529, 	{'train/accuracy': 0.7158800959587097, 'train/loss': 1.1090245246887207, 'validation/accuracy': 0.6350600123405457, 'validation/loss': 1.5164011716842651, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.2564914226531982, 'test/num_examples': 10000, 'score': 12804.038206338882, 'total_duration': 13357.855704545975, 'accumulated_submission_time': 12804.038206338882, 'accumulated_eval_time': 551.5062041282654, 'accumulated_logging_time': 1.0308482646942139}
I0229 08:53:46.881097 139589725452032 logging_writer.py:48] [37529] accumulated_eval_time=551.506204, accumulated_logging_time=1.030848, accumulated_submission_time=12804.038206, global_step=37529, preemption_count=0, score=12804.038206, test/accuracy=0.503700, test/loss=2.256491, test/num_examples=10000, total_duration=13357.855705, train/accuracy=0.715880, train/loss=1.109025, validation/accuracy=0.635060, validation/loss=1.516401, validation/num_examples=50000
I0229 08:54:11.300279 139590144857856 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.8170548677444458, loss=1.552810788154602
I0229 08:54:45.257622 139589725452032 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.8570287227630615, loss=1.62777841091156
I0229 08:55:19.223253 139590144857856 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.1292364597320557, loss=1.6405459642410278
I0229 08:55:53.191456 139589725452032 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.7990200519561768, loss=1.7576639652252197
I0229 08:56:27.156808 139590144857856 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.0562334060668945, loss=1.6285535097122192
I0229 08:57:01.078706 139589725452032 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.969807744026184, loss=1.7436681985855103
I0229 08:57:35.028084 139590144857856 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.9387538433074951, loss=1.7846782207489014
I0229 08:58:08.965331 139589725452032 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.07666015625, loss=1.6609278917312622
I0229 08:58:42.899217 139590144857856 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.8814226388931274, loss=1.6305662393569946
I0229 08:59:16.863506 139589725452032 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.833814024925232, loss=1.7298362255096436
I0229 08:59:50.795973 139590144857856 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.9555304050445557, loss=1.6635342836380005
I0229 09:00:24.739564 139589725452032 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.140000104904175, loss=1.759729266166687
I0229 09:00:58.774907 139590144857856 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.9173235893249512, loss=1.6530685424804688
I0229 09:01:32.708789 139589725452032 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8238911628723145, loss=1.6634684801101685
I0229 09:02:06.678444 139590144857856 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.7800147533416748, loss=1.6169888973236084
I0229 09:02:16.996996 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:02:24.268705 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:02:37.690579 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:02:39.867007 139753105983296 submission_runner.py:411] Time since start: 13890.87s, 	Step: 39032, 	{'train/accuracy': 0.71683669090271, 'train/loss': 1.1103540658950806, 'validation/accuracy': 0.62909996509552, 'validation/loss': 1.5288817882537842, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.224526882171631, 'test/num_examples': 10000, 'score': 13314.092184782028, 'total_duration': 13890.870227098465, 'accumulated_submission_time': 13314.092184782028, 'accumulated_eval_time': 574.3761565685272, 'accumulated_logging_time': 1.0700502395629883}
I0229 09:02:39.890807 139589708666624 logging_writer.py:48] [39032] accumulated_eval_time=574.376157, accumulated_logging_time=1.070050, accumulated_submission_time=13314.092185, global_step=39032, preemption_count=0, score=13314.092185, test/accuracy=0.509100, test/loss=2.224527, test/num_examples=10000, total_duration=13890.870227, train/accuracy=0.716837, train/loss=1.110354, validation/accuracy=0.629100, validation/loss=1.528882, validation/num_examples=50000
I0229 09:03:03.278309 139589717059328 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.8399066925048828, loss=1.6279499530792236
I0229 09:03:37.191217 139589708666624 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.9519710540771484, loss=1.7195332050323486
I0229 09:04:11.138629 139589717059328 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.8671222925186157, loss=1.6652954816818237
I0229 09:04:45.113761 139589708666624 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.8721482753753662, loss=1.7696813344955444
I0229 09:05:19.096811 139589717059328 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.8110417127609253, loss=1.5238714218139648
I0229 09:05:53.070879 139589708666624 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7776285409927368, loss=1.6185320615768433
I0229 09:06:27.052168 139589717059328 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.8482247591018677, loss=1.6480803489685059
I0229 09:07:01.139646 139589708666624 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.744138240814209, loss=1.6463356018066406
I0229 09:07:35.128753 139589717059328 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.8665107488632202, loss=1.711652398109436
I0229 09:08:09.107856 139589708666624 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.9853979349136353, loss=1.7638911008834839
I0229 09:08:43.089414 139589717059328 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.873981237411499, loss=1.5896599292755127
I0229 09:09:17.076030 139589708666624 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.9773738384246826, loss=1.6796424388885498
I0229 09:09:51.056481 139589717059328 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.0746986865997314, loss=1.7608284950256348
I0229 09:10:25.063300 139589708666624 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.7719988822937012, loss=1.6628224849700928
I0229 09:10:59.042647 139589717059328 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.858338713645935, loss=1.633457899093628
I0229 09:11:10.038861 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:11:17.148252 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:11:30.875015 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:11:33.017524 139753105983296 submission_runner.py:411] Time since start: 14424.02s, 	Step: 40534, 	{'train/accuracy': 0.7096619606018066, 'train/loss': 1.1346745491027832, 'validation/accuracy': 0.6363999843597412, 'validation/loss': 1.4892843961715698, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.232386827468872, 'test/num_examples': 10000, 'score': 13824.17870426178, 'total_duration': 14424.020713090897, 'accumulated_submission_time': 13824.17870426178, 'accumulated_eval_time': 597.3547255992889, 'accumulated_logging_time': 1.1036403179168701}
I0229 09:11:33.042917 139590178428672 logging_writer.py:48] [40534] accumulated_eval_time=597.354726, accumulated_logging_time=1.103640, accumulated_submission_time=13824.178704, global_step=40534, preemption_count=0, score=13824.178704, test/accuracy=0.507600, test/loss=2.232387, test/num_examples=10000, total_duration=14424.020713, train/accuracy=0.709662, train/loss=1.134675, validation/accuracy=0.636400, validation/loss=1.489284, validation/num_examples=50000
I0229 09:11:55.750294 139590186821376 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.7749851942062378, loss=1.729004979133606
I0229 09:12:29.671854 139590178428672 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.8073110580444336, loss=1.6510396003723145
I0229 09:13:03.795608 139590186821376 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6947107315063477, loss=1.5407403707504272
I0229 09:13:37.752080 139590178428672 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7877317667007446, loss=1.662184476852417
I0229 09:14:11.716108 139590186821376 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.9945266246795654, loss=1.6705979108810425
I0229 09:14:45.663950 139590178428672 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7807399034500122, loss=1.8171017169952393
I0229 09:15:19.619885 139590186821376 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.8560552597045898, loss=1.5128639936447144
I0229 09:15:53.591670 139590178428672 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.8857781887054443, loss=1.7111440896987915
I0229 09:16:27.545537 139590186821376 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7198541164398193, loss=1.603300929069519
I0229 09:17:01.518929 139590178428672 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.6916677951812744, loss=1.64077889919281
I0229 09:17:35.478495 139590186821376 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.8894022703170776, loss=1.7437907457351685
I0229 09:18:09.443602 139590178428672 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.1859028339385986, loss=1.7433714866638184
I0229 09:18:43.381564 139590186821376 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.8113412857055664, loss=1.5904772281646729
I0229 09:19:17.483430 139590178428672 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.037060022354126, loss=1.6153315305709839
I0229 09:19:51.466891 139590186821376 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.997277021408081, loss=1.7760413885116577
I0229 09:20:03.157277 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:20:10.169727 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:20:26.436728 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:20:28.630247 139753105983296 submission_runner.py:411] Time since start: 14959.63s, 	Step: 42036, 	{'train/accuracy': 0.6986008882522583, 'train/loss': 1.179208517074585, 'validation/accuracy': 0.6325599551200867, 'validation/loss': 1.5215106010437012, 'validation/num_examples': 50000, 'test/accuracy': 0.5042000412940979, 'test/loss': 2.279670000076294, 'test/num_examples': 10000, 'score': 14334.231875896454, 'total_duration': 14959.633465051651, 'accumulated_submission_time': 14334.231875896454, 'accumulated_eval_time': 622.8276555538177, 'accumulated_logging_time': 1.1378414630889893}
I0229 09:20:28.659929 139589725452032 logging_writer.py:48] [42036] accumulated_eval_time=622.827656, accumulated_logging_time=1.137841, accumulated_submission_time=14334.231876, global_step=42036, preemption_count=0, score=14334.231876, test/accuracy=0.504200, test/loss=2.279670, test/num_examples=10000, total_duration=14959.633465, train/accuracy=0.698601, train/loss=1.179209, validation/accuracy=0.632560, validation/loss=1.521511, validation/num_examples=50000
I0229 09:20:50.692289 139590144857856 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.7204995155334473, loss=1.4555175304412842
I0229 09:21:24.572696 139589725452032 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.9985040426254272, loss=1.6599293947219849
I0229 09:21:58.552382 139590144857856 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.71462082862854, loss=1.604530930519104
I0229 09:22:32.512779 139589725452032 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.63974928855896, loss=1.563327431678772
I0229 09:23:06.506242 139590144857856 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.8011153936386108, loss=1.4963301420211792
I0229 09:23:40.482386 139589725452032 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.771713137626648, loss=1.6905320882797241
I0229 09:24:14.478621 139590144857856 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.9273877143859863, loss=1.6592328548431396
I0229 09:24:48.478055 139589725452032 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.7286714315414429, loss=1.6191598176956177
I0229 09:25:22.489902 139590144857856 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.915557861328125, loss=1.753981113433838
I0229 09:25:56.561305 139589725452032 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8044649362564087, loss=1.6656627655029297
I0229 09:26:30.517395 139590144857856 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.733269214630127, loss=1.5167958736419678
I0229 09:27:04.504916 139589725452032 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.756089687347412, loss=1.6711658239364624
I0229 09:27:38.457556 139590144857856 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.030189275741577, loss=1.7431368827819824
I0229 09:28:12.423668 139589725452032 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.9230722188949585, loss=1.549133539199829
I0229 09:28:46.367747 139590144857856 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.940514326095581, loss=1.493316888809204
I0229 09:28:58.710388 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:29:05.618150 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:29:19.660684 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:29:21.798034 139753105983296 submission_runner.py:411] Time since start: 15492.80s, 	Step: 43538, 	{'train/accuracy': 0.6833944320678711, 'train/loss': 1.2552924156188965, 'validation/accuracy': 0.6233400106430054, 'validation/loss': 1.5577534437179565, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.3413572311401367, 'test/num_examples': 10000, 'score': 14844.221347093582, 'total_duration': 15492.801275491714, 'accumulated_submission_time': 14844.221347093582, 'accumulated_eval_time': 645.9152612686157, 'accumulated_logging_time': 1.1770331859588623}
I0229 09:29:21.824355 139589708666624 logging_writer.py:48] [43538] accumulated_eval_time=645.915261, accumulated_logging_time=1.177033, accumulated_submission_time=14844.221347, global_step=43538, preemption_count=0, score=14844.221347, test/accuracy=0.491900, test/loss=2.341357, test/num_examples=10000, total_duration=15492.801275, train/accuracy=0.683394, train/loss=1.255292, validation/accuracy=0.623340, validation/loss=1.557753, validation/num_examples=50000
I0229 09:29:43.163606 139589717059328 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.980611801147461, loss=1.619931697845459
I0229 09:30:17.078377 139589708666624 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.9240535497665405, loss=1.6199448108673096
I0229 09:30:51.035845 139589717059328 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.8871760368347168, loss=1.5843465328216553
I0229 09:31:24.975267 139589708666624 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.665673017501831, loss=1.5143228769302368
I0229 09:31:58.999686 139589717059328 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8237427473068237, loss=1.5803401470184326
I0229 09:32:32.959283 139589708666624 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.0198400020599365, loss=1.616819143295288
I0229 09:33:06.933824 139589717059328 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.5713510513305664, loss=1.6393070220947266
I0229 09:33:40.901854 139589708666624 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.9535571336746216, loss=1.5046218633651733
I0229 09:34:14.882677 139589717059328 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.6429020166397095, loss=1.6873509883880615
I0229 09:34:48.813626 139589708666624 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.8416683673858643, loss=1.7444679737091064
I0229 09:35:22.751795 139589717059328 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8619993925094604, loss=1.6365289688110352
I0229 09:35:56.700310 139589708666624 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.0307936668395996, loss=1.6695613861083984
I0229 09:36:30.676517 139589717059328 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.981695294380188, loss=1.8264482021331787
I0229 09:37:04.643638 139589708666624 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.930479884147644, loss=1.5810794830322266
I0229 09:37:38.618951 139589717059328 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.8074684143066406, loss=1.651618242263794
I0229 09:37:52.003129 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:37:59.111436 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:38:13.109478 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:38:15.273045 139753105983296 submission_runner.py:411] Time since start: 16026.28s, 	Step: 45041, 	{'train/accuracy': 0.6790497303009033, 'train/loss': 1.2766070365905762, 'validation/accuracy': 0.6209200024604797, 'validation/loss': 1.573642611503601, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.3068480491638184, 'test/num_examples': 10000, 'score': 15354.339327812195, 'total_duration': 16026.276261806488, 'accumulated_submission_time': 15354.339327812195, 'accumulated_eval_time': 669.1851181983948, 'accumulated_logging_time': 1.2121222019195557}
I0229 09:38:15.300271 139589708666624 logging_writer.py:48] [45041] accumulated_eval_time=669.185118, accumulated_logging_time=1.212122, accumulated_submission_time=15354.339328, global_step=45041, preemption_count=0, score=15354.339328, test/accuracy=0.487800, test/loss=2.306848, test/num_examples=10000, total_duration=16026.276262, train/accuracy=0.679050, train/loss=1.276607, validation/accuracy=0.620920, validation/loss=1.573643, validation/num_examples=50000
I0229 09:38:35.642496 139589717059328 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.9061243534088135, loss=1.7250573635101318
I0229 09:39:09.512615 139589708666624 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.042713165283203, loss=1.6401290893554688
I0229 09:39:43.462389 139589717059328 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8868685960769653, loss=1.6702957153320312
I0229 09:40:17.419524 139589708666624 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.733434796333313, loss=1.5488901138305664
I0229 09:40:51.379320 139589717059328 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7822552919387817, loss=1.7501033544540405
I0229 09:41:25.345489 139589708666624 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.8592878580093384, loss=1.6802833080291748
I0229 09:41:59.283041 139589717059328 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.8831971883773804, loss=1.5400669574737549
I0229 09:42:33.275563 139589708666624 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.8982031345367432, loss=1.7474384307861328
I0229 09:43:07.268231 139589717059328 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7194949388504028, loss=1.6615633964538574
I0229 09:43:41.224047 139589708666624 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7533259391784668, loss=1.5534577369689941
I0229 09:44:15.218450 139589717059328 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.9426748752593994, loss=1.6754697561264038
I0229 09:44:49.175255 139589708666624 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.18172025680542, loss=1.599888801574707
I0229 09:45:23.124747 139589717059328 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.1166598796844482, loss=1.649649977684021
I0229 09:45:57.078979 139589708666624 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.817089557647705, loss=1.5847629308700562
I0229 09:46:31.014965 139589717059328 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.054236650466919, loss=1.6426348686218262
I0229 09:46:45.391351 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:46:52.207859 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:47:06.108095 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:47:08.306410 139753105983296 submission_runner.py:411] Time since start: 16559.31s, 	Step: 46544, 	{'train/accuracy': 0.7046595811843872, 'train/loss': 1.1451612710952759, 'validation/accuracy': 0.6420800089836121, 'validation/loss': 1.48219633102417, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.218715190887451, 'test/num_examples': 10000, 'score': 15864.367563009262, 'total_duration': 16559.309529066086, 'accumulated_submission_time': 15864.367563009262, 'accumulated_eval_time': 692.1000168323517, 'accumulated_logging_time': 1.25059175491333}
I0229 09:47:08.330155 139590186821376 logging_writer.py:48] [46544] accumulated_eval_time=692.100017, accumulated_logging_time=1.250592, accumulated_submission_time=15864.367563, global_step=46544, preemption_count=0, score=15864.367563, test/accuracy=0.509200, test/loss=2.218715, test/num_examples=10000, total_duration=16559.309529, train/accuracy=0.704660, train/loss=1.145161, validation/accuracy=0.642080, validation/loss=1.482196, validation/num_examples=50000
I0229 09:47:27.658942 139590195214080 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7243621349334717, loss=1.6209332942962646
I0229 09:48:01.563408 139590186821376 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7550493478775024, loss=1.6115493774414062
I0229 09:48:35.480461 139590195214080 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.1912789344787598, loss=1.6605700254440308
I0229 09:49:09.427947 139590186821376 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.838915467262268, loss=1.6764403581619263
I0229 09:49:43.408292 139590195214080 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.7927011251449585, loss=1.5317763090133667
I0229 09:50:17.325807 139590186821376 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.686957836151123, loss=1.64884614944458
I0229 09:50:51.271746 139590195214080 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.8937050104141235, loss=1.4774324893951416
I0229 09:51:25.226716 139590186821376 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.016493797302246, loss=1.512620210647583
I0229 09:51:59.149646 139590195214080 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.0025582313537598, loss=1.667978286743164
I0229 09:52:33.090301 139590186821376 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.8520734310150146, loss=1.628528356552124
I0229 09:53:07.039103 139590195214080 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.8727517127990723, loss=1.7152109146118164
I0229 09:53:40.984834 139590186821376 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7333428859710693, loss=1.561711311340332
I0229 09:54:14.928392 139590195214080 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.708577275276184, loss=1.5860334634780884
I0229 09:54:48.868876 139590186821376 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7538188695907593, loss=1.5369764566421509
I0229 09:55:22.813947 139590195214080 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.879963994026184, loss=1.5237135887145996
I0229 09:55:38.528037 139753105983296 spec.py:321] Evaluating on the training split.
I0229 09:55:45.264107 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 09:55:58.968569 139753105983296 spec.py:349] Evaluating on the test split.
I0229 09:56:01.117023 139753105983296 submission_runner.py:411] Time since start: 17092.12s, 	Step: 48048, 	{'train/accuracy': 0.7367067933082581, 'train/loss': 0.9979020357131958, 'validation/accuracy': 0.6473199725151062, 'validation/loss': 1.4513506889343262, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.187633752822876, 'test/num_examples': 10000, 'score': 16374.504691123962, 'total_duration': 17092.12025952339, 'accumulated_submission_time': 16374.504691123962, 'accumulated_eval_time': 714.6889681816101, 'accumulated_logging_time': 1.2839748859405518}
I0229 09:56:01.139096 139589717059328 logging_writer.py:48] [48048] accumulated_eval_time=714.688968, accumulated_logging_time=1.283975, accumulated_submission_time=16374.504691, global_step=48048, preemption_count=0, score=16374.504691, test/accuracy=0.521700, test/loss=2.187634, test/num_examples=10000, total_duration=17092.120260, train/accuracy=0.736707, train/loss=0.997902, validation/accuracy=0.647320, validation/loss=1.451351, validation/num_examples=50000
I0229 09:56:19.159883 139589725452032 logging_writer.py:48] [48100] global_step=48100, grad_norm=2.0339205265045166, loss=1.5578263998031616
I0229 09:56:53.115500 139589717059328 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7543071508407593, loss=1.671058177947998
I0229 09:57:27.103818 139589725452032 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.9048969745635986, loss=1.5980671644210815
I0229 09:58:01.072591 139589717059328 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.8360453844070435, loss=1.6922054290771484
I0229 09:58:35.047198 139589725452032 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.0520179271698, loss=1.7467865943908691
I0229 09:59:09.006411 139589717059328 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8480874300003052, loss=1.4778650999069214
I0229 09:59:42.942332 139589725452032 logging_writer.py:48] [48700] global_step=48700, grad_norm=2.047790288925171, loss=1.6022753715515137
I0229 10:00:16.918870 139589717059328 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.702527403831482, loss=1.5714471340179443
I0229 10:00:50.873878 139589725452032 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7436950206756592, loss=1.6670942306518555
I0229 10:01:24.861611 139589717059328 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.8117250204086304, loss=1.679884672164917
I0229 10:01:58.836314 139589725452032 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.000351905822754, loss=1.6282188892364502
I0229 10:02:32.808716 139589717059328 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.8829001188278198, loss=1.6377720832824707
I0229 10:03:06.822450 139589725452032 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.7868541479110718, loss=1.5773849487304688
I0229 10:03:40.788910 139589717059328 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7714492082595825, loss=1.560898780822754
I0229 10:04:14.704806 139589725452032 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8601282835006714, loss=1.5925766229629517
I0229 10:04:31.457467 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:04:37.992960 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:04:51.948391 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:04:54.098706 139753105983296 submission_runner.py:411] Time since start: 17625.10s, 	Step: 49551, 	{'train/accuracy': 0.7074099183082581, 'train/loss': 1.1301013231277466, 'validation/accuracy': 0.6358799934387207, 'validation/loss': 1.5119726657867432, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.310176134109497, 'test/num_examples': 10000, 'score': 16884.7590508461, 'total_duration': 17625.10194325447, 'accumulated_submission_time': 16884.7590508461, 'accumulated_eval_time': 737.3301610946655, 'accumulated_logging_time': 1.3194658756256104}
I0229 10:04:54.124032 139590170035968 logging_writer.py:48] [49551] accumulated_eval_time=737.330161, accumulated_logging_time=1.319466, accumulated_submission_time=16884.759051, global_step=49551, preemption_count=0, score=16884.759051, test/accuracy=0.503100, test/loss=2.310176, test/num_examples=10000, total_duration=17625.101943, train/accuracy=0.707410, train/loss=1.130101, validation/accuracy=0.635880, validation/loss=1.511973, validation/num_examples=50000
I0229 10:05:11.084466 139590178428672 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.9434797763824463, loss=1.5100419521331787
I0229 10:05:44.962604 139590170035968 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.978325605392456, loss=1.6413440704345703
I0229 10:06:18.931798 139590178428672 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8354891538619995, loss=1.6624743938446045
I0229 10:06:52.902281 139590170035968 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9134467840194702, loss=1.5716724395751953
I0229 10:07:26.888008 139590178428672 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.8481069803237915, loss=1.6218019723892212
I0229 10:08:00.861840 139590170035968 logging_writer.py:48] [50100] global_step=50100, grad_norm=2.1589131355285645, loss=1.780334711074829
I0229 10:08:34.841727 139590178428672 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.893219232559204, loss=1.6041456460952759
I0229 10:09:08.866044 139590170035968 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.9951057434082031, loss=1.6409060955047607
I0229 10:09:42.806237 139590178428672 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.8058146238327026, loss=1.5061415433883667
I0229 10:10:16.762170 139590170035968 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.9804476499557495, loss=1.7523353099822998
I0229 10:10:50.729564 139590178428672 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8844480514526367, loss=1.636717677116394
I0229 10:11:24.665920 139590170035968 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.7045830488204956, loss=1.5543748140335083
I0229 10:11:58.631454 139590178428672 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.7144752740859985, loss=1.6262450218200684
I0229 10:12:32.604949 139590170035968 logging_writer.py:48] [50900] global_step=50900, grad_norm=2.0199031829833984, loss=1.7834558486938477
I0229 10:13:06.542049 139590178428672 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.9010424613952637, loss=1.7130030393600464
I0229 10:13:24.327316 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:13:30.829152 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:13:43.211829 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:13:45.319863 139753105983296 submission_runner.py:411] Time since start: 18156.32s, 	Step: 51054, 	{'train/accuracy': 0.7250877022743225, 'train/loss': 1.0787639617919922, 'validation/accuracy': 0.6547200083732605, 'validation/loss': 1.407345175743103, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.135467767715454, 'test/num_examples': 10000, 'score': 17394.90104007721, 'total_duration': 18156.323054790497, 'accumulated_submission_time': 17394.90104007721, 'accumulated_eval_time': 758.3226172924042, 'accumulated_logging_time': 1.3547601699829102}
I0229 10:13:45.345421 139589717059328 logging_writer.py:48] [51054] accumulated_eval_time=758.322617, accumulated_logging_time=1.354760, accumulated_submission_time=17394.901040, global_step=51054, preemption_count=0, score=17394.901040, test/accuracy=0.527400, test/loss=2.135468, test/num_examples=10000, total_duration=18156.323055, train/accuracy=0.725088, train/loss=1.078764, validation/accuracy=0.654720, validation/loss=1.407345, validation/num_examples=50000
I0229 10:14:01.285682 139589725452032 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.0005578994750977, loss=1.6292978525161743
I0229 10:14:35.214018 139589717059328 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.9001344442367554, loss=1.60846745967865
I0229 10:15:09.153833 139589725452032 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.7739394903182983, loss=1.6206331253051758
I0229 10:15:43.171885 139589717059328 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.8420547246932983, loss=1.5099174976348877
I0229 10:16:17.145961 139589725452032 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.802862286567688, loss=1.6308331489562988
I0229 10:16:51.101714 139589717059328 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.075396776199341, loss=1.6891183853149414
I0229 10:17:25.060501 139589725452032 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.7640094757080078, loss=1.5725531578063965
I0229 10:17:59.018848 139589717059328 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8664515018463135, loss=1.5093063116073608
I0229 10:18:32.974258 139589725452032 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.8740568161010742, loss=1.6133521795272827
I0229 10:19:06.932906 139589717059328 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.915500521659851, loss=1.6660667657852173
I0229 10:19:40.902597 139589725452032 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8854374885559082, loss=1.4870468378067017
I0229 10:20:14.842479 139589717059328 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.901473045349121, loss=1.578925609588623
I0229 10:20:48.827760 139589725452032 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.8417829275131226, loss=1.566478967666626
I0229 10:21:22.846008 139589717059328 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.9368338584899902, loss=1.5039856433868408
I0229 10:21:56.840348 139589725452032 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8719857931137085, loss=1.6126188039779663
I0229 10:22:15.612646 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:22:22.069068 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:22:35.569386 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:22:37.749693 139753105983296 submission_runner.py:411] Time since start: 18688.75s, 	Step: 52557, 	{'train/accuracy': 0.7002750039100647, 'train/loss': 1.1716082096099854, 'validation/accuracy': 0.6354599595069885, 'validation/loss': 1.5122188329696655, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2743141651153564, 'test/num_examples': 10000, 'score': 17905.1085562706, 'total_duration': 18688.75293493271, 'accumulated_submission_time': 17905.1085562706, 'accumulated_eval_time': 780.4596221446991, 'accumulated_logging_time': 1.3891429901123047}
I0229 10:22:37.773157 139590170035968 logging_writer.py:48] [52557] accumulated_eval_time=780.459622, accumulated_logging_time=1.389143, accumulated_submission_time=17905.108556, global_step=52557, preemption_count=0, score=17905.108556, test/accuracy=0.510500, test/loss=2.274314, test/num_examples=10000, total_duration=18688.752935, train/accuracy=0.700275, train/loss=1.171608, validation/accuracy=0.635460, validation/loss=1.512219, validation/num_examples=50000
I0229 10:22:52.701054 139590178428672 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9215515851974487, loss=1.579938292503357
I0229 10:23:26.597723 139590170035968 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.9962724447250366, loss=1.6198161840438843
I0229 10:24:00.526939 139590178428672 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.68662428855896, loss=1.5161924362182617
I0229 10:24:34.484891 139590170035968 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8737519979476929, loss=1.5956084728240967
I0229 10:25:08.412037 139590178428672 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9090341329574585, loss=1.6203444004058838
I0229 10:25:42.339881 139590170035968 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.7916361093521118, loss=1.4867960214614868
I0229 10:26:16.295218 139590178428672 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.72617769241333, loss=1.5212461948394775
I0229 10:26:50.219760 139590170035968 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.1416544914245605, loss=1.7432570457458496
I0229 10:27:24.152791 139590178428672 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.7384490966796875, loss=1.6169254779815674
I0229 10:27:58.162753 139590170035968 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.0236802101135254, loss=1.613899827003479
I0229 10:28:32.107311 139590178428672 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8896045684814453, loss=1.6110942363739014
I0229 10:29:06.038596 139590170035968 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.797484278678894, loss=1.639713168144226
I0229 10:29:39.969177 139590178428672 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.6437817811965942, loss=1.4593902826309204
I0229 10:30:13.919921 139590170035968 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.016968250274658, loss=1.654709815979004
I0229 10:30:47.861974 139590178428672 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8694456815719604, loss=1.4665236473083496
I0229 10:31:07.980959 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:31:14.352033 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:31:26.888496 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:31:29.112679 139753105983296 submission_runner.py:411] Time since start: 19220.12s, 	Step: 54061, 	{'train/accuracy': 0.704500138759613, 'train/loss': 1.1456981897354126, 'validation/accuracy': 0.6450799703598022, 'validation/loss': 1.4624508619308472, 'validation/num_examples': 50000, 'test/accuracy': 0.5192000269889832, 'test/loss': 2.1851463317871094, 'test/num_examples': 10000, 'score': 18415.254390001297, 'total_duration': 19220.11591911316, 'accumulated_submission_time': 18415.254390001297, 'accumulated_eval_time': 801.5913171768188, 'accumulated_logging_time': 1.4218056201934814}
I0229 10:31:29.137207 139590144857856 logging_writer.py:48] [54061] accumulated_eval_time=801.591317, accumulated_logging_time=1.421806, accumulated_submission_time=18415.254390, global_step=54061, preemption_count=0, score=18415.254390, test/accuracy=0.519200, test/loss=2.185146, test/num_examples=10000, total_duration=19220.115919, train/accuracy=0.704500, train/loss=1.145698, validation/accuracy=0.645080, validation/loss=1.462451, validation/num_examples=50000
I0229 10:31:42.712592 139590153250560 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.17375111579895, loss=1.6591594219207764
I0229 10:32:16.593398 139590144857856 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8972066640853882, loss=1.5155589580535889
I0229 10:32:50.533771 139590153250560 logging_writer.py:48] [54300] global_step=54300, grad_norm=2.046684503555298, loss=1.58305025100708
I0229 10:33:24.526127 139590144857856 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.7431280612945557, loss=1.5298759937286377
I0229 10:33:58.560830 139590153250560 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9524011611938477, loss=1.537712574005127
I0229 10:34:32.504810 139590144857856 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.8995918035507202, loss=1.5762531757354736
I0229 10:35:06.464347 139590153250560 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.120206832885742, loss=1.6337000131607056
I0229 10:35:40.440140 139590144857856 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.9961223602294922, loss=1.6394442319869995
I0229 10:36:14.360986 139590153250560 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.926513910293579, loss=1.6461718082427979
I0229 10:36:48.303807 139590144857856 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.8770655393600464, loss=1.6818346977233887
I0229 10:37:22.258905 139590153250560 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.911943793296814, loss=1.6108542680740356
I0229 10:37:56.205911 139590144857856 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.7806189060211182, loss=1.4998326301574707
I0229 10:38:30.145257 139590153250560 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.0535356998443604, loss=1.6272433996200562
I0229 10:39:04.126721 139590144857856 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8566489219665527, loss=1.5437177419662476
I0229 10:39:38.091275 139590153250560 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.9394010305404663, loss=1.5955893993377686
I0229 10:39:59.391366 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:40:05.767063 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:40:17.094376 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:40:19.425891 139753105983296 submission_runner.py:411] Time since start: 19750.43s, 	Step: 55564, 	{'train/accuracy': 0.7084661722183228, 'train/loss': 1.1322740316390991, 'validation/accuracy': 0.649679958820343, 'validation/loss': 1.445713996887207, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1526975631713867, 'test/num_examples': 10000, 'score': 18925.44615507126, 'total_duration': 19750.42909526825, 'accumulated_submission_time': 18925.44615507126, 'accumulated_eval_time': 821.6257679462433, 'accumulated_logging_time': 1.4563281536102295}
I0229 10:40:19.452213 139589725452032 logging_writer.py:48] [55564] accumulated_eval_time=821.625768, accumulated_logging_time=1.456328, accumulated_submission_time=18925.446155, global_step=55564, preemption_count=0, score=18925.446155, test/accuracy=0.522500, test/loss=2.152698, test/num_examples=10000, total_duration=19750.429095, train/accuracy=0.708466, train/loss=1.132274, validation/accuracy=0.649680, validation/loss=1.445714, validation/num_examples=50000
I0229 10:40:32.000483 139590144857856 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.157296657562256, loss=1.6286518573760986
I0229 10:41:05.876415 139589725452032 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.8714263439178467, loss=1.6100666522979736
I0229 10:41:39.790859 139590144857856 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.6850875616073608, loss=1.5713279247283936
I0229 10:42:13.735867 139589725452032 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.8148608207702637, loss=1.5837956666946411
I0229 10:42:47.664812 139590144857856 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.0211267471313477, loss=1.6130648851394653
I0229 10:43:21.589478 139589725452032 logging_writer.py:48] [56100] global_step=56100, grad_norm=2.1790249347686768, loss=1.6527824401855469
I0229 10:43:55.540284 139590144857856 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8542499542236328, loss=1.4672025442123413
I0229 10:44:29.481707 139589725452032 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.9098427295684814, loss=1.536042332649231
I0229 10:45:03.415359 139590144857856 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.810597538948059, loss=1.5349316596984863
I0229 10:45:37.321894 139589725452032 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.87862229347229, loss=1.6150963306427002
I0229 10:46:11.411109 139590144857856 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9992583990097046, loss=1.479659914970398
I0229 10:46:45.360630 139589725452032 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.969660758972168, loss=1.7151967287063599
I0229 10:47:19.299662 139590144857856 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9462391138076782, loss=1.668809413909912
I0229 10:47:53.233942 139589725452032 logging_writer.py:48] [56900] global_step=56900, grad_norm=2.3454036712646484, loss=1.4829691648483276
I0229 10:48:27.195452 139590144857856 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.134361743927002, loss=1.6419804096221924
I0229 10:48:49.713388 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:48:55.916097 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:49:04.681308 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:49:06.977772 139753105983296 submission_runner.py:411] Time since start: 20277.98s, 	Step: 57068, 	{'train/accuracy': 0.7385801672935486, 'train/loss': 1.0057047605514526, 'validation/accuracy': 0.6440799832344055, 'validation/loss': 1.4641975164413452, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.2163994312286377, 'test/num_examples': 10000, 'score': 19435.642174959183, 'total_duration': 20277.98100042343, 'accumulated_submission_time': 19435.642174959183, 'accumulated_eval_time': 838.8901033401489, 'accumulated_logging_time': 1.4961392879486084}
I0229 10:49:07.009070 139589708666624 logging_writer.py:48] [57068] accumulated_eval_time=838.890103, accumulated_logging_time=1.496139, accumulated_submission_time=19435.642175, global_step=57068, preemption_count=0, score=19435.642175, test/accuracy=0.508000, test/loss=2.216399, test/num_examples=10000, total_duration=20277.981000, train/accuracy=0.738580, train/loss=1.005705, validation/accuracy=0.644080, validation/loss=1.464198, validation/num_examples=50000
I0229 10:49:18.208283 139590161643264 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8451145887374878, loss=1.585118293762207
I0229 10:49:52.120968 139589708666624 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.8469412326812744, loss=1.6942070722579956
I0229 10:50:26.071868 139590161643264 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.901918888092041, loss=1.6458780765533447
I0229 10:51:00.031916 139589708666624 logging_writer.py:48] [57400] global_step=57400, grad_norm=2.050605058670044, loss=1.6075905561447144
I0229 10:51:33.970937 139590161643264 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.9897464513778687, loss=1.6490788459777832
I0229 10:52:07.922621 139589708666624 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9069104194641113, loss=1.547394037246704
I0229 10:52:41.964843 139590161643264 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9745312929153442, loss=1.6484239101409912
I0229 10:53:15.921032 139589708666624 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.889043927192688, loss=1.5919826030731201
I0229 10:53:49.877990 139590161643264 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.967323899269104, loss=1.5196642875671387
I0229 10:54:23.834183 139589708666624 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.0689167976379395, loss=1.5934088230133057
I0229 10:54:57.816547 139590161643264 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.7569799423217773, loss=1.499263048171997
I0229 10:55:31.751820 139589708666624 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.2283823490142822, loss=1.5777589082717896
I0229 10:56:05.736989 139590161643264 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.9059282541275024, loss=1.5904786586761475
I0229 10:56:39.704604 139589708666624 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.0311176776885986, loss=1.6360176801681519
I0229 10:57:13.662882 139590161643264 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9294943809509277, loss=1.5191477537155151
I0229 10:57:37.230130 139753105983296 spec.py:321] Evaluating on the training split.
I0229 10:57:43.499981 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 10:57:52.151606 139753105983296 spec.py:349] Evaluating on the test split.
I0229 10:57:54.471834 139753105983296 submission_runner.py:411] Time since start: 20805.48s, 	Step: 58571, 	{'train/accuracy': 0.7267019748687744, 'train/loss': 1.0357704162597656, 'validation/accuracy': 0.646619975566864, 'validation/loss': 1.45211923122406, 'validation/num_examples': 50000, 'test/accuracy': 0.522100031375885, 'test/loss': 2.1788055896759033, 'test/num_examples': 10000, 'score': 19945.802860736847, 'total_duration': 20805.475059747696, 'accumulated_submission_time': 19945.802860736847, 'accumulated_eval_time': 856.131756067276, 'accumulated_logging_time': 1.5372190475463867}
I0229 10:57:54.505531 139590144857856 logging_writer.py:48] [58571] accumulated_eval_time=856.131756, accumulated_logging_time=1.537219, accumulated_submission_time=19945.802861, global_step=58571, preemption_count=0, score=19945.802861, test/accuracy=0.522100, test/loss=2.178806, test/num_examples=10000, total_duration=20805.475060, train/accuracy=0.726702, train/loss=1.035770, validation/accuracy=0.646620, validation/loss=1.452119, validation/num_examples=50000
I0229 10:58:04.667461 139590153250560 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9784307479858398, loss=1.5098249912261963
I0229 10:58:38.673086 139590144857856 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.0514161586761475, loss=1.531502366065979
I0229 10:59:12.596747 139590153250560 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.0127758979797363, loss=1.623828649520874
I0229 10:59:46.548577 139590144857856 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.9400403499603271, loss=1.6072568893432617
I0229 11:00:20.477480 139590153250560 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.8581434488296509, loss=1.5175490379333496
I0229 11:00:54.427378 139590144857856 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.871436357498169, loss=1.6570520401000977
I0229 11:01:28.347892 139590153250560 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.0856966972351074, loss=1.5051782131195068
I0229 11:02:02.307145 139590144857856 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8839797973632812, loss=1.6024399995803833
I0229 11:02:36.243468 139590153250560 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.9811781644821167, loss=1.5757005214691162
I0229 11:03:10.183399 139590144857856 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.2339320182800293, loss=1.532041072845459
I0229 11:03:44.125227 139590153250560 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.006800413131714, loss=1.5126346349716187
I0229 11:04:18.055903 139590144857856 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.082864761352539, loss=1.560956597328186
I0229 11:04:52.041974 139590153250560 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.0272114276885986, loss=1.4949951171875
I0229 11:05:25.997607 139590144857856 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.7844592332839966, loss=1.5717989206314087
I0229 11:05:59.937467 139590153250560 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.9447585344314575, loss=1.4644783735275269
I0229 11:06:24.503563 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:06:30.750193 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:06:39.462682 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:06:41.737481 139753105983296 submission_runner.py:411] Time since start: 21332.74s, 	Step: 60074, 	{'train/accuracy': 0.7245694994926453, 'train/loss': 1.055361032485962, 'validation/accuracy': 0.6547600030899048, 'validation/loss': 1.4124293327331543, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.112003803253174, 'test/num_examples': 10000, 'score': 20455.73877811432, 'total_duration': 21332.740632534027, 'accumulated_submission_time': 20455.73877811432, 'accumulated_eval_time': 873.3655483722687, 'accumulated_logging_time': 1.5809953212738037}
I0229 11:06:41.772526 139589717059328 logging_writer.py:48] [60074] accumulated_eval_time=873.365548, accumulated_logging_time=1.580995, accumulated_submission_time=20455.738778, global_step=60074, preemption_count=0, score=20455.738778, test/accuracy=0.526300, test/loss=2.112004, test/num_examples=10000, total_duration=21332.740633, train/accuracy=0.724569, train/loss=1.055361, validation/accuracy=0.654760, validation/loss=1.412429, validation/num_examples=50000
I0229 11:06:50.923302 139589725452032 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.9056084156036377, loss=1.530175805091858
I0229 11:07:24.827368 139589717059328 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.032822847366333, loss=1.5804561376571655
I0229 11:07:58.780030 139589725452032 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.821578860282898, loss=1.5689640045166016
I0229 11:08:32.727074 139589717059328 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.0106990337371826, loss=1.4741289615631104
I0229 11:09:06.685942 139589725452032 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8099993467330933, loss=1.640377402305603
I0229 11:09:40.642013 139589717059328 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.0556185245513916, loss=1.4607166051864624
I0229 11:10:14.590273 139589725452032 logging_writer.py:48] [60700] global_step=60700, grad_norm=2.3890938758850098, loss=1.6154358386993408
I0229 11:10:48.630299 139589717059328 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.939677357673645, loss=1.53004789352417
I0229 11:11:22.542272 139589725452032 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.037804126739502, loss=1.676940679550171
I0229 11:11:56.494852 139589717059328 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8215563297271729, loss=1.5801860094070435
I0229 11:12:30.443274 139589725452032 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.1449460983276367, loss=1.5388025045394897
I0229 11:13:04.403861 139589717059328 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.0354385375976562, loss=1.65557062625885
I0229 11:13:38.378753 139589725452032 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.150381565093994, loss=1.589953899383545
I0229 11:14:12.332513 139589717059328 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8533741235733032, loss=1.503702163696289
I0229 11:14:46.282146 139589725452032 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.088155508041382, loss=1.7029348611831665
I0229 11:15:11.868062 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:15:18.051105 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:15:26.799675 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:15:29.099029 139753105983296 submission_runner.py:411] Time since start: 21860.10s, 	Step: 61577, 	{'train/accuracy': 0.7218191623687744, 'train/loss': 1.0774519443511963, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.4308249950408936, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.1299495697021484, 'test/num_examples': 10000, 'score': 20965.771684646606, 'total_duration': 21860.102259159088, 'accumulated_submission_time': 20965.771684646606, 'accumulated_eval_time': 890.5964822769165, 'accumulated_logging_time': 1.6265153884887695}
I0229 11:15:29.128784 139589717059328 logging_writer.py:48] [61577] accumulated_eval_time=890.596482, accumulated_logging_time=1.626515, accumulated_submission_time=20965.771685, global_step=61577, preemption_count=0, score=20965.771685, test/accuracy=0.526900, test/loss=2.129950, test/num_examples=10000, total_duration=21860.102259, train/accuracy=0.721819, train/loss=1.077452, validation/accuracy=0.650560, validation/loss=1.430825, validation/num_examples=50000
I0229 11:15:37.261279 139590153250560 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.9097262620925903, loss=1.5264564752578735
I0229 11:16:11.212633 139589717059328 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.911447525024414, loss=1.5287678241729736
I0229 11:16:45.156546 139590153250560 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9871772527694702, loss=1.5841785669326782
I0229 11:17:19.122489 139589717059328 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.017737627029419, loss=1.5820610523223877
I0229 11:17:53.079653 139590153250560 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.8043277263641357, loss=1.4629071950912476
I0229 11:18:27.013490 139589717059328 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.1473920345306396, loss=1.5594942569732666
I0229 11:19:00.943012 139590153250560 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.148669481277466, loss=1.4576716423034668
I0229 11:19:34.874625 139589717059328 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.9235965013504028, loss=1.4634811878204346
I0229 11:20:08.800926 139590153250560 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.151075601577759, loss=1.6190462112426758
I0229 11:20:42.726260 139589717059328 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9646862745285034, loss=1.4925917387008667
I0229 11:21:16.665670 139590153250560 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.0896058082580566, loss=1.6871912479400635
I0229 11:21:50.610398 139589717059328 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.7207661867141724, loss=1.5619151592254639
I0229 11:22:24.568372 139590153250560 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.964038610458374, loss=1.5492584705352783
I0229 11:22:58.457715 139589717059328 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.9744857549667358, loss=1.505895733833313
I0229 11:23:32.458986 139590153250560 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9587137699127197, loss=1.5737279653549194
I0229 11:23:59.377858 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:24:05.610017 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:24:14.043012 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:24:16.287682 139753105983296 submission_runner.py:411] Time since start: 22387.29s, 	Step: 63081, 	{'train/accuracy': 0.7154416441917419, 'train/loss': 1.1013087034225464, 'validation/accuracy': 0.6513800024986267, 'validation/loss': 1.4529109001159668, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.185504913330078, 'test/num_examples': 10000, 'score': 21475.956223487854, 'total_duration': 22387.29090833664, 'accumulated_submission_time': 21475.956223487854, 'accumulated_eval_time': 907.5062689781189, 'accumulated_logging_time': 1.6687884330749512}
I0229 11:24:16.314530 139589717059328 logging_writer.py:48] [63081] accumulated_eval_time=907.506269, accumulated_logging_time=1.668788, accumulated_submission_time=21475.956223, global_step=63081, preemption_count=0, score=21475.956223, test/accuracy=0.521300, test/loss=2.185505, test/num_examples=10000, total_duration=22387.290908, train/accuracy=0.715442, train/loss=1.101309, validation/accuracy=0.651380, validation/loss=1.452911, validation/num_examples=50000
I0229 11:24:23.111380 139589725452032 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.8713397979736328, loss=1.5469907522201538
I0229 11:24:56.988355 139589717059328 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.072061777114868, loss=1.581416368484497
I0229 11:25:30.915186 139589725452032 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.421891212463379, loss=1.589678168296814
I0229 11:26:04.863871 139589717059328 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.0257351398468018, loss=1.4842174053192139
I0229 11:26:38.797837 139589725452032 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.001302719116211, loss=1.5216641426086426
I0229 11:27:12.732164 139589717059328 logging_writer.py:48] [63600] global_step=63600, grad_norm=2.0022950172424316, loss=1.5134549140930176
I0229 11:27:46.672068 139589725452032 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.9130115509033203, loss=1.5592780113220215
I0229 11:28:20.601219 139589717059328 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.110712766647339, loss=1.5425941944122314
I0229 11:28:54.546510 139589725452032 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.977459192276001, loss=1.5989248752593994
I0229 11:29:28.640753 139589717059328 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.9850763082504272, loss=1.5255684852600098
I0229 11:30:02.613702 139589725452032 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.9194198846817017, loss=1.4523123502731323
I0229 11:30:36.559339 139589717059328 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.866007685661316, loss=1.5243000984191895
I0229 11:31:10.528350 139589725452032 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.997139573097229, loss=1.4850351810455322
I0229 11:31:44.454291 139589717059328 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8903539180755615, loss=1.5065687894821167
I0229 11:32:18.393330 139589725452032 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.0655102729797363, loss=1.5989048480987549
I0229 11:32:46.362076 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:32:52.443576 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:33:00.852879 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:33:03.149029 139753105983296 submission_runner.py:411] Time since start: 22914.15s, 	Step: 64584, 	{'train/accuracy': 0.7234733700752258, 'train/loss': 1.0683307647705078, 'validation/accuracy': 0.6577000021934509, 'validation/loss': 1.4033039808273315, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.1605608463287354, 'test/num_examples': 10000, 'score': 21985.941433429718, 'total_duration': 22914.152262449265, 'accumulated_submission_time': 21985.941433429718, 'accumulated_eval_time': 924.2931768894196, 'accumulated_logging_time': 1.7055230140686035}
I0229 11:33:03.179118 139589717059328 logging_writer.py:48] [64584] accumulated_eval_time=924.293177, accumulated_logging_time=1.705523, accumulated_submission_time=21985.941433, global_step=64584, preemption_count=0, score=21985.941433, test/accuracy=0.528600, test/loss=2.160561, test/num_examples=10000, total_duration=22914.152262, train/accuracy=0.723473, train/loss=1.068331, validation/accuracy=0.657700, validation/loss=1.403304, validation/num_examples=50000
I0229 11:33:08.946322 139589725452032 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.1775131225585938, loss=1.5256963968276978
I0229 11:33:42.835515 139589717059328 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9890081882476807, loss=1.519282341003418
I0229 11:34:16.760836 139589725452032 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.8892399072647095, loss=1.5591566562652588
I0229 11:34:50.691794 139589717059328 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.2229557037353516, loss=1.5856289863586426
I0229 11:35:24.773089 139589725452032 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.9607871770858765, loss=1.5528669357299805
I0229 11:35:58.710230 139589717059328 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.144832134246826, loss=1.5456323623657227
I0229 11:36:32.665283 139589725452032 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8933682441711426, loss=1.6114037036895752
I0229 11:37:06.597382 139589717059328 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9039376974105835, loss=1.4430676698684692
I0229 11:37:40.548700 139589725452032 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9014217853546143, loss=1.45607590675354
I0229 11:38:14.512210 139589717059328 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.140929698944092, loss=1.514458417892456
I0229 11:38:48.452876 139589725452032 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.237123966217041, loss=1.6688718795776367
I0229 11:39:22.363734 139589717059328 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.1475043296813965, loss=1.4470620155334473
I0229 11:39:56.323156 139589725452032 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.138462543487549, loss=1.584532380104065
I0229 11:40:30.249902 139589717059328 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.040085792541504, loss=1.4867026805877686
I0229 11:41:04.198334 139589725452032 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.214658498764038, loss=1.5581302642822266
I0229 11:41:33.276342 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:41:39.420326 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:41:47.890826 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:41:50.090334 139753105983296 submission_runner.py:411] Time since start: 23441.09s, 	Step: 66087, 	{'train/accuracy': 0.7430046200752258, 'train/loss': 0.9785423874855042, 'validation/accuracy': 0.6419000029563904, 'validation/loss': 1.4796708822250366, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2493345737457275, 'test/num_examples': 10000, 'score': 22495.97627878189, 'total_duration': 23441.093533992767, 'accumulated_submission_time': 22495.97627878189, 'accumulated_eval_time': 941.1070840358734, 'accumulated_logging_time': 1.7455470561981201}
I0229 11:41:50.122292 139589717059328 logging_writer.py:48] [66087] accumulated_eval_time=941.107084, accumulated_logging_time=1.745547, accumulated_submission_time=22495.976279, global_step=66087, preemption_count=0, score=22495.976279, test/accuracy=0.505900, test/loss=2.249335, test/num_examples=10000, total_duration=23441.093534, train/accuracy=0.743005, train/loss=0.978542, validation/accuracy=0.641900, validation/loss=1.479671, validation/num_examples=50000
I0229 11:41:54.874350 139590144857856 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9857358932495117, loss=1.6178481578826904
I0229 11:42:28.756423 139589717059328 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.010377883911133, loss=1.445326566696167
I0229 11:43:02.728084 139590144857856 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.2725186347961426, loss=1.642730474472046
I0229 11:43:36.693112 139589717059328 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.137619972229004, loss=1.4079285860061646
I0229 11:44:10.667252 139590144857856 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.044611692428589, loss=1.524580478668213
I0229 11:44:44.627998 139589717059328 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.9832452535629272, loss=1.6162773370742798
I0229 11:45:18.583909 139590144857856 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.8065555095672607, loss=1.4961214065551758
I0229 11:45:52.554035 139589717059328 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.048112154006958, loss=1.5147287845611572
I0229 11:46:26.494241 139590144857856 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.2141385078430176, loss=1.4909684658050537
I0229 11:47:00.469053 139589717059328 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.937351107597351, loss=1.431351900100708
I0229 11:47:34.504557 139590144857856 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.9092189073562622, loss=1.4850040674209595
I0229 11:48:08.454434 139589717059328 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.0852627754211426, loss=1.614473581314087
I0229 11:48:42.389405 139590144857856 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.061490297317505, loss=1.4655648469924927
I0229 11:49:16.333051 139589717059328 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.241908311843872, loss=1.51679527759552
I0229 11:49:50.301748 139590144857856 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.9903100728988647, loss=1.529077172279358
I0229 11:50:20.319119 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:50:26.519772 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:50:34.834335 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:50:37.105053 139753105983296 submission_runner.py:411] Time since start: 23968.11s, 	Step: 67590, 	{'train/accuracy': 0.7223373651504517, 'train/loss': 1.0661697387695312, 'validation/accuracy': 0.6440399885177612, 'validation/loss': 1.467403531074524, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.2431344985961914, 'test/num_examples': 10000, 'score': 23006.104821681976, 'total_duration': 23968.108250379562, 'accumulated_submission_time': 23006.104821681976, 'accumulated_eval_time': 957.8929336071014, 'accumulated_logging_time': 1.7923862934112549}
I0229 11:50:37.145956 139590161643264 logging_writer.py:48] [67590] accumulated_eval_time=957.892934, accumulated_logging_time=1.792386, accumulated_submission_time=23006.104822, global_step=67590, preemption_count=0, score=23006.104822, test/accuracy=0.507400, test/loss=2.243134, test/num_examples=10000, total_duration=23968.108250, train/accuracy=0.722337, train/loss=1.066170, validation/accuracy=0.644040, validation/loss=1.467404, validation/num_examples=50000
I0229 11:50:40.873532 139590170035968 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.9235135316848755, loss=1.5557187795639038
I0229 11:51:14.753996 139590161643264 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.319995641708374, loss=1.5099880695343018
I0229 11:51:48.672564 139590170035968 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.1040775775909424, loss=1.564383864402771
I0229 11:52:22.598503 139590161643264 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.9080629348754883, loss=1.447507381439209
I0229 11:52:56.535468 139590170035968 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.9922970533370972, loss=1.5061901807785034
I0229 11:53:30.477857 139590161643264 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.3629491329193115, loss=1.5623266696929932
I0229 11:54:04.466921 139590170035968 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.8563988208770752, loss=1.600021243095398
I0229 11:54:38.416215 139590161643264 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.0756595134735107, loss=1.5488075017929077
I0229 11:55:12.368807 139590170035968 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.043174982070923, loss=1.5731531381607056
I0229 11:55:46.361675 139590161643264 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.11926007270813, loss=1.519928216934204
I0229 11:56:20.315885 139590170035968 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.982427954673767, loss=1.5261482000350952
I0229 11:56:54.255572 139590161643264 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.360672950744629, loss=1.6600581407546997
I0229 11:57:28.199381 139590170035968 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.1915855407714844, loss=1.475829005241394
I0229 11:58:02.135529 139590161643264 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.089179039001465, loss=1.5245260000228882
I0229 11:58:36.085680 139590170035968 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.9689489603042603, loss=1.4734523296356201
I0229 11:59:07.112142 139753105983296 spec.py:321] Evaluating on the training split.
I0229 11:59:13.133894 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 11:59:21.669818 139753105983296 spec.py:349] Evaluating on the test split.
I0229 11:59:23.933265 139753105983296 submission_runner.py:411] Time since start: 24494.94s, 	Step: 69093, 	{'train/accuracy': 0.7189094424247742, 'train/loss': 1.0872693061828613, 'validation/accuracy': 0.6390399932861328, 'validation/loss': 1.490094780921936, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.2381787300109863, 'test/num_examples': 10000, 'score': 23516.008523225784, 'total_duration': 24494.936494350433, 'accumulated_submission_time': 23516.008523225784, 'accumulated_eval_time': 974.7140092849731, 'accumulated_logging_time': 1.8431601524353027}
I0229 11:59:23.962437 139589725452032 logging_writer.py:48] [69093] accumulated_eval_time=974.714009, accumulated_logging_time=1.843160, accumulated_submission_time=23516.008523, global_step=69093, preemption_count=0, score=23516.008523, test/accuracy=0.507800, test/loss=2.238179, test/num_examples=10000, total_duration=24494.936494, train/accuracy=0.718909, train/loss=1.087269, validation/accuracy=0.639040, validation/loss=1.490095, validation/num_examples=50000
I0229 11:59:26.680549 139590144857856 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.074728488922119, loss=1.5350550413131714
I0229 12:00:00.683498 139589725452032 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.0441830158233643, loss=1.5081138610839844
I0229 12:00:34.650040 139590144857856 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.091871500015259, loss=1.5313279628753662
I0229 12:01:08.601793 139589725452032 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.9899682998657227, loss=1.505481243133545
I0229 12:01:42.524485 139590144857856 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.015911817550659, loss=1.4793165922164917
I0229 12:02:16.497901 139589725452032 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.0720999240875244, loss=1.5438971519470215
I0229 12:02:50.430728 139590144857856 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.171196222305298, loss=1.5947471857070923
I0229 12:03:24.358878 139589725452032 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.240280866622925, loss=1.51241135597229
I0229 12:03:58.324395 139590144857856 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.1430742740631104, loss=1.5922685861587524
I0229 12:04:32.274137 139589725452032 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.0766074657440186, loss=1.5030910968780518
I0229 12:05:06.203076 139590144857856 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.080124616622925, loss=1.466068983078003
I0229 12:05:40.180000 139589725452032 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.1453042030334473, loss=1.416203260421753
I0229 12:06:14.239933 139590144857856 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.264327049255371, loss=1.6026452779769897
I0229 12:06:48.191656 139589725452032 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.8871572017669678, loss=1.5179705619812012
I0229 12:07:22.167131 139590144857856 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.8596855401992798, loss=1.4355971813201904
I0229 12:07:54.247438 139753105983296 spec.py:321] Evaluating on the training split.
I0229 12:08:00.302534 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 12:08:08.744588 139753105983296 spec.py:349] Evaluating on the test split.
I0229 12:08:11.023954 139753105983296 submission_runner.py:411] Time since start: 25022.03s, 	Step: 70596, 	{'train/accuracy': 0.7314253449440002, 'train/loss': 1.0208046436309814, 'validation/accuracy': 0.6607199907302856, 'validation/loss': 1.391520619392395, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.1112637519836426, 'test/num_examples': 10000, 'score': 24026.230861902237, 'total_duration': 25022.027183532715, 'accumulated_submission_time': 24026.230861902237, 'accumulated_eval_time': 991.4904737472534, 'accumulated_logging_time': 1.882537841796875}
I0229 12:08:11.056308 139590170035968 logging_writer.py:48] [70596] accumulated_eval_time=991.490474, accumulated_logging_time=1.882538, accumulated_submission_time=24026.230862, global_step=70596, preemption_count=0, score=24026.230862, test/accuracy=0.535700, test/loss=2.111264, test/num_examples=10000, total_duration=25022.027184, train/accuracy=0.731425, train/loss=1.020805, validation/accuracy=0.660720, validation/loss=1.391521, validation/num_examples=50000
I0229 12:08:12.777231 139590178428672 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0942323207855225, loss=1.5144158601760864
I0229 12:08:46.718274 139590170035968 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.9221091270446777, loss=1.4564473628997803
I0229 12:09:20.654266 139590178428672 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.9197317361831665, loss=1.4907011985778809
I0229 12:09:54.565931 139590170035968 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.223487138748169, loss=1.6489695310592651
I0229 12:10:28.521551 139590178428672 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.2080178260803223, loss=1.5073745250701904
I0229 12:11:02.513345 139590170035968 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.893375039100647, loss=1.5280705690383911
I0229 12:11:36.463360 139590178428672 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.0054447650909424, loss=1.5417388677597046
I0229 12:12:10.415676 139590170035968 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.507568120956421, loss=1.5098211765289307
I0229 12:12:44.513837 139590178428672 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.0799262523651123, loss=1.4430795907974243
I0229 12:13:18.449600 139590170035968 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.9545540809631348, loss=1.508446455001831
I0229 12:13:52.388549 139590178428672 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.8232145309448242, loss=1.4351576566696167
I0229 12:14:26.312938 139590170035968 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.8963624238967896, loss=1.5029418468475342
I0229 12:15:00.241289 139590178428672 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.99250328540802, loss=1.5023999214172363
I0229 12:15:34.206683 139590170035968 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.06370210647583, loss=1.5288386344909668
I0229 12:16:08.143785 139590178428672 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.2042694091796875, loss=1.6262978315353394
I0229 12:16:41.240239 139753105983296 spec.py:321] Evaluating on the training split.
I0229 12:16:47.210301 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 12:16:55.718486 139753105983296 spec.py:349] Evaluating on the test split.
I0229 12:16:57.945377 139753105983296 submission_runner.py:411] Time since start: 25548.95s, 	Step: 72099, 	{'train/accuracy': 0.7379224896430969, 'train/loss': 0.9979417324066162, 'validation/accuracy': 0.667639970779419, 'validation/loss': 1.3583799600601196, 'validation/num_examples': 50000, 'test/accuracy': 0.5326000452041626, 'test/loss': 2.116264581680298, 'test/num_examples': 10000, 'score': 24536.351742506027, 'total_duration': 25548.94845175743, 'accumulated_submission_time': 24536.351742506027, 'accumulated_eval_time': 1008.1954228878021, 'accumulated_logging_time': 1.924994707107544}
I0229 12:16:57.978441 139589725452032 logging_writer.py:48] [72099] accumulated_eval_time=1008.195423, accumulated_logging_time=1.924995, accumulated_submission_time=24536.351743, global_step=72099, preemption_count=0, score=24536.351743, test/accuracy=0.532600, test/loss=2.116265, test/num_examples=10000, total_duration=25548.948452, train/accuracy=0.737922, train/loss=0.997942, validation/accuracy=0.667640, validation/loss=1.358380, validation/num_examples=50000
I0229 12:16:58.672894 139590144857856 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.2045159339904785, loss=1.6129810810089111
I0229 12:17:32.574981 139589725452032 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.2411670684814453, loss=1.5921883583068848
I0229 12:18:06.490447 139590144857856 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.0136327743530273, loss=1.5339114665985107
I0229 12:18:40.571260 139589725452032 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.161487102508545, loss=1.5217849016189575
I0229 12:19:14.511494 139590144857856 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.202056646347046, loss=1.5158532857894897
I0229 12:19:48.445021 139589725452032 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9176441431045532, loss=1.5638277530670166
I0229 12:20:22.389818 139590144857856 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.129338264465332, loss=1.4853200912475586
I0229 12:20:56.324842 139589725452032 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.191641092300415, loss=1.533606767654419
I0229 12:21:30.257425 139590144857856 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.0072975158691406, loss=1.3577136993408203
I0229 12:22:04.205817 139589725452032 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.1013548374176025, loss=1.555591106414795
I0229 12:22:38.134881 139590144857856 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.9729517698287964, loss=1.499664306640625
I0229 12:23:12.079560 139589725452032 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.012833595275879, loss=1.491731882095337
I0229 12:23:45.999521 139590144857856 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.080930471420288, loss=1.5033093690872192
I0229 12:24:19.940260 139589725452032 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.0398426055908203, loss=1.538330316543579
I0229 12:24:53.965140 139590144857856 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.255314350128174, loss=1.6088918447494507
I0229 12:25:27.873364 139589725452032 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.0716552734375, loss=1.5260024070739746
I0229 12:25:28.026434 139753105983296 spec.py:321] Evaluating on the training split.
I0229 12:25:34.122413 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 12:25:42.476883 139753105983296 spec.py:349] Evaluating on the test split.
I0229 12:25:44.740933 139753105983296 submission_runner.py:411] Time since start: 26075.74s, 	Step: 73602, 	{'train/accuracy': 0.7327207922935486, 'train/loss': 1.0299476385116577, 'validation/accuracy': 0.6582599878311157, 'validation/loss': 1.3956197500228882, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.101027011871338, 'test/num_examples': 10000, 'score': 25046.33527326584, 'total_duration': 26075.744012355804, 'accumulated_submission_time': 25046.33527326584, 'accumulated_eval_time': 1024.909749507904, 'accumulated_logging_time': 1.9686222076416016}
I0229 12:25:44.773740 139590170035968 logging_writer.py:48] [73602] accumulated_eval_time=1024.909750, accumulated_logging_time=1.968622, accumulated_submission_time=25046.335273, global_step=73602, preemption_count=0, score=25046.335273, test/accuracy=0.534400, test/loss=2.101027, test/num_examples=10000, total_duration=26075.744012, train/accuracy=0.732721, train/loss=1.029948, validation/accuracy=0.658260, validation/loss=1.395620, validation/num_examples=50000
I0229 12:26:18.296006 139590178428672 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.88200843334198, loss=1.4519932270050049
I0229 12:26:52.199202 139590170035968 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.9209762811660767, loss=1.4382466077804565
I0229 12:27:26.114161 139590178428672 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.958168625831604, loss=1.3986562490463257
I0229 12:28:00.040585 139590170035968 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9599376916885376, loss=1.465395450592041
I0229 12:28:33.967998 139590178428672 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.2471764087677, loss=1.571173071861267
I0229 12:29:07.910942 139590170035968 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.2032089233398438, loss=1.4710880517959595
I0229 12:29:41.849731 139590178428672 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.041729688644409, loss=1.4505137205123901
I0229 12:30:15.782968 139590170035968 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.167354106903076, loss=1.5486762523651123
I0229 12:30:49.817471 139590178428672 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.2661380767822266, loss=1.4534013271331787
I0229 12:31:23.744513 139590170035968 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.959511399269104, loss=1.372307300567627
I0229 12:31:57.673005 139590178428672 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.219545602798462, loss=1.5110836029052734
I0229 12:32:31.602900 139590170035968 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.087547540664673, loss=1.4532580375671387
I0229 12:33:05.528807 139590178428672 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.1763975620269775, loss=1.4835087060928345
I0229 12:33:39.508118 139590170035968 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1028242111206055, loss=1.517145037651062
I0229 12:34:13.433001 139590178428672 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.270542621612549, loss=1.5036916732788086
I0229 12:34:14.931775 139753105983296 spec.py:321] Evaluating on the training split.
I0229 12:34:21.697549 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 12:34:30.056253 139753105983296 spec.py:349] Evaluating on the test split.
I0229 12:34:32.307389 139753105983296 submission_runner.py:411] Time since start: 26603.31s, 	Step: 75106, 	{'train/accuracy': 0.7759885191917419, 'train/loss': 0.8355568647384644, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.3705966472625732, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.1002209186553955, 'test/num_examples': 10000, 'score': 25556.429697752, 'total_duration': 26603.310527324677, 'accumulated_submission_time': 25556.429697752, 'accumulated_eval_time': 1042.2852289676666, 'accumulated_logging_time': 2.011807918548584}
I0229 12:34:32.339137 139590144857856 logging_writer.py:48] [75106] accumulated_eval_time=1042.285229, accumulated_logging_time=2.011808, accumulated_submission_time=25556.429698, global_step=75106, preemption_count=0, score=25556.429698, test/accuracy=0.529800, test/loss=2.100221, test/num_examples=10000, total_duration=26603.310527, train/accuracy=0.775989, train/loss=0.835557, validation/accuracy=0.662080, validation/loss=1.370597, validation/num_examples=50000
I0229 12:35:04.531883 139590153250560 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.3358826637268066, loss=1.544589877128601
I0229 12:35:38.459008 139590144857856 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1685376167297363, loss=1.524181842803955
I0229 12:36:12.406988 139590153250560 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.0141875743865967, loss=1.4801543951034546
I0229 12:36:46.380877 139590144857856 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.2751541137695312, loss=1.488152027130127
I0229 12:37:20.500802 139590153250560 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.276397943496704, loss=1.4796785116195679
I0229 12:37:54.435731 139590144857856 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.5769307613372803, loss=1.6264718770980835
I0229 12:38:28.394811 139590153250560 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9906373023986816, loss=1.4936647415161133
I0229 12:39:02.313916 139590144857856 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.1707570552825928, loss=1.4874309301376343
I0229 12:39:36.304880 139590153250560 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0800886154174805, loss=1.5476919412612915
I0229 12:40:10.251259 139590144857856 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.3198635578155518, loss=1.4058284759521484
I0229 12:40:44.215850 139590153250560 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.291214942932129, loss=1.5315189361572266
I0229 12:41:18.185800 139590144857856 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.970496416091919, loss=1.4807443618774414
I0229 12:41:52.138888 139590153250560 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.9975193738937378, loss=1.4683372974395752
I0229 12:42:26.100105 139590144857856 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.247145175933838, loss=1.4591665267944336
I0229 12:43:00.074681 139590153250560 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.1685376167297363, loss=1.5085874795913696
I0229 12:43:02.589421 139753105983296 spec.py:321] Evaluating on the training split.
I0229 12:43:08.890615 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 12:43:17.320322 139753105983296 spec.py:349] Evaluating on the test split.
I0229 12:43:19.637846 139753105983296 submission_runner.py:411] Time since start: 27130.64s, 	Step: 76609, 	{'train/accuracy': 0.7373644709587097, 'train/loss': 0.9887303709983826, 'validation/accuracy': 0.6548199653625488, 'validation/loss': 1.4147918224334717, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.1413140296936035, 'test/num_examples': 10000, 'score': 26066.617539405823, 'total_duration': 27130.641062498093, 'accumulated_submission_time': 26066.617539405823, 'accumulated_eval_time': 1059.3335857391357, 'accumulated_logging_time': 2.0537490844726562}
I0229 12:43:19.671313 139590170035968 logging_writer.py:48] [76609] accumulated_eval_time=1059.333586, accumulated_logging_time=2.053749, accumulated_submission_time=26066.617539, global_step=76609, preemption_count=0, score=26066.617539, test/accuracy=0.521200, test/loss=2.141314, test/num_examples=10000, total_duration=27130.641062, train/accuracy=0.737364, train/loss=0.988730, validation/accuracy=0.654820, validation/loss=1.414792, validation/num_examples=50000
I0229 12:43:50.876172 139590178428672 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.680102825164795, loss=1.5221854448318481
I0229 12:44:24.766671 139590170035968 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.953171730041504, loss=1.4950858354568481
I0229 12:44:58.700889 139590178428672 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.9562722444534302, loss=1.5729089975357056
I0229 12:45:32.894249 139590170035968 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.189513683319092, loss=1.5404481887817383
I0229 12:46:06.847569 139590178428672 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.3679134845733643, loss=1.4889565706253052
I0229 12:46:40.812360 139590170035968 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.218311071395874, loss=1.5237922668457031
I0229 12:47:14.766073 139590178428672 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.9515504837036133, loss=1.428293228149414
I0229 12:47:48.712929 139590170035968 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1285085678100586, loss=1.4927653074264526
I0229 12:48:22.663109 139590178428672 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.349506139755249, loss=1.4802947044372559
I0229 12:48:56.602133 139590170035968 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.2079625129699707, loss=1.458092451095581
I0229 12:49:30.611470 139590178428672 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.9942594766616821, loss=1.440967321395874
I0229 12:50:04.554642 139590170035968 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.0832369327545166, loss=1.4718530178070068
I0229 12:50:38.466617 139590178428672 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.311997413635254, loss=1.4012500047683716
I0229 12:51:12.378921 139590170035968 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.2038896083831787, loss=1.5242929458618164
I0229 12:51:46.305330 139590178428672 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.2109298706054688, loss=1.4395244121551514
I0229 12:51:49.851568 139753105983296 spec.py:321] Evaluating on the training split.
I0229 12:51:55.955417 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 12:52:04.379832 139753105983296 spec.py:349] Evaluating on the test split.
I0229 12:52:06.671572 139753105983296 submission_runner.py:411] Time since start: 27657.67s, 	Step: 78112, 	{'train/accuracy': 0.7537468075752258, 'train/loss': 0.9159765839576721, 'validation/accuracy': 0.6726399660110474, 'validation/loss': 1.3231079578399658, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.074337959289551, 'test/num_examples': 10000, 'score': 26576.7337744236, 'total_duration': 27657.674805641174, 'accumulated_submission_time': 26576.7337744236, 'accumulated_eval_time': 1076.1535539627075, 'accumulated_logging_time': 2.098949909210205}
I0229 12:52:06.705353 139590144857856 logging_writer.py:48] [78112] accumulated_eval_time=1076.153554, accumulated_logging_time=2.098950, accumulated_submission_time=26576.733774, global_step=78112, preemption_count=0, score=26576.733774, test/accuracy=0.539600, test/loss=2.074338, test/num_examples=10000, total_duration=27657.674806, train/accuracy=0.753747, train/loss=0.915977, validation/accuracy=0.672640, validation/loss=1.323108, validation/num_examples=50000
I0229 12:52:36.868584 139590153250560 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.0649619102478027, loss=1.3668179512023926
I0229 12:53:10.796920 139590144857856 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.0526249408721924, loss=1.451294183731079
I0229 12:53:44.753984 139590153250560 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.2438745498657227, loss=1.3225914239883423
I0229 12:54:18.691176 139590144857856 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.0939290523529053, loss=1.5390011072158813
I0229 12:54:52.645739 139590153250560 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.9945911169052124, loss=1.3623361587524414
I0229 12:55:26.659008 139590144857856 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.0032031536102295, loss=1.4194468259811401
I0229 12:56:00.625295 139590153250560 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.8260096311569214, loss=1.4306640625
I0229 12:56:34.571475 139590144857856 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.939894676208496, loss=1.3548744916915894
I0229 12:57:08.547333 139590153250560 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.1625633239746094, loss=1.37538480758667
I0229 12:57:42.500836 139590144857856 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.2263712882995605, loss=1.3709203004837036
I0229 12:58:16.454294 139590153250560 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.1011898517608643, loss=1.2392187118530273
I0229 12:58:50.423673 139590144857856 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.3371756076812744, loss=1.4857399463653564
I0229 12:59:24.381547 139590153250560 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.347709894180298, loss=1.4825167655944824
I0229 12:59:58.322648 139590144857856 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.419491767883301, loss=1.607370138168335
I0229 13:00:32.281695 139590153250560 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.299611806869507, loss=1.53647780418396
I0229 13:00:36.846528 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:00:42.881865 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:00:51.144585 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:00:53.396947 139753105983296 submission_runner.py:411] Time since start: 28184.40s, 	Step: 79615, 	{'train/accuracy': 0.74906325340271, 'train/loss': 0.9586185216903687, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.339442491531372, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.0593948364257812, 'test/num_examples': 10000, 'score': 27086.811772346497, 'total_duration': 28184.400028944016, 'accumulated_submission_time': 27086.811772346497, 'accumulated_eval_time': 1092.703783750534, 'accumulated_logging_time': 2.143179416656494}
I0229 13:00:53.430853 139589717059328 logging_writer.py:48] [79615] accumulated_eval_time=1092.703784, accumulated_logging_time=2.143179, accumulated_submission_time=27086.811772, global_step=79615, preemption_count=0, score=27086.811772, test/accuracy=0.545000, test/loss=2.059395, test/num_examples=10000, total_duration=28184.400029, train/accuracy=0.749063, train/loss=0.958619, validation/accuracy=0.670280, validation/loss=1.339442, validation/num_examples=50000
I0229 13:01:22.590171 139589725452032 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.278231143951416, loss=1.5128796100616455
I0229 13:01:56.598501 139589717059328 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.0779812335968018, loss=1.5334070920944214
I0229 13:02:30.468569 139589725452032 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.132131814956665, loss=1.5680774450302124
I0229 13:03:04.406467 139589717059328 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.2918057441711426, loss=1.3957624435424805
I0229 13:03:38.369644 139589725452032 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.892016887664795, loss=1.4347915649414062
I0229 13:04:12.320912 139589717059328 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.9979298114776611, loss=1.3642054796218872
I0229 13:04:46.257176 139589725452032 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.0060980319976807, loss=1.4853334426879883
I0229 13:05:20.204473 139589717059328 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.9939924478530884, loss=1.3873672485351562
I0229 13:05:54.163959 139589725452032 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.2136406898498535, loss=1.4253621101379395
I0229 13:06:28.091482 139589717059328 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.9820210933685303, loss=1.360900640487671
I0229 13:07:02.080787 139589725452032 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.1708223819732666, loss=1.3853776454925537
I0229 13:07:36.031049 139589717059328 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.3269331455230713, loss=1.5851136445999146
I0229 13:08:10.034410 139589725452032 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.3001303672790527, loss=1.5179286003112793
I0229 13:08:43.992146 139589717059328 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.2387466430664062, loss=1.3579888343811035
I0229 13:09:17.928586 139589725452032 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.2272443771362305, loss=1.441328525543213
I0229 13:09:23.495453 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:09:29.538981 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:09:37.874221 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:09:40.193018 139753105983296 submission_runner.py:411] Time since start: 28711.20s, 	Step: 81118, 	{'train/accuracy': 0.7498006820678711, 'train/loss': 0.9481151700019836, 'validation/accuracy': 0.6739799976348877, 'validation/loss': 1.3297719955444336, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0442447662353516, 'test/num_examples': 10000, 'score': 27596.813438415527, 'total_duration': 28711.19625043869, 'accumulated_submission_time': 27596.813438415527, 'accumulated_eval_time': 1109.4012954235077, 'accumulated_logging_time': 2.186863899230957}
I0229 13:09:40.223495 139589717059328 logging_writer.py:48] [81118] accumulated_eval_time=1109.401295, accumulated_logging_time=2.186864, accumulated_submission_time=27596.813438, global_step=81118, preemption_count=0, score=27596.813438, test/accuracy=0.540600, test/loss=2.044245, test/num_examples=10000, total_duration=28711.196250, train/accuracy=0.749801, train/loss=0.948115, validation/accuracy=0.673980, validation/loss=1.329772, validation/num_examples=50000
I0229 13:10:08.387366 139590170035968 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.172678232192993, loss=1.4956891536712646
I0229 13:10:42.303822 139589717059328 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.38482403755188, loss=1.5649932622909546
I0229 13:11:16.228056 139590170035968 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.9678550958633423, loss=1.3797361850738525
I0229 13:11:50.175174 139589717059328 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.2966670989990234, loss=1.3801637887954712
I0229 13:12:24.138856 139590170035968 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.09293532371521, loss=1.4216899871826172
I0229 13:12:58.094331 139589717059328 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.1708924770355225, loss=1.4815016984939575
I0229 13:13:32.033743 139590170035968 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.2456202507019043, loss=1.4677761793136597
I0229 13:14:06.045186 139589717059328 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.286710739135742, loss=1.5705726146697998
I0229 13:14:39.986976 139590170035968 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.5017786026000977, loss=1.518242597579956
I0229 13:15:13.934875 139589717059328 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.2257447242736816, loss=1.45731520652771
I0229 13:15:47.869048 139590170035968 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.204388380050659, loss=1.442527174949646
I0229 13:16:21.825891 139589717059328 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.3031294345855713, loss=1.4644051790237427
I0229 13:16:55.764549 139590170035968 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.1217188835144043, loss=1.4909909963607788
I0229 13:17:29.722808 139589717059328 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2915544509887695, loss=1.410029649734497
I0229 13:18:03.686964 139590170035968 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.2560644149780273, loss=1.5150606632232666
I0229 13:18:10.296737 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:18:16.281968 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:18:24.560628 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:18:26.842665 139753105983296 submission_runner.py:411] Time since start: 29237.85s, 	Step: 82621, 	{'train/accuracy': 0.7277582883834839, 'train/loss': 1.0413271188735962, 'validation/accuracy': 0.6600399613380432, 'validation/loss': 1.386773705482483, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.1522934436798096, 'test/num_examples': 10000, 'score': 28106.822259902954, 'total_duration': 29237.845888853073, 'accumulated_submission_time': 28106.822259902954, 'accumulated_eval_time': 1125.9471654891968, 'accumulated_logging_time': 2.227048873901367}
I0229 13:18:26.880165 139589717059328 logging_writer.py:48] [82621] accumulated_eval_time=1125.947165, accumulated_logging_time=2.227049, accumulated_submission_time=28106.822260, global_step=82621, preemption_count=0, score=28106.822260, test/accuracy=0.524700, test/loss=2.152293, test/num_examples=10000, total_duration=29237.845889, train/accuracy=0.727758, train/loss=1.041327, validation/accuracy=0.660040, validation/loss=1.386774, validation/num_examples=50000
I0229 13:18:54.006927 139589725452032 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.083510160446167, loss=1.496429443359375
I0229 13:19:27.926293 139589717059328 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.04455304145813, loss=1.3924884796142578
I0229 13:20:01.865926 139589725452032 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.39345383644104, loss=1.4825913906097412
I0229 13:20:35.862246 139589717059328 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.1882967948913574, loss=1.4059661626815796
I0229 13:21:09.751865 139589725452032 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.264026165008545, loss=1.419587254524231
I0229 13:21:43.664651 139589717059328 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.1007471084594727, loss=1.4043490886688232
I0229 13:22:17.587937 139589725452032 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.282895565032959, loss=1.3342807292938232
I0229 13:22:51.517588 139589717059328 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.583822250366211, loss=1.3741238117218018
I0229 13:23:25.435398 139589725452032 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.7960727214813232, loss=1.4674184322357178
I0229 13:23:59.368368 139589717059328 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.3843233585357666, loss=1.562299132347107
I0229 13:24:33.271317 139589725452032 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.1532435417175293, loss=1.396705150604248
I0229 13:25:07.209364 139589717059328 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.333829164505005, loss=1.4914630651474
I0229 13:25:41.154964 139589725452032 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.2332606315612793, loss=1.4328938722610474
I0229 13:26:15.164844 139589717059328 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.2393059730529785, loss=1.5271435976028442
I0229 13:26:49.076126 139589725452032 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.308901786804199, loss=1.508776068687439
I0229 13:26:57.029886 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:27:03.083786 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:27:11.434128 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:27:13.757226 139753105983296 submission_runner.py:411] Time since start: 29764.76s, 	Step: 84125, 	{'train/accuracy': 0.7925701141357422, 'train/loss': 0.7795647382736206, 'validation/accuracy': 0.6730599999427795, 'validation/loss': 1.3342093229293823, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.0880985260009766, 'test/num_examples': 10000, 'score': 28616.90827870369, 'total_duration': 29764.76045846939, 'accumulated_submission_time': 28616.90827870369, 'accumulated_eval_time': 1142.674451828003, 'accumulated_logging_time': 2.274683952331543}
I0229 13:27:13.789282 139590161643264 logging_writer.py:48] [84125] accumulated_eval_time=1142.674452, accumulated_logging_time=2.274684, accumulated_submission_time=28616.908279, global_step=84125, preemption_count=0, score=28616.908279, test/accuracy=0.540000, test/loss=2.088099, test/num_examples=10000, total_duration=29764.760458, train/accuracy=0.792570, train/loss=0.779565, validation/accuracy=0.673060, validation/loss=1.334209, validation/num_examples=50000
I0229 13:27:39.564509 139590170035968 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.463738203048706, loss=1.3948951959609985
I0229 13:28:13.450683 139590161643264 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.2917442321777344, loss=1.4155104160308838
I0229 13:28:47.407656 139590170035968 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.37460994720459, loss=1.44220769405365
I0229 13:29:21.348382 139590161643264 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.124323844909668, loss=1.4027438163757324
I0229 13:29:55.290507 139590170035968 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.289435863494873, loss=1.414146065711975
I0229 13:30:29.267599 139590161643264 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.1269898414611816, loss=1.4331740140914917
I0229 13:31:03.224649 139590170035968 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.262824296951294, loss=1.5442312955856323
I0229 13:31:37.187109 139590161643264 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.470571756362915, loss=1.5937296152114868
I0229 13:32:11.154182 139590170035968 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.3974416255950928, loss=1.4107846021652222
I0229 13:32:45.167150 139590161643264 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.436234951019287, loss=1.5270991325378418
I0229 13:33:19.122945 139590170035968 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.6568844318389893, loss=1.5779914855957031
I0229 13:33:53.095921 139590161643264 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.083766222000122, loss=1.4383975267410278
I0229 13:34:27.056764 139590170035968 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.1286778450012207, loss=1.3583872318267822
I0229 13:35:01.035520 139590161643264 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.1890482902526855, loss=1.5102229118347168
I0229 13:35:34.994081 139590170035968 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.174748420715332, loss=1.359661340713501
I0229 13:35:43.986297 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:35:50.057752 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:35:58.519203 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:36:00.736704 139753105983296 submission_runner.py:411] Time since start: 30291.74s, 	Step: 85628, 	{'train/accuracy': 0.7692721486091614, 'train/loss': 0.8613221049308777, 'validation/accuracy': 0.6786800026893616, 'validation/loss': 1.3224866390228271, 'validation/num_examples': 50000, 'test/accuracy': 0.5486000180244446, 'test/loss': 2.0616767406463623, 'test/num_examples': 10000, 'score': 29127.042145967484, 'total_duration': 30291.739934682846, 'accumulated_submission_time': 29127.042145967484, 'accumulated_eval_time': 1159.42480802536, 'accumulated_logging_time': 2.3172881603240967}
I0229 13:36:00.770536 139590144857856 logging_writer.py:48] [85628] accumulated_eval_time=1159.424808, accumulated_logging_time=2.317288, accumulated_submission_time=29127.042146, global_step=85628, preemption_count=0, score=29127.042146, test/accuracy=0.548600, test/loss=2.061677, test/num_examples=10000, total_duration=30291.739935, train/accuracy=0.769272, train/loss=0.861322, validation/accuracy=0.678680, validation/loss=1.322487, validation/num_examples=50000
I0229 13:36:25.525243 139590153250560 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2389609813690186, loss=1.487518072128296
I0229 13:36:59.442461 139590144857856 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.092529296875, loss=1.3770076036453247
I0229 13:37:33.398410 139590153250560 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.3820252418518066, loss=1.4600963592529297
I0229 13:38:07.341150 139590144857856 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.235640525817871, loss=1.4665865898132324
I0229 13:38:41.334279 139590153250560 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.2733254432678223, loss=1.3556170463562012
I0229 13:39:15.268747 139590144857856 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.3188319206237793, loss=1.3870331048965454
I0229 13:39:49.185842 139590153250560 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.067190408706665, loss=1.4322383403778076
I0229 13:40:23.146636 139590144857856 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.204815626144409, loss=1.4420280456542969
I0229 13:40:57.098738 139590153250560 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.333657741546631, loss=1.5489487648010254
I0229 13:41:31.041162 139590144857856 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.300236225128174, loss=1.3411386013031006
I0229 13:42:04.982330 139590153250560 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.2046008110046387, loss=1.4227309226989746
I0229 13:42:38.955327 139590144857856 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.1644623279571533, loss=1.2761399745941162
I0229 13:43:12.895033 139590153250560 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.262016534805298, loss=1.4063973426818848
I0229 13:43:46.835092 139590144857856 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.1765401363372803, loss=1.4236481189727783
I0229 13:44:20.782008 139590153250560 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.0774009227752686, loss=1.3658109903335571
I0229 13:44:30.777938 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:44:36.766216 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:44:45.073283 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:44:47.355861 139753105983296 submission_runner.py:411] Time since start: 30818.36s, 	Step: 87131, 	{'train/accuracy': 0.7654455900192261, 'train/loss': 0.869311511516571, 'validation/accuracy': 0.6791200041770935, 'validation/loss': 1.3054094314575195, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.015770673751831, 'test/num_examples': 10000, 'score': 29636.98632788658, 'total_duration': 30818.359090328217, 'accumulated_submission_time': 29636.98632788658, 'accumulated_eval_time': 1176.0026772022247, 'accumulated_logging_time': 2.3611364364624023}
I0229 13:44:47.389298 139590153250560 logging_writer.py:48] [87131] accumulated_eval_time=1176.002677, accumulated_logging_time=2.361136, accumulated_submission_time=29636.986328, global_step=87131, preemption_count=0, score=29636.986328, test/accuracy=0.552100, test/loss=2.015771, test/num_examples=10000, total_duration=30818.359090, train/accuracy=0.765446, train/loss=0.869312, validation/accuracy=0.679120, validation/loss=1.305409, validation/num_examples=50000
I0229 13:45:11.257934 139590161643264 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.2404706478118896, loss=1.4256224632263184
I0229 13:45:45.159494 139590153250560 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2625131607055664, loss=1.3958759307861328
I0229 13:46:19.102031 139590161643264 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.3896584510803223, loss=1.4635361433029175
I0229 13:46:53.055889 139590153250560 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.2923777103424072, loss=1.4023667573928833
I0229 13:47:27.000226 139590161643264 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.2479655742645264, loss=1.4051785469055176
I0229 13:48:00.935620 139590153250560 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.476346492767334, loss=1.3459616899490356
I0229 13:48:34.876597 139590161643264 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.220921277999878, loss=1.516273856163025
I0229 13:49:08.821317 139590153250560 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.1324708461761475, loss=1.358299732208252
I0229 13:49:42.763767 139590161643264 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.125584602355957, loss=1.3522101640701294
I0229 13:50:16.709625 139590153250560 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.204192876815796, loss=1.4573954343795776
I0229 13:50:50.657957 139590161643264 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.150911808013916, loss=1.4407151937484741
I0229 13:51:24.724262 139590153250560 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.15952467918396, loss=1.3785979747772217
I0229 13:51:58.665248 139590161643264 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.2579758167266846, loss=1.3100868463516235
I0229 13:52:32.599408 139590153250560 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.4077906608581543, loss=1.4206515550613403
I0229 13:53:06.557592 139590161643264 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.268211603164673, loss=1.3626302480697632
I0229 13:53:17.569458 139753105983296 spec.py:321] Evaluating on the training split.
I0229 13:53:23.592469 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 13:53:31.750253 139753105983296 spec.py:349] Evaluating on the test split.
I0229 13:53:34.051956 139753105983296 submission_runner.py:411] Time since start: 31345.06s, 	Step: 88634, 	{'train/accuracy': 0.7592872977256775, 'train/loss': 0.9058119654655457, 'validation/accuracy': 0.6819199919700623, 'validation/loss': 1.291403889656067, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.0037930011749268, 'test/num_examples': 10000, 'score': 30147.10192131996, 'total_duration': 31345.05516934395, 'accumulated_submission_time': 30147.10192131996, 'accumulated_eval_time': 1192.4851064682007, 'accumulated_logging_time': 2.4045522212982178}
I0229 13:53:34.088785 139589717059328 logging_writer.py:48] [88634] accumulated_eval_time=1192.485106, accumulated_logging_time=2.404552, accumulated_submission_time=30147.101921, global_step=88634, preemption_count=0, score=30147.101921, test/accuracy=0.548900, test/loss=2.003793, test/num_examples=10000, total_duration=31345.055169, train/accuracy=0.759287, train/loss=0.905812, validation/accuracy=0.681920, validation/loss=1.291404, validation/num_examples=50000
I0229 13:53:56.796471 139589725452032 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.5144078731536865, loss=1.3227801322937012
I0229 13:54:30.693906 139589717059328 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.3786957263946533, loss=1.4160048961639404
I0229 13:55:04.603336 139589725452032 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.2965002059936523, loss=1.3845694065093994
I0229 13:55:38.540302 139589717059328 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.4940345287323, loss=1.441204309463501
I0229 13:56:12.523161 139589725452032 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3197996616363525, loss=1.4189703464508057
I0229 13:56:46.470921 139589717059328 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.17691707611084, loss=1.4027924537658691
I0229 13:57:20.541414 139589725452032 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.290595769882202, loss=1.4451395273208618
I0229 13:57:54.484396 139589717059328 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.6974520683288574, loss=1.4330823421478271
I0229 13:58:28.398947 139589725452032 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.2728822231292725, loss=1.5168967247009277
I0229 13:59:02.361087 139589717059328 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.59562611579895, loss=1.3683117628097534
I0229 13:59:36.346110 139589725452032 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.0458271503448486, loss=1.2980852127075195
I0229 14:00:10.288202 139589717059328 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.3409523963928223, loss=1.3947340250015259
I0229 14:00:44.217236 139589725452032 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.3101601600646973, loss=1.4508769512176514
I0229 14:01:18.164932 139589717059328 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.501126527786255, loss=1.4982049465179443
I0229 14:01:52.130409 139589725452032 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.628633737564087, loss=1.432536005973816
I0229 14:02:04.153702 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:02:10.167458 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:02:18.632545 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:02:20.902912 139753105983296 submission_runner.py:411] Time since start: 31871.91s, 	Step: 90137, 	{'train/accuracy': 0.7499800324440002, 'train/loss': 0.9388789534568787, 'validation/accuracy': 0.6706199645996094, 'validation/loss': 1.3405901193618774, 'validation/num_examples': 50000, 'test/accuracy': 0.5453000068664551, 'test/loss': 2.0540900230407715, 'test/num_examples': 10000, 'score': 30657.10430932045, 'total_duration': 31871.90614414215, 'accumulated_submission_time': 30657.10430932045, 'accumulated_eval_time': 1209.234266757965, 'accumulated_logging_time': 2.4512953758239746}
I0229 14:02:20.937367 139589717059328 logging_writer.py:48] [90137] accumulated_eval_time=1209.234267, accumulated_logging_time=2.451295, accumulated_submission_time=30657.104309, global_step=90137, preemption_count=0, score=30657.104309, test/accuracy=0.545300, test/loss=2.054090, test/num_examples=10000, total_duration=31871.906144, train/accuracy=0.749980, train/loss=0.938879, validation/accuracy=0.670620, validation/loss=1.340590, validation/num_examples=50000
I0229 14:02:42.632616 139590170035968 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.355811357498169, loss=1.439793586730957
I0229 14:03:16.551282 139589717059328 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.303518772125244, loss=1.3905220031738281
I0229 14:03:50.587522 139590170035968 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.3006093502044678, loss=1.407959222793579
I0229 14:04:24.547935 139589717059328 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.375229597091675, loss=1.43239426612854
I0229 14:04:58.524458 139590170035968 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.667470693588257, loss=1.4734746217727661
I0229 14:05:32.450084 139589717059328 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.2447144985198975, loss=1.3719431161880493
I0229 14:06:06.364705 139590170035968 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.4565343856811523, loss=1.450463891029358
I0229 14:06:40.321296 139589717059328 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.266435384750366, loss=1.4516866207122803
I0229 14:07:14.274869 139590170035968 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.4045181274414062, loss=1.372429370880127
I0229 14:07:48.213811 139589717059328 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.340346336364746, loss=1.3690354824066162
I0229 14:08:22.172011 139590170035968 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.469998359680176, loss=1.3757061958312988
I0229 14:08:56.113550 139589717059328 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.235468626022339, loss=1.3353374004364014
I0229 14:09:30.105833 139590170035968 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.7163243293762207, loss=1.4359444379806519
I0229 14:10:04.050965 139589717059328 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.201362133026123, loss=1.3705157041549683
I0229 14:10:37.990870 139590170035968 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.274139642715454, loss=1.4336587190628052
I0229 14:10:51.027339 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:10:57.043088 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:11:05.440778 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:11:07.715914 139753105983296 submission_runner.py:411] Time since start: 32398.72s, 	Step: 91640, 	{'train/accuracy': 0.7515146732330322, 'train/loss': 0.9365392327308655, 'validation/accuracy': 0.6793999671936035, 'validation/loss': 1.3209984302520752, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.101987361907959, 'test/num_examples': 10000, 'score': 31167.13133573532, 'total_duration': 32398.719130277634, 'accumulated_submission_time': 31167.13133573532, 'accumulated_eval_time': 1225.9227805137634, 'accumulated_logging_time': 2.495468854904175}
I0229 14:11:07.751564 139589725452032 logging_writer.py:48] [91640] accumulated_eval_time=1225.922781, accumulated_logging_time=2.495469, accumulated_submission_time=31167.131336, global_step=91640, preemption_count=0, score=31167.131336, test/accuracy=0.546400, test/loss=2.101987, test/num_examples=10000, total_duration=32398.719130, train/accuracy=0.751515, train/loss=0.936539, validation/accuracy=0.679400, validation/loss=1.320998, validation/num_examples=50000
I0229 14:11:28.434058 139590144857856 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3906500339508057, loss=1.396911382675171
I0229 14:12:02.310862 139589725452032 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.379943609237671, loss=1.4010215997695923
I0229 14:12:36.220372 139590144857856 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3057749271392822, loss=1.323644757270813
I0229 14:13:10.146227 139589725452032 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.249774932861328, loss=1.3586567640304565
I0229 14:13:44.056527 139590144857856 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.416372299194336, loss=1.463260293006897
I0229 14:14:17.981564 139589725452032 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.736640214920044, loss=1.50856351852417
I0229 14:14:51.901947 139590144857856 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.07580828666687, loss=1.3150091171264648
I0229 14:15:25.801530 139589725452032 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.3498406410217285, loss=1.4009199142456055
I0229 14:15:59.824469 139590144857856 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4822423458099365, loss=1.3737658262252808
I0229 14:16:33.754672 139589725452032 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3967111110687256, loss=1.4159690141677856
I0229 14:17:07.709865 139590144857856 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.273684501647949, loss=1.4724922180175781
I0229 14:17:41.653573 139589725452032 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.401050329208374, loss=1.3648498058319092
I0229 14:18:15.593251 139590144857856 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.2264981269836426, loss=1.3169363737106323
I0229 14:18:49.515214 139589725452032 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.3787832260131836, loss=1.311184287071228
I0229 14:19:23.433246 139590144857856 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.281309127807617, loss=1.3755478858947754
I0229 14:19:37.822390 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:19:43.867244 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:19:52.174968 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:19:54.451848 139753105983296 submission_runner.py:411] Time since start: 32925.46s, 	Step: 93144, 	{'train/accuracy': 0.7772839665412903, 'train/loss': 0.8345317840576172, 'validation/accuracy': 0.6767199635505676, 'validation/loss': 1.3265360593795776, 'validation/num_examples': 50000, 'test/accuracy': 0.5480000376701355, 'test/loss': 2.0646495819091797, 'test/num_examples': 10000, 'score': 31677.139016866684, 'total_duration': 32925.45507359505, 'accumulated_submission_time': 31677.139016866684, 'accumulated_eval_time': 1242.5521783828735, 'accumulated_logging_time': 2.5414443016052246}
I0229 14:19:54.485421 139590170035968 logging_writer.py:48] [93144] accumulated_eval_time=1242.552178, accumulated_logging_time=2.541444, accumulated_submission_time=31677.139017, global_step=93144, preemption_count=0, score=31677.139017, test/accuracy=0.548000, test/loss=2.064650, test/num_examples=10000, total_duration=32925.455074, train/accuracy=0.777284, train/loss=0.834532, validation/accuracy=0.676720, validation/loss=1.326536, validation/num_examples=50000
I0229 14:20:13.819202 139590178428672 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.2824594974517822, loss=1.5481712818145752
I0229 14:20:47.736101 139590170035968 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.5004656314849854, loss=1.4544672966003418
I0229 14:21:21.689118 139590178428672 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.7942616939544678, loss=1.360436201095581
I0229 14:21:55.733518 139590170035968 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.3945348262786865, loss=1.3680249452590942
I0229 14:22:29.693171 139590178428672 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.3037242889404297, loss=1.3990546464920044
I0229 14:23:03.625354 139590170035968 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.4177815914154053, loss=1.444638967514038
I0229 14:23:37.583599 139590178428672 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.3575637340545654, loss=1.3481870889663696
I0229 14:24:11.565336 139590170035968 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.4841318130493164, loss=1.4049423933029175
I0229 14:24:45.523048 139590178428672 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.452427387237549, loss=1.2625432014465332
I0229 14:25:19.481717 139590170035968 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.258863925933838, loss=1.3321248292922974
I0229 14:25:53.451666 139590178428672 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.5602002143859863, loss=1.4270272254943848
I0229 14:26:27.411195 139590170035968 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.3389079570770264, loss=1.4180816411972046
I0229 14:27:01.347234 139590178428672 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.360023021697998, loss=1.4315348863601685
I0229 14:27:35.272734 139590170035968 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.529278516769409, loss=1.4327757358551025
I0229 14:28:09.310414 139590178428672 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.530541181564331, loss=1.3775697946548462
I0229 14:28:24.714530 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:28:30.712407 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:28:39.077214 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:28:41.347296 139753105983296 submission_runner.py:411] Time since start: 33452.35s, 	Step: 94647, 	{'train/accuracy': 0.7825653553009033, 'train/loss': 0.8143873810768127, 'validation/accuracy': 0.6804599761962891, 'validation/loss': 1.2941557168960571, 'validation/num_examples': 50000, 'test/accuracy': 0.5538000464439392, 'test/loss': 2.0050389766693115, 'test/num_examples': 10000, 'score': 32187.304002285004, 'total_duration': 33452.35047388077, 'accumulated_submission_time': 32187.304002285004, 'accumulated_eval_time': 1259.1848402023315, 'accumulated_logging_time': 2.5860958099365234}
I0229 14:28:41.403969 139590144857856 logging_writer.py:48] [94647] accumulated_eval_time=1259.184840, accumulated_logging_time=2.586096, accumulated_submission_time=32187.304002, global_step=94647, preemption_count=0, score=32187.304002, test/accuracy=0.553800, test/loss=2.005039, test/num_examples=10000, total_duration=33452.350474, train/accuracy=0.782565, train/loss=0.814387, validation/accuracy=0.680460, validation/loss=1.294156, validation/num_examples=50000
I0229 14:28:59.736062 139590153250560 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.2847344875335693, loss=1.343843936920166
I0229 14:29:33.601419 139590144857856 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.489149332046509, loss=1.372481346130371
I0229 14:30:07.542123 139590153250560 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.37039852142334, loss=1.3299460411071777
I0229 14:30:41.510354 139590144857856 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.5399959087371826, loss=1.3517537117004395
I0229 14:31:15.444669 139590153250560 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.341813564300537, loss=1.4604458808898926
I0229 14:31:49.351669 139590144857856 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.461667060852051, loss=1.310409426689148
I0229 14:32:23.274870 139590153250560 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.532041311264038, loss=1.4387705326080322
I0229 14:32:57.226443 139590144857856 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.380190372467041, loss=1.3392508029937744
I0229 14:33:31.174901 139590153250560 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.2190027236938477, loss=1.2903478145599365
I0229 14:34:05.088945 139590144857856 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.3694770336151123, loss=1.3293190002441406
I0229 14:34:39.230606 139590153250560 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.3908815383911133, loss=1.398097038269043
I0229 14:35:13.182634 139590144857856 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.369839906692505, loss=1.4958852529525757
I0229 14:35:47.121009 139590153250560 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.4060018062591553, loss=1.410056710243225
I0229 14:36:21.074949 139590144857856 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.383061647415161, loss=1.3907439708709717
I0229 14:36:55.026521 139590153250560 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.3264923095703125, loss=1.4215021133422852
I0229 14:37:11.476491 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:37:17.475779 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:37:25.872391 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:37:28.111763 139753105983296 submission_runner.py:411] Time since start: 33979.11s, 	Step: 96150, 	{'train/accuracy': 0.7772839665412903, 'train/loss': 0.8267792463302612, 'validation/accuracy': 0.6888599991798401, 'validation/loss': 1.2830692529678345, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 2.0027637481689453, 'test/num_examples': 10000, 'score': 32697.30992746353, 'total_duration': 33979.11499476433, 'accumulated_submission_time': 32697.30992746353, 'accumulated_eval_time': 1275.8200645446777, 'accumulated_logging_time': 2.6573374271392822}
I0229 14:37:28.149586 139589725452032 logging_writer.py:48] [96150] accumulated_eval_time=1275.820065, accumulated_logging_time=2.657337, accumulated_submission_time=32697.309927, global_step=96150, preemption_count=0, score=32697.309927, test/accuracy=0.561300, test/loss=2.002764, test/num_examples=10000, total_duration=33979.114995, train/accuracy=0.777284, train/loss=0.826779, validation/accuracy=0.688860, validation/loss=1.283069, validation/num_examples=50000
I0229 14:37:45.447586 139590144857856 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.4924073219299316, loss=1.324124813079834
I0229 14:38:19.349359 139589725452032 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.2776150703430176, loss=1.2811819314956665
I0229 14:38:53.259685 139590144857856 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.397857427597046, loss=1.2812128067016602
I0229 14:39:27.216259 139589725452032 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.505331039428711, loss=1.3168413639068604
I0229 14:40:01.183389 139590144857856 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.659832000732422, loss=1.368740200996399
I0229 14:40:35.240759 139589725452032 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.474531650543213, loss=1.3113983869552612
I0229 14:41:09.163943 139590144857856 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.4668161869049072, loss=1.4810879230499268
I0229 14:41:43.101699 139589725452032 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.7848803997039795, loss=1.3731880187988281
I0229 14:42:17.073450 139590144857856 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.793067455291748, loss=1.3597265481948853
I0229 14:42:51.027068 139589725452032 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.1853744983673096, loss=1.1984267234802246
I0229 14:43:24.977002 139590144857856 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.3013417720794678, loss=1.4180904626846313
I0229 14:43:58.931588 139589725452032 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.234484910964966, loss=1.3169260025024414
I0229 14:44:32.865930 139590144857856 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.362630844116211, loss=1.336159110069275
I0229 14:45:06.841524 139589725452032 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.777308225631714, loss=1.383196234703064
I0229 14:45:40.790984 139590144857856 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.5054996013641357, loss=1.378283143043518
I0229 14:45:58.258846 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:46:04.416409 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:46:12.725239 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:46:14.987548 139753105983296 submission_runner.py:411] Time since start: 34505.99s, 	Step: 97653, 	{'train/accuracy': 0.7453164458274841, 'train/loss': 0.9852451086044312, 'validation/accuracy': 0.6612799763679504, 'validation/loss': 1.3965210914611816, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.141735076904297, 'test/num_examples': 10000, 'score': 33207.35705137253, 'total_duration': 34505.99077963829, 'accumulated_submission_time': 33207.35705137253, 'accumulated_eval_time': 1292.5487146377563, 'accumulated_logging_time': 2.7052574157714844}
I0229 14:46:15.025399 139589717059328 logging_writer.py:48] [97653] accumulated_eval_time=1292.548715, accumulated_logging_time=2.705257, accumulated_submission_time=33207.357051, global_step=97653, preemption_count=0, score=33207.357051, test/accuracy=0.532500, test/loss=2.141735, test/num_examples=10000, total_duration=34505.990780, train/accuracy=0.745316, train/loss=0.985245, validation/accuracy=0.661280, validation/loss=1.396521, validation/num_examples=50000
I0229 14:46:31.420495 139589725452032 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4564478397369385, loss=1.3478479385375977
I0229 14:47:05.322530 139589717059328 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.4247655868530273, loss=1.3983099460601807
I0229 14:47:39.239608 139589725452032 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.3680009841918945, loss=1.419708490371704
I0229 14:48:13.185824 139589717059328 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.45772385597229, loss=1.3342026472091675
I0229 14:48:47.113974 139589725452032 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.426852226257324, loss=1.378777027130127
I0229 14:49:21.071293 139589717059328 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.6041336059570312, loss=1.3202121257781982
I0229 14:49:54.981500 139589725452032 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.3640055656433105, loss=1.344282865524292
I0229 14:50:28.900002 139589717059328 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.4214694499969482, loss=1.3115952014923096
I0229 14:51:02.861105 139589725452032 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.479849338531494, loss=1.3030455112457275
I0229 14:51:36.793378 139589717059328 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.2879791259765625, loss=1.3778630495071411
I0229 14:52:10.724642 139589725452032 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.3081533908843994, loss=1.292352318763733
I0229 14:52:44.747702 139589717059328 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.468930244445801, loss=1.3984103202819824
I0229 14:53:18.654008 139589725452032 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.4174556732177734, loss=1.3367457389831543
I0229 14:53:52.604549 139589717059328 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.3755695819854736, loss=1.401481032371521
I0229 14:54:26.571179 139589725452032 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.741873025894165, loss=1.4031646251678467
I0229 14:54:45.051019 139753105983296 spec.py:321] Evaluating on the training split.
I0229 14:54:51.062072 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 14:54:59.493565 139753105983296 spec.py:349] Evaluating on the test split.
I0229 14:55:01.743405 139753105983296 submission_runner.py:411] Time since start: 35032.75s, 	Step: 99156, 	{'train/accuracy': 0.7657644748687744, 'train/loss': 0.8797435760498047, 'validation/accuracy': 0.6818999648094177, 'validation/loss': 1.3018945455551147, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.0254077911376953, 'test/num_examples': 10000, 'score': 33717.318086624146, 'total_duration': 35032.746633291245, 'accumulated_submission_time': 33717.318086624146, 'accumulated_eval_time': 1309.2410578727722, 'accumulated_logging_time': 2.7550249099731445}
I0229 14:55:01.780613 139590170035968 logging_writer.py:48] [99156] accumulated_eval_time=1309.241058, accumulated_logging_time=2.755025, accumulated_submission_time=33717.318087, global_step=99156, preemption_count=0, score=33717.318087, test/accuracy=0.552800, test/loss=2.025408, test/num_examples=10000, total_duration=35032.746633, train/accuracy=0.765764, train/loss=0.879744, validation/accuracy=0.681900, validation/loss=1.301895, validation/num_examples=50000
I0229 14:55:17.020570 139590178428672 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.685884475708008, loss=1.340411901473999
I0229 14:55:50.939973 139590170035968 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.4672048091888428, loss=1.2446556091308594
I0229 14:56:24.900764 139590178428672 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.5443527698516846, loss=1.4082008600234985
I0229 14:56:58.870778 139590170035968 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.307216167449951, loss=1.3765746355056763
I0229 14:57:32.778150 139590178428672 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.579258680343628, loss=1.4001950025558472
I0229 14:58:06.727106 139590170035968 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.380364179611206, loss=1.4378911256790161
I0229 14:58:40.662108 139590178428672 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.612300157546997, loss=1.3988748788833618
I0229 14:59:14.681897 139590170035968 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.4843194484710693, loss=1.3432384729385376
I0229 14:59:48.648772 139590178428672 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.527620553970337, loss=1.344268798828125
I0229 15:00:22.606356 139590170035968 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.5854973793029785, loss=1.3402832746505737
I0229 15:00:56.548967 139590178428672 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.318648099899292, loss=1.23736572265625
I0229 15:01:30.501938 139590170035968 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.544466495513916, loss=1.383063793182373
I0229 15:02:04.462703 139590178428672 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4305078983306885, loss=1.3911759853363037
I0229 15:02:38.402359 139590170035968 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.4201488494873047, loss=1.3473643064498901
I0229 15:03:12.300918 139590178428672 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.4441494941711426, loss=1.2016842365264893
I0229 15:03:31.801989 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:03:37.792544 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:03:46.277752 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:03:48.581866 139753105983296 submission_runner.py:411] Time since start: 35559.59s, 	Step: 100659, 	{'train/accuracy': 0.7707070708274841, 'train/loss': 0.8599854111671448, 'validation/accuracy': 0.6876400113105774, 'validation/loss': 1.271263599395752, 'validation/num_examples': 50000, 'test/accuracy': 0.5570999979972839, 'test/loss': 2.000819206237793, 'test/num_examples': 10000, 'score': 34227.27606058121, 'total_duration': 35559.58508205414, 'accumulated_submission_time': 34227.27606058121, 'accumulated_eval_time': 1326.0208716392517, 'accumulated_logging_time': 2.801799774169922}
I0229 15:03:48.616216 139589708666624 logging_writer.py:48] [100659] accumulated_eval_time=1326.020872, accumulated_logging_time=2.801800, accumulated_submission_time=34227.276061, global_step=100659, preemption_count=0, score=34227.276061, test/accuracy=0.557100, test/loss=2.000819, test/num_examples=10000, total_duration=35559.585082, train/accuracy=0.770707, train/loss=0.859985, validation/accuracy=0.687640, validation/loss=1.271264, validation/num_examples=50000
I0229 15:04:02.840838 139589717059328 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.5471274852752686, loss=1.41380774974823
I0229 15:04:36.698865 139589708666624 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.581789970397949, loss=1.2959215641021729
I0229 15:05:10.717261 139589717059328 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.342440128326416, loss=1.2668389081954956
I0229 15:05:44.626455 139589708666624 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.5246503353118896, loss=1.326187252998352
I0229 15:06:18.557757 139589717059328 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.7043964862823486, loss=1.4122134447097778
I0229 15:06:52.516082 139589708666624 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.5323047637939453, loss=1.3449002504348755
I0229 15:07:26.464784 139589717059328 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.5638837814331055, loss=1.3417116403579712
I0229 15:08:00.374269 139589708666624 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.3714258670806885, loss=1.3263381719589233
I0229 15:08:34.331511 139589717059328 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.6539578437805176, loss=1.3994717597961426
I0229 15:09:08.301897 139589708666624 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.685157299041748, loss=1.397252082824707
I0229 15:09:42.267391 139589717059328 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.479630947113037, loss=1.3309305906295776
I0229 15:10:16.227164 139589708666624 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.74878191947937, loss=1.3505247831344604
I0229 15:10:50.153485 139589717059328 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.3226122856140137, loss=1.2691363096237183
I0229 15:11:24.187546 139589708666624 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.4240846633911133, loss=1.3298112154006958
I0229 15:11:58.131838 139589717059328 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.5637636184692383, loss=1.24006986618042
I0229 15:12:18.630717 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:12:24.594135 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:12:32.936269 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:12:35.321402 139753105983296 submission_runner.py:411] Time since start: 36086.32s, 	Step: 102162, 	{'train/accuracy': 0.7767059803009033, 'train/loss': 0.8202096223831177, 'validation/accuracy': 0.6881600022315979, 'validation/loss': 1.2708592414855957, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 2.031827449798584, 'test/num_examples': 10000, 'score': 34737.225987672806, 'total_duration': 36086.324605703354, 'accumulated_submission_time': 34737.225987672806, 'accumulated_eval_time': 1342.7114791870117, 'accumulated_logging_time': 2.8475756645202637}
I0229 15:12:35.374541 139590161643264 logging_writer.py:48] [102162] accumulated_eval_time=1342.711479, accumulated_logging_time=2.847576, accumulated_submission_time=34737.225988, global_step=102162, preemption_count=0, score=34737.225988, test/accuracy=0.558700, test/loss=2.031827, test/num_examples=10000, total_duration=36086.324606, train/accuracy=0.776706, train/loss=0.820210, validation/accuracy=0.688160, validation/loss=1.270859, validation/num_examples=50000
I0229 15:12:48.596747 139590170035968 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.5367209911346436, loss=1.383926510810852
I0229 15:13:22.465256 139590161643264 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.754875898361206, loss=1.418840765953064
I0229 15:13:56.392259 139590170035968 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.5933949947357178, loss=1.3208876848220825
I0229 15:14:30.328452 139590161643264 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.395601272583008, loss=1.2142304182052612
I0229 15:15:04.280847 139590170035968 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.3074896335601807, loss=1.2587392330169678
I0229 15:15:38.236588 139590161643264 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.41582989692688, loss=1.3060166835784912
I0229 15:16:12.170882 139590170035968 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.6541969776153564, loss=1.32339346408844
I0229 15:16:46.126053 139590161643264 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.5858123302459717, loss=1.3242900371551514
I0229 15:17:20.130988 139590170035968 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.6436984539031982, loss=1.3522089719772339
I0229 15:17:54.077517 139590161643264 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.6502199172973633, loss=1.3386248350143433
I0229 15:18:28.038096 139590170035968 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.6711995601654053, loss=1.3876138925552368
I0229 15:19:01.974560 139590161643264 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.889047384262085, loss=1.3582355976104736
I0229 15:19:35.908548 139590170035968 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.413198232650757, loss=1.3540823459625244
I0229 15:20:09.836976 139590161643264 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.478562355041504, loss=1.347066044807434
I0229 15:20:43.773365 139590170035968 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.6008572578430176, loss=1.250622272491455
I0229 15:21:05.637621 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:21:11.648965 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:21:20.051780 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:21:22.360901 139753105983296 submission_runner.py:411] Time since start: 36613.36s, 	Step: 103666, 	{'train/accuracy': 0.7948023080825806, 'train/loss': 0.742831289768219, 'validation/accuracy': 0.6871799826622009, 'validation/loss': 1.2739155292510986, 'validation/num_examples': 50000, 'test/accuracy': 0.5575000047683716, 'test/loss': 2.007575273513794, 'test/num_examples': 10000, 'score': 35247.42342591286, 'total_duration': 36613.36411499977, 'accumulated_submission_time': 35247.42342591286, 'accumulated_eval_time': 1359.4346933364868, 'accumulated_logging_time': 2.912957191467285}
I0229 15:21:22.397950 139590144857856 logging_writer.py:48] [103666] accumulated_eval_time=1359.434693, accumulated_logging_time=2.912957, accumulated_submission_time=35247.423426, global_step=103666, preemption_count=0, score=35247.423426, test/accuracy=0.557500, test/loss=2.007575, test/num_examples=10000, total_duration=36613.364115, train/accuracy=0.794802, train/loss=0.742831, validation/accuracy=0.687180, validation/loss=1.273916, validation/num_examples=50000
I0229 15:21:34.253064 139590153250560 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.476325273513794, loss=1.3265154361724854
I0229 15:22:08.162496 139590144857856 logging_writer.py:48] [103800] global_step=103800, grad_norm=3.0748369693756104, loss=1.3548153638839722
I0229 15:22:42.100253 139590153250560 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.786128520965576, loss=1.3647271394729614
I0229 15:23:16.039370 139590144857856 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.5411758422851562, loss=1.3704251050949097
I0229 15:23:50.060333 139590153250560 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.336195468902588, loss=1.2899998426437378
I0229 15:24:24.014865 139590144857856 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.5815484523773193, loss=1.2652376890182495
I0229 15:24:57.938421 139590153250560 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.647538185119629, loss=1.3014729022979736
I0229 15:25:31.859275 139590144857856 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.7818615436553955, loss=1.2913225889205933
I0229 15:26:05.809774 139590153250560 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.4992599487304688, loss=1.2897679805755615
I0229 15:26:39.766996 139590144857856 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.545170545578003, loss=1.3452143669128418
I0229 15:27:13.695299 139590153250560 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.678603410720825, loss=1.3016815185546875
I0229 15:27:47.610697 139590144857856 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.5959043502807617, loss=1.2718183994293213
I0229 15:28:21.571045 139590153250560 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.688629388809204, loss=1.3160979747772217
I0229 15:28:55.532891 139590144857856 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.706416130065918, loss=1.3677552938461304
I0229 15:29:29.479147 139590153250560 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.821835994720459, loss=1.2775837182998657
I0229 15:29:52.450291 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:29:58.462134 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:30:06.829488 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:30:09.131240 139753105983296 submission_runner.py:411] Time since start: 37140.13s, 	Step: 105169, 	{'train/accuracy': 0.7901785373687744, 'train/loss': 0.779336154460907, 'validation/accuracy': 0.6885799765586853, 'validation/loss': 1.2575336694717407, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 1.9892528057098389, 'test/num_examples': 10000, 'score': 35757.409264564514, 'total_duration': 37140.134474515915, 'accumulated_submission_time': 35757.409264564514, 'accumulated_eval_time': 1376.1155910491943, 'accumulated_logging_time': 2.963505744934082}
I0229 15:30:09.166872 139590161643264 logging_writer.py:48] [105169] accumulated_eval_time=1376.115591, accumulated_logging_time=2.963506, accumulated_submission_time=35757.409265, global_step=105169, preemption_count=0, score=35757.409265, test/accuracy=0.560000, test/loss=1.989253, test/num_examples=10000, total_duration=37140.134475, train/accuracy=0.790179, train/loss=0.779336, validation/accuracy=0.688580, validation/loss=1.257534, validation/num_examples=50000
I0229 15:30:20.001781 139590170035968 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.613997220993042, loss=1.3768353462219238
I0229 15:30:53.899542 139590161643264 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.3921029567718506, loss=1.20017671585083
I0229 15:31:27.860917 139590170035968 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.5806589126586914, loss=1.3114545345306396
I0229 15:32:01.796765 139590161643264 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.645171880722046, loss=1.2713000774383545
I0229 15:32:35.702923 139590170035968 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.64262318611145, loss=1.3113179206848145
I0229 15:33:09.634355 139590161643264 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.8589491844177246, loss=1.3499237298965454
I0229 15:33:43.578350 139590170035968 logging_writer.py:48] [105800] global_step=105800, grad_norm=3.075364112854004, loss=1.4156205654144287
I0229 15:34:17.515010 139590161643264 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.6756820678710938, loss=1.3329846858978271
I0229 15:34:51.445608 139590170035968 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.484692335128784, loss=1.326080083847046
I0229 15:35:25.391989 139590161643264 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.9559388160705566, loss=1.3902544975280762
I0229 15:35:59.520090 139590170035968 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.787865400314331, loss=1.335521936416626
I0229 15:36:33.468075 139590161643264 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.729987382888794, loss=1.3018791675567627
I0229 15:37:07.420493 139590170035968 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.594097137451172, loss=1.30441415309906
I0229 15:37:41.366820 139590161643264 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.572463274002075, loss=1.3712341785430908
I0229 15:38:15.292210 139590170035968 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.8874661922454834, loss=1.3521075248718262
I0229 15:38:39.203778 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:38:45.233775 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:38:53.489180 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:38:55.793203 139753105983296 submission_runner.py:411] Time since start: 37666.80s, 	Step: 106672, 	{'train/accuracy': 0.7915935516357422, 'train/loss': 0.7726999521255493, 'validation/accuracy': 0.6963399648666382, 'validation/loss': 1.2351707220077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.9701182842254639, 'test/num_examples': 10000, 'score': 36267.38300538063, 'total_duration': 37666.79642248154, 'accumulated_submission_time': 36267.38300538063, 'accumulated_eval_time': 1392.7049579620361, 'accumulated_logging_time': 3.009145498275757}
I0229 15:38:55.831769 139589717059328 logging_writer.py:48] [106672] accumulated_eval_time=1392.704958, accumulated_logging_time=3.009145, accumulated_submission_time=36267.383005, global_step=106672, preemption_count=0, score=36267.383005, test/accuracy=0.566300, test/loss=1.970118, test/num_examples=10000, total_duration=37666.796422, train/accuracy=0.791594, train/loss=0.772700, validation/accuracy=0.696340, validation/loss=1.235171, validation/num_examples=50000
I0229 15:39:05.690880 139589725452032 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.751305341720581, loss=1.3677935600280762
I0229 15:39:39.556690 139589717059328 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.6792609691619873, loss=1.4041168689727783
I0229 15:40:13.437847 139589725452032 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.863619327545166, loss=1.3304548263549805
I0229 15:40:47.383667 139589717059328 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.6854677200317383, loss=1.4056743383407593
I0229 15:41:21.311542 139589725452032 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.8841781616210938, loss=1.318994164466858
I0229 15:41:55.345107 139589717059328 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.388000011444092, loss=1.3416157960891724
I0229 15:42:29.260421 139589725452032 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.476881504058838, loss=1.2110769748687744
I0229 15:43:03.169360 139589717059328 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.938120126724243, loss=1.381316065788269
I0229 15:43:37.092919 139589725452032 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.6128652095794678, loss=1.3716912269592285
I0229 15:44:11.027299 139589717059328 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.7132370471954346, loss=1.2897207736968994
I0229 15:44:44.974018 139589725452032 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.88500714302063, loss=1.2429611682891846
I0229 15:45:18.898239 139589717059328 logging_writer.py:48] [107800] global_step=107800, grad_norm=3.030113935470581, loss=1.3178476095199585
I0229 15:45:52.812013 139589725452032 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.595405101776123, loss=1.225890874862671
I0229 15:46:26.762787 139589717059328 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.953993082046509, loss=1.3260657787322998
I0229 15:47:00.677866 139589725452032 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.5704901218414307, loss=1.1847765445709229
I0229 15:47:25.952538 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:47:32.023595 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:47:40.315385 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:47:42.633525 139753105983296 submission_runner.py:411] Time since start: 38193.64s, 	Step: 108176, 	{'train/accuracy': 0.7795758843421936, 'train/loss': 0.8351767063140869, 'validation/accuracy': 0.6855999827384949, 'validation/loss': 1.2827911376953125, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.026097297668457, 'test/num_examples': 10000, 'score': 36777.43883371353, 'total_duration': 38193.63676953316, 'accumulated_submission_time': 36777.43883371353, 'accumulated_eval_time': 1409.3859219551086, 'accumulated_logging_time': 3.06013560295105}
I0229 15:47:42.666487 139590161643264 logging_writer.py:48] [108176] accumulated_eval_time=1409.385922, accumulated_logging_time=3.060136, accumulated_submission_time=36777.438834, global_step=108176, preemption_count=0, score=36777.438834, test/accuracy=0.553300, test/loss=2.026097, test/num_examples=10000, total_duration=38193.636770, train/accuracy=0.779576, train/loss=0.835177, validation/accuracy=0.685600, validation/loss=1.282791, validation/num_examples=50000
I0229 15:47:51.159066 139590170035968 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.601741313934326, loss=1.2872759103775024
I0229 15:48:25.196449 139590161643264 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.5039072036743164, loss=1.2742106914520264
I0229 15:48:59.129538 139590170035968 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.8060755729675293, loss=1.2600483894348145
I0229 15:49:33.074594 139590161643264 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.8823845386505127, loss=1.291029691696167
I0229 15:50:07.009598 139590170035968 logging_writer.py:48] [108600] global_step=108600, grad_norm=3.3184654712677, loss=1.3567843437194824
I0229 15:50:40.954509 139590161643264 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6549224853515625, loss=1.264832854270935
I0229 15:51:14.910181 139590170035968 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.5800836086273193, loss=1.3089210987091064
I0229 15:51:48.853927 139590161643264 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.5970301628112793, loss=1.304154634475708
I0229 15:52:22.776051 139590170035968 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.9156439304351807, loss=1.2467241287231445
I0229 15:52:56.685497 139590161643264 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.875971555709839, loss=1.2889957427978516
I0229 15:53:30.656561 139590170035968 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.5772762298583984, loss=1.3287359476089478
I0229 15:54:04.678500 139590161643264 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.8170180320739746, loss=1.2258367538452148
I0229 15:54:38.616793 139590170035968 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.5714738368988037, loss=1.263686180114746
I0229 15:55:12.551554 139590161643264 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.920339584350586, loss=1.303271770477295
I0229 15:55:46.474948 139590170035968 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.696802854537964, loss=1.3659878969192505
I0229 15:56:12.743654 139753105983296 spec.py:321] Evaluating on the training split.
I0229 15:56:18.757071 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 15:56:27.122001 139753105983296 spec.py:349] Evaluating on the test split.
I0229 15:56:29.351504 139753105983296 submission_runner.py:411] Time since start: 38720.35s, 	Step: 109679, 	{'train/accuracy': 0.7897002100944519, 'train/loss': 0.7726017832756042, 'validation/accuracy': 0.6967399716377258, 'validation/loss': 1.228294849395752, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.9748430252075195, 'test/num_examples': 10000, 'score': 37287.453733205795, 'total_duration': 38720.35473489761, 'accumulated_submission_time': 37287.453733205795, 'accumulated_eval_time': 1425.993721485138, 'accumulated_logging_time': 3.102454423904419}
I0229 15:56:29.391977 139589708666624 logging_writer.py:48] [109679] accumulated_eval_time=1425.993721, accumulated_logging_time=3.102454, accumulated_submission_time=37287.453733, global_step=109679, preemption_count=0, score=37287.453733, test/accuracy=0.567300, test/loss=1.974843, test/num_examples=10000, total_duration=38720.354735, train/accuracy=0.789700, train/loss=0.772602, validation/accuracy=0.696740, validation/loss=1.228295, validation/num_examples=50000
I0229 15:56:36.882253 139589717059328 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.8237428665161133, loss=1.3800321817398071
I0229 15:57:10.735503 139589708666624 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.7560665607452393, loss=1.1751749515533447
I0229 15:57:44.646445 139589717059328 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.6039466857910156, loss=1.2073849439620972
I0229 15:58:18.535847 139589708666624 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.847862720489502, loss=1.2668378353118896
I0229 15:58:52.453015 139589717059328 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.455491065979004, loss=1.1424905061721802
I0229 15:59:26.397175 139589708666624 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.542755365371704, loss=1.2591503858566284
I0229 16:00:00.293127 139589717059328 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.5000905990600586, loss=1.2813585996627808
I0229 16:00:34.396852 139589708666624 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.810098648071289, loss=1.2565704584121704
I0229 16:01:08.311334 139589717059328 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.715386390686035, loss=1.268216609954834
I0229 16:01:42.235311 139589708666624 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.743452787399292, loss=1.2778958082199097
I0229 16:02:16.159923 139589717059328 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.872565507888794, loss=1.371367335319519
I0229 16:02:50.046308 139589708666624 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.837682008743286, loss=1.2061541080474854
I0229 16:03:23.966199 139589717059328 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.799581527709961, loss=1.2944443225860596
I0229 16:03:57.883485 139589708666624 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.867133378982544, loss=1.2994412183761597
I0229 16:04:31.830024 139589717059328 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.827040672302246, loss=1.2819429636001587
I0229 16:04:59.455821 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:05:05.542124 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:05:13.930563 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:05:16.226747 139753105983296 submission_runner.py:411] Time since start: 39247.23s, 	Step: 111183, 	{'train/accuracy': 0.7920718789100647, 'train/loss': 0.7628602385520935, 'validation/accuracy': 0.7012999653816223, 'validation/loss': 1.2078379392623901, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9123485088348389, 'test/num_examples': 10000, 'score': 37797.4540207386, 'total_duration': 39247.22995352745, 'accumulated_submission_time': 37797.4540207386, 'accumulated_eval_time': 1442.764579296112, 'accumulated_logging_time': 3.153353452682495}
I0229 16:05:16.266482 139589717059328 logging_writer.py:48] [111183] accumulated_eval_time=1442.764579, accumulated_logging_time=3.153353, accumulated_submission_time=37797.454021, global_step=111183, preemption_count=0, score=37797.454021, test/accuracy=0.574900, test/loss=1.912349, test/num_examples=10000, total_duration=39247.229954, train/accuracy=0.792072, train/loss=0.762860, validation/accuracy=0.701300, validation/loss=1.207838, validation/num_examples=50000
I0229 16:05:22.363462 139590170035968 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.7509233951568604, loss=1.2317559719085693
I0229 16:05:56.244807 139589717059328 logging_writer.py:48] [111300] global_step=111300, grad_norm=3.3264875411987305, loss=1.2986876964569092
I0229 16:06:30.261496 139590170035968 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.722203016281128, loss=1.2584584951400757
I0229 16:07:04.190763 139589717059328 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.7908360958099365, loss=1.2455660104751587
I0229 16:07:38.099549 139590170035968 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.9493892192840576, loss=1.268045425415039
I0229 16:08:12.032124 139589717059328 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.845623016357422, loss=1.3265986442565918
I0229 16:08:45.957314 139590170035968 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.594707489013672, loss=1.3130983114242554
I0229 16:09:19.903809 139589717059328 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.701638698577881, loss=1.1811977624893188
I0229 16:09:53.850222 139590170035968 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.938361644744873, loss=1.2909033298492432
I0229 16:10:27.778997 139589717059328 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.706951379776001, loss=1.1297693252563477
I0229 16:11:01.722452 139590170035968 logging_writer.py:48] [112200] global_step=112200, grad_norm=3.017488479614258, loss=1.4053318500518799
I0229 16:11:35.658185 139589717059328 logging_writer.py:48] [112300] global_step=112300, grad_norm=3.059870481491089, loss=1.2224209308624268
I0229 16:12:09.619926 139590170035968 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.7737655639648438, loss=1.3107378482818604
I0229 16:12:43.592019 139589717059328 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.850177049636841, loss=1.216675877571106
I0229 16:13:17.537410 139590170035968 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.7591569423675537, loss=1.246660828590393
I0229 16:13:46.555604 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:13:52.581660 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:14:00.827794 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:14:03.128818 139753105983296 submission_runner.py:411] Time since start: 39774.13s, 	Step: 112687, 	{'train/accuracy': 0.816824734210968, 'train/loss': 0.6701944470405579, 'validation/accuracy': 0.6967799663543701, 'validation/loss': 1.2372926473617554, 'validation/num_examples': 50000, 'test/accuracy': 0.5703999996185303, 'test/loss': 1.9807078838348389, 'test/num_examples': 10000, 'score': 38307.6800494194, 'total_duration': 39774.13205599785, 'accumulated_submission_time': 38307.6800494194, 'accumulated_eval_time': 1459.3377630710602, 'accumulated_logging_time': 3.2031338214874268}
I0229 16:14:03.165158 139590144857856 logging_writer.py:48] [112687] accumulated_eval_time=1459.337763, accumulated_logging_time=3.203134, accumulated_submission_time=38307.680049, global_step=112687, preemption_count=0, score=38307.680049, test/accuracy=0.570400, test/loss=1.980708, test/num_examples=10000, total_duration=39774.132056, train/accuracy=0.816825, train/loss=0.670194, validation/accuracy=0.696780, validation/loss=1.237293, validation/num_examples=50000
I0229 16:14:07.912755 139590153250560 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.9081978797912598, loss=1.3221176862716675
I0229 16:14:41.783473 139590144857856 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.7177679538726807, loss=1.3457450866699219
I0229 16:15:15.667440 139590153250560 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.561539888381958, loss=1.1964447498321533
I0229 16:15:49.610076 139590144857856 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.786245107650757, loss=1.2321420907974243
I0229 16:16:23.532636 139590153250560 logging_writer.py:48] [113100] global_step=113100, grad_norm=3.016824245452881, loss=1.2648885250091553
I0229 16:16:57.425329 139590144857856 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.6602413654327393, loss=1.3806893825531006
I0229 16:17:31.330371 139590153250560 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.559911012649536, loss=1.1741664409637451
I0229 16:18:05.219895 139590144857856 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.8747684955596924, loss=1.231886386871338
I0229 16:18:39.154056 139590153250560 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.7551257610321045, loss=1.2923169136047363
I0229 16:19:13.143080 139590144857856 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.9885895252227783, loss=1.2341976165771484
I0229 16:19:47.088274 139590153250560 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.7821755409240723, loss=1.187084436416626
I0229 16:20:21.009045 139590144857856 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.530953884124756, loss=1.124297022819519
I0229 16:20:54.934553 139590153250560 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.791598320007324, loss=1.2253668308258057
I0229 16:21:28.873322 139590144857856 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.7559216022491455, loss=1.2908061742782593
I0229 16:22:02.827221 139590153250560 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.5673863887786865, loss=1.2093384265899658
I0229 16:22:33.192707 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:22:39.273853 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:22:47.484148 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:22:49.829889 139753105983296 submission_runner.py:411] Time since start: 40300.83s, 	Step: 114191, 	{'train/accuracy': 0.807039201259613, 'train/loss': 0.6915223002433777, 'validation/accuracy': 0.6977799534797668, 'validation/loss': 1.2234848737716675, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 1.964166522026062, 'test/num_examples': 10000, 'score': 38817.644235134125, 'total_duration': 40300.83305287361, 'accumulated_submission_time': 38817.644235134125, 'accumulated_eval_time': 1475.9748423099518, 'accumulated_logging_time': 3.2496707439422607}
I0229 16:22:49.866111 139589725452032 logging_writer.py:48] [114191] accumulated_eval_time=1475.974842, accumulated_logging_time=3.249671, accumulated_submission_time=38817.644235, global_step=114191, preemption_count=0, score=38817.644235, test/accuracy=0.570600, test/loss=1.964167, test/num_examples=10000, total_duration=40300.833053, train/accuracy=0.807039, train/loss=0.691522, validation/accuracy=0.697780, validation/loss=1.223485, validation/num_examples=50000
I0229 16:22:53.276706 139590144857856 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.677480697631836, loss=1.2284008264541626
I0229 16:23:27.176241 139589725452032 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.8643600940704346, loss=1.2548022270202637
I0229 16:24:01.058368 139590144857856 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.8915982246398926, loss=1.3037707805633545
I0229 16:24:35.000429 139589725452032 logging_writer.py:48] [114500] global_step=114500, grad_norm=3.0142247676849365, loss=1.3728327751159668
I0229 16:25:09.145295 139590144857856 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.7268359661102295, loss=1.2900068759918213
I0229 16:25:43.096443 139589725452032 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.650336980819702, loss=1.261757731437683
I0229 16:26:17.036564 139590144857856 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.7579140663146973, loss=1.2405705451965332
I0229 16:26:50.955122 139589725452032 logging_writer.py:48] [114900] global_step=114900, grad_norm=3.1938161849975586, loss=1.2187190055847168
I0229 16:27:24.903646 139590144857856 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.9854886531829834, loss=1.1454739570617676
I0229 16:27:58.852830 139589725452032 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.7290096282958984, loss=1.143027663230896
I0229 16:28:32.802730 139590144857856 logging_writer.py:48] [115200] global_step=115200, grad_norm=3.298966646194458, loss=1.3065307140350342
I0229 16:29:06.749327 139589725452032 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.680992841720581, loss=1.1835922002792358
I0229 16:29:40.645251 139590144857856 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.0004353523254395, loss=1.285509705543518
I0229 16:30:14.581036 139589725452032 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.8313634395599365, loss=1.3057180643081665
I0229 16:30:48.538931 139590144857856 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.8670644760131836, loss=1.3136283159255981
I0229 16:31:20.087664 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:31:26.191047 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:31:34.549247 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:31:36.799679 139753105983296 submission_runner.py:411] Time since start: 40827.80s, 	Step: 115694, 	{'train/accuracy': 0.8061423897743225, 'train/loss': 0.6987681984901428, 'validation/accuracy': 0.7060799598693848, 'validation/loss': 1.1993192434310913, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.946460485458374, 'test/num_examples': 10000, 'score': 39327.79904174805, 'total_duration': 40827.80290222168, 'accumulated_submission_time': 39327.79904174805, 'accumulated_eval_time': 1492.6868102550507, 'accumulated_logging_time': 3.298171281814575}
I0229 16:31:36.838574 139590144857856 logging_writer.py:48] [115694] accumulated_eval_time=1492.686810, accumulated_logging_time=3.298171, accumulated_submission_time=39327.799042, global_step=115694, preemption_count=0, score=39327.799042, test/accuracy=0.572200, test/loss=1.946460, test/num_examples=10000, total_duration=40827.802902, train/accuracy=0.806142, train/loss=0.698768, validation/accuracy=0.706080, validation/loss=1.199319, validation/num_examples=50000
I0229 16:31:39.224050 139590153250560 logging_writer.py:48] [115700] global_step=115700, grad_norm=3.0282504558563232, loss=1.2472643852233887
I0229 16:32:13.129059 139590144857856 logging_writer.py:48] [115800] global_step=115800, grad_norm=3.0431394577026367, loss=1.261707067489624
I0229 16:32:47.008345 139590153250560 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.9022486209869385, loss=1.2094337940216064
I0229 16:33:20.944754 139590144857856 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.797208070755005, loss=1.2263476848602295
I0229 16:33:54.904149 139590153250560 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.850215196609497, loss=1.2635612487792969
I0229 16:34:28.841375 139590144857856 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.7259223461151123, loss=1.2426365613937378
I0229 16:35:02.779004 139590153250560 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.887342691421509, loss=1.2408864498138428
I0229 16:35:36.705193 139590144857856 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.804475784301758, loss=1.2734720706939697
I0229 16:36:10.642349 139590153250560 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.890045404434204, loss=1.247415542602539
I0229 16:36:44.574450 139590144857856 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.659649610519409, loss=1.2211161851882935
I0229 16:37:18.602694 139590153250560 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.7334914207458496, loss=1.2741740942001343
I0229 16:37:52.518152 139590144857856 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.9524505138397217, loss=1.2600016593933105
I0229 16:38:26.441258 139590153250560 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.0000343322753906, loss=1.24977445602417
I0229 16:39:00.370895 139590144857856 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.8276140689849854, loss=1.228757381439209
I0229 16:39:34.315260 139590153250560 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.7394206523895264, loss=1.3296122550964355
I0229 16:40:07.045184 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:40:13.046441 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:40:21.294910 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:40:23.599467 139753105983296 submission_runner.py:411] Time since start: 41354.60s, 	Step: 117198, 	{'train/accuracy': 0.7992864847183228, 'train/loss': 0.7257764339447021, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2185938358306885, 'validation/num_examples': 50000, 'test/accuracy': 0.5745000243186951, 'test/loss': 1.972318410873413, 'test/num_examples': 10000, 'score': 39837.94040846825, 'total_duration': 41354.602694272995, 'accumulated_submission_time': 39837.94040846825, 'accumulated_eval_time': 1509.241043329239, 'accumulated_logging_time': 3.349168300628662}
I0229 16:40:23.639202 139589725452032 logging_writer.py:48] [117198] accumulated_eval_time=1509.241043, accumulated_logging_time=3.349168, accumulated_submission_time=39837.940408, global_step=117198, preemption_count=0, score=39837.940408, test/accuracy=0.574500, test/loss=1.972318, test/num_examples=10000, total_duration=41354.602694, train/accuracy=0.799286, train/loss=0.725776, validation/accuracy=0.701260, validation/loss=1.218594, validation/num_examples=50000
I0229 16:40:24.671225 139590144857856 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.873065948486328, loss=1.3306796550750732
I0229 16:40:58.547180 139589725452032 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.684007406234741, loss=1.2148723602294922
I0229 16:41:32.465518 139590144857856 logging_writer.py:48] [117400] global_step=117400, grad_norm=3.112776041030884, loss=1.1617064476013184
I0229 16:42:06.430260 139589725452032 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.8590569496154785, loss=1.126204252243042
I0229 16:42:40.373936 139590144857856 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.815908670425415, loss=1.1901308298110962
I0229 16:43:14.318974 139589725452032 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.7730634212493896, loss=1.1976869106292725
I0229 16:43:48.366887 139590144857856 logging_writer.py:48] [117800] global_step=117800, grad_norm=3.2767810821533203, loss=1.2632365226745605
I0229 16:44:22.294692 139589725452032 logging_writer.py:48] [117900] global_step=117900, grad_norm=3.039266586303711, loss=1.187010407447815
I0229 16:44:56.251626 139590144857856 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.7594547271728516, loss=1.1828511953353882
I0229 16:45:30.207582 139589725452032 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.783298969268799, loss=1.2527968883514404
I0229 16:46:04.151118 139590144857856 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8510990142822266, loss=1.1965380907058716
I0229 16:46:38.097128 139589725452032 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.802964925765991, loss=1.1429030895233154
I0229 16:47:12.041512 139590144857856 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.820770263671875, loss=1.250436782836914
I0229 16:47:45.996343 139589725452032 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.297938346862793, loss=1.201146125793457
I0229 16:48:19.967737 139590144857856 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.3550779819488525, loss=1.3369760513305664
I0229 16:48:53.936717 139589725452032 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9405601024627686, loss=1.1660457849502563
I0229 16:48:53.944731 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:48:59.920301 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:49:08.265455 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:49:10.564726 139753105983296 submission_runner.py:411] Time since start: 41881.57s, 	Step: 118701, 	{'train/accuracy': 0.8131775856018066, 'train/loss': 0.685913622379303, 'validation/accuracy': 0.7095999717712402, 'validation/loss': 1.1763111352920532, 'validation/num_examples': 50000, 'test/accuracy': 0.5827000141143799, 'test/loss': 1.896999478340149, 'test/num_examples': 10000, 'score': 40348.18306827545, 'total_duration': 41881.56796193123, 'accumulated_submission_time': 40348.18306827545, 'accumulated_eval_time': 1525.8609673976898, 'accumulated_logging_time': 3.40002703666687}
I0229 16:49:10.603751 139590153250560 logging_writer.py:48] [118701] accumulated_eval_time=1525.860967, accumulated_logging_time=3.400027, accumulated_submission_time=40348.183068, global_step=118701, preemption_count=0, score=40348.183068, test/accuracy=0.582700, test/loss=1.896999, test/num_examples=10000, total_duration=41881.567962, train/accuracy=0.813178, train/loss=0.685914, validation/accuracy=0.709600, validation/loss=1.176311, validation/num_examples=50000
I0229 16:49:44.672764 139590161643264 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.781292676925659, loss=1.168334722518921
I0229 16:50:18.580398 139590153250560 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.8305273056030273, loss=1.2672863006591797
I0229 16:50:52.527751 139590161643264 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.7350451946258545, loss=1.0848159790039062
I0229 16:51:26.446382 139590153250560 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.387561559677124, loss=1.3616631031036377
I0229 16:52:00.377176 139590161643264 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.9502742290496826, loss=1.1769791841506958
I0229 16:52:34.295542 139590153250560 logging_writer.py:48] [119300] global_step=119300, grad_norm=3.0200486183166504, loss=1.1834852695465088
I0229 16:53:08.203622 139590161643264 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.810267448425293, loss=1.1820952892303467
I0229 16:53:42.140877 139590153250560 logging_writer.py:48] [119500] global_step=119500, grad_norm=3.2394609451293945, loss=1.2279406785964966
I0229 16:54:16.105696 139590161643264 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.011289119720459, loss=1.2149220705032349
I0229 16:54:50.034983 139590153250560 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.7696282863616943, loss=1.1946754455566406
I0229 16:55:23.946831 139590161643264 logging_writer.py:48] [119800] global_step=119800, grad_norm=3.0895538330078125, loss=1.137717843055725
I0229 16:55:57.995087 139590153250560 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.1756339073181152, loss=1.1942731142044067
I0229 16:56:31.972074 139590161643264 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.735926628112793, loss=1.1773253679275513
I0229 16:57:05.910982 139590153250560 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.9179766178131104, loss=1.214236855506897
I0229 16:57:39.789031 139590161643264 logging_writer.py:48] [120200] global_step=120200, grad_norm=3.345414876937866, loss=1.1727824211120605
I0229 16:57:40.620205 139753105983296 spec.py:321] Evaluating on the training split.
I0229 16:57:46.717339 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 16:57:55.084597 139753105983296 spec.py:349] Evaluating on the test split.
I0229 16:57:57.307267 139753105983296 submission_runner.py:411] Time since start: 42408.31s, 	Step: 120204, 	{'train/accuracy': 0.8085139989852905, 'train/loss': 0.6942028999328613, 'validation/accuracy': 0.7087599635124207, 'validation/loss': 1.1854325532913208, 'validation/num_examples': 50000, 'test/accuracy': 0.5776000022888184, 'test/loss': 1.9270565509796143, 'test/num_examples': 10000, 'score': 40858.13369560242, 'total_duration': 42408.31049633026, 'accumulated_submission_time': 40858.13369560242, 'accumulated_eval_time': 1542.5479755401611, 'accumulated_logging_time': 3.452460289001465}
I0229 16:57:57.347558 139589717059328 logging_writer.py:48] [120204] accumulated_eval_time=1542.547976, accumulated_logging_time=3.452460, accumulated_submission_time=40858.133696, global_step=120204, preemption_count=0, score=40858.133696, test/accuracy=0.577600, test/loss=1.927057, test/num_examples=10000, total_duration=42408.310496, train/accuracy=0.808514, train/loss=0.694203, validation/accuracy=0.708760, validation/loss=1.185433, validation/num_examples=50000
I0229 16:58:30.215576 139589725452032 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.052236318588257, loss=1.249289870262146
I0229 16:59:04.146575 139589717059328 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.1559131145477295, loss=1.2361884117126465
I0229 16:59:38.135988 139589725452032 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.896512031555176, loss=1.2282236814498901
I0229 17:00:12.083451 139589717059328 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.8669238090515137, loss=1.2505764961242676
I0229 17:00:46.027462 139589725452032 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.997549533843994, loss=1.1657216548919678
I0229 17:01:20.002139 139589717059328 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.1695210933685303, loss=1.3172463178634644
I0229 17:01:54.032297 139589725452032 logging_writer.py:48] [120900] global_step=120900, grad_norm=3.4426517486572266, loss=1.2329185009002686
I0229 17:02:27.993225 139589717059328 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.13128924369812, loss=1.2464271783828735
I0229 17:03:01.931563 139589725452032 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.91629958152771, loss=1.1704756021499634
I0229 17:03:35.882512 139589717059328 logging_writer.py:48] [121200] global_step=121200, grad_norm=3.099613904953003, loss=1.1579333543777466
I0229 17:04:09.807021 139589725452032 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.1381633281707764, loss=1.176875114440918
I0229 17:04:43.771035 139589717059328 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.0949506759643555, loss=1.2721078395843506
I0229 17:05:17.747149 139589725452032 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.90708589553833, loss=1.1385318040847778
I0229 17:05:51.716548 139589717059328 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.0165374279022217, loss=1.1641452312469482
I0229 17:06:25.686479 139589725452032 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.185624361038208, loss=1.1823101043701172
I0229 17:06:27.534147 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:06:33.571897 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:06:41.898019 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:06:44.285197 139753105983296 submission_runner.py:411] Time since start: 42935.29s, 	Step: 121707, 	{'train/accuracy': 0.8465999364852905, 'train/loss': 0.56064772605896, 'validation/accuracy': 0.7129799723625183, 'validation/loss': 1.1660146713256836, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.906941294670105, 'test/num_examples': 10000, 'score': 41368.25840616226, 'total_duration': 42935.28839254379, 'accumulated_submission_time': 41368.25840616226, 'accumulated_eval_time': 1559.2989346981049, 'accumulated_logging_time': 3.5025715827941895}
I0229 17:06:44.328754 139590161643264 logging_writer.py:48] [121707] accumulated_eval_time=1559.298935, accumulated_logging_time=3.502572, accumulated_submission_time=41368.258406, global_step=121707, preemption_count=0, score=41368.258406, test/accuracy=0.579200, test/loss=1.906941, test/num_examples=10000, total_duration=42935.288393, train/accuracy=0.846600, train/loss=0.560648, validation/accuracy=0.712980, validation/loss=1.166015, validation/num_examples=50000
I0229 17:07:16.175175 139590170035968 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.13515043258667, loss=1.2586264610290527
I0229 17:07:50.071665 139590161643264 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.0546298027038574, loss=1.2276878356933594
I0229 17:08:24.079848 139590170035968 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.1627840995788574, loss=1.243478775024414
I0229 17:08:58.011269 139590161643264 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.176406145095825, loss=1.1490854024887085
I0229 17:09:31.944402 139590170035968 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.138310670852661, loss=1.3037971258163452
I0229 17:10:05.835398 139590161643264 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.010270833969116, loss=1.2148985862731934
I0229 17:10:39.780684 139590170035968 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.0873711109161377, loss=1.1387512683868408
I0229 17:11:13.708902 139590161643264 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.9325459003448486, loss=1.1981093883514404
I0229 17:11:47.608178 139590170035968 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.15078067779541, loss=1.174545168876648
I0229 17:12:21.561581 139590161643264 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.889904022216797, loss=1.1987463235855103
I0229 17:12:55.509262 139590170035968 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.0113885402679443, loss=1.1442512273788452
I0229 17:13:29.463602 139590161643264 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.011864423751831, loss=1.1997991800308228
I0229 17:14:03.449405 139590170035968 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.9203689098358154, loss=1.1713449954986572
I0229 17:14:37.351884 139590161643264 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.1209867000579834, loss=1.1317981481552124
I0229 17:15:11.271235 139590170035968 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.9905827045440674, loss=1.1715866327285767
I0229 17:15:14.454664 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:15:20.460219 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:15:28.762694 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:15:31.081413 139753105983296 submission_runner.py:411] Time since start: 43462.08s, 	Step: 123211, 	{'train/accuracy': 0.8325493931770325, 'train/loss': 0.5962198376655579, 'validation/accuracy': 0.7136200070381165, 'validation/loss': 1.1775128841400146, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.8746788501739502, 'test/num_examples': 10000, 'score': 41878.31901669502, 'total_duration': 43462.08464598656, 'accumulated_submission_time': 41878.31901669502, 'accumulated_eval_time': 1575.9256281852722, 'accumulated_logging_time': 3.5578701496124268}
I0229 17:15:31.122813 139590144857856 logging_writer.py:48] [123211] accumulated_eval_time=1575.925628, accumulated_logging_time=3.557870, accumulated_submission_time=41878.319017, global_step=123211, preemption_count=0, score=41878.319017, test/accuracy=0.587500, test/loss=1.874679, test/num_examples=10000, total_duration=43462.084646, train/accuracy=0.832549, train/loss=0.596220, validation/accuracy=0.713620, validation/loss=1.177513, validation/num_examples=50000
I0229 17:16:01.620717 139590153250560 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.224278688430786, loss=1.1678409576416016
I0229 17:16:35.550433 139590144857856 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.171384811401367, loss=1.2392957210540771
I0229 17:17:09.486533 139590153250560 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.1177265644073486, loss=1.1934152841567993
I0229 17:17:43.442293 139590144857856 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.0270698070526123, loss=1.1131988763809204
I0229 17:18:17.371569 139590153250560 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.7185170650482178, loss=1.0470212697982788
I0229 17:18:51.296527 139590144857856 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.9870588779449463, loss=1.139707088470459
I0229 17:19:25.228262 139590153250560 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.182386636734009, loss=1.2001748085021973
I0229 17:19:59.185608 139590144857856 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.149164915084839, loss=1.1280543804168701
I0229 17:20:33.226662 139590153250560 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.0604796409606934, loss=1.201361894607544
I0229 17:21:07.154006 139590144857856 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.9851956367492676, loss=1.0785365104675293
I0229 17:21:41.094822 139590153250560 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.1042168140411377, loss=1.1186106204986572
I0229 17:22:15.054952 139590144857856 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.8770320415496826, loss=1.1495158672332764
I0229 17:22:48.988987 139590153250560 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.1877284049987793, loss=1.196897268295288
I0229 17:23:22.951387 139590144857856 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.891674041748047, loss=1.1298497915267944
I0229 17:23:56.931756 139590153250560 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.178849220275879, loss=1.1400152444839478
I0229 17:24:01.143774 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:24:07.255958 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:24:15.542669 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:24:17.953502 139753105983296 submission_runner.py:411] Time since start: 43988.96s, 	Step: 124714, 	{'train/accuracy': 0.8289819955825806, 'train/loss': 0.6051926016807556, 'validation/accuracy': 0.7158199548721313, 'validation/loss': 1.1632517576217651, 'validation/num_examples': 50000, 'test/accuracy': 0.5853000283241272, 'test/loss': 1.8980510234832764, 'test/num_examples': 10000, 'score': 42388.27456307411, 'total_duration': 43988.95673203468, 'accumulated_submission_time': 42388.27456307411, 'accumulated_eval_time': 1592.7352993488312, 'accumulated_logging_time': 3.611999750137329}
I0229 17:24:17.991933 139589717059328 logging_writer.py:48] [124714] accumulated_eval_time=1592.735299, accumulated_logging_time=3.612000, accumulated_submission_time=42388.274563, global_step=124714, preemption_count=0, score=42388.274563, test/accuracy=0.585300, test/loss=1.898051, test/num_examples=10000, total_duration=43988.956732, train/accuracy=0.828982, train/loss=0.605193, validation/accuracy=0.715820, validation/loss=1.163252, validation/num_examples=50000
I0229 17:24:47.501755 139589725452032 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.021332263946533, loss=1.076285719871521
I0229 17:25:21.405327 139589717059328 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.267489433288574, loss=1.1238112449645996
I0229 17:25:55.343133 139589725452032 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.1802597045898438, loss=1.0797362327575684
I0229 17:26:29.357686 139589717059328 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.0327649116516113, loss=1.1752564907073975
I0229 17:27:03.275589 139589725452032 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.208298921585083, loss=1.1343212127685547
I0229 17:27:37.173205 139589717059328 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.053131103515625, loss=1.1370248794555664
I0229 17:28:11.102124 139589725452032 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.1790802478790283, loss=1.1392738819122314
I0229 17:28:45.021949 139589717059328 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.081793785095215, loss=1.0523555278778076
I0229 17:29:18.942329 139589725452032 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.2385337352752686, loss=1.1550136804580688
I0229 17:29:52.888270 139589717059328 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.992542028427124, loss=1.171639323234558
I0229 17:30:26.816442 139589725452032 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.2827439308166504, loss=1.1682064533233643
I0229 17:31:00.759796 139589717059328 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.1590170860290527, loss=1.111600637435913
I0229 17:31:34.699749 139589725452032 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.1720359325408936, loss=1.077208399772644
I0229 17:32:08.603281 139589717059328 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.4476211071014404, loss=1.1509976387023926
I0229 17:32:42.623085 139589725452032 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.1375160217285156, loss=1.1455981731414795
I0229 17:32:48.196736 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:32:54.209159 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:33:02.528394 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:33:04.802167 139753105983296 submission_runner.py:411] Time since start: 44515.81s, 	Step: 126218, 	{'train/accuracy': 0.8241987824440002, 'train/loss': 0.6316837072372437, 'validation/accuracy': 0.7112999558448792, 'validation/loss': 1.1798099279403687, 'validation/num_examples': 50000, 'test/accuracy': 0.5856000185012817, 'test/loss': 1.908242106437683, 'test/num_examples': 10000, 'score': 42898.41350364685, 'total_duration': 44515.80540394783, 'accumulated_submission_time': 42898.41350364685, 'accumulated_eval_time': 1609.340696811676, 'accumulated_logging_time': 3.663426637649536}
I0229 17:33:04.843957 139590170035968 logging_writer.py:48] [126218] accumulated_eval_time=1609.340697, accumulated_logging_time=3.663427, accumulated_submission_time=42898.413504, global_step=126218, preemption_count=0, score=42898.413504, test/accuracy=0.585600, test/loss=1.908242, test/num_examples=10000, total_duration=44515.805404, train/accuracy=0.824199, train/loss=0.631684, validation/accuracy=0.711300, validation/loss=1.179810, validation/num_examples=50000
I0229 17:33:32.974097 139590178428672 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.174682378768921, loss=1.0925822257995605
I0229 17:34:06.910113 139590170035968 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.8264293670654297, loss=1.138604760169983
I0229 17:34:40.874506 139590178428672 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.07558012008667, loss=1.1756336688995361
I0229 17:35:14.818729 139590170035968 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.8493268489837646, loss=1.2038280963897705
I0229 17:35:48.719075 139590178428672 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.251922607421875, loss=1.106865406036377
I0229 17:36:22.648100 139590170035968 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.1034936904907227, loss=1.1046934127807617
I0229 17:36:56.618040 139590178428672 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.430492401123047, loss=1.1514252424240112
I0229 17:37:30.556863 139590170035968 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.1616766452789307, loss=1.1141626834869385
I0229 17:38:04.499386 139590178428672 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.161954879760742, loss=1.1414453983306885
I0229 17:38:38.786131 139590170035968 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.94321346282959, loss=1.0104482173919678
I0229 17:39:12.698203 139590178428672 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.6546754837036133, loss=1.176802158355713
I0229 17:39:46.625883 139590170035968 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.0914394855499268, loss=1.1194425821304321
I0229 17:40:20.571291 139590178428672 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.1687610149383545, loss=1.0858654975891113
I0229 17:40:54.527150 139590170035968 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.2008674144744873, loss=1.1307246685028076
I0229 17:41:28.464734 139590178428672 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.1277589797973633, loss=1.1263649463653564
I0229 17:41:35.034921 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:41:41.096307 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:41:49.345621 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:41:51.673404 139753105983296 submission_runner.py:411] Time since start: 45042.68s, 	Step: 127721, 	{'train/accuracy': 0.8290417790412903, 'train/loss': 0.6082040667533875, 'validation/accuracy': 0.7193999886512756, 'validation/loss': 1.1457984447479248, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.8633819818496704, 'test/num_examples': 10000, 'score': 43408.54190301895, 'total_duration': 45042.67664170265, 'accumulated_submission_time': 43408.54190301895, 'accumulated_eval_time': 1625.9791305065155, 'accumulated_logging_time': 3.715073585510254}
I0229 17:41:51.715541 139589708666624 logging_writer.py:48] [127721] accumulated_eval_time=1625.979131, accumulated_logging_time=3.715074, accumulated_submission_time=43408.541903, global_step=127721, preemption_count=0, score=43408.541903, test/accuracy=0.592400, test/loss=1.863382, test/num_examples=10000, total_duration=45042.676642, train/accuracy=0.829042, train/loss=0.608204, validation/accuracy=0.719400, validation/loss=1.145798, validation/num_examples=50000
I0229 17:42:18.829929 139589717059328 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.025491714477539, loss=1.1155191659927368
I0229 17:42:52.751710 139589708666624 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.1875362396240234, loss=1.1427240371704102
I0229 17:43:26.656428 139589717059328 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.2324092388153076, loss=0.9897080659866333
I0229 17:44:00.573428 139589708666624 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.3788657188415527, loss=1.0406460762023926
I0229 17:44:34.596815 139589717059328 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.2405381202697754, loss=1.0421758890151978
I0229 17:45:08.534814 139589708666624 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.2876954078674316, loss=1.165145993232727
I0229 17:45:42.450363 139589717059328 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.4634740352630615, loss=1.0981695652008057
I0229 17:46:16.367665 139589708666624 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.4321141242980957, loss=1.0901421308517456
I0229 17:46:50.316198 139589717059328 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.0816597938537598, loss=1.1493523120880127
I0229 17:47:24.252180 139589708666624 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.3530795574188232, loss=1.1525226831436157
I0229 17:47:58.172306 139589717059328 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.04093861579895, loss=1.1093635559082031
I0229 17:48:32.105727 139589708666624 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.283987522125244, loss=1.118342638015747
I0229 17:49:06.059311 139589717059328 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.0993638038635254, loss=1.0673813819885254
I0229 17:49:39.992417 139589708666624 logging_writer.py:48] [129100] global_step=129100, grad_norm=4.104764938354492, loss=1.132873773574829
I0229 17:50:13.903068 139589717059328 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.128363609313965, loss=1.072627305984497
I0229 17:50:21.878056 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:50:27.877838 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:50:36.215409 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:50:38.494071 139753105983296 submission_runner.py:411] Time since start: 45569.50s, 	Step: 129225, 	{'train/accuracy': 0.817781388759613, 'train/loss': 0.6508827209472656, 'validation/accuracy': 0.7068399786949158, 'validation/loss': 1.1904618740081787, 'validation/num_examples': 50000, 'test/accuracy': 0.5823000073432922, 'test/loss': 1.93605637550354, 'test/num_examples': 10000, 'score': 43918.640490055084, 'total_duration': 45569.49729180336, 'accumulated_submission_time': 43918.640490055084, 'accumulated_eval_time': 1642.5950877666473, 'accumulated_logging_time': 3.7680022716522217}
I0229 17:50:38.534090 139590161643264 logging_writer.py:48] [129225] accumulated_eval_time=1642.595088, accumulated_logging_time=3.768002, accumulated_submission_time=43918.640490, global_step=129225, preemption_count=0, score=43918.640490, test/accuracy=0.582300, test/loss=1.936056, test/num_examples=10000, total_duration=45569.497292, train/accuracy=0.817781, train/loss=0.650883, validation/accuracy=0.706840, validation/loss=1.190462, validation/num_examples=50000
I0229 17:51:04.364145 139590170035968 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.123180627822876, loss=1.0247306823730469
I0229 17:51:38.279461 139590161643264 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.2515246868133545, loss=1.0285733938217163
I0229 17:52:12.251890 139590170035968 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.1911613941192627, loss=1.1077090501785278
I0229 17:52:46.195206 139590161643264 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.065004587173462, loss=1.040497899055481
I0229 17:53:20.144455 139590170035968 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.3054275512695312, loss=1.1399195194244385
I0229 17:53:54.117173 139590161643264 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.18634295463562, loss=1.0950038433074951
I0229 17:54:28.035337 139590170035968 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.2137818336486816, loss=1.091207504272461
I0229 17:55:01.978972 139590161643264 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.311786413192749, loss=1.120348334312439
I0229 17:55:35.925920 139590170035968 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.0126078128814697, loss=1.0344409942626953
I0229 17:56:09.874711 139590161643264 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.571584701538086, loss=1.1390258073806763
I0229 17:56:43.851691 139590170035968 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.12648868560791, loss=1.0789015293121338
I0229 17:57:17.847573 139590161643264 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.8463821411132812, loss=1.2184228897094727
I0229 17:57:51.815021 139590170035968 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.3751165866851807, loss=1.13875412940979
I0229 17:58:25.767277 139590161643264 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.5060300827026367, loss=1.0978634357452393
I0229 17:58:59.715170 139590170035968 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.1216678619384766, loss=1.0368156433105469
I0229 17:59:08.674673 139753105983296 spec.py:321] Evaluating on the training split.
I0229 17:59:14.688658 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 17:59:22.933477 139753105983296 spec.py:349] Evaluating on the test split.
I0229 17:59:25.239750 139753105983296 submission_runner.py:411] Time since start: 46096.24s, 	Step: 130728, 	{'train/accuracy': 0.8703563213348389, 'train/loss': 0.4560602903366089, 'validation/accuracy': 0.7164199948310852, 'validation/loss': 1.169049859046936, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.8778047561645508, 'test/num_examples': 10000, 'score': 44428.71792554855, 'total_duration': 46096.24298453331, 'accumulated_submission_time': 44428.71792554855, 'accumulated_eval_time': 1659.1601164340973, 'accumulated_logging_time': 3.817939519882202}
I0229 17:59:25.279340 139590144857856 logging_writer.py:48] [130728] accumulated_eval_time=1659.160116, accumulated_logging_time=3.817940, accumulated_submission_time=44428.717926, global_step=130728, preemption_count=0, score=44428.717926, test/accuracy=0.594300, test/loss=1.877805, test/num_examples=10000, total_duration=46096.242985, train/accuracy=0.870356, train/loss=0.456060, validation/accuracy=0.716420, validation/loss=1.169050, validation/num_examples=50000
I0229 17:59:50.046512 139590153250560 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.4483275413513184, loss=1.0985944271087646
I0229 18:00:23.954718 139590144857856 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.5643646717071533, loss=1.1083214282989502
I0229 18:00:57.884531 139590153250560 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.429542303085327, loss=1.0969288349151611
I0229 18:01:31.823881 139590144857856 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.354553699493408, loss=0.9989458322525024
I0229 18:02:05.774819 139590153250560 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.268123149871826, loss=1.0793613195419312
I0229 18:02:39.739060 139590144857856 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.379122734069824, loss=1.1284339427947998
I0229 18:03:13.752583 139590153250560 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.3076694011688232, loss=1.1264880895614624
I0229 18:03:47.715498 139590144857856 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.4228057861328125, loss=1.0822229385375977
I0229 18:04:21.643941 139590153250560 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.461573600769043, loss=1.0792691707611084
I0229 18:04:55.580921 139590144857856 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.3433101177215576, loss=1.115901231765747
I0229 18:05:29.526626 139590153250560 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.3372485637664795, loss=1.051545262336731
I0229 18:06:03.466467 139590144857856 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.287235736846924, loss=1.0643401145935059
I0229 18:06:37.397895 139590153250560 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.49605393409729, loss=1.1206073760986328
I0229 18:07:11.344376 139590144857856 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.3705575466156006, loss=1.145107388496399
I0229 18:07:45.286630 139590153250560 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.2956173419952393, loss=0.999654233455658
I0229 18:07:55.585341 139753105983296 spec.py:321] Evaluating on the training split.
I0229 18:08:01.667208 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 18:08:09.918818 139753105983296 spec.py:349] Evaluating on the test split.
I0229 18:08:12.183443 139753105983296 submission_runner.py:411] Time since start: 46623.19s, 	Step: 132232, 	{'train/accuracy': 0.8533163070678711, 'train/loss': 0.5247998833656311, 'validation/accuracy': 0.7186799645423889, 'validation/loss': 1.150518774986267, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8805676698684692, 'test/num_examples': 10000, 'score': 44938.95955300331, 'total_duration': 46623.186673641205, 'accumulated_submission_time': 44938.95955300331, 'accumulated_eval_time': 1675.7581803798676, 'accumulated_logging_time': 3.868992805480957}
I0229 18:08:12.229395 139589717059328 logging_writer.py:48] [132232] accumulated_eval_time=1675.758180, accumulated_logging_time=3.868993, accumulated_submission_time=44938.959553, global_step=132232, preemption_count=0, score=44938.959553, test/accuracy=0.591800, test/loss=1.880568, test/num_examples=10000, total_duration=46623.186674, train/accuracy=0.853316, train/loss=0.524800, validation/accuracy=0.718680, validation/loss=1.150519, validation/num_examples=50000
I0229 18:08:35.623129 139589725452032 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.3423285484313965, loss=1.0598556995391846
I0229 18:09:09.586903 139589717059328 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.1121628284454346, loss=0.9847007393836975
I0229 18:09:43.515762 139589725452032 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.385045051574707, loss=1.0844038724899292
I0229 18:10:17.462972 139589717059328 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.7447550296783447, loss=1.0216021537780762
I0229 18:10:51.400949 139589725452032 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.503061056137085, loss=1.1509802341461182
I0229 18:11:25.361649 139589717059328 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.3308467864990234, loss=1.0962926149368286
I0229 18:11:59.307208 139589725452032 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.47491455078125, loss=1.0946635007858276
I0229 18:12:33.211971 139589717059328 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.361647844314575, loss=1.1017616987228394
I0229 18:13:07.164061 139589725452032 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.04439377784729, loss=1.0156911611557007
I0229 18:13:41.097340 139589717059328 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.5842630863189697, loss=1.1416821479797363
I0229 18:14:15.049017 139589725452032 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.3873376846313477, loss=1.0868163108825684
I0229 18:14:49.007409 139589717059328 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.3500783443450928, loss=1.0708523988723755
I0229 18:15:23.036610 139589725452032 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.4089395999908447, loss=1.0282551050186157
I0229 18:15:57.011142 139589717059328 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.4230165481567383, loss=1.087632179260254
I0229 18:16:30.960517 139589725452032 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.396915912628174, loss=1.0258721113204956
I0229 18:16:42.314754 139753105983296 spec.py:321] Evaluating on the training split.
I0229 18:16:48.296146 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 18:16:56.603224 139753105983296 spec.py:349] Evaluating on the test split.
I0229 18:16:58.838691 139753105983296 submission_runner.py:411] Time since start: 47149.84s, 	Step: 133735, 	{'train/accuracy': 0.8556680083274841, 'train/loss': 0.5154725313186646, 'validation/accuracy': 0.7227599620819092, 'validation/loss': 1.1428040266036987, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.8898875713348389, 'test/num_examples': 10000, 'score': 45448.979655981064, 'total_duration': 47149.84192085266, 'accumulated_submission_time': 45448.979655981064, 'accumulated_eval_time': 1692.2820625305176, 'accumulated_logging_time': 3.927493095397949}
I0229 18:16:58.881182 139590178428672 logging_writer.py:48] [133735] accumulated_eval_time=1692.282063, accumulated_logging_time=3.927493, accumulated_submission_time=45448.979656, global_step=133735, preemption_count=0, score=45448.979656, test/accuracy=0.594100, test/loss=1.889888, test/num_examples=10000, total_duration=47149.841921, train/accuracy=0.855668, train/loss=0.515473, validation/accuracy=0.722760, validation/loss=1.142804, validation/num_examples=50000
I0229 18:17:21.260875 139590186821376 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.385453462600708, loss=0.982554018497467
I0229 18:17:55.184454 139590178428672 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.459747552871704, loss=1.0750770568847656
I0229 18:18:29.081336 139590186821376 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.2951314449310303, loss=1.0436689853668213
I0229 18:19:02.987508 139590178428672 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.650411367416382, loss=0.9912002682685852
I0229 18:19:36.895822 139590186821376 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.6120615005493164, loss=1.05454683303833
I0229 18:20:10.824259 139590178428672 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.318413496017456, loss=0.9819097518920898
I0229 18:20:44.766967 139590186821376 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.3348662853240967, loss=1.0839259624481201
I0229 18:21:18.713102 139590178428672 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.4616329669952393, loss=1.0669296979904175
I0229 18:21:52.737566 139590186821376 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3739919662475586, loss=1.1268845796585083
I0229 18:22:26.692952 139590178428672 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.3989412784576416, loss=1.0620489120483398
I0229 18:23:00.650022 139590186821376 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.4922454357147217, loss=1.0174286365509033
I0229 18:23:34.621903 139590178428672 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.4639203548431396, loss=1.0445857048034668
I0229 18:24:08.570898 139590186821376 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.2908482551574707, loss=1.0026850700378418
I0229 18:24:42.487418 139590178428672 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.591047763824463, loss=1.020498514175415
I0229 18:25:16.431631 139590186821376 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.604900598526001, loss=0.9789882898330688
I0229 18:25:29.140626 139753105983296 spec.py:321] Evaluating on the training split.
I0229 18:25:35.167886 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 18:25:43.380186 139753105983296 spec.py:349] Evaluating on the test split.
I0229 18:25:45.687784 139753105983296 submission_runner.py:411] Time since start: 47676.69s, 	Step: 135239, 	{'train/accuracy': 0.8517617583274841, 'train/loss': 0.5228502750396729, 'validation/accuracy': 0.7201799750328064, 'validation/loss': 1.1425071954727173, 'validation/num_examples': 50000, 'test/accuracy': 0.5942000150680542, 'test/loss': 1.8866969347000122, 'test/num_examples': 10000, 'score': 45959.17137622833, 'total_duration': 47676.69100522995, 'accumulated_submission_time': 45959.17137622833, 'accumulated_eval_time': 1708.8291599750519, 'accumulated_logging_time': 3.9842641353607178}
I0229 18:25:45.728559 139590144857856 logging_writer.py:48] [135239] accumulated_eval_time=1708.829160, accumulated_logging_time=3.984264, accumulated_submission_time=45959.171376, global_step=135239, preemption_count=0, score=45959.171376, test/accuracy=0.594200, test/loss=1.886697, test/num_examples=10000, total_duration=47676.691005, train/accuracy=0.851762, train/loss=0.522850, validation/accuracy=0.720180, validation/loss=1.142507, validation/num_examples=50000
I0229 18:26:06.766238 139590153250560 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.561513662338257, loss=0.9553185701370239
I0229 18:26:40.697130 139590144857856 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.415464401245117, loss=0.9890676736831665
I0229 18:27:14.595046 139590153250560 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.5833635330200195, loss=1.10174560546875
I0229 18:27:48.623505 139590144857856 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.399013042449951, loss=0.9917645454406738
I0229 18:28:22.576937 139590153250560 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.464343309402466, loss=1.0089962482452393
I0229 18:28:56.554200 139590144857856 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.1802830696105957, loss=1.0149004459381104
I0229 18:29:30.509380 139590153250560 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.5908241271972656, loss=0.9797831773757935
I0229 18:30:04.432450 139590144857856 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.4864606857299805, loss=1.0267210006713867
I0229 18:30:38.374961 139590153250560 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.5033109188079834, loss=1.074116587638855
I0229 18:31:12.337676 139590144857856 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.416713237762451, loss=0.9737241268157959
I0229 18:31:46.338097 139590153250560 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.7570676803588867, loss=1.14463472366333
I0229 18:32:20.293521 139590144857856 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.7465479373931885, loss=1.0632257461547852
I0229 18:32:54.276743 139590153250560 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.440505266189575, loss=1.0142446756362915
I0229 18:33:28.208172 139590144857856 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.708918809890747, loss=1.0451406240463257
I0229 18:34:02.238842 139590153250560 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.3778493404388428, loss=0.9783011674880981
I0229 18:34:15.974460 139753105983296 spec.py:321] Evaluating on the training split.
I0229 18:34:21.955923 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 18:34:30.155820 139753105983296 spec.py:349] Evaluating on the test split.
I0229 18:34:32.490522 139753105983296 submission_runner.py:411] Time since start: 48203.49s, 	Step: 136742, 	{'train/accuracy': 0.8520607352256775, 'train/loss': 0.5236783027648926, 'validation/accuracy': 0.7238399982452393, 'validation/loss': 1.1254109144210815, 'validation/num_examples': 50000, 'test/accuracy': 0.5951000452041626, 'test/loss': 1.8549350500106812, 'test/num_examples': 10000, 'score': 46469.35333895683, 'total_duration': 48203.49363040924, 'accumulated_submission_time': 46469.35333895683, 'accumulated_eval_time': 1725.3450455665588, 'accumulated_logging_time': 4.035556793212891}
I0229 18:34:32.530285 139589725452032 logging_writer.py:48] [136742] accumulated_eval_time=1725.345046, accumulated_logging_time=4.035557, accumulated_submission_time=46469.353339, global_step=136742, preemption_count=0, score=46469.353339, test/accuracy=0.595100, test/loss=1.854935, test/num_examples=10000, total_duration=48203.493630, train/accuracy=0.852061, train/loss=0.523678, validation/accuracy=0.723840, validation/loss=1.125411, validation/num_examples=50000
I0229 18:34:52.548280 139590144857856 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.2935376167297363, loss=0.9895857572555542
I0229 18:35:26.410574 139589725452032 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.330125570297241, loss=0.9857641458511353
I0229 18:36:00.359579 139590144857856 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.3274307250976562, loss=0.9542866349220276
I0229 18:36:34.290915 139589725452032 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.505110263824463, loss=0.9734166264533997
I0229 18:37:08.212954 139590144857856 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.4338438510894775, loss=0.9733458757400513
I0229 18:37:42.186784 139589725452032 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.654917001724243, loss=1.0173513889312744
I0229 18:38:16.137780 139590144857856 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.305387496948242, loss=0.9446379542350769
I0229 18:38:50.101571 139589725452032 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.829005479812622, loss=1.0313727855682373
I0229 18:39:24.063664 139590144857856 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.6655848026275635, loss=1.0671684741973877
I0229 18:39:58.075178 139589725452032 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.5690598487854004, loss=0.9767548441886902
I0229 18:40:32.031957 139590144857856 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.485642433166504, loss=1.0958566665649414
I0229 18:41:05.972702 139589725452032 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.533510208129883, loss=1.022453784942627
I0229 18:41:39.895653 139590144857856 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.696058511734009, loss=0.9237281084060669
I0229 18:42:13.824686 139589725452032 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.641364574432373, loss=1.033861756324768
I0229 18:42:47.739083 139590144857856 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.7095742225646973, loss=1.0898597240447998
I0229 18:43:02.805312 139753105983296 spec.py:321] Evaluating on the training split.
I0229 18:43:08.873785 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 18:43:17.208383 139753105983296 spec.py:349] Evaluating on the test split.
I0229 18:43:19.502948 139753105983296 submission_runner.py:411] Time since start: 48730.51s, 	Step: 138246, 	{'train/accuracy': 0.8571826815605164, 'train/loss': 0.49044495820999146, 'validation/accuracy': 0.7291399836540222, 'validation/loss': 1.1260720491409302, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.8454172611236572, 'test/num_examples': 10000, 'score': 46979.564589738846, 'total_duration': 48730.506182432175, 'accumulated_submission_time': 46979.564589738846, 'accumulated_eval_time': 1742.0426452159882, 'accumulated_logging_time': 4.085598707199097}
I0229 18:43:19.546973 139589717059328 logging_writer.py:48] [138246] accumulated_eval_time=1742.042645, accumulated_logging_time=4.085599, accumulated_submission_time=46979.564590, global_step=138246, preemption_count=0, score=46979.564590, test/accuracy=0.601600, test/loss=1.845417, test/num_examples=10000, total_duration=48730.506182, train/accuracy=0.857183, train/loss=0.490445, validation/accuracy=0.729140, validation/loss=1.126072, validation/num_examples=50000
I0229 18:43:38.224578 139590170035968 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.6742992401123047, loss=1.010987639427185
I0229 18:44:12.143519 139589717059328 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.5342817306518555, loss=0.9529579281806946
I0229 18:44:46.092525 139590170035968 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.4015445709228516, loss=0.9853612780570984
I0229 18:45:20.062152 139589717059328 logging_writer.py:48] [138600] global_step=138600, grad_norm=4.037551403045654, loss=1.1021268367767334
I0229 18:45:53.980996 139590170035968 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.440683603286743, loss=1.0451380014419556
I0229 18:46:28.005993 139589717059328 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.5674333572387695, loss=1.0455336570739746
I0229 18:47:01.946592 139590170035968 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.4805245399475098, loss=0.9720905423164368
I0229 18:47:35.911068 139589717059328 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.3808650970458984, loss=0.9346081614494324
I0229 18:48:09.845855 139590170035968 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.5897843837738037, loss=1.0466063022613525
I0229 18:48:43.790424 139589717059328 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.6172072887420654, loss=1.0265181064605713
I0229 18:49:17.768043 139590170035968 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.437314748764038, loss=0.9100928902626038
I0229 18:49:51.737797 139589717059328 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.507652759552002, loss=0.9510971307754517
I0229 18:50:25.700592 139590170035968 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.911839008331299, loss=1.0122201442718506
I0229 18:50:59.682499 139589717059328 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.432561159133911, loss=0.9396030902862549
I0229 18:51:33.647569 139590170035968 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.5951216220855713, loss=0.8411062955856323
I0229 18:51:49.757734 139753105983296 spec.py:321] Evaluating on the training split.
I0229 18:51:55.783404 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 18:52:04.059009 139753105983296 spec.py:349] Evaluating on the test split.
I0229 18:52:06.369814 139753105983296 submission_runner.py:411] Time since start: 49257.37s, 	Step: 139749, 	{'train/accuracy': 0.8938735723495483, 'train/loss': 0.3696881830692291, 'validation/accuracy': 0.7278199791908264, 'validation/loss': 1.112565040588379, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8640631437301636, 'test/num_examples': 10000, 'score': 47489.711589336395, 'total_duration': 49257.37302136421, 'accumulated_submission_time': 47489.711589336395, 'accumulated_eval_time': 1758.6546468734741, 'accumulated_logging_time': 4.139785051345825}
I0229 18:52:06.413543 139589725452032 logging_writer.py:48] [139749] accumulated_eval_time=1758.654647, accumulated_logging_time=4.139785, accumulated_submission_time=47489.711589, global_step=139749, preemption_count=0, score=47489.711589, test/accuracy=0.601200, test/loss=1.864063, test/num_examples=10000, total_duration=49257.373021, train/accuracy=0.893874, train/loss=0.369688, validation/accuracy=0.727820, validation/loss=1.112565, validation/num_examples=50000
I0229 18:52:24.113050 139590144857856 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.8357255458831787, loss=1.074804663658142
I0229 18:52:58.032607 139589725452032 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.659728527069092, loss=0.9070724844932556
I0229 18:53:31.969630 139590144857856 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.5087039470672607, loss=0.936959981918335
I0229 18:54:05.876748 139589725452032 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.9300785064697266, loss=0.9443106055259705
I0229 18:54:39.804144 139590144857856 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.7632365226745605, loss=1.0564424991607666
I0229 18:55:13.745876 139589725452032 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.780897617340088, loss=1.010435700416565
I0229 18:55:47.674478 139590144857856 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.900623321533203, loss=1.02727472782135
I0229 18:56:21.545381 139589725452032 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.7540781497955322, loss=1.0130690336227417
I0229 18:56:55.479693 139590144857856 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.8745789527893066, loss=0.9137954711914062
I0229 18:57:29.443170 139589725452032 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.789717197418213, loss=1.0346075296401978
I0229 18:58:03.376589 139590144857856 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.9611966609954834, loss=1.0927109718322754
I0229 18:58:37.342012 139589725452032 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.543189287185669, loss=0.8788126707077026
I0229 18:59:11.277513 139590144857856 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.742907762527466, loss=1.054577112197876
I0229 18:59:45.200815 139589725452032 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.445746660232544, loss=0.9848156571388245
I0229 19:00:19.131554 139590144857856 logging_writer.py:48] [141200] global_step=141200, grad_norm=4.086365222930908, loss=1.0915470123291016
I0229 19:00:36.560910 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:00:42.542791 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:00:50.777085 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:00:53.113269 139753105983296 submission_runner.py:411] Time since start: 49784.12s, 	Step: 141253, 	{'train/accuracy': 0.88578200340271, 'train/loss': 0.39926984906196594, 'validation/accuracy': 0.7300199866294861, 'validation/loss': 1.1068578958511353, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.8223786354064941, 'test/num_examples': 10000, 'score': 47999.79533600807, 'total_duration': 49784.116453409195, 'accumulated_submission_time': 47999.79533600807, 'accumulated_eval_time': 1775.2069325447083, 'accumulated_logging_time': 4.194054841995239}
I0229 19:00:53.157392 139589717059328 logging_writer.py:48] [141253] accumulated_eval_time=1775.206933, accumulated_logging_time=4.194055, accumulated_submission_time=47999.795336, global_step=141253, preemption_count=0, score=47999.795336, test/accuracy=0.606000, test/loss=1.822379, test/num_examples=10000, total_duration=49784.116453, train/accuracy=0.885782, train/loss=0.399270, validation/accuracy=0.730020, validation/loss=1.106858, validation/num_examples=50000
I0229 19:01:09.438779 139590161643264 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.9494810104370117, loss=0.9934409856796265
I0229 19:01:43.323429 139589717059328 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.46911358833313, loss=0.933702290058136
I0229 19:02:17.234538 139590161643264 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.7232534885406494, loss=1.0182442665100098
I0229 19:02:51.195225 139589717059328 logging_writer.py:48] [141600] global_step=141600, grad_norm=4.061850547790527, loss=1.0178335905075073
I0229 19:03:25.140254 139590161643264 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.6698641777038574, loss=0.9831698536872864
I0229 19:03:59.098048 139589717059328 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.85416841506958, loss=1.0442545413970947
I0229 19:04:33.043074 139590161643264 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.638292074203491, loss=0.9360799789428711
I0229 19:05:07.153817 139589717059328 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.6985785961151123, loss=0.9939402341842651
I0229 19:05:41.100835 139590161643264 logging_writer.py:48] [142100] global_step=142100, grad_norm=4.13999080657959, loss=1.0496271848678589
I0229 19:06:15.069735 139589717059328 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.4790306091308594, loss=0.9051560163497925
I0229 19:06:49.012769 139590161643264 logging_writer.py:48] [142300] global_step=142300, grad_norm=4.039945125579834, loss=0.9826825857162476
I0229 19:07:22.982488 139589717059328 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.511260509490967, loss=0.8990346193313599
I0229 19:07:56.937209 139590161643264 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.4121665954589844, loss=0.8536168336868286
I0229 19:08:30.886922 139589717059328 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6769039630889893, loss=0.9046080112457275
I0229 19:09:04.827772 139590161643264 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.608173370361328, loss=0.9964444041252136
I0229 19:09:23.291966 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:09:29.338654 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:09:37.659927 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:09:39.967941 139753105983296 submission_runner.py:411] Time since start: 50310.97s, 	Step: 142756, 	{'train/accuracy': 0.8810786008834839, 'train/loss': 0.4117041528224945, 'validation/accuracy': 0.7327799797058105, 'validation/loss': 1.0963129997253418, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.838681936264038, 'test/num_examples': 10000, 'score': 48509.867886304855, 'total_duration': 50310.97115421295, 'accumulated_submission_time': 48509.867886304855, 'accumulated_eval_time': 1791.8828384876251, 'accumulated_logging_time': 4.2479026317596436}
I0229 19:09:40.010435 139590144857856 logging_writer.py:48] [142756] accumulated_eval_time=1791.882838, accumulated_logging_time=4.247903, accumulated_submission_time=48509.867886, global_step=142756, preemption_count=0, score=48509.867886, test/accuracy=0.603700, test/loss=1.838682, test/num_examples=10000, total_duration=50310.971154, train/accuracy=0.881079, train/loss=0.411704, validation/accuracy=0.732780, validation/loss=1.096313, validation/num_examples=50000
I0229 19:09:55.262440 139590153250560 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.989473819732666, loss=1.0333609580993652
I0229 19:10:29.108626 139590144857856 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.650817632675171, loss=0.9332288503646851
I0229 19:11:03.113268 139590153250560 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.6568655967712402, loss=0.9306274652481079
I0229 19:11:37.044420 139590144857856 logging_writer.py:48] [143100] global_step=143100, grad_norm=4.4996867179870605, loss=1.042750597000122
I0229 19:12:10.964579 139590153250560 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.839463233947754, loss=0.9710123538970947
I0229 19:12:44.898313 139590144857856 logging_writer.py:48] [143300] global_step=143300, grad_norm=4.099977016448975, loss=0.9823927879333496
I0229 19:13:18.807466 139590153250560 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.8100125789642334, loss=0.9963710308074951
I0229 19:13:52.725344 139590144857856 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.4653007984161377, loss=0.9139747023582458
I0229 19:14:26.635903 139590153250560 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.7794201374053955, loss=1.0492244958877563
I0229 19:15:00.513533 139590144857856 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.9452216625213623, loss=0.9245266318321228
I0229 19:15:34.437367 139590153250560 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.7466156482696533, loss=0.9612571001052856
I0229 19:16:08.368951 139590144857856 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.9470832347869873, loss=0.9152936339378357
I0229 19:16:42.309496 139590153250560 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.6982505321502686, loss=1.0062874555587769
I0229 19:17:16.318182 139590144857856 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.907727003097534, loss=0.974024772644043
I0229 19:17:50.263895 139590153250560 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.6445538997650146, loss=0.9745261073112488
I0229 19:18:10.086230 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:18:16.217088 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:18:24.328162 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:18:26.619750 139753105983296 submission_runner.py:411] Time since start: 50837.62s, 	Step: 144260, 	{'train/accuracy': 0.8727877736091614, 'train/loss': 0.44132199883461, 'validation/accuracy': 0.7282800078392029, 'validation/loss': 1.1258344650268555, 'validation/num_examples': 50000, 'test/accuracy': 0.5979000329971313, 'test/loss': 1.8827852010726929, 'test/num_examples': 10000, 'score': 49019.87905693054, 'total_duration': 50837.62298154831, 'accumulated_submission_time': 49019.87905693054, 'accumulated_eval_time': 1808.416305065155, 'accumulated_logging_time': 4.30181097984314}
I0229 19:18:26.664732 139590161643264 logging_writer.py:48] [144260] accumulated_eval_time=1808.416305, accumulated_logging_time=4.301811, accumulated_submission_time=49019.879057, global_step=144260, preemption_count=0, score=49019.879057, test/accuracy=0.597900, test/loss=1.882785, test/num_examples=10000, total_duration=50837.622982, train/accuracy=0.872788, train/loss=0.441322, validation/accuracy=0.728280, validation/loss=1.125834, validation/num_examples=50000
I0229 19:18:40.579389 139590170035968 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.7884209156036377, loss=0.8638268113136292
I0229 19:19:14.481251 139590161643264 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.803366184234619, loss=0.9386944770812988
I0229 19:19:48.397487 139590170035968 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.7412102222442627, loss=0.8339146375656128
I0229 19:20:22.366068 139590161643264 logging_writer.py:48] [144600] global_step=144600, grad_norm=4.222320556640625, loss=0.9375225901603699
I0229 19:20:56.334163 139590170035968 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.7918779850006104, loss=1.0001015663146973
I0229 19:21:30.311766 139590161643264 logging_writer.py:48] [144800] global_step=144800, grad_norm=4.281848430633545, loss=0.9470915794372559
I0229 19:22:04.290419 139590170035968 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.7877626419067383, loss=0.8972283601760864
I0229 19:22:38.233873 139590161643264 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.6415281295776367, loss=0.8676386475563049
I0229 19:23:12.265902 139590170035968 logging_writer.py:48] [145100] global_step=145100, grad_norm=4.046581745147705, loss=0.9056485295295715
I0229 19:23:46.213562 139590161643264 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.5380966663360596, loss=0.9218180179595947
I0229 19:24:20.191741 139590170035968 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.772174119949341, loss=0.8594279289245605
I0229 19:24:54.154254 139590161643264 logging_writer.py:48] [145400] global_step=145400, grad_norm=4.173232078552246, loss=0.9324262142181396
I0229 19:25:28.132705 139590170035968 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.778884172439575, loss=0.9119656682014465
I0229 19:26:02.094861 139590161643264 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.838240385055542, loss=0.9379470944404602
I0229 19:26:36.073760 139590170035968 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.709012031555176, loss=0.9555389881134033
I0229 19:26:56.939682 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:27:03.141979 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:27:11.281703 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:27:13.573596 139753105983296 submission_runner.py:411] Time since start: 51364.58s, 	Step: 145763, 	{'train/accuracy': 0.8825733065605164, 'train/loss': 0.40182381868362427, 'validation/accuracy': 0.7343399524688721, 'validation/loss': 1.1096502542495728, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.843134880065918, 'test/num_examples': 10000, 'score': 49530.088250398636, 'total_duration': 51364.57683515549, 'accumulated_submission_time': 49530.088250398636, 'accumulated_eval_time': 1825.0501840114594, 'accumulated_logging_time': 4.359186172485352}
I0229 19:27:13.615099 139589725452032 logging_writer.py:48] [145763] accumulated_eval_time=1825.050184, accumulated_logging_time=4.359186, accumulated_submission_time=49530.088250, global_step=145763, preemption_count=0, score=49530.088250, test/accuracy=0.607300, test/loss=1.843135, test/num_examples=10000, total_duration=51364.576835, train/accuracy=0.882573, train/loss=0.401824, validation/accuracy=0.734340, validation/loss=1.109650, validation/num_examples=50000
I0229 19:27:26.495621 139590144857856 logging_writer.py:48] [145800] global_step=145800, grad_norm=4.351055145263672, loss=0.9574890732765198
I0229 19:28:00.410424 139589725452032 logging_writer.py:48] [145900] global_step=145900, grad_norm=4.001442909240723, loss=0.9467641115188599
I0229 19:28:34.312371 139590144857856 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.637685775756836, loss=0.7709277272224426
I0229 19:29:08.259773 139589725452032 logging_writer.py:48] [146100] global_step=146100, grad_norm=4.045341968536377, loss=0.9243967533111572
I0229 19:29:42.254260 139590144857856 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.598942756652832, loss=0.9637089371681213
I0229 19:30:16.194623 139589725452032 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.6716012954711914, loss=0.8450325727462769
I0229 19:30:50.142019 139590144857856 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.008127689361572, loss=0.8907579779624939
I0229 19:31:24.089035 139589725452032 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.560692071914673, loss=0.8705250024795532
I0229 19:31:58.020941 139590144857856 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.760841131210327, loss=0.8521062135696411
I0229 19:32:31.959967 139589725452032 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.8301804065704346, loss=0.9060859084129333
I0229 19:33:05.932061 139590144857856 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.989980936050415, loss=0.9568710923194885
I0229 19:33:39.876287 139589725452032 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.75516414642334, loss=0.9324926137924194
I0229 19:34:13.797964 139590144857856 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.763481855392456, loss=0.8877270221710205
I0229 19:34:47.726236 139589725452032 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.978153705596924, loss=0.8758991360664368
I0229 19:35:21.727806 139590144857856 logging_writer.py:48] [147200] global_step=147200, grad_norm=4.127256870269775, loss=0.9177939891815186
I0229 19:35:43.606544 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:35:49.614261 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:35:57.983485 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:36:00.317438 139753105983296 submission_runner.py:411] Time since start: 51891.32s, 	Step: 147266, 	{'train/accuracy': 0.88671875, 'train/loss': 0.38843807578086853, 'validation/accuracy': 0.7356199622154236, 'validation/loss': 1.0984673500061035, 'validation/num_examples': 50000, 'test/accuracy': 0.6115000247955322, 'test/loss': 1.8455555438995361, 'test/num_examples': 10000, 'score': 50040.01444983482, 'total_duration': 51891.32067155838, 'accumulated_submission_time': 50040.01444983482, 'accumulated_eval_time': 1841.7610247135162, 'accumulated_logging_time': 4.412938833236694}
I0229 19:36:00.363493 139590161643264 logging_writer.py:48] [147266] accumulated_eval_time=1841.761025, accumulated_logging_time=4.412939, accumulated_submission_time=50040.014450, global_step=147266, preemption_count=0, score=50040.014450, test/accuracy=0.611500, test/loss=1.845556, test/num_examples=10000, total_duration=51891.320672, train/accuracy=0.886719, train/loss=0.388438, validation/accuracy=0.735620, validation/loss=1.098467, validation/num_examples=50000
I0229 19:36:12.233314 139590170035968 logging_writer.py:48] [147300] global_step=147300, grad_norm=4.2257890701293945, loss=0.9187134504318237
I0229 19:36:46.094922 139590161643264 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.9220263957977295, loss=0.9011257886886597
I0229 19:37:20.019944 139590170035968 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.118757724761963, loss=0.8524843454360962
I0229 19:37:53.977655 139590161643264 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.7979369163513184, loss=0.9143201112747192
I0229 19:38:27.921660 139590170035968 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.974616527557373, loss=0.9520372152328491
I0229 19:39:01.841182 139590161643264 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.6925275325775146, loss=0.8911271095275879
I0229 19:39:35.771418 139590170035968 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.8504481315612793, loss=0.9223284125328064
I0229 19:40:09.738292 139590161643264 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.206732749938965, loss=0.950323224067688
I0229 19:40:43.681194 139590170035968 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.970564365386963, loss=0.8542361855506897
I0229 19:41:17.615610 139590161643264 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.018375873565674, loss=0.9215766191482544
I0229 19:41:51.717736 139590170035968 logging_writer.py:48] [148300] global_step=148300, grad_norm=4.251964569091797, loss=1.002772331237793
I0229 19:42:25.632965 139590161643264 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.216193675994873, loss=0.9253464341163635
I0229 19:42:59.548333 139590170035968 logging_writer.py:48] [148500] global_step=148500, grad_norm=4.499095439910889, loss=1.0034713745117188
I0229 19:43:33.478891 139590161643264 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.164833068847656, loss=0.8928433060646057
I0229 19:44:07.367106 139590170035968 logging_writer.py:48] [148700] global_step=148700, grad_norm=4.09880256652832, loss=0.8629998564720154
I0229 19:44:30.608417 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:44:36.663680 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:44:44.994548 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:44:47.380424 139753105983296 submission_runner.py:411] Time since start: 52418.38s, 	Step: 148770, 	{'train/accuracy': 0.9123883843421936, 'train/loss': 0.31480342149734497, 'validation/accuracy': 0.7336199879646301, 'validation/loss': 1.1054151058197021, 'validation/num_examples': 50000, 'test/accuracy': 0.6085000038146973, 'test/loss': 1.8452770709991455, 'test/num_examples': 10000, 'score': 50550.195132255554, 'total_duration': 52418.38363933563, 'accumulated_submission_time': 50550.195132255554, 'accumulated_eval_time': 1858.532964706421, 'accumulated_logging_time': 4.470274448394775}
I0229 19:44:47.424216 139589725452032 logging_writer.py:48] [148770] accumulated_eval_time=1858.532965, accumulated_logging_time=4.470274, accumulated_submission_time=50550.195132, global_step=148770, preemption_count=0, score=50550.195132, test/accuracy=0.608500, test/loss=1.845277, test/num_examples=10000, total_duration=52418.383639, train/accuracy=0.912388, train/loss=0.314803, validation/accuracy=0.733620, validation/loss=1.105415, validation/num_examples=50000
I0229 19:44:57.909945 139590144857856 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.9941370487213135, loss=0.828652560710907
I0229 19:45:31.800566 139589725452032 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.5856826305389404, loss=0.8044618368148804
I0229 19:46:05.715463 139590144857856 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.651124954223633, loss=0.8148676156997681
I0229 19:46:39.664193 139589725452032 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.8866045475006104, loss=0.8426917791366577
I0229 19:47:13.582998 139590144857856 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.007616996765137, loss=0.8835854530334473
I0229 19:47:47.599467 139589725452032 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.841461420059204, loss=0.9187766313552856
I0229 19:48:21.551349 139590144857856 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.995462656021118, loss=0.8858986496925354
I0229 19:48:55.459629 139589725452032 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.503806114196777, loss=0.8796670436859131
I0229 19:49:29.359241 139590144857856 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.912986993789673, loss=0.8592236042022705
I0229 19:50:03.261031 139589725452032 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.87583065032959, loss=0.8919997811317444
I0229 19:50:37.184737 139590144857856 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.9004058837890625, loss=0.8163119554519653
I0229 19:51:11.111338 139589725452032 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.061520576477051, loss=0.920404851436615
I0229 19:51:45.024314 139590144857856 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.397602558135986, loss=0.8790373802185059
I0229 19:52:18.965003 139589725452032 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.8583874702453613, loss=0.8115388751029968
I0229 19:52:52.899780 139590144857856 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.9696648120880127, loss=0.8985230922698975
I0229 19:53:17.451197 139753105983296 spec.py:321] Evaluating on the training split.
I0229 19:53:23.588220 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 19:53:31.875948 139753105983296 spec.py:349] Evaluating on the test split.
I0229 19:53:34.187071 139753105983296 submission_runner.py:411] Time since start: 52945.19s, 	Step: 150274, 	{'train/accuracy': 0.9133051633834839, 'train/loss': 0.2986312806606293, 'validation/accuracy': 0.7386999726295471, 'validation/loss': 1.0863254070281982, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.8343807458877563, 'test/num_examples': 10000, 'score': 51060.157440423965, 'total_duration': 52945.19025826454, 'accumulated_submission_time': 51060.157440423965, 'accumulated_eval_time': 1875.2687742710114, 'accumulated_logging_time': 4.525710821151733}
I0229 19:53:34.231878 139589708666624 logging_writer.py:48] [150274] accumulated_eval_time=1875.268774, accumulated_logging_time=4.525711, accumulated_submission_time=51060.157440, global_step=150274, preemption_count=0, score=51060.157440, test/accuracy=0.609300, test/loss=1.834381, test/num_examples=10000, total_duration=52945.190258, train/accuracy=0.913305, train/loss=0.298631, validation/accuracy=0.738700, validation/loss=1.086325, validation/num_examples=50000
I0229 19:53:43.520476 139590161643264 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.264492034912109, loss=0.9032928943634033
I0229 19:54:17.443121 139589708666624 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.1224493980407715, loss=0.8167673349380493
I0229 19:54:51.361592 139590161643264 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.933229446411133, loss=0.8680644035339355
I0229 19:55:25.253736 139589708666624 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.808293104171753, loss=0.8517118692398071
I0229 19:55:59.182801 139590161643264 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.1016526222229, loss=0.9542524814605713
I0229 19:56:33.146684 139589708666624 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.051410675048828, loss=0.8699180483818054
I0229 19:57:07.080949 139590161643264 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.938344955444336, loss=0.8905399441719055
I0229 19:57:41.005176 139589708666624 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.964040756225586, loss=0.8619807958602905
I0229 19:58:14.961910 139590161643264 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.8461058139801025, loss=0.8720518350601196
I0229 19:58:48.923612 139589708666624 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.110650539398193, loss=0.9138465523719788
I0229 19:59:22.869072 139590161643264 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.843804359436035, loss=0.7867295742034912
I0229 19:59:56.879281 139589708666624 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.059046745300293, loss=0.852765679359436
I0229 20:00:30.843894 139590161643264 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.056373119354248, loss=0.8182262778282166
I0229 20:01:04.794371 139589708666624 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.020625114440918, loss=0.87587571144104
I0229 20:01:38.747600 139590161643264 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.4028849601745605, loss=0.9177490472793579
I0229 20:02:04.370608 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:02:10.985681 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:02:19.368982 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:02:21.775704 139753105983296 submission_runner.py:411] Time since start: 53472.78s, 	Step: 151777, 	{'train/accuracy': 0.9090202450752258, 'train/loss': 0.3202269375324249, 'validation/accuracy': 0.7390799522399902, 'validation/loss': 1.08871328830719, 'validation/num_examples': 50000, 'test/accuracy': 0.6095000505447388, 'test/loss': 1.837144374847412, 'test/num_examples': 10000, 'score': 51570.23234939575, 'total_duration': 53472.77893829346, 'accumulated_submission_time': 51570.23234939575, 'accumulated_eval_time': 1892.6738231182098, 'accumulated_logging_time': 4.5812482833862305}
I0229 20:02:21.822255 139590144857856 logging_writer.py:48] [151777] accumulated_eval_time=1892.673823, accumulated_logging_time=4.581248, accumulated_submission_time=51570.232349, global_step=151777, preemption_count=0, score=51570.232349, test/accuracy=0.609500, test/loss=1.837144, test/num_examples=10000, total_duration=53472.778938, train/accuracy=0.909020, train/loss=0.320227, validation/accuracy=0.739080, validation/loss=1.088713, validation/num_examples=50000
I0229 20:02:29.985511 139590153250560 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.947674036026001, loss=0.8395513892173767
I0229 20:03:03.870274 139590144857856 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.361172199249268, loss=0.9094161987304688
I0229 20:03:37.805249 139590153250560 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.3036017417907715, loss=0.8986930251121521
I0229 20:04:11.746125 139590144857856 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.01270055770874, loss=0.8460245132446289
I0229 20:04:45.674393 139590153250560 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.511346817016602, loss=0.8503572344779968
I0229 20:05:19.607874 139590144857856 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.316383361816406, loss=0.880203127861023
I0229 20:05:53.564852 139590153250560 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.0295915603637695, loss=0.7907652258872986
I0229 20:06:27.602050 139590144857856 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.12441873550415, loss=0.8424712419509888
I0229 20:07:01.538386 139590153250560 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.352835178375244, loss=0.7967892289161682
I0229 20:07:35.480617 139590144857856 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.2233381271362305, loss=0.8775657415390015
I0229 20:08:09.375611 139590153250560 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.457142353057861, loss=0.8671171069145203
I0229 20:08:43.301978 139590144857856 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.689692974090576, loss=0.8957247138023376
I0229 20:09:17.227325 139590153250560 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.9758853912353516, loss=0.8135013580322266
I0229 20:09:51.179771 139590144857856 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.15259313583374, loss=0.8517259955406189
I0229 20:10:25.089203 139590153250560 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.487970352172852, loss=0.9506109356880188
I0229 20:10:52.049623 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:10:58.079782 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:11:06.509458 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:11:08.822352 139753105983296 submission_runner.py:411] Time since start: 53999.83s, 	Step: 153281, 	{'train/accuracy': 0.9080038070678711, 'train/loss': 0.30983033776283264, 'validation/accuracy': 0.7390999794006348, 'validation/loss': 1.0933698415756226, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.8251116275787354, 'test/num_examples': 10000, 'score': 52080.39623951912, 'total_duration': 53999.825573682785, 'accumulated_submission_time': 52080.39623951912, 'accumulated_eval_time': 1909.4464931488037, 'accumulated_logging_time': 4.638153791427612}
I0229 20:11:08.865971 139590170035968 logging_writer.py:48] [153281] accumulated_eval_time=1909.446493, accumulated_logging_time=4.638154, accumulated_submission_time=52080.396240, global_step=153281, preemption_count=0, score=52080.396240, test/accuracy=0.614000, test/loss=1.825112, test/num_examples=10000, total_duration=53999.825574, train/accuracy=0.908004, train/loss=0.309830, validation/accuracy=0.739100, validation/loss=1.093370, validation/num_examples=50000
I0229 20:11:15.636626 139590178428672 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.079632759094238, loss=0.8286208510398865
I0229 20:11:49.528280 139590170035968 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.276010036468506, loss=0.7887939214706421
I0229 20:12:23.501441 139590178428672 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.8485660552978516, loss=0.758491575717926
I0229 20:12:57.421582 139590170035968 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.898345708847046, loss=0.8105829358100891
I0229 20:13:31.327225 139590178428672 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.146114826202393, loss=0.9293485879898071
I0229 20:14:05.279106 139590170035968 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.030537128448486, loss=0.9015321731567383
I0229 20:14:39.218573 139590178428672 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.439580917358398, loss=0.8829155564308167
I0229 20:15:13.165486 139590170035968 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.007077693939209, loss=0.7620096802711487
I0229 20:15:47.052868 139590178428672 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.180389404296875, loss=0.8485448360443115
I0229 20:16:20.987401 139590170035968 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.00824499130249, loss=0.8283742070198059
I0229 20:16:54.926612 139590178428672 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.062664031982422, loss=0.7384594082832336
I0229 20:17:28.858334 139590170035968 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.690441131591797, loss=0.9102402925491333
I0229 20:18:02.749756 139590178428672 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.196573257446289, loss=0.8164758682250977
I0229 20:18:36.725325 139590170035968 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.072262763977051, loss=0.8029928207397461
I0229 20:19:10.650217 139590178428672 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.127448558807373, loss=0.8128080368041992
I0229 20:19:38.929569 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:19:44.938321 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:19:53.301432 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:19:55.591715 139753105983296 submission_runner.py:411] Time since start: 54526.59s, 	Step: 154785, 	{'train/accuracy': 0.9130859375, 'train/loss': 0.2970570921897888, 'validation/accuracy': 0.7426199913024902, 'validation/loss': 1.0836670398712158, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.8184280395507812, 'test/num_examples': 10000, 'score': 52590.397317409515, 'total_duration': 54526.59492826462, 'accumulated_submission_time': 52590.397317409515, 'accumulated_eval_time': 1926.1085708141327, 'accumulated_logging_time': 4.691672086715698}
I0229 20:19:55.633206 139589717059328 logging_writer.py:48] [154785] accumulated_eval_time=1926.108571, accumulated_logging_time=4.691672, accumulated_submission_time=52590.397317, global_step=154785, preemption_count=0, score=52590.397317, test/accuracy=0.622000, test/loss=1.818428, test/num_examples=10000, total_duration=54526.594928, train/accuracy=0.913086, train/loss=0.297057, validation/accuracy=0.742620, validation/loss=1.083667, validation/num_examples=50000
I0229 20:20:01.085579 139589725452032 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.915846347808838, loss=0.8439186215400696
I0229 20:20:34.954269 139589717059328 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.043757438659668, loss=0.7960033416748047
I0229 20:21:08.897481 139589725452032 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.2955169677734375, loss=0.7829102873802185
I0229 20:21:42.808033 139589717059328 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.074323654174805, loss=0.8054810762405396
I0229 20:22:16.729952 139589725452032 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.125277996063232, loss=0.7837333083152771
I0229 20:22:50.705127 139589717059328 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.078988075256348, loss=0.8750863075256348
I0229 20:23:24.654835 139589725452032 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.724542617797852, loss=0.8385893106460571
I0229 20:23:58.583326 139589717059328 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.2552361488342285, loss=0.8826903700828552
I0229 20:24:32.585612 139589725452032 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.894237756729126, loss=0.7287481427192688
I0229 20:25:06.521837 139589717059328 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.291484832763672, loss=0.801060676574707
I0229 20:25:40.421684 139589725452032 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.9843156337738037, loss=0.7947866916656494
I0229 20:26:14.369724 139589717059328 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.319777488708496, loss=0.8148752450942993
I0229 20:26:48.312888 139589725452032 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.242866039276123, loss=0.8003326654434204
I0229 20:27:22.242655 139589717059328 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.06120491027832, loss=0.8220688700675964
I0229 20:27:56.170146 139589725452032 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.763928413391113, loss=0.8003500699996948
I0229 20:28:25.846554 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:28:31.915919 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:28:40.285723 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:28:42.686604 139753105983296 submission_runner.py:411] Time since start: 55053.69s, 	Step: 156289, 	{'train/accuracy': 0.9122289419174194, 'train/loss': 0.3007284998893738, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.0831379890441895, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.8322269916534424, 'test/num_examples': 10000, 'score': 53100.54725623131, 'total_duration': 55053.689846515656, 'accumulated_submission_time': 53100.54725623131, 'accumulated_eval_time': 1942.9485993385315, 'accumulated_logging_time': 4.743105173110962}
I0229 20:28:42.722829 139589708666624 logging_writer.py:48] [156289] accumulated_eval_time=1942.948599, accumulated_logging_time=4.743105, accumulated_submission_time=53100.547256, global_step=156289, preemption_count=0, score=53100.547256, test/accuracy=0.617000, test/loss=1.832227, test/num_examples=10000, total_duration=55053.689847, train/accuracy=0.912229, train/loss=0.300728, validation/accuracy=0.743280, validation/loss=1.083138, validation/num_examples=50000
I0229 20:28:46.810355 139590161643264 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.411568641662598, loss=0.8139876127243042
I0229 20:29:20.722992 139589708666624 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.110631942749023, loss=0.7885791659355164
I0229 20:29:54.689799 139590161643264 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.8323240280151367, loss=0.7592721581459045
I0229 20:30:28.619043 139589708666624 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.481786251068115, loss=0.7898776531219482
I0229 20:31:02.687392 139590161643264 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.14638614654541, loss=0.7596086263656616
I0229 20:31:36.611597 139589708666624 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.6358795166015625, loss=0.8345525860786438
I0229 20:32:10.565944 139590161643264 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.268369197845459, loss=0.743951141834259
I0229 20:32:44.507954 139589708666624 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.044331073760986, loss=0.8042999505996704
I0229 20:33:18.455035 139590161643264 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.9157965183258057, loss=0.7532613277435303
I0229 20:33:52.377379 139589708666624 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.173933982849121, loss=0.8173172473907471
I0229 20:34:26.301632 139590161643264 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.214865207672119, loss=0.7859830856323242
I0229 20:35:00.264010 139589708666624 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.519688606262207, loss=0.8195273280143738
I0229 20:35:34.235749 139590161643264 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.9760348796844482, loss=0.7487969994544983
I0229 20:36:08.184553 139589708666624 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.8528361320495605, loss=0.746932864189148
I0229 20:36:42.137064 139590161643264 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.743187427520752, loss=0.8407353758811951
I0229 20:37:12.916607 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:37:18.923110 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:37:27.166949 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:37:29.454310 139753105983296 submission_runner.py:411] Time since start: 55580.46s, 	Step: 157792, 	{'train/accuracy': 0.9258809089660645, 'train/loss': 0.26029375195503235, 'validation/accuracy': 0.7439599633216858, 'validation/loss': 1.0839968919754028, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.8258272409439087, 'test/num_examples': 10000, 'score': 53610.67733025551, 'total_duration': 55580.45754265785, 'accumulated_submission_time': 53610.67733025551, 'accumulated_eval_time': 1959.4862580299377, 'accumulated_logging_time': 4.7898218631744385}
I0229 20:37:29.501121 139590144857856 logging_writer.py:48] [157792] accumulated_eval_time=1959.486258, accumulated_logging_time=4.789822, accumulated_submission_time=53610.677330, global_step=157792, preemption_count=0, score=53610.677330, test/accuracy=0.617700, test/loss=1.825827, test/num_examples=10000, total_duration=55580.457543, train/accuracy=0.925881, train/loss=0.260294, validation/accuracy=0.743960, validation/loss=1.083997, validation/num_examples=50000
I0229 20:37:32.566122 139590153250560 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.671835899353027, loss=0.7613278031349182
I0229 20:38:06.457209 139590144857856 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.124069690704346, loss=0.8350735306739807
I0229 20:38:40.396140 139590153250560 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.321442127227783, loss=0.7826951742172241
I0229 20:39:14.339825 139590144857856 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.336199760437012, loss=0.7992326021194458
I0229 20:39:48.277507 139590153250560 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.028483867645264, loss=0.7392942905426025
I0229 20:40:22.222835 139590144857856 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.63507080078125, loss=0.7889716029167175
I0229 20:40:56.152239 139590153250560 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.18143892288208, loss=0.762129008769989
I0229 20:41:30.117410 139590144857856 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.268211841583252, loss=0.7358075380325317
I0229 20:42:04.041718 139590153250560 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.507076740264893, loss=0.8298161625862122
I0229 20:42:37.943334 139590144857856 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.436162948608398, loss=0.7601255774497986
I0229 20:43:11.947763 139590153250560 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.528302192687988, loss=0.7001299858093262
I0229 20:43:45.908936 139590144857856 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.329645156860352, loss=0.7575974464416504
I0229 20:44:19.849061 139590153250560 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.488338947296143, loss=0.7284255027770996
I0229 20:44:53.762711 139590144857856 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.431907653808594, loss=0.8402345180511475
I0229 20:45:27.723217 139590153250560 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.533658027648926, loss=0.838957667350769
I0229 20:45:59.780303 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:46:05.897411 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:46:14.417223 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:46:16.695985 139753105983296 submission_runner.py:411] Time since start: 56107.70s, 	Step: 159296, 	{'train/accuracy': 0.9353076815605164, 'train/loss': 0.22725597023963928, 'validation/accuracy': 0.7450799942016602, 'validation/loss': 1.0768632888793945, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.8430743217468262, 'test/num_examples': 10000, 'score': 54120.890498161316, 'total_duration': 56107.6992123127, 'accumulated_submission_time': 54120.890498161316, 'accumulated_eval_time': 1976.401884317398, 'accumulated_logging_time': 4.849618911743164}
I0229 20:46:16.745956 139589708666624 logging_writer.py:48] [159296] accumulated_eval_time=1976.401884, accumulated_logging_time=4.849619, accumulated_submission_time=54120.890498, global_step=159296, preemption_count=0, score=54120.890498, test/accuracy=0.619500, test/loss=1.843074, test/num_examples=10000, total_duration=56107.699212, train/accuracy=0.935308, train/loss=0.227256, validation/accuracy=0.745080, validation/loss=1.076863, validation/num_examples=50000
I0229 20:46:18.451534 139589717059328 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.9397146701812744, loss=0.8074092268943787
I0229 20:46:52.404411 139589708666624 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.414804935455322, loss=0.7424099445343018
I0229 20:47:26.343833 139589717059328 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.3274641036987305, loss=0.7449513673782349
I0229 20:48:00.293325 139589708666624 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.148322105407715, loss=0.6839549541473389
I0229 20:48:34.247341 139589717059328 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.391464710235596, loss=0.7960301637649536
I0229 20:49:08.279322 139589708666624 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.513505935668945, loss=0.7609571218490601
I0229 20:49:42.250738 139589717059328 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.289285182952881, loss=0.7939094305038452
I0229 20:50:16.174075 139589708666624 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.565673828125, loss=0.7469953894615173
I0229 20:50:50.130517 139589717059328 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.435072898864746, loss=0.8038086891174316
I0229 20:51:24.056611 139589708666624 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.0628533363342285, loss=0.678694486618042
I0229 20:51:58.040809 139589717059328 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.1988091468811035, loss=0.7615655064582825
I0229 20:52:32.017980 139589708666624 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.484155178070068, loss=0.7910187840461731
I0229 20:53:05.957259 139589717059328 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.448665618896484, loss=0.7831785082817078
I0229 20:53:39.919615 139589708666624 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.175004482269287, loss=0.7553874850273132
I0229 20:54:13.874410 139589717059328 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.319369792938232, loss=0.7793545722961426
I0229 20:54:46.974207 139753105983296 spec.py:321] Evaluating on the training split.
I0229 20:54:52.989543 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 20:55:01.302570 139753105983296 spec.py:349] Evaluating on the test split.
I0229 20:55:03.634989 139753105983296 submission_runner.py:411] Time since start: 56634.64s, 	Step: 160799, 	{'train/accuracy': 0.9308434128761292, 'train/loss': 0.24075892567634583, 'validation/accuracy': 0.7454400062561035, 'validation/loss': 1.0781235694885254, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.8339414596557617, 'test/num_examples': 10000, 'score': 54631.05508708954, 'total_duration': 56634.638201236725, 'accumulated_submission_time': 54631.05508708954, 'accumulated_eval_time': 1993.0626058578491, 'accumulated_logging_time': 4.90923810005188}
I0229 20:55:03.681304 139590178428672 logging_writer.py:48] [160799] accumulated_eval_time=1993.062606, accumulated_logging_time=4.909238, accumulated_submission_time=54631.055087, global_step=160799, preemption_count=0, score=54631.055087, test/accuracy=0.616200, test/loss=1.833941, test/num_examples=10000, total_duration=56634.638201, train/accuracy=0.930843, train/loss=0.240759, validation/accuracy=0.745440, validation/loss=1.078124, validation/num_examples=50000
I0229 20:55:04.365665 139590186821376 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.432508945465088, loss=0.7759724855422974
I0229 20:55:38.305673 139590178428672 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.243972301483154, loss=0.7427142858505249
I0229 20:56:12.226055 139590186821376 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.471935272216797, loss=0.7745198607444763
I0229 20:56:46.156846 139590178428672 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.288595676422119, loss=0.7921667098999023
I0229 20:57:20.069583 139590186821376 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.61025333404541, loss=0.7492714524269104
I0229 20:57:53.988042 139590178428672 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.123922824859619, loss=0.6972183585166931
I0229 20:58:27.919108 139590186821376 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.40574312210083, loss=0.7743291854858398
I0229 20:59:01.870839 139590178428672 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.388391017913818, loss=0.7773182392120361
I0229 20:59:35.794118 139590186821376 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.9988231658935547, loss=0.6942461133003235
I0229 21:00:09.737668 139590178428672 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.281946659088135, loss=0.7178752422332764
I0229 21:00:43.664910 139590186821376 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.073988914489746, loss=0.7384880185127258
I0229 21:01:17.597760 139590178428672 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.140622138977051, loss=0.7392317056655884
I0229 21:01:51.629494 139590186821376 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.501324653625488, loss=0.7767137289047241
I0229 21:02:25.559731 139590178428672 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.8131096363067627, loss=0.7275447249412537
I0229 21:02:59.497866 139590186821376 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.424633979797363, loss=0.7203191518783569
I0229 21:03:33.410861 139590178428672 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.1434712409973145, loss=0.7006995677947998
I0229 21:03:33.903828 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:03:40.013838 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:03:48.295959 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:03:50.559777 139753105983296 submission_runner.py:411] Time since start: 57161.56s, 	Step: 162303, 	{'train/accuracy': 0.9325773119926453, 'train/loss': 0.2340480387210846, 'validation/accuracy': 0.7475000023841858, 'validation/loss': 1.0697925090789795, 'validation/num_examples': 50000, 'test/accuracy': 0.6193000078201294, 'test/loss': 1.8274227380752563, 'test/num_examples': 10000, 'score': 55141.21358561516, 'total_duration': 57161.56299781799, 'accumulated_submission_time': 55141.21358561516, 'accumulated_eval_time': 2009.718491077423, 'accumulated_logging_time': 4.966087818145752}
I0229 21:03:50.607337 139589717059328 logging_writer.py:48] [162303] accumulated_eval_time=2009.718491, accumulated_logging_time=4.966088, accumulated_submission_time=55141.213586, global_step=162303, preemption_count=0, score=55141.213586, test/accuracy=0.619300, test/loss=1.827423, test/num_examples=10000, total_duration=57161.562998, train/accuracy=0.932577, train/loss=0.234048, validation/accuracy=0.747500, validation/loss=1.069793, validation/num_examples=50000
I0229 21:04:23.857072 139589725452032 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.92626428604126, loss=0.7315203547477722
I0229 21:04:57.791437 139589717059328 logging_writer.py:48] [162500] global_step=162500, grad_norm=5.3816118240356445, loss=0.8564984202384949
I0229 21:05:31.762394 139589725452032 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.170860290527344, loss=0.6498843431472778
I0229 21:06:05.724233 139589717059328 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.529082298278809, loss=0.7540438771247864
I0229 21:06:39.674940 139589725452032 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.12381649017334, loss=0.7645222544670105
I0229 21:07:13.634921 139589717059328 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.2199602127075195, loss=0.7834064364433289
I0229 21:07:47.674492 139589725452032 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.255202770233154, loss=0.6640268564224243
I0229 21:08:21.627682 139589717059328 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.390723705291748, loss=0.6478941440582275
I0229 21:08:55.586166 139589725452032 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.858844757080078, loss=0.7492631673812866
I0229 21:09:29.556303 139589717059328 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.42872428894043, loss=0.7083877921104431
I0229 21:10:03.517112 139589725452032 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.754787921905518, loss=0.7501652240753174
I0229 21:10:37.481398 139589717059328 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.294005393981934, loss=0.7175357341766357
I0229 21:11:11.431705 139589725452032 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.452932357788086, loss=0.7274811267852783
I0229 21:11:45.397730 139589717059328 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.306739330291748, loss=0.7184698581695557
I0229 21:12:19.344614 139589725452032 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.0437846183776855, loss=0.708309531211853
I0229 21:12:20.848403 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:12:26.953460 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:12:35.248914 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:12:37.546502 139753105983296 submission_runner.py:411] Time since start: 57688.55s, 	Step: 163806, 	{'train/accuracy': 0.9353276491165161, 'train/loss': 0.2216317504644394, 'validation/accuracy': 0.7468799948692322, 'validation/loss': 1.0788114070892334, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.8333741426467896, 'test/num_examples': 10000, 'score': 55651.39206695557, 'total_duration': 57688.54973888397, 'accumulated_submission_time': 55651.39206695557, 'accumulated_eval_time': 2026.4165422916412, 'accumulated_logging_time': 5.023638486862183}
I0229 21:12:37.591014 139589708666624 logging_writer.py:48] [163806] accumulated_eval_time=2026.416542, accumulated_logging_time=5.023638, accumulated_submission_time=55651.392067, global_step=163806, preemption_count=0, score=55651.392067, test/accuracy=0.622900, test/loss=1.833374, test/num_examples=10000, total_duration=57688.549739, train/accuracy=0.935328, train/loss=0.221632, validation/accuracy=0.746880, validation/loss=1.078811, validation/num_examples=50000
I0229 21:13:09.778603 139589717059328 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.1317596435546875, loss=0.7008355259895325
I0229 21:13:43.788910 139589708666624 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.7957117557525635, loss=0.620983898639679
I0229 21:14:17.742587 139589717059328 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.599354267120361, loss=0.7529415488243103
I0229 21:14:51.704684 139589708666624 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.270910739898682, loss=0.7011319994926453
I0229 21:15:25.645589 139589717059328 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.490181922912598, loss=0.7150181531906128
I0229 21:15:59.601473 139589708666624 logging_writer.py:48] [164400] global_step=164400, grad_norm=3.8110430240631104, loss=0.6504331827163696
I0229 21:16:33.542454 139589717059328 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.039782524108887, loss=0.6228806376457214
I0229 21:17:07.463334 139589708666624 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.472105026245117, loss=0.7071042656898499
I0229 21:17:41.438800 139589717059328 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.415144443511963, loss=0.7186987400054932
I0229 21:18:15.369185 139589708666624 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.506405830383301, loss=0.7200693488121033
I0229 21:18:49.322048 139589717059328 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.123008728027344, loss=0.6503599286079407
I0229 21:19:23.257148 139589708666624 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.341414451599121, loss=0.672684907913208
I0229 21:19:57.302901 139589717059328 logging_writer.py:48] [165100] global_step=165100, grad_norm=3.9862494468688965, loss=0.677267849445343
I0229 21:20:31.226993 139589708666624 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.080936908721924, loss=0.7150222063064575
I0229 21:21:05.165158 139589717059328 logging_writer.py:48] [165300] global_step=165300, grad_norm=5.198390960693359, loss=0.7379381656646729
I0229 21:21:07.689132 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:21:13.707656 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:21:21.955510 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:21:24.248418 139753105983296 submission_runner.py:411] Time since start: 58215.25s, 	Step: 165309, 	{'train/accuracy': 0.9376594424247742, 'train/loss': 0.21621958911418915, 'validation/accuracy': 0.7461599707603455, 'validation/loss': 1.0782948732376099, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.8346995115280151, 'test/num_examples': 10000, 'score': 56161.42722654343, 'total_duration': 58215.25165247917, 'accumulated_submission_time': 56161.42722654343, 'accumulated_eval_time': 2042.9757788181305, 'accumulated_logging_time': 5.078388690948486}
I0229 21:21:24.297354 139589708666624 logging_writer.py:48] [165309] accumulated_eval_time=2042.975779, accumulated_logging_time=5.078389, accumulated_submission_time=56161.427227, global_step=165309, preemption_count=0, score=56161.427227, test/accuracy=0.624600, test/loss=1.834700, test/num_examples=10000, total_duration=58215.251652, train/accuracy=0.937659, train/loss=0.216220, validation/accuracy=0.746160, validation/loss=1.078295, validation/num_examples=50000
I0229 21:21:55.519176 139589717059328 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.543760299682617, loss=0.7581883668899536
I0229 21:22:29.449982 139589708666624 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.336225986480713, loss=0.7382668852806091
I0229 21:23:03.423848 139589717059328 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.926661491394043, loss=0.7420830726623535
I0229 21:23:37.378444 139589708666624 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.363719463348389, loss=0.7025368809700012
I0229 21:24:11.348239 139589717059328 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.068697929382324, loss=0.7307518720626831
I0229 21:24:45.305836 139589708666624 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.852088451385498, loss=0.7533509135246277
I0229 21:25:19.237692 139589717059328 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.432714462280273, loss=0.7072247266769409
I0229 21:25:53.193653 139589708666624 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.516636848449707, loss=0.6952342391014099
I0229 21:26:27.223333 139589717059328 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.435959339141846, loss=0.7266787886619568
I0229 21:27:01.178396 139589708666624 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.305624008178711, loss=0.6904480457305908
I0229 21:27:35.135382 139589717059328 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.213830471038818, loss=0.6597991585731506
I0229 21:28:09.074544 139589708666624 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.485586166381836, loss=0.6559361219406128
I0229 21:28:43.000471 139589717059328 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.635883808135986, loss=0.6849200129508972
I0229 21:29:16.947763 139589708666624 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.942111015319824, loss=0.7219034433364868
I0229 21:29:50.923229 139589717059328 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.600071430206299, loss=0.6922174692153931
I0229 21:29:54.473078 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:30:00.491210 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:30:08.889884 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:30:11.239301 139753105983296 submission_runner.py:411] Time since start: 58742.24s, 	Step: 166812, 	{'train/accuracy': 0.9435387253761292, 'train/loss': 0.20255078375339508, 'validation/accuracy': 0.7520999908447266, 'validation/loss': 1.061427116394043, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.825334906578064, 'test/num_examples': 10000, 'score': 56671.53956079483, 'total_duration': 58742.242524147034, 'accumulated_submission_time': 56671.53956079483, 'accumulated_eval_time': 2059.741940021515, 'accumulated_logging_time': 5.1376566886901855}
I0229 21:30:11.290815 139590178428672 logging_writer.py:48] [166812] accumulated_eval_time=2059.741940, accumulated_logging_time=5.137657, accumulated_submission_time=56671.539561, global_step=166812, preemption_count=0, score=56671.539561, test/accuracy=0.624100, test/loss=1.825335, test/num_examples=10000, total_duration=58742.242524, train/accuracy=0.943539, train/loss=0.202551, validation/accuracy=0.752100, validation/loss=1.061427, validation/num_examples=50000
I0229 21:30:41.477122 139590186821376 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.334383964538574, loss=0.635112464427948
I0229 21:31:15.401654 139590178428672 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.454265594482422, loss=0.7040321230888367
I0229 21:31:49.320227 139590186821376 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.728109836578369, loss=0.6642284989356995
I0229 21:32:23.364091 139590178428672 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.373293399810791, loss=0.6781747341156006
I0229 21:32:57.314429 139590186821376 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.755098342895508, loss=0.7657737731933594
I0229 21:33:31.263097 139590178428672 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.232797622680664, loss=0.784981369972229
I0229 21:34:05.219259 139590186821376 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.796151638031006, loss=0.6878570914268494
I0229 21:34:39.159293 139590178428672 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.2113423347473145, loss=0.6692563891410828
I0229 21:35:13.105731 139590186821376 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.426733493804932, loss=0.7055706977844238
I0229 21:35:47.046309 139590178428672 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.7095723152160645, loss=0.751430094242096
I0229 21:36:20.976986 139590186821376 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.668946266174316, loss=0.7227253913879395
I0229 21:36:54.914561 139590178428672 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.874836444854736, loss=0.781840443611145
I0229 21:37:28.864110 139590186821376 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.655740261077881, loss=0.6663782596588135
I0229 21:38:02.812158 139590178428672 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.6693339347839355, loss=0.6965256333351135
I0229 21:38:36.877715 139590186821376 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.144399642944336, loss=0.6635294556617737
I0229 21:38:41.433375 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:38:47.456355 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:38:55.773879 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:38:58.081300 139753105983296 submission_runner.py:411] Time since start: 59269.08s, 	Step: 168315, 	{'train/accuracy': 0.95121169090271, 'train/loss': 0.17348705232143402, 'validation/accuracy': 0.7513599991798401, 'validation/loss': 1.0714797973632812, 'validation/num_examples': 50000, 'test/accuracy': 0.6273000240325928, 'test/loss': 1.837319254875183, 'test/num_examples': 10000, 'score': 57181.618061065674, 'total_duration': 59269.084530353546, 'accumulated_submission_time': 57181.618061065674, 'accumulated_eval_time': 2076.389808654785, 'accumulated_logging_time': 5.199169874191284}
I0229 21:38:58.128154 139589725452032 logging_writer.py:48] [168315] accumulated_eval_time=2076.389809, accumulated_logging_time=5.199170, accumulated_submission_time=57181.618061, global_step=168315, preemption_count=0, score=57181.618061, test/accuracy=0.627300, test/loss=1.837319, test/num_examples=10000, total_duration=59269.084530, train/accuracy=0.951212, train/loss=0.173487, validation/accuracy=0.751360, validation/loss=1.071480, validation/num_examples=50000
I0229 21:39:27.274615 139590144857856 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.205739974975586, loss=0.6677281856536865
I0229 21:40:01.209625 139589725452032 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.17152738571167, loss=0.6784316897392273
I0229 21:40:35.177321 139590144857856 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.2425432205200195, loss=0.6547572612762451
I0229 21:41:09.120951 139589725452032 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.230220317840576, loss=0.6723533272743225
I0229 21:41:43.041395 139590144857856 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.71824836730957, loss=0.7233495712280273
I0229 21:42:16.974373 139589725452032 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.778257846832275, loss=0.6657401323318481
I0229 21:42:50.930892 139590144857856 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.554125785827637, loss=0.7212132215499878
I0229 21:43:24.886106 139589725452032 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.81447696685791, loss=0.7172644138336182
I0229 21:43:58.857004 139590144857856 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.683720111846924, loss=0.7301767468452454
I0229 21:44:32.792715 139589725452032 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.40002965927124, loss=0.6545008420944214
I0229 21:45:06.840042 139590144857856 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.754868984222412, loss=0.718550980091095
I0229 21:45:40.773259 139589725452032 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.4462890625, loss=0.5613285899162292
I0229 21:46:14.715152 139590144857856 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.84255838394165, loss=0.6857817769050598
I0229 21:46:48.666142 139589725452032 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.455948352813721, loss=0.6577600836753845
I0229 21:47:22.588986 139590144857856 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.663597583770752, loss=0.6531503796577454
I0229 21:47:28.179224 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:47:34.154402 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:47:42.525192 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:47:44.849282 139753105983296 submission_runner.py:411] Time since start: 59795.85s, 	Step: 169818, 	{'train/accuracy': 0.9519690275192261, 'train/loss': 0.1739388257265091, 'validation/accuracy': 0.7509399652481079, 'validation/loss': 1.0662760734558105, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.8304463624954224, 'test/num_examples': 10000, 'score': 57691.601484537125, 'total_duration': 59795.85251188278, 'accumulated_submission_time': 57691.601484537125, 'accumulated_eval_time': 2093.059808731079, 'accumulated_logging_time': 5.259644985198975}
I0229 21:47:44.899234 139590170035968 logging_writer.py:48] [169818] accumulated_eval_time=2093.059809, accumulated_logging_time=5.259645, accumulated_submission_time=57691.601485, global_step=169818, preemption_count=0, score=57691.601485, test/accuracy=0.625500, test/loss=1.830446, test/num_examples=10000, total_duration=59795.852512, train/accuracy=0.951969, train/loss=0.173939, validation/accuracy=0.750940, validation/loss=1.066276, validation/num_examples=50000
I0229 21:48:13.052301 139590178428672 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.886040210723877, loss=0.7146909236907959
I0229 21:48:46.947251 139590170035968 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.347682952880859, loss=0.6450196504592896
I0229 21:49:20.887305 139590178428672 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.46247673034668, loss=0.6642923951148987
I0229 21:49:54.832130 139590170035968 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.421603679656982, loss=0.6673153042793274
I0229 21:50:28.770960 139590178428672 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.464075565338135, loss=0.6943649053573608
I0229 21:51:02.765499 139590170035968 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.174059867858887, loss=0.6238155364990234
I0229 21:51:36.712228 139590178428672 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.453634262084961, loss=0.6532489657402039
I0229 21:52:10.668418 139590170035968 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.270028114318848, loss=0.6585346460342407
I0229 21:52:44.633930 139590178428672 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.616811275482178, loss=0.6994360089302063
I0229 21:53:18.586121 139590170035968 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.61141300201416, loss=0.6733986139297485
I0229 21:53:52.511789 139590178428672 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.7207560539245605, loss=0.727818489074707
I0229 21:54:26.468477 139590170035968 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.436223983764648, loss=0.6416158676147461
I0229 21:55:00.422409 139590178428672 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.362251281738281, loss=0.6818569898605347
I0229 21:55:34.371543 139590170035968 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.828527450561523, loss=0.6515007019042969
I0229 21:56:08.320394 139590178428672 logging_writer.py:48] [171300] global_step=171300, grad_norm=5.141845226287842, loss=0.736139178276062
I0229 21:56:14.921221 139753105983296 spec.py:321] Evaluating on the training split.
I0229 21:56:20.947300 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 21:56:29.285606 139753105983296 spec.py:349] Evaluating on the test split.
I0229 21:56:31.581872 139753105983296 submission_runner.py:411] Time since start: 60322.59s, 	Step: 171321, 	{'train/accuracy': 0.9516900181770325, 'train/loss': 0.1759648621082306, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.066061019897461, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.834411859512329, 'test/num_examples': 10000, 'score': 58201.560238838196, 'total_duration': 60322.585090875626, 'accumulated_submission_time': 58201.560238838196, 'accumulated_eval_time': 2109.7203953266144, 'accumulated_logging_time': 5.319650173187256}
I0229 21:56:31.632744 139589717059328 logging_writer.py:48] [171321] accumulated_eval_time=2109.720395, accumulated_logging_time=5.319650, accumulated_submission_time=58201.560239, global_step=171321, preemption_count=0, score=58201.560239, test/accuracy=0.627600, test/loss=1.834412, test/num_examples=10000, total_duration=60322.585091, train/accuracy=0.951690, train/loss=0.175965, validation/accuracy=0.751200, validation/loss=1.066061, validation/num_examples=50000
I0229 21:56:58.865215 139589725452032 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.5573906898498535, loss=0.6873592734336853
I0229 21:57:32.798492 139589717059328 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.773684501647949, loss=0.6951721906661987
I0229 21:58:06.771335 139589725452032 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.799680709838867, loss=0.7228293418884277
I0229 21:58:40.740399 139589717059328 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.4972920417785645, loss=0.7427629232406616
I0229 21:59:14.698851 139589725452032 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.454110145568848, loss=0.6920897960662842
I0229 21:59:48.671185 139589717059328 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.487870216369629, loss=0.6787678599357605
I0229 22:00:22.657140 139589725452032 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.2041401863098145, loss=0.621213436126709
I0229 22:00:56.613764 139589717059328 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.66279935836792, loss=0.74244225025177
I0229 22:01:30.579930 139589725452032 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.360721111297607, loss=0.619794487953186
I0229 22:02:04.493132 139589717059328 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.956520080566406, loss=0.7598919868469238
I0229 22:02:38.463941 139589725452032 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.393011093139648, loss=0.6417142748832703
I0229 22:03:12.470464 139589717059328 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.544193267822266, loss=0.5799786448478699
I0229 22:03:46.441851 139589725452032 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.754577159881592, loss=0.6879285573959351
I0229 22:04:20.409975 139589717059328 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.84761381149292, loss=0.7729530334472656
I0229 22:04:54.356345 139589725452032 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.208423614501953, loss=0.6205124855041504
I0229 22:05:01.648593 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:05:07.680127 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:05:16.014333 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:05:18.287749 139753105983296 submission_runner.py:411] Time since start: 60849.29s, 	Step: 172823, 	{'train/accuracy': 0.9527662396430969, 'train/loss': 0.16730628907680511, 'validation/accuracy': 0.7521599531173706, 'validation/loss': 1.0650585889816284, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8261528015136719, 'test/num_examples': 10000, 'score': 58711.51387619972, 'total_duration': 60849.290974617004, 'accumulated_submission_time': 58711.51387619972, 'accumulated_eval_time': 2126.3594963550568, 'accumulated_logging_time': 5.380169868469238}
I0229 22:05:18.337450 139590170035968 logging_writer.py:48] [172823] accumulated_eval_time=2126.359496, accumulated_logging_time=5.380170, accumulated_submission_time=58711.513876, global_step=172823, preemption_count=0, score=58711.513876, test/accuracy=0.628100, test/loss=1.826153, test/num_examples=10000, total_duration=60849.290975, train/accuracy=0.952766, train/loss=0.167306, validation/accuracy=0.752160, validation/loss=1.065059, validation/num_examples=50000
I0229 22:05:44.787017 139590178428672 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.469806671142578, loss=0.6647306680679321
I0229 22:06:18.709926 139590170035968 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.618072509765625, loss=0.7129653692245483
I0229 22:06:52.634783 139590178428672 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.7472429275512695, loss=0.5819073915481567
I0229 22:07:26.531605 139590170035968 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.356642723083496, loss=0.6054695248603821
I0229 22:08:00.462080 139590178428672 logging_writer.py:48] [173300] global_step=173300, grad_norm=5.217586040496826, loss=0.7337254881858826
I0229 22:08:34.423314 139590170035968 logging_writer.py:48] [173400] global_step=173400, grad_norm=5.012625217437744, loss=0.7641284465789795
I0229 22:09:08.377435 139590178428672 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.217950820922852, loss=0.6365388631820679
I0229 22:09:42.407265 139590170035968 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.730823040008545, loss=0.6753082871437073
I0229 22:10:16.354709 139590178428672 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.993631362915039, loss=0.6062936186790466
I0229 22:10:50.327638 139590170035968 logging_writer.py:48] [173800] global_step=173800, grad_norm=5.048521041870117, loss=0.6558293104171753
I0229 22:11:24.295789 139590178428672 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.703835487365723, loss=0.701728105545044
I0229 22:11:58.242835 139590170035968 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.605978012084961, loss=0.6126372814178467
I0229 22:12:32.201569 139590178428672 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.404384613037109, loss=0.6058486104011536
I0229 22:13:06.137915 139590170035968 logging_writer.py:48] [174200] global_step=174200, grad_norm=5.066009998321533, loss=0.6789844036102295
I0229 22:13:40.077152 139590178428672 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.368226528167725, loss=0.6173815727233887
I0229 22:13:48.366006 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:13:54.431880 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:14:02.724054 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:14:04.962212 139753105983296 submission_runner.py:411] Time since start: 61375.97s, 	Step: 174326, 	{'train/accuracy': 0.9549385905265808, 'train/loss': 0.1686282455921173, 'validation/accuracy': 0.7537800073623657, 'validation/loss': 1.060943841934204, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.8231186866760254, 'test/num_examples': 10000, 'score': 59221.47832560539, 'total_duration': 61375.96544504166, 'accumulated_submission_time': 59221.47832560539, 'accumulated_eval_time': 2142.9556527137756, 'accumulated_logging_time': 5.441788911819458}
I0229 22:14:05.009128 139589725452032 logging_writer.py:48] [174326] accumulated_eval_time=2142.955653, accumulated_logging_time=5.441789, accumulated_submission_time=59221.478326, global_step=174326, preemption_count=0, score=59221.478326, test/accuracy=0.623900, test/loss=1.823119, test/num_examples=10000, total_duration=61375.965445, train/accuracy=0.954939, train/loss=0.168628, validation/accuracy=0.753780, validation/loss=1.060944, validation/num_examples=50000
I0229 22:14:30.476375 139590144857856 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.519282341003418, loss=0.6423968076705933
I0229 22:15:04.385736 139589725452032 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.334303855895996, loss=0.6320711970329285
I0229 22:15:38.444185 139590144857856 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.3224053382873535, loss=0.6484999060630798
I0229 22:16:12.410867 139589725452032 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.156060218811035, loss=0.6018517017364502
I0229 22:16:46.322631 139590144857856 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.850980281829834, loss=0.6714946627616882
I0229 22:17:20.274292 139589725452032 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.164820671081543, loss=0.5462133884429932
I0229 22:17:54.224474 139590144857856 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.250010013580322, loss=0.6441618800163269
I0229 22:18:28.194400 139589725452032 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.396732330322266, loss=0.5863374471664429
I0229 22:19:02.185335 139590144857856 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.552371501922607, loss=0.6407950520515442
I0229 22:19:36.157354 139589725452032 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.642414093017578, loss=0.6083332300186157
I0229 22:20:10.135546 139590144857856 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.750075340270996, loss=0.6324993968009949
I0229 22:20:44.119134 139589725452032 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.746579647064209, loss=0.6535888910293579
I0229 22:21:18.090662 139590144857856 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.603451251983643, loss=0.6686451435089111
I0229 22:21:52.203664 139589725452032 logging_writer.py:48] [175700] global_step=175700, grad_norm=5.349549770355225, loss=0.7059026956558228
I0229 22:22:26.157750 139590144857856 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.361855506896973, loss=0.6714544892311096
I0229 22:22:35.104144 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:22:41.094496 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:22:49.489628 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:22:51.731883 139753105983296 submission_runner.py:411] Time since start: 61902.74s, 	Step: 175828, 	{'train/accuracy': 0.9548588991165161, 'train/loss': 0.16530942916870117, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0575079917907715, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.8286449909210205, 'test/num_examples': 10000, 'score': 59731.51045560837, 'total_duration': 61902.73510289192, 'accumulated_submission_time': 59731.51045560837, 'accumulated_eval_time': 2159.5833282470703, 'accumulated_logging_time': 5.498790264129639}
I0229 22:22:51.789711 139590170035968 logging_writer.py:48] [175828] accumulated_eval_time=2159.583328, accumulated_logging_time=5.498790, accumulated_submission_time=59731.510456, global_step=175828, preemption_count=0, score=59731.510456, test/accuracy=0.628400, test/loss=1.828645, test/num_examples=10000, total_duration=61902.735103, train/accuracy=0.954859, train/loss=0.165309, validation/accuracy=0.755360, validation/loss=1.057508, validation/num_examples=50000
I0229 22:23:16.528742 139590178428672 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.464927673339844, loss=0.6225318908691406
I0229 22:23:50.451500 139590170035968 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.772296905517578, loss=0.691748321056366
I0229 22:24:24.415819 139590178428672 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.092470645904541, loss=0.6113526821136475
I0229 22:24:58.336453 139590170035968 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.61828088760376, loss=0.6820130944252014
I0229 22:25:32.289869 139590178428672 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.417872428894043, loss=0.6443196535110474
I0229 22:26:06.226853 139590170035968 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.567388534545898, loss=0.600175142288208
I0229 22:26:40.163475 139590178428672 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.668826580047607, loss=0.7123299837112427
I0229 22:27:14.107090 139590170035968 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.295347213745117, loss=0.5782437920570374
I0229 22:27:48.198149 139590178428672 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.594871997833252, loss=0.6362220644950867
I0229 22:28:22.165917 139590170035968 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.985579013824463, loss=0.6926563382148743
I0229 22:28:56.113130 139590178428672 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.68699312210083, loss=0.6372118592262268
I0229 22:29:30.040818 139590170035968 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.379354953765869, loss=0.5913998484611511
I0229 22:30:03.964329 139590178428672 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.339746475219727, loss=0.6346355080604553
I0229 22:30:37.897002 139590170035968 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.746844291687012, loss=0.5837855935096741
I0229 22:31:12.128132 139590178428672 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.560555934906006, loss=0.7012298107147217
I0229 22:31:21.797622 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:31:27.902868 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:31:36.199524 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:31:38.501132 139753105983296 submission_runner.py:411] Time since start: 62429.50s, 	Step: 177330, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14505362510681152, 'validation/accuracy': 0.7543799877166748, 'validation/loss': 1.057165503501892, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.8258801698684692, 'test/num_examples': 10000, 'score': 60241.45378923416, 'total_duration': 62429.50436306, 'accumulated_submission_time': 60241.45378923416, 'accumulated_eval_time': 2176.286799430847, 'accumulated_logging_time': 5.566740036010742}
I0229 22:31:38.556364 139590144857856 logging_writer.py:48] [177330] accumulated_eval_time=2176.286799, accumulated_logging_time=5.566740, accumulated_submission_time=60241.453789, global_step=177330, preemption_count=0, score=60241.453789, test/accuracy=0.628500, test/loss=1.825880, test/num_examples=10000, total_duration=62429.504363, train/accuracy=0.960658, train/loss=0.145054, validation/accuracy=0.754380, validation/loss=1.057166, validation/num_examples=50000
I0229 22:32:02.597419 139590153250560 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.681236743927002, loss=0.6550411581993103
I0229 22:32:36.519044 139590144857856 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.214510917663574, loss=0.6714349389076233
I0229 22:33:10.478763 139590153250560 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.577908039093018, loss=0.6841115951538086
I0229 22:33:44.545099 139590144857856 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.425955295562744, loss=0.580849826335907
I0229 22:34:18.480913 139590153250560 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.420991897583008, loss=0.6553903222084045
I0229 22:34:52.425846 139590144857856 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.7341437339782715, loss=0.6345449686050415
I0229 22:35:26.375727 139590153250560 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.310418605804443, loss=0.6551181077957153
I0229 22:36:00.336226 139590144857856 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.327065467834473, loss=0.5034319162368774
I0229 22:36:34.294424 139590153250560 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.449086666107178, loss=0.6000413298606873
I0229 22:37:08.246892 139590144857856 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.881739139556885, loss=0.6234791278839111
I0229 22:37:42.163530 139590153250560 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.493472576141357, loss=0.5852396488189697
I0229 22:38:16.162042 139590144857856 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.452103614807129, loss=0.6541027426719666
I0229 22:38:50.097157 139590153250560 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.236204624176025, loss=0.6114649772644043
I0229 22:39:24.075996 139590144857856 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.2756781578063965, loss=0.5375025868415833
I0229 22:39:58.126057 139590153250560 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.191654682159424, loss=0.675230860710144
I0229 22:40:08.804019 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:40:14.822914 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:40:23.111479 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:40:25.337559 139753105983296 submission_runner.py:411] Time since start: 62956.34s, 	Step: 178833, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.15042605996131897, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.055589199066162, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8220672607421875, 'test/num_examples': 10000, 'score': 60751.63723874092, 'total_duration': 62956.34075641632, 'accumulated_submission_time': 60751.63723874092, 'accumulated_eval_time': 2192.8202583789825, 'accumulated_logging_time': 5.632821083068848}
I0229 22:40:25.383263 139589717059328 logging_writer.py:48] [178833] accumulated_eval_time=2192.820258, accumulated_logging_time=5.632821, accumulated_submission_time=60751.637239, global_step=178833, preemption_count=0, score=60751.637239, test/accuracy=0.629100, test/loss=1.822067, test/num_examples=10000, total_duration=62956.340756, train/accuracy=0.959263, train/loss=0.150426, validation/accuracy=0.755880, validation/loss=1.055589, validation/num_examples=50000
I0229 22:40:48.440993 139589725452032 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.375551700592041, loss=0.6229932308197021
I0229 22:41:22.370994 139589717059328 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.710381984710693, loss=0.6473647356033325
I0229 22:41:56.315234 139589725452032 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.410055637359619, loss=0.6136695742607117
I0229 22:42:30.220031 139589717059328 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.551148414611816, loss=0.7397487163543701
I0229 22:43:04.156444 139589725452032 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.467660903930664, loss=0.6272413730621338
I0229 22:43:38.109838 139589717059328 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.171731472015381, loss=0.6388733386993408
I0229 22:44:12.056861 139589725452032 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.982967376708984, loss=0.6559559106826782
I0229 22:44:45.994295 139589717059328 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.388621807098389, loss=0.6754570603370667
I0229 22:45:19.937707 139589725452032 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.516138076782227, loss=0.567000687122345
I0229 22:45:53.875519 139589717059328 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.81421422958374, loss=0.6636143326759338
I0229 22:46:27.968286 139589725452032 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.712740421295166, loss=0.7719606161117554
I0229 22:47:01.897899 139589717059328 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.338340759277344, loss=0.5864077210426331
I0229 22:47:35.843105 139589725452032 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.546252250671387, loss=0.57909095287323
I0229 22:48:09.780500 139589717059328 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.635471343994141, loss=0.6205201745033264
I0229 22:48:43.722098 139589725452032 logging_writer.py:48] [180300] global_step=180300, grad_norm=5.081221103668213, loss=0.6616928577423096
I0229 22:48:55.422911 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:49:01.448385 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:49:09.712300 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:49:11.997385 139753105983296 submission_runner.py:411] Time since start: 63483.00s, 	Step: 180336, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.1449994593858719, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0576988458633423, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.8224239349365234, 'test/num_examples': 10000, 'score': 61261.6109521389, 'total_duration': 63483.00061035156, 'accumulated_submission_time': 61261.6109521389, 'accumulated_eval_time': 2209.394678592682, 'accumulated_logging_time': 5.691540241241455}
I0229 22:49:12.050089 139590144857856 logging_writer.py:48] [180336] accumulated_eval_time=2209.394679, accumulated_logging_time=5.691540, accumulated_submission_time=61261.610952, global_step=180336, preemption_count=0, score=61261.610952, test/accuracy=0.628800, test/loss=1.822424, test/num_examples=10000, total_duration=63483.000610, train/accuracy=0.960439, train/loss=0.144999, validation/accuracy=0.754980, validation/loss=1.057699, validation/num_examples=50000
I0229 22:49:34.133502 139590153250560 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.487531661987305, loss=0.6110373735427856
I0229 22:50:08.026862 139590144857856 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.553494930267334, loss=0.6525601148605347
I0229 22:50:41.981730 139590153250560 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.3006978034973145, loss=0.6293109655380249
I0229 22:51:15.954962 139590144857856 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.082719802856445, loss=0.5426608920097351
I0229 22:51:49.901704 139590153250560 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.991302013397217, loss=0.6047529578208923
I0229 22:52:23.900531 139590144857856 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.7338056564331055, loss=0.6603808999061584
I0229 22:52:57.864590 139590153250560 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.348855972290039, loss=0.5384823679924011
I0229 22:53:31.807696 139590144857856 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.873868465423584, loss=0.6646155118942261
I0229 22:54:05.755178 139590153250560 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.335473537445068, loss=0.6394369602203369
I0229 22:54:39.709022 139590144857856 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.47743558883667, loss=0.6344918608665466
I0229 22:55:13.652463 139590153250560 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.366468906402588, loss=0.6312546133995056
I0229 22:55:47.597156 139590144857856 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.2580108642578125, loss=0.6629174947738647
I0229 22:56:21.541928 139590153250560 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.403819561004639, loss=0.6153274774551392
I0229 22:56:55.472626 139590144857856 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.769076347351074, loss=0.6402781009674072
I0229 22:57:29.415976 139590153250560 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.621946811676025, loss=0.7384472489356995
I0229 22:57:42.115116 139753105983296 spec.py:321] Evaluating on the training split.
I0229 22:57:48.105288 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 22:57:56.430266 139753105983296 spec.py:349] Evaluating on the test split.
I0229 22:57:58.788797 139753105983296 submission_runner.py:411] Time since start: 64009.79s, 	Step: 181839, 	{'train/accuracy': 0.9586654901504517, 'train/loss': 0.15044260025024414, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0552231073379517, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.8193235397338867, 'test/num_examples': 10000, 'score': 61771.61212205887, 'total_duration': 64009.79198360443, 'accumulated_submission_time': 61771.61212205887, 'accumulated_eval_time': 2226.068264245987, 'accumulated_logging_time': 5.75496768951416}
I0229 22:57:58.849930 139589708666624 logging_writer.py:48] [181839] accumulated_eval_time=2226.068264, accumulated_logging_time=5.754968, accumulated_submission_time=61771.612122, global_step=181839, preemption_count=0, score=61771.612122, test/accuracy=0.629200, test/loss=1.819324, test/num_examples=10000, total_duration=64009.791984, train/accuracy=0.958665, train/loss=0.150443, validation/accuracy=0.754680, validation/loss=1.055223, validation/num_examples=50000
I0229 22:58:19.962370 139589717059328 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.767163276672363, loss=0.6323729157447815
I0229 22:58:53.862498 139589708666624 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.640102386474609, loss=0.6566730737686157
I0229 22:59:27.818304 139589717059328 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.947020530700684, loss=0.5950340032577515
I0229 23:00:01.778946 139589708666624 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.324021816253662, loss=0.5763313174247742
I0229 23:00:35.745016 139589717059328 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.654804706573486, loss=0.6324051022529602
I0229 23:01:09.692752 139589708666624 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.557960510253906, loss=0.5970964431762695
I0229 23:01:43.626525 139589717059328 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.061407089233398, loss=0.5620955228805542
I0229 23:02:17.583758 139589708666624 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.747359752655029, loss=0.550370454788208
I0229 23:02:51.524331 139589717059328 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.521382808685303, loss=0.6052676439285278
I0229 23:03:25.418595 139589708666624 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.591207027435303, loss=0.6724327206611633
I0229 23:03:59.344667 139589717059328 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.512075901031494, loss=0.6488370895385742
I0229 23:04:33.357263 139589708666624 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.160223960876465, loss=0.6361685395240784
I0229 23:05:07.299294 139589717059328 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.9028401374816895, loss=0.6216848492622375
I0229 23:05:41.255100 139589708666624 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.686583995819092, loss=0.5562751293182373
I0229 23:06:15.182535 139589717059328 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.653592109680176, loss=0.6461313366889954
I0229 23:06:28.907098 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:06:35.061237 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:06:43.264050 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:06:45.539639 139753105983296 submission_runner.py:411] Time since start: 64536.54s, 	Step: 183342, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14801247417926788, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0551658868789673, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8188600540161133, 'test/num_examples': 10000, 'score': 62281.60619163513, 'total_duration': 64536.54286932945, 'accumulated_submission_time': 62281.60619163513, 'accumulated_eval_time': 2242.700765132904, 'accumulated_logging_time': 5.826450347900391}
I0229 23:06:45.592607 139589717059328 logging_writer.py:48] [183342] accumulated_eval_time=2242.700765, accumulated_logging_time=5.826450, accumulated_submission_time=62281.606192, global_step=183342, preemption_count=0, score=62281.606192, test/accuracy=0.630900, test/loss=1.818860, test/num_examples=10000, total_duration=64536.542869, train/accuracy=0.960818, train/loss=0.148012, validation/accuracy=0.754960, validation/loss=1.055166, validation/num_examples=50000
I0229 23:07:05.609395 139590170035968 logging_writer.py:48] [183400] global_step=183400, grad_norm=5.110903739929199, loss=0.638007640838623
I0229 23:07:39.500155 139589717059328 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.432575225830078, loss=0.5871615409851074
I0229 23:08:13.464144 139590170035968 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.9157023429870605, loss=0.6808961033821106
I0229 23:08:47.396523 139589717059328 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.078160762786865, loss=0.524037778377533
I0229 23:09:21.377254 139590170035968 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.613051891326904, loss=0.6302706003189087
I0229 23:09:55.326970 139589717059328 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.79344367980957, loss=0.6452531814575195
I0229 23:10:29.271076 139590170035968 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.864182472229004, loss=0.6236957311630249
I0229 23:11:03.394269 139589717059328 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.4279890060424805, loss=0.6276809573173523
I0229 23:11:37.334445 139590170035968 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.637365341186523, loss=0.6310084462165833
I0229 23:12:11.290409 139589717059328 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.636573314666748, loss=0.5465291738510132
I0229 23:12:45.210666 139590170035968 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.553296089172363, loss=0.6897597908973694
I0229 23:13:19.165862 139589717059328 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.878467559814453, loss=0.6571046113967896
I0229 23:13:53.150954 139590170035968 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.567739009857178, loss=0.6209319829940796
I0229 23:14:27.073719 139589717059328 logging_writer.py:48] [184700] global_step=184700, grad_norm=5.383213043212891, loss=0.6338778734207153
I0229 23:15:01.026107 139590170035968 logging_writer.py:48] [184800] global_step=184800, grad_norm=5.008075714111328, loss=0.6137478947639465
I0229 23:15:15.779759 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:15:21.850832 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:15:30.113184 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:15:32.372118 139753105983296 submission_runner.py:411] Time since start: 65063.38s, 	Step: 184845, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14803920686244965, 'validation/accuracy': 0.7543999552726746, 'validation/loss': 1.0552953481674194, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.818251609802246, 'test/num_examples': 10000, 'score': 62791.727311849594, 'total_duration': 65063.375341653824, 'accumulated_submission_time': 62791.727311849594, 'accumulated_eval_time': 2259.293065071106, 'accumulated_logging_time': 5.892799139022827}
I0229 23:15:32.419205 139590144857856 logging_writer.py:48] [184845] accumulated_eval_time=2259.293065, accumulated_logging_time=5.892799, accumulated_submission_time=62791.727312, global_step=184845, preemption_count=0, score=62791.727312, test/accuracy=0.629900, test/loss=1.818252, test/num_examples=10000, total_duration=65063.375342, train/accuracy=0.960379, train/loss=0.148039, validation/accuracy=0.754400, validation/loss=1.055295, validation/num_examples=50000
I0229 23:15:51.396876 139590153250560 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.526794910430908, loss=0.6058255434036255
I0229 23:16:25.300305 139590144857856 logging_writer.py:48] [185000] global_step=185000, grad_norm=5.0083160400390625, loss=0.6779237985610962
I0229 23:16:59.314863 139590153250560 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.6913347244262695, loss=0.6521331667900085
I0229 23:17:33.244290 139590144857856 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.580557823181152, loss=0.6293379664421082
I0229 23:18:07.231966 139590153250560 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.539513111114502, loss=0.6280806660652161
I0229 23:18:41.176210 139590144857856 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.443520545959473, loss=0.6817372441291809
I0229 23:19:15.120614 139590153250560 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.424330234527588, loss=0.5769428014755249
I0229 23:19:49.081395 139590144857856 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.989299774169922, loss=0.627758800983429
I0229 23:20:22.996428 139590153250560 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.51716423034668, loss=0.6388080716133118
I0229 23:20:56.943804 139590144857856 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.258082389831543, loss=0.6384242177009583
I0229 23:21:30.904715 139590153250560 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.605823993682861, loss=0.5958894491195679
I0229 23:22:04.850445 139590144857856 logging_writer.py:48] [186000] global_step=186000, grad_norm=4.650317668914795, loss=0.6213629245758057
I0229 23:22:38.811772 139590153250560 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.946948528289795, loss=0.6826270818710327
I0229 23:23:12.900063 139590144857856 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.704723834991455, loss=0.6735736131668091
I0229 23:23:46.882847 139590153250560 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.853357315063477, loss=0.6662822961807251
I0229 23:24:02.644241 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:24:08.720357 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:24:17.040419 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:24:19.297399 139753105983296 submission_runner.py:411] Time since start: 65590.30s, 	Step: 186348, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14171895384788513, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0548179149627686, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8192886114120483, 'test/num_examples': 10000, 'score': 63301.88521718979, 'total_duration': 65590.30063033104, 'accumulated_submission_time': 63301.88521718979, 'accumulated_eval_time': 2275.9461719989777, 'accumulated_logging_time': 5.953683853149414}
I0229 23:24:19.352945 139589708666624 logging_writer.py:48] [186348] accumulated_eval_time=2275.946172, accumulated_logging_time=5.953684, accumulated_submission_time=63301.885217, global_step=186348, preemption_count=0, score=63301.885217, test/accuracy=0.629800, test/loss=1.819289, test/num_examples=10000, total_duration=65590.300630, train/accuracy=0.961316, train/loss=0.141719, validation/accuracy=0.754800, validation/loss=1.054818, validation/num_examples=50000
I0229 23:24:37.355869 139589725452032 logging_writer.py:48] [186400] global_step=186400, grad_norm=5.14175271987915, loss=0.6059872508049011
I0229 23:25:11.265436 139589708666624 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.886258602142334, loss=0.6546594500541687
I0229 23:25:45.205462 139589725452032 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.610167980194092, loss=0.6104903221130371
I0229 23:26:19.188932 139589708666624 logging_writer.py:48] [186700] global_step=186700, grad_norm=4.54405403137207, loss=0.6325597763061523
I0229 23:26:53.114896 139589725452032 logging_writer.py:48] [186800] global_step=186800, grad_norm=4.788012504577637, loss=0.5923463702201843
I0229 23:27:27.101808 139589708666624 logging_writer.py:48] [186900] global_step=186900, grad_norm=4.4889020919799805, loss=0.6484779715538025
I0229 23:28:01.058163 139589725452032 logging_writer.py:48] [187000] global_step=187000, grad_norm=4.48250150680542, loss=0.5867407321929932
I0229 23:28:35.007397 139589708666624 logging_writer.py:48] [187100] global_step=187100, grad_norm=4.942682266235352, loss=0.6423218250274658
I0229 23:29:09.093644 139589725452032 logging_writer.py:48] [187200] global_step=187200, grad_norm=4.6267218589782715, loss=0.7047038674354553
I0229 23:29:43.052787 139589708666624 logging_writer.py:48] [187300] global_step=187300, grad_norm=4.6105637550354, loss=0.6867920160293579
I0229 23:30:16.958774 139589725452032 logging_writer.py:48] [187400] global_step=187400, grad_norm=4.614561080932617, loss=0.6177014112472534
I0229 23:30:50.926532 139589708666624 logging_writer.py:48] [187500] global_step=187500, grad_norm=4.468789100646973, loss=0.6912786960601807
I0229 23:31:24.883888 139589725452032 logging_writer.py:48] [187600] global_step=187600, grad_norm=4.873732566833496, loss=0.6859568357467651
I0229 23:31:58.826405 139589708666624 logging_writer.py:48] [187700] global_step=187700, grad_norm=4.777505397796631, loss=0.6438257098197937
I0229 23:32:32.778784 139589725452032 logging_writer.py:48] [187800] global_step=187800, grad_norm=4.574542045593262, loss=0.6605297327041626
I0229 23:32:49.555194 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:32:55.587840 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:33:03.989717 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:33:06.233185 139753105983296 submission_runner.py:411] Time since start: 66117.24s, 	Step: 187851, 	{'train/accuracy': 0.9618940949440002, 'train/loss': 0.14343789219856262, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.056044578552246, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.818678379058838, 'test/num_examples': 10000, 'score': 63812.025695085526, 'total_duration': 66117.23640704155, 'accumulated_submission_time': 63812.025695085526, 'accumulated_eval_time': 2292.6241064071655, 'accumulated_logging_time': 6.019184112548828}
I0229 23:33:06.286116 139589717059328 logging_writer.py:48] [187851] accumulated_eval_time=2292.624106, accumulated_logging_time=6.019184, accumulated_submission_time=63812.025695, global_step=187851, preemption_count=0, score=63812.025695, test/accuracy=0.630400, test/loss=1.818678, test/num_examples=10000, total_duration=66117.236407, train/accuracy=0.961894, train/loss=0.143438, validation/accuracy=0.754720, validation/loss=1.056045, validation/num_examples=50000
I0229 23:33:23.251179 139589725452032 logging_writer.py:48] [187900] global_step=187900, grad_norm=4.948176383972168, loss=0.6889769434928894
I0229 23:33:57.180837 139589717059328 logging_writer.py:48] [188000] global_step=188000, grad_norm=5.378170490264893, loss=0.6275103092193604
I0229 23:34:31.111740 139589725452032 logging_writer.py:48] [188100] global_step=188100, grad_norm=3.9356627464294434, loss=0.571187436580658
I0229 23:35:05.046528 139589717059328 logging_writer.py:48] [188200] global_step=188200, grad_norm=4.3528571128845215, loss=0.566870391368866
I0229 23:35:39.033945 139589725452032 logging_writer.py:48] [188300] global_step=188300, grad_norm=4.394617080688477, loss=0.560356616973877
I0229 23:36:13.000998 139589717059328 logging_writer.py:48] [188400] global_step=188400, grad_norm=4.404196262359619, loss=0.5844163298606873
I0229 23:36:46.922407 139589725452032 logging_writer.py:48] [188500] global_step=188500, grad_norm=4.6110029220581055, loss=0.6690147519111633
I0229 23:37:20.832643 139589717059328 logging_writer.py:48] [188600] global_step=188600, grad_norm=5.0319390296936035, loss=0.7105364203453064
I0229 23:37:54.776106 139589725452032 logging_writer.py:48] [188700] global_step=188700, grad_norm=4.566154479980469, loss=0.5723403692245483
I0229 23:38:28.748894 139589717059328 logging_writer.py:48] [188800] global_step=188800, grad_norm=4.260942459106445, loss=0.5586286783218384
I0229 23:39:02.681385 139589725452032 logging_writer.py:48] [188900] global_step=188900, grad_norm=4.631011962890625, loss=0.5880992412567139
I0229 23:39:36.587940 139589717059328 logging_writer.py:48] [189000] global_step=189000, grad_norm=4.143344879150391, loss=0.5356655716896057
I0229 23:40:10.521160 139589725452032 logging_writer.py:48] [189100] global_step=189100, grad_norm=4.609040260314941, loss=0.6406069993972778
I0229 23:40:44.456826 139589717059328 logging_writer.py:48] [189200] global_step=189200, grad_norm=4.4450578689575195, loss=0.5596858263015747
I0229 23:41:18.405932 139589725452032 logging_writer.py:48] [189300] global_step=189300, grad_norm=4.598607063293457, loss=0.7351138591766357
I0229 23:41:36.248441 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:41:42.286111 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:41:50.625361 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:41:52.831586 139753105983296 submission_runner.py:411] Time since start: 66643.83s, 	Step: 189354, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.14786013960838318, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0551180839538574, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.817767858505249, 'test/num_examples': 10000, 'score': 64321.925748348236, 'total_duration': 66643.83481454849, 'accumulated_submission_time': 64321.925748348236, 'accumulated_eval_time': 2309.2071928977966, 'accumulated_logging_time': 6.081565856933594}
I0229 23:41:52.883676 139590178428672 logging_writer.py:48] [189354] accumulated_eval_time=2309.207193, accumulated_logging_time=6.081566, accumulated_submission_time=64321.925748, global_step=189354, preemption_count=0, score=64321.925748, test/accuracy=0.630200, test/loss=1.817768, test/num_examples=10000, total_duration=66643.834815, train/accuracy=0.959841, train/loss=0.147860, validation/accuracy=0.754840, validation/loss=1.055118, validation/num_examples=50000
I0229 23:42:08.829979 139590186821376 logging_writer.py:48] [189400] global_step=189400, grad_norm=4.636979103088379, loss=0.6384138464927673
I0229 23:42:42.724772 139590178428672 logging_writer.py:48] [189500] global_step=189500, grad_norm=4.291907787322998, loss=0.6070518493652344
I0229 23:43:16.680998 139590186821376 logging_writer.py:48] [189600] global_step=189600, grad_norm=4.839509010314941, loss=0.6446060538291931
I0229 23:43:50.611286 139590178428672 logging_writer.py:48] [189700] global_step=189700, grad_norm=4.68881893157959, loss=0.5930764079093933
I0229 23:44:24.551053 139590186821376 logging_writer.py:48] [189800] global_step=189800, grad_norm=4.345756530761719, loss=0.5816332101821899
I0229 23:44:58.483244 139590178428672 logging_writer.py:48] [189900] global_step=189900, grad_norm=4.4948811531066895, loss=0.6593352556228638
I0229 23:45:32.453017 139590186821376 logging_writer.py:48] [190000] global_step=190000, grad_norm=4.877439498901367, loss=0.5707115530967712
I0229 23:46:06.387739 139590178428672 logging_writer.py:48] [190100] global_step=190100, grad_norm=4.5033369064331055, loss=0.5800170302391052
I0229 23:46:40.323347 139590186821376 logging_writer.py:48] [190200] global_step=190200, grad_norm=4.478919506072998, loss=0.6399882435798645
I0229 23:47:14.270707 139590178428672 logging_writer.py:48] [190300] global_step=190300, grad_norm=4.409389495849609, loss=0.6095002293586731
I0229 23:47:48.320173 139590186821376 logging_writer.py:48] [190400] global_step=190400, grad_norm=4.167552947998047, loss=0.5614685416221619
I0229 23:48:22.252428 139590178428672 logging_writer.py:48] [190500] global_step=190500, grad_norm=4.164231300354004, loss=0.5800512433052063
I0229 23:48:56.217628 139590186821376 logging_writer.py:48] [190600] global_step=190600, grad_norm=4.546360969543457, loss=0.6032562255859375
I0229 23:49:30.151424 139590178428672 logging_writer.py:48] [190700] global_step=190700, grad_norm=4.546757698059082, loss=0.6343883872032166
I0229 23:50:04.119619 139590186821376 logging_writer.py:48] [190800] global_step=190800, grad_norm=4.810105323791504, loss=0.6333047151565552
I0229 23:50:22.938390 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:50:29.659365 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:50:38.001662 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:50:40.299691 139753105983296 submission_runner.py:411] Time since start: 67171.30s, 	Step: 190857, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.14525823295116425, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0554226636886597, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8206629753112793, 'test/num_examples': 10000, 'score': 64831.91565823555, 'total_duration': 67171.30288815498, 'accumulated_submission_time': 64831.91565823555, 'accumulated_eval_time': 2326.568407535553, 'accumulated_logging_time': 6.143704175949097}
I0229 23:50:40.357532 139589725452032 logging_writer.py:48] [190857] accumulated_eval_time=2326.568408, accumulated_logging_time=6.143704, accumulated_submission_time=64831.915658, global_step=190857, preemption_count=0, score=64831.915658, test/accuracy=0.630900, test/loss=1.820663, test/num_examples=10000, total_duration=67171.302888, train/accuracy=0.960898, train/loss=0.145258, validation/accuracy=0.754840, validation/loss=1.055423, validation/num_examples=50000
I0229 23:50:55.281653 139590144857856 logging_writer.py:48] [190900] global_step=190900, grad_norm=5.125054836273193, loss=0.6353626251220703
I0229 23:51:29.192717 139589725452032 logging_writer.py:48] [191000] global_step=191000, grad_norm=4.487861156463623, loss=0.6117735505104065
I0229 23:52:03.147397 139590144857856 logging_writer.py:48] [191100] global_step=191100, grad_norm=4.7667365074157715, loss=0.6364688873291016
I0229 23:52:37.083123 139589725452032 logging_writer.py:48] [191200] global_step=191200, grad_norm=4.2372870445251465, loss=0.6116393208503723
I0229 23:53:11.020019 139590144857856 logging_writer.py:48] [191300] global_step=191300, grad_norm=4.150355815887451, loss=0.5625326037406921
I0229 23:53:44.970433 139589725452032 logging_writer.py:48] [191400] global_step=191400, grad_norm=4.627120018005371, loss=0.6491022109985352
I0229 23:54:19.013505 139590144857856 logging_writer.py:48] [191500] global_step=191500, grad_norm=4.309606075286865, loss=0.5874300003051758
I0229 23:54:52.974563 139589725452032 logging_writer.py:48] [191600] global_step=191600, grad_norm=4.490201473236084, loss=0.629910409450531
I0229 23:55:26.936106 139590144857856 logging_writer.py:48] [191700] global_step=191700, grad_norm=4.168018341064453, loss=0.6178492903709412
I0229 23:56:00.886278 139589725452032 logging_writer.py:48] [191800] global_step=191800, grad_norm=4.171288013458252, loss=0.5724349021911621
I0229 23:56:34.846210 139590144857856 logging_writer.py:48] [191900] global_step=191900, grad_norm=5.08785343170166, loss=0.6770921945571899
I0229 23:57:08.762272 139589725452032 logging_writer.py:48] [192000] global_step=192000, grad_norm=4.869200229644775, loss=0.6692296266555786
I0229 23:57:42.694903 139590144857856 logging_writer.py:48] [192100] global_step=192100, grad_norm=4.934837341308594, loss=0.6095461845397949
I0229 23:58:16.648751 139589725452032 logging_writer.py:48] [192200] global_step=192200, grad_norm=4.677646636962891, loss=0.665780246257782
I0229 23:58:50.624231 139590144857856 logging_writer.py:48] [192300] global_step=192300, grad_norm=4.2038068771362305, loss=0.5360293984413147
I0229 23:59:10.461473 139753105983296 spec.py:321] Evaluating on the training split.
I0229 23:59:16.460366 139753105983296 spec.py:333] Evaluating on the validation split.
I0229 23:59:24.784051 139753105983296 spec.py:349] Evaluating on the test split.
I0229 23:59:27.046715 139753105983296 submission_runner.py:411] Time since start: 67698.05s, 	Step: 192360, 	{'train/accuracy': 0.9600207209587097, 'train/loss': 0.14829525351524353, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.055193305015564, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.818163514137268, 'test/num_examples': 10000, 'score': 65341.9563536644, 'total_duration': 67698.0499484539, 'accumulated_submission_time': 65341.9563536644, 'accumulated_eval_time': 2343.1536016464233, 'accumulated_logging_time': 6.2114434242248535}
I0229 23:59:27.097024 139590170035968 logging_writer.py:48] [192360] accumulated_eval_time=2343.153602, accumulated_logging_time=6.211443, accumulated_submission_time=65341.956354, global_step=192360, preemption_count=0, score=65341.956354, test/accuracy=0.629900, test/loss=1.818164, test/num_examples=10000, total_duration=67698.049948, train/accuracy=0.960021, train/loss=0.148295, validation/accuracy=0.754940, validation/loss=1.055193, validation/num_examples=50000
I0229 23:59:41.017582 139590178428672 logging_writer.py:48] [192400] global_step=192400, grad_norm=5.129816055297852, loss=0.6872509717941284
I0301 00:00:14.995192 139590170035968 logging_writer.py:48] [192500] global_step=192500, grad_norm=4.162949085235596, loss=0.583122968673706
I0301 00:00:48.929000 139590178428672 logging_writer.py:48] [192600] global_step=192600, grad_norm=4.327064514160156, loss=0.5969085693359375
I0301 00:01:22.934512 139590170035968 logging_writer.py:48] [192700] global_step=192700, grad_norm=4.904703140258789, loss=0.6274247765541077
I0301 00:01:56.864366 139590178428672 logging_writer.py:48] [192800] global_step=192800, grad_norm=4.260664939880371, loss=0.5463632941246033
I0301 00:02:30.817126 139590170035968 logging_writer.py:48] [192900] global_step=192900, grad_norm=4.3538713455200195, loss=0.6744174957275391
I0301 00:03:04.759901 139590178428672 logging_writer.py:48] [193000] global_step=193000, grad_norm=4.601413249969482, loss=0.6205029487609863
I0301 00:03:38.734858 139590170035968 logging_writer.py:48] [193100] global_step=193100, grad_norm=4.762205123901367, loss=0.6456582546234131
I0301 00:04:12.680835 139590178428672 logging_writer.py:48] [193200] global_step=193200, grad_norm=4.240967750549316, loss=0.5709806680679321
I0301 00:04:46.643563 139590170035968 logging_writer.py:48] [193300] global_step=193300, grad_norm=4.194613933563232, loss=0.5857936143875122
I0301 00:05:20.615463 139590178428672 logging_writer.py:48] [193400] global_step=193400, grad_norm=4.277589321136475, loss=0.5460131168365479
I0301 00:05:54.580389 139590170035968 logging_writer.py:48] [193500] global_step=193500, grad_norm=4.560871601104736, loss=0.5934300422668457
I0301 00:06:28.626737 139590178428672 logging_writer.py:48] [193600] global_step=193600, grad_norm=4.541850566864014, loss=0.6798176765441895
I0301 00:07:02.583701 139590170035968 logging_writer.py:48] [193700] global_step=193700, grad_norm=4.922850608825684, loss=0.5938185453414917
I0301 00:07:36.545460 139590178428672 logging_writer.py:48] [193800] global_step=193800, grad_norm=4.725083827972412, loss=0.6011613607406616
I0301 00:07:57.081968 139753105983296 spec.py:321] Evaluating on the training split.
I0301 00:08:03.179347 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 00:08:11.471953 139753105983296 spec.py:349] Evaluating on the test split.
I0301 00:08:13.745539 139753105983296 submission_runner.py:411] Time since start: 68224.75s, 	Step: 193862, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.1444314867258072, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0553990602493286, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8183344602584839, 'test/num_examples': 10000, 'score': 65851.87741112709, 'total_duration': 68224.74874973297, 'accumulated_submission_time': 65851.87741112709, 'accumulated_eval_time': 2359.8171005249023, 'accumulated_logging_time': 6.272164344787598}
I0301 00:08:13.796060 139589717059328 logging_writer.py:48] [193862] accumulated_eval_time=2359.817101, accumulated_logging_time=6.272164, accumulated_submission_time=65851.877411, global_step=193862, preemption_count=0, score=65851.877411, test/accuracy=0.630000, test/loss=1.818334, test/num_examples=10000, total_duration=68224.748750, train/accuracy=0.961655, train/loss=0.144431, validation/accuracy=0.754740, validation/loss=1.055399, validation/num_examples=50000
I0301 00:08:27.009735 139589725452032 logging_writer.py:48] [193900] global_step=193900, grad_norm=4.334592342376709, loss=0.5360867977142334
I0301 00:09:00.924225 139589717059328 logging_writer.py:48] [194000] global_step=194000, grad_norm=4.3688459396362305, loss=0.638604998588562
I0301 00:09:34.823906 139589725452032 logging_writer.py:48] [194100] global_step=194100, grad_norm=4.419130802154541, loss=0.6177734732627869
I0301 00:10:08.763894 139589717059328 logging_writer.py:48] [194200] global_step=194200, grad_norm=4.688048839569092, loss=0.5976004004478455
I0301 00:10:42.713629 139589725452032 logging_writer.py:48] [194300] global_step=194300, grad_norm=4.561029434204102, loss=0.6832868456840515
I0301 00:11:16.658826 139589717059328 logging_writer.py:48] [194400] global_step=194400, grad_norm=4.7301836013793945, loss=0.691105306148529
I0301 00:11:50.574208 139589725452032 logging_writer.py:48] [194500] global_step=194500, grad_norm=4.876349925994873, loss=0.6184296607971191
I0301 00:12:24.557676 139589717059328 logging_writer.py:48] [194600] global_step=194600, grad_norm=4.582612037658691, loss=0.6014583706855774
I0301 00:12:58.465546 139589725452032 logging_writer.py:48] [194700] global_step=194700, grad_norm=4.906228542327881, loss=0.6061867475509644
I0301 00:13:32.386356 139589717059328 logging_writer.py:48] [194800] global_step=194800, grad_norm=4.37920618057251, loss=0.6623775362968445
I0301 00:14:06.328091 139589725452032 logging_writer.py:48] [194900] global_step=194900, grad_norm=4.709978103637695, loss=0.6750240325927734
I0301 00:14:40.264379 139589717059328 logging_writer.py:48] [195000] global_step=195000, grad_norm=4.329906463623047, loss=0.6686623096466064
I0301 00:15:14.158318 139589725452032 logging_writer.py:48] [195100] global_step=195100, grad_norm=4.8273701667785645, loss=0.6380525231361389
I0301 00:15:48.076699 139589717059328 logging_writer.py:48] [195200] global_step=195200, grad_norm=4.489956378936768, loss=0.5999771356582642
I0301 00:16:22.013756 139589725452032 logging_writer.py:48] [195300] global_step=195300, grad_norm=4.5314249992370605, loss=0.632421612739563
I0301 00:16:43.881065 139753105983296 spec.py:321] Evaluating on the training split.
I0301 00:16:49.941909 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 00:16:58.252048 139753105983296 spec.py:349] Evaluating on the test split.
I0301 00:17:00.577670 139753105983296 submission_runner.py:411] Time since start: 68751.58s, 	Step: 195366, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.1464710235595703, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.0569325685501099, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8191959857940674, 'test/num_examples': 10000, 'score': 66361.8992049694, 'total_duration': 68751.58087086678, 'accumulated_submission_time': 66361.8992049694, 'accumulated_eval_time': 2376.5136275291443, 'accumulated_logging_time': 6.332875728607178}
I0301 00:17:00.631029 139589725452032 logging_writer.py:48] [195366] accumulated_eval_time=2376.513628, accumulated_logging_time=6.332876, accumulated_submission_time=66361.899205, global_step=195366, preemption_count=0, score=66361.899205, test/accuracy=0.629700, test/loss=1.819196, test/num_examples=10000, total_duration=68751.580871, train/accuracy=0.960858, train/loss=0.146471, validation/accuracy=0.754620, validation/loss=1.056933, validation/num_examples=50000
I0301 00:17:12.485442 139590170035968 logging_writer.py:48] [195400] global_step=195400, grad_norm=4.544072151184082, loss=0.6506471633911133
I0301 00:17:46.409540 139589725452032 logging_writer.py:48] [195500] global_step=195500, grad_norm=4.370595455169678, loss=0.6356485486030579
I0301 00:18:20.353707 139590170035968 logging_writer.py:48] [195600] global_step=195600, grad_norm=4.40052604675293, loss=0.6042194366455078
I0301 00:18:54.385805 139589725452032 logging_writer.py:48] [195700] global_step=195700, grad_norm=4.155817031860352, loss=0.549572765827179
I0301 00:19:28.332705 139590170035968 logging_writer.py:48] [195800] global_step=195800, grad_norm=4.293005466461182, loss=0.641959547996521
I0301 00:20:02.279401 139589725452032 logging_writer.py:48] [195900] global_step=195900, grad_norm=4.206173419952393, loss=0.5687947273254395
I0301 00:20:36.223580 139590170035968 logging_writer.py:48] [196000] global_step=196000, grad_norm=5.056619167327881, loss=0.6751983165740967
I0301 00:21:10.171208 139589725452032 logging_writer.py:48] [196100] global_step=196100, grad_norm=4.882359027862549, loss=0.7379398345947266
I0301 00:21:44.145957 139590170035968 logging_writer.py:48] [196200] global_step=196200, grad_norm=4.619370460510254, loss=0.5915281772613525
I0301 00:22:18.114923 139589725452032 logging_writer.py:48] [196300] global_step=196300, grad_norm=4.492049694061279, loss=0.6528401374816895
I0301 00:22:52.056754 139590170035968 logging_writer.py:48] [196400] global_step=196400, grad_norm=4.5985941886901855, loss=0.5650957226753235
I0301 00:23:26.003788 139589725452032 logging_writer.py:48] [196500] global_step=196500, grad_norm=4.122310638427734, loss=0.503649890422821
I0301 00:23:59.980133 139590170035968 logging_writer.py:48] [196600] global_step=196600, grad_norm=4.65647554397583, loss=0.5657277703285217
I0301 00:24:33.909651 139589725452032 logging_writer.py:48] [196700] global_step=196700, grad_norm=4.373471260070801, loss=0.5924002528190613
I0301 00:25:08.013880 139590170035968 logging_writer.py:48] [196800] global_step=196800, grad_norm=4.186634540557861, loss=0.6326253414154053
I0301 00:25:30.916352 139753105983296 spec.py:321] Evaluating on the training split.
I0301 00:25:36.933739 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 00:25:45.173871 139753105983296 spec.py:349] Evaluating on the test split.
I0301 00:25:47.482702 139753105983296 submission_runner.py:411] Time since start: 69278.49s, 	Step: 196869, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14386673271656036, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.055078148841858, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8178199529647827, 'test/num_examples': 10000, 'score': 66872.12175607681, 'total_duration': 69278.48593592644, 'accumulated_submission_time': 66872.12175607681, 'accumulated_eval_time': 2393.079930305481, 'accumulated_logging_time': 6.3962483406066895}
I0301 00:25:47.531473 139590144857856 logging_writer.py:48] [196869] accumulated_eval_time=2393.079930, accumulated_logging_time=6.396248, accumulated_submission_time=66872.121756, global_step=196869, preemption_count=0, score=66872.121756, test/accuracy=0.629900, test/loss=1.817820, test/num_examples=10000, total_duration=69278.485936, train/accuracy=0.961137, train/loss=0.143867, validation/accuracy=0.754780, validation/loss=1.055078, validation/num_examples=50000
I0301 00:25:58.390401 139590153250560 logging_writer.py:48] [196900] global_step=196900, grad_norm=4.602718353271484, loss=0.6605056524276733
I0301 00:26:32.268186 139590144857856 logging_writer.py:48] [197000] global_step=197000, grad_norm=4.310225963592529, loss=0.6212317943572998
I0301 00:27:06.186810 139590153250560 logging_writer.py:48] [197100] global_step=197100, grad_norm=4.59121561050415, loss=0.6396099925041199
I0301 00:27:40.115448 139590144857856 logging_writer.py:48] [197200] global_step=197200, grad_norm=4.317368030548096, loss=0.6190197467803955
I0301 00:28:14.057749 139590153250560 logging_writer.py:48] [197300] global_step=197300, grad_norm=4.603696823120117, loss=0.6401897072792053
I0301 00:28:48.013177 139590144857856 logging_writer.py:48] [197400] global_step=197400, grad_norm=4.2722907066345215, loss=0.5632177591323853
I0301 00:29:21.973020 139590153250560 logging_writer.py:48] [197500] global_step=197500, grad_norm=4.639630317687988, loss=0.588223397731781
I0301 00:29:55.893227 139590144857856 logging_writer.py:48] [197600] global_step=197600, grad_norm=4.643184661865234, loss=0.5936465859413147
I0301 00:30:29.842550 139590153250560 logging_writer.py:48] [197700] global_step=197700, grad_norm=4.220898151397705, loss=0.5740565061569214
I0301 00:31:03.986727 139590144857856 logging_writer.py:48] [197800] global_step=197800, grad_norm=4.302344799041748, loss=0.5826438665390015
I0301 00:31:37.923930 139590153250560 logging_writer.py:48] [197900] global_step=197900, grad_norm=4.649181842803955, loss=0.6114782094955444
I0301 00:32:11.820162 139590144857856 logging_writer.py:48] [198000] global_step=198000, grad_norm=5.13132381439209, loss=0.6272963881492615
I0301 00:32:45.770588 139590153250560 logging_writer.py:48] [198100] global_step=198100, grad_norm=4.450011253356934, loss=0.6438817381858826
I0301 00:33:19.708153 139590144857856 logging_writer.py:48] [198200] global_step=198200, grad_norm=4.415189266204834, loss=0.6497215628623962
I0301 00:33:53.649256 139590153250560 logging_writer.py:48] [198300] global_step=198300, grad_norm=4.6644182205200195, loss=0.6344941854476929
I0301 00:34:17.558127 139753105983296 spec.py:321] Evaluating on the training split.
I0301 00:34:23.576771 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 00:34:31.948490 139753105983296 spec.py:349] Evaluating on the test split.
I0301 00:34:34.224231 139753105983296 submission_runner.py:411] Time since start: 69805.23s, 	Step: 198372, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14633330702781677, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0546435117721558, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8186239004135132, 'test/num_examples': 10000, 'score': 67382.08436250687, 'total_duration': 69805.22742986679, 'accumulated_submission_time': 67382.08436250687, 'accumulated_eval_time': 2409.745950460434, 'accumulated_logging_time': 6.455162048339844}
I0301 00:34:34.278112 139589717059328 logging_writer.py:48] [198372] accumulated_eval_time=2409.745950, accumulated_logging_time=6.455162, accumulated_submission_time=67382.084363, global_step=198372, preemption_count=0, score=67382.084363, test/accuracy=0.631400, test/loss=1.818624, test/num_examples=10000, total_duration=69805.227430, train/accuracy=0.961037, train/loss=0.146333, validation/accuracy=0.754760, validation/loss=1.054644, validation/num_examples=50000
I0301 00:34:44.105502 139589725452032 logging_writer.py:48] [198400] global_step=198400, grad_norm=5.041673183441162, loss=0.5937651991844177
I0301 00:35:18.054593 139589717059328 logging_writer.py:48] [198500] global_step=198500, grad_norm=4.084846019744873, loss=0.5369559526443481
I0301 00:35:52.012486 139589725452032 logging_writer.py:48] [198600] global_step=198600, grad_norm=4.694020748138428, loss=0.5939376354217529
I0301 00:36:25.996609 139589717059328 logging_writer.py:48] [198700] global_step=198700, grad_norm=4.754132270812988, loss=0.6089307069778442
I0301 00:36:59.979081 139589725452032 logging_writer.py:48] [198800] global_step=198800, grad_norm=4.278288841247559, loss=0.5691409111022949
I0301 00:37:34.054351 139589717059328 logging_writer.py:48] [198900] global_step=198900, grad_norm=4.663445472717285, loss=0.6748983263969421
I0301 00:38:08.021621 139589725452032 logging_writer.py:48] [199000] global_step=199000, grad_norm=4.669063568115234, loss=0.5594989061355591
I0301 00:38:41.933197 139589717059328 logging_writer.py:48] [199100] global_step=199100, grad_norm=4.360884666442871, loss=0.577778160572052
I0301 00:39:15.888577 139589725452032 logging_writer.py:48] [199200] global_step=199200, grad_norm=4.676558494567871, loss=0.5808748602867126
I0301 00:39:49.861703 139589717059328 logging_writer.py:48] [199300] global_step=199300, grad_norm=4.755580902099609, loss=0.5864955186843872
I0301 00:40:23.823394 139589725452032 logging_writer.py:48] [199400] global_step=199400, grad_norm=4.4908857345581055, loss=0.6354643106460571
I0301 00:40:57.796215 139589717059328 logging_writer.py:48] [199500] global_step=199500, grad_norm=4.326759338378906, loss=0.5617085695266724
I0301 00:41:31.759826 139589725452032 logging_writer.py:48] [199600] global_step=199600, grad_norm=4.508458137512207, loss=0.646581768989563
I0301 00:42:05.705583 139589717059328 logging_writer.py:48] [199700] global_step=199700, grad_norm=4.2159271240234375, loss=0.6133835911750793
I0301 00:42:39.657306 139589725452032 logging_writer.py:48] [199800] global_step=199800, grad_norm=4.658958911895752, loss=0.6296266317367554
I0301 00:43:04.240526 139753105983296 spec.py:321] Evaluating on the training split.
I0301 00:43:10.586768 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 00:43:18.966927 139753105983296 spec.py:349] Evaluating on the test split.
I0301 00:43:21.224963 139753105983296 submission_runner.py:411] Time since start: 70332.23s, 	Step: 199874, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14503231644630432, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0563466548919678, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8194706439971924, 'test/num_examples': 10000, 'score': 67891.98360681534, 'total_duration': 70332.2281923294, 'accumulated_submission_time': 67891.98360681534, 'accumulated_eval_time': 2426.7303347587585, 'accumulated_logging_time': 6.520244121551514}
I0301 00:43:21.279506 139589725452032 logging_writer.py:48] [199874] accumulated_eval_time=2426.730335, accumulated_logging_time=6.520244, accumulated_submission_time=67891.983607, global_step=199874, preemption_count=0, score=67891.983607, test/accuracy=0.630000, test/loss=1.819471, test/num_examples=10000, total_duration=70332.228192, train/accuracy=0.960977, train/loss=0.145032, validation/accuracy=0.754660, validation/loss=1.056347, validation/num_examples=50000
I0301 00:43:30.448517 139590144857856 logging_writer.py:48] [199900] global_step=199900, grad_norm=4.345987319946289, loss=0.5715043544769287
I0301 00:44:04.332510 139589725452032 logging_writer.py:48] [200000] global_step=200000, grad_norm=4.3861403465271, loss=0.5767499804496765
I0301 00:44:38.265708 139590144857856 logging_writer.py:48] [200100] global_step=200100, grad_norm=4.611813068389893, loss=0.676750898361206
I0301 00:45:12.218701 139589725452032 logging_writer.py:48] [200200] global_step=200200, grad_norm=4.992525100708008, loss=0.6816195249557495
I0301 00:45:46.157919 139590144857856 logging_writer.py:48] [200300] global_step=200300, grad_norm=4.754812717437744, loss=0.6094454526901245
I0301 00:46:20.123472 139589725452032 logging_writer.py:48] [200400] global_step=200400, grad_norm=5.028341770172119, loss=0.6341446042060852
I0301 00:46:54.065233 139590144857856 logging_writer.py:48] [200500] global_step=200500, grad_norm=4.3348894119262695, loss=0.6257182359695435
I0301 00:47:28.014435 139589725452032 logging_writer.py:48] [200600] global_step=200600, grad_norm=4.731578350067139, loss=0.6323053240776062
I0301 00:48:01.922032 139590144857856 logging_writer.py:48] [200700] global_step=200700, grad_norm=4.16508674621582, loss=0.5347893834114075
I0301 00:48:35.841511 139589725452032 logging_writer.py:48] [200800] global_step=200800, grad_norm=4.499577522277832, loss=0.6330586671829224
I0301 00:49:09.789987 139590144857856 logging_writer.py:48] [200900] global_step=200900, grad_norm=4.6560821533203125, loss=0.6294810175895691
I0301 00:49:43.775860 139589725452032 logging_writer.py:48] [201000] global_step=201000, grad_norm=4.32464075088501, loss=0.6029366254806519
I0301 00:50:17.707459 139590144857856 logging_writer.py:48] [201100] global_step=201100, grad_norm=4.469735622406006, loss=0.5652675628662109
I0301 00:50:51.658975 139589725452032 logging_writer.py:48] [201200] global_step=201200, grad_norm=5.505691051483154, loss=0.6188172101974487
I0301 00:51:25.562500 139590144857856 logging_writer.py:48] [201300] global_step=201300, grad_norm=4.156332015991211, loss=0.5420931577682495
I0301 00:51:51.507525 139753105983296 spec.py:321] Evaluating on the training split.
I0301 00:51:57.596042 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 00:52:05.946326 139753105983296 spec.py:349] Evaluating on the test split.
I0301 00:52:08.246369 139753105983296 submission_runner.py:411] Time since start: 70859.25s, 	Step: 201378, 	{'train/accuracy': 0.9603993892669678, 'train/loss': 0.14662966132164001, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0545313358306885, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8182106018066406, 'test/num_examples': 10000, 'score': 68402.14937877655, 'total_duration': 70859.24960279465, 'accumulated_submission_time': 68402.14937877655, 'accumulated_eval_time': 2443.469132423401, 'accumulated_logging_time': 6.584799528121948}
I0301 00:52:08.299435 139589717059328 logging_writer.py:48] [201378] accumulated_eval_time=2443.469132, accumulated_logging_time=6.584800, accumulated_submission_time=68402.149379, global_step=201378, preemption_count=0, score=68402.149379, test/accuracy=0.629700, test/loss=1.818211, test/num_examples=10000, total_duration=70859.249603, train/accuracy=0.960399, train/loss=0.146630, validation/accuracy=0.754980, validation/loss=1.054531, validation/num_examples=50000
I0301 00:52:16.115734 139589725452032 logging_writer.py:48] [201400] global_step=201400, grad_norm=4.533996105194092, loss=0.639640212059021
I0301 00:52:50.028525 139589717059328 logging_writer.py:48] [201500] global_step=201500, grad_norm=4.304296493530273, loss=0.6243269443511963
I0301 00:53:23.979509 139589725452032 logging_writer.py:48] [201600] global_step=201600, grad_norm=4.187157154083252, loss=0.5653846859931946
I0301 00:53:57.954246 139589717059328 logging_writer.py:48] [201700] global_step=201700, grad_norm=4.338296413421631, loss=0.5648330450057983
I0301 00:54:31.933153 139589725452032 logging_writer.py:48] [201800] global_step=201800, grad_norm=4.432356357574463, loss=0.6478402614593506
I0301 00:55:05.917806 139589717059328 logging_writer.py:48] [201900] global_step=201900, grad_norm=4.344829559326172, loss=0.5842165350914001
I0301 00:55:39.978840 139589725452032 logging_writer.py:48] [202000] global_step=202000, grad_norm=4.669882297515869, loss=0.6401825547218323
I0301 00:56:13.937068 139589717059328 logging_writer.py:48] [202100] global_step=202100, grad_norm=4.431425094604492, loss=0.6557202339172363
I0301 00:56:47.900079 139589725452032 logging_writer.py:48] [202200] global_step=202200, grad_norm=4.063714981079102, loss=0.5276501774787903
I0301 00:57:21.862073 139589717059328 logging_writer.py:48] [202300] global_step=202300, grad_norm=4.6303839683532715, loss=0.6548616290092468
I0301 00:57:55.839132 139589725452032 logging_writer.py:48] [202400] global_step=202400, grad_norm=4.390069007873535, loss=0.5914697051048279
I0301 00:58:29.789396 139589717059328 logging_writer.py:48] [202500] global_step=202500, grad_norm=4.44761848449707, loss=0.6121150851249695
I0301 00:59:03.783630 139589725452032 logging_writer.py:48] [202600] global_step=202600, grad_norm=4.093761444091797, loss=0.528691828250885
I0301 00:59:37.734637 139589717059328 logging_writer.py:48] [202700] global_step=202700, grad_norm=4.3896307945251465, loss=0.657371461391449
I0301 01:00:11.701408 139589725452032 logging_writer.py:48] [202800] global_step=202800, grad_norm=4.836143970489502, loss=0.6251967549324036
I0301 01:00:38.305601 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:00:44.320022 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:00:52.582035 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:00:54.910444 139753105983296 submission_runner.py:411] Time since start: 71385.91s, 	Step: 202880, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14688700437545776, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0562556982040405, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.819766640663147, 'test/num_examples': 10000, 'score': 68912.09048843384, 'total_duration': 71385.91350674629, 'accumulated_submission_time': 68912.09048843384, 'accumulated_eval_time': 2460.0737595558167, 'accumulated_logging_time': 6.650354385375977}
I0301 01:00:54.971768 139590178428672 logging_writer.py:48] [202880] accumulated_eval_time=2460.073760, accumulated_logging_time=6.650354, accumulated_submission_time=68912.090488, global_step=202880, preemption_count=0, score=68912.090488, test/accuracy=0.629700, test/loss=1.819767, test/num_examples=10000, total_duration=71385.913507, train/accuracy=0.960758, train/loss=0.146887, validation/accuracy=0.754920, validation/loss=1.056256, validation/num_examples=50000
I0301 01:01:02.134010 139590186821376 logging_writer.py:48] [202900] global_step=202900, grad_norm=5.010505199432373, loss=0.6643204689025879
I0301 01:01:36.073873 139590178428672 logging_writer.py:48] [203000] global_step=203000, grad_norm=4.322498798370361, loss=0.6322661638259888
I0301 01:02:09.980107 139590186821376 logging_writer.py:48] [203100] global_step=203100, grad_norm=4.421834468841553, loss=0.6049023270606995
I0301 01:02:43.919099 139590178428672 logging_writer.py:48] [203200] global_step=203200, grad_norm=4.655712604522705, loss=0.58392333984375
I0301 01:03:17.860738 139590186821376 logging_writer.py:48] [203300] global_step=203300, grad_norm=4.728853225708008, loss=0.6204216480255127
I0301 01:03:51.784482 139590178428672 logging_writer.py:48] [203400] global_step=203400, grad_norm=4.6053948402404785, loss=0.5939348936080933
I0301 01:04:25.704355 139590186821376 logging_writer.py:48] [203500] global_step=203500, grad_norm=4.416017055511475, loss=0.5662615895271301
I0301 01:04:59.617693 139590178428672 logging_writer.py:48] [203600] global_step=203600, grad_norm=4.6468424797058105, loss=0.6828344464302063
I0301 01:05:33.577831 139590186821376 logging_writer.py:48] [203700] global_step=203700, grad_norm=4.489858627319336, loss=0.5989218354225159
I0301 01:06:07.531236 139590178428672 logging_writer.py:48] [203800] global_step=203800, grad_norm=4.853452205657959, loss=0.6796545386314392
I0301 01:06:41.450851 139590186821376 logging_writer.py:48] [203900] global_step=203900, grad_norm=4.545426368713379, loss=0.6393911838531494
I0301 01:07:15.403000 139590178428672 logging_writer.py:48] [204000] global_step=204000, grad_norm=4.955739974975586, loss=0.6830124258995056
I0301 01:07:49.447540 139590186821376 logging_writer.py:48] [204100] global_step=204100, grad_norm=4.314431667327881, loss=0.6036312580108643
I0301 01:08:23.399333 139590178428672 logging_writer.py:48] [204200] global_step=204200, grad_norm=4.813520908355713, loss=0.6813642978668213
I0301 01:08:57.320213 139590186821376 logging_writer.py:48] [204300] global_step=204300, grad_norm=4.192523956298828, loss=0.5781774520874023
I0301 01:09:24.963021 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:09:30.988804 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:09:39.380578 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:09:41.677435 139753105983296 submission_runner.py:411] Time since start: 71912.68s, 	Step: 204383, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14751145243644714, 'validation/accuracy': 0.7545999884605408, 'validation/loss': 1.0560839176177979, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8184700012207031, 'test/num_examples': 10000, 'score': 69422.0167837143, 'total_duration': 71912.68066692352, 'accumulated_submission_time': 69422.0167837143, 'accumulated_eval_time': 2476.7881228923798, 'accumulated_logging_time': 6.723001718521118}
I0301 01:09:41.737361 139589725452032 logging_writer.py:48] [204383] accumulated_eval_time=2476.788123, accumulated_logging_time=6.723002, accumulated_submission_time=69422.016784, global_step=204383, preemption_count=0, score=69422.016784, test/accuracy=0.630900, test/loss=1.818470, test/num_examples=10000, total_duration=71912.680667, train/accuracy=0.961256, train/loss=0.147511, validation/accuracy=0.754600, validation/loss=1.056084, validation/num_examples=50000
I0301 01:09:47.935887 139590144857856 logging_writer.py:48] [204400] global_step=204400, grad_norm=4.695456027984619, loss=0.6327949166297913
I0301 01:10:21.852626 139589725452032 logging_writer.py:48] [204500] global_step=204500, grad_norm=4.66640043258667, loss=0.6247625946998596
I0301 01:10:55.781117 139590144857856 logging_writer.py:48] [204600] global_step=204600, grad_norm=5.011876106262207, loss=0.7913292646408081
I0301 01:11:29.716861 139589725452032 logging_writer.py:48] [204700] global_step=204700, grad_norm=4.229589939117432, loss=0.6102675795555115
I0301 01:12:03.690458 139590144857856 logging_writer.py:48] [204800] global_step=204800, grad_norm=4.97903299331665, loss=0.6297594308853149
I0301 01:12:37.594016 139589725452032 logging_writer.py:48] [204900] global_step=204900, grad_norm=4.88607931137085, loss=0.6502387523651123
I0301 01:13:11.538110 139590144857856 logging_writer.py:48] [205000] global_step=205000, grad_norm=5.018700122833252, loss=0.6620497703552246
I0301 01:13:45.514595 139589725452032 logging_writer.py:48] [205100] global_step=205100, grad_norm=4.7374749183654785, loss=0.622096598148346
I0301 01:14:19.564599 139590144857856 logging_writer.py:48] [205200] global_step=205200, grad_norm=4.434010982513428, loss=0.6036663055419922
I0301 01:14:53.529476 139589725452032 logging_writer.py:48] [205300] global_step=205300, grad_norm=4.239039421081543, loss=0.5793845653533936
I0301 01:15:27.484100 139590144857856 logging_writer.py:48] [205400] global_step=205400, grad_norm=4.277502536773682, loss=0.6110742688179016
I0301 01:16:01.422157 139589725452032 logging_writer.py:48] [205500] global_step=205500, grad_norm=4.768789291381836, loss=0.6881811618804932
I0301 01:16:35.355069 139590144857856 logging_writer.py:48] [205600] global_step=205600, grad_norm=4.705533027648926, loss=0.6040555238723755
I0301 01:17:09.335588 139589725452032 logging_writer.py:48] [205700] global_step=205700, grad_norm=4.197648048400879, loss=0.5710391998291016
I0301 01:17:43.285011 139590144857856 logging_writer.py:48] [205800] global_step=205800, grad_norm=4.383714199066162, loss=0.5969743132591248
I0301 01:18:11.954516 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:18:17.964875 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:18:26.257725 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:18:28.592543 139753105983296 submission_runner.py:411] Time since start: 72439.60s, 	Step: 205886, 	{'train/accuracy': 0.958984375, 'train/loss': 0.14982397854328156, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0548142194747925, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.818460464477539, 'test/num_examples': 10000, 'score': 69932.16599369049, 'total_duration': 72439.5957725048, 'accumulated_submission_time': 69932.16599369049, 'accumulated_eval_time': 2493.426098585129, 'accumulated_logging_time': 6.797011137008667}
I0301 01:18:28.650237 139589717059328 logging_writer.py:48] [205886] accumulated_eval_time=2493.426099, accumulated_logging_time=6.797011, accumulated_submission_time=69932.165994, global_step=205886, preemption_count=0, score=69932.165994, test/accuracy=0.630700, test/loss=1.818460, test/num_examples=10000, total_duration=72439.595773, train/accuracy=0.958984, train/loss=0.149824, validation/accuracy=0.754900, validation/loss=1.054814, validation/num_examples=50000
I0301 01:18:33.735116 139589725452032 logging_writer.py:48] [205900] global_step=205900, grad_norm=4.391523838043213, loss=0.5363457202911377
I0301 01:19:07.654629 139589717059328 logging_writer.py:48] [206000] global_step=206000, grad_norm=5.053140640258789, loss=0.733525276184082
I0301 01:19:41.591315 139589725452032 logging_writer.py:48] [206100] global_step=206100, grad_norm=4.762786388397217, loss=0.6582516431808472
I0301 01:20:15.633992 139589717059328 logging_writer.py:48] [206200] global_step=206200, grad_norm=4.611969470977783, loss=0.6584228277206421
I0301 01:20:49.584404 139589725452032 logging_writer.py:48] [206300] global_step=206300, grad_norm=4.212099552154541, loss=0.5271904468536377
I0301 01:21:23.535756 139589717059328 logging_writer.py:48] [206400] global_step=206400, grad_norm=4.690535068511963, loss=0.6120375394821167
I0301 01:21:57.471751 139589725452032 logging_writer.py:48] [206500] global_step=206500, grad_norm=4.493221282958984, loss=0.6764976382255554
I0301 01:22:31.443312 139589717059328 logging_writer.py:48] [206600] global_step=206600, grad_norm=4.634244918823242, loss=0.620034396648407
I0301 01:23:05.386132 139589725452032 logging_writer.py:48] [206700] global_step=206700, grad_norm=4.492920875549316, loss=0.6435827016830444
I0301 01:23:39.311903 139589717059328 logging_writer.py:48] [206800] global_step=206800, grad_norm=4.697288513183594, loss=0.6363675594329834
I0301 01:24:13.271061 139589725452032 logging_writer.py:48] [206900] global_step=206900, grad_norm=4.652371883392334, loss=0.6554885506629944
I0301 01:24:47.228310 139589717059328 logging_writer.py:48] [207000] global_step=207000, grad_norm=4.682149410247803, loss=0.6216313242912292
I0301 01:25:21.161145 139589725452032 logging_writer.py:48] [207100] global_step=207100, grad_norm=4.658478736877441, loss=0.6769974231719971
I0301 01:25:55.111537 139589717059328 logging_writer.py:48] [207200] global_step=207200, grad_norm=4.541083812713623, loss=0.5864093899726868
I0301 01:26:29.123449 139589725452032 logging_writer.py:48] [207300] global_step=207300, grad_norm=4.255620956420898, loss=0.6172641515731812
I0301 01:26:58.786173 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:27:04.922657 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:27:13.209792 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:27:15.559118 139753105983296 submission_runner.py:411] Time since start: 72966.56s, 	Step: 207389, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1428539901971817, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0566010475158691, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8208805322647095, 'test/num_examples': 10000, 'score': 70442.23829221725, 'total_duration': 72966.56232953072, 'accumulated_submission_time': 70442.23829221725, 'accumulated_eval_time': 2510.198974609375, 'accumulated_logging_time': 6.865321636199951}
I0301 01:27:15.615637 139589717059328 logging_writer.py:48] [207389] accumulated_eval_time=2510.198975, accumulated_logging_time=6.865322, accumulated_submission_time=70442.238292, global_step=207389, preemption_count=0, score=70442.238292, test/accuracy=0.630300, test/loss=1.820881, test/num_examples=10000, total_duration=72966.562330, train/accuracy=0.961217, train/loss=0.142854, validation/accuracy=0.755160, validation/loss=1.056601, validation/num_examples=50000
I0301 01:27:19.699286 139589725452032 logging_writer.py:48] [207400] global_step=207400, grad_norm=4.7789435386657715, loss=0.6731351613998413
I0301 01:27:53.591474 139589717059328 logging_writer.py:48] [207500] global_step=207500, grad_norm=4.350133895874023, loss=0.612865686416626
I0301 01:28:27.542220 139589725452032 logging_writer.py:48] [207600] global_step=207600, grad_norm=4.762288570404053, loss=0.6376029253005981
I0301 01:29:01.518256 139589717059328 logging_writer.py:48] [207700] global_step=207700, grad_norm=4.400956630706787, loss=0.6600474715232849
I0301 01:29:35.462243 139589725452032 logging_writer.py:48] [207800] global_step=207800, grad_norm=4.52308464050293, loss=0.6670171618461609
I0301 01:30:09.423978 139589717059328 logging_writer.py:48] [207900] global_step=207900, grad_norm=4.485256671905518, loss=0.6485733985900879
I0301 01:30:43.374763 139589725452032 logging_writer.py:48] [208000] global_step=208000, grad_norm=4.531042098999023, loss=0.6620137095451355
I0301 01:31:17.311666 139589717059328 logging_writer.py:48] [208100] global_step=208100, grad_norm=4.322378635406494, loss=0.6119750142097473
I0301 01:31:51.280426 139589725452032 logging_writer.py:48] [208200] global_step=208200, grad_norm=5.121443748474121, loss=0.7388129830360413
I0301 01:32:25.300612 139589717059328 logging_writer.py:48] [208300] global_step=208300, grad_norm=4.705967426300049, loss=0.6501342058181763
I0301 01:32:59.256053 139589725452032 logging_writer.py:48] [208400] global_step=208400, grad_norm=4.242734909057617, loss=0.6213065981864929
I0301 01:33:33.206694 139589717059328 logging_writer.py:48] [208500] global_step=208500, grad_norm=4.395547866821289, loss=0.665273904800415
I0301 01:34:07.180062 139589725452032 logging_writer.py:48] [208600] global_step=208600, grad_norm=4.821498870849609, loss=0.6026338934898376
I0301 01:34:41.120159 139589717059328 logging_writer.py:48] [208700] global_step=208700, grad_norm=4.848248481750488, loss=0.6257492303848267
I0301 01:35:15.074435 139589725452032 logging_writer.py:48] [208800] global_step=208800, grad_norm=4.559478282928467, loss=0.5832681655883789
I0301 01:35:45.754479 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:35:51.791022 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:36:00.045404 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:36:02.348063 139753105983296 submission_runner.py:411] Time since start: 73493.35s, 	Step: 208892, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.14741432666778564, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.055539608001709, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8183298110961914, 'test/num_examples': 10000, 'score': 70952.31413340569, 'total_duration': 73493.35128498077, 'accumulated_submission_time': 70952.31413340569, 'accumulated_eval_time': 2526.792496442795, 'accumulated_logging_time': 6.931980609893799}
I0301 01:36:02.404323 139589725452032 logging_writer.py:48] [208892] accumulated_eval_time=2526.792496, accumulated_logging_time=6.931981, accumulated_submission_time=70952.314133, global_step=208892, preemption_count=0, score=70952.314133, test/accuracy=0.630300, test/loss=1.818330, test/num_examples=10000, total_duration=73493.351285, train/accuracy=0.960360, train/loss=0.147414, validation/accuracy=0.754900, validation/loss=1.055540, validation/num_examples=50000
I0301 01:36:05.478688 139590178428672 logging_writer.py:48] [208900] global_step=208900, grad_norm=4.846644878387451, loss=0.669636607170105
I0301 01:36:39.376772 139589725452032 logging_writer.py:48] [209000] global_step=209000, grad_norm=4.432084083557129, loss=0.5705925226211548
I0301 01:37:13.311747 139590178428672 logging_writer.py:48] [209100] global_step=209100, grad_norm=4.377460479736328, loss=0.5997058153152466
I0301 01:37:47.279824 139589725452032 logging_writer.py:48] [209200] global_step=209200, grad_norm=4.539652347564697, loss=0.6172483563423157
I0301 01:38:21.230322 139590178428672 logging_writer.py:48] [209300] global_step=209300, grad_norm=4.851040363311768, loss=0.6231663227081299
I0301 01:38:55.264260 139589725452032 logging_writer.py:48] [209400] global_step=209400, grad_norm=5.256971836090088, loss=0.628350019454956
I0301 01:39:29.195647 139590178428672 logging_writer.py:48] [209500] global_step=209500, grad_norm=4.884725093841553, loss=0.6650568842887878
I0301 01:40:03.143701 139589725452032 logging_writer.py:48] [209600] global_step=209600, grad_norm=4.474121570587158, loss=0.6273568868637085
I0301 01:40:37.071613 139590178428672 logging_writer.py:48] [209700] global_step=209700, grad_norm=4.983091831207275, loss=0.6923843622207642
I0301 01:41:11.033987 139589725452032 logging_writer.py:48] [209800] global_step=209800, grad_norm=4.192979335784912, loss=0.6310232877731323
I0301 01:41:44.970460 139590178428672 logging_writer.py:48] [209900] global_step=209900, grad_norm=4.59036111831665, loss=0.6078646183013916
I0301 01:42:18.900234 139589725452032 logging_writer.py:48] [210000] global_step=210000, grad_norm=4.496585845947266, loss=0.6770458817481995
I0301 01:42:52.850900 139590178428672 logging_writer.py:48] [210100] global_step=210100, grad_norm=4.461989402770996, loss=0.6374011039733887
I0301 01:43:26.784296 139589725452032 logging_writer.py:48] [210200] global_step=210200, grad_norm=4.416947841644287, loss=0.5996605753898621
I0301 01:44:00.685120 139590178428672 logging_writer.py:48] [210300] global_step=210300, grad_norm=4.561623573303223, loss=0.6066773533821106
I0301 01:44:32.397212 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:44:38.651585 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:44:47.028776 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:44:49.216118 139753105983296 submission_runner.py:411] Time since start: 74020.22s, 	Step: 210395, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.15008175373077393, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0549089908599854, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8177235126495361, 'test/num_examples': 10000, 'score': 71462.24423503876, 'total_duration': 74020.21932840347, 'accumulated_submission_time': 71462.24423503876, 'accumulated_eval_time': 2543.6113333702087, 'accumulated_logging_time': 6.999336004257202}
I0301 01:44:49.265095 139590153250560 logging_writer.py:48] [210395] accumulated_eval_time=2543.611333, accumulated_logging_time=6.999336, accumulated_submission_time=71462.244235, global_step=210395, preemption_count=0, score=71462.244235, test/accuracy=0.630700, test/loss=1.817724, test/num_examples=10000, total_duration=74020.219328, train/accuracy=0.959602, train/loss=0.150082, validation/accuracy=0.755180, validation/loss=1.054909, validation/num_examples=50000
I0301 01:44:51.315489 139590161643264 logging_writer.py:48] [210400] global_step=210400, grad_norm=5.068379878997803, loss=0.6244586706161499
I0301 01:45:25.207732 139590153250560 logging_writer.py:48] [210500] global_step=210500, grad_norm=4.392064571380615, loss=0.6654383540153503
I0301 01:45:59.107594 139590161643264 logging_writer.py:48] [210600] global_step=210600, grad_norm=4.697453022003174, loss=0.6429682970046997
I0301 01:46:33.069497 139590153250560 logging_writer.py:48] [210700] global_step=210700, grad_norm=4.360239028930664, loss=0.5931079983711243
I0301 01:47:07.043571 139590161643264 logging_writer.py:48] [210800] global_step=210800, grad_norm=4.53683614730835, loss=0.6146430373191833
I0301 01:47:40.985635 139590153250560 logging_writer.py:48] [210900] global_step=210900, grad_norm=4.8581156730651855, loss=0.6278618574142456
I0301 01:48:14.907166 139590161643264 logging_writer.py:48] [211000] global_step=211000, grad_norm=4.841800212860107, loss=0.6689351201057434
I0301 01:48:48.882715 139590153250560 logging_writer.py:48] [211100] global_step=211100, grad_norm=4.490025997161865, loss=0.5870134830474854
I0301 01:49:22.823495 139590161643264 logging_writer.py:48] [211200] global_step=211200, grad_norm=4.855200290679932, loss=0.612801194190979
I0301 01:49:56.796065 139590153250560 logging_writer.py:48] [211300] global_step=211300, grad_norm=4.4382147789001465, loss=0.6175436973571777
I0301 01:50:30.730615 139590161643264 logging_writer.py:48] [211400] global_step=211400, grad_norm=4.5642523765563965, loss=0.5955230593681335
I0301 01:51:04.771744 139590153250560 logging_writer.py:48] [211500] global_step=211500, grad_norm=4.322938442230225, loss=0.6434891223907471
I0301 01:51:38.704013 139590161643264 logging_writer.py:48] [211600] global_step=211600, grad_norm=4.613452434539795, loss=0.6543478965759277
I0301 01:52:12.674248 139590153250560 logging_writer.py:48] [211700] global_step=211700, grad_norm=4.692051887512207, loss=0.6229230165481567
I0301 01:52:46.626655 139590161643264 logging_writer.py:48] [211800] global_step=211800, grad_norm=4.442145347595215, loss=0.5532269477844238
I0301 01:53:19.327245 139753105983296 spec.py:321] Evaluating on the training split.
I0301 01:53:25.327782 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 01:53:33.653080 139753105983296 spec.py:349] Evaluating on the test split.
I0301 01:53:35.928940 139753105983296 submission_runner.py:411] Time since start: 74546.93s, 	Step: 211898, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.1484949290752411, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0549051761627197, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8179594278335571, 'test/num_examples': 10000, 'score': 71972.2426841259, 'total_duration': 74546.93216991425, 'accumulated_submission_time': 71972.2426841259, 'accumulated_eval_time': 2560.2129786014557, 'accumulated_logging_time': 7.059044599533081}
I0301 01:53:35.986487 139589708666624 logging_writer.py:48] [211898] accumulated_eval_time=2560.212979, accumulated_logging_time=7.059045, accumulated_submission_time=71972.242684, global_step=211898, preemption_count=0, score=71972.242684, test/accuracy=0.630300, test/loss=1.817959, test/num_examples=10000, total_duration=74546.932170, train/accuracy=0.959901, train/loss=0.148495, validation/accuracy=0.754780, validation/loss=1.054905, validation/num_examples=50000
I0301 01:53:37.005990 139589717059328 logging_writer.py:48] [211900] global_step=211900, grad_norm=4.949579238891602, loss=0.6240807771682739
I0301 01:54:10.890440 139589708666624 logging_writer.py:48] [212000] global_step=212000, grad_norm=4.466612339019775, loss=0.6364896297454834
I0301 01:54:44.817138 139589717059328 logging_writer.py:48] [212100] global_step=212100, grad_norm=4.49520206451416, loss=0.6276836395263672
I0301 01:55:18.772824 139589708666624 logging_writer.py:48] [212200] global_step=212200, grad_norm=4.094289779663086, loss=0.5627135038375854
I0301 01:55:52.679183 139589717059328 logging_writer.py:48] [212300] global_step=212300, grad_norm=4.3639092445373535, loss=0.5785880088806152
I0301 01:56:26.641167 139589708666624 logging_writer.py:48] [212400] global_step=212400, grad_norm=4.62720251083374, loss=0.6459490656852722
I0301 01:57:00.745526 139589717059328 logging_writer.py:48] [212500] global_step=212500, grad_norm=4.314957141876221, loss=0.6247217655181885
I0301 01:57:34.693973 139589708666624 logging_writer.py:48] [212600] global_step=212600, grad_norm=4.685252666473389, loss=0.6674583554267883
I0301 01:58:08.646057 139589717059328 logging_writer.py:48] [212700] global_step=212700, grad_norm=4.3212504386901855, loss=0.6411017775535583
I0301 01:58:42.566685 139589708666624 logging_writer.py:48] [212800] global_step=212800, grad_norm=4.853323936462402, loss=0.5982764959335327
I0301 01:59:16.486266 139589717059328 logging_writer.py:48] [212900] global_step=212900, grad_norm=4.418807029724121, loss=0.627679169178009
I0301 01:59:50.411774 139589708666624 logging_writer.py:48] [213000] global_step=213000, grad_norm=4.434024810791016, loss=0.5808513164520264
I0301 02:00:24.382532 139589717059328 logging_writer.py:48] [213100] global_step=213100, grad_norm=4.447816848754883, loss=0.6353281736373901
I0301 02:00:58.313140 139589708666624 logging_writer.py:48] [213200] global_step=213200, grad_norm=4.948568820953369, loss=0.6468777656555176
I0301 02:01:32.234999 139589717059328 logging_writer.py:48] [213300] global_step=213300, grad_norm=4.089999675750732, loss=0.5372250080108643
I0301 02:02:06.182879 139589708666624 logging_writer.py:48] [213400] global_step=213400, grad_norm=4.455142498016357, loss=0.5709900856018066
I0301 02:02:06.193608 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:02:12.273462 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:02:20.514103 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:02:22.799206 139753105983296 submission_runner.py:411] Time since start: 75073.80s, 	Step: 213401, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14808925986289978, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0552589893341064, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8180344104766846, 'test/num_examples': 10000, 'score': 72482.38831710815, 'total_duration': 75073.80241346359, 'accumulated_submission_time': 72482.38831710815, 'accumulated_eval_time': 2576.818477153778, 'accumulated_logging_time': 7.12625527381897}
I0301 02:02:22.859778 139590161643264 logging_writer.py:48] [213401] accumulated_eval_time=2576.818477, accumulated_logging_time=7.126255, accumulated_submission_time=72482.388317, global_step=213401, preemption_count=0, score=72482.388317, test/accuracy=0.629900, test/loss=1.818034, test/num_examples=10000, total_duration=75073.802413, train/accuracy=0.960938, train/loss=0.148089, validation/accuracy=0.755100, validation/loss=1.055259, validation/num_examples=50000
I0301 02:02:56.762171 139590170035968 logging_writer.py:48] [213500] global_step=213500, grad_norm=4.622672080993652, loss=0.5835426449775696
I0301 02:03:30.815263 139590161643264 logging_writer.py:48] [213600] global_step=213600, grad_norm=4.520693778991699, loss=0.6513879299163818
I0301 02:04:04.744261 139590170035968 logging_writer.py:48] [213700] global_step=213700, grad_norm=4.445093631744385, loss=0.5301494598388672
I0301 02:04:38.690274 139590161643264 logging_writer.py:48] [213800] global_step=213800, grad_norm=4.280167579650879, loss=0.6175273656845093
I0301 02:05:12.614881 139590170035968 logging_writer.py:48] [213900] global_step=213900, grad_norm=4.636725902557373, loss=0.6271734237670898
I0301 02:05:46.541238 139590161643264 logging_writer.py:48] [214000] global_step=214000, grad_norm=4.502315521240234, loss=0.6079256534576416
I0301 02:06:20.490783 139590170035968 logging_writer.py:48] [214100] global_step=214100, grad_norm=4.574617385864258, loss=0.6437144875526428
I0301 02:06:54.433303 139590161643264 logging_writer.py:48] [214200] global_step=214200, grad_norm=4.4886345863342285, loss=0.6328895092010498
I0301 02:07:28.404658 139590170035968 logging_writer.py:48] [214300] global_step=214300, grad_norm=4.216073036193848, loss=0.5220087170600891
I0301 02:08:02.353080 139590161643264 logging_writer.py:48] [214400] global_step=214400, grad_norm=5.01645565032959, loss=0.6175795197486877
I0301 02:08:36.270214 139590170035968 logging_writer.py:48] [214500] global_step=214500, grad_norm=4.750494003295898, loss=0.6611948609352112
I0301 02:09:10.297404 139590161643264 logging_writer.py:48] [214600] global_step=214600, grad_norm=4.678512096405029, loss=0.6871854662895203
I0301 02:09:44.226567 139590170035968 logging_writer.py:48] [214700] global_step=214700, grad_norm=4.634603023529053, loss=0.6769684553146362
I0301 02:10:18.179424 139590161643264 logging_writer.py:48] [214800] global_step=214800, grad_norm=4.078034400939941, loss=0.5585946440696716
I0301 02:10:52.164176 139590170035968 logging_writer.py:48] [214900] global_step=214900, grad_norm=4.360347270965576, loss=0.6332703828811646
I0301 02:10:52.996879 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:10:59.023189 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:11:07.372206 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:11:09.730435 139753105983296 submission_runner.py:411] Time since start: 75600.73s, 	Step: 214904, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.146467387676239, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.054847002029419, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8187536001205444, 'test/num_examples': 10000, 'score': 72992.4620001316, 'total_duration': 75600.73349714279, 'accumulated_submission_time': 72992.4620001316, 'accumulated_eval_time': 2593.551813840866, 'accumulated_logging_time': 7.196750164031982}
I0301 02:11:09.793585 139589717059328 logging_writer.py:48] [214904] accumulated_eval_time=2593.551814, accumulated_logging_time=7.196750, accumulated_submission_time=72992.462000, global_step=214904, preemption_count=0, score=72992.462000, test/accuracy=0.630000, test/loss=1.818754, test/num_examples=10000, total_duration=75600.733497, train/accuracy=0.960898, train/loss=0.146467, validation/accuracy=0.754640, validation/loss=1.054847, validation/num_examples=50000
I0301 02:11:42.594871 139589725452032 logging_writer.py:48] [215000] global_step=215000, grad_norm=4.918219566345215, loss=0.6164414882659912
I0301 02:12:16.471646 139589717059328 logging_writer.py:48] [215100] global_step=215100, grad_norm=4.570638179779053, loss=0.6730424165725708
I0301 02:12:50.406791 139589725452032 logging_writer.py:48] [215200] global_step=215200, grad_norm=4.239744663238525, loss=0.5828908681869507
I0301 02:13:24.333559 139589717059328 logging_writer.py:48] [215300] global_step=215300, grad_norm=4.574961185455322, loss=0.6304481029510498
I0301 02:13:58.274317 139589725452032 logging_writer.py:48] [215400] global_step=215400, grad_norm=4.842038154602051, loss=0.6287133693695068
I0301 02:14:32.203414 139589717059328 logging_writer.py:48] [215500] global_step=215500, grad_norm=4.879257678985596, loss=0.7062769532203674
I0301 02:15:06.098887 139589725452032 logging_writer.py:48] [215600] global_step=215600, grad_norm=4.55068302154541, loss=0.5978552103042603
I0301 02:15:40.115036 139589717059328 logging_writer.py:48] [215700] global_step=215700, grad_norm=4.154956340789795, loss=0.5773179531097412
I0301 02:16:14.047713 139589725452032 logging_writer.py:48] [215800] global_step=215800, grad_norm=4.514639854431152, loss=0.6722005605697632
I0301 02:16:47.996628 139589717059328 logging_writer.py:48] [215900] global_step=215900, grad_norm=4.623530387878418, loss=0.649854302406311
I0301 02:17:21.888827 139589725452032 logging_writer.py:48] [216000] global_step=216000, grad_norm=4.651731491088867, loss=0.6429768204689026
I0301 02:17:55.832358 139589717059328 logging_writer.py:48] [216100] global_step=216100, grad_norm=4.482143878936768, loss=0.6440648436546326
I0301 02:18:29.763332 139589725452032 logging_writer.py:48] [216200] global_step=216200, grad_norm=4.803357124328613, loss=0.6697015762329102
I0301 02:19:03.706670 139589717059328 logging_writer.py:48] [216300] global_step=216300, grad_norm=4.336062908172607, loss=0.5911597013473511
I0301 02:19:37.633081 139589725452032 logging_writer.py:48] [216400] global_step=216400, grad_norm=4.115187168121338, loss=0.5575509071350098
I0301 02:19:39.816679 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:19:45.821057 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:19:54.164554 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:19:56.454955 139753105983296 submission_runner.py:411] Time since start: 76127.46s, 	Step: 216408, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14510172605514526, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0557056665420532, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8173750638961792, 'test/num_examples': 10000, 'score': 73502.42083287239, 'total_duration': 76127.45818305016, 'accumulated_submission_time': 73502.42083287239, 'accumulated_eval_time': 2610.1900346279144, 'accumulated_logging_time': 7.270415544509888}
I0301 02:19:56.512442 139589717059328 logging_writer.py:48] [216408] accumulated_eval_time=2610.190035, accumulated_logging_time=7.270416, accumulated_submission_time=73502.420833, global_step=216408, preemption_count=0, score=73502.420833, test/accuracy=0.630400, test/loss=1.817375, test/num_examples=10000, total_duration=76127.458183, train/accuracy=0.960100, train/loss=0.145102, validation/accuracy=0.754680, validation/loss=1.055706, validation/num_examples=50000
I0301 02:20:28.067700 139589725452032 logging_writer.py:48] [216500] global_step=216500, grad_norm=4.047078609466553, loss=0.5485358834266663
I0301 02:21:01.995608 139589717059328 logging_writer.py:48] [216600] global_step=216600, grad_norm=5.12824010848999, loss=0.591545581817627
I0301 02:21:36.053575 139589725452032 logging_writer.py:48] [216700] global_step=216700, grad_norm=4.65768575668335, loss=0.6205930709838867
I0301 02:22:09.993457 139589717059328 logging_writer.py:48] [216800] global_step=216800, grad_norm=4.474451541900635, loss=0.5709636211395264
I0301 02:22:43.912624 139589725452032 logging_writer.py:48] [216900] global_step=216900, grad_norm=4.627324104309082, loss=0.6912631988525391
I0301 02:23:17.858348 139589717059328 logging_writer.py:48] [217000] global_step=217000, grad_norm=4.358676910400391, loss=0.622380256652832
I0301 02:23:51.810473 139589725452032 logging_writer.py:48] [217100] global_step=217100, grad_norm=4.0157599449157715, loss=0.576797366142273
I0301 02:24:25.754844 139589717059328 logging_writer.py:48] [217200] global_step=217200, grad_norm=5.050167083740234, loss=0.6527056694030762
I0301 02:24:59.689341 139589725452032 logging_writer.py:48] [217300] global_step=217300, grad_norm=4.661038398742676, loss=0.7002866864204407
I0301 02:25:33.655712 139589717059328 logging_writer.py:48] [217400] global_step=217400, grad_norm=4.477973937988281, loss=0.569662868976593
I0301 02:26:07.582940 139589725452032 logging_writer.py:48] [217500] global_step=217500, grad_norm=4.09093713760376, loss=0.5954020023345947
I0301 02:26:41.553194 139589717059328 logging_writer.py:48] [217600] global_step=217600, grad_norm=4.280153274536133, loss=0.599166750907898
I0301 02:27:15.511806 139589725452032 logging_writer.py:48] [217700] global_step=217700, grad_norm=4.797590255737305, loss=0.6506609916687012
I0301 02:27:49.616425 139589717059328 logging_writer.py:48] [217800] global_step=217800, grad_norm=4.2265753746032715, loss=0.5778908729553223
I0301 02:28:23.592603 139589725452032 logging_writer.py:48] [217900] global_step=217900, grad_norm=4.357815265655518, loss=0.6111232042312622
I0301 02:28:26.785606 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:28:32.828303 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:28:41.179465 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:28:43.593162 139753105983296 submission_runner.py:411] Time since start: 76654.60s, 	Step: 217911, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14692118763923645, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0558857917785645, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8198227882385254, 'test/num_examples': 10000, 'score': 74012.63173341751, 'total_duration': 76654.59640598297, 'accumulated_submission_time': 74012.63173341751, 'accumulated_eval_time': 2626.9975488185883, 'accumulated_logging_time': 7.337841749191284}
I0301 02:28:43.642908 139589708666624 logging_writer.py:48] [217911] accumulated_eval_time=2626.997549, accumulated_logging_time=7.337842, accumulated_submission_time=74012.631733, global_step=217911, preemption_count=0, score=74012.631733, test/accuracy=0.630900, test/loss=1.819823, test/num_examples=10000, total_duration=76654.596406, train/accuracy=0.960320, train/loss=0.146921, validation/accuracy=0.754660, validation/loss=1.055886, validation/num_examples=50000
I0301 02:29:14.153404 139590153250560 logging_writer.py:48] [218000] global_step=218000, grad_norm=4.829213619232178, loss=0.7332442998886108
I0301 02:29:48.005691 139589708666624 logging_writer.py:48] [218100] global_step=218100, grad_norm=4.6568450927734375, loss=0.6539448499679565
I0301 02:30:21.931248 139590153250560 logging_writer.py:48] [218200] global_step=218200, grad_norm=4.679887294769287, loss=0.637952983379364
I0301 02:30:55.875172 139589708666624 logging_writer.py:48] [218300] global_step=218300, grad_norm=4.696272850036621, loss=0.6084087491035461
I0301 02:31:29.823927 139590153250560 logging_writer.py:48] [218400] global_step=218400, grad_norm=4.849786758422852, loss=0.7115429639816284
I0301 02:32:03.742739 139589708666624 logging_writer.py:48] [218500] global_step=218500, grad_norm=4.8129353523254395, loss=0.693868100643158
I0301 02:32:37.655841 139590153250560 logging_writer.py:48] [218600] global_step=218600, grad_norm=4.791424751281738, loss=0.5926294326782227
I0301 02:33:11.607392 139589708666624 logging_writer.py:48] [218700] global_step=218700, grad_norm=4.5184006690979, loss=0.6193211078643799
I0301 02:33:45.631029 139590153250560 logging_writer.py:48] [218800] global_step=218800, grad_norm=4.622010707855225, loss=0.6233322024345398
I0301 02:34:19.586597 139589708666624 logging_writer.py:48] [218900] global_step=218900, grad_norm=4.8655171394348145, loss=0.6473813056945801
I0301 02:34:53.523715 139590153250560 logging_writer.py:48] [219000] global_step=219000, grad_norm=4.8603196144104, loss=0.689486026763916
I0301 02:35:27.457726 139589708666624 logging_writer.py:48] [219100] global_step=219100, grad_norm=4.926337718963623, loss=0.632042407989502
I0301 02:36:01.378158 139590153250560 logging_writer.py:48] [219200] global_step=219200, grad_norm=4.137515544891357, loss=0.6265382170677185
I0301 02:36:35.294245 139589708666624 logging_writer.py:48] [219300] global_step=219300, grad_norm=4.325101375579834, loss=0.5509548783302307
I0301 02:37:09.230035 139590153250560 logging_writer.py:48] [219400] global_step=219400, grad_norm=4.3971967697143555, loss=0.5770219564437866
I0301 02:37:13.785187 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:37:19.788149 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:37:28.137644 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:37:30.438546 139753105983296 submission_runner.py:411] Time since start: 77181.44s, 	Step: 219415, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1473052203655243, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0550016164779663, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.819336175918579, 'test/num_examples': 10000, 'score': 74522.71133613586, 'total_duration': 77181.44177699089, 'accumulated_submission_time': 74522.71133613586, 'accumulated_eval_time': 2643.6508531570435, 'accumulated_logging_time': 7.39803147315979}
I0301 02:37:30.493383 139589717059328 logging_writer.py:48] [219415] accumulated_eval_time=2643.650853, accumulated_logging_time=7.398031, accumulated_submission_time=74522.711336, global_step=219415, preemption_count=0, score=74522.711336, test/accuracy=0.630200, test/loss=1.819336, test/num_examples=10000, total_duration=77181.441777, train/accuracy=0.960479, train/loss=0.147305, validation/accuracy=0.755060, validation/loss=1.055002, validation/num_examples=50000
I0301 02:37:59.685468 139590144857856 logging_writer.py:48] [219500] global_step=219500, grad_norm=4.5920634269714355, loss=0.6488285064697266
I0301 02:38:33.601927 139589717059328 logging_writer.py:48] [219600] global_step=219600, grad_norm=4.502433776855469, loss=0.5728384852409363
I0301 02:39:07.554974 139590144857856 logging_writer.py:48] [219700] global_step=219700, grad_norm=4.584465026855469, loss=0.5973207950592041
I0301 02:39:41.517917 139589717059328 logging_writer.py:48] [219800] global_step=219800, grad_norm=4.845952987670898, loss=0.6691724061965942
I0301 02:40:15.546474 139590144857856 logging_writer.py:48] [219900] global_step=219900, grad_norm=4.529532432556152, loss=0.6573666930198669
I0301 02:40:49.466278 139589717059328 logging_writer.py:48] [220000] global_step=220000, grad_norm=4.5469560623168945, loss=0.6292045712471008
I0301 02:41:23.427930 139590144857856 logging_writer.py:48] [220100] global_step=220100, grad_norm=4.8590216636657715, loss=0.6341379880905151
I0301 02:41:57.397170 139589717059328 logging_writer.py:48] [220200] global_step=220200, grad_norm=4.4261345863342285, loss=0.5899193286895752
I0301 02:42:31.384732 139590144857856 logging_writer.py:48] [220300] global_step=220300, grad_norm=4.491733074188232, loss=0.5713551640510559
I0301 02:43:05.360882 139589717059328 logging_writer.py:48] [220400] global_step=220400, grad_norm=4.512107849121094, loss=0.6289481520652771
I0301 02:43:39.330697 139590144857856 logging_writer.py:48] [220500] global_step=220500, grad_norm=4.335201263427734, loss=0.5925596952438354
I0301 02:44:13.299907 139589717059328 logging_writer.py:48] [220600] global_step=220600, grad_norm=5.259044647216797, loss=0.5530174970626831
I0301 02:44:47.262997 139590144857856 logging_writer.py:48] [220700] global_step=220700, grad_norm=4.474543571472168, loss=0.559466540813446
I0301 02:45:21.198808 139589717059328 logging_writer.py:48] [220800] global_step=220800, grad_norm=4.507503986358643, loss=0.5970436334609985
I0301 02:45:55.178872 139590144857856 logging_writer.py:48] [220900] global_step=220900, grad_norm=4.318393707275391, loss=0.6271514296531677
I0301 02:46:00.764268 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:46:07.223123 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:46:15.600144 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:46:17.869354 139753105983296 submission_runner.py:411] Time since start: 77708.87s, 	Step: 220918, 	{'train/accuracy': 0.9607182741165161, 'train/loss': 0.14824384450912476, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0545982122421265, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8186966180801392, 'test/num_examples': 10000, 'score': 75032.91843128204, 'total_duration': 77708.87258601189, 'accumulated_submission_time': 75032.91843128204, 'accumulated_eval_time': 2660.755885362625, 'accumulated_logging_time': 7.464325189590454}
I0301 02:46:17.927951 139589717059328 logging_writer.py:48] [220918] accumulated_eval_time=2660.755885, accumulated_logging_time=7.464325, accumulated_submission_time=75032.918431, global_step=220918, preemption_count=0, score=75032.918431, test/accuracy=0.630700, test/loss=1.818697, test/num_examples=10000, total_duration=77708.872586, train/accuracy=0.960718, train/loss=0.148244, validation/accuracy=0.755040, validation/loss=1.054598, validation/num_examples=50000
I0301 02:46:46.062062 139589725452032 logging_writer.py:48] [221000] global_step=221000, grad_norm=4.444400787353516, loss=0.6183751821517944
I0301 02:47:19.975606 139589717059328 logging_writer.py:48] [221100] global_step=221100, grad_norm=4.309749603271484, loss=0.5623570680618286
I0301 02:47:53.889969 139589725452032 logging_writer.py:48] [221200] global_step=221200, grad_norm=4.301889419555664, loss=0.6097984313964844
I0301 02:48:27.810349 139589717059328 logging_writer.py:48] [221300] global_step=221300, grad_norm=4.46658182144165, loss=0.6381154656410217
I0301 02:49:01.772128 139589725452032 logging_writer.py:48] [221400] global_step=221400, grad_norm=4.6519455909729, loss=0.6102911233901978
I0301 02:49:35.716382 139589717059328 logging_writer.py:48] [221500] global_step=221500, grad_norm=4.665374279022217, loss=0.6128765344619751
I0301 02:50:09.625236 139589725452032 logging_writer.py:48] [221600] global_step=221600, grad_norm=4.472236633300781, loss=0.585014820098877
I0301 02:50:43.554508 139589717059328 logging_writer.py:48] [221700] global_step=221700, grad_norm=4.521023273468018, loss=0.613567054271698
I0301 02:51:17.484130 139589725452032 logging_writer.py:48] [221800] global_step=221800, grad_norm=4.345770359039307, loss=0.5510717034339905
I0301 02:51:51.412430 139589717059328 logging_writer.py:48] [221900] global_step=221900, grad_norm=4.329227447509766, loss=0.6173574924468994
I0301 02:52:25.446365 139589725452032 logging_writer.py:48] [222000] global_step=222000, grad_norm=4.350508689880371, loss=0.5976887345314026
I0301 02:52:59.400904 139589717059328 logging_writer.py:48] [222100] global_step=222100, grad_norm=4.842837333679199, loss=0.6111454963684082
I0301 02:53:33.363772 139589725452032 logging_writer.py:48] [222200] global_step=222200, grad_norm=4.945315361022949, loss=0.7309585213661194
I0301 02:54:07.261865 139589717059328 logging_writer.py:48] [222300] global_step=222300, grad_norm=4.271414279937744, loss=0.6567894816398621
I0301 02:54:41.207684 139589725452032 logging_writer.py:48] [222400] global_step=222400, grad_norm=4.642645359039307, loss=0.675030529499054
I0301 02:54:48.148312 139753105983296 spec.py:321] Evaluating on the training split.
I0301 02:54:54.187113 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 02:55:02.567854 139753105983296 spec.py:349] Evaluating on the test split.
I0301 02:55:04.874168 139753105983296 submission_runner.py:411] Time since start: 78235.88s, 	Step: 222422, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.1466655135154724, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0557494163513184, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.819899082183838, 'test/num_examples': 10000, 'score': 75543.07302880287, 'total_duration': 78235.87724661827, 'accumulated_submission_time': 75543.07302880287, 'accumulated_eval_time': 2677.481550216675, 'accumulated_logging_time': 7.5350916385650635}
I0301 02:55:04.929031 139590178428672 logging_writer.py:48] [222422] accumulated_eval_time=2677.481550, accumulated_logging_time=7.535092, accumulated_submission_time=75543.073029, global_step=222422, preemption_count=0, score=75543.073029, test/accuracy=0.630400, test/loss=1.819899, test/num_examples=10000, total_duration=78235.877247, train/accuracy=0.960559, train/loss=0.146666, validation/accuracy=0.754860, validation/loss=1.055749, validation/num_examples=50000
I0301 02:55:31.755765 139590186821376 logging_writer.py:48] [222500] global_step=222500, grad_norm=4.270459175109863, loss=0.6631026268005371
I0301 02:56:05.661762 139590178428672 logging_writer.py:48] [222600] global_step=222600, grad_norm=4.886745929718018, loss=0.621124267578125
I0301 02:56:39.570038 139590186821376 logging_writer.py:48] [222700] global_step=222700, grad_norm=4.0809006690979, loss=0.5964005589485168
I0301 02:57:13.527947 139590178428672 logging_writer.py:48] [222800] global_step=222800, grad_norm=4.248613357543945, loss=0.5941015481948853
I0301 02:57:47.495854 139590186821376 logging_writer.py:48] [222900] global_step=222900, grad_norm=4.592131614685059, loss=0.6188218593597412
I0301 02:58:21.552889 139590178428672 logging_writer.py:48] [223000] global_step=223000, grad_norm=4.335833549499512, loss=0.6174375414848328
I0301 02:58:55.480731 139590186821376 logging_writer.py:48] [223100] global_step=223100, grad_norm=4.302359580993652, loss=0.6034096479415894
I0301 02:59:29.414824 139590178428672 logging_writer.py:48] [223200] global_step=223200, grad_norm=4.7129340171813965, loss=0.6732503175735474
I0301 03:00:03.387932 139590186821376 logging_writer.py:48] [223300] global_step=223300, grad_norm=4.440859794616699, loss=0.6356477737426758
I0301 03:00:37.330417 139590178428672 logging_writer.py:48] [223400] global_step=223400, grad_norm=4.553955554962158, loss=0.5897372961044312
I0301 03:01:11.298129 139590186821376 logging_writer.py:48] [223500] global_step=223500, grad_norm=4.3621826171875, loss=0.6058299541473389
I0301 03:01:45.268060 139590178428672 logging_writer.py:48] [223600] global_step=223600, grad_norm=4.48509407043457, loss=0.6520913243293762
I0301 03:02:19.198430 139590186821376 logging_writer.py:48] [223700] global_step=223700, grad_norm=4.448266983032227, loss=0.6596547365188599
I0301 03:02:53.149856 139590178428672 logging_writer.py:48] [223800] global_step=223800, grad_norm=4.5809855461120605, loss=0.6826620697975159
I0301 03:03:27.113732 139590186821376 logging_writer.py:48] [223900] global_step=223900, grad_norm=4.474717617034912, loss=0.6355926990509033
I0301 03:03:35.079241 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:03:41.139611 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:03:49.508062 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:03:51.747596 139753105983296 submission_runner.py:411] Time since start: 78762.75s, 	Step: 223925, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14315280318260193, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0563209056854248, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.8206843137741089, 'test/num_examples': 10000, 'score': 76053.1585996151, 'total_duration': 78762.75082278252, 'accumulated_submission_time': 76053.1585996151, 'accumulated_eval_time': 2694.1498594284058, 'accumulated_logging_time': 7.6019062995910645}
I0301 03:03:51.805758 139590144857856 logging_writer.py:48] [223925] accumulated_eval_time=2694.149859, accumulated_logging_time=7.601906, accumulated_submission_time=76053.158600, global_step=223925, preemption_count=0, score=76053.158600, test/accuracy=0.629300, test/loss=1.820684, test/num_examples=10000, total_duration=78762.750823, train/accuracy=0.961217, train/loss=0.143153, validation/accuracy=0.754740, validation/loss=1.056321, validation/num_examples=50000
I0301 03:04:17.574679 139590153250560 logging_writer.py:48] [224000] global_step=224000, grad_norm=4.513647556304932, loss=0.5463937520980835
I0301 03:04:51.575679 139590144857856 logging_writer.py:48] [224100] global_step=224100, grad_norm=4.800495624542236, loss=0.6503602862358093
I0301 03:05:25.519474 139590153250560 logging_writer.py:48] [224200] global_step=224200, grad_norm=4.344839572906494, loss=0.6279740333557129
I0301 03:05:59.444161 139590144857856 logging_writer.py:48] [224300] global_step=224300, grad_norm=4.612037658691406, loss=0.6293047070503235
I0301 03:06:33.365488 139590153250560 logging_writer.py:48] [224400] global_step=224400, grad_norm=4.863407611846924, loss=0.7264888882637024
I0301 03:07:07.297926 139590144857856 logging_writer.py:48] [224500] global_step=224500, grad_norm=4.564668655395508, loss=0.5864496231079102
I0301 03:07:41.250905 139590153250560 logging_writer.py:48] [224600] global_step=224600, grad_norm=4.613659381866455, loss=0.5794081091880798
I0301 03:08:15.197115 139590144857856 logging_writer.py:48] [224700] global_step=224700, grad_norm=4.070610046386719, loss=0.5729716420173645
I0301 03:08:49.098914 139590153250560 logging_writer.py:48] [224800] global_step=224800, grad_norm=4.393396377563477, loss=0.5709253549575806
I0301 03:09:23.035702 139590144857856 logging_writer.py:48] [224900] global_step=224900, grad_norm=4.3527703285217285, loss=0.6113235950469971
I0301 03:09:56.973117 139590153250560 logging_writer.py:48] [225000] global_step=225000, grad_norm=4.991698741912842, loss=0.6503417491912842
I0301 03:10:30.922928 139590144857856 logging_writer.py:48] [225100] global_step=225100, grad_norm=5.426240921020508, loss=0.7577948570251465
I0301 03:11:05.047375 139590153250560 logging_writer.py:48] [225200] global_step=225200, grad_norm=4.6938886642456055, loss=0.6325258612632751
I0301 03:11:38.966269 139590144857856 logging_writer.py:48] [225300] global_step=225300, grad_norm=4.813969612121582, loss=0.6042240262031555
I0301 03:12:12.887240 139590153250560 logging_writer.py:48] [225400] global_step=225400, grad_norm=4.227648735046387, loss=0.6345874071121216
I0301 03:12:21.873672 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:12:27.897458 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:12:36.271302 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:12:38.552818 139753105983296 submission_runner.py:411] Time since start: 79289.56s, 	Step: 225428, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14210452139377594, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0561600923538208, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8175678253173828, 'test/num_examples': 10000, 'score': 76563.16371178627, 'total_duration': 79289.55604577065, 'accumulated_submission_time': 76563.16371178627, 'accumulated_eval_time': 2710.828951358795, 'accumulated_logging_time': 7.669835329055786}
I0301 03:12:38.610930 139590178428672 logging_writer.py:48] [225428] accumulated_eval_time=2710.828951, accumulated_logging_time=7.669835, accumulated_submission_time=76563.163712, global_step=225428, preemption_count=0, score=76563.163712, test/accuracy=0.631100, test/loss=1.817568, test/num_examples=10000, total_duration=79289.556046, train/accuracy=0.961635, train/loss=0.142105, validation/accuracy=0.754800, validation/loss=1.056160, validation/num_examples=50000
I0301 03:13:03.384233 139590186821376 logging_writer.py:48] [225500] global_step=225500, grad_norm=4.558340072631836, loss=0.6394553184509277
I0301 03:13:37.319175 139590178428672 logging_writer.py:48] [225600] global_step=225600, grad_norm=4.546347141265869, loss=0.6004008054733276
I0301 03:14:11.294547 139590186821376 logging_writer.py:48] [225700] global_step=225700, grad_norm=4.811129093170166, loss=0.6509653329849243
I0301 03:14:45.257875 139590178428672 logging_writer.py:48] [225800] global_step=225800, grad_norm=4.709550380706787, loss=0.7205734252929688
I0301 03:15:19.183101 139590186821376 logging_writer.py:48] [225900] global_step=225900, grad_norm=4.416845798492432, loss=0.6925674676895142
I0301 03:15:53.148772 139590178428672 logging_writer.py:48] [226000] global_step=226000, grad_norm=4.706453323364258, loss=0.6222112774848938
I0301 03:16:27.114649 139590186821376 logging_writer.py:48] [226100] global_step=226100, grad_norm=4.801679611206055, loss=0.6125414967536926
I0301 03:17:01.184457 139590178428672 logging_writer.py:48] [226200] global_step=226200, grad_norm=4.63214635848999, loss=0.6282500624656677
I0301 03:17:35.129840 139590186821376 logging_writer.py:48] [226300] global_step=226300, grad_norm=4.710727691650391, loss=0.6479447484016418
I0301 03:18:09.084673 139590178428672 logging_writer.py:48] [226400] global_step=226400, grad_norm=4.832634925842285, loss=0.6649705171585083
I0301 03:18:43.050964 139590186821376 logging_writer.py:48] [226500] global_step=226500, grad_norm=4.3703293800354, loss=0.6309771537780762
I0301 03:19:17.013022 139590178428672 logging_writer.py:48] [226600] global_step=226600, grad_norm=4.7454609870910645, loss=0.6385496854782104
I0301 03:19:50.970252 139590186821376 logging_writer.py:48] [226700] global_step=226700, grad_norm=4.35270881652832, loss=0.6560918688774109
I0301 03:20:24.917942 139590178428672 logging_writer.py:48] [226800] global_step=226800, grad_norm=4.528379917144775, loss=0.5837420225143433
I0301 03:20:58.868799 139590186821376 logging_writer.py:48] [226900] global_step=226900, grad_norm=4.248752593994141, loss=0.6047943234443665
I0301 03:21:08.852867 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:21:14.977129 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:21:23.232246 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:21:25.501946 139753105983296 submission_runner.py:411] Time since start: 79816.51s, 	Step: 226931, 	{'train/accuracy': 0.9616150856018066, 'train/loss': 0.14582249522209167, 'validation/accuracy': 0.7543599605560303, 'validation/loss': 1.0568692684173584, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8199219703674316, 'test/num_examples': 10000, 'score': 77073.34233403206, 'total_duration': 79816.50515580177, 'accumulated_submission_time': 77073.34233403206, 'accumulated_eval_time': 2727.477970123291, 'accumulated_logging_time': 7.7380051612854}
I0301 03:21:25.558946 139589717059328 logging_writer.py:48] [226931] accumulated_eval_time=2727.477970, accumulated_logging_time=7.738005, accumulated_submission_time=77073.342334, global_step=226931, preemption_count=0, score=77073.342334, test/accuracy=0.630100, test/loss=1.819922, test/num_examples=10000, total_duration=79816.505156, train/accuracy=0.961615, train/loss=0.145822, validation/accuracy=0.754360, validation/loss=1.056869, validation/num_examples=50000
I0301 03:21:50.155619 139589725452032 logging_writer.py:48] [227000] global_step=227000, grad_norm=4.598678112030029, loss=0.622342586517334
I0301 03:22:24.058248 139589717059328 logging_writer.py:48] [227100] global_step=227100, grad_norm=5.110321998596191, loss=0.662053108215332
I0301 03:22:58.014767 139589725452032 logging_writer.py:48] [227200] global_step=227200, grad_norm=5.055907726287842, loss=0.593826174736023
I0301 03:23:32.077531 139589717059328 logging_writer.py:48] [227300] global_step=227300, grad_norm=4.521833419799805, loss=0.7071201801300049
I0301 03:24:06.064958 139589725452032 logging_writer.py:48] [227400] global_step=227400, grad_norm=4.0388641357421875, loss=0.5567830204963684
I0301 03:24:40.030296 139589717059328 logging_writer.py:48] [227500] global_step=227500, grad_norm=4.568904399871826, loss=0.6339555978775024
I0301 03:25:13.972266 139589725452032 logging_writer.py:48] [227600] global_step=227600, grad_norm=4.55052375793457, loss=0.6909048557281494
I0301 03:25:47.946560 139589717059328 logging_writer.py:48] [227700] global_step=227700, grad_norm=5.254883289337158, loss=0.6373999118804932
I0301 03:26:21.889272 139589725452032 logging_writer.py:48] [227800] global_step=227800, grad_norm=4.740312099456787, loss=0.6563377380371094
I0301 03:26:55.827098 139589717059328 logging_writer.py:48] [227900] global_step=227900, grad_norm=4.636908054351807, loss=0.635945200920105
I0301 03:27:29.767914 139589725452032 logging_writer.py:48] [228000] global_step=228000, grad_norm=4.5612311363220215, loss=0.6036890745162964
I0301 03:28:03.692234 139589717059328 logging_writer.py:48] [228100] global_step=228100, grad_norm=5.216763496398926, loss=0.6222137808799744
I0301 03:28:37.650095 139589725452032 logging_writer.py:48] [228200] global_step=228200, grad_norm=4.536275386810303, loss=0.5970606207847595
I0301 03:29:11.682046 139589717059328 logging_writer.py:48] [228300] global_step=228300, grad_norm=4.8793792724609375, loss=0.6849731802940369
I0301 03:29:45.617060 139589725452032 logging_writer.py:48] [228400] global_step=228400, grad_norm=4.447443962097168, loss=0.5494317412376404
I0301 03:29:55.626454 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:30:02.405013 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:30:10.677509 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:30:12.974709 139753105983296 submission_runner.py:411] Time since start: 80343.98s, 	Step: 228431, 	{'train/accuracy': 0.9591238498687744, 'train/loss': 0.14711764454841614, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0547051429748535, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8176454305648804, 'test/num_examples': 10000, 'score': 77582.4801542759, 'total_duration': 80343.97794318199, 'accumulated_submission_time': 77582.4801542759, 'accumulated_eval_time': 2744.826174020767, 'accumulated_logging_time': 8.672579765319824}
I0301 03:30:13.030706 139589717059328 logging_writer.py:48] [228431] accumulated_eval_time=2744.826174, accumulated_logging_time=8.672580, accumulated_submission_time=77582.480154, global_step=228431, preemption_count=0, score=77582.480154, test/accuracy=0.630000, test/loss=1.817645, test/num_examples=10000, total_duration=80343.977943, train/accuracy=0.959124, train/loss=0.147118, validation/accuracy=0.755180, validation/loss=1.054705, validation/num_examples=50000
I0301 03:30:36.781554 139590161643264 logging_writer.py:48] [228500] global_step=228500, grad_norm=4.524982929229736, loss=0.5922111868858337
I0301 03:31:10.711183 139589717059328 logging_writer.py:48] [228600] global_step=228600, grad_norm=4.4181365966796875, loss=0.6501584053039551
I0301 03:31:44.637546 139590161643264 logging_writer.py:48] [228700] global_step=228700, grad_norm=4.49468469619751, loss=0.6039584875106812
I0301 03:32:18.585593 139589717059328 logging_writer.py:48] [228800] global_step=228800, grad_norm=4.590641021728516, loss=0.6404988169670105
I0301 03:32:52.511793 139590161643264 logging_writer.py:48] [228900] global_step=228900, grad_norm=4.3633036613464355, loss=0.5756600499153137
I0301 03:33:26.494821 139589717059328 logging_writer.py:48] [229000] global_step=229000, grad_norm=5.034459114074707, loss=0.6627808809280396
I0301 03:34:00.436753 139590161643264 logging_writer.py:48] [229100] global_step=229100, grad_norm=4.938570499420166, loss=0.6885237693786621
I0301 03:34:34.369237 139589717059328 logging_writer.py:48] [229200] global_step=229200, grad_norm=5.205765724182129, loss=0.6699907183647156
I0301 03:35:08.320554 139590161643264 logging_writer.py:48] [229300] global_step=229300, grad_norm=4.690001964569092, loss=0.6187435388565063
I0301 03:35:42.415388 139589717059328 logging_writer.py:48] [229400] global_step=229400, grad_norm=4.7921671867370605, loss=0.6947027444839478
I0301 03:36:16.334497 139590161643264 logging_writer.py:48] [229500] global_step=229500, grad_norm=4.513064384460449, loss=0.6943891048431396
I0301 03:36:50.271778 139589717059328 logging_writer.py:48] [229600] global_step=229600, grad_norm=4.339657306671143, loss=0.6137723326683044
I0301 03:37:24.230959 139590161643264 logging_writer.py:48] [229700] global_step=229700, grad_norm=4.760198593139648, loss=0.646589994430542
I0301 03:37:58.161217 139589717059328 logging_writer.py:48] [229800] global_step=229800, grad_norm=4.981839656829834, loss=0.6452908515930176
I0301 03:38:32.096923 139590161643264 logging_writer.py:48] [229900] global_step=229900, grad_norm=4.513069152832031, loss=0.6513221263885498
I0301 03:38:43.077315 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:38:49.120619 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:38:57.423544 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:38:59.815911 139753105983296 submission_runner.py:411] Time since start: 80870.82s, 	Step: 229934, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.1465269774198532, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.0559604167938232, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.818885087966919, 'test/num_examples': 10000, 'score': 78092.46252202988, 'total_duration': 80870.81914234161, 'accumulated_submission_time': 78092.46252202988, 'accumulated_eval_time': 2761.564717531204, 'accumulated_logging_time': 8.739872694015503}
I0301 03:38:59.875054 139589708666624 logging_writer.py:48] [229934] accumulated_eval_time=2761.564718, accumulated_logging_time=8.739873, accumulated_submission_time=78092.462522, global_step=229934, preemption_count=0, score=78092.462522, test/accuracy=0.629200, test/loss=1.818885, test/num_examples=10000, total_duration=80870.819142, train/accuracy=0.960898, train/loss=0.146527, validation/accuracy=0.754640, validation/loss=1.055960, validation/num_examples=50000
I0301 03:39:22.573036 139589717059328 logging_writer.py:48] [230000] global_step=230000, grad_norm=4.556103229522705, loss=0.6380091905593872
I0301 03:39:56.475881 139589708666624 logging_writer.py:48] [230100] global_step=230100, grad_norm=4.556563854217529, loss=0.6506498456001282
I0301 03:40:30.387644 139589717059328 logging_writer.py:48] [230200] global_step=230200, grad_norm=4.557671546936035, loss=0.6123285293579102
I0301 03:41:04.311149 139589708666624 logging_writer.py:48] [230300] global_step=230300, grad_norm=4.620895862579346, loss=0.5954629182815552
I0301 03:41:38.333941 139589717059328 logging_writer.py:48] [230400] global_step=230400, grad_norm=4.934391975402832, loss=0.630877673625946
I0301 03:42:12.287520 139589708666624 logging_writer.py:48] [230500] global_step=230500, grad_norm=4.334074974060059, loss=0.6262456774711609
I0301 03:42:46.218344 139589717059328 logging_writer.py:48] [230600] global_step=230600, grad_norm=4.468430042266846, loss=0.6115224361419678
I0301 03:43:20.120559 139589708666624 logging_writer.py:48] [230700] global_step=230700, grad_norm=4.54075288772583, loss=0.5958894491195679
I0301 03:43:54.065879 139589717059328 logging_writer.py:48] [230800] global_step=230800, grad_norm=4.418082237243652, loss=0.5562329292297363
I0301 03:44:28.010530 139589708666624 logging_writer.py:48] [230900] global_step=230900, grad_norm=5.012314319610596, loss=0.6197922825813293
I0301 03:45:01.954964 139589717059328 logging_writer.py:48] [231000] global_step=231000, grad_norm=4.992563724517822, loss=0.6748835444450378
I0301 03:45:35.862900 139589708666624 logging_writer.py:48] [231100] global_step=231100, grad_norm=4.790584564208984, loss=0.6976383328437805
I0301 03:46:09.804432 139589717059328 logging_writer.py:48] [231200] global_step=231200, grad_norm=4.428905963897705, loss=0.6333863735198975
I0301 03:46:43.760535 139589708666624 logging_writer.py:48] [231300] global_step=231300, grad_norm=4.156131267547607, loss=0.5973443388938904
I0301 03:47:17.692533 139589717059328 logging_writer.py:48] [231400] global_step=231400, grad_norm=4.216188430786133, loss=0.6045573353767395
I0301 03:47:30.066970 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:47:36.390040 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:47:44.614077 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:47:46.928530 139753105983296 submission_runner.py:411] Time since start: 81397.93s, 	Step: 231438, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14481164515018463, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0556319952011108, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8184458017349243, 'test/num_examples': 10000, 'score': 78602.5905714035, 'total_duration': 81397.9317638874, 'accumulated_submission_time': 78602.5905714035, 'accumulated_eval_time': 2778.42622590065, 'accumulated_logging_time': 8.809640645980835}
I0301 03:47:46.988110 139590170035968 logging_writer.py:48] [231438] accumulated_eval_time=2778.426226, accumulated_logging_time=8.809641, accumulated_submission_time=78602.590571, global_step=231438, preemption_count=0, score=78602.590571, test/accuracy=0.631300, test/loss=1.818446, test/num_examples=10000, total_duration=81397.931764, train/accuracy=0.961655, train/loss=0.144812, validation/accuracy=0.754680, validation/loss=1.055632, validation/num_examples=50000
I0301 03:48:08.326304 139590178428672 logging_writer.py:48] [231500] global_step=231500, grad_norm=4.486207485198975, loss=0.5555098652839661
I0301 03:48:42.234624 139590170035968 logging_writer.py:48] [231600] global_step=231600, grad_norm=4.302326679229736, loss=0.6317631602287292
I0301 03:49:16.172018 139590178428672 logging_writer.py:48] [231700] global_step=231700, grad_norm=4.5784687995910645, loss=0.5846920609474182
I0301 03:49:50.136925 139590170035968 logging_writer.py:48] [231800] global_step=231800, grad_norm=5.39864444732666, loss=0.5964654684066772
I0301 03:50:24.068640 139590178428672 logging_writer.py:48] [231900] global_step=231900, grad_norm=4.437118053436279, loss=0.6174350380897522
I0301 03:50:58.047162 139590170035968 logging_writer.py:48] [232000] global_step=232000, grad_norm=4.641787528991699, loss=0.6036717891693115
I0301 03:51:31.966488 139590178428672 logging_writer.py:48] [232100] global_step=232100, grad_norm=4.607439041137695, loss=0.6329048275947571
I0301 03:52:05.919335 139590170035968 logging_writer.py:48] [232200] global_step=232200, grad_norm=4.16985559463501, loss=0.5636798143386841
I0301 03:52:39.907216 139590178428672 logging_writer.py:48] [232300] global_step=232300, grad_norm=4.293247699737549, loss=0.5262507796287537
I0301 03:53:13.855604 139590170035968 logging_writer.py:48] [232400] global_step=232400, grad_norm=4.205501556396484, loss=0.5591827034950256
I0301 03:53:47.871659 139590178428672 logging_writer.py:48] [232500] global_step=232500, grad_norm=4.859640598297119, loss=0.6901623606681824
I0301 03:54:21.808014 139590170035968 logging_writer.py:48] [232600] global_step=232600, grad_norm=4.590458393096924, loss=0.6911671161651611
I0301 03:54:55.770999 139590178428672 logging_writer.py:48] [232700] global_step=232700, grad_norm=4.1174139976501465, loss=0.5641652941703796
I0301 03:55:29.710312 139590170035968 logging_writer.py:48] [232800] global_step=232800, grad_norm=4.662381172180176, loss=0.6674590110778809
I0301 03:56:03.627015 139590178428672 logging_writer.py:48] [232900] global_step=232900, grad_norm=4.861542701721191, loss=0.6445028781890869
I0301 03:56:17.005734 139753105983296 spec.py:321] Evaluating on the training split.
I0301 03:56:23.037605 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 03:56:31.302473 139753105983296 spec.py:349] Evaluating on the test split.
I0301 03:56:33.588952 139753105983296 submission_runner.py:411] Time since start: 81924.59s, 	Step: 232941, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.14631828665733337, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.056311011314392, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8192987442016602, 'test/num_examples': 10000, 'score': 79112.5441596508, 'total_duration': 81924.59214305878, 'accumulated_submission_time': 79112.5441596508, 'accumulated_eval_time': 2795.0093553066254, 'accumulated_logging_time': 8.879449844360352}
I0301 03:56:33.643531 139589725452032 logging_writer.py:48] [232941] accumulated_eval_time=2795.009355, accumulated_logging_time=8.879450, accumulated_submission_time=79112.544160, global_step=232941, preemption_count=0, score=79112.544160, test/accuracy=0.631200, test/loss=1.819299, test/num_examples=10000, total_duration=81924.592143, train/accuracy=0.960918, train/loss=0.146318, validation/accuracy=0.754660, validation/loss=1.056311, validation/num_examples=50000
I0301 03:56:53.988170 139590144857856 logging_writer.py:48] [233000] global_step=233000, grad_norm=4.777609348297119, loss=0.6190457344055176
I0301 03:57:27.870906 139589725452032 logging_writer.py:48] [233100] global_step=233100, grad_norm=4.512396812438965, loss=0.6187301874160767
I0301 03:58:01.788177 139590144857856 logging_writer.py:48] [233200] global_step=233200, grad_norm=4.655734539031982, loss=0.596711277961731
I0301 03:58:35.724962 139589725452032 logging_writer.py:48] [233300] global_step=233300, grad_norm=4.500774383544922, loss=0.5792642831802368
I0301 03:59:09.677466 139590144857856 logging_writer.py:48] [233400] global_step=233400, grad_norm=4.477718353271484, loss=0.6240588426589966
I0301 03:59:43.605509 139589725452032 logging_writer.py:48] [233500] global_step=233500, grad_norm=4.967400074005127, loss=0.6342310309410095
I0301 04:00:17.690695 139590144857856 logging_writer.py:48] [233600] global_step=233600, grad_norm=4.67849063873291, loss=0.6460922360420227
I0301 04:00:51.638386 139589725452032 logging_writer.py:48] [233700] global_step=233700, grad_norm=4.435519695281982, loss=0.612351655960083
I0301 04:01:25.581764 139590144857856 logging_writer.py:48] [233800] global_step=233800, grad_norm=4.163155555725098, loss=0.5818379521369934
I0301 04:01:59.531738 139589725452032 logging_writer.py:48] [233900] global_step=233900, grad_norm=4.368228912353516, loss=0.6224074363708496
I0301 04:02:33.430941 139590144857856 logging_writer.py:48] [234000] global_step=234000, grad_norm=4.6454949378967285, loss=0.6076486706733704
I0301 04:03:07.385180 139589725452032 logging_writer.py:48] [234100] global_step=234100, grad_norm=4.566007614135742, loss=0.6922793388366699
I0301 04:03:41.316468 139590144857856 logging_writer.py:48] [234200] global_step=234200, grad_norm=4.884230136871338, loss=0.70148766040802
I0301 04:04:15.256124 139589725452032 logging_writer.py:48] [234300] global_step=234300, grad_norm=5.012974262237549, loss=0.654792845249176
I0301 04:04:49.156239 139590144857856 logging_writer.py:48] [234400] global_step=234400, grad_norm=4.3568620681762695, loss=0.6839306950569153
I0301 04:05:03.859048 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:05:09.859884 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:05:18.208064 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:05:20.538219 139753105983296 submission_runner.py:411] Time since start: 82451.54s, 	Step: 234445, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.1448947787284851, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.055671215057373, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.819419264793396, 'test/num_examples': 10000, 'score': 79622.69375491142, 'total_duration': 82451.54144382477, 'accumulated_submission_time': 79622.69375491142, 'accumulated_eval_time': 2811.688470363617, 'accumulated_logging_time': 8.947274923324585}
I0301 04:05:20.596446 139590170035968 logging_writer.py:48] [234445] accumulated_eval_time=2811.688470, accumulated_logging_time=8.947275, accumulated_submission_time=79622.693755, global_step=234445, preemption_count=0, score=79622.693755, test/accuracy=0.630000, test/loss=1.819419, test/num_examples=10000, total_duration=82451.541444, train/accuracy=0.961316, train/loss=0.144895, validation/accuracy=0.754880, validation/loss=1.055671, validation/num_examples=50000
I0301 04:05:39.579339 139590178428672 logging_writer.py:48] [234500] global_step=234500, grad_norm=4.45529317855835, loss=0.5957691669464111
I0301 04:06:13.592207 139590170035968 logging_writer.py:48] [234600] global_step=234600, grad_norm=4.640989780426025, loss=0.6130111217498779
I0301 04:06:47.515755 139590178428672 logging_writer.py:48] [234700] global_step=234700, grad_norm=4.5336713790893555, loss=0.6172856092453003
I0301 04:07:21.484664 139590170035968 logging_writer.py:48] [234800] global_step=234800, grad_norm=4.678274154663086, loss=0.6571842432022095
I0301 04:07:55.435335 139590178428672 logging_writer.py:48] [234900] global_step=234900, grad_norm=4.185665130615234, loss=0.5385773777961731
I0301 04:08:29.403872 139590170035968 logging_writer.py:48] [235000] global_step=235000, grad_norm=4.62907600402832, loss=0.5901739597320557
I0301 04:09:03.351433 139590178428672 logging_writer.py:48] [235100] global_step=235100, grad_norm=4.4402995109558105, loss=0.5582867860794067
I0301 04:09:37.288934 139590170035968 logging_writer.py:48] [235200] global_step=235200, grad_norm=4.596871376037598, loss=0.6431170105934143
I0301 04:10:11.252337 139590178428672 logging_writer.py:48] [235300] global_step=235300, grad_norm=4.7238311767578125, loss=0.6755791902542114
I0301 04:10:45.202715 139590170035968 logging_writer.py:48] [235400] global_step=235400, grad_norm=4.088647842407227, loss=0.6377003788948059
I0301 04:11:19.165606 139590178428672 logging_writer.py:48] [235500] global_step=235500, grad_norm=4.288642883300781, loss=0.6204878091812134
I0301 04:11:53.112492 139590170035968 logging_writer.py:48] [235600] global_step=235600, grad_norm=4.76092529296875, loss=0.6713317632675171
I0301 04:12:27.123526 139590178428672 logging_writer.py:48] [235700] global_step=235700, grad_norm=4.536201477050781, loss=0.5988389849662781
I0301 04:13:01.045848 139590170035968 logging_writer.py:48] [235800] global_step=235800, grad_norm=4.5747551918029785, loss=0.6277254223823547
I0301 04:13:35.018309 139590178428672 logging_writer.py:48] [235900] global_step=235900, grad_norm=4.492733478546143, loss=0.6178126335144043
I0301 04:13:50.778865 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:13:56.804570 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:14:05.158383 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:14:07.435473 139753105983296 submission_runner.py:411] Time since start: 82978.44s, 	Step: 235948, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14692173898220062, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0547840595245361, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8185477256774902, 'test/num_examples': 10000, 'score': 80132.81121706963, 'total_duration': 82978.43870472908, 'accumulated_submission_time': 80132.81121706963, 'accumulated_eval_time': 2828.3450248241425, 'accumulated_logging_time': 9.016587018966675}
I0301 04:14:07.494803 139589708666624 logging_writer.py:48] [235948] accumulated_eval_time=2828.345025, accumulated_logging_time=9.016587, accumulated_submission_time=80132.811217, global_step=235948, preemption_count=0, score=80132.811217, test/accuracy=0.630400, test/loss=1.818548, test/num_examples=10000, total_duration=82978.438705, train/accuracy=0.960798, train/loss=0.146922, validation/accuracy=0.754860, validation/loss=1.054784, validation/num_examples=50000
I0301 04:14:25.476011 139590144857856 logging_writer.py:48] [236000] global_step=236000, grad_norm=4.533451080322266, loss=0.6316549777984619
I0301 04:14:59.403818 139589708666624 logging_writer.py:48] [236100] global_step=236100, grad_norm=4.342545509338379, loss=0.5668765902519226
I0301 04:15:33.288640 139590144857856 logging_writer.py:48] [236200] global_step=236200, grad_norm=4.5401930809021, loss=0.6547504663467407
I0301 04:16:07.210168 139589708666624 logging_writer.py:48] [236300] global_step=236300, grad_norm=4.058407306671143, loss=0.6410712003707886
I0301 04:16:41.142712 139590144857856 logging_writer.py:48] [236400] global_step=236400, grad_norm=4.245387554168701, loss=0.5837785601615906
I0301 04:17:15.063758 139589708666624 logging_writer.py:48] [236500] global_step=236500, grad_norm=4.631583213806152, loss=0.6482181549072266
I0301 04:17:48.989659 139590144857856 logging_writer.py:48] [236600] global_step=236600, grad_norm=4.651899337768555, loss=0.594805121421814
I0301 04:18:23.121350 139589708666624 logging_writer.py:48] [236700] global_step=236700, grad_norm=4.734711647033691, loss=0.6628589034080505
I0301 04:18:57.068480 139590144857856 logging_writer.py:48] [236800] global_step=236800, grad_norm=4.080299377441406, loss=0.5641473531723022
I0301 04:19:31.007962 139589708666624 logging_writer.py:48] [236900] global_step=236900, grad_norm=4.367515563964844, loss=0.6616078615188599
I0301 04:20:04.947790 139590144857856 logging_writer.py:48] [237000] global_step=237000, grad_norm=3.951232671737671, loss=0.5322434902191162
I0301 04:20:38.860360 139589708666624 logging_writer.py:48] [237100] global_step=237100, grad_norm=4.87598991394043, loss=0.6584438681602478
I0301 04:21:12.785276 139590144857856 logging_writer.py:48] [237200] global_step=237200, grad_norm=4.935192584991455, loss=0.6947005987167358
I0301 04:21:46.691036 139589708666624 logging_writer.py:48] [237300] global_step=237300, grad_norm=6.130764007568359, loss=0.6723591685295105
I0301 04:22:20.634211 139590144857856 logging_writer.py:48] [237400] global_step=237400, grad_norm=4.703343391418457, loss=0.717310905456543
I0301 04:22:37.755893 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:22:43.771903 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:22:52.150790 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:22:54.443130 139753105983296 submission_runner.py:411] Time since start: 83505.45s, 	Step: 237452, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14442911744117737, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0559808015823364, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.819985270500183, 'test/num_examples': 10000, 'score': 80643.00874638557, 'total_duration': 83505.44635510445, 'accumulated_submission_time': 80643.00874638557, 'accumulated_eval_time': 2845.0322086811066, 'accumulated_logging_time': 9.085235595703125}
I0301 04:22:54.506069 139590170035968 logging_writer.py:48] [237452] accumulated_eval_time=2845.032209, accumulated_logging_time=9.085236, accumulated_submission_time=80643.008746, global_step=237452, preemption_count=0, score=80643.008746, test/accuracy=0.630100, test/loss=1.819985, test/num_examples=10000, total_duration=83505.446355, train/accuracy=0.961515, train/loss=0.144429, validation/accuracy=0.755100, validation/loss=1.055981, validation/num_examples=50000
I0301 04:23:11.131961 139590178428672 logging_writer.py:48] [237500] global_step=237500, grad_norm=4.824872970581055, loss=0.7126274704933167
I0301 04:23:45.025164 139590170035968 logging_writer.py:48] [237600] global_step=237600, grad_norm=4.635234832763672, loss=0.6854991912841797
I0301 04:24:18.980254 139590178428672 logging_writer.py:48] [237700] global_step=237700, grad_norm=4.618545055389404, loss=0.5895498991012573
I0301 04:24:53.063199 139590170035968 logging_writer.py:48] [237800] global_step=237800, grad_norm=4.348556041717529, loss=0.5493577122688293
I0301 04:25:27.024370 139590178428672 logging_writer.py:48] [237900] global_step=237900, grad_norm=4.532609939575195, loss=0.612055778503418
I0301 04:26:00.945578 139590170035968 logging_writer.py:48] [238000] global_step=238000, grad_norm=4.236177444458008, loss=0.5825310349464417
I0301 04:26:34.914245 139590178428672 logging_writer.py:48] [238100] global_step=238100, grad_norm=4.501850128173828, loss=0.6008460521697998
I0301 04:27:08.851849 139590170035968 logging_writer.py:48] [238200] global_step=238200, grad_norm=4.244544506072998, loss=0.6010358929634094
I0301 04:27:42.822341 139590178428672 logging_writer.py:48] [238300] global_step=238300, grad_norm=4.568321228027344, loss=0.7040226459503174
I0301 04:28:16.775186 139590170035968 logging_writer.py:48] [238400] global_step=238400, grad_norm=4.341843605041504, loss=0.6127731204032898
I0301 04:28:50.741095 139590178428672 logging_writer.py:48] [238500] global_step=238500, grad_norm=4.754669189453125, loss=0.682079553604126
I0301 04:29:24.667894 139590170035968 logging_writer.py:48] [238600] global_step=238600, grad_norm=4.330005168914795, loss=0.5710278749465942
I0301 04:29:58.636864 139590178428672 logging_writer.py:48] [238700] global_step=238700, grad_norm=4.495813846588135, loss=0.6223422884941101
I0301 04:30:32.655460 139590170035968 logging_writer.py:48] [238800] global_step=238800, grad_norm=4.218008518218994, loss=0.5654904246330261
I0301 04:31:06.603347 139590178428672 logging_writer.py:48] [238900] global_step=238900, grad_norm=4.699310779571533, loss=0.6426049470901489
I0301 04:31:24.740035 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:31:30.771583 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:31:39.090354 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:31:41.394446 139753105983296 submission_runner.py:411] Time since start: 84032.40s, 	Step: 238955, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.14695745706558228, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.055910587310791, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8194414377212524, 'test/num_examples': 10000, 'score': 81153.17875504494, 'total_duration': 84032.39768075943, 'accumulated_submission_time': 81153.17875504494, 'accumulated_eval_time': 2861.686569929123, 'accumulated_logging_time': 9.158106327056885}
I0301 04:31:41.453866 139589717059328 logging_writer.py:48] [238955] accumulated_eval_time=2861.686570, accumulated_logging_time=9.158106, accumulated_submission_time=81153.178755, global_step=238955, preemption_count=0, score=81153.178755, test/accuracy=0.631100, test/loss=1.819441, test/num_examples=10000, total_duration=84032.397681, train/accuracy=0.960081, train/loss=0.146957, validation/accuracy=0.754920, validation/loss=1.055911, validation/num_examples=50000
I0301 04:31:57.055186 139589725452032 logging_writer.py:48] [239000] global_step=239000, grad_norm=4.210718631744385, loss=0.588998556137085
I0301 04:32:30.959336 139589717059328 logging_writer.py:48] [239100] global_step=239100, grad_norm=4.377852916717529, loss=0.5716156363487244
I0301 04:33:04.868191 139589725452032 logging_writer.py:48] [239200] global_step=239200, grad_norm=4.781363487243652, loss=0.6734290719032288
I0301 04:33:38.820068 139589717059328 logging_writer.py:48] [239300] global_step=239300, grad_norm=4.74427604675293, loss=0.6616032123565674
I0301 04:34:12.727894 139589725452032 logging_writer.py:48] [239400] global_step=239400, grad_norm=4.845458507537842, loss=0.6350786089897156
I0301 04:34:46.657627 139589717059328 logging_writer.py:48] [239500] global_step=239500, grad_norm=4.915727615356445, loss=0.588591456413269
I0301 04:35:20.597937 139589725452032 logging_writer.py:48] [239600] global_step=239600, grad_norm=5.034118175506592, loss=0.6378436088562012
I0301 04:35:54.535635 139589717059328 logging_writer.py:48] [239700] global_step=239700, grad_norm=4.43643045425415, loss=0.6266658902168274
I0301 04:36:28.482258 139589725452032 logging_writer.py:48] [239800] global_step=239800, grad_norm=4.957208156585693, loss=0.6252798438072205
I0301 04:37:02.467372 139589717059328 logging_writer.py:48] [239900] global_step=239900, grad_norm=4.389240741729736, loss=0.6377627849578857
I0301 04:37:36.366781 139589725452032 logging_writer.py:48] [240000] global_step=240000, grad_norm=4.084123134613037, loss=0.5994213223457336
I0301 04:38:10.266606 139589717059328 logging_writer.py:48] [240100] global_step=240100, grad_norm=4.8714423179626465, loss=0.6095860004425049
I0301 04:38:44.190434 139589725452032 logging_writer.py:48] [240200] global_step=240200, grad_norm=4.174088954925537, loss=0.5707887411117554
I0301 04:39:18.105231 139589717059328 logging_writer.py:48] [240300] global_step=240300, grad_norm=4.421713829040527, loss=0.5921738743782043
I0301 04:39:52.030647 139589725452032 logging_writer.py:48] [240400] global_step=240400, grad_norm=4.278258323669434, loss=0.5638226270675659
I0301 04:40:11.523971 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:40:17.533598 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:40:25.856842 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:40:28.166075 139753105983296 submission_runner.py:411] Time since start: 84559.17s, 	Step: 240459, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14766736328601837, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0556063652038574, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8203246593475342, 'test/num_examples': 10000, 'score': 81663.18556928635, 'total_duration': 84559.16914653778, 'accumulated_submission_time': 81663.18556928635, 'accumulated_eval_time': 2878.328460454941, 'accumulated_logging_time': 9.227638959884644}
I0301 04:40:28.227059 139590178428672 logging_writer.py:48] [240459] accumulated_eval_time=2878.328460, accumulated_logging_time=9.227639, accumulated_submission_time=81663.185569, global_step=240459, preemption_count=0, score=81663.185569, test/accuracy=0.630700, test/loss=1.820325, test/num_examples=10000, total_duration=84559.169147, train/accuracy=0.960041, train/loss=0.147667, validation/accuracy=0.754740, validation/loss=1.055606, validation/num_examples=50000
I0301 04:40:42.483923 139590186821376 logging_writer.py:48] [240500] global_step=240500, grad_norm=4.554989337921143, loss=0.5979252457618713
I0301 04:41:16.420006 139590178428672 logging_writer.py:48] [240600] global_step=240600, grad_norm=4.824353218078613, loss=0.6397880911827087
I0301 04:41:50.405837 139590186821376 logging_writer.py:48] [240700] global_step=240700, grad_norm=4.510098934173584, loss=0.6372234225273132
I0301 04:42:24.370965 139590178428672 logging_writer.py:48] [240800] global_step=240800, grad_norm=4.808760643005371, loss=0.5990955233573914
I0301 04:42:58.514451 139590186821376 logging_writer.py:48] [240900] global_step=240900, grad_norm=4.647965908050537, loss=0.6508631706237793
I0301 04:43:32.450977 139590178428672 logging_writer.py:48] [241000] global_step=241000, grad_norm=4.222624778747559, loss=0.5280026197433472
I0301 04:44:06.440845 139590186821376 logging_writer.py:48] [241100] global_step=241100, grad_norm=4.54520845413208, loss=0.688357949256897
I0301 04:44:40.380571 139590178428672 logging_writer.py:48] [241200] global_step=241200, grad_norm=4.375191688537598, loss=0.5956894755363464
I0301 04:45:14.346661 139590186821376 logging_writer.py:48] [241300] global_step=241300, grad_norm=4.960473537445068, loss=0.6590898633003235
I0301 04:45:48.320790 139590178428672 logging_writer.py:48] [241400] global_step=241400, grad_norm=4.9925642013549805, loss=0.6962779760360718
I0301 04:46:22.312543 139590186821376 logging_writer.py:48] [241500] global_step=241500, grad_norm=4.555334091186523, loss=0.5538092851638794
I0301 04:46:56.297822 139590178428672 logging_writer.py:48] [241600] global_step=241600, grad_norm=4.529752254486084, loss=0.628572940826416
I0301 04:47:30.274631 139590186821376 logging_writer.py:48] [241700] global_step=241700, grad_norm=4.165694236755371, loss=0.5249791145324707
I0301 04:48:04.232994 139590178428672 logging_writer.py:48] [241800] global_step=241800, grad_norm=4.646185398101807, loss=0.6728216409683228
I0301 04:48:38.209466 139590186821376 logging_writer.py:48] [241900] global_step=241900, grad_norm=4.444906234741211, loss=0.6010085344314575
I0301 04:48:58.403270 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:49:04.804800 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:49:13.081027 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:49:15.364178 139753105983296 submission_runner.py:411] Time since start: 85086.37s, 	Step: 241961, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.1453227698802948, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0559238195419312, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8199002742767334, 'test/num_examples': 10000, 'score': 82173.29926466942, 'total_duration': 85086.36740994453, 'accumulated_submission_time': 82173.29926466942, 'accumulated_eval_time': 2895.289322376251, 'accumulated_logging_time': 9.298870325088501}
I0301 04:49:15.422860 139589708666624 logging_writer.py:48] [241961] accumulated_eval_time=2895.289322, accumulated_logging_time=9.298870, accumulated_submission_time=82173.299265, global_step=241961, preemption_count=0, score=82173.299265, test/accuracy=0.630700, test/loss=1.819900, test/num_examples=10000, total_duration=85086.367410, train/accuracy=0.961316, train/loss=0.145323, validation/accuracy=0.755060, validation/loss=1.055924, validation/num_examples=50000
I0301 04:49:28.976225 139589717059328 logging_writer.py:48] [242000] global_step=242000, grad_norm=4.704634666442871, loss=0.6217312216758728
I0301 04:50:02.880188 139589708666624 logging_writer.py:48] [242100] global_step=242100, grad_norm=4.297754287719727, loss=0.597243070602417
I0301 04:50:36.784294 139589717059328 logging_writer.py:48] [242200] global_step=242200, grad_norm=4.3999738693237305, loss=0.6917204260826111
I0301 04:51:10.738632 139589708666624 logging_writer.py:48] [242300] global_step=242300, grad_norm=4.630455493927002, loss=0.6668976545333862
I0301 04:51:44.668361 139589717059328 logging_writer.py:48] [242400] global_step=242400, grad_norm=4.5779290199279785, loss=0.6264145374298096
I0301 04:52:18.600834 139589708666624 logging_writer.py:48] [242500] global_step=242500, grad_norm=4.712551116943359, loss=0.6465135812759399
I0301 04:52:52.579306 139589717059328 logging_writer.py:48] [242600] global_step=242600, grad_norm=4.666219711303711, loss=0.669432520866394
I0301 04:53:26.526961 139589708666624 logging_writer.py:48] [242700] global_step=242700, grad_norm=4.313623905181885, loss=0.5216816663742065
I0301 04:54:00.472361 139589717059328 logging_writer.py:48] [242800] global_step=242800, grad_norm=4.873144149780273, loss=0.6558735370635986
I0301 04:54:34.429708 139589708666624 logging_writer.py:48] [242900] global_step=242900, grad_norm=4.296282768249512, loss=0.6385273337364197
I0301 04:55:08.354317 139589717059328 logging_writer.py:48] [243000] global_step=243000, grad_norm=4.751169681549072, loss=0.640662670135498
I0301 04:55:42.396706 139589708666624 logging_writer.py:48] [243100] global_step=243100, grad_norm=4.577003479003906, loss=0.6252343654632568
I0301 04:56:16.323848 139589717059328 logging_writer.py:48] [243200] global_step=243200, grad_norm=4.410194396972656, loss=0.5807067155838013
I0301 04:56:50.277540 139589708666624 logging_writer.py:48] [243300] global_step=243300, grad_norm=4.431614875793457, loss=0.5900260210037231
I0301 04:57:24.245718 139589717059328 logging_writer.py:48] [243400] global_step=243400, grad_norm=4.9348297119140625, loss=0.6313247084617615
I0301 04:57:45.431288 139753105983296 spec.py:321] Evaluating on the training split.
I0301 04:57:51.456682 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 04:57:59.665119 139753105983296 spec.py:349] Evaluating on the test split.
I0301 04:58:01.959518 139753105983296 submission_runner.py:411] Time since start: 85612.96s, 	Step: 243464, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.1468772292137146, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.054276943206787, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8192369937896729, 'test/num_examples': 10000, 'score': 82683.24376130104, 'total_duration': 85612.96273779869, 'accumulated_submission_time': 82683.24376130104, 'accumulated_eval_time': 2911.817486524582, 'accumulated_logging_time': 9.36837124824524}
I0301 04:58:02.021117 139590161643264 logging_writer.py:48] [243464] accumulated_eval_time=2911.817487, accumulated_logging_time=9.368371, accumulated_submission_time=82683.243761, global_step=243464, preemption_count=0, score=82683.243761, test/accuracy=0.629800, test/loss=1.819237, test/num_examples=10000, total_duration=85612.962738, train/accuracy=0.961276, train/loss=0.146877, validation/accuracy=0.755220, validation/loss=1.054277, validation/num_examples=50000
I0301 04:58:14.596750 139590170035968 logging_writer.py:48] [243500] global_step=243500, grad_norm=4.9627909660339355, loss=0.6695019602775574
I0301 04:58:48.494161 139590161643264 logging_writer.py:48] [243600] global_step=243600, grad_norm=5.099064826965332, loss=0.6543493270874023
I0301 04:59:22.423496 139590170035968 logging_writer.py:48] [243700] global_step=243700, grad_norm=4.99312686920166, loss=0.622981071472168
I0301 04:59:56.379375 139590161643264 logging_writer.py:48] [243800] global_step=243800, grad_norm=4.485688209533691, loss=0.6282597184181213
I0301 05:00:30.340213 139590170035968 logging_writer.py:48] [243900] global_step=243900, grad_norm=4.371480941772461, loss=0.5986489057540894
I0301 05:01:04.301988 139590161643264 logging_writer.py:48] [244000] global_step=244000, grad_norm=4.546472549438477, loss=0.602306604385376
I0301 05:01:38.366124 139590170035968 logging_writer.py:48] [244100] global_step=244100, grad_norm=4.538158893585205, loss=0.6298432350158691
I0301 05:02:12.322716 139590161643264 logging_writer.py:48] [244200] global_step=244200, grad_norm=4.56079626083374, loss=0.6333818435668945
I0301 05:02:46.287298 139590170035968 logging_writer.py:48] [244300] global_step=244300, grad_norm=4.1689581871032715, loss=0.604167103767395
I0301 05:03:20.233348 139590161643264 logging_writer.py:48] [244400] global_step=244400, grad_norm=4.921389102935791, loss=0.6678054332733154
I0301 05:03:54.203589 139590170035968 logging_writer.py:48] [244500] global_step=244500, grad_norm=4.770162105560303, loss=0.6890289783477783
I0301 05:04:28.174192 139590161643264 logging_writer.py:48] [244600] global_step=244600, grad_norm=4.194312572479248, loss=0.5700026750564575
I0301 05:05:02.145769 139590170035968 logging_writer.py:48] [244700] global_step=244700, grad_norm=5.438496112823486, loss=0.708433210849762
I0301 05:05:36.115922 139590161643264 logging_writer.py:48] [244800] global_step=244800, grad_norm=4.089120388031006, loss=0.5349739193916321
I0301 05:06:10.085441 139590170035968 logging_writer.py:48] [244900] global_step=244900, grad_norm=4.254844665527344, loss=0.6678516864776611
I0301 05:06:31.970034 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:06:38.028006 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:06:46.466791 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:06:48.753310 139753105983296 submission_runner.py:411] Time since start: 86139.76s, 	Step: 244966, 	{'train/accuracy': 0.9592832922935486, 'train/loss': 0.14743292331695557, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.0548502206802368, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8175748586654663, 'test/num_examples': 10000, 'score': 83193.1289358139, 'total_duration': 86139.756534338, 'accumulated_submission_time': 83193.1289358139, 'accumulated_eval_time': 2928.6007051467896, 'accumulated_logging_time': 9.440913200378418}
I0301 05:06:48.818215 139590153250560 logging_writer.py:48] [244966] accumulated_eval_time=2928.600705, accumulated_logging_time=9.440913, accumulated_submission_time=83193.128936, global_step=244966, preemption_count=0, score=83193.128936, test/accuracy=0.630200, test/loss=1.817575, test/num_examples=10000, total_duration=86139.756534, train/accuracy=0.959283, train/loss=0.147433, validation/accuracy=0.755160, validation/loss=1.054850, validation/num_examples=50000
I0301 05:07:00.685627 139590186821376 logging_writer.py:48] [245000] global_step=245000, grad_norm=4.17725133895874, loss=0.5549262166023254
I0301 05:07:34.631239 139590153250560 logging_writer.py:48] [245100] global_step=245100, grad_norm=4.53795862197876, loss=0.5623531341552734
I0301 05:08:08.576671 139590186821376 logging_writer.py:48] [245200] global_step=245200, grad_norm=4.66339635848999, loss=0.5612860321998596
I0301 05:08:42.547386 139590153250560 logging_writer.py:48] [245300] global_step=245300, grad_norm=4.568584442138672, loss=0.6276851892471313
I0301 05:09:16.499898 139590186821376 logging_writer.py:48] [245400] global_step=245400, grad_norm=5.0264363288879395, loss=0.6481078863143921
I0301 05:09:50.412273 139590153250560 logging_writer.py:48] [245500] global_step=245500, grad_norm=4.5313944816589355, loss=0.6678858995437622
I0301 05:10:24.380415 139590186821376 logging_writer.py:48] [245600] global_step=245600, grad_norm=4.243261814117432, loss=0.5850228667259216
I0301 05:10:58.312363 139590153250560 logging_writer.py:48] [245700] global_step=245700, grad_norm=4.546770095825195, loss=0.6248000860214233
I0301 05:11:32.261386 139590186821376 logging_writer.py:48] [245800] global_step=245800, grad_norm=4.810342788696289, loss=0.5972116589546204
I0301 05:12:06.245623 139590153250560 logging_writer.py:48] [245900] global_step=245900, grad_norm=4.199813365936279, loss=0.571535050868988
I0301 05:12:40.194146 139590186821376 logging_writer.py:48] [246000] global_step=246000, grad_norm=4.567996978759766, loss=0.7025907039642334
I0301 05:13:14.161419 139590153250560 logging_writer.py:48] [246100] global_step=246100, grad_norm=4.567004203796387, loss=0.641077995300293
I0301 05:13:48.182687 139590186821376 logging_writer.py:48] [246200] global_step=246200, grad_norm=4.452367305755615, loss=0.565322995185852
I0301 05:14:22.131948 139590153250560 logging_writer.py:48] [246300] global_step=246300, grad_norm=4.878686428070068, loss=0.6370190978050232
I0301 05:14:56.082669 139590186821376 logging_writer.py:48] [246400] global_step=246400, grad_norm=4.759286403656006, loss=0.629013180732727
I0301 05:15:18.965494 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:15:25.064164 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:15:33.452103 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:15:35.730524 139753105983296 submission_runner.py:411] Time since start: 86666.73s, 	Step: 246469, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14578300714492798, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.0571213960647583, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8201065063476562, 'test/num_examples': 10000, 'score': 83703.21215891838, 'total_duration': 86666.73375701904, 'accumulated_submission_time': 83703.21215891838, 'accumulated_eval_time': 2945.3656992912292, 'accumulated_logging_time': 9.517107009887695}
I0301 05:15:35.792736 139589717059328 logging_writer.py:48] [246469] accumulated_eval_time=2945.365699, accumulated_logging_time=9.517107, accumulated_submission_time=83703.212159, global_step=246469, preemption_count=0, score=83703.212159, test/accuracy=0.631900, test/loss=1.820107, test/num_examples=10000, total_duration=86666.733757, train/accuracy=0.961097, train/loss=0.145783, validation/accuracy=0.754620, validation/loss=1.057121, validation/num_examples=50000
I0301 05:15:46.682940 139589725452032 logging_writer.py:48] [246500] global_step=246500, grad_norm=4.570634841918945, loss=0.6954320073127747
I0301 05:16:20.572551 139589717059328 logging_writer.py:48] [246600] global_step=246600, grad_norm=5.006012439727783, loss=0.6709103584289551
I0301 05:16:54.519308 139589725452032 logging_writer.py:48] [246700] global_step=246700, grad_norm=4.9332356452941895, loss=0.6079771518707275
I0301 05:17:28.470492 139589717059328 logging_writer.py:48] [246800] global_step=246800, grad_norm=4.804222583770752, loss=0.6276933550834656
I0301 05:18:02.419003 139589725452032 logging_writer.py:48] [246900] global_step=246900, grad_norm=4.582887649536133, loss=0.567111611366272
I0301 05:18:36.366050 139589717059328 logging_writer.py:48] [247000] global_step=247000, grad_norm=4.298156261444092, loss=0.6236333250999451
I0301 05:19:10.313594 139589725452032 logging_writer.py:48] [247100] global_step=247100, grad_norm=4.811503887176514, loss=0.662140965461731
I0301 05:19:44.216296 139589717059328 logging_writer.py:48] [247200] global_step=247200, grad_norm=4.500728607177734, loss=0.62221759557724
I0301 05:20:18.259073 139589725452032 logging_writer.py:48] [247300] global_step=247300, grad_norm=5.028311252593994, loss=0.6634019017219543
I0301 05:20:52.168689 139589717059328 logging_writer.py:48] [247400] global_step=247400, grad_norm=4.8414130210876465, loss=0.6527836918830872
I0301 05:21:26.136247 139589725452032 logging_writer.py:48] [247500] global_step=247500, grad_norm=4.624152183532715, loss=0.6940410733222961
I0301 05:22:00.088899 139589717059328 logging_writer.py:48] [247600] global_step=247600, grad_norm=4.748118877410889, loss=0.6532147526741028
I0301 05:22:34.046652 139589725452032 logging_writer.py:48] [247700] global_step=247700, grad_norm=4.544793128967285, loss=0.6030587553977966
I0301 05:23:08.003286 139589717059328 logging_writer.py:48] [247800] global_step=247800, grad_norm=4.226062297821045, loss=0.5991322994232178
I0301 05:23:41.958206 139589725452032 logging_writer.py:48] [247900] global_step=247900, grad_norm=4.83856725692749, loss=0.6373656392097473
I0301 05:24:05.852336 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:24:11.855630 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:24:20.174512 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:24:22.408002 139753105983296 submission_runner.py:411] Time since start: 87193.41s, 	Step: 247972, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.14910946786403656, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0558416843414307, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8187205791473389, 'test/num_examples': 10000, 'score': 84213.20818805695, 'total_duration': 87193.41122412682, 'accumulated_submission_time': 84213.20818805695, 'accumulated_eval_time': 2961.9213037490845, 'accumulated_logging_time': 9.590791940689087}
I0301 05:24:22.471503 139589725452032 logging_writer.py:48] [247972] accumulated_eval_time=2961.921304, accumulated_logging_time=9.590792, accumulated_submission_time=84213.208188, global_step=247972, preemption_count=0, score=84213.208188, test/accuracy=0.630500, test/loss=1.818721, test/num_examples=10000, total_duration=87193.411224, train/accuracy=0.959263, train/loss=0.149109, validation/accuracy=0.754840, validation/loss=1.055842, validation/num_examples=50000
I0301 05:24:32.326644 139590161643264 logging_writer.py:48] [248000] global_step=248000, grad_norm=4.425165176391602, loss=0.5758693218231201
I0301 05:25:06.193498 139589725452032 logging_writer.py:48] [248100] global_step=248100, grad_norm=4.6839189529418945, loss=0.624606728553772
I0301 05:25:40.095560 139590161643264 logging_writer.py:48] [248200] global_step=248200, grad_norm=4.278870105743408, loss=0.5740731954574585
I0301 05:26:14.136401 139589725452032 logging_writer.py:48] [248300] global_step=248300, grad_norm=4.591132640838623, loss=0.6724980473518372
I0301 05:26:48.044611 139590161643264 logging_writer.py:48] [248400] global_step=248400, grad_norm=4.303495407104492, loss=0.6303426623344421
I0301 05:27:21.966908 139589725452032 logging_writer.py:48] [248500] global_step=248500, grad_norm=4.296769618988037, loss=0.5373712182044983
I0301 05:27:55.930839 139590161643264 logging_writer.py:48] [248600] global_step=248600, grad_norm=4.045967102050781, loss=0.4961410164833069
I0301 05:28:29.868069 139589725452032 logging_writer.py:48] [248700] global_step=248700, grad_norm=4.6868672370910645, loss=0.6207304000854492
I0301 05:29:03.822055 139590161643264 logging_writer.py:48] [248800] global_step=248800, grad_norm=4.126866340637207, loss=0.5614234805107117
I0301 05:29:37.789690 139589725452032 logging_writer.py:48] [248900] global_step=248900, grad_norm=4.557384967803955, loss=0.5825261473655701
I0301 05:30:11.736907 139590161643264 logging_writer.py:48] [249000] global_step=249000, grad_norm=4.384479999542236, loss=0.6366672515869141
I0301 05:30:45.677797 139589725452032 logging_writer.py:48] [249100] global_step=249100, grad_norm=4.474860668182373, loss=0.6019818186759949
I0301 05:31:19.648648 139590161643264 logging_writer.py:48] [249200] global_step=249200, grad_norm=4.857781410217285, loss=0.7273701429367065
I0301 05:31:53.604085 139589725452032 logging_writer.py:48] [249300] global_step=249300, grad_norm=4.559070110321045, loss=0.5838997960090637
I0301 05:32:27.620908 139590161643264 logging_writer.py:48] [249400] global_step=249400, grad_norm=5.175415992736816, loss=0.6471655368804932
I0301 05:32:52.554544 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:32:58.573364 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:33:06.996634 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:33:09.244337 139753105983296 submission_runner.py:411] Time since start: 87720.25s, 	Step: 249475, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.14736899733543396, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.05515718460083, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8202266693115234, 'test/num_examples': 10000, 'score': 84723.22751140594, 'total_duration': 87720.24756479263, 'accumulated_submission_time': 84723.22751140594, 'accumulated_eval_time': 2978.6110417842865, 'accumulated_logging_time': 9.664908409118652}
I0301 05:33:09.315221 139589717059328 logging_writer.py:48] [249475] accumulated_eval_time=2978.611042, accumulated_logging_time=9.664908, accumulated_submission_time=84723.227511, global_step=249475, preemption_count=0, score=84723.227511, test/accuracy=0.629900, test/loss=1.820227, test/num_examples=10000, total_duration=87720.247565, train/accuracy=0.960379, train/loss=0.147369, validation/accuracy=0.754880, validation/loss=1.055157, validation/num_examples=50000
I0301 05:33:18.156622 139589725452032 logging_writer.py:48] [249500] global_step=249500, grad_norm=4.309726238250732, loss=0.6253176927566528
I0301 05:33:52.007522 139589717059328 logging_writer.py:48] [249600] global_step=249600, grad_norm=4.488729953765869, loss=0.6297668218612671
I0301 05:34:25.907914 139589725452032 logging_writer.py:48] [249700] global_step=249700, grad_norm=4.27882719039917, loss=0.5512200593948364
I0301 05:34:59.844877 139589717059328 logging_writer.py:48] [249800] global_step=249800, grad_norm=4.229405879974365, loss=0.6220717430114746
I0301 05:35:33.754653 139589725452032 logging_writer.py:48] [249900] global_step=249900, grad_norm=4.615704536437988, loss=0.5983607769012451
I0301 05:36:07.699223 139589717059328 logging_writer.py:48] [250000] global_step=250000, grad_norm=3.9199962615966797, loss=0.5049950480461121
I0301 05:36:41.665821 139589725452032 logging_writer.py:48] [250100] global_step=250100, grad_norm=4.723220348358154, loss=0.6220695972442627
I0301 05:37:15.609530 139589717059328 logging_writer.py:48] [250200] global_step=250200, grad_norm=4.275181293487549, loss=0.5532416105270386
I0301 05:37:49.540301 139589725452032 logging_writer.py:48] [250300] global_step=250300, grad_norm=4.84572172164917, loss=0.6670762300491333
I0301 05:38:23.640518 139589717059328 logging_writer.py:48] [250400] global_step=250400, grad_norm=3.98555326461792, loss=0.5530561804771423
I0301 05:38:57.612467 139589725452032 logging_writer.py:48] [250500] global_step=250500, grad_norm=4.54718542098999, loss=0.6677196025848389
I0301 05:39:31.556469 139589717059328 logging_writer.py:48] [250600] global_step=250600, grad_norm=4.462062358856201, loss=0.6075407862663269
I0301 05:40:05.484592 139589725452032 logging_writer.py:48] [250700] global_step=250700, grad_norm=4.101652145385742, loss=0.5734722018241882
I0301 05:40:39.463319 139589717059328 logging_writer.py:48] [250800] global_step=250800, grad_norm=4.439602375030518, loss=0.61888587474823
I0301 05:41:13.385729 139589725452032 logging_writer.py:48] [250900] global_step=250900, grad_norm=4.727574825286865, loss=0.5881204605102539
I0301 05:41:39.343170 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:41:45.425111 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:41:53.797611 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:41:56.153342 139753105983296 submission_runner.py:411] Time since start: 88247.16s, 	Step: 250978, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.15125492215156555, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.055238127708435, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8185988664627075, 'test/num_examples': 10000, 'score': 85233.19104790688, 'total_duration': 88247.156447649, 'accumulated_submission_time': 85233.19104790688, 'accumulated_eval_time': 2995.421051502228, 'accumulated_logging_time': 9.746666669845581}
I0301 05:41:56.213804 139590161643264 logging_writer.py:48] [250978] accumulated_eval_time=2995.421052, accumulated_logging_time=9.746667, accumulated_submission_time=85233.191048, global_step=250978, preemption_count=0, score=85233.191048, test/accuracy=0.630000, test/loss=1.818599, test/num_examples=10000, total_duration=88247.156448, train/accuracy=0.959702, train/loss=0.151255, validation/accuracy=0.755320, validation/loss=1.055238, validation/num_examples=50000
I0301 05:42:04.019216 139590170035968 logging_writer.py:48] [251000] global_step=251000, grad_norm=4.366495132446289, loss=0.6303606033325195
I0301 05:42:37.914712 139590161643264 logging_writer.py:48] [251100] global_step=251100, grad_norm=4.2787275314331055, loss=0.6090966463088989
I0301 05:43:11.863454 139590170035968 logging_writer.py:48] [251200] global_step=251200, grad_norm=4.565652370452881, loss=0.6576109528541565
I0301 05:43:45.812053 139590161643264 logging_writer.py:48] [251300] global_step=251300, grad_norm=4.696869373321533, loss=0.5598041415214539
I0301 05:44:19.718790 139590170035968 logging_writer.py:48] [251400] global_step=251400, grad_norm=4.5725250244140625, loss=0.6346040964126587
I0301 05:44:53.813658 139590161643264 logging_writer.py:48] [251500] global_step=251500, grad_norm=4.356639385223389, loss=0.6709712743759155
I0301 05:45:27.774612 139590170035968 logging_writer.py:48] [251600] global_step=251600, grad_norm=4.157313823699951, loss=0.5491652488708496
I0301 05:46:01.731069 139590161643264 logging_writer.py:48] [251700] global_step=251700, grad_norm=4.11245584487915, loss=0.6422364711761475
I0301 05:46:35.697818 139590170035968 logging_writer.py:48] [251800] global_step=251800, grad_norm=4.801654815673828, loss=0.6535689830780029
I0301 05:47:09.644916 139590161643264 logging_writer.py:48] [251900] global_step=251900, grad_norm=4.4468607902526855, loss=0.6112119555473328
I0301 05:47:43.571768 139590170035968 logging_writer.py:48] [252000] global_step=252000, grad_norm=4.562572956085205, loss=0.5895152688026428
I0301 05:48:17.536406 139590161643264 logging_writer.py:48] [252100] global_step=252100, grad_norm=4.646164894104004, loss=0.60431307554245
I0301 05:48:51.488192 139590170035968 logging_writer.py:48] [252200] global_step=252200, grad_norm=4.773221969604492, loss=0.6801362633705139
I0301 05:49:25.428015 139590161643264 logging_writer.py:48] [252300] global_step=252300, grad_norm=4.8525800704956055, loss=0.6168351173400879
I0301 05:49:59.387069 139590170035968 logging_writer.py:48] [252400] global_step=252400, grad_norm=4.824807643890381, loss=0.668026328086853
I0301 05:50:26.320551 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:50:32.434492 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:50:40.678175 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:50:42.968381 139753105983296 submission_runner.py:411] Time since start: 88773.97s, 	Step: 252481, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14660915732383728, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0560367107391357, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.818914532661438, 'test/num_examples': 10000, 'score': 85743.23414087296, 'total_duration': 88773.97137570381, 'accumulated_submission_time': 85743.23414087296, 'accumulated_eval_time': 3012.068596839905, 'accumulated_logging_time': 9.817442417144775}
I0301 05:50:43.062820 139589717059328 logging_writer.py:48] [252481] accumulated_eval_time=3012.068597, accumulated_logging_time=9.817442, accumulated_submission_time=85743.234141, global_step=252481, preemption_count=0, score=85743.234141, test/accuracy=0.630600, test/loss=1.818915, test/num_examples=10000, total_duration=88773.971376, train/accuracy=0.961515, train/loss=0.146609, validation/accuracy=0.754780, validation/loss=1.056037, validation/num_examples=50000
I0301 05:50:49.990827 139589725452032 logging_writer.py:48] [252500] global_step=252500, grad_norm=4.487349033355713, loss=0.6373031735420227
I0301 05:51:23.938925 139589717059328 logging_writer.py:48] [252600] global_step=252600, grad_norm=4.848590850830078, loss=0.5852717161178589
I0301 05:51:57.874538 139589725452032 logging_writer.py:48] [252700] global_step=252700, grad_norm=4.352996349334717, loss=0.6087225675582886
I0301 05:52:31.802986 139589717059328 logging_writer.py:48] [252800] global_step=252800, grad_norm=4.025001049041748, loss=0.6060580015182495
I0301 05:53:05.784176 139589725452032 logging_writer.py:48] [252900] global_step=252900, grad_norm=5.267693996429443, loss=0.6881918907165527
I0301 05:53:39.712769 139589717059328 logging_writer.py:48] [253000] global_step=253000, grad_norm=4.525062561035156, loss=0.6452539563179016
I0301 05:54:13.659237 139589725452032 logging_writer.py:48] [253100] global_step=253100, grad_norm=4.207703113555908, loss=0.5977027416229248
I0301 05:54:47.621800 139589717059328 logging_writer.py:48] [253200] global_step=253200, grad_norm=4.4305548667907715, loss=0.5982965230941772
I0301 05:55:21.560162 139589725452032 logging_writer.py:48] [253300] global_step=253300, grad_norm=4.148341178894043, loss=0.5885164737701416
I0301 05:55:55.512871 139589717059328 logging_writer.py:48] [253400] global_step=253400, grad_norm=4.667759895324707, loss=0.6075830459594727
I0301 05:56:29.471503 139589725452032 logging_writer.py:48] [253500] global_step=253500, grad_norm=4.857079982757568, loss=0.6087775230407715
I0301 05:57:03.466745 139589717059328 logging_writer.py:48] [253600] global_step=253600, grad_norm=4.693117618560791, loss=0.6154962778091431
I0301 05:57:37.412304 139589725452032 logging_writer.py:48] [253700] global_step=253700, grad_norm=4.72765588760376, loss=0.6298989057540894
I0301 05:58:11.348408 139589717059328 logging_writer.py:48] [253800] global_step=253800, grad_norm=4.55733060836792, loss=0.6304861307144165
I0301 05:58:45.302472 139589725452032 logging_writer.py:48] [253900] global_step=253900, grad_norm=4.564087390899658, loss=0.6208744049072266
I0301 05:59:13.279849 139753105983296 spec.py:321] Evaluating on the training split.
I0301 05:59:19.336487 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 05:59:27.556435 139753105983296 spec.py:349] Evaluating on the test split.
I0301 05:59:29.868697 139753105983296 submission_runner.py:411] Time since start: 89300.87s, 	Step: 253984, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14542751014232635, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0555272102355957, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8176671266555786, 'test/num_examples': 10000, 'score': 86253.38255238533, 'total_duration': 89300.87192893028, 'accumulated_submission_time': 86253.38255238533, 'accumulated_eval_time': 3028.657395362854, 'accumulated_logging_time': 9.926298141479492}
I0301 05:59:29.931535 139590178428672 logging_writer.py:48] [253984] accumulated_eval_time=3028.657395, accumulated_logging_time=9.926298, accumulated_submission_time=86253.382552, global_step=253984, preemption_count=0, score=86253.382552, test/accuracy=0.631200, test/loss=1.817667, test/num_examples=10000, total_duration=89300.871929, train/accuracy=0.961117, train/loss=0.145428, validation/accuracy=0.754680, validation/loss=1.055527, validation/num_examples=50000
I0301 05:59:35.706770 139590186821376 logging_writer.py:48] [254000] global_step=254000, grad_norm=4.7134294509887695, loss=0.5982732176780701
I0301 06:00:09.577958 139590178428672 logging_writer.py:48] [254100] global_step=254100, grad_norm=4.653197288513184, loss=0.5510917901992798
I0301 06:00:43.483682 139590186821376 logging_writer.py:48] [254200] global_step=254200, grad_norm=4.137948036193848, loss=0.6072260141372681
I0301 06:01:17.436113 139590178428672 logging_writer.py:48] [254300] global_step=254300, grad_norm=4.916045188903809, loss=0.6250545382499695
I0301 06:01:51.382817 139590186821376 logging_writer.py:48] [254400] global_step=254400, grad_norm=4.809860706329346, loss=0.6286520957946777
I0301 06:02:25.319987 139590178428672 logging_writer.py:48] [254500] global_step=254500, grad_norm=4.484287261962891, loss=0.6435930728912354
I0301 06:02:59.275858 139590186821376 logging_writer.py:48] [254600] global_step=254600, grad_norm=4.686001777648926, loss=0.6305476427078247
I0301 06:03:33.262850 139590178428672 logging_writer.py:48] [254700] global_step=254700, grad_norm=4.1376190185546875, loss=0.5463696122169495
I0301 06:04:07.204044 139590186821376 logging_writer.py:48] [254800] global_step=254800, grad_norm=4.5539422035217285, loss=0.6378952860832214
I0301 06:04:41.152130 139590178428672 logging_writer.py:48] [254900] global_step=254900, grad_norm=4.844319820404053, loss=0.6408288478851318
I0301 06:05:15.063249 139590186821376 logging_writer.py:48] [255000] global_step=255000, grad_norm=4.7406158447265625, loss=0.5916871428489685
I0301 06:05:49.000795 139590178428672 logging_writer.py:48] [255100] global_step=255100, grad_norm=4.3788042068481445, loss=0.579609215259552
I0301 06:06:22.911738 139590186821376 logging_writer.py:48] [255200] global_step=255200, grad_norm=4.088733673095703, loss=0.5476762056350708
I0301 06:06:56.837078 139590178428672 logging_writer.py:48] [255300] global_step=255300, grad_norm=4.30828332901001, loss=0.5860859155654907
I0301 06:07:30.791033 139590186821376 logging_writer.py:48] [255400] global_step=255400, grad_norm=4.444745063781738, loss=0.630563497543335
I0301 06:08:00.138599 139753105983296 spec.py:321] Evaluating on the training split.
I0301 06:08:06.214070 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 06:08:14.461441 139753105983296 spec.py:349] Evaluating on the test split.
I0301 06:08:16.806241 139753105983296 submission_runner.py:411] Time since start: 89827.81s, 	Step: 255488, 	{'train/accuracy': 0.9581273794174194, 'train/loss': 0.15092483162879944, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0555461645126343, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.818804144859314, 'test/num_examples': 10000, 'score': 86763.52758550644, 'total_duration': 89827.80944180489, 'accumulated_submission_time': 86763.52758550644, 'accumulated_eval_time': 3045.324955224991, 'accumulated_logging_time': 9.99882435798645}
I0301 06:08:16.872406 139590153250560 logging_writer.py:48] [255488] accumulated_eval_time=3045.324955, accumulated_logging_time=9.998824, accumulated_submission_time=86763.527586, global_step=255488, preemption_count=0, score=86763.527586, test/accuracy=0.629700, test/loss=1.818804, test/num_examples=10000, total_duration=89827.809442, train/accuracy=0.958127, train/loss=0.150925, validation/accuracy=0.755020, validation/loss=1.055546, validation/num_examples=50000
I0301 06:08:21.273097 139590186821376 logging_writer.py:48] [255500] global_step=255500, grad_norm=4.265021324157715, loss=0.5441461801528931
I0301 06:08:55.200236 139590153250560 logging_writer.py:48] [255600] global_step=255600, grad_norm=5.216573715209961, loss=0.6274350881576538
I0301 06:09:29.195395 139590186821376 logging_writer.py:48] [255700] global_step=255700, grad_norm=4.437361717224121, loss=0.6150762438774109
I0301 06:10:03.162241 139590153250560 logging_writer.py:48] [255800] global_step=255800, grad_norm=4.659209251403809, loss=0.6386899948120117
I0301 06:10:37.115948 139590186821376 logging_writer.py:48] [255900] global_step=255900, grad_norm=4.3535308837890625, loss=0.6380308866500854
I0301 06:11:11.079034 139590153250560 logging_writer.py:48] [256000] global_step=256000, grad_norm=4.339451789855957, loss=0.58319091796875
I0301 06:11:45.033015 139590186821376 logging_writer.py:48] [256100] global_step=256100, grad_norm=4.508395195007324, loss=0.6526489853858948
I0301 06:12:18.968696 139590153250560 logging_writer.py:48] [256200] global_step=256200, grad_norm=4.534720420837402, loss=0.7163084149360657
I0301 06:12:52.929165 139590186821376 logging_writer.py:48] [256300] global_step=256300, grad_norm=4.6999897956848145, loss=0.7026475667953491
I0301 06:13:26.832728 139590153250560 logging_writer.py:48] [256400] global_step=256400, grad_norm=4.465214252471924, loss=0.6185998916625977
I0301 06:14:00.765719 139590186821376 logging_writer.py:48] [256500] global_step=256500, grad_norm=4.668034553527832, loss=0.6933616399765015
I0301 06:14:34.747014 139590153250560 logging_writer.py:48] [256600] global_step=256600, grad_norm=4.485945224761963, loss=0.6118371486663818
I0301 06:15:08.696630 139590186821376 logging_writer.py:48] [256700] global_step=256700, grad_norm=4.998796463012695, loss=0.6297663450241089
I0301 06:15:42.730478 139590153250560 logging_writer.py:48] [256800] global_step=256800, grad_norm=4.291040897369385, loss=0.6264665722846985
I0301 06:16:16.696318 139590186821376 logging_writer.py:48] [256900] global_step=256900, grad_norm=4.363977432250977, loss=0.5909799933433533
I0301 06:16:47.048888 139753105983296 spec.py:321] Evaluating on the training split.
I0301 06:16:53.138487 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 06:17:01.336465 139753105983296 spec.py:349] Evaluating on the test split.
I0301 06:17:03.685245 139753105983296 submission_runner.py:411] Time since start: 90354.69s, 	Step: 256991, 	{'train/accuracy': 0.9624322056770325, 'train/loss': 0.14165836572647095, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0557940006256104, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8188364505767822, 'test/num_examples': 10000, 'score': 87273.6397869587, 'total_duration': 90354.68846487999, 'accumulated_submission_time': 87273.6397869587, 'accumulated_eval_time': 3061.9612522125244, 'accumulated_logging_time': 10.076414108276367}
I0301 06:17:03.749647 139589725452032 logging_writer.py:48] [256991] accumulated_eval_time=3061.961252, accumulated_logging_time=10.076414, accumulated_submission_time=87273.639787, global_step=256991, preemption_count=0, score=87273.639787, test/accuracy=0.630600, test/loss=1.818836, test/num_examples=10000, total_duration=90354.688465, train/accuracy=0.962432, train/loss=0.141658, validation/accuracy=0.755020, validation/loss=1.055794, validation/num_examples=50000
I0301 06:17:07.146318 139590144857856 logging_writer.py:48] [257000] global_step=257000, grad_norm=4.296825408935547, loss=0.5866943597793579
I0301 06:17:41.049137 139589725452032 logging_writer.py:48] [257100] global_step=257100, grad_norm=4.483091831207275, loss=0.7001030445098877
I0301 06:18:14.977357 139590144857856 logging_writer.py:48] [257200] global_step=257200, grad_norm=4.396214962005615, loss=0.5420430302619934
I0301 06:18:48.898831 139589725452032 logging_writer.py:48] [257300] global_step=257300, grad_norm=4.635922431945801, loss=0.6437842845916748
I0301 06:19:22.871070 139590144857856 logging_writer.py:48] [257400] global_step=257400, grad_norm=4.369867324829102, loss=0.625603199005127
I0301 06:19:56.814762 139589725452032 logging_writer.py:48] [257500] global_step=257500, grad_norm=4.630429744720459, loss=0.6212368011474609
I0301 06:20:30.772103 139590144857856 logging_writer.py:48] [257600] global_step=257600, grad_norm=4.857839584350586, loss=0.6380627155303955
I0301 06:21:04.722018 139589725452032 logging_writer.py:48] [257700] global_step=257700, grad_norm=4.945950984954834, loss=0.6338290572166443
I0301 06:21:38.737778 139590144857856 logging_writer.py:48] [257800] global_step=257800, grad_norm=5.502039909362793, loss=0.6413244605064392
I0301 06:22:12.667377 139589725452032 logging_writer.py:48] [257900] global_step=257900, grad_norm=4.738562107086182, loss=0.645095944404602
I0301 06:22:46.593275 139590144857856 logging_writer.py:48] [258000] global_step=258000, grad_norm=4.381515026092529, loss=0.614460289478302
I0301 06:23:20.536352 139589725452032 logging_writer.py:48] [258100] global_step=258100, grad_norm=4.586317539215088, loss=0.6356962323188782
I0301 06:23:54.458838 139590144857856 logging_writer.py:48] [258200] global_step=258200, grad_norm=4.655429840087891, loss=0.6887485980987549
I0301 06:24:28.412794 139589725452032 logging_writer.py:48] [258300] global_step=258300, grad_norm=4.672913074493408, loss=0.5521803498268127
I0301 06:25:02.357911 139590144857856 logging_writer.py:48] [258400] global_step=258400, grad_norm=4.393299102783203, loss=0.5659414529800415
I0301 06:25:33.740135 139753105983296 spec.py:321] Evaluating on the training split.
I0301 06:25:39.791347 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 06:25:48.097563 139753105983296 spec.py:349] Evaluating on the test split.
I0301 06:25:50.377436 139753105983296 submission_runner.py:411] Time since start: 90881.38s, 	Step: 258494, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.14743536710739136, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0550732612609863, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8188350200653076, 'test/num_examples': 10000, 'score': 87783.56503915787, 'total_duration': 90881.38066411018, 'accumulated_submission_time': 87783.56503915787, 'accumulated_eval_time': 3078.59850025177, 'accumulated_logging_time': 10.152153968811035}
I0301 06:25:50.438001 139589717059328 logging_writer.py:48] [258494] accumulated_eval_time=3078.598500, accumulated_logging_time=10.152154, accumulated_submission_time=87783.565039, global_step=258494, preemption_count=0, score=87783.565039, test/accuracy=0.630000, test/loss=1.818835, test/num_examples=10000, total_duration=90881.380664, train/accuracy=0.960240, train/loss=0.147435, validation/accuracy=0.754740, validation/loss=1.055073, validation/num_examples=50000
I0301 06:25:52.840675 139590153250560 logging_writer.py:48] [258500] global_step=258500, grad_norm=4.2759599685668945, loss=0.6069576740264893
I0301 06:26:26.750528 139589717059328 logging_writer.py:48] [258600] global_step=258600, grad_norm=4.449985504150391, loss=0.6161192655563354
I0301 06:27:00.692759 139590153250560 logging_writer.py:48] [258700] global_step=258700, grad_norm=3.9297187328338623, loss=0.5152648091316223
I0301 06:27:34.577700 139589717059328 logging_writer.py:48] [258800] global_step=258800, grad_norm=4.710989952087402, loss=0.6740055084228516
I0301 06:28:08.610819 139590153250560 logging_writer.py:48] [258900] global_step=258900, grad_norm=4.674829006195068, loss=0.6169691681861877
I0301 06:28:42.544726 139589717059328 logging_writer.py:48] [259000] global_step=259000, grad_norm=4.597278594970703, loss=0.6002135872840881
I0301 06:29:16.473742 139590153250560 logging_writer.py:48] [259100] global_step=259100, grad_norm=4.34826135635376, loss=0.6343057751655579
I0301 06:29:50.435743 139589717059328 logging_writer.py:48] [259200] global_step=259200, grad_norm=4.585849285125732, loss=0.5882729291915894
I0301 06:30:24.377878 139590153250560 logging_writer.py:48] [259300] global_step=259300, grad_norm=4.326679229736328, loss=0.5873234272003174
I0301 06:30:58.337604 139589717059328 logging_writer.py:48] [259400] global_step=259400, grad_norm=4.4553608894348145, loss=0.5662224888801575
I0301 06:31:32.293275 139590153250560 logging_writer.py:48] [259500] global_step=259500, grad_norm=4.824383735656738, loss=0.6345751285552979
I0301 06:32:06.220434 139589717059328 logging_writer.py:48] [259600] global_step=259600, grad_norm=5.607529163360596, loss=0.6357538104057312
I0301 06:32:40.140269 139590153250560 logging_writer.py:48] [259700] global_step=259700, grad_norm=4.146114826202393, loss=0.5857607126235962
I0301 06:33:14.085390 139589717059328 logging_writer.py:48] [259800] global_step=259800, grad_norm=5.108639240264893, loss=0.6304928660392761
I0301 06:33:48.031676 139590153250560 logging_writer.py:48] [259900] global_step=259900, grad_norm=4.3010358810424805, loss=0.5676484107971191
I0301 06:34:20.502536 139753105983296 spec.py:321] Evaluating on the training split.
I0301 06:34:26.518494 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 06:34:34.802793 139753105983296 spec.py:349] Evaluating on the test split.
I0301 06:34:37.124380 139753105983296 submission_runner.py:411] Time since start: 91408.13s, 	Step: 259997, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14647991955280304, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0550395250320435, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8166371583938599, 'test/num_examples': 10000, 'score': 88293.56649446487, 'total_duration': 91408.12760949135, 'accumulated_submission_time': 88293.56649446487, 'accumulated_eval_time': 3095.220307826996, 'accumulated_logging_time': 10.222743272781372}
I0301 06:34:37.188504 139590161643264 logging_writer.py:48] [259997] accumulated_eval_time=3095.220308, accumulated_logging_time=10.222743, accumulated_submission_time=88293.566494, global_step=259997, preemption_count=0, score=88293.566494, test/accuracy=0.630900, test/loss=1.816637, test/num_examples=10000, total_duration=91408.127609, train/accuracy=0.961316, train/loss=0.146480, validation/accuracy=0.754940, validation/loss=1.055040, validation/num_examples=50000
I0301 06:34:38.554344 139590170035968 logging_writer.py:48] [260000] global_step=260000, grad_norm=4.781817436218262, loss=0.6726358532905579
I0301 06:35:12.440019 139590161643264 logging_writer.py:48] [260100] global_step=260100, grad_norm=4.111415863037109, loss=0.5978155732154846
I0301 06:35:46.328017 139590170035968 logging_writer.py:48] [260200] global_step=260200, grad_norm=4.144652366638184, loss=0.5855161547660828
I0301 06:36:20.258878 139590161643264 logging_writer.py:48] [260300] global_step=260300, grad_norm=4.425302505493164, loss=0.618733286857605
I0301 06:36:54.211672 139590170035968 logging_writer.py:48] [260400] global_step=260400, grad_norm=4.642807960510254, loss=0.6154227256774902
I0301 06:37:28.162234 139590161643264 logging_writer.py:48] [260500] global_step=260500, grad_norm=4.781333923339844, loss=0.6671370267868042
I0301 06:38:02.065649 139590170035968 logging_writer.py:48] [260600] global_step=260600, grad_norm=4.957956790924072, loss=0.6626242995262146
I0301 06:38:35.987956 139590161643264 logging_writer.py:48] [260700] global_step=260700, grad_norm=4.460141181945801, loss=0.6004658341407776
I0301 06:39:09.920733 139590170035968 logging_writer.py:48] [260800] global_step=260800, grad_norm=4.736580848693848, loss=0.5770400166511536
I0301 06:39:43.900449 139590161643264 logging_writer.py:48] [260900] global_step=260900, grad_norm=4.508200168609619, loss=0.5875137448310852
I0301 06:40:17.994250 139590170035968 logging_writer.py:48] [261000] global_step=261000, grad_norm=4.391471862792969, loss=0.59209144115448
I0301 06:40:51.941688 139590161643264 logging_writer.py:48] [261100] global_step=261100, grad_norm=4.132915496826172, loss=0.5797938108444214
I0301 06:41:25.914685 139590170035968 logging_writer.py:48] [261200] global_step=261200, grad_norm=4.643073558807373, loss=0.6145614385604858
I0301 06:41:59.863514 139590161643264 logging_writer.py:48] [261300] global_step=261300, grad_norm=4.45202112197876, loss=0.6669092774391174
I0301 06:42:33.802906 139590170035968 logging_writer.py:48] [261400] global_step=261400, grad_norm=4.8207316398620605, loss=0.6738735437393188
I0301 06:43:07.216046 139753105983296 spec.py:321] Evaluating on the training split.
I0301 06:43:13.230901 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 06:43:21.605802 139753105983296 spec.py:349] Evaluating on the test split.
I0301 06:43:23.889019 139753105983296 submission_runner.py:411] Time since start: 91934.89s, 	Step: 261500, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.1482132524251938, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.0562045574188232, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8203959465026855, 'test/num_examples': 10000, 'score': 88803.53046774864, 'total_duration': 91934.89223361015, 'accumulated_submission_time': 88803.53046774864, 'accumulated_eval_time': 3111.8932209014893, 'accumulated_logging_time': 10.29693603515625}
I0301 06:43:23.955236 139589708666624 logging_writer.py:48] [261500] accumulated_eval_time=3111.893221, accumulated_logging_time=10.296936, accumulated_submission_time=88803.530468, global_step=261500, preemption_count=0, score=88803.530468, test/accuracy=0.630700, test/loss=1.820396, test/num_examples=10000, total_duration=91934.892234, train/accuracy=0.960140, train/loss=0.148213, validation/accuracy=0.754520, validation/loss=1.056205, validation/num_examples=50000
I0301 06:43:24.305269 139589717059328 logging_writer.py:48] [261500] global_step=261500, grad_norm=4.489765167236328, loss=0.5456593036651611
I0301 06:43:58.253359 139589708666624 logging_writer.py:48] [261600] global_step=261600, grad_norm=4.120705604553223, loss=0.6532004475593567
I0301 06:44:32.190665 139589717059328 logging_writer.py:48] [261700] global_step=261700, grad_norm=3.9943904876708984, loss=0.6005455851554871
I0301 06:45:06.120572 139589708666624 logging_writer.py:48] [261800] global_step=261800, grad_norm=4.21640682220459, loss=0.632564127445221
I0301 06:45:40.059978 139589717059328 logging_writer.py:48] [261900] global_step=261900, grad_norm=4.610467910766602, loss=0.5905355215072632
I0301 06:46:14.166973 139589708666624 logging_writer.py:48] [262000] global_step=262000, grad_norm=4.457852363586426, loss=0.5322637557983398
I0301 06:46:48.146022 139589717059328 logging_writer.py:48] [262100] global_step=262100, grad_norm=4.669999122619629, loss=0.6340690851211548
I0301 06:47:22.116484 139589708666624 logging_writer.py:48] [262200] global_step=262200, grad_norm=4.252845287322998, loss=0.5629566311836243
I0301 06:47:56.059525 139589717059328 logging_writer.py:48] [262300] global_step=262300, grad_norm=4.642935752868652, loss=0.595001220703125
I0301 06:48:30.011679 139589708666624 logging_writer.py:48] [262400] global_step=262400, grad_norm=4.788388729095459, loss=0.6367482542991638
I0301 06:49:03.981065 139589717059328 logging_writer.py:48] [262500] global_step=262500, grad_norm=4.68860387802124, loss=0.6966089606285095
I0301 06:49:37.944662 139589708666624 logging_writer.py:48] [262600] global_step=262600, grad_norm=4.354910850524902, loss=0.6222792863845825
I0301 06:50:11.913994 139589717059328 logging_writer.py:48] [262700] global_step=262700, grad_norm=4.7148356437683105, loss=0.6580938100814819
I0301 06:50:45.821339 139589708666624 logging_writer.py:48] [262800] global_step=262800, grad_norm=4.455192565917969, loss=0.5875049233436584
I0301 06:51:19.780971 139589717059328 logging_writer.py:48] [262900] global_step=262900, grad_norm=4.905035018920898, loss=0.6193621158599854
I0301 06:51:53.747081 139589708666624 logging_writer.py:48] [263000] global_step=263000, grad_norm=4.389554977416992, loss=0.632337749004364
I0301 06:51:53.910299 139753105983296 spec.py:321] Evaluating on the training split.
I0301 06:51:59.955316 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 06:52:08.412552 139753105983296 spec.py:349] Evaluating on the test split.
I0301 06:52:10.683866 139753105983296 submission_runner.py:411] Time since start: 92461.69s, 	Step: 263002, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14088335633277893, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0564838647842407, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8195748329162598, 'test/num_examples': 10000, 'score': 89313.42373251915, 'total_duration': 92461.68709921837, 'accumulated_submission_time': 89313.42373251915, 'accumulated_eval_time': 3128.666751384735, 'accumulated_logging_time': 10.373113632202148}
I0301 06:52:10.745588 139590170035968 logging_writer.py:48] [263002] accumulated_eval_time=3128.666751, accumulated_logging_time=10.373114, accumulated_submission_time=89313.423733, global_step=263002, preemption_count=0, score=89313.423733, test/accuracy=0.630600, test/loss=1.819575, test/num_examples=10000, total_duration=92461.687099, train/accuracy=0.961515, train/loss=0.140883, validation/accuracy=0.755060, validation/loss=1.056484, validation/num_examples=50000
I0301 06:52:44.374940 139590178428672 logging_writer.py:48] [263100] global_step=263100, grad_norm=5.2717976570129395, loss=0.6317700147628784
I0301 06:53:18.290623 139590170035968 logging_writer.py:48] [263200] global_step=263200, grad_norm=4.462082386016846, loss=0.6500347852706909
I0301 06:53:52.205702 139590178428672 logging_writer.py:48] [263300] global_step=263300, grad_norm=4.519573211669922, loss=0.6514629125595093
I0301 06:54:26.138107 139590170035968 logging_writer.py:48] [263400] global_step=263400, grad_norm=5.087186336517334, loss=0.7562637329101562
I0301 06:55:00.057898 139590178428672 logging_writer.py:48] [263500] global_step=263500, grad_norm=4.76984167098999, loss=0.7120477557182312
I0301 06:55:33.991285 139590170035968 logging_writer.py:48] [263600] global_step=263600, grad_norm=4.86726188659668, loss=0.6957653760910034
I0301 06:56:07.922511 139590178428672 logging_writer.py:48] [263700] global_step=263700, grad_norm=4.221425533294678, loss=0.5665501356124878
I0301 06:56:41.876507 139590170035968 logging_writer.py:48] [263800] global_step=263800, grad_norm=4.66339635848999, loss=0.6600037217140198
I0301 06:57:15.837039 139590178428672 logging_writer.py:48] [263900] global_step=263900, grad_norm=4.386867523193359, loss=0.6293960809707642
I0301 06:57:49.771314 139590170035968 logging_writer.py:48] [264000] global_step=264000, grad_norm=4.408480644226074, loss=0.6360280513763428
I0301 06:58:23.694492 139590178428672 logging_writer.py:48] [264100] global_step=264100, grad_norm=5.024837017059326, loss=0.6176484227180481
I0301 06:58:57.714462 139590170035968 logging_writer.py:48] [264200] global_step=264200, grad_norm=4.486323356628418, loss=0.6367357969284058
I0301 06:59:31.645899 139590178428672 logging_writer.py:48] [264300] global_step=264300, grad_norm=4.888790607452393, loss=0.5811771154403687
I0301 07:00:05.581970 139590170035968 logging_writer.py:48] [264400] global_step=264400, grad_norm=4.551436424255371, loss=0.585338294506073
I0301 07:00:39.518590 139590178428672 logging_writer.py:48] [264500] global_step=264500, grad_norm=4.575132846832275, loss=0.6710574626922607
I0301 07:00:40.684266 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:00:46.713692 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:00:55.045753 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:00:57.352079 139753105983296 submission_runner.py:411] Time since start: 92988.36s, 	Step: 264505, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14603760838508606, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.054930329322815, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.818839192390442, 'test/num_examples': 10000, 'score': 89823.2990591526, 'total_duration': 92988.3552968502, 'accumulated_submission_time': 89823.2990591526, 'accumulated_eval_time': 3145.3344972133636, 'accumulated_logging_time': 10.445182085037231}
I0301 07:00:57.421482 139590153250560 logging_writer.py:48] [264505] accumulated_eval_time=3145.334497, accumulated_logging_time=10.445182, accumulated_submission_time=89823.299059, global_step=264505, preemption_count=0, score=89823.299059, test/accuracy=0.629900, test/loss=1.818839, test/num_examples=10000, total_duration=92988.355297, train/accuracy=0.961117, train/loss=0.146038, validation/accuracy=0.754880, validation/loss=1.054930, validation/num_examples=50000
I0301 07:01:29.933899 139590161643264 logging_writer.py:48] [264600] global_step=264600, grad_norm=4.773852825164795, loss=0.6287997961044312
I0301 07:02:03.862230 139590153250560 logging_writer.py:48] [264700] global_step=264700, grad_norm=4.592093467712402, loss=0.6044291853904724
I0301 07:02:37.784116 139590161643264 logging_writer.py:48] [264800] global_step=264800, grad_norm=4.714787006378174, loss=0.6537933349609375
I0301 07:03:11.764626 139590153250560 logging_writer.py:48] [264900] global_step=264900, grad_norm=4.466465473175049, loss=0.6088728308677673
I0301 07:03:45.707790 139590161643264 logging_writer.py:48] [265000] global_step=265000, grad_norm=4.532871246337891, loss=0.6138262152671814
I0301 07:04:19.666416 139590153250560 logging_writer.py:48] [265100] global_step=265100, grad_norm=4.608161449432373, loss=0.5377252101898193
I0301 07:04:53.723374 139590161643264 logging_writer.py:48] [265200] global_step=265200, grad_norm=4.220465660095215, loss=0.5665263533592224
I0301 07:05:27.680512 139590153250560 logging_writer.py:48] [265300] global_step=265300, grad_norm=4.261970520019531, loss=0.5881665945053101
I0301 07:06:01.661702 139590161643264 logging_writer.py:48] [265400] global_step=265400, grad_norm=4.793124675750732, loss=0.6404138207435608
I0301 07:06:35.621685 139590153250560 logging_writer.py:48] [265500] global_step=265500, grad_norm=4.602265357971191, loss=0.6394851207733154
I0301 07:07:09.563585 139590161643264 logging_writer.py:48] [265600] global_step=265600, grad_norm=4.6882405281066895, loss=0.6327234506607056
I0301 07:07:43.528882 139590153250560 logging_writer.py:48] [265700] global_step=265700, grad_norm=4.294091701507568, loss=0.5667135715484619
I0301 07:08:17.473709 139590161643264 logging_writer.py:48] [265800] global_step=265800, grad_norm=4.554755687713623, loss=0.6040341258049011
I0301 07:08:51.438275 139590153250560 logging_writer.py:48] [265900] global_step=265900, grad_norm=4.453344345092773, loss=0.6565029621124268
I0301 07:09:25.403127 139590161643264 logging_writer.py:48] [266000] global_step=266000, grad_norm=4.655697345733643, loss=0.6676836013793945
I0301 07:09:27.590436 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:09:33.629329 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:09:42.003274 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:09:44.543516 139753105983296 submission_runner.py:411] Time since start: 93515.55s, 	Step: 266008, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.1456584334373474, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0558500289916992, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.820417881011963, 'test/num_examples': 10000, 'score': 90333.40451908112, 'total_duration': 93515.54674649239, 'accumulated_submission_time': 90333.40451908112, 'accumulated_eval_time': 3162.2875208854675, 'accumulated_logging_time': 10.524413585662842}
I0301 07:09:44.594057 139589725452032 logging_writer.py:48] [266008] accumulated_eval_time=3162.287521, accumulated_logging_time=10.524414, accumulated_submission_time=90333.404519, global_step=266008, preemption_count=0, score=90333.404519, test/accuracy=0.630000, test/loss=1.820418, test/num_examples=10000, total_duration=93515.546746, train/accuracy=0.961376, train/loss=0.145658, validation/accuracy=0.754800, validation/loss=1.055850, validation/num_examples=50000
I0301 07:10:16.137588 139590144857856 logging_writer.py:48] [266100] global_step=266100, grad_norm=4.922719478607178, loss=0.7523910999298096
I0301 07:10:50.020860 139589725452032 logging_writer.py:48] [266200] global_step=266200, grad_norm=4.533259868621826, loss=0.5764684081077576
I0301 07:11:24.054053 139590144857856 logging_writer.py:48] [266300] global_step=266300, grad_norm=4.688419818878174, loss=0.6643224954605103
I0301 07:11:57.971834 139589725452032 logging_writer.py:48] [266400] global_step=266400, grad_norm=4.4188032150268555, loss=0.5939798355102539
I0301 07:12:31.917574 139590144857856 logging_writer.py:48] [266500] global_step=266500, grad_norm=4.188050270080566, loss=0.6529900431632996
I0301 07:13:05.883276 139589725452032 logging_writer.py:48] [266600] global_step=266600, grad_norm=4.825826168060303, loss=0.6686299443244934
I0301 07:13:39.826923 139590144857856 logging_writer.py:48] [266700] global_step=266700, grad_norm=4.541390419006348, loss=0.6635611653327942
I0301 07:14:13.784164 139589725452032 logging_writer.py:48] [266800] global_step=266800, grad_norm=4.412820816040039, loss=0.654340386390686
I0301 07:14:47.746905 139590144857856 logging_writer.py:48] [266900] global_step=266900, grad_norm=4.803372859954834, loss=0.696397066116333
I0301 07:15:21.641684 139589725452032 logging_writer.py:48] [267000] global_step=267000, grad_norm=4.888148307800293, loss=0.6674640774726868
I0301 07:15:55.600868 139590144857856 logging_writer.py:48] [267100] global_step=267100, grad_norm=4.590930461883545, loss=0.6154903173446655
I0301 07:16:29.598276 139589725452032 logging_writer.py:48] [267200] global_step=267200, grad_norm=4.580894470214844, loss=0.6314786076545715
I0301 07:17:03.548081 139590144857856 logging_writer.py:48] [267300] global_step=267300, grad_norm=4.720229625701904, loss=0.7782131433486938
I0301 07:17:37.651034 139589725452032 logging_writer.py:48] [267400] global_step=267400, grad_norm=4.332906246185303, loss=0.5703965425491333
I0301 07:18:11.582007 139590144857856 logging_writer.py:48] [267500] global_step=267500, grad_norm=4.298264980316162, loss=0.6422408819198608
I0301 07:18:14.792090 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:18:21.526871 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:18:29.846081 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:18:32.146335 139753105983296 submission_runner.py:411] Time since start: 94043.15s, 	Step: 267511, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.14656920731067657, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0563976764678955, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8201713562011719, 'test/num_examples': 10000, 'score': 90843.54014015198, 'total_duration': 94043.14956951141, 'accumulated_submission_time': 90843.54014015198, 'accumulated_eval_time': 3179.641728401184, 'accumulated_logging_time': 10.585381507873535}
I0301 07:18:32.212122 139590161643264 logging_writer.py:48] [267511] accumulated_eval_time=3179.641728, accumulated_logging_time=10.585382, accumulated_submission_time=90843.540140, global_step=267511, preemption_count=0, score=90843.540140, test/accuracy=0.630300, test/loss=1.820171, test/num_examples=10000, total_duration=94043.149570, train/accuracy=0.960240, train/loss=0.146569, validation/accuracy=0.754720, validation/loss=1.056398, validation/num_examples=50000
I0301 07:19:02.746675 139590170035968 logging_writer.py:48] [267600] global_step=267600, grad_norm=5.042192459106445, loss=0.7018424272537231
I0301 07:19:36.657516 139590161643264 logging_writer.py:48] [267700] global_step=267700, grad_norm=4.61834716796875, loss=0.626529335975647
I0301 07:20:10.601765 139590170035968 logging_writer.py:48] [267800] global_step=267800, grad_norm=4.658086776733398, loss=0.6485108733177185
I0301 07:20:44.586837 139590161643264 logging_writer.py:48] [267900] global_step=267900, grad_norm=4.316448211669922, loss=0.5762508511543274
I0301 07:21:18.531536 139590170035968 logging_writer.py:48] [268000] global_step=268000, grad_norm=4.903774738311768, loss=0.6492294669151306
I0301 07:21:52.520478 139590161643264 logging_writer.py:48] [268100] global_step=268100, grad_norm=4.542123317718506, loss=0.6569809913635254
I0301 07:22:26.466051 139590170035968 logging_writer.py:48] [268200] global_step=268200, grad_norm=4.477989673614502, loss=0.6814383268356323
I0301 07:23:00.448154 139590161643264 logging_writer.py:48] [268300] global_step=268300, grad_norm=4.727153778076172, loss=0.6853344440460205
I0301 07:23:34.514906 139590170035968 logging_writer.py:48] [268400] global_step=268400, grad_norm=4.702011585235596, loss=0.6168736815452576
I0301 07:24:08.460573 139590161643264 logging_writer.py:48] [268500] global_step=268500, grad_norm=4.4387125968933105, loss=0.6461179852485657
I0301 07:24:42.424802 139590170035968 logging_writer.py:48] [268600] global_step=268600, grad_norm=4.402172088623047, loss=0.5498827695846558
I0301 07:25:16.381123 139590161643264 logging_writer.py:48] [268700] global_step=268700, grad_norm=4.983817100524902, loss=0.6498037576675415
I0301 07:25:50.329902 139590170035968 logging_writer.py:48] [268800] global_step=268800, grad_norm=4.353926181793213, loss=0.5506218075752258
I0301 07:26:24.301280 139590161643264 logging_writer.py:48] [268900] global_step=268900, grad_norm=4.348499298095703, loss=0.5900212526321411
I0301 07:26:58.262953 139590170035968 logging_writer.py:48] [269000] global_step=269000, grad_norm=4.513923645019531, loss=0.6545752286911011
I0301 07:27:02.472995 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:27:08.516854 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:27:16.811748 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:27:19.114374 139753105983296 submission_runner.py:411] Time since start: 94570.12s, 	Step: 269014, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.14800925552845, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.0555812120437622, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8191250562667847, 'test/num_examples': 10000, 'score': 91353.7362806797, 'total_duration': 94570.11760783195, 'accumulated_submission_time': 91353.7362806797, 'accumulated_eval_time': 3196.283055782318, 'accumulated_logging_time': 10.662968635559082}
I0301 07:27:19.181639 139589725452032 logging_writer.py:48] [269014] accumulated_eval_time=3196.283056, accumulated_logging_time=10.662969, accumulated_submission_time=91353.736281, global_step=269014, preemption_count=0, score=91353.736281, test/accuracy=0.630200, test/loss=1.819125, test/num_examples=10000, total_duration=94570.117608, train/accuracy=0.960081, train/loss=0.148009, validation/accuracy=0.754640, validation/loss=1.055581, validation/num_examples=50000
I0301 07:27:48.702451 139590144857856 logging_writer.py:48] [269100] global_step=269100, grad_norm=4.273741245269775, loss=0.6043133735656738
I0301 07:28:22.597187 139589725452032 logging_writer.py:48] [269200] global_step=269200, grad_norm=4.338586807250977, loss=0.6708480715751648
I0301 07:28:56.534116 139590144857856 logging_writer.py:48] [269300] global_step=269300, grad_norm=4.685598850250244, loss=0.6363697648048401
I0301 07:29:30.598264 139589725452032 logging_writer.py:48] [269400] global_step=269400, grad_norm=4.704870700836182, loss=0.5863942503929138
I0301 07:30:04.549371 139590144857856 logging_writer.py:48] [269500] global_step=269500, grad_norm=4.374706268310547, loss=0.6350962519645691
I0301 07:30:38.478652 139589725452032 logging_writer.py:48] [269600] global_step=269600, grad_norm=4.421262741088867, loss=0.562980055809021
I0301 07:31:12.425204 139590144857856 logging_writer.py:48] [269700] global_step=269700, grad_norm=4.500646114349365, loss=0.6475111246109009
I0301 07:31:46.330291 139589725452032 logging_writer.py:48] [269800] global_step=269800, grad_norm=4.8884453773498535, loss=0.5616782903671265
I0301 07:32:20.303327 139590144857856 logging_writer.py:48] [269900] global_step=269900, grad_norm=4.559682846069336, loss=0.6239814758300781
I0301 07:32:54.225553 139589725452032 logging_writer.py:48] [270000] global_step=270000, grad_norm=4.549536228179932, loss=0.6409617066383362
I0301 07:33:28.190136 139590144857856 logging_writer.py:48] [270100] global_step=270100, grad_norm=5.058125972747803, loss=0.6816349029541016
I0301 07:34:02.163805 139589725452032 logging_writer.py:48] [270200] global_step=270200, grad_norm=4.5805277824401855, loss=0.628136932849884
I0301 07:34:36.113810 139590144857856 logging_writer.py:48] [270300] global_step=270300, grad_norm=4.685869216918945, loss=0.6003302335739136
I0301 07:35:10.094979 139589725452032 logging_writer.py:48] [270400] global_step=270400, grad_norm=4.433741092681885, loss=0.606850266456604
I0301 07:35:44.112379 139590144857856 logging_writer.py:48] [270500] global_step=270500, grad_norm=4.423679828643799, loss=0.6160404682159424
I0301 07:35:49.352076 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:35:55.359829 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:36:03.688243 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:36:05.946924 139753105983296 submission_runner.py:411] Time since start: 95096.95s, 	Step: 270517, 	{'train/accuracy': 0.9622727632522583, 'train/loss': 0.14243380725383759, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.0561974048614502, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8198072910308838, 'test/num_examples': 10000, 'score': 91863.84095406532, 'total_duration': 95096.95014238358, 'accumulated_submission_time': 91863.84095406532, 'accumulated_eval_time': 3212.8778393268585, 'accumulated_logging_time': 10.743399143218994}
I0301 07:36:06.009534 139590161643264 logging_writer.py:48] [270517] accumulated_eval_time=3212.877839, accumulated_logging_time=10.743399, accumulated_submission_time=91863.840954, global_step=270517, preemption_count=0, score=91863.840954, test/accuracy=0.631300, test/loss=1.819807, test/num_examples=10000, total_duration=95096.950142, train/accuracy=0.962273, train/loss=0.142434, validation/accuracy=0.754520, validation/loss=1.056197, validation/num_examples=50000
I0301 07:36:34.495239 139590170035968 logging_writer.py:48] [270600] global_step=270600, grad_norm=4.746498107910156, loss=0.6415423154830933
I0301 07:37:08.445395 139590161643264 logging_writer.py:48] [270700] global_step=270700, grad_norm=5.120275020599365, loss=0.644305944442749
I0301 07:37:42.344440 139590170035968 logging_writer.py:48] [270800] global_step=270800, grad_norm=4.5732526779174805, loss=0.6202967166900635
I0301 07:38:16.287566 139590161643264 logging_writer.py:48] [270900] global_step=270900, grad_norm=4.558112144470215, loss=0.6814424991607666
I0301 07:38:50.275517 139590170035968 logging_writer.py:48] [271000] global_step=271000, grad_norm=4.58964204788208, loss=0.6023821830749512
I0301 07:39:24.242841 139590161643264 logging_writer.py:48] [271100] global_step=271100, grad_norm=4.745219707489014, loss=0.6901277899742126
I0301 07:39:58.221767 139590170035968 logging_writer.py:48] [271200] global_step=271200, grad_norm=4.759986877441406, loss=0.6156458854675293
I0301 07:40:32.193340 139590161643264 logging_writer.py:48] [271300] global_step=271300, grad_norm=4.817010879516602, loss=0.7097685933113098
I0301 07:41:06.164184 139590170035968 logging_writer.py:48] [271400] global_step=271400, grad_norm=4.539449691772461, loss=0.6268447637557983
I0301 07:41:40.124489 139590161643264 logging_writer.py:48] [271500] global_step=271500, grad_norm=4.927643299102783, loss=0.6495751738548279
I0301 07:42:14.149100 139590170035968 logging_writer.py:48] [271600] global_step=271600, grad_norm=4.642505168914795, loss=0.5942661166191101
I0301 07:42:48.110650 139590161643264 logging_writer.py:48] [271700] global_step=271700, grad_norm=4.447492599487305, loss=0.659925103187561
I0301 07:43:22.090888 139590170035968 logging_writer.py:48] [271800] global_step=271800, grad_norm=4.778495788574219, loss=0.592881977558136
I0301 07:43:56.029335 139590161643264 logging_writer.py:48] [271900] global_step=271900, grad_norm=4.617981433868408, loss=0.6606292724609375
I0301 07:44:30.006223 139590170035968 logging_writer.py:48] [272000] global_step=272000, grad_norm=4.283930778503418, loss=0.5854684114456177
I0301 07:44:36.265018 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:44:42.261137 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:44:50.643503 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:44:52.924551 139753105983296 submission_runner.py:411] Time since start: 95623.93s, 	Step: 272020, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.1462656706571579, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.0556144714355469, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.819312572479248, 'test/num_examples': 10000, 'score': 92374.03491449356, 'total_duration': 95623.9277703762, 'accumulated_submission_time': 92374.03491449356, 'accumulated_eval_time': 3229.5373075008392, 'accumulated_logging_time': 10.815797090530396}
I0301 07:44:52.989274 139589717059328 logging_writer.py:48] [272020] accumulated_eval_time=3229.537308, accumulated_logging_time=10.815797, accumulated_submission_time=92374.034914, global_step=272020, preemption_count=0, score=92374.034914, test/accuracy=0.630200, test/loss=1.819313, test/num_examples=10000, total_duration=95623.927770, train/accuracy=0.960639, train/loss=0.146266, validation/accuracy=0.754640, validation/loss=1.055614, validation/num_examples=50000
I0301 07:45:20.495528 139589725452032 logging_writer.py:48] [272100] global_step=272100, grad_norm=4.861930847167969, loss=0.589152455329895
I0301 07:45:54.434201 139589717059328 logging_writer.py:48] [272200] global_step=272200, grad_norm=4.794425010681152, loss=0.6478132009506226
I0301 07:46:28.372352 139589725452032 logging_writer.py:48] [272300] global_step=272300, grad_norm=4.861374855041504, loss=0.7013952732086182
I0301 07:47:02.303700 139589717059328 logging_writer.py:48] [272400] global_step=272400, grad_norm=4.630774021148682, loss=0.6170637607574463
I0301 07:47:36.251140 139589725452032 logging_writer.py:48] [272500] global_step=272500, grad_norm=4.691495418548584, loss=0.6002885103225708
I0301 07:48:10.284561 139589717059328 logging_writer.py:48] [272600] global_step=272600, grad_norm=4.581877708435059, loss=0.6485482454299927
I0301 07:48:44.234876 139589725452032 logging_writer.py:48] [272700] global_step=272700, grad_norm=5.446156024932861, loss=0.5994027853012085
I0301 07:49:18.187871 139589717059328 logging_writer.py:48] [272800] global_step=272800, grad_norm=4.771671295166016, loss=0.6649444103240967
I0301 07:49:52.122570 139589725452032 logging_writer.py:48] [272900] global_step=272900, grad_norm=5.28853702545166, loss=0.6782928705215454
I0301 07:50:26.067181 139589717059328 logging_writer.py:48] [273000] global_step=273000, grad_norm=4.983479022979736, loss=0.7333223819732666
I0301 07:51:00.034517 139589725452032 logging_writer.py:48] [273100] global_step=273100, grad_norm=4.473094940185547, loss=0.6301888823509216
I0301 07:51:33.960731 139589717059328 logging_writer.py:48] [273200] global_step=273200, grad_norm=4.609169006347656, loss=0.6630771160125732
I0301 07:52:07.926327 139589725452032 logging_writer.py:48] [273300] global_step=273300, grad_norm=4.681831359863281, loss=0.6330084800720215
I0301 07:52:41.892319 139589717059328 logging_writer.py:48] [273400] global_step=273400, grad_norm=4.380355358123779, loss=0.6566515564918518
I0301 07:53:15.867541 139589725452032 logging_writer.py:48] [273500] global_step=273500, grad_norm=4.869090557098389, loss=0.6458621621131897
I0301 07:53:23.142319 139753105983296 spec.py:321] Evaluating on the training split.
I0301 07:53:29.182755 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 07:53:37.476592 139753105983296 spec.py:349] Evaluating on the test split.
I0301 07:53:39.771503 139753105983296 submission_runner.py:411] Time since start: 96150.77s, 	Step: 273523, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14522315561771393, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0557005405426025, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.820548176765442, 'test/num_examples': 10000, 'score': 92884.12191414833, 'total_duration': 96150.77471852303, 'accumulated_submission_time': 92884.12191414833, 'accumulated_eval_time': 3246.1664230823517, 'accumulated_logging_time': 10.89329195022583}
I0301 07:53:39.843074 139590161643264 logging_writer.py:48] [273523] accumulated_eval_time=3246.166423, accumulated_logging_time=10.893292, accumulated_submission_time=92884.121914, global_step=273523, preemption_count=0, score=92884.121914, test/accuracy=0.630600, test/loss=1.820548, test/num_examples=10000, total_duration=96150.774719, train/accuracy=0.961256, train/loss=0.145223, validation/accuracy=0.755040, validation/loss=1.055701, validation/num_examples=50000
I0301 07:54:06.400753 139590170035968 logging_writer.py:48] [273600] global_step=273600, grad_norm=4.338964939117432, loss=0.6039186120033264
I0301 07:54:40.319855 139590161643264 logging_writer.py:48] [273700] global_step=273700, grad_norm=4.470088481903076, loss=0.6493445634841919
I0301 07:55:14.270216 139590170035968 logging_writer.py:48] [273800] global_step=273800, grad_norm=4.329052925109863, loss=0.5886374115943909
I0301 07:55:48.237904 139590161643264 logging_writer.py:48] [273900] global_step=273900, grad_norm=4.574874401092529, loss=0.6255877017974854
I0301 07:56:22.212800 139590170035968 logging_writer.py:48] [274000] global_step=274000, grad_norm=4.937193393707275, loss=0.6453034281730652
I0301 07:56:56.202782 139590161643264 logging_writer.py:48] [274100] global_step=274100, grad_norm=4.462228775024414, loss=0.5919672846794128
I0301 07:57:30.139865 139590170035968 logging_writer.py:48] [274200] global_step=274200, grad_norm=4.4399237632751465, loss=0.6495559215545654
I0301 07:58:04.122021 139590161643264 logging_writer.py:48] [274300] global_step=274300, grad_norm=4.636017799377441, loss=0.6747283935546875
I0301 07:58:38.073761 139590170035968 logging_writer.py:48] [274400] global_step=274400, grad_norm=4.644835948944092, loss=0.5876849889755249
I0301 07:59:12.033345 139590161643264 logging_writer.py:48] [274500] global_step=274500, grad_norm=4.751684188842773, loss=0.6745966672897339
I0301 07:59:45.995661 139590170035968 logging_writer.py:48] [274600] global_step=274600, grad_norm=4.532438278198242, loss=0.6415214538574219
I0301 08:00:20.040522 139590161643264 logging_writer.py:48] [274700] global_step=274700, grad_norm=4.753876209259033, loss=0.5998985767364502
I0301 08:00:53.984617 139590170035968 logging_writer.py:48] [274800] global_step=274800, grad_norm=4.352747440338135, loss=0.6256710290908813
I0301 08:01:27.950945 139590161643264 logging_writer.py:48] [274900] global_step=274900, grad_norm=4.71307373046875, loss=0.574821412563324
I0301 08:02:01.877073 139590170035968 logging_writer.py:48] [275000] global_step=275000, grad_norm=4.6525044441223145, loss=0.6648988127708435
I0301 08:02:09.841866 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:02:15.878142 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:02:24.173553 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:02:26.454244 139753105983296 submission_runner.py:411] Time since start: 96677.46s, 	Step: 275025, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14588487148284912, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.056044578552246, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8186932802200317, 'test/num_examples': 10000, 'score': 93394.05655407906, 'total_duration': 96677.45746946335, 'accumulated_submission_time': 93394.05655407906, 'accumulated_eval_time': 3262.778742313385, 'accumulated_logging_time': 10.976376295089722}
I0301 08:02:26.520025 139589708666624 logging_writer.py:48] [275025] accumulated_eval_time=3262.778742, accumulated_logging_time=10.976376, accumulated_submission_time=93394.056554, global_step=275025, preemption_count=0, score=93394.056554, test/accuracy=0.630400, test/loss=1.818693, test/num_examples=10000, total_duration=96677.457469, train/accuracy=0.961057, train/loss=0.145885, validation/accuracy=0.754720, validation/loss=1.056045, validation/num_examples=50000
I0301 08:02:52.291781 139589717059328 logging_writer.py:48] [275100] global_step=275100, grad_norm=4.594578742980957, loss=0.5594378113746643
I0301 08:03:26.249520 139589708666624 logging_writer.py:48] [275200] global_step=275200, grad_norm=4.463320255279541, loss=0.6707655787467957
I0301 08:04:00.222533 139589717059328 logging_writer.py:48] [275300] global_step=275300, grad_norm=4.667099475860596, loss=0.5949264764785767
I0301 08:04:34.186394 139589708666624 logging_writer.py:48] [275400] global_step=275400, grad_norm=4.820570945739746, loss=0.726360559463501
I0301 08:05:08.132399 139589717059328 logging_writer.py:48] [275500] global_step=275500, grad_norm=4.764895915985107, loss=0.706963300704956
I0301 08:05:42.068547 139589708666624 logging_writer.py:48] [275600] global_step=275600, grad_norm=4.60247278213501, loss=0.6855822801589966
I0301 08:06:16.117208 139589717059328 logging_writer.py:48] [275700] global_step=275700, grad_norm=4.611275672912598, loss=0.6984716653823853
I0301 08:06:50.077690 139589708666624 logging_writer.py:48] [275800] global_step=275800, grad_norm=4.024224281311035, loss=0.5273171067237854
I0301 08:07:24.041681 139589717059328 logging_writer.py:48] [275900] global_step=275900, grad_norm=5.529275417327881, loss=0.6427468061447144
I0301 08:07:57.976642 139589708666624 logging_writer.py:48] [276000] global_step=276000, grad_norm=4.93137788772583, loss=0.5821325182914734
I0301 08:08:31.941277 139589717059328 logging_writer.py:48] [276100] global_step=276100, grad_norm=4.698892593383789, loss=0.5684554576873779
I0301 08:09:05.880665 139589708666624 logging_writer.py:48] [276200] global_step=276200, grad_norm=4.545559406280518, loss=0.6318273544311523
I0301 08:09:39.840514 139589717059328 logging_writer.py:48] [276300] global_step=276300, grad_norm=4.659305095672607, loss=0.6157177090644836
I0301 08:10:13.774964 139589708666624 logging_writer.py:48] [276400] global_step=276400, grad_norm=4.9817214012146, loss=0.7271920442581177
I0301 08:10:47.717725 139589717059328 logging_writer.py:48] [276500] global_step=276500, grad_norm=4.510344982147217, loss=0.6445131897926331
I0301 08:10:56.683514 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:11:02.717838 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:11:11.050015 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:11:13.393560 139753105983296 submission_runner.py:411] Time since start: 97204.40s, 	Step: 276528, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1463463455438614, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.055619716644287, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8192275762557983, 'test/num_examples': 10000, 'score': 93904.15710663795, 'total_duration': 97204.39679145813, 'accumulated_submission_time': 93904.15710663795, 'accumulated_eval_time': 3279.4887397289276, 'accumulated_logging_time': 11.052030563354492}
I0301 08:11:13.456171 139590161643264 logging_writer.py:48] [276528] accumulated_eval_time=3279.488740, accumulated_logging_time=11.052031, accumulated_submission_time=93904.157107, global_step=276528, preemption_count=0, score=93904.157107, test/accuracy=0.630500, test/loss=1.819228, test/num_examples=10000, total_duration=97204.396791, train/accuracy=0.961217, train/loss=0.146346, validation/accuracy=0.754760, validation/loss=1.055620, validation/num_examples=50000
I0301 08:11:38.258532 139590170035968 logging_writer.py:48] [276600] global_step=276600, grad_norm=4.4218878746032715, loss=0.6143739223480225
I0301 08:12:12.217870 139590161643264 logging_writer.py:48] [276700] global_step=276700, grad_norm=4.429823875427246, loss=0.6142318248748779
I0301 08:12:46.270494 139590170035968 logging_writer.py:48] [276800] global_step=276800, grad_norm=4.250680446624756, loss=0.5882394909858704
I0301 08:13:20.221841 139590161643264 logging_writer.py:48] [276900] global_step=276900, grad_norm=4.681280612945557, loss=0.636448860168457
I0301 08:13:54.215307 139590170035968 logging_writer.py:48] [277000] global_step=277000, grad_norm=4.161380290985107, loss=0.5729243755340576
I0301 08:14:28.187083 139590161643264 logging_writer.py:48] [277100] global_step=277100, grad_norm=4.819599628448486, loss=0.6890630722045898
I0301 08:15:02.411819 139590170035968 logging_writer.py:48] [277200] global_step=277200, grad_norm=4.662034511566162, loss=0.5472917556762695
I0301 08:15:36.379450 139590161643264 logging_writer.py:48] [277300] global_step=277300, grad_norm=5.36189603805542, loss=0.6626859307289124
I0301 08:16:10.335560 139590170035968 logging_writer.py:48] [277400] global_step=277400, grad_norm=4.670047283172607, loss=0.6828041076660156
I0301 08:16:44.315407 139590161643264 logging_writer.py:48] [277500] global_step=277500, grad_norm=4.261203289031982, loss=0.5572015047073364
I0301 08:17:18.259025 139590170035968 logging_writer.py:48] [277600] global_step=277600, grad_norm=4.519212245941162, loss=0.6197013854980469
I0301 08:17:52.217977 139590161643264 logging_writer.py:48] [277700] global_step=277700, grad_norm=4.110622406005859, loss=0.5800426006317139
I0301 08:18:26.189143 139590170035968 logging_writer.py:48] [277800] global_step=277800, grad_norm=4.397546291351318, loss=0.5957375764846802
I0301 08:19:00.314106 139590161643264 logging_writer.py:48] [277900] global_step=277900, grad_norm=4.437071800231934, loss=0.590072512626648
I0301 08:19:34.275455 139590170035968 logging_writer.py:48] [278000] global_step=278000, grad_norm=4.634616851806641, loss=0.6033585071563721
I0301 08:19:43.577437 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:19:49.639596 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:19:58.063160 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:20:00.309876 139753105983296 submission_runner.py:411] Time since start: 97731.31s, 	Step: 278029, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14574436843395233, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.055456519126892, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8184598684310913, 'test/num_examples': 10000, 'score': 94414.21587204933, 'total_duration': 97731.3130865097, 'accumulated_submission_time': 94414.21587204933, 'accumulated_eval_time': 3296.2211039066315, 'accumulated_logging_time': 11.124534606933594}
I0301 08:20:00.375747 139590144857856 logging_writer.py:48] [278029] accumulated_eval_time=3296.221104, accumulated_logging_time=11.124535, accumulated_submission_time=94414.215872, global_step=278029, preemption_count=0, score=94414.215872, test/accuracy=0.630600, test/loss=1.818460, test/num_examples=10000, total_duration=97731.313087, train/accuracy=0.960041, train/loss=0.145744, validation/accuracy=0.754880, validation/loss=1.055457, validation/num_examples=50000
I0301 08:20:24.783956 139590153250560 logging_writer.py:48] [278100] global_step=278100, grad_norm=4.305518627166748, loss=0.5475919842720032
I0301 08:20:58.714625 139590144857856 logging_writer.py:48] [278200] global_step=278200, grad_norm=4.537627696990967, loss=0.6063932180404663
I0301 08:21:32.643553 139590153250560 logging_writer.py:48] [278300] global_step=278300, grad_norm=4.80166482925415, loss=0.6403645277023315
I0301 08:22:06.630136 139590144857856 logging_writer.py:48] [278400] global_step=278400, grad_norm=4.827303409576416, loss=0.6099267601966858
I0301 08:22:40.579712 139590153250560 logging_writer.py:48] [278500] global_step=278500, grad_norm=4.649003505706787, loss=0.6226598024368286
I0301 08:23:14.502582 139590144857856 logging_writer.py:48] [278600] global_step=278600, grad_norm=4.055168151855469, loss=0.6106327772140503
I0301 08:23:48.448432 139590153250560 logging_writer.py:48] [278700] global_step=278700, grad_norm=4.62792444229126, loss=0.7034103274345398
I0301 08:24:22.418245 139590144857856 logging_writer.py:48] [278800] global_step=278800, grad_norm=4.798775672912598, loss=0.5883387327194214
I0301 08:24:56.483133 139590153250560 logging_writer.py:48] [278900] global_step=278900, grad_norm=4.119258880615234, loss=0.6159918308258057
I0301 08:25:30.414562 139590144857856 logging_writer.py:48] [279000] global_step=279000, grad_norm=4.465229511260986, loss=0.635779082775116
I0301 08:26:04.372551 139590153250560 logging_writer.py:48] [279100] global_step=279100, grad_norm=4.242983818054199, loss=0.5755026936531067
I0301 08:26:38.317505 139590144857856 logging_writer.py:48] [279200] global_step=279200, grad_norm=4.586860179901123, loss=0.7054569721221924
I0301 08:27:12.253206 139590153250560 logging_writer.py:48] [279300] global_step=279300, grad_norm=4.472287654876709, loss=0.5986859202384949
I0301 08:27:46.220447 139590144857856 logging_writer.py:48] [279400] global_step=279400, grad_norm=4.404055118560791, loss=0.5830453038215637
I0301 08:28:20.154243 139590153250560 logging_writer.py:48] [279500] global_step=279500, grad_norm=4.980191230773926, loss=0.7599167227745056
I0301 08:28:30.476877 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:28:36.482236 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:28:44.787349 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:28:47.057637 139753105983296 submission_runner.py:411] Time since start: 98258.06s, 	Step: 279532, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.1474865823984146, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0558282136917114, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8213742971420288, 'test/num_examples': 10000, 'score': 94924.25241136551, 'total_duration': 98258.06078863144, 'accumulated_submission_time': 94924.25241136551, 'accumulated_eval_time': 3312.801731109619, 'accumulated_logging_time': 11.201616048812866}
I0301 08:28:47.125537 139589708666624 logging_writer.py:48] [279532] accumulated_eval_time=3312.801731, accumulated_logging_time=11.201616, accumulated_submission_time=94924.252411, global_step=279532, preemption_count=0, score=94924.252411, test/accuracy=0.630000, test/loss=1.821374, test/num_examples=10000, total_duration=98258.060789, train/accuracy=0.960300, train/loss=0.147487, validation/accuracy=0.754880, validation/loss=1.055828, validation/num_examples=50000
I0301 08:29:10.558685 139589717059328 logging_writer.py:48] [279600] global_step=279600, grad_norm=4.529358863830566, loss=0.558273434638977
I0301 08:29:44.474998 139589708666624 logging_writer.py:48] [279700] global_step=279700, grad_norm=4.769355773925781, loss=0.6187707185745239
I0301 08:30:18.407400 139589717059328 logging_writer.py:48] [279800] global_step=279800, grad_norm=4.091335296630859, loss=0.509191632270813
I0301 08:30:52.388450 139589708666624 logging_writer.py:48] [279900] global_step=279900, grad_norm=4.609040260314941, loss=0.6015447974205017
I0301 08:31:26.457360 139589717059328 logging_writer.py:48] [280000] global_step=280000, grad_norm=4.391031265258789, loss=0.5630833506584167
I0301 08:32:00.410899 139589708666624 logging_writer.py:48] [280100] global_step=280100, grad_norm=4.7053985595703125, loss=0.6200203895568848
I0301 08:32:34.345429 139589717059328 logging_writer.py:48] [280200] global_step=280200, grad_norm=4.334695339202881, loss=0.670381486415863
I0301 08:33:08.334003 139589708666624 logging_writer.py:48] [280300] global_step=280300, grad_norm=4.718348979949951, loss=0.6004582047462463
I0301 08:33:42.265891 139589717059328 logging_writer.py:48] [280400] global_step=280400, grad_norm=4.424239635467529, loss=0.6048349738121033
I0301 08:34:16.242533 139589708666624 logging_writer.py:48] [280500] global_step=280500, grad_norm=4.8478007316589355, loss=0.7141032218933105
I0301 08:34:50.214436 139589717059328 logging_writer.py:48] [280600] global_step=280600, grad_norm=4.441291332244873, loss=0.5863434076309204
I0301 08:35:24.205350 139589708666624 logging_writer.py:48] [280700] global_step=280700, grad_norm=4.618833541870117, loss=0.6435324549674988
I0301 08:35:58.176477 139589717059328 logging_writer.py:48] [280800] global_step=280800, grad_norm=4.7890191078186035, loss=0.6896086931228638
I0301 08:36:32.147291 139589708666624 logging_writer.py:48] [280900] global_step=280900, grad_norm=4.295518398284912, loss=0.5814701914787292
I0301 08:37:06.132021 139589717059328 logging_writer.py:48] [281000] global_step=281000, grad_norm=4.759366512298584, loss=0.652779221534729
I0301 08:37:17.256821 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:37:23.313207 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:37:31.589482 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:37:33.973831 139753105983296 submission_runner.py:411] Time since start: 98784.98s, 	Step: 281034, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14675664901733398, 'validation/accuracy': 0.7548199892044067, 'validation/loss': 1.0551109313964844, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8184127807617188, 'test/num_examples': 10000, 'score': 95434.31954813004, 'total_duration': 98784.97705054283, 'accumulated_submission_time': 95434.31954813004, 'accumulated_eval_time': 3329.5186789035797, 'accumulated_logging_time': 11.280039310455322}
I0301 08:37:34.041483 139589725452032 logging_writer.py:48] [281034] accumulated_eval_time=3329.518679, accumulated_logging_time=11.280039, accumulated_submission_time=95434.319548, global_step=281034, preemption_count=0, score=95434.319548, test/accuracy=0.630500, test/loss=1.818413, test/num_examples=10000, total_duration=98784.977051, train/accuracy=0.961097, train/loss=0.146757, validation/accuracy=0.754820, validation/loss=1.055111, validation/num_examples=50000
I0301 08:37:56.742289 139590170035968 logging_writer.py:48] [281100] global_step=281100, grad_norm=4.599737167358398, loss=0.6235671639442444
I0301 08:38:30.678726 139589725452032 logging_writer.py:48] [281200] global_step=281200, grad_norm=4.801353454589844, loss=0.6401925086975098
I0301 08:39:04.608152 139590170035968 logging_writer.py:48] [281300] global_step=281300, grad_norm=4.394761562347412, loss=0.5729126930236816
I0301 08:39:38.543103 139589725452032 logging_writer.py:48] [281400] global_step=281400, grad_norm=4.814740180969238, loss=0.6419711709022522
I0301 08:40:12.507640 139590170035968 logging_writer.py:48] [281500] global_step=281500, grad_norm=4.356834888458252, loss=0.5729215145111084
I0301 08:40:46.447781 139589725452032 logging_writer.py:48] [281600] global_step=281600, grad_norm=4.152677059173584, loss=0.5596897602081299
I0301 08:41:20.397558 139590170035968 logging_writer.py:48] [281700] global_step=281700, grad_norm=5.146658420562744, loss=0.6800351142883301
I0301 08:41:54.315178 139589725452032 logging_writer.py:48] [281800] global_step=281800, grad_norm=4.574680805206299, loss=0.6836037635803223
I0301 08:42:28.274137 139590170035968 logging_writer.py:48] [281900] global_step=281900, grad_norm=4.900394916534424, loss=0.6480003595352173
I0301 08:43:02.210045 139589725452032 logging_writer.py:48] [282000] global_step=282000, grad_norm=4.801625728607178, loss=0.6836158633232117
I0301 08:43:36.187262 139590170035968 logging_writer.py:48] [282100] global_step=282100, grad_norm=5.188896179199219, loss=0.5675363540649414
I0301 08:44:10.143263 139589725452032 logging_writer.py:48] [282200] global_step=282200, grad_norm=4.8584136962890625, loss=0.6180424690246582
I0301 08:44:44.097859 139590170035968 logging_writer.py:48] [282300] global_step=282300, grad_norm=4.456429958343506, loss=0.5898153185844421
I0301 08:45:18.046702 139589725452032 logging_writer.py:48] [282400] global_step=282400, grad_norm=4.204184055328369, loss=0.5884501934051514
I0301 08:45:51.989106 139590170035968 logging_writer.py:48] [282500] global_step=282500, grad_norm=4.184652805328369, loss=0.5714788436889648
I0301 08:46:04.010720 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:46:10.153207 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:46:18.524911 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:46:20.833890 139753105983296 submission_runner.py:411] Time since start: 99311.84s, 	Step: 282537, 	{'train/accuracy': 0.959382951259613, 'train/loss': 0.14842136204242706, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0551875829696655, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8191053867340088, 'test/num_examples': 10000, 'score': 95944.22466540337, 'total_duration': 99311.83710670471, 'accumulated_submission_time': 95944.22466540337, 'accumulated_eval_time': 3346.3417830467224, 'accumulated_logging_time': 11.358751773834229}
I0301 08:46:20.902292 139590144857856 logging_writer.py:48] [282537] accumulated_eval_time=3346.341783, accumulated_logging_time=11.358752, accumulated_submission_time=95944.224665, global_step=282537, preemption_count=0, score=95944.224665, test/accuracy=0.630200, test/loss=1.819105, test/num_examples=10000, total_duration=99311.837107, train/accuracy=0.959383, train/loss=0.148421, validation/accuracy=0.754980, validation/loss=1.055188, validation/num_examples=50000
I0301 08:46:42.634342 139590153250560 logging_writer.py:48] [282600] global_step=282600, grad_norm=4.445306301116943, loss=0.6292786002159119
I0301 08:47:16.558692 139590144857856 logging_writer.py:48] [282700] global_step=282700, grad_norm=4.913227558135986, loss=0.6308487057685852
I0301 08:47:50.482499 139590153250560 logging_writer.py:48] [282800] global_step=282800, grad_norm=4.395463943481445, loss=0.6165128350257874
I0301 08:48:24.421271 139590144857856 logging_writer.py:48] [282900] global_step=282900, grad_norm=4.437028408050537, loss=0.6320371031761169
I0301 08:48:58.370609 139590153250560 logging_writer.py:48] [283000] global_step=283000, grad_norm=5.099212646484375, loss=0.6365967988967896
I0301 08:49:32.355415 139590144857856 logging_writer.py:48] [283100] global_step=283100, grad_norm=4.5632524490356445, loss=0.5979289412498474
I0301 08:50:06.301340 139590153250560 logging_writer.py:48] [283200] global_step=283200, grad_norm=4.154499530792236, loss=0.5682894587516785
I0301 08:50:40.244499 139590144857856 logging_writer.py:48] [283300] global_step=283300, grad_norm=4.378478527069092, loss=0.644492506980896
I0301 08:51:14.156268 139590153250560 logging_writer.py:48] [283400] global_step=283400, grad_norm=4.617864608764648, loss=0.5570461750030518
I0301 08:51:48.111938 139590144857856 logging_writer.py:48] [283500] global_step=283500, grad_norm=4.583433628082275, loss=0.5871236324310303
I0301 08:52:22.094677 139590153250560 logging_writer.py:48] [283600] global_step=283600, grad_norm=4.816460132598877, loss=0.6689368486404419
I0301 08:52:56.050286 139590144857856 logging_writer.py:48] [283700] global_step=283700, grad_norm=4.387839317321777, loss=0.6462327241897583
I0301 08:53:29.980965 139590153250560 logging_writer.py:48] [283800] global_step=283800, grad_norm=5.175591945648193, loss=0.6678134202957153
I0301 08:54:03.939472 139590144857856 logging_writer.py:48] [283900] global_step=283900, grad_norm=4.631604194641113, loss=0.6445614099502563
I0301 08:54:37.895390 139590153250560 logging_writer.py:48] [284000] global_step=284000, grad_norm=4.32620096206665, loss=0.5791000127792358
I0301 08:54:50.942167 139753105983296 spec.py:321] Evaluating on the training split.
I0301 08:54:56.945465 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 08:55:05.379453 139753105983296 spec.py:349] Evaluating on the test split.
I0301 08:55:07.620598 139753105983296 submission_runner.py:411] Time since start: 99838.62s, 	Step: 284040, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.1470494121313095, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0552425384521484, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8197104930877686, 'test/num_examples': 10000, 'score': 96454.19933009148, 'total_duration': 99838.62381887436, 'accumulated_submission_time': 96454.19933009148, 'accumulated_eval_time': 3363.020151615143, 'accumulated_logging_time': 11.438347816467285}
I0301 08:55:07.686290 139589708666624 logging_writer.py:48] [284040] accumulated_eval_time=3363.020152, accumulated_logging_time=11.438348, accumulated_submission_time=96454.199330, global_step=284040, preemption_count=0, score=96454.199330, test/accuracy=0.629900, test/loss=1.819710, test/num_examples=10000, total_duration=99838.623819, train/accuracy=0.960539, train/loss=0.147049, validation/accuracy=0.754920, validation/loss=1.055243, validation/num_examples=50000
I0301 08:55:28.364507 139589717059328 logging_writer.py:48] [284100] global_step=284100, grad_norm=4.748497486114502, loss=0.6303601264953613
I0301 08:56:02.409412 139589708666624 logging_writer.py:48] [284200] global_step=284200, grad_norm=4.633962631225586, loss=0.6801576614379883
I0301 08:56:36.344021 139589717059328 logging_writer.py:48] [284300] global_step=284300, grad_norm=4.482699394226074, loss=0.6353291273117065
I0301 08:57:10.278222 139589708666624 logging_writer.py:48] [284400] global_step=284400, grad_norm=4.690807819366455, loss=0.6464551687240601
I0301 08:57:44.247569 139589717059328 logging_writer.py:48] [284500] global_step=284500, grad_norm=4.540895938873291, loss=0.615237295627594
I0301 08:58:18.181395 139589708666624 logging_writer.py:48] [284600] global_step=284600, grad_norm=4.593664169311523, loss=0.6542850732803345
I0301 08:58:52.115950 139589717059328 logging_writer.py:48] [284700] global_step=284700, grad_norm=4.303920745849609, loss=0.5178061127662659
I0301 08:59:26.084281 139589708666624 logging_writer.py:48] [284800] global_step=284800, grad_norm=4.413873672485352, loss=0.6789180040359497
I0301 09:00:00.032876 139589717059328 logging_writer.py:48] [284900] global_step=284900, grad_norm=4.628491401672363, loss=0.5925799608230591
I0301 09:00:33.947888 139589708666624 logging_writer.py:48] [285000] global_step=285000, grad_norm=4.93242073059082, loss=0.6122670769691467
I0301 09:01:07.887319 139589717059328 logging_writer.py:48] [285100] global_step=285100, grad_norm=4.6278252601623535, loss=0.6264044642448425
I0301 09:01:41.866586 139589708666624 logging_writer.py:48] [285200] global_step=285200, grad_norm=4.717774868011475, loss=0.6455190777778625
I0301 09:02:15.976833 139589717059328 logging_writer.py:48] [285300] global_step=285300, grad_norm=4.8235554695129395, loss=0.6878135204315186
I0301 09:02:49.923936 139589708666624 logging_writer.py:48] [285400] global_step=285400, grad_norm=4.124755382537842, loss=0.532541811466217
I0301 09:03:23.896140 139589717059328 logging_writer.py:48] [285500] global_step=285500, grad_norm=5.207393646240234, loss=0.613775372505188
I0301 09:03:37.924913 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:03:44.035983 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:03:52.359189 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:03:54.598319 139753105983296 submission_runner.py:411] Time since start: 100365.60s, 	Step: 285543, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14566434919834137, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0539931058883667, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8172407150268555, 'test/num_examples': 10000, 'score': 96964.37281227112, 'total_duration': 100365.60154628754, 'accumulated_submission_time': 96964.37281227112, 'accumulated_eval_time': 3379.6935136318207, 'accumulated_logging_time': 11.514539003372192}
I0301 09:03:54.668524 139590144857856 logging_writer.py:48] [285543] accumulated_eval_time=3379.693514, accumulated_logging_time=11.514539, accumulated_submission_time=96964.372812, global_step=285543, preemption_count=0, score=96964.372812, test/accuracy=0.630900, test/loss=1.817241, test/num_examples=10000, total_duration=100365.601546, train/accuracy=0.960619, train/loss=0.145664, validation/accuracy=0.754860, validation/loss=1.053993, validation/num_examples=50000
I0301 09:04:14.350777 139590153250560 logging_writer.py:48] [285600] global_step=285600, grad_norm=4.580620288848877, loss=0.6528704762458801
I0301 09:04:48.230349 139590144857856 logging_writer.py:48] [285700] global_step=285700, grad_norm=4.2073469161987305, loss=0.5792279839515686
I0301 09:05:22.195063 139590153250560 logging_writer.py:48] [285800] global_step=285800, grad_norm=4.763081073760986, loss=0.6609417796134949
I0301 09:05:56.162245 139590144857856 logging_writer.py:48] [285900] global_step=285900, grad_norm=4.626682758331299, loss=0.6212981343269348
I0301 09:06:30.097478 139590153250560 logging_writer.py:48] [286000] global_step=286000, grad_norm=4.651864528656006, loss=0.6118757724761963
I0301 09:07:04.064572 139590144857856 logging_writer.py:48] [286100] global_step=286100, grad_norm=5.168097496032715, loss=0.6757223010063171
I0301 09:07:38.026013 139590153250560 logging_writer.py:48] [286200] global_step=286200, grad_norm=4.307511806488037, loss=0.6157498359680176
I0301 09:08:12.101355 139590144857856 logging_writer.py:48] [286300] global_step=286300, grad_norm=4.262273788452148, loss=0.5911999344825745
I0301 09:08:46.048482 139590153250560 logging_writer.py:48] [286400] global_step=286400, grad_norm=5.1241960525512695, loss=0.6670414209365845
I0301 09:09:19.978101 139590144857856 logging_writer.py:48] [286500] global_step=286500, grad_norm=4.8410444259643555, loss=0.6380501389503479
I0301 09:09:53.939175 139590153250560 logging_writer.py:48] [286600] global_step=286600, grad_norm=4.59425687789917, loss=0.5388388633728027
I0301 09:10:27.922503 139590144857856 logging_writer.py:48] [286700] global_step=286700, grad_norm=5.860010623931885, loss=0.6103842258453369
I0301 09:11:01.869251 139590153250560 logging_writer.py:48] [286800] global_step=286800, grad_norm=4.552155017852783, loss=0.6179620623588562
I0301 09:11:35.822559 139590144857856 logging_writer.py:48] [286900] global_step=286900, grad_norm=4.5698676109313965, loss=0.6114151477813721
I0301 09:12:09.779732 139590153250560 logging_writer.py:48] [287000] global_step=287000, grad_norm=5.308345794677734, loss=0.7366518974304199
I0301 09:12:24.866463 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:12:30.917746 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:12:39.333202 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:12:41.588158 139753105983296 submission_runner.py:411] Time since start: 100892.59s, 	Step: 287046, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14898185431957245, 'validation/accuracy': 0.7545999884605408, 'validation/loss': 1.0550166368484497, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8189139366149902, 'test/num_examples': 10000, 'score': 97474.50636482239, 'total_duration': 100892.59136629105, 'accumulated_submission_time': 97474.50636482239, 'accumulated_eval_time': 3396.415134191513, 'accumulated_logging_time': 11.59593391418457}
I0301 09:12:41.653291 139589717059328 logging_writer.py:48] [287046] accumulated_eval_time=3396.415134, accumulated_logging_time=11.595934, accumulated_submission_time=97474.506365, global_step=287046, preemption_count=0, score=97474.506365, test/accuracy=0.629900, test/loss=1.818914, test/num_examples=10000, total_duration=100892.591366, train/accuracy=0.960120, train/loss=0.148982, validation/accuracy=0.754600, validation/loss=1.055017, validation/num_examples=50000
I0301 09:13:00.289734 139589725452032 logging_writer.py:48] [287100] global_step=287100, grad_norm=4.310671806335449, loss=0.6454386115074158
I0301 09:13:34.173711 139589717059328 logging_writer.py:48] [287200] global_step=287200, grad_norm=4.8593549728393555, loss=0.6007013320922852
I0301 09:14:08.092082 139589725452032 logging_writer.py:48] [287300] global_step=287300, grad_norm=4.802829265594482, loss=0.6384754776954651
I0301 09:14:42.113749 139589717059328 logging_writer.py:48] [287400] global_step=287400, grad_norm=4.404942035675049, loss=0.6509126424789429
I0301 09:15:16.053183 139589725452032 logging_writer.py:48] [287500] global_step=287500, grad_norm=4.537387371063232, loss=0.5844685435295105
I0301 09:15:49.944309 139589717059328 logging_writer.py:48] [287600] global_step=287600, grad_norm=4.911635875701904, loss=0.7117103338241577
I0301 09:16:23.895771 139589725452032 logging_writer.py:48] [287700] global_step=287700, grad_norm=4.151876926422119, loss=0.5488564968109131
I0301 09:16:57.838631 139589717059328 logging_writer.py:48] [287800] global_step=287800, grad_norm=4.199894905090332, loss=0.5905232429504395
I0301 09:17:31.753128 139589725452032 logging_writer.py:48] [287900] global_step=287900, grad_norm=4.693896293640137, loss=0.6341041326522827
I0301 09:18:05.697191 139589717059328 logging_writer.py:48] [288000] global_step=288000, grad_norm=4.3314619064331055, loss=0.6100468039512634
I0301 09:18:39.616496 139589725452032 logging_writer.py:48] [288100] global_step=288100, grad_norm=4.3773016929626465, loss=0.518154501914978
I0301 09:19:13.561198 139589717059328 logging_writer.py:48] [288200] global_step=288200, grad_norm=4.301124095916748, loss=0.6113461852073669
I0301 09:19:47.484408 139589725452032 logging_writer.py:48] [288300] global_step=288300, grad_norm=4.57390022277832, loss=0.6630823612213135
I0301 09:20:21.499346 139589717059328 logging_writer.py:48] [288400] global_step=288400, grad_norm=4.827586650848389, loss=0.7094685435295105
I0301 09:20:55.447085 139589725452032 logging_writer.py:48] [288500] global_step=288500, grad_norm=4.351393699645996, loss=0.5790197849273682
I0301 09:21:11.888188 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:21:18.049859 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:21:26.251642 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:21:28.562693 139753105983296 submission_runner.py:411] Time since start: 101419.57s, 	Step: 288550, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14679870009422302, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0558570623397827, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.819604754447937, 'test/num_examples': 10000, 'score': 97984.6793088913, 'total_duration': 101419.5659172535, 'accumulated_submission_time': 97984.6793088913, 'accumulated_eval_time': 3413.089588880539, 'accumulated_logging_time': 11.67100715637207}
I0301 09:21:28.627679 139589717059328 logging_writer.py:48] [288550] accumulated_eval_time=3413.089589, accumulated_logging_time=11.671007, accumulated_submission_time=97984.679309, global_step=288550, preemption_count=0, score=97984.679309, test/accuracy=0.630300, test/loss=1.819605, test/num_examples=10000, total_duration=101419.565917, train/accuracy=0.960858, train/loss=0.146799, validation/accuracy=0.754880, validation/loss=1.055857, validation/num_examples=50000
I0301 09:21:45.947926 139590170035968 logging_writer.py:48] [288600] global_step=288600, grad_norm=4.92424201965332, loss=0.6958777904510498
I0301 09:22:19.877474 139589717059328 logging_writer.py:48] [288700] global_step=288700, grad_norm=4.508177757263184, loss=0.5901862978935242
I0301 09:22:53.839413 139590170035968 logging_writer.py:48] [288800] global_step=288800, grad_norm=4.642492771148682, loss=0.628445565700531
I0301 09:23:27.803033 139589717059328 logging_writer.py:48] [288900] global_step=288900, grad_norm=4.460747241973877, loss=0.6196591258049011
I0301 09:24:01.746355 139590170035968 logging_writer.py:48] [289000] global_step=289000, grad_norm=4.487797737121582, loss=0.6093310117721558
I0301 09:24:35.712643 139589717059328 logging_writer.py:48] [289100] global_step=289100, grad_norm=4.588367462158203, loss=0.6138402819633484
I0301 09:25:09.650548 139590170035968 logging_writer.py:48] [289200] global_step=289200, grad_norm=4.311280250549316, loss=0.5880945324897766
I0301 09:25:43.620042 139589717059328 logging_writer.py:48] [289300] global_step=289300, grad_norm=4.837091445922852, loss=0.707310140132904
I0301 09:26:17.559013 139590170035968 logging_writer.py:48] [289400] global_step=289400, grad_norm=4.555822372436523, loss=0.7005888223648071
I0301 09:26:51.668832 139589717059328 logging_writer.py:48] [289500] global_step=289500, grad_norm=4.475236415863037, loss=0.6057751178741455
I0301 09:27:25.641087 139590170035968 logging_writer.py:48] [289600] global_step=289600, grad_norm=4.981299877166748, loss=0.6170611381530762
I0301 09:27:59.600018 139589717059328 logging_writer.py:48] [289700] global_step=289700, grad_norm=4.424391269683838, loss=0.618955135345459
I0301 09:28:33.577708 139590170035968 logging_writer.py:48] [289800] global_step=289800, grad_norm=4.8087334632873535, loss=0.5921281576156616
I0301 09:29:07.534461 139589717059328 logging_writer.py:48] [289900] global_step=289900, grad_norm=4.68692684173584, loss=0.5747972130775452
I0301 09:29:41.494498 139590170035968 logging_writer.py:48] [290000] global_step=290000, grad_norm=5.07934045791626, loss=0.6246993541717529
I0301 09:29:58.621907 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:30:04.733597 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:30:13.097349 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:30:15.351488 139753105983296 submission_runner.py:411] Time since start: 101946.35s, 	Step: 290052, 	{'train/accuracy': 0.9595623016357422, 'train/loss': 0.15050145983695984, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0554848909378052, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.818774700164795, 'test/num_examples': 10000, 'score': 98494.60548329353, 'total_duration': 101946.35471343994, 'accumulated_submission_time': 98494.60548329353, 'accumulated_eval_time': 3429.8191237449646, 'accumulated_logging_time': 11.751428842544556}
I0301 09:30:15.418767 139590161643264 logging_writer.py:48] [290052] accumulated_eval_time=3429.819124, accumulated_logging_time=11.751429, accumulated_submission_time=98494.605483, global_step=290052, preemption_count=0, score=98494.605483, test/accuracy=0.630400, test/loss=1.818775, test/num_examples=10000, total_duration=101946.354713, train/accuracy=0.959562, train/loss=0.150501, validation/accuracy=0.754680, validation/loss=1.055485, validation/num_examples=50000
I0301 09:30:32.072135 139590178428672 logging_writer.py:48] [290100] global_step=290100, grad_norm=4.623533725738525, loss=0.6924007534980774
I0301 09:31:05.939965 139590161643264 logging_writer.py:48] [290200] global_step=290200, grad_norm=4.220268726348877, loss=0.5771104693412781
I0301 09:31:39.893274 139590178428672 logging_writer.py:48] [290300] global_step=290300, grad_norm=4.585339546203613, loss=0.6313004493713379
I0301 09:32:13.825890 139590161643264 logging_writer.py:48] [290400] global_step=290400, grad_norm=5.059264183044434, loss=0.6307212114334106
I0301 09:32:47.901860 139590178428672 logging_writer.py:48] [290500] global_step=290500, grad_norm=4.650864601135254, loss=0.651053786277771
I0301 09:33:21.861667 139590161643264 logging_writer.py:48] [290600] global_step=290600, grad_norm=4.517809867858887, loss=0.5763530135154724
I0301 09:33:55.814362 139590178428672 logging_writer.py:48] [290700] global_step=290700, grad_norm=4.44780969619751, loss=0.6147788763046265
I0301 09:34:29.752090 139590161643264 logging_writer.py:48] [290800] global_step=290800, grad_norm=4.578832149505615, loss=0.6448917388916016
I0301 09:35:03.666461 139590178428672 logging_writer.py:48] [290900] global_step=290900, grad_norm=5.01564359664917, loss=0.627293586730957
I0301 09:35:37.628376 139590161643264 logging_writer.py:48] [291000] global_step=291000, grad_norm=4.621520519256592, loss=0.6880955696105957
I0301 09:36:11.550484 139590178428672 logging_writer.py:48] [291100] global_step=291100, grad_norm=4.540779113769531, loss=0.6428783535957336
I0301 09:36:45.485805 139590161643264 logging_writer.py:48] [291200] global_step=291200, grad_norm=4.415858745574951, loss=0.6386745572090149
I0301 09:37:19.415746 139590178428672 logging_writer.py:48] [291300] global_step=291300, grad_norm=4.261141777038574, loss=0.5857670903205872
I0301 09:37:53.362226 139590161643264 logging_writer.py:48] [291400] global_step=291400, grad_norm=4.430929660797119, loss=0.5734375715255737
I0301 09:38:27.315618 139590178428672 logging_writer.py:48] [291500] global_step=291500, grad_norm=4.990004062652588, loss=0.5966079235076904
I0301 09:38:45.460453 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:38:51.777359 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:39:00.089508 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:39:02.501000 139753105983296 submission_runner.py:411] Time since start: 102473.50s, 	Step: 291555, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.14723780751228333, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.054668664932251, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.817091703414917, 'test/num_examples': 10000, 'score': 99004.58482909203, 'total_duration': 102473.50423121452, 'accumulated_submission_time': 99004.58482909203, 'accumulated_eval_time': 3446.859624147415, 'accumulated_logging_time': 11.829417705535889}
I0301 09:39:02.573919 139589725452032 logging_writer.py:48] [291555] accumulated_eval_time=3446.859624, accumulated_logging_time=11.829418, accumulated_submission_time=99004.584829, global_step=291555, preemption_count=0, score=99004.584829, test/accuracy=0.630400, test/loss=1.817092, test/num_examples=10000, total_duration=102473.504231, train/accuracy=0.960898, train/loss=0.147238, validation/accuracy=0.754620, validation/loss=1.054669, validation/num_examples=50000
I0301 09:39:18.216657 139590144857856 logging_writer.py:48] [291600] global_step=291600, grad_norm=4.834880828857422, loss=0.7008501887321472
I0301 09:39:52.138573 139589725452032 logging_writer.py:48] [291700] global_step=291700, grad_norm=4.3057861328125, loss=0.6054266691207886
I0301 09:40:26.100597 139590144857856 logging_writer.py:48] [291800] global_step=291800, grad_norm=4.645522117614746, loss=0.6402470469474792
I0301 09:41:00.037247 139589725452032 logging_writer.py:48] [291900] global_step=291900, grad_norm=4.693510055541992, loss=0.632596492767334
I0301 09:41:33.983803 139590144857856 logging_writer.py:48] [292000] global_step=292000, grad_norm=4.578701019287109, loss=0.5802006125450134
I0301 09:42:07.941353 139589725452032 logging_writer.py:48] [292100] global_step=292100, grad_norm=4.0986504554748535, loss=0.5404345393180847
I0301 09:42:41.892216 139590144857856 logging_writer.py:48] [292200] global_step=292200, grad_norm=4.380582809448242, loss=0.6535015106201172
I0301 09:43:15.854652 139589725452032 logging_writer.py:48] [292300] global_step=292300, grad_norm=4.530787467956543, loss=0.726479709148407
I0301 09:43:49.817717 139590144857856 logging_writer.py:48] [292400] global_step=292400, grad_norm=4.289435386657715, loss=0.6693568229675293
I0301 09:44:23.809427 139589725452032 logging_writer.py:48] [292500] global_step=292500, grad_norm=4.344250202178955, loss=0.6139042377471924
I0301 09:44:57.852955 139590144857856 logging_writer.py:48] [292600] global_step=292600, grad_norm=4.553712368011475, loss=0.5480436682701111
I0301 09:45:31.816926 139589725452032 logging_writer.py:48] [292700] global_step=292700, grad_norm=4.835085868835449, loss=0.67555171251297
I0301 09:46:05.773447 139590144857856 logging_writer.py:48] [292800] global_step=292800, grad_norm=4.081231117248535, loss=0.5817475318908691
I0301 09:46:39.743472 139589725452032 logging_writer.py:48] [292900] global_step=292900, grad_norm=4.394536972045898, loss=0.6044142842292786
I0301 09:47:13.718473 139590144857856 logging_writer.py:48] [293000] global_step=293000, grad_norm=4.515456199645996, loss=0.5703125
I0301 09:47:32.564608 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:47:38.597088 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:47:46.917844 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:47:49.202383 139753105983296 submission_runner.py:411] Time since start: 103000.21s, 	Step: 293057, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14538633823394775, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.055548071861267, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8181191682815552, 'test/num_examples': 10000, 'score': 99514.50890374184, 'total_duration': 103000.20554113388, 'accumulated_submission_time': 99514.50890374184, 'accumulated_eval_time': 3463.4972772598267, 'accumulated_logging_time': 11.915416479110718}
I0301 09:47:49.272816 139590170035968 logging_writer.py:48] [293057] accumulated_eval_time=3463.497277, accumulated_logging_time=11.915416, accumulated_submission_time=99514.508904, global_step=293057, preemption_count=0, score=99514.508904, test/accuracy=0.630000, test/loss=1.818119, test/num_examples=10000, total_duration=103000.205541, train/accuracy=0.961276, train/loss=0.145386, validation/accuracy=0.754560, validation/loss=1.055548, validation/num_examples=50000
I0301 09:48:04.207266 139590178428672 logging_writer.py:48] [293100] global_step=293100, grad_norm=4.767456531524658, loss=0.6609418392181396
I0301 09:48:38.151968 139590170035968 logging_writer.py:48] [293200] global_step=293200, grad_norm=4.954801082611084, loss=0.6369732618331909
I0301 09:49:12.098980 139590178428672 logging_writer.py:48] [293300] global_step=293300, grad_norm=4.439201831817627, loss=0.5730469226837158
I0301 09:49:45.999152 139590170035968 logging_writer.py:48] [293400] global_step=293400, grad_norm=4.495462417602539, loss=0.6503325700759888
I0301 09:50:19.940296 139590178428672 logging_writer.py:48] [293500] global_step=293500, grad_norm=4.473230838775635, loss=0.6932951211929321
I0301 09:50:53.924386 139590170035968 logging_writer.py:48] [293600] global_step=293600, grad_norm=4.816959381103516, loss=0.6846942901611328
I0301 09:51:27.938269 139590178428672 logging_writer.py:48] [293700] global_step=293700, grad_norm=4.548345565795898, loss=0.6636536121368408
I0301 09:52:01.904978 139590170035968 logging_writer.py:48] [293800] global_step=293800, grad_norm=4.462738037109375, loss=0.6488267183303833
I0301 09:52:35.843177 139590178428672 logging_writer.py:48] [293900] global_step=293900, grad_norm=4.415861129760742, loss=0.6389278173446655
I0301 09:53:09.824115 139590170035968 logging_writer.py:48] [294000] global_step=294000, grad_norm=4.517619609832764, loss=0.6109541654586792
I0301 09:53:43.791200 139590178428672 logging_writer.py:48] [294100] global_step=294100, grad_norm=5.220797538757324, loss=0.7093958258628845
I0301 09:54:17.712112 139590170035968 logging_writer.py:48] [294200] global_step=294200, grad_norm=4.980051517486572, loss=0.6331257224082947
I0301 09:54:51.670114 139590178428672 logging_writer.py:48] [294300] global_step=294300, grad_norm=4.5449419021606445, loss=0.6317502856254578
I0301 09:55:25.625056 139590170035968 logging_writer.py:48] [294400] global_step=294400, grad_norm=4.537516117095947, loss=0.6308976411819458
I0301 09:55:59.585195 139590178428672 logging_writer.py:48] [294500] global_step=294500, grad_norm=4.424420356750488, loss=0.5917247533798218
I0301 09:56:19.426329 139753105983296 spec.py:321] Evaluating on the training split.
I0301 09:56:25.510334 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 09:56:33.796517 139753105983296 spec.py:349] Evaluating on the test split.
I0301 09:56:36.224916 139753105983296 submission_runner.py:411] Time since start: 103527.23s, 	Step: 294560, 	{'train/accuracy': 0.9592036008834839, 'train/loss': 0.1480921506881714, 'validation/accuracy': 0.754539966583252, 'validation/loss': 1.0552234649658203, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8201440572738647, 'test/num_examples': 10000, 'score': 100024.59973978996, 'total_duration': 103527.22813105583, 'accumulated_submission_time': 100024.59973978996, 'accumulated_eval_time': 3480.2958142757416, 'accumulated_logging_time': 11.99556565284729}
I0301 09:56:36.287060 139589708666624 logging_writer.py:48] [294560] accumulated_eval_time=3480.295814, accumulated_logging_time=11.995566, accumulated_submission_time=100024.599740, global_step=294560, preemption_count=0, score=100024.599740, test/accuracy=0.629600, test/loss=1.820144, test/num_examples=10000, total_duration=103527.228131, train/accuracy=0.959204, train/loss=0.148092, validation/accuracy=0.754540, validation/loss=1.055223, validation/num_examples=50000
I0301 09:56:50.200602 139589717059328 logging_writer.py:48] [294600] global_step=294600, grad_norm=5.033000469207764, loss=0.6344968676567078
I0301 09:57:24.185858 139589708666624 logging_writer.py:48] [294700] global_step=294700, grad_norm=4.752355098724365, loss=0.6602835059165955
I0301 09:57:58.127090 139589717059328 logging_writer.py:48] [294800] global_step=294800, grad_norm=4.6764750480651855, loss=0.6203958988189697
I0301 09:58:32.107257 139589708666624 logging_writer.py:48] [294900] global_step=294900, grad_norm=4.861627578735352, loss=0.6286400556564331
I0301 09:59:06.061051 139589717059328 logging_writer.py:48] [295000] global_step=295000, grad_norm=4.379411220550537, loss=0.5444369912147522
I0301 09:59:40.044578 139589708666624 logging_writer.py:48] [295100] global_step=295100, grad_norm=4.673783302307129, loss=0.6510001420974731
I0301 10:00:14.020664 139589717059328 logging_writer.py:48] [295200] global_step=295200, grad_norm=4.741812705993652, loss=0.573415219783783
I0301 10:00:47.998173 139589708666624 logging_writer.py:48] [295300] global_step=295300, grad_norm=4.954729080200195, loss=0.6443339586257935
I0301 10:01:21.982628 139589717059328 logging_writer.py:48] [295400] global_step=295400, grad_norm=4.555179595947266, loss=0.6395264863967896
I0301 10:01:55.922376 139589708666624 logging_writer.py:48] [295500] global_step=295500, grad_norm=4.319151878356934, loss=0.5882555246353149
I0301 10:02:29.902319 139589717059328 logging_writer.py:48] [295600] global_step=295600, grad_norm=4.2111687660217285, loss=0.5729844570159912
I0301 10:03:03.861607 139589708666624 logging_writer.py:48] [295700] global_step=295700, grad_norm=4.11695671081543, loss=0.5663107633590698
I0301 10:03:37.917041 139589717059328 logging_writer.py:48] [295800] global_step=295800, grad_norm=4.614612102508545, loss=0.6570990085601807
I0301 10:04:11.866300 139589708666624 logging_writer.py:48] [295900] global_step=295900, grad_norm=4.447692394256592, loss=0.6307159662246704
I0301 10:04:45.837852 139589717059328 logging_writer.py:48] [296000] global_step=296000, grad_norm=4.4632439613342285, loss=0.6627281308174133
I0301 10:05:06.355121 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:05:12.385040 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:05:20.718051 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:05:22.994041 139753105983296 submission_runner.py:411] Time since start: 104054.00s, 	Step: 296062, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.1463489681482315, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0558689832687378, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8213645219802856, 'test/num_examples': 10000, 'score': 100534.60390925407, 'total_duration': 104053.99726223946, 'accumulated_submission_time': 100534.60390925407, 'accumulated_eval_time': 3496.9346759319305, 'accumulated_logging_time': 12.068589448928833}
I0301 10:05:23.063424 139589717059328 logging_writer.py:48] [296062] accumulated_eval_time=3496.934676, accumulated_logging_time=12.068589, accumulated_submission_time=100534.603909, global_step=296062, preemption_count=0, score=100534.603909, test/accuracy=0.629800, test/loss=1.821365, test/num_examples=10000, total_duration=104053.997262, train/accuracy=0.960918, train/loss=0.146349, validation/accuracy=0.755060, validation/loss=1.055869, validation/num_examples=50000
I0301 10:05:36.303967 139590161643264 logging_writer.py:48] [296100] global_step=296100, grad_norm=4.411514759063721, loss=0.5931521654129028
I0301 10:06:10.189603 139589717059328 logging_writer.py:48] [296200] global_step=296200, grad_norm=4.465900421142578, loss=0.6707099080085754
I0301 10:06:44.110546 139590161643264 logging_writer.py:48] [296300] global_step=296300, grad_norm=4.349364280700684, loss=0.5861214995384216
I0301 10:07:18.077958 139589717059328 logging_writer.py:48] [296400] global_step=296400, grad_norm=4.514224052429199, loss=0.6444782018661499
I0301 10:07:52.028018 139590161643264 logging_writer.py:48] [296500] global_step=296500, grad_norm=5.163383960723877, loss=0.6476787328720093
I0301 10:08:25.984465 139589717059328 logging_writer.py:48] [296600] global_step=296600, grad_norm=4.577058792114258, loss=0.5771190524101257
I0301 10:08:59.919755 139590161643264 logging_writer.py:48] [296700] global_step=296700, grad_norm=4.361802101135254, loss=0.5908722281455994
I0301 10:09:33.917293 139589717059328 logging_writer.py:48] [296800] global_step=296800, grad_norm=4.516820907592773, loss=0.674130380153656
I0301 10:10:07.853477 139590161643264 logging_writer.py:48] [296900] global_step=296900, grad_norm=4.408257961273193, loss=0.6499362587928772
I0301 10:10:41.821386 139589717059328 logging_writer.py:48] [297000] global_step=297000, grad_norm=4.825485706329346, loss=0.6464589834213257
I0301 10:11:15.776454 139590161643264 logging_writer.py:48] [297100] global_step=297100, grad_norm=4.52858829498291, loss=0.6334787607192993
I0301 10:11:49.736627 139589717059328 logging_writer.py:48] [297200] global_step=297200, grad_norm=4.8853631019592285, loss=0.6433824896812439
I0301 10:12:23.683507 139590161643264 logging_writer.py:48] [297300] global_step=297300, grad_norm=4.782644748687744, loss=0.6062623262405396
I0301 10:12:57.648010 139589717059328 logging_writer.py:48] [297400] global_step=297400, grad_norm=4.646327495574951, loss=0.6958087682723999
I0301 10:13:31.603411 139590161643264 logging_writer.py:48] [297500] global_step=297500, grad_norm=4.686313152313232, loss=0.662777304649353
I0301 10:13:53.130707 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:13:59.107003 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:14:07.510248 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:14:09.761528 139753105983296 submission_runner.py:411] Time since start: 104580.76s, 	Step: 297565, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14500145614147186, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.055658221244812, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8192485570907593, 'test/num_examples': 10000, 'score': 101044.60762357712, 'total_duration': 104580.7647485733, 'accumulated_submission_time': 101044.60762357712, 'accumulated_eval_time': 3513.5654361248016, 'accumulated_logging_time': 12.149300336837769}
I0301 10:14:09.828031 139590153250560 logging_writer.py:48] [297565] accumulated_eval_time=3513.565436, accumulated_logging_time=12.149300, accumulated_submission_time=101044.607624, global_step=297565, preemption_count=0, score=101044.607624, test/accuracy=0.631300, test/loss=1.819249, test/num_examples=10000, total_duration=104580.764749, train/accuracy=0.961276, train/loss=0.145001, validation/accuracy=0.754560, validation/loss=1.055658, validation/num_examples=50000
I0301 10:14:22.047526 139590178428672 logging_writer.py:48] [297600] global_step=297600, grad_norm=4.582380294799805, loss=0.5693010091781616
I0301 10:14:55.973505 139590153250560 logging_writer.py:48] [297700] global_step=297700, grad_norm=4.94819450378418, loss=0.6246393918991089
I0301 10:15:29.894517 139590178428672 logging_writer.py:48] [297800] global_step=297800, grad_norm=4.808440685272217, loss=0.6713544726371765
I0301 10:16:03.924313 139590153250560 logging_writer.py:48] [297900] global_step=297900, grad_norm=4.498941421508789, loss=0.5670342445373535
I0301 10:16:37.884306 139590178428672 logging_writer.py:48] [298000] global_step=298000, grad_norm=4.350444316864014, loss=0.6389784812927246
I0301 10:17:11.830045 139590153250560 logging_writer.py:48] [298100] global_step=298100, grad_norm=4.1652140617370605, loss=0.5881106853485107
I0301 10:17:45.769896 139590178428672 logging_writer.py:48] [298200] global_step=298200, grad_norm=4.585805892944336, loss=0.6726166605949402
I0301 10:18:19.729551 139590153250560 logging_writer.py:48] [298300] global_step=298300, grad_norm=4.634526252746582, loss=0.6404677033424377
I0301 10:18:53.644461 139590178428672 logging_writer.py:48] [298400] global_step=298400, grad_norm=4.538455486297607, loss=0.6269853711128235
I0301 10:19:27.590374 139590153250560 logging_writer.py:48] [298500] global_step=298500, grad_norm=4.746157169342041, loss=0.6184163093566895
I0301 10:20:01.557168 139590178428672 logging_writer.py:48] [298600] global_step=298600, grad_norm=4.60095739364624, loss=0.5709688663482666
I0301 10:20:35.492520 139590153250560 logging_writer.py:48] [298700] global_step=298700, grad_norm=4.598601341247559, loss=0.5856342911720276
I0301 10:21:09.477280 139590178428672 logging_writer.py:48] [298800] global_step=298800, grad_norm=4.343307018280029, loss=0.5818920135498047
I0301 10:21:43.420639 139590153250560 logging_writer.py:48] [298900] global_step=298900, grad_norm=4.64053201675415, loss=0.6425647139549255
I0301 10:22:17.424533 139590178428672 logging_writer.py:48] [299000] global_step=299000, grad_norm=4.6663055419921875, loss=0.6545464992523193
I0301 10:22:39.966794 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:22:46.064890 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:22:54.307865 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:22:56.604887 139753105983296 submission_runner.py:411] Time since start: 105107.61s, 	Step: 299068, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14883920550346375, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.056084394454956, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.818241834640503, 'test/num_examples': 10000, 'score': 101554.6838247776, 'total_duration': 105107.60811972618, 'accumulated_submission_time': 101554.6838247776, 'accumulated_eval_time': 3530.203478574753, 'accumulated_logging_time': 12.22605037689209}
I0301 10:22:56.674731 139590144857856 logging_writer.py:48] [299068] accumulated_eval_time=3530.203479, accumulated_logging_time=12.226050, accumulated_submission_time=101554.683825, global_step=299068, preemption_count=0, score=101554.683825, test/accuracy=0.630100, test/loss=1.818242, test/num_examples=10000, total_duration=105107.608120, train/accuracy=0.960200, train/loss=0.148839, validation/accuracy=0.754920, validation/loss=1.056084, validation/num_examples=50000
I0301 10:23:07.888043 139590161643264 logging_writer.py:48] [299100] global_step=299100, grad_norm=4.767714500427246, loss=0.6722087264060974
I0301 10:23:41.792311 139590144857856 logging_writer.py:48] [299200] global_step=299200, grad_norm=4.679300308227539, loss=0.6917511820793152
I0301 10:24:15.696657 139590161643264 logging_writer.py:48] [299300] global_step=299300, grad_norm=4.374549388885498, loss=0.6146056652069092
I0301 10:24:49.649072 139590144857856 logging_writer.py:48] [299400] global_step=299400, grad_norm=4.820163726806641, loss=0.6229029893875122
I0301 10:25:23.603259 139590161643264 logging_writer.py:48] [299500] global_step=299500, grad_norm=4.7257161140441895, loss=0.6426162123680115
I0301 10:25:57.563730 139590144857856 logging_writer.py:48] [299600] global_step=299600, grad_norm=4.401589870452881, loss=0.5914350748062134
I0301 10:26:31.488505 139590161643264 logging_writer.py:48] [299700] global_step=299700, grad_norm=4.661478519439697, loss=0.6156447529792786
I0301 10:27:05.431468 139590144857856 logging_writer.py:48] [299800] global_step=299800, grad_norm=4.505107879638672, loss=0.6170018911361694
I0301 10:27:39.378471 139590161643264 logging_writer.py:48] [299900] global_step=299900, grad_norm=4.321244716644287, loss=0.6116169691085815
I0301 10:28:13.431558 139590144857856 logging_writer.py:48] [300000] global_step=300000, grad_norm=4.3618388175964355, loss=0.6225213408470154
I0301 10:28:47.348911 139590161643264 logging_writer.py:48] [300100] global_step=300100, grad_norm=4.777414321899414, loss=0.6323065757751465
I0301 10:29:21.284084 139590144857856 logging_writer.py:48] [300200] global_step=300200, grad_norm=4.3744635581970215, loss=0.6156417727470398
I0301 10:29:55.200378 139590161643264 logging_writer.py:48] [300300] global_step=300300, grad_norm=4.441311359405518, loss=0.6488407850265503
I0301 10:30:29.157433 139590144857856 logging_writer.py:48] [300400] global_step=300400, grad_norm=4.375291347503662, loss=0.6207171678543091
I0301 10:31:03.104084 139590161643264 logging_writer.py:48] [300500] global_step=300500, grad_norm=4.4649338722229, loss=0.6359528303146362
I0301 10:31:26.680709 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:31:32.684812 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:31:41.021986 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:31:43.333119 139753105983296 submission_runner.py:411] Time since start: 105634.34s, 	Step: 300571, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14404296875, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0556014776229858, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8185244798660278, 'test/num_examples': 10000, 'score': 102064.62591266632, 'total_duration': 105634.33630681038, 'accumulated_submission_time': 102064.62591266632, 'accumulated_eval_time': 3546.8557929992676, 'accumulated_logging_time': 12.306427717208862}
I0301 10:31:43.443191 139589717059328 logging_writer.py:48] [300571] accumulated_eval_time=3546.855793, accumulated_logging_time=12.306428, accumulated_submission_time=102064.625913, global_step=300571, preemption_count=0, score=102064.625913, test/accuracy=0.630000, test/loss=1.818524, test/num_examples=10000, total_duration=105634.336307, train/accuracy=0.961137, train/loss=0.144043, validation/accuracy=0.755100, validation/loss=1.055601, validation/num_examples=50000
I0301 10:31:53.631896 139589725452032 logging_writer.py:48] [300600] global_step=300600, grad_norm=4.460083961486816, loss=0.5845610499382019
I0301 10:32:27.562930 139589717059328 logging_writer.py:48] [300700] global_step=300700, grad_norm=4.784212112426758, loss=0.6184616684913635
I0301 10:33:01.532411 139589725452032 logging_writer.py:48] [300800] global_step=300800, grad_norm=4.851625442504883, loss=0.6225022077560425
I0301 10:33:35.482326 139589717059328 logging_writer.py:48] [300900] global_step=300900, grad_norm=5.025807857513428, loss=0.6907832622528076
I0301 10:34:09.473763 139589725452032 logging_writer.py:48] [301000] global_step=301000, grad_norm=4.273773193359375, loss=0.6123898029327393
I0301 10:34:43.515253 139589717059328 logging_writer.py:48] [301100] global_step=301100, grad_norm=4.041907787322998, loss=0.5655382871627808
I0301 10:35:17.493677 139589725452032 logging_writer.py:48] [301200] global_step=301200, grad_norm=4.369907855987549, loss=0.6265636682510376
I0301 10:35:51.469050 139589717059328 logging_writer.py:48] [301300] global_step=301300, grad_norm=4.458452224731445, loss=0.5761131048202515
I0301 10:36:25.410447 139589725452032 logging_writer.py:48] [301400] global_step=301400, grad_norm=4.3414201736450195, loss=0.5872271060943604
I0301 10:36:59.358587 139589717059328 logging_writer.py:48] [301500] global_step=301500, grad_norm=4.727514743804932, loss=0.5999850034713745
I0301 10:37:33.290327 139589725452032 logging_writer.py:48] [301600] global_step=301600, grad_norm=4.589132308959961, loss=0.6552362442016602
I0301 10:38:07.232505 139589717059328 logging_writer.py:48] [301700] global_step=301700, grad_norm=4.545343399047852, loss=0.5840845704078674
I0301 10:38:41.193557 139589725452032 logging_writer.py:48] [301800] global_step=301800, grad_norm=4.558760166168213, loss=0.6302982568740845
I0301 10:39:15.158141 139589717059328 logging_writer.py:48] [301900] global_step=301900, grad_norm=4.610836505889893, loss=0.5975464582443237
I0301 10:39:49.100613 139589725452032 logging_writer.py:48] [302000] global_step=302000, grad_norm=4.94364070892334, loss=0.7098438143730164
I0301 10:40:13.669927 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:40:19.695292 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:40:27.989204 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:40:30.296149 139753105983296 submission_runner.py:411] Time since start: 106161.30s, 	Step: 302074, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14128488302230835, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0565521717071533, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8204947710037231, 'test/num_examples': 10000, 'score': 102574.78446245193, 'total_duration': 106161.29858207703, 'accumulated_submission_time': 102574.78446245193, 'accumulated_eval_time': 3563.481164932251, 'accumulated_logging_time': 12.430617094039917}
I0301 10:40:30.368420 139590170035968 logging_writer.py:48] [302074] accumulated_eval_time=3563.481165, accumulated_logging_time=12.430617, accumulated_submission_time=102574.784462, global_step=302074, preemption_count=0, score=102574.784462, test/accuracy=0.630000, test/loss=1.820495, test/num_examples=10000, total_duration=106161.298582, train/accuracy=0.961476, train/loss=0.141285, validation/accuracy=0.755060, validation/loss=1.056552, validation/num_examples=50000
I0301 10:40:39.517698 139590186821376 logging_writer.py:48] [302100] global_step=302100, grad_norm=4.081705093383789, loss=0.5704353451728821
I0301 10:41:13.421651 139590170035968 logging_writer.py:48] [302200] global_step=302200, grad_norm=4.895318984985352, loss=0.6855615973472595
I0301 10:41:47.338069 139590186821376 logging_writer.py:48] [302300] global_step=302300, grad_norm=4.2732954025268555, loss=0.5989055037498474
I0301 10:42:21.272851 139590170035968 logging_writer.py:48] [302400] global_step=302400, grad_norm=4.205894947052002, loss=0.530921459197998
I0301 10:42:55.206342 139590186821376 logging_writer.py:48] [302500] global_step=302500, grad_norm=4.2623748779296875, loss=0.5758197903633118
I0301 10:43:29.173615 139590170035968 logging_writer.py:48] [302600] global_step=302600, grad_norm=4.413178443908691, loss=0.6074094772338867
I0301 10:44:03.146457 139590186821376 logging_writer.py:48] [302700] global_step=302700, grad_norm=4.59870719909668, loss=0.6279575824737549
I0301 10:44:37.123296 139590170035968 logging_writer.py:48] [302800] global_step=302800, grad_norm=4.546324729919434, loss=0.6171327829360962
I0301 10:45:11.072471 139590186821376 logging_writer.py:48] [302900] global_step=302900, grad_norm=4.548403739929199, loss=0.6655872464179993
I0301 10:45:45.049259 139590170035968 logging_writer.py:48] [303000] global_step=303000, grad_norm=4.2948079109191895, loss=0.6274145245552063
I0301 10:46:18.990872 139590186821376 logging_writer.py:48] [303100] global_step=303100, grad_norm=4.905478000640869, loss=0.6581862568855286
I0301 10:46:53.005560 139590170035968 logging_writer.py:48] [303200] global_step=303200, grad_norm=4.745430946350098, loss=0.6041421890258789
I0301 10:47:26.959017 139590186821376 logging_writer.py:48] [303300] global_step=303300, grad_norm=4.688486576080322, loss=0.6539692878723145
I0301 10:48:00.947648 139590170035968 logging_writer.py:48] [303400] global_step=303400, grad_norm=4.3225555419921875, loss=0.6081633567810059
I0301 10:48:34.900593 139590186821376 logging_writer.py:48] [303500] global_step=303500, grad_norm=4.449761867523193, loss=0.568703830242157
I0301 10:49:00.515922 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:49:06.588950 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:49:14.930270 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:49:17.217464 139753105983296 submission_runner.py:411] Time since start: 106688.22s, 	Step: 303577, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.1461625099182129, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0551890134811401, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.8195948600769043, 'test/num_examples': 10000, 'score': 103084.86869740486, 'total_duration': 106688.22069692612, 'accumulated_submission_time': 103084.86869740486, 'accumulated_eval_time': 3580.18265748024, 'accumulated_logging_time': 12.513375520706177}
I0301 10:49:17.285088 139589725452032 logging_writer.py:48] [303577] accumulated_eval_time=3580.182657, accumulated_logging_time=12.513376, accumulated_submission_time=103084.868697, global_step=303577, preemption_count=0, score=103084.868697, test/accuracy=0.629200, test/loss=1.819595, test/num_examples=10000, total_duration=106688.220697, train/accuracy=0.961316, train/loss=0.146163, validation/accuracy=0.754780, validation/loss=1.055189, validation/num_examples=50000
I0301 10:49:25.425633 139590144857856 logging_writer.py:48] [303600] global_step=303600, grad_norm=4.659459590911865, loss=0.6241112947463989
I0301 10:49:59.344632 139589725452032 logging_writer.py:48] [303700] global_step=303700, grad_norm=4.613417148590088, loss=0.6165175437927246
I0301 10:50:33.285401 139590144857856 logging_writer.py:48] [303800] global_step=303800, grad_norm=4.568647861480713, loss=0.603966474533081
I0301 10:51:07.198902 139589725452032 logging_writer.py:48] [303900] global_step=303900, grad_norm=4.52085018157959, loss=0.605087161064148
I0301 10:51:41.136556 139590144857856 logging_writer.py:48] [304000] global_step=304000, grad_norm=4.797451496124268, loss=0.6851918697357178
I0301 10:52:15.077300 139589725452032 logging_writer.py:48] [304100] global_step=304100, grad_norm=4.238214492797852, loss=0.5570317506790161
I0301 10:52:49.089279 139590144857856 logging_writer.py:48] [304200] global_step=304200, grad_norm=4.953930377960205, loss=0.7105344533920288
I0301 10:53:23.018113 139589725452032 logging_writer.py:48] [304300] global_step=304300, grad_norm=4.4934492111206055, loss=0.6623733639717102
I0301 10:53:56.963800 139590144857856 logging_writer.py:48] [304400] global_step=304400, grad_norm=4.375714302062988, loss=0.5609744191169739
I0301 10:54:30.929860 139589725452032 logging_writer.py:48] [304500] global_step=304500, grad_norm=4.551898002624512, loss=0.6269975304603577
I0301 10:55:04.889968 139590144857856 logging_writer.py:48] [304600] global_step=304600, grad_norm=4.065757751464844, loss=0.5223852396011353
I0301 10:55:38.864851 139589725452032 logging_writer.py:48] [304700] global_step=304700, grad_norm=4.609410285949707, loss=0.6782046556472778
I0301 10:56:12.834475 139590144857856 logging_writer.py:48] [304800] global_step=304800, grad_norm=4.643483638763428, loss=0.6579921841621399
I0301 10:56:46.778452 139589725452032 logging_writer.py:48] [304900] global_step=304900, grad_norm=4.3645782470703125, loss=0.6914639472961426
I0301 10:57:20.733522 139590144857856 logging_writer.py:48] [305000] global_step=305000, grad_norm=4.8523783683776855, loss=0.6768315434455872
I0301 10:57:47.336394 139753105983296 spec.py:321] Evaluating on the training split.
I0301 10:57:54.017179 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 10:58:02.462591 139753105983296 spec.py:349] Evaluating on the test split.
I0301 10:58:04.759356 139753105983296 submission_runner.py:411] Time since start: 107215.76s, 	Step: 305080, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14485853910446167, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.054583191871643, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.817049264907837, 'test/num_examples': 10000, 'score': 103594.85424780846, 'total_duration': 107215.7625875473, 'accumulated_submission_time': 103594.85424780846, 'accumulated_eval_time': 3597.605567932129, 'accumulated_logging_time': 12.594209432601929}
I0301 10:58:04.826168 139589717059328 logging_writer.py:48] [305080] accumulated_eval_time=3597.605568, accumulated_logging_time=12.594209, accumulated_submission_time=103594.854248, global_step=305080, preemption_count=0, score=103594.854248, test/accuracy=0.630600, test/loss=1.817049, test/num_examples=10000, total_duration=107215.762588, train/accuracy=0.960539, train/loss=0.144859, validation/accuracy=0.755020, validation/loss=1.054583, validation/num_examples=50000
I0301 10:58:11.965765 139589725452032 logging_writer.py:48] [305100] global_step=305100, grad_norm=4.491613388061523, loss=0.6134049296379089
I0301 10:58:45.859688 139589717059328 logging_writer.py:48] [305200] global_step=305200, grad_norm=4.35255765914917, loss=0.5828510522842407
I0301 10:59:19.856508 139589725452032 logging_writer.py:48] [305300] global_step=305300, grad_norm=4.447255611419678, loss=0.5761626958847046
I0301 10:59:53.802279 139589717059328 logging_writer.py:48] [305400] global_step=305400, grad_norm=4.906490325927734, loss=0.6402114033699036
I0301 11:00:27.710161 139589725452032 logging_writer.py:48] [305500] global_step=305500, grad_norm=4.82205867767334, loss=0.6371433734893799
I0301 11:01:01.644303 139589717059328 logging_writer.py:48] [305600] global_step=305600, grad_norm=4.827860355377197, loss=0.6511695981025696
I0301 11:01:35.598735 139589725452032 logging_writer.py:48] [305700] global_step=305700, grad_norm=4.447051525115967, loss=0.621913731098175
I0301 11:02:09.526487 139589717059328 logging_writer.py:48] [305800] global_step=305800, grad_norm=4.409055709838867, loss=0.6039386987686157
I0301 11:02:43.447659 139589725452032 logging_writer.py:48] [305900] global_step=305900, grad_norm=4.439431667327881, loss=0.6437197923660278
I0301 11:03:17.384608 139589717059328 logging_writer.py:48] [306000] global_step=306000, grad_norm=4.75062894821167, loss=0.6144705414772034
I0301 11:03:51.330619 139589725452032 logging_writer.py:48] [306100] global_step=306100, grad_norm=4.780335903167725, loss=0.6446459293365479
I0301 11:04:25.289002 139589717059328 logging_writer.py:48] [306200] global_step=306200, grad_norm=4.724404335021973, loss=0.6575568318367004
I0301 11:04:59.210617 139589725452032 logging_writer.py:48] [306300] global_step=306300, grad_norm=4.5932393074035645, loss=0.6135722398757935
I0301 11:05:33.252820 139589717059328 logging_writer.py:48] [306400] global_step=306400, grad_norm=5.243932723999023, loss=0.6321778297424316
I0301 11:06:07.187321 139589725452032 logging_writer.py:48] [306500] global_step=306500, grad_norm=4.6857829093933105, loss=0.6957780122756958
I0301 11:06:34.825296 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:06:40.865012 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:06:49.225161 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:06:51.523201 139753105983296 submission_runner.py:411] Time since start: 107742.53s, 	Step: 306583, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14891111850738525, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0559635162353516, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8187658786773682, 'test/num_examples': 10000, 'score': 104104.78870105743, 'total_duration': 107742.52642440796, 'accumulated_submission_time': 104104.78870105743, 'accumulated_eval_time': 3614.303423643112, 'accumulated_logging_time': 12.67265248298645}
I0301 11:06:51.597056 139589708666624 logging_writer.py:48] [306583] accumulated_eval_time=3614.303424, accumulated_logging_time=12.672652, accumulated_submission_time=104104.788701, global_step=306583, preemption_count=0, score=104104.788701, test/accuracy=0.630700, test/loss=1.818766, test/num_examples=10000, total_duration=107742.526424, train/accuracy=0.959981, train/loss=0.148911, validation/accuracy=0.755220, validation/loss=1.055964, validation/num_examples=50000
I0301 11:06:57.729445 139589717059328 logging_writer.py:48] [306600] global_step=306600, grad_norm=4.994738578796387, loss=0.6461935043334961
I0301 11:07:31.670312 139589708666624 logging_writer.py:48] [306700] global_step=306700, grad_norm=4.161959648132324, loss=0.5533604621887207
I0301 11:08:05.617164 139589717059328 logging_writer.py:48] [306800] global_step=306800, grad_norm=4.649177551269531, loss=0.6042929887771606
I0301 11:08:39.590431 139589708666624 logging_writer.py:48] [306900] global_step=306900, grad_norm=4.434726715087891, loss=0.6330582499504089
I0301 11:09:13.568692 139589717059328 logging_writer.py:48] [307000] global_step=307000, grad_norm=4.4518208503723145, loss=0.579018235206604
I0301 11:09:47.532425 139589708666624 logging_writer.py:48] [307100] global_step=307100, grad_norm=4.397008419036865, loss=0.5600735545158386
I0301 11:10:21.495594 139589717059328 logging_writer.py:48] [307200] global_step=307200, grad_norm=4.530730724334717, loss=0.5834287405014038
I0301 11:10:55.451689 139589708666624 logging_writer.py:48] [307300] global_step=307300, grad_norm=4.253230571746826, loss=0.6101791858673096
I0301 11:11:29.470124 139589717059328 logging_writer.py:48] [307400] global_step=307400, grad_norm=4.859138011932373, loss=0.6339982151985168
I0301 11:12:03.430978 139589708666624 logging_writer.py:48] [307500] global_step=307500, grad_norm=4.455912113189697, loss=0.6360889673233032
I0301 11:12:37.394262 139589717059328 logging_writer.py:48] [307600] global_step=307600, grad_norm=4.350178241729736, loss=0.5870643258094788
I0301 11:13:11.335272 139589708666624 logging_writer.py:48] [307700] global_step=307700, grad_norm=4.60698127746582, loss=0.6691881418228149
I0301 11:13:45.265498 139589717059328 logging_writer.py:48] [307800] global_step=307800, grad_norm=4.227586269378662, loss=0.6606632471084595
I0301 11:14:19.225139 139589708666624 logging_writer.py:48] [307900] global_step=307900, grad_norm=4.184296607971191, loss=0.6303196549415588
I0301 11:14:53.176498 139589717059328 logging_writer.py:48] [308000] global_step=308000, grad_norm=4.146613121032715, loss=0.6242437362670898
I0301 11:15:21.856089 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:15:27.871750 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:15:36.210757 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:15:38.471398 139753105983296 submission_runner.py:411] Time since start: 108269.47s, 	Step: 308086, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14382977783679962, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0567333698272705, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.8203606605529785, 'test/num_examples': 10000, 'score': 104614.98310089111, 'total_duration': 108269.47462749481, 'accumulated_submission_time': 104614.98310089111, 'accumulated_eval_time': 3630.9186856746674, 'accumulated_logging_time': 12.757277011871338}
I0301 11:15:38.541839 139589725452032 logging_writer.py:48] [308086] accumulated_eval_time=3630.918686, accumulated_logging_time=12.757277, accumulated_submission_time=104614.983101, global_step=308086, preemption_count=0, score=104614.983101, test/accuracy=0.629400, test/loss=1.820361, test/num_examples=10000, total_duration=108269.474627, train/accuracy=0.961416, train/loss=0.143830, validation/accuracy=0.754840, validation/loss=1.056733, validation/num_examples=50000
I0301 11:15:43.618303 139590170035968 logging_writer.py:48] [308100] global_step=308100, grad_norm=4.862570285797119, loss=0.7053971290588379
I0301 11:16:17.548171 139589725452032 logging_writer.py:48] [308200] global_step=308200, grad_norm=5.070724010467529, loss=0.6914692521095276
I0301 11:16:51.519201 139590170035968 logging_writer.py:48] [308300] global_step=308300, grad_norm=4.952988624572754, loss=0.6259452700614929
I0301 11:17:25.551204 139589725452032 logging_writer.py:48] [308400] global_step=308400, grad_norm=4.736148834228516, loss=0.680543065071106
I0301 11:17:59.512032 139590170035968 logging_writer.py:48] [308500] global_step=308500, grad_norm=4.612627983093262, loss=0.6133902668952942
I0301 11:18:33.456151 139589725452032 logging_writer.py:48] [308600] global_step=308600, grad_norm=4.750888824462891, loss=0.6760348081588745
I0301 11:19:07.407035 139590170035968 logging_writer.py:48] [308700] global_step=308700, grad_norm=4.533961772918701, loss=0.5944027304649353
I0301 11:19:41.343008 139589725452032 logging_writer.py:48] [308800] global_step=308800, grad_norm=3.9666740894317627, loss=0.5424541234970093
I0301 11:20:15.270772 139590170035968 logging_writer.py:48] [308900] global_step=308900, grad_norm=4.268986225128174, loss=0.6685325503349304
I0301 11:20:49.215167 139589725452032 logging_writer.py:48] [309000] global_step=309000, grad_norm=4.599936485290527, loss=0.6621402502059937
I0301 11:21:23.188331 139590170035968 logging_writer.py:48] [309100] global_step=309100, grad_norm=4.557214260101318, loss=0.5764824151992798
I0301 11:21:57.139608 139589725452032 logging_writer.py:48] [309200] global_step=309200, grad_norm=4.619488716125488, loss=0.6712316274642944
I0301 11:22:31.093116 139590170035968 logging_writer.py:48] [309300] global_step=309300, grad_norm=4.325839996337891, loss=0.5778481364250183
I0301 11:23:05.031538 139589725452032 logging_writer.py:48] [309400] global_step=309400, grad_norm=4.191194534301758, loss=0.6086317300796509
I0301 11:23:39.065741 139590170035968 logging_writer.py:48] [309500] global_step=309500, grad_norm=4.811461448669434, loss=0.643078088760376
I0301 11:24:08.722280 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:24:14.740984 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:24:23.045573 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:24:25.315163 139753105983296 submission_runner.py:411] Time since start: 108796.32s, 	Step: 309589, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.147440105676651, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0556894540786743, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8195446729660034, 'test/num_examples': 10000, 'score': 105125.10056447983, 'total_duration': 108796.3183927536, 'accumulated_submission_time': 105125.10056447983, 'accumulated_eval_time': 3647.5115189552307, 'accumulated_logging_time': 12.838103294372559}
I0301 11:24:25.386115 139589717059328 logging_writer.py:48] [309589] accumulated_eval_time=3647.511519, accumulated_logging_time=12.838103, accumulated_submission_time=105125.100564, global_step=309589, preemption_count=0, score=105125.100564, test/accuracy=0.631000, test/loss=1.819545, test/num_examples=10000, total_duration=108796.318393, train/accuracy=0.960559, train/loss=0.147440, validation/accuracy=0.755040, validation/loss=1.055689, validation/num_examples=50000
I0301 11:24:29.480439 139590153250560 logging_writer.py:48] [309600] global_step=309600, grad_norm=4.127655982971191, loss=0.5635330080986023
I0301 11:25:03.355503 139589717059328 logging_writer.py:48] [309700] global_step=309700, grad_norm=4.897232532501221, loss=0.5670650005340576
I0301 11:25:37.279552 139590153250560 logging_writer.py:48] [309800] global_step=309800, grad_norm=4.347842216491699, loss=0.6366144418716431
I0301 11:26:11.211532 139589717059328 logging_writer.py:48] [309900] global_step=309900, grad_norm=4.684250831604004, loss=0.6492468118667603
I0301 11:26:45.196818 139590153250560 logging_writer.py:48] [310000] global_step=310000, grad_norm=4.917282581329346, loss=0.6019916534423828
I0301 11:27:19.140122 139589717059328 logging_writer.py:48] [310100] global_step=310100, grad_norm=4.6907525062561035, loss=0.7173933982849121
I0301 11:27:53.094383 139590153250560 logging_writer.py:48] [310200] global_step=310200, grad_norm=4.340658664703369, loss=0.5730916261672974
I0301 11:28:27.049159 139589717059328 logging_writer.py:48] [310300] global_step=310300, grad_norm=4.58438777923584, loss=0.5856803059577942
I0301 11:29:01.018298 139590153250560 logging_writer.py:48] [310400] global_step=310400, grad_norm=4.21505069732666, loss=0.5330518484115601
I0301 11:29:34.974718 139589717059328 logging_writer.py:48] [310500] global_step=310500, grad_norm=4.202101230621338, loss=0.5833165645599365
I0301 11:30:09.037074 139590153250560 logging_writer.py:48] [310600] global_step=310600, grad_norm=4.399425506591797, loss=0.5633492469787598
I0301 11:30:42.976368 139589717059328 logging_writer.py:48] [310700] global_step=310700, grad_norm=4.489243030548096, loss=0.6716870069503784
I0301 11:31:16.927675 139590153250560 logging_writer.py:48] [310800] global_step=310800, grad_norm=4.2840728759765625, loss=0.6098592877388
I0301 11:31:50.886309 139589717059328 logging_writer.py:48] [310900] global_step=310900, grad_norm=5.2194719314575195, loss=0.7010786533355713
I0301 11:32:24.831462 139590153250560 logging_writer.py:48] [311000] global_step=311000, grad_norm=4.721805095672607, loss=0.564732551574707
I0301 11:32:55.522846 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:33:01.569554 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:33:09.972503 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:33:12.241747 139753105983296 submission_runner.py:411] Time since start: 109323.24s, 	Step: 311092, 	{'train/accuracy': 0.9615154266357422, 'train/loss': 0.14495202898979187, 'validation/accuracy': 0.7554399967193604, 'validation/loss': 1.0552821159362793, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8196454048156738, 'test/num_examples': 10000, 'score': 105635.1733725071, 'total_duration': 109323.24497246742, 'accumulated_submission_time': 105635.1733725071, 'accumulated_eval_time': 3664.230366706848, 'accumulated_logging_time': 12.919663667678833}
I0301 11:33:12.312367 139589717059328 logging_writer.py:48] [311092] accumulated_eval_time=3664.230367, accumulated_logging_time=12.919664, accumulated_submission_time=105635.173373, global_step=311092, preemption_count=0, score=105635.173373, test/accuracy=0.630000, test/loss=1.819645, test/num_examples=10000, total_duration=109323.244972, train/accuracy=0.961515, train/loss=0.144952, validation/accuracy=0.755440, validation/loss=1.055282, validation/num_examples=50000
I0301 11:33:15.379684 139589725452032 logging_writer.py:48] [311100] global_step=311100, grad_norm=4.365855693817139, loss=0.6327678561210632
I0301 11:33:49.293135 139589717059328 logging_writer.py:48] [311200] global_step=311200, grad_norm=4.208761692047119, loss=0.579776406288147
I0301 11:34:23.218321 139589725452032 logging_writer.py:48] [311300] global_step=311300, grad_norm=4.499244213104248, loss=0.5655733942985535
I0301 11:34:57.175833 139589717059328 logging_writer.py:48] [311400] global_step=311400, grad_norm=5.224897861480713, loss=0.6179529428482056
I0301 11:35:31.119597 139589725452032 logging_writer.py:48] [311500] global_step=311500, grad_norm=4.518592834472656, loss=0.6503281593322754
I0301 11:36:05.153717 139589717059328 logging_writer.py:48] [311600] global_step=311600, grad_norm=4.668553352355957, loss=0.5745048522949219
I0301 11:36:39.101267 139589725452032 logging_writer.py:48] [311700] global_step=311700, grad_norm=4.487619876861572, loss=0.6433386206626892
I0301 11:37:13.072277 139589717059328 logging_writer.py:48] [311800] global_step=311800, grad_norm=4.719853401184082, loss=0.6965488791465759
I0301 11:37:47.046971 139589725452032 logging_writer.py:48] [311900] global_step=311900, grad_norm=4.659566879272461, loss=0.6893951296806335
I0301 11:38:21.015494 139589717059328 logging_writer.py:48] [312000] global_step=312000, grad_norm=4.420672416687012, loss=0.5635514259338379
I0301 11:38:54.977772 139589725452032 logging_writer.py:48] [312100] global_step=312100, grad_norm=4.598171234130859, loss=0.7101185321807861
I0301 11:39:28.927061 139589717059328 logging_writer.py:48] [312200] global_step=312200, grad_norm=4.709574222564697, loss=0.6399523615837097
I0301 11:40:02.909580 139589725452032 logging_writer.py:48] [312300] global_step=312300, grad_norm=4.667969226837158, loss=0.6263930797576904
I0301 11:40:36.829586 139589717059328 logging_writer.py:48] [312400] global_step=312400, grad_norm=4.459035396575928, loss=0.6087453961372375
I0301 11:41:10.772194 139589725452032 logging_writer.py:48] [312500] global_step=312500, grad_norm=5.041800498962402, loss=0.7261675596237183
I0301 11:41:42.511054 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:41:48.540031 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:41:56.829611 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:41:59.107509 139753105983296 submission_runner.py:411] Time since start: 109850.11s, 	Step: 312595, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.14550036191940308, 'validation/accuracy': 0.7548199892044067, 'validation/loss': 1.0551706552505493, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8177525997161865, 'test/num_examples': 10000, 'score': 106145.30757308006, 'total_duration': 109850.11073994637, 'accumulated_submission_time': 106145.30757308006, 'accumulated_eval_time': 3680.8267703056335, 'accumulated_logging_time': 13.000634670257568}
I0301 11:41:59.182193 139589717059328 logging_writer.py:48] [312595] accumulated_eval_time=3680.826770, accumulated_logging_time=13.000635, accumulated_submission_time=106145.307573, global_step=312595, preemption_count=0, score=106145.307573, test/accuracy=0.630800, test/loss=1.817753, test/num_examples=10000, total_duration=109850.110740, train/accuracy=0.960499, train/loss=0.145500, validation/accuracy=0.754820, validation/loss=1.055171, validation/num_examples=50000
I0301 11:42:01.221664 139590161643264 logging_writer.py:48] [312600] global_step=312600, grad_norm=4.885910511016846, loss=0.6428027153015137
I0301 11:42:35.267427 139589717059328 logging_writer.py:48] [312700] global_step=312700, grad_norm=4.034131050109863, loss=0.5495277643203735
I0301 11:43:09.178831 139590161643264 logging_writer.py:48] [312800] global_step=312800, grad_norm=4.688289165496826, loss=0.6612493991851807
I0301 11:43:43.159331 139589717059328 logging_writer.py:48] [312900] global_step=312900, grad_norm=4.529872417449951, loss=0.592108428478241
I0301 11:44:17.133449 139590161643264 logging_writer.py:48] [313000] global_step=313000, grad_norm=4.430996894836426, loss=0.5910899043083191
I0301 11:44:51.082540 139589717059328 logging_writer.py:48] [313100] global_step=313100, grad_norm=4.611146926879883, loss=0.6305727958679199
I0301 11:45:25.046627 139590161643264 logging_writer.py:48] [313200] global_step=313200, grad_norm=4.277066707611084, loss=0.5939042568206787
I0301 11:45:59.008991 139589717059328 logging_writer.py:48] [313300] global_step=313300, grad_norm=4.683011531829834, loss=0.5636371374130249
I0301 11:46:32.976789 139590161643264 logging_writer.py:48] [313400] global_step=313400, grad_norm=4.461544990539551, loss=0.6730296611785889
I0301 11:47:06.946602 139589717059328 logging_writer.py:48] [313500] global_step=313500, grad_norm=5.093850612640381, loss=0.6412481069564819
I0301 11:47:40.914066 139590161643264 logging_writer.py:48] [313600] global_step=313600, grad_norm=4.7885661125183105, loss=0.6033768057823181
I0301 11:48:14.961962 139589717059328 logging_writer.py:48] [313700] global_step=313700, grad_norm=5.081588268280029, loss=0.6664552688598633
I0301 11:48:48.941702 139590161643264 logging_writer.py:48] [313800] global_step=313800, grad_norm=4.455204010009766, loss=0.6392036080360413
I0301 11:49:22.907463 139589717059328 logging_writer.py:48] [313900] global_step=313900, grad_norm=4.516899108886719, loss=0.6125686764717102
I0301 11:49:56.870687 139590161643264 logging_writer.py:48] [314000] global_step=314000, grad_norm=4.319028854370117, loss=0.5961319804191589
I0301 11:50:29.299278 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:50:35.468234 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:50:43.705214 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:50:45.994702 139753105983296 submission_runner.py:411] Time since start: 110377.00s, 	Step: 314097, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.1466144472360611, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0555564165115356, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.818840742111206, 'test/num_examples': 10000, 'score': 106655.35900592804, 'total_duration': 110376.99791812897, 'accumulated_submission_time': 106655.35900592804, 'accumulated_eval_time': 3697.5221333503723, 'accumulated_logging_time': 13.088707447052002}
I0301 11:50:46.071796 139590170035968 logging_writer.py:48] [314097] accumulated_eval_time=3697.522133, accumulated_logging_time=13.088707, accumulated_submission_time=106655.359006, global_step=314097, preemption_count=0, score=106655.359006, test/accuracy=0.630600, test/loss=1.818841, test/num_examples=10000, total_duration=110376.997918, train/accuracy=0.960459, train/loss=0.146614, validation/accuracy=0.755040, validation/loss=1.055556, validation/num_examples=50000
I0301 11:50:47.447323 139590178428672 logging_writer.py:48] [314100] global_step=314100, grad_norm=4.359051704406738, loss=0.6608095765113831
I0301 11:51:21.358131 139590170035968 logging_writer.py:48] [314200] global_step=314200, grad_norm=4.6550822257995605, loss=0.574951708316803
I0301 11:51:55.301037 139590178428672 logging_writer.py:48] [314300] global_step=314300, grad_norm=4.32136344909668, loss=0.5998119115829468
I0301 11:52:29.245408 139590170035968 logging_writer.py:48] [314400] global_step=314400, grad_norm=4.272491455078125, loss=0.5981131792068481
I0301 11:53:03.170969 139590178428672 logging_writer.py:48] [314500] global_step=314500, grad_norm=4.791520595550537, loss=0.5944384336471558
I0301 11:53:37.104377 139590170035968 logging_writer.py:48] [314600] global_step=314600, grad_norm=4.260318279266357, loss=0.6109197735786438
I0301 11:54:11.029967 139590178428672 logging_writer.py:48] [314700] global_step=314700, grad_norm=4.140230178833008, loss=0.6109173893928528
I0301 11:54:45.041954 139590170035968 logging_writer.py:48] [314800] global_step=314800, grad_norm=4.299886703491211, loss=0.5861746072769165
I0301 11:55:19.008534 139590178428672 logging_writer.py:48] [314900] global_step=314900, grad_norm=4.469362735748291, loss=0.6231996417045593
I0301 11:55:52.959633 139590170035968 logging_writer.py:48] [315000] global_step=315000, grad_norm=4.145863056182861, loss=0.5468047261238098
I0301 11:56:26.886192 139590178428672 logging_writer.py:48] [315100] global_step=315100, grad_norm=4.268566608428955, loss=0.6174005270004272
I0301 11:57:00.837539 139590170035968 logging_writer.py:48] [315200] global_step=315200, grad_norm=4.433666229248047, loss=0.6196885704994202
I0301 11:57:34.766271 139590178428672 logging_writer.py:48] [315300] global_step=315300, grad_norm=4.549914836883545, loss=0.65605628490448
I0301 11:58:08.694686 139590170035968 logging_writer.py:48] [315400] global_step=315400, grad_norm=4.572461128234863, loss=0.5951183438301086
I0301 11:58:42.624998 139590178428672 logging_writer.py:48] [315500] global_step=315500, grad_norm=4.716640472412109, loss=0.6128539443016052
I0301 11:59:16.055701 139753105983296 spec.py:321] Evaluating on the training split.
I0301 11:59:22.041776 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 11:59:30.349025 139753105983296 spec.py:349] Evaluating on the test split.
I0301 11:59:32.632644 139753105983296 submission_runner.py:411] Time since start: 110903.64s, 	Step: 315600, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14607059955596924, 'validation/accuracy': 0.7547000050544739, 'validation/loss': 1.0562903881072998, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.820388913154602, 'test/num_examples': 10000, 'score': 107165.2802259922, 'total_duration': 110903.63587498665, 'accumulated_submission_time': 107165.2802259922, 'accumulated_eval_time': 3714.099026441574, 'accumulated_logging_time': 13.175846815109253}
I0301 11:59:32.701371 139590161643264 logging_writer.py:48] [315600] accumulated_eval_time=3714.099026, accumulated_logging_time=13.175847, accumulated_submission_time=107165.280226, global_step=315600, preemption_count=0, score=107165.280226, test/accuracy=0.631100, test/loss=1.820389, test/num_examples=10000, total_duration=110903.635875, train/accuracy=0.960539, train/loss=0.146071, validation/accuracy=0.754700, validation/loss=1.056290, validation/num_examples=50000
I0301 11:59:33.056252 139590178428672 logging_writer.py:48] [315600] global_step=315600, grad_norm=4.642123699188232, loss=0.6485971808433533
I0301 12:00:06.960026 139590161643264 logging_writer.py:48] [315700] global_step=315700, grad_norm=4.428014278411865, loss=0.6550856828689575
I0301 12:00:40.970705 139590178428672 logging_writer.py:48] [315800] global_step=315800, grad_norm=4.0889506340026855, loss=0.5689422488212585
I0301 12:01:14.943244 139590161643264 logging_writer.py:48] [315900] global_step=315900, grad_norm=4.44691276550293, loss=0.6599504351615906
I0301 12:01:48.867151 139590178428672 logging_writer.py:48] [316000] global_step=316000, grad_norm=4.734027862548828, loss=0.6260380744934082
I0301 12:02:22.813334 139590161643264 logging_writer.py:48] [316100] global_step=316100, grad_norm=4.566153049468994, loss=0.6192421317100525
I0301 12:02:56.735964 139590178428672 logging_writer.py:48] [316200] global_step=316200, grad_norm=4.684785842895508, loss=0.6800867319107056
I0301 12:03:30.702573 139590161643264 logging_writer.py:48] [316300] global_step=316300, grad_norm=3.994063377380371, loss=0.5214246511459351
I0301 12:04:04.641950 139590178428672 logging_writer.py:48] [316400] global_step=316400, grad_norm=4.176706314086914, loss=0.5917670130729675
I0301 12:04:38.607963 139590161643264 logging_writer.py:48] [316500] global_step=316500, grad_norm=4.197539329528809, loss=0.5286061763763428
I0301 12:05:12.536173 139590178428672 logging_writer.py:48] [316600] global_step=316600, grad_norm=5.330904483795166, loss=0.6685230135917664
I0301 12:05:46.507533 139590161643264 logging_writer.py:48] [316700] global_step=316700, grad_norm=4.761462688446045, loss=0.6028087735176086
I0301 12:06:20.426201 139590178428672 logging_writer.py:48] [316800] global_step=316800, grad_norm=4.202160835266113, loss=0.6154396533966064
I0301 12:06:54.541064 139590161643264 logging_writer.py:48] [316900] global_step=316900, grad_norm=4.511776924133301, loss=0.6411250829696655
I0301 12:07:28.513906 139590178428672 logging_writer.py:48] [317000] global_step=317000, grad_norm=4.637373447418213, loss=0.6041350364685059
I0301 12:08:02.481594 139590161643264 logging_writer.py:48] [317100] global_step=317100, grad_norm=4.07605504989624, loss=0.619914174079895
I0301 12:08:02.645413 139753105983296 spec.py:321] Evaluating on the training split.
I0301 12:08:08.720761 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 12:08:17.028653 139753105983296 spec.py:349] Evaluating on the test split.
I0301 12:08:19.304445 139753105983296 submission_runner.py:411] Time since start: 111430.31s, 	Step: 317102, 	{'train/accuracy': 0.9609972834587097, 'train/loss': 0.14397582411766052, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0555888414382935, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8175864219665527, 'test/num_examples': 10000, 'score': 107675.16096973419, 'total_duration': 111430.3076775074, 'accumulated_submission_time': 107675.16096973419, 'accumulated_eval_time': 3730.7580292224884, 'accumulated_logging_time': 13.25473165512085}
I0301 12:08:19.384244 139589708666624 logging_writer.py:48] [317102] accumulated_eval_time=3730.758029, accumulated_logging_time=13.254732, accumulated_submission_time=107675.160970, global_step=317102, preemption_count=0, score=107675.160970, test/accuracy=0.631000, test/loss=1.817586, test/num_examples=10000, total_duration=111430.307678, train/accuracy=0.960997, train/loss=0.143976, validation/accuracy=0.754840, validation/loss=1.055589, validation/num_examples=50000
I0301 12:08:52.942281 139589717059328 logging_writer.py:48] [317200] global_step=317200, grad_norm=4.892163276672363, loss=0.6643458604812622
I0301 12:09:26.833945 139589708666624 logging_writer.py:48] [317300] global_step=317300, grad_norm=4.1450419425964355, loss=0.6509392857551575
I0301 12:10:00.787734 139589717059328 logging_writer.py:48] [317400] global_step=317400, grad_norm=4.442380905151367, loss=0.5552836060523987
I0301 12:10:34.727392 139589708666624 logging_writer.py:48] [317500] global_step=317500, grad_norm=4.548356533050537, loss=0.6311010122299194
I0301 12:11:08.681714 139589717059328 logging_writer.py:48] [317600] global_step=317600, grad_norm=4.879207611083984, loss=0.6700791120529175
I0301 12:11:42.650447 139589708666624 logging_writer.py:48] [317700] global_step=317700, grad_norm=4.612504959106445, loss=0.6735276579856873
I0301 12:12:16.593645 139589717059328 logging_writer.py:48] [317800] global_step=317800, grad_norm=4.497043132781982, loss=0.6179057955741882
I0301 12:12:50.525439 139589708666624 logging_writer.py:48] [317900] global_step=317900, grad_norm=4.38590669631958, loss=0.586686909198761
I0301 12:13:24.581286 139589717059328 logging_writer.py:48] [318000] global_step=318000, grad_norm=4.907556533813477, loss=0.738355278968811
I0301 12:13:58.541168 139589708666624 logging_writer.py:48] [318100] global_step=318100, grad_norm=5.068277359008789, loss=0.6772171258926392
I0301 12:14:32.464883 139589717059328 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.503885269165039, loss=0.5900583267211914
I0301 12:15:06.397013 139589708666624 logging_writer.py:48] [318300] global_step=318300, grad_norm=5.115860939025879, loss=0.6623965501785278
I0301 12:15:40.342004 139589717059328 logging_writer.py:48] [318400] global_step=318400, grad_norm=4.6195220947265625, loss=0.6384906768798828
I0301 12:16:14.307898 139589708666624 logging_writer.py:48] [318500] global_step=318500, grad_norm=4.3985819816589355, loss=0.5843786001205444
I0301 12:16:48.242235 139589717059328 logging_writer.py:48] [318600] global_step=318600, grad_norm=4.5822834968566895, loss=0.7158265709877014
I0301 12:16:49.390393 139753105983296 spec.py:321] Evaluating on the training split.
I0301 12:16:55.529534 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 12:17:03.809908 139753105983296 spec.py:349] Evaluating on the test split.
I0301 12:17:06.118613 139753105983296 submission_runner.py:411] Time since start: 111957.12s, 	Step: 318605, 	{'train/accuracy': 0.9608378410339355, 'train/loss': 0.1478148102760315, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.056528925895691, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8220266103744507, 'test/num_examples': 10000, 'score': 108185.10398316383, 'total_duration': 111957.12183737755, 'accumulated_submission_time': 108185.10398316383, 'accumulated_eval_time': 3747.4862003326416, 'accumulated_logging_time': 13.34488844871521}
I0301 12:17:06.185874 139590186821376 logging_writer.py:48] [318605] accumulated_eval_time=3747.486200, accumulated_logging_time=13.344888, accumulated_submission_time=108185.103983, global_step=318605, preemption_count=0, score=108185.103983, test/accuracy=0.630000, test/loss=1.822027, test/num_examples=10000, total_duration=111957.121837, train/accuracy=0.960838, train/loss=0.147815, validation/accuracy=0.754880, validation/loss=1.056529, validation/num_examples=50000
I0301 12:17:38.792179 139590195214080 logging_writer.py:48] [318700] global_step=318700, grad_norm=4.554542064666748, loss=0.6728677749633789
I0301 12:18:12.728209 139590186821376 logging_writer.py:48] [318800] global_step=318800, grad_norm=4.382538795471191, loss=0.572691023349762
I0301 12:18:46.654337 139590195214080 logging_writer.py:48] [318900] global_step=318900, grad_norm=4.805075645446777, loss=0.7169289588928223
I0301 12:19:20.754856 139590186821376 logging_writer.py:48] [319000] global_step=319000, grad_norm=4.329330921173096, loss=0.6196047067642212
I0301 12:19:54.721596 139590195214080 logging_writer.py:48] [319100] global_step=319100, grad_norm=4.580333709716797, loss=0.5807710289955139
I0301 12:20:28.678406 139590186821376 logging_writer.py:48] [319200] global_step=319200, grad_norm=4.421289443969727, loss=0.6697440147399902
I0301 12:21:02.640401 139590195214080 logging_writer.py:48] [319300] global_step=319300, grad_norm=4.483067512512207, loss=0.6206912398338318
I0301 12:21:36.612324 139590186821376 logging_writer.py:48] [319400] global_step=319400, grad_norm=5.126794815063477, loss=0.6278388500213623
I0301 12:22:10.581908 139590195214080 logging_writer.py:48] [319500] global_step=319500, grad_norm=4.681538105010986, loss=0.6454672813415527
I0301 12:22:44.537054 139590186821376 logging_writer.py:48] [319600] global_step=319600, grad_norm=4.9736175537109375, loss=0.6761189699172974
I0301 12:23:18.473514 139590195214080 logging_writer.py:48] [319700] global_step=319700, grad_norm=4.855915069580078, loss=0.6683545708656311
I0301 12:23:52.454941 139590186821376 logging_writer.py:48] [319800] global_step=319800, grad_norm=4.622559547424316, loss=0.6437622308731079
I0301 12:24:26.395557 139590195214080 logging_writer.py:48] [319900] global_step=319900, grad_norm=4.5000176429748535, loss=0.6442986130714417
I0301 12:25:00.361686 139590186821376 logging_writer.py:48] [320000] global_step=320000, grad_norm=4.64084529876709, loss=0.6249524354934692
I0301 12:25:34.482549 139590195214080 logging_writer.py:48] [320100] global_step=320100, grad_norm=4.223118782043457, loss=0.6133301258087158
I0301 12:25:36.327272 139753105983296 spec.py:321] Evaluating on the training split.
I0301 12:25:42.364369 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 12:25:50.735990 139753105983296 spec.py:349] Evaluating on the test split.
I0301 12:25:53.028455 139753105983296 submission_runner.py:411] Time since start: 112484.03s, 	Step: 320107, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14673425257205963, 'validation/accuracy': 0.7544999718666077, 'validation/loss': 1.055534839630127, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8197250366210938, 'test/num_examples': 10000, 'score': 108695.18180251122, 'total_duration': 112484.0316901207, 'accumulated_submission_time': 108695.18180251122, 'accumulated_eval_time': 3764.187334537506, 'accumulated_logging_time': 13.422353982925415}
I0301 12:25:53.101797 139589708666624 logging_writer.py:48] [320107] accumulated_eval_time=3764.187335, accumulated_logging_time=13.422354, accumulated_submission_time=108695.181803, global_step=320107, preemption_count=0, score=108695.181803, test/accuracy=0.630000, test/loss=1.819725, test/num_examples=10000, total_duration=112484.031690, train/accuracy=0.961117, train/loss=0.146734, validation/accuracy=0.754500, validation/loss=1.055535, validation/num_examples=50000
I0301 12:26:24.946298 139589717059328 logging_writer.py:48] [320200] global_step=320200, grad_norm=4.858447551727295, loss=0.7136049270629883
I0301 12:26:58.872136 139589708666624 logging_writer.py:48] [320300] global_step=320300, grad_norm=4.390007019042969, loss=0.5865672826766968
I0301 12:27:32.780143 139589717059328 logging_writer.py:48] [320400] global_step=320400, grad_norm=4.70072078704834, loss=0.6066725254058838
I0301 12:28:06.686663 139589708666624 logging_writer.py:48] [320500] global_step=320500, grad_norm=4.424073696136475, loss=0.6615808010101318
I0301 12:28:40.637586 139589717059328 logging_writer.py:48] [320600] global_step=320600, grad_norm=4.418983459472656, loss=0.6169336438179016
I0301 12:29:14.595694 139589708666624 logging_writer.py:48] [320700] global_step=320700, grad_norm=5.007519721984863, loss=0.6293521523475647
I0301 12:29:48.529913 139589717059328 logging_writer.py:48] [320800] global_step=320800, grad_norm=4.494737148284912, loss=0.6282806396484375
I0301 12:30:22.468684 139589708666624 logging_writer.py:48] [320900] global_step=320900, grad_norm=4.455681800842285, loss=0.6193296313285828
I0301 12:30:56.432169 139589717059328 logging_writer.py:48] [321000] global_step=321000, grad_norm=4.469502925872803, loss=0.6164135932922363
I0301 12:31:30.458181 139589708666624 logging_writer.py:48] [321100] global_step=321100, grad_norm=4.600615501403809, loss=0.7384667992591858
I0301 12:32:04.425125 139589717059328 logging_writer.py:48] [321200] global_step=321200, grad_norm=4.545538425445557, loss=0.6496896743774414
I0301 12:32:38.363332 139589708666624 logging_writer.py:48] [321300] global_step=321300, grad_norm=4.869739532470703, loss=0.657687246799469
I0301 12:33:12.311453 139589717059328 logging_writer.py:48] [321400] global_step=321400, grad_norm=4.580707550048828, loss=0.6133389472961426
I0301 12:33:46.275753 139589708666624 logging_writer.py:48] [321500] global_step=321500, grad_norm=4.270568370819092, loss=0.6253185272216797
I0301 12:34:20.208912 139589717059328 logging_writer.py:48] [321600] global_step=321600, grad_norm=4.708189010620117, loss=0.6675350666046143
I0301 12:34:23.072164 139753105983296 spec.py:321] Evaluating on the training split.
I0301 12:34:29.079915 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 12:34:37.388164 139753105983296 spec.py:349] Evaluating on the test split.
I0301 12:34:39.670238 139753105983296 submission_runner.py:411] Time since start: 113010.67s, 	Step: 321610, 	{'train/accuracy': 0.9595623016357422, 'train/loss': 0.14896921813488007, 'validation/accuracy': 0.7551999688148499, 'validation/loss': 1.0555588006973267, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8181936740875244, 'test/num_examples': 10000, 'score': 109205.0882525444, 'total_duration': 113010.67347025871, 'accumulated_submission_time': 109205.0882525444, 'accumulated_eval_time': 3780.785356760025, 'accumulated_logging_time': 13.505805730819702}
I0301 12:34:39.742182 139589717059328 logging_writer.py:48] [321610] accumulated_eval_time=3780.785357, accumulated_logging_time=13.505806, accumulated_submission_time=109205.088253, global_step=321610, preemption_count=0, score=109205.088253, test/accuracy=0.630100, test/loss=1.818194, test/num_examples=10000, total_duration=113010.673470, train/accuracy=0.959562, train/loss=0.148969, validation/accuracy=0.755200, validation/loss=1.055559, validation/num_examples=50000
I0301 12:35:10.623313 139590170035968 logging_writer.py:48] [321700] global_step=321700, grad_norm=4.52276086807251, loss=0.5814782977104187
I0301 12:35:44.549198 139589717059328 logging_writer.py:48] [321800] global_step=321800, grad_norm=4.4388203620910645, loss=0.6003831624984741
I0301 12:36:18.528400 139590170035968 logging_writer.py:48] [321900] global_step=321900, grad_norm=4.227534770965576, loss=0.5549988150596619
I0301 12:36:52.491196 139589717059328 logging_writer.py:48] [322000] global_step=322000, grad_norm=4.581529140472412, loss=0.6718258857727051
I0301 12:37:26.486488 139590170035968 logging_writer.py:48] [322100] global_step=322100, grad_norm=5.0079569816589355, loss=0.6025592088699341
I0301 12:38:00.514121 139589717059328 logging_writer.py:48] [322200] global_step=322200, grad_norm=4.467868328094482, loss=0.629564642906189
I0301 12:38:34.476595 139590170035968 logging_writer.py:48] [322300] global_step=322300, grad_norm=4.434221267700195, loss=0.5911705493927002
I0301 12:39:08.444677 139589717059328 logging_writer.py:48] [322400] global_step=322400, grad_norm=4.521470546722412, loss=0.6987066864967346
I0301 12:39:42.416682 139590170035968 logging_writer.py:48] [322500] global_step=322500, grad_norm=4.8924713134765625, loss=0.6547757983207703
I0301 12:40:16.382889 139589717059328 logging_writer.py:48] [322600] global_step=322600, grad_norm=4.691053867340088, loss=0.6821058392524719
I0301 12:40:50.370601 139590170035968 logging_writer.py:48] [322700] global_step=322700, grad_norm=4.260876178741455, loss=0.6045083999633789
I0301 12:41:24.347327 139589717059328 logging_writer.py:48] [322800] global_step=322800, grad_norm=4.276057243347168, loss=0.5363080501556396
I0301 12:41:58.314650 139590170035968 logging_writer.py:48] [322900] global_step=322900, grad_norm=4.969973087310791, loss=0.630686342716217
I0301 12:42:32.274350 139589717059328 logging_writer.py:48] [323000] global_step=323000, grad_norm=4.961698055267334, loss=0.6193493604660034
I0301 12:43:06.259164 139590170035968 logging_writer.py:48] [323100] global_step=323100, grad_norm=4.82952356338501, loss=0.634564995765686
I0301 12:43:09.796668 139753105983296 spec.py:321] Evaluating on the training split.
I0301 12:43:15.867907 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 12:43:24.148240 139753105983296 spec.py:349] Evaluating on the test split.
I0301 12:43:26.416036 139753105983296 submission_runner.py:411] Time since start: 113537.42s, 	Step: 323112, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14476008713245392, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0555732250213623, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8190803527832031, 'test/num_examples': 10000, 'score': 109715.07874393463, 'total_duration': 113537.41925096512, 'accumulated_submission_time': 109715.07874393463, 'accumulated_eval_time': 3797.4046709537506, 'accumulated_logging_time': 13.588683843612671}
I0301 12:43:26.484762 139590144857856 logging_writer.py:48] [323112] accumulated_eval_time=3797.404671, accumulated_logging_time=13.588684, accumulated_submission_time=109715.078744, global_step=323112, preemption_count=0, score=109715.078744, test/accuracy=0.630100, test/loss=1.819080, test/num_examples=10000, total_duration=113537.419251, train/accuracy=0.960658, train/loss=0.144760, validation/accuracy=0.754900, validation/loss=1.055573, validation/num_examples=50000
I0301 12:43:56.728972 139590153250560 logging_writer.py:48] [323200] global_step=323200, grad_norm=4.361828327178955, loss=0.5890151262283325
I0301 12:44:30.688564 139590144857856 logging_writer.py:48] [323300] global_step=323300, grad_norm=4.515459060668945, loss=0.6243224143981934
I0301 12:45:04.641463 139590153250560 logging_writer.py:48] [323400] global_step=323400, grad_norm=4.2148756980896, loss=0.6327394247055054
I0301 12:45:38.615475 139590144857856 logging_writer.py:48] [323500] global_step=323500, grad_norm=4.095170497894287, loss=0.5840763449668884
I0301 12:46:12.547330 139590153250560 logging_writer.py:48] [323600] global_step=323600, grad_norm=4.385571002960205, loss=0.643444299697876
I0301 12:46:46.476266 139590144857856 logging_writer.py:48] [323700] global_step=323700, grad_norm=4.293613433837891, loss=0.6098436117172241
I0301 12:47:20.441691 139590153250560 logging_writer.py:48] [323800] global_step=323800, grad_norm=4.665775299072266, loss=0.6685463190078735
I0301 12:47:54.395091 139590144857856 logging_writer.py:48] [323900] global_step=323900, grad_norm=4.676974773406982, loss=0.5255253314971924
I0301 12:48:28.365190 139590153250560 logging_writer.py:48] [324000] global_step=324000, grad_norm=4.884424209594727, loss=0.6505995392799377
I0301 12:49:02.325252 139590144857856 logging_writer.py:48] [324100] global_step=324100, grad_norm=4.409693241119385, loss=0.6510458588600159
I0301 12:49:36.267071 139590153250560 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.367233753204346, loss=0.5847453474998474
I0301 12:50:10.361521 139590144857856 logging_writer.py:48] [324300] global_step=324300, grad_norm=4.809767246246338, loss=0.6585035920143127
I0301 12:50:44.320255 139590153250560 logging_writer.py:48] [324400] global_step=324400, grad_norm=4.902720928192139, loss=0.680118203163147
I0301 12:51:18.239660 139590144857856 logging_writer.py:48] [324500] global_step=324500, grad_norm=4.295259952545166, loss=0.6427482962608337
I0301 12:51:52.207677 139590153250560 logging_writer.py:48] [324600] global_step=324600, grad_norm=4.778590679168701, loss=0.7236830592155457
I0301 12:51:56.441962 139753105983296 spec.py:321] Evaluating on the training split.
I0301 12:52:02.524058 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 12:52:10.849937 139753105983296 spec.py:349] Evaluating on the test split.
I0301 12:52:13.115012 139753105983296 submission_runner.py:411] Time since start: 114064.12s, 	Step: 324614, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.1485663652420044, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.055357575416565, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8191184997558594, 'test/num_examples': 10000, 'score': 110224.97239756584, 'total_duration': 114064.11824631691, 'accumulated_submission_time': 110224.97239756584, 'accumulated_eval_time': 3814.077670812607, 'accumulated_logging_time': 13.667658567428589}
I0301 12:52:13.191225 139589717059328 logging_writer.py:48] [324614] accumulated_eval_time=3814.077671, accumulated_logging_time=13.667659, accumulated_submission_time=110224.972398, global_step=324614, preemption_count=0, score=110224.972398, test/accuracy=0.630500, test/loss=1.819118, test/num_examples=10000, total_duration=114064.118246, train/accuracy=0.960081, train/loss=0.148566, validation/accuracy=0.754760, validation/loss=1.055358, validation/num_examples=50000
I0301 12:52:42.685540 139589725452032 logging_writer.py:48] [324700] global_step=324700, grad_norm=4.489307403564453, loss=0.6081466674804688
I0301 12:53:16.621280 139589717059328 logging_writer.py:48] [324800] global_step=324800, grad_norm=4.680655479431152, loss=0.6510447263717651
I0301 12:53:50.575578 139589725452032 logging_writer.py:48] [324900] global_step=324900, grad_norm=5.072727203369141, loss=0.6357805728912354
I0301 12:54:24.557083 139589717059328 logging_writer.py:48] [325000] global_step=325000, grad_norm=4.3164520263671875, loss=0.607212245464325
I0301 12:54:58.535648 139589725452032 logging_writer.py:48] [325100] global_step=325100, grad_norm=4.5198140144348145, loss=0.6099593043327332
I0301 12:55:32.497538 139589717059328 logging_writer.py:48] [325200] global_step=325200, grad_norm=5.0739264488220215, loss=0.6549484729766846
I0301 12:56:06.557686 139589725452032 logging_writer.py:48] [325300] global_step=325300, grad_norm=4.3222832679748535, loss=0.5306863784790039
I0301 12:56:40.538872 139589717059328 logging_writer.py:48] [325400] global_step=325400, grad_norm=4.475063323974609, loss=0.6957173347473145
I0301 12:57:14.499459 139589725452032 logging_writer.py:48] [325500] global_step=325500, grad_norm=4.617097854614258, loss=0.6494637727737427
I0301 12:57:48.473667 139589717059328 logging_writer.py:48] [325600] global_step=325600, grad_norm=4.320603370666504, loss=0.5752745270729065
I0301 12:58:22.434179 139589725452032 logging_writer.py:48] [325700] global_step=325700, grad_norm=4.4193549156188965, loss=0.6830295324325562
I0301 12:58:56.370930 139589717059328 logging_writer.py:48] [325800] global_step=325800, grad_norm=4.65273904800415, loss=0.614779531955719
I0301 12:59:30.356812 139589725452032 logging_writer.py:48] [325900] global_step=325900, grad_norm=4.590580463409424, loss=0.6281876564025879
I0301 13:00:04.285362 139589717059328 logging_writer.py:48] [326000] global_step=326000, grad_norm=4.662740230560303, loss=0.6578330397605896
I0301 13:00:38.236451 139589725452032 logging_writer.py:48] [326100] global_step=326100, grad_norm=4.505796432495117, loss=0.5989297032356262
I0301 13:00:43.122411 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:00:49.118047 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:00:57.453390 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:00:59.700465 139753105983296 submission_runner.py:411] Time since start: 114590.70s, 	Step: 326116, 	{'train/accuracy': 0.9595822691917419, 'train/loss': 0.14804087579250336, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.055404782295227, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8191167116165161, 'test/num_examples': 10000, 'score': 110734.83861660957, 'total_duration': 114590.7036960125, 'accumulated_submission_time': 110734.83861660957, 'accumulated_eval_time': 3830.655671596527, 'accumulated_logging_time': 13.755608558654785}
I0301 13:00:59.771318 139589708666624 logging_writer.py:48] [326116] accumulated_eval_time=3830.655672, accumulated_logging_time=13.755609, accumulated_submission_time=110734.838617, global_step=326116, preemption_count=0, score=110734.838617, test/accuracy=0.630400, test/loss=1.819117, test/num_examples=10000, total_duration=114590.703696, train/accuracy=0.959582, train/loss=0.148041, validation/accuracy=0.754860, validation/loss=1.055405, validation/num_examples=50000
I0301 13:01:28.606868 139589717059328 logging_writer.py:48] [326200] global_step=326200, grad_norm=5.242364883422852, loss=0.687227725982666
I0301 13:02:02.547393 139589708666624 logging_writer.py:48] [326300] global_step=326300, grad_norm=4.500059604644775, loss=0.625713586807251
I0301 13:02:36.579608 139589717059328 logging_writer.py:48] [326400] global_step=326400, grad_norm=4.388049602508545, loss=0.5581121444702148
I0301 13:03:10.508481 139589708666624 logging_writer.py:48] [326500] global_step=326500, grad_norm=4.285135269165039, loss=0.5621544122695923
I0301 13:03:44.442598 139589717059328 logging_writer.py:48] [326600] global_step=326600, grad_norm=4.2766008377075195, loss=0.48534879088401794
I0301 13:04:18.383419 139589708666624 logging_writer.py:48] [326700] global_step=326700, grad_norm=4.098321914672852, loss=0.5136809349060059
I0301 13:04:52.341085 139589717059328 logging_writer.py:48] [326800] global_step=326800, grad_norm=4.427433013916016, loss=0.5779629349708557
I0301 13:05:26.326130 139589708666624 logging_writer.py:48] [326900] global_step=326900, grad_norm=4.2588887214660645, loss=0.5516618490219116
I0301 13:06:00.282546 139589717059328 logging_writer.py:48] [327000] global_step=327000, grad_norm=4.488790988922119, loss=0.6437305808067322
I0301 13:06:34.236403 139589708666624 logging_writer.py:48] [327100] global_step=327100, grad_norm=4.500253677368164, loss=0.5811488032341003
I0301 13:07:08.185354 139589717059328 logging_writer.py:48] [327200] global_step=327200, grad_norm=4.471536159515381, loss=0.5743820071220398
I0301 13:07:42.449910 139589708666624 logging_writer.py:48] [327300] global_step=327300, grad_norm=4.3653564453125, loss=0.6049577593803406
I0301 13:08:16.390658 139589717059328 logging_writer.py:48] [327400] global_step=327400, grad_norm=4.672766208648682, loss=0.5939002633094788
I0301 13:08:50.418837 139589708666624 logging_writer.py:48] [327500] global_step=327500, grad_norm=4.373144149780273, loss=0.6139822006225586
I0301 13:09:24.361122 139589717059328 logging_writer.py:48] [327600] global_step=327600, grad_norm=4.287039756774902, loss=0.5631225109100342
I0301 13:09:29.934933 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:09:36.081907 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:09:44.296546 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:09:46.589856 139753105983296 submission_runner.py:411] Time since start: 115117.59s, 	Step: 327618, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14778195321559906, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0543818473815918, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8186113834381104, 'test/num_examples': 10000, 'score': 111244.93863582611, 'total_duration': 115117.5930685997, 'accumulated_submission_time': 111244.93863582611, 'accumulated_eval_time': 3847.3105340003967, 'accumulated_logging_time': 13.836986780166626}
I0301 13:09:46.662272 139589708666624 logging_writer.py:48] [327618] accumulated_eval_time=3847.310534, accumulated_logging_time=13.836987, accumulated_submission_time=111244.938636, global_step=327618, preemption_count=0, score=111244.938636, test/accuracy=0.630000, test/loss=1.818611, test/num_examples=10000, total_duration=115117.593069, train/accuracy=0.961356, train/loss=0.147782, validation/accuracy=0.754980, validation/loss=1.054382, validation/num_examples=50000
I0301 13:10:14.818143 139589717059328 logging_writer.py:48] [327700] global_step=327700, grad_norm=4.4574761390686035, loss=0.671917200088501
I0301 13:10:48.759358 139589708666624 logging_writer.py:48] [327800] global_step=327800, grad_norm=4.28163480758667, loss=0.5575703978538513
I0301 13:11:22.706526 139589717059328 logging_writer.py:48] [327900] global_step=327900, grad_norm=5.018852710723877, loss=0.6764561533927917
I0301 13:11:56.664832 139589708666624 logging_writer.py:48] [328000] global_step=328000, grad_norm=4.529715061187744, loss=0.6598358154296875
I0301 13:12:30.645661 139589717059328 logging_writer.py:48] [328100] global_step=328100, grad_norm=4.949714660644531, loss=0.6126416921615601
I0301 13:13:04.599889 139589708666624 logging_writer.py:48] [328200] global_step=328200, grad_norm=4.592005252838135, loss=0.6122139692306519
I0301 13:13:38.559497 139589717059328 logging_writer.py:48] [328300] global_step=328300, grad_norm=4.55061149597168, loss=0.6478144526481628
I0301 13:14:12.523758 139589708666624 logging_writer.py:48] [328400] global_step=328400, grad_norm=4.354013442993164, loss=0.645075261592865
I0301 13:14:46.538110 139589717059328 logging_writer.py:48] [328500] global_step=328500, grad_norm=4.305087566375732, loss=0.5975203514099121
I0301 13:15:20.499760 139589708666624 logging_writer.py:48] [328600] global_step=328600, grad_norm=4.523380279541016, loss=0.6491208076477051
I0301 13:15:54.465038 139589717059328 logging_writer.py:48] [328700] global_step=328700, grad_norm=4.667985916137695, loss=0.6570116877555847
I0301 13:16:28.436074 139589708666624 logging_writer.py:48] [328800] global_step=328800, grad_norm=4.662254810333252, loss=0.6427434682846069
I0301 13:17:02.402795 139589717059328 logging_writer.py:48] [328900] global_step=328900, grad_norm=4.644317150115967, loss=0.6263423562049866
I0301 13:17:36.348912 139589708666624 logging_writer.py:48] [329000] global_step=329000, grad_norm=4.491535186767578, loss=0.5595481991767883
I0301 13:18:10.323613 139589717059328 logging_writer.py:48] [329100] global_step=329100, grad_norm=5.034070014953613, loss=0.724000096321106
I0301 13:18:16.898060 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:18:22.879176 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:18:31.163069 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:18:33.443966 139753105983296 submission_runner.py:411] Time since start: 115644.45s, 	Step: 329121, 	{'train/accuracy': 0.9593630433082581, 'train/loss': 0.1508469581604004, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.055037498474121, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8170280456542969, 'test/num_examples': 10000, 'score': 111755.11136102676, 'total_duration': 115644.44719862938, 'accumulated_submission_time': 111755.11136102676, 'accumulated_eval_time': 3863.8564035892487, 'accumulated_logging_time': 13.919602155685425}
I0301 13:18:33.521673 139590178428672 logging_writer.py:48] [329121] accumulated_eval_time=3863.856404, accumulated_logging_time=13.919602, accumulated_submission_time=111755.111361, global_step=329121, preemption_count=0, score=111755.111361, test/accuracy=0.629900, test/loss=1.817028, test/num_examples=10000, total_duration=115644.447199, train/accuracy=0.959363, train/loss=0.150847, validation/accuracy=0.754980, validation/loss=1.055037, validation/num_examples=50000
I0301 13:19:00.649728 139590186821376 logging_writer.py:48] [329200] global_step=329200, grad_norm=4.42170524597168, loss=0.6469875574111938
I0301 13:19:34.592478 139590178428672 logging_writer.py:48] [329300] global_step=329300, grad_norm=4.294109344482422, loss=0.6745327711105347
I0301 13:20:08.541627 139590186821376 logging_writer.py:48] [329400] global_step=329400, grad_norm=4.457993030548096, loss=0.6290841102600098
I0301 13:20:42.519822 139590178428672 logging_writer.py:48] [329500] global_step=329500, grad_norm=4.668226718902588, loss=0.670927882194519
I0301 13:21:16.537687 139590186821376 logging_writer.py:48] [329600] global_step=329600, grad_norm=4.797455787658691, loss=0.6028699278831482
I0301 13:21:50.499249 139590178428672 logging_writer.py:48] [329700] global_step=329700, grad_norm=4.473568439483643, loss=0.5999580025672913
I0301 13:22:24.421241 139590186821376 logging_writer.py:48] [329800] global_step=329800, grad_norm=4.602670669555664, loss=0.6916443705558777
I0301 13:22:58.388315 139590178428672 logging_writer.py:48] [329900] global_step=329900, grad_norm=4.451037883758545, loss=0.6277570128440857
I0301 13:23:32.343974 139590186821376 logging_writer.py:48] [330000] global_step=330000, grad_norm=4.204299449920654, loss=0.5627341866493225
I0301 13:24:06.268422 139590178428672 logging_writer.py:48] [330100] global_step=330100, grad_norm=4.320463180541992, loss=0.6087973117828369
I0301 13:24:40.212412 139590186821376 logging_writer.py:48] [330200] global_step=330200, grad_norm=4.191807270050049, loss=0.5732953548431396
I0301 13:25:14.163908 139590178428672 logging_writer.py:48] [330300] global_step=330300, grad_norm=4.1471123695373535, loss=0.5747551918029785
I0301 13:25:48.118377 139590186821376 logging_writer.py:48] [330400] global_step=330400, grad_norm=4.129161357879639, loss=0.5011483430862427
I0301 13:26:22.067968 139590178428672 logging_writer.py:48] [330500] global_step=330500, grad_norm=4.887811660766602, loss=0.6421849131584167
I0301 13:26:56.025417 139590186821376 logging_writer.py:48] [330600] global_step=330600, grad_norm=5.851085662841797, loss=0.6855693459510803
I0301 13:27:03.707959 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:27:09.727103 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:27:17.984756 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:27:20.291489 139753105983296 submission_runner.py:411] Time since start: 116171.29s, 	Step: 330624, 	{'train/accuracy': 0.961336076259613, 'train/loss': 0.1432124525308609, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.0553230047225952, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8186101913452148, 'test/num_examples': 10000, 'score': 112265.23484659195, 'total_duration': 116171.29470396042, 'accumulated_submission_time': 112265.23484659195, 'accumulated_eval_time': 3880.439862012863, 'accumulated_logging_time': 14.007933139801025}
I0301 13:27:20.366040 139590144857856 logging_writer.py:48] [330624] accumulated_eval_time=3880.439862, accumulated_logging_time=14.007933, accumulated_submission_time=112265.234847, global_step=330624, preemption_count=0, score=112265.234847, test/accuracy=0.630500, test/loss=1.818610, test/num_examples=10000, total_duration=116171.294704, train/accuracy=0.961336, train/loss=0.143212, validation/accuracy=0.754560, validation/loss=1.055323, validation/num_examples=50000
I0301 13:27:46.492508 139590153250560 logging_writer.py:48] [330700] global_step=330700, grad_norm=4.441886901855469, loss=0.5665959715843201
I0301 13:28:20.384719 139590144857856 logging_writer.py:48] [330800] global_step=330800, grad_norm=4.445451736450195, loss=0.6630131006240845
I0301 13:28:54.303840 139590153250560 logging_writer.py:48] [330900] global_step=330900, grad_norm=5.12943696975708, loss=0.7419308423995972
I0301 13:29:28.257292 139590144857856 logging_writer.py:48] [331000] global_step=331000, grad_norm=4.7408318519592285, loss=0.6995487809181213
I0301 13:30:02.195599 139590153250560 logging_writer.py:48] [331100] global_step=331100, grad_norm=4.149785041809082, loss=0.609625518321991
I0301 13:30:36.163223 139590144857856 logging_writer.py:48] [331200] global_step=331200, grad_norm=4.67066764831543, loss=0.5856329202651978
I0301 13:31:10.121729 139590153250560 logging_writer.py:48] [331300] global_step=331300, grad_norm=4.023396968841553, loss=0.5799275636672974
I0301 13:31:44.063134 139590144857856 logging_writer.py:48] [331400] global_step=331400, grad_norm=4.353947639465332, loss=0.6441066265106201
I0301 13:32:18.015102 139590153250560 logging_writer.py:48] [331500] global_step=331500, grad_norm=4.254154682159424, loss=0.5393471121788025
I0301 13:32:51.963871 139590144857856 logging_writer.py:48] [331600] global_step=331600, grad_norm=4.946590900421143, loss=0.5897904634475708
I0301 13:33:26.016795 139590153250560 logging_writer.py:48] [331700] global_step=331700, grad_norm=4.102004528045654, loss=0.5791366696357727
I0301 13:33:59.976116 139590144857856 logging_writer.py:48] [331800] global_step=331800, grad_norm=4.430948257446289, loss=0.5751157402992249
I0301 13:34:33.914385 139590153250560 logging_writer.py:48] [331900] global_step=331900, grad_norm=4.324338436126709, loss=0.5940650701522827
I0301 13:35:07.878699 139590144857856 logging_writer.py:48] [332000] global_step=332000, grad_norm=4.758174419403076, loss=0.6421605944633484
I0301 13:35:41.809934 139590153250560 logging_writer.py:48] [332100] global_step=332100, grad_norm=4.632113456726074, loss=0.6492534279823303
I0301 13:35:50.451297 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:35:56.445502 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:36:04.854644 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:36:07.128735 139753105983296 submission_runner.py:411] Time since start: 116698.13s, 	Step: 332127, 	{'train/accuracy': 0.9591836333274841, 'train/loss': 0.15074175596237183, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0550819635391235, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8171299695968628, 'test/num_examples': 10000, 'score': 112775.25321817398, 'total_duration': 116698.1319694519, 'accumulated_submission_time': 112775.25321817398, 'accumulated_eval_time': 3897.1172499656677, 'accumulated_logging_time': 14.096839189529419}
I0301 13:36:07.207436 139589717059328 logging_writer.py:48] [332127] accumulated_eval_time=3897.117250, accumulated_logging_time=14.096839, accumulated_submission_time=112775.253218, global_step=332127, preemption_count=0, score=112775.253218, test/accuracy=0.630500, test/loss=1.817130, test/num_examples=10000, total_duration=116698.131969, train/accuracy=0.959184, train/loss=0.150742, validation/accuracy=0.754840, validation/loss=1.055082, validation/num_examples=50000
I0301 13:36:32.294278 139590170035968 logging_writer.py:48] [332200] global_step=332200, grad_norm=4.187258243560791, loss=0.569377064704895
I0301 13:37:06.203830 139589717059328 logging_writer.py:48] [332300] global_step=332300, grad_norm=4.673283100128174, loss=0.6333774924278259
I0301 13:37:40.117492 139590170035968 logging_writer.py:48] [332400] global_step=332400, grad_norm=4.853924751281738, loss=0.6678622364997864
I0301 13:38:14.093320 139589717059328 logging_writer.py:48] [332500] global_step=332500, grad_norm=4.587961673736572, loss=0.6958402991294861
I0301 13:38:48.052051 139590170035968 logging_writer.py:48] [332600] global_step=332600, grad_norm=4.30447244644165, loss=0.5439239740371704
I0301 13:39:22.062798 139589717059328 logging_writer.py:48] [332700] global_step=332700, grad_norm=4.345585823059082, loss=0.6993987560272217
I0301 13:39:55.988696 139590170035968 logging_writer.py:48] [332800] global_step=332800, grad_norm=4.4065680503845215, loss=0.5936411619186401
I0301 13:40:29.913508 139589717059328 logging_writer.py:48] [332900] global_step=332900, grad_norm=4.08530330657959, loss=0.5784397721290588
I0301 13:41:03.887019 139590170035968 logging_writer.py:48] [333000] global_step=333000, grad_norm=4.3517255783081055, loss=0.6168005466461182
I0301 13:41:37.801454 139589717059328 logging_writer.py:48] [333100] global_step=333100, grad_norm=4.895383834838867, loss=0.6378641128540039
I0301 13:42:11.739435 139590170035968 logging_writer.py:48] [333200] global_step=333200, grad_norm=4.594938278198242, loss=0.5774846076965332
I0301 13:42:45.682828 139589717059328 logging_writer.py:48] [333300] global_step=333300, grad_norm=4.435333251953125, loss=0.6269224882125854
I0301 13:43:19.619325 139590170035968 logging_writer.py:48] [333400] global_step=333400, grad_norm=4.800835132598877, loss=0.6648929119110107
I0301 13:43:53.554134 139589717059328 logging_writer.py:48] [333500] global_step=333500, grad_norm=4.258481979370117, loss=0.6201127171516418
I0301 13:44:27.491122 139590170035968 logging_writer.py:48] [333600] global_step=333600, grad_norm=4.486859321594238, loss=0.6019940376281738
I0301 13:44:37.454763 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:44:43.456410 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:44:51.781460 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:44:54.052545 139753105983296 submission_runner.py:411] Time since start: 117225.06s, 	Step: 333631, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.1435246765613556, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0559492111206055, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.819736361503601, 'test/num_examples': 10000, 'score': 113285.43510556221, 'total_duration': 117225.05577325821, 'accumulated_submission_time': 113285.43510556221, 'accumulated_eval_time': 3913.7149760723114, 'accumulated_logging_time': 14.186609506607056}
I0301 13:44:54.127896 139589717059328 logging_writer.py:48] [333631] accumulated_eval_time=3913.714976, accumulated_logging_time=14.186610, accumulated_submission_time=113285.435106, global_step=333631, preemption_count=0, score=113285.435106, test/accuracy=0.630600, test/loss=1.819736, test/num_examples=10000, total_duration=117225.055773, train/accuracy=0.961256, train/loss=0.143525, validation/accuracy=0.754760, validation/loss=1.055949, validation/num_examples=50000
I0301 13:45:17.894871 139589725452032 logging_writer.py:48] [333700] global_step=333700, grad_norm=5.059516906738281, loss=0.6428604125976562
I0301 13:45:51.891478 139589717059328 logging_writer.py:48] [333800] global_step=333800, grad_norm=4.782355785369873, loss=0.5818596482276917
I0301 13:46:25.828281 139589725452032 logging_writer.py:48] [333900] global_step=333900, grad_norm=4.826760768890381, loss=0.6780442595481873
I0301 13:46:59.770098 139589717059328 logging_writer.py:48] [334000] global_step=334000, grad_norm=4.075006484985352, loss=0.587500810623169
I0301 13:47:33.730350 139589725452032 logging_writer.py:48] [334100] global_step=334100, grad_norm=4.820220470428467, loss=0.6506853699684143
I0301 13:48:07.688791 139589717059328 logging_writer.py:48] [334200] global_step=334200, grad_norm=4.7272419929504395, loss=0.6213223934173584
I0301 13:48:41.653381 139589725452032 logging_writer.py:48] [334300] global_step=334300, grad_norm=4.314309120178223, loss=0.5809319019317627
I0301 13:49:15.588893 139589717059328 logging_writer.py:48] [334400] global_step=334400, grad_norm=4.377478122711182, loss=0.5503746271133423
I0301 13:49:49.539311 139589725452032 logging_writer.py:48] [334500] global_step=334500, grad_norm=4.40825891494751, loss=0.5911116600036621
I0301 13:50:23.500165 139589717059328 logging_writer.py:48] [334600] global_step=334600, grad_norm=4.640265941619873, loss=0.6998857259750366
I0301 13:50:57.452240 139589725452032 logging_writer.py:48] [334700] global_step=334700, grad_norm=4.479219913482666, loss=0.6046260595321655
I0301 13:51:31.365145 139589717059328 logging_writer.py:48] [334800] global_step=334800, grad_norm=4.801910877227783, loss=0.5865971446037292
I0301 13:52:05.401959 139589725452032 logging_writer.py:48] [334900] global_step=334900, grad_norm=4.447976112365723, loss=0.6275383234024048
I0301 13:52:39.361333 139589717059328 logging_writer.py:48] [335000] global_step=335000, grad_norm=5.116127014160156, loss=0.6087797284126282
I0301 13:53:13.300813 139589725452032 logging_writer.py:48] [335100] global_step=335100, grad_norm=4.440822124481201, loss=0.6355953812599182
I0301 13:53:24.300807 139753105983296 spec.py:321] Evaluating on the training split.
I0301 13:53:30.300165 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 13:53:38.679839 139753105983296 spec.py:349] Evaluating on the test split.
I0301 13:53:40.955178 139753105983296 submission_runner.py:411] Time since start: 117751.96s, 	Step: 335134, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.1471763551235199, 'validation/accuracy': 0.7553199529647827, 'validation/loss': 1.055428147315979, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8180768489837646, 'test/num_examples': 10000, 'score': 113795.54326581955, 'total_duration': 117751.9583992958, 'accumulated_submission_time': 113795.54326581955, 'accumulated_eval_time': 3930.3692829608917, 'accumulated_logging_time': 14.27349853515625}
I0301 13:53:41.032011 139590144857856 logging_writer.py:48] [335134] accumulated_eval_time=3930.369283, accumulated_logging_time=14.273499, accumulated_submission_time=113795.543266, global_step=335134, preemption_count=0, score=113795.543266, test/accuracy=0.631000, test/loss=1.818077, test/num_examples=10000, total_duration=117751.958399, train/accuracy=0.959861, train/loss=0.147176, validation/accuracy=0.755320, validation/loss=1.055428, validation/num_examples=50000
I0301 13:54:03.767151 139590170035968 logging_writer.py:48] [335200] global_step=335200, grad_norm=4.618178844451904, loss=0.6177138686180115
I0301 13:54:37.659544 139590144857856 logging_writer.py:48] [335300] global_step=335300, grad_norm=4.57056999206543, loss=0.5645501613616943
I0301 13:55:11.588735 139590170035968 logging_writer.py:48] [335400] global_step=335400, grad_norm=4.651554107666016, loss=0.6655446290969849
I0301 13:55:45.497278 139590144857856 logging_writer.py:48] [335500] global_step=335500, grad_norm=4.33299446105957, loss=0.6093943119049072
I0301 13:56:19.449325 139590170035968 logging_writer.py:48] [335600] global_step=335600, grad_norm=4.813784122467041, loss=0.6874055862426758
I0301 13:56:53.379306 139590144857856 logging_writer.py:48] [335700] global_step=335700, grad_norm=4.304147243499756, loss=0.5323468446731567
I0301 13:57:27.326529 139590170035968 logging_writer.py:48] [335800] global_step=335800, grad_norm=4.472890853881836, loss=0.5982519388198853
I0301 13:58:01.306684 139590144857856 logging_writer.py:48] [335900] global_step=335900, grad_norm=4.899918556213379, loss=0.6220365762710571
I0301 13:58:35.251470 139590170035968 logging_writer.py:48] [336000] global_step=336000, grad_norm=4.1704630851745605, loss=0.5922155380249023
I0301 13:59:09.218419 139590144857856 logging_writer.py:48] [336100] global_step=336100, grad_norm=4.755028247833252, loss=0.6544291973114014
I0301 13:59:43.173184 139590170035968 logging_writer.py:48] [336200] global_step=336200, grad_norm=4.546856880187988, loss=0.5821170210838318
I0301 14:00:17.114349 139590144857856 logging_writer.py:48] [336300] global_step=336300, grad_norm=4.970860004425049, loss=0.6090041995048523
I0301 14:00:51.058272 139590170035968 logging_writer.py:48] [336400] global_step=336400, grad_norm=4.664763450622559, loss=0.5901786088943481
I0301 14:01:25.031961 139590144857856 logging_writer.py:48] [336500] global_step=336500, grad_norm=4.512291431427002, loss=0.593540370464325
I0301 14:01:58.966607 139590170035968 logging_writer.py:48] [336600] global_step=336600, grad_norm=4.356977462768555, loss=0.6116704940795898
I0301 14:02:10.985139 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:02:17.031352 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:02:25.437903 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:02:27.744689 139753105983296 submission_runner.py:411] Time since start: 118278.75s, 	Step: 336637, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14789387583732605, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0557163953781128, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8188215494155884, 'test/num_examples': 10000, 'score': 114305.43227887154, 'total_duration': 118278.74791812897, 'accumulated_submission_time': 114305.43227887154, 'accumulated_eval_time': 3947.1287870407104, 'accumulated_logging_time': 14.36074447631836}
I0301 14:02:27.818899 139589725452032 logging_writer.py:48] [336637] accumulated_eval_time=3947.128787, accumulated_logging_time=14.360744, accumulated_submission_time=114305.432279, global_step=336637, preemption_count=0, score=114305.432279, test/accuracy=0.630200, test/loss=1.818822, test/num_examples=10000, total_duration=118278.747918, train/accuracy=0.961137, train/loss=0.147894, validation/accuracy=0.754920, validation/loss=1.055716, validation/num_examples=50000
I0301 14:02:49.521332 139590153250560 logging_writer.py:48] [336700] global_step=336700, grad_norm=4.55327844619751, loss=0.6239457130432129
I0301 14:03:23.421820 139589725452032 logging_writer.py:48] [336800] global_step=336800, grad_norm=4.410200119018555, loss=0.6551038026809692
I0301 14:03:57.408067 139590153250560 logging_writer.py:48] [336900] global_step=336900, grad_norm=3.9511468410491943, loss=0.6196783781051636
I0301 14:04:31.517982 139589725452032 logging_writer.py:48] [337000] global_step=337000, grad_norm=4.76038122177124, loss=0.6270128488540649
I0301 14:05:05.494538 139590153250560 logging_writer.py:48] [337100] global_step=337100, grad_norm=4.740754127502441, loss=0.643038272857666
I0301 14:05:39.458107 139589725452032 logging_writer.py:48] [337200] global_step=337200, grad_norm=4.774389266967773, loss=0.6299129724502563
I0301 14:06:13.422894 139590153250560 logging_writer.py:48] [337300] global_step=337300, grad_norm=4.598021507263184, loss=0.6489676237106323
I0301 14:06:47.390768 139589725452032 logging_writer.py:48] [337400] global_step=337400, grad_norm=4.763514518737793, loss=0.595237135887146
I0301 14:07:21.360675 139590153250560 logging_writer.py:48] [337500] global_step=337500, grad_norm=4.195911884307861, loss=0.6427494883537292
I0301 14:07:55.322493 139589725452032 logging_writer.py:48] [337600] global_step=337600, grad_norm=4.553154945373535, loss=0.6523431539535522
I0301 14:08:29.272398 139590153250560 logging_writer.py:48] [337700] global_step=337700, grad_norm=4.512036323547363, loss=0.5909631252288818
I0301 14:09:03.240376 139589725452032 logging_writer.py:48] [337800] global_step=337800, grad_norm=4.621058940887451, loss=0.6729547381401062
I0301 14:09:37.185819 139590153250560 logging_writer.py:48] [337900] global_step=337900, grad_norm=4.6280622482299805, loss=0.5748946070671082
I0301 14:10:11.221997 139589725452032 logging_writer.py:48] [338000] global_step=338000, grad_norm=4.583059787750244, loss=0.6900489330291748
I0301 14:10:45.201431 139590153250560 logging_writer.py:48] [338100] global_step=338100, grad_norm=4.468167304992676, loss=0.6455144286155701
I0301 14:10:57.906018 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:11:03.992261 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:11:12.322404 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:11:14.629105 139753105983296 submission_runner.py:411] Time since start: 118805.63s, 	Step: 338139, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1472291797399521, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.055741786956787, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8178565502166748, 'test/num_examples': 10000, 'score': 114815.45590782166, 'total_duration': 118805.63232278824, 'accumulated_submission_time': 114815.45590782166, 'accumulated_eval_time': 3963.851808786392, 'accumulated_logging_time': 14.4454824924469}
I0301 14:11:14.704986 139589717059328 logging_writer.py:48] [338139] accumulated_eval_time=3963.851809, accumulated_logging_time=14.445482, accumulated_submission_time=114815.455908, global_step=338139, preemption_count=0, score=114815.455908, test/accuracy=0.631000, test/loss=1.817857, test/num_examples=10000, total_duration=118805.632323, train/accuracy=0.960479, train/loss=0.147229, validation/accuracy=0.754840, validation/loss=1.055742, validation/num_examples=50000
I0301 14:11:35.687625 139589725452032 logging_writer.py:48] [338200] global_step=338200, grad_norm=4.528594493865967, loss=0.6435713171958923
I0301 14:12:09.585196 139589717059328 logging_writer.py:48] [338300] global_step=338300, grad_norm=4.443125247955322, loss=0.6266655325889587
I0301 14:12:43.510892 139589725452032 logging_writer.py:48] [338400] global_step=338400, grad_norm=4.748491287231445, loss=0.6940090656280518
I0301 14:13:17.444627 139589717059328 logging_writer.py:48] [338500] global_step=338500, grad_norm=4.4579315185546875, loss=0.6386269330978394
I0301 14:13:51.392110 139589725452032 logging_writer.py:48] [338600] global_step=338600, grad_norm=4.825385570526123, loss=0.6318199634552002
I0301 14:14:25.345018 139589717059328 logging_writer.py:48] [338700] global_step=338700, grad_norm=4.178450584411621, loss=0.5904199481010437
I0301 14:14:59.291062 139589725452032 logging_writer.py:48] [338800] global_step=338800, grad_norm=4.248763084411621, loss=0.617767870426178
I0301 14:15:33.244807 139589717059328 logging_writer.py:48] [338900] global_step=338900, grad_norm=4.674217700958252, loss=0.5733839273452759
I0301 14:16:07.173493 139589725452032 logging_writer.py:48] [339000] global_step=339000, grad_norm=4.590399265289307, loss=0.6404710412025452
I0301 14:16:41.172765 139589717059328 logging_writer.py:48] [339100] global_step=339100, grad_norm=4.447722434997559, loss=0.5486289262771606
I0301 14:17:15.128919 139589725452032 logging_writer.py:48] [339200] global_step=339200, grad_norm=4.661248207092285, loss=0.6066473126411438
I0301 14:17:49.039615 139589717059328 logging_writer.py:48] [339300] global_step=339300, grad_norm=4.475864887237549, loss=0.5875927805900574
I0301 14:18:22.954225 139589725452032 logging_writer.py:48] [339400] global_step=339400, grad_norm=4.5776686668396, loss=0.6858230829238892
I0301 14:18:56.893629 139589717059328 logging_writer.py:48] [339500] global_step=339500, grad_norm=4.544762134552002, loss=0.6276111006736755
I0301 14:19:30.829395 139589725452032 logging_writer.py:48] [339600] global_step=339600, grad_norm=4.824916362762451, loss=0.7220891714096069
I0301 14:19:44.885930 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:19:50.871903 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:19:59.129167 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:20:01.434606 139753105983296 submission_runner.py:411] Time since start: 119332.44s, 	Step: 339643, 	{'train/accuracy': 0.9617944359779358, 'train/loss': 0.14093098044395447, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0552440881729126, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8199061155319214, 'test/num_examples': 10000, 'score': 115325.57354831696, 'total_duration': 119332.43783807755, 'accumulated_submission_time': 115325.57354831696, 'accumulated_eval_time': 3980.400431394577, 'accumulated_logging_time': 14.531226396560669}
I0301 14:20:01.508152 139589717059328 logging_writer.py:48] [339643] accumulated_eval_time=3980.400431, accumulated_logging_time=14.531226, accumulated_submission_time=115325.573548, global_step=339643, preemption_count=0, score=115325.573548, test/accuracy=0.630300, test/loss=1.819906, test/num_examples=10000, total_duration=119332.437838, train/accuracy=0.961794, train/loss=0.140931, validation/accuracy=0.754960, validation/loss=1.055244, validation/num_examples=50000
I0301 14:20:21.166301 139589725452032 logging_writer.py:48] [339700] global_step=339700, grad_norm=4.365549564361572, loss=0.6200803518295288
I0301 14:20:55.048070 139589717059328 logging_writer.py:48] [339800] global_step=339800, grad_norm=4.513542175292969, loss=0.6585265398025513
I0301 14:21:28.979893 139589725452032 logging_writer.py:48] [339900] global_step=339900, grad_norm=4.410650730133057, loss=0.5987098217010498
I0301 14:22:02.928058 139589717059328 logging_writer.py:48] [340000] global_step=340000, grad_norm=4.8733229637146, loss=0.6222078204154968
I0301 14:22:36.952764 139589725452032 logging_writer.py:48] [340100] global_step=340100, grad_norm=4.5223612785339355, loss=0.6005266904830933
I0301 14:23:10.900092 139589717059328 logging_writer.py:48] [340200] global_step=340200, grad_norm=4.383688449859619, loss=0.6356501579284668
I0301 14:23:44.845563 139589725452032 logging_writer.py:48] [340300] global_step=340300, grad_norm=4.276381492614746, loss=0.5756211876869202
I0301 14:24:18.741110 139589717059328 logging_writer.py:48] [340400] global_step=340400, grad_norm=4.734240531921387, loss=0.6281896233558655
I0301 14:24:52.662894 139589725452032 logging_writer.py:48] [340500] global_step=340500, grad_norm=4.181526184082031, loss=0.6033042073249817
I0301 14:25:26.613486 139589717059328 logging_writer.py:48] [340600] global_step=340600, grad_norm=4.337678909301758, loss=0.631478488445282
I0301 14:26:00.566379 139589725452032 logging_writer.py:48] [340700] global_step=340700, grad_norm=4.472884654998779, loss=0.6609470844268799
I0301 14:26:34.482940 139589717059328 logging_writer.py:48] [340800] global_step=340800, grad_norm=4.766816139221191, loss=0.6169734001159668
I0301 14:27:08.451618 139589725452032 logging_writer.py:48] [340900] global_step=340900, grad_norm=4.1962666511535645, loss=0.5591377019882202
I0301 14:27:42.413414 139589717059328 logging_writer.py:48] [341000] global_step=341000, grad_norm=4.504271507263184, loss=0.6564364433288574
I0301 14:28:16.341942 139589725452032 logging_writer.py:48] [341100] global_step=341100, grad_norm=4.460854530334473, loss=0.6614733338356018
I0301 14:28:31.751029 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:28:37.789507 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:28:46.023573 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:28:48.645674 139753105983296 submission_runner.py:411] Time since start: 119859.65s, 	Step: 341147, 	{'train/accuracy': 0.9620137214660645, 'train/loss': 0.1430947631597519, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0565391778945923, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8208842277526855, 'test/num_examples': 10000, 'score': 115835.75323462486, 'total_duration': 119859.64888954163, 'accumulated_submission_time': 115835.75323462486, 'accumulated_eval_time': 3997.29501080513, 'accumulated_logging_time': 14.61514163017273}
I0301 14:28:48.721951 139590153250560 logging_writer.py:48] [341147] accumulated_eval_time=3997.295011, accumulated_logging_time=14.615142, accumulated_submission_time=115835.753235, global_step=341147, preemption_count=0, score=115835.753235, test/accuracy=0.630500, test/loss=1.820884, test/num_examples=10000, total_duration=119859.648890, train/accuracy=0.962014, train/loss=0.143095, validation/accuracy=0.754900, validation/loss=1.056539, validation/num_examples=50000
I0301 14:29:07.039518 139590178428672 logging_writer.py:48] [341200] global_step=341200, grad_norm=4.491217613220215, loss=0.6330322027206421
I0301 14:29:40.934461 139590153250560 logging_writer.py:48] [341300] global_step=341300, grad_norm=4.4591898918151855, loss=0.6372284293174744
I0301 14:30:14.875133 139590178428672 logging_writer.py:48] [341400] global_step=341400, grad_norm=4.67073392868042, loss=0.5734317302703857
I0301 14:30:48.820924 139590153250560 logging_writer.py:48] [341500] global_step=341500, grad_norm=4.046666622161865, loss=0.5927836894989014
I0301 14:31:22.779067 139590178428672 logging_writer.py:48] [341600] global_step=341600, grad_norm=4.220182418823242, loss=0.6044133305549622
I0301 14:31:56.747152 139590153250560 logging_writer.py:48] [341700] global_step=341700, grad_norm=4.785768032073975, loss=0.6544172167778015
I0301 14:32:30.696513 139590178428672 logging_writer.py:48] [341800] global_step=341800, grad_norm=4.44448184967041, loss=0.5755709409713745
I0301 14:33:04.616444 139590153250560 logging_writer.py:48] [341900] global_step=341900, grad_norm=4.222631454467773, loss=0.6283389925956726
I0301 14:33:38.572452 139590178428672 logging_writer.py:48] [342000] global_step=342000, grad_norm=4.941826343536377, loss=0.6387060880661011
I0301 14:34:12.546201 139590153250560 logging_writer.py:48] [342100] global_step=342100, grad_norm=4.2379255294799805, loss=0.5960323810577393
I0301 14:34:46.477029 139590178428672 logging_writer.py:48] [342200] global_step=342200, grad_norm=4.85594367980957, loss=0.6497814655303955
I0301 14:35:20.519298 139590153250560 logging_writer.py:48] [342300] global_step=342300, grad_norm=4.313659191131592, loss=0.6055019497871399
I0301 14:35:54.448789 139590178428672 logging_writer.py:48] [342400] global_step=342400, grad_norm=4.310543060302734, loss=0.5660569071769714
I0301 14:36:28.417568 139590153250560 logging_writer.py:48] [342500] global_step=342500, grad_norm=4.838654041290283, loss=0.5994397401809692
I0301 14:37:02.366798 139590178428672 logging_writer.py:48] [342600] global_step=342600, grad_norm=5.418806076049805, loss=0.6075527667999268
I0301 14:37:18.794236 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:37:24.791055 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:37:33.208980 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:37:35.447211 139753105983296 submission_runner.py:411] Time since start: 120386.45s, 	Step: 342650, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14671747386455536, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0551903247833252, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8172508478164673, 'test/num_examples': 10000, 'score': 116345.76007342339, 'total_duration': 120386.4504327774, 'accumulated_submission_time': 116345.76007342339, 'accumulated_eval_time': 4013.947923898697, 'accumulated_logging_time': 14.70347285270691}
I0301 14:37:35.525989 139589717059328 logging_writer.py:48] [342650] accumulated_eval_time=4013.947924, accumulated_logging_time=14.703473, accumulated_submission_time=116345.760073, global_step=342650, preemption_count=0, score=116345.760073, test/accuracy=0.630500, test/loss=1.817251, test/num_examples=10000, total_duration=120386.450433, train/accuracy=0.960678, train/loss=0.146717, validation/accuracy=0.754660, validation/loss=1.055190, validation/num_examples=50000
I0301 14:37:52.836009 139589725452032 logging_writer.py:48] [342700] global_step=342700, grad_norm=4.661401271820068, loss=0.6156491041183472
I0301 14:38:26.725697 139589717059328 logging_writer.py:48] [342800] global_step=342800, grad_norm=4.755158424377441, loss=0.7000365853309631
I0301 14:39:00.691464 139589725452032 logging_writer.py:48] [342900] global_step=342900, grad_norm=4.6008687019348145, loss=0.6024552583694458
I0301 14:39:34.655495 139589717059328 logging_writer.py:48] [343000] global_step=343000, grad_norm=4.663407325744629, loss=0.6138490438461304
I0301 14:40:08.630712 139589725452032 logging_writer.py:48] [343100] global_step=343100, grad_norm=4.96686315536499, loss=0.6790108680725098
I0301 14:40:42.589214 139589717059328 logging_writer.py:48] [343200] global_step=343200, grad_norm=4.946245193481445, loss=0.7243621945381165
I0301 14:41:16.682949 139589725452032 logging_writer.py:48] [343300] global_step=343300, grad_norm=5.020683765411377, loss=0.6548986434936523
I0301 14:41:50.630606 139589717059328 logging_writer.py:48] [343400] global_step=343400, grad_norm=4.606339454650879, loss=0.6384851336479187
I0301 14:42:24.591866 139589725452032 logging_writer.py:48] [343500] global_step=343500, grad_norm=4.498445510864258, loss=0.6430084109306335
I0301 14:42:58.519342 139589717059328 logging_writer.py:48] [343600] global_step=343600, grad_norm=4.601219177246094, loss=0.6317125558853149
I0301 14:43:32.475493 139589725452032 logging_writer.py:48] [343700] global_step=343700, grad_norm=4.781588554382324, loss=0.6758373975753784
I0301 14:44:06.421690 139589717059328 logging_writer.py:48] [343800] global_step=343800, grad_norm=4.503326892852783, loss=0.6072694063186646
I0301 14:44:40.370714 139589725452032 logging_writer.py:48] [343900] global_step=343900, grad_norm=4.657603740692139, loss=0.6453084349632263
I0301 14:45:14.297573 139589717059328 logging_writer.py:48] [344000] global_step=344000, grad_norm=4.540172576904297, loss=0.6918345093727112
I0301 14:45:48.264068 139589725452032 logging_writer.py:48] [344100] global_step=344100, grad_norm=4.285007476806641, loss=0.6236906051635742
I0301 14:46:05.710653 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:46:12.445768 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:46:20.819734 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:46:23.151973 139753105983296 submission_runner.py:411] Time since start: 120914.16s, 	Step: 344153, 	{'train/accuracy': 0.9591438174247742, 'train/loss': 0.14920060336589813, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0547455549240112, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8171762228012085, 'test/num_examples': 10000, 'score': 116855.8805770874, 'total_duration': 120914.15520620346, 'accumulated_submission_time': 116855.8805770874, 'accumulated_eval_time': 4031.3891978263855, 'accumulated_logging_time': 14.792497396469116}
I0301 14:46:23.233129 139589708666624 logging_writer.py:48] [344153] accumulated_eval_time=4031.389198, accumulated_logging_time=14.792497, accumulated_submission_time=116855.880577, global_step=344153, preemption_count=0, score=116855.880577, test/accuracy=0.630200, test/loss=1.817176, test/num_examples=10000, total_duration=120914.155206, train/accuracy=0.959144, train/loss=0.149201, validation/accuracy=0.754720, validation/loss=1.054746, validation/num_examples=50000
I0301 14:46:39.519957 139590161643264 logging_writer.py:48] [344200] global_step=344200, grad_norm=4.787954330444336, loss=0.6366283297538757
I0301 14:47:13.419724 139589708666624 logging_writer.py:48] [344300] global_step=344300, grad_norm=4.505719184875488, loss=0.5952588319778442
I0301 14:47:47.481431 139590161643264 logging_writer.py:48] [344400] global_step=344400, grad_norm=4.99105167388916, loss=0.6972768902778625
I0301 14:48:21.442325 139589708666624 logging_writer.py:48] [344500] global_step=344500, grad_norm=4.295059680938721, loss=0.6442977786064148
I0301 14:48:55.380858 139590161643264 logging_writer.py:48] [344600] global_step=344600, grad_norm=5.034932613372803, loss=0.5786328911781311
I0301 14:49:29.305989 139589708666624 logging_writer.py:48] [344700] global_step=344700, grad_norm=4.527635097503662, loss=0.6642848253250122
I0301 14:50:03.245699 139590161643264 logging_writer.py:48] [344800] global_step=344800, grad_norm=4.747159004211426, loss=0.6729187369346619
I0301 14:50:37.194729 139589708666624 logging_writer.py:48] [344900] global_step=344900, grad_norm=4.3959503173828125, loss=0.6013164520263672
I0301 14:51:11.121377 139590161643264 logging_writer.py:48] [345000] global_step=345000, grad_norm=4.183075428009033, loss=0.5604217648506165
I0301 14:51:45.028469 139589708666624 logging_writer.py:48] [345100] global_step=345100, grad_norm=4.543614387512207, loss=0.5807880163192749
I0301 14:52:19.001434 139590161643264 logging_writer.py:48] [345200] global_step=345200, grad_norm=4.687422752380371, loss=0.715823769569397
I0301 14:52:52.964181 139589708666624 logging_writer.py:48] [345300] global_step=345300, grad_norm=4.594462871551514, loss=0.6018516421318054
I0301 14:53:26.988799 139590161643264 logging_writer.py:48] [345400] global_step=345400, grad_norm=4.646453380584717, loss=0.6788854598999023
I0301 14:54:00.931073 139589708666624 logging_writer.py:48] [345500] global_step=345500, grad_norm=4.4942827224731445, loss=0.5942767262458801
I0301 14:54:34.846267 139590161643264 logging_writer.py:48] [345600] global_step=345600, grad_norm=5.784596920013428, loss=0.7405857443809509
I0301 14:54:53.319204 139753105983296 spec.py:321] Evaluating on the training split.
I0301 14:54:59.391996 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 14:55:07.677290 139753105983296 spec.py:349] Evaluating on the test split.
I0301 14:55:09.991816 139753105983296 submission_runner.py:411] Time since start: 121441.00s, 	Step: 345656, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.1464512199163437, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0546934604644775, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8182249069213867, 'test/num_examples': 10000, 'score': 117365.90293121338, 'total_duration': 121440.9950222969, 'accumulated_submission_time': 117365.90293121338, 'accumulated_eval_time': 4048.061734199524, 'accumulated_logging_time': 14.883959531784058}
I0301 14:55:10.084804 139590144857856 logging_writer.py:48] [345656] accumulated_eval_time=4048.061734, accumulated_logging_time=14.883960, accumulated_submission_time=117365.902931, global_step=345656, preemption_count=0, score=117365.902931, test/accuracy=0.630300, test/loss=1.818225, test/num_examples=10000, total_duration=121440.995022, train/accuracy=0.961276, train/loss=0.146451, validation/accuracy=0.754900, validation/loss=1.054693, validation/num_examples=50000
I0301 14:55:25.377746 139590153250560 logging_writer.py:48] [345700] global_step=345700, grad_norm=4.655551433563232, loss=0.6436165571212769
I0301 14:55:59.320768 139590144857856 logging_writer.py:48] [345800] global_step=345800, grad_norm=4.47128963470459, loss=0.6599098443984985
I0301 14:56:33.273140 139590153250560 logging_writer.py:48] [345900] global_step=345900, grad_norm=4.216454029083252, loss=0.6459055542945862
I0301 14:57:07.210139 139590144857856 logging_writer.py:48] [346000] global_step=346000, grad_norm=4.490058422088623, loss=0.610498309135437
I0301 14:57:41.156962 139590153250560 logging_writer.py:48] [346100] global_step=346100, grad_norm=4.98559045791626, loss=0.6662598848342896
I0301 14:58:15.117326 139590144857856 logging_writer.py:48] [346200] global_step=346200, grad_norm=4.638574123382568, loss=0.6349681615829468
I0301 14:58:49.057497 139590153250560 logging_writer.py:48] [346300] global_step=346300, grad_norm=4.380735397338867, loss=0.5871941447257996
I0301 14:59:23.009055 139590144857856 logging_writer.py:48] [346400] global_step=346400, grad_norm=4.588400840759277, loss=0.6350859999656677
I0301 14:59:57.154854 139590153250560 logging_writer.py:48] [346500] global_step=346500, grad_norm=5.214553356170654, loss=0.652948260307312
I0301 15:00:31.115074 139590144857856 logging_writer.py:48] [346600] global_step=346600, grad_norm=4.179401397705078, loss=0.5774828791618347
I0301 15:01:05.093523 139590153250560 logging_writer.py:48] [346700] global_step=346700, grad_norm=4.3441338539123535, loss=0.5813108086585999
I0301 15:01:39.030401 139590144857856 logging_writer.py:48] [346800] global_step=346800, grad_norm=4.42317008972168, loss=0.6752750873565674
I0301 15:02:12.997265 139590153250560 logging_writer.py:48] [346900] global_step=346900, grad_norm=4.148390769958496, loss=0.5641872882843018
I0301 15:02:46.953883 139590144857856 logging_writer.py:48] [347000] global_step=347000, grad_norm=4.155679702758789, loss=0.6039997339248657
I0301 15:03:20.883294 139590153250560 logging_writer.py:48] [347100] global_step=347100, grad_norm=4.477062225341797, loss=0.6314527988433838
I0301 15:03:40.043563 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:03:46.024087 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:03:54.322877 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:03:56.607067 139753105983296 submission_runner.py:411] Time since start: 121967.61s, 	Step: 347158, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1453184336423874, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 1.0555799007415771, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8174842596054077, 'test/num_examples': 10000, 'score': 117875.79716300964, 'total_duration': 121967.61029624939, 'accumulated_submission_time': 117875.79716300964, 'accumulated_eval_time': 4064.6251826286316, 'accumulated_logging_time': 14.9880850315094}
I0301 15:03:56.683056 139589717059328 logging_writer.py:48] [347158] accumulated_eval_time=4064.625183, accumulated_logging_time=14.988085, accumulated_submission_time=117875.797163, global_step=347158, preemption_count=0, score=117875.797163, test/accuracy=0.630800, test/loss=1.817484, test/num_examples=10000, total_duration=121967.610296, train/accuracy=0.961217, train/loss=0.145318, validation/accuracy=0.754460, validation/loss=1.055580, validation/num_examples=50000
I0301 15:04:11.232637 139589725452032 logging_writer.py:48] [347200] global_step=347200, grad_norm=4.436183452606201, loss=0.623502790927887
I0301 15:04:45.150563 139589717059328 logging_writer.py:48] [347300] global_step=347300, grad_norm=4.685626983642578, loss=0.7096119523048401
I0301 15:05:19.079315 139589725452032 logging_writer.py:48] [347400] global_step=347400, grad_norm=4.756906509399414, loss=0.5974501371383667
I0301 15:05:53.112398 139589717059328 logging_writer.py:48] [347500] global_step=347500, grad_norm=4.927601337432861, loss=0.6876087784767151
I0301 15:06:27.047275 139589725452032 logging_writer.py:48] [347600] global_step=347600, grad_norm=4.679993152618408, loss=0.6051992774009705
I0301 15:07:00.996943 139589717059328 logging_writer.py:48] [347700] global_step=347700, grad_norm=5.003016471862793, loss=0.6302017569541931
I0301 15:07:34.909465 139589725452032 logging_writer.py:48] [347800] global_step=347800, grad_norm=4.381631851196289, loss=0.610011100769043
I0301 15:08:08.822346 139589717059328 logging_writer.py:48] [347900] global_step=347900, grad_norm=4.714785575866699, loss=0.6380300521850586
I0301 15:08:42.756920 139589725452032 logging_writer.py:48] [348000] global_step=348000, grad_norm=4.794813632965088, loss=0.5492575168609619
I0301 15:09:16.703975 139589717059328 logging_writer.py:48] [348100] global_step=348100, grad_norm=4.5691962242126465, loss=0.5994176864624023
I0301 15:09:50.632592 139589725452032 logging_writer.py:48] [348200] global_step=348200, grad_norm=4.1922078132629395, loss=0.6770363450050354
I0301 15:10:24.587886 139589717059328 logging_writer.py:48] [348300] global_step=348300, grad_norm=5.069793701171875, loss=0.6570512652397156
I0301 15:10:58.533661 139589725452032 logging_writer.py:48] [348400] global_step=348400, grad_norm=4.608715534210205, loss=0.5862046480178833
I0301 15:11:32.484257 139589717059328 logging_writer.py:48] [348500] global_step=348500, grad_norm=5.351998329162598, loss=0.6305833458900452
I0301 15:12:06.506342 139589725452032 logging_writer.py:48] [348600] global_step=348600, grad_norm=4.540013313293457, loss=0.5660470128059387
I0301 15:12:26.673637 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:12:32.696325 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:12:40.947618 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:12:43.238124 139753105983296 submission_runner.py:411] Time since start: 122494.24s, 	Step: 348661, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.14423301815986633, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.055746078491211, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8207018375396729, 'test/num_examples': 10000, 'score': 118385.72451281548, 'total_duration': 122494.24131369591, 'accumulated_submission_time': 118385.72451281548, 'accumulated_eval_time': 4081.1895790100098, 'accumulated_logging_time': 15.074573993682861}
I0301 15:12:43.361469 139590144857856 logging_writer.py:48] [348661] accumulated_eval_time=4081.189579, accumulated_logging_time=15.074574, accumulated_submission_time=118385.724513, global_step=348661, preemption_count=0, score=118385.724513, test/accuracy=0.631300, test/loss=1.820702, test/num_examples=10000, total_duration=122494.241314, train/accuracy=0.961296, train/loss=0.144233, validation/accuracy=0.754900, validation/loss=1.055746, validation/num_examples=50000
I0301 15:12:56.926297 139590153250560 logging_writer.py:48] [348700] global_step=348700, grad_norm=4.643257141113281, loss=0.5872740745544434
I0301 15:13:30.871205 139590144857856 logging_writer.py:48] [348800] global_step=348800, grad_norm=4.517816543579102, loss=0.600767195224762
I0301 15:14:04.821441 139590153250560 logging_writer.py:48] [348900] global_step=348900, grad_norm=4.482520580291748, loss=0.5410457253456116
I0301 15:14:38.769352 139590144857856 logging_writer.py:48] [349000] global_step=349000, grad_norm=4.6545538902282715, loss=0.6295834183692932
I0301 15:15:12.725151 139590153250560 logging_writer.py:48] [349100] global_step=349100, grad_norm=4.253130912780762, loss=0.5868739485740662
I0301 15:15:46.693986 139590144857856 logging_writer.py:48] [349200] global_step=349200, grad_norm=4.653882026672363, loss=0.5787695646286011
I0301 15:16:20.640720 139590153250560 logging_writer.py:48] [349300] global_step=349300, grad_norm=4.828479290008545, loss=0.6951608061790466
I0301 15:16:54.624181 139590144857856 logging_writer.py:48] [349400] global_step=349400, grad_norm=4.29090690612793, loss=0.5436998605728149
I0301 15:17:28.579150 139590153250560 logging_writer.py:48] [349500] global_step=349500, grad_norm=4.429657459259033, loss=0.5954999923706055
I0301 15:18:02.534688 139590144857856 logging_writer.py:48] [349600] global_step=349600, grad_norm=4.250487804412842, loss=0.6219310760498047
I0301 15:18:36.589567 139590153250560 logging_writer.py:48] [349700] global_step=349700, grad_norm=4.498272895812988, loss=0.6451689004898071
I0301 15:19:10.573566 139590144857856 logging_writer.py:48] [349800] global_step=349800, grad_norm=4.533229351043701, loss=0.6381840705871582
I0301 15:19:44.564352 139590153250560 logging_writer.py:48] [349900] global_step=349900, grad_norm=4.411858081817627, loss=0.6386125087738037
I0301 15:20:18.544813 139590144857856 logging_writer.py:48] [350000] global_step=350000, grad_norm=4.805538654327393, loss=0.6177998185157776
I0301 15:20:52.503219 139590153250560 logging_writer.py:48] [350100] global_step=350100, grad_norm=4.336232662200928, loss=0.6265150308609009
I0301 15:21:13.368286 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:21:19.373132 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:21:27.683374 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:21:29.943345 139753105983296 submission_runner.py:411] Time since start: 123020.95s, 	Step: 350163, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14508046209812164, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.0556223392486572, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.819610834121704, 'test/num_examples': 10000, 'score': 118895.66305184364, 'total_duration': 123020.94657683372, 'accumulated_submission_time': 118895.66305184364, 'accumulated_eval_time': 4097.764590501785, 'accumulated_logging_time': 15.211965799331665}
I0301 15:21:30.018812 139589717059328 logging_writer.py:48] [350163] accumulated_eval_time=4097.764591, accumulated_logging_time=15.211966, accumulated_submission_time=118895.663052, global_step=350163, preemption_count=0, score=118895.663052, test/accuracy=0.630400, test/loss=1.819611, test/num_examples=10000, total_duration=123020.946577, train/accuracy=0.961635, train/loss=0.145080, validation/accuracy=0.754620, validation/loss=1.055622, validation/num_examples=50000
I0301 15:21:42.871281 139589725452032 logging_writer.py:48] [350200] global_step=350200, grad_norm=4.291773319244385, loss=0.6117655038833618
I0301 15:22:16.789595 139589717059328 logging_writer.py:48] [350300] global_step=350300, grad_norm=5.2191596031188965, loss=0.6930599212646484
I0301 15:22:50.702865 139589725452032 logging_writer.py:48] [350400] global_step=350400, grad_norm=4.291999340057373, loss=0.7021950483322144
I0301 15:23:24.623300 139589717059328 logging_writer.py:48] [350500] global_step=350500, grad_norm=4.803481578826904, loss=0.6321287155151367
I0301 15:23:58.562470 139589725452032 logging_writer.py:48] [350600] global_step=350600, grad_norm=4.406034469604492, loss=0.6522461175918579
I0301 15:24:32.591556 139589717059328 logging_writer.py:48] [350700] global_step=350700, grad_norm=4.522531032562256, loss=0.5723842978477478
I0301 15:25:06.528606 139589725452032 logging_writer.py:48] [350800] global_step=350800, grad_norm=4.421379089355469, loss=0.6466653347015381
I0301 15:25:40.474141 139589717059328 logging_writer.py:48] [350900] global_step=350900, grad_norm=4.5608110427856445, loss=0.589513897895813
I0301 15:26:14.421914 139589725452032 logging_writer.py:48] [351000] global_step=351000, grad_norm=4.717130184173584, loss=0.595614492893219
I0301 15:26:48.303009 139589717059328 logging_writer.py:48] [351100] global_step=351100, grad_norm=5.316677093505859, loss=0.6451445817947388
I0301 15:27:22.233068 139589725452032 logging_writer.py:48] [351200] global_step=351200, grad_norm=4.667237281799316, loss=0.6560817956924438
I0301 15:27:56.194108 139589717059328 logging_writer.py:48] [351300] global_step=351300, grad_norm=4.551947116851807, loss=0.641575038433075
I0301 15:28:30.134505 139589725452032 logging_writer.py:48] [351400] global_step=351400, grad_norm=5.119693279266357, loss=0.5835290551185608
I0301 15:29:04.063185 139589717059328 logging_writer.py:48] [351500] global_step=351500, grad_norm=4.620759010314941, loss=0.6083876490592957
I0301 15:29:38.013835 139589725452032 logging_writer.py:48] [351600] global_step=351600, grad_norm=4.1438422203063965, loss=0.5021490454673767
I0301 15:30:00.259189 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:30:06.288984 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:30:14.615716 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:30:16.897579 139753105983296 submission_runner.py:411] Time since start: 123547.90s, 	Step: 351667, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14617449045181274, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0564955472946167, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8202815055847168, 'test/num_examples': 10000, 'score': 119405.83976817131, 'total_duration': 123547.90081143379, 'accumulated_submission_time': 119405.83976817131, 'accumulated_eval_time': 4114.402928113937, 'accumulated_logging_time': 15.298362493515015}
I0301 15:30:16.980288 139589717059328 logging_writer.py:48] [351667] accumulated_eval_time=4114.402928, accumulated_logging_time=15.298362, accumulated_submission_time=119405.839768, global_step=351667, preemption_count=0, score=119405.839768, test/accuracy=0.630000, test/loss=1.820282, test/num_examples=10000, total_duration=123547.900811, train/accuracy=0.960858, train/loss=0.146174, validation/accuracy=0.754860, validation/loss=1.056496, validation/num_examples=50000
I0301 15:30:28.517042 139590144857856 logging_writer.py:48] [351700] global_step=351700, grad_norm=4.299671649932861, loss=0.5556328296661377
I0301 15:31:02.665482 139589717059328 logging_writer.py:48] [351800] global_step=351800, grad_norm=4.29890775680542, loss=0.608607292175293
I0301 15:31:36.605863 139590144857856 logging_writer.py:48] [351900] global_step=351900, grad_norm=4.569862365722656, loss=0.6767575740814209
I0301 15:32:10.599807 139589717059328 logging_writer.py:48] [352000] global_step=352000, grad_norm=4.2193827629089355, loss=0.6385989785194397
I0301 15:32:44.567007 139590144857856 logging_writer.py:48] [352100] global_step=352100, grad_norm=4.6520915031433105, loss=0.6237276792526245
I0301 15:33:18.551377 139589717059328 logging_writer.py:48] [352200] global_step=352200, grad_norm=4.611705780029297, loss=0.6378605365753174
I0301 15:33:52.517100 139590144857856 logging_writer.py:48] [352300] global_step=352300, grad_norm=4.340338706970215, loss=0.6010226011276245
I0301 15:34:26.493354 139589717059328 logging_writer.py:48] [352400] global_step=352400, grad_norm=4.316783905029297, loss=0.6366068124771118
I0301 15:35:00.455215 139590144857856 logging_writer.py:48] [352500] global_step=352500, grad_norm=4.8357462882995605, loss=0.5569735169410706
I0301 15:35:34.415993 139589717059328 logging_writer.py:48] [352600] global_step=352600, grad_norm=4.7339982986450195, loss=0.6839163899421692
I0301 15:36:08.371638 139590144857856 logging_writer.py:48] [352700] global_step=352700, grad_norm=4.589917182922363, loss=0.6393219232559204
I0301 15:36:42.405238 139589717059328 logging_writer.py:48] [352800] global_step=352800, grad_norm=4.422487735748291, loss=0.5735319256782532
I0301 15:37:16.374668 139590144857856 logging_writer.py:48] [352900] global_step=352900, grad_norm=4.6000542640686035, loss=0.6190341711044312
I0301 15:37:50.327391 139589717059328 logging_writer.py:48] [353000] global_step=353000, grad_norm=4.7765092849731445, loss=0.657730221748352
I0301 15:38:24.297574 139590144857856 logging_writer.py:48] [353100] global_step=353100, grad_norm=4.952615261077881, loss=0.6172911524772644
I0301 15:38:47.214059 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:38:53.236524 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:39:01.623409 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:39:04.017553 139753105983296 submission_runner.py:411] Time since start: 124075.02s, 	Step: 353169, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14446964859962463, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.0556888580322266, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.819525122642517, 'test/num_examples': 10000, 'score': 119916.00923371315, 'total_duration': 124075.02059555054, 'accumulated_submission_time': 119916.00923371315, 'accumulated_eval_time': 4131.2061858177185, 'accumulated_logging_time': 15.391158103942871}
I0301 15:39:04.098954 139589708666624 logging_writer.py:48] [353169] accumulated_eval_time=4131.206186, accumulated_logging_time=15.391158, accumulated_submission_time=119916.009234, global_step=353169, preemption_count=0, score=119916.009234, test/accuracy=0.630000, test/loss=1.819525, test/num_examples=10000, total_duration=124075.020596, train/accuracy=0.961416, train/loss=0.144470, validation/accuracy=0.754520, validation/loss=1.055689, validation/num_examples=50000
I0301 15:39:14.940167 139590161643264 logging_writer.py:48] [353200] global_step=353200, grad_norm=4.586704254150391, loss=0.6031123995780945
I0301 15:39:48.829085 139589708666624 logging_writer.py:48] [353300] global_step=353300, grad_norm=4.426141738891602, loss=0.6141551733016968
I0301 15:40:22.722965 139590161643264 logging_writer.py:48] [353400] global_step=353400, grad_norm=4.253071308135986, loss=0.5656969547271729
I0301 15:40:56.677760 139589708666624 logging_writer.py:48] [353500] global_step=353500, grad_norm=4.552642822265625, loss=0.6192415356636047
I0301 15:41:30.663168 139590161643264 logging_writer.py:48] [353600] global_step=353600, grad_norm=4.792585372924805, loss=0.6794592142105103
I0301 15:42:04.597388 139589708666624 logging_writer.py:48] [353700] global_step=353700, grad_norm=4.405065536499023, loss=0.5803033709526062
I0301 15:42:38.569618 139590161643264 logging_writer.py:48] [353800] global_step=353800, grad_norm=4.3780951499938965, loss=0.5856307744979858
I0301 15:43:12.568532 139589708666624 logging_writer.py:48] [353900] global_step=353900, grad_norm=5.00272274017334, loss=0.7034953236579895
I0301 15:43:46.531837 139590161643264 logging_writer.py:48] [354000] global_step=354000, grad_norm=4.517963886260986, loss=0.5998294949531555
I0301 15:44:20.479096 139589708666624 logging_writer.py:48] [354100] global_step=354100, grad_norm=4.70422887802124, loss=0.6210556030273438
I0301 15:44:54.424050 139590161643264 logging_writer.py:48] [354200] global_step=354200, grad_norm=4.5318098068237305, loss=0.6873328685760498
I0301 15:45:28.361292 139589708666624 logging_writer.py:48] [354300] global_step=354300, grad_norm=4.936063766479492, loss=0.6549367308616638
I0301 15:46:02.308157 139590161643264 logging_writer.py:48] [354400] global_step=354400, grad_norm=4.818939208984375, loss=0.5987644791603088
I0301 15:46:36.243087 139589708666624 logging_writer.py:48] [354500] global_step=354500, grad_norm=4.5826616287231445, loss=0.6066969037055969
I0301 15:47:10.182192 139590161643264 logging_writer.py:48] [354600] global_step=354600, grad_norm=4.526602268218994, loss=0.6216675639152527
I0301 15:47:34.109835 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:47:40.121583 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:47:48.463148 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:47:50.829004 139753105983296 submission_runner.py:411] Time since start: 124601.83s, 	Step: 354672, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14707031846046448, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0547600984573364, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8176758289337158, 'test/num_examples': 10000, 'score': 120425.9566578865, 'total_duration': 124601.83222198486, 'accumulated_submission_time': 120425.9566578865, 'accumulated_eval_time': 4147.925310850143, 'accumulated_logging_time': 15.483352661132812}
I0301 15:47:50.904292 139589708666624 logging_writer.py:48] [354672] accumulated_eval_time=4147.925311, accumulated_logging_time=15.483353, accumulated_submission_time=120425.956658, global_step=354672, preemption_count=0, score=120425.956658, test/accuracy=0.630400, test/loss=1.817676, test/num_examples=10000, total_duration=124601.832222, train/accuracy=0.960619, train/loss=0.147070, validation/accuracy=0.755100, validation/loss=1.054760, validation/num_examples=50000
I0301 15:48:00.748927 139589717059328 logging_writer.py:48] [354700] global_step=354700, grad_norm=4.8237810134887695, loss=0.654421329498291
I0301 15:48:34.641097 139589708666624 logging_writer.py:48] [354800] global_step=354800, grad_norm=4.398550510406494, loss=0.6495457887649536
I0301 15:49:08.640474 139589717059328 logging_writer.py:48] [354900] global_step=354900, grad_norm=4.5921173095703125, loss=0.590644896030426
I0301 15:49:42.617944 139589708666624 logging_writer.py:48] [355000] global_step=355000, grad_norm=4.7738261222839355, loss=0.6118516325950623
I0301 15:50:16.548490 139589717059328 logging_writer.py:48] [355100] global_step=355100, grad_norm=4.39538049697876, loss=0.5380592346191406
I0301 15:50:50.500870 139589708666624 logging_writer.py:48] [355200] global_step=355200, grad_norm=3.988492965698242, loss=0.5461437106132507
I0301 15:51:24.452605 139589717059328 logging_writer.py:48] [355300] global_step=355300, grad_norm=4.623208522796631, loss=0.7092207670211792
I0301 15:51:58.393239 139589708666624 logging_writer.py:48] [355400] global_step=355400, grad_norm=4.247690200805664, loss=0.6101371049880981
I0301 15:52:32.363429 139589717059328 logging_writer.py:48] [355500] global_step=355500, grad_norm=4.49092435836792, loss=0.6295192837715149
I0301 15:53:06.330075 139589708666624 logging_writer.py:48] [355600] global_step=355600, grad_norm=4.423524379730225, loss=0.6048136949539185
I0301 15:53:40.286917 139589717059328 logging_writer.py:48] [355700] global_step=355700, grad_norm=4.517351150512695, loss=0.5875056982040405
I0301 15:54:14.255534 139589708666624 logging_writer.py:48] [355800] global_step=355800, grad_norm=4.85573673248291, loss=0.6721755266189575
I0301 15:54:48.164822 139589717059328 logging_writer.py:48] [355900] global_step=355900, grad_norm=4.615782737731934, loss=0.5884407758712769
I0301 15:55:22.185602 139589708666624 logging_writer.py:48] [356000] global_step=356000, grad_norm=4.73924446105957, loss=0.6817089319229126
I0301 15:55:56.117127 139589717059328 logging_writer.py:48] [356100] global_step=356100, grad_norm=4.72349739074707, loss=0.62711501121521
I0301 15:56:21.044888 139753105983296 spec.py:321] Evaluating on the training split.
I0301 15:56:27.065383 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 15:56:35.378299 139753105983296 spec.py:349] Evaluating on the test split.
I0301 15:56:37.673065 139753105983296 submission_runner.py:411] Time since start: 125128.68s, 	Step: 356175, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14726388454437256, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 1.056190848350525, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8197356462478638, 'test/num_examples': 10000, 'score': 120936.03289294243, 'total_duration': 125128.67629480362, 'accumulated_submission_time': 120936.03289294243, 'accumulated_eval_time': 4164.55343580246, 'accumulated_logging_time': 15.569127082824707}
I0301 15:56:37.754736 139589717059328 logging_writer.py:48] [356175] accumulated_eval_time=4164.553436, accumulated_logging_time=15.569127, accumulated_submission_time=120936.032893, global_step=356175, preemption_count=0, score=120936.032893, test/accuracy=0.629800, test/loss=1.819736, test/num_examples=10000, total_duration=125128.676295, train/accuracy=0.960459, train/loss=0.147264, validation/accuracy=0.755420, validation/loss=1.056191, validation/num_examples=50000
I0301 15:56:46.605473 139590170035968 logging_writer.py:48] [356200] global_step=356200, grad_norm=4.512811183929443, loss=0.6190814971923828
I0301 15:57:20.534779 139589717059328 logging_writer.py:48] [356300] global_step=356300, grad_norm=4.826376914978027, loss=0.7152944803237915
I0301 15:57:54.433063 139590170035968 logging_writer.py:48] [356400] global_step=356400, grad_norm=4.481049060821533, loss=0.5676017999649048
I0301 15:58:28.382866 139589717059328 logging_writer.py:48] [356500] global_step=356500, grad_norm=4.373455047607422, loss=0.5670356750488281
I0301 15:59:02.371780 139590170035968 logging_writer.py:48] [356600] global_step=356600, grad_norm=4.783160209655762, loss=0.6726175546646118
I0301 15:59:36.327234 139589717059328 logging_writer.py:48] [356700] global_step=356700, grad_norm=4.642400741577148, loss=0.5718486905097961
I0301 16:00:10.282364 139590170035968 logging_writer.py:48] [356800] global_step=356800, grad_norm=4.709981441497803, loss=0.6135857105255127
I0301 16:00:44.214872 139589717059328 logging_writer.py:48] [356900] global_step=356900, grad_norm=4.555363655090332, loss=0.6447990536689758
I0301 16:01:18.224362 139590170035968 logging_writer.py:48] [357000] global_step=357000, grad_norm=4.878528118133545, loss=0.6782490015029907
I0301 16:01:52.166202 139589717059328 logging_writer.py:48] [357100] global_step=357100, grad_norm=4.298526287078857, loss=0.6437116861343384
I0301 16:02:26.109679 139590170035968 logging_writer.py:48] [357200] global_step=357200, grad_norm=4.805004119873047, loss=0.6735035181045532
I0301 16:03:00.086180 139589717059328 logging_writer.py:48] [357300] global_step=357300, grad_norm=4.81852388381958, loss=0.6715335845947266
I0301 16:03:34.026992 139590170035968 logging_writer.py:48] [357400] global_step=357400, grad_norm=4.675189018249512, loss=0.7269580960273743
I0301 16:04:07.999289 139589717059328 logging_writer.py:48] [357500] global_step=357500, grad_norm=4.645999431610107, loss=0.6228237748146057
I0301 16:04:41.952573 139590170035968 logging_writer.py:48] [357600] global_step=357600, grad_norm=4.356459617614746, loss=0.5662741661071777
I0301 16:05:07.915506 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:05:13.922189 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:05:22.201534 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:05:24.503207 139753105983296 submission_runner.py:411] Time since start: 125655.51s, 	Step: 357678, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14655163884162903, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0553967952728271, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8201425075531006, 'test/num_examples': 10000, 'score': 121446.13084578514, 'total_duration': 125655.50641322136, 'accumulated_submission_time': 121446.13084578514, 'accumulated_eval_time': 4181.1410665512085, 'accumulated_logging_time': 15.66112756729126}
I0301 16:05:24.593883 139590153250560 logging_writer.py:48] [357678] accumulated_eval_time=4181.141067, accumulated_logging_time=15.661128, accumulated_submission_time=121446.130846, global_step=357678, preemption_count=0, score=121446.130846, test/accuracy=0.630100, test/loss=1.820143, test/num_examples=10000, total_duration=125655.506413, train/accuracy=0.961476, train/loss=0.146552, validation/accuracy=0.754740, validation/loss=1.055397, validation/num_examples=50000
I0301 16:05:32.420088 139590161643264 logging_writer.py:48] [357700] global_step=357700, grad_norm=4.642704486846924, loss=0.5854441523551941
I0301 16:06:06.332863 139590153250560 logging_writer.py:48] [357800] global_step=357800, grad_norm=4.847048759460449, loss=0.5675410628318787
I0301 16:06:40.277820 139590161643264 logging_writer.py:48] [357900] global_step=357900, grad_norm=5.26246976852417, loss=0.703604519367218
I0301 16:07:14.241686 139590153250560 logging_writer.py:48] [358000] global_step=358000, grad_norm=4.55529260635376, loss=0.6469465494155884
I0301 16:07:48.285352 139590161643264 logging_writer.py:48] [358100] global_step=358100, grad_norm=4.588151931762695, loss=0.5817654132843018
I0301 16:08:22.233375 139590153250560 logging_writer.py:48] [358200] global_step=358200, grad_norm=4.436253070831299, loss=0.6081218123435974
I0301 16:08:56.201642 139590161643264 logging_writer.py:48] [358300] global_step=358300, grad_norm=4.544790744781494, loss=0.5865188837051392
I0301 16:09:30.167687 139590153250560 logging_writer.py:48] [358400] global_step=358400, grad_norm=4.522936820983887, loss=0.6530047059059143
I0301 16:10:04.150469 139590161643264 logging_writer.py:48] [358500] global_step=358500, grad_norm=4.5456929206848145, loss=0.5410258173942566
I0301 16:10:38.117730 139590153250560 logging_writer.py:48] [358600] global_step=358600, grad_norm=4.745326995849609, loss=0.6148635149002075
I0301 16:11:12.059148 139590161643264 logging_writer.py:48] [358700] global_step=358700, grad_norm=4.628909111022949, loss=0.6115061044692993
I0301 16:11:46.013087 139590153250560 logging_writer.py:48] [358800] global_step=358800, grad_norm=4.425398349761963, loss=0.596258282661438
I0301 16:12:19.968135 139590161643264 logging_writer.py:48] [358900] global_step=358900, grad_norm=4.238412380218506, loss=0.6319093704223633
I0301 16:12:53.924723 139590153250560 logging_writer.py:48] [359000] global_step=359000, grad_norm=4.454974174499512, loss=0.6046752333641052
I0301 16:13:27.888430 139590161643264 logging_writer.py:48] [359100] global_step=359100, grad_norm=4.518299102783203, loss=0.5599872469902039
I0301 16:13:54.574549 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:14:00.660696 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:14:09.111073 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:14:11.333103 139753105983296 submission_runner.py:411] Time since start: 126182.34s, 	Step: 359180, 	{'train/accuracy': 0.9594826102256775, 'train/loss': 0.14744140207767487, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.055631160736084, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8196020126342773, 'test/num_examples': 10000, 'score': 121956.04799413681, 'total_duration': 126182.3363289833, 'accumulated_submission_time': 121956.04799413681, 'accumulated_eval_time': 4197.899563074112, 'accumulated_logging_time': 15.761876106262207}
I0301 16:14:11.413595 139590144857856 logging_writer.py:48] [359180] accumulated_eval_time=4197.899563, accumulated_logging_time=15.761876, accumulated_submission_time=121956.047994, global_step=359180, preemption_count=0, score=121956.047994, test/accuracy=0.630600, test/loss=1.819602, test/num_examples=10000, total_duration=126182.336329, train/accuracy=0.959483, train/loss=0.147441, validation/accuracy=0.755180, validation/loss=1.055631, validation/num_examples=50000
I0301 16:14:18.557544 139590170035968 logging_writer.py:48] [359200] global_step=359200, grad_norm=4.810804843902588, loss=0.5983824133872986
I0301 16:14:52.398559 139590144857856 logging_writer.py:48] [359300] global_step=359300, grad_norm=4.805422306060791, loss=0.622992992401123
I0301 16:15:26.331068 139590170035968 logging_writer.py:48] [359400] global_step=359400, grad_norm=4.057391166687012, loss=0.5776434540748596
I0301 16:16:00.269618 139590144857856 logging_writer.py:48] [359500] global_step=359500, grad_norm=4.438570022583008, loss=0.5476163029670715
I0301 16:16:34.183439 139590170035968 logging_writer.py:48] [359600] global_step=359600, grad_norm=4.768903732299805, loss=0.578580379486084
I0301 16:17:08.142515 139590144857856 logging_writer.py:48] [359700] global_step=359700, grad_norm=4.4496331214904785, loss=0.6836733818054199
I0301 16:17:42.067561 139590170035968 logging_writer.py:48] [359800] global_step=359800, grad_norm=5.259255886077881, loss=0.689855694770813
I0301 16:18:15.991861 139590144857856 logging_writer.py:48] [359900] global_step=359900, grad_norm=4.581419944763184, loss=0.653825581073761
I0301 16:18:49.952167 139590170035968 logging_writer.py:48] [360000] global_step=360000, grad_norm=4.495987415313721, loss=0.6930422782897949
I0301 16:19:23.874656 139590144857856 logging_writer.py:48] [360100] global_step=360100, grad_norm=5.097063064575195, loss=0.6810641288757324
I0301 16:19:57.886845 139590170035968 logging_writer.py:48] [360200] global_step=360200, grad_norm=4.527072429656982, loss=0.6164076328277588
I0301 16:20:31.827076 139590144857856 logging_writer.py:48] [360300] global_step=360300, grad_norm=4.577744960784912, loss=0.6029503345489502
I0301 16:21:05.751250 139590170035968 logging_writer.py:48] [360400] global_step=360400, grad_norm=4.480232238769531, loss=0.5780148506164551
I0301 16:21:39.678738 139590144857856 logging_writer.py:48] [360500] global_step=360500, grad_norm=4.501287460327148, loss=0.6509613990783691
I0301 16:22:13.608435 139590170035968 logging_writer.py:48] [360600] global_step=360600, grad_norm=4.965163707733154, loss=0.6215691566467285
I0301 16:22:41.591291 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:22:47.573340 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:22:55.883896 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:22:58.157938 139753105983296 submission_runner.py:411] Time since start: 126709.16s, 	Step: 360684, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14818784594535828, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0551674365997314, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.8188576698303223, 'test/num_examples': 10000, 'score': 122466.16347789764, 'total_duration': 126709.16116809845, 'accumulated_submission_time': 122466.16347789764, 'accumulated_eval_time': 4214.466181278229, 'accumulated_logging_time': 15.852198362350464}
I0301 16:22:58.237174 139590153250560 logging_writer.py:48] [360684] accumulated_eval_time=4214.466181, accumulated_logging_time=15.852198, accumulated_submission_time=122466.163478, global_step=360684, preemption_count=0, score=122466.163478, test/accuracy=0.629400, test/loss=1.818858, test/num_examples=10000, total_duration=126709.161168, train/accuracy=0.960061, train/loss=0.148188, validation/accuracy=0.755060, validation/loss=1.055167, validation/num_examples=50000
I0301 16:23:04.004049 139590161643264 logging_writer.py:48] [360700] global_step=360700, grad_norm=4.363109111785889, loss=0.6164065599441528
I0301 16:23:37.888779 139590153250560 logging_writer.py:48] [360800] global_step=360800, grad_norm=4.686255931854248, loss=0.5622087121009827
I0301 16:24:11.819125 139590161643264 logging_writer.py:48] [360900] global_step=360900, grad_norm=4.733986854553223, loss=0.6833311915397644
I0301 16:24:45.778784 139590153250560 logging_writer.py:48] [361000] global_step=361000, grad_norm=4.498837471008301, loss=0.5864400267601013
I0301 16:25:19.742030 139590161643264 logging_writer.py:48] [361100] global_step=361100, grad_norm=4.491715908050537, loss=0.579085111618042
I0301 16:25:53.714136 139590153250560 logging_writer.py:48] [361200] global_step=361200, grad_norm=4.3463358879089355, loss=0.6593241095542908
I0301 16:26:27.743895 139590161643264 logging_writer.py:48] [361300] global_step=361300, grad_norm=4.966346263885498, loss=0.668595552444458
I0301 16:27:01.684721 139590153250560 logging_writer.py:48] [361400] global_step=361400, grad_norm=4.484511852264404, loss=0.6429175138473511
I0301 16:27:35.645933 139590161643264 logging_writer.py:48] [361500] global_step=361500, grad_norm=4.0878682136535645, loss=0.5145403146743774
I0301 16:28:09.598494 139590153250560 logging_writer.py:48] [361600] global_step=361600, grad_norm=5.212381839752197, loss=0.6960489749908447
I0301 16:28:43.551520 139590161643264 logging_writer.py:48] [361700] global_step=361700, grad_norm=4.299857139587402, loss=0.6346780061721802
I0301 16:29:17.534754 139590153250560 logging_writer.py:48] [361800] global_step=361800, grad_norm=4.678682327270508, loss=0.6606388688087463
I0301 16:29:51.485177 139590161643264 logging_writer.py:48] [361900] global_step=361900, grad_norm=4.812150478363037, loss=0.6530660390853882
I0301 16:30:25.419269 139590153250560 logging_writer.py:48] [362000] global_step=362000, grad_norm=4.888434410095215, loss=0.5662021636962891
I0301 16:30:59.387507 139590161643264 logging_writer.py:48] [362100] global_step=362100, grad_norm=4.625249862670898, loss=0.6668566465377808
I0301 16:31:28.396006 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:31:34.428271 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:31:42.687352 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:31:45.593436 139753105983296 submission_runner.py:411] Time since start: 127236.60s, 	Step: 362187, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14490057528018951, 'validation/accuracy': 0.7542600035667419, 'validation/loss': 1.055108666419983, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.818588137626648, 'test/num_examples': 10000, 'score': 122976.25839185715, 'total_duration': 127236.59667944908, 'accumulated_submission_time': 122976.25839185715, 'accumulated_eval_time': 4231.663587808609, 'accumulated_logging_time': 15.942219734191895}
I0301 16:31:45.659624 139589717059328 logging_writer.py:48] [362187] accumulated_eval_time=4231.663588, accumulated_logging_time=15.942220, accumulated_submission_time=122976.258392, global_step=362187, preemption_count=0, score=122976.258392, test/accuracy=0.630700, test/loss=1.818588, test/num_examples=10000, total_duration=127236.596679, train/accuracy=0.961037, train/loss=0.144901, validation/accuracy=0.754260, validation/loss=1.055109, validation/num_examples=50000
I0301 16:31:50.403550 139589725452032 logging_writer.py:48] [362200] global_step=362200, grad_norm=4.633202075958252, loss=0.6720463633537292
I0301 16:32:24.361204 139589717059328 logging_writer.py:48] [362300] global_step=362300, grad_norm=4.425605773925781, loss=0.6566342115402222
I0301 16:32:58.303242 139589725452032 logging_writer.py:48] [362400] global_step=362400, grad_norm=4.329494953155518, loss=0.5847631692886353
I0301 16:33:32.266479 139589717059328 logging_writer.py:48] [362500] global_step=362500, grad_norm=4.564737796783447, loss=0.5953779220581055
I0301 16:34:06.229891 139589725452032 logging_writer.py:48] [362600] global_step=362600, grad_norm=4.892200469970703, loss=0.658859372138977
I0301 16:34:40.183452 139589717059328 logging_writer.py:48] [362700] global_step=362700, grad_norm=4.792594909667969, loss=0.5812323093414307
I0301 16:35:14.121877 139589725452032 logging_writer.py:48] [362800] global_step=362800, grad_norm=4.684661388397217, loss=0.6342817544937134
I0301 16:35:48.094958 139589717059328 logging_writer.py:48] [362900] global_step=362900, grad_norm=4.709565162658691, loss=0.6744386553764343
I0301 16:36:22.038389 139589725452032 logging_writer.py:48] [363000] global_step=363000, grad_norm=4.200915813446045, loss=0.6138164401054382
I0301 16:36:55.981881 139589717059328 logging_writer.py:48] [363100] global_step=363100, grad_norm=4.539706707000732, loss=0.5950527787208557
I0301 16:37:29.930668 139589725452032 logging_writer.py:48] [363200] global_step=363200, grad_norm=4.42209529876709, loss=0.6445336937904358
I0301 16:38:03.900878 139589717059328 logging_writer.py:48] [363300] global_step=363300, grad_norm=4.541995048522949, loss=0.5715702772140503
I0301 16:38:37.914629 139589725452032 logging_writer.py:48] [363400] global_step=363400, grad_norm=4.855817794799805, loss=0.5969581007957458
I0301 16:39:11.853584 139589717059328 logging_writer.py:48] [363500] global_step=363500, grad_norm=4.562251091003418, loss=0.6630313992500305
I0301 16:39:45.803106 139589725452032 logging_writer.py:48] [363600] global_step=363600, grad_norm=4.7276411056518555, loss=0.6551087498664856
I0301 16:40:15.845957 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:40:21.858063 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:40:30.202596 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:40:32.502264 139753105983296 submission_runner.py:411] Time since start: 127763.51s, 	Step: 363690, 	{'train/accuracy': 0.9592434167861938, 'train/loss': 0.14947111904621124, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0559954643249512, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8202086687088013, 'test/num_examples': 10000, 'score': 123486.3828933239, 'total_duration': 127763.50550031662, 'accumulated_submission_time': 123486.3828933239, 'accumulated_eval_time': 4248.319851875305, 'accumulated_logging_time': 16.017550230026245}
I0301 16:40:32.580988 139590170035968 logging_writer.py:48] [363690] accumulated_eval_time=4248.319852, accumulated_logging_time=16.017550, accumulated_submission_time=123486.382893, global_step=363690, preemption_count=0, score=123486.382893, test/accuracy=0.630600, test/loss=1.820209, test/num_examples=10000, total_duration=127763.505500, train/accuracy=0.959243, train/loss=0.149471, validation/accuracy=0.755220, validation/loss=1.055995, validation/num_examples=50000
I0301 16:40:36.314998 139590178428672 logging_writer.py:48] [363700] global_step=363700, grad_norm=4.462072372436523, loss=0.6100743412971497
I0301 16:41:10.224237 139590170035968 logging_writer.py:48] [363800] global_step=363800, grad_norm=4.5472941398620605, loss=0.6858971118927002
I0301 16:41:44.183712 139590178428672 logging_writer.py:48] [363900] global_step=363900, grad_norm=4.685418605804443, loss=0.60676109790802
I0301 16:42:18.138972 139590170035968 logging_writer.py:48] [364000] global_step=364000, grad_norm=4.667872905731201, loss=0.6420559287071228
I0301 16:42:52.078466 139590178428672 logging_writer.py:48] [364100] global_step=364100, grad_norm=4.733679294586182, loss=0.707459568977356
I0301 16:43:26.021252 139590170035968 logging_writer.py:48] [364200] global_step=364200, grad_norm=4.556568622589111, loss=0.6305165886878967
I0301 16:44:00.002005 139590178428672 logging_writer.py:48] [364300] global_step=364300, grad_norm=4.127305030822754, loss=0.6104499101638794
I0301 16:44:33.990853 139590170035968 logging_writer.py:48] [364400] global_step=364400, grad_norm=4.7064595222473145, loss=0.6239577531814575
I0301 16:45:07.959146 139590178428672 logging_writer.py:48] [364500] global_step=364500, grad_norm=4.628087997436523, loss=0.5928633213043213
I0301 16:45:41.919107 139590170035968 logging_writer.py:48] [364600] global_step=364600, grad_norm=5.112932205200195, loss=0.6498413681983948
I0301 16:46:15.902889 139590178428672 logging_writer.py:48] [364700] global_step=364700, grad_norm=4.9604716300964355, loss=0.6962602734565735
I0301 16:46:49.864301 139590170035968 logging_writer.py:48] [364800] global_step=364800, grad_norm=4.442452430725098, loss=0.6510546803474426
I0301 16:47:23.858251 139590178428672 logging_writer.py:48] [364900] global_step=364900, grad_norm=4.363899230957031, loss=0.6393281817436218
I0301 16:47:57.840142 139590170035968 logging_writer.py:48] [365000] global_step=365000, grad_norm=3.795949697494507, loss=0.5200188159942627
I0301 16:48:31.838326 139590178428672 logging_writer.py:48] [365100] global_step=365100, grad_norm=4.406937599182129, loss=0.5958365797996521
I0301 16:49:02.570610 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:49:08.607683 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:49:17.014673 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:49:19.246748 139753105983296 submission_runner.py:411] Time since start: 128290.25s, 	Step: 365192, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.14690491557121277, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0571703910827637, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8223227262496948, 'test/num_examples': 10000, 'score': 123996.3074054718, 'total_duration': 128290.24997830391, 'accumulated_submission_time': 123996.3074054718, 'accumulated_eval_time': 4264.995944738388, 'accumulated_logging_time': 16.10771369934082}
I0301 16:49:19.332320 139590144857856 logging_writer.py:48] [365192] accumulated_eval_time=4264.995945, accumulated_logging_time=16.107714, accumulated_submission_time=123996.307405, global_step=365192, preemption_count=0, score=123996.307405, test/accuracy=0.629900, test/loss=1.822323, test/num_examples=10000, total_duration=128290.249978, train/accuracy=0.959682, train/loss=0.146905, validation/accuracy=0.754940, validation/loss=1.057170, validation/num_examples=50000
I0301 16:49:22.401154 139590153250560 logging_writer.py:48] [365200] global_step=365200, grad_norm=4.16384744644165, loss=0.5587250590324402
I0301 16:49:56.317955 139590144857856 logging_writer.py:48] [365300] global_step=365300, grad_norm=4.428802013397217, loss=0.6066614985466003
I0301 16:50:30.245922 139590153250560 logging_writer.py:48] [365400] global_step=365400, grad_norm=4.405909538269043, loss=0.6315620541572571
I0301 16:51:04.316949 139590144857856 logging_writer.py:48] [365500] global_step=365500, grad_norm=4.573992729187012, loss=0.6279377937316895
I0301 16:51:38.250293 139590153250560 logging_writer.py:48] [365600] global_step=365600, grad_norm=4.627209663391113, loss=0.6689496040344238
I0301 16:52:12.211719 139590144857856 logging_writer.py:48] [365700] global_step=365700, grad_norm=4.61626672744751, loss=0.6246092319488525
I0301 16:52:46.121576 139590153250560 logging_writer.py:48] [365800] global_step=365800, grad_norm=4.695004463195801, loss=0.6306505799293518
I0301 16:53:20.064884 139590144857856 logging_writer.py:48] [365900] global_step=365900, grad_norm=4.641969680786133, loss=0.613793134689331
I0301 16:53:54.004550 139590153250560 logging_writer.py:48] [366000] global_step=366000, grad_norm=4.733806133270264, loss=0.6469508409500122
I0301 16:54:27.953760 139590144857856 logging_writer.py:48] [366100] global_step=366100, grad_norm=4.404716968536377, loss=0.573249101638794
I0301 16:55:01.880696 139590153250560 logging_writer.py:48] [366200] global_step=366200, grad_norm=4.839430332183838, loss=0.6052296161651611
I0301 16:55:35.858816 139590144857856 logging_writer.py:48] [366300] global_step=366300, grad_norm=4.288985252380371, loss=0.5793105959892273
I0301 16:56:09.795352 139590153250560 logging_writer.py:48] [366400] global_step=366400, grad_norm=4.952956676483154, loss=0.6722639799118042
I0301 16:56:43.739156 139590144857856 logging_writer.py:48] [366500] global_step=366500, grad_norm=4.426789283752441, loss=0.5807778835296631
I0301 16:57:17.752844 139590153250560 logging_writer.py:48] [366600] global_step=366600, grad_norm=5.117855072021484, loss=0.6654873490333557
I0301 16:57:49.523351 139753105983296 spec.py:321] Evaluating on the training split.
I0301 16:57:55.600751 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 16:58:03.958591 139753105983296 spec.py:349] Evaluating on the test split.
I0301 16:58:06.226305 139753105983296 submission_runner.py:411] Time since start: 128817.23s, 	Step: 366695, 	{'train/accuracy': 0.9611567258834839, 'train/loss': 0.1489018052816391, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.055810809135437, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8210707902908325, 'test/num_examples': 10000, 'score': 124506.4336566925, 'total_duration': 128817.22952365875, 'accumulated_submission_time': 124506.4336566925, 'accumulated_eval_time': 4281.69885134697, 'accumulated_logging_time': 16.204543352127075}
I0301 16:58:06.304923 139589725452032 logging_writer.py:48] [366695] accumulated_eval_time=4281.698851, accumulated_logging_time=16.204543, accumulated_submission_time=124506.433657, global_step=366695, preemption_count=0, score=124506.433657, test/accuracy=0.630200, test/loss=1.821071, test/num_examples=10000, total_duration=128817.229524, train/accuracy=0.961157, train/loss=0.148902, validation/accuracy=0.755000, validation/loss=1.055811, validation/num_examples=50000
I0301 16:58:08.359829 139590144857856 logging_writer.py:48] [366700] global_step=366700, grad_norm=4.560957431793213, loss=0.6401480436325073
I0301 16:58:42.341230 139589725452032 logging_writer.py:48] [366800] global_step=366800, grad_norm=4.4471354484558105, loss=0.624986469745636
I0301 16:59:16.330810 139590144857856 logging_writer.py:48] [366900] global_step=366900, grad_norm=4.552096843719482, loss=0.6524820923805237
I0301 16:59:50.313204 139589725452032 logging_writer.py:48] [367000] global_step=367000, grad_norm=4.86428165435791, loss=0.6773732900619507
I0301 17:00:24.277884 139590144857856 logging_writer.py:48] [367100] global_step=367100, grad_norm=4.503818511962891, loss=0.5789216756820679
I0301 17:00:58.250974 139589725452032 logging_writer.py:48] [367200] global_step=367200, grad_norm=4.339549541473389, loss=0.6313645839691162
I0301 17:01:32.208923 139590144857856 logging_writer.py:48] [367300] global_step=367300, grad_norm=4.516002655029297, loss=0.6269004344940186
I0301 17:02:06.168669 139589725452032 logging_writer.py:48] [367400] global_step=367400, grad_norm=4.575046062469482, loss=0.5851525664329529
I0301 17:02:40.132903 139590144857856 logging_writer.py:48] [367500] global_step=367500, grad_norm=4.449867248535156, loss=0.5883498191833496
I0301 17:03:14.171923 139589725452032 logging_writer.py:48] [367600] global_step=367600, grad_norm=4.550697326660156, loss=0.5666783452033997
I0301 17:03:48.158462 139590144857856 logging_writer.py:48] [367700] global_step=367700, grad_norm=4.617774486541748, loss=0.643670916557312
I0301 17:04:22.144267 139589725452032 logging_writer.py:48] [367800] global_step=367800, grad_norm=4.234102725982666, loss=0.5942462682723999
I0301 17:04:56.106911 139590144857856 logging_writer.py:48] [367900] global_step=367900, grad_norm=4.613662242889404, loss=0.6231982111930847
I0301 17:05:30.062873 139589725452032 logging_writer.py:48] [368000] global_step=368000, grad_norm=4.273669242858887, loss=0.5835589170455933
I0301 17:06:04.014319 139590144857856 logging_writer.py:48] [368100] global_step=368100, grad_norm=4.521653652191162, loss=0.6375806331634521
I0301 17:06:36.410567 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:06:42.430613 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:06:50.721041 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:06:53.003262 139753105983296 submission_runner.py:411] Time since start: 129344.01s, 	Step: 368197, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.14846695959568024, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0560575723648071, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8189374208450317, 'test/num_examples': 10000, 'score': 125016.474984169, 'total_duration': 129344.00648355484, 'accumulated_submission_time': 125016.474984169, 'accumulated_eval_time': 4298.291492462158, 'accumulated_logging_time': 16.294630765914917}
I0301 17:06:53.083270 139589717059328 logging_writer.py:48] [368197] accumulated_eval_time=4298.291492, accumulated_logging_time=16.294631, accumulated_submission_time=125016.474984, global_step=368197, preemption_count=0, score=125016.474984, test/accuracy=0.629900, test/loss=1.818937, test/num_examples=10000, total_duration=129344.006484, train/accuracy=0.959702, train/loss=0.148467, validation/accuracy=0.755000, validation/loss=1.056058, validation/num_examples=50000
I0301 17:06:54.458849 139589725452032 logging_writer.py:48] [368200] global_step=368200, grad_norm=4.533120155334473, loss=0.6231264472007751
I0301 17:07:28.315132 139589717059328 logging_writer.py:48] [368300] global_step=368300, grad_norm=4.529740810394287, loss=0.6009660363197327
I0301 17:08:02.269948 139589725452032 logging_writer.py:48] [368400] global_step=368400, grad_norm=4.488867282867432, loss=0.6016563177108765
I0301 17:08:36.191340 139589717059328 logging_writer.py:48] [368500] global_step=368500, grad_norm=4.137635707855225, loss=0.5830737352371216
I0301 17:09:10.188710 139589725452032 logging_writer.py:48] [368600] global_step=368600, grad_norm=5.0081939697265625, loss=0.5865345597267151
I0301 17:09:44.088311 139589717059328 logging_writer.py:48] [368700] global_step=368700, grad_norm=4.595426559448242, loss=0.7076293230056763
I0301 17:10:18.019244 139589725452032 logging_writer.py:48] [368800] global_step=368800, grad_norm=4.815210342407227, loss=0.6133245825767517
I0301 17:10:51.944922 139589717059328 logging_writer.py:48] [368900] global_step=368900, grad_norm=4.41265869140625, loss=0.6240714192390442
I0301 17:11:25.889188 139589725452032 logging_writer.py:48] [369000] global_step=369000, grad_norm=4.338740348815918, loss=0.6423051953315735
I0301 17:11:59.835810 139589717059328 logging_writer.py:48] [369100] global_step=369100, grad_norm=4.50605583190918, loss=0.6610225439071655
I0301 17:12:33.753564 139589725452032 logging_writer.py:48] [369200] global_step=369200, grad_norm=4.714667320251465, loss=0.594728410243988
I0301 17:13:07.671755 139589717059328 logging_writer.py:48] [369300] global_step=369300, grad_norm=4.516739845275879, loss=0.6589219570159912
I0301 17:13:41.596525 139589725452032 logging_writer.py:48] [369400] global_step=369400, grad_norm=4.842733383178711, loss=0.7171181440353394
I0301 17:14:15.522506 139589717059328 logging_writer.py:48] [369500] global_step=369500, grad_norm=4.363638401031494, loss=0.6010211706161499
I0301 17:14:49.450295 139589725452032 logging_writer.py:48] [369600] global_step=369600, grad_norm=4.44777774810791, loss=0.6025497913360596
I0301 17:15:23.435884 139589717059328 logging_writer.py:48] [369700] global_step=369700, grad_norm=4.684122085571289, loss=0.6596342325210571
I0301 17:15:23.442932 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:15:29.476106 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:15:37.848952 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:15:40.149365 139753105983296 submission_runner.py:411] Time since start: 129871.15s, 	Step: 369701, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.14550314843654633, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0541020631790161, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8172831535339355, 'test/num_examples': 10000, 'score': 125526.76858377457, 'total_duration': 129871.15259885788, 'accumulated_submission_time': 125526.76858377457, 'accumulated_eval_time': 4314.997852563858, 'accumulated_logging_time': 16.387300729751587}
I0301 17:15:40.233754 139589708666624 logging_writer.py:48] [369701] accumulated_eval_time=4314.997853, accumulated_logging_time=16.387301, accumulated_submission_time=125526.768584, global_step=369701, preemption_count=0, score=125526.768584, test/accuracy=0.631200, test/loss=1.817283, test/num_examples=10000, total_duration=129871.152599, train/accuracy=0.961396, train/loss=0.145503, validation/accuracy=0.754880, validation/loss=1.054102, validation/num_examples=50000
I0301 17:16:14.161742 139589717059328 logging_writer.py:48] [369800] global_step=369800, grad_norm=4.4601030349731445, loss=0.6051419973373413
I0301 17:16:48.089596 139589708666624 logging_writer.py:48] [369900] global_step=369900, grad_norm=4.207464694976807, loss=0.6324998736381531
I0301 17:17:22.032755 139589717059328 logging_writer.py:48] [370000] global_step=370000, grad_norm=4.729281425476074, loss=0.6884520649909973
I0301 17:17:55.985030 139589708666624 logging_writer.py:48] [370100] global_step=370100, grad_norm=4.449036121368408, loss=0.5959459543228149
I0301 17:18:29.950929 139589717059328 logging_writer.py:48] [370200] global_step=370200, grad_norm=4.285002708435059, loss=0.5647155046463013
I0301 17:19:03.901928 139589708666624 logging_writer.py:48] [370300] global_step=370300, grad_norm=4.2740068435668945, loss=0.5467580556869507
I0301 17:19:37.880483 139589717059328 logging_writer.py:48] [370400] global_step=370400, grad_norm=4.220735549926758, loss=0.5504849553108215
I0301 17:20:11.851525 139589708666624 logging_writer.py:48] [370500] global_step=370500, grad_norm=4.510064125061035, loss=0.6791266202926636
I0301 17:20:45.832253 139589717059328 logging_writer.py:48] [370600] global_step=370600, grad_norm=4.859060287475586, loss=0.655305802822113
I0301 17:21:19.774890 139589708666624 logging_writer.py:48] [370700] global_step=370700, grad_norm=4.437414169311523, loss=0.6476307511329651
I0301 17:21:53.827759 139589717059328 logging_writer.py:48] [370800] global_step=370800, grad_norm=4.730083465576172, loss=0.571188747882843
I0301 17:22:27.809240 139589708666624 logging_writer.py:48] [370900] global_step=370900, grad_norm=4.796700954437256, loss=0.6632314920425415
I0301 17:23:01.774744 139589717059328 logging_writer.py:48] [371000] global_step=371000, grad_norm=4.502114772796631, loss=0.6747361421585083
I0301 17:23:35.717204 139589708666624 logging_writer.py:48] [371100] global_step=371100, grad_norm=4.2822136878967285, loss=0.5879766941070557
I0301 17:24:09.659569 139589717059328 logging_writer.py:48] [371200] global_step=371200, grad_norm=4.152616500854492, loss=0.5766547322273254
I0301 17:24:10.481256 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:24:16.456861 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:24:24.803601 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:24:27.202626 139753105983296 submission_runner.py:411] Time since start: 130398.21s, 	Step: 371204, 	{'train/accuracy': 0.959382951259613, 'train/loss': 0.14712736010551453, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.0547292232513428, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.81916081905365, 'test/num_examples': 10000, 'score': 126036.95020675659, 'total_duration': 130398.2058544159, 'accumulated_submission_time': 126036.95020675659, 'accumulated_eval_time': 4331.719168186188, 'accumulated_logging_time': 16.48409938812256}
I0301 17:24:27.284691 139590161643264 logging_writer.py:48] [371204] accumulated_eval_time=4331.719168, accumulated_logging_time=16.484099, accumulated_submission_time=126036.950207, global_step=371204, preemption_count=0, score=126036.950207, test/accuracy=0.630400, test/loss=1.819161, test/num_examples=10000, total_duration=130398.205854, train/accuracy=0.959383, train/loss=0.147127, validation/accuracy=0.755340, validation/loss=1.054729, validation/num_examples=50000
I0301 17:25:00.163006 139590170035968 logging_writer.py:48] [371300] global_step=371300, grad_norm=4.421265602111816, loss=0.6758712530136108
I0301 17:25:34.111627 139590161643264 logging_writer.py:48] [371400] global_step=371400, grad_norm=4.466701984405518, loss=0.5813185572624207
I0301 17:26:08.058537 139590170035968 logging_writer.py:48] [371500] global_step=371500, grad_norm=4.9215312004089355, loss=0.6009435057640076
I0301 17:26:41.956774 139590161643264 logging_writer.py:48] [371600] global_step=371600, grad_norm=4.587085247039795, loss=0.6478904485702515
I0301 17:27:15.888465 139590170035968 logging_writer.py:48] [371700] global_step=371700, grad_norm=4.429030895233154, loss=0.6124222278594971
I0301 17:27:49.918705 139590161643264 logging_writer.py:48] [371800] global_step=371800, grad_norm=4.447406768798828, loss=0.6244933009147644
I0301 17:28:23.871984 139590170035968 logging_writer.py:48] [371900] global_step=371900, grad_norm=5.120126247406006, loss=0.7146453261375427
I0301 17:28:57.827692 139590161643264 logging_writer.py:48] [372000] global_step=372000, grad_norm=4.497021198272705, loss=0.646969199180603
I0301 17:29:31.775233 139590170035968 logging_writer.py:48] [372100] global_step=372100, grad_norm=4.482021331787109, loss=0.6241421103477478
I0301 17:30:05.719820 139590161643264 logging_writer.py:48] [372200] global_step=372200, grad_norm=5.066617012023926, loss=0.7212448120117188
I0301 17:30:39.673025 139590170035968 logging_writer.py:48] [372300] global_step=372300, grad_norm=4.898768901824951, loss=0.6168557405471802
I0301 17:31:13.580200 139590161643264 logging_writer.py:48] [372400] global_step=372400, grad_norm=4.201321601867676, loss=0.5897526144981384
I0301 17:31:47.525480 139590170035968 logging_writer.py:48] [372500] global_step=372500, grad_norm=4.577886581420898, loss=0.6598913073539734
I0301 17:32:21.482759 139590161643264 logging_writer.py:48] [372600] global_step=372600, grad_norm=4.409183025360107, loss=0.6228974461555481
I0301 17:32:55.410028 139590170035968 logging_writer.py:48] [372700] global_step=372700, grad_norm=4.255634784698486, loss=0.6027622222900391
I0301 17:32:57.243903 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:33:03.326865 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:33:11.527939 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:33:13.820886 139753105983296 submission_runner.py:411] Time since start: 130924.82s, 	Step: 372707, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14588505029678345, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.0556190013885498, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8193237781524658, 'test/num_examples': 10000, 'score': 126546.84605765343, 'total_duration': 130924.82411670685, 'accumulated_submission_time': 126546.84605765343, 'accumulated_eval_time': 4348.296098232269, 'accumulated_logging_time': 16.576444387435913}
I0301 17:33:13.903457 139589708666624 logging_writer.py:48] [372707] accumulated_eval_time=4348.296098, accumulated_logging_time=16.576444, accumulated_submission_time=126546.846058, global_step=372707, preemption_count=0, score=126546.846058, test/accuracy=0.630600, test/loss=1.819324, test/num_examples=10000, total_duration=130924.824117, train/accuracy=0.960778, train/loss=0.145885, validation/accuracy=0.755040, validation/loss=1.055619, validation/num_examples=50000
I0301 17:33:45.745469 139589717059328 logging_writer.py:48] [372800] global_step=372800, grad_norm=4.541955471038818, loss=0.6171865463256836
I0301 17:34:19.758428 139589708666624 logging_writer.py:48] [372900] global_step=372900, grad_norm=4.869366645812988, loss=0.644301176071167
I0301 17:34:53.719269 139589717059328 logging_writer.py:48] [373000] global_step=373000, grad_norm=4.793102741241455, loss=0.6967258453369141
I0301 17:35:27.694851 139589708666624 logging_writer.py:48] [373100] global_step=373100, grad_norm=4.480053901672363, loss=0.6361749768257141
I0301 17:36:01.668446 139589717059328 logging_writer.py:48] [373200] global_step=373200, grad_norm=4.305131912231445, loss=0.5819454193115234
I0301 17:36:35.648485 139589708666624 logging_writer.py:48] [373300] global_step=373300, grad_norm=4.428554058074951, loss=0.6452394723892212
I0301 17:37:09.639330 139589717059328 logging_writer.py:48] [373400] global_step=373400, grad_norm=4.503958225250244, loss=0.5475761890411377
I0301 17:37:43.562202 139589708666624 logging_writer.py:48] [373500] global_step=373500, grad_norm=4.469639301300049, loss=0.5823144316673279
I0301 17:38:17.512971 139589717059328 logging_writer.py:48] [373600] global_step=373600, grad_norm=4.613125801086426, loss=0.6455519199371338
I0301 17:38:51.459691 139589708666624 logging_writer.py:48] [373700] global_step=373700, grad_norm=4.643853664398193, loss=0.6599021553993225
I0301 17:39:25.402405 139589717059328 logging_writer.py:48] [373800] global_step=373800, grad_norm=4.690078258514404, loss=0.6286929845809937
I0301 17:39:59.384328 139589708666624 logging_writer.py:48] [373900] global_step=373900, grad_norm=4.862955570220947, loss=0.679978609085083
I0301 17:40:33.416789 139589717059328 logging_writer.py:48] [374000] global_step=374000, grad_norm=4.517126560211182, loss=0.6917694211006165
I0301 17:41:07.351345 139589708666624 logging_writer.py:48] [374100] global_step=374100, grad_norm=4.6809821128845215, loss=0.6488970518112183
I0301 17:41:41.316982 139589717059328 logging_writer.py:48] [374200] global_step=374200, grad_norm=4.437772750854492, loss=0.6367930769920349
I0301 17:41:43.841377 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:41:49.936445 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:41:58.199498 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:42:00.492195 139753105983296 submission_runner.py:411] Time since start: 131451.50s, 	Step: 374209, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14660421013832092, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.054998755455017, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8189723491668701, 'test/num_examples': 10000, 'score': 127056.72100305557, 'total_duration': 131451.49540233612, 'accumulated_submission_time': 127056.72100305557, 'accumulated_eval_time': 4364.946834564209, 'accumulated_logging_time': 16.668809413909912}
I0301 17:42:00.574451 139590161643264 logging_writer.py:48] [374209] accumulated_eval_time=4364.946835, accumulated_logging_time=16.668809, accumulated_submission_time=127056.721003, global_step=374209, preemption_count=0, score=127056.721003, test/accuracy=0.630100, test/loss=1.818972, test/num_examples=10000, total_duration=131451.495402, train/accuracy=0.960818, train/loss=0.146604, validation/accuracy=0.754960, validation/loss=1.054999, validation/num_examples=50000
I0301 17:42:31.737604 139590170035968 logging_writer.py:48] [374300] global_step=374300, grad_norm=4.551849842071533, loss=0.6115007400512695
I0301 17:43:05.700053 139590161643264 logging_writer.py:48] [374400] global_step=374400, grad_norm=4.64643669128418, loss=0.6464105844497681
I0301 17:43:39.671883 139590170035968 logging_writer.py:48] [374500] global_step=374500, grad_norm=4.357004642486572, loss=0.5923747420310974
I0301 17:44:13.636554 139590161643264 logging_writer.py:48] [374600] global_step=374600, grad_norm=4.799164295196533, loss=0.6442834734916687
I0301 17:44:47.621066 139590170035968 logging_writer.py:48] [374700] global_step=374700, grad_norm=4.3114914894104, loss=0.5795071721076965
I0301 17:45:21.583755 139590161643264 logging_writer.py:48] [374800] global_step=374800, grad_norm=4.261528015136719, loss=0.5983906388282776
I0301 17:45:55.539001 139590170035968 logging_writer.py:48] [374900] global_step=374900, grad_norm=4.753312110900879, loss=0.6033986210823059
I0301 17:46:29.638607 139590161643264 logging_writer.py:48] [375000] global_step=375000, grad_norm=4.103306293487549, loss=0.5921526551246643
I0301 17:47:03.598141 139590170035968 logging_writer.py:48] [375100] global_step=375100, grad_norm=4.386291027069092, loss=0.623710036277771
I0301 17:47:37.538476 139590161643264 logging_writer.py:48] [375200] global_step=375200, grad_norm=4.79246187210083, loss=0.6161143779754639
I0301 17:48:11.510007 139590170035968 logging_writer.py:48] [375300] global_step=375300, grad_norm=4.197732448577881, loss=0.6132412552833557
I0301 17:48:45.473561 139590161643264 logging_writer.py:48] [375400] global_step=375400, grad_norm=4.436910629272461, loss=0.5860282182693481
I0301 17:49:19.439539 139590170035968 logging_writer.py:48] [375500] global_step=375500, grad_norm=4.507449626922607, loss=0.5723238587379456
I0301 17:49:53.417988 139590161643264 logging_writer.py:48] [375600] global_step=375600, grad_norm=4.442520618438721, loss=0.5468535423278809
I0301 17:50:27.358175 139590170035968 logging_writer.py:48] [375700] global_step=375700, grad_norm=5.1317138671875, loss=0.7394917607307434
I0301 17:50:30.556643 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:50:36.513516 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:50:44.679416 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:50:46.975616 139753105983296 submission_runner.py:411] Time since start: 131977.98s, 	Step: 375711, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.1465323269367218, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0567834377288818, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8201284408569336, 'test/num_examples': 10000, 'score': 127566.63928079605, 'total_duration': 131977.97884607315, 'accumulated_submission_time': 127566.63928079605, 'accumulated_eval_time': 4381.365758657455, 'accumulated_logging_time': 16.76178002357483}
I0301 17:50:47.064928 139589708666624 logging_writer.py:48] [375711] accumulated_eval_time=4381.365759, accumulated_logging_time=16.761780, accumulated_submission_time=127566.639281, global_step=375711, preemption_count=0, score=127566.639281, test/accuracy=0.631000, test/loss=1.820128, test/num_examples=10000, total_duration=131977.978846, train/accuracy=0.960818, train/loss=0.146532, validation/accuracy=0.754860, validation/loss=1.056783, validation/num_examples=50000
I0301 17:51:17.615856 139589717059328 logging_writer.py:48] [375800] global_step=375800, grad_norm=4.9014573097229, loss=0.6933857798576355
I0301 17:51:51.546720 139589708666624 logging_writer.py:48] [375900] global_step=375900, grad_norm=4.629518508911133, loss=0.6812881827354431
I0301 17:52:25.475911 139589717059328 logging_writer.py:48] [376000] global_step=376000, grad_norm=4.538992404937744, loss=0.6792281866073608
I0301 17:52:59.553280 139589708666624 logging_writer.py:48] [376100] global_step=376100, grad_norm=4.641888618469238, loss=0.6826813220977783
I0301 17:53:33.531959 139589717059328 logging_writer.py:48] [376200] global_step=376200, grad_norm=4.542138576507568, loss=0.6372070908546448
I0301 17:54:07.467729 139589708666624 logging_writer.py:48] [376300] global_step=376300, grad_norm=5.16646671295166, loss=0.618382453918457
I0301 17:54:41.417138 139589717059328 logging_writer.py:48] [376400] global_step=376400, grad_norm=4.840360164642334, loss=0.6414507031440735
I0301 17:55:15.343564 139589708666624 logging_writer.py:48] [376500] global_step=376500, grad_norm=4.7764506340026855, loss=0.721932053565979
I0301 17:55:49.288062 139589717059328 logging_writer.py:48] [376600] global_step=376600, grad_norm=4.934951305389404, loss=0.6863608360290527
I0301 17:56:23.251571 139589708666624 logging_writer.py:48] [376700] global_step=376700, grad_norm=4.7539472579956055, loss=0.5880393981933594
I0301 17:56:57.248551 139589717059328 logging_writer.py:48] [376800] global_step=376800, grad_norm=4.563851356506348, loss=0.6466915607452393
I0301 17:57:31.205151 139589708666624 logging_writer.py:48] [376900] global_step=376900, grad_norm=5.0245513916015625, loss=0.6698639988899231
I0301 17:58:05.165733 139589717059328 logging_writer.py:48] [377000] global_step=377000, grad_norm=4.621695041656494, loss=0.5971981287002563
I0301 17:58:39.199904 139589708666624 logging_writer.py:48] [377100] global_step=377100, grad_norm=4.850654125213623, loss=0.6242392063140869
I0301 17:59:13.148084 139589717059328 logging_writer.py:48] [377200] global_step=377200, grad_norm=4.453404426574707, loss=0.6222288608551025
I0301 17:59:17.031724 139753105983296 spec.py:321] Evaluating on the training split.
I0301 17:59:23.107163 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 17:59:31.271776 139753105983296 spec.py:349] Evaluating on the test split.
I0301 17:59:33.527369 139753105983296 submission_runner.py:411] Time since start: 132504.53s, 	Step: 377213, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.14687810838222504, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0555857419967651, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8202074766159058, 'test/num_examples': 10000, 'score': 128076.54186677933, 'total_duration': 132504.5306007862, 'accumulated_submission_time': 128076.54186677933, 'accumulated_eval_time': 4397.861366271973, 'accumulated_logging_time': 16.862693548202515}
I0301 17:59:33.608454 139589708666624 logging_writer.py:48] [377213] accumulated_eval_time=4397.861366, accumulated_logging_time=16.862694, accumulated_submission_time=128076.541867, global_step=377213, preemption_count=0, score=128076.541867, test/accuracy=0.630200, test/loss=1.820207, test/num_examples=10000, total_duration=132504.530601, train/accuracy=0.960260, train/loss=0.146878, validation/accuracy=0.754880, validation/loss=1.055586, validation/num_examples=50000
I0301 18:00:04.277846 139589717059328 logging_writer.py:48] [377300] global_step=377300, grad_norm=4.831271171569824, loss=0.7420295476913452
I0301 18:00:38.220692 139589708666624 logging_writer.py:48] [377400] global_step=377400, grad_norm=4.4187397956848145, loss=0.6489371657371521
I0301 18:01:12.170857 139589717059328 logging_writer.py:48] [377500] global_step=377500, grad_norm=3.8946971893310547, loss=0.558850884437561
I0301 18:01:46.134590 139589708666624 logging_writer.py:48] [377600] global_step=377600, grad_norm=4.466658115386963, loss=0.6659738421440125
I0301 18:02:20.090261 139589717059328 logging_writer.py:48] [377700] global_step=377700, grad_norm=4.425111770629883, loss=0.6577680706977844
I0301 18:02:54.030451 139589708666624 logging_writer.py:48] [377800] global_step=377800, grad_norm=4.848902702331543, loss=0.6518923044204712
I0301 18:03:27.966320 139589717059328 logging_writer.py:48] [377900] global_step=377900, grad_norm=4.421111106872559, loss=0.6078450679779053
I0301 18:04:01.922945 139589708666624 logging_writer.py:48] [378000] global_step=378000, grad_norm=4.843784332275391, loss=0.5857076048851013
I0301 18:04:35.896969 139589717059328 logging_writer.py:48] [378100] global_step=378100, grad_norm=4.4265360832214355, loss=0.5764873623847961
I0301 18:05:09.902207 139589708666624 logging_writer.py:48] [378200] global_step=378200, grad_norm=4.967883586883545, loss=0.6366094350814819
I0301 18:05:43.853291 139589717059328 logging_writer.py:48] [378300] global_step=378300, grad_norm=4.696030139923096, loss=0.6572853922843933
I0301 18:06:17.816362 139589708666624 logging_writer.py:48] [378400] global_step=378400, grad_norm=4.566534519195557, loss=0.6422206163406372
I0301 18:06:51.752899 139589717059328 logging_writer.py:48] [378500] global_step=378500, grad_norm=4.438756465911865, loss=0.606713056564331
I0301 18:07:25.700217 139589708666624 logging_writer.py:48] [378600] global_step=378600, grad_norm=4.508890151977539, loss=0.6200337409973145
I0301 18:07:59.646790 139589717059328 logging_writer.py:48] [378700] global_step=378700, grad_norm=4.609316825866699, loss=0.6215254068374634
I0301 18:08:03.859968 139753105983296 spec.py:321] Evaluating on the training split.
I0301 18:08:09.901017 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 18:08:18.193356 139753105983296 spec.py:349] Evaluating on the test split.
I0301 18:08:20.531456 139753105983296 submission_runner.py:411] Time since start: 133031.53s, 	Step: 378714, 	{'train/accuracy': 0.9616948366165161, 'train/loss': 0.14120759069919586, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.055570125579834, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.818804383277893, 'test/num_examples': 10000, 'score': 128585.89695334435, 'total_duration': 133031.53467178345, 'accumulated_submission_time': 128585.89695334435, 'accumulated_eval_time': 4414.532790660858, 'accumulated_logging_time': 17.786174058914185}
I0301 18:08:20.617366 139590161643264 logging_writer.py:48] [378714] accumulated_eval_time=4414.532791, accumulated_logging_time=17.786174, accumulated_submission_time=128585.896953, global_step=378714, preemption_count=0, score=128585.896953, test/accuracy=0.629700, test/loss=1.818804, test/num_examples=10000, total_duration=133031.534672, train/accuracy=0.961695, train/loss=0.141208, validation/accuracy=0.755040, validation/loss=1.055570, validation/num_examples=50000
I0301 18:08:50.088914 139590170035968 logging_writer.py:48] [378800] global_step=378800, grad_norm=4.365295886993408, loss=0.5914195775985718
I0301 18:09:24.030974 139590161643264 logging_writer.py:48] [378900] global_step=378900, grad_norm=4.912748336791992, loss=0.6622220277786255
I0301 18:09:57.997279 139590170035968 logging_writer.py:48] [379000] global_step=379000, grad_norm=4.8576273918151855, loss=0.6693946123123169
I0301 18:10:31.959525 139590161643264 logging_writer.py:48] [379100] global_step=379100, grad_norm=5.0925703048706055, loss=0.5879472494125366
I0301 18:11:05.962014 139590170035968 logging_writer.py:48] [379200] global_step=379200, grad_norm=4.721039295196533, loss=0.6397620439529419
I0301 18:11:39.903707 139590161643264 logging_writer.py:48] [379300] global_step=379300, grad_norm=4.705067157745361, loss=0.6515154838562012
I0301 18:12:13.885493 139590170035968 logging_writer.py:48] [379400] global_step=379400, grad_norm=4.208280086517334, loss=0.5976681113243103
I0301 18:12:47.844763 139590161643264 logging_writer.py:48] [379500] global_step=379500, grad_norm=4.102468013763428, loss=0.6067878007888794
I0301 18:13:21.780713 139590170035968 logging_writer.py:48] [379600] global_step=379600, grad_norm=4.51213264465332, loss=0.6040210127830505
I0301 18:13:55.739935 139590161643264 logging_writer.py:48] [379700] global_step=379700, grad_norm=4.836202144622803, loss=0.6675963997840881
I0301 18:14:29.699912 139590170035968 logging_writer.py:48] [379800] global_step=379800, grad_norm=4.660202503204346, loss=0.6156497001647949
I0301 18:15:03.647043 139590161643264 logging_writer.py:48] [379900] global_step=379900, grad_norm=4.505194187164307, loss=0.597554624080658
I0301 18:15:37.572500 139590170035968 logging_writer.py:48] [380000] global_step=380000, grad_norm=4.4867448806762695, loss=0.6075389385223389
I0301 18:16:11.530730 139590161643264 logging_writer.py:48] [380100] global_step=380100, grad_norm=4.50570821762085, loss=0.603725790977478
I0301 18:16:45.471706 139590170035968 logging_writer.py:48] [380200] global_step=380200, grad_norm=4.959755897521973, loss=0.6437274217605591
I0301 18:16:50.706498 139753105983296 spec.py:321] Evaluating on the training split.
I0301 18:16:56.695433 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 18:17:05.044498 139753105983296 spec.py:349] Evaluating on the test split.
I0301 18:17:07.335471 139753105983296 submission_runner.py:411] Time since start: 133558.34s, 	Step: 380217, 	{'train/accuracy': 0.9613161683082581, 'train/loss': 0.14605680108070374, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.05453622341156, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8188520669937134, 'test/num_examples': 10000, 'score': 129095.92270565033, 'total_duration': 133558.33870458603, 'accumulated_submission_time': 129095.92270565033, 'accumulated_eval_time': 4431.161715507507, 'accumulated_logging_time': 17.883363485336304}
I0301 18:17:07.420204 139589725452032 logging_writer.py:48] [380217] accumulated_eval_time=4431.161716, accumulated_logging_time=17.883363, accumulated_submission_time=129095.922706, global_step=380217, preemption_count=0, score=129095.922706, test/accuracy=0.630000, test/loss=1.818852, test/num_examples=10000, total_duration=133558.338705, train/accuracy=0.961316, train/loss=0.146057, validation/accuracy=0.754740, validation/loss=1.054536, validation/num_examples=50000
I0301 18:17:35.950416 139590144857856 logging_writer.py:48] [380300] global_step=380300, grad_norm=5.082980155944824, loss=0.7017855644226074
I0301 18:18:09.857718 139589725452032 logging_writer.py:48] [380400] global_step=380400, grad_norm=4.457267761230469, loss=0.6007947325706482
I0301 18:18:43.782874 139590144857856 logging_writer.py:48] [380500] global_step=380500, grad_norm=4.588928699493408, loss=0.5950180888175964
I0301 18:19:17.698094 139589725452032 logging_writer.py:48] [380600] global_step=380600, grad_norm=4.567938804626465, loss=0.6416438817977905
I0301 18:19:51.649285 139590144857856 logging_writer.py:48] [380700] global_step=380700, grad_norm=4.766164302825928, loss=0.6304553747177124
I0301 18:20:25.594572 139589725452032 logging_writer.py:48] [380800] global_step=380800, grad_norm=4.740108013153076, loss=0.6349660754203796
I0301 18:20:59.535171 139590144857856 logging_writer.py:48] [380900] global_step=380900, grad_norm=4.778120517730713, loss=0.6627703905105591
I0301 18:21:33.465481 139589725452032 logging_writer.py:48] [381000] global_step=381000, grad_norm=4.434341907501221, loss=0.6656627058982849
I0301 18:22:07.409118 139590144857856 logging_writer.py:48] [381100] global_step=381100, grad_norm=4.1776652336120605, loss=0.5947453379631042
I0301 18:22:41.359932 139589725452032 logging_writer.py:48] [381200] global_step=381200, grad_norm=4.055746078491211, loss=0.5937839150428772
I0301 18:23:15.295844 139590144857856 logging_writer.py:48] [381300] global_step=381300, grad_norm=4.437899112701416, loss=0.5991803407669067
I0301 18:23:49.324648 139589725452032 logging_writer.py:48] [381400] global_step=381400, grad_norm=4.769244194030762, loss=0.656090259552002
I0301 18:24:23.239923 139590144857856 logging_writer.py:48] [381500] global_step=381500, grad_norm=4.513900279998779, loss=0.6627627611160278
I0301 18:24:57.161999 139589725452032 logging_writer.py:48] [381600] global_step=381600, grad_norm=4.50853967666626, loss=0.6219190955162048
I0301 18:25:31.089653 139590144857856 logging_writer.py:48] [381700] global_step=381700, grad_norm=4.555629253387451, loss=0.6634572148323059
I0301 18:25:37.365375 139753105983296 spec.py:321] Evaluating on the training split.
I0301 18:25:44.075772 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 18:25:52.311197 139753105983296 spec.py:349] Evaluating on the test split.
I0301 18:25:54.565471 139753105983296 submission_runner.py:411] Time since start: 134085.57s, 	Step: 381720, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.1449822038412094, 'validation/accuracy': 0.7547000050544739, 'validation/loss': 1.0562328100204468, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8185116052627563, 'test/num_examples': 10000, 'score': 129605.80332398415, 'total_duration': 134085.56870532036, 'accumulated_submission_time': 129605.80332398415, 'accumulated_eval_time': 4448.361759185791, 'accumulated_logging_time': 17.9791898727417}
I0301 18:25:54.644412 139589717059328 logging_writer.py:48] [381720] accumulated_eval_time=4448.361759, accumulated_logging_time=17.979190, accumulated_submission_time=129605.803324, global_step=381720, preemption_count=0, score=129605.803324, test/accuracy=0.630500, test/loss=1.818512, test/num_examples=10000, total_duration=134085.568705, train/accuracy=0.960738, train/loss=0.144982, validation/accuracy=0.754700, validation/loss=1.056233, validation/num_examples=50000
I0301 18:26:22.125591 139589725452032 logging_writer.py:48] [381800] global_step=381800, grad_norm=4.737782955169678, loss=0.6407829523086548
I0301 18:26:56.059670 139589717059328 logging_writer.py:48] [381900] global_step=381900, grad_norm=4.539868354797363, loss=0.588409423828125
I0301 18:27:29.981247 139589725452032 logging_writer.py:48] [382000] global_step=382000, grad_norm=4.693835258483887, loss=0.7026680111885071
I0301 18:28:03.949661 139589717059328 logging_writer.py:48] [382100] global_step=382100, grad_norm=4.8257832527160645, loss=0.6455657482147217
I0301 18:28:37.901992 139589725452032 logging_writer.py:48] [382200] global_step=382200, grad_norm=4.110800743103027, loss=0.5724510550498962
I0301 18:29:11.855711 139589717059328 logging_writer.py:48] [382300] global_step=382300, grad_norm=4.2837233543396, loss=0.5813170671463013
I0301 18:29:45.922803 139589725452032 logging_writer.py:48] [382400] global_step=382400, grad_norm=4.469791412353516, loss=0.6368153095245361
I0301 18:30:19.872588 139589717059328 logging_writer.py:48] [382500] global_step=382500, grad_norm=4.5719804763793945, loss=0.6045545339584351
I0301 18:30:53.812773 139589725452032 logging_writer.py:48] [382600] global_step=382600, grad_norm=4.414940357208252, loss=0.5906704664230347
I0301 18:31:27.738915 139589717059328 logging_writer.py:48] [382700] global_step=382700, grad_norm=4.228670120239258, loss=0.5575515031814575
I0301 18:32:01.688557 139589725452032 logging_writer.py:48] [382800] global_step=382800, grad_norm=4.384300231933594, loss=0.648272693157196
I0301 18:32:35.624067 139589717059328 logging_writer.py:48] [382900] global_step=382900, grad_norm=4.800521373748779, loss=0.6527708768844604
I0301 18:33:09.544022 139589725452032 logging_writer.py:48] [383000] global_step=383000, grad_norm=5.568657398223877, loss=0.7316944599151611
I0301 18:33:43.515327 139589717059328 logging_writer.py:48] [383100] global_step=383100, grad_norm=4.386294364929199, loss=0.6156778931617737
I0301 18:34:17.459294 139589725452032 logging_writer.py:48] [383200] global_step=383200, grad_norm=4.420341968536377, loss=0.5907250046730042
I0301 18:34:24.720589 139753105983296 spec.py:321] Evaluating on the training split.
I0301 18:34:30.739789 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 18:34:39.089957 139753105983296 spec.py:349] Evaluating on the test split.
I0301 18:34:41.406024 139753105983296 submission_runner.py:411] Time since start: 134612.41s, 	Step: 383223, 	{'train/accuracy': 0.9597018361091614, 'train/loss': 0.1486869752407074, 'validation/accuracy': 0.7551800012588501, 'validation/loss': 1.0554879903793335, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8197861909866333, 'test/num_examples': 10000, 'score': 130115.81632757187, 'total_duration': 134612.40925621986, 'accumulated_submission_time': 130115.81632757187, 'accumulated_eval_time': 4465.047143936157, 'accumulated_logging_time': 18.068235158920288}
I0301 18:34:41.490345 139589717059328 logging_writer.py:48] [383223] accumulated_eval_time=4465.047144, accumulated_logging_time=18.068235, accumulated_submission_time=130115.816328, global_step=383223, preemption_count=0, score=130115.816328, test/accuracy=0.630300, test/loss=1.819786, test/num_examples=10000, total_duration=134612.409256, train/accuracy=0.959702, train/loss=0.148687, validation/accuracy=0.755180, validation/loss=1.055488, validation/num_examples=50000
I0301 18:35:07.938809 139590144857856 logging_writer.py:48] [383300] global_step=383300, grad_norm=4.523345947265625, loss=0.6449986696243286
I0301 18:35:41.989468 139589717059328 logging_writer.py:48] [383400] global_step=383400, grad_norm=4.484549045562744, loss=0.6141001582145691
I0301 18:36:15.912924 139590144857856 logging_writer.py:48] [383500] global_step=383500, grad_norm=4.599277019500732, loss=0.6038917303085327
I0301 18:36:49.865207 139589717059328 logging_writer.py:48] [383600] global_step=383600, grad_norm=4.264463424682617, loss=0.6196516156196594
I0301 18:37:23.812755 139590144857856 logging_writer.py:48] [383700] global_step=383700, grad_norm=4.557859897613525, loss=0.6413416266441345
I0301 18:37:57.783046 139589717059328 logging_writer.py:48] [383800] global_step=383800, grad_norm=5.143210411071777, loss=0.584665060043335
I0301 18:38:31.709529 139590144857856 logging_writer.py:48] [383900] global_step=383900, grad_norm=4.175206661224365, loss=0.5587533712387085
I0301 18:39:05.638669 139589717059328 logging_writer.py:48] [384000] global_step=384000, grad_norm=4.415483474731445, loss=0.6327131390571594
I0301 18:39:39.580562 139590144857856 logging_writer.py:48] [384100] global_step=384100, grad_norm=4.5868239402771, loss=0.6192377209663391
I0301 18:40:13.558519 139589717059328 logging_writer.py:48] [384200] global_step=384200, grad_norm=4.180722713470459, loss=0.5817961692810059
I0301 18:40:47.494383 139590144857856 logging_writer.py:48] [384300] global_step=384300, grad_norm=5.335934638977051, loss=0.6877368688583374
I0301 18:41:21.407586 139589717059328 logging_writer.py:48] [384400] global_step=384400, grad_norm=4.842203617095947, loss=0.6261900067329407
I0301 18:41:55.458549 139590144857856 logging_writer.py:48] [384500] global_step=384500, grad_norm=4.287075042724609, loss=0.6251664757728577
I0301 18:42:29.383534 139589717059328 logging_writer.py:48] [384600] global_step=384600, grad_norm=4.907041549682617, loss=0.6656292676925659
I0301 18:43:03.334176 139590144857856 logging_writer.py:48] [384700] global_step=384700, grad_norm=4.180335998535156, loss=0.5913099050521851
I0301 18:43:11.602512 139753105983296 spec.py:321] Evaluating on the training split.
I0301 18:43:17.583812 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 18:43:25.872596 139753105983296 spec.py:349] Evaluating on the test split.
I0301 18:43:28.105403 139753105983296 submission_runner.py:411] Time since start: 135139.11s, 	Step: 384726, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.14592614769935608, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.055842638015747, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.818486213684082, 'test/num_examples': 10000, 'score': 130625.86515212059, 'total_duration': 135139.1086113453, 'accumulated_submission_time': 130625.86515212059, 'accumulated_eval_time': 4481.549957990646, 'accumulated_logging_time': 18.162591457366943}
I0301 18:43:28.201975 139590170035968 logging_writer.py:48] [384726] accumulated_eval_time=4481.549958, accumulated_logging_time=18.162591, accumulated_submission_time=130625.865152, global_step=384726, preemption_count=0, score=130625.865152, test/accuracy=0.629800, test/loss=1.818486, test/num_examples=10000, total_duration=135139.108611, train/accuracy=0.960898, train/loss=0.145926, validation/accuracy=0.754920, validation/loss=1.055843, validation/num_examples=50000
I0301 18:43:53.635560 139590178428672 logging_writer.py:48] [384800] global_step=384800, grad_norm=4.674042701721191, loss=0.7066287994384766
I0301 18:44:27.579908 139590170035968 logging_writer.py:48] [384900] global_step=384900, grad_norm=4.309901714324951, loss=0.5863502025604248
I0301 18:45:01.539791 139590178428672 logging_writer.py:48] [385000] global_step=385000, grad_norm=4.5091552734375, loss=0.6058056354522705
I0301 18:45:35.488603 139590170035968 logging_writer.py:48] [385100] global_step=385100, grad_norm=4.1123948097229, loss=0.5477012395858765
I0301 18:46:09.438525 139590178428672 logging_writer.py:48] [385200] global_step=385200, grad_norm=4.369302272796631, loss=0.6275420188903809
I0301 18:46:43.376727 139590170035968 logging_writer.py:48] [385300] global_step=385300, grad_norm=4.735095500946045, loss=0.5497841835021973
I0301 18:47:17.321428 139590178428672 logging_writer.py:48] [385400] global_step=385400, grad_norm=3.8475146293640137, loss=0.49766066670417786
I0301 18:47:51.303512 139590170035968 logging_writer.py:48] [385500] global_step=385500, grad_norm=5.341919422149658, loss=0.6916251182556152
I0301 18:48:25.374025 139590178428672 logging_writer.py:48] [385600] global_step=385600, grad_norm=4.623228549957275, loss=0.6616137623786926
I0301 18:48:59.333411 139590170035968 logging_writer.py:48] [385700] global_step=385700, grad_norm=5.581785202026367, loss=0.6628161668777466
I0301 18:49:33.286548 139590178428672 logging_writer.py:48] [385800] global_step=385800, grad_norm=4.607107162475586, loss=0.6322326064109802
I0301 18:50:07.263490 139590170035968 logging_writer.py:48] [385900] global_step=385900, grad_norm=4.37586784362793, loss=0.5916978120803833
I0301 18:50:41.182340 139590178428672 logging_writer.py:48] [386000] global_step=386000, grad_norm=4.861161231994629, loss=0.6829038858413696
I0301 18:51:15.159755 139590170035968 logging_writer.py:48] [386100] global_step=386100, grad_norm=4.258176803588867, loss=0.5756085515022278
I0301 18:51:49.136100 139590178428672 logging_writer.py:48] [386200] global_step=386200, grad_norm=4.440913677215576, loss=0.5926011204719543
I0301 18:51:58.111031 139753105983296 spec.py:321] Evaluating on the training split.
I0301 18:52:04.174027 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 18:52:12.441287 139753105983296 spec.py:349] Evaluating on the test split.
I0301 18:52:14.708029 139753105983296 submission_runner.py:411] Time since start: 135665.71s, 	Step: 386228, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.1451815962791443, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0552767515182495, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.817418098449707, 'test/num_examples': 10000, 'score': 131135.7101225853, 'total_duration': 135665.71124458313, 'accumulated_submission_time': 131135.7101225853, 'accumulated_eval_time': 4498.146886110306, 'accumulated_logging_time': 18.27062964439392}
I0301 18:52:14.794365 139589725452032 logging_writer.py:48] [386228] accumulated_eval_time=4498.146886, accumulated_logging_time=18.270630, accumulated_submission_time=131135.710123, global_step=386228, preemption_count=0, score=131135.710123, test/accuracy=0.630100, test/loss=1.817418, test/num_examples=10000, total_duration=135665.711245, train/accuracy=0.961436, train/loss=0.145182, validation/accuracy=0.755000, validation/loss=1.055277, validation/num_examples=50000
I0301 18:52:39.563048 139590144857856 logging_writer.py:48] [386300] global_step=386300, grad_norm=4.339303493499756, loss=0.5698749423027039
I0301 18:53:13.475197 139589725452032 logging_writer.py:48] [386400] global_step=386400, grad_norm=4.917611122131348, loss=0.6326561570167542
I0301 18:53:47.352639 139590144857856 logging_writer.py:48] [386500] global_step=386500, grad_norm=4.259799957275391, loss=0.5543631911277771
I0301 18:54:21.407425 139589725452032 logging_writer.py:48] [386600] global_step=386600, grad_norm=4.151169776916504, loss=0.5698498487472534
I0301 18:54:55.333615 139590144857856 logging_writer.py:48] [386700] global_step=386700, grad_norm=5.158968925476074, loss=0.6248279213905334
I0301 18:55:29.289018 139589725452032 logging_writer.py:48] [386800] global_step=386800, grad_norm=4.280202865600586, loss=0.5514373183250427
I0301 18:56:03.244390 139590144857856 logging_writer.py:48] [386900] global_step=386900, grad_norm=5.253531455993652, loss=0.6589161157608032
I0301 18:56:37.207554 139589725452032 logging_writer.py:48] [387000] global_step=387000, grad_norm=4.555609226226807, loss=0.6543715596199036
I0301 18:57:11.150862 139590144857856 logging_writer.py:48] [387100] global_step=387100, grad_norm=4.651663780212402, loss=0.6091402769088745
I0301 18:57:45.095107 139589725452032 logging_writer.py:48] [387200] global_step=387200, grad_norm=4.310652256011963, loss=0.5458353161811829
I0301 18:58:19.049542 139590144857856 logging_writer.py:48] [387300] global_step=387300, grad_norm=4.457752704620361, loss=0.5746122598648071
I0301 18:58:52.987088 139589725452032 logging_writer.py:48] [387400] global_step=387400, grad_norm=4.311168193817139, loss=0.6682074666023254
I0301 18:59:26.935263 139590144857856 logging_writer.py:48] [387500] global_step=387500, grad_norm=4.701090335845947, loss=0.6276953816413879
I0301 19:00:00.865216 139589725452032 logging_writer.py:48] [387600] global_step=387600, grad_norm=4.285146713256836, loss=0.6267824769020081
I0301 19:00:34.990030 139590144857856 logging_writer.py:48] [387700] global_step=387700, grad_norm=4.283141136169434, loss=0.6430389285087585
I0301 19:00:44.984964 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:00:51.024393 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:00:59.314429 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:01:01.573010 139753105983296 submission_runner.py:411] Time since start: 136192.58s, 	Step: 387731, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14512765407562256, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0561199188232422, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8203297853469849, 'test/num_examples': 10000, 'score': 131645.83796977997, 'total_duration': 136192.5762116909, 'accumulated_submission_time': 131645.83796977997, 'accumulated_eval_time': 4514.734850406647, 'accumulated_logging_time': 18.367199420928955}
I0301 19:01:01.666881 139589717059328 logging_writer.py:48] [387731] accumulated_eval_time=4514.734850, accumulated_logging_time=18.367199, accumulated_submission_time=131645.837970, global_step=387731, preemption_count=0, score=131645.837970, test/accuracy=0.630300, test/loss=1.820330, test/num_examples=10000, total_duration=136192.576212, train/accuracy=0.961416, train/loss=0.145128, validation/accuracy=0.754920, validation/loss=1.056120, validation/num_examples=50000
I0301 19:01:25.393121 139590161643264 logging_writer.py:48] [387800] global_step=387800, grad_norm=5.170555591583252, loss=0.6620732545852661
I0301 19:01:59.326605 139589717059328 logging_writer.py:48] [387900] global_step=387900, grad_norm=5.03318452835083, loss=0.637742817401886
I0301 19:02:33.302414 139590161643264 logging_writer.py:48] [388000] global_step=388000, grad_norm=4.64141845703125, loss=0.6184975504875183
I0301 19:03:07.261189 139589717059328 logging_writer.py:48] [388100] global_step=388100, grad_norm=4.295775413513184, loss=0.5512469410896301
I0301 19:03:41.234812 139590161643264 logging_writer.py:48] [388200] global_step=388200, grad_norm=4.775701999664307, loss=0.6860880851745605
I0301 19:04:15.187475 139589717059328 logging_writer.py:48] [388300] global_step=388300, grad_norm=4.33234167098999, loss=0.6049087047576904
I0301 19:04:49.147455 139590161643264 logging_writer.py:48] [388400] global_step=388400, grad_norm=5.087586402893066, loss=0.6359260082244873
I0301 19:05:23.107136 139589717059328 logging_writer.py:48] [388500] global_step=388500, grad_norm=4.220754623413086, loss=0.5476382374763489
I0301 19:05:57.065426 139590161643264 logging_writer.py:48] [388600] global_step=388600, grad_norm=4.354969501495361, loss=0.5409433841705322
I0301 19:06:31.146279 139589717059328 logging_writer.py:48] [388700] global_step=388700, grad_norm=4.184281349182129, loss=0.5630080699920654
I0301 19:07:05.082315 139590161643264 logging_writer.py:48] [388800] global_step=388800, grad_norm=4.587976932525635, loss=0.6064553260803223
I0301 19:07:39.053906 139589717059328 logging_writer.py:48] [388900] global_step=388900, grad_norm=4.483295917510986, loss=0.6424121260643005
I0301 19:08:13.032792 139590161643264 logging_writer.py:48] [389000] global_step=389000, grad_norm=4.4624176025390625, loss=0.6011795997619629
I0301 19:08:46.994992 139589717059328 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.172320365905762, loss=0.5432224869728088
I0301 19:09:20.976548 139590161643264 logging_writer.py:48] [389200] global_step=389200, grad_norm=4.638873100280762, loss=0.7042183876037598
I0301 19:09:31.633995 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:09:37.703834 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:09:45.952278 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:09:48.249868 139753105983296 submission_runner.py:411] Time since start: 136719.25s, 	Step: 389233, 	{'train/accuracy': 0.9611567258834839, 'train/loss': 0.14561446011066437, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.055548906326294, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8190268278121948, 'test/num_examples': 10000, 'score': 132155.74128174782, 'total_duration': 136719.25308156013, 'accumulated_submission_time': 132155.74128174782, 'accumulated_eval_time': 4531.350663900375, 'accumulated_logging_time': 18.472204446792603}
I0301 19:09:48.337758 139590144857856 logging_writer.py:48] [389233] accumulated_eval_time=4531.350664, accumulated_logging_time=18.472204, accumulated_submission_time=132155.741282, global_step=389233, preemption_count=0, score=132155.741282, test/accuracy=0.630100, test/loss=1.819027, test/num_examples=10000, total_duration=136719.253082, train/accuracy=0.961157, train/loss=0.145614, validation/accuracy=0.754740, validation/loss=1.055549, validation/num_examples=50000
I0301 19:10:11.382910 139590153250560 logging_writer.py:48] [389300] global_step=389300, grad_norm=4.478790283203125, loss=0.6086121201515198
I0301 19:10:45.254823 139590144857856 logging_writer.py:48] [389400] global_step=389400, grad_norm=4.189093589782715, loss=0.5872354507446289
I0301 19:11:19.177105 139590153250560 logging_writer.py:48] [389500] global_step=389500, grad_norm=4.291103839874268, loss=0.5778357982635498
I0301 19:11:53.168017 139590144857856 logging_writer.py:48] [389600] global_step=389600, grad_norm=4.424327850341797, loss=0.6349072456359863
I0301 19:12:27.101827 139590153250560 logging_writer.py:48] [389700] global_step=389700, grad_norm=4.966230869293213, loss=0.6506282091140747
I0301 19:13:01.091844 139590144857856 logging_writer.py:48] [389800] global_step=389800, grad_norm=4.389244079589844, loss=0.6122890710830688
I0301 19:13:35.047273 139590153250560 logging_writer.py:48] [389900] global_step=389900, grad_norm=4.710490703582764, loss=0.6642279624938965
I0301 19:14:08.966545 139590144857856 logging_writer.py:48] [390000] global_step=390000, grad_norm=4.672346115112305, loss=0.6806714534759521
I0301 19:14:42.928044 139590153250560 logging_writer.py:48] [390100] global_step=390100, grad_norm=4.86236572265625, loss=0.6835240125656128
I0301 19:15:16.858782 139590144857856 logging_writer.py:48] [390200] global_step=390200, grad_norm=4.495903015136719, loss=0.6916103959083557
I0301 19:15:50.795420 139590153250560 logging_writer.py:48] [390300] global_step=390300, grad_norm=4.773118019104004, loss=0.6578632593154907
I0301 19:16:24.756178 139590144857856 logging_writer.py:48] [390400] global_step=390400, grad_norm=4.595170497894287, loss=0.6507917642593384
I0301 19:16:58.708145 139590153250560 logging_writer.py:48] [390500] global_step=390500, grad_norm=4.2976202964782715, loss=0.6053955554962158
I0301 19:17:32.611934 139590144857856 logging_writer.py:48] [390600] global_step=390600, grad_norm=4.240650177001953, loss=0.6092056632041931
I0301 19:18:06.581271 139590153250560 logging_writer.py:48] [390700] global_step=390700, grad_norm=4.45929479598999, loss=0.6653208136558533
I0301 19:18:18.262927 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:18:24.285550 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:18:32.638779 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:18:34.940028 139753105983296 submission_runner.py:411] Time since start: 137245.94s, 	Step: 390736, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.14833804965019226, 'validation/accuracy': 0.7553600072860718, 'validation/loss': 1.0564292669296265, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8202205896377563, 'test/num_examples': 10000, 'score': 132665.60343956947, 'total_duration': 137245.94326090813, 'accumulated_submission_time': 132665.60343956947, 'accumulated_eval_time': 4548.0277144908905, 'accumulated_logging_time': 18.570212364196777}
I0301 19:18:35.025720 139589708666624 logging_writer.py:48] [390736] accumulated_eval_time=4548.027714, accumulated_logging_time=18.570212, accumulated_submission_time=132665.603440, global_step=390736, preemption_count=0, score=132665.603440, test/accuracy=0.629900, test/loss=1.820221, test/num_examples=10000, total_duration=137245.943261, train/accuracy=0.960081, train/loss=0.148338, validation/accuracy=0.755360, validation/loss=1.056429, validation/num_examples=50000
I0301 19:18:57.130775 139589717059328 logging_writer.py:48] [390800] global_step=390800, grad_norm=4.491873264312744, loss=0.6439435482025146
I0301 19:19:30.987638 139589708666624 logging_writer.py:48] [390900] global_step=390900, grad_norm=4.984084606170654, loss=0.6600123047828674
I0301 19:20:04.925655 139589717059328 logging_writer.py:48] [391000] global_step=391000, grad_norm=4.417689323425293, loss=0.6359531879425049
I0301 19:20:38.869881 139589708666624 logging_writer.py:48] [391100] global_step=391100, grad_norm=4.551446914672852, loss=0.6209936141967773
I0301 19:21:12.829421 139589717059328 logging_writer.py:48] [391200] global_step=391200, grad_norm=4.814939498901367, loss=0.6515449285507202
I0301 19:21:46.788587 139589708666624 logging_writer.py:48] [391300] global_step=391300, grad_norm=4.466135025024414, loss=0.6291963458061218
I0301 19:22:20.718841 139589717059328 logging_writer.py:48] [391400] global_step=391400, grad_norm=4.700719833374023, loss=0.674770712852478
I0301 19:22:54.647908 139589708666624 logging_writer.py:48] [391500] global_step=391500, grad_norm=5.122284889221191, loss=0.6751298904418945
I0301 19:23:28.616099 139589717059328 logging_writer.py:48] [391600] global_step=391600, grad_norm=4.38692045211792, loss=0.6027424335479736
I0301 19:24:02.585005 139589708666624 logging_writer.py:48] [391700] global_step=391700, grad_norm=4.805905818939209, loss=0.6532449722290039
I0301 19:24:36.526825 139589717059328 logging_writer.py:48] [391800] global_step=391800, grad_norm=4.330143451690674, loss=0.5481788516044617
I0301 19:25:10.540433 139589708666624 logging_writer.py:48] [391900] global_step=391900, grad_norm=4.573307037353516, loss=0.62187260389328
I0301 19:25:44.483551 139589717059328 logging_writer.py:48] [392000] global_step=392000, grad_norm=4.762360095977783, loss=0.6749143004417419
I0301 19:26:18.439119 139589708666624 logging_writer.py:48] [392100] global_step=392100, grad_norm=4.704995155334473, loss=0.7175303101539612
I0301 19:26:52.394802 139589717059328 logging_writer.py:48] [392200] global_step=392200, grad_norm=4.379871845245361, loss=0.6232026815414429
I0301 19:27:05.104114 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:27:11.076373 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:27:19.256364 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:27:21.533894 139753105983296 submission_runner.py:411] Time since start: 137772.54s, 	Step: 392239, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.14371973276138306, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.055400013923645, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8189407587051392, 'test/num_examples': 10000, 'score': 133175.61843442917, 'total_duration': 137772.5371003151, 'accumulated_submission_time': 133175.61843442917, 'accumulated_eval_time': 4564.4574275016785, 'accumulated_logging_time': 18.666483402252197}
I0301 19:27:21.620501 139590186821376 logging_writer.py:48] [392239] accumulated_eval_time=4564.457428, accumulated_logging_time=18.666483, accumulated_submission_time=133175.618434, global_step=392239, preemption_count=0, score=133175.618434, test/accuracy=0.630100, test/loss=1.818941, test/num_examples=10000, total_duration=137772.537100, train/accuracy=0.961396, train/loss=0.143720, validation/accuracy=0.754760, validation/loss=1.055400, validation/num_examples=50000
I0301 19:27:42.655628 139590195214080 logging_writer.py:48] [392300] global_step=392300, grad_norm=4.43947696685791, loss=0.6195335388183594
I0301 19:28:16.563610 139590186821376 logging_writer.py:48] [392400] global_step=392400, grad_norm=4.341838359832764, loss=0.6093719005584717
I0301 19:28:50.492292 139590195214080 logging_writer.py:48] [392500] global_step=392500, grad_norm=4.274928569793701, loss=0.5892322659492493
I0301 19:29:24.402647 139590186821376 logging_writer.py:48] [392600] global_step=392600, grad_norm=4.629275798797607, loss=0.5756738185882568
I0301 19:29:58.334578 139590195214080 logging_writer.py:48] [392700] global_step=392700, grad_norm=4.798662185668945, loss=0.6062874794006348
I0301 19:30:32.258427 139590186821376 logging_writer.py:48] [392800] global_step=392800, grad_norm=4.914732456207275, loss=0.654498279094696
I0301 19:31:06.273418 139590195214080 logging_writer.py:48] [392900] global_step=392900, grad_norm=4.751748561859131, loss=0.6392477750778198
I0301 19:31:40.163480 139590186821376 logging_writer.py:48] [393000] global_step=393000, grad_norm=4.548764705657959, loss=0.6295245289802551
I0301 19:32:14.113527 139590195214080 logging_writer.py:48] [393100] global_step=393100, grad_norm=4.863618850708008, loss=0.6337649822235107
I0301 19:32:48.059846 139590186821376 logging_writer.py:48] [393200] global_step=393200, grad_norm=4.517271041870117, loss=0.6409263610839844
I0301 19:33:21.977223 139590195214080 logging_writer.py:48] [393300] global_step=393300, grad_norm=4.407440185546875, loss=0.6082104444503784
I0301 19:33:55.894567 139590186821376 logging_writer.py:48] [393400] global_step=393400, grad_norm=4.606418609619141, loss=0.6236991882324219
I0301 19:34:29.802989 139590195214080 logging_writer.py:48] [393500] global_step=393500, grad_norm=4.33311653137207, loss=0.6557775139808655
I0301 19:35:03.743461 139590186821376 logging_writer.py:48] [393600] global_step=393600, grad_norm=4.957788944244385, loss=0.6210233569145203
I0301 19:35:37.657393 139590195214080 logging_writer.py:48] [393700] global_step=393700, grad_norm=4.7398200035095215, loss=0.6541617512702942
I0301 19:35:51.707905 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:35:57.708976 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:36:05.915781 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:36:08.188599 139753105983296 submission_runner.py:411] Time since start: 138299.19s, 	Step: 393743, 	{'train/accuracy': 0.9607780575752258, 'train/loss': 0.14491522312164307, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0558115243911743, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8172541856765747, 'test/num_examples': 10000, 'score': 133685.6428039074, 'total_duration': 138299.19182229042, 'accumulated_submission_time': 133685.6428039074, 'accumulated_eval_time': 4580.938060998917, 'accumulated_logging_time': 18.762927532196045}
I0301 19:36:08.271920 139590144857856 logging_writer.py:48] [393743] accumulated_eval_time=4580.938061, accumulated_logging_time=18.762928, accumulated_submission_time=133685.642804, global_step=393743, preemption_count=0, score=133685.642804, test/accuracy=0.630200, test/loss=1.817254, test/num_examples=10000, total_duration=138299.191822, train/accuracy=0.960778, train/loss=0.144915, validation/accuracy=0.754720, validation/loss=1.055812, validation/num_examples=50000
I0301 19:36:27.950839 139590153250560 logging_writer.py:48] [393800] global_step=393800, grad_norm=4.507060527801514, loss=0.6290961503982544
I0301 19:37:01.842629 139590144857856 logging_writer.py:48] [393900] global_step=393900, grad_norm=4.536407947540283, loss=0.572744607925415
I0301 19:37:35.867328 139590153250560 logging_writer.py:48] [394000] global_step=394000, grad_norm=4.943962574005127, loss=0.6639097929000854
I0301 19:38:09.800576 139590144857856 logging_writer.py:48] [394100] global_step=394100, grad_norm=4.425804615020752, loss=0.6476610898971558
I0301 19:38:43.773010 139590153250560 logging_writer.py:48] [394200] global_step=394200, grad_norm=4.554972171783447, loss=0.5441808104515076
I0301 19:39:17.700004 139590144857856 logging_writer.py:48] [394300] global_step=394300, grad_norm=4.352388381958008, loss=0.5164172053337097
I0301 19:39:51.664073 139590153250560 logging_writer.py:48] [394400] global_step=394400, grad_norm=4.562161922454834, loss=0.6546354293823242
I0301 19:40:25.618122 139590144857856 logging_writer.py:48] [394500] global_step=394500, grad_norm=5.025949954986572, loss=0.7233954668045044
I0301 19:40:59.532827 139590153250560 logging_writer.py:48] [394600] global_step=394600, grad_norm=4.504915714263916, loss=0.6113567352294922
I0301 19:41:33.470117 139590144857856 logging_writer.py:48] [394700] global_step=394700, grad_norm=4.707104206085205, loss=0.6518083214759827
I0301 19:42:07.446753 139590153250560 logging_writer.py:48] [394800] global_step=394800, grad_norm=4.194502830505371, loss=0.5925700068473816
I0301 19:42:41.379702 139590144857856 logging_writer.py:48] [394900] global_step=394900, grad_norm=4.363624095916748, loss=0.5807573199272156
I0301 19:43:15.302296 139590153250560 logging_writer.py:48] [395000] global_step=395000, grad_norm=4.577681064605713, loss=0.6071146726608276
I0301 19:43:49.303698 139590144857856 logging_writer.py:48] [395100] global_step=395100, grad_norm=4.470434188842773, loss=0.6171196103096008
I0301 19:44:23.243897 139590153250560 logging_writer.py:48] [395200] global_step=395200, grad_norm=4.110757827758789, loss=0.5628485679626465
I0301 19:44:38.308220 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:44:44.308550 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:44:52.618625 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:44:54.897361 139753105983296 submission_runner.py:411] Time since start: 138825.90s, 	Step: 395246, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14853791892528534, 'validation/accuracy': 0.7544999718666077, 'validation/loss': 1.0541654825210571, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8179938793182373, 'test/num_examples': 10000, 'score': 134195.6141886711, 'total_duration': 138825.90058135986, 'accumulated_submission_time': 134195.6141886711, 'accumulated_eval_time': 4597.527137756348, 'accumulated_logging_time': 18.85831356048584}
I0301 19:44:54.979904 139589717059328 logging_writer.py:48] [395246] accumulated_eval_time=4597.527138, accumulated_logging_time=18.858314, accumulated_submission_time=134195.614189, global_step=395246, preemption_count=0, score=134195.614189, test/accuracy=0.630500, test/loss=1.817994, test/num_examples=10000, total_duration=138825.900581, train/accuracy=0.960180, train/loss=0.148538, validation/accuracy=0.754500, validation/loss=1.054165, validation/num_examples=50000
I0301 19:45:13.598192 139590170035968 logging_writer.py:48] [395300] global_step=395300, grad_norm=4.328653812408447, loss=0.6149198412895203
I0301 19:45:47.516163 139589717059328 logging_writer.py:48] [395400] global_step=395400, grad_norm=4.504677772521973, loss=0.6297559142112732
I0301 19:46:21.444067 139590170035968 logging_writer.py:48] [395500] global_step=395500, grad_norm=4.834356784820557, loss=0.6103907227516174
I0301 19:46:55.380084 139589717059328 logging_writer.py:48] [395600] global_step=395600, grad_norm=4.199299335479736, loss=0.5503857731819153
I0301 19:47:29.294149 139590170035968 logging_writer.py:48] [395700] global_step=395700, grad_norm=4.101824760437012, loss=0.5967011451721191
I0301 19:48:03.241922 139589717059328 logging_writer.py:48] [395800] global_step=395800, grad_norm=4.905062198638916, loss=0.7113781571388245
I0301 19:48:37.172408 139590170035968 logging_writer.py:48] [395900] global_step=395900, grad_norm=4.846850395202637, loss=0.6267701387405396
I0301 19:49:11.106720 139589717059328 logging_writer.py:48] [396000] global_step=396000, grad_norm=4.132651329040527, loss=0.6105446219444275
I0301 19:49:45.140327 139590170035968 logging_writer.py:48] [396100] global_step=396100, grad_norm=5.0033650398254395, loss=0.6989718079566956
I0301 19:50:19.075649 139589717059328 logging_writer.py:48] [396200] global_step=396200, grad_norm=4.547433853149414, loss=0.6415530443191528
I0301 19:50:53.026369 139590170035968 logging_writer.py:48] [396300] global_step=396300, grad_norm=5.04098653793335, loss=0.6757462024688721
I0301 19:51:26.990373 139589717059328 logging_writer.py:48] [396400] global_step=396400, grad_norm=4.591949462890625, loss=0.6206619143486023
I0301 19:52:00.939939 139590170035968 logging_writer.py:48] [396500] global_step=396500, grad_norm=4.357961654663086, loss=0.5914724469184875
I0301 19:52:34.870042 139589717059328 logging_writer.py:48] [396600] global_step=396600, grad_norm=4.5965776443481445, loss=0.6228537559509277
I0301 19:53:08.801701 139590170035968 logging_writer.py:48] [396700] global_step=396700, grad_norm=4.650771141052246, loss=0.6311421394348145
I0301 19:53:24.902994 139753105983296 spec.py:321] Evaluating on the training split.
I0301 19:53:30.922284 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 19:53:39.229737 139753105983296 spec.py:349] Evaluating on the test split.
I0301 19:53:41.609812 139753105983296 submission_runner.py:411] Time since start: 139352.61s, 	Step: 396749, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14720356464385986, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.054998517036438, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8182374238967896, 'test/num_examples': 10000, 'score': 134705.46764349937, 'total_duration': 139352.6130232811, 'accumulated_submission_time': 134705.46764349937, 'accumulated_eval_time': 4614.233896970749, 'accumulated_logging_time': 18.956045627593994}
I0301 19:53:41.697300 139590153250560 logging_writer.py:48] [396749] accumulated_eval_time=4614.233897, accumulated_logging_time=18.956046, accumulated_submission_time=134705.467643, global_step=396749, preemption_count=0, score=134705.467643, test/accuracy=0.630600, test/loss=1.818237, test/num_examples=10000, total_duration=139352.613023, train/accuracy=0.960619, train/loss=0.147204, validation/accuracy=0.755140, validation/loss=1.054999, validation/num_examples=50000
I0301 19:53:59.317883 139590161643264 logging_writer.py:48] [396800] global_step=396800, grad_norm=4.87697172164917, loss=0.6296865344047546
I0301 19:54:33.238370 139590153250560 logging_writer.py:48] [396900] global_step=396900, grad_norm=4.78206205368042, loss=0.6599224805831909
I0301 19:55:07.183418 139590161643264 logging_writer.py:48] [397000] global_step=397000, grad_norm=4.603403091430664, loss=0.6432663798332214
I0301 19:55:41.150090 139590153250560 logging_writer.py:48] [397100] global_step=397100, grad_norm=4.608016014099121, loss=0.5835849046707153
I0301 19:56:15.179943 139590161643264 logging_writer.py:48] [397200] global_step=397200, grad_norm=4.662546157836914, loss=0.6698967814445496
I0301 19:56:49.145146 139590153250560 logging_writer.py:48] [397300] global_step=397300, grad_norm=4.435478210449219, loss=0.5689547061920166
I0301 19:57:23.104567 139590161643264 logging_writer.py:48] [397400] global_step=397400, grad_norm=4.185401916503906, loss=0.641313910484314
I0301 19:57:57.064048 139590153250560 logging_writer.py:48] [397500] global_step=397500, grad_norm=4.119821548461914, loss=0.5880486965179443
I0301 19:58:31.012781 139590161643264 logging_writer.py:48] [397600] global_step=397600, grad_norm=4.538800239562988, loss=0.6162604689598083
I0301 19:59:04.964941 139590153250560 logging_writer.py:48] [397700] global_step=397700, grad_norm=4.523952960968018, loss=0.6887535452842712
I0301 19:59:38.922674 139590161643264 logging_writer.py:48] [397800] global_step=397800, grad_norm=4.3239850997924805, loss=0.5208784341812134
I0301 20:00:12.881332 139590153250560 logging_writer.py:48] [397900] global_step=397900, grad_norm=4.942667484283447, loss=0.6172739863395691
I0301 20:00:46.832392 139590161643264 logging_writer.py:48] [398000] global_step=398000, grad_norm=4.4796648025512695, loss=0.5870046615600586
I0301 20:01:20.793701 139590153250560 logging_writer.py:48] [398100] global_step=398100, grad_norm=4.916224956512451, loss=0.7044073343276978
I0301 20:01:54.839403 139590161643264 logging_writer.py:48] [398200] global_step=398200, grad_norm=4.330235481262207, loss=0.618699848651886
I0301 20:02:11.946661 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:02:17.972712 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:02:26.253203 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:02:28.511318 139753105983296 submission_runner.py:411] Time since start: 139879.51s, 	Step: 398252, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14786209166049957, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.055959701538086, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8208487033843994, 'test/num_examples': 10000, 'score': 135215.65347242355, 'total_duration': 139879.5144290924, 'accumulated_submission_time': 135215.65347242355, 'accumulated_eval_time': 4630.798380851746, 'accumulated_logging_time': 19.05357837677002}
I0301 20:02:28.595437 139590178428672 logging_writer.py:48] [398252] accumulated_eval_time=4630.798381, accumulated_logging_time=19.053578, accumulated_submission_time=135215.653472, global_step=398252, preemption_count=0, score=135215.653472, test/accuracy=0.630300, test/loss=1.820849, test/num_examples=10000, total_duration=139879.514429, train/accuracy=0.960698, train/loss=0.147862, validation/accuracy=0.754860, validation/loss=1.055960, validation/num_examples=50000
I0301 20:02:45.217646 139590186821376 logging_writer.py:48] [398300] global_step=398300, grad_norm=4.79116678237915, loss=0.6495583057403564
I0301 20:03:19.095602 139590178428672 logging_writer.py:48] [398400] global_step=398400, grad_norm=4.636440277099609, loss=0.6151980757713318
I0301 20:03:53.023705 139590186821376 logging_writer.py:48] [398500] global_step=398500, grad_norm=4.705796718597412, loss=0.6434421539306641
I0301 20:04:26.979089 139590178428672 logging_writer.py:48] [398600] global_step=398600, grad_norm=4.277837753295898, loss=0.5497615337371826
I0301 20:05:00.910200 139590186821376 logging_writer.py:48] [398700] global_step=398700, grad_norm=5.360932350158691, loss=0.6385470032691956
I0301 20:05:34.818399 139590178428672 logging_writer.py:48] [398800] global_step=398800, grad_norm=4.18234395980835, loss=0.6009388566017151
I0301 20:06:08.776715 139590186821376 logging_writer.py:48] [398900] global_step=398900, grad_norm=4.8094682693481445, loss=0.6607162952423096
I0301 20:06:42.749449 139590178428672 logging_writer.py:48] [399000] global_step=399000, grad_norm=4.519230365753174, loss=0.5537916421890259
I0301 20:07:16.687063 139590186821376 logging_writer.py:48] [399100] global_step=399100, grad_norm=4.415037631988525, loss=0.5600634813308716
I0301 20:07:50.639450 139590178428672 logging_writer.py:48] [399200] global_step=399200, grad_norm=4.330626964569092, loss=0.6358568668365479
I0301 20:08:24.648542 139590186821376 logging_writer.py:48] [399300] global_step=399300, grad_norm=4.505422592163086, loss=0.6584869623184204
I0301 20:08:58.595216 139590178428672 logging_writer.py:48] [399400] global_step=399400, grad_norm=4.597559928894043, loss=0.7056275606155396
I0301 20:09:32.571694 139590186821376 logging_writer.py:48] [399500] global_step=399500, grad_norm=4.589556694030762, loss=0.6357910633087158
I0301 20:10:06.523574 139590178428672 logging_writer.py:48] [399600] global_step=399600, grad_norm=4.26392126083374, loss=0.5989339351654053
I0301 20:10:40.489639 139590186821376 logging_writer.py:48] [399700] global_step=399700, grad_norm=4.395482063293457, loss=0.6197187900543213
I0301 20:10:58.605950 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:11:04.710628 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:11:12.913251 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:11:15.180293 139753105983296 submission_runner.py:411] Time since start: 140406.18s, 	Step: 399755, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14491653442382812, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.054701566696167, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8177335262298584, 'test/num_examples': 10000, 'score': 135725.60052633286, 'total_duration': 140406.1835153103, 'accumulated_submission_time': 135725.60052633286, 'accumulated_eval_time': 4647.372666358948, 'accumulated_logging_time': 19.147751808166504}
I0301 20:11:15.263264 139589725452032 logging_writer.py:48] [399755] accumulated_eval_time=4647.372666, accumulated_logging_time=19.147752, accumulated_submission_time=135725.600526, global_step=399755, preemption_count=0, score=135725.600526, test/accuracy=0.629700, test/loss=1.817734, test/num_examples=10000, total_duration=140406.183515, train/accuracy=0.960100, train/loss=0.144917, validation/accuracy=0.754880, validation/loss=1.054702, validation/num_examples=50000
I0301 20:11:30.874294 139590144857856 logging_writer.py:48] [399800] global_step=399800, grad_norm=4.764786243438721, loss=0.6486101150512695
I0301 20:12:04.829474 139589725452032 logging_writer.py:48] [399900] global_step=399900, grad_norm=4.67467737197876, loss=0.6294525265693665
I0301 20:12:38.778340 139590144857856 logging_writer.py:48] [400000] global_step=400000, grad_norm=4.668046951293945, loss=0.594698429107666
I0301 20:13:12.748862 139589725452032 logging_writer.py:48] [400100] global_step=400100, grad_norm=4.260594844818115, loss=0.6221679449081421
I0301 20:13:46.695575 139590144857856 logging_writer.py:48] [400200] global_step=400200, grad_norm=4.6450676918029785, loss=0.6088160276412964
I0301 20:14:20.747489 139589725452032 logging_writer.py:48] [400300] global_step=400300, grad_norm=4.689005374908447, loss=0.6192322969436646
I0301 20:14:54.694650 139590144857856 logging_writer.py:48] [400400] global_step=400400, grad_norm=4.570538520812988, loss=0.5881889462471008
I0301 20:15:28.651873 139589725452032 logging_writer.py:48] [400500] global_step=400500, grad_norm=4.305154800415039, loss=0.5784358978271484
I0301 20:16:02.597932 139590144857856 logging_writer.py:48] [400600] global_step=400600, grad_norm=4.792025566101074, loss=0.5920359492301941
I0301 20:16:36.551157 139589725452032 logging_writer.py:48] [400700] global_step=400700, grad_norm=4.665762424468994, loss=0.6947072744369507
I0301 20:17:10.491231 139590144857856 logging_writer.py:48] [400800] global_step=400800, grad_norm=4.8999433517456055, loss=0.638657808303833
I0301 20:17:44.446658 139589725452032 logging_writer.py:48] [400900] global_step=400900, grad_norm=4.905945777893066, loss=0.6875254511833191
I0301 20:18:18.404378 139590144857856 logging_writer.py:48] [401000] global_step=401000, grad_norm=4.427989959716797, loss=0.6210772395133972
I0301 20:18:52.367489 139589725452032 logging_writer.py:48] [401100] global_step=401100, grad_norm=4.589073181152344, loss=0.61891108751297
I0301 20:19:26.316407 139590144857856 logging_writer.py:48] [401200] global_step=401200, grad_norm=5.0848069190979, loss=0.5877421498298645
I0301 20:19:45.465056 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:19:51.457371 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:19:59.791558 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:20:02.079978 139753105983296 submission_runner.py:411] Time since start: 140933.08s, 	Step: 401258, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.14990736544132233, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0554331541061401, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8172955513000488, 'test/num_examples': 10000, 'score': 136235.73774003983, 'total_duration': 140933.08319306374, 'accumulated_submission_time': 136235.73774003983, 'accumulated_eval_time': 4663.987517356873, 'accumulated_logging_time': 19.24118137359619}
I0301 20:20:02.187389 139590170035968 logging_writer.py:48] [401258] accumulated_eval_time=4663.987517, accumulated_logging_time=19.241181, accumulated_submission_time=136235.737740, global_step=401258, preemption_count=0, score=136235.737740, test/accuracy=0.630400, test/loss=1.817296, test/num_examples=10000, total_duration=140933.083193, train/accuracy=0.959841, train/loss=0.149907, validation/accuracy=0.754760, validation/loss=1.055433, validation/num_examples=50000
I0301 20:20:16.800089 139590178428672 logging_writer.py:48] [401300] global_step=401300, grad_norm=4.440237045288086, loss=0.6002965569496155
I0301 20:20:50.775930 139590170035968 logging_writer.py:48] [401400] global_step=401400, grad_norm=4.824005603790283, loss=0.6701077222824097
I0301 20:21:24.730482 139590178428672 logging_writer.py:48] [401500] global_step=401500, grad_norm=4.284061908721924, loss=0.6094726920127869
I0301 20:21:58.672703 139590170035968 logging_writer.py:48] [401600] global_step=401600, grad_norm=4.409287929534912, loss=0.6026721000671387
I0301 20:22:32.626232 139590178428672 logging_writer.py:48] [401700] global_step=401700, grad_norm=4.459064960479736, loss=0.6556158661842346
I0301 20:23:06.585029 139590170035968 logging_writer.py:48] [401800] global_step=401800, grad_norm=4.580273628234863, loss=0.605846643447876
I0301 20:23:40.544905 139590178428672 logging_writer.py:48] [401900] global_step=401900, grad_norm=4.503741264343262, loss=0.6449998617172241
I0301 20:24:14.478818 139590170035968 logging_writer.py:48] [402000] global_step=402000, grad_norm=4.721362113952637, loss=0.6260439157485962
I0301 20:24:48.430355 139590178428672 logging_writer.py:48] [402100] global_step=402100, grad_norm=4.399996280670166, loss=0.5900295972824097
I0301 20:25:22.377706 139590170035968 logging_writer.py:48] [402200] global_step=402200, grad_norm=4.426968097686768, loss=0.5910435914993286
I0301 20:25:56.336247 139590178428672 logging_writer.py:48] [402300] global_step=402300, grad_norm=4.418058395385742, loss=0.646113932132721
I0301 20:26:30.291378 139590170035968 logging_writer.py:48] [402400] global_step=402400, grad_norm=4.749893665313721, loss=0.6263829469680786
I0301 20:27:04.402371 139590178428672 logging_writer.py:48] [402500] global_step=402500, grad_norm=4.196653842926025, loss=0.6149591207504272
I0301 20:27:38.332684 139590170035968 logging_writer.py:48] [402600] global_step=402600, grad_norm=4.612720489501953, loss=0.6434136033058167
I0301 20:28:12.271620 139590178428672 logging_writer.py:48] [402700] global_step=402700, grad_norm=4.738899230957031, loss=0.6127243041992188
I0301 20:28:32.082805 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:28:38.067911 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:28:46.375441 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:28:48.625290 139753105983296 submission_runner.py:411] Time since start: 141459.63s, 	Step: 402760, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.14752644300460815, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.056088924407959, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8198953866958618, 'test/num_examples': 10000, 'score': 136745.5668039322, 'total_duration': 141459.62825584412, 'accumulated_submission_time': 136745.5668039322, 'accumulated_eval_time': 4680.529687643051, 'accumulated_logging_time': 19.362505674362183}
I0301 20:28:48.715425 139589725452032 logging_writer.py:48] [402760] accumulated_eval_time=4680.529688, accumulated_logging_time=19.362506, accumulated_submission_time=136745.566804, global_step=402760, preemption_count=0, score=136745.566804, test/accuracy=0.630400, test/loss=1.819895, test/num_examples=10000, total_duration=141459.628256, train/accuracy=0.960260, train/loss=0.147526, validation/accuracy=0.754960, validation/loss=1.056089, validation/num_examples=50000
I0301 20:29:02.621482 139590144857856 logging_writer.py:48] [402800] global_step=402800, grad_norm=5.129146575927734, loss=0.728264331817627
I0301 20:29:36.497322 139589725452032 logging_writer.py:48] [402900] global_step=402900, grad_norm=4.403960227966309, loss=0.591538667678833
I0301 20:30:10.471081 139590144857856 logging_writer.py:48] [403000] global_step=403000, grad_norm=4.511471748352051, loss=0.5788421034812927
I0301 20:30:44.411024 139589725452032 logging_writer.py:48] [403100] global_step=403100, grad_norm=4.507660865783691, loss=0.6546890735626221
I0301 20:31:18.383227 139590144857856 logging_writer.py:48] [403200] global_step=403200, grad_norm=5.083110332489014, loss=0.7027173042297363
I0301 20:31:52.358236 139589725452032 logging_writer.py:48] [403300] global_step=403300, grad_norm=4.8149542808532715, loss=0.639842689037323
I0301 20:32:26.292515 139590144857856 logging_writer.py:48] [403400] global_step=403400, grad_norm=5.134887218475342, loss=0.6315457820892334
I0301 20:33:00.421360 139589725452032 logging_writer.py:48] [403500] global_step=403500, grad_norm=4.6691741943359375, loss=0.6452908515930176
I0301 20:33:34.347722 139590144857856 logging_writer.py:48] [403600] global_step=403600, grad_norm=4.774650573730469, loss=0.7158358097076416
I0301 20:34:08.299937 139589725452032 logging_writer.py:48] [403700] global_step=403700, grad_norm=4.468629360198975, loss=0.6471080183982849
I0301 20:34:42.279303 139590144857856 logging_writer.py:48] [403800] global_step=403800, grad_norm=4.755936622619629, loss=0.653434157371521
I0301 20:35:16.214996 139589725452032 logging_writer.py:48] [403900] global_step=403900, grad_norm=4.126957416534424, loss=0.577000617980957
I0301 20:35:50.148322 139590144857856 logging_writer.py:48] [404000] global_step=404000, grad_norm=4.591041088104248, loss=0.6132566928863525
I0301 20:36:24.107160 139589725452032 logging_writer.py:48] [404100] global_step=404100, grad_norm=4.836190223693848, loss=0.6331513524055481
I0301 20:36:58.014562 139590144857856 logging_writer.py:48] [404200] global_step=404200, grad_norm=4.531872272491455, loss=0.625104546546936
I0301 20:37:18.865196 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:37:24.889658 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:37:33.121426 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:37:35.509433 139753105983296 submission_runner.py:411] Time since start: 141986.51s, 	Step: 404263, 	{'train/accuracy': 0.9608378410339355, 'train/loss': 0.1480744481086731, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0558913946151733, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8172743320465088, 'test/num_examples': 10000, 'score': 137255.6495103836, 'total_duration': 141986.512663126, 'accumulated_submission_time': 137255.6495103836, 'accumulated_eval_time': 4697.173872232437, 'accumulated_logging_time': 19.46660351753235}
I0301 20:37:35.593938 139589725452032 logging_writer.py:48] [404263] accumulated_eval_time=4697.173872, accumulated_logging_time=19.466604, accumulated_submission_time=137255.649510, global_step=404263, preemption_count=0, score=137255.649510, test/accuracy=0.630400, test/loss=1.817274, test/num_examples=10000, total_duration=141986.512663, train/accuracy=0.960838, train/loss=0.148074, validation/accuracy=0.754660, validation/loss=1.055891, validation/num_examples=50000
I0301 20:37:48.466272 139590170035968 logging_writer.py:48] [404300] global_step=404300, grad_norm=4.743274211883545, loss=0.655937671661377
I0301 20:38:22.335689 139589725452032 logging_writer.py:48] [404400] global_step=404400, grad_norm=4.889874458312988, loss=0.569317638874054
I0301 20:38:56.274376 139590170035968 logging_writer.py:48] [404500] global_step=404500, grad_norm=4.508694171905518, loss=0.5976192951202393
I0301 20:39:30.401011 139589725452032 logging_writer.py:48] [404600] global_step=404600, grad_norm=4.448760509490967, loss=0.5675826668739319
I0301 20:40:04.369987 139590170035968 logging_writer.py:48] [404700] global_step=404700, grad_norm=4.611186504364014, loss=0.6440641283988953
I0301 20:40:38.293797 139589725452032 logging_writer.py:48] [404800] global_step=404800, grad_norm=4.294622898101807, loss=0.5579978227615356
I0301 20:41:12.238286 139590170035968 logging_writer.py:48] [404900] global_step=404900, grad_norm=4.771697044372559, loss=0.6686446666717529
I0301 20:41:46.188740 139589725452032 logging_writer.py:48] [405000] global_step=405000, grad_norm=4.884231090545654, loss=0.6560733914375305
I0301 20:42:20.102831 139590170035968 logging_writer.py:48] [405100] global_step=405100, grad_norm=4.542092800140381, loss=0.6374030709266663
I0301 20:42:54.028284 139589725452032 logging_writer.py:48] [405200] global_step=405200, grad_norm=4.225919723510742, loss=0.5671316981315613
I0301 20:43:27.958966 139590170035968 logging_writer.py:48] [405300] global_step=405300, grad_norm=4.732245445251465, loss=0.6357765197753906
I0301 20:44:01.861477 139589725452032 logging_writer.py:48] [405400] global_step=405400, grad_norm=4.420747756958008, loss=0.6647357940673828
I0301 20:44:35.802571 139590170035968 logging_writer.py:48] [405500] global_step=405500, grad_norm=4.557046890258789, loss=0.6781449317932129
I0301 20:45:09.868026 139589725452032 logging_writer.py:48] [405600] global_step=405600, grad_norm=4.232450008392334, loss=0.6484047770500183
I0301 20:45:43.786006 139590170035968 logging_writer.py:48] [405700] global_step=405700, grad_norm=4.62449312210083, loss=0.610375702381134
I0301 20:46:05.659604 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:46:11.694145 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:46:19.978587 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:46:22.264248 139753105983296 submission_runner.py:411] Time since start: 142513.27s, 	Step: 405766, 	{'train/accuracy': 0.9597815275192261, 'train/loss': 0.14900541305541992, 'validation/accuracy': 0.754539966583252, 'validation/loss': 1.0548299551010132, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.818644642829895, 'test/num_examples': 10000, 'score': 137765.65123844147, 'total_duration': 142513.2674689293, 'accumulated_submission_time': 137765.65123844147, 'accumulated_eval_time': 4713.778460979462, 'accumulated_logging_time': 19.561405658721924}
I0301 20:46:22.343446 139589717059328 logging_writer.py:48] [405766] accumulated_eval_time=4713.778461, accumulated_logging_time=19.561406, accumulated_submission_time=137765.651238, global_step=405766, preemption_count=0, score=137765.651238, test/accuracy=0.630000, test/loss=1.818645, test/num_examples=10000, total_duration=142513.267469, train/accuracy=0.959782, train/loss=0.149005, validation/accuracy=0.754540, validation/loss=1.054830, validation/num_examples=50000
I0301 20:46:34.224378 139590153250560 logging_writer.py:48] [405800] global_step=405800, grad_norm=4.542658805847168, loss=0.6398065090179443
I0301 20:47:08.145117 139589717059328 logging_writer.py:48] [405900] global_step=405900, grad_norm=4.335453510284424, loss=0.6070207953453064
I0301 20:47:42.087946 139590153250560 logging_writer.py:48] [406000] global_step=406000, grad_norm=5.2922282218933105, loss=0.7055321931838989
I0301 20:48:16.026251 139589717059328 logging_writer.py:48] [406100] global_step=406100, grad_norm=4.504239082336426, loss=0.6549109220504761
I0301 20:48:49.992286 139590153250560 logging_writer.py:48] [406200] global_step=406200, grad_norm=4.490591049194336, loss=0.605979323387146
I0301 20:49:23.922837 139589717059328 logging_writer.py:48] [406300] global_step=406300, grad_norm=4.36264705657959, loss=0.6414850354194641
I0301 20:49:57.863144 139590153250560 logging_writer.py:48] [406400] global_step=406400, grad_norm=5.188692569732666, loss=0.690380334854126
I0301 20:50:31.833310 139589717059328 logging_writer.py:48] [406500] global_step=406500, grad_norm=4.301257610321045, loss=0.5584520697593689
I0301 20:51:05.775293 139590153250560 logging_writer.py:48] [406600] global_step=406600, grad_norm=4.704710960388184, loss=0.614808201789856
I0301 20:51:39.876035 139589717059328 logging_writer.py:48] [406700] global_step=406700, grad_norm=4.455601692199707, loss=0.6067584156990051
I0301 20:52:13.820556 139590153250560 logging_writer.py:48] [406800] global_step=406800, grad_norm=4.486076354980469, loss=0.6066858172416687
I0301 20:52:47.755221 139589717059328 logging_writer.py:48] [406900] global_step=406900, grad_norm=4.193342685699463, loss=0.5665724277496338
I0301 20:53:21.681119 139590153250560 logging_writer.py:48] [407000] global_step=407000, grad_norm=4.4028706550598145, loss=0.5959540605545044
I0301 20:53:55.637586 139589717059328 logging_writer.py:48] [407100] global_step=407100, grad_norm=4.452267169952393, loss=0.6155657172203064
I0301 20:54:29.566547 139590153250560 logging_writer.py:48] [407200] global_step=407200, grad_norm=4.475986003875732, loss=0.5859758853912354
I0301 20:54:52.424639 139753105983296 spec.py:321] Evaluating on the training split.
I0301 20:54:58.473404 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 20:55:06.826468 139753105983296 spec.py:349] Evaluating on the test split.
I0301 20:55:09.081688 139753105983296 submission_runner.py:411] Time since start: 143040.08s, 	Step: 407269, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14444862306118011, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.0550346374511719, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8174784183502197, 'test/num_examples': 10000, 'score': 138275.66917204857, 'total_duration': 143040.0849058628, 'accumulated_submission_time': 138275.66917204857, 'accumulated_eval_time': 4730.435454368591, 'accumulated_logging_time': 19.65113377571106}
I0301 20:55:09.164067 139589725452032 logging_writer.py:48] [407269] accumulated_eval_time=4730.435454, accumulated_logging_time=19.651134, accumulated_submission_time=138275.669172, global_step=407269, preemption_count=0, score=138275.669172, test/accuracy=0.630300, test/loss=1.817478, test/num_examples=10000, total_duration=143040.084906, train/accuracy=0.961057, train/loss=0.144449, validation/accuracy=0.754620, validation/loss=1.055035, validation/num_examples=50000
I0301 20:55:19.997665 139590144857856 logging_writer.py:48] [407300] global_step=407300, grad_norm=4.466887474060059, loss=0.6234943270683289
I0301 20:55:53.896715 139589725452032 logging_writer.py:48] [407400] global_step=407400, grad_norm=4.912968158721924, loss=0.6298993825912476
I0301 20:56:27.806704 139590144857856 logging_writer.py:48] [407500] global_step=407500, grad_norm=4.527434825897217, loss=0.5800659656524658
I0301 20:57:01.744811 139589725452032 logging_writer.py:48] [407600] global_step=407600, grad_norm=4.437252521514893, loss=0.6607637405395508
I0301 20:57:35.676020 139590144857856 logging_writer.py:48] [407700] global_step=407700, grad_norm=4.014692783355713, loss=0.5490814447402954
I0301 20:58:09.776884 139589725452032 logging_writer.py:48] [407800] global_step=407800, grad_norm=4.655611515045166, loss=0.6368611454963684
I0301 20:58:43.720153 139590144857856 logging_writer.py:48] [407900] global_step=407900, grad_norm=4.359073162078857, loss=0.6223359107971191
I0301 20:59:17.604827 139589725452032 logging_writer.py:48] [408000] global_step=408000, grad_norm=4.607272148132324, loss=0.7059160470962524
I0301 20:59:51.571490 139590144857856 logging_writer.py:48] [408100] global_step=408100, grad_norm=4.5894622802734375, loss=0.5886417627334595
I0301 21:00:25.516045 139589725452032 logging_writer.py:48] [408200] global_step=408200, grad_norm=4.165447235107422, loss=0.5897653102874756
I0301 21:00:59.464045 139590144857856 logging_writer.py:48] [408300] global_step=408300, grad_norm=4.529992580413818, loss=0.5986781120300293
I0301 21:01:33.398549 139589725452032 logging_writer.py:48] [408400] global_step=408400, grad_norm=4.678433895111084, loss=0.6786041855812073
I0301 21:02:07.355831 139590144857856 logging_writer.py:48] [408500] global_step=408500, grad_norm=4.282090187072754, loss=0.670573353767395
I0301 21:02:41.288085 139589725452032 logging_writer.py:48] [408600] global_step=408600, grad_norm=4.598855972290039, loss=0.644405722618103
I0301 21:03:15.236196 139590144857856 logging_writer.py:48] [408700] global_step=408700, grad_norm=4.5685529708862305, loss=0.5588611364364624
I0301 21:03:39.148553 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:03:45.150260 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:03:53.518966 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:03:55.829526 139753105983296 submission_runner.py:411] Time since start: 143566.83s, 	Step: 408772, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.15054501593112946, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0552719831466675, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8169559240341187, 'test/num_examples': 10000, 'score': 138785.59047579765, 'total_duration': 143566.8327577114, 'accumulated_submission_time': 138785.59047579765, 'accumulated_eval_time': 4747.116379022598, 'accumulated_logging_time': 19.74397087097168}
I0301 21:03:55.915405 139589717059328 logging_writer.py:48] [408772] accumulated_eval_time=4747.116379, accumulated_logging_time=19.743971, accumulated_submission_time=138785.590476, global_step=408772, preemption_count=0, score=138785.590476, test/accuracy=0.630400, test/loss=1.816956, test/num_examples=10000, total_duration=143566.832758, train/accuracy=0.959263, train/loss=0.150545, validation/accuracy=0.755240, validation/loss=1.055272, validation/num_examples=50000
I0301 21:04:05.768541 139589725452032 logging_writer.py:48] [408800] global_step=408800, grad_norm=4.773853302001953, loss=0.7175203561782837
I0301 21:04:39.666870 139589717059328 logging_writer.py:48] [408900] global_step=408900, grad_norm=4.823192119598389, loss=0.6552329063415527
I0301 21:05:13.598772 139589725452032 logging_writer.py:48] [409000] global_step=409000, grad_norm=4.38663911819458, loss=0.5960173010826111
I0301 21:05:47.592029 139589717059328 logging_writer.py:48] [409100] global_step=409100, grad_norm=4.852063179016113, loss=0.6307457685470581
I0301 21:06:21.552723 139589725452032 logging_writer.py:48] [409200] global_step=409200, grad_norm=4.420924186706543, loss=0.5986853837966919
I0301 21:06:55.541553 139589717059328 logging_writer.py:48] [409300] global_step=409300, grad_norm=4.142236232757568, loss=0.6578056216239929
I0301 21:07:29.505886 139589725452032 logging_writer.py:48] [409400] global_step=409400, grad_norm=4.3802337646484375, loss=0.6583649516105652
I0301 21:08:03.479222 139589717059328 logging_writer.py:48] [409500] global_step=409500, grad_norm=4.624549388885498, loss=0.6543461084365845
I0301 21:08:37.430857 139589725452032 logging_writer.py:48] [409600] global_step=409600, grad_norm=4.44714879989624, loss=0.6505247950553894
I0301 21:09:11.404464 139589717059328 logging_writer.py:48] [409700] global_step=409700, grad_norm=4.100200653076172, loss=0.5366408824920654
I0301 21:09:45.370615 139589725452032 logging_writer.py:48] [409800] global_step=409800, grad_norm=4.530747890472412, loss=0.5602962970733643
I0301 21:10:19.423103 139589717059328 logging_writer.py:48] [409900] global_step=409900, grad_norm=5.02154016494751, loss=0.6333495378494263
I0301 21:10:53.376267 139589725452032 logging_writer.py:48] [410000] global_step=410000, grad_norm=4.570985794067383, loss=0.5745331645011902
I0301 21:11:27.346385 139589717059328 logging_writer.py:48] [410100] global_step=410100, grad_norm=4.142336368560791, loss=0.6220152974128723
I0301 21:12:01.296539 139589725452032 logging_writer.py:48] [410200] global_step=410200, grad_norm=4.872875690460205, loss=0.6824395060539246
I0301 21:12:25.907885 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:12:31.903021 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:12:40.207733 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:12:42.491110 139753105983296 submission_runner.py:411] Time since start: 144093.49s, 	Step: 410274, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14515945315361023, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.0560662746429443, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8201110363006592, 'test/num_examples': 10000, 'score': 139295.51828718185, 'total_duration': 144093.49433231354, 'accumulated_submission_time': 139295.51828718185, 'accumulated_eval_time': 4763.699547767639, 'accumulated_logging_time': 19.84065055847168}
I0301 21:12:42.582020 139589725452032 logging_writer.py:48] [410274] accumulated_eval_time=4763.699548, accumulated_logging_time=19.840651, accumulated_submission_time=139295.518287, global_step=410274, preemption_count=0, score=139295.518287, test/accuracy=0.629700, test/loss=1.820111, test/num_examples=10000, total_duration=144093.494332, train/accuracy=0.960738, train/loss=0.145159, validation/accuracy=0.755100, validation/loss=1.056066, validation/num_examples=50000
I0301 21:12:51.747562 139590153250560 logging_writer.py:48] [410300] global_step=410300, grad_norm=4.2149658203125, loss=0.5662628412246704
I0301 21:13:25.670903 139589725452032 logging_writer.py:48] [410400] global_step=410400, grad_norm=4.668247222900391, loss=0.5766804814338684
I0301 21:13:59.609873 139590153250560 logging_writer.py:48] [410500] global_step=410500, grad_norm=4.401419639587402, loss=0.5381469130516052
I0301 21:14:33.550786 139589725452032 logging_writer.py:48] [410600] global_step=410600, grad_norm=4.285789489746094, loss=0.5766773819923401
I0301 21:15:07.460787 139590153250560 logging_writer.py:48] [410700] global_step=410700, grad_norm=4.269411087036133, loss=0.6159927248954773
I0301 21:15:41.385647 139589725452032 logging_writer.py:48] [410800] global_step=410800, grad_norm=4.6578497886657715, loss=0.5811992883682251
I0301 21:16:15.381016 139590153250560 logging_writer.py:48] [410900] global_step=410900, grad_norm=4.318172931671143, loss=0.6218450665473938
I0301 21:16:49.317735 139589725452032 logging_writer.py:48] [411000] global_step=411000, grad_norm=4.493006229400635, loss=0.6225974559783936
I0301 21:17:23.238989 139590153250560 logging_writer.py:48] [411100] global_step=411100, grad_norm=4.4249186515808105, loss=0.612008810043335
I0301 21:17:57.175196 139589725452032 logging_writer.py:48] [411200] global_step=411200, grad_norm=4.807603359222412, loss=0.6891853213310242
I0301 21:18:31.106414 139590153250560 logging_writer.py:48] [411300] global_step=411300, grad_norm=4.513813018798828, loss=0.6450884342193604
I0301 21:19:05.050313 139589725452032 logging_writer.py:48] [411400] global_step=411400, grad_norm=4.689009666442871, loss=0.6616309285163879
I0301 21:19:39.011191 139590153250560 logging_writer.py:48] [411500] global_step=411500, grad_norm=4.421548366546631, loss=0.6023977994918823
I0301 21:20:12.943109 139589725452032 logging_writer.py:48] [411600] global_step=411600, grad_norm=4.476534366607666, loss=0.5858388543128967
I0301 21:20:46.884569 139590153250560 logging_writer.py:48] [411700] global_step=411700, grad_norm=4.765864849090576, loss=0.5998842120170593
I0301 21:21:12.811764 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:21:18.813012 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:21:27.122676 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:21:29.415017 139753105983296 submission_runner.py:411] Time since start: 144620.42s, 	Step: 411778, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14583516120910645, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0559715032577515, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8194156885147095, 'test/num_examples': 10000, 'score': 139805.68612885475, 'total_duration': 144620.41824531555, 'accumulated_submission_time': 139805.68612885475, 'accumulated_eval_time': 4780.302748441696, 'accumulated_logging_time': 19.94161033630371}
I0301 21:21:29.504948 139590161643264 logging_writer.py:48] [411778] accumulated_eval_time=4780.302748, accumulated_logging_time=19.941610, accumulated_submission_time=139805.686129, global_step=411778, preemption_count=0, score=139805.686129, test/accuracy=0.629700, test/loss=1.819416, test/num_examples=10000, total_duration=144620.418245, train/accuracy=0.960738, train/loss=0.145835, validation/accuracy=0.754740, validation/loss=1.055972, validation/num_examples=50000
I0301 21:21:37.339263 139590186821376 logging_writer.py:48] [411800] global_step=411800, grad_norm=4.395965099334717, loss=0.5964378118515015
I0301 21:22:11.236250 139590161643264 logging_writer.py:48] [411900] global_step=411900, grad_norm=5.066912651062012, loss=0.6380829215049744
I0301 21:22:45.270307 139590186821376 logging_writer.py:48] [412000] global_step=412000, grad_norm=4.550548553466797, loss=0.6826500296592712
I0301 21:23:19.234508 139590161643264 logging_writer.py:48] [412100] global_step=412100, grad_norm=4.370044708251953, loss=0.6590040326118469
I0301 21:23:53.170660 139590186821376 logging_writer.py:48] [412200] global_step=412200, grad_norm=4.281890392303467, loss=0.5162797570228577
I0301 21:24:27.121319 139590161643264 logging_writer.py:48] [412300] global_step=412300, grad_norm=4.315255165100098, loss=0.6765651702880859
I0301 21:25:01.095932 139590186821376 logging_writer.py:48] [412400] global_step=412400, grad_norm=4.550618648529053, loss=0.6221835017204285
I0301 21:25:35.053949 139590161643264 logging_writer.py:48] [412500] global_step=412500, grad_norm=5.195230960845947, loss=0.6977741718292236
I0301 21:26:09.012248 139590186821376 logging_writer.py:48] [412600] global_step=412600, grad_norm=4.598077774047852, loss=0.6447569727897644
I0301 21:26:42.978021 139590161643264 logging_writer.py:48] [412700] global_step=412700, grad_norm=4.464300155639648, loss=0.5566928386688232
I0301 21:27:16.911408 139590186821376 logging_writer.py:48] [412800] global_step=412800, grad_norm=4.606499195098877, loss=0.5880651473999023
I0301 21:27:50.848636 139590161643264 logging_writer.py:48] [412900] global_step=412900, grad_norm=4.296964645385742, loss=0.6613922119140625
I0301 21:28:24.781424 139590186821376 logging_writer.py:48] [413000] global_step=413000, grad_norm=4.055041313171387, loss=0.6002702713012695
I0301 21:28:58.802483 139590161643264 logging_writer.py:48] [413100] global_step=413100, grad_norm=4.614755630493164, loss=0.6583332419395447
I0301 21:29:32.775854 139590186821376 logging_writer.py:48] [413200] global_step=413200, grad_norm=4.624172210693359, loss=0.7116013169288635
I0301 21:29:59.727750 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:30:05.988028 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:30:14.066108 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:30:16.351425 139753105983296 submission_runner.py:411] Time since start: 145147.35s, 	Step: 413281, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14741158485412598, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.05522882938385, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.818820595741272, 'test/num_examples': 10000, 'score': 140315.84532666206, 'total_duration': 145147.35459661484, 'accumulated_submission_time': 140315.84532666206, 'accumulated_eval_time': 4796.926322221756, 'accumulated_logging_time': 20.04148244857788}
I0301 21:30:16.441177 139590144857856 logging_writer.py:48] [413281] accumulated_eval_time=4796.926322, accumulated_logging_time=20.041482, accumulated_submission_time=140315.845327, global_step=413281, preemption_count=0, score=140315.845327, test/accuracy=0.630700, test/loss=1.818821, test/num_examples=10000, total_duration=145147.354597, train/accuracy=0.961017, train/loss=0.147412, validation/accuracy=0.754560, validation/loss=1.055229, validation/num_examples=50000
I0301 21:30:23.226533 139590153250560 logging_writer.py:48] [413300] global_step=413300, grad_norm=4.272008895874023, loss=0.5552223920822144
I0301 21:30:57.125535 139590144857856 logging_writer.py:48] [413400] global_step=413400, grad_norm=4.405101776123047, loss=0.5931195616722107
I0301 21:31:31.051652 139590153250560 logging_writer.py:48] [413500] global_step=413500, grad_norm=4.587960720062256, loss=0.6063327789306641
I0301 21:32:04.986571 139590144857856 logging_writer.py:48] [413600] global_step=413600, grad_norm=4.695224761962891, loss=0.5919525623321533
I0301 21:32:38.955329 139590153250560 logging_writer.py:48] [413700] global_step=413700, grad_norm=4.880111217498779, loss=0.6701786518096924
I0301 21:33:12.896613 139590144857856 logging_writer.py:48] [413800] global_step=413800, grad_norm=4.411767959594727, loss=0.6223753690719604
I0301 21:33:46.859931 139590153250560 logging_writer.py:48] [413900] global_step=413900, grad_norm=4.897469520568848, loss=0.691116213798523
I0301 21:34:20.800327 139590144857856 logging_writer.py:48] [414000] global_step=414000, grad_norm=4.271490097045898, loss=0.6042084693908691
I0301 21:34:54.848950 139590153250560 logging_writer.py:48] [414100] global_step=414100, grad_norm=4.5105299949646, loss=0.6551546454429626
I0301 21:35:28.777713 139590144857856 logging_writer.py:48] [414200] global_step=414200, grad_norm=5.036640644073486, loss=0.6740937829017639
I0301 21:36:02.683234 139590153250560 logging_writer.py:48] [414300] global_step=414300, grad_norm=4.783891201019287, loss=0.6836760640144348
I0301 21:36:36.623191 139590144857856 logging_writer.py:48] [414400] global_step=414400, grad_norm=4.214298725128174, loss=0.6196635365486145
I0301 21:37:10.592607 139590153250560 logging_writer.py:48] [414500] global_step=414500, grad_norm=4.86247444152832, loss=0.6499580144882202
I0301 21:37:44.540564 139590144857856 logging_writer.py:48] [414600] global_step=414600, grad_norm=4.366042137145996, loss=0.6093838214874268
I0301 21:38:18.444828 139590153250560 logging_writer.py:48] [414700] global_step=414700, grad_norm=4.427710056304932, loss=0.6363431215286255
I0301 21:38:46.425428 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:38:52.407678 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:39:00.699918 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:39:03.051584 139753105983296 submission_runner.py:411] Time since start: 145674.05s, 	Step: 414784, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.14735473692417145, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0550575256347656, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.818278431892395, 'test/num_examples': 10000, 'score': 140825.76637721062, 'total_duration': 145674.05481624603, 'accumulated_submission_time': 140825.76637721062, 'accumulated_eval_time': 4813.552445888519, 'accumulated_logging_time': 20.141366481781006}
I0301 21:39:03.138938 139589717059328 logging_writer.py:48] [414784] accumulated_eval_time=4813.552446, accumulated_logging_time=20.141366, accumulated_submission_time=140825.766377, global_step=414784, preemption_count=0, score=140825.766377, test/accuracy=0.630200, test/loss=1.818278, test/num_examples=10000, total_duration=145674.054816, train/accuracy=0.959861, train/loss=0.147355, validation/accuracy=0.754880, validation/loss=1.055058, validation/num_examples=50000
I0301 21:39:08.923098 139589725452032 logging_writer.py:48] [414800] global_step=414800, grad_norm=4.505484104156494, loss=0.5849979519844055
I0301 21:39:42.802754 139589717059328 logging_writer.py:48] [414900] global_step=414900, grad_norm=5.307786464691162, loss=0.693347692489624
I0301 21:40:16.723140 139589725452032 logging_writer.py:48] [415000] global_step=415000, grad_norm=4.258073806762695, loss=0.6529495716094971
I0301 21:40:50.736855 139589717059328 logging_writer.py:48] [415100] global_step=415100, grad_norm=4.592393398284912, loss=0.5986616611480713
I0301 21:41:24.671192 139589725452032 logging_writer.py:48] [415200] global_step=415200, grad_norm=4.540938854217529, loss=0.6041219234466553
I0301 21:41:58.593589 139589717059328 logging_writer.py:48] [415300] global_step=415300, grad_norm=4.340109348297119, loss=0.6501943469047546
I0301 21:42:32.560114 139589725452032 logging_writer.py:48] [415400] global_step=415400, grad_norm=4.523680210113525, loss=0.6024397015571594
I0301 21:43:06.490788 139589717059328 logging_writer.py:48] [415500] global_step=415500, grad_norm=4.303312301635742, loss=0.5822584629058838
I0301 21:43:40.424032 139589725452032 logging_writer.py:48] [415600] global_step=415600, grad_norm=4.690174579620361, loss=0.6482622027397156
I0301 21:44:14.367866 139589717059328 logging_writer.py:48] [415700] global_step=415700, grad_norm=4.320748805999756, loss=0.5930274724960327
I0301 21:44:48.311668 139589725452032 logging_writer.py:48] [415800] global_step=415800, grad_norm=4.675334453582764, loss=0.6396183371543884
I0301 21:45:22.229867 139589717059328 logging_writer.py:48] [415900] global_step=415900, grad_norm=4.393703460693359, loss=0.557276725769043
I0301 21:45:56.177461 139589725452032 logging_writer.py:48] [416000] global_step=416000, grad_norm=4.443148612976074, loss=0.6086598038673401
I0301 21:46:30.125174 139589717059328 logging_writer.py:48] [416100] global_step=416100, grad_norm=5.333603858947754, loss=0.6242889761924744
I0301 21:47:04.255394 139589725452032 logging_writer.py:48] [416200] global_step=416200, grad_norm=4.406230449676514, loss=0.6463248133659363
I0301 21:47:33.234073 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:47:39.291635 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:47:47.532313 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:47:49.821664 139753105983296 submission_runner.py:411] Time since start: 146200.82s, 	Step: 416287, 	{'train/accuracy': 0.9617745280265808, 'train/loss': 0.1414497345685959, 'validation/accuracy': 0.7548199892044067, 'validation/loss': 1.056381106376648, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8195641040802002, 'test/num_examples': 10000, 'score': 141335.79673457146, 'total_duration': 146200.8248922825, 'accumulated_submission_time': 141335.79673457146, 'accumulated_eval_time': 4830.139992237091, 'accumulated_logging_time': 20.239493131637573}
I0301 21:47:49.913837 139590153250560 logging_writer.py:48] [416287] accumulated_eval_time=4830.139992, accumulated_logging_time=20.239493, accumulated_submission_time=141335.796735, global_step=416287, preemption_count=0, score=141335.796735, test/accuracy=0.629800, test/loss=1.819564, test/num_examples=10000, total_duration=146200.824892, train/accuracy=0.961775, train/loss=0.141450, validation/accuracy=0.754820, validation/loss=1.056381, validation/num_examples=50000
I0301 21:47:54.687404 139590170035968 logging_writer.py:48] [416300] global_step=416300, grad_norm=4.681917190551758, loss=0.5891886353492737
I0301 21:48:28.581602 139590153250560 logging_writer.py:48] [416400] global_step=416400, grad_norm=4.663512706756592, loss=0.704387366771698
I0301 21:49:02.455822 139590170035968 logging_writer.py:48] [416500] global_step=416500, grad_norm=4.465437412261963, loss=0.5645412802696228
I0301 21:49:36.373430 139590153250560 logging_writer.py:48] [416600] global_step=416600, grad_norm=4.682072162628174, loss=0.594109833240509
I0301 21:50:10.320249 139590170035968 logging_writer.py:48] [416700] global_step=416700, grad_norm=4.312528610229492, loss=0.6170027852058411
I0301 21:50:44.257345 139590153250560 logging_writer.py:48] [416800] global_step=416800, grad_norm=5.11793851852417, loss=0.6490902304649353
I0301 21:51:18.172422 139590170035968 logging_writer.py:48] [416900] global_step=416900, grad_norm=4.692628383636475, loss=0.6225113272666931
I0301 21:51:52.096044 139590153250560 logging_writer.py:48] [417000] global_step=417000, grad_norm=4.6560845375061035, loss=0.5720444321632385
I0301 21:52:26.009397 139590170035968 logging_writer.py:48] [417100] global_step=417100, grad_norm=4.726490020751953, loss=0.6287523508071899
I0301 21:52:59.945063 139590153250560 logging_writer.py:48] [417200] global_step=417200, grad_norm=5.029876708984375, loss=0.6453076601028442
I0301 21:53:33.987154 139590170035968 logging_writer.py:48] [417300] global_step=417300, grad_norm=4.988024711608887, loss=0.5775570869445801
I0301 21:54:07.931848 139590153250560 logging_writer.py:48] [417400] global_step=417400, grad_norm=4.3324360847473145, loss=0.5942142605781555
I0301 21:54:41.868907 139590170035968 logging_writer.py:48] [417500] global_step=417500, grad_norm=4.798089027404785, loss=0.6483806371688843
I0301 21:55:15.798604 139590153250560 logging_writer.py:48] [417600] global_step=417600, grad_norm=4.601357460021973, loss=0.6665639877319336
I0301 21:55:49.752848 139590170035968 logging_writer.py:48] [417700] global_step=417700, grad_norm=5.08656120300293, loss=0.6967304348945618
I0301 21:56:20.107110 139753105983296 spec.py:321] Evaluating on the training split.
I0301 21:56:26.127115 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 21:56:34.409339 139753105983296 spec.py:349] Evaluating on the test split.
I0301 21:56:36.692023 139753105983296 submission_runner.py:411] Time since start: 146727.70s, 	Step: 417791, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14592915773391724, 'validation/accuracy': 0.7548199892044067, 'validation/loss': 1.0547126531600952, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.818116307258606, 'test/num_examples': 10000, 'score': 141845.92551875114, 'total_duration': 146727.69525814056, 'accumulated_submission_time': 141845.92551875114, 'accumulated_eval_time': 4846.724861383438, 'accumulated_logging_time': 20.341834783554077}
I0301 21:56:36.783158 139589725452032 logging_writer.py:48] [417791] accumulated_eval_time=4846.724861, accumulated_logging_time=20.341835, accumulated_submission_time=141845.925519, global_step=417791, preemption_count=0, score=141845.925519, test/accuracy=0.630100, test/loss=1.818116, test/num_examples=10000, total_duration=146727.695258, train/accuracy=0.961456, train/loss=0.145929, validation/accuracy=0.754820, validation/loss=1.054713, validation/num_examples=50000
I0301 21:56:40.179842 139590144857856 logging_writer.py:48] [417800] global_step=417800, grad_norm=4.791294097900391, loss=0.5680727958679199
I0301 21:57:14.115031 139589725452032 logging_writer.py:48] [417900] global_step=417900, grad_norm=4.302074909210205, loss=0.6017645597457886
I0301 21:57:48.046646 139590144857856 logging_writer.py:48] [418000] global_step=418000, grad_norm=4.28958797454834, loss=0.6055653095245361
I0301 21:58:21.970843 139589725452032 logging_writer.py:48] [418100] global_step=418100, grad_norm=4.940049648284912, loss=0.67420893907547
I0301 21:58:55.947038 139590144857856 logging_writer.py:48] [418200] global_step=418200, grad_norm=4.497308731079102, loss=0.5637437701225281
I0301 21:59:30.008061 139589725452032 logging_writer.py:48] [418300] global_step=418300, grad_norm=4.797788143157959, loss=0.6850857734680176
I0301 22:00:03.967843 139590144857856 logging_writer.py:48] [418400] global_step=418400, grad_norm=4.7244720458984375, loss=0.5833501815795898
I0301 22:00:37.936352 139589725452032 logging_writer.py:48] [418500] global_step=418500, grad_norm=4.575892925262451, loss=0.6524370312690735
I0301 22:01:11.893605 139590144857856 logging_writer.py:48] [418600] global_step=418600, grad_norm=4.696251392364502, loss=0.5993037223815918
I0301 22:01:45.834079 139589725452032 logging_writer.py:48] [418700] global_step=418700, grad_norm=4.816346645355225, loss=0.6162849068641663
I0301 22:02:19.809386 139590144857856 logging_writer.py:48] [418800] global_step=418800, grad_norm=4.404024600982666, loss=0.632358193397522
I0301 22:02:53.721237 139589725452032 logging_writer.py:48] [418900] global_step=418900, grad_norm=5.09177303314209, loss=0.6368645429611206
I0301 22:03:27.656697 139590144857856 logging_writer.py:48] [419000] global_step=419000, grad_norm=4.312394142150879, loss=0.5777206420898438
I0301 22:04:01.637606 139589725452032 logging_writer.py:48] [419100] global_step=419100, grad_norm=4.917533874511719, loss=0.6542860865592957
I0301 22:04:35.595329 139590144857856 logging_writer.py:48] [419200] global_step=419200, grad_norm=5.352524280548096, loss=0.7001067996025085
I0301 22:05:06.956057 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:05:13.136920 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:05:21.275216 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:05:23.564633 139753105983296 submission_runner.py:411] Time since start: 147254.57s, 	Step: 419294, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.1447632610797882, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0562463998794556, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8181205987930298, 'test/num_examples': 10000, 'score': 142356.03491735458, 'total_duration': 147254.5678577423, 'accumulated_submission_time': 142356.03491735458, 'accumulated_eval_time': 4863.333392381668, 'accumulated_logging_time': 20.443277597427368}
I0301 22:05:23.656406 139589717059328 logging_writer.py:48] [419294] accumulated_eval_time=4863.333392, accumulated_logging_time=20.443278, accumulated_submission_time=142356.034917, global_step=419294, preemption_count=0, score=142356.034917, test/accuracy=0.630600, test/loss=1.818121, test/num_examples=10000, total_duration=147254.567858, train/accuracy=0.961057, train/loss=0.144763, validation/accuracy=0.754760, validation/loss=1.056246, validation/num_examples=50000
I0301 22:05:26.160428 139590161643264 logging_writer.py:48] [419300] global_step=419300, grad_norm=4.822105884552002, loss=0.6291188597679138
I0301 22:06:00.055718 139589717059328 logging_writer.py:48] [419400] global_step=419400, grad_norm=5.134276390075684, loss=0.6609163880348206
I0301 22:06:33.959107 139590161643264 logging_writer.py:48] [419500] global_step=419500, grad_norm=4.664518356323242, loss=0.6228076219558716
I0301 22:07:07.908725 139589717059328 logging_writer.py:48] [419600] global_step=419600, grad_norm=4.375907897949219, loss=0.6464999318122864
I0301 22:07:41.818511 139590161643264 logging_writer.py:48] [419700] global_step=419700, grad_norm=4.573884010314941, loss=0.6110643148422241
I0301 22:08:15.743414 139589717059328 logging_writer.py:48] [419800] global_step=419800, grad_norm=4.737335205078125, loss=0.6278257369995117
I0301 22:08:49.678951 139590161643264 logging_writer.py:48] [419900] global_step=419900, grad_norm=4.3250813484191895, loss=0.5926982164382935
I0301 22:09:23.611829 139589717059328 logging_writer.py:48] [420000] global_step=420000, grad_norm=4.705246448516846, loss=0.6118391156196594
I0301 22:09:57.509106 139590161643264 logging_writer.py:48] [420100] global_step=420100, grad_norm=4.629210948944092, loss=0.6255112886428833
I0301 22:10:31.448489 139589717059328 logging_writer.py:48] [420200] global_step=420200, grad_norm=4.372713565826416, loss=0.6916620135307312
I0301 22:11:05.395585 139590161643264 logging_writer.py:48] [420300] global_step=420300, grad_norm=4.84769868850708, loss=0.5218260288238525
I0301 22:11:39.408097 139589717059328 logging_writer.py:48] [420400] global_step=420400, grad_norm=4.617722511291504, loss=0.6383640170097351
I0301 22:12:13.373280 139590161643264 logging_writer.py:48] [420500] global_step=420500, grad_norm=5.033563137054443, loss=0.6609500646591187
I0301 22:12:47.336623 139589717059328 logging_writer.py:48] [420600] global_step=420600, grad_norm=4.560108661651611, loss=0.6210708618164062
I0301 22:13:21.275321 139590161643264 logging_writer.py:48] [420700] global_step=420700, grad_norm=4.633073806762695, loss=0.6041841506958008
I0301 22:13:53.658542 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:14:00.316948 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:14:08.640484 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:14:10.923248 139753105983296 submission_runner.py:411] Time since start: 147781.93s, 	Step: 420797, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14628393948078156, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.056235671043396, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8204907178878784, 'test/num_examples': 10000, 'score': 142865.97271060944, 'total_duration': 147781.92643356323, 'accumulated_submission_time': 142865.97271060944, 'accumulated_eval_time': 4880.598002910614, 'accumulated_logging_time': 20.545676708221436}
I0301 22:14:11.008549 139590178428672 logging_writer.py:48] [420797] accumulated_eval_time=4880.598003, accumulated_logging_time=20.545677, accumulated_submission_time=142865.972711, global_step=420797, preemption_count=0, score=142865.972711, test/accuracy=0.630400, test/loss=1.820491, test/num_examples=10000, total_duration=147781.926434, train/accuracy=0.960180, train/loss=0.146284, validation/accuracy=0.754900, validation/loss=1.056236, validation/num_examples=50000
I0301 22:14:12.378479 139590186821376 logging_writer.py:48] [420800] global_step=420800, grad_norm=4.5749897956848145, loss=0.6092191934585571
I0301 22:14:46.313976 139590178428672 logging_writer.py:48] [420900] global_step=420900, grad_norm=5.291666030883789, loss=0.6140314936637878
I0301 22:15:20.274459 139590186821376 logging_writer.py:48] [421000] global_step=421000, grad_norm=4.300329208374023, loss=0.6366811990737915
I0301 22:15:54.247977 139590178428672 logging_writer.py:48] [421100] global_step=421100, grad_norm=4.563142776489258, loss=0.6006735563278198
I0301 22:16:28.210340 139590186821376 logging_writer.py:48] [421200] global_step=421200, grad_norm=4.372463226318359, loss=0.6082197427749634
I0301 22:17:02.183717 139590178428672 logging_writer.py:48] [421300] global_step=421300, grad_norm=4.285223484039307, loss=0.5449825525283813
I0301 22:17:36.145495 139590186821376 logging_writer.py:48] [421400] global_step=421400, grad_norm=4.5392255783081055, loss=0.5966020226478577
I0301 22:18:10.191177 139590178428672 logging_writer.py:48] [421500] global_step=421500, grad_norm=4.612164497375488, loss=0.6270973682403564
I0301 22:18:44.165819 139590186821376 logging_writer.py:48] [421600] global_step=421600, grad_norm=4.485142707824707, loss=0.621631920337677
I0301 22:19:18.088950 139590178428672 logging_writer.py:48] [421700] global_step=421700, grad_norm=4.714012622833252, loss=0.6062276363372803
I0301 22:19:52.049256 139590186821376 logging_writer.py:48] [421800] global_step=421800, grad_norm=4.604984760284424, loss=0.6126742362976074
I0301 22:20:25.976008 139590178428672 logging_writer.py:48] [421900] global_step=421900, grad_norm=4.684322357177734, loss=0.667250394821167
I0301 22:20:59.938450 139590186821376 logging_writer.py:48] [422000] global_step=422000, grad_norm=4.510712146759033, loss=0.7157436013221741
I0301 22:21:33.909695 139590178428672 logging_writer.py:48] [422100] global_step=422100, grad_norm=4.861582279205322, loss=0.6607205867767334
I0301 22:22:07.866855 139590186821376 logging_writer.py:48] [422200] global_step=422200, grad_norm=5.023228645324707, loss=0.5577423572540283
I0301 22:22:40.937502 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:22:46.919931 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:22:55.241247 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:22:57.546361 139753105983296 submission_runner.py:411] Time since start: 148308.55s, 	Step: 422299, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.14662905037403107, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.054916501045227, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.817649483680725, 'test/num_examples': 10000, 'score': 143375.83768200874, 'total_duration': 148308.54959082603, 'accumulated_submission_time': 143375.83768200874, 'accumulated_eval_time': 4897.206827640533, 'accumulated_logging_time': 20.6411075592041}
I0301 22:22:57.637229 139590153250560 logging_writer.py:48] [422299] accumulated_eval_time=4897.206828, accumulated_logging_time=20.641108, accumulated_submission_time=143375.837682, global_step=422299, preemption_count=0, score=143375.837682, test/accuracy=0.630000, test/loss=1.817649, test/num_examples=10000, total_duration=148308.549591, train/accuracy=0.960918, train/loss=0.146629, validation/accuracy=0.754920, validation/loss=1.054917, validation/num_examples=50000
I0301 22:22:58.328077 139590161643264 logging_writer.py:48] [422300] global_step=422300, grad_norm=4.851452827453613, loss=0.6694138050079346
I0301 22:23:32.251091 139590153250560 logging_writer.py:48] [422400] global_step=422400, grad_norm=5.10667610168457, loss=0.7062039375305176
I0301 22:24:06.223663 139590161643264 logging_writer.py:48] [422500] global_step=422500, grad_norm=4.381242275238037, loss=0.6587539911270142
I0301 22:24:40.175030 139590153250560 logging_writer.py:48] [422600] global_step=422600, grad_norm=4.767599105834961, loss=0.6903798580169678
I0301 22:25:14.158269 139590161643264 logging_writer.py:48] [422700] global_step=422700, grad_norm=4.609969139099121, loss=0.6301774978637695
I0301 22:25:48.110050 139590153250560 logging_writer.py:48] [422800] global_step=422800, grad_norm=5.014753341674805, loss=0.6147267818450928
I0301 22:26:22.041082 139590161643264 logging_writer.py:48] [422900] global_step=422900, grad_norm=4.4004807472229, loss=0.6025856137275696
I0301 22:26:55.986641 139590153250560 logging_writer.py:48] [423000] global_step=423000, grad_norm=5.425434589385986, loss=0.6321499943733215
I0301 22:27:29.908648 139590161643264 logging_writer.py:48] [423100] global_step=423100, grad_norm=5.078042030334473, loss=0.6686834692955017
I0301 22:28:03.877143 139590153250560 logging_writer.py:48] [423200] global_step=423200, grad_norm=4.260482311248779, loss=0.5932316780090332
I0301 22:28:37.864165 139590161643264 logging_writer.py:48] [423300] global_step=423300, grad_norm=4.258662700653076, loss=0.5983129739761353
I0301 22:29:11.818250 139590153250560 logging_writer.py:48] [423400] global_step=423400, grad_norm=4.895008563995361, loss=0.6727644801139832
I0301 22:29:45.750353 139590161643264 logging_writer.py:48] [423500] global_step=423500, grad_norm=4.478713035583496, loss=0.7066735625267029
I0301 22:30:19.760261 139590153250560 logging_writer.py:48] [423600] global_step=423600, grad_norm=4.796432971954346, loss=0.6658342480659485
I0301 22:30:53.660562 139590161643264 logging_writer.py:48] [423700] global_step=423700, grad_norm=4.564424514770508, loss=0.6309090852737427
I0301 22:31:27.619645 139590153250560 logging_writer.py:48] [423800] global_step=423800, grad_norm=5.047667026519775, loss=0.674972414970398
I0301 22:31:27.626629 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:31:33.645648 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:31:41.880018 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:31:44.207250 139753105983296 submission_runner.py:411] Time since start: 148835.21s, 	Step: 423801, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.14624059200286865, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 1.0567693710327148, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8199653625488281, 'test/num_examples': 10000, 'score': 143885.76449489594, 'total_duration': 148835.2104690075, 'accumulated_submission_time': 143885.76449489594, 'accumulated_eval_time': 4913.787359714508, 'accumulated_logging_time': 20.741970777511597}
I0301 22:31:44.303510 139589717059328 logging_writer.py:48] [423801] accumulated_eval_time=4913.787360, accumulated_logging_time=20.741971, accumulated_submission_time=143885.764495, global_step=423801, preemption_count=0, score=143885.764495, test/accuracy=0.629900, test/loss=1.819965, test/num_examples=10000, total_duration=148835.210469, train/accuracy=0.960300, train/loss=0.146241, validation/accuracy=0.754460, validation/loss=1.056769, validation/num_examples=50000
I0301 22:32:18.278615 139589725452032 logging_writer.py:48] [423900] global_step=423900, grad_norm=4.160585880279541, loss=0.5682365894317627
I0301 22:32:52.211088 139589717059328 logging_writer.py:48] [424000] global_step=424000, grad_norm=4.327620983123779, loss=0.5934615731239319
I0301 22:33:26.140958 139589725452032 logging_writer.py:48] [424100] global_step=424100, grad_norm=4.534139156341553, loss=0.6257095336914062
I0301 22:34:00.114891 139589717059328 logging_writer.py:48] [424200] global_step=424200, grad_norm=4.5100274085998535, loss=0.6365102529525757
I0301 22:34:34.096238 139589725452032 logging_writer.py:48] [424300] global_step=424300, grad_norm=5.128875255584717, loss=0.6113232970237732
I0301 22:35:08.053272 139589717059328 logging_writer.py:48] [424400] global_step=424400, grad_norm=4.600407600402832, loss=0.5877425670623779
I0301 22:35:42.024477 139589725452032 logging_writer.py:48] [424500] global_step=424500, grad_norm=4.488547325134277, loss=0.5996220111846924
I0301 22:36:16.064641 139589717059328 logging_writer.py:48] [424600] global_step=424600, grad_norm=4.401983737945557, loss=0.6629649996757507
I0301 22:36:49.983404 139589725452032 logging_writer.py:48] [424700] global_step=424700, grad_norm=4.739172458648682, loss=0.6687939167022705
I0301 22:37:23.921666 139589717059328 logging_writer.py:48] [424800] global_step=424800, grad_norm=4.534512042999268, loss=0.6539660692214966
I0301 22:37:57.903377 139589725452032 logging_writer.py:48] [424900] global_step=424900, grad_norm=4.384973526000977, loss=0.6158824563026428
I0301 22:38:31.854529 139589717059328 logging_writer.py:48] [425000] global_step=425000, grad_norm=4.127937316894531, loss=0.506009042263031
I0301 22:39:05.837868 139589725452032 logging_writer.py:48] [425100] global_step=425100, grad_norm=4.191041946411133, loss=0.5853485465049744
I0301 22:39:39.791181 139589717059328 logging_writer.py:48] [425200] global_step=425200, grad_norm=4.081361293792725, loss=0.5824660062789917
I0301 22:40:13.765823 139589725452032 logging_writer.py:48] [425300] global_step=425300, grad_norm=4.243622779846191, loss=0.6112812757492065
I0301 22:40:14.240342 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:40:20.313628 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:40:28.613935 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:40:30.911689 139753105983296 submission_runner.py:411] Time since start: 149361.91s, 	Step: 425303, 	{'train/accuracy': 0.9620934128761292, 'train/loss': 0.14483579993247986, 'validation/accuracy': 0.7547000050544739, 'validation/loss': 1.054429292678833, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8174110651016235, 'test/num_examples': 10000, 'score': 144395.63716816902, 'total_duration': 149361.91491818428, 'accumulated_submission_time': 144395.63716816902, 'accumulated_eval_time': 4930.458664178848, 'accumulated_logging_time': 20.84931969642639}
I0301 22:40:31.011616 139590161643264 logging_writer.py:48] [425303] accumulated_eval_time=4930.458664, accumulated_logging_time=20.849320, accumulated_submission_time=144395.637168, global_step=425303, preemption_count=0, score=144395.637168, test/accuracy=0.631100, test/loss=1.817411, test/num_examples=10000, total_duration=149361.914918, train/accuracy=0.962093, train/loss=0.144836, validation/accuracy=0.754700, validation/loss=1.054429, validation/num_examples=50000
I0301 22:41:04.233149 139590170035968 logging_writer.py:48] [425400] global_step=425400, grad_norm=4.351776599884033, loss=0.5943647027015686
I0301 22:41:38.145409 139590161643264 logging_writer.py:48] [425500] global_step=425500, grad_norm=4.229060649871826, loss=0.5819373726844788
I0301 22:42:12.074012 139590170035968 logging_writer.py:48] [425600] global_step=425600, grad_norm=4.584677696228027, loss=0.6254620552062988
I0301 22:42:46.073325 139590161643264 logging_writer.py:48] [425700] global_step=425700, grad_norm=5.133809566497803, loss=0.6191918849945068
I0301 22:43:20.012078 139590170035968 logging_writer.py:48] [425800] global_step=425800, grad_norm=4.1939287185668945, loss=0.5787472128868103
I0301 22:43:53.951671 139590161643264 logging_writer.py:48] [425900] global_step=425900, grad_norm=4.824487209320068, loss=0.5897023677825928
I0301 22:44:27.887491 139590170035968 logging_writer.py:48] [426000] global_step=426000, grad_norm=4.494778633117676, loss=0.6552876234054565
I0301 22:45:01.827769 139590161643264 logging_writer.py:48] [426100] global_step=426100, grad_norm=4.598774433135986, loss=0.5716115236282349
I0301 22:45:35.765933 139590170035968 logging_writer.py:48] [426200] global_step=426200, grad_norm=4.496585845947266, loss=0.6474311351776123
I0301 22:46:09.695844 139590161643264 logging_writer.py:48] [426300] global_step=426300, grad_norm=4.880253791809082, loss=0.724437415599823
I0301 22:46:43.614785 139590170035968 logging_writer.py:48] [426400] global_step=426400, grad_norm=4.1255998611450195, loss=0.6162796020507812
I0301 22:47:17.554270 139590161643264 logging_writer.py:48] [426500] global_step=426500, grad_norm=4.658007621765137, loss=0.6454841494560242
I0301 22:47:51.495207 139590170035968 logging_writer.py:48] [426600] global_step=426600, grad_norm=4.963362693786621, loss=0.6508391499519348
I0301 22:48:25.435642 139590161643264 logging_writer.py:48] [426700] global_step=426700, grad_norm=4.799275875091553, loss=0.6346073150634766
I0301 22:48:59.447668 139590170035968 logging_writer.py:48] [426800] global_step=426800, grad_norm=4.791227340698242, loss=0.6019836664199829
I0301 22:49:00.951973 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:49:07.032578 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:49:15.317577 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:49:17.567583 139753105983296 submission_runner.py:411] Time since start: 149888.57s, 	Step: 426806, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14606167376041412, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0565731525421143, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8200547695159912, 'test/num_examples': 10000, 'score': 144905.5118675232, 'total_duration': 149888.57080936432, 'accumulated_submission_time': 144905.5118675232, 'accumulated_eval_time': 4947.074217557907, 'accumulated_logging_time': 20.96125626564026}
I0301 22:49:17.658111 139590153250560 logging_writer.py:48] [426806] accumulated_eval_time=4947.074218, accumulated_logging_time=20.961256, accumulated_submission_time=144905.511868, global_step=426806, preemption_count=0, score=144905.511868, test/accuracy=0.629600, test/loss=1.820055, test/num_examples=10000, total_duration=149888.570809, train/accuracy=0.961057, train/loss=0.146062, validation/accuracy=0.754960, validation/loss=1.056573, validation/num_examples=50000
I0301 22:49:49.873283 139590178428672 logging_writer.py:48] [426900] global_step=426900, grad_norm=4.5949578285217285, loss=0.5354052782058716
I0301 22:50:23.797805 139590153250560 logging_writer.py:48] [427000] global_step=427000, grad_norm=4.393679141998291, loss=0.60511714220047
I0301 22:50:57.716824 139590178428672 logging_writer.py:48] [427100] global_step=427100, grad_norm=4.727766990661621, loss=0.6550671458244324
I0301 22:51:31.933058 139590153250560 logging_writer.py:48] [427200] global_step=427200, grad_norm=4.478632926940918, loss=0.5648586750030518
I0301 22:52:05.837048 139590178428672 logging_writer.py:48] [427300] global_step=427300, grad_norm=4.290370464324951, loss=0.5378311276435852
I0301 22:52:39.794379 139590153250560 logging_writer.py:48] [427400] global_step=427400, grad_norm=4.5212297439575195, loss=0.6083918809890747
I0301 22:53:13.745093 139590178428672 logging_writer.py:48] [427500] global_step=427500, grad_norm=4.610950946807861, loss=0.6030593514442444
I0301 22:53:47.698820 139590153250560 logging_writer.py:48] [427600] global_step=427600, grad_norm=4.329817771911621, loss=0.566487193107605
I0301 22:54:21.642256 139590178428672 logging_writer.py:48] [427700] global_step=427700, grad_norm=4.408999443054199, loss=0.5974711775779724
I0301 22:54:55.661351 139590153250560 logging_writer.py:48] [427800] global_step=427800, grad_norm=4.463711738586426, loss=0.610590398311615
I0301 22:55:29.620039 139590178428672 logging_writer.py:48] [427900] global_step=427900, grad_norm=4.533087730407715, loss=0.5974959135055542
I0301 22:56:03.576144 139590153250560 logging_writer.py:48] [428000] global_step=428000, grad_norm=4.787342071533203, loss=0.6391379237174988
I0301 22:56:37.504736 139590178428672 logging_writer.py:48] [428100] global_step=428100, grad_norm=4.893131732940674, loss=0.6133363246917725
I0301 22:57:11.442332 139590153250560 logging_writer.py:48] [428200] global_step=428200, grad_norm=4.784419536590576, loss=0.6217510104179382
I0301 22:57:45.398562 139590178428672 logging_writer.py:48] [428300] global_step=428300, grad_norm=5.154541492462158, loss=0.672171950340271
I0301 22:57:47.584809 139753105983296 spec.py:321] Evaluating on the training split.
I0301 22:57:53.631582 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 22:58:01.982514 139753105983296 spec.py:349] Evaluating on the test split.
I0301 22:58:04.185168 139753105983296 submission_runner.py:411] Time since start: 150415.19s, 	Step: 428308, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.1456134170293808, 'validation/accuracy': 0.7544999718666077, 'validation/loss': 1.0552159547805786, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8188358545303345, 'test/num_examples': 10000, 'score': 145415.37442803383, 'total_duration': 150415.18838214874, 'accumulated_submission_time': 145415.37442803383, 'accumulated_eval_time': 4963.67450594902, 'accumulated_logging_time': 21.061894416809082}
I0301 22:58:04.275190 139589717059328 logging_writer.py:48] [428308] accumulated_eval_time=4963.674506, accumulated_logging_time=21.061894, accumulated_submission_time=145415.374428, global_step=428308, preemption_count=0, score=145415.374428, test/accuracy=0.629800, test/loss=1.818836, test/num_examples=10000, total_duration=150415.188382, train/accuracy=0.961057, train/loss=0.145613, validation/accuracy=0.754500, validation/loss=1.055216, validation/num_examples=50000
I0301 22:58:35.797676 139589725452032 logging_writer.py:48] [428400] global_step=428400, grad_norm=4.507128715515137, loss=0.5957286953926086
I0301 22:59:09.735393 139589717059328 logging_writer.py:48] [428500] global_step=428500, grad_norm=4.68731164932251, loss=0.6617112755775452
I0301 22:59:43.706993 139589725452032 logging_writer.py:48] [428600] global_step=428600, grad_norm=4.707421779632568, loss=0.6452279090881348
I0301 23:00:17.636078 139589717059328 logging_writer.py:48] [428700] global_step=428700, grad_norm=4.4829630851745605, loss=0.585475742816925
I0301 23:00:51.570112 139589725452032 logging_writer.py:48] [428800] global_step=428800, grad_norm=4.795496463775635, loss=0.6836004853248596
I0301 23:01:25.591786 139589717059328 logging_writer.py:48] [428900] global_step=428900, grad_norm=4.723217964172363, loss=0.6401405334472656
I0301 23:01:59.531899 139589725452032 logging_writer.py:48] [429000] global_step=429000, grad_norm=4.522614479064941, loss=0.6155422925949097
I0301 23:02:33.519848 139589717059328 logging_writer.py:48] [429100] global_step=429100, grad_norm=4.702437400817871, loss=0.5700149536132812
I0301 23:03:07.470831 139589725452032 logging_writer.py:48] [429200] global_step=429200, grad_norm=4.460949897766113, loss=0.620266318321228
I0301 23:03:41.423539 139589717059328 logging_writer.py:48] [429300] global_step=429300, grad_norm=5.073362827301025, loss=0.6460462808609009
I0301 23:04:15.380749 139589725452032 logging_writer.py:48] [429400] global_step=429400, grad_norm=4.526002883911133, loss=0.5364037752151489
I0301 23:04:49.268641 139589717059328 logging_writer.py:48] [429500] global_step=429500, grad_norm=4.625528335571289, loss=0.655220091342926
I0301 23:05:23.215302 139589725452032 logging_writer.py:48] [429600] global_step=429600, grad_norm=3.9289073944091797, loss=0.5589567422866821
I0301 23:05:57.122776 139589717059328 logging_writer.py:48] [429700] global_step=429700, grad_norm=4.607248306274414, loss=0.6875927448272705
I0301 23:06:31.074537 139589725452032 logging_writer.py:48] [429800] global_step=429800, grad_norm=4.329460620880127, loss=0.6285905838012695
I0301 23:06:34.284341 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:06:40.254293 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:06:48.549605 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:06:50.802752 139753105983296 submission_runner.py:411] Time since start: 150941.81s, 	Step: 429811, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.14603422582149506, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.054571509361267, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8180526494979858, 'test/num_examples': 10000, 'score': 145925.31875014305, 'total_duration': 150941.80598330498, 'accumulated_submission_time': 145925.31875014305, 'accumulated_eval_time': 4980.192862272263, 'accumulated_logging_time': 21.162484407424927}
I0301 23:06:50.895577 139590153250560 logging_writer.py:48] [429811] accumulated_eval_time=4980.192862, accumulated_logging_time=21.162484, accumulated_submission_time=145925.318750, global_step=429811, preemption_count=0, score=145925.318750, test/accuracy=0.630100, test/loss=1.818053, test/num_examples=10000, total_duration=150941.805983, train/accuracy=0.961057, train/loss=0.146034, validation/accuracy=0.754760, validation/loss=1.054572, validation/num_examples=50000
I0301 23:07:21.503674 139590161643264 logging_writer.py:48] [429900] global_step=429900, grad_norm=4.726432800292969, loss=0.67052161693573
I0301 23:07:55.441497 139590153250560 logging_writer.py:48] [430000] global_step=430000, grad_norm=4.669063568115234, loss=0.6849786043167114
I0301 23:08:29.399803 139590161643264 logging_writer.py:48] [430100] global_step=430100, grad_norm=4.528314590454102, loss=0.6281174421310425
I0301 23:09:03.345757 139590153250560 logging_writer.py:48] [430200] global_step=430200, grad_norm=4.185776710510254, loss=0.5641352534294128
I0301 23:09:37.314605 139590161643264 logging_writer.py:48] [430300] global_step=430300, grad_norm=4.341466903686523, loss=0.5558766722679138
I0301 23:10:11.246354 139590153250560 logging_writer.py:48] [430400] global_step=430400, grad_norm=4.520668029785156, loss=0.6913184523582458
I0301 23:10:45.192490 139590161643264 logging_writer.py:48] [430500] global_step=430500, grad_norm=5.246302127838135, loss=0.6863328814506531
I0301 23:11:19.144584 139590153250560 logging_writer.py:48] [430600] global_step=430600, grad_norm=5.360830783843994, loss=0.6672297120094299
I0301 23:11:53.118252 139590161643264 logging_writer.py:48] [430700] global_step=430700, grad_norm=4.141262531280518, loss=0.6277316212654114
I0301 23:12:27.100167 139590153250560 logging_writer.py:48] [430800] global_step=430800, grad_norm=4.837697982788086, loss=0.6754395961761475
I0301 23:13:01.078925 139590161643264 logging_writer.py:48] [430900] global_step=430900, grad_norm=4.465784072875977, loss=0.6286476850509644
I0301 23:13:35.073956 139590153250560 logging_writer.py:48] [431000] global_step=431000, grad_norm=4.138949871063232, loss=0.5635068416595459
I0301 23:14:09.023471 139590161643264 logging_writer.py:48] [431100] global_step=431100, grad_norm=4.802348613739014, loss=0.6863440275192261
I0301 23:14:43.003406 139590153250560 logging_writer.py:48] [431200] global_step=431200, grad_norm=4.684779644012451, loss=0.5700715780258179
I0301 23:15:16.961668 139590161643264 logging_writer.py:48] [431300] global_step=431300, grad_norm=4.459244251251221, loss=0.5934725999832153
I0301 23:15:20.825375 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:15:26.900293 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:15:35.083895 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:15:37.398110 139753105983296 submission_runner.py:411] Time since start: 151468.40s, 	Step: 431313, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.1460363268852234, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0550318956375122, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8184103965759277, 'test/num_examples': 10000, 'score': 146435.1837067604, 'total_duration': 151468.40133213997, 'accumulated_submission_time': 146435.1837067604, 'accumulated_eval_time': 4996.765533208847, 'accumulated_logging_time': 21.267829179763794}
I0301 23:15:37.502347 139589708666624 logging_writer.py:48] [431313] accumulated_eval_time=4996.765533, accumulated_logging_time=21.267829, accumulated_submission_time=146435.183707, global_step=431313, preemption_count=0, score=146435.183707, test/accuracy=0.630600, test/loss=1.818410, test/num_examples=10000, total_duration=151468.401332, train/accuracy=0.960260, train/loss=0.146036, validation/accuracy=0.754680, validation/loss=1.055032, validation/num_examples=50000
I0301 23:16:07.361559 139589717059328 logging_writer.py:48] [431400] global_step=431400, grad_norm=4.5053534507751465, loss=0.6341933608055115
I0301 23:16:41.265543 139589708666624 logging_writer.py:48] [431500] global_step=431500, grad_norm=4.099432945251465, loss=0.5943338871002197
I0301 23:17:15.221284 139589717059328 logging_writer.py:48] [431600] global_step=431600, grad_norm=4.338674068450928, loss=0.6176875829696655
I0301 23:17:49.189572 139589708666624 logging_writer.py:48] [431700] global_step=431700, grad_norm=4.475767612457275, loss=0.608070969581604
I0301 23:18:23.162957 139589717059328 logging_writer.py:48] [431800] global_step=431800, grad_norm=4.301230430603027, loss=0.5845873355865479
I0301 23:18:57.101641 139589708666624 logging_writer.py:48] [431900] global_step=431900, grad_norm=4.576783180236816, loss=0.6551298499107361
I0301 23:19:31.140899 139589717059328 logging_writer.py:48] [432000] global_step=432000, grad_norm=4.875570297241211, loss=0.6768483519554138
I0301 23:20:05.084922 139589708666624 logging_writer.py:48] [432100] global_step=432100, grad_norm=4.524021148681641, loss=0.6058484315872192
I0301 23:20:39.033316 139589717059328 logging_writer.py:48] [432200] global_step=432200, grad_norm=4.448991775512695, loss=0.5466421842575073
I0301 23:21:12.963964 139589708666624 logging_writer.py:48] [432300] global_step=432300, grad_norm=4.470451831817627, loss=0.6188269853591919
I0301 23:21:46.907802 139589717059328 logging_writer.py:48] [432400] global_step=432400, grad_norm=4.709572792053223, loss=0.6969540119171143
I0301 23:22:20.854943 139589708666624 logging_writer.py:48] [432500] global_step=432500, grad_norm=4.094546318054199, loss=0.556015133857727
I0301 23:22:54.814527 139589717059328 logging_writer.py:48] [432600] global_step=432600, grad_norm=5.034280300140381, loss=0.5969806909561157
I0301 23:23:28.793684 139589708666624 logging_writer.py:48] [432700] global_step=432700, grad_norm=4.448649883270264, loss=0.6221062541007996
I0301 23:24:02.741148 139589717059328 logging_writer.py:48] [432800] global_step=432800, grad_norm=4.962776184082031, loss=0.6563206911087036
I0301 23:24:07.634297 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:24:13.663438 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:24:21.895426 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:24:24.169063 139753105983296 submission_runner.py:411] Time since start: 151995.17s, 	Step: 432816, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.14579735696315765, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0549418926239014, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8184632062911987, 'test/num_examples': 10000, 'score': 146945.2528398037, 'total_duration': 151995.17208194733, 'accumulated_submission_time': 146945.2528398037, 'accumulated_eval_time': 5013.30003285408, 'accumulated_logging_time': 21.38243818283081}
I0301 23:24:24.284964 139589717059328 logging_writer.py:48] [432816] accumulated_eval_time=5013.300033, accumulated_logging_time=21.382438, accumulated_submission_time=146945.252840, global_step=432816, preemption_count=0, score=146945.252840, test/accuracy=0.629600, test/loss=1.818463, test/num_examples=10000, total_duration=151995.172082, train/accuracy=0.961177, train/loss=0.145797, validation/accuracy=0.754660, validation/loss=1.054942, validation/num_examples=50000
I0301 23:24:53.117855 139589725452032 logging_writer.py:48] [432900] global_step=432900, grad_norm=4.969817638397217, loss=0.6764606237411499
I0301 23:25:27.048260 139589717059328 logging_writer.py:48] [433000] global_step=433000, grad_norm=4.650033473968506, loss=0.6036052107810974
I0301 23:26:01.065999 139589725452032 logging_writer.py:48] [433100] global_step=433100, grad_norm=4.605005264282227, loss=0.6566351652145386
I0301 23:26:35.025607 139589717059328 logging_writer.py:48] [433200] global_step=433200, grad_norm=4.35205078125, loss=0.6151857376098633
I0301 23:27:08.982604 139589725452032 logging_writer.py:48] [433300] global_step=433300, grad_norm=4.454176425933838, loss=0.6570418477058411
I0301 23:27:42.956826 139589717059328 logging_writer.py:48] [433400] global_step=433400, grad_norm=4.727546691894531, loss=0.6587057709693909
I0301 23:28:16.915351 139589725452032 logging_writer.py:48] [433500] global_step=433500, grad_norm=4.432338714599609, loss=0.5953670740127563
I0301 23:28:50.867865 139589717059328 logging_writer.py:48] [433600] global_step=433600, grad_norm=4.822308540344238, loss=0.6500109434127808
I0301 23:29:24.841272 139589725452032 logging_writer.py:48] [433700] global_step=433700, grad_norm=4.991104602813721, loss=0.5932148098945618
I0301 23:29:58.797769 139589717059328 logging_writer.py:48] [433800] global_step=433800, grad_norm=4.981650352478027, loss=0.6878871321678162
I0301 23:30:32.777850 139589725452032 logging_writer.py:48] [433900] global_step=433900, grad_norm=4.776163101196289, loss=0.7011697292327881
I0301 23:31:06.734270 139589717059328 logging_writer.py:48] [434000] global_step=434000, grad_norm=4.486212253570557, loss=0.6557306051254272
I0301 23:31:40.717103 139589725452032 logging_writer.py:48] [434100] global_step=434100, grad_norm=4.492610931396484, loss=0.5410333275794983
I0301 23:32:14.848876 139589717059328 logging_writer.py:48] [434200] global_step=434200, grad_norm=4.766206741333008, loss=0.6428843140602112
I0301 23:32:48.814400 139589725452032 logging_writer.py:48] [434300] global_step=434300, grad_norm=4.731169700622559, loss=0.6590390801429749
I0301 23:32:54.388801 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:33:00.406577 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:33:08.719616 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:33:10.937670 139753105983296 submission_runner.py:411] Time since start: 152521.94s, 	Step: 434318, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14679791033267975, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.0560564994812012, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8182473182678223, 'test/num_examples': 10000, 'score': 147455.2933781147, 'total_duration': 152521.94089126587, 'accumulated_submission_time': 147455.2933781147, 'accumulated_eval_time': 5029.848837137222, 'accumulated_logging_time': 21.50903558731079}
I0301 23:33:11.027134 139590161643264 logging_writer.py:48] [434318] accumulated_eval_time=5029.848837, accumulated_logging_time=21.509036, accumulated_submission_time=147455.293378, global_step=434318, preemption_count=0, score=147455.293378, test/accuracy=0.629900, test/loss=1.818247, test/num_examples=10000, total_duration=152521.940891, train/accuracy=0.961017, train/loss=0.146798, validation/accuracy=0.755000, validation/loss=1.056056, validation/num_examples=50000
I0301 23:33:39.189110 139590178428672 logging_writer.py:48] [434400] global_step=434400, grad_norm=4.2325358390808105, loss=0.6087271571159363
I0301 23:34:13.076298 139590161643264 logging_writer.py:48] [434500] global_step=434500, grad_norm=4.656027793884277, loss=0.5860138535499573
I0301 23:34:46.986218 139590178428672 logging_writer.py:48] [434600] global_step=434600, grad_norm=4.384393215179443, loss=0.6816869974136353
I0301 23:35:20.912106 139590161643264 logging_writer.py:48] [434700] global_step=434700, grad_norm=4.535080432891846, loss=0.5704840421676636
I0301 23:35:54.858420 139590178428672 logging_writer.py:48] [434800] global_step=434800, grad_norm=4.544582366943359, loss=0.6561218500137329
I0301 23:36:28.805707 139590161643264 logging_writer.py:48] [434900] global_step=434900, grad_norm=5.6895575523376465, loss=0.6948480606079102
I0301 23:37:02.749382 139590178428672 logging_writer.py:48] [435000] global_step=435000, grad_norm=4.702703475952148, loss=0.6368825435638428
I0301 23:37:36.688838 139590161643264 logging_writer.py:48] [435100] global_step=435100, grad_norm=4.25160551071167, loss=0.5544906258583069
I0301 23:38:10.715437 139590178428672 logging_writer.py:48] [435200] global_step=435200, grad_norm=5.328227996826172, loss=0.6563500165939331
I0301 23:38:44.653179 139590161643264 logging_writer.py:48] [435300] global_step=435300, grad_norm=4.597473621368408, loss=0.6536657810211182
I0301 23:39:18.575049 139590178428672 logging_writer.py:48] [435400] global_step=435400, grad_norm=4.192617893218994, loss=0.6343148946762085
I0301 23:39:52.530998 139590161643264 logging_writer.py:48] [435500] global_step=435500, grad_norm=3.811396360397339, loss=0.5597193241119385
I0301 23:40:26.500173 139590178428672 logging_writer.py:48] [435600] global_step=435600, grad_norm=4.909618854522705, loss=0.6669520735740662
I0301 23:41:00.449286 139590161643264 logging_writer.py:48] [435700] global_step=435700, grad_norm=4.942248344421387, loss=0.5584808588027954
I0301 23:41:34.381538 139590178428672 logging_writer.py:48] [435800] global_step=435800, grad_norm=5.284457683563232, loss=0.730450451374054
I0301 23:41:40.978051 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:41:46.990760 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:41:55.203957 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:41:57.528343 139753105983296 submission_runner.py:411] Time since start: 153048.53s, 	Step: 435821, 	{'train/accuracy': 0.9590640664100647, 'train/loss': 0.15130969882011414, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0555171966552734, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8191215991973877, 'test/num_examples': 10000, 'score': 147965.1800661087, 'total_duration': 153048.53157043457, 'accumulated_submission_time': 147965.1800661087, 'accumulated_eval_time': 5046.399070739746, 'accumulated_logging_time': 21.609692573547363}
I0301 23:41:57.621754 139589725452032 logging_writer.py:48] [435821] accumulated_eval_time=5046.399071, accumulated_logging_time=21.609693, accumulated_submission_time=147965.180066, global_step=435821, preemption_count=0, score=147965.180066, test/accuracy=0.630500, test/loss=1.819122, test/num_examples=10000, total_duration=153048.531570, train/accuracy=0.959064, train/loss=0.151310, validation/accuracy=0.754980, validation/loss=1.055517, validation/num_examples=50000
I0301 23:42:24.768756 139590144857856 logging_writer.py:48] [435900] global_step=435900, grad_norm=4.376330852508545, loss=0.618423342704773
I0301 23:42:58.682691 139589725452032 logging_writer.py:48] [436000] global_step=436000, grad_norm=4.37907075881958, loss=0.6670122146606445
I0301 23:43:32.632238 139590144857856 logging_writer.py:48] [436100] global_step=436100, grad_norm=4.856941223144531, loss=0.6534957885742188
I0301 23:44:06.589931 139589725452032 logging_writer.py:48] [436200] global_step=436200, grad_norm=4.713265895843506, loss=0.6653479337692261
I0301 23:44:40.600608 139590144857856 logging_writer.py:48] [436300] global_step=436300, grad_norm=4.617205619812012, loss=0.6465684175491333
I0301 23:45:14.566004 139589725452032 logging_writer.py:48] [436400] global_step=436400, grad_norm=4.589522838592529, loss=0.6359788179397583
I0301 23:45:48.507558 139590144857856 logging_writer.py:48] [436500] global_step=436500, grad_norm=4.563660144805908, loss=0.6324155926704407
I0301 23:46:22.437089 139589725452032 logging_writer.py:48] [436600] global_step=436600, grad_norm=4.945996284484863, loss=0.6830524206161499
I0301 23:46:56.368995 139590144857856 logging_writer.py:48] [436700] global_step=436700, grad_norm=4.285344123840332, loss=0.609501838684082
I0301 23:47:30.318461 139589725452032 logging_writer.py:48] [436800] global_step=436800, grad_norm=4.485426425933838, loss=0.5689182281494141
I0301 23:48:04.282561 139590144857856 logging_writer.py:48] [436900] global_step=436900, grad_norm=4.419229984283447, loss=0.6261205673217773
I0301 23:48:38.224268 139589725452032 logging_writer.py:48] [437000] global_step=437000, grad_norm=4.833223819732666, loss=0.6547520756721497
I0301 23:49:12.176376 139590144857856 logging_writer.py:48] [437100] global_step=437100, grad_norm=4.387971878051758, loss=0.6083616614341736
I0301 23:49:46.115575 139589725452032 logging_writer.py:48] [437200] global_step=437200, grad_norm=4.4076738357543945, loss=0.5950968861579895
I0301 23:50:20.145569 139590144857856 logging_writer.py:48] [437300] global_step=437300, grad_norm=5.094561576843262, loss=0.7274592518806458
I0301 23:50:27.783183 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:50:33.808566 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:50:42.035536 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:50:44.327711 139753105983296 submission_runner.py:411] Time since start: 153575.33s, 	Step: 437324, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14540372788906097, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0546343326568604, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.817741870880127, 'test/num_examples': 10000, 'score': 148475.27742362022, 'total_duration': 153575.33093428612, 'accumulated_submission_time': 148475.27742362022, 'accumulated_eval_time': 5062.943537712097, 'accumulated_logging_time': 21.714293003082275}
I0301 23:50:44.423625 139590161643264 logging_writer.py:48] [437324] accumulated_eval_time=5062.943538, accumulated_logging_time=21.714293, accumulated_submission_time=148475.277424, global_step=437324, preemption_count=0, score=148475.277424, test/accuracy=0.630300, test/loss=1.817742, test/num_examples=10000, total_duration=153575.330934, train/accuracy=0.960798, train/loss=0.145404, validation/accuracy=0.754780, validation/loss=1.054634, validation/num_examples=50000
I0301 23:51:10.523312 139590170035968 logging_writer.py:48] [437400] global_step=437400, grad_norm=4.249218940734863, loss=0.6044760942459106
I0301 23:51:44.443491 139590161643264 logging_writer.py:48] [437500] global_step=437500, grad_norm=4.318390846252441, loss=0.6001615524291992
I0301 23:52:18.392050 139590170035968 logging_writer.py:48] [437600] global_step=437600, grad_norm=4.587712287902832, loss=0.609997570514679
I0301 23:52:52.337227 139590161643264 logging_writer.py:48] [437700] global_step=437700, grad_norm=4.7493815422058105, loss=0.615740180015564
I0301 23:53:26.286202 139590170035968 logging_writer.py:48] [437800] global_step=437800, grad_norm=4.654356479644775, loss=0.6502621173858643
I0301 23:54:00.238105 139590161643264 logging_writer.py:48] [437900] global_step=437900, grad_norm=4.427391529083252, loss=0.6315404176712036
I0301 23:54:34.180428 139590170035968 logging_writer.py:48] [438000] global_step=438000, grad_norm=4.698531150817871, loss=0.6378716230392456
I0301 23:55:08.109082 139590161643264 logging_writer.py:48] [438100] global_step=438100, grad_norm=4.441324710845947, loss=0.6522169709205627
I0301 23:55:42.063773 139590170035968 logging_writer.py:48] [438200] global_step=438200, grad_norm=4.989773750305176, loss=0.6521729230880737
I0301 23:56:16.025206 139590161643264 logging_writer.py:48] [438300] global_step=438300, grad_norm=4.637719631195068, loss=0.6627835631370544
I0301 23:56:50.023516 139590170035968 logging_writer.py:48] [438400] global_step=438400, grad_norm=4.856115341186523, loss=0.6061611771583557
I0301 23:57:23.993979 139590161643264 logging_writer.py:48] [438500] global_step=438500, grad_norm=4.735694885253906, loss=0.5907420516014099
I0301 23:57:57.942684 139590170035968 logging_writer.py:48] [438600] global_step=438600, grad_norm=4.787603378295898, loss=0.5904734134674072
I0301 23:58:31.869576 139590161643264 logging_writer.py:48] [438700] global_step=438700, grad_norm=5.1820597648620605, loss=0.6631856560707092
I0301 23:59:05.816406 139590170035968 logging_writer.py:48] [438800] global_step=438800, grad_norm=5.0197062492370605, loss=0.6786225438117981
I0301 23:59:14.454216 139753105983296 spec.py:321] Evaluating on the training split.
I0301 23:59:20.502377 139753105983296 spec.py:333] Evaluating on the validation split.
I0301 23:59:28.730342 139753105983296 spec.py:349] Evaluating on the test split.
I0301 23:59:31.046637 139753105983296 submission_runner.py:411] Time since start: 154102.05s, 	Step: 438827, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.1466187983751297, 'validation/accuracy': 0.7544199824333191, 'validation/loss': 1.0557987689971924, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.8190243244171143, 'test/num_examples': 10000, 'score': 148985.24048876762, 'total_duration': 154102.04985761642, 'accumulated_submission_time': 148985.24048876762, 'accumulated_eval_time': 5079.535908937454, 'accumulated_logging_time': 21.82430601119995}
I0301 23:59:31.138364 139589708666624 logging_writer.py:48] [438827] accumulated_eval_time=5079.535909, accumulated_logging_time=21.824306, accumulated_submission_time=148985.240489, global_step=438827, preemption_count=0, score=148985.240489, test/accuracy=0.629300, test/loss=1.819024, test/num_examples=10000, total_duration=154102.049858, train/accuracy=0.960300, train/loss=0.146619, validation/accuracy=0.754420, validation/loss=1.055799, validation/num_examples=50000
I0301 23:59:56.248905 139589717059328 logging_writer.py:48] [438900] global_step=438900, grad_norm=4.827385902404785, loss=0.6649095416069031
I0302 00:00:30.206228 139589708666624 logging_writer.py:48] [439000] global_step=439000, grad_norm=4.402606964111328, loss=0.6069844365119934
I0302 00:01:04.141573 139589717059328 logging_writer.py:48] [439100] global_step=439100, grad_norm=4.621312141418457, loss=0.630336344242096
I0302 00:01:38.086004 139589708666624 logging_writer.py:48] [439200] global_step=439200, grad_norm=4.492908000946045, loss=0.6100112795829773
I0302 00:02:12.029930 139589717059328 logging_writer.py:48] [439300] global_step=439300, grad_norm=5.144295692443848, loss=0.6563407778739929
I0302 00:02:45.965679 139589708666624 logging_writer.py:48] [439400] global_step=439400, grad_norm=4.781479835510254, loss=0.6695472002029419
I0302 00:03:19.983719 139589717059328 logging_writer.py:48] [439500] global_step=439500, grad_norm=4.658749580383301, loss=0.6768590211868286
I0302 00:03:53.919836 139589708666624 logging_writer.py:48] [439600] global_step=439600, grad_norm=4.570790767669678, loss=0.5274761319160461
I0302 00:04:27.862487 139589717059328 logging_writer.py:48] [439700] global_step=439700, grad_norm=5.030036449432373, loss=0.7109072804450989
I0302 00:05:01.786597 139589708666624 logging_writer.py:48] [439800] global_step=439800, grad_norm=4.679322242736816, loss=0.6140069961547852
I0302 00:05:35.746722 139589717059328 logging_writer.py:48] [439900] global_step=439900, grad_norm=4.734030723571777, loss=0.6448274254798889
I0302 00:06:09.709359 139589708666624 logging_writer.py:48] [440000] global_step=440000, grad_norm=4.515605449676514, loss=0.5968499779701233
I0302 00:06:43.677332 139589717059328 logging_writer.py:48] [440100] global_step=440100, grad_norm=4.429743766784668, loss=0.657227635383606
I0302 00:07:17.621402 139589708666624 logging_writer.py:48] [440200] global_step=440200, grad_norm=4.234334945678711, loss=0.5647298693656921
I0302 00:07:51.576403 139589717059328 logging_writer.py:48] [440300] global_step=440300, grad_norm=4.55200719833374, loss=0.7441312670707703
I0302 00:08:01.242855 139753105983296 spec.py:321] Evaluating on the training split.
I0302 00:08:07.317939 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 00:08:15.486641 139753105983296 spec.py:349] Evaluating on the test split.
I0302 00:08:17.885464 139753105983296 submission_runner.py:411] Time since start: 154628.89s, 	Step: 440330, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14733676612377167, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.0556747913360596, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8187315464019775, 'test/num_examples': 10000, 'score': 149495.2818260193, 'total_duration': 154628.88868403435, 'accumulated_submission_time': 149495.2818260193, 'accumulated_eval_time': 5096.1784517765045, 'accumulated_logging_time': 21.92604899406433}
I0302 00:08:17.971207 139590178428672 logging_writer.py:48] [440330] accumulated_eval_time=5096.178452, accumulated_logging_time=21.926049, accumulated_submission_time=149495.281826, global_step=440330, preemption_count=0, score=149495.281826, test/accuracy=0.629800, test/loss=1.818732, test/num_examples=10000, total_duration=154628.888684, train/accuracy=0.960678, train/loss=0.147337, validation/accuracy=0.754620, validation/loss=1.055675, validation/num_examples=50000
I0302 00:08:42.067297 139590186821376 logging_writer.py:48] [440400] global_step=440400, grad_norm=5.625494956970215, loss=0.6481719017028809
I0302 00:09:16.040523 139590178428672 logging_writer.py:48] [440500] global_step=440500, grad_norm=4.942294120788574, loss=0.632246732711792
I0302 00:09:49.959511 139590186821376 logging_writer.py:48] [440600] global_step=440600, grad_norm=4.490052700042725, loss=0.624701738357544
I0302 00:10:23.893795 139590178428672 logging_writer.py:48] [440700] global_step=440700, grad_norm=4.960639953613281, loss=0.7002005577087402
I0302 00:10:57.846637 139590186821376 logging_writer.py:48] [440800] global_step=440800, grad_norm=4.663464069366455, loss=0.5562436580657959
I0302 00:11:31.787733 139590178428672 logging_writer.py:48] [440900] global_step=440900, grad_norm=4.513772964477539, loss=0.5936307907104492
I0302 00:12:05.735380 139590186821376 logging_writer.py:48] [441000] global_step=441000, grad_norm=4.566147327423096, loss=0.6238968372344971
I0302 00:12:39.653896 139590178428672 logging_writer.py:48] [441100] global_step=441100, grad_norm=4.589431285858154, loss=0.6537272334098816
I0302 00:13:13.616100 139590186821376 logging_writer.py:48] [441200] global_step=441200, grad_norm=4.533839702606201, loss=0.5875596404075623
I0302 00:13:47.573650 139590178428672 logging_writer.py:48] [441300] global_step=441300, grad_norm=4.601805686950684, loss=0.6900469064712524
I0302 00:14:21.530977 139590186821376 logging_writer.py:48] [441400] global_step=441400, grad_norm=4.814210891723633, loss=0.6147148013114929
I0302 00:14:55.492815 139590178428672 logging_writer.py:48] [441500] global_step=441500, grad_norm=4.060977458953857, loss=0.5388174057006836
I0302 00:15:29.497756 139590186821376 logging_writer.py:48] [441600] global_step=441600, grad_norm=4.363614082336426, loss=0.6466912031173706
I0302 00:16:03.446465 139590178428672 logging_writer.py:48] [441700] global_step=441700, grad_norm=4.203586101531982, loss=0.6095101237297058
I0302 00:16:37.413353 139590186821376 logging_writer.py:48] [441800] global_step=441800, grad_norm=4.625627040863037, loss=0.6877883672714233
I0302 00:16:48.069634 139753105983296 spec.py:321] Evaluating on the training split.
I0302 00:16:54.042803 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 00:17:02.255661 139753105983296 spec.py:349] Evaluating on the test split.
I0302 00:17:04.542062 139753105983296 submission_runner.py:411] Time since start: 155155.55s, 	Step: 441833, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.14844952523708344, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0566083192825317, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8205649852752686, 'test/num_examples': 10000, 'score': 150005.31732654572, 'total_duration': 155155.54529619217, 'accumulated_submission_time': 150005.31732654572, 'accumulated_eval_time': 5112.650840759277, 'accumulated_logging_time': 22.022234678268433}
I0302 00:17:04.632576 139589708666624 logging_writer.py:48] [441833] accumulated_eval_time=5112.650841, accumulated_logging_time=22.022235, accumulated_submission_time=150005.317327, global_step=441833, preemption_count=0, score=150005.317327, test/accuracy=0.629600, test/loss=1.820565, test/num_examples=10000, total_duration=155155.545296, train/accuracy=0.959682, train/loss=0.148450, validation/accuracy=0.754960, validation/loss=1.056608, validation/num_examples=50000
I0302 00:17:27.685236 139589717059328 logging_writer.py:48] [441900] global_step=441900, grad_norm=5.074806213378906, loss=0.6708959341049194
I0302 00:18:01.599643 139589708666624 logging_writer.py:48] [442000] global_step=442000, grad_norm=4.653225421905518, loss=0.6071940660476685
I0302 00:18:35.541217 139589717059328 logging_writer.py:48] [442100] global_step=442100, grad_norm=4.344447135925293, loss=0.6594855189323425
I0302 00:19:09.489063 139589708666624 logging_writer.py:48] [442200] global_step=442200, grad_norm=4.809324741363525, loss=0.6125473380088806
I0302 00:19:43.396181 139589717059328 logging_writer.py:48] [442300] global_step=442300, grad_norm=4.993722438812256, loss=0.7127718925476074
I0302 00:20:17.367225 139589708666624 logging_writer.py:48] [442400] global_step=442400, grad_norm=4.82642126083374, loss=0.6065670251846313
I0302 00:20:51.326382 139589717059328 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.425431728363037, loss=0.6249027848243713
I0302 00:21:25.365476 139589708666624 logging_writer.py:48] [442600] global_step=442600, grad_norm=4.0953688621521, loss=0.6235315799713135
I0302 00:21:59.297844 139589717059328 logging_writer.py:48] [442700] global_step=442700, grad_norm=5.052289962768555, loss=0.6886677742004395
I0302 00:22:33.224524 139589708666624 logging_writer.py:48] [442800] global_step=442800, grad_norm=4.569058895111084, loss=0.5645977258682251
I0302 00:23:07.200691 139589717059328 logging_writer.py:48] [442900] global_step=442900, grad_norm=4.445967674255371, loss=0.5917953848838806
I0302 00:23:41.166830 139589708666624 logging_writer.py:48] [443000] global_step=443000, grad_norm=4.4798502922058105, loss=0.6417536735534668
I0302 00:24:15.094479 139589717059328 logging_writer.py:48] [443100] global_step=443100, grad_norm=4.65214729309082, loss=0.6838364601135254
I0302 00:24:49.065773 139589708666624 logging_writer.py:48] [443200] global_step=443200, grad_norm=4.365181922912598, loss=0.5866682529449463
I0302 00:25:23.002888 139589717059328 logging_writer.py:48] [443300] global_step=443300, grad_norm=4.448697090148926, loss=0.646528959274292
I0302 00:25:34.695754 139753105983296 spec.py:321] Evaluating on the training split.
I0302 00:25:40.702378 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 00:25:49.050637 139753105983296 spec.py:349] Evaluating on the test split.
I0302 00:25:51.334835 139753105983296 submission_runner.py:411] Time since start: 155682.34s, 	Step: 443336, 	{'train/accuracy': 0.9596021771430969, 'train/loss': 0.15016694366931915, 'validation/accuracy': 0.754539966583252, 'validation/loss': 1.0561749935150146, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8199372291564941, 'test/num_examples': 10000, 'score': 150515.3161327839, 'total_duration': 155682.33803749084, 'accumulated_submission_time': 150515.3161327839, 'accumulated_eval_time': 5129.289839744568, 'accumulated_logging_time': 22.123254776000977}
I0302 00:25:51.432401 139589708666624 logging_writer.py:48] [443336] accumulated_eval_time=5129.289840, accumulated_logging_time=22.123255, accumulated_submission_time=150515.316133, global_step=443336, preemption_count=0, score=150515.316133, test/accuracy=0.629600, test/loss=1.819937, test/num_examples=10000, total_duration=155682.338037, train/accuracy=0.959602, train/loss=0.150167, validation/accuracy=0.754540, validation/loss=1.056175, validation/num_examples=50000
I0302 00:26:13.465257 139589717059328 logging_writer.py:48] [443400] global_step=443400, grad_norm=4.738765716552734, loss=0.659355640411377
I0302 00:26:47.386299 139589708666624 logging_writer.py:48] [443500] global_step=443500, grad_norm=4.604542255401611, loss=0.643454372882843
I0302 00:27:21.324434 139589717059328 logging_writer.py:48] [443600] global_step=443600, grad_norm=4.717679500579834, loss=0.6496419906616211
I0302 00:27:55.308394 139589708666624 logging_writer.py:48] [443700] global_step=443700, grad_norm=4.145215034484863, loss=0.6489891409873962
I0302 00:28:29.200118 139589717059328 logging_writer.py:48] [443800] global_step=443800, grad_norm=4.726840972900391, loss=0.6320135593414307
I0302 00:29:03.133170 139589708666624 logging_writer.py:48] [443900] global_step=443900, grad_norm=4.465815544128418, loss=0.6438389420509338
I0302 00:29:37.123311 139589717059328 logging_writer.py:48] [444000] global_step=444000, grad_norm=5.007316589355469, loss=0.6420278549194336
I0302 00:30:11.049080 139589708666624 logging_writer.py:48] [444100] global_step=444100, grad_norm=4.551946640014648, loss=0.6016212701797485
I0302 00:30:44.968765 139589717059328 logging_writer.py:48] [444200] global_step=444200, grad_norm=4.437527179718018, loss=0.6059980988502502
I0302 00:31:18.892249 139589708666624 logging_writer.py:48] [444300] global_step=444300, grad_norm=4.642657279968262, loss=0.6591920852661133
I0302 00:31:52.815646 139589717059328 logging_writer.py:48] [444400] global_step=444400, grad_norm=4.542306423187256, loss=0.5977322459220886
I0302 00:32:26.757946 139589708666624 logging_writer.py:48] [444500] global_step=444500, grad_norm=3.8564229011535645, loss=0.5475740432739258
I0302 00:33:00.667596 139589717059328 logging_writer.py:48] [444600] global_step=444600, grad_norm=4.371551513671875, loss=0.6040568351745605
I0302 00:33:34.597828 139589708666624 logging_writer.py:48] [444700] global_step=444700, grad_norm=4.736368179321289, loss=0.6526598334312439
I0302 00:34:08.715616 139589717059328 logging_writer.py:48] [444800] global_step=444800, grad_norm=4.531575679779053, loss=0.6342782974243164
I0302 00:34:21.418837 139753105983296 spec.py:321] Evaluating on the training split.
I0302 00:34:27.439903 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 00:34:35.621456 139753105983296 spec.py:349] Evaluating on the test split.
I0302 00:34:37.861095 139753105983296 submission_runner.py:411] Time since start: 156208.86s, 	Step: 444839, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14626845717430115, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0548075437545776, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8183879852294922, 'test/num_examples': 10000, 'score': 151025.23944425583, 'total_duration': 156208.86432909966, 'accumulated_submission_time': 151025.23944425583, 'accumulated_eval_time': 5145.732047557831, 'accumulated_logging_time': 22.230733633041382}
I0302 00:34:37.952632 139590161643264 logging_writer.py:48] [444839] accumulated_eval_time=5145.732048, accumulated_logging_time=22.230734, accumulated_submission_time=151025.239444, global_step=444839, preemption_count=0, score=151025.239444, test/accuracy=0.630600, test/loss=1.818388, test/num_examples=10000, total_duration=156208.864329, train/accuracy=0.960738, train/loss=0.146268, validation/accuracy=0.754900, validation/loss=1.054808, validation/num_examples=50000
I0302 00:34:58.969579 139590178428672 logging_writer.py:48] [444900] global_step=444900, grad_norm=4.273254871368408, loss=0.5725367069244385
I0302 00:35:32.872113 139590161643264 logging_writer.py:48] [445000] global_step=445000, grad_norm=4.22341775894165, loss=0.5959039330482483
I0302 00:36:06.814476 139590178428672 logging_writer.py:48] [445100] global_step=445100, grad_norm=4.894319534301758, loss=0.6261401176452637
I0302 00:36:40.785318 139590161643264 logging_writer.py:48] [445200] global_step=445200, grad_norm=4.364627361297607, loss=0.5848822593688965
I0302 00:37:14.730540 139590178428672 logging_writer.py:48] [445300] global_step=445300, grad_norm=4.286052227020264, loss=0.5853873491287231
I0302 00:37:48.680445 139590161643264 logging_writer.py:48] [445400] global_step=445400, grad_norm=4.67488431930542, loss=0.6463242769241333
I0302 00:38:22.630395 139590178428672 logging_writer.py:48] [445500] global_step=445500, grad_norm=4.287184715270996, loss=0.6156880259513855
I0302 00:38:56.588331 139590161643264 logging_writer.py:48] [445600] global_step=445600, grad_norm=4.953124046325684, loss=0.6629122495651245
I0302 00:39:30.541068 139590178428672 logging_writer.py:48] [445700] global_step=445700, grad_norm=4.587626934051514, loss=0.6029719710350037
I0302 00:40:04.570460 139590161643264 logging_writer.py:48] [445800] global_step=445800, grad_norm=4.616664886474609, loss=0.6267387866973877
I0302 00:40:38.531998 139590178428672 logging_writer.py:48] [445900] global_step=445900, grad_norm=4.68855094909668, loss=0.6512451171875
I0302 00:41:12.480509 139590161643264 logging_writer.py:48] [446000] global_step=446000, grad_norm=5.020218372344971, loss=0.6491259336471558
I0302 00:41:46.443501 139590178428672 logging_writer.py:48] [446100] global_step=446100, grad_norm=4.023860931396484, loss=0.5468146204948425
I0302 00:42:20.385432 139590161643264 logging_writer.py:48] [446200] global_step=446200, grad_norm=4.6357197761535645, loss=0.6394559144973755
I0302 00:42:54.353055 139590178428672 logging_writer.py:48] [446300] global_step=446300, grad_norm=4.748284339904785, loss=0.6543838381767273
I0302 00:43:08.062625 139753105983296 spec.py:321] Evaluating on the training split.
I0302 00:43:14.115154 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 00:43:22.309036 139753105983296 spec.py:349] Evaluating on the test split.
I0302 00:43:24.553194 139753105983296 submission_runner.py:411] Time since start: 156735.56s, 	Step: 446342, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.14775201678276062, 'validation/accuracy': 0.7553399801254272, 'validation/loss': 1.055067777633667, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.81734037399292, 'test/num_examples': 10000, 'score': 151535.28600239754, 'total_duration': 156735.5563902855, 'accumulated_submission_time': 151535.28600239754, 'accumulated_eval_time': 5162.222529888153, 'accumulated_logging_time': 22.332300186157227}
I0302 00:43:24.685548 139589717059328 logging_writer.py:48] [446342] accumulated_eval_time=5162.222530, accumulated_logging_time=22.332300, accumulated_submission_time=151535.286002, global_step=446342, preemption_count=0, score=151535.286002, test/accuracy=0.630500, test/loss=1.817340, test/num_examples=10000, total_duration=156735.556390, train/accuracy=0.960140, train/loss=0.147752, validation/accuracy=0.755340, validation/loss=1.055068, validation/num_examples=50000
I0302 00:43:44.697376 139589725452032 logging_writer.py:48] [446400] global_step=446400, grad_norm=4.763607501983643, loss=0.6012958288192749
I0302 00:44:18.548167 139589717059328 logging_writer.py:48] [446500] global_step=446500, grad_norm=4.71166467666626, loss=0.6429246664047241
I0302 00:44:52.479166 139589725452032 logging_writer.py:48] [446600] global_step=446600, grad_norm=4.4337005615234375, loss=0.64905846118927
I0302 00:45:26.400538 139589717059328 logging_writer.py:48] [446700] global_step=446700, grad_norm=4.433863162994385, loss=0.6274892091751099
I0302 00:46:00.347564 139589725452032 logging_writer.py:48] [446800] global_step=446800, grad_norm=4.536269187927246, loss=0.6260837912559509
I0302 00:46:34.378031 139589717059328 logging_writer.py:48] [446900] global_step=446900, grad_norm=4.8679304122924805, loss=0.6459125876426697
I0302 00:47:08.295620 139589725452032 logging_writer.py:48] [447000] global_step=447000, grad_norm=4.554263114929199, loss=0.6207680106163025
I0302 00:47:42.214010 139589717059328 logging_writer.py:48] [447100] global_step=447100, grad_norm=4.797448635101318, loss=0.5979012250900269
I0302 00:48:16.143163 139589725452032 logging_writer.py:48] [447200] global_step=447200, grad_norm=4.7177629470825195, loss=0.6794584393501282
I0302 00:48:50.059294 139589717059328 logging_writer.py:48] [447300] global_step=447300, grad_norm=4.715860366821289, loss=0.6774920225143433
I0302 00:49:24.000086 139589725452032 logging_writer.py:48] [447400] global_step=447400, grad_norm=4.476365089416504, loss=0.6066709756851196
I0302 00:49:57.955611 139589717059328 logging_writer.py:48] [447500] global_step=447500, grad_norm=4.746511459350586, loss=0.5730149149894714
I0302 00:50:31.897698 139589725452032 logging_writer.py:48] [447600] global_step=447600, grad_norm=4.185764789581299, loss=0.6372167468070984
I0302 00:51:05.817293 139589717059328 logging_writer.py:48] [447700] global_step=447700, grad_norm=4.395339012145996, loss=0.56223064661026
I0302 00:51:39.759801 139589725452032 logging_writer.py:48] [447800] global_step=447800, grad_norm=4.8821330070495605, loss=0.6841095685958862
I0302 00:51:54.849197 139753105983296 spec.py:321] Evaluating on the training split.
I0302 00:52:00.837813 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 00:52:09.259400 139753105983296 spec.py:349] Evaluating on the test split.
I0302 00:52:11.548585 139753105983296 submission_runner.py:411] Time since start: 157262.55s, 	Step: 447846, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.14454177021980286, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0557647943496704, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8188778162002563, 'test/num_examples': 10000, 'score': 152045.3865249157, 'total_duration': 157262.55180740356, 'accumulated_submission_time': 152045.3865249157, 'accumulated_eval_time': 5178.921859025955, 'accumulated_logging_time': 22.474713563919067}
I0302 00:52:11.651003 139590170035968 logging_writer.py:48] [447846] accumulated_eval_time=5178.921859, accumulated_logging_time=22.474714, accumulated_submission_time=152045.386525, global_step=447846, preemption_count=0, score=152045.386525, test/accuracy=0.629700, test/loss=1.818878, test/num_examples=10000, total_duration=157262.551807, train/accuracy=0.960479, train/loss=0.144542, validation/accuracy=0.754920, validation/loss=1.055765, validation/num_examples=50000
I0302 00:52:30.367791 139590178428672 logging_writer.py:48] [447900] global_step=447900, grad_norm=4.521477222442627, loss=0.6706768870353699
I0302 00:53:04.318862 139590170035968 logging_writer.py:48] [448000] global_step=448000, grad_norm=4.619439125061035, loss=0.6137625575065613
I0302 00:53:38.259586 139590178428672 logging_writer.py:48] [448100] global_step=448100, grad_norm=4.460115432739258, loss=0.6808004975318909
I0302 00:54:12.203569 139590170035968 logging_writer.py:48] [448200] global_step=448200, grad_norm=4.980027198791504, loss=0.6581283807754517
I0302 00:54:46.137653 139590178428672 logging_writer.py:48] [448300] global_step=448300, grad_norm=4.7952046394348145, loss=0.5671222805976868
I0302 00:55:20.090845 139590170035968 logging_writer.py:48] [448400] global_step=448400, grad_norm=4.6123552322387695, loss=0.6101694107055664
I0302 00:55:54.044250 139590178428672 logging_writer.py:48] [448500] global_step=448500, grad_norm=4.657400131225586, loss=0.6302291750907898
I0302 00:56:27.980998 139590170035968 logging_writer.py:48] [448600] global_step=448600, grad_norm=4.6415934562683105, loss=0.5899872779846191
I0302 00:57:01.939134 139590178428672 logging_writer.py:48] [448700] global_step=448700, grad_norm=4.746483325958252, loss=0.6758837699890137
I0302 00:57:35.896778 139590170035968 logging_writer.py:48] [448800] global_step=448800, grad_norm=4.6152801513671875, loss=0.543988823890686
I0302 00:58:09.857018 139590178428672 logging_writer.py:48] [448900] global_step=448900, grad_norm=4.613501071929932, loss=0.6339381337165833
I0302 00:58:43.894329 139590170035968 logging_writer.py:48] [449000] global_step=449000, grad_norm=4.150634765625, loss=0.55084627866745
I0302 00:59:17.845860 139590178428672 logging_writer.py:48] [449100] global_step=449100, grad_norm=4.334634304046631, loss=0.606823205947876
I0302 00:59:51.836041 139590170035968 logging_writer.py:48] [449200] global_step=449200, grad_norm=4.151512145996094, loss=0.5868350863456726
I0302 01:00:25.793995 139590178428672 logging_writer.py:48] [449300] global_step=449300, grad_norm=4.689627647399902, loss=0.7315096855163574
I0302 01:00:41.558758 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:00:47.587753 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:00:55.733941 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:00:58.019194 139753105983296 submission_runner.py:411] Time since start: 157789.02s, 	Step: 449348, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14490355551242828, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.055970549583435, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8194371461868286, 'test/num_examples': 10000, 'score': 152555.23027157784, 'total_duration': 157789.02242159843, 'accumulated_submission_time': 152555.23027157784, 'accumulated_eval_time': 5195.382237672806, 'accumulated_logging_time': 22.58767342567444}
I0302 01:00:58.112875 139589725452032 logging_writer.py:48] [449348] accumulated_eval_time=5195.382238, accumulated_logging_time=22.587673, accumulated_submission_time=152555.230272, global_step=449348, preemption_count=0, score=152555.230272, test/accuracy=0.629900, test/loss=1.819437, test/num_examples=10000, total_duration=157789.022422, train/accuracy=0.961017, train/loss=0.144904, validation/accuracy=0.754680, validation/loss=1.055971, validation/num_examples=50000
I0302 01:01:16.099468 139590144857856 logging_writer.py:48] [449400] global_step=449400, grad_norm=4.63079309463501, loss=0.6515305042266846
I0302 01:01:50.001059 139589725452032 logging_writer.py:48] [449500] global_step=449500, grad_norm=4.564713954925537, loss=0.6464794874191284
I0302 01:02:23.915141 139590144857856 logging_writer.py:48] [449600] global_step=449600, grad_norm=4.179976463317871, loss=0.5536312460899353
I0302 01:02:57.838716 139589725452032 logging_writer.py:48] [449700] global_step=449700, grad_norm=4.8502326011657715, loss=0.6460955142974854
I0302 01:03:31.756905 139590144857856 logging_writer.py:48] [449800] global_step=449800, grad_norm=4.539185523986816, loss=0.644650399684906
I0302 01:04:05.673421 139589725452032 logging_writer.py:48] [449900] global_step=449900, grad_norm=4.4009013175964355, loss=0.6108392477035522
I0302 01:04:39.628522 139590144857856 logging_writer.py:48] [450000] global_step=450000, grad_norm=5.0342559814453125, loss=0.6852282285690308
I0302 01:05:13.627760 139589725452032 logging_writer.py:48] [450100] global_step=450100, grad_norm=4.778533458709717, loss=0.6757283806800842
I0302 01:05:47.587491 139590144857856 logging_writer.py:48] [450200] global_step=450200, grad_norm=3.9542338848114014, loss=0.5273140668869019
I0302 01:06:21.528815 139589725452032 logging_writer.py:48] [450300] global_step=450300, grad_norm=4.548329830169678, loss=0.5898592472076416
I0302 01:06:55.453196 139590144857856 logging_writer.py:48] [450400] global_step=450400, grad_norm=4.880631446838379, loss=0.6449220776557922
I0302 01:07:29.411287 139589725452032 logging_writer.py:48] [450500] global_step=450500, grad_norm=4.7045512199401855, loss=0.6178263425827026
I0302 01:08:03.359192 139590144857856 logging_writer.py:48] [450600] global_step=450600, grad_norm=4.583980560302734, loss=0.6725080013275146
I0302 01:08:37.309577 139589725452032 logging_writer.py:48] [450700] global_step=450700, grad_norm=4.714043617248535, loss=0.6968064308166504
I0302 01:09:11.248209 139590144857856 logging_writer.py:48] [450800] global_step=450800, grad_norm=4.157135486602783, loss=0.6041492223739624
I0302 01:09:28.358830 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:09:34.349298 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:09:42.615930 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:09:44.964181 139753105983296 submission_runner.py:411] Time since start: 158315.97s, 	Step: 450852, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14822295308113098, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.054293155670166, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8175337314605713, 'test/num_examples': 10000, 'score': 153065.41314435005, 'total_duration': 158315.9674050808, 'accumulated_submission_time': 153065.41314435005, 'accumulated_eval_time': 5211.9875292778015, 'accumulated_logging_time': 22.691334009170532}
I0302 01:09:45.051096 139589708666624 logging_writer.py:48] [450852] accumulated_eval_time=5211.987529, accumulated_logging_time=22.691334, accumulated_submission_time=153065.413144, global_step=450852, preemption_count=0, score=153065.413144, test/accuracy=0.630100, test/loss=1.817534, test/num_examples=10000, total_duration=158315.967405, train/accuracy=0.960200, train/loss=0.148223, validation/accuracy=0.754720, validation/loss=1.054293, validation/num_examples=50000
I0302 01:10:01.668617 139589717059328 logging_writer.py:48] [450900] global_step=450900, grad_norm=4.441130638122559, loss=0.5905844569206238
I0302 01:10:35.614570 139589708666624 logging_writer.py:48] [451000] global_step=451000, grad_norm=4.483917236328125, loss=0.6172317266464233
I0302 01:11:09.651177 139589717059328 logging_writer.py:48] [451100] global_step=451100, grad_norm=4.694943428039551, loss=0.6466566920280457
I0302 01:11:43.615940 139589708666624 logging_writer.py:48] [451200] global_step=451200, grad_norm=4.664324760437012, loss=0.6241956949234009
I0302 01:12:17.564667 139589717059328 logging_writer.py:48] [451300] global_step=451300, grad_norm=4.687014102935791, loss=0.6507664918899536
I0302 01:12:51.524922 139589708666624 logging_writer.py:48] [451400] global_step=451400, grad_norm=4.42739725112915, loss=0.5851184129714966
I0302 01:13:25.471502 139589717059328 logging_writer.py:48] [451500] global_step=451500, grad_norm=4.592471599578857, loss=0.6269406676292419
I0302 01:13:59.429985 139589708666624 logging_writer.py:48] [451600] global_step=451600, grad_norm=4.786027908325195, loss=0.6263870000839233
I0302 01:14:33.362480 139589717059328 logging_writer.py:48] [451700] global_step=451700, grad_norm=3.9966771602630615, loss=0.5444902777671814
I0302 01:15:07.332831 139589708666624 logging_writer.py:48] [451800] global_step=451800, grad_norm=4.9319891929626465, loss=0.7097274661064148
I0302 01:15:41.280597 139589717059328 logging_writer.py:48] [451900] global_step=451900, grad_norm=4.566288948059082, loss=0.591290295124054
I0302 01:16:15.247992 139589708666624 logging_writer.py:48] [452000] global_step=452000, grad_norm=4.4268107414245605, loss=0.645316481590271
I0302 01:16:49.189680 139589717059328 logging_writer.py:48] [452100] global_step=452100, grad_norm=5.119250297546387, loss=0.6340032815933228
I0302 01:17:23.256800 139589708666624 logging_writer.py:48] [452200] global_step=452200, grad_norm=4.271176338195801, loss=0.6215757727622986
I0302 01:17:57.207641 139589717059328 logging_writer.py:48] [452300] global_step=452300, grad_norm=4.588552951812744, loss=0.5720518827438354
I0302 01:18:15.049887 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:18:21.121380 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:18:29.355988 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:18:31.661358 139753105983296 submission_runner.py:411] Time since start: 158842.66s, 	Step: 452354, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14755219221115112, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.055489182472229, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8206629753112793, 'test/num_examples': 10000, 'score': 153575.34843325615, 'total_duration': 158842.66458582878, 'accumulated_submission_time': 153575.34843325615, 'accumulated_eval_time': 5228.598965406418, 'accumulated_logging_time': 22.787771224975586}
I0302 01:18:31.754795 139590161643264 logging_writer.py:48] [452354] accumulated_eval_time=5228.598965, accumulated_logging_time=22.787771, accumulated_submission_time=153575.348433, global_step=452354, preemption_count=0, score=153575.348433, test/accuracy=0.631000, test/loss=1.820663, test/num_examples=10000, total_duration=158842.664586, train/accuracy=0.960639, train/loss=0.147552, validation/accuracy=0.754880, validation/loss=1.055489, validation/num_examples=50000
I0302 01:18:47.729015 139590178428672 logging_writer.py:48] [452400] global_step=452400, grad_norm=4.334027290344238, loss=0.6120949983596802
I0302 01:19:21.644190 139590161643264 logging_writer.py:48] [452500] global_step=452500, grad_norm=4.587306022644043, loss=0.6312413215637207
I0302 01:19:55.549040 139590178428672 logging_writer.py:48] [452600] global_step=452600, grad_norm=4.34537935256958, loss=0.6148936152458191
I0302 01:20:29.473458 139590161643264 logging_writer.py:48] [452700] global_step=452700, grad_norm=4.637790679931641, loss=0.5824716091156006
I0302 01:21:03.437271 139590178428672 logging_writer.py:48] [452800] global_step=452800, grad_norm=4.284831523895264, loss=0.596952497959137
I0302 01:21:37.391670 139590161643264 logging_writer.py:48] [452900] global_step=452900, grad_norm=4.585682392120361, loss=0.686490535736084
I0302 01:22:11.355097 139590178428672 logging_writer.py:48] [453000] global_step=453000, grad_norm=4.1860761642456055, loss=0.6098508238792419
I0302 01:22:45.315219 139590161643264 logging_writer.py:48] [453100] global_step=453100, grad_norm=4.4831013679504395, loss=0.602914571762085
I0302 01:23:19.342054 139590178428672 logging_writer.py:48] [453200] global_step=453200, grad_norm=4.91696310043335, loss=0.6499678492546082
I0302 01:23:53.290372 139590161643264 logging_writer.py:48] [453300] global_step=453300, grad_norm=4.401229381561279, loss=0.5605059266090393
I0302 01:24:27.259061 139590178428672 logging_writer.py:48] [453400] global_step=453400, grad_norm=4.860856533050537, loss=0.6837126016616821
I0302 01:25:01.218516 139590161643264 logging_writer.py:48] [453500] global_step=453500, grad_norm=4.429689407348633, loss=0.6065699458122253
I0302 01:25:35.119796 139590178428672 logging_writer.py:48] [453600] global_step=453600, grad_norm=4.880331993103027, loss=0.6409962773323059
I0302 01:26:09.041699 139590161643264 logging_writer.py:48] [453700] global_step=453700, grad_norm=4.578432559967041, loss=0.5999497771263123
I0302 01:26:43.017597 139590178428672 logging_writer.py:48] [453800] global_step=453800, grad_norm=4.349488735198975, loss=0.6621907949447632
I0302 01:27:01.836377 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:27:07.863433 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:27:16.015329 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:27:18.343271 139753105983296 submission_runner.py:411] Time since start: 159369.35s, 	Step: 453857, 	{'train/accuracy': 0.9617147445678711, 'train/loss': 0.14442770183086395, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0558409690856934, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8203938007354736, 'test/num_examples': 10000, 'score': 154085.36639356613, 'total_duration': 159369.3465027809, 'accumulated_submission_time': 154085.36639356613, 'accumulated_eval_time': 5245.10581445694, 'accumulated_logging_time': 22.891539573669434}
I0302 01:27:18.438462 139589717059328 logging_writer.py:48] [453857] accumulated_eval_time=5245.105814, accumulated_logging_time=22.891540, accumulated_submission_time=154085.366394, global_step=453857, preemption_count=0, score=154085.366394, test/accuracy=0.630000, test/loss=1.820394, test/num_examples=10000, total_duration=159369.346503, train/accuracy=0.961715, train/loss=0.144428, validation/accuracy=0.755020, validation/loss=1.055841, validation/num_examples=50000
I0302 01:27:33.389520 139589725452032 logging_writer.py:48] [453900] global_step=453900, grad_norm=5.085463047027588, loss=0.6057950854301453
I0302 01:28:07.280460 139589717059328 logging_writer.py:48] [454000] global_step=454000, grad_norm=4.365826606750488, loss=0.5719025731086731
I0302 01:28:41.258499 139589725452032 logging_writer.py:48] [454100] global_step=454100, grad_norm=4.349543571472168, loss=0.6102554202079773
I0302 01:29:15.235424 139589717059328 logging_writer.py:48] [454200] global_step=454200, grad_norm=5.235949516296387, loss=0.6338425278663635
I0302 01:29:49.303265 139589725452032 logging_writer.py:48] [454300] global_step=454300, grad_norm=4.811553001403809, loss=0.6387763023376465
I0302 01:30:23.236151 139589717059328 logging_writer.py:48] [454400] global_step=454400, grad_norm=4.359438896179199, loss=0.54679936170578
I0302 01:30:57.174553 139589725452032 logging_writer.py:48] [454500] global_step=454500, grad_norm=4.656164169311523, loss=0.6251172423362732
I0302 01:31:31.151727 139589717059328 logging_writer.py:48] [454600] global_step=454600, grad_norm=4.748198986053467, loss=0.6583821773529053
I0302 01:32:05.107496 139589725452032 logging_writer.py:48] [454700] global_step=454700, grad_norm=4.256293296813965, loss=0.5946834683418274
I0302 01:32:39.092601 139589717059328 logging_writer.py:48] [454800] global_step=454800, grad_norm=4.749783992767334, loss=0.6374221444129944
I0302 01:33:13.040800 139589725452032 logging_writer.py:48] [454900] global_step=454900, grad_norm=4.365522384643555, loss=0.6051391959190369
I0302 01:33:47.000954 139589717059328 logging_writer.py:48] [455000] global_step=455000, grad_norm=4.145780086517334, loss=0.6004366278648376
I0302 01:34:20.954446 139589725452032 logging_writer.py:48] [455100] global_step=455100, grad_norm=4.633495330810547, loss=0.6440310478210449
I0302 01:34:54.885757 139589717059328 logging_writer.py:48] [455200] global_step=455200, grad_norm=4.433235168457031, loss=0.6205627918243408
I0302 01:35:28.831883 139589725452032 logging_writer.py:48] [455300] global_step=455300, grad_norm=4.386227607727051, loss=0.6178159117698669
I0302 01:35:48.438050 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:35:54.477233 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:36:02.738168 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:36:05.096005 139753105983296 submission_runner.py:411] Time since start: 159896.10s, 	Step: 455359, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.142574280500412, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0553386211395264, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.819143295288086, 'test/num_examples': 10000, 'score': 154595.30282139778, 'total_duration': 159896.09922385216, 'accumulated_submission_time': 154595.30282139778, 'accumulated_eval_time': 5261.763719320297, 'accumulated_logging_time': 22.997020959854126}
I0302 01:36:05.197990 139589717059328 logging_writer.py:48] [455359] accumulated_eval_time=5261.763719, accumulated_logging_time=22.997021, accumulated_submission_time=154595.302821, global_step=455359, preemption_count=0, score=154595.302821, test/accuracy=0.629400, test/loss=1.819143, test/num_examples=10000, total_duration=159896.099224, train/accuracy=0.961037, train/loss=0.142574, validation/accuracy=0.754680, validation/loss=1.055339, validation/num_examples=50000
I0302 01:36:19.434392 139589725452032 logging_writer.py:48] [455400] global_step=455400, grad_norm=4.385340213775635, loss=0.6185846328735352
I0302 01:36:53.363029 139589717059328 logging_writer.py:48] [455500] global_step=455500, grad_norm=4.955837249755859, loss=0.6612972021102905
I0302 01:37:27.296995 139589725452032 logging_writer.py:48] [455600] global_step=455600, grad_norm=4.696740627288818, loss=0.6257580518722534
I0302 01:38:01.241515 139589717059328 logging_writer.py:48] [455700] global_step=455700, grad_norm=4.5617570877075195, loss=0.6122385263442993
I0302 01:38:35.199612 139589725452032 logging_writer.py:48] [455800] global_step=455800, grad_norm=4.444332599639893, loss=0.6213282942771912
I0302 01:39:09.132952 139589717059328 logging_writer.py:48] [455900] global_step=455900, grad_norm=4.555322647094727, loss=0.6138042211532593
I0302 01:39:43.043129 139589725452032 logging_writer.py:48] [456000] global_step=456000, grad_norm=4.74487829208374, loss=0.6318095922470093
I0302 01:40:16.973297 139589717059328 logging_writer.py:48] [456100] global_step=456100, grad_norm=4.930373668670654, loss=0.7109233140945435
I0302 01:40:50.931798 139589725452032 logging_writer.py:48] [456200] global_step=456200, grad_norm=4.170379161834717, loss=0.6556218266487122
I0302 01:41:24.837463 139589717059328 logging_writer.py:48] [456300] global_step=456300, grad_norm=4.770196914672852, loss=0.6261885762214661
I0302 01:41:58.860098 139589725452032 logging_writer.py:48] [456400] global_step=456400, grad_norm=4.989431381225586, loss=0.6052433848381042
I0302 01:42:32.755035 139589717059328 logging_writer.py:48] [456500] global_step=456500, grad_norm=4.373340129852295, loss=0.6157203316688538
I0302 01:43:06.711835 139589725452032 logging_writer.py:48] [456600] global_step=456600, grad_norm=4.361763954162598, loss=0.6272760629653931
I0302 01:43:40.663124 139589717059328 logging_writer.py:48] [456700] global_step=456700, grad_norm=5.107465744018555, loss=0.6397452354431152
I0302 01:44:14.593909 139589725452032 logging_writer.py:48] [456800] global_step=456800, grad_norm=4.606607437133789, loss=0.5459293127059937
I0302 01:44:35.430433 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:44:41.516909 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:44:49.665375 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:44:51.961895 139753105983296 submission_runner.py:411] Time since start: 160422.97s, 	Step: 456863, 	{'train/accuracy': 0.9611567258834839, 'train/loss': 0.14571698009967804, 'validation/accuracy': 0.754859983921051, 'validation/loss': 1.0554897785186768, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.819894790649414, 'test/num_examples': 10000, 'score': 155105.47327399254, 'total_duration': 160422.96509361267, 'accumulated_submission_time': 155105.47327399254, 'accumulated_eval_time': 5278.295104265213, 'accumulated_logging_time': 23.10880136489868}
I0302 01:44:52.055032 139589717059328 logging_writer.py:48] [456863] accumulated_eval_time=5278.295104, accumulated_logging_time=23.108801, accumulated_submission_time=155105.473274, global_step=456863, preemption_count=0, score=155105.473274, test/accuracy=0.629800, test/loss=1.819895, test/num_examples=10000, total_duration=160422.965094, train/accuracy=0.961157, train/loss=0.145717, validation/accuracy=0.754860, validation/loss=1.055490, validation/num_examples=50000
I0302 01:45:04.938482 139589725452032 logging_writer.py:48] [456900] global_step=456900, grad_norm=4.350852966308594, loss=0.6246055960655212
I0302 01:45:38.874928 139589717059328 logging_writer.py:48] [457000] global_step=457000, grad_norm=4.521660804748535, loss=0.6159913539886475
I0302 01:46:12.812783 139589725452032 logging_writer.py:48] [457100] global_step=457100, grad_norm=4.633159637451172, loss=0.6216118931770325
I0302 01:46:46.761830 139589717059328 logging_writer.py:48] [457200] global_step=457200, grad_norm=4.188624858856201, loss=0.6163749098777771
I0302 01:47:20.724303 139589725452032 logging_writer.py:48] [457300] global_step=457300, grad_norm=4.092503547668457, loss=0.5832221508026123
I0302 01:47:54.626273 139589717059328 logging_writer.py:48] [457400] global_step=457400, grad_norm=4.791614532470703, loss=0.6404651403427124
I0302 01:48:28.655511 139589725452032 logging_writer.py:48] [457500] global_step=457500, grad_norm=5.030297756195068, loss=0.6869245171546936
I0302 01:49:02.592097 139589717059328 logging_writer.py:48] [457600] global_step=457600, grad_norm=4.684845447540283, loss=0.5955577492713928
I0302 01:49:36.543614 139589725452032 logging_writer.py:48] [457700] global_step=457700, grad_norm=4.944962501525879, loss=0.6755212545394897
I0302 01:50:10.473421 139589717059328 logging_writer.py:48] [457800] global_step=457800, grad_norm=4.792212009429932, loss=0.6687548160552979
I0302 01:50:44.415261 139589725452032 logging_writer.py:48] [457900] global_step=457900, grad_norm=4.362329006195068, loss=0.7022760510444641
I0302 01:51:18.361059 139589717059328 logging_writer.py:48] [458000] global_step=458000, grad_norm=4.465014457702637, loss=0.5444589853286743
I0302 01:51:52.296926 139589725452032 logging_writer.py:48] [458100] global_step=458100, grad_norm=4.467026710510254, loss=0.6230267882347107
I0302 01:52:26.220485 139589717059328 logging_writer.py:48] [458200] global_step=458200, grad_norm=5.14114236831665, loss=0.5798166990280151
I0302 01:53:00.159716 139589725452032 logging_writer.py:48] [458300] global_step=458300, grad_norm=4.517415523529053, loss=0.6353765726089478
I0302 01:53:22.033698 139753105983296 spec.py:321] Evaluating on the training split.
I0302 01:53:28.107175 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 01:53:36.850120 139753105983296 spec.py:349] Evaluating on the test split.
I0302 01:53:39.138075 139753105983296 submission_runner.py:411] Time since start: 160950.14s, 	Step: 458366, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14727406203746796, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0557348728179932, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8191497325897217, 'test/num_examples': 10000, 'score': 155615.38880991936, 'total_duration': 160950.1413064003, 'accumulated_submission_time': 155615.38880991936, 'accumulated_eval_time': 5295.399443626404, 'accumulated_logging_time': 23.212135314941406}
I0302 01:53:39.232245 139590178428672 logging_writer.py:48] [458366] accumulated_eval_time=5295.399444, accumulated_logging_time=23.212135, accumulated_submission_time=155615.388810, global_step=458366, preemption_count=0, score=155615.388810, test/accuracy=0.629900, test/loss=1.819150, test/num_examples=10000, total_duration=160950.141306, train/accuracy=0.959801, train/loss=0.147274, validation/accuracy=0.754780, validation/loss=1.055735, validation/num_examples=50000
I0302 01:53:51.127358 139590186821376 logging_writer.py:48] [458400] global_step=458400, grad_norm=4.4615888595581055, loss=0.627213716506958
I0302 01:54:25.125338 139590178428672 logging_writer.py:48] [458500] global_step=458500, grad_norm=4.334534168243408, loss=0.6135427355766296
I0302 01:54:59.026963 139590186821376 logging_writer.py:48] [458600] global_step=458600, grad_norm=4.171209335327148, loss=0.5762330293655396
I0302 01:55:32.949214 139590178428672 logging_writer.py:48] [458700] global_step=458700, grad_norm=4.455705165863037, loss=0.5712522268295288
I0302 01:56:06.888170 139590186821376 logging_writer.py:48] [458800] global_step=458800, grad_norm=4.6075968742370605, loss=0.5784789323806763
I0302 01:56:40.827844 139590178428672 logging_writer.py:48] [458900] global_step=458900, grad_norm=4.261724472045898, loss=0.673277735710144
I0302 01:57:14.727522 139590186821376 logging_writer.py:48] [459000] global_step=459000, grad_norm=4.6237006187438965, loss=0.6571585536003113
I0302 01:57:48.655114 139590178428672 logging_writer.py:48] [459100] global_step=459100, grad_norm=4.420873165130615, loss=0.6307849884033203
I0302 01:58:22.609881 139590186821376 logging_writer.py:48] [459200] global_step=459200, grad_norm=4.582703590393066, loss=0.6436957716941833
I0302 01:58:56.537672 139590178428672 logging_writer.py:48] [459300] global_step=459300, grad_norm=4.1892242431640625, loss=0.6021537780761719
I0302 01:59:30.432406 139590186821376 logging_writer.py:48] [459400] global_step=459400, grad_norm=4.679246425628662, loss=0.6849677562713623
I0302 02:00:04.366985 139590178428672 logging_writer.py:48] [459500] global_step=459500, grad_norm=4.745262622833252, loss=0.6974388957023621
I0302 02:00:38.485811 139590186821376 logging_writer.py:48] [459600] global_step=459600, grad_norm=4.612053871154785, loss=0.6841118931770325
I0302 02:01:12.385253 139590178428672 logging_writer.py:48] [459700] global_step=459700, grad_norm=4.906313896179199, loss=0.6622589826583862
I0302 02:01:46.349943 139590186821376 logging_writer.py:48] [459800] global_step=459800, grad_norm=4.507938385009766, loss=0.6532267928123474
I0302 02:02:09.224638 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:02:15.422297 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:02:23.724173 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:02:25.938453 139753105983296 submission_runner.py:411] Time since start: 161476.94s, 	Step: 459869, 	{'train/accuracy': 0.9605986475944519, 'train/loss': 0.14671580493450165, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0555691719055176, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8184266090393066, 'test/num_examples': 10000, 'score': 156125.31659150124, 'total_duration': 161476.9416666031, 'accumulated_submission_time': 156125.31659150124, 'accumulated_eval_time': 5312.113205909729, 'accumulated_logging_time': 23.317948579788208}
I0302 02:02:26.036415 139589725452032 logging_writer.py:48] [459869] accumulated_eval_time=5312.113206, accumulated_logging_time=23.317949, accumulated_submission_time=156125.316592, global_step=459869, preemption_count=0, score=156125.316592, test/accuracy=0.629700, test/loss=1.818427, test/num_examples=10000, total_duration=161476.941667, train/accuracy=0.960599, train/loss=0.146716, validation/accuracy=0.754940, validation/loss=1.055569, validation/num_examples=50000
I0302 02:02:36.920251 139590144857856 logging_writer.py:48] [459900] global_step=459900, grad_norm=4.924103260040283, loss=0.7338888645172119
I0302 02:03:10.858073 139589725452032 logging_writer.py:48] [460000] global_step=460000, grad_norm=4.609463214874268, loss=0.6215493083000183
I0302 02:03:44.807057 139590144857856 logging_writer.py:48] [460100] global_step=460100, grad_norm=4.454116344451904, loss=0.6266754865646362
I0302 02:04:18.772498 139589725452032 logging_writer.py:48] [460200] global_step=460200, grad_norm=4.764055252075195, loss=0.5925872921943665
I0302 02:04:52.735153 139590144857856 logging_writer.py:48] [460300] global_step=460300, grad_norm=4.242945671081543, loss=0.6640698909759521
I0302 02:05:26.677028 139589725452032 logging_writer.py:48] [460400] global_step=460400, grad_norm=4.467499732971191, loss=0.6191610097885132
I0302 02:06:00.610327 139590144857856 logging_writer.py:48] [460500] global_step=460500, grad_norm=4.4246931076049805, loss=0.6121707558631897
I0302 02:06:34.568173 139589725452032 logging_writer.py:48] [460600] global_step=460600, grad_norm=4.547890663146973, loss=0.6055267453193665
I0302 02:07:08.648219 139590144857856 logging_writer.py:48] [460700] global_step=460700, grad_norm=4.410737991333008, loss=0.6124487519264221
I0302 02:07:42.628169 139589725452032 logging_writer.py:48] [460800] global_step=460800, grad_norm=4.968472480773926, loss=0.6548987627029419
I0302 02:08:16.571591 139590144857856 logging_writer.py:48] [460900] global_step=460900, grad_norm=5.03113317489624, loss=0.6139526963233948
I0302 02:08:50.557652 139589725452032 logging_writer.py:48] [461000] global_step=461000, grad_norm=4.774617671966553, loss=0.6625508069992065
I0302 02:09:24.507766 139590144857856 logging_writer.py:48] [461100] global_step=461100, grad_norm=4.734381675720215, loss=0.6279288530349731
I0302 02:09:58.462541 139589725452032 logging_writer.py:48] [461200] global_step=461200, grad_norm=4.821389675140381, loss=0.6152235269546509
I0302 02:10:32.406709 139590144857856 logging_writer.py:48] [461300] global_step=461300, grad_norm=4.5613017082214355, loss=0.5702061057090759
I0302 02:10:55.962295 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:11:02.023421 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:11:10.348021 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:11:12.592461 139753105983296 submission_runner.py:411] Time since start: 162003.60s, 	Step: 461371, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14683839678764343, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.056372046470642, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8204811811447144, 'test/num_examples': 10000, 'score': 156635.17824530602, 'total_duration': 162003.59568810463, 'accumulated_submission_time': 156635.17824530602, 'accumulated_eval_time': 5328.743317604065, 'accumulated_logging_time': 23.42711114883423}
I0302 02:11:12.687082 139590170035968 logging_writer.py:48] [461371] accumulated_eval_time=5328.743318, accumulated_logging_time=23.427111, accumulated_submission_time=156635.178245, global_step=461371, preemption_count=0, score=156635.178245, test/accuracy=0.630100, test/loss=1.820481, test/num_examples=10000, total_duration=162003.595688, train/accuracy=0.960280, train/loss=0.146838, validation/accuracy=0.754640, validation/loss=1.056372, validation/num_examples=50000
I0302 02:11:22.856550 139590178428672 logging_writer.py:48] [461400] global_step=461400, grad_norm=4.203383922576904, loss=0.5461490154266357
I0302 02:11:56.741438 139590170035968 logging_writer.py:48] [461500] global_step=461500, grad_norm=4.596578598022461, loss=0.5940541625022888
I0302 02:12:30.648902 139590178428672 logging_writer.py:48] [461600] global_step=461600, grad_norm=4.01926326751709, loss=0.6340936422348022
I0302 02:13:04.652275 139590170035968 logging_writer.py:48] [461700] global_step=461700, grad_norm=4.38205623626709, loss=0.557237982749939
I0302 02:13:38.603819 139590178428672 logging_writer.py:48] [461800] global_step=461800, grad_norm=4.951347351074219, loss=0.6375632882118225
I0302 02:14:12.567208 139590170035968 logging_writer.py:48] [461900] global_step=461900, grad_norm=4.73294734954834, loss=0.639825701713562
I0302 02:14:46.542510 139590178428672 logging_writer.py:48] [462000] global_step=462000, grad_norm=4.726665019989014, loss=0.6536871194839478
I0302 02:15:20.496924 139590170035968 logging_writer.py:48] [462100] global_step=462100, grad_norm=4.319535255432129, loss=0.6364742517471313
I0302 02:15:54.423901 139590178428672 logging_writer.py:48] [462200] global_step=462200, grad_norm=4.9475884437561035, loss=0.6645917892456055
I0302 02:16:28.356215 139590170035968 logging_writer.py:48] [462300] global_step=462300, grad_norm=5.059623718261719, loss=0.673241913318634
I0302 02:17:02.304352 139590178428672 logging_writer.py:48] [462400] global_step=462400, grad_norm=4.583376407623291, loss=0.624880850315094
I0302 02:17:36.236666 139590170035968 logging_writer.py:48] [462500] global_step=462500, grad_norm=4.4141621589660645, loss=0.6135720014572144
I0302 02:18:10.136079 139590178428672 logging_writer.py:48] [462600] global_step=462600, grad_norm=4.446281909942627, loss=0.6128959059715271
I0302 02:18:44.069997 139590170035968 logging_writer.py:48] [462700] global_step=462700, grad_norm=4.543391704559326, loss=0.650532066822052
I0302 02:19:18.081842 139590178428672 logging_writer.py:48] [462800] global_step=462800, grad_norm=4.427188873291016, loss=0.6319301128387451
I0302 02:19:42.653563 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:19:48.707465 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:19:57.006439 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:19:59.281841 139753105983296 submission_runner.py:411] Time since start: 162530.29s, 	Step: 462874, 	{'train/accuracy': 0.9627510905265808, 'train/loss': 0.14425154030323029, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0549752712249756, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8176995515823364, 'test/num_examples': 10000, 'score': 157145.0807697773, 'total_duration': 162530.2850341797, 'accumulated_submission_time': 157145.0807697773, 'accumulated_eval_time': 5345.371515035629, 'accumulated_logging_time': 23.53261113166809}
I0302 02:19:59.372977 139589725452032 logging_writer.py:48] [462874] accumulated_eval_time=5345.371515, accumulated_logging_time=23.532611, accumulated_submission_time=157145.080770, global_step=462874, preemption_count=0, score=157145.080770, test/accuracy=0.629800, test/loss=1.817700, test/num_examples=10000, total_duration=162530.285034, train/accuracy=0.962751, train/loss=0.144252, validation/accuracy=0.754740, validation/loss=1.054975, validation/num_examples=50000
I0302 02:20:08.525255 139590144857856 logging_writer.py:48] [462900] global_step=462900, grad_norm=4.397459983825684, loss=0.6506314873695374
I0302 02:20:42.455108 139589725452032 logging_writer.py:48] [463000] global_step=463000, grad_norm=4.522628307342529, loss=0.6077723503112793
I0302 02:21:16.380908 139590144857856 logging_writer.py:48] [463100] global_step=463100, grad_norm=4.898839473724365, loss=0.6195806264877319
I0302 02:21:50.308184 139589725452032 logging_writer.py:48] [463200] global_step=463200, grad_norm=4.9934234619140625, loss=0.634084939956665
I0302 02:22:24.262986 139590144857856 logging_writer.py:48] [463300] global_step=463300, grad_norm=4.051302909851074, loss=0.5302331447601318
I0302 02:22:58.223325 139589725452032 logging_writer.py:48] [463400] global_step=463400, grad_norm=4.222212314605713, loss=0.5598433017730713
I0302 02:23:32.179284 139590144857856 logging_writer.py:48] [463500] global_step=463500, grad_norm=4.1628828048706055, loss=0.5953872203826904
I0302 02:24:06.161592 139589725452032 logging_writer.py:48] [463600] global_step=463600, grad_norm=4.581837177276611, loss=0.5952357053756714
I0302 02:24:40.096918 139590144857856 logging_writer.py:48] [463700] global_step=463700, grad_norm=5.046290397644043, loss=0.6350154876708984
I0302 02:25:14.216063 139589725452032 logging_writer.py:48] [463800] global_step=463800, grad_norm=4.518089294433594, loss=0.6483243703842163
I0302 02:25:48.167962 139590144857856 logging_writer.py:48] [463900] global_step=463900, grad_norm=4.67189884185791, loss=0.6171338558197021
I0302 02:26:22.114091 139589725452032 logging_writer.py:48] [464000] global_step=464000, grad_norm=4.664796352386475, loss=0.5796148777008057
I0302 02:26:56.065083 139590144857856 logging_writer.py:48] [464100] global_step=464100, grad_norm=5.096555709838867, loss=0.6330296993255615
I0302 02:27:30.023014 139589725452032 logging_writer.py:48] [464200] global_step=464200, grad_norm=4.311280250549316, loss=0.5652511715888977
I0302 02:28:03.988621 139590144857856 logging_writer.py:48] [464300] global_step=464300, grad_norm=4.626739501953125, loss=0.6563559174537659
I0302 02:28:29.584641 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:28:35.539398 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:28:43.795197 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:28:46.732507 139753105983296 submission_runner.py:411] Time since start: 163057.74s, 	Step: 464377, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14622552692890167, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.0557774305343628, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8174927234649658, 'test/num_examples': 10000, 'score': 157655.2273361683, 'total_duration': 163057.73574876785, 'accumulated_submission_time': 157655.2273361683, 'accumulated_eval_time': 5362.519360303879, 'accumulated_logging_time': 23.63612127304077}
I0302 02:28:46.809468 139589717059328 logging_writer.py:48] [464377] accumulated_eval_time=5362.519360, accumulated_logging_time=23.636121, accumulated_submission_time=157655.227336, global_step=464377, preemption_count=0, score=157655.227336, test/accuracy=0.629800, test/loss=1.817493, test/num_examples=10000, total_duration=163057.735749, train/accuracy=0.961017, train/loss=0.146226, validation/accuracy=0.754520, validation/loss=1.055777, validation/num_examples=50000
I0302 02:28:54.933127 139589725452032 logging_writer.py:48] [464400] global_step=464400, grad_norm=4.6057000160217285, loss=0.6343820691108704
I0302 02:29:28.805026 139589717059328 logging_writer.py:48] [464500] global_step=464500, grad_norm=4.305858612060547, loss=0.5935413837432861
I0302 02:30:02.724128 139589725452032 logging_writer.py:48] [464600] global_step=464600, grad_norm=4.250085353851318, loss=0.5907198190689087
I0302 02:30:36.652735 139589717059328 logging_writer.py:48] [464700] global_step=464700, grad_norm=4.574508190155029, loss=0.5606269836425781
I0302 02:31:10.620004 139589725452032 logging_writer.py:48] [464800] global_step=464800, grad_norm=4.829471588134766, loss=0.7295733094215393
I0302 02:31:44.675010 139589717059328 logging_writer.py:48] [464900] global_step=464900, grad_norm=4.4644670486450195, loss=0.6039635539054871
I0302 02:32:18.617646 139589725452032 logging_writer.py:48] [465000] global_step=465000, grad_norm=4.56643009185791, loss=0.6396538019180298
I0302 02:32:52.552924 139589717059328 logging_writer.py:48] [465100] global_step=465100, grad_norm=4.384734153747559, loss=0.6897902488708496
I0302 02:33:26.516941 139589725452032 logging_writer.py:48] [465200] global_step=465200, grad_norm=4.275132656097412, loss=0.5889567136764526
I0302 02:34:00.485369 139589717059328 logging_writer.py:48] [465300] global_step=465300, grad_norm=4.454345703125, loss=0.5657326579093933
I0302 02:34:34.440396 139589725452032 logging_writer.py:48] [465400] global_step=465400, grad_norm=4.481168746948242, loss=0.5642389059066772
I0302 02:35:08.378134 139589717059328 logging_writer.py:48] [465500] global_step=465500, grad_norm=4.308664321899414, loss=0.6041147708892822
I0302 02:35:42.320703 139589725452032 logging_writer.py:48] [465600] global_step=465600, grad_norm=4.423694610595703, loss=0.6475976705551147
I0302 02:36:16.267923 139589717059328 logging_writer.py:48] [465700] global_step=465700, grad_norm=4.807055473327637, loss=0.6717922687530518
I0302 02:36:50.167929 139589725452032 logging_writer.py:48] [465800] global_step=465800, grad_norm=4.798255443572998, loss=0.6080697774887085
I0302 02:37:16.783102 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:37:22.791080 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:37:30.991248 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:37:33.592084 139753105983296 submission_runner.py:411] Time since start: 163584.60s, 	Step: 465880, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14370304346084595, 'validation/accuracy': 0.7551199793815613, 'validation/loss': 1.0570862293243408, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.82122802734375, 'test/num_examples': 10000, 'score': 158165.1404414177, 'total_duration': 163584.59530687332, 'accumulated_submission_time': 158165.1404414177, 'accumulated_eval_time': 5379.328293085098, 'accumulated_logging_time': 23.72211003303528}
I0302 02:37:33.699149 139589717059328 logging_writer.py:48] [465880] accumulated_eval_time=5379.328293, accumulated_logging_time=23.722110, accumulated_submission_time=158165.140441, global_step=465880, preemption_count=0, score=158165.140441, test/accuracy=0.629300, test/loss=1.821228, test/num_examples=10000, total_duration=163584.595307, train/accuracy=0.960439, train/loss=0.143703, validation/accuracy=0.755120, validation/loss=1.057086, validation/num_examples=50000
I0302 02:37:40.843491 139590153250560 logging_writer.py:48] [465900] global_step=465900, grad_norm=5.960526943206787, loss=0.6544654369354248
I0302 02:38:14.720796 139589717059328 logging_writer.py:48] [466000] global_step=466000, grad_norm=4.425174713134766, loss=0.6231729388237
I0302 02:38:48.667281 139590153250560 logging_writer.py:48] [466100] global_step=466100, grad_norm=5.060242652893066, loss=0.6312605142593384
I0302 02:39:22.632363 139589717059328 logging_writer.py:48] [466200] global_step=466200, grad_norm=4.62234354019165, loss=0.6978090405464172
I0302 02:39:56.556156 139590153250560 logging_writer.py:48] [466300] global_step=466300, grad_norm=4.595223903656006, loss=0.6237115263938904
I0302 02:40:30.517264 139589717059328 logging_writer.py:48] [466400] global_step=466400, grad_norm=4.359504699707031, loss=0.506698489189148
I0302 02:41:04.480880 139590153250560 logging_writer.py:48] [466500] global_step=466500, grad_norm=4.491110324859619, loss=0.6264076232910156
I0302 02:41:38.419837 139589717059328 logging_writer.py:48] [466600] global_step=466600, grad_norm=4.424755096435547, loss=0.6060826778411865
I0302 02:42:12.360478 139590153250560 logging_writer.py:48] [466700] global_step=466700, grad_norm=4.664782524108887, loss=0.6417829394340515
I0302 02:42:46.299371 139589717059328 logging_writer.py:48] [466800] global_step=466800, grad_norm=4.7011399269104, loss=0.6496426463127136
I0302 02:43:20.260308 139590153250560 logging_writer.py:48] [466900] global_step=466900, grad_norm=4.43378210067749, loss=0.6144936084747314
I0302 02:43:54.401639 139589717059328 logging_writer.py:48] [467000] global_step=467000, grad_norm=4.697722911834717, loss=0.6127021312713623
I0302 02:44:28.360206 139590153250560 logging_writer.py:48] [467100] global_step=467100, grad_norm=4.501438617706299, loss=0.6424920558929443
I0302 02:45:02.287110 139589717059328 logging_writer.py:48] [467200] global_step=467200, grad_norm=4.490715026855469, loss=0.5762460231781006
I0302 02:45:36.260830 139590153250560 logging_writer.py:48] [467300] global_step=467300, grad_norm=4.755551815032959, loss=0.6472195386886597
I0302 02:46:03.871658 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:46:09.897011 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:46:18.389829 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:46:20.607966 139753105983296 submission_runner.py:411] Time since start: 164111.61s, 	Step: 467383, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.14661674201488495, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0551986694335938, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.818740963935852, 'test/num_examples': 10000, 'score': 158675.24805808067, 'total_duration': 164111.61119008064, 'accumulated_submission_time': 158675.24805808067, 'accumulated_eval_time': 5396.06454372406, 'accumulated_logging_time': 23.839457273483276}
I0302 02:46:20.707464 139590178428672 logging_writer.py:48] [467383] accumulated_eval_time=5396.064544, accumulated_logging_time=23.839457, accumulated_submission_time=158675.248058, global_step=467383, preemption_count=0, score=158675.248058, test/accuracy=0.631000, test/loss=1.818741, test/num_examples=10000, total_duration=164111.611190, train/accuracy=0.960878, train/loss=0.146617, validation/accuracy=0.754720, validation/loss=1.055199, validation/num_examples=50000
I0302 02:46:26.808870 139590186821376 logging_writer.py:48] [467400] global_step=467400, grad_norm=4.5689377784729, loss=0.6544322371482849
I0302 02:47:00.719506 139590178428672 logging_writer.py:48] [467500] global_step=467500, grad_norm=4.906528472900391, loss=0.6300009489059448
I0302 02:47:34.663686 139590186821376 logging_writer.py:48] [467600] global_step=467600, grad_norm=4.839114665985107, loss=0.6581404209136963
I0302 02:48:08.615402 139590178428672 logging_writer.py:48] [467700] global_step=467700, grad_norm=4.592394828796387, loss=0.6556248068809509
I0302 02:48:42.526347 139590186821376 logging_writer.py:48] [467800] global_step=467800, grad_norm=4.5175886154174805, loss=0.6451589465141296
I0302 02:49:16.477452 139590178428672 logging_writer.py:48] [467900] global_step=467900, grad_norm=4.455342769622803, loss=0.6167240142822266
I0302 02:49:50.533114 139590186821376 logging_writer.py:48] [468000] global_step=468000, grad_norm=4.652411460876465, loss=0.6538342237472534
I0302 02:50:24.464031 139590178428672 logging_writer.py:48] [468100] global_step=468100, grad_norm=4.593379020690918, loss=0.6258121728897095
I0302 02:50:58.369795 139590186821376 logging_writer.py:48] [468200] global_step=468200, grad_norm=4.2648725509643555, loss=0.5801978707313538
I0302 02:51:32.301431 139590178428672 logging_writer.py:48] [468300] global_step=468300, grad_norm=4.641261100769043, loss=0.5924957990646362
I0302 02:52:06.283176 139590186821376 logging_writer.py:48] [468400] global_step=468400, grad_norm=4.387359142303467, loss=0.5957158803939819
I0302 02:52:40.247725 139590178428672 logging_writer.py:48] [468500] global_step=468500, grad_norm=4.52122688293457, loss=0.6200220584869385
I0302 02:53:14.216273 139590186821376 logging_writer.py:48] [468600] global_step=468600, grad_norm=4.612805366516113, loss=0.6394824385643005
I0302 02:53:48.140286 139590178428672 logging_writer.py:48] [468700] global_step=468700, grad_norm=4.569273471832275, loss=0.6428325176239014
I0302 02:54:22.026334 139590186821376 logging_writer.py:48] [468800] global_step=468800, grad_norm=4.191397666931152, loss=0.6211140155792236
I0302 02:54:50.686214 139753105983296 spec.py:321] Evaluating on the training split.
I0302 02:54:56.736953 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 02:55:05.105290 139753105983296 spec.py:349] Evaluating on the test split.
I0302 02:55:07.339064 139753105983296 submission_runner.py:411] Time since start: 164638.34s, 	Step: 468886, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14591769874095917, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.0558143854141235, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8203456401824951, 'test/num_examples': 10000, 'score': 159185.16336750984, 'total_duration': 164638.34228634834, 'accumulated_submission_time': 159185.16336750984, 'accumulated_eval_time': 5412.717334508896, 'accumulated_logging_time': 23.949141263961792}
I0302 02:55:07.433992 139589725452032 logging_writer.py:48] [468886] accumulated_eval_time=5412.717335, accumulated_logging_time=23.949141, accumulated_submission_time=159185.163368, global_step=468886, preemption_count=0, score=159185.163368, test/accuracy=0.630000, test/loss=1.820346, test/num_examples=10000, total_duration=164638.342286, train/accuracy=0.960658, train/loss=0.145918, validation/accuracy=0.754520, validation/loss=1.055814, validation/num_examples=50000
I0302 02:55:12.552765 139590144857856 logging_writer.py:48] [468900] global_step=468900, grad_norm=4.6135125160217285, loss=0.6377984285354614
I0302 02:55:46.417713 139589725452032 logging_writer.py:48] [469000] global_step=469000, grad_norm=4.80851411819458, loss=0.6439103484153748
I0302 02:56:20.410760 139590144857856 logging_writer.py:48] [469100] global_step=469100, grad_norm=4.362887859344482, loss=0.6240060925483704
I0302 02:56:54.356831 139589725452032 logging_writer.py:48] [469200] global_step=469200, grad_norm=4.502004146575928, loss=0.5837523937225342
I0302 02:57:28.314054 139590144857856 logging_writer.py:48] [469300] global_step=469300, grad_norm=4.145622730255127, loss=0.5813462138175964
I0302 02:58:02.244670 139589725452032 logging_writer.py:48] [469400] global_step=469400, grad_norm=4.910519599914551, loss=0.65256267786026
I0302 02:58:36.227840 139590144857856 logging_writer.py:48] [469500] global_step=469500, grad_norm=4.405111789703369, loss=0.6022868156433105
I0302 02:59:10.173721 139589725452032 logging_writer.py:48] [469600] global_step=469600, grad_norm=4.719533443450928, loss=0.659812867641449
I0302 02:59:44.105878 139590144857856 logging_writer.py:48] [469700] global_step=469700, grad_norm=5.013726711273193, loss=0.6344220042228699
I0302 03:00:18.038562 139589725452032 logging_writer.py:48] [469800] global_step=469800, grad_norm=4.720258712768555, loss=0.6551415920257568
I0302 03:00:51.985613 139590144857856 logging_writer.py:48] [469900] global_step=469900, grad_norm=4.616366386413574, loss=0.644221842288971
I0302 03:01:25.913722 139589725452032 logging_writer.py:48] [470000] global_step=470000, grad_norm=4.423070430755615, loss=0.6172167658805847
I0302 03:01:59.863547 139590144857856 logging_writer.py:48] [470100] global_step=470100, grad_norm=4.564639091491699, loss=0.6440877914428711
I0302 03:02:33.925135 139589725452032 logging_writer.py:48] [470200] global_step=470200, grad_norm=4.733328819274902, loss=0.6705282330513
I0302 03:03:07.867326 139590144857856 logging_writer.py:48] [470300] global_step=470300, grad_norm=4.4672136306762695, loss=0.5860652327537537
I0302 03:03:37.545110 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:03:43.562796 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:03:51.795239 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:03:54.054392 139753105983296 submission_runner.py:411] Time since start: 165165.06s, 	Step: 470389, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14633849263191223, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.055783748626709, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.819016456604004, 'test/num_examples': 10000, 'score': 159695.21085381508, 'total_duration': 165165.05762171745, 'accumulated_submission_time': 159695.21085381508, 'accumulated_eval_time': 5429.2265625, 'accumulated_logging_time': 24.054170608520508}
I0302 03:03:54.147725 139589717059328 logging_writer.py:48] [470389] accumulated_eval_time=5429.226562, accumulated_logging_time=24.054171, accumulated_submission_time=159695.210854, global_step=470389, preemption_count=0, score=159695.210854, test/accuracy=0.630100, test/loss=1.819016, test/num_examples=10000, total_duration=165165.057622, train/accuracy=0.960280, train/loss=0.146338, validation/accuracy=0.754980, validation/loss=1.055784, validation/num_examples=50000
I0302 03:03:58.232453 139589725452032 logging_writer.py:48] [470400] global_step=470400, grad_norm=4.933287143707275, loss=0.6402313709259033
I0302 03:04:32.139210 139589717059328 logging_writer.py:48] [470500] global_step=470500, grad_norm=4.536186695098877, loss=0.6390542387962341
I0302 03:05:06.054651 139589725452032 logging_writer.py:48] [470600] global_step=470600, grad_norm=4.739933490753174, loss=0.5366061925888062
I0302 03:05:39.993313 139589717059328 logging_writer.py:48] [470700] global_step=470700, grad_norm=4.780167102813721, loss=0.6672347187995911
I0302 03:06:13.956093 139589725452032 logging_writer.py:48] [470800] global_step=470800, grad_norm=4.372766971588135, loss=0.6177037358283997
I0302 03:06:47.849982 139589717059328 logging_writer.py:48] [470900] global_step=470900, grad_norm=4.694869041442871, loss=0.5749213099479675
I0302 03:07:21.779980 139589725452032 logging_writer.py:48] [471000] global_step=471000, grad_norm=4.3362202644348145, loss=0.6155096292495728
I0302 03:07:55.718522 139589717059328 logging_writer.py:48] [471100] global_step=471100, grad_norm=4.444323539733887, loss=0.647550106048584
I0302 03:08:29.705909 139589725452032 logging_writer.py:48] [471200] global_step=471200, grad_norm=4.66510009765625, loss=0.629451334476471
I0302 03:09:03.669504 139589717059328 logging_writer.py:48] [471300] global_step=471300, grad_norm=4.051562786102295, loss=0.5552549958229065
I0302 03:09:37.653924 139589725452032 logging_writer.py:48] [471400] global_step=471400, grad_norm=4.62997579574585, loss=0.6865822076797485
I0302 03:10:11.601287 139589717059328 logging_writer.py:48] [471500] global_step=471500, grad_norm=4.708894729614258, loss=0.6391501426696777
I0302 03:10:45.543015 139589725452032 logging_writer.py:48] [471600] global_step=471600, grad_norm=4.965396881103516, loss=0.7187565565109253
I0302 03:11:19.487565 139589717059328 logging_writer.py:48] [471700] global_step=471700, grad_norm=4.912416458129883, loss=0.6746183633804321
I0302 03:11:53.434687 139589725452032 logging_writer.py:48] [471800] global_step=471800, grad_norm=4.616433143615723, loss=0.6125717163085938
I0302 03:12:24.122047 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:12:30.120996 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:12:38.315945 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:12:40.589872 139753105983296 submission_runner.py:411] Time since start: 165691.59s, 	Step: 471892, 	{'train/accuracy': 0.9616948366165161, 'train/loss': 0.1453837901353836, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0557386875152588, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8196965456008911, 'test/num_examples': 10000, 'score': 160205.1214659214, 'total_duration': 165691.59308218956, 'accumulated_submission_time': 160205.1214659214, 'accumulated_eval_time': 5445.694318056107, 'accumulated_logging_time': 24.158228158950806}
I0302 03:12:40.693114 139590170035968 logging_writer.py:48] [471892] accumulated_eval_time=5445.694318, accumulated_logging_time=24.158228, accumulated_submission_time=160205.121466, global_step=471892, preemption_count=0, score=160205.121466, test/accuracy=0.630000, test/loss=1.819697, test/num_examples=10000, total_duration=165691.593082, train/accuracy=0.961695, train/loss=0.145384, validation/accuracy=0.754880, validation/loss=1.055739, validation/num_examples=50000
I0302 03:12:43.762170 139590178428672 logging_writer.py:48] [471900] global_step=471900, grad_norm=4.832509517669678, loss=0.6184825301170349
I0302 03:13:17.661124 139590170035968 logging_writer.py:48] [472000] global_step=472000, grad_norm=4.807911396026611, loss=0.6154745817184448
I0302 03:13:51.595964 139590178428672 logging_writer.py:48] [472100] global_step=472100, grad_norm=4.615966320037842, loss=0.6779488921165466
I0302 03:14:25.577190 139590170035968 logging_writer.py:48] [472200] global_step=472200, grad_norm=4.505924224853516, loss=0.6096489429473877
I0302 03:14:59.603762 139590178428672 logging_writer.py:48] [472300] global_step=472300, grad_norm=5.071249961853027, loss=0.7152225971221924
I0302 03:15:33.548581 139590170035968 logging_writer.py:48] [472400] global_step=472400, grad_norm=4.752769947052002, loss=0.630038321018219
I0302 03:16:07.513636 139590178428672 logging_writer.py:48] [472500] global_step=472500, grad_norm=4.229182243347168, loss=0.5225917100906372
I0302 03:16:41.492855 139590170035968 logging_writer.py:48] [472600] global_step=472600, grad_norm=3.9747633934020996, loss=0.5485945343971252
I0302 03:17:15.473839 139590178428672 logging_writer.py:48] [472700] global_step=472700, grad_norm=4.329615592956543, loss=0.6319494247436523
I0302 03:17:49.446728 139590170035968 logging_writer.py:48] [472800] global_step=472800, grad_norm=4.200072288513184, loss=0.5522372722625732
I0302 03:18:23.385545 139590178428672 logging_writer.py:48] [472900] global_step=472900, grad_norm=4.987361907958984, loss=0.630139946937561
I0302 03:18:57.341550 139590170035968 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.702504634857178, loss=0.5725620985031128
I0302 03:19:31.289328 139590178428672 logging_writer.py:48] [473100] global_step=473100, grad_norm=4.510390281677246, loss=0.6022881865501404
I0302 03:20:05.251230 139590170035968 logging_writer.py:48] [473200] global_step=473200, grad_norm=4.305400848388672, loss=0.6430752873420715
I0302 03:20:39.220951 139590178428672 logging_writer.py:48] [473300] global_step=473300, grad_norm=4.427530288696289, loss=0.5803067684173584
I0302 03:21:10.698067 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:21:16.761198 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:21:24.922003 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:21:27.242558 139753105983296 submission_runner.py:411] Time since start: 166218.25s, 	Step: 473394, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.1482415646314621, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.055686593055725, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8173385858535767, 'test/num_examples': 10000, 'score': 160715.06251072884, 'total_duration': 166218.24579143524, 'accumulated_submission_time': 160715.06251072884, 'accumulated_eval_time': 5462.238760948181, 'accumulated_logging_time': 24.271573543548584}
I0302 03:21:27.336222 139590161643264 logging_writer.py:48] [473394] accumulated_eval_time=5462.238761, accumulated_logging_time=24.271574, accumulated_submission_time=160715.062511, global_step=473394, preemption_count=0, score=160715.062511, test/accuracy=0.630700, test/loss=1.817339, test/num_examples=10000, total_duration=166218.245791, train/accuracy=0.960260, train/loss=0.148242, validation/accuracy=0.755060, validation/loss=1.055687, validation/num_examples=50000
I0302 03:21:29.715959 139590186821376 logging_writer.py:48] [473400] global_step=473400, grad_norm=4.776554584503174, loss=0.6809009313583374
I0302 03:22:03.624920 139590161643264 logging_writer.py:48] [473500] global_step=473500, grad_norm=4.7769269943237305, loss=0.68115234375
I0302 03:22:37.573510 139590186821376 logging_writer.py:48] [473600] global_step=473600, grad_norm=4.326913356781006, loss=0.5877760648727417
I0302 03:23:11.518215 139590161643264 logging_writer.py:48] [473700] global_step=473700, grad_norm=4.473325729370117, loss=0.6171855330467224
I0302 03:23:45.474343 139590186821376 logging_writer.py:48] [473800] global_step=473800, grad_norm=4.996452331542969, loss=0.7041149139404297
I0302 03:24:19.463525 139590161643264 logging_writer.py:48] [473900] global_step=473900, grad_norm=4.372721195220947, loss=0.5844024419784546
I0302 03:24:53.404060 139590186821376 logging_writer.py:48] [474000] global_step=474000, grad_norm=4.283492565155029, loss=0.6410123705863953
I0302 03:25:27.322648 139590161643264 logging_writer.py:48] [474100] global_step=474100, grad_norm=4.279615879058838, loss=0.6254273653030396
I0302 03:26:01.246500 139590186821376 logging_writer.py:48] [474200] global_step=474200, grad_norm=4.263339042663574, loss=0.5925373435020447
I0302 03:26:35.187231 139590161643264 logging_writer.py:48] [474300] global_step=474300, grad_norm=4.741440773010254, loss=0.6029351949691772
I0302 03:27:09.189353 139590186821376 logging_writer.py:48] [474400] global_step=474400, grad_norm=4.561340808868408, loss=0.6497256755828857
I0302 03:27:43.147838 139590161643264 logging_writer.py:48] [474500] global_step=474500, grad_norm=4.566822052001953, loss=0.6457803845405579
I0302 03:28:17.077350 139590186821376 logging_writer.py:48] [474600] global_step=474600, grad_norm=4.681765079498291, loss=0.6697857975959778
I0302 03:28:51.015114 139590161643264 logging_writer.py:48] [474700] global_step=474700, grad_norm=4.230012893676758, loss=0.6112686395645142
I0302 03:29:24.933964 139590186821376 logging_writer.py:48] [474800] global_step=474800, grad_norm=4.598315238952637, loss=0.6787594556808472
I0302 03:29:57.316689 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:30:03.440187 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:30:11.692598 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:30:13.933697 139753105983296 submission_runner.py:411] Time since start: 166744.94s, 	Step: 474897, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.14620094001293182, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0547475814819336, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8180257081985474, 'test/num_examples': 10000, 'score': 161224.98149490356, 'total_duration': 166744.93678593636, 'accumulated_submission_time': 161224.98149490356, 'accumulated_eval_time': 5478.855576515198, 'accumulated_logging_time': 24.374639987945557}
I0302 03:30:14.027298 139589717059328 logging_writer.py:48] [474897] accumulated_eval_time=5478.855577, accumulated_logging_time=24.374640, accumulated_submission_time=161224.981495, global_step=474897, preemption_count=0, score=161224.981495, test/accuracy=0.631100, test/loss=1.818026, test/num_examples=10000, total_duration=166744.936786, train/accuracy=0.959961, train/loss=0.146201, validation/accuracy=0.754740, validation/loss=1.054748, validation/num_examples=50000
I0302 03:30:15.394443 139589725452032 logging_writer.py:48] [474900] global_step=474900, grad_norm=3.9641668796539307, loss=0.5813699960708618
I0302 03:30:49.339488 139589717059328 logging_writer.py:48] [475000] global_step=475000, grad_norm=4.958728313446045, loss=0.6238588094711304
I0302 03:31:23.278045 139589725452032 logging_writer.py:48] [475100] global_step=475100, grad_norm=4.417669773101807, loss=0.5841708183288574
I0302 03:31:57.226034 139589717059328 logging_writer.py:48] [475200] global_step=475200, grad_norm=4.474330902099609, loss=0.6319429874420166
I0302 03:32:31.205216 139589725452032 logging_writer.py:48] [475300] global_step=475300, grad_norm=5.179582118988037, loss=0.6966427564620972
I0302 03:33:05.184730 139589717059328 logging_writer.py:48] [475400] global_step=475400, grad_norm=4.579897403717041, loss=0.6318764090538025
I0302 03:33:39.234930 139589725452032 logging_writer.py:48] [475500] global_step=475500, grad_norm=4.583521366119385, loss=0.7180334329605103
I0302 03:34:13.203746 139589717059328 logging_writer.py:48] [475600] global_step=475600, grad_norm=4.689384937286377, loss=0.6093568205833435
I0302 03:34:47.174011 139589725452032 logging_writer.py:48] [475700] global_step=475700, grad_norm=4.783316135406494, loss=0.5683461427688599
I0302 03:35:21.126294 139589717059328 logging_writer.py:48] [475800] global_step=475800, grad_norm=4.36922025680542, loss=0.6853272318840027
I0302 03:35:55.080355 139589725452032 logging_writer.py:48] [475900] global_step=475900, grad_norm=4.765791416168213, loss=0.6325078010559082
I0302 03:36:29.058737 139589717059328 logging_writer.py:48] [476000] global_step=476000, grad_norm=4.342453479766846, loss=0.6283551454544067
I0302 03:37:03.032627 139589725452032 logging_writer.py:48] [476100] global_step=476100, grad_norm=3.987764358520508, loss=0.5216706991195679
I0302 03:37:37.000496 139589717059328 logging_writer.py:48] [476200] global_step=476200, grad_norm=4.592535972595215, loss=0.6286729574203491
I0302 03:38:10.979787 139589725452032 logging_writer.py:48] [476300] global_step=476300, grad_norm=4.2940897941589355, loss=0.5999380946159363
I0302 03:38:44.091974 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:38:50.120593 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:38:58.348988 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:39:00.629567 139753105983296 submission_runner.py:411] Time since start: 167271.63s, 	Step: 476399, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14753426611423492, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.056140422821045, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8198281526565552, 'test/num_examples': 10000, 'score': 161734.98222899437, 'total_duration': 167271.63280415535, 'accumulated_submission_time': 161734.98222899437, 'accumulated_eval_time': 5495.393129825592, 'accumulated_logging_time': 24.47951650619507}
I0302 03:39:00.726478 139590161643264 logging_writer.py:48] [476399] accumulated_eval_time=5495.393130, accumulated_logging_time=24.479517, accumulated_submission_time=161734.982229, global_step=476399, preemption_count=0, score=161734.982229, test/accuracy=0.629900, test/loss=1.819828, test/num_examples=10000, total_duration=167271.632804, train/accuracy=0.960519, train/loss=0.147534, validation/accuracy=0.754760, validation/loss=1.056140, validation/num_examples=50000
I0302 03:39:01.429669 139590170035968 logging_writer.py:48] [476400] global_step=476400, grad_norm=4.092146873474121, loss=0.6127933263778687
I0302 03:39:35.389706 139590161643264 logging_writer.py:48] [476500] global_step=476500, grad_norm=4.569879531860352, loss=0.6448312997817993
I0302 03:40:09.278112 139590170035968 logging_writer.py:48] [476600] global_step=476600, grad_norm=4.77041482925415, loss=0.6530281901359558
I0302 03:40:43.208166 139590161643264 logging_writer.py:48] [476700] global_step=476700, grad_norm=4.536249160766602, loss=0.665619969367981
I0302 03:41:17.176930 139590170035968 logging_writer.py:48] [476800] global_step=476800, grad_norm=4.076843738555908, loss=0.5872345566749573
I0302 03:41:51.109005 139590161643264 logging_writer.py:48] [476900] global_step=476900, grad_norm=4.916934490203857, loss=0.6489003896713257
I0302 03:42:25.039725 139590170035968 logging_writer.py:48] [477000] global_step=477000, grad_norm=4.363613605499268, loss=0.5951895713806152
I0302 03:42:58.978703 139590161643264 logging_writer.py:48] [477100] global_step=477100, grad_norm=4.061722755432129, loss=0.5478605628013611
I0302 03:43:32.940243 139590170035968 logging_writer.py:48] [477200] global_step=477200, grad_norm=4.453901767730713, loss=0.679751992225647
I0302 03:44:07.168610 139590161643264 logging_writer.py:48] [477300] global_step=477300, grad_norm=4.58022403717041, loss=0.616431474685669
I0302 03:44:41.118883 139590170035968 logging_writer.py:48] [477400] global_step=477400, grad_norm=4.787124156951904, loss=0.5780277252197266
I0302 03:45:15.056196 139590161643264 logging_writer.py:48] [477500] global_step=477500, grad_norm=4.560954570770264, loss=0.6415978074073792
I0302 03:45:49.034073 139590170035968 logging_writer.py:48] [477600] global_step=477600, grad_norm=4.712048530578613, loss=0.6546725034713745
I0302 03:46:22.950265 139590161643264 logging_writer.py:48] [477700] global_step=477700, grad_norm=4.328527927398682, loss=0.6612786054611206
I0302 03:46:56.857920 139590170035968 logging_writer.py:48] [477800] global_step=477800, grad_norm=4.420804500579834, loss=0.5931852459907532
I0302 03:47:30.791757 139590161643264 logging_writer.py:48] [477900] global_step=477900, grad_norm=4.787209987640381, loss=0.6477124691009521
I0302 03:47:30.800380 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:47:36.836555 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:47:45.238667 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:47:47.537014 139753105983296 submission_runner.py:411] Time since start: 167798.54s, 	Step: 477901, 	{'train/accuracy': 0.9598014950752258, 'train/loss': 0.14899609982967377, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0541421175003052, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.815615177154541, 'test/num_examples': 10000, 'score': 162244.99106502533, 'total_duration': 167798.54024600983, 'accumulated_submission_time': 162244.99106502533, 'accumulated_eval_time': 5512.12969326973, 'accumulated_logging_time': 24.58859372138977}
I0302 03:47:47.634523 139589708666624 logging_writer.py:48] [477901] accumulated_eval_time=5512.129693, accumulated_logging_time=24.588594, accumulated_submission_time=162244.991065, global_step=477901, preemption_count=0, score=162244.991065, test/accuracy=0.630900, test/loss=1.815615, test/num_examples=10000, total_duration=167798.540246, train/accuracy=0.959801, train/loss=0.148996, validation/accuracy=0.754880, validation/loss=1.054142, validation/num_examples=50000
I0302 03:48:21.574389 139590144857856 logging_writer.py:48] [478000] global_step=478000, grad_norm=3.988713026046753, loss=0.498512327671051
I0302 03:48:55.479911 139589708666624 logging_writer.py:48] [478100] global_step=478100, grad_norm=4.553509712219238, loss=0.580461323261261
I0302 03:49:29.424886 139590144857856 logging_writer.py:48] [478200] global_step=478200, grad_norm=4.395310878753662, loss=0.5736479759216309
I0302 03:50:03.364975 139589708666624 logging_writer.py:48] [478300] global_step=478300, grad_norm=4.508459091186523, loss=0.5452585220336914
I0302 03:50:37.307541 139590144857856 logging_writer.py:48] [478400] global_step=478400, grad_norm=4.597637176513672, loss=0.6764572858810425
I0302 03:51:11.279169 139589708666624 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.229554176330566, loss=0.5720009803771973
I0302 03:51:45.324160 139590144857856 logging_writer.py:48] [478600] global_step=478600, grad_norm=4.639333724975586, loss=0.6514277458190918
I0302 03:52:19.268344 139589708666624 logging_writer.py:48] [478700] global_step=478700, grad_norm=4.533493518829346, loss=0.6557682752609253
I0302 03:52:53.242843 139590144857856 logging_writer.py:48] [478800] global_step=478800, grad_norm=4.2067036628723145, loss=0.6132130026817322
I0302 03:53:27.189001 139589708666624 logging_writer.py:48] [478900] global_step=478900, grad_norm=4.5499677658081055, loss=0.6746436953544617
I0302 03:54:01.137773 139590144857856 logging_writer.py:48] [479000] global_step=479000, grad_norm=4.388152122497559, loss=0.6617928147315979
I0302 03:54:35.078425 139589708666624 logging_writer.py:48] [479100] global_step=479100, grad_norm=4.79589319229126, loss=0.6163565516471863
I0302 03:55:09.034535 139590144857856 logging_writer.py:48] [479200] global_step=479200, grad_norm=4.265920639038086, loss=0.5956533551216125
I0302 03:55:42.990100 139589708666624 logging_writer.py:48] [479300] global_step=479300, grad_norm=4.532554626464844, loss=0.6292546987533569
I0302 03:56:16.917098 139590144857856 logging_writer.py:48] [479400] global_step=479400, grad_norm=4.646998882293701, loss=0.6013180017471313
I0302 03:56:17.741832 139753105983296 spec.py:321] Evaluating on the training split.
I0302 03:56:23.748823 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 03:56:32.014764 139753105983296 spec.py:349] Evaluating on the test split.
I0302 03:56:34.283833 139753105983296 submission_runner.py:411] Time since start: 168325.29s, 	Step: 479404, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14705008268356323, 'validation/accuracy': 0.7549999952316284, 'validation/loss': 1.056383728981018, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8202873468399048, 'test/num_examples': 10000, 'score': 162755.0349202156, 'total_duration': 168325.28706121445, 'accumulated_submission_time': 162755.0349202156, 'accumulated_eval_time': 5528.671658039093, 'accumulated_logging_time': 24.69663667678833}
I0302 03:56:34.381283 139590170035968 logging_writer.py:48] [479404] accumulated_eval_time=5528.671658, accumulated_logging_time=24.696637, accumulated_submission_time=162755.034920, global_step=479404, preemption_count=0, score=162755.034920, test/accuracy=0.629800, test/loss=1.820287, test/num_examples=10000, total_duration=168325.287061, train/accuracy=0.960698, train/loss=0.147050, validation/accuracy=0.755000, validation/loss=1.056384, validation/num_examples=50000
I0302 03:57:07.291666 139590178428672 logging_writer.py:48] [479500] global_step=479500, grad_norm=4.444680213928223, loss=0.5779136419296265
I0302 03:57:41.194481 139590170035968 logging_writer.py:48] [479600] global_step=479600, grad_norm=4.612618923187256, loss=0.6907477378845215
I0302 03:58:15.220660 139590178428672 logging_writer.py:48] [479700] global_step=479700, grad_norm=4.2912750244140625, loss=0.6013170480728149
I0302 03:58:49.158336 139590170035968 logging_writer.py:48] [479800] global_step=479800, grad_norm=4.398990154266357, loss=0.6563634872436523
I0302 03:59:23.118167 139590178428672 logging_writer.py:48] [479900] global_step=479900, grad_norm=4.510451316833496, loss=0.5999348759651184
I0302 03:59:57.050826 139590170035968 logging_writer.py:48] [480000] global_step=480000, grad_norm=4.331907749176025, loss=0.598743200302124
I0302 04:00:30.986193 139590178428672 logging_writer.py:48] [480100] global_step=480100, grad_norm=5.017056465148926, loss=0.6006070375442505
I0302 04:01:04.918244 139590170035968 logging_writer.py:48] [480200] global_step=480200, grad_norm=4.716225624084473, loss=0.6695061326026917
I0302 04:01:38.857989 139590178428672 logging_writer.py:48] [480300] global_step=480300, grad_norm=4.602456569671631, loss=0.6630475521087646
I0302 04:02:12.759579 139590170035968 logging_writer.py:48] [480400] global_step=480400, grad_norm=4.24315071105957, loss=0.6522742509841919
I0302 04:02:46.700374 139590178428672 logging_writer.py:48] [480500] global_step=480500, grad_norm=4.786201477050781, loss=0.6567262411117554
I0302 04:03:20.646056 139590170035968 logging_writer.py:48] [480600] global_step=480600, grad_norm=4.677649021148682, loss=0.5910847187042236
I0302 04:03:54.573205 139590178428672 logging_writer.py:48] [480700] global_step=480700, grad_norm=4.437899589538574, loss=0.5846744775772095
I0302 04:04:28.583491 139590170035968 logging_writer.py:48] [480800] global_step=480800, grad_norm=4.859798431396484, loss=0.6380723714828491
I0302 04:05:02.514271 139590178428672 logging_writer.py:48] [480900] global_step=480900, grad_norm=4.736668109893799, loss=0.7042428255081177
I0302 04:05:04.358836 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:05:10.368555 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:05:18.653834 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:05:20.938880 139753105983296 submission_runner.py:411] Time since start: 168851.94s, 	Step: 480907, 	{'train/accuracy': 0.9596220850944519, 'train/loss': 0.15105994045734406, 'validation/accuracy': 0.7543599605560303, 'validation/loss': 1.0561161041259766, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.8191709518432617, 'test/num_examples': 10000, 'score': 163264.94740009308, 'total_duration': 168851.94211268425, 'accumulated_submission_time': 163264.94740009308, 'accumulated_eval_time': 5545.251647233963, 'accumulated_logging_time': 24.805399894714355}
I0302 04:05:21.039019 139590161643264 logging_writer.py:48] [480907] accumulated_eval_time=5545.251647, accumulated_logging_time=24.805400, accumulated_submission_time=163264.947400, global_step=480907, preemption_count=0, score=163264.947400, test/accuracy=0.629500, test/loss=1.819171, test/num_examples=10000, total_duration=168851.942113, train/accuracy=0.959622, train/loss=0.151060, validation/accuracy=0.754360, validation/loss=1.056116, validation/num_examples=50000
I0302 04:05:52.929950 139590186821376 logging_writer.py:48] [481000] global_step=481000, grad_norm=4.219681262969971, loss=0.6095712184906006
I0302 04:06:26.857556 139590161643264 logging_writer.py:48] [481100] global_step=481100, grad_norm=5.088113307952881, loss=0.7307230830192566
I0302 04:07:00.801373 139590186821376 logging_writer.py:48] [481200] global_step=481200, grad_norm=4.766136646270752, loss=0.6240671277046204
I0302 04:07:34.729854 139590161643264 logging_writer.py:48] [481300] global_step=481300, grad_norm=4.877439022064209, loss=0.6665626168251038
I0302 04:08:08.691629 139590186821376 logging_writer.py:48] [481400] global_step=481400, grad_norm=4.6631574630737305, loss=0.669956624507904
I0302 04:08:42.617843 139590161643264 logging_writer.py:48] [481500] global_step=481500, grad_norm=4.249386787414551, loss=0.5791686177253723
I0302 04:09:16.527714 139590186821376 logging_writer.py:48] [481600] global_step=481600, grad_norm=4.540999889373779, loss=0.6650530099868774
I0302 04:09:50.463935 139590161643264 logging_writer.py:48] [481700] global_step=481700, grad_norm=4.663241386413574, loss=0.7195668816566467
I0302 04:10:24.599376 139590186821376 logging_writer.py:48] [481800] global_step=481800, grad_norm=4.754528045654297, loss=0.5958423614501953
I0302 04:10:58.552060 139590161643264 logging_writer.py:48] [481900] global_step=481900, grad_norm=4.745999813079834, loss=0.6110571026802063
I0302 04:11:32.535338 139590186821376 logging_writer.py:48] [482000] global_step=482000, grad_norm=4.713070869445801, loss=0.6283562183380127
I0302 04:12:06.498044 139590161643264 logging_writer.py:48] [482100] global_step=482100, grad_norm=4.7349138259887695, loss=0.6058924794197083
I0302 04:12:40.472188 139590186821376 logging_writer.py:48] [482200] global_step=482200, grad_norm=4.050816059112549, loss=0.6065673232078552
I0302 04:13:14.432610 139590161643264 logging_writer.py:48] [482300] global_step=482300, grad_norm=4.509664535522461, loss=0.588951587677002
I0302 04:13:48.395150 139590186821376 logging_writer.py:48] [482400] global_step=482400, grad_norm=4.225365161895752, loss=0.6144806146621704
I0302 04:13:51.255669 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:13:57.250601 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:14:05.571856 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:14:07.832018 139753105983296 submission_runner.py:411] Time since start: 169378.84s, 	Step: 482410, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14681753516197205, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.0554178953170776, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8195371627807617, 'test/num_examples': 10000, 'score': 163775.10062289238, 'total_duration': 169378.83524656296, 'accumulated_submission_time': 163775.10062289238, 'accumulated_eval_time': 5561.8279457092285, 'accumulated_logging_time': 24.915863752365112}
I0302 04:14:07.929350 139589708666624 logging_writer.py:48] [482410] accumulated_eval_time=5561.827946, accumulated_logging_time=24.915864, accumulated_submission_time=163775.100623, global_step=482410, preemption_count=0, score=163775.100623, test/accuracy=0.631000, test/loss=1.819537, test/num_examples=10000, total_duration=169378.835247, train/accuracy=0.960619, train/loss=0.146818, validation/accuracy=0.754920, validation/loss=1.055418, validation/num_examples=50000
I0302 04:14:38.801850 139589717059328 logging_writer.py:48] [482500] global_step=482500, grad_norm=4.626242160797119, loss=0.6678019762039185
I0302 04:15:12.730480 139589708666624 logging_writer.py:48] [482600] global_step=482600, grad_norm=4.738931179046631, loss=0.6102690696716309
I0302 04:15:46.681092 139589717059328 logging_writer.py:48] [482700] global_step=482700, grad_norm=4.356785774230957, loss=0.6230438947677612
I0302 04:16:20.634220 139589708666624 logging_writer.py:48] [482800] global_step=482800, grad_norm=4.597537994384766, loss=0.6088926792144775
I0302 04:16:54.668042 139589717059328 logging_writer.py:48] [482900] global_step=482900, grad_norm=4.261546611785889, loss=0.6102876663208008
I0302 04:17:28.591947 139589708666624 logging_writer.py:48] [483000] global_step=483000, grad_norm=4.478723526000977, loss=0.607644259929657
I0302 04:18:02.561133 139589717059328 logging_writer.py:48] [483100] global_step=483100, grad_norm=4.740333557128906, loss=0.6176162958145142
I0302 04:18:36.504295 139589708666624 logging_writer.py:48] [483200] global_step=483200, grad_norm=4.350610256195068, loss=0.6372977495193481
I0302 04:19:10.435973 139589717059328 logging_writer.py:48] [483300] global_step=483300, grad_norm=4.338303565979004, loss=0.5832909345626831
I0302 04:19:44.363178 139589708666624 logging_writer.py:48] [483400] global_step=483400, grad_norm=4.468110084533691, loss=0.6060156226158142
I0302 04:20:18.327499 139589717059328 logging_writer.py:48] [483500] global_step=483500, grad_norm=4.590846538543701, loss=0.6851255893707275
I0302 04:20:52.275370 139589708666624 logging_writer.py:48] [483600] global_step=483600, grad_norm=4.241175174713135, loss=0.6003453731536865
I0302 04:21:26.235272 139589717059328 logging_writer.py:48] [483700] global_step=483700, grad_norm=4.6672444343566895, loss=0.6129828095436096
I0302 04:22:00.173089 139589708666624 logging_writer.py:48] [483800] global_step=483800, grad_norm=4.292731285095215, loss=0.6404742002487183
I0302 04:22:34.110467 139589717059328 logging_writer.py:48] [483900] global_step=483900, grad_norm=4.445612907409668, loss=0.6092022657394409
I0302 04:22:38.091592 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:22:44.082498 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:22:52.376769 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:22:54.666059 139753105983296 submission_runner.py:411] Time since start: 169905.67s, 	Step: 483913, 	{'train/accuracy': 0.9611965417861938, 'train/loss': 0.1441107988357544, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0558921098709106, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8194700479507446, 'test/num_examples': 10000, 'score': 164285.20070242882, 'total_duration': 169905.66928815842, 'accumulated_submission_time': 164285.20070242882, 'accumulated_eval_time': 5578.402358531952, 'accumulated_logging_time': 25.023176670074463}
I0302 04:22:54.763513 139590178428672 logging_writer.py:48] [483913] accumulated_eval_time=5578.402359, accumulated_logging_time=25.023177, accumulated_submission_time=164285.200702, global_step=483913, preemption_count=0, score=164285.200702, test/accuracy=0.629600, test/loss=1.819470, test/num_examples=10000, total_duration=169905.669288, train/accuracy=0.961197, train/loss=0.144111, validation/accuracy=0.754660, validation/loss=1.055892, validation/num_examples=50000
I0302 04:23:24.593043 139590186821376 logging_writer.py:48] [484000] global_step=484000, grad_norm=4.376335620880127, loss=0.622674286365509
I0302 04:23:58.511054 139590178428672 logging_writer.py:48] [484100] global_step=484100, grad_norm=4.918313980102539, loss=0.643474280834198
I0302 04:24:32.448868 139590186821376 logging_writer.py:48] [484200] global_step=484200, grad_norm=4.301512241363525, loss=0.5900144577026367
I0302 04:25:06.382810 139590178428672 logging_writer.py:48] [484300] global_step=484300, grad_norm=4.415302276611328, loss=0.5043274164199829
I0302 04:25:40.320354 139590186821376 logging_writer.py:48] [484400] global_step=484400, grad_norm=4.4391069412231445, loss=0.6617986559867859
I0302 04:26:14.276220 139590178428672 logging_writer.py:48] [484500] global_step=484500, grad_norm=4.668586254119873, loss=0.6394717693328857
I0302 04:26:48.209856 139590186821376 logging_writer.py:48] [484600] global_step=484600, grad_norm=4.1348419189453125, loss=0.4968486428260803
I0302 04:27:22.119451 139590178428672 logging_writer.py:48] [484700] global_step=484700, grad_norm=4.539277076721191, loss=0.6393744945526123
I0302 04:27:56.039680 139590186821376 logging_writer.py:48] [484800] global_step=484800, grad_norm=4.780457973480225, loss=0.5588225722312927
I0302 04:28:29.985017 139590178428672 logging_writer.py:48] [484900] global_step=484900, grad_norm=4.375036716461182, loss=0.5696508288383484
I0302 04:29:03.984817 139590186821376 logging_writer.py:48] [485000] global_step=485000, grad_norm=4.497659206390381, loss=0.7362635135650635
I0302 04:29:37.920495 139590178428672 logging_writer.py:48] [485100] global_step=485100, grad_norm=4.480397701263428, loss=0.6849148273468018
I0302 04:30:11.851107 139590186821376 logging_writer.py:48] [485200] global_step=485200, grad_norm=4.240052700042725, loss=0.5896676778793335
I0302 04:30:45.771181 139590178428672 logging_writer.py:48] [485300] global_step=485300, grad_norm=4.618766784667969, loss=0.6625521183013916
I0302 04:31:19.703537 139590186821376 logging_writer.py:48] [485400] global_step=485400, grad_norm=4.1155805587768555, loss=0.5552558898925781
I0302 04:31:24.953100 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:31:30.978529 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:31:39.266649 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:31:41.534256 139753105983296 submission_runner.py:411] Time since start: 170432.54s, 	Step: 485417, 	{'train/accuracy': 0.9585259556770325, 'train/loss': 0.14979276061058044, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.0551444292068481, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8180348873138428, 'test/num_examples': 10000, 'score': 164795.32609844208, 'total_duration': 170432.5374853611, 'accumulated_submission_time': 164795.32609844208, 'accumulated_eval_time': 5594.983488559723, 'accumulated_logging_time': 25.13081169128418}
I0302 04:31:41.634197 139590153250560 logging_writer.py:48] [485417] accumulated_eval_time=5594.983489, accumulated_logging_time=25.130812, accumulated_submission_time=164795.326098, global_step=485417, preemption_count=0, score=164795.326098, test/accuracy=0.630900, test/loss=1.818035, test/num_examples=10000, total_duration=170432.537485, train/accuracy=0.958526, train/loss=0.149793, validation/accuracy=0.754720, validation/loss=1.055144, validation/num_examples=50000
I0302 04:32:10.090908 139590161643264 logging_writer.py:48] [485500] global_step=485500, grad_norm=4.440897464752197, loss=0.5845698714256287
I0302 04:32:43.970570 139590153250560 logging_writer.py:48] [485600] global_step=485600, grad_norm=4.623741626739502, loss=0.5825823545455933
I0302 04:33:17.899876 139590161643264 logging_writer.py:48] [485700] global_step=485700, grad_norm=4.5272650718688965, loss=0.6279522180557251
I0302 04:33:51.817249 139590153250560 logging_writer.py:48] [485800] global_step=485800, grad_norm=4.629924774169922, loss=0.6598221063613892
I0302 04:34:25.744433 139590161643264 logging_writer.py:48] [485900] global_step=485900, grad_norm=4.325462341308594, loss=0.6023404598236084
I0302 04:34:59.715730 139590153250560 logging_writer.py:48] [486000] global_step=486000, grad_norm=5.0979437828063965, loss=0.6496990323066711
I0302 04:35:33.829615 139590161643264 logging_writer.py:48] [486100] global_step=486100, grad_norm=4.715453624725342, loss=0.6523127555847168
I0302 04:36:07.719255 139590153250560 logging_writer.py:48] [486200] global_step=486200, grad_norm=4.783864974975586, loss=0.5840790867805481
I0302 04:36:41.665721 139590161643264 logging_writer.py:48] [486300] global_step=486300, grad_norm=4.59492826461792, loss=0.6524852514266968
I0302 04:37:15.608995 139590153250560 logging_writer.py:48] [486400] global_step=486400, grad_norm=5.047608375549316, loss=0.6428613066673279
I0302 04:37:49.534809 139590161643264 logging_writer.py:48] [486500] global_step=486500, grad_norm=4.40792179107666, loss=0.601618766784668
I0302 04:38:23.429461 139590153250560 logging_writer.py:48] [486600] global_step=486600, grad_norm=4.196035861968994, loss=0.5776437520980835
I0302 04:38:57.391294 139590161643264 logging_writer.py:48] [486700] global_step=486700, grad_norm=4.3613505363464355, loss=0.5423996448516846
I0302 04:39:31.325205 139590153250560 logging_writer.py:48] [486800] global_step=486800, grad_norm=4.9391303062438965, loss=0.6764326691627502
I0302 04:40:05.267225 139590161643264 logging_writer.py:48] [486900] global_step=486900, grad_norm=4.538414478302002, loss=0.5755763649940491
I0302 04:40:11.839231 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:40:17.843445 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:40:26.110475 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:40:28.401700 139753105983296 submission_runner.py:411] Time since start: 170959.40s, 	Step: 486921, 	{'train/accuracy': 0.9615553021430969, 'train/loss': 0.14339344203472137, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.0567063093185425, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8195090293884277, 'test/num_examples': 10000, 'score': 165305.46842598915, 'total_duration': 170959.40493369102, 'accumulated_submission_time': 165305.46842598915, 'accumulated_eval_time': 5611.545911312103, 'accumulated_logging_time': 25.240298748016357}
I0302 04:40:28.500988 139589717059328 logging_writer.py:48] [486921] accumulated_eval_time=5611.545911, accumulated_logging_time=25.240299, accumulated_submission_time=165305.468426, global_step=486921, preemption_count=0, score=165305.468426, test/accuracy=0.631100, test/loss=1.819509, test/num_examples=10000, total_duration=170959.404934, train/accuracy=0.961555, train/loss=0.143393, validation/accuracy=0.754740, validation/loss=1.056706, validation/num_examples=50000
I0302 04:40:55.607156 139589725452032 logging_writer.py:48] [487000] global_step=487000, grad_norm=4.253830432891846, loss=0.5596091747283936
I0302 04:41:29.712535 139589717059328 logging_writer.py:48] [487100] global_step=487100, grad_norm=5.077206611633301, loss=0.6465476751327515
I0302 04:42:03.667185 139589725452032 logging_writer.py:48] [487200] global_step=487200, grad_norm=4.749892711639404, loss=0.6723484396934509
I0302 04:42:37.634223 139589717059328 logging_writer.py:48] [487300] global_step=487300, grad_norm=4.202512741088867, loss=0.6177377700805664
I0302 04:43:11.596806 139589725452032 logging_writer.py:48] [487400] global_step=487400, grad_norm=4.427300930023193, loss=0.5900532007217407
I0302 04:43:45.523192 139589717059328 logging_writer.py:48] [487500] global_step=487500, grad_norm=4.997274398803711, loss=0.6972042322158813
I0302 04:44:19.454021 139589725452032 logging_writer.py:48] [487600] global_step=487600, grad_norm=4.4667229652404785, loss=0.5974190831184387
I0302 04:44:53.392735 139589717059328 logging_writer.py:48] [487700] global_step=487700, grad_norm=4.918332099914551, loss=0.6784461736679077
I0302 04:45:27.332188 139589725452032 logging_writer.py:48] [487800] global_step=487800, grad_norm=4.113088130950928, loss=0.5988519787788391
I0302 04:46:01.274321 139589717059328 logging_writer.py:48] [487900] global_step=487900, grad_norm=4.525140285491943, loss=0.5583122968673706
I0302 04:46:35.241828 139589725452032 logging_writer.py:48] [488000] global_step=488000, grad_norm=4.373252868652344, loss=0.6769945025444031
I0302 04:47:09.197072 139589717059328 logging_writer.py:48] [488100] global_step=488100, grad_norm=4.348942279815674, loss=0.6222705245018005
I0302 04:47:43.206468 139589725452032 logging_writer.py:48] [488200] global_step=488200, grad_norm=4.894864082336426, loss=0.6875073909759521
I0302 04:48:17.177356 139589717059328 logging_writer.py:48] [488300] global_step=488300, grad_norm=4.532741546630859, loss=0.5601791739463806
I0302 04:48:51.131515 139589725452032 logging_writer.py:48] [488400] global_step=488400, grad_norm=4.471933364868164, loss=0.6243710517883301
I0302 04:48:58.428445 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:49:04.565302 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:49:12.766862 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:49:15.031062 139753105983296 submission_runner.py:411] Time since start: 171486.03s, 	Step: 488423, 	{'train/accuracy': 0.9604790806770325, 'train/loss': 0.1482679396867752, 'validation/accuracy': 0.7546199560165405, 'validation/loss': 1.0550260543823242, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8203799724578857, 'test/num_examples': 10000, 'score': 165815.3304874897, 'total_duration': 171486.03428840637, 'accumulated_submission_time': 165815.3304874897, 'accumulated_eval_time': 5628.148483276367, 'accumulated_logging_time': 25.350980043411255}
I0302 04:49:15.131559 139590178428672 logging_writer.py:48] [488423] accumulated_eval_time=5628.148483, accumulated_logging_time=25.350980, accumulated_submission_time=165815.330487, global_step=488423, preemption_count=0, score=165815.330487, test/accuracy=0.629700, test/loss=1.820380, test/num_examples=10000, total_duration=171486.034288, train/accuracy=0.960479, train/loss=0.148268, validation/accuracy=0.754620, validation/loss=1.055026, validation/num_examples=50000
I0302 04:49:41.574197 139590186821376 logging_writer.py:48] [488500] global_step=488500, grad_norm=4.7889909744262695, loss=0.6905145645141602
I0302 04:50:15.509330 139590178428672 logging_writer.py:48] [488600] global_step=488600, grad_norm=4.98171329498291, loss=0.628065824508667
I0302 04:50:49.426919 139590186821376 logging_writer.py:48] [488700] global_step=488700, grad_norm=4.372802734375, loss=0.5917617678642273
I0302 04:51:23.372741 139590178428672 logging_writer.py:48] [488800] global_step=488800, grad_norm=4.785152912139893, loss=0.6120177507400513
I0302 04:51:57.319720 139590186821376 logging_writer.py:48] [488900] global_step=488900, grad_norm=4.566945552825928, loss=0.5879924893379211
I0302 04:52:31.253857 139590178428672 logging_writer.py:48] [489000] global_step=489000, grad_norm=4.370153903961182, loss=0.5795862674713135
I0302 04:53:05.201497 139590186821376 logging_writer.py:48] [489100] global_step=489100, grad_norm=4.318577766418457, loss=0.6518935561180115
I0302 04:53:39.277275 139590178428672 logging_writer.py:48] [489200] global_step=489200, grad_norm=4.534053325653076, loss=0.5469487905502319
I0302 04:54:13.228622 139590186821376 logging_writer.py:48] [489300] global_step=489300, grad_norm=4.120802879333496, loss=0.6206634640693665
I0302 04:54:47.173729 139590178428672 logging_writer.py:48] [489400] global_step=489400, grad_norm=4.854889392852783, loss=0.6930490136146545
I0302 04:55:21.086683 139590186821376 logging_writer.py:48] [489500] global_step=489500, grad_norm=4.222372055053711, loss=0.5911487936973572
I0302 04:55:55.012387 139590178428672 logging_writer.py:48] [489600] global_step=489600, grad_norm=4.5983452796936035, loss=0.609226405620575
I0302 04:56:28.946959 139590186821376 logging_writer.py:48] [489700] global_step=489700, grad_norm=4.602273941040039, loss=0.5985174775123596
I0302 04:57:02.895292 139590178428672 logging_writer.py:48] [489800] global_step=489800, grad_norm=4.616477012634277, loss=0.6580823063850403
I0302 04:57:36.839393 139590186821376 logging_writer.py:48] [489900] global_step=489900, grad_norm=4.230660915374756, loss=0.6112281084060669
I0302 04:57:45.143604 139753105983296 spec.py:321] Evaluating on the training split.
I0302 04:57:51.163638 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 04:57:59.580065 139753105983296 spec.py:349] Evaluating on the test split.
I0302 04:58:01.833352 139753105983296 submission_runner.py:411] Time since start: 172012.84s, 	Step: 489926, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.1450057327747345, 'validation/accuracy': 0.7552199959754944, 'validation/loss': 1.0549418926239014, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8186701536178589, 'test/num_examples': 10000, 'score': 166325.27960824966, 'total_duration': 172012.83657836914, 'accumulated_submission_time': 166325.27960824966, 'accumulated_eval_time': 5644.838176488876, 'accumulated_logging_time': 25.461592197418213}
I0302 04:58:01.934715 139590161643264 logging_writer.py:48] [489926] accumulated_eval_time=5644.838176, accumulated_logging_time=25.461592, accumulated_submission_time=166325.279608, global_step=489926, preemption_count=0, score=166325.279608, test/accuracy=0.630500, test/loss=1.818670, test/num_examples=10000, total_duration=172012.836578, train/accuracy=0.961396, train/loss=0.145006, validation/accuracy=0.755220, validation/loss=1.054942, validation/num_examples=50000
I0302 04:58:27.359759 139590170035968 logging_writer.py:48] [490000] global_step=490000, grad_norm=4.791656970977783, loss=0.6587551832199097
I0302 04:59:01.286418 139590161643264 logging_writer.py:48] [490100] global_step=490100, grad_norm=4.333276271820068, loss=0.5985405445098877
I0302 04:59:35.248566 139590170035968 logging_writer.py:48] [490200] global_step=490200, grad_norm=4.473752498626709, loss=0.6218289732933044
I0302 05:00:09.302301 139590161643264 logging_writer.py:48] [490300] global_step=490300, grad_norm=4.236987113952637, loss=0.558922290802002
I0302 05:00:43.246961 139590170035968 logging_writer.py:48] [490400] global_step=490400, grad_norm=4.6631388664245605, loss=0.6107265949249268
I0302 05:01:17.235554 139590161643264 logging_writer.py:48] [490500] global_step=490500, grad_norm=4.2050981521606445, loss=0.5823363661766052
I0302 05:01:51.176989 139590170035968 logging_writer.py:48] [490600] global_step=490600, grad_norm=4.426661014556885, loss=0.5520921945571899
I0302 05:02:25.127589 139590161643264 logging_writer.py:48] [490700] global_step=490700, grad_norm=4.860509872436523, loss=0.6183604001998901
I0302 05:02:59.085486 139590170035968 logging_writer.py:48] [490800] global_step=490800, grad_norm=4.212010383605957, loss=0.6239960193634033
I0302 05:03:33.052267 139590161643264 logging_writer.py:48] [490900] global_step=490900, grad_norm=4.5340256690979, loss=0.6280686259269714
I0302 05:04:07.040122 139590170035968 logging_writer.py:48] [491000] global_step=491000, grad_norm=4.487118244171143, loss=0.6442890763282776
I0302 05:04:41.016443 139590161643264 logging_writer.py:48] [491100] global_step=491100, grad_norm=4.576570987701416, loss=0.6576504707336426
I0302 05:05:15.005408 139590170035968 logging_writer.py:48] [491200] global_step=491200, grad_norm=4.52494478225708, loss=0.6442121863365173
I0302 05:05:49.030534 139590161643264 logging_writer.py:48] [491300] global_step=491300, grad_norm=4.629593849182129, loss=0.6249898672103882
I0302 05:06:22.993917 139590170035968 logging_writer.py:48] [491400] global_step=491400, grad_norm=4.689288139343262, loss=0.6368967294692993
I0302 05:06:31.952840 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:06:37.980636 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:06:46.267764 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:06:48.533571 139753105983296 submission_runner.py:411] Time since start: 172539.54s, 	Step: 491428, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14781656861305237, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.055351972579956, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.818878173828125, 'test/num_examples': 10000, 'score': 166835.23479795456, 'total_duration': 172539.53677415848, 'accumulated_submission_time': 166835.23479795456, 'accumulated_eval_time': 5661.418847084045, 'accumulated_logging_time': 25.57334852218628}
I0302 05:06:48.639490 139590144857856 logging_writer.py:48] [491428] accumulated_eval_time=5661.418847, accumulated_logging_time=25.573349, accumulated_submission_time=166835.234798, global_step=491428, preemption_count=0, score=166835.234798, test/accuracy=0.629900, test/loss=1.818878, test/num_examples=10000, total_duration=172539.536774, train/accuracy=0.960340, train/loss=0.147817, validation/accuracy=0.754840, validation/loss=1.055352, validation/num_examples=50000
I0302 05:07:13.368299 139590153250560 logging_writer.py:48] [491500] global_step=491500, grad_norm=4.780458450317383, loss=0.6458179354667664
I0302 05:07:47.260752 139590144857856 logging_writer.py:48] [491600] global_step=491600, grad_norm=4.495977401733398, loss=0.5668248534202576
I0302 05:08:21.246624 139590153250560 logging_writer.py:48] [491700] global_step=491700, grad_norm=4.98423957824707, loss=0.6343211531639099
I0302 05:08:55.172153 139590144857856 logging_writer.py:48] [491800] global_step=491800, grad_norm=4.545475959777832, loss=0.6046040654182434
I0302 05:09:29.087310 139590153250560 logging_writer.py:48] [491900] global_step=491900, grad_norm=4.411980628967285, loss=0.5897784233093262
I0302 05:10:03.047837 139590144857856 logging_writer.py:48] [492000] global_step=492000, grad_norm=4.586620807647705, loss=0.5991491079330444
I0302 05:10:36.994670 139590153250560 logging_writer.py:48] [492100] global_step=492100, grad_norm=4.712201118469238, loss=0.610322892665863
I0302 05:11:10.926231 139590144857856 logging_writer.py:48] [492200] global_step=492200, grad_norm=4.546975135803223, loss=0.5888413190841675
I0302 05:11:44.911168 139590153250560 logging_writer.py:48] [492300] global_step=492300, grad_norm=4.279239654541016, loss=0.6536273956298828
I0302 05:12:18.925412 139590144857856 logging_writer.py:48] [492400] global_step=492400, grad_norm=4.847093105316162, loss=0.6747477650642395
I0302 05:12:52.901705 139590153250560 logging_writer.py:48] [492500] global_step=492500, grad_norm=4.5604753494262695, loss=0.6648110151290894
I0302 05:13:26.845211 139590144857856 logging_writer.py:48] [492600] global_step=492600, grad_norm=4.6238908767700195, loss=0.6321880221366882
I0302 05:14:00.787170 139590153250560 logging_writer.py:48] [492700] global_step=492700, grad_norm=4.727695941925049, loss=0.6370916962623596
I0302 05:14:34.736986 139590144857856 logging_writer.py:48] [492800] global_step=492800, grad_norm=4.704227447509766, loss=0.6340534687042236
I0302 05:15:08.679217 139590153250560 logging_writer.py:48] [492900] global_step=492900, grad_norm=4.392617702484131, loss=0.6654293537139893
I0302 05:15:18.671836 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:15:24.698538 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:15:32.907763 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:15:35.206663 139753105983296 submission_runner.py:411] Time since start: 173066.21s, 	Step: 492931, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14278019964694977, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 1.0560132265090942, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8192788362503052, 'test/num_examples': 10000, 'score': 167345.2049202919, 'total_duration': 173066.20989394188, 'accumulated_submission_time': 167345.2049202919, 'accumulated_eval_time': 5677.953634738922, 'accumulated_logging_time': 25.689466953277588}
I0302 05:15:35.317032 139590161643264 logging_writer.py:48] [492931] accumulated_eval_time=5677.953635, accumulated_logging_time=25.689467, accumulated_submission_time=167345.204920, global_step=492931, preemption_count=0, score=167345.204920, test/accuracy=0.630300, test/loss=1.819279, test/num_examples=10000, total_duration=173066.209894, train/accuracy=0.961834, train/loss=0.142780, validation/accuracy=0.754660, validation/loss=1.056013, validation/num_examples=50000
I0302 05:15:59.079118 139590170035968 logging_writer.py:48] [493000] global_step=493000, grad_norm=4.285786151885986, loss=0.6275659799575806
I0302 05:16:32.976577 139590161643264 logging_writer.py:48] [493100] global_step=493100, grad_norm=4.568181991577148, loss=0.6212998032569885
I0302 05:17:06.901047 139590170035968 logging_writer.py:48] [493200] global_step=493200, grad_norm=4.426967144012451, loss=0.6089684367179871
I0302 05:17:40.834786 139590161643264 logging_writer.py:48] [493300] global_step=493300, grad_norm=5.484103679656982, loss=0.6778925657272339
I0302 05:18:14.878365 139590170035968 logging_writer.py:48] [493400] global_step=493400, grad_norm=4.386788845062256, loss=0.6725953817367554
I0302 05:18:48.846993 139590161643264 logging_writer.py:48] [493500] global_step=493500, grad_norm=4.653014183044434, loss=0.6622527241706848
I0302 05:19:22.813018 139590170035968 logging_writer.py:48] [493600] global_step=493600, grad_norm=4.59588623046875, loss=0.6312482357025146
I0302 05:19:56.764839 139590161643264 logging_writer.py:48] [493700] global_step=493700, grad_norm=4.53880500793457, loss=0.6827738285064697
I0302 05:20:30.718310 139590170035968 logging_writer.py:48] [493800] global_step=493800, grad_norm=4.440252780914307, loss=0.5859044194221497
I0302 05:21:04.647057 139590161643264 logging_writer.py:48] [493900] global_step=493900, grad_norm=4.357003211975098, loss=0.6587009429931641
I0302 05:21:38.582337 139590170035968 logging_writer.py:48] [494000] global_step=494000, grad_norm=4.9712724685668945, loss=0.6545199155807495
I0302 05:22:12.567160 139590161643264 logging_writer.py:48] [494100] global_step=494100, grad_norm=4.39933967590332, loss=0.6305615901947021
I0302 05:22:46.513105 139590170035968 logging_writer.py:48] [494200] global_step=494200, grad_norm=4.542269229888916, loss=0.6454525589942932
I0302 05:23:20.487585 139590161643264 logging_writer.py:48] [494300] global_step=494300, grad_norm=4.57148551940918, loss=0.648876428604126
I0302 05:23:54.418753 139590170035968 logging_writer.py:48] [494400] global_step=494400, grad_norm=4.529894828796387, loss=0.64659184217453
I0302 05:24:05.428045 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:24:11.434416 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:24:19.790181 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:24:22.016293 139753105983296 submission_runner.py:411] Time since start: 173593.02s, 	Step: 494434, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.1443609893321991, 'validation/accuracy': 0.7543999552726746, 'validation/loss': 1.0561788082122803, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.820483684539795, 'test/num_examples': 10000, 'score': 167855.2519850731, 'total_duration': 173593.01951646805, 'accumulated_submission_time': 167855.2519850731, 'accumulated_eval_time': 5694.541827440262, 'accumulated_logging_time': 25.811455249786377}
I0302 05:24:22.113455 139589717059328 logging_writer.py:48] [494434] accumulated_eval_time=5694.541827, accumulated_logging_time=25.811455, accumulated_submission_time=167855.251985, global_step=494434, preemption_count=0, score=167855.251985, test/accuracy=0.629600, test/loss=1.820484, test/num_examples=10000, total_duration=173593.019516, train/accuracy=0.960379, train/loss=0.144361, validation/accuracy=0.754400, validation/loss=1.056179, validation/num_examples=50000
I0302 05:24:44.894405 139589725452032 logging_writer.py:48] [494500] global_step=494500, grad_norm=4.918063163757324, loss=0.5867152810096741
I0302 05:25:18.814628 139589717059328 logging_writer.py:48] [494600] global_step=494600, grad_norm=5.074079513549805, loss=0.6495277285575867
I0302 05:25:52.761070 139589725452032 logging_writer.py:48] [494700] global_step=494700, grad_norm=4.646927833557129, loss=0.7012120485305786
I0302 05:26:26.702840 139589717059328 logging_writer.py:48] [494800] global_step=494800, grad_norm=4.428909778594971, loss=0.584520697593689
I0302 05:27:00.646493 139589725452032 logging_writer.py:48] [494900] global_step=494900, grad_norm=4.536915302276611, loss=0.5759195685386658
I0302 05:27:34.618700 139589717059328 logging_writer.py:48] [495000] global_step=495000, grad_norm=4.345409870147705, loss=0.6117345690727234
I0302 05:28:08.574109 139589725452032 logging_writer.py:48] [495100] global_step=495100, grad_norm=5.1049933433532715, loss=0.6522165536880493
I0302 05:28:42.527391 139589717059328 logging_writer.py:48] [495200] global_step=495200, grad_norm=4.707794666290283, loss=0.6191565990447998
I0302 05:29:16.485411 139589725452032 logging_writer.py:48] [495300] global_step=495300, grad_norm=4.561286926269531, loss=0.5965927839279175
I0302 05:29:50.402600 139589717059328 logging_writer.py:48] [495400] global_step=495400, grad_norm=4.788998603820801, loss=0.6447002291679382
I0302 05:30:24.343571 139589725452032 logging_writer.py:48] [495500] global_step=495500, grad_norm=4.461764335632324, loss=0.5971555113792419
I0302 05:30:58.361744 139589717059328 logging_writer.py:48] [495600] global_step=495600, grad_norm=4.418680191040039, loss=0.5888816118240356
I0302 05:31:32.298864 139589725452032 logging_writer.py:48] [495700] global_step=495700, grad_norm=4.463953971862793, loss=0.6119902729988098
I0302 05:32:06.228309 139589717059328 logging_writer.py:48] [495800] global_step=495800, grad_norm=4.406938552856445, loss=0.6862403154373169
I0302 05:32:40.177888 139589725452032 logging_writer.py:48] [495900] global_step=495900, grad_norm=4.407433986663818, loss=0.6713235974311829
I0302 05:32:52.177232 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:32:58.217472 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:33:06.470610 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:33:08.716334 139753105983296 submission_runner.py:411] Time since start: 174119.72s, 	Step: 495937, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14516934752464294, 'validation/accuracy': 0.7547399997711182, 'validation/loss': 1.056144118309021, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.8213483095169067, 'test/num_examples': 10000, 'score': 168365.25228381157, 'total_duration': 174119.71955490112, 'accumulated_submission_time': 168365.25228381157, 'accumulated_eval_time': 5711.08086681366, 'accumulated_logging_time': 25.91838574409485}
I0302 05:33:08.820372 139590170035968 logging_writer.py:48] [495937] accumulated_eval_time=5711.080867, accumulated_logging_time=25.918386, accumulated_submission_time=168365.252284, global_step=495937, preemption_count=0, score=168365.252284, test/accuracy=0.629400, test/loss=1.821348, test/num_examples=10000, total_duration=174119.719555, train/accuracy=0.961535, train/loss=0.145169, validation/accuracy=0.754740, validation/loss=1.056144, validation/num_examples=50000
I0302 05:33:30.510411 139590178428672 logging_writer.py:48] [496000] global_step=496000, grad_norm=4.28167200088501, loss=0.5581915378570557
I0302 05:34:04.401824 139590170035968 logging_writer.py:48] [496100] global_step=496100, grad_norm=4.40263032913208, loss=0.6717513203620911
I0302 05:34:38.378386 139590178428672 logging_writer.py:48] [496200] global_step=496200, grad_norm=4.7758941650390625, loss=0.5697280764579773
I0302 05:35:12.313630 139590170035968 logging_writer.py:48] [496300] global_step=496300, grad_norm=4.948330879211426, loss=0.6847445964813232
I0302 05:35:46.258931 139590178428672 logging_writer.py:48] [496400] global_step=496400, grad_norm=4.284386157989502, loss=0.5963988900184631
I0302 05:36:20.216929 139590170035968 logging_writer.py:48] [496500] global_step=496500, grad_norm=4.346347332000732, loss=0.6418591141700745
I0302 05:36:54.196247 139590178428672 logging_writer.py:48] [496600] global_step=496600, grad_norm=4.453672885894775, loss=0.6630610823631287
I0302 05:37:28.178849 139590170035968 logging_writer.py:48] [496700] global_step=496700, grad_norm=4.651514053344727, loss=0.5729725360870361
I0302 05:38:02.105173 139590178428672 logging_writer.py:48] [496800] global_step=496800, grad_norm=4.699202537536621, loss=0.6548373103141785
I0302 05:38:36.023086 139590170035968 logging_writer.py:48] [496900] global_step=496900, grad_norm=4.615037441253662, loss=0.706477165222168
I0302 05:39:09.954967 139590178428672 logging_writer.py:48] [497000] global_step=497000, grad_norm=5.0610175132751465, loss=0.6162649989128113
I0302 05:39:43.917886 139590170035968 logging_writer.py:48] [497100] global_step=497100, grad_norm=4.546040058135986, loss=0.6486850380897522
I0302 05:40:17.851234 139590178428672 logging_writer.py:48] [497200] global_step=497200, grad_norm=4.450809001922607, loss=0.6697492003440857
I0302 05:40:51.792326 139590170035968 logging_writer.py:48] [497300] global_step=497300, grad_norm=4.600510120391846, loss=0.6187819242477417
I0302 05:41:25.767283 139590178428672 logging_writer.py:48] [497400] global_step=497400, grad_norm=4.446871757507324, loss=0.5935993194580078
I0302 05:41:38.810475 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:41:45.708607 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:41:53.904801 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:41:56.203800 139753105983296 submission_runner.py:411] Time since start: 174647.21s, 	Step: 497440, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.1484004706144333, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0548858642578125, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8195267915725708, 'test/num_examples': 10000, 'score': 168875.17960882187, 'total_duration': 174647.20702934265, 'accumulated_submission_time': 168875.17960882187, 'accumulated_eval_time': 5728.474135398865, 'accumulated_logging_time': 26.03202795982361}
I0302 05:41:56.303755 139589725452032 logging_writer.py:48] [497440] accumulated_eval_time=5728.474135, accumulated_logging_time=26.032028, accumulated_submission_time=168875.179609, global_step=497440, preemption_count=0, score=168875.179609, test/accuracy=0.630500, test/loss=1.819527, test/num_examples=10000, total_duration=174647.207029, train/accuracy=0.960001, train/loss=0.148400, validation/accuracy=0.755020, validation/loss=1.054886, validation/num_examples=50000
I0302 05:42:16.964471 139590144857856 logging_writer.py:48] [497500] global_step=497500, grad_norm=4.541403293609619, loss=0.6209383606910706
I0302 05:42:50.866470 139589725452032 logging_writer.py:48] [497600] global_step=497600, grad_norm=4.372631549835205, loss=0.6069551110267639
I0302 05:43:24.939985 139590144857856 logging_writer.py:48] [497700] global_step=497700, grad_norm=4.536016941070557, loss=0.6025699377059937
I0302 05:43:58.841033 139589725452032 logging_writer.py:48] [497800] global_step=497800, grad_norm=4.639922618865967, loss=0.6199446320533752
I0302 05:44:32.798814 139590144857856 logging_writer.py:48] [497900] global_step=497900, grad_norm=4.4883317947387695, loss=0.6804248690605164
I0302 05:45:06.764194 139589725452032 logging_writer.py:48] [498000] global_step=498000, grad_norm=4.159521579742432, loss=0.6065312027931213
I0302 05:45:40.674476 139590144857856 logging_writer.py:48] [498100] global_step=498100, grad_norm=4.571805477142334, loss=0.589226245880127
I0302 05:46:14.636384 139589725452032 logging_writer.py:48] [498200] global_step=498200, grad_norm=4.435497283935547, loss=0.5825590491294861
I0302 05:46:48.603081 139590144857856 logging_writer.py:48] [498300] global_step=498300, grad_norm=4.268978118896484, loss=0.6204898357391357
I0302 05:47:22.566352 139589725452032 logging_writer.py:48] [498400] global_step=498400, grad_norm=4.765244960784912, loss=0.6433408856391907
I0302 05:47:56.534919 139590144857856 logging_writer.py:48] [498500] global_step=498500, grad_norm=4.767629146575928, loss=0.6207414269447327
I0302 05:48:30.481237 139589725452032 logging_writer.py:48] [498600] global_step=498600, grad_norm=4.256734371185303, loss=0.5544245839118958
I0302 05:49:04.442007 139590144857856 logging_writer.py:48] [498700] global_step=498700, grad_norm=5.051207542419434, loss=0.6497704386711121
I0302 05:49:38.482983 139589725452032 logging_writer.py:48] [498800] global_step=498800, grad_norm=4.880685329437256, loss=0.640860378742218
I0302 05:50:12.458986 139590144857856 logging_writer.py:48] [498900] global_step=498900, grad_norm=5.512232303619385, loss=0.688994824886322
I0302 05:50:26.503841 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:50:32.561445 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:50:40.777175 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:50:43.056579 139753105983296 submission_runner.py:411] Time since start: 175174.06s, 	Step: 498943, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.1457280069589615, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.054988145828247, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8172327280044556, 'test/num_examples': 10000, 'score': 169385.31666755676, 'total_duration': 175174.05980086327, 'accumulated_submission_time': 169385.31666755676, 'accumulated_eval_time': 5745.026813030243, 'accumulated_logging_time': 26.141904592514038}
I0302 05:50:43.159136 139590170035968 logging_writer.py:48] [498943] accumulated_eval_time=5745.026813, accumulated_logging_time=26.141905, accumulated_submission_time=169385.316668, global_step=498943, preemption_count=0, score=169385.316668, test/accuracy=0.631000, test/loss=1.817233, test/num_examples=10000, total_duration=175174.059801, train/accuracy=0.960678, train/loss=0.145728, validation/accuracy=0.754900, validation/loss=1.054988, validation/num_examples=50000
I0302 05:51:02.847726 139590178428672 logging_writer.py:48] [499000] global_step=499000, grad_norm=4.679072380065918, loss=0.6926723122596741
I0302 05:51:36.755020 139590170035968 logging_writer.py:48] [499100] global_step=499100, grad_norm=4.399652004241943, loss=0.5797658562660217
I0302 05:52:10.703945 139590178428672 logging_writer.py:48] [499200] global_step=499200, grad_norm=4.683992862701416, loss=0.642799437046051
I0302 05:52:44.636431 139590170035968 logging_writer.py:48] [499300] global_step=499300, grad_norm=4.623725414276123, loss=0.6814795136451721
I0302 05:53:18.580223 139590178428672 logging_writer.py:48] [499400] global_step=499400, grad_norm=4.949005126953125, loss=0.684715986251831
I0302 05:53:52.539362 139590170035968 logging_writer.py:48] [499500] global_step=499500, grad_norm=4.494584083557129, loss=0.6212235689163208
I0302 05:54:26.490957 139590178428672 logging_writer.py:48] [499600] global_step=499600, grad_norm=4.504217624664307, loss=0.6685411930084229
I0302 05:55:00.453930 139590170035968 logging_writer.py:48] [499700] global_step=499700, grad_norm=4.336154937744141, loss=0.6102066040039062
I0302 05:55:34.450984 139590178428672 logging_writer.py:48] [499800] global_step=499800, grad_norm=4.26151180267334, loss=0.5826004147529602
I0302 05:56:08.409021 139590170035968 logging_writer.py:48] [499900] global_step=499900, grad_norm=4.663660526275635, loss=0.6878208518028259
I0302 05:56:42.353988 139590178428672 logging_writer.py:48] [500000] global_step=500000, grad_norm=4.596056938171387, loss=0.6453535556793213
I0302 05:57:16.293945 139590170035968 logging_writer.py:48] [500100] global_step=500100, grad_norm=4.257620334625244, loss=0.5926871299743652
I0302 05:57:50.238751 139590178428672 logging_writer.py:48] [500200] global_step=500200, grad_norm=4.090487957000732, loss=0.5696676969528198
I0302 05:58:24.219773 139590170035968 logging_writer.py:48] [500300] global_step=500300, grad_norm=4.132413387298584, loss=0.5495413541793823
I0302 05:58:58.172406 139590178428672 logging_writer.py:48] [500400] global_step=500400, grad_norm=4.253633499145508, loss=0.5723553895950317
I0302 05:59:13.256426 139753105983296 spec.py:321] Evaluating on the training split.
I0302 05:59:19.303095 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 05:59:27.521572 139753105983296 spec.py:349] Evaluating on the test split.
I0302 05:59:29.745231 139753105983296 submission_runner.py:411] Time since start: 175700.75s, 	Step: 500446, 	{'train/accuracy': 0.9609972834587097, 'train/loss': 0.1472698599100113, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0556048154830933, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8181790113449097, 'test/num_examples': 10000, 'score': 169895.3501522541, 'total_duration': 175700.7484512329, 'accumulated_submission_time': 169895.3501522541, 'accumulated_eval_time': 5761.515566825867, 'accumulated_logging_time': 26.25598978996277}
I0302 05:59:29.853557 139589717059328 logging_writer.py:48] [500446] accumulated_eval_time=5761.515567, accumulated_logging_time=26.255990, accumulated_submission_time=169895.350152, global_step=500446, preemption_count=0, score=169895.350152, test/accuracy=0.630300, test/loss=1.818179, test/num_examples=10000, total_duration=175700.748451, train/accuracy=0.960997, train/loss=0.147270, validation/accuracy=0.754940, validation/loss=1.055605, validation/num_examples=50000
I0302 05:59:48.534715 139589725452032 logging_writer.py:48] [500500] global_step=500500, grad_norm=4.496496677398682, loss=0.6094292402267456
I0302 06:00:22.435592 139589717059328 logging_writer.py:48] [500600] global_step=500600, grad_norm=5.282808303833008, loss=0.5963250398635864
I0302 06:00:56.371592 139589725452032 logging_writer.py:48] [500700] global_step=500700, grad_norm=4.904516696929932, loss=0.6668264865875244
I0302 06:01:30.317895 139589717059328 logging_writer.py:48] [500800] global_step=500800, grad_norm=4.887486934661865, loss=0.7442158460617065
I0302 06:02:04.315677 139589725452032 logging_writer.py:48] [500900] global_step=500900, grad_norm=4.484681129455566, loss=0.6231586933135986
I0302 06:02:38.266749 139589717059328 logging_writer.py:48] [501000] global_step=501000, grad_norm=4.467573642730713, loss=0.6118356585502625
I0302 06:03:12.186213 139589725452032 logging_writer.py:48] [501100] global_step=501100, grad_norm=4.680453777313232, loss=0.6114375591278076
I0302 06:03:46.138801 139589717059328 logging_writer.py:48] [501200] global_step=501200, grad_norm=4.556838512420654, loss=0.5713054537773132
I0302 06:04:20.096936 139589725452032 logging_writer.py:48] [501300] global_step=501300, grad_norm=4.612157344818115, loss=0.6617633104324341
I0302 06:04:54.012958 139589717059328 logging_writer.py:48] [501400] global_step=501400, grad_norm=4.465975284576416, loss=0.6453011631965637
I0302 06:05:27.945524 139589725452032 logging_writer.py:48] [501500] global_step=501500, grad_norm=4.573818206787109, loss=0.6663013100624084
I0302 06:06:01.880069 139589717059328 logging_writer.py:48] [501600] global_step=501600, grad_norm=4.676293849945068, loss=0.5797022581100464
I0302 06:06:35.839472 139589725452032 logging_writer.py:48] [501700] global_step=501700, grad_norm=4.380262851715088, loss=0.5883979797363281
I0302 06:07:09.764802 139589717059328 logging_writer.py:48] [501800] global_step=501800, grad_norm=4.785308361053467, loss=0.6707169413566589
I0302 06:07:43.708613 139589725452032 logging_writer.py:48] [501900] global_step=501900, grad_norm=4.433662414550781, loss=0.6519431471824646
I0302 06:07:59.849045 139753105983296 spec.py:321] Evaluating on the training split.
I0302 06:08:05.992936 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 06:08:14.250427 139753105983296 spec.py:349] Evaluating on the test split.
I0302 06:08:16.548203 139753105983296 submission_runner.py:411] Time since start: 176227.55s, 	Step: 501949, 	{'train/accuracy': 0.9622927308082581, 'train/loss': 0.1432228684425354, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.055835485458374, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8182381391525269, 'test/num_examples': 10000, 'score': 170405.28198862076, 'total_duration': 176227.55142569542, 'accumulated_submission_time': 170405.28198862076, 'accumulated_eval_time': 5778.214676856995, 'accumulated_logging_time': 26.37480115890503}
I0302 06:08:16.658273 139590170035968 logging_writer.py:48] [501949] accumulated_eval_time=5778.214677, accumulated_logging_time=26.374801, accumulated_submission_time=170405.281989, global_step=501949, preemption_count=0, score=170405.281989, test/accuracy=0.630600, test/loss=1.818238, test/num_examples=10000, total_duration=176227.551426, train/accuracy=0.962293, train/loss=0.143223, validation/accuracy=0.755020, validation/loss=1.055835, validation/num_examples=50000
I0302 06:08:34.293556 139590178428672 logging_writer.py:48] [502000] global_step=502000, grad_norm=4.144149303436279, loss=0.5937063694000244
I0302 06:09:08.239691 139590170035968 logging_writer.py:48] [502100] global_step=502100, grad_norm=4.721565246582031, loss=0.5987951755523682
I0302 06:09:42.200513 139590178428672 logging_writer.py:48] [502200] global_step=502200, grad_norm=5.056081771850586, loss=0.6950994729995728
I0302 06:10:16.124600 139590170035968 logging_writer.py:48] [502300] global_step=502300, grad_norm=4.6117472648620605, loss=0.664261519908905
I0302 06:10:50.037239 139590178428672 logging_writer.py:48] [502400] global_step=502400, grad_norm=4.789760589599609, loss=0.6745108962059021
I0302 06:11:23.987923 139590170035968 logging_writer.py:48] [502500] global_step=502500, grad_norm=4.453398704528809, loss=0.57780522108078
I0302 06:11:57.935121 139590178428672 logging_writer.py:48] [502600] global_step=502600, grad_norm=4.199606895446777, loss=0.6368277668952942
I0302 06:12:31.901223 139590170035968 logging_writer.py:48] [502700] global_step=502700, grad_norm=4.860527515411377, loss=0.5987406373023987
I0302 06:13:05.861649 139590178428672 logging_writer.py:48] [502800] global_step=502800, grad_norm=4.478031158447266, loss=0.6167528033256531
I0302 06:13:39.776933 139590170035968 logging_writer.py:48] [502900] global_step=502900, grad_norm=4.426761150360107, loss=0.6497121453285217
I0302 06:14:13.760860 139590178428672 logging_writer.py:48] [503000] global_step=503000, grad_norm=4.439366340637207, loss=0.6075722575187683
I0302 06:14:47.661649 139590170035968 logging_writer.py:48] [503100] global_step=503100, grad_norm=4.7021484375, loss=0.6104969382286072
I0302 06:15:21.613568 139590178428672 logging_writer.py:48] [503200] global_step=503200, grad_norm=4.644081115722656, loss=0.6588855385780334
I0302 06:15:55.579127 139590170035968 logging_writer.py:48] [503300] global_step=503300, grad_norm=4.22601842880249, loss=0.5979518890380859
I0302 06:16:29.519778 139590178428672 logging_writer.py:48] [503400] global_step=503400, grad_norm=4.576145648956299, loss=0.6086773872375488
I0302 06:16:46.650790 139753105983296 spec.py:321] Evaluating on the training split.
I0302 06:16:52.768547 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 06:17:00.908844 139753105983296 spec.py:349] Evaluating on the test split.
I0302 06:17:03.318954 139753105983296 submission_runner.py:411] Time since start: 176754.32s, 	Step: 503452, 	{'train/accuracy': 0.9615752100944519, 'train/loss': 0.14450353384017944, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.0563068389892578, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8219772577285767, 'test/num_examples': 10000, 'score': 170915.20975255966, 'total_duration': 176754.32218694687, 'accumulated_submission_time': 170915.20975255966, 'accumulated_eval_time': 5794.882792234421, 'accumulated_logging_time': 26.4967200756073}
I0302 06:17:03.422486 139589717059328 logging_writer.py:48] [503452] accumulated_eval_time=5794.882792, accumulated_logging_time=26.496720, accumulated_submission_time=170915.209753, global_step=503452, preemption_count=0, score=170915.209753, test/accuracy=0.630100, test/loss=1.821977, test/num_examples=10000, total_duration=176754.322187, train/accuracy=0.961575, train/loss=0.144504, validation/accuracy=0.754780, validation/loss=1.056307, validation/num_examples=50000
I0302 06:17:20.033839 139589725452032 logging_writer.py:48] [503500] global_step=503500, grad_norm=4.577713489532471, loss=0.6140385270118713
I0302 06:17:53.912863 139589717059328 logging_writer.py:48] [503600] global_step=503600, grad_norm=4.6631574630737305, loss=0.5615637898445129
I0302 06:18:27.840456 139589725452032 logging_writer.py:48] [503700] global_step=503700, grad_norm=4.55682897567749, loss=0.6377931237220764
I0302 06:19:01.774673 139589717059328 logging_writer.py:48] [503800] global_step=503800, grad_norm=4.761727333068848, loss=0.6197357177734375
I0302 06:19:35.737399 139589725452032 logging_writer.py:48] [503900] global_step=503900, grad_norm=4.518820762634277, loss=0.6392263174057007
I0302 06:20:09.666061 139589717059328 logging_writer.py:48] [504000] global_step=504000, grad_norm=4.409248352050781, loss=0.6011019349098206
I0302 06:20:43.794594 139589725452032 logging_writer.py:48] [504100] global_step=504100, grad_norm=5.14876651763916, loss=0.6775089502334595
I0302 06:21:17.728002 139589717059328 logging_writer.py:48] [504200] global_step=504200, grad_norm=4.708798885345459, loss=0.6566236615180969
I0302 06:21:51.651585 139589725452032 logging_writer.py:48] [504300] global_step=504300, grad_norm=4.565459251403809, loss=0.6850041747093201
I0302 06:22:25.559470 139589717059328 logging_writer.py:48] [504400] global_step=504400, grad_norm=4.740234851837158, loss=0.5791963934898376
I0302 06:22:59.475929 139589725452032 logging_writer.py:48] [504500] global_step=504500, grad_norm=4.499914169311523, loss=0.6055557131767273
I0302 06:23:33.416772 139589717059328 logging_writer.py:48] [504600] global_step=504600, grad_norm=4.522093296051025, loss=0.6865234375
I0302 06:24:07.331712 139589725452032 logging_writer.py:48] [504700] global_step=504700, grad_norm=4.292994022369385, loss=0.5757204294204712
I0302 06:24:41.269613 139589717059328 logging_writer.py:48] [504800] global_step=504800, grad_norm=4.369391441345215, loss=0.6224868893623352
I0302 06:25:15.205405 139589725452032 logging_writer.py:48] [504900] global_step=504900, grad_norm=4.537532329559326, loss=0.6530177593231201
I0302 06:25:33.655809 139753105983296 spec.py:321] Evaluating on the training split.
I0302 06:25:39.657379 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 06:25:47.921465 139753105983296 spec.py:349] Evaluating on the test split.
I0302 06:25:50.229194 139753105983296 submission_runner.py:411] Time since start: 177281.23s, 	Step: 504956, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14705686271190643, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0555027723312378, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.818542718887329, 'test/num_examples': 10000, 'score': 171425.38033485413, 'total_duration': 177281.23242735863, 'accumulated_submission_time': 171425.38033485413, 'accumulated_eval_time': 5811.456128358841, 'accumulated_logging_time': 26.61021900177002}
I0302 06:25:50.335519 139590161643264 logging_writer.py:48] [504956] accumulated_eval_time=5811.456128, accumulated_logging_time=26.610219, accumulated_submission_time=171425.380335, global_step=504956, preemption_count=0, score=171425.380335, test/accuracy=0.630400, test/loss=1.818543, test/num_examples=10000, total_duration=177281.232427, train/accuracy=0.960061, train/loss=0.147057, validation/accuracy=0.754980, validation/loss=1.055503, validation/num_examples=50000
I0302 06:26:05.602550 139590170035968 logging_writer.py:48] [505000] global_step=505000, grad_norm=4.827128887176514, loss=0.7021135687828064
I0302 06:26:39.611785 139590161643264 logging_writer.py:48] [505100] global_step=505100, grad_norm=4.7581400871276855, loss=0.7580577731132507
I0302 06:27:13.552376 139590170035968 logging_writer.py:48] [505200] global_step=505200, grad_norm=4.228585243225098, loss=0.634803295135498
I0302 06:27:47.504896 139590161643264 logging_writer.py:48] [505300] global_step=505300, grad_norm=4.036176681518555, loss=0.5599040985107422
I0302 06:28:21.465388 139590170035968 logging_writer.py:48] [505400] global_step=505400, grad_norm=4.727111339569092, loss=0.6238501667976379
I0302 06:28:55.424715 139590161643264 logging_writer.py:48] [505500] global_step=505500, grad_norm=4.47544527053833, loss=0.5962169766426086
I0302 06:29:29.402946 139590170035968 logging_writer.py:48] [505600] global_step=505600, grad_norm=4.747056007385254, loss=0.6788207292556763
I0302 06:30:03.348526 139590161643264 logging_writer.py:48] [505700] global_step=505700, grad_norm=4.330714225769043, loss=0.599573016166687
I0302 06:30:37.304610 139590170035968 logging_writer.py:48] [505800] global_step=505800, grad_norm=4.944920063018799, loss=0.6620355844497681
I0302 06:31:11.257759 139590161643264 logging_writer.py:48] [505900] global_step=505900, grad_norm=4.587808609008789, loss=0.6366725564002991
I0302 06:31:45.211394 139590170035968 logging_writer.py:48] [506000] global_step=506000, grad_norm=4.400267601013184, loss=0.663066565990448
I0302 06:32:19.185151 139590161643264 logging_writer.py:48] [506100] global_step=506100, grad_norm=4.575571060180664, loss=0.5847426056861877
I0302 06:32:53.196296 139590170035968 logging_writer.py:48] [506200] global_step=506200, grad_norm=4.882518768310547, loss=0.5645524263381958
I0302 06:33:27.140043 139590161643264 logging_writer.py:48] [506300] global_step=506300, grad_norm=4.4070868492126465, loss=0.5658456087112427
I0302 06:34:01.087285 139590170035968 logging_writer.py:48] [506400] global_step=506400, grad_norm=4.666515827178955, loss=0.614201009273529
I0302 06:34:20.234255 139753105983296 spec.py:321] Evaluating on the training split.
I0302 06:34:26.221226 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 06:34:34.476320 139753105983296 spec.py:349] Evaluating on the test split.
I0302 06:34:36.800619 139753105983296 submission_runner.py:411] Time since start: 177807.80s, 	Step: 506458, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14615938067436218, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0542012453079224, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.81696355342865, 'test/num_examples': 10000, 'score': 171935.21598100662, 'total_duration': 177807.80382156372, 'accumulated_submission_time': 171935.21598100662, 'accumulated_eval_time': 5828.022411584854, 'accumulated_logging_time': 26.726796627044678}
I0302 06:34:36.908592 139590144857856 logging_writer.py:48] [506458] accumulated_eval_time=5828.022412, accumulated_logging_time=26.726797, accumulated_submission_time=171935.215981, global_step=506458, preemption_count=0, score=171935.215981, test/accuracy=0.630300, test/loss=1.816964, test/num_examples=10000, total_duration=177807.803822, train/accuracy=0.961456, train/loss=0.146159, validation/accuracy=0.755140, validation/loss=1.054201, validation/num_examples=50000
I0302 06:34:51.493585 139590153250560 logging_writer.py:48] [506500] global_step=506500, grad_norm=4.540949821472168, loss=0.6426970958709717
I0302 06:35:25.353880 139590144857856 logging_writer.py:48] [506600] global_step=506600, grad_norm=4.538149833679199, loss=0.593005895614624
I0302 06:35:59.287816 139590153250560 logging_writer.py:48] [506700] global_step=506700, grad_norm=5.048117160797119, loss=0.6298810839653015
I0302 06:36:33.234643 139590144857856 logging_writer.py:48] [506800] global_step=506800, grad_norm=4.12501335144043, loss=0.5519872903823853
I0302 06:37:07.164148 139590153250560 logging_writer.py:48] [506900] global_step=506900, grad_norm=4.68060302734375, loss=0.5613324642181396
I0302 06:37:41.114442 139590144857856 logging_writer.py:48] [507000] global_step=507000, grad_norm=5.006222248077393, loss=0.6446349024772644
I0302 06:38:15.036684 139590153250560 logging_writer.py:48] [507100] global_step=507100, grad_norm=4.188406944274902, loss=0.5938933491706848
I0302 06:38:49.074598 139590144857856 logging_writer.py:48] [507200] global_step=507200, grad_norm=4.583040714263916, loss=0.6688304543495178
I0302 06:39:22.967329 139590153250560 logging_writer.py:48] [507300] global_step=507300, grad_norm=4.298020362854004, loss=0.6098766922950745
I0302 06:39:56.883160 139590144857856 logging_writer.py:48] [507400] global_step=507400, grad_norm=4.212306499481201, loss=0.6690136194229126
I0302 06:40:30.853195 139590153250560 logging_writer.py:48] [507500] global_step=507500, grad_norm=4.182784557342529, loss=0.6282302737236023
I0302 06:41:04.797915 139590144857856 logging_writer.py:48] [507600] global_step=507600, grad_norm=4.665341854095459, loss=0.627802848815918
I0302 06:41:38.696817 139590153250560 logging_writer.py:48] [507700] global_step=507700, grad_norm=5.250021457672119, loss=0.6622472405433655
I0302 06:42:12.617475 139590144857856 logging_writer.py:48] [507800] global_step=507800, grad_norm=4.526848316192627, loss=0.6529568433761597
I0302 06:42:46.541817 139590153250560 logging_writer.py:48] [507900] global_step=507900, grad_norm=4.246101379394531, loss=0.5920252203941345
I0302 06:43:07.042288 139753105983296 spec.py:321] Evaluating on the training split.
I0302 06:43:13.105739 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 06:43:21.269559 139753105983296 spec.py:349] Evaluating on the test split.
I0302 06:43:23.551039 139753105983296 submission_runner.py:411] Time since start: 178334.55s, 	Step: 507962, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.1463172882795334, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0554317235946655, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8180532455444336, 'test/num_examples': 10000, 'score': 172445.2868115902, 'total_duration': 178334.55427193642, 'accumulated_submission_time': 172445.2868115902, 'accumulated_eval_time': 5844.531116485596, 'accumulated_logging_time': 26.84491229057312}
I0302 06:43:23.650966 139590161643264 logging_writer.py:48] [507962] accumulated_eval_time=5844.531116, accumulated_logging_time=26.844912, accumulated_submission_time=172445.286812, global_step=507962, preemption_count=0, score=172445.286812, test/accuracy=0.630700, test/loss=1.818053, test/num_examples=10000, total_duration=178334.554272, train/accuracy=0.959961, train/loss=0.146317, validation/accuracy=0.754880, validation/loss=1.055432, validation/num_examples=50000
I0302 06:43:36.906462 139590170035968 logging_writer.py:48] [508000] global_step=508000, grad_norm=4.849797248840332, loss=0.6337699890136719
I0302 06:44:10.766336 139590161643264 logging_writer.py:48] [508100] global_step=508100, grad_norm=4.320602893829346, loss=0.6070546507835388
I0302 06:44:44.722924 139590170035968 logging_writer.py:48] [508200] global_step=508200, grad_norm=5.170579433441162, loss=0.6607329249382019
I0302 06:45:18.839882 139590161643264 logging_writer.py:48] [508300] global_step=508300, grad_norm=4.6917195320129395, loss=0.6409693956375122
I0302 06:45:52.768006 139590170035968 logging_writer.py:48] [508400] global_step=508400, grad_norm=4.290613651275635, loss=0.6088624000549316
I0302 06:46:26.722859 139590161643264 logging_writer.py:48] [508500] global_step=508500, grad_norm=4.5825653076171875, loss=0.6516368389129639
I0302 06:47:00.672541 139590170035968 logging_writer.py:48] [508600] global_step=508600, grad_norm=4.661929130554199, loss=0.5861798524856567
I0302 06:47:34.640728 139590161643264 logging_writer.py:48] [508700] global_step=508700, grad_norm=5.052545070648193, loss=0.5755942463874817
I0302 06:48:08.561232 139590170035968 logging_writer.py:48] [508800] global_step=508800, grad_norm=4.398309230804443, loss=0.6541403532028198
I0302 06:48:42.497838 139590161643264 logging_writer.py:48] [508900] global_step=508900, grad_norm=4.569026470184326, loss=0.6449593901634216
I0302 06:49:16.434005 139590170035968 logging_writer.py:48] [509000] global_step=509000, grad_norm=4.209661960601807, loss=0.536457896232605
I0302 06:49:50.396791 139590161643264 logging_writer.py:48] [509100] global_step=509100, grad_norm=4.446143627166748, loss=0.6509854197502136
I0302 06:50:24.327188 139590170035968 logging_writer.py:48] [509200] global_step=509200, grad_norm=4.719208717346191, loss=0.666801393032074
I0302 06:50:58.270314 139590161643264 logging_writer.py:48] [509300] global_step=509300, grad_norm=4.427080154418945, loss=0.5740965008735657
I0302 06:51:32.342039 139590170035968 logging_writer.py:48] [509400] global_step=509400, grad_norm=4.450066566467285, loss=0.5698984265327454
I0302 06:51:53.858907 139753105983296 spec.py:321] Evaluating on the training split.
I0302 06:51:59.860162 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 06:52:08.233357 139753105983296 spec.py:349] Evaluating on the test split.
I0302 06:52:10.503636 139753105983296 submission_runner.py:411] Time since start: 178861.51s, 	Step: 509465, 	{'train/accuracy': 0.9617546200752258, 'train/loss': 0.14424748718738556, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.0556154251098633, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8183221817016602, 'test/num_examples': 10000, 'score': 172955.43111920357, 'total_duration': 178861.50686740875, 'accumulated_submission_time': 172955.43111920357, 'accumulated_eval_time': 5861.175798654556, 'accumulated_logging_time': 26.954604864120483}
I0302 06:52:10.607387 139589717059328 logging_writer.py:48] [509465] accumulated_eval_time=5861.175799, accumulated_logging_time=26.954605, accumulated_submission_time=172955.431119, global_step=509465, preemption_count=0, score=172955.431119, test/accuracy=0.631600, test/loss=1.818322, test/num_examples=10000, total_duration=178861.506867, train/accuracy=0.961755, train/loss=0.144247, validation/accuracy=0.754880, validation/loss=1.055615, validation/num_examples=50000
I0302 06:52:22.810705 139589725452032 logging_writer.py:48] [509500] global_step=509500, grad_norm=4.31110954284668, loss=0.6392862796783447
I0302 06:52:56.679664 139589717059328 logging_writer.py:48] [509600] global_step=509600, grad_norm=4.068763732910156, loss=0.5611976385116577
I0302 06:53:30.619053 139589725452032 logging_writer.py:48] [509700] global_step=509700, grad_norm=4.7237396240234375, loss=0.732947051525116
I0302 06:54:04.551897 139589717059328 logging_writer.py:48] [509800] global_step=509800, grad_norm=4.786019325256348, loss=0.6661226749420166
I0302 06:54:38.497986 139589725452032 logging_writer.py:48] [509900] global_step=509900, grad_norm=4.548778057098389, loss=0.5609312057495117
I0302 06:55:12.418275 139589717059328 logging_writer.py:48] [510000] global_step=510000, grad_norm=4.538491249084473, loss=0.6553337574005127
I0302 06:55:46.340386 139589725452032 logging_writer.py:48] [510100] global_step=510100, grad_norm=4.755947113037109, loss=0.6377465724945068
I0302 06:56:20.266317 139589717059328 logging_writer.py:48] [510200] global_step=510200, grad_norm=4.2667317390441895, loss=0.6068632006645203
I0302 06:56:54.177061 139589725452032 logging_writer.py:48] [510300] global_step=510300, grad_norm=4.1166863441467285, loss=0.5467608571052551
I0302 06:57:28.176861 139589717059328 logging_writer.py:48] [510400] global_step=510400, grad_norm=4.500521659851074, loss=0.764653742313385
I0302 06:58:02.089063 139589725452032 logging_writer.py:48] [510500] global_step=510500, grad_norm=4.625408172607422, loss=0.6483133435249329
I0302 06:58:36.037477 139589717059328 logging_writer.py:48] [510600] global_step=510600, grad_norm=4.3287811279296875, loss=0.5491654276847839
I0302 06:59:09.997978 139589725452032 logging_writer.py:48] [510700] global_step=510700, grad_norm=4.4197282791137695, loss=0.5309813618659973
I0302 06:59:43.937695 139589717059328 logging_writer.py:48] [510800] global_step=510800, grad_norm=4.7572712898254395, loss=0.6190143823623657
I0302 07:00:17.855756 139589725452032 logging_writer.py:48] [510900] global_step=510900, grad_norm=4.673861026763916, loss=0.6477586030960083
I0302 07:00:40.746634 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:00:46.730960 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:00:55.055082 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:00:57.370629 139753105983296 submission_runner.py:411] Time since start: 179388.37s, 	Step: 510969, 	{'train/accuracy': 0.9611567258834839, 'train/loss': 0.1486804485321045, 'validation/accuracy': 0.7549200057983398, 'validation/loss': 1.055416226387024, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8184680938720703, 'test/num_examples': 10000, 'score': 173465.5064251423, 'total_duration': 179388.3738567829, 'accumulated_submission_time': 173465.5064251423, 'accumulated_eval_time': 5877.799740791321, 'accumulated_logging_time': 27.069482803344727}
I0302 07:00:57.472774 139589717059328 logging_writer.py:48] [510969] accumulated_eval_time=5877.799741, accumulated_logging_time=27.069483, accumulated_submission_time=173465.506425, global_step=510969, preemption_count=0, score=173465.506425, test/accuracy=0.630200, test/loss=1.818468, test/num_examples=10000, total_duration=179388.373857, train/accuracy=0.961157, train/loss=0.148680, validation/accuracy=0.754920, validation/loss=1.055416, validation/num_examples=50000
I0302 07:01:08.334450 139589725452032 logging_writer.py:48] [511000] global_step=511000, grad_norm=4.315184116363525, loss=0.5903949737548828
I0302 07:01:42.273234 139589717059328 logging_writer.py:48] [511100] global_step=511100, grad_norm=4.445082664489746, loss=0.6157629489898682
I0302 07:02:16.239589 139589725452032 logging_writer.py:48] [511200] global_step=511200, grad_norm=4.1909332275390625, loss=0.5725083947181702
I0302 07:02:50.171611 139589717059328 logging_writer.py:48] [511300] global_step=511300, grad_norm=4.536377429962158, loss=0.6201043128967285
I0302 07:03:24.160356 139589725452032 logging_writer.py:48] [511400] global_step=511400, grad_norm=4.658381462097168, loss=0.6346285343170166
I0302 07:03:58.228511 139589717059328 logging_writer.py:48] [511500] global_step=511500, grad_norm=4.473149299621582, loss=0.6386825442314148
I0302 07:04:32.165867 139589725452032 logging_writer.py:48] [511600] global_step=511600, grad_norm=4.361110687255859, loss=0.6404532790184021
I0302 07:05:06.146106 139589717059328 logging_writer.py:48] [511700] global_step=511700, grad_norm=4.451910495758057, loss=0.7008448839187622
I0302 07:05:40.088626 139589725452032 logging_writer.py:48] [511800] global_step=511800, grad_norm=4.750197887420654, loss=0.6313171982765198
I0302 07:06:14.061399 139589717059328 logging_writer.py:48] [511900] global_step=511900, grad_norm=4.7218146324157715, loss=0.6222955584526062
I0302 07:06:48.010032 139589725452032 logging_writer.py:48] [512000] global_step=512000, grad_norm=4.461634635925293, loss=0.6320555210113525
I0302 07:07:21.965235 139589717059328 logging_writer.py:48] [512100] global_step=512100, grad_norm=4.237423419952393, loss=0.5974311828613281
I0302 07:07:55.917968 139589725452032 logging_writer.py:48] [512200] global_step=512200, grad_norm=4.14237642288208, loss=0.5636895298957825
I0302 07:08:29.847017 139589717059328 logging_writer.py:48] [512300] global_step=512300, grad_norm=4.83090877532959, loss=0.6144846677780151
I0302 07:09:03.815804 139589725452032 logging_writer.py:48] [512400] global_step=512400, grad_norm=4.50380802154541, loss=0.607949435710907
I0302 07:09:27.387636 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:09:33.410971 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:09:41.729060 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:09:44.334124 139753105983296 submission_runner.py:411] Time since start: 179915.34s, 	Step: 512471, 	{'train/accuracy': 0.9588049650192261, 'train/loss': 0.14983156323432922, 'validation/accuracy': 0.7547799944877625, 'validation/loss': 1.055154800415039, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8177188634872437, 'test/num_examples': 10000, 'score': 173975.3585202694, 'total_duration': 179915.33734679222, 'accumulated_submission_time': 173975.3585202694, 'accumulated_eval_time': 5894.746170282364, 'accumulated_logging_time': 27.181657075881958}
I0302 07:09:44.443794 139590161643264 logging_writer.py:48] [512471] accumulated_eval_time=5894.746170, accumulated_logging_time=27.181657, accumulated_submission_time=173975.358520, global_step=512471, preemption_count=0, score=173975.358520, test/accuracy=0.630300, test/loss=1.817719, test/num_examples=10000, total_duration=179915.337347, train/accuracy=0.958805, train/loss=0.149832, validation/accuracy=0.754780, validation/loss=1.055155, validation/num_examples=50000
I0302 07:09:54.663375 139590170035968 logging_writer.py:48] [512500] global_step=512500, grad_norm=4.599853515625, loss=0.6620360612869263
I0302 07:10:28.529848 139590161643264 logging_writer.py:48] [512600] global_step=512600, grad_norm=4.617401599884033, loss=0.6433905363082886
I0302 07:11:02.461327 139590170035968 logging_writer.py:48] [512700] global_step=512700, grad_norm=4.447751998901367, loss=0.5701342225074768
I0302 07:11:36.384629 139590161643264 logging_writer.py:48] [512800] global_step=512800, grad_norm=4.553596496582031, loss=0.6607383489608765
I0302 07:12:10.323670 139590170035968 logging_writer.py:48] [512900] global_step=512900, grad_norm=4.898054599761963, loss=0.6957341432571411
I0302 07:12:44.254770 139590161643264 logging_writer.py:48] [513000] global_step=513000, grad_norm=4.9509596824646, loss=0.6929744482040405
I0302 07:13:18.177402 139590170035968 logging_writer.py:48] [513100] global_step=513100, grad_norm=4.806122779846191, loss=0.6697189807891846
I0302 07:13:52.136082 139590161643264 logging_writer.py:48] [513200] global_step=513200, grad_norm=4.6836137771606445, loss=0.6563180685043335
I0302 07:14:26.070080 139590170035968 logging_writer.py:48] [513300] global_step=513300, grad_norm=4.176218032836914, loss=0.5251191854476929
I0302 07:14:59.980371 139590161643264 logging_writer.py:48] [513400] global_step=513400, grad_norm=4.376937389373779, loss=0.5658622980117798
I0302 07:15:33.904697 139590170035968 logging_writer.py:48] [513500] global_step=513500, grad_norm=4.727534770965576, loss=0.6483044028282166
I0302 07:16:07.919561 139590161643264 logging_writer.py:48] [513600] global_step=513600, grad_norm=4.44589900970459, loss=0.6549892425537109
I0302 07:16:41.853741 139590170035968 logging_writer.py:48] [513700] global_step=513700, grad_norm=5.004423141479492, loss=0.6667224764823914
I0302 07:17:15.779018 139590161643264 logging_writer.py:48] [513800] global_step=513800, grad_norm=4.633975028991699, loss=0.6527950167655945
I0302 07:17:49.708928 139590170035968 logging_writer.py:48] [513900] global_step=513900, grad_norm=4.458872318267822, loss=0.6531866192817688
I0302 07:18:14.621765 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:18:20.623751 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:18:28.794541 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:18:31.110386 139753105983296 submission_runner.py:411] Time since start: 180442.11s, 	Step: 513975, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14640827476978302, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0548641681671143, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8174024820327759, 'test/num_examples': 10000, 'score': 174485.4700343609, 'total_duration': 180442.11360645294, 'accumulated_submission_time': 174485.4700343609, 'accumulated_eval_time': 5911.234726428986, 'accumulated_logging_time': 27.304914474487305}
I0302 07:18:31.207606 139589708666624 logging_writer.py:48] [513975] accumulated_eval_time=5911.234726, accumulated_logging_time=27.304914, accumulated_submission_time=174485.470034, global_step=513975, preemption_count=0, score=174485.470034, test/accuracy=0.630400, test/loss=1.817402, test/num_examples=10000, total_duration=180442.113606, train/accuracy=0.961097, train/loss=0.146408, validation/accuracy=0.754960, validation/loss=1.054864, validation/num_examples=50000
I0302 07:18:40.032916 139589717059328 logging_writer.py:48] [514000] global_step=514000, grad_norm=4.8155999183654785, loss=0.7150183320045471
I0302 07:19:13.902764 139589708666624 logging_writer.py:48] [514100] global_step=514100, grad_norm=4.183018684387207, loss=0.6020623445510864
I0302 07:19:47.843914 139589717059328 logging_writer.py:48] [514200] global_step=514200, grad_norm=4.552661418914795, loss=0.598334550857544
I0302 07:20:21.783017 139589708666624 logging_writer.py:48] [514300] global_step=514300, grad_norm=4.357539176940918, loss=0.6014909744262695
I0302 07:20:55.698482 139589717059328 logging_writer.py:48] [514400] global_step=514400, grad_norm=4.13205099105835, loss=0.5427355766296387
I0302 07:21:29.672293 139589708666624 logging_writer.py:48] [514500] global_step=514500, grad_norm=3.973458766937256, loss=0.5778841376304626
I0302 07:22:03.623631 139589717059328 logging_writer.py:48] [514600] global_step=514600, grad_norm=4.412158489227295, loss=0.6334553956985474
I0302 07:22:37.672271 139589708666624 logging_writer.py:48] [514700] global_step=514700, grad_norm=4.854241847991943, loss=0.6877122521400452
I0302 07:23:11.648142 139589717059328 logging_writer.py:48] [514800] global_step=514800, grad_norm=4.2887139320373535, loss=0.6862876415252686
I0302 07:23:45.610512 139589708666624 logging_writer.py:48] [514900] global_step=514900, grad_norm=4.470761775970459, loss=0.5526591539382935
I0302 07:24:19.587129 139589717059328 logging_writer.py:48] [515000] global_step=515000, grad_norm=4.653054237365723, loss=0.701620876789093
I0302 07:24:53.518118 139589708666624 logging_writer.py:48] [515100] global_step=515100, grad_norm=4.467099189758301, loss=0.6163531541824341
I0302 07:25:27.482899 139589717059328 logging_writer.py:48] [515200] global_step=515200, grad_norm=4.602632522583008, loss=0.631033182144165
I0302 07:26:01.433334 139589708666624 logging_writer.py:48] [515300] global_step=515300, grad_norm=4.533714294433594, loss=0.6610602140426636
I0302 07:26:35.412098 139589717059328 logging_writer.py:48] [515400] global_step=515400, grad_norm=4.832489013671875, loss=0.6432555317878723
I0302 07:27:01.365106 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:27:07.399102 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:27:15.621255 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:27:17.878177 139753105983296 submission_runner.py:411] Time since start: 180968.88s, 	Step: 515478, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.14478006958961487, 'validation/accuracy': 0.7547199726104736, 'validation/loss': 1.055174469947815, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8177951574325562, 'test/num_examples': 10000, 'score': 174995.56325149536, 'total_duration': 180968.88140940666, 'accumulated_submission_time': 174995.56325149536, 'accumulated_eval_time': 5927.747765779495, 'accumulated_logging_time': 27.412547826766968}
I0302 07:27:17.980502 139589725452032 logging_writer.py:48] [515478] accumulated_eval_time=5927.747766, accumulated_logging_time=27.412548, accumulated_submission_time=174995.563251, global_step=515478, preemption_count=0, score=174995.563251, test/accuracy=0.630500, test/loss=1.817795, test/num_examples=10000, total_duration=180968.881409, train/accuracy=0.960419, train/loss=0.144780, validation/accuracy=0.754720, validation/loss=1.055174, validation/num_examples=50000
I0302 07:27:25.790941 139590161643264 logging_writer.py:48] [515500] global_step=515500, grad_norm=4.543211936950684, loss=0.6297647356987
I0302 07:27:59.696940 139589725452032 logging_writer.py:48] [515600] global_step=515600, grad_norm=5.237192630767822, loss=0.6544537544250488
I0302 07:28:33.694963 139590161643264 logging_writer.py:48] [515700] global_step=515700, grad_norm=4.97505521774292, loss=0.7486928701400757
I0302 07:29:07.633923 139589725452032 logging_writer.py:48] [515800] global_step=515800, grad_norm=4.5241594314575195, loss=0.6206790208816528
I0302 07:29:41.575300 139590161643264 logging_writer.py:48] [515900] global_step=515900, grad_norm=4.339130878448486, loss=0.6571033000946045
I0302 07:30:15.531482 139589725452032 logging_writer.py:48] [516000] global_step=516000, grad_norm=4.488760948181152, loss=0.6656899452209473
I0302 07:30:49.472617 139590161643264 logging_writer.py:48] [516100] global_step=516100, grad_norm=4.584394454956055, loss=0.5910342931747437
I0302 07:31:23.412204 139589725452032 logging_writer.py:48] [516200] global_step=516200, grad_norm=4.918493747711182, loss=0.6155127286911011
I0302 07:31:57.376090 139590161643264 logging_writer.py:48] [516300] global_step=516300, grad_norm=4.371155738830566, loss=0.6228957176208496
I0302 07:32:31.334448 139589725452032 logging_writer.py:48] [516400] global_step=516400, grad_norm=4.436220169067383, loss=0.5738883018493652
I0302 07:33:05.313368 139590161643264 logging_writer.py:48] [516500] global_step=516500, grad_norm=4.132079601287842, loss=0.61211758852005
I0302 07:33:39.265759 139589725452032 logging_writer.py:48] [516600] global_step=516600, grad_norm=5.054606914520264, loss=0.732250988483429
I0302 07:34:13.176814 139590161643264 logging_writer.py:48] [516700] global_step=516700, grad_norm=4.773072719573975, loss=0.6361935138702393
I0302 07:34:47.183212 139589725452032 logging_writer.py:48] [516800] global_step=516800, grad_norm=4.404765605926514, loss=0.6843181252479553
I0302 07:35:21.078494 139590161643264 logging_writer.py:48] [516900] global_step=516900, grad_norm=4.926522731781006, loss=0.5826024413108826
I0302 07:35:48.028667 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:35:54.054100 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:36:02.368147 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:36:04.654701 139753105983296 submission_runner.py:411] Time since start: 181495.66s, 	Step: 516981, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14951801300048828, 'validation/accuracy': 0.7548799514770508, 'validation/loss': 1.055079460144043, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.818442702293396, 'test/num_examples': 10000, 'score': 175505.54929184914, 'total_duration': 181495.65792012215, 'accumulated_submission_time': 175505.54929184914, 'accumulated_eval_time': 5944.373743772507, 'accumulated_logging_time': 27.52469778060913}
I0302 07:36:04.757074 139590144857856 logging_writer.py:48] [516981] accumulated_eval_time=5944.373744, accumulated_logging_time=27.524698, accumulated_submission_time=175505.549292, global_step=516981, preemption_count=0, score=175505.549292, test/accuracy=0.630800, test/loss=1.818443, test/num_examples=10000, total_duration=181495.657920, train/accuracy=0.959881, train/loss=0.149518, validation/accuracy=0.754880, validation/loss=1.055079, validation/num_examples=50000
I0302 07:36:11.588087 139590153250560 logging_writer.py:48] [517000] global_step=517000, grad_norm=4.371556758880615, loss=0.6000540852546692
I0302 07:36:45.509283 139590144857856 logging_writer.py:48] [517100] global_step=517100, grad_norm=4.519733428955078, loss=0.6170653104782104
I0302 07:37:19.436168 139590153250560 logging_writer.py:48] [517200] global_step=517200, grad_norm=4.52047061920166, loss=0.6640902161598206
I0302 07:37:53.410368 139590144857856 logging_writer.py:48] [517300] global_step=517300, grad_norm=4.700306415557861, loss=0.6627454161643982
I0302 07:38:27.335194 139590153250560 logging_writer.py:48] [517400] global_step=517400, grad_norm=4.682663917541504, loss=0.6079846620559692
I0302 07:39:01.306925 139590144857856 logging_writer.py:48] [517500] global_step=517500, grad_norm=4.5662922859191895, loss=0.644093930721283
I0302 07:39:35.268622 139590153250560 logging_writer.py:48] [517600] global_step=517600, grad_norm=4.3769659996032715, loss=0.6049581170082092
I0302 07:40:09.253364 139590144857856 logging_writer.py:48] [517700] global_step=517700, grad_norm=4.721017360687256, loss=0.6508562564849854
I0302 07:40:43.291325 139590153250560 logging_writer.py:48] [517800] global_step=517800, grad_norm=4.791057109832764, loss=0.5963627099990845
I0302 07:41:17.259158 139590144857856 logging_writer.py:48] [517900] global_step=517900, grad_norm=5.089178562164307, loss=0.6442400813102722
I0302 07:41:51.231986 139590153250560 logging_writer.py:48] [518000] global_step=518000, grad_norm=4.344508171081543, loss=0.5776070356369019
I0302 07:42:25.204843 139590144857856 logging_writer.py:48] [518100] global_step=518100, grad_norm=5.401501655578613, loss=0.6650357246398926
I0302 07:42:59.180963 139590153250560 logging_writer.py:48] [518200] global_step=518200, grad_norm=4.916210651397705, loss=0.5960898399353027
I0302 07:43:33.183406 139590144857856 logging_writer.py:48] [518300] global_step=518300, grad_norm=4.403847694396973, loss=0.6359214782714844
I0302 07:44:07.126761 139590153250560 logging_writer.py:48] [518400] global_step=518400, grad_norm=4.388827800750732, loss=0.6546310186386108
I0302 07:44:34.794561 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:44:40.775826 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:44:48.952046 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:44:51.249640 139753105983296 submission_runner.py:411] Time since start: 182022.25s, 	Step: 518483, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.14840850234031677, 'validation/accuracy': 0.7550199627876282, 'validation/loss': 1.0548003911972046, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8194637298583984, 'test/num_examples': 10000, 'score': 176015.523532629, 'total_duration': 182022.25287151337, 'accumulated_submission_time': 176015.523532629, 'accumulated_eval_time': 5960.828776597977, 'accumulated_logging_time': 27.63849425315857}
I0302 07:44:51.354718 139589717059328 logging_writer.py:48] [518483] accumulated_eval_time=5960.828777, accumulated_logging_time=27.638494, accumulated_submission_time=176015.523533, global_step=518483, preemption_count=0, score=176015.523533, test/accuracy=0.629700, test/loss=1.819464, test/num_examples=10000, total_duration=182022.252872, train/accuracy=0.959263, train/loss=0.148409, validation/accuracy=0.755020, validation/loss=1.054800, validation/num_examples=50000
I0302 07:44:57.495885 139589725452032 logging_writer.py:48] [518500] global_step=518500, grad_norm=4.153208255767822, loss=0.5649150609970093
I0302 07:45:31.405324 139589717059328 logging_writer.py:48] [518600] global_step=518600, grad_norm=4.504209995269775, loss=0.5322369933128357
I0302 07:46:05.330432 139589725452032 logging_writer.py:48] [518700] global_step=518700, grad_norm=4.7073140144348145, loss=0.597014307975769
I0302 07:46:39.277707 139589717059328 logging_writer.py:48] [518800] global_step=518800, grad_norm=4.4606804847717285, loss=0.6110891103744507
I0302 07:47:13.376237 139589725452032 logging_writer.py:48] [518900] global_step=518900, grad_norm=4.5799407958984375, loss=0.6859555244445801
I0302 07:47:47.339743 139589717059328 logging_writer.py:48] [519000] global_step=519000, grad_norm=4.287018775939941, loss=0.6108815670013428
I0302 07:48:21.289345 139589725452032 logging_writer.py:48] [519100] global_step=519100, grad_norm=4.90582275390625, loss=0.7109441757202148
I0302 07:48:55.230643 139589717059328 logging_writer.py:48] [519200] global_step=519200, grad_norm=4.012998580932617, loss=0.5604530572891235
I0302 07:49:29.183184 139589725452032 logging_writer.py:48] [519300] global_step=519300, grad_norm=4.387790203094482, loss=0.6053627133369446
I0302 07:50:03.139056 139589717059328 logging_writer.py:48] [519400] global_step=519400, grad_norm=4.384330749511719, loss=0.5269297361373901
I0302 07:50:37.089672 139589725452032 logging_writer.py:48] [519500] global_step=519500, grad_norm=4.449895858764648, loss=0.5654069185256958
I0302 07:51:11.014885 139589717059328 logging_writer.py:48] [519600] global_step=519600, grad_norm=4.373903274536133, loss=0.6000918745994568
I0302 07:51:44.960942 139589725452032 logging_writer.py:48] [519700] global_step=519700, grad_norm=4.88511323928833, loss=0.6797745227813721
I0302 07:52:18.897538 139589717059328 logging_writer.py:48] [519800] global_step=519800, grad_norm=4.62956428527832, loss=0.6397259831428528
I0302 07:52:52.835756 139589725452032 logging_writer.py:48] [519900] global_step=519900, grad_norm=4.5740180015563965, loss=0.6665199398994446
I0302 07:53:21.556245 139753105983296 spec.py:321] Evaluating on the training split.
I0302 07:53:27.633732 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 07:53:35.812950 139753105983296 spec.py:349] Evaluating on the test split.
I0302 07:53:38.080504 139753105983296 submission_runner.py:411] Time since start: 182549.08s, 	Step: 519986, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.14823658764362335, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0555709600448608, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8197120428085327, 'test/num_examples': 10000, 'score': 176525.6594619751, 'total_duration': 182549.08373689651, 'accumulated_submission_time': 176525.6594619751, 'accumulated_eval_time': 5977.352998971939, 'accumulated_logging_time': 27.755635499954224}
I0302 07:53:38.184941 139590153250560 logging_writer.py:48] [519986] accumulated_eval_time=5977.352999, accumulated_logging_time=27.755635, accumulated_submission_time=176525.659462, global_step=519986, preemption_count=0, score=176525.659462, test/accuracy=0.631100, test/loss=1.819712, test/num_examples=10000, total_duration=182549.083737, train/accuracy=0.960878, train/loss=0.148237, validation/accuracy=0.755140, validation/loss=1.055571, validation/num_examples=50000
I0302 07:53:43.277687 139590161643264 logging_writer.py:48] [520000] global_step=520000, grad_norm=4.450395584106445, loss=0.5789885520935059
I0302 07:54:17.196146 139590153250560 logging_writer.py:48] [520100] global_step=520100, grad_norm=4.264204978942871, loss=0.6154175996780396
I0302 07:54:51.142208 139590161643264 logging_writer.py:48] [520200] global_step=520200, grad_norm=4.28725528717041, loss=0.628142774105072
I0302 07:55:25.075610 139590153250560 logging_writer.py:48] [520300] global_step=520300, grad_norm=4.831702709197998, loss=0.6257048845291138
I0302 07:55:59.026674 139590161643264 logging_writer.py:48] [520400] global_step=520400, grad_norm=4.762196063995361, loss=0.6631290912628174
I0302 07:56:32.972642 139590153250560 logging_writer.py:48] [520500] global_step=520500, grad_norm=4.747769832611084, loss=0.6196022033691406
I0302 07:57:06.924520 139590161643264 logging_writer.py:48] [520600] global_step=520600, grad_norm=4.7789435386657715, loss=0.6892419457435608
I0302 07:57:40.886711 139590153250560 logging_writer.py:48] [520700] global_step=520700, grad_norm=4.655876159667969, loss=0.6305797696113586
I0302 07:58:14.848675 139590161643264 logging_writer.py:48] [520800] global_step=520800, grad_norm=4.699562072753906, loss=0.6304444074630737
I0302 07:58:48.844244 139590153250560 logging_writer.py:48] [520900] global_step=520900, grad_norm=4.925689697265625, loss=0.6275538206100464
I0302 07:59:22.901446 139590161643264 logging_writer.py:48] [521000] global_step=521000, grad_norm=4.916604042053223, loss=0.6570016741752625
I0302 07:59:56.868896 139590153250560 logging_writer.py:48] [521100] global_step=521100, grad_norm=5.092896938323975, loss=0.6900054216384888
I0302 08:00:30.824317 139590161643264 logging_writer.py:48] [521200] global_step=521200, grad_norm=4.346256256103516, loss=0.5978744626045227
I0302 08:01:04.772202 139590153250560 logging_writer.py:48] [521300] global_step=521300, grad_norm=4.38779354095459, loss=0.6142399907112122
I0302 08:01:38.736808 139590161643264 logging_writer.py:48] [521400] global_step=521400, grad_norm=4.451271057128906, loss=0.6459522247314453
I0302 08:02:08.100535 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:02:14.081140 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:02:22.285925 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:02:24.600747 139753105983296 submission_runner.py:411] Time since start: 183075.60s, 	Step: 521488, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.15067487955093384, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.055654764175415, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.818725824356079, 'test/num_examples': 10000, 'score': 177035.51152539253, 'total_duration': 183075.60394525528, 'accumulated_submission_time': 177035.51152539253, 'accumulated_eval_time': 5993.85312795639, 'accumulated_logging_time': 27.870992183685303}
I0302 08:02:24.713913 139589725452032 logging_writer.py:48] [521488] accumulated_eval_time=5993.853128, accumulated_logging_time=27.870992, accumulated_submission_time=177035.511525, global_step=521488, preemption_count=0, score=177035.511525, test/accuracy=0.630800, test/loss=1.818726, test/num_examples=10000, total_duration=183075.603945, train/accuracy=0.959841, train/loss=0.150675, validation/accuracy=0.755060, validation/loss=1.055655, validation/num_examples=50000
I0302 08:02:29.139213 139590144857856 logging_writer.py:48] [521500] global_step=521500, grad_norm=5.010956764221191, loss=0.6549896597862244
I0302 08:03:03.017766 139589725452032 logging_writer.py:48] [521600] global_step=521600, grad_norm=4.6355671882629395, loss=0.6297869086265564
I0302 08:03:36.962767 139590144857856 logging_writer.py:48] [521700] global_step=521700, grad_norm=4.486823081970215, loss=0.5776697993278503
I0302 08:04:10.915941 139589725452032 logging_writer.py:48] [521800] global_step=521800, grad_norm=4.222353458404541, loss=0.5606255531311035
I0302 08:04:44.887216 139590144857856 logging_writer.py:48] [521900] global_step=521900, grad_norm=4.733635425567627, loss=0.6761277318000793
I0302 08:05:18.854628 139589725452032 logging_writer.py:48] [522000] global_step=522000, grad_norm=4.983746528625488, loss=0.7267976999282837
I0302 08:05:52.900031 139590144857856 logging_writer.py:48] [522100] global_step=522100, grad_norm=4.703833103179932, loss=0.6075641512870789
I0302 08:06:26.849132 139589725452032 logging_writer.py:48] [522200] global_step=522200, grad_norm=4.643271446228027, loss=0.5598157644271851
I0302 08:07:00.828181 139590144857856 logging_writer.py:48] [522300] global_step=522300, grad_norm=4.452628135681152, loss=0.6149530410766602
I0302 08:07:34.792648 139589725452032 logging_writer.py:48] [522400] global_step=522400, grad_norm=4.291633605957031, loss=0.6734028458595276
I0302 08:08:08.764649 139590144857856 logging_writer.py:48] [522500] global_step=522500, grad_norm=4.6681294441223145, loss=0.7045044898986816
I0302 08:08:42.743864 139589725452032 logging_writer.py:48] [522600] global_step=522600, grad_norm=4.445432186126709, loss=0.6814764738082886
I0302 08:09:16.704937 139590144857856 logging_writer.py:48] [522700] global_step=522700, grad_norm=4.478111267089844, loss=0.6389231085777283
I0302 08:09:50.674620 139589725452032 logging_writer.py:48] [522800] global_step=522800, grad_norm=4.508353233337402, loss=0.5823768377304077
I0302 08:10:24.622614 139590144857856 logging_writer.py:48] [522900] global_step=522900, grad_norm=4.664663791656494, loss=0.6532372832298279
I0302 08:10:54.665023 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:11:00.675422 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:11:09.003697 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:11:11.290676 139753105983296 submission_runner.py:411] Time since start: 183602.29s, 	Step: 522990, 	{'train/accuracy': 0.9611168503761292, 'train/loss': 0.14433829486370087, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.0562598705291748, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.819977045059204, 'test/num_examples': 10000, 'score': 177545.4000184536, 'total_duration': 183602.29390764236, 'accumulated_submission_time': 177545.4000184536, 'accumulated_eval_time': 6010.478732824326, 'accumulated_logging_time': 27.994094133377075}
I0302 08:11:11.394176 139589725452032 logging_writer.py:48] [522990] accumulated_eval_time=6010.478733, accumulated_logging_time=27.994094, accumulated_submission_time=177545.400018, global_step=522990, preemption_count=0, score=177545.400018, test/accuracy=0.630900, test/loss=1.819977, test/num_examples=10000, total_duration=183602.293908, train/accuracy=0.961117, train/loss=0.144338, validation/accuracy=0.754560, validation/loss=1.056260, validation/num_examples=50000
I0302 08:11:15.129322 139590153250560 logging_writer.py:48] [523000] global_step=523000, grad_norm=4.867748260498047, loss=0.6880502104759216
I0302 08:11:49.141223 139589725452032 logging_writer.py:48] [523100] global_step=523100, grad_norm=4.968080043792725, loss=0.6920185685157776
I0302 08:12:23.054950 139590153250560 logging_writer.py:48] [523200] global_step=523200, grad_norm=4.704550743103027, loss=0.6277213096618652
I0302 08:12:57.019180 139589725452032 logging_writer.py:48] [523300] global_step=523300, grad_norm=4.900797367095947, loss=0.6377182602882385
I0302 08:13:30.988064 139590153250560 logging_writer.py:48] [523400] global_step=523400, grad_norm=4.265733242034912, loss=0.6258332133293152
I0302 08:14:04.937643 139589725452032 logging_writer.py:48] [523500] global_step=523500, grad_norm=4.41820764541626, loss=0.6036269664764404
I0302 08:14:38.903595 139590153250560 logging_writer.py:48] [523600] global_step=523600, grad_norm=4.3626203536987305, loss=0.6079839468002319
I0302 08:15:12.852952 139589725452032 logging_writer.py:48] [523700] global_step=523700, grad_norm=4.760715007781982, loss=0.6621500253677368
I0302 08:15:46.829198 139590153250560 logging_writer.py:48] [523800] global_step=523800, grad_norm=4.528813362121582, loss=0.6334854364395142
I0302 08:16:20.783203 139589725452032 logging_writer.py:48] [523900] global_step=523900, grad_norm=4.43744421005249, loss=0.5776556730270386
I0302 08:16:54.734950 139590153250560 logging_writer.py:48] [524000] global_step=524000, grad_norm=4.817258834838867, loss=0.6497170329093933
I0302 08:17:28.701622 139589725452032 logging_writer.py:48] [524100] global_step=524100, grad_norm=4.334692478179932, loss=0.5902520418167114
I0302 08:18:02.741366 139590153250560 logging_writer.py:48] [524200] global_step=524200, grad_norm=4.179399013519287, loss=0.6378256678581238
I0302 08:18:36.715440 139589725452032 logging_writer.py:48] [524300] global_step=524300, grad_norm=4.414332389831543, loss=0.5954492688179016
I0302 08:19:10.668080 139590153250560 logging_writer.py:48] [524400] global_step=524400, grad_norm=4.260861873626709, loss=0.5639889240264893
I0302 08:19:41.390737 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:19:47.404608 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:19:55.586283 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:19:57.841899 139753105983296 submission_runner.py:411] Time since start: 184128.85s, 	Step: 524492, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.14630965888500214, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0564887523651123, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8197730779647827, 'test/num_examples': 10000, 'score': 178055.3327858448, 'total_duration': 184128.84509038925, 'accumulated_submission_time': 178055.3327858448, 'accumulated_eval_time': 6026.929802417755, 'accumulated_logging_time': 28.10876989364624}
I0302 08:19:57.946595 139590153250560 logging_writer.py:48] [524492] accumulated_eval_time=6026.929802, accumulated_logging_time=28.108770, accumulated_submission_time=178055.332786, global_step=524492, preemption_count=0, score=178055.332786, test/accuracy=0.630400, test/loss=1.819773, test/num_examples=10000, total_duration=184128.845090, train/accuracy=0.960041, train/loss=0.146310, validation/accuracy=0.754760, validation/loss=1.056489, validation/num_examples=50000
I0302 08:20:01.016416 139590161643264 logging_writer.py:48] [524500] global_step=524500, grad_norm=4.3383283615112305, loss=0.6407742500305176
I0302 08:20:34.890219 139590153250560 logging_writer.py:48] [524600] global_step=524600, grad_norm=4.924895286560059, loss=0.703302264213562
I0302 08:21:08.836636 139590161643264 logging_writer.py:48] [524700] global_step=524700, grad_norm=4.415283203125, loss=0.5889543294906616
I0302 08:21:42.814364 139590153250560 logging_writer.py:48] [524800] global_step=524800, grad_norm=4.7980780601501465, loss=0.6613543629646301
I0302 08:22:16.761264 139590161643264 logging_writer.py:48] [524900] global_step=524900, grad_norm=4.5313005447387695, loss=0.6191830039024353
I0302 08:22:50.732476 139590153250560 logging_writer.py:48] [525000] global_step=525000, grad_norm=4.867355823516846, loss=0.6026478409767151
I0302 08:23:24.694264 139590161643264 logging_writer.py:48] [525100] global_step=525100, grad_norm=4.26405668258667, loss=0.5850197076797485
I0302 08:23:58.621306 139590153250560 logging_writer.py:48] [525200] global_step=525200, grad_norm=4.692548751831055, loss=0.6155405640602112
I0302 08:24:32.647115 139590161643264 logging_writer.py:48] [525300] global_step=525300, grad_norm=4.779125213623047, loss=0.6070189476013184
I0302 08:25:06.621268 139590153250560 logging_writer.py:48] [525400] global_step=525400, grad_norm=5.507846832275391, loss=0.5967274904251099
I0302 08:25:40.587781 139590161643264 logging_writer.py:48] [525500] global_step=525500, grad_norm=4.559791564941406, loss=0.6491976976394653
I0302 08:26:14.577950 139590153250560 logging_writer.py:48] [525600] global_step=525600, grad_norm=4.306491851806641, loss=0.5522065162658691
I0302 08:26:48.561484 139590161643264 logging_writer.py:48] [525700] global_step=525700, grad_norm=4.2068095207214355, loss=0.6067659258842468
I0302 08:27:22.513931 139590153250560 logging_writer.py:48] [525800] global_step=525800, grad_norm=4.5581955909729, loss=0.5823789834976196
I0302 08:27:56.494086 139590161643264 logging_writer.py:48] [525900] global_step=525900, grad_norm=4.589929103851318, loss=0.6617361903190613
I0302 08:28:27.879938 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:28:33.921213 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:28:42.283733 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:28:44.566309 139753105983296 submission_runner.py:411] Time since start: 184655.57s, 	Step: 525994, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14503511786460876, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 1.0553193092346191, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8185182809829712, 'test/num_examples': 10000, 'score': 178565.2026667595, 'total_duration': 184655.56948399544, 'accumulated_submission_time': 178565.2026667595, 'accumulated_eval_time': 6043.616086244583, 'accumulated_logging_time': 28.223960638046265}
I0302 08:28:44.735381 139590144857856 logging_writer.py:48] [525994] accumulated_eval_time=6043.616086, accumulated_logging_time=28.223961, accumulated_submission_time=178565.202667, global_step=525994, preemption_count=0, score=178565.202667, test/accuracy=0.631000, test/loss=1.818518, test/num_examples=10000, total_duration=184655.569484, train/accuracy=0.960639, train/loss=0.145035, validation/accuracy=0.755060, validation/loss=1.055319, validation/num_examples=50000
I0302 08:28:47.151205 139590186821376 logging_writer.py:48] [526000] global_step=526000, grad_norm=4.608287334442139, loss=0.6613143086433411
I0302 08:29:21.043539 139590144857856 logging_writer.py:48] [526100] global_step=526100, grad_norm=5.14763069152832, loss=0.6876236796379089
I0302 08:29:54.969215 139590186821376 logging_writer.py:48] [526200] global_step=526200, grad_norm=4.306858062744141, loss=0.5998950600624084
I0302 08:30:29.024895 139590144857856 logging_writer.py:48] [526300] global_step=526300, grad_norm=4.768186092376709, loss=0.6316666603088379
I0302 08:31:02.972944 139590186821376 logging_writer.py:48] [526400] global_step=526400, grad_norm=4.361755847930908, loss=0.6610450744628906
I0302 08:31:36.920526 139590144857856 logging_writer.py:48] [526500] global_step=526500, grad_norm=4.921753406524658, loss=0.6774120330810547
I0302 08:32:10.876384 139590186821376 logging_writer.py:48] [526600] global_step=526600, grad_norm=4.383228778839111, loss=0.6333427429199219
I0302 08:32:44.845239 139590144857856 logging_writer.py:48] [526700] global_step=526700, grad_norm=4.34712028503418, loss=0.5459983944892883
I0302 08:33:18.796349 139590186821376 logging_writer.py:48] [526800] global_step=526800, grad_norm=4.469886302947998, loss=0.6118039488792419
I0302 08:33:52.758260 139590144857856 logging_writer.py:48] [526900] global_step=526900, grad_norm=4.528797626495361, loss=0.6147116422653198
I0302 08:34:26.717512 139590186821376 logging_writer.py:48] [527000] global_step=527000, grad_norm=4.463212490081787, loss=0.6847518682479858
I0302 08:35:00.676973 139590144857856 logging_writer.py:48] [527100] global_step=527100, grad_norm=4.530501842498779, loss=0.5976051092147827
I0302 08:35:34.658710 139590186821376 logging_writer.py:48] [527200] global_step=527200, grad_norm=4.537571430206299, loss=0.5945672988891602
I0302 08:36:08.617518 139590144857856 logging_writer.py:48] [527300] global_step=527300, grad_norm=4.817507266998291, loss=0.611477255821228
I0302 08:36:42.643746 139590186821376 logging_writer.py:48] [527400] global_step=527400, grad_norm=4.571957588195801, loss=0.608805775642395
I0302 08:37:14.654408 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:37:20.680170 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:37:29.020139 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:37:31.320811 139753105983296 submission_runner.py:411] Time since start: 185182.32s, 	Step: 527495, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.1455485224723816, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0551986694335938, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8199076652526855, 'test/num_examples': 10000, 'score': 179075.05401158333, 'total_duration': 185182.32403969765, 'accumulated_submission_time': 179075.05401158333, 'accumulated_eval_time': 6060.282440185547, 'accumulated_logging_time': 28.40710473060608}
I0302 08:37:31.427000 139590170035968 logging_writer.py:48] [527495] accumulated_eval_time=6060.282440, accumulated_logging_time=28.407105, accumulated_submission_time=179075.054012, global_step=527495, preemption_count=0, score=179075.054012, test/accuracy=0.630300, test/loss=1.819908, test/num_examples=10000, total_duration=185182.324040, train/accuracy=0.961037, train/loss=0.145549, validation/accuracy=0.755140, validation/loss=1.055199, validation/num_examples=50000
I0302 08:37:33.472386 139590178428672 logging_writer.py:48] [527500] global_step=527500, grad_norm=4.283684730529785, loss=0.6328105926513672
I0302 08:38:07.362534 139590170035968 logging_writer.py:48] [527600] global_step=527600, grad_norm=4.860678195953369, loss=0.674281656742096
I0302 08:38:41.256386 139590178428672 logging_writer.py:48] [527700] global_step=527700, grad_norm=4.7418999671936035, loss=0.5974234938621521
I0302 08:39:15.209861 139590170035968 logging_writer.py:48] [527800] global_step=527800, grad_norm=4.881025791168213, loss=0.7180585265159607
I0302 08:39:49.146440 139590178428672 logging_writer.py:48] [527900] global_step=527900, grad_norm=4.594942569732666, loss=0.6280964016914368
I0302 08:40:23.093352 139590170035968 logging_writer.py:48] [528000] global_step=528000, grad_norm=4.8037590980529785, loss=0.650813639163971
I0302 08:40:57.047422 139590178428672 logging_writer.py:48] [528100] global_step=528100, grad_norm=4.363041877746582, loss=0.6149499416351318
I0302 08:41:30.995983 139590170035968 logging_writer.py:48] [528200] global_step=528200, grad_norm=5.674734115600586, loss=0.7621087431907654
I0302 08:42:04.919883 139590178428672 logging_writer.py:48] [528300] global_step=528300, grad_norm=4.27501106262207, loss=0.5772767663002014
I0302 08:42:38.957476 139590170035968 logging_writer.py:48] [528400] global_step=528400, grad_norm=4.540348052978516, loss=0.6392735242843628
I0302 08:43:12.875164 139590178428672 logging_writer.py:48] [528500] global_step=528500, grad_norm=4.336678504943848, loss=0.6998388767242432
I0302 08:43:46.829126 139590170035968 logging_writer.py:48] [528600] global_step=528600, grad_norm=4.417611122131348, loss=0.6048517227172852
I0302 08:44:20.782819 139590178428672 logging_writer.py:48] [528700] global_step=528700, grad_norm=4.516427040100098, loss=0.6734845638275146
I0302 08:44:54.746409 139590170035968 logging_writer.py:48] [528800] global_step=528800, grad_norm=4.500466823577881, loss=0.6548750400543213
I0302 08:45:28.679708 139590178428672 logging_writer.py:48] [528900] global_step=528900, grad_norm=4.97728157043457, loss=0.6541681289672852
I0302 08:46:01.404938 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:46:07.426484 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:46:15.875509 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:46:18.149249 139753105983296 submission_runner.py:411] Time since start: 185709.15s, 	Step: 528998, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14688728749752045, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0558708906173706, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8188236951828003, 'test/num_examples': 10000, 'score': 179584.9684138298, 'total_duration': 185709.15247774124, 'accumulated_submission_time': 179584.9684138298, 'accumulated_eval_time': 6077.02671456337, 'accumulated_logging_time': 28.523298025131226}
I0302 08:46:18.255105 139590153250560 logging_writer.py:48] [528998] accumulated_eval_time=6077.026715, accumulated_logging_time=28.523298, accumulated_submission_time=179584.968414, global_step=528998, preemption_count=0, score=179584.968414, test/accuracy=0.630800, test/loss=1.818824, test/num_examples=10000, total_duration=185709.152478, train/accuracy=0.961137, train/loss=0.146887, validation/accuracy=0.754840, validation/loss=1.055871, validation/num_examples=50000
I0302 08:46:19.282905 139590161643264 logging_writer.py:48] [529000] global_step=529000, grad_norm=4.666402339935303, loss=0.6606186628341675
I0302 08:46:53.183870 139590153250560 logging_writer.py:48] [529100] global_step=529100, grad_norm=5.152900695800781, loss=0.6435092687606812
I0302 08:47:27.127755 139590161643264 logging_writer.py:48] [529200] global_step=529200, grad_norm=4.852245330810547, loss=0.6106501817703247
I0302 08:48:01.067001 139590153250560 logging_writer.py:48] [529300] global_step=529300, grad_norm=4.639767169952393, loss=0.6639271974563599
I0302 08:48:35.013935 139590161643264 logging_writer.py:48] [529400] global_step=529400, grad_norm=4.922439098358154, loss=0.6173038482666016
I0302 08:49:09.051918 139590153250560 logging_writer.py:48] [529500] global_step=529500, grad_norm=4.769683361053467, loss=0.6310331225395203
I0302 08:49:42.999469 139590161643264 logging_writer.py:48] [529600] global_step=529600, grad_norm=4.861830711364746, loss=0.7275863885879517
I0302 08:50:16.947717 139590153250560 logging_writer.py:48] [529700] global_step=529700, grad_norm=4.631624221801758, loss=0.6393152475357056
I0302 08:50:50.883463 139590161643264 logging_writer.py:48] [529800] global_step=529800, grad_norm=4.765164852142334, loss=0.6345339417457581
I0302 08:51:24.808777 139590153250560 logging_writer.py:48] [529900] global_step=529900, grad_norm=4.678308486938477, loss=0.6649608016014099
I0302 08:51:58.767100 139590161643264 logging_writer.py:48] [530000] global_step=530000, grad_norm=4.201841354370117, loss=0.5703553557395935
I0302 08:52:32.731587 139590153250560 logging_writer.py:48] [530100] global_step=530100, grad_norm=4.42370080947876, loss=0.6252802014350891
I0302 08:53:06.676766 139590161643264 logging_writer.py:48] [530200] global_step=530200, grad_norm=4.933915138244629, loss=0.595888614654541
I0302 08:53:40.596287 139590153250560 logging_writer.py:48] [530300] global_step=530300, grad_norm=5.024883270263672, loss=0.6632274389266968
I0302 08:54:14.554942 139590161643264 logging_writer.py:48] [530400] global_step=530400, grad_norm=4.491268157958984, loss=0.6793513298034668
I0302 08:54:48.503614 139590153250560 logging_writer.py:48] [530500] global_step=530500, grad_norm=4.517191410064697, loss=0.7256905436515808
I0302 08:54:48.510769 139753105983296 spec.py:321] Evaluating on the training split.
I0302 08:54:54.519535 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 08:55:02.779029 139753105983296 spec.py:349] Evaluating on the test split.
I0302 08:55:05.392046 139753105983296 submission_runner.py:411] Time since start: 186236.40s, 	Step: 530501, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14666077494621277, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 1.0554156303405762, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8195521831512451, 'test/num_examples': 10000, 'score': 180095.15888261795, 'total_duration': 186236.39528346062, 'accumulated_submission_time': 180095.15888261795, 'accumulated_eval_time': 6093.907920360565, 'accumulated_logging_time': 28.641568899154663}
I0302 08:55:05.498509 139590144857856 logging_writer.py:48] [530501] accumulated_eval_time=6093.907920, accumulated_logging_time=28.641569, accumulated_submission_time=180095.158883, global_step=530501, preemption_count=0, score=180095.158883, test/accuracy=0.631000, test/loss=1.819552, test/num_examples=10000, total_duration=186236.395283, train/accuracy=0.960280, train/loss=0.146661, validation/accuracy=0.754460, validation/loss=1.055416, validation/num_examples=50000
I0302 08:55:39.423941 139590170035968 logging_writer.py:48] [530600] global_step=530600, grad_norm=4.6803083419799805, loss=0.6797192096710205
I0302 08:56:13.358369 139590144857856 logging_writer.py:48] [530700] global_step=530700, grad_norm=5.276914119720459, loss=0.6266775131225586
I0302 08:56:47.251113 139590170035968 logging_writer.py:48] [530800] global_step=530800, grad_norm=4.577754020690918, loss=0.659220814704895
I0302 08:57:21.213250 139590144857856 logging_writer.py:48] [530900] global_step=530900, grad_norm=4.629185199737549, loss=0.6733078360557556
I0302 08:57:55.179607 139590170035968 logging_writer.py:48] [531000] global_step=531000, grad_norm=4.6680121421813965, loss=0.6353493332862854
I0302 08:58:29.123375 139590144857856 logging_writer.py:48] [531100] global_step=531100, grad_norm=4.562836647033691, loss=0.5990725159645081
I0302 08:59:03.060257 139590170035968 logging_writer.py:48] [531200] global_step=531200, grad_norm=4.737791538238525, loss=0.5995814800262451
I0302 08:59:36.992321 139590144857856 logging_writer.py:48] [531300] global_step=531300, grad_norm=4.446206092834473, loss=0.6220210194587708
I0302 09:00:10.940705 139590170035968 logging_writer.py:48] [531400] global_step=531400, grad_norm=4.724215030670166, loss=0.5911235213279724
I0302 09:00:44.904747 139590144857856 logging_writer.py:48] [531500] global_step=531500, grad_norm=4.859038829803467, loss=0.6619341373443604
I0302 09:01:18.957222 139590170035968 logging_writer.py:48] [531600] global_step=531600, grad_norm=4.777089595794678, loss=0.5967976450920105
I0302 09:01:52.921128 139590144857856 logging_writer.py:48] [531700] global_step=531700, grad_norm=4.297779560089111, loss=0.6204103827476501
I0302 09:02:26.855463 139590170035968 logging_writer.py:48] [531800] global_step=531800, grad_norm=4.814717769622803, loss=0.6251645088195801
I0302 09:03:00.783710 139590144857856 logging_writer.py:48] [531900] global_step=531900, grad_norm=4.199002742767334, loss=0.5731678009033203
I0302 09:03:34.730798 139590170035968 logging_writer.py:48] [532000] global_step=532000, grad_norm=4.460878849029541, loss=0.6139379143714905
I0302 09:03:35.553037 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:03:41.607987 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:03:49.955860 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:03:52.156463 139753105983296 submission_runner.py:411] Time since start: 186763.16s, 	Step: 532004, 	{'train/accuracy': 0.9609972834587097, 'train/loss': 0.14301763474941254, 'validation/accuracy': 0.7551400065422058, 'validation/loss': 1.0556042194366455, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8177498579025269, 'test/num_examples': 10000, 'score': 180605.15130925179, 'total_duration': 186763.15968847275, 'accumulated_submission_time': 180605.15130925179, 'accumulated_eval_time': 6110.511300086975, 'accumulated_logging_time': 28.757806062698364}
I0302 09:03:52.261908 139589725452032 logging_writer.py:48] [532004] accumulated_eval_time=6110.511300, accumulated_logging_time=28.757806, accumulated_submission_time=180605.151309, global_step=532004, preemption_count=0, score=180605.151309, test/accuracy=0.631700, test/loss=1.817750, test/num_examples=10000, total_duration=186763.159688, train/accuracy=0.960997, train/loss=0.143018, validation/accuracy=0.755140, validation/loss=1.055604, validation/num_examples=50000
I0302 09:04:25.154938 139590144857856 logging_writer.py:48] [532100] global_step=532100, grad_norm=4.211201190948486, loss=0.6066969037055969
I0302 09:04:59.098815 139589725452032 logging_writer.py:48] [532200] global_step=532200, grad_norm=4.360454559326172, loss=0.6697958111763
I0302 09:05:33.054052 139590144857856 logging_writer.py:48] [532300] global_step=532300, grad_norm=4.086059093475342, loss=0.5283468961715698
I0302 09:06:06.971575 139589725452032 logging_writer.py:48] [532400] global_step=532400, grad_norm=4.807953357696533, loss=0.660306990146637
I0302 09:06:40.919000 139590144857856 logging_writer.py:48] [532500] global_step=532500, grad_norm=4.850554466247559, loss=0.5905120372772217
I0302 09:07:14.890636 139589725452032 logging_writer.py:48] [532600] global_step=532600, grad_norm=4.352219581604004, loss=0.584781289100647
I0302 09:07:48.915463 139590144857856 logging_writer.py:48] [532700] global_step=532700, grad_norm=4.6490936279296875, loss=0.6747679710388184
I0302 09:08:22.895890 139589725452032 logging_writer.py:48] [532800] global_step=532800, grad_norm=4.593184471130371, loss=0.6562131643295288
I0302 09:08:56.842570 139590144857856 logging_writer.py:48] [532900] global_step=532900, grad_norm=4.315092086791992, loss=0.6176701784133911
I0302 09:09:30.795322 139589725452032 logging_writer.py:48] [533000] global_step=533000, grad_norm=4.42773962020874, loss=0.6308175921440125
I0302 09:10:04.758345 139590144857856 logging_writer.py:48] [533100] global_step=533100, grad_norm=4.308998107910156, loss=0.6248818039894104
I0302 09:10:38.730126 139589725452032 logging_writer.py:48] [533200] global_step=533200, grad_norm=4.797354221343994, loss=0.6194431185722351
I0302 09:11:12.694176 139590144857856 logging_writer.py:48] [533300] global_step=533300, grad_norm=4.747437477111816, loss=0.6170139312744141
I0302 09:11:46.630980 139589725452032 logging_writer.py:48] [533400] global_step=533400, grad_norm=4.3910346031188965, loss=0.6844958662986755
I0302 09:12:20.603140 139590144857856 logging_writer.py:48] [533500] global_step=533500, grad_norm=4.571995258331299, loss=0.617164671421051
I0302 09:12:22.436221 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:12:28.563357 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:12:36.753304 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:12:39.037626 139753105983296 submission_runner.py:411] Time since start: 187290.04s, 	Step: 533507, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14593279361724854, 'validation/accuracy': 0.7546799778938293, 'validation/loss': 1.0548255443572998, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8190414905548096, 'test/num_examples': 10000, 'score': 181115.2619752884, 'total_duration': 187290.04085493088, 'accumulated_submission_time': 181115.2619752884, 'accumulated_eval_time': 6127.112663269043, 'accumulated_logging_time': 28.87390160560608}
I0302 09:12:39.146496 139590170035968 logging_writer.py:48] [533507] accumulated_eval_time=6127.112663, accumulated_logging_time=28.873902, accumulated_submission_time=181115.261975, global_step=533507, preemption_count=0, score=181115.261975, test/accuracy=0.630300, test/loss=1.819041, test/num_examples=10000, total_duration=187290.040855, train/accuracy=0.961097, train/loss=0.145933, validation/accuracy=0.754680, validation/loss=1.054826, validation/num_examples=50000
I0302 09:13:11.037036 139590178428672 logging_writer.py:48] [533600] global_step=533600, grad_norm=4.2631754875183105, loss=0.57956862449646
I0302 09:13:45.045678 139590170035968 logging_writer.py:48] [533700] global_step=533700, grad_norm=4.8043060302734375, loss=0.6483579874038696
I0302 09:14:18.981306 139590178428672 logging_writer.py:48] [533800] global_step=533800, grad_norm=4.784504413604736, loss=0.6774155497550964
I0302 09:14:52.949321 139590170035968 logging_writer.py:48] [533900] global_step=533900, grad_norm=4.638867378234863, loss=0.6591373682022095
I0302 09:15:26.891621 139590178428672 logging_writer.py:48] [534000] global_step=534000, grad_norm=4.481601715087891, loss=0.6219505667686462
I0302 09:16:00.816104 139590170035968 logging_writer.py:48] [534100] global_step=534100, grad_norm=4.493186950683594, loss=0.6382551789283752
I0302 09:16:34.754122 139590178428672 logging_writer.py:48] [534200] global_step=534200, grad_norm=4.561385154724121, loss=0.6642476320266724
I0302 09:17:08.707911 139590170035968 logging_writer.py:48] [534300] global_step=534300, grad_norm=4.257505416870117, loss=0.5887435674667358
I0302 09:17:42.647598 139590178428672 logging_writer.py:48] [534400] global_step=534400, grad_norm=4.259161472320557, loss=0.5389224886894226
I0302 09:18:16.586282 139590170035968 logging_writer.py:48] [534500] global_step=534500, grad_norm=4.725248336791992, loss=0.6576407551765442
I0302 09:18:50.533877 139590178428672 logging_writer.py:48] [534600] global_step=534600, grad_norm=4.583422660827637, loss=0.553376317024231
I0302 09:19:24.468860 139590170035968 logging_writer.py:48] [534700] global_step=534700, grad_norm=4.46978235244751, loss=0.5863982439041138
I0302 09:19:58.503789 139590178428672 logging_writer.py:48] [534800] global_step=534800, grad_norm=4.500234127044678, loss=0.5818182826042175
I0302 09:20:32.434143 139590170035968 logging_writer.py:48] [534900] global_step=534900, grad_norm=4.544118881225586, loss=0.647707998752594
I0302 09:21:06.354510 139590178428672 logging_writer.py:48] [535000] global_step=535000, grad_norm=5.284553050994873, loss=0.6616950035095215
I0302 09:21:09.216145 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:21:15.268826 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:21:23.830202 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:21:26.116168 139753105983296 submission_runner.py:411] Time since start: 187817.12s, 	Step: 535010, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14399445056915283, 'validation/accuracy': 0.7545799612998962, 'validation/loss': 1.0565119981765747, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8202309608459473, 'test/num_examples': 10000, 'score': 181625.2687883377, 'total_duration': 187817.1193869114, 'accumulated_submission_time': 181625.2687883377, 'accumulated_eval_time': 6144.01261806488, 'accumulated_logging_time': 28.99262237548828}
I0302 09:21:26.216920 139590144857856 logging_writer.py:48] [535010] accumulated_eval_time=6144.012618, accumulated_logging_time=28.992622, accumulated_submission_time=181625.268788, global_step=535010, preemption_count=0, score=181625.268788, test/accuracy=0.630100, test/loss=1.820231, test/num_examples=10000, total_duration=187817.119387, train/accuracy=0.961456, train/loss=0.143994, validation/accuracy=0.754580, validation/loss=1.056512, validation/num_examples=50000
I0302 09:21:57.078724 139590153250560 logging_writer.py:48] [535100] global_step=535100, grad_norm=4.851151466369629, loss=0.6495145559310913
I0302 09:22:30.982445 139590144857856 logging_writer.py:48] [535200] global_step=535200, grad_norm=4.5402679443359375, loss=0.6114785671234131
I0302 09:23:04.942991 139590153250560 logging_writer.py:48] [535300] global_step=535300, grad_norm=4.308427810668945, loss=0.6154154539108276
I0302 09:23:38.867819 139590144857856 logging_writer.py:48] [535400] global_step=535400, grad_norm=4.614857196807861, loss=0.5774359107017517
I0302 09:24:12.822950 139590153250560 logging_writer.py:48] [535500] global_step=535500, grad_norm=4.885735988616943, loss=0.6609164476394653
I0302 09:24:46.814672 139590144857856 logging_writer.py:48] [535600] global_step=535600, grad_norm=4.40004825592041, loss=0.6123464703559875
I0302 09:25:20.777706 139590153250560 logging_writer.py:48] [535700] global_step=535700, grad_norm=4.259109020233154, loss=0.5661628246307373
I0302 09:25:54.757097 139590144857856 logging_writer.py:48] [535800] global_step=535800, grad_norm=4.088525772094727, loss=0.5435060262680054
I0302 09:26:28.799681 139590153250560 logging_writer.py:48] [535900] global_step=535900, grad_norm=4.600008487701416, loss=0.544222354888916
I0302 09:27:02.737951 139590144857856 logging_writer.py:48] [536000] global_step=536000, grad_norm=4.637012481689453, loss=0.6352881193161011
I0302 09:27:36.711441 139590153250560 logging_writer.py:48] [536100] global_step=536100, grad_norm=4.8465447425842285, loss=0.6375628709793091
I0302 09:28:10.660299 139590144857856 logging_writer.py:48] [536200] global_step=536200, grad_norm=5.026890277862549, loss=0.7071170806884766
I0302 09:28:44.643803 139590153250560 logging_writer.py:48] [536300] global_step=536300, grad_norm=4.267107963562012, loss=0.5785236358642578
I0302 09:29:18.573172 139590144857856 logging_writer.py:48] [536400] global_step=536400, grad_norm=4.517509460449219, loss=0.651969313621521
I0302 09:29:52.542185 139590153250560 logging_writer.py:48] [536500] global_step=536500, grad_norm=4.949796676635742, loss=0.7028029561042786
I0302 09:29:56.429116 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:30:02.530323 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:30:10.762650 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:30:13.071205 139753105983296 submission_runner.py:411] Time since start: 188344.07s, 	Step: 536513, 	{'train/accuracy': 0.9588249325752258, 'train/loss': 0.15054012835025787, 'validation/accuracy': 0.7545199990272522, 'validation/loss': 1.0554897785186768, 'validation/num_examples': 50000, 'test/accuracy': 0.6297000050544739, 'test/loss': 1.8187036514282227, 'test/num_examples': 10000, 'score': 182135.4176516533, 'total_duration': 188344.07443284988, 'accumulated_submission_time': 182135.4176516533, 'accumulated_eval_time': 6160.654649019241, 'accumulated_logging_time': 29.103929042816162}
I0302 09:30:13.178585 139589708666624 logging_writer.py:48] [536513] accumulated_eval_time=6160.654649, accumulated_logging_time=29.103929, accumulated_submission_time=182135.417652, global_step=536513, preemption_count=0, score=182135.417652, test/accuracy=0.629700, test/loss=1.818704, test/num_examples=10000, total_duration=188344.074433, train/accuracy=0.958825, train/loss=0.150540, validation/accuracy=0.754520, validation/loss=1.055490, validation/num_examples=50000
I0302 09:30:43.051980 139589717059328 logging_writer.py:48] [536600] global_step=536600, grad_norm=4.878097057342529, loss=0.6124823093414307
I0302 09:31:16.940095 139589708666624 logging_writer.py:48] [536700] global_step=536700, grad_norm=4.057792663574219, loss=0.5500786900520325
I0302 09:31:50.875344 139589717059328 logging_writer.py:48] [536800] global_step=536800, grad_norm=4.6141204833984375, loss=0.6430356502532959
I0302 09:32:24.906702 139589708666624 logging_writer.py:48] [536900] global_step=536900, grad_norm=4.4019060134887695, loss=0.584908127784729
I0302 09:32:58.863171 139589717059328 logging_writer.py:48] [537000] global_step=537000, grad_norm=4.659241676330566, loss=0.6694923043251038
I0302 09:33:32.819242 139589708666624 logging_writer.py:48] [537100] global_step=537100, grad_norm=4.845806121826172, loss=0.6190512776374817
I0302 09:34:06.762779 139589717059328 logging_writer.py:48] [537200] global_step=537200, grad_norm=4.316847324371338, loss=0.5921905636787415
I0302 09:34:40.731232 139589708666624 logging_writer.py:48] [537300] global_step=537300, grad_norm=4.3694891929626465, loss=0.6159914135932922
I0302 09:35:14.664188 139589717059328 logging_writer.py:48] [537400] global_step=537400, grad_norm=4.837569713592529, loss=0.7005303502082825
I0302 09:35:48.632287 139589708666624 logging_writer.py:48] [537500] global_step=537500, grad_norm=5.001805305480957, loss=0.6608300805091858
I0302 09:36:22.587661 139589717059328 logging_writer.py:48] [537600] global_step=537600, grad_norm=4.287075042724609, loss=0.6548837423324585
I0302 09:36:56.522807 139589708666624 logging_writer.py:48] [537700] global_step=537700, grad_norm=4.388454437255859, loss=0.5862178206443787
I0302 09:37:30.438850 139589717059328 logging_writer.py:48] [537800] global_step=537800, grad_norm=4.826873302459717, loss=0.6368868947029114
I0302 09:38:04.362743 139589708666624 logging_writer.py:48] [537900] global_step=537900, grad_norm=4.731173038482666, loss=0.605669379234314
I0302 09:38:38.420010 139589717059328 logging_writer.py:48] [538000] global_step=538000, grad_norm=4.9969048500061035, loss=0.6519613862037659
I0302 09:38:43.307859 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:38:49.277454 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:38:57.472198 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:38:59.817531 139753105983296 submission_runner.py:411] Time since start: 188870.82s, 	Step: 538016, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.1456693708896637, 'validation/accuracy': 0.754539966583252, 'validation/loss': 1.0547114610671997, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.8173961639404297, 'test/num_examples': 10000, 'score': 182645.48416352272, 'total_duration': 188870.82076334953, 'accumulated_submission_time': 182645.48416352272, 'accumulated_eval_time': 6177.164265871048, 'accumulated_logging_time': 29.22213888168335}
I0302 09:38:59.931938 139590178428672 logging_writer.py:48] [538016] accumulated_eval_time=6177.164266, accumulated_logging_time=29.222139, accumulated_submission_time=182645.484164, global_step=538016, preemption_count=0, score=182645.484164, test/accuracy=0.630000, test/loss=1.817396, test/num_examples=10000, total_duration=188870.820763, train/accuracy=0.961436, train/loss=0.145669, validation/accuracy=0.754540, validation/loss=1.054711, validation/num_examples=50000
I0302 09:39:28.771967 139590186821376 logging_writer.py:48] [538100] global_step=538100, grad_norm=4.123256683349609, loss=0.5517666339874268
I0302 09:40:02.700896 139590178428672 logging_writer.py:48] [538200] global_step=538200, grad_norm=4.561939239501953, loss=0.6140081286430359
I0302 09:40:36.656229 139590186821376 logging_writer.py:48] [538300] global_step=538300, grad_norm=4.643345355987549, loss=0.6319561004638672
I0302 09:41:10.600636 139590178428672 logging_writer.py:48] [538400] global_step=538400, grad_norm=4.439245700836182, loss=0.6228046417236328
I0302 09:41:44.568639 139590186821376 logging_writer.py:48] [538500] global_step=538500, grad_norm=4.644228935241699, loss=0.5948712825775146
I0302 09:42:18.526115 139590178428672 logging_writer.py:48] [538600] global_step=538600, grad_norm=4.153244972229004, loss=0.5739328265190125
I0302 09:42:52.480180 139590186821376 logging_writer.py:48] [538700] global_step=538700, grad_norm=4.197042465209961, loss=0.5732921361923218
I0302 09:43:26.437736 139590178428672 logging_writer.py:48] [538800] global_step=538800, grad_norm=4.256139755249023, loss=0.570341944694519
I0302 09:44:00.380344 139590186821376 logging_writer.py:48] [538900] global_step=538900, grad_norm=4.182219505310059, loss=0.5885088443756104
I0302 09:44:34.402177 139590178428672 logging_writer.py:48] [539000] global_step=539000, grad_norm=4.655495643615723, loss=0.6511595845222473
I0302 09:45:08.383010 139590186821376 logging_writer.py:48] [539100] global_step=539100, grad_norm=5.139120578765869, loss=0.5892687439918518
I0302 09:45:42.350529 139590178428672 logging_writer.py:48] [539200] global_step=539200, grad_norm=4.2364678382873535, loss=0.5995768308639526
I0302 09:46:16.315615 139590186821376 logging_writer.py:48] [539300] global_step=539300, grad_norm=4.6244635581970215, loss=0.6083716750144958
I0302 09:46:50.273022 139590178428672 logging_writer.py:48] [539400] global_step=539400, grad_norm=4.2580461502075195, loss=0.5739209651947021
I0302 09:47:24.202363 139590186821376 logging_writer.py:48] [539500] global_step=539500, grad_norm=4.540413856506348, loss=0.5600625872612
I0302 09:47:30.136065 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:47:36.189570 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:47:44.449111 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:47:47.049012 139753105983296 submission_runner.py:411] Time since start: 189398.05s, 	Step: 539519, 	{'train/accuracy': 0.9617546200752258, 'train/loss': 0.14454230666160583, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0554250478744507, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8193137645721436, 'test/num_examples': 10000, 'score': 183155.62402701378, 'total_duration': 189398.0522556305, 'accumulated_submission_time': 183155.62402701378, 'accumulated_eval_time': 6194.077189683914, 'accumulated_logging_time': 29.346582889556885}
I0302 09:47:47.135749 139589725452032 logging_writer.py:48] [539519] accumulated_eval_time=6194.077190, accumulated_logging_time=29.346583, accumulated_submission_time=183155.624027, global_step=539519, preemption_count=0, score=183155.624027, test/accuracy=0.630300, test/loss=1.819314, test/num_examples=10000, total_duration=189398.052256, train/accuracy=0.961755, train/loss=0.144542, validation/accuracy=0.754760, validation/loss=1.055425, validation/num_examples=50000
I0302 09:48:14.923646 139590144857856 logging_writer.py:48] [539600] global_step=539600, grad_norm=4.498615264892578, loss=0.6532831192016602
I0302 09:48:48.842334 139589725452032 logging_writer.py:48] [539700] global_step=539700, grad_norm=4.465049743652344, loss=0.6263131499290466
I0302 09:49:22.741767 139590144857856 logging_writer.py:48] [539800] global_step=539800, grad_norm=4.462378025054932, loss=0.6166239976882935
I0302 09:49:56.687151 139589725452032 logging_writer.py:48] [539900] global_step=539900, grad_norm=4.400333404541016, loss=0.5971899032592773
I0302 09:50:30.632241 139590144857856 logging_writer.py:48] [540000] global_step=540000, grad_norm=4.3829874992370605, loss=0.6316253542900085
I0302 09:51:04.646923 139589725452032 logging_writer.py:48] [540100] global_step=540100, grad_norm=4.201786518096924, loss=0.640776515007019
I0302 09:51:38.582650 139590144857856 logging_writer.py:48] [540200] global_step=540200, grad_norm=4.611417770385742, loss=0.6146001219749451
I0302 09:52:12.530429 139589725452032 logging_writer.py:48] [540300] global_step=540300, grad_norm=4.407629013061523, loss=0.6205464601516724
I0302 09:52:46.464242 139590144857856 logging_writer.py:48] [540400] global_step=540400, grad_norm=4.238863468170166, loss=0.5850587487220764
I0302 09:53:20.402057 139589725452032 logging_writer.py:48] [540500] global_step=540500, grad_norm=5.058208465576172, loss=0.6666151285171509
I0302 09:53:54.312448 139590144857856 logging_writer.py:48] [540600] global_step=540600, grad_norm=4.479547023773193, loss=0.6111245155334473
I0302 09:54:28.255158 139589725452032 logging_writer.py:48] [540700] global_step=540700, grad_norm=4.758965015411377, loss=0.6142264008522034
I0302 09:55:02.195168 139590144857856 logging_writer.py:48] [540800] global_step=540800, grad_norm=4.5662336349487305, loss=0.6651481986045837
I0302 09:55:36.152612 139589725452032 logging_writer.py:48] [540900] global_step=540900, grad_norm=4.537378787994385, loss=0.5871961116790771
I0302 09:56:10.064679 139590144857856 logging_writer.py:48] [541000] global_step=541000, grad_norm=5.188475131988525, loss=0.677054762840271
I0302 09:56:17.332494 139753105983296 spec.py:321] Evaluating on the training split.
I0302 09:56:23.409537 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 09:56:31.726081 139753105983296 spec.py:349] Evaluating on the test split.
I0302 09:56:34.005794 139753105983296 submission_runner.py:411] Time since start: 189925.01s, 	Step: 541023, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14378254115581512, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0555442571640015, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8197301626205444, 'test/num_examples': 10000, 'score': 183665.75982761383, 'total_duration': 189925.00900816917, 'accumulated_submission_time': 183665.75982761383, 'accumulated_eval_time': 6210.7504341602325, 'accumulated_logging_time': 29.442301750183105}
I0302 09:56:34.113463 139589717059328 logging_writer.py:48] [541023] accumulated_eval_time=6210.750434, accumulated_logging_time=29.442302, accumulated_submission_time=183665.759828, global_step=541023, preemption_count=0, score=183665.759828, test/accuracy=0.630400, test/loss=1.819730, test/num_examples=10000, total_duration=189925.009008, train/accuracy=0.961535, train/loss=0.143783, validation/accuracy=0.754940, validation/loss=1.055544, validation/num_examples=50000
I0302 09:57:00.679725 139589725452032 logging_writer.py:48] [541100] global_step=541100, grad_norm=4.911186218261719, loss=0.6056455969810486
I0302 09:57:34.612060 139589717059328 logging_writer.py:48] [541200] global_step=541200, grad_norm=4.666568756103516, loss=0.6591843366622925
I0302 09:58:08.554640 139589725452032 logging_writer.py:48] [541300] global_step=541300, grad_norm=4.96243953704834, loss=0.6435585618019104
I0302 09:58:42.533152 139589717059328 logging_writer.py:48] [541400] global_step=541400, grad_norm=4.294126987457275, loss=0.5314223170280457
I0302 09:59:16.493972 139589725452032 logging_writer.py:48] [541500] global_step=541500, grad_norm=4.609609127044678, loss=0.6363441944122314
I0302 09:59:50.464907 139589717059328 logging_writer.py:48] [541600] global_step=541600, grad_norm=5.268931865692139, loss=0.6731624603271484
I0302 10:00:24.413710 139589725452032 logging_writer.py:48] [541700] global_step=541700, grad_norm=4.652616024017334, loss=0.5700268149375916
I0302 10:00:58.375079 139589717059328 logging_writer.py:48] [541800] global_step=541800, grad_norm=4.703241348266602, loss=0.6318449378013611
I0302 10:01:32.334062 139589725452032 logging_writer.py:48] [541900] global_step=541900, grad_norm=4.362869739532471, loss=0.6408864259719849
I0302 10:02:06.308666 139589717059328 logging_writer.py:48] [542000] global_step=542000, grad_norm=5.20244836807251, loss=0.6769692897796631
I0302 10:02:40.280342 139589725452032 logging_writer.py:48] [542100] global_step=542100, grad_norm=4.2614426612854, loss=0.5273610949516296
I0302 10:03:14.410292 139589717059328 logging_writer.py:48] [542200] global_step=542200, grad_norm=4.648861885070801, loss=0.6519941687583923
I0302 10:03:48.380196 139589725452032 logging_writer.py:48] [542300] global_step=542300, grad_norm=4.716884136199951, loss=0.7139299511909485
I0302 10:04:22.332220 139589717059328 logging_writer.py:48] [542400] global_step=542400, grad_norm=4.194831371307373, loss=0.5535378456115723
I0302 10:04:56.292898 139589725452032 logging_writer.py:48] [542500] global_step=542500, grad_norm=4.712031841278076, loss=0.5554248094558716
I0302 10:05:04.230066 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:05:10.298749 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:05:18.572945 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:05:20.833093 139753105983296 submission_runner.py:411] Time since start: 190451.84s, 	Step: 542525, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14631134271621704, 'validation/accuracy': 0.7544199824333191, 'validation/loss': 1.055719256401062, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.8193159103393555, 'test/num_examples': 10000, 'score': 184175.8125114441, 'total_duration': 190451.83632421494, 'accumulated_submission_time': 184175.8125114441, 'accumulated_eval_time': 6227.353454113007, 'accumulated_logging_time': 29.56045937538147}
I0302 10:05:20.938556 139590144857856 logging_writer.py:48] [542525] accumulated_eval_time=6227.353454, accumulated_logging_time=29.560459, accumulated_submission_time=184175.812511, global_step=542525, preemption_count=0, score=184175.812511, test/accuracy=0.629400, test/loss=1.819316, test/num_examples=10000, total_duration=190451.836324, train/accuracy=0.960957, train/loss=0.146311, validation/accuracy=0.754420, validation/loss=1.055719, validation/num_examples=50000
I0302 10:05:46.734522 139590153250560 logging_writer.py:48] [542600] global_step=542600, grad_norm=4.74021053314209, loss=0.6814678311347961
I0302 10:06:20.641342 139590144857856 logging_writer.py:48] [542700] global_step=542700, grad_norm=4.480473041534424, loss=0.5728590488433838
I0302 10:06:54.584935 139590153250560 logging_writer.py:48] [542800] global_step=542800, grad_norm=5.100034713745117, loss=0.6932468414306641
I0302 10:07:28.544541 139590144857856 logging_writer.py:48] [542900] global_step=542900, grad_norm=4.824052810668945, loss=0.603055477142334
I0302 10:08:02.465895 139590153250560 logging_writer.py:48] [543000] global_step=543000, grad_norm=4.402355670928955, loss=0.5771675109863281
I0302 10:08:36.392834 139590144857856 logging_writer.py:48] [543100] global_step=543100, grad_norm=4.5642170906066895, loss=0.6583625078201294
I0302 10:09:10.343921 139590153250560 logging_writer.py:48] [543200] global_step=543200, grad_norm=4.533954620361328, loss=0.6465708017349243
I0302 10:09:44.409159 139590144857856 logging_writer.py:48] [543300] global_step=543300, grad_norm=4.520082950592041, loss=0.5713895559310913
I0302 10:10:18.332188 139590153250560 logging_writer.py:48] [543400] global_step=543400, grad_norm=4.442670822143555, loss=0.6368463039398193
I0302 10:10:52.251638 139590144857856 logging_writer.py:48] [543500] global_step=543500, grad_norm=4.3187103271484375, loss=0.6276887059211731
I0302 10:11:26.178777 139590153250560 logging_writer.py:48] [543600] global_step=543600, grad_norm=4.056344509124756, loss=0.5789906978607178
I0302 10:12:00.136439 139590144857856 logging_writer.py:48] [543700] global_step=543700, grad_norm=4.700508117675781, loss=0.7162031531333923
I0302 10:12:34.080286 139590153250560 logging_writer.py:48] [543800] global_step=543800, grad_norm=4.2036027908325195, loss=0.5670108199119568
I0302 10:13:07.994337 139590144857856 logging_writer.py:48] [543900] global_step=543900, grad_norm=4.90978479385376, loss=0.6196097135543823
I0302 10:13:41.940730 139590153250560 logging_writer.py:48] [544000] global_step=544000, grad_norm=4.724023342132568, loss=0.595650315284729
I0302 10:13:50.926391 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:13:56.978141 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:14:05.379848 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:14:07.572842 139753105983296 submission_runner.py:411] Time since start: 190978.58s, 	Step: 544028, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14564251899719238, 'validation/accuracy': 0.7549799680709839, 'validation/loss': 1.0556423664093018, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.817673921585083, 'test/num_examples': 10000, 'score': 184685.73765444756, 'total_duration': 190978.57606816292, 'accumulated_submission_time': 184685.73765444756, 'accumulated_eval_time': 6243.999848365784, 'accumulated_logging_time': 29.675877332687378}
I0302 10:14:07.679884 139590170035968 logging_writer.py:48] [544028] accumulated_eval_time=6243.999848, accumulated_logging_time=29.675877, accumulated_submission_time=184685.737654, global_step=544028, preemption_count=0, score=184685.737654, test/accuracy=0.630500, test/loss=1.817674, test/num_examples=10000, total_duration=190978.576068, train/accuracy=0.960658, train/loss=0.145643, validation/accuracy=0.754980, validation/loss=1.055642, validation/num_examples=50000
I0302 10:14:32.427662 139590178428672 logging_writer.py:48] [544100] global_step=544100, grad_norm=5.50107479095459, loss=0.6523802876472473
I0302 10:15:06.350000 139590170035968 logging_writer.py:48] [544200] global_step=544200, grad_norm=4.796152591705322, loss=0.6253256797790527
I0302 10:15:40.351972 139590178428672 logging_writer.py:48] [544300] global_step=544300, grad_norm=4.377123832702637, loss=0.5984011888504028
I0302 10:16:14.292188 139590170035968 logging_writer.py:48] [544400] global_step=544400, grad_norm=5.188958644866943, loss=0.7472495436668396
I0302 10:16:48.240855 139590178428672 logging_writer.py:48] [544500] global_step=544500, grad_norm=4.518951416015625, loss=0.620536208152771
I0302 10:17:22.229506 139590170035968 logging_writer.py:48] [544600] global_step=544600, grad_norm=4.806697368621826, loss=0.6329498291015625
I0302 10:17:56.168701 139590178428672 logging_writer.py:48] [544700] global_step=544700, grad_norm=4.599152088165283, loss=0.6053519248962402
I0302 10:18:30.125130 139590170035968 logging_writer.py:48] [544800] global_step=544800, grad_norm=4.634496688842773, loss=0.6016876697540283
I0302 10:19:04.062611 139590178428672 logging_writer.py:48] [544900] global_step=544900, grad_norm=4.458311080932617, loss=0.6264128088951111
I0302 10:19:38.008303 139590170035968 logging_writer.py:48] [545000] global_step=545000, grad_norm=4.594884395599365, loss=0.6623603105545044
I0302 10:20:11.955305 139590178428672 logging_writer.py:48] [545100] global_step=545100, grad_norm=4.254585266113281, loss=0.604174017906189
I0302 10:20:45.920135 139590170035968 logging_writer.py:48] [545200] global_step=545200, grad_norm=4.674294471740723, loss=0.6257401704788208
I0302 10:21:19.853425 139590178428672 logging_writer.py:48] [545300] global_step=545300, grad_norm=4.753695011138916, loss=0.6326377987861633
I0302 10:21:53.874069 139590170035968 logging_writer.py:48] [545400] global_step=545400, grad_norm=4.424811840057373, loss=0.5398005843162537
I0302 10:22:27.801548 139590178428672 logging_writer.py:48] [545500] global_step=545500, grad_norm=4.732205867767334, loss=0.6854745149612427
I0302 10:22:37.780781 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:22:43.769181 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:22:52.046482 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:22:54.384573 139753105983296 submission_runner.py:411] Time since start: 191505.39s, 	Step: 545531, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.14664138853549957, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0558973550796509, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.8188940286636353, 'test/num_examples': 10000, 'score': 185195.77567243576, 'total_duration': 191505.38779973984, 'accumulated_submission_time': 185195.77567243576, 'accumulated_eval_time': 6260.603581190109, 'accumulated_logging_time': 29.792681217193604}
I0302 10:22:54.496414 139589717059328 logging_writer.py:48] [545531] accumulated_eval_time=6260.603581, accumulated_logging_time=29.792681, accumulated_submission_time=185195.775672, global_step=545531, preemption_count=0, score=185195.775672, test/accuracy=0.630100, test/loss=1.818894, test/num_examples=10000, total_duration=191505.387800, train/accuracy=0.960240, train/loss=0.146641, validation/accuracy=0.754760, validation/loss=1.055897, validation/num_examples=50000
I0302 10:23:18.237865 139589725452032 logging_writer.py:48] [545600] global_step=545600, grad_norm=4.562023162841797, loss=0.6174666285514832
I0302 10:23:52.166418 139589717059328 logging_writer.py:48] [545700] global_step=545700, grad_norm=4.690965175628662, loss=0.6256298422813416
I0302 10:24:26.129052 139589725452032 logging_writer.py:48] [545800] global_step=545800, grad_norm=4.450286388397217, loss=0.5783733129501343
I0302 10:25:00.066467 139589717059328 logging_writer.py:48] [545900] global_step=545900, grad_norm=4.471319198608398, loss=0.6249307990074158
I0302 10:25:33.991227 139589725452032 logging_writer.py:48] [546000] global_step=546000, grad_norm=4.570835590362549, loss=0.6140064001083374
I0302 10:26:07.936858 139589717059328 logging_writer.py:48] [546100] global_step=546100, grad_norm=4.504136085510254, loss=0.6007082462310791
I0302 10:26:41.916781 139589725452032 logging_writer.py:48] [546200] global_step=546200, grad_norm=4.688201904296875, loss=0.519354522228241
I0302 10:27:15.868721 139589717059328 logging_writer.py:48] [546300] global_step=546300, grad_norm=4.441826820373535, loss=0.5992525219917297
I0302 10:27:49.916222 139589725452032 logging_writer.py:48] [546400] global_step=546400, grad_norm=4.291338920593262, loss=0.5737329721450806
I0302 10:28:23.884823 139589717059328 logging_writer.py:48] [546500] global_step=546500, grad_norm=4.596846103668213, loss=0.6636860370635986
I0302 10:28:57.843373 139589725452032 logging_writer.py:48] [546600] global_step=546600, grad_norm=4.164474010467529, loss=0.5878202319145203
I0302 10:29:31.773427 139589717059328 logging_writer.py:48] [546700] global_step=546700, grad_norm=4.860409736633301, loss=0.5930957794189453
I0302 10:30:05.747662 139589725452032 logging_writer.py:48] [546800] global_step=546800, grad_norm=4.941304683685303, loss=0.6402665376663208
I0302 10:30:39.690448 139589717059328 logging_writer.py:48] [546900] global_step=546900, grad_norm=4.454998016357422, loss=0.6412054896354675
I0302 10:31:13.631580 139589725452032 logging_writer.py:48] [547000] global_step=547000, grad_norm=4.235206604003906, loss=0.609946072101593
I0302 10:31:24.652719 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:31:30.709331 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:31:38.903775 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:31:41.169533 139753105983296 submission_runner.py:411] Time since start: 192032.17s, 	Step: 547034, 	{'train/accuracy': 0.9611965417861938, 'train/loss': 0.14488346874713898, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0554053783416748, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8190935850143433, 'test/num_examples': 10000, 'score': 185705.86748671532, 'total_duration': 192032.17276000977, 'accumulated_submission_time': 185705.86748671532, 'accumulated_eval_time': 6277.120337963104, 'accumulated_logging_time': 29.91552424430847}
I0302 10:31:41.275607 139590178428672 logging_writer.py:48] [547034] accumulated_eval_time=6277.120338, accumulated_logging_time=29.915524, accumulated_submission_time=185705.867487, global_step=547034, preemption_count=0, score=185705.867487, test/accuracy=0.630300, test/loss=1.819094, test/num_examples=10000, total_duration=192032.172760, train/accuracy=0.961197, train/loss=0.144883, validation/accuracy=0.754840, validation/loss=1.055405, validation/num_examples=50000
I0302 10:32:04.008845 139590186821376 logging_writer.py:48] [547100] global_step=547100, grad_norm=4.833081245422363, loss=0.6580642461776733
I0302 10:32:37.939833 139590178428672 logging_writer.py:48] [547200] global_step=547200, grad_norm=4.495759963989258, loss=0.6100534200668335
I0302 10:33:11.853528 139590186821376 logging_writer.py:48] [547300] global_step=547300, grad_norm=4.578560829162598, loss=0.6247236132621765
I0302 10:33:45.839808 139590178428672 logging_writer.py:48] [547400] global_step=547400, grad_norm=4.414730548858643, loss=0.6238795518875122
I0302 10:34:19.834622 139590186821376 logging_writer.py:48] [547500] global_step=547500, grad_norm=4.5113301277160645, loss=0.6260362863540649
I0302 10:34:53.786780 139590178428672 logging_writer.py:48] [547600] global_step=547600, grad_norm=4.908443450927734, loss=0.6496868133544922
I0302 10:35:27.715848 139590186821376 logging_writer.py:48] [547700] global_step=547700, grad_norm=4.95920467376709, loss=0.6099676489830017
I0302 10:36:01.676868 139590178428672 logging_writer.py:48] [547800] global_step=547800, grad_norm=4.567937850952148, loss=0.592688798904419
I0302 10:36:35.618493 139590186821376 logging_writer.py:48] [547900] global_step=547900, grad_norm=4.408442974090576, loss=0.5966587066650391
I0302 10:37:09.577749 139590178428672 logging_writer.py:48] [548000] global_step=548000, grad_norm=4.785994529724121, loss=0.5745489597320557
I0302 10:37:43.529979 139590186821376 logging_writer.py:48] [548100] global_step=548100, grad_norm=4.01850700378418, loss=0.4622683823108673
I0302 10:38:17.473664 139590178428672 logging_writer.py:48] [548200] global_step=548200, grad_norm=4.131735324859619, loss=0.6351557970046997
I0302 10:38:51.447740 139590186821376 logging_writer.py:48] [548300] global_step=548300, grad_norm=4.675598621368408, loss=0.6282520294189453
I0302 10:39:25.427696 139590178428672 logging_writer.py:48] [548400] global_step=548400, grad_norm=4.5221452713012695, loss=0.5668780207633972
I0302 10:39:59.392933 139590186821376 logging_writer.py:48] [548500] global_step=548500, grad_norm=4.338371276855469, loss=0.6470852494239807
I0302 10:40:11.488056 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:40:17.469338 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:40:25.699344 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:40:27.964920 139753105983296 submission_runner.py:411] Time since start: 192558.97s, 	Step: 548537, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14754562079906464, 'validation/accuracy': 0.7551599740982056, 'validation/loss': 1.054714322090149, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8188832998275757, 'test/num_examples': 10000, 'score': 186216.01438760757, 'total_duration': 192558.96811819077, 'accumulated_submission_time': 186216.01438760757, 'accumulated_eval_time': 6293.597116947174, 'accumulated_logging_time': 30.034253358840942}
I0302 10:40:28.078982 139589717059328 logging_writer.py:48] [548537] accumulated_eval_time=6293.597117, accumulated_logging_time=30.034253, accumulated_submission_time=186216.014388, global_step=548537, preemption_count=0, score=186216.014388, test/accuracy=0.630500, test/loss=1.818883, test/num_examples=10000, total_duration=192558.968118, train/accuracy=0.960639, train/loss=0.147546, validation/accuracy=0.755160, validation/loss=1.054714, validation/num_examples=50000
I0302 10:40:49.776981 139589725452032 logging_writer.py:48] [548600] global_step=548600, grad_norm=4.570659160614014, loss=0.6078354120254517
I0302 10:41:23.709632 139589717059328 logging_writer.py:48] [548700] global_step=548700, grad_norm=4.716041088104248, loss=0.6818444132804871
I0302 10:41:57.642654 139589725452032 logging_writer.py:48] [548800] global_step=548800, grad_norm=4.55094051361084, loss=0.6626136302947998
I0302 10:42:31.582646 139589717059328 logging_writer.py:48] [548900] global_step=548900, grad_norm=4.587649345397949, loss=0.5599087476730347
I0302 10:43:05.507749 139589725452032 logging_writer.py:48] [549000] global_step=549000, grad_norm=4.316720008850098, loss=0.5884766578674316
I0302 10:43:39.441618 139589717059328 logging_writer.py:48] [549100] global_step=549100, grad_norm=4.74556827545166, loss=0.6768292188644409
I0302 10:44:13.354594 139589725452032 logging_writer.py:48] [549200] global_step=549200, grad_norm=4.275367259979248, loss=0.5732440948486328
I0302 10:44:47.296452 139589717059328 logging_writer.py:48] [549300] global_step=549300, grad_norm=4.728213787078857, loss=0.5700536370277405
I0302 10:45:21.256444 139589725452032 logging_writer.py:48] [549400] global_step=549400, grad_norm=4.464273452758789, loss=0.6257020831108093
I0302 10:45:55.185859 139589717059328 logging_writer.py:48] [549500] global_step=549500, grad_norm=4.591401100158691, loss=0.5789797902107239
I0302 10:46:29.239259 139589725452032 logging_writer.py:48] [549600] global_step=549600, grad_norm=5.24043607711792, loss=0.6638205647468567
I0302 10:47:03.194370 139589717059328 logging_writer.py:48] [549700] global_step=549700, grad_norm=5.026885509490967, loss=0.6716681718826294
I0302 10:47:37.140142 139589725452032 logging_writer.py:48] [549800] global_step=549800, grad_norm=4.864651203155518, loss=0.698527991771698
I0302 10:48:11.106019 139589717059328 logging_writer.py:48] [549900] global_step=549900, grad_norm=4.3816447257995605, loss=0.5982945561408997
I0302 10:48:45.020245 139589725452032 logging_writer.py:48] [550000] global_step=550000, grad_norm=5.014962196350098, loss=0.643621563911438
I0302 10:48:58.086189 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:49:04.261420 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:49:12.338747 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:49:14.599497 139753105983296 submission_runner.py:411] Time since start: 193085.60s, 	Step: 550040, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.1470227688550949, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.056078553199768, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8195624351501465, 'test/num_examples': 10000, 'score': 186725.95565152168, 'total_duration': 193085.60273361206, 'accumulated_submission_time': 186725.95565152168, 'accumulated_eval_time': 6310.110383272171, 'accumulated_logging_time': 30.16148805618286}
I0302 10:49:14.707130 139590170035968 logging_writer.py:48] [550040] accumulated_eval_time=6310.110383, accumulated_logging_time=30.161488, accumulated_submission_time=186725.955652, global_step=550040, preemption_count=0, score=186725.955652, test/accuracy=0.630600, test/loss=1.819562, test/num_examples=10000, total_duration=193085.602734, train/accuracy=0.960898, train/loss=0.147023, validation/accuracy=0.755100, validation/loss=1.056079, validation/num_examples=50000
I0302 10:49:35.453531 139590178428672 logging_writer.py:48] [550100] global_step=550100, grad_norm=4.673921585083008, loss=0.6576535701751709
I0302 10:50:09.376847 139590170035968 logging_writer.py:48] [550200] global_step=550200, grad_norm=4.327538013458252, loss=0.5604889392852783
I0302 10:50:43.335349 139590178428672 logging_writer.py:48] [550300] global_step=550300, grad_norm=4.742980003356934, loss=0.5714658498764038
I0302 10:51:17.298357 139590170035968 logging_writer.py:48] [550400] global_step=550400, grad_norm=4.631010055541992, loss=0.6244359016418457
I0302 10:51:51.263537 139590178428672 logging_writer.py:48] [550500] global_step=550500, grad_norm=4.5694122314453125, loss=0.6082877516746521
I0302 10:52:25.252949 139590170035968 logging_writer.py:48] [550600] global_step=550600, grad_norm=4.664480209350586, loss=0.6229223608970642
I0302 10:52:59.297060 139590178428672 logging_writer.py:48] [550700] global_step=550700, grad_norm=4.486903190612793, loss=0.6037562489509583
I0302 10:53:33.242541 139590170035968 logging_writer.py:48] [550800] global_step=550800, grad_norm=4.557654857635498, loss=0.5828641057014465
I0302 10:54:07.234214 139590178428672 logging_writer.py:48] [550900] global_step=550900, grad_norm=4.314731597900391, loss=0.6002185344696045
I0302 10:54:41.158580 139590170035968 logging_writer.py:48] [551000] global_step=551000, grad_norm=4.735623836517334, loss=0.605678915977478
I0302 10:55:15.111946 139590178428672 logging_writer.py:48] [551100] global_step=551100, grad_norm=4.846410274505615, loss=0.672793447971344
I0302 10:55:49.109115 139590170035968 logging_writer.py:48] [551200] global_step=551200, grad_norm=4.454305171966553, loss=0.6086350679397583
I0302 10:56:23.035403 139590178428672 logging_writer.py:48] [551300] global_step=551300, grad_norm=4.7156596183776855, loss=0.6296782493591309
I0302 10:56:57.008668 139590170035968 logging_writer.py:48] [551400] global_step=551400, grad_norm=4.587637901306152, loss=0.6867759227752686
I0302 10:57:30.973913 139590178428672 logging_writer.py:48] [551500] global_step=551500, grad_norm=4.359225273132324, loss=0.6092556715011597
I0302 10:57:44.735288 139753105983296 spec.py:321] Evaluating on the training split.
I0302 10:57:50.749492 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 10:57:59.020562 139753105983296 spec.py:349] Evaluating on the test split.
I0302 10:58:01.294597 139753105983296 submission_runner.py:411] Time since start: 193612.30s, 	Step: 551542, 	{'train/accuracy': 0.9594626426696777, 'train/loss': 0.1485990285873413, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.0570076704025269, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8204457759857178, 'test/num_examples': 10000, 'score': 187235.92059636116, 'total_duration': 193612.29782891273, 'accumulated_submission_time': 187235.92059636116, 'accumulated_eval_time': 6326.669639825821, 'accumulated_logging_time': 30.279273509979248}
I0302 10:58:01.402045 139589717059328 logging_writer.py:48] [551542] accumulated_eval_time=6326.669640, accumulated_logging_time=30.279274, accumulated_submission_time=187235.920596, global_step=551542, preemption_count=0, score=187235.920596, test/accuracy=0.629900, test/loss=1.820446, test/num_examples=10000, total_duration=193612.297829, train/accuracy=0.959463, train/loss=0.148599, validation/accuracy=0.754940, validation/loss=1.057008, validation/num_examples=50000
I0302 10:58:21.387769 139589725452032 logging_writer.py:48] [551600] global_step=551600, grad_norm=4.331942558288574, loss=0.5980253219604492
I0302 10:58:55.393849 139589717059328 logging_writer.py:48] [551700] global_step=551700, grad_norm=4.166226863861084, loss=0.550039529800415
I0302 10:59:29.304654 139589725452032 logging_writer.py:48] [551800] global_step=551800, grad_norm=4.364104270935059, loss=0.6157571077346802
I0302 11:00:03.270119 139589717059328 logging_writer.py:48] [551900] global_step=551900, grad_norm=4.399776458740234, loss=0.6588957905769348
I0302 11:00:37.240966 139589725452032 logging_writer.py:48] [552000] global_step=552000, grad_norm=5.045413017272949, loss=0.6047970056533813
I0302 11:01:11.194275 139589717059328 logging_writer.py:48] [552100] global_step=552100, grad_norm=4.059403419494629, loss=0.5902402400970459
I0302 11:01:45.139304 139589725452032 logging_writer.py:48] [552200] global_step=552200, grad_norm=4.621665954589844, loss=0.6759582757949829
I0302 11:02:19.085049 139589717059328 logging_writer.py:48] [552300] global_step=552300, grad_norm=4.155996322631836, loss=0.5353800654411316
I0302 11:02:53.040225 139589725452032 logging_writer.py:48] [552400] global_step=552400, grad_norm=4.4104814529418945, loss=0.6647201180458069
I0302 11:03:26.983711 139589717059328 logging_writer.py:48] [552500] global_step=552500, grad_norm=4.504343509674072, loss=0.6041385531425476
I0302 11:04:00.935445 139589725452032 logging_writer.py:48] [552600] global_step=552600, grad_norm=4.453758716583252, loss=0.6522226929664612
I0302 11:04:34.863068 139589717059328 logging_writer.py:48] [552700] global_step=552700, grad_norm=4.355545997619629, loss=0.6767268776893616
I0302 11:05:08.912932 139589725452032 logging_writer.py:48] [552800] global_step=552800, grad_norm=4.681774139404297, loss=0.606134831905365
I0302 11:05:42.871999 139589717059328 logging_writer.py:48] [552900] global_step=552900, grad_norm=4.518557071685791, loss=0.6010901927947998
I0302 11:06:16.808692 139589725452032 logging_writer.py:48] [553000] global_step=553000, grad_norm=4.333343029022217, loss=0.59156334400177
I0302 11:06:31.531102 139753105983296 spec.py:321] Evaluating on the training split.
I0302 11:06:37.559355 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 11:06:45.729345 139753105983296 spec.py:349] Evaluating on the test split.
I0302 11:06:47.986202 139753105983296 submission_runner.py:411] Time since start: 194138.99s, 	Step: 553045, 	{'train/accuracy': 0.9600207209587097, 'train/loss': 0.1465989500284195, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0555554628372192, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8178973197937012, 'test/num_examples': 10000, 'score': 187745.98685359955, 'total_duration': 194138.98943257332, 'accumulated_submission_time': 187745.98685359955, 'accumulated_eval_time': 6343.1246864795685, 'accumulated_logging_time': 30.39686608314514}
I0302 11:06:48.099343 139590161643264 logging_writer.py:48] [553045] accumulated_eval_time=6343.124686, accumulated_logging_time=30.396866, accumulated_submission_time=187745.986854, global_step=553045, preemption_count=0, score=187745.986854, test/accuracy=0.630500, test/loss=1.817897, test/num_examples=10000, total_duration=194138.989433, train/accuracy=0.960021, train/loss=0.146599, validation/accuracy=0.754900, validation/loss=1.055555, validation/num_examples=50000
I0302 11:07:07.116717 139590170035968 logging_writer.py:48] [553100] global_step=553100, grad_norm=4.735567569732666, loss=0.6240540742874146
I0302 11:07:41.022867 139590161643264 logging_writer.py:48] [553200] global_step=553200, grad_norm=4.667707920074463, loss=0.6335376501083374
I0302 11:08:14.964658 139590170035968 logging_writer.py:48] [553300] global_step=553300, grad_norm=4.4851460456848145, loss=0.6066455841064453
I0302 11:08:48.909276 139590161643264 logging_writer.py:48] [553400] global_step=553400, grad_norm=4.524937629699707, loss=0.6323269605636597
I0302 11:09:22.852587 139590170035968 logging_writer.py:48] [553500] global_step=553500, grad_norm=3.9383788108825684, loss=0.49129238724708557
I0302 11:09:56.820120 139590161643264 logging_writer.py:48] [553600] global_step=553600, grad_norm=4.7000203132629395, loss=0.6715995669364929
I0302 11:10:30.772094 139590170035968 logging_writer.py:48] [553700] global_step=553700, grad_norm=4.924473285675049, loss=0.6411848664283752
I0302 11:11:04.782840 139590161643264 logging_writer.py:48] [553800] global_step=553800, grad_norm=4.554538726806641, loss=0.651081919670105
I0302 11:11:38.739017 139590170035968 logging_writer.py:48] [553900] global_step=553900, grad_norm=4.5350141525268555, loss=0.6362175345420837
I0302 11:12:12.694727 139590161643264 logging_writer.py:48] [554000] global_step=554000, grad_norm=4.4418110847473145, loss=0.6157656311988831
I0302 11:12:46.643226 139590170035968 logging_writer.py:48] [554100] global_step=554100, grad_norm=4.297197341918945, loss=0.627568244934082
I0302 11:13:20.621650 139590161643264 logging_writer.py:48] [554200] global_step=554200, grad_norm=4.5750885009765625, loss=0.6251688599586487
I0302 11:13:54.572558 139590170035968 logging_writer.py:48] [554300] global_step=554300, grad_norm=4.5497612953186035, loss=0.6161004304885864
I0302 11:14:28.531553 139590161643264 logging_writer.py:48] [554400] global_step=554400, grad_norm=4.6848344802856445, loss=0.6564337015151978
I0302 11:15:02.477406 139590170035968 logging_writer.py:48] [554500] global_step=554500, grad_norm=4.546899795532227, loss=0.607796311378479
I0302 11:15:18.231140 139753105983296 spec.py:321] Evaluating on the training split.
I0302 11:15:24.227669 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 11:15:32.558273 139753105983296 spec.py:349] Evaluating on the test split.
I0302 11:15:34.885394 139753105983296 submission_runner.py:411] Time since start: 194665.89s, 	Step: 554548, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14739346504211426, 'validation/accuracy': 0.7547000050544739, 'validation/loss': 1.0555559396743774, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8189270496368408, 'test/num_examples': 10000, 'score': 188256.05421233177, 'total_duration': 194665.88861370087, 'accumulated_submission_time': 188256.05421233177, 'accumulated_eval_time': 6359.778885602951, 'accumulated_logging_time': 30.522332429885864}
I0302 11:15:34.999624 139589717059328 logging_writer.py:48] [554548] accumulated_eval_time=6359.778886, accumulated_logging_time=30.522332, accumulated_submission_time=188256.054212, global_step=554548, preemption_count=0, score=188256.054212, test/accuracy=0.631200, test/loss=1.818927, test/num_examples=10000, total_duration=194665.888614, train/accuracy=0.960340, train/loss=0.147393, validation/accuracy=0.754700, validation/loss=1.055556, validation/num_examples=50000
I0302 11:15:52.981082 139589725452032 logging_writer.py:48] [554600] global_step=554600, grad_norm=4.391654968261719, loss=0.6516108512878418
I0302 11:16:26.908663 139589717059328 logging_writer.py:48] [554700] global_step=554700, grad_norm=4.747573375701904, loss=0.6376633644104004
I0302 11:17:00.861835 139589725452032 logging_writer.py:48] [554800] global_step=554800, grad_norm=4.4460554122924805, loss=0.6663498878479004
I0302 11:17:34.864438 139589717059328 logging_writer.py:48] [554900] global_step=554900, grad_norm=4.29511833190918, loss=0.595931351184845
I0302 11:18:08.816720 139589725452032 logging_writer.py:48] [555000] global_step=555000, grad_norm=4.4343767166137695, loss=0.6303101778030396
I0302 11:18:42.795735 139589717059328 logging_writer.py:48] [555100] global_step=555100, grad_norm=3.9856748580932617, loss=0.545887291431427
I0302 11:19:16.728911 139589725452032 logging_writer.py:48] [555200] global_step=555200, grad_norm=4.541196823120117, loss=0.5879051685333252
I0302 11:19:50.673896 139589717059328 logging_writer.py:48] [555300] global_step=555300, grad_norm=4.484066009521484, loss=0.6582726240158081
I0302 11:20:24.647582 139589725452032 logging_writer.py:48] [555400] global_step=555400, grad_norm=4.625047206878662, loss=0.6135712265968323
I0302 11:20:58.592003 139589717059328 logging_writer.py:48] [555500] global_step=555500, grad_norm=4.665305137634277, loss=0.6664536595344543
I0302 11:21:32.516393 139589725452032 logging_writer.py:48] [555600] global_step=555600, grad_norm=4.552158832550049, loss=0.6313073635101318
I0302 11:22:06.476463 139589717059328 logging_writer.py:48] [555700] global_step=555700, grad_norm=4.373166084289551, loss=0.6484849452972412
I0302 11:22:40.432695 139589725452032 logging_writer.py:48] [555800] global_step=555800, grad_norm=4.802425861358643, loss=0.6659203767776489
I0302 11:23:14.389694 139589717059328 logging_writer.py:48] [555900] global_step=555900, grad_norm=4.6542534828186035, loss=0.6778798699378967
I0302 11:23:48.383515 139589725452032 logging_writer.py:48] [556000] global_step=556000, grad_norm=4.00663423538208, loss=0.6318301558494568
I0302 11:24:05.154874 139753105983296 spec.py:321] Evaluating on the training split.
I0302 11:24:11.174146 139753105983296 spec.py:333] Evaluating on the validation split.
I0302 11:24:19.401757 139753105983296 spec.py:349] Evaluating on the test split.
I0302 11:24:21.669853 139753105983296 submission_runner.py:411] Time since start: 195192.67s, 	Step: 556051, 	{'train/accuracy': 0.9599210619926453, 'train/loss': 0.1484605371952057, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0562946796417236, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8204028606414795, 'test/num_examples': 10000, 'score': 188766.14553546906, 'total_duration': 195192.6730442047, 'accumulated_submission_time': 188766.14553546906, 'accumulated_eval_time': 6376.293786287308, 'accumulated_logging_time': 30.646925687789917}
I0302 11:24:21.785255 139590161643264 logging_writer.py:48] [556051] accumulated_eval_time=6376.293786, accumulated_logging_time=30.646926, accumulated_submission_time=188766.145535, global_step=556051, preemption_count=0, score=188766.145535, test/accuracy=0.629900, test/loss=1.820403, test/num_examples=10000, total_duration=195192.673044, train/accuracy=0.959921, train/loss=0.148461, validation/accuracy=0.754760, validation/loss=1.056295, validation/num_examples=50000
I0302 11:24:38.754998 139590170035968 logging_writer.py:48] [556100] global_step=556100, grad_norm=4.55432653427124, loss=0.6029210686683655
I0302 11:25:12.667031 139590161643264 logging_writer.py:48] [556200] global_step=556200, grad_norm=3.986217737197876, loss=0.5787693858146667
I0302 11:25:46.603574 139590170035968 logging_writer.py:48] [556300] global_step=556300, grad_norm=4.522660255432129, loss=0.6114692687988281
I0302 11:26:20.556721 139590161643264 logging_writer.py:48] [556400] global_step=556400, grad_norm=4.697061538696289, loss=0.6881147623062134
I0302 11:26:54.522420 139590170035968 logging_writer.py:48] [556500] global_step=556500, grad_norm=4.532405853271484, loss=0.6510275602340698
I0302 11:27:28.468552 139590161643264 logging_writer.py:48] [556600] global_step=556600, grad_norm=4.323005676269531, loss=0.5942986011505127
I0302 11:28:02.431212 139590170035968 logging_writer.py:48] [556700] global_step=556700, grad_norm=4.35573673248291, loss=0.5582313537597656
I0302 11:28:36.391965 139590161643264 logging_writer.py:48] [556800] global_step=556800, grad_norm=4.705089092254639, loss=0.664691150188446
I0302 11:28:40.012651 139590170035968 logging_writer.py:48] [556812] global_step=556812, preemption_count=0, score=189024.259904
I0302 11:28:40.453242 139753105983296 checkpoints.py:490] Saving checkpoint at step: 556812
I0302 11:28:41.565070 139753105983296 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax/trial_1/checkpoint_556812
I0302 11:28:41.592941 139753105983296 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_resnet_jax/trial_1/checkpoint_556812.
I0302 11:28:42.356253 139753105983296 submission_runner.py:676] Final imagenet_resnet score: 189024.25990390778
