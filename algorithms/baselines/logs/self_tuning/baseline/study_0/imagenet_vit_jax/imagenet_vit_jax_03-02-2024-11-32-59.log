python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_0 --overwrite=true --save_checkpoints=false --rng_seed=1815066111 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_vit_jax_03-02-2024-11-32-59.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0302 11:33:20.828083 140575196817216 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax because --overwrite was set.
I0302 11:33:20.829823 140575196817216 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax.
I0302 11:33:21.898156 140575196817216 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0302 11:33:21.898962 140575196817216 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0302 11:33:21.899119 140575196817216 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0302 11:33:22.811411 140575196817216 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax/trial_1.
I0302 11:33:23.012972 140575196817216 submission_runner.py:206] Initializing dataset.
I0302 11:33:23.029531 140575196817216 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0302 11:33:23.039844 140575196817216 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0302 11:33:23.425892 140575196817216 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0302 11:33:32.751713 140575196817216 submission_runner.py:213] Initializing model.
I0302 11:33:42.631710 140575196817216 submission_runner.py:255] Initializing optimizer.
I0302 11:33:43.690651 140575196817216 submission_runner.py:262] Initializing metrics bundle.
I0302 11:33:43.690933 140575196817216 submission_runner.py:280] Initializing checkpoint and logger.
I0302 11:33:43.692025 140575196817216 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0302 11:33:43.692192 140575196817216 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0302 11:33:44.055875 140575196817216 logger_utils.py:220] Unable to record git information. Continuing without it.
I0302 11:33:44.387799 140575196817216 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax/trial_1/flags_0.json.
I0302 11:33:44.397901 140575196817216 submission_runner.py:314] Starting training loop.
I0302 11:34:27.277735 140412299765504 logging_writer.py:48] [0] global_step=0, grad_norm=0.3656449615955353, loss=6.9077558517456055
I0302 11:34:27.297283 140575196817216 spec.py:321] Evaluating on the training split.
I0302 11:34:27.495576 140575196817216 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0302 11:34:27.504881 140575196817216 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0302 11:34:27.590148 140575196817216 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0302 11:34:44.885062 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 11:34:44.896574 140575196817216 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0302 11:34:44.916999 140575196817216 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0302 11:34:44.998102 140575196817216 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0302 11:35:02.419417 140575196817216 spec.py:349] Evaluating on the test split.
I0302 11:35:02.426581 140575196817216 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0302 11:35:02.432499 140575196817216 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0302 11:35:02.481449 140575196817216 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0302 11:35:07.869284 140575196817216 submission_runner.py:411] Time since start: 83.47s, 	Step: 1, 	{'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.89926743507385, 'total_duration': 83.47133111953735, 'accumulated_submission_time': 42.89926743507385, 'accumulated_eval_time': 40.57194924354553, 'accumulated_logging_time': 0}
I0302 11:35:07.886879 140380318193408 logging_writer.py:48] [1] accumulated_eval_time=40.571949, accumulated_logging_time=0, accumulated_submission_time=42.899267, global_step=1, preemption_count=0, score=42.899267, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=83.471331, train/accuracy=0.000937, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0302 11:36:11.115730 140410714314496 logging_writer.py:48] [100] global_step=100, grad_norm=0.44641295075416565, loss=6.885093688964844
I0302 11:36:53.588210 140410722707200 logging_writer.py:48] [200] global_step=200, grad_norm=0.5942963361740112, loss=6.770656585693359
I0302 11:37:37.860289 140410714314496 logging_writer.py:48] [300] global_step=300, grad_norm=0.7501222491264343, loss=6.665348529815674
I0302 11:38:22.276920 140410722707200 logging_writer.py:48] [400] global_step=400, grad_norm=1.4276396036148071, loss=6.554960250854492
I0302 11:39:06.979779 140410714314496 logging_writer.py:48] [500] global_step=500, grad_norm=0.9807071089744568, loss=6.455837249755859
I0302 11:39:51.339423 140410722707200 logging_writer.py:48] [600] global_step=600, grad_norm=0.8911657929420471, loss=6.540367603302002
I0302 11:40:35.578119 140410714314496 logging_writer.py:48] [700] global_step=700, grad_norm=1.0438047647476196, loss=6.286005973815918
I0302 11:41:19.620819 140410722707200 logging_writer.py:48] [800] global_step=800, grad_norm=1.11043381690979, loss=6.312709808349609
I0302 11:42:03.786565 140410714314496 logging_writer.py:48] [900] global_step=900, grad_norm=1.3335994482040405, loss=6.1665940284729
I0302 11:42:07.922296 140575196817216 spec.py:321] Evaluating on the training split.
I0302 11:42:19.692600 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 11:42:27.815569 140575196817216 spec.py:349] Evaluating on the test split.
I0302 11:42:29.512841 140575196817216 submission_runner.py:411] Time since start: 525.11s, 	Step: 911, 	{'train/accuracy': 0.039042968302965164, 'train/loss': 5.780252933502197, 'validation/accuracy': 0.0384799987077713, 'validation/loss': 5.81451940536499, 'validation/num_examples': 50000, 'test/accuracy': 0.02980000153183937, 'test/loss': 5.954464435577393, 'test/num_examples': 10000, 'score': 462.8738512992859, 'total_duration': 525.1148769855499, 'accumulated_submission_time': 462.8738512992859, 'accumulated_eval_time': 62.162476539611816, 'accumulated_logging_time': 0.028006315231323242}
I0302 11:42:29.530339 140380326586112 logging_writer.py:48] [911] accumulated_eval_time=62.162477, accumulated_logging_time=0.028006, accumulated_submission_time=462.873851, global_step=911, preemption_count=0, score=462.873851, test/accuracy=0.029800, test/loss=5.954464, test/num_examples=10000, total_duration=525.114877, train/accuracy=0.039043, train/loss=5.780253, validation/accuracy=0.038480, validation/loss=5.814519, validation/num_examples=50000
I0302 11:43:05.289306 140380334978816 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1211135387420654, loss=6.072832107543945
I0302 11:43:48.489041 140380326586112 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.9412233233451843, loss=6.629085063934326
I0302 11:44:32.886885 140380334978816 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1078755855560303, loss=6.578500270843506
I0302 11:45:17.003543 140380326586112 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9145699739456177, loss=6.550778388977051
I0302 11:46:01.190701 140380334978816 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.7578623294830322, loss=5.880758285522461
I0302 11:46:45.501053 140380326586112 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.148094654083252, loss=5.810706615447998
I0302 11:47:29.483332 140380334978816 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0829169750213623, loss=5.717920303344727
I0302 11:48:13.534694 140380326586112 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.0758239030838013, loss=5.707068920135498
I0302 11:48:57.711182 140380334978816 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0715761184692383, loss=5.730659484863281
I0302 11:49:29.640906 140575196817216 spec.py:321] Evaluating on the training split.
I0302 11:49:41.209975 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 11:49:49.371371 140575196817216 spec.py:349] Evaluating on the test split.
I0302 11:49:51.036085 140575196817216 submission_runner.py:411] Time since start: 966.64s, 	Step: 1874, 	{'train/accuracy': 0.08015625178813934, 'train/loss': 5.216738700866699, 'validation/accuracy': 0.07485999912023544, 'validation/loss': 5.251400947570801, 'validation/num_examples': 50000, 'test/accuracy': 0.059300001710653305, 'test/loss': 5.495050430297852, 'test/num_examples': 10000, 'score': 882.9174020290375, 'total_duration': 966.6380949020386, 'accumulated_submission_time': 882.9174020290375, 'accumulated_eval_time': 83.55760931968689, 'accumulated_logging_time': 0.05858421325683594}
I0302 11:49:51.055849 140380326586112 logging_writer.py:48] [1874] accumulated_eval_time=83.557609, accumulated_logging_time=0.058584, accumulated_submission_time=882.917402, global_step=1874, preemption_count=0, score=882.917402, test/accuracy=0.059300, test/loss=5.495050, test/num_examples=10000, total_duration=966.638095, train/accuracy=0.080156, train/loss=5.216739, validation/accuracy=0.074860, validation/loss=5.251401, validation/num_examples=50000
I0302 11:50:01.819329 140380334978816 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9890037775039673, loss=5.69713020324707
I0302 11:50:41.797444 140380326586112 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0384347438812256, loss=5.597604751586914
I0302 11:51:25.675347 140380334978816 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0560569763183594, loss=6.593939781188965
I0302 11:52:09.774703 140380326586112 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.977156400680542, loss=5.4322662353515625
I0302 11:52:53.951302 140380334978816 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.1425566673278809, loss=5.491124629974365
I0302 11:53:37.889516 140380326586112 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.084606409072876, loss=5.480777740478516
I0302 11:54:22.184326 140380334978816 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2235746383666992, loss=6.211878299713135
I0302 11:55:06.332114 140380326586112 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.5482127666473389, loss=6.1991286277771
I0302 11:55:50.555821 140380334978816 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.964584231376648, loss=6.3630475997924805
I0302 11:56:34.902110 140380326586112 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8806314468383789, loss=5.484236240386963
I0302 11:56:51.372436 140575196817216 spec.py:321] Evaluating on the training split.
I0302 11:57:03.127129 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 11:57:11.222280 140575196817216 spec.py:349] Evaluating on the test split.
I0302 11:57:12.896374 140575196817216 submission_runner.py:411] Time since start: 1408.50s, 	Step: 2839, 	{'train/accuracy': 0.15458984673023224, 'train/loss': 4.480022430419922, 'validation/accuracy': 0.1401599943637848, 'validation/loss': 4.580586910247803, 'validation/num_examples': 50000, 'test/accuracy': 0.1096000075340271, 'test/loss': 4.916996479034424, 'test/num_examples': 10000, 'score': 1303.1719748973846, 'total_duration': 1408.4984166622162, 'accumulated_submission_time': 1303.1719748973846, 'accumulated_eval_time': 105.08153319358826, 'accumulated_logging_time': 0.08928251266479492}
I0302 11:57:12.913524 140380334978816 logging_writer.py:48] [2839] accumulated_eval_time=105.081533, accumulated_logging_time=0.089283, accumulated_submission_time=1303.171975, global_step=2839, preemption_count=0, score=1303.171975, test/accuracy=0.109600, test/loss=4.916996, test/num_examples=10000, total_duration=1408.498417, train/accuracy=0.154590, train/loss=4.480022, validation/accuracy=0.140160, validation/loss=4.580587, validation/num_examples=50000
I0302 11:57:37.626525 140380326586112 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.1373488903045654, loss=5.184527397155762
I0302 11:58:19.127438 140380334978816 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.053133249282837, loss=6.426363468170166
I0302 11:59:03.302592 140380326586112 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8735631108283997, loss=5.459171295166016
I0302 11:59:47.407300 140380334978816 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0570148229599, loss=4.9707231521606445
I0302 12:00:31.560258 140380326586112 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1487817764282227, loss=5.0494489669799805
I0302 12:01:15.593017 140380334978816 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8650463223457336, loss=4.840730667114258
I0302 12:02:00.098935 140380326586112 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.057589054107666, loss=5.898914337158203
I0302 12:02:43.904769 140380334978816 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1604441404342651, loss=4.856368064880371
I0302 12:03:28.084404 140380326586112 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7954930067062378, loss=5.416158199310303
I0302 12:04:12.232880 140380334978816 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1482832431793213, loss=4.904322624206543
I0302 12:04:13.243082 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:04:25.052779 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:04:33.188668 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:04:34.874667 140575196817216 submission_runner.py:411] Time since start: 1850.48s, 	Step: 3804, 	{'train/accuracy': 0.2083788961172104, 'train/loss': 4.025781154632568, 'validation/accuracy': 0.1943800002336502, 'validation/loss': 4.114321231842041, 'validation/num_examples': 50000, 'test/accuracy': 0.14800000190734863, 'test/loss': 4.542527198791504, 'test/num_examples': 10000, 'score': 1723.4397399425507, 'total_duration': 1850.4767088890076, 'accumulated_submission_time': 1723.4397399425507, 'accumulated_eval_time': 126.71310567855835, 'accumulated_logging_time': 0.11670374870300293}
I0302 12:04:34.891525 140380326586112 logging_writer.py:48] [3804] accumulated_eval_time=126.713106, accumulated_logging_time=0.116704, accumulated_submission_time=1723.439740, global_step=3804, preemption_count=0, score=1723.439740, test/accuracy=0.148000, test/loss=4.542527, test/num_examples=10000, total_duration=1850.476709, train/accuracy=0.208379, train/loss=4.025781, validation/accuracy=0.194380, validation/loss=4.114321, validation/num_examples=50000
I0302 12:05:15.108549 140380334978816 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.2882366180419922, loss=6.396989822387695
I0302 12:05:59.157243 140380326586112 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8550987243652344, loss=4.906580924987793
I0302 12:06:43.365013 140380334978816 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.1589505672454834, loss=5.312845706939697
I0302 12:07:27.703215 140380326586112 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.790741503238678, loss=4.846510410308838
I0302 12:08:11.843214 140380334978816 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8137458562850952, loss=5.16247034072876
I0302 12:08:55.999270 140380326586112 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9549643993377686, loss=4.634535789489746
I0302 12:09:40.372204 140380334978816 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9477628469467163, loss=4.779750347137451
I0302 12:10:24.747428 140380326586112 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8755977153778076, loss=4.385534763336182
I0302 12:11:08.928333 140380334978816 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.181031346321106, loss=4.523566722869873
I0302 12:11:35.038682 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:11:46.850986 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:11:55.242047 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:11:56.902042 140575196817216 submission_runner.py:411] Time since start: 2292.50s, 	Step: 4761, 	{'train/accuracy': 0.26851561665534973, 'train/loss': 3.618990182876587, 'validation/accuracy': 0.2489599883556366, 'validation/loss': 3.72690749168396, 'validation/num_examples': 50000, 'test/accuracy': 0.19100001454353333, 'test/loss': 4.1918511390686035, 'test/num_examples': 10000, 'score': 2143.524181365967, 'total_duration': 2292.5040712356567, 'accumulated_submission_time': 2143.524181365967, 'accumulated_eval_time': 148.57643723487854, 'accumulated_logging_time': 0.1443471908569336}
I0302 12:11:56.921948 140380326586112 logging_writer.py:48] [4761] accumulated_eval_time=148.576437, accumulated_logging_time=0.144347, accumulated_submission_time=2143.524181, global_step=4761, preemption_count=0, score=2143.524181, test/accuracy=0.191000, test/loss=4.191851, test/num_examples=10000, total_duration=2292.504071, train/accuracy=0.268516, train/loss=3.618990, validation/accuracy=0.248960, validation/loss=3.726907, validation/num_examples=50000
I0302 12:12:12.879196 140380334978816 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7555922269821167, loss=6.210031032562256
I0302 12:12:53.816803 140380326586112 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5125963687896729, loss=6.148513317108154
I0302 12:13:38.374480 140380334978816 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.3275591135025024, loss=4.542590618133545
I0302 12:14:23.204285 140380326586112 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8211698532104492, loss=4.830718517303467
I0302 12:15:08.200304 140380334978816 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7953400015830994, loss=4.604518890380859
I0302 12:15:52.492696 140380326586112 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9252960085868835, loss=4.3744354248046875
I0302 12:16:37.118933 140380334978816 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0141841173171997, loss=4.323858261108398
I0302 12:17:21.233588 140380326586112 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8674108982086182, loss=4.765592575073242
I0302 12:18:05.805612 140380334978816 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7517289519309998, loss=5.35795783996582
I0302 12:18:50.421850 140380326586112 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9331361651420593, loss=4.160134315490723
I0302 12:18:57.024618 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:19:08.826267 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:19:16.935453 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:19:18.605584 140575196817216 submission_runner.py:411] Time since start: 2734.21s, 	Step: 5717, 	{'train/accuracy': 0.3191015422344208, 'train/loss': 3.2768993377685547, 'validation/accuracy': 0.2922999858856201, 'validation/loss': 3.4312615394592285, 'validation/num_examples': 50000, 'test/accuracy': 0.22070001065731049, 'test/loss': 3.9930953979492188, 'test/num_examples': 10000, 'score': 2563.565025806427, 'total_duration': 2734.2075912952423, 'accumulated_submission_time': 2563.565025806427, 'accumulated_eval_time': 170.15734696388245, 'accumulated_logging_time': 0.17464566230773926}
I0302 12:19:18.622610 140380334978816 logging_writer.py:48] [5717] accumulated_eval_time=170.157347, accumulated_logging_time=0.174646, accumulated_submission_time=2563.565026, global_step=5717, preemption_count=0, score=2563.565026, test/accuracy=0.220700, test/loss=3.993095, test/num_examples=10000, total_duration=2734.207591, train/accuracy=0.319102, train/loss=3.276899, validation/accuracy=0.292300, validation/loss=3.431262, validation/num_examples=50000
I0302 12:19:52.089220 140380326586112 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9091072082519531, loss=4.139925479888916
I0302 12:20:34.822201 140380334978816 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1782764196395874, loss=4.750612735748291
I0302 12:21:19.004086 140380326586112 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8452624082565308, loss=3.972339630126953
I0302 12:22:03.210480 140380334978816 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7538952231407166, loss=4.638941764831543
I0302 12:22:47.461718 140380326586112 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.979491114616394, loss=4.595807075500488
I0302 12:23:32.650822 140380334978816 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.705272376537323, loss=5.211636066436768
I0302 12:24:17.716074 140380326586112 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8142691850662231, loss=4.236677646636963
I0302 12:25:03.109655 140380334978816 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.053675651550293, loss=4.012180805206299
I0302 12:25:47.521652 140380326586112 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7363752126693726, loss=6.031337261199951
I0302 12:26:18.704440 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:26:30.382965 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:26:38.530169 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:26:40.202760 140575196817216 submission_runner.py:411] Time since start: 3175.80s, 	Step: 6672, 	{'train/accuracy': 0.3853124976158142, 'train/loss': 2.8786144256591797, 'validation/accuracy': 0.3353399932384491, 'validation/loss': 3.149968147277832, 'validation/num_examples': 50000, 'test/accuracy': 0.2574000060558319, 'test/loss': 3.691765785217285, 'test/num_examples': 10000, 'score': 2983.585864305496, 'total_duration': 3175.8048005104065, 'accumulated_submission_time': 2983.585864305496, 'accumulated_eval_time': 191.65565276145935, 'accumulated_logging_time': 0.2017805576324463}
I0302 12:26:40.220649 140380334978816 logging_writer.py:48] [6672] accumulated_eval_time=191.655653, accumulated_logging_time=0.201781, accumulated_submission_time=2983.585864, global_step=6672, preemption_count=0, score=2983.585864, test/accuracy=0.257400, test/loss=3.691766, test/num_examples=10000, total_duration=3175.804801, train/accuracy=0.385312, train/loss=2.878614, validation/accuracy=0.335340, validation/loss=3.149968, validation/num_examples=50000
I0302 12:26:51.804439 140380326586112 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8748788237571716, loss=3.779139518737793
I0302 12:27:32.634402 140380334978816 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.938457727432251, loss=4.309936046600342
I0302 12:28:17.033158 140380326586112 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9588226675987244, loss=3.774580240249634
I0302 12:29:01.308610 140380334978816 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7901158332824707, loss=4.67694091796875
I0302 12:29:45.860677 140380326586112 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8630303144454956, loss=4.655183792114258
I0302 12:30:30.306934 140380334978816 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.851693868637085, loss=3.811164617538452
I0302 12:31:14.715506 140380326586112 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9004421830177307, loss=3.8119957447052
I0302 12:31:58.945127 140380334978816 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7874618768692017, loss=5.799334526062012
I0302 12:32:43.946556 140380326586112 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9681680798530579, loss=3.819983959197998
I0302 12:33:29.573607 140380334978816 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5761421322822571, loss=5.9456562995910645
I0302 12:33:40.523419 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:33:52.482142 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:34:06.723948 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:34:08.405944 140575196817216 submission_runner.py:411] Time since start: 3624.01s, 	Step: 7626, 	{'train/accuracy': 0.3844531178474426, 'train/loss': 2.8820815086364746, 'validation/accuracy': 0.3578200042247772, 'validation/loss': 3.024298667907715, 'validation/num_examples': 50000, 'test/accuracy': 0.2787000238895416, 'test/loss': 3.58750057220459, 'test/num_examples': 10000, 'score': 3403.8257796764374, 'total_duration': 3624.0079770088196, 'accumulated_submission_time': 3403.8257796764374, 'accumulated_eval_time': 219.53815126419067, 'accumulated_logging_time': 0.23094701766967773}
I0302 12:34:08.430652 140380326586112 logging_writer.py:48] [7626] accumulated_eval_time=219.538151, accumulated_logging_time=0.230947, accumulated_submission_time=3403.825780, global_step=7626, preemption_count=0, score=3403.825780, test/accuracy=0.278700, test/loss=3.587501, test/num_examples=10000, total_duration=3624.007977, train/accuracy=0.384453, train/loss=2.882082, validation/accuracy=0.357820, validation/loss=3.024299, validation/num_examples=50000
I0302 12:34:38.304237 140380334978816 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5759247541427612, loss=5.844638347625732
I0302 12:35:20.965652 140380326586112 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7973207831382751, loss=5.269500732421875
I0302 12:36:05.366639 140380334978816 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9121862053871155, loss=3.81011962890625
I0302 12:36:49.605849 140380326586112 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8136796951293945, loss=4.039853572845459
I0302 12:37:33.900522 140380334978816 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9640451669692993, loss=3.7255334854125977
I0302 12:38:18.314932 140380326586112 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7631283402442932, loss=5.7169342041015625
I0302 12:39:02.640276 140380334978816 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9347070455551147, loss=4.022758960723877
I0302 12:39:47.209473 140380326586112 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0274012088775635, loss=3.489875078201294
I0302 12:40:31.647731 140380334978816 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8569691181182861, loss=4.326713562011719
I0302 12:41:08.569485 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:41:20.857457 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:41:31.890006 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:41:33.568427 140575196817216 submission_runner.py:411] Time since start: 4069.17s, 	Step: 8585, 	{'train/accuracy': 0.4216406047344208, 'train/loss': 2.6589698791503906, 'validation/accuracy': 0.3878999948501587, 'validation/loss': 2.8341987133026123, 'validation/num_examples': 50000, 'test/accuracy': 0.29750001430511475, 'test/loss': 3.4198501110076904, 'test/num_examples': 10000, 'score': 3823.8984911441803, 'total_duration': 4069.170470237732, 'accumulated_submission_time': 3823.8984911441803, 'accumulated_eval_time': 244.53706979751587, 'accumulated_logging_time': 0.27056431770324707}
I0302 12:41:33.590394 140380326586112 logging_writer.py:48] [8585] accumulated_eval_time=244.537070, accumulated_logging_time=0.270564, accumulated_submission_time=3823.898491, global_step=8585, preemption_count=0, score=3823.898491, test/accuracy=0.297500, test/loss=3.419850, test/num_examples=10000, total_duration=4069.170470, train/accuracy=0.421641, train/loss=2.658970, validation/accuracy=0.387900, validation/loss=2.834199, validation/num_examples=50000
I0302 12:41:40.001330 140380334978816 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8064131140708923, loss=3.7271125316619873
I0302 12:42:20.140439 140380326586112 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.9954580068588257, loss=3.5761122703552246
I0302 12:43:04.866638 140380334978816 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9827459454536438, loss=3.7806448936462402
I0302 12:43:49.323931 140380326586112 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9998303055763245, loss=3.752964973449707
I0302 12:44:33.926753 140380334978816 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7851835489273071, loss=4.854452133178711
I0302 12:45:18.372732 140380326586112 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.861687183380127, loss=4.4293599128723145
I0302 12:46:02.821352 140380334978816 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6807358860969543, loss=4.866567611694336
I0302 12:46:47.240751 140380326586112 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7052720785140991, loss=4.815254211425781
I0302 12:47:31.860803 140380334978816 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7305721044540405, loss=5.721297264099121
I0302 12:48:16.023038 140380326586112 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8332074880599976, loss=4.3794684410095215
I0302 12:48:33.577141 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:48:46.012293 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:48:58.535127 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:49:00.203575 140575196817216 submission_runner.py:411] Time since start: 4515.81s, 	Step: 9541, 	{'train/accuracy': 0.4546484351158142, 'train/loss': 2.4515960216522217, 'validation/accuracy': 0.41405999660491943, 'validation/loss': 2.6631486415863037, 'validation/num_examples': 50000, 'test/accuracy': 0.3157000243663788, 'test/loss': 3.2867302894592285, 'test/num_examples': 10000, 'score': 4243.820187568665, 'total_duration': 4515.805619239807, 'accumulated_submission_time': 4243.820187568665, 'accumulated_eval_time': 271.1634876728058, 'accumulated_logging_time': 0.3059976100921631}
I0302 12:49:00.222575 140380334978816 logging_writer.py:48] [9541] accumulated_eval_time=271.163488, accumulated_logging_time=0.305998, accumulated_submission_time=4243.820188, global_step=9541, preemption_count=0, score=4243.820188, test/accuracy=0.315700, test/loss=3.286730, test/num_examples=10000, total_duration=4515.805619, train/accuracy=0.454648, train/loss=2.451596, validation/accuracy=0.414060, validation/loss=2.663149, validation/num_examples=50000
I0302 12:49:24.122729 140380326586112 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.843298077583313, loss=5.801352500915527
I0302 12:50:06.689200 140380334978816 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9591341018676758, loss=3.8859591484069824
I0302 12:50:50.999576 140380326586112 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.1717112064361572, loss=3.5129849910736084
I0302 12:51:35.211405 140380334978816 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9900577068328857, loss=3.405388355255127
I0302 12:52:20.435297 140380326586112 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7924250960350037, loss=5.703725337982178
I0302 12:53:04.783906 140380334978816 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9500302672386169, loss=3.3624441623687744
I0302 12:53:48.860994 140380326586112 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.8915834426879883, loss=4.3475799560546875
I0302 12:54:33.137560 140380334978816 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7709022164344788, loss=5.334181308746338
I0302 12:55:17.490473 140380326586112 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.1453725099563599, loss=5.765778064727783
I0302 12:56:00.219541 140575196817216 spec.py:321] Evaluating on the training split.
I0302 12:56:13.135978 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 12:56:29.036870 140575196817216 spec.py:349] Evaluating on the test split.
I0302 12:56:30.713033 140575196817216 submission_runner.py:411] Time since start: 4966.32s, 	Step: 10498, 	{'train/accuracy': 0.45869138836860657, 'train/loss': 2.4536895751953125, 'validation/accuracy': 0.4279399812221527, 'validation/loss': 2.6150896549224854, 'validation/num_examples': 50000, 'test/accuracy': 0.3304000198841095, 'test/loss': 3.208634853363037, 'test/num_examples': 10000, 'score': 4663.750853538513, 'total_duration': 4966.315071105957, 'accumulated_submission_time': 4663.750853538513, 'accumulated_eval_time': 301.65695548057556, 'accumulated_logging_time': 0.3401913642883301}
I0302 12:56:30.735389 140380334978816 logging_writer.py:48] [10498] accumulated_eval_time=301.656955, accumulated_logging_time=0.340191, accumulated_submission_time=4663.750854, global_step=10498, preemption_count=0, score=4663.750854, test/accuracy=0.330400, test/loss=3.208635, test/num_examples=10000, total_duration=4966.315071, train/accuracy=0.458691, train/loss=2.453690, validation/accuracy=0.427940, validation/loss=2.615090, validation/num_examples=50000
I0302 12:56:31.974353 140380326586112 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9138611555099487, loss=3.4700160026550293
I0302 12:57:11.845680 140380334978816 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7413039803504944, loss=4.872937202453613
I0302 12:57:55.791177 140380326586112 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9145887494087219, loss=3.7786331176757812
I0302 12:58:40.271560 140380334978816 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.028114914894104, loss=3.3368148803710938
I0302 12:59:24.700952 140380326586112 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7299125790596008, loss=5.344300270080566
I0302 13:00:09.223296 140380334978816 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9742235541343689, loss=5.815938472747803
I0302 13:00:53.419347 140380326586112 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9758488535881042, loss=3.718808650970459
I0302 13:01:37.987260 140380334978816 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9461639523506165, loss=3.6830050945281982
I0302 13:02:22.627822 140380326586112 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0713484287261963, loss=3.3565196990966797
I0302 13:03:07.013930 140380334978816 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.8324617743492126, loss=5.745739936828613
I0302 13:03:30.773580 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:03:43.953221 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:04:02.467159 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:04:04.137916 140575196817216 submission_runner.py:411] Time since start: 5419.74s, 	Step: 11454, 	{'train/accuracy': 0.47783201932907104, 'train/loss': 2.3402180671691895, 'validation/accuracy': 0.44377997517585754, 'validation/loss': 2.5133609771728516, 'validation/num_examples': 50000, 'test/accuracy': 0.3394000232219696, 'test/loss': 3.1516380310058594, 'test/num_examples': 10000, 'score': 5083.722537994385, 'total_duration': 5419.739944219589, 'accumulated_submission_time': 5083.722537994385, 'accumulated_eval_time': 335.0212616920471, 'accumulated_logging_time': 0.377582311630249}
I0302 13:04:04.161107 140380326586112 logging_writer.py:48] [11454] accumulated_eval_time=335.021262, accumulated_logging_time=0.377582, accumulated_submission_time=5083.722538, global_step=11454, preemption_count=0, score=5083.722538, test/accuracy=0.339400, test/loss=3.151638, test/num_examples=10000, total_duration=5419.739944, train/accuracy=0.477832, train/loss=2.340218, validation/accuracy=0.443780, validation/loss=2.513361, validation/num_examples=50000
I0302 13:04:22.849417 140380334978816 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8099215030670166, loss=4.356706142425537
I0302 13:05:04.723761 140380326586112 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.0084093809127808, loss=3.3295063972473145
I0302 13:05:48.939699 140380334978816 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0578826665878296, loss=3.3654165267944336
I0302 13:06:33.428541 140380326586112 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.1621356010437012, loss=3.2436347007751465
I0302 13:07:17.908169 140380334978816 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.185123324394226, loss=3.6395044326782227
I0302 13:08:02.047099 140380326586112 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.982342541217804, loss=3.2828352451324463
I0302 13:08:46.266194 140380334978816 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7983050346374512, loss=4.590821266174316
I0302 13:09:30.642581 140380326586112 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.1119407415390015, loss=3.5558876991271973
I0302 13:10:15.044393 140380334978816 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9097070693969727, loss=3.4298813343048096
I0302 13:10:59.960434 140380326586112 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0787982940673828, loss=3.225039005279541
I0302 13:11:04.624684 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:11:18.064085 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:11:31.028981 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:11:32.707826 140575196817216 submission_runner.py:411] Time since start: 5868.31s, 	Step: 12412, 	{'train/accuracy': 0.5044335722923279, 'train/loss': 2.1601967811584473, 'validation/accuracy': 0.46087998151779175, 'validation/loss': 2.383046865463257, 'validation/num_examples': 50000, 'test/accuracy': 0.35600000619888306, 'test/loss': 3.023467540740967, 'test/num_examples': 10000, 'score': 5504.124020576477, 'total_duration': 5868.309863567352, 'accumulated_submission_time': 5504.124020576477, 'accumulated_eval_time': 363.1043710708618, 'accumulated_logging_time': 0.4115440845489502}
I0302 13:11:32.731372 140380334978816 logging_writer.py:48] [12412] accumulated_eval_time=363.104371, accumulated_logging_time=0.411544, accumulated_submission_time=5504.124021, global_step=12412, preemption_count=0, score=5504.124021, test/accuracy=0.356000, test/loss=3.023468, test/num_examples=10000, total_duration=5868.309864, train/accuracy=0.504434, train/loss=2.160197, validation/accuracy=0.460880, validation/loss=2.383047, validation/num_examples=50000
I0302 13:12:08.496266 140380326586112 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8439696431159973, loss=4.16262674331665
I0302 13:12:52.979412 140380334978816 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7281259894371033, loss=5.2612738609313965
I0302 13:13:37.475146 140380326586112 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1536405086517334, loss=3.2304399013519287
I0302 13:14:21.737036 140380334978816 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0027904510498047, loss=5.209627151489258
I0302 13:15:05.903024 140380326586112 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.1483519077301025, loss=3.3347644805908203
I0302 13:15:50.119611 140380334978816 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.1557141542434692, loss=3.314680576324463
I0302 13:16:34.658021 140380326586112 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7436491250991821, loss=5.7309136390686035
I0302 13:17:18.926944 140380334978816 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.967635989189148, loss=4.61651086807251
I0302 13:18:04.113023 140380326586112 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.8307290077209473, loss=5.004030704498291
I0302 13:18:32.925650 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:18:45.105206 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:18:57.130300 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:18:58.808543 140575196817216 submission_runner.py:411] Time since start: 6314.41s, 	Step: 13365, 	{'train/accuracy': 0.52685546875, 'train/loss': 2.067809820175171, 'validation/accuracy': 0.4763000011444092, 'validation/loss': 2.321436882019043, 'validation/num_examples': 50000, 'test/accuracy': 0.36730000376701355, 'test/loss': 2.974306106567383, 'test/num_examples': 10000, 'score': 5924.25744843483, 'total_duration': 6314.410578966141, 'accumulated_submission_time': 5924.25744843483, 'accumulated_eval_time': 388.98726081848145, 'accumulated_logging_time': 0.44565749168395996}
I0302 13:18:58.832908 140380334978816 logging_writer.py:48] [13365] accumulated_eval_time=388.987261, accumulated_logging_time=0.445657, accumulated_submission_time=5924.257448, global_step=13365, preemption_count=0, score=5924.257448, test/accuracy=0.367300, test/loss=2.974306, test/num_examples=10000, total_duration=6314.410579, train/accuracy=0.526855, train/loss=2.067810, validation/accuracy=0.476300, validation/loss=2.321437, validation/num_examples=50000
I0302 13:19:13.146539 140380326586112 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9627872705459595, loss=3.514448404312134
I0302 13:19:54.411749 140380334978816 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0267380475997925, loss=3.1492769718170166
I0302 13:20:38.692574 140380326586112 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.3194011449813843, loss=3.1976191997528076
I0302 13:21:23.280893 140380334978816 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.073473572731018, loss=3.2211978435516357
I0302 13:22:08.203331 140380326586112 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9847028255462646, loss=5.500371932983398
I0302 13:22:52.623612 140380334978816 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8122860193252563, loss=5.7002716064453125
I0302 13:23:37.359109 140380326586112 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8704051375389099, loss=5.490352630615234
I0302 13:24:22.586443 140380334978816 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.760973334312439, loss=4.996299743652344
I0302 13:25:07.642854 140380326586112 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7598652839660645, loss=4.605926036834717
I0302 13:25:52.309585 140380334978816 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0378227233886719, loss=2.9976305961608887
I0302 13:25:59.078477 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:26:12.532468 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:26:28.192096 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:26:29.852916 140575196817216 submission_runner.py:411] Time since start: 6765.45s, 	Step: 14317, 	{'train/accuracy': 0.5179492235183716, 'train/loss': 2.1016087532043457, 'validation/accuracy': 0.4814999997615814, 'validation/loss': 2.2885682582855225, 'validation/num_examples': 50000, 'test/accuracy': 0.3791000247001648, 'test/loss': 2.91217041015625, 'test/num_examples': 10000, 'score': 6344.441004276276, 'total_duration': 6765.454939126968, 'accumulated_submission_time': 6344.441004276276, 'accumulated_eval_time': 419.7616608142853, 'accumulated_logging_time': 0.4812161922454834}
I0302 13:26:29.872708 140380326586112 logging_writer.py:48] [14317] accumulated_eval_time=419.761661, accumulated_logging_time=0.481216, accumulated_submission_time=6344.441004, global_step=14317, preemption_count=0, score=6344.441004, test/accuracy=0.379100, test/loss=2.912170, test/num_examples=10000, total_duration=6765.454939, train/accuracy=0.517949, train/loss=2.101609, validation/accuracy=0.481500, validation/loss=2.288568, validation/num_examples=50000
I0302 13:27:03.262760 140380334978816 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.1686522960662842, loss=3.1205177307128906
I0302 13:27:46.003350 140380326586112 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9526010155677795, loss=3.436035633087158
I0302 13:28:30.510981 140380334978816 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0830715894699097, loss=2.947888135910034
I0302 13:29:15.572159 140380326586112 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.072916030883789, loss=3.026848316192627
I0302 13:30:00.893027 140380334978816 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.993890643119812, loss=3.170177936553955
I0302 13:30:45.885187 140380326586112 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0098018646240234, loss=3.0468714237213135
I0302 13:31:31.555937 140380334978816 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.826662003993988, loss=5.257897853851318
I0302 13:32:16.582769 140380326586112 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9954066872596741, loss=3.07649302482605
I0302 13:33:01.380671 140380334978816 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1041021347045898, loss=2.977179765701294
I0302 13:33:30.042435 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:33:43.889540 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:34:00.948086 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:34:02.609929 140575196817216 submission_runner.py:411] Time since start: 7218.21s, 	Step: 15266, 	{'train/accuracy': 0.5394921898841858, 'train/loss': 1.976670503616333, 'validation/accuracy': 0.4970199763774872, 'validation/loss': 2.2036519050598145, 'validation/num_examples': 50000, 'test/accuracy': 0.38840001821517944, 'test/loss': 2.8618249893188477, 'test/num_examples': 10000, 'score': 6764.550142526627, 'total_duration': 7218.211961269379, 'accumulated_submission_time': 6764.550142526627, 'accumulated_eval_time': 452.32911682128906, 'accumulated_logging_time': 0.5112817287445068}
I0302 13:34:02.629326 140380326586112 logging_writer.py:48] [15266] accumulated_eval_time=452.329117, accumulated_logging_time=0.511282, accumulated_submission_time=6764.550143, global_step=15266, preemption_count=0, score=6764.550143, test/accuracy=0.388400, test/loss=2.861825, test/num_examples=10000, total_duration=7218.211961, train/accuracy=0.539492, train/loss=1.976671, validation/accuracy=0.497020, validation/loss=2.203652, validation/num_examples=50000
I0302 13:34:16.565073 140380334978816 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.061515212059021, loss=3.0776896476745605
I0302 13:34:56.531219 140380326586112 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.2154467105865479, loss=3.198774576187134
I0302 13:35:41.762631 140380334978816 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.0708370208740234, loss=3.081469774246216
I0302 13:36:26.910562 140380326586112 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.040626049041748, loss=3.240863800048828
I0302 13:37:11.835079 140380334978816 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9093350172042847, loss=3.836559295654297
I0302 13:37:56.569568 140380326586112 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9261623620986938, loss=4.308678150177002
I0302 13:38:40.851032 140380334978816 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.3063470125198364, loss=2.930438280105591
I0302 13:39:25.690405 140380326586112 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9679214358329773, loss=3.331622362136841
I0302 13:40:10.614575 140380334978816 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.1955877542495728, loss=3.0592360496520996
I0302 13:40:55.770459 140380326586112 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0449415445327759, loss=2.9930667877197266
I0302 13:41:02.854483 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:41:17.435049 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:41:36.002807 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:41:37.673234 140575196817216 submission_runner.py:411] Time since start: 7673.28s, 	Step: 16217, 	{'train/accuracy': 0.5404882431030273, 'train/loss': 2.0090866088867188, 'validation/accuracy': 0.4944399893283844, 'validation/loss': 2.2409517765045166, 'validation/num_examples': 50000, 'test/accuracy': 0.3890000283718109, 'test/loss': 2.8712475299835205, 'test/num_examples': 10000, 'score': 7184.7117347717285, 'total_duration': 7673.275277376175, 'accumulated_submission_time': 7184.7117347717285, 'accumulated_eval_time': 487.14783573150635, 'accumulated_logging_time': 0.5434060096740723}
I0302 13:41:37.693466 140380334978816 logging_writer.py:48] [16217] accumulated_eval_time=487.147836, accumulated_logging_time=0.543406, accumulated_submission_time=7184.711735, global_step=16217, preemption_count=0, score=7184.711735, test/accuracy=0.389000, test/loss=2.871248, test/num_examples=10000, total_duration=7673.275277, train/accuracy=0.540488, train/loss=2.009087, validation/accuracy=0.494440, validation/loss=2.240952, validation/num_examples=50000
I0302 13:42:11.133975 140380326586112 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.077309489250183, loss=2.9110794067382812
I0302 13:42:54.212487 140380334978816 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0656754970550537, loss=4.518390655517578
I0302 13:43:39.089382 140380326586112 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.8148491382598877, loss=5.323902130126953
I0302 13:44:24.085325 140380334978816 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.8343547582626343, loss=3.847172737121582
I0302 13:45:09.009419 140380326586112 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0457768440246582, loss=3.053571939468384
I0302 13:45:53.670989 140380334978816 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9283580780029297, loss=3.5386123657226562
I0302 13:46:38.519269 140380326586112 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0564125776290894, loss=3.004607915878296
I0302 13:47:23.227056 140380334978816 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1082401275634766, loss=3.1359987258911133
I0302 13:48:08.361320 140380326586112 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.0829131603240967, loss=3.0144762992858887
I0302 13:48:37.805551 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:48:53.361369 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:49:12.009560 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:49:13.657537 140575196817216 submission_runner.py:411] Time since start: 8129.26s, 	Step: 17168, 	{'train/accuracy': 0.5452538728713989, 'train/loss': 1.9710559844970703, 'validation/accuracy': 0.5032199621200562, 'validation/loss': 2.1825075149536133, 'validation/num_examples': 50000, 'test/accuracy': 0.38960000872612, 'test/loss': 2.8298895359039307, 'test/num_examples': 10000, 'score': 7604.762713670731, 'total_duration': 8129.259591817856, 'accumulated_submission_time': 7604.762713670731, 'accumulated_eval_time': 522.9998071193695, 'accumulated_logging_time': 0.5738611221313477}
I0302 13:49:13.675544 140380334978816 logging_writer.py:48] [17168] accumulated_eval_time=522.999807, accumulated_logging_time=0.573861, accumulated_submission_time=7604.762714, global_step=17168, preemption_count=0, score=7604.762714, test/accuracy=0.389600, test/loss=2.829890, test/num_examples=10000, total_duration=8129.259592, train/accuracy=0.545254, train/loss=1.971056, validation/accuracy=0.503220, validation/loss=2.182508, validation/num_examples=50000
I0302 13:49:26.786263 140380326586112 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.1098694801330566, loss=2.9627597332000732
I0302 13:50:07.957553 140380334978816 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.074019432067871, loss=2.9138855934143066
I0302 13:50:52.557414 140380326586112 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9195349812507629, loss=3.8201241493225098
I0302 13:51:37.600134 140380334978816 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.00933039188385, loss=3.462486982345581
I0302 13:52:22.290429 140380326586112 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8558601140975952, loss=5.580917835235596
I0302 13:53:07.201292 140380334978816 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9884202480316162, loss=3.5423569679260254
I0302 13:53:52.028886 140380326586112 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.302574872970581, loss=3.021735668182373
I0302 13:54:36.927995 140380334978816 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.049126386642456, loss=2.819654941558838
I0302 13:55:21.817099 140380326586112 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1256219148635864, loss=2.858281135559082
I0302 13:56:06.612170 140380334978816 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0154645442962646, loss=5.000115394592285
I0302 13:56:13.975441 140575196817216 spec.py:321] Evaluating on the training split.
I0302 13:56:29.473670 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 13:56:47.776108 140575196817216 spec.py:349] Evaluating on the test split.
I0302 13:56:49.449369 140575196817216 submission_runner.py:411] Time since start: 8585.05s, 	Step: 18118, 	{'train/accuracy': 0.5589648485183716, 'train/loss': 1.9002271890640259, 'validation/accuracy': 0.5151199698448181, 'validation/loss': 2.097113847732544, 'validation/num_examples': 50000, 'test/accuracy': 0.40290001034736633, 'test/loss': 2.7592999935150146, 'test/num_examples': 10000, 'score': 8025.002263069153, 'total_duration': 8585.051396369934, 'accumulated_submission_time': 8025.002263069153, 'accumulated_eval_time': 558.4737074375153, 'accumulated_logging_time': 0.6011612415313721}
I0302 13:56:49.470728 140380326586112 logging_writer.py:48] [18118] accumulated_eval_time=558.473707, accumulated_logging_time=0.601161, accumulated_submission_time=8025.002263, global_step=18118, preemption_count=0, score=8025.002263, test/accuracy=0.402900, test/loss=2.759300, test/num_examples=10000, total_duration=8585.051396, train/accuracy=0.558965, train/loss=1.900227, validation/accuracy=0.515120, validation/loss=2.097114, validation/num_examples=50000
I0302 13:57:22.665229 140380334978816 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.0957443714141846, loss=2.9311342239379883
I0302 13:58:07.369336 140380326586112 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.1295630931854248, loss=2.9205615520477295
I0302 13:58:52.087233 140380334978816 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.00370192527771, loss=4.7933783531188965
I0302 13:59:36.667120 140380326586112 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.137363076210022, loss=3.2081215381622314
I0302 14:00:21.511156 140380334978816 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8738094568252563, loss=5.225572109222412
I0302 14:01:06.684311 140380326586112 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8492576479911804, loss=4.908679008483887
I0302 14:01:51.678799 140380334978816 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.8737152218818665, loss=4.394935131072998
I0302 14:02:36.696735 140380326586112 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2545123100280762, loss=2.8492136001586914
I0302 14:03:21.642716 140380334978816 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.1802606582641602, loss=2.995211124420166
I0302 14:03:49.587872 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:04:03.121488 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:04:25.502049 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:04:27.160453 140575196817216 submission_runner.py:411] Time since start: 9042.76s, 	Step: 19064, 	{'train/accuracy': 0.5706835985183716, 'train/loss': 1.8196309804916382, 'validation/accuracy': 0.5245599746704102, 'validation/loss': 2.0479772090911865, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.712984800338745, 'test/num_examples': 10000, 'score': 8445.054428339005, 'total_duration': 9042.762500047684, 'accumulated_submission_time': 8445.054428339005, 'accumulated_eval_time': 596.0462672710419, 'accumulated_logging_time': 0.6375689506530762}
I0302 14:04:27.183434 140380326586112 logging_writer.py:48] [19064] accumulated_eval_time=596.046267, accumulated_logging_time=0.637569, accumulated_submission_time=8445.054428, global_step=19064, preemption_count=0, score=8445.054428, test/accuracy=0.407500, test/loss=2.712985, test/num_examples=10000, total_duration=9042.762500, train/accuracy=0.570684, train/loss=1.819631, validation/accuracy=0.524560, validation/loss=2.047977, validation/num_examples=50000
I0302 14:04:41.883753 140380334978816 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.1190083026885986, loss=2.995011568069458
I0302 14:05:23.247197 140380326586112 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.1094483137130737, loss=3.199235439300537
I0302 14:06:07.675869 140380334978816 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.2771611213684082, loss=3.445659875869751
I0302 14:06:52.789441 140380326586112 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.007530927658081, loss=5.381777763366699
I0302 14:07:37.512312 140380334978816 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9033008813858032, loss=5.423044204711914
I0302 14:08:22.014237 140380326586112 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8636128306388855, loss=4.436237335205078
I0302 14:09:06.567816 140380334978816 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.3493930101394653, loss=2.949632406234741
I0302 14:09:51.211164 140380326586112 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1100780963897705, loss=2.9041998386383057
I0302 14:10:36.278869 140380334978816 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8216443657875061, loss=5.185606002807617
I0302 14:11:20.914693 140380326586112 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9323991537094116, loss=3.99717116355896
I0302 14:11:27.287920 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:11:40.800137 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:12:00.637101 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:12:02.307936 140575196817216 submission_runner.py:411] Time since start: 9497.91s, 	Step: 20015, 	{'train/accuracy': 0.5750390291213989, 'train/loss': 1.813962697982788, 'validation/accuracy': 0.5193799734115601, 'validation/loss': 2.0880651473999023, 'validation/num_examples': 50000, 'test/accuracy': 0.4067000150680542, 'test/loss': 2.7463629245758057, 'test/num_examples': 10000, 'score': 8865.098269224167, 'total_duration': 9497.909968614578, 'accumulated_submission_time': 8865.098269224167, 'accumulated_eval_time': 631.0662536621094, 'accumulated_logging_time': 0.6704673767089844}
I0302 14:12:02.329248 140380334978816 logging_writer.py:48] [20015] accumulated_eval_time=631.066254, accumulated_logging_time=0.670467, accumulated_submission_time=8865.098269, global_step=20015, preemption_count=0, score=8865.098269, test/accuracy=0.406700, test/loss=2.746363, test/num_examples=10000, total_duration=9497.909969, train/accuracy=0.575039, train/loss=1.813963, validation/accuracy=0.519380, validation/loss=2.088065, validation/num_examples=50000
I0302 14:12:36.853054 140380326586112 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9265168309211731, loss=4.618973731994629
I0302 14:13:21.467601 140380334978816 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.998802125453949, loss=5.50414514541626
I0302 14:14:06.263524 140380326586112 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.9391288161277771, loss=3.702348470687866
I0302 14:14:50.878509 140380334978816 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0751596689224243, loss=2.9851603507995605
I0302 14:15:36.096169 140380326586112 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9022649526596069, loss=5.3384175300598145
I0302 14:16:21.525867 140380334978816 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.145189881324768, loss=2.8263325691223145
I0302 14:17:06.592517 140380326586112 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9839916825294495, loss=5.053564071655273
I0302 14:17:51.568023 140380334978816 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.9469698071479797, loss=4.440008640289307
I0302 14:18:36.783750 140380326586112 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.138358235359192, loss=2.8538973331451416
I0302 14:19:02.650718 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:19:15.744280 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:19:38.039984 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:19:39.700594 140575196817216 submission_runner.py:411] Time since start: 9955.30s, 	Step: 20959, 	{'train/accuracy': 0.5743163824081421, 'train/loss': 1.8025161027908325, 'validation/accuracy': 0.5355199575424194, 'validation/loss': 2.0042288303375244, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.6817190647125244, 'test/num_examples': 10000, 'score': 9285.359723567963, 'total_duration': 9955.302630662918, 'accumulated_submission_time': 9285.359723567963, 'accumulated_eval_time': 668.1161289215088, 'accumulated_logging_time': 0.7022011280059814}
I0302 14:19:39.720995 140380334978816 logging_writer.py:48] [20959] accumulated_eval_time=668.116129, accumulated_logging_time=0.702201, accumulated_submission_time=9285.359724, global_step=20959, preemption_count=0, score=9285.359724, test/accuracy=0.419200, test/loss=2.681719, test/num_examples=10000, total_duration=9955.302631, train/accuracy=0.574316, train/loss=1.802516, validation/accuracy=0.535520, validation/loss=2.004229, validation/num_examples=50000
I0302 14:19:56.425608 140380326586112 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.1485456228256226, loss=2.88134765625
I0302 14:20:38.990915 140380334978816 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.1765329837799072, loss=2.941209316253662
I0302 14:21:23.602932 140380326586112 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.1144516468048096, loss=2.8269314765930176
I0302 14:22:08.825080 140380334978816 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.2183396816253662, loss=2.9798641204833984
I0302 14:22:53.758919 140380326586112 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9927148222923279, loss=3.6772053241729736
I0302 14:23:38.662710 140380334978816 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.1346313953399658, loss=2.9321272373199463
I0302 14:24:23.848489 140380326586112 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.9130316972732544, loss=4.938065528869629
I0302 14:25:08.628267 140380334978816 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2042564153671265, loss=2.728801965713501
I0302 14:25:53.475418 140380326586112 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8538020253181458, loss=5.208266735076904
I0302 14:26:38.462329 140380334978816 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.2969528436660767, loss=2.754676342010498
I0302 14:26:39.913618 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:26:54.193836 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:27:12.499985 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:27:14.208863 140575196817216 submission_runner.py:411] Time since start: 10409.81s, 	Step: 21905, 	{'train/accuracy': 0.5809569954872131, 'train/loss': 1.7895175218582153, 'validation/accuracy': 0.5389999747276306, 'validation/loss': 1.9918287992477417, 'validation/num_examples': 50000, 'test/accuracy': 0.42740002274513245, 'test/loss': 2.6510984897613525, 'test/num_examples': 10000, 'score': 9705.488340377808, 'total_duration': 10409.810875177383, 'accumulated_submission_time': 9705.488340377808, 'accumulated_eval_time': 702.4113335609436, 'accumulated_logging_time': 0.7362079620361328}
I0302 14:27:14.241107 140380326586112 logging_writer.py:48] [21905] accumulated_eval_time=702.411334, accumulated_logging_time=0.736208, accumulated_submission_time=9705.488340, global_step=21905, preemption_count=0, score=9705.488340, test/accuracy=0.427400, test/loss=2.651098, test/num_examples=10000, total_duration=10409.810875, train/accuracy=0.580957, train/loss=1.789518, validation/accuracy=0.539000, validation/loss=1.991829, validation/num_examples=50000
I0302 14:27:53.671517 140380334978816 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9747676253318787, loss=4.094473838806152
I0302 14:28:38.786047 140380326586112 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0523631572723389, loss=3.402070999145508
I0302 14:29:23.870465 140380334978816 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.1646888256072998, loss=2.9801783561706543
I0302 14:30:08.926764 140380326586112 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.1958049535751343, loss=2.918635845184326
I0302 14:30:53.637152 140380334978816 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0439562797546387, loss=2.6909754276275635
I0302 14:31:38.477619 140380326586112 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.2085916996002197, loss=2.7769176959991455
I0302 14:32:23.598936 140380334978816 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.1699060201644897, loss=2.661442756652832
I0302 14:33:08.822449 140380326586112 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.8304616212844849, loss=5.205864906311035
I0302 14:33:53.662109 140380334978816 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0287967920303345, loss=4.301105499267578
I0302 14:34:14.513102 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:34:25.692730 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:34:48.469223 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:34:50.118915 140575196817216 submission_runner.py:411] Time since start: 10865.72s, 	Step: 22848, 	{'train/accuracy': 0.5851757526397705, 'train/loss': 1.7660349607467651, 'validation/accuracy': 0.5362200140953064, 'validation/loss': 2.0083796977996826, 'validation/num_examples': 50000, 'test/accuracy': 0.4195000231266022, 'test/loss': 2.6842269897460938, 'test/num_examples': 10000, 'score': 10125.69636964798, 'total_duration': 10865.72094798088, 'accumulated_submission_time': 10125.69636964798, 'accumulated_eval_time': 738.0171258449554, 'accumulated_logging_time': 0.7833480834960938}
I0302 14:34:50.140925 140380326586112 logging_writer.py:48] [22848] accumulated_eval_time=738.017126, accumulated_logging_time=0.783348, accumulated_submission_time=10125.696370, global_step=22848, preemption_count=0, score=10125.696370, test/accuracy=0.419500, test/loss=2.684227, test/num_examples=10000, total_duration=10865.720948, train/accuracy=0.585176, train/loss=1.766035, validation/accuracy=0.536220, validation/loss=2.008380, validation/num_examples=50000
I0302 14:35:11.237686 140380334978816 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8284900784492493, loss=5.346827030181885
I0302 14:35:52.691740 140380326586112 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9003316164016724, loss=4.030158519744873
I0302 14:36:37.751060 140380334978816 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1126939058303833, loss=2.855842113494873
I0302 14:37:22.577160 140380326586112 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.1580651998519897, loss=2.8678500652313232
I0302 14:38:07.516894 140380334978816 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.149263620376587, loss=2.949953556060791
I0302 14:38:51.985101 140380326586112 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.026780366897583, loss=2.9825894832611084
I0302 14:39:36.828726 140380334978816 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0949496030807495, loss=2.7792930603027344
I0302 14:40:21.903796 140380326586112 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.1368783712387085, loss=2.877460241317749
I0302 14:41:06.461122 140380334978816 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.165116548538208, loss=2.8757553100585938
I0302 14:41:50.204497 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:42:00.679969 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:42:24.073740 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:42:25.732212 140575196817216 submission_runner.py:411] Time since start: 11321.33s, 	Step: 23799, 	{'train/accuracy': 0.6068359017372131, 'train/loss': 1.6197729110717773, 'validation/accuracy': 0.5496999621391296, 'validation/loss': 1.906225562095642, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.58562970161438, 'test/num_examples': 10000, 'score': 10545.699042081833, 'total_duration': 11321.33425951004, 'accumulated_submission_time': 10545.699042081833, 'accumulated_eval_time': 773.5448224544525, 'accumulated_logging_time': 0.8156635761260986}
I0302 14:42:25.756962 140380326586112 logging_writer.py:48] [23799] accumulated_eval_time=773.544822, accumulated_logging_time=0.815664, accumulated_submission_time=10545.699042, global_step=23799, preemption_count=0, score=10545.699042, test/accuracy=0.435300, test/loss=2.585630, test/num_examples=10000, total_duration=11321.334260, train/accuracy=0.606836, train/loss=1.619773, validation/accuracy=0.549700, validation/loss=1.906226, validation/num_examples=50000
I0302 14:42:26.558104 140380334978816 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1692426204681396, loss=5.064230918884277
I0302 14:43:06.385833 140380326586112 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.0264992713928223, loss=5.308258056640625
I0302 14:43:50.879104 140380334978816 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9803458452224731, loss=4.662922382354736
I0302 14:44:35.714024 140380326586112 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.102751612663269, loss=5.2092366218566895
I0302 14:45:20.344914 140380334978816 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9890531897544861, loss=3.5819549560546875
I0302 14:46:05.012709 140380326586112 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9872322678565979, loss=5.425140857696533
I0302 14:46:49.757220 140380334978816 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8707541823387146, loss=4.709910869598389
I0302 14:47:34.321215 140380326586112 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.1115895509719849, loss=3.283979892730713
I0302 14:48:18.874631 140380334978816 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0176455974578857, loss=4.467211723327637
I0302 14:49:03.577049 140380326586112 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.221378207206726, loss=2.8313050270080566
I0302 14:49:25.797387 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:49:36.469228 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:49:58.832841 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:50:00.490423 140575196817216 submission_runner.py:411] Time since start: 11776.09s, 	Step: 24751, 	{'train/accuracy': 0.5919336080551147, 'train/loss': 1.7640339136123657, 'validation/accuracy': 0.5448200106620789, 'validation/loss': 1.9703313112258911, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.6117215156555176, 'test/num_examples': 10000, 'score': 10965.678258657455, 'total_duration': 11776.092476606369, 'accumulated_submission_time': 10965.678258657455, 'accumulated_eval_time': 808.2378516197205, 'accumulated_logging_time': 0.850771427154541}
I0302 14:50:00.509162 140380334978816 logging_writer.py:48] [24751] accumulated_eval_time=808.237852, accumulated_logging_time=0.850771, accumulated_submission_time=10965.678259, global_step=24751, preemption_count=0, score=10965.678259, test/accuracy=0.431400, test/loss=2.611722, test/num_examples=10000, total_duration=11776.092477, train/accuracy=0.591934, train/loss=1.764034, validation/accuracy=0.544820, validation/loss=1.970331, validation/num_examples=50000
I0302 14:50:20.410171 140380326586112 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.1539719104766846, loss=2.7291924953460693
I0302 14:51:02.710222 140380334978816 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.2879319190979004, loss=2.581296443939209
I0302 14:51:47.153374 140380326586112 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0081450939178467, loss=3.1070518493652344
I0302 14:52:33.090613 140380334978816 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0138052701950073, loss=3.7035024166107178
I0302 14:53:17.966327 140380326586112 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2074387073516846, loss=2.747579574584961
I0302 14:54:03.128615 140380334978816 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.3384493589401245, loss=2.7647781372070312
I0302 14:54:47.874477 140380326586112 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.1882435083389282, loss=2.695925712585449
I0302 14:55:33.019122 140380334978816 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.0554063320159912, loss=4.296904563903809
I0302 14:56:18.342061 140380326586112 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0830888748168945, loss=2.7139949798583984
I0302 14:57:00.609047 140575196817216 spec.py:321] Evaluating on the training split.
I0302 14:57:11.229361 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 14:57:34.656608 140575196817216 spec.py:349] Evaluating on the test split.
I0302 14:57:36.310557 140575196817216 submission_runner.py:411] Time since start: 12231.91s, 	Step: 25696, 	{'train/accuracy': 0.6055663824081421, 'train/loss': 1.681242823600769, 'validation/accuracy': 0.560539960861206, 'validation/loss': 1.8968688249588013, 'validation/num_examples': 50000, 'test/accuracy': 0.4390000104904175, 'test/loss': 2.579681873321533, 'test/num_examples': 10000, 'score': 11385.718565702438, 'total_duration': 12231.912591457367, 'accumulated_submission_time': 11385.718565702438, 'accumulated_eval_time': 843.9393339157104, 'accumulated_logging_time': 0.8788700103759766}
I0302 14:57:36.338577 140380334978816 logging_writer.py:48] [25696] accumulated_eval_time=843.939334, accumulated_logging_time=0.878870, accumulated_submission_time=11385.718566, global_step=25696, preemption_count=0, score=11385.718566, test/accuracy=0.439000, test/loss=2.579682, test/num_examples=10000, total_duration=12231.912591, train/accuracy=0.605566, train/loss=1.681243, validation/accuracy=0.560540, validation/loss=1.896869, validation/num_examples=50000
I0302 14:57:38.658792 140380326586112 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1717474460601807, loss=2.7460246086120605
I0302 14:58:18.692280 140380334978816 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.0246893167495728, loss=3.7627053260803223
I0302 14:59:05.529511 140380326586112 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0378081798553467, loss=3.8598227500915527
I0302 14:59:51.945044 140380334978816 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.2990140914916992, loss=2.766554117202759
I0302 15:00:37.138044 140380326586112 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9516642689704895, loss=3.289452075958252
I0302 15:01:21.978596 140380334978816 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0831042528152466, loss=4.444577693939209
I0302 15:02:07.099649 140380326586112 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.2646903991699219, loss=2.6900863647460938
I0302 15:02:52.256919 140380334978816 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.93513023853302, loss=4.128637313842773
I0302 15:03:37.294336 140380326586112 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.2170974016189575, loss=2.699636220932007
I0302 15:04:22.488301 140380334978816 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9672952890396118, loss=4.691208362579346
I0302 15:04:36.599121 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:04:47.331622 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:05:09.321578 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:05:10.989533 140575196817216 submission_runner.py:411] Time since start: 12686.59s, 	Step: 26633, 	{'train/accuracy': 0.6142968535423279, 'train/loss': 1.5852680206298828, 'validation/accuracy': 0.5594599843025208, 'validation/loss': 1.8710987567901611, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.5757434368133545, 'test/num_examples': 10000, 'score': 11805.5884308815, 'total_duration': 12686.59158539772, 'accumulated_submission_time': 11805.5884308815, 'accumulated_eval_time': 878.3297283649445, 'accumulated_logging_time': 1.2480542659759521}
I0302 15:05:11.009439 140380326586112 logging_writer.py:48] [26633] accumulated_eval_time=878.329728, accumulated_logging_time=1.248054, accumulated_submission_time=11805.588431, global_step=26633, preemption_count=0, score=11805.588431, test/accuracy=0.431000, test/loss=2.575743, test/num_examples=10000, total_duration=12686.591585, train/accuracy=0.614297, train/loss=1.585268, validation/accuracy=0.559460, validation/loss=1.871099, validation/num_examples=50000
I0302 15:05:38.101155 140380334978816 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9327284693717957, loss=4.06357479095459
I0302 15:06:22.044692 140380326586112 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.14304780960083, loss=5.198188304901123
I0302 15:07:07.034272 140380334978816 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.8879929184913635, loss=4.3335795402526855
I0302 15:07:51.862532 140380326586112 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.2153502702713013, loss=3.14056396484375
I0302 15:08:36.810956 140380334978816 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9762219786643982, loss=3.4674887657165527
I0302 15:09:21.676352 140380326586112 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.02584969997406, loss=4.206315040588379
I0302 15:10:06.828532 140380334978816 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9778505563735962, loss=3.6398491859436035
I0302 15:10:51.697876 140380326586112 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0980967283248901, loss=4.470772743225098
I0302 15:11:36.615426 140380334978816 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0500874519348145, loss=4.383660793304443
I0302 15:12:11.399467 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:12:22.077137 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:12:44.171284 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:12:45.830907 140575196817216 submission_runner.py:411] Time since start: 13141.43s, 	Step: 27578, 	{'train/accuracy': 0.6026562452316284, 'train/loss': 1.6675916910171509, 'validation/accuracy': 0.5617600083351135, 'validation/loss': 1.8550418615341187, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.531644105911255, 'test/num_examples': 10000, 'score': 12225.917182445526, 'total_duration': 13141.4329662323, 'accumulated_submission_time': 12225.917182445526, 'accumulated_eval_time': 912.7611672878265, 'accumulated_logging_time': 1.279726266860962}
I0302 15:12:45.851566 140380326586112 logging_writer.py:48] [27578] accumulated_eval_time=912.761167, accumulated_logging_time=1.279726, accumulated_submission_time=12225.917182, global_step=27578, preemption_count=0, score=12225.917182, test/accuracy=0.442300, test/loss=2.531644, test/num_examples=10000, total_duration=13141.432966, train/accuracy=0.602656, train/loss=1.667592, validation/accuracy=0.561760, validation/loss=1.855042, validation/num_examples=50000
I0302 15:12:55.004404 140380334978816 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1996073722839355, loss=5.387237071990967
I0302 15:13:36.461260 140380326586112 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.2048624753952026, loss=2.96901535987854
I0302 15:14:21.567414 140380334978816 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.1004432439804077, loss=2.7240428924560547
I0302 15:15:07.110495 140380326586112 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.166371464729309, loss=3.311405658721924
I0302 15:15:52.200129 140380334978816 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.165513515472412, loss=2.8441858291625977
I0302 15:16:37.663656 140380326586112 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.303802490234375, loss=2.6330084800720215
I0302 15:17:22.695887 140380334978816 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.2535076141357422, loss=2.722537040710449
I0302 15:18:07.942591 140380326586112 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1157653331756592, loss=2.580021858215332
I0302 15:18:52.688185 140380334978816 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.2156059741973877, loss=2.6019811630249023
I0302 15:19:37.604624 140380326586112 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.2119446992874146, loss=2.6951358318328857
I0302 15:19:45.975414 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:19:56.552166 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:20:19.444803 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:20:21.095790 140575196817216 submission_runner.py:411] Time since start: 13596.70s, 	Step: 28520, 	{'train/accuracy': 0.6154882907867432, 'train/loss': 1.6122013330459595, 'validation/accuracy': 0.5725399851799011, 'validation/loss': 1.8285466432571411, 'validation/num_examples': 50000, 'test/accuracy': 0.4536000192165375, 'test/loss': 2.510180950164795, 'test/num_examples': 10000, 'score': 12645.979301929474, 'total_duration': 13596.697838544846, 'accumulated_submission_time': 12645.979301929474, 'accumulated_eval_time': 947.8815426826477, 'accumulated_logging_time': 1.3121697902679443}
I0302 15:20:21.115316 140380334978816 logging_writer.py:48] [28520] accumulated_eval_time=947.881543, accumulated_logging_time=1.312170, accumulated_submission_time=12645.979302, global_step=28520, preemption_count=0, score=12645.979302, test/accuracy=0.453600, test/loss=2.510181, test/num_examples=10000, total_duration=13596.697839, train/accuracy=0.615488, train/loss=1.612201, validation/accuracy=0.572540, validation/loss=1.828547, validation/num_examples=50000
I0302 15:20:53.351833 140380326586112 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.208792805671692, loss=2.748814344406128
I0302 15:21:37.456674 140380334978816 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1809030771255493, loss=2.734424352645874
I0302 15:22:22.615786 140380326586112 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.233312726020813, loss=2.6318225860595703
I0302 15:23:07.796899 140380334978816 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.114018201828003, loss=2.6875813007354736
I0302 15:23:52.821123 140380326586112 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.2054870128631592, loss=2.8019795417785645
I0302 15:24:37.724624 140380334978816 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.110916018486023, loss=2.8678791522979736
I0302 15:25:22.586018 140380326586112 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9572118520736694, loss=3.8906493186950684
I0302 15:26:07.466464 140380334978816 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.9970533847808838, loss=3.4100937843322754
I0302 15:26:52.630038 140380326586112 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9651463031768799, loss=4.618183612823486
I0302 15:27:21.559086 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:27:32.236431 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:27:55.155836 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:27:56.804335 140575196817216 submission_runner.py:411] Time since start: 14052.41s, 	Step: 29466, 	{'train/accuracy': 0.6235937476158142, 'train/loss': 1.5766786336898804, 'validation/accuracy': 0.5673199892044067, 'validation/loss': 1.8261879682540894, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.499765634536743, 'test/num_examples': 10000, 'score': 13066.364196300507, 'total_duration': 14052.40638923645, 'accumulated_submission_time': 13066.364196300507, 'accumulated_eval_time': 983.1267809867859, 'accumulated_logging_time': 1.3409326076507568}
I0302 15:27:56.823377 140380334978816 logging_writer.py:48] [29466] accumulated_eval_time=983.126781, accumulated_logging_time=1.340933, accumulated_submission_time=13066.364196, global_step=29466, preemption_count=0, score=13066.364196, test/accuracy=0.446200, test/loss=2.499766, test/num_examples=10000, total_duration=14052.406389, train/accuracy=0.623594, train/loss=1.576679, validation/accuracy=0.567320, validation/loss=1.826188, validation/num_examples=50000
I0302 15:28:10.763425 140380326586112 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1732419729232788, loss=2.7551183700561523
I0302 15:28:52.634527 140380334978816 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.0471138954162598, loss=3.7761785984039307
I0302 15:29:37.673074 140380326586112 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.2699002027511597, loss=2.641781806945801
I0302 15:30:22.970969 140380334978816 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.1431059837341309, loss=2.6345319747924805
I0302 15:31:07.818997 140380326586112 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1287999153137207, loss=2.5970988273620605
I0302 15:31:52.726840 140380334978816 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.060691475868225, loss=3.768216133117676
I0302 15:32:38.118877 140380326586112 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9206793308258057, loss=4.1795125007629395
I0302 15:33:22.862506 140380334978816 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.0434010028839111, loss=4.777623176574707
I0302 15:34:08.199643 140380326586112 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.1442254781723022, loss=2.680455207824707
I0302 15:34:53.173085 140380334978816 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.213431477546692, loss=2.4926159381866455
I0302 15:34:56.837630 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:35:07.591983 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:35:30.030006 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:35:31.696396 140575196817216 submission_runner.py:411] Time since start: 14507.30s, 	Step: 30410, 	{'train/accuracy': 0.6274023056030273, 'train/loss': 1.5770841836929321, 'validation/accuracy': 0.5665199756622314, 'validation/loss': 1.86709463596344, 'validation/num_examples': 50000, 'test/accuracy': 0.44280001521110535, 'test/loss': 2.5430333614349365, 'test/num_examples': 10000, 'score': 13486.317937612534, 'total_duration': 14507.29845237732, 'accumulated_submission_time': 13486.317937612534, 'accumulated_eval_time': 1017.9855418205261, 'accumulated_logging_time': 1.370903491973877}
I0302 15:35:31.718489 140380326586112 logging_writer.py:48] [30410] accumulated_eval_time=1017.985542, accumulated_logging_time=1.370903, accumulated_submission_time=13486.317938, global_step=30410, preemption_count=0, score=13486.317938, test/accuracy=0.442800, test/loss=2.543033, test/num_examples=10000, total_duration=14507.298452, train/accuracy=0.627402, train/loss=1.577084, validation/accuracy=0.566520, validation/loss=1.867095, validation/num_examples=50000
I0302 15:36:08.015665 140380334978816 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.2227534055709839, loss=2.578115940093994
I0302 15:36:52.950127 140380326586112 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.9843422770500183, loss=4.710114002227783
I0302 15:37:37.913113 140380334978816 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0783491134643555, loss=2.601804494857788
I0302 15:38:22.842334 140380326586112 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.1064752340316772, loss=2.6067264080047607
I0302 15:39:07.766237 140380334978816 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.9362550973892212, loss=4.925210952758789
I0302 15:39:52.803277 140380326586112 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1584302186965942, loss=2.5746145248413086
I0302 15:40:37.821599 140380334978816 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.046558141708374, loss=3.0778257846832275
I0302 15:41:22.531921 140380326586112 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9938766360282898, loss=5.11161994934082
I0302 15:42:07.702548 140380334978816 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.9469478130340576, loss=3.756180763244629
I0302 15:42:32.133237 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:42:42.762871 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:43:05.445330 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:43:07.100717 140575196817216 submission_runner.py:411] Time since start: 14962.70s, 	Step: 31356, 	{'train/accuracy': 0.6219531297683716, 'train/loss': 1.5759414434432983, 'validation/accuracy': 0.581279993057251, 'validation/loss': 1.7813587188720703, 'validation/num_examples': 50000, 'test/accuracy': 0.4594000279903412, 'test/loss': 2.448460102081299, 'test/num_examples': 10000, 'score': 13906.673711299896, 'total_duration': 14962.702766418457, 'accumulated_submission_time': 13906.673711299896, 'accumulated_eval_time': 1052.953050851822, 'accumulated_logging_time': 1.4022631645202637}
I0302 15:43:07.125264 140380326586112 logging_writer.py:48] [31356] accumulated_eval_time=1052.953051, accumulated_logging_time=1.402263, accumulated_submission_time=13906.673711, global_step=31356, preemption_count=0, score=13906.673711, test/accuracy=0.459400, test/loss=2.448460, test/num_examples=10000, total_duration=14962.702766, train/accuracy=0.621953, train/loss=1.575941, validation/accuracy=0.581280, validation/loss=1.781359, validation/num_examples=50000
I0302 15:43:25.038192 140380334978816 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1077690124511719, loss=3.2238690853118896
I0302 15:44:07.284366 140380326586112 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.974518358707428, loss=5.002713203430176
I0302 15:44:52.347292 140380334978816 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.1284818649291992, loss=5.270131587982178
I0302 15:45:37.503186 140380326586112 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1607067584991455, loss=2.4907970428466797
I0302 15:46:22.648215 140380334978816 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.9866376519203186, loss=4.148787498474121
I0302 15:47:07.822860 140380326586112 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.2243937253952026, loss=2.5679235458374023
I0302 15:47:52.964025 140380334978816 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.1120294332504272, loss=2.8853659629821777
I0302 15:48:37.903860 140380326586112 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0291950702667236, loss=5.009278297424316
I0302 15:49:22.678618 140380334978816 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.0282150506973267, loss=4.32282829284668
I0302 15:50:07.625928 140380326586112 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.164371371269226, loss=2.9376373291015625
I0302 15:50:07.637739 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:50:18.181512 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:50:41.054716 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:50:42.712074 140575196817216 submission_runner.py:411] Time since start: 15418.31s, 	Step: 32301, 	{'train/accuracy': 0.6274804472923279, 'train/loss': 1.536577582359314, 'validation/accuracy': 0.5806599855422974, 'validation/loss': 1.76521897315979, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.435957193374634, 'test/num_examples': 10000, 'score': 14327.127075195312, 'total_duration': 15418.314130306244, 'accumulated_submission_time': 14327.127075195312, 'accumulated_eval_time': 1088.0273683071136, 'accumulated_logging_time': 1.4366016387939453}
I0302 15:50:42.735160 140380334978816 logging_writer.py:48] [32301] accumulated_eval_time=1088.027368, accumulated_logging_time=1.436602, accumulated_submission_time=14327.127075, global_step=32301, preemption_count=0, score=14327.127075, test/accuracy=0.462900, test/loss=2.435957, test/num_examples=10000, total_duration=15418.314130, train/accuracy=0.627480, train/loss=1.536578, validation/accuracy=0.580660, validation/loss=1.765219, validation/num_examples=50000
I0302 15:51:22.967939 140380326586112 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.0127713680267334, loss=4.067986965179443
I0302 15:52:07.415146 140380334978816 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1966801881790161, loss=2.75181245803833
I0302 15:52:52.542551 140380326586112 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.349716305732727, loss=2.533778190612793
I0302 15:53:37.554962 140380334978816 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1375375986099243, loss=2.549854278564453
I0302 15:54:22.548137 140380326586112 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.1550465822219849, loss=2.689779758453369
I0302 15:55:07.299741 140380334978816 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1640840768814087, loss=2.6090188026428223
I0302 15:55:51.943030 140380326586112 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.1242729425430298, loss=3.540297508239746
I0302 15:56:37.155492 140380334978816 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.010533332824707, loss=4.609522819519043
I0302 15:57:22.081104 140380326586112 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.3133430480957031, loss=2.6492602825164795
I0302 15:57:42.816581 140575196817216 spec.py:321] Evaluating on the training split.
I0302 15:57:53.358481 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 15:58:14.154484 140575196817216 spec.py:349] Evaluating on the test split.
I0302 15:58:15.815105 140575196817216 submission_runner.py:411] Time since start: 15871.42s, 	Step: 33248, 	{'train/accuracy': 0.6382226347923279, 'train/loss': 1.4830291271209717, 'validation/accuracy': 0.5817999839782715, 'validation/loss': 1.7519644498825073, 'validation/num_examples': 50000, 'test/accuracy': 0.45980003476142883, 'test/loss': 2.4462242126464844, 'test/num_examples': 10000, 'score': 14747.1489007473, 'total_duration': 15871.41714978218, 'accumulated_submission_time': 14747.1489007473, 'accumulated_eval_time': 1121.0258762836456, 'accumulated_logging_time': 1.4687433242797852}
I0302 15:58:15.837596 140380334978816 logging_writer.py:48] [33248] accumulated_eval_time=1121.025876, accumulated_logging_time=1.468743, accumulated_submission_time=14747.148901, global_step=33248, preemption_count=0, score=14747.148901, test/accuracy=0.459800, test/loss=2.446224, test/num_examples=10000, total_duration=15871.417150, train/accuracy=0.638223, train/loss=1.483029, validation/accuracy=0.581800, validation/loss=1.751964, validation/num_examples=50000
I0302 15:58:36.931785 140380326586112 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1314347982406616, loss=2.560492515563965
I0302 15:59:19.714724 140380334978816 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.058029055595398, loss=3.221877098083496
I0302 16:00:04.700878 140380326586112 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.1758695840835571, loss=2.627962589263916
I0302 16:00:49.664977 140380334978816 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.0763654708862305, loss=4.929599761962891
I0302 16:01:34.251881 140380326586112 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.2253872156143188, loss=2.6703743934631348
I0302 16:02:19.545746 140380334978816 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1306495666503906, loss=2.545767068862915
I0302 16:03:04.711380 140380326586112 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2468762397766113, loss=2.570528507232666
I0302 16:03:49.764076 140380334978816 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1202001571655273, loss=2.9879353046417236
I0302 16:04:34.779218 140380326586112 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2254157066345215, loss=2.5606937408447266
I0302 16:05:15.824253 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:05:26.517054 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:05:49.014147 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:05:50.677341 140575196817216 submission_runner.py:411] Time since start: 16326.28s, 	Step: 34193, 	{'train/accuracy': 0.6274804472923279, 'train/loss': 1.5200376510620117, 'validation/accuracy': 0.5874999761581421, 'validation/loss': 1.736883521080017, 'validation/num_examples': 50000, 'test/accuracy': 0.4635000228881836, 'test/loss': 2.436673402786255, 'test/num_examples': 10000, 'score': 15167.075635671616, 'total_duration': 16326.279378652573, 'accumulated_submission_time': 15167.075635671616, 'accumulated_eval_time': 1155.8789296150208, 'accumulated_logging_time': 1.502777338027954}
I0302 16:05:50.708467 140380334978816 logging_writer.py:48] [34193] accumulated_eval_time=1155.878930, accumulated_logging_time=1.502777, accumulated_submission_time=15167.075636, global_step=34193, preemption_count=0, score=15167.075636, test/accuracy=0.463500, test/loss=2.436673, test/num_examples=10000, total_duration=16326.279379, train/accuracy=0.627480, train/loss=1.520038, validation/accuracy=0.587500, validation/loss=1.736884, validation/num_examples=50000
I0302 16:05:53.893033 140380326586112 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.1666812896728516, loss=2.4958853721618652
I0302 16:06:34.533361 140380334978816 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.1916382312774658, loss=2.6767289638519287
I0302 16:07:19.378418 140380326586112 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.2513338327407837, loss=2.7171225547790527
I0302 16:08:04.518707 140380334978816 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.1907944679260254, loss=2.563633918762207
I0302 16:08:49.344001 140380326586112 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.3269028663635254, loss=4.841179847717285
I0302 16:09:34.103540 140380334978816 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.2091470956802368, loss=2.676708221435547
I0302 16:10:19.241855 140380326586112 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.248185396194458, loss=2.3846793174743652
I0302 16:11:04.203351 140380334978816 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.2784520387649536, loss=2.6589674949645996
I0302 16:11:48.866336 140380326586112 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.1271840333938599, loss=2.8154921531677246
I0302 16:12:34.024254 140380334978816 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1007189750671387, loss=2.5409226417541504
I0302 16:12:50.770976 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:13:01.607844 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:13:23.551630 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:13:25.222117 140575196817216 submission_runner.py:411] Time since start: 16780.82s, 	Step: 35139, 	{'train/accuracy': 0.6370312571525574, 'train/loss': 1.5092811584472656, 'validation/accuracy': 0.5901399850845337, 'validation/loss': 1.7288663387298584, 'validation/num_examples': 50000, 'test/accuracy': 0.4652000367641449, 'test/loss': 2.4026684761047363, 'test/num_examples': 10000, 'score': 15587.07857298851, 'total_duration': 16780.82415175438, 'accumulated_submission_time': 15587.07857298851, 'accumulated_eval_time': 1190.3300416469574, 'accumulated_logging_time': 1.5448389053344727}
I0302 16:13:25.246816 140380326586112 logging_writer.py:48] [35139] accumulated_eval_time=1190.330042, accumulated_logging_time=1.544839, accumulated_submission_time=15587.078573, global_step=35139, preemption_count=0, score=15587.078573, test/accuracy=0.465200, test/loss=2.402668, test/num_examples=10000, total_duration=16780.824152, train/accuracy=0.637031, train/loss=1.509281, validation/accuracy=0.590140, validation/loss=1.728866, validation/num_examples=50000
I0302 16:13:49.894151 140380334978816 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.1092387437820435, loss=3.0234220027923584
I0302 16:14:33.301700 140380326586112 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0983182191848755, loss=3.319131851196289
I0302 16:15:18.172655 140380334978816 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.1826049089431763, loss=2.5367698669433594
I0302 16:16:03.086835 140380326586112 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0120948553085327, loss=3.333737850189209
I0302 16:16:47.984311 140380334978816 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.1279292106628418, loss=2.5431549549102783
I0302 16:17:32.815009 140380326586112 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.1318368911743164, loss=4.9609198570251465
I0302 16:18:17.809405 140380334978816 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1697958707809448, loss=3.1015775203704834
I0302 16:19:02.749797 140380326586112 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.273455023765564, loss=2.5008792877197266
I0302 16:19:47.568701 140380334978816 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.3824542760849, loss=2.611745595932007
I0302 16:20:25.312993 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:20:35.903264 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:20:59.114114 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:21:00.775740 140575196817216 submission_runner.py:411] Time since start: 17236.38s, 	Step: 36086, 	{'train/accuracy': 0.6399999856948853, 'train/loss': 1.4712377786636353, 'validation/accuracy': 0.5904200077056885, 'validation/loss': 1.7305066585540771, 'validation/num_examples': 50000, 'test/accuracy': 0.46820002794265747, 'test/loss': 2.4120638370513916, 'test/num_examples': 10000, 'score': 16007.0850918293, 'total_duration': 17236.377789497375, 'accumulated_submission_time': 16007.0850918293, 'accumulated_eval_time': 1225.7927763462067, 'accumulated_logging_time': 1.5797712802886963}
I0302 16:21:00.800335 140380326586112 logging_writer.py:48] [36086] accumulated_eval_time=1225.792776, accumulated_logging_time=1.579771, accumulated_submission_time=16007.085092, global_step=36086, preemption_count=0, score=16007.085092, test/accuracy=0.468200, test/loss=2.412064, test/num_examples=10000, total_duration=17236.377789, train/accuracy=0.640000, train/loss=1.471238, validation/accuracy=0.590420, validation/loss=1.730507, validation/num_examples=50000
I0302 16:21:06.774972 140380334978816 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.9945327639579773, loss=4.078174591064453
I0302 16:21:47.621578 140380326586112 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1864842176437378, loss=2.5826573371887207
I0302 16:22:32.337761 140380334978816 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1444932222366333, loss=2.4265475273132324
I0302 16:23:17.779102 140380326586112 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.0106571912765503, loss=5.147316932678223
I0302 16:24:03.277323 140380334978816 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1780400276184082, loss=2.470813512802124
I0302 16:24:48.293481 140380326586112 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.220170021057129, loss=2.6863596439361572
I0302 16:25:33.187056 140380334978816 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.090097188949585, loss=2.9219367504119873
I0302 16:26:18.452543 140380326586112 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1601639986038208, loss=4.012141227722168
I0302 16:27:03.473450 140380334978816 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.2970410585403442, loss=2.591495990753174
I0302 16:27:48.424778 140380326586112 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0876386165618896, loss=4.006054401397705
I0302 16:28:01.184105 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:28:11.772875 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:28:34.573143 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:28:36.225067 140575196817216 submission_runner.py:411] Time since start: 17691.83s, 	Step: 37030, 	{'train/accuracy': 0.6620116829872131, 'train/loss': 1.3794057369232178, 'validation/accuracy': 0.592519998550415, 'validation/loss': 1.7107194662094116, 'validation/num_examples': 50000, 'test/accuracy': 0.4718000292778015, 'test/loss': 2.398625612258911, 'test/num_examples': 10000, 'score': 16427.407952070236, 'total_duration': 17691.82711672783, 'accumulated_submission_time': 16427.407952070236, 'accumulated_eval_time': 1260.8337228298187, 'accumulated_logging_time': 1.616624116897583}
I0302 16:28:36.248112 140380334978816 logging_writer.py:48] [37030] accumulated_eval_time=1260.833723, accumulated_logging_time=1.616624, accumulated_submission_time=16427.407952, global_step=37030, preemption_count=0, score=16427.407952, test/accuracy=0.471800, test/loss=2.398626, test/num_examples=10000, total_duration=17691.827117, train/accuracy=0.662012, train/loss=1.379406, validation/accuracy=0.592520, validation/loss=1.710719, validation/num_examples=50000
I0302 16:29:04.506581 140380326586112 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.1151641607284546, loss=2.5998120307922363
I0302 16:29:48.530024 140380334978816 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.2081199884414673, loss=4.782210350036621
I0302 16:30:34.078975 140380326586112 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.0370001792907715, loss=2.812225818634033
I0302 16:31:18.925458 140380334978816 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.9590251445770264, loss=4.101744651794434
I0302 16:32:03.426872 140380326586112 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.196675181388855, loss=2.8011302947998047
I0302 16:32:48.919484 140380334978816 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.2338337898254395, loss=2.665105104446411
I0302 16:33:33.903028 140380326586112 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.0814588069915771, loss=5.1131591796875
I0302 16:34:18.997531 140380334978816 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.280544400215149, loss=2.59063982963562
I0302 16:35:03.963483 140380326586112 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1804500818252563, loss=3.152355432510376
I0302 16:35:36.641291 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:35:47.392333 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:36:10.142869 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:36:11.802242 140575196817216 submission_runner.py:411] Time since start: 18147.40s, 	Step: 37975, 	{'train/accuracy': 0.6415038704872131, 'train/loss': 1.4643170833587646, 'validation/accuracy': 0.5976799726486206, 'validation/loss': 1.683479905128479, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.3753652572631836, 'test/num_examples': 10000, 'score': 16847.74187517166, 'total_duration': 18147.40429544449, 'accumulated_submission_time': 16847.74187517166, 'accumulated_eval_time': 1295.9946694374084, 'accumulated_logging_time': 1.6496033668518066}
I0302 16:36:11.828295 140380334978816 logging_writer.py:48] [37975] accumulated_eval_time=1295.994669, accumulated_logging_time=1.649603, accumulated_submission_time=16847.741875, global_step=37975, preemption_count=0, score=16847.741875, test/accuracy=0.476300, test/loss=2.375365, test/num_examples=10000, total_duration=18147.404295, train/accuracy=0.641504, train/loss=1.464317, validation/accuracy=0.597680, validation/loss=1.683480, validation/num_examples=50000
I0302 16:36:22.174637 140380326586112 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.032671570777893, loss=3.045433521270752
I0302 16:37:03.746041 140380334978816 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.2194753885269165, loss=2.577742576599121
I0302 16:37:48.376305 140380326586112 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.2307519912719727, loss=2.634812593460083
I0302 16:38:33.634829 140380334978816 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.1450178623199463, loss=2.5230813026428223
I0302 16:39:18.477494 140380326586112 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.1725555658340454, loss=2.5320816040039062
I0302 16:40:03.330394 140380334978816 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.0296061038970947, loss=3.941514492034912
I0302 16:40:48.189665 140380326586112 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.314652681350708, loss=2.5381481647491455
I0302 16:41:33.038784 140380334978816 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.266895055770874, loss=2.640030860900879
I0302 16:42:18.106328 140380326586112 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.09196138381958, loss=5.2997307777404785
I0302 16:43:03.092137 140380334978816 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2112704515457153, loss=2.5850095748901367
I0302 16:43:12.102777 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:43:22.717089 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:43:44.609823 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:43:46.258922 140575196817216 submission_runner.py:411] Time since start: 18601.86s, 	Step: 38922, 	{'train/accuracy': 0.6445702910423279, 'train/loss': 1.4891875982284546, 'validation/accuracy': 0.5958200097084045, 'validation/loss': 1.7240655422210693, 'validation/num_examples': 50000, 'test/accuracy': 0.47370001673698425, 'test/loss': 2.377290725708008, 'test/num_examples': 10000, 'score': 17267.9571518898, 'total_duration': 18601.86097741127, 'accumulated_submission_time': 17267.9571518898, 'accumulated_eval_time': 1330.1508178710938, 'accumulated_logging_time': 1.68623948097229}
I0302 16:43:46.282950 140380326586112 logging_writer.py:48] [38922] accumulated_eval_time=1330.150818, accumulated_logging_time=1.686239, accumulated_submission_time=17267.957152, global_step=38922, preemption_count=0, score=17267.957152, test/accuracy=0.473700, test/loss=2.377291, test/num_examples=10000, total_duration=18601.860977, train/accuracy=0.644570, train/loss=1.489188, validation/accuracy=0.595820, validation/loss=1.724066, validation/num_examples=50000
I0302 16:44:17.703383 140380334978816 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3427855968475342, loss=2.517732858657837
I0302 16:45:01.902099 140380326586112 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.045702338218689, loss=3.5520036220550537
I0302 16:45:47.322926 140380334978816 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.0044053792953491, loss=3.6856186389923096
I0302 16:46:32.245102 140380326586112 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.3048045635223389, loss=2.5182197093963623
I0302 16:47:16.922793 140380334978816 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.2089934349060059, loss=2.457125663757324
I0302 16:48:01.570693 140380326586112 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.9835786819458008, loss=3.8829073905944824
I0302 16:48:46.560622 140380334978816 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0898388624191284, loss=3.479741096496582
I0302 16:49:31.393551 140380326586112 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.3156639337539673, loss=3.0004124641418457
I0302 16:50:16.378567 140380334978816 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.2007627487182617, loss=2.3792507648468018
I0302 16:50:46.469543 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:50:57.151328 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:51:19.457628 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:51:21.105224 140575196817216 submission_runner.py:411] Time since start: 19056.71s, 	Step: 39869, 	{'train/accuracy': 0.6550585627555847, 'train/loss': 1.4371448755264282, 'validation/accuracy': 0.5964199900627136, 'validation/loss': 1.7216540575027466, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.372420072555542, 'test/num_examples': 10000, 'score': 17688.084655046463, 'total_duration': 19056.70727801323, 'accumulated_submission_time': 17688.084655046463, 'accumulated_eval_time': 1364.786494731903, 'accumulated_logging_time': 1.719895601272583}
I0302 16:51:21.126554 140380326586112 logging_writer.py:48] [39869] accumulated_eval_time=1364.786495, accumulated_logging_time=1.719896, accumulated_submission_time=17688.084655, global_step=39869, preemption_count=0, score=17688.084655, test/accuracy=0.475100, test/loss=2.372420, test/num_examples=10000, total_duration=19056.707278, train/accuracy=0.655059, train/loss=1.437145, validation/accuracy=0.596420, validation/loss=1.721654, validation/num_examples=50000
I0302 16:51:33.867827 140380334978816 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.2824270725250244, loss=2.5478439331054688
I0302 16:52:15.448567 140380326586112 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.2197319269180298, loss=2.4750001430511475
I0302 16:53:00.767390 140380334978816 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.2674891948699951, loss=2.6249091625213623
I0302 16:53:45.904202 140380326586112 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.1555836200714111, loss=2.802363634109497
I0302 16:54:31.003211 140380334978816 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.2659059762954712, loss=2.539503574371338
I0302 16:55:15.910411 140380326586112 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0337005853652954, loss=4.277535438537598
I0302 16:56:00.846562 140380334978816 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.2394334077835083, loss=2.4550349712371826
I0302 16:56:45.900016 140380326586112 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.198485016822815, loss=2.500161647796631
I0302 16:57:30.869156 140380334978816 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1383970975875854, loss=2.402198314666748
I0302 16:58:15.767899 140380326586112 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.0095877647399902, loss=3.6620428562164307
I0302 16:58:21.296534 140575196817216 spec.py:321] Evaluating on the training split.
I0302 16:58:32.021209 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 16:58:52.912016 140575196817216 spec.py:349] Evaluating on the test split.
I0302 16:58:54.572710 140575196817216 submission_runner.py:411] Time since start: 19510.17s, 	Step: 40814, 	{'train/accuracy': 0.6373828053474426, 'train/loss': 1.492069959640503, 'validation/accuracy': 0.5970399975776672, 'validation/loss': 1.6841896772384644, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.3751423358917236, 'test/num_examples': 10000, 'score': 18108.195221424103, 'total_duration': 19510.174768209457, 'accumulated_submission_time': 18108.195221424103, 'accumulated_eval_time': 1398.0626657009125, 'accumulated_logging_time': 1.7514064311981201}
I0302 16:58:54.597283 140380334978816 logging_writer.py:48] [40814] accumulated_eval_time=1398.062666, accumulated_logging_time=1.751406, accumulated_submission_time=18108.195221, global_step=40814, preemption_count=0, score=18108.195221, test/accuracy=0.470800, test/loss=2.375142, test/num_examples=10000, total_duration=19510.174768, train/accuracy=0.637383, train/loss=1.492070, validation/accuracy=0.597040, validation/loss=1.684190, validation/num_examples=50000
I0302 16:59:29.397449 140380326586112 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.1044613122940063, loss=3.309589385986328
I0302 17:00:14.039558 140380334978816 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.1546869277954102, loss=2.9371535778045654
I0302 17:00:59.294657 140380326586112 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1509374380111694, loss=4.654409885406494
I0302 17:01:44.343317 140380334978816 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.1980944871902466, loss=2.3960514068603516
I0302 17:02:29.274425 140380326586112 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.9959208369255066, loss=3.9499189853668213
I0302 17:03:14.478757 140380334978816 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.193576455116272, loss=2.5192246437072754
I0302 17:03:59.548302 140380326586112 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.023717999458313, loss=4.286364555358887
I0302 17:04:44.464169 140380334978816 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1874920129776, loss=2.5485074520111084
I0302 17:05:29.528509 140380326586112 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.18586266040802, loss=3.0692636966705322
I0302 17:05:54.836499 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:06:05.650454 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:06:27.626755 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:06:29.295666 140575196817216 submission_runner.py:411] Time since start: 19964.90s, 	Step: 41758, 	{'train/accuracy': 0.6490429639816284, 'train/loss': 1.4463257789611816, 'validation/accuracy': 0.5996599793434143, 'validation/loss': 1.6779924631118774, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.345698833465576, 'test/num_examples': 10000, 'score': 18528.375126600266, 'total_duration': 19964.897725105286, 'accumulated_submission_time': 18528.375126600266, 'accumulated_eval_time': 1432.521831035614, 'accumulated_logging_time': 1.785491704940796}
I0302 17:06:29.320323 140380334978816 logging_writer.py:48] [41758] accumulated_eval_time=1432.521831, accumulated_logging_time=1.785492, accumulated_submission_time=18528.375127, global_step=41758, preemption_count=0, score=18528.375127, test/accuracy=0.480400, test/loss=2.345699, test/num_examples=10000, total_duration=19964.897725, train/accuracy=0.649043, train/loss=1.446326, validation/accuracy=0.599660, validation/loss=1.677992, validation/num_examples=50000
I0302 17:06:46.431586 140380326586112 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.0048257112503052, loss=5.073668003082275
I0302 17:07:28.736793 140380334978816 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.0179520845413208, loss=4.946970462799072
I0302 17:08:13.886844 140380326586112 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.2505323886871338, loss=2.5062689781188965
I0302 17:08:58.755031 140380334978816 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0241601467132568, loss=3.677769660949707
I0302 17:09:43.856012 140380326586112 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.2116492986679077, loss=2.4534435272216797
I0302 17:10:28.763080 140380334978816 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.2588037252426147, loss=3.8861165046691895
I0302 17:11:13.682609 140380326586112 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.0383237600326538, loss=5.073443412780762
I0302 17:11:58.076482 140380334978816 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.2499638795852661, loss=2.5322089195251465
I0302 17:12:43.283603 140380326586112 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.2224222421646118, loss=2.8047075271606445
I0302 17:13:28.267112 140380334978816 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.255147933959961, loss=2.4338414669036865
I0302 17:13:29.313977 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:13:39.855784 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:14:02.864957 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:14:04.523117 140575196817216 submission_runner.py:411] Time since start: 20420.13s, 	Step: 42704, 	{'train/accuracy': 0.6508007645606995, 'train/loss': 1.487926721572876, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.734683871269226, 'validation/num_examples': 50000, 'test/accuracy': 0.4749000370502472, 'test/loss': 2.391242742538452, 'test/num_examples': 10000, 'score': 18948.309423685074, 'total_duration': 20420.125157356262, 'accumulated_submission_time': 18948.309423685074, 'accumulated_eval_time': 1467.7309651374817, 'accumulated_logging_time': 1.8197968006134033}
I0302 17:14:04.548949 140380326586112 logging_writer.py:48] [42704] accumulated_eval_time=1467.730965, accumulated_logging_time=1.819797, accumulated_submission_time=18948.309424, global_step=42704, preemption_count=0, score=18948.309424, test/accuracy=0.474900, test/loss=2.391243, test/num_examples=10000, total_duration=20420.125157, train/accuracy=0.650801, train/loss=1.487927, validation/accuracy=0.597440, validation/loss=1.734684, validation/num_examples=50000
I0302 17:14:43.142201 140380334978816 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5763700008392334, loss=5.279995918273926
I0302 17:15:27.803877 140380326586112 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.208265781402588, loss=2.360727310180664
I0302 17:16:13.032159 140380334978816 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9591841101646423, loss=4.7133049964904785
I0302 17:16:57.941174 140380326586112 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.194236397743225, loss=2.3527069091796875
I0302 17:17:42.996611 140380334978816 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.288153052330017, loss=2.4317522048950195
I0302 17:18:27.997184 140380326586112 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.4293161630630493, loss=2.5008864402770996
I0302 17:19:12.976396 140380334978816 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.3250049352645874, loss=2.5397486686706543
I0302 17:19:57.936379 140380326586112 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.3180738687515259, loss=2.6978111267089844
I0302 17:20:42.816103 140380334978816 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2245242595672607, loss=2.4993386268615723
I0302 17:21:04.687969 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:21:15.212275 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:21:38.916553 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:21:40.571612 140575196817216 submission_runner.py:411] Time since start: 20876.17s, 	Step: 43650, 	{'train/accuracy': 0.684277355670929, 'train/loss': 1.2981736660003662, 'validation/accuracy': 0.6021400094032288, 'validation/loss': 1.6690380573272705, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.350553274154663, 'test/num_examples': 10000, 'score': 19368.389864444733, 'total_duration': 20876.173667669296, 'accumulated_submission_time': 19368.389864444733, 'accumulated_eval_time': 1503.6146020889282, 'accumulated_logging_time': 1.8551933765411377}
I0302 17:21:40.593198 140380326586112 logging_writer.py:48] [43650] accumulated_eval_time=1503.614602, accumulated_logging_time=1.855193, accumulated_submission_time=19368.389864, global_step=43650, preemption_count=0, score=19368.389864, test/accuracy=0.478600, test/loss=2.350553, test/num_examples=10000, total_duration=20876.173668, train/accuracy=0.684277, train/loss=1.298174, validation/accuracy=0.602140, validation/loss=1.669038, validation/num_examples=50000
I0302 17:22:00.893945 140380334978816 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1106586456298828, loss=2.3971338272094727
I0302 17:22:43.523640 140380326586112 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1954166889190674, loss=2.430954933166504
I0302 17:23:28.238262 140380334978816 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.346030354499817, loss=2.8051350116729736
I0302 17:24:13.203020 140380326586112 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.2412327527999878, loss=2.4466543197631836
I0302 17:24:58.005412 140380334978816 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.156968116760254, loss=3.990565299987793
I0302 17:25:42.932508 140380326586112 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.3819702863693237, loss=2.4895973205566406
I0302 17:26:28.083613 140380334978816 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.0685633420944214, loss=4.911519527435303
I0302 17:27:12.955465 140380326586112 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1751837730407715, loss=3.0016000270843506
I0302 17:27:57.784261 140380334978816 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7045406103134155, loss=5.262194633483887
I0302 17:28:40.977721 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:28:51.507363 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:29:13.807191 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:29:15.460289 140575196817216 submission_runner.py:411] Time since start: 21331.06s, 	Step: 44598, 	{'train/accuracy': 0.6547460556030273, 'train/loss': 1.4286702871322632, 'validation/accuracy': 0.6058399677276611, 'validation/loss': 1.6572929620742798, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.3217082023620605, 'test/num_examples': 10000, 'score': 19788.714443922043, 'total_duration': 21331.062341213226, 'accumulated_submission_time': 19788.714443922043, 'accumulated_eval_time': 1538.0971751213074, 'accumulated_logging_time': 1.8865134716033936}
I0302 17:29:15.485857 140380326586112 logging_writer.py:48] [44598] accumulated_eval_time=1538.097175, accumulated_logging_time=1.886513, accumulated_submission_time=19788.714444, global_step=44598, preemption_count=0, score=19788.714444, test/accuracy=0.482300, test/loss=2.321708, test/num_examples=10000, total_duration=21331.062341, train/accuracy=0.654746, train/loss=1.428670, validation/accuracy=0.605840, validation/loss=1.657293, validation/num_examples=50000
I0302 17:29:16.685472 140380334978816 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2854479551315308, loss=2.41282057762146
I0302 17:29:56.893569 140380326586112 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.2644718885421753, loss=4.8309526443481445
I0302 17:30:41.805946 140380334978816 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.3102405071258545, loss=2.7144248485565186
I0302 17:31:27.120719 140380326586112 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.2217313051223755, loss=2.558382749557495
I0302 17:32:12.236066 140380334978816 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.256101369857788, loss=2.4853515625
I0302 17:32:57.504245 140380326586112 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.4044193029403687, loss=2.4187464714050293
I0302 17:33:42.691113 140380334978816 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.215717077255249, loss=2.462747812271118
I0302 17:34:27.655903 140380326586112 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.211978554725647, loss=2.6113176345825195
I0302 17:35:13.012219 140380334978816 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.2639310359954834, loss=2.4410462379455566
I0302 17:35:58.137818 140380326586112 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.113531470298767, loss=3.340137481689453
I0302 17:36:15.652930 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:36:26.344021 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:36:48.016250 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:36:49.681331 140575196817216 submission_runner.py:411] Time since start: 21785.28s, 	Step: 45540, 	{'train/accuracy': 0.660351574420929, 'train/loss': 1.3766939640045166, 'validation/accuracy': 0.6067799925804138, 'validation/loss': 1.6368731260299683, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.2972004413604736, 'test/num_examples': 10000, 'score': 20208.822594165802, 'total_duration': 21785.28338742256, 'accumulated_submission_time': 20208.822594165802, 'accumulated_eval_time': 1572.1255660057068, 'accumulated_logging_time': 1.9221677780151367}
I0302 17:36:49.703311 140380334978816 logging_writer.py:48] [45540] accumulated_eval_time=1572.125566, accumulated_logging_time=1.922168, accumulated_submission_time=20208.822594, global_step=45540, preemption_count=0, score=20208.822594, test/accuracy=0.483400, test/loss=2.297200, test/num_examples=10000, total_duration=21785.283387, train/accuracy=0.660352, train/loss=1.376694, validation/accuracy=0.606780, validation/loss=1.636873, validation/num_examples=50000
I0302 17:37:13.964226 140380326586112 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.1385724544525146, loss=2.2905898094177246
I0302 17:37:57.562643 140380334978816 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.001657485961914, loss=4.9439921379089355
I0302 17:38:42.738131 140380326586112 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.2956100702285767, loss=2.4106953144073486
I0302 17:39:27.897771 140380334978816 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.0603933334350586, loss=3.6077589988708496
I0302 17:40:13.051528 140380326586112 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1277891397476196, loss=2.734884262084961
I0302 17:40:58.113637 140380334978816 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2294440269470215, loss=2.3299074172973633
I0302 17:41:43.076129 140380326586112 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.2574617862701416, loss=2.4354324340820312
I0302 17:42:28.250423 140380334978816 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.2121251821517944, loss=2.4833879470825195
I0302 17:43:13.297718 140380326586112 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.0461829900741577, loss=4.692722320556641
I0302 17:43:49.900924 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:44:00.516157 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:44:22.926414 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:44:24.587084 140575196817216 submission_runner.py:411] Time since start: 22240.19s, 	Step: 46483, 	{'train/accuracy': 0.6699609160423279, 'train/loss': 1.3838269710540771, 'validation/accuracy': 0.6090999841690063, 'validation/loss': 1.6606667041778564, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.329615592956543, 'test/num_examples': 10000, 'score': 20628.961042165756, 'total_duration': 22240.189120054245, 'accumulated_submission_time': 20628.961042165756, 'accumulated_eval_time': 1606.8116953372955, 'accumulated_logging_time': 1.9543015956878662}
I0302 17:44:24.617078 140380334978816 logging_writer.py:48] [46483] accumulated_eval_time=1606.811695, accumulated_logging_time=1.954302, accumulated_submission_time=20628.961042, global_step=46483, preemption_count=0, score=20628.961042, test/accuracy=0.481600, test/loss=2.329616, test/num_examples=10000, total_duration=22240.189120, train/accuracy=0.669961, train/loss=1.383827, validation/accuracy=0.609100, validation/loss=1.660667, validation/num_examples=50000
I0302 17:44:31.785330 140380326586112 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.2690716981887817, loss=2.381743907928467
I0302 17:45:12.568034 140380334978816 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1556662321090698, loss=2.630903482437134
I0302 17:45:57.552982 140380326586112 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.3073222637176514, loss=2.4341135025024414
I0302 17:46:43.196953 140380334978816 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1523336172103882, loss=2.8250956535339355
I0302 17:47:28.266569 140380326586112 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.3411190509796143, loss=2.3936872482299805
I0302 17:48:13.309187 140380334978816 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.4729268550872803, loss=5.043161869049072
I0302 17:48:58.059732 140380326586112 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.324662685394287, loss=2.4440722465515137
I0302 17:49:43.269434 140380334978816 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.2534250020980835, loss=2.504995107650757
I0302 17:50:28.499460 140380326586112 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.5105829238891602, loss=5.096036434173584
I0302 17:51:13.528259 140380334978816 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.2432115077972412, loss=2.352388858795166
I0302 17:51:24.771633 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:51:35.218959 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:51:58.912448 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:52:00.576619 140575196817216 submission_runner.py:411] Time since start: 22696.18s, 	Step: 47427, 	{'train/accuracy': 0.6600781083106995, 'train/loss': 1.3950390815734863, 'validation/accuracy': 0.6157999634742737, 'validation/loss': 1.612318754196167, 'validation/num_examples': 50000, 'test/accuracy': 0.4983000159263611, 'test/loss': 2.286940097808838, 'test/num_examples': 10000, 'score': 21049.05564045906, 'total_duration': 22696.17867732048, 'accumulated_submission_time': 21049.05564045906, 'accumulated_eval_time': 1642.6166734695435, 'accumulated_logging_time': 1.9951858520507812}
I0302 17:52:00.604671 140380326586112 logging_writer.py:48] [47427] accumulated_eval_time=1642.616673, accumulated_logging_time=1.995186, accumulated_submission_time=21049.055640, global_step=47427, preemption_count=0, score=21049.055640, test/accuracy=0.498300, test/loss=2.286940, test/num_examples=10000, total_duration=22696.178677, train/accuracy=0.660078, train/loss=1.395039, validation/accuracy=0.615800, validation/loss=1.612319, validation/num_examples=50000
I0302 17:52:30.024985 140380334978816 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.075292706489563, loss=3.219031572341919
I0302 17:53:13.710784 140380326586112 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.0694994926452637, loss=3.7035229206085205
I0302 17:53:58.398141 140380334978816 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.211093544960022, loss=2.82502818107605
I0302 17:54:43.569886 140380326586112 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1444891691207886, loss=2.5160155296325684
I0302 17:55:28.786412 140380334978816 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.3358709812164307, loss=2.446124792098999
I0302 17:56:13.948707 140380326586112 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0037881135940552, loss=4.710058689117432
I0302 17:56:58.892444 140380334978816 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.0735784769058228, loss=3.4067306518554688
I0302 17:57:43.824531 140380326586112 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0489411354064941, loss=3.7467474937438965
I0302 17:58:28.748836 140380334978816 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0181801319122314, loss=3.64980149269104
I0302 17:59:00.670167 140575196817216 spec.py:321] Evaluating on the training split.
I0302 17:59:11.451261 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 17:59:35.438771 140575196817216 spec.py:349] Evaluating on the test split.
I0302 17:59:37.103745 140575196817216 submission_runner.py:411] Time since start: 23152.71s, 	Step: 48373, 	{'train/accuracy': 0.6630077958106995, 'train/loss': 1.3899929523468018, 'validation/accuracy': 0.6116600036621094, 'validation/loss': 1.618957281112671, 'validation/num_examples': 50000, 'test/accuracy': 0.4912000298500061, 'test/loss': 2.2766523361206055, 'test/num_examples': 10000, 'score': 21469.06161904335, 'total_duration': 23152.705780267715, 'accumulated_submission_time': 21469.06161904335, 'accumulated_eval_time': 1679.0502254962921, 'accumulated_logging_time': 2.0333759784698486}
I0302 17:59:37.132402 140380326586112 logging_writer.py:48] [48373] accumulated_eval_time=1679.050225, accumulated_logging_time=2.033376, accumulated_submission_time=21469.061619, global_step=48373, preemption_count=0, score=21469.061619, test/accuracy=0.491200, test/loss=2.276652, test/num_examples=10000, total_duration=23152.705780, train/accuracy=0.663008, train/loss=1.389993, validation/accuracy=0.611660, validation/loss=1.618957, validation/num_examples=50000
I0302 17:59:48.283631 140380334978816 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.181086778640747, loss=2.5131092071533203
I0302 18:00:29.489570 140380326586112 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.325120449066162, loss=2.220573902130127
I0302 18:01:14.636528 140380334978816 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.2093982696533203, loss=2.4902498722076416
I0302 18:01:59.936035 140380326586112 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.2868280410766602, loss=2.332409620285034
I0302 18:02:45.180297 140380334978816 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.9953921437263489, loss=4.176358222961426
I0302 18:03:30.284313 140380326586112 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0445399284362793, loss=4.218763828277588
I0302 18:04:15.495275 140380334978816 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.2162266969680786, loss=2.689358949661255
I0302 18:05:00.330837 140380326586112 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.2550758123397827, loss=2.3826746940612793
I0302 18:05:45.294086 140380334978816 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.19895339012146, loss=2.895768165588379
I0302 18:06:30.420793 140380326586112 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.4034770727157593, loss=2.4259495735168457
I0302 18:06:37.393271 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:06:48.206001 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:07:10.853963 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:07:12.510693 140575196817216 submission_runner.py:411] Time since start: 23608.11s, 	Step: 49317, 	{'train/accuracy': 0.6714062094688416, 'train/loss': 1.346708059310913, 'validation/accuracy': 0.617680013179779, 'validation/loss': 1.5923179388046265, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.2634685039520264, 'test/num_examples': 10000, 'score': 21889.25936102867, 'total_duration': 23608.112749814987, 'accumulated_submission_time': 21889.25936102867, 'accumulated_eval_time': 1714.1676445007324, 'accumulated_logging_time': 2.0756874084472656}
I0302 18:07:12.532732 140380334978816 logging_writer.py:48] [49317] accumulated_eval_time=1714.167645, accumulated_logging_time=2.075687, accumulated_submission_time=21889.259361, global_step=49317, preemption_count=0, score=21889.259361, test/accuracy=0.490500, test/loss=2.263469, test/num_examples=10000, total_duration=23608.112750, train/accuracy=0.671406, train/loss=1.346708, validation/accuracy=0.617680, validation/loss=1.592318, validation/num_examples=50000
I0302 18:07:46.109373 140380326586112 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.979451596736908, loss=3.7782576084136963
I0302 18:08:30.273874 140380334978816 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1884050369262695, loss=2.4948904514312744
I0302 18:09:15.478735 140380326586112 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.0286293029785156, loss=3.876314163208008
I0302 18:10:00.265401 140380334978816 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.072370171546936, loss=3.678457021713257
I0302 18:10:45.409898 140380326586112 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1930760145187378, loss=3.5825014114379883
I0302 18:11:30.123584 140380334978816 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2086849212646484, loss=2.6545028686523438
I0302 18:12:14.930480 140380326586112 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1760798692703247, loss=4.402931213378906
I0302 18:13:00.358445 140380334978816 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.0431121587753296, loss=3.4686474800109863
I0302 18:13:45.301903 140380326586112 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.1546870470046997, loss=3.5757901668548584
I0302 18:14:12.860832 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:14:23.621765 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:14:46.614901 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:14:48.262924 140575196817216 submission_runner.py:411] Time since start: 24063.86s, 	Step: 50263, 	{'train/accuracy': 0.696582019329071, 'train/loss': 1.2489502429962158, 'validation/accuracy': 0.6180999875068665, 'validation/loss': 1.5910358428955078, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.265782594680786, 'test/num_examples': 10000, 'score': 22309.52768969536, 'total_duration': 24063.864980220795, 'accumulated_submission_time': 22309.52768969536, 'accumulated_eval_time': 1749.5697557926178, 'accumulated_logging_time': 2.1086506843566895}
I0302 18:14:48.287181 140380334978816 logging_writer.py:48] [50263] accumulated_eval_time=1749.569756, accumulated_logging_time=2.108651, accumulated_submission_time=22309.527690, global_step=50263, preemption_count=0, score=22309.527690, test/accuracy=0.495800, test/loss=2.265783, test/num_examples=10000, total_duration=24063.864980, train/accuracy=0.696582, train/loss=1.248950, validation/accuracy=0.618100, validation/loss=1.591036, validation/num_examples=50000
I0302 18:15:03.425937 140380326586112 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.0495953559875488, loss=3.081568717956543
I0302 18:15:45.594702 140380334978816 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.166016936302185, loss=2.477205514907837
I0302 18:16:30.935347 140380326586112 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.2326931953430176, loss=2.447540283203125
I0302 18:17:16.227656 140380334978816 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.2791818380355835, loss=2.4830641746520996
I0302 18:18:01.078423 140380326586112 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.2646825313568115, loss=2.276827335357666
I0302 18:18:46.245911 140380334978816 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.3687489032745361, loss=2.295955181121826
I0302 18:19:31.150359 140380326586112 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.3146697282791138, loss=2.575453519821167
I0302 18:20:16.370070 140380334978816 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1031839847564697, loss=3.281890630722046
I0302 18:21:01.154811 140380326586112 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.315629243850708, loss=2.3214895725250244
I0302 18:21:45.856331 140380334978816 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.9723650217056274, loss=4.325509071350098
I0302 18:21:48.617396 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:21:59.307685 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:22:22.844549 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:22:24.502706 140575196817216 submission_runner.py:411] Time since start: 24520.10s, 	Step: 51208, 	{'train/accuracy': 0.6667382717132568, 'train/loss': 1.3661245107650757, 'validation/accuracy': 0.6186599731445312, 'validation/loss': 1.5887244939804077, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.2476613521575928, 'test/num_examples': 10000, 'score': 22729.79816842079, 'total_duration': 24520.104761600494, 'accumulated_submission_time': 22729.79816842079, 'accumulated_eval_time': 1785.455048084259, 'accumulated_logging_time': 2.143319845199585}
I0302 18:22:24.526576 140380326586112 logging_writer.py:48] [51208] accumulated_eval_time=1785.455048, accumulated_logging_time=2.143320, accumulated_submission_time=22729.798168, global_step=51208, preemption_count=0, score=22729.798168, test/accuracy=0.498100, test/loss=2.247661, test/num_examples=10000, total_duration=24520.104762, train/accuracy=0.666738, train/loss=1.366125, validation/accuracy=0.618660, validation/loss=1.588724, validation/num_examples=50000
I0302 18:23:01.659373 140380334978816 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2458324432373047, loss=2.7613656520843506
I0302 18:23:46.541130 140380326586112 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.1698038578033447, loss=3.4156112670898438
I0302 18:24:31.601242 140380334978816 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2595270872116089, loss=2.483682155609131
I0302 18:25:16.472091 140380326586112 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.3154393434524536, loss=2.8043198585510254
I0302 18:26:01.387655 140380334978816 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.2021135091781616, loss=2.445071220397949
I0302 18:26:46.407027 140380326586112 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.1923868656158447, loss=3.175577402114868
I0302 18:27:31.564836 140380334978816 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.0262476205825806, loss=4.6870503425598145
I0302 18:28:16.474251 140380326586112 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.1217188835144043, loss=4.513057708740234
I0302 18:29:01.261562 140380334978816 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.25395667552948, loss=2.711135149002075
I0302 18:29:24.892460 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:29:35.681908 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:29:59.243142 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:30:00.892663 140575196817216 submission_runner.py:411] Time since start: 24976.49s, 	Step: 52154, 	{'train/accuracy': 0.6708202958106995, 'train/loss': 1.3567956686019897, 'validation/accuracy': 0.6210799813270569, 'validation/loss': 1.5886139869689941, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.2570228576660156, 'test/num_examples': 10000, 'score': 23150.10472869873, 'total_duration': 24976.49471473694, 'accumulated_submission_time': 23150.10472869873, 'accumulated_eval_time': 1821.4552278518677, 'accumulated_logging_time': 2.177870750427246}
I0302 18:30:00.915522 140380326586112 logging_writer.py:48] [52154] accumulated_eval_time=1821.455228, accumulated_logging_time=2.177871, accumulated_submission_time=23150.104729, global_step=52154, preemption_count=0, score=23150.104729, test/accuracy=0.494600, test/loss=2.257023, test/num_examples=10000, total_duration=24976.494715, train/accuracy=0.670820, train/loss=1.356796, validation/accuracy=0.621080, validation/loss=1.588614, validation/num_examples=50000
I0302 18:30:19.614952 140380334978816 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.2256590127944946, loss=2.343620777130127
I0302 18:31:01.959481 140380326586112 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.3463443517684937, loss=2.3430421352386475
I0302 18:31:47.045072 140380334978816 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.0457696914672852, loss=3.285733222961426
I0302 18:32:31.798874 140380326586112 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.120076298713684, loss=2.8590009212493896
I0302 18:33:17.252470 140380334978816 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.249572992324829, loss=2.3070428371429443
I0302 18:34:02.383033 140380326586112 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.0739338397979736, loss=4.8199286460876465
I0302 18:34:47.273961 140380334978816 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.362216591835022, loss=4.753701686859131
I0302 18:35:32.189910 140380326586112 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1451817750930786, loss=3.193796396255493
I0302 18:36:17.076634 140380334978816 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.2755463123321533, loss=2.3841075897216797
I0302 18:37:01.304135 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:37:12.108356 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:37:35.204125 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:37:36.865050 140575196817216 submission_runner.py:411] Time since start: 25432.47s, 	Step: 53100, 	{'train/accuracy': 0.6807812452316284, 'train/loss': 1.2781988382339478, 'validation/accuracy': 0.6184599995613098, 'validation/loss': 1.5739054679870605, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.253964424133301, 'test/num_examples': 10000, 'score': 23570.434467315674, 'total_duration': 25432.46706700325, 'accumulated_submission_time': 23570.434467315674, 'accumulated_eval_time': 1857.0161018371582, 'accumulated_logging_time': 2.2102932929992676}
I0302 18:37:36.896766 140380326586112 logging_writer.py:48] [53100] accumulated_eval_time=1857.016102, accumulated_logging_time=2.210293, accumulated_submission_time=23570.434467, global_step=53100, preemption_count=0, score=23570.434467, test/accuracy=0.494600, test/loss=2.253964, test/num_examples=10000, total_duration=25432.467067, train/accuracy=0.680781, train/loss=1.278199, validation/accuracy=0.618460, validation/loss=1.573905, validation/num_examples=50000
I0302 18:37:37.300650 140380334978816 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.0756607055664062, loss=3.5330684185028076
I0302 18:38:17.569051 140380326586112 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2334946393966675, loss=3.928938627243042
I0302 18:39:02.222154 140380334978816 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.1797622442245483, loss=2.6645262241363525
I0302 18:39:47.627463 140380326586112 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2022268772125244, loss=2.480184316635132
I0302 18:40:32.375277 140380334978816 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.3374263048171997, loss=2.345292091369629
I0302 18:41:17.480219 140380326586112 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.0829118490219116, loss=4.284131050109863
I0302 18:42:02.234319 140380334978816 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.2724443674087524, loss=2.300612449645996
I0302 18:42:47.313185 140380326586112 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.1962907314300537, loss=4.360743522644043
I0302 18:43:32.370597 140380334978816 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.3155661821365356, loss=2.292205810546875
I0302 18:44:17.377598 140380326586112 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.1977858543395996, loss=2.8546013832092285
I0302 18:44:37.278823 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:44:48.046487 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:45:10.866473 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:45:12.523877 140575196817216 submission_runner.py:411] Time since start: 25888.13s, 	Step: 54046, 	{'train/accuracy': 0.6664257645606995, 'train/loss': 1.3784408569335938, 'validation/accuracy': 0.621679961681366, 'validation/loss': 1.5963414907455444, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.259164571762085, 'test/num_examples': 10000, 'score': 23990.754993915558, 'total_duration': 25888.12592768669, 'accumulated_submission_time': 23990.754993915558, 'accumulated_eval_time': 1892.2611465454102, 'accumulated_logging_time': 2.2538437843322754}
I0302 18:45:12.549565 140380334978816 logging_writer.py:48] [54046] accumulated_eval_time=1892.261147, accumulated_logging_time=2.253844, accumulated_submission_time=23990.754994, global_step=54046, preemption_count=0, score=23990.754994, test/accuracy=0.497600, test/loss=2.259165, test/num_examples=10000, total_duration=25888.125928, train/accuracy=0.666426, train/loss=1.378441, validation/accuracy=0.621680, validation/loss=1.596341, validation/num_examples=50000
I0302 18:45:34.434091 140380326586112 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.1663216352462769, loss=4.6282639503479
I0302 18:46:17.410699 140380334978816 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.3057572841644287, loss=3.661151647567749
I0302 18:47:02.795748 140380326586112 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.308371901512146, loss=2.3664400577545166
I0302 18:47:47.996171 140380334978816 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.1140979528427124, loss=4.744065761566162
I0302 18:48:33.049757 140380326586112 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2776103019714355, loss=4.681521415710449
I0302 18:49:17.911060 140380334978816 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.3071446418762207, loss=2.2238855361938477
I0302 18:50:02.816640 140380326586112 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2424851655960083, loss=2.361487865447998
I0302 18:50:47.780166 140380334978816 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.131073236465454, loss=2.4259836673736572
I0302 18:51:32.713532 140380326586112 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.2210773229599, loss=2.682621955871582
I0302 18:52:12.620986 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:52:23.488722 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 18:52:46.877564 140575196817216 spec.py:349] Evaluating on the test split.
I0302 18:52:48.532344 140575196817216 submission_runner.py:411] Time since start: 26344.13s, 	Step: 54991, 	{'train/accuracy': 0.6720898151397705, 'train/loss': 1.3343063592910767, 'validation/accuracy': 0.6214399933815002, 'validation/loss': 1.5615304708480835, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2374267578125, 'test/num_examples': 10000, 'score': 24410.766496419907, 'total_duration': 26344.1343998909, 'accumulated_submission_time': 24410.766496419907, 'accumulated_eval_time': 1928.1724963188171, 'accumulated_logging_time': 2.2901113033294678}
I0302 18:52:48.555665 140380334978816 logging_writer.py:48] [54991] accumulated_eval_time=1928.172496, accumulated_logging_time=2.290111, accumulated_submission_time=24410.766496, global_step=54991, preemption_count=0, score=24410.766496, test/accuracy=0.500100, test/loss=2.237427, test/num_examples=10000, total_duration=26344.134400, train/accuracy=0.672090, train/loss=1.334306, validation/accuracy=0.621440, validation/loss=1.561530, validation/num_examples=50000
I0302 18:52:52.538319 140380326586112 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1514586210250854, loss=2.9928619861602783
I0302 18:53:33.051831 140380334978816 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.287116289138794, loss=4.906394958496094
I0302 18:54:17.684074 140380326586112 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.3135361671447754, loss=2.290788412094116
I0302 18:55:02.963947 140380334978816 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.3753559589385986, loss=2.6499338150024414
I0302 18:55:47.929137 140380326586112 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.202193021774292, loss=2.5697672367095947
I0302 18:56:33.181805 140380334978816 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2399688959121704, loss=2.285022735595703
I0302 18:57:18.106188 140380326586112 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.215134620666504, loss=3.2521939277648926
I0302 18:58:03.062907 140380334978816 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.1375732421875, loss=3.3117544651031494
I0302 18:58:48.121927 140380326586112 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.1810095310211182, loss=4.836485385894775
I0302 18:59:33.247394 140380334978816 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.242401123046875, loss=2.333728313446045
I0302 18:59:48.632570 140575196817216 spec.py:321] Evaluating on the training split.
I0302 18:59:59.358710 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:00:22.721707 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:00:24.382926 140575196817216 submission_runner.py:411] Time since start: 26799.98s, 	Step: 55936, 	{'train/accuracy': 0.6838671565055847, 'train/loss': 1.2733802795410156, 'validation/accuracy': 0.6282599568367004, 'validation/loss': 1.539625644683838, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.2062501907348633, 'test/num_examples': 10000, 'score': 24830.783933877945, 'total_duration': 26799.984984874725, 'accumulated_submission_time': 24830.783933877945, 'accumulated_eval_time': 1963.9228575229645, 'accumulated_logging_time': 2.3235397338867188}
I0302 19:00:24.409148 140380326586112 logging_writer.py:48] [55936] accumulated_eval_time=1963.922858, accumulated_logging_time=2.323540, accumulated_submission_time=24830.783934, global_step=55936, preemption_count=0, score=24830.783934, test/accuracy=0.503500, test/loss=2.206250, test/num_examples=10000, total_duration=26799.984985, train/accuracy=0.683867, train/loss=1.273380, validation/accuracy=0.628260, validation/loss=1.539626, validation/num_examples=50000
I0302 19:00:50.266473 140380334978816 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1989585161209106, loss=2.3476696014404297
I0302 19:01:33.733922 140380326586112 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.3889895677566528, loss=2.368788957595825
I0302 19:02:18.670637 140380334978816 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0833176374435425, loss=3.418140411376953
I0302 19:03:04.104332 140380326586112 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.4506293535232544, loss=2.281190872192383
I0302 19:03:48.864129 140380334978816 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2557791471481323, loss=4.444821834564209
I0302 19:04:33.890912 140380326586112 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1352428197860718, loss=2.891871452331543
I0302 19:05:18.641561 140380334978816 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.3776867389678955, loss=2.4496326446533203
I0302 19:06:03.496890 140380326586112 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2527059316635132, loss=2.2997686862945557
I0302 19:06:48.435514 140380334978816 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1814851760864258, loss=3.170395851135254
I0302 19:07:24.519253 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:07:35.300634 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:07:57.255088 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:07:58.920830 140575196817216 submission_runner.py:411] Time since start: 27254.52s, 	Step: 56882, 	{'train/accuracy': 0.7085351347923279, 'train/loss': 1.1974025964736938, 'validation/accuracy': 0.6280999779701233, 'validation/loss': 1.5620676279067993, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.223431348800659, 'test/num_examples': 10000, 'score': 25250.83206653595, 'total_duration': 27254.522886514664, 'accumulated_submission_time': 25250.83206653595, 'accumulated_eval_time': 1998.3244211673737, 'accumulated_logging_time': 2.3617398738861084}
I0302 19:07:58.950087 140380326586112 logging_writer.py:48] [56882] accumulated_eval_time=1998.324421, accumulated_logging_time=2.361740, accumulated_submission_time=25250.832067, global_step=56882, preemption_count=0, score=25250.832067, test/accuracy=0.505600, test/loss=2.223431, test/num_examples=10000, total_duration=27254.522887, train/accuracy=0.708535, train/loss=1.197403, validation/accuracy=0.628100, validation/loss=1.562068, validation/num_examples=50000
I0302 19:08:06.520250 140380334978816 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2645643949508667, loss=2.438499927520752
I0302 19:08:47.957263 140380326586112 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1028140783309937, loss=2.448591709136963
I0302 19:09:32.939224 140380334978816 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.3170148134231567, loss=2.321573257446289
I0302 19:10:18.078288 140380326586112 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.1888469457626343, loss=3.0159974098205566
I0302 19:11:03.098933 140380334978816 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.037057638168335, loss=3.9875450134277344
I0302 19:11:48.031533 140380326586112 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.147612452507019, loss=4.866636276245117
I0302 19:12:32.607042 140380334978816 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.193670392036438, loss=2.378848075866699
I0302 19:13:18.158012 140380326586112 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.1259890794754028, loss=4.1572370529174805
I0302 19:14:03.426565 140380334978816 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.1449191570281982, loss=3.3853259086608887
I0302 19:14:48.655363 140380326586112 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.270041823387146, loss=3.5562312602996826
I0302 19:14:59.346618 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:15:10.172118 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:15:32.717694 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:15:34.372065 140575196817216 submission_runner.py:411] Time since start: 27709.97s, 	Step: 57826, 	{'train/accuracy': 0.6792578101158142, 'train/loss': 1.2947694063186646, 'validation/accuracy': 0.6309199929237366, 'validation/loss': 1.5316017866134644, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.1904778480529785, 'test/num_examples': 10000, 'score': 25671.169291496277, 'total_duration': 27709.973189353943, 'accumulated_submission_time': 25671.169291496277, 'accumulated_eval_time': 2033.3489353656769, 'accumulated_logging_time': 2.401756525039673}
I0302 19:15:34.395091 140380334978816 logging_writer.py:48] [57826] accumulated_eval_time=2033.348935, accumulated_logging_time=2.401757, accumulated_submission_time=25671.169291, global_step=57826, preemption_count=0, score=25671.169291, test/accuracy=0.507400, test/loss=2.190478, test/num_examples=10000, total_duration=27709.973189, train/accuracy=0.679258, train/loss=1.294769, validation/accuracy=0.630920, validation/loss=1.531602, validation/num_examples=50000
I0302 19:16:04.214940 140380326586112 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.167101502418518, loss=4.857051849365234
I0302 19:16:48.254876 140380334978816 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2932265996932983, loss=2.2448809146881104
I0302 19:17:33.502530 140380326586112 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.3694398403167725, loss=2.3838634490966797
I0302 19:18:18.590390 140380334978816 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.258932113647461, loss=2.4506332874298096
I0302 19:19:03.973937 140380326586112 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.3572505712509155, loss=4.959522247314453
I0302 19:19:48.787005 140380334978816 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.2339421510696411, loss=2.209113121032715
I0302 19:20:33.539878 140380326586112 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.2772489786148071, loss=4.578297138214111
I0302 19:21:18.770719 140380334978816 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.3190503120422363, loss=2.2829318046569824
I0302 19:22:03.739018 140380326586112 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.216736912727356, loss=3.274716854095459
I0302 19:22:34.701658 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:22:45.164728 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:23:08.004829 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:23:09.662456 140575196817216 submission_runner.py:411] Time since start: 28165.26s, 	Step: 58771, 	{'train/accuracy': 0.6832422018051147, 'train/loss': 1.3048343658447266, 'validation/accuracy': 0.6307799816131592, 'validation/loss': 1.5494695901870728, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.2117326259613037, 'test/num_examples': 10000, 'score': 26091.416990995407, 'total_duration': 28165.263494968414, 'accumulated_submission_time': 26091.416990995407, 'accumulated_eval_time': 2068.3087170124054, 'accumulated_logging_time': 2.434241533279419}
I0302 19:23:09.688994 140380334978816 logging_writer.py:48] [58771] accumulated_eval_time=2068.308717, accumulated_logging_time=2.434242, accumulated_submission_time=26091.416991, global_step=58771, preemption_count=0, score=26091.416991, test/accuracy=0.505100, test/loss=2.211733, test/num_examples=10000, total_duration=28165.263495, train/accuracy=0.683242, train/loss=1.304834, validation/accuracy=0.630780, validation/loss=1.549470, validation/num_examples=50000
I0302 19:23:21.633158 140380326586112 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.0541620254516602, loss=4.877701282501221
I0302 19:24:03.038065 140380334978816 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.1392178535461426, loss=3.660971164703369
I0302 19:24:48.083956 140380326586112 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.347030758857727, loss=2.6784465312957764
I0302 19:25:33.440821 140380334978816 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.2229610681533813, loss=4.577741622924805
I0302 19:26:18.274848 140380326586112 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1043872833251953, loss=4.171634197235107
I0302 19:27:03.507932 140380334978816 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2248547077178955, loss=2.340247869491577
I0302 19:27:48.296897 140380326586112 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.157828450202942, loss=2.5837204456329346
I0302 19:28:33.306454 140380334978816 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1717041730880737, loss=2.6005828380584717
I0302 19:29:18.190467 140380326586112 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.298615574836731, loss=2.347226142883301
I0302 19:30:03.162798 140380334978816 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.275865077972412, loss=2.376271963119507
I0302 19:30:10.160138 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:30:20.738800 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:30:43.366763 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:30:45.023224 140575196817216 submission_runner.py:411] Time since start: 28620.62s, 	Step: 59717, 	{'train/accuracy': 0.6934570074081421, 'train/loss': 1.2318331003189087, 'validation/accuracy': 0.6297599673271179, 'validation/loss': 1.5253384113311768, 'validation/num_examples': 50000, 'test/accuracy': 0.501300036907196, 'test/loss': 2.2092301845550537, 'test/num_examples': 10000, 'score': 26511.82786488533, 'total_duration': 28620.624248981476, 'accumulated_submission_time': 26511.82786488533, 'accumulated_eval_time': 2103.1707775592804, 'accumulated_logging_time': 2.470738172531128}
I0302 19:30:45.047672 140380326586112 logging_writer.py:48] [59717] accumulated_eval_time=2103.170778, accumulated_logging_time=2.470738, accumulated_submission_time=26511.827865, global_step=59717, preemption_count=0, score=26511.827865, test/accuracy=0.501300, test/loss=2.209230, test/num_examples=10000, total_duration=28620.624249, train/accuracy=0.693457, train/loss=1.231833, validation/accuracy=0.629760, validation/loss=1.525338, validation/num_examples=50000
I0302 19:31:18.493248 140380334978816 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.2161132097244263, loss=2.251323699951172
I0302 19:32:02.424426 140380326586112 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2431777715682983, loss=4.721274375915527
I0302 19:32:47.256415 140380334978816 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.173766851425171, loss=2.9020345211029053
I0302 19:33:33.215250 140380326586112 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3502379655838013, loss=2.297133445739746
I0302 19:34:18.237709 140380334978816 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.2225170135498047, loss=4.703114032745361
I0302 19:35:03.390359 140380326586112 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.2748180627822876, loss=2.4412002563476562
I0302 19:35:48.118590 140380334978816 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.2456250190734863, loss=2.315793514251709
I0302 19:36:33.510741 140380326586112 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.153908371925354, loss=3.9858884811401367
I0302 19:37:18.277986 140380334978816 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1591466665267944, loss=4.893887042999268
I0302 19:37:45.025730 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:37:55.641611 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:38:18.840193 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:38:20.506469 140575196817216 submission_runner.py:411] Time since start: 29076.11s, 	Step: 60661, 	{'train/accuracy': 0.6733202934265137, 'train/loss': 1.314584732055664, 'validation/accuracy': 0.6304999589920044, 'validation/loss': 1.5325448513031006, 'validation/num_examples': 50000, 'test/accuracy': 0.5042000412940979, 'test/loss': 2.225181818008423, 'test/num_examples': 10000, 'score': 26931.74744296074, 'total_duration': 29076.107455968857, 'accumulated_submission_time': 26931.74744296074, 'accumulated_eval_time': 2138.6504290103912, 'accumulated_logging_time': 2.5045390129089355}
I0302 19:38:20.530909 140380326586112 logging_writer.py:48] [60661] accumulated_eval_time=2138.650429, accumulated_logging_time=2.504539, accumulated_submission_time=26931.747443, global_step=60661, preemption_count=0, score=26931.747443, test/accuracy=0.504200, test/loss=2.225182, test/num_examples=10000, total_duration=29076.107456, train/accuracy=0.673320, train/loss=1.314585, validation/accuracy=0.630500, validation/loss=1.532545, validation/num_examples=50000
I0302 19:38:36.437718 140380334978816 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.193329930305481, loss=2.235867977142334
I0302 19:39:18.529484 140380326586112 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.2244863510131836, loss=3.3263115882873535
I0302 19:40:03.664797 140380334978816 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.251173496246338, loss=4.659090518951416
I0302 19:40:48.786231 140380326586112 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.4414310455322266, loss=2.3186845779418945
I0302 19:41:33.697102 140380334978816 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2113896608352661, loss=3.835420608520508
I0302 19:42:18.867048 140380326586112 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.3394649028778076, loss=2.257789373397827
I0302 19:43:04.226121 140380334978816 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.385346531867981, loss=2.4746484756469727
I0302 19:43:49.349241 140380326586112 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.3867168426513672, loss=2.243767023086548
I0302 19:44:34.364392 140380334978816 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.3136342763900757, loss=2.4121134281158447
I0302 19:45:19.530288 140380326586112 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0809552669525146, loss=4.905401229858398
I0302 19:45:20.957594 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:45:31.659086 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:45:55.019013 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:45:56.680375 140575196817216 submission_runner.py:411] Time since start: 29532.28s, 	Step: 61605, 	{'train/accuracy': 0.6815820336341858, 'train/loss': 1.2969701290130615, 'validation/accuracy': 0.6304599642753601, 'validation/loss': 1.544524908065796, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.2018449306488037, 'test/num_examples': 10000, 'score': 27352.115279197693, 'total_duration': 29532.281440734863, 'accumulated_submission_time': 27352.115279197693, 'accumulated_eval_time': 2174.3722081184387, 'accumulated_logging_time': 2.5386993885040283}
I0302 19:45:56.710311 140380334978816 logging_writer.py:48] [61605] accumulated_eval_time=2174.372208, accumulated_logging_time=2.538699, accumulated_submission_time=27352.115279, global_step=61605, preemption_count=0, score=27352.115279, test/accuracy=0.507200, test/loss=2.201845, test/num_examples=10000, total_duration=29532.281441, train/accuracy=0.681582, train/loss=1.296970, validation/accuracy=0.630460, validation/loss=1.544525, validation/num_examples=50000
I0302 19:46:35.133937 140380326586112 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.2167054414749146, loss=2.453853130340576
I0302 19:47:20.086894 140380334978816 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.4202040433883667, loss=2.445927619934082
I0302 19:48:05.212430 140380326586112 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.3458654880523682, loss=4.813558101654053
I0302 19:48:49.862222 140380334978816 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2868156433105469, loss=2.428565502166748
I0302 19:49:35.002840 140380326586112 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.249071717262268, loss=3.3936190605163574
I0302 19:50:20.076882 140380334978816 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.2631886005401611, loss=2.2550864219665527
I0302 19:51:05.088299 140380326586112 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.3270078897476196, loss=2.236873149871826
I0302 19:51:50.144017 140380334978816 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.3101344108581543, loss=2.283923387527466
I0302 19:52:34.866637 140380326586112 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.1566905975341797, loss=3.2080423831939697
I0302 19:52:56.738710 140575196817216 spec.py:321] Evaluating on the training split.
I0302 19:53:07.524303 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 19:53:30.394350 140575196817216 spec.py:349] Evaluating on the test split.
I0302 19:53:32.044690 140575196817216 submission_runner.py:411] Time since start: 29987.65s, 	Step: 62550, 	{'train/accuracy': 0.694531261920929, 'train/loss': 1.2573353052139282, 'validation/accuracy': 0.6335799694061279, 'validation/loss': 1.5339800119400024, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.191729784011841, 'test/num_examples': 10000, 'score': 27772.08389544487, 'total_duration': 29987.64568257332, 'accumulated_submission_time': 27772.08389544487, 'accumulated_eval_time': 2209.677117586136, 'accumulated_logging_time': 2.578078031539917}
I0302 19:53:32.072403 140380334978816 logging_writer.py:48] [62550] accumulated_eval_time=2209.677118, accumulated_logging_time=2.578078, accumulated_submission_time=27772.083895, global_step=62550, preemption_count=0, score=27772.083895, test/accuracy=0.512900, test/loss=2.191730, test/num_examples=10000, total_duration=29987.645683, train/accuracy=0.694531, train/loss=1.257335, validation/accuracy=0.633580, validation/loss=1.533980, validation/num_examples=50000
I0302 19:53:52.405212 140380326586112 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.1467814445495605, loss=2.7970402240753174
I0302 19:54:34.794872 140380334978816 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.3508343696594238, loss=2.3563432693481445
I0302 19:55:20.043533 140380326586112 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3172590732574463, loss=2.295083522796631
I0302 19:56:05.335802 140380334978816 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.3284595012664795, loss=2.2818143367767334
I0302 19:56:50.649952 140380326586112 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3404685258865356, loss=2.1881251335144043
I0302 19:57:35.644951 140380334978816 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.3191417455673218, loss=4.469483375549316
I0302 19:58:20.662488 140380326586112 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.313765525817871, loss=2.528333902359009
I0302 19:59:05.558649 140380334978816 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.2739912271499634, loss=2.1549389362335205
I0302 19:59:50.373321 140380326586112 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3540732860565186, loss=2.377631902694702
I0302 20:00:32.257114 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:00:42.824144 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:01:06.181142 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:01:07.840273 140575196817216 submission_runner.py:411] Time since start: 30443.44s, 	Step: 63495, 	{'train/accuracy': 0.719531238079071, 'train/loss': 1.1414695978164673, 'validation/accuracy': 0.6376999616622925, 'validation/loss': 1.5069578886032104, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.1697821617126465, 'test/num_examples': 10000, 'score': 28192.20817756653, 'total_duration': 30443.441133499146, 'accumulated_submission_time': 28192.20817756653, 'accumulated_eval_time': 2245.2590703964233, 'accumulated_logging_time': 2.6166160106658936}
I0302 20:01:07.870794 140380334978816 logging_writer.py:48] [63495] accumulated_eval_time=2245.259070, accumulated_logging_time=2.616616, accumulated_submission_time=28192.208178, global_step=63495, preemption_count=0, score=28192.208178, test/accuracy=0.515500, test/loss=2.169782, test/num_examples=10000, total_duration=30443.441133, train/accuracy=0.719531, train/loss=1.141470, validation/accuracy=0.637700, validation/loss=1.506958, validation/num_examples=50000
I0302 20:01:10.267667 140380326586112 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.9803231954574585, loss=4.5327630043029785
I0302 20:01:50.814755 140380334978816 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3841363191604614, loss=2.2862086296081543
I0302 20:02:35.380943 140380326586112 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.209931492805481, loss=2.380859613418579
I0302 20:03:20.662177 140380334978816 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.2119510173797607, loss=3.928802490234375
I0302 20:04:05.827404 140380326586112 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3160568475723267, loss=2.39552903175354
I0302 20:04:50.519818 140380334978816 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.2321666479110718, loss=2.8450074195861816
I0302 20:05:35.546099 140380326586112 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2225958108901978, loss=3.0613200664520264
I0302 20:06:20.626327 140380334978816 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0169520378112793, loss=3.6245248317718506
I0302 20:07:05.656702 140380326586112 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1713950634002686, loss=3.98211932182312
I0302 20:07:50.549652 140380334978816 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.296381950378418, loss=2.2918577194213867
I0302 20:08:07.851278 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:08:18.994980 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:08:41.328684 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:08:42.992518 140575196817216 submission_runner.py:411] Time since start: 30898.59s, 	Step: 64440, 	{'train/accuracy': 0.6832422018051147, 'train/loss': 1.289946436882019, 'validation/accuracy': 0.6335200071334839, 'validation/loss': 1.5233887434005737, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.1913552284240723, 'test/num_examples': 10000, 'score': 28612.125376462936, 'total_duration': 30898.59340786934, 'accumulated_submission_time': 28612.125376462936, 'accumulated_eval_time': 2280.39914727211, 'accumulated_logging_time': 2.6603734493255615}
I0302 20:08:43.028549 140380326586112 logging_writer.py:48] [64440] accumulated_eval_time=2280.399147, accumulated_logging_time=2.660373, accumulated_submission_time=28612.125376, global_step=64440, preemption_count=0, score=28612.125376, test/accuracy=0.505400, test/loss=2.191355, test/num_examples=10000, total_duration=30898.593408, train/accuracy=0.683242, train/loss=1.289946, validation/accuracy=0.633520, validation/loss=1.523389, validation/num_examples=50000
I0302 20:09:07.524175 140380334978816 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1984292268753052, loss=3.114473342895508
I0302 20:09:50.550131 140380326586112 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1485236883163452, loss=4.3904500007629395
I0302 20:10:35.558574 140380334978816 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1312386989593506, loss=4.299748420715332
I0302 20:11:20.466880 140380326586112 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3122714757919312, loss=2.1937267780303955
I0302 20:12:05.436710 140380334978816 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2868040800094604, loss=2.26208758354187
I0302 20:12:50.153004 140380326586112 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2020474672317505, loss=3.991811752319336
I0302 20:13:35.261618 140380334978816 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.3364684581756592, loss=2.333493232727051
I0302 20:14:20.415782 140380326586112 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.190552830696106, loss=3.6135592460632324
I0302 20:15:05.465651 140380334978816 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1215544939041138, loss=4.508276462554932
I0302 20:15:43.174402 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:15:53.927307 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:16:17.175690 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:16:18.826503 140575196817216 submission_runner.py:411] Time since start: 31354.43s, 	Step: 65386, 	{'train/accuracy': 0.694042980670929, 'train/loss': 1.2231028079986572, 'validation/accuracy': 0.6383799910545349, 'validation/loss': 1.484677791595459, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.162203073501587, 'test/num_examples': 10000, 'score': 29032.208420038223, 'total_duration': 31354.427513837814, 'accumulated_submission_time': 29032.208420038223, 'accumulated_eval_time': 2316.05020904541, 'accumulated_logging_time': 2.7098233699798584}
I0302 20:16:18.852931 140380326586112 logging_writer.py:48] [65386] accumulated_eval_time=2316.050209, accumulated_logging_time=2.709823, accumulated_submission_time=29032.208420, global_step=65386, preemption_count=0, score=29032.208420, test/accuracy=0.507900, test/loss=2.162203, test/num_examples=10000, total_duration=31354.427514, train/accuracy=0.694043, train/loss=1.223103, validation/accuracy=0.638380, validation/loss=1.484678, validation/num_examples=50000
I0302 20:16:24.816386 140380334978816 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.368504524230957, loss=3.650059223175049
I0302 20:17:05.517861 140380326586112 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3615431785583496, loss=2.2471115589141846
I0302 20:17:50.285230 140380334978816 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.1063262224197388, loss=4.755644798278809
I0302 20:18:35.641888 140380326586112 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.3184894323349, loss=2.243375062942505
I0302 20:19:20.506230 140380334978816 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.2738465070724487, loss=2.2438361644744873
I0302 20:20:05.356866 140380326586112 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2207382917404175, loss=4.756521701812744
I0302 20:20:50.172986 140380334978816 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.234154224395752, loss=2.2340500354766846
I0302 20:21:35.113217 140380326586112 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1678937673568726, loss=3.2999377250671387
I0302 20:22:20.033394 140380334978816 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.4187551736831665, loss=2.274707317352295
I0302 20:23:05.077403 140380326586112 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3120038509368896, loss=4.9165754318237305
I0302 20:23:19.011311 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:23:29.719596 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:23:53.465179 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:23:55.133040 140575196817216 submission_runner.py:411] Time since start: 31810.73s, 	Step: 66332, 	{'train/accuracy': 0.707226574420929, 'train/loss': 1.1690928936004639, 'validation/accuracy': 0.6391199827194214, 'validation/loss': 1.4746456146240234, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.1579055786132812, 'test/num_examples': 10000, 'score': 29452.30614376068, 'total_duration': 31810.7338078022, 'accumulated_submission_time': 29452.30614376068, 'accumulated_eval_time': 2352.170639514923, 'accumulated_logging_time': 2.747178792953491}
I0302 20:23:55.163012 140380334978816 logging_writer.py:48] [66332] accumulated_eval_time=2352.170640, accumulated_logging_time=2.747179, accumulated_submission_time=29452.306144, global_step=66332, preemption_count=0, score=29452.306144, test/accuracy=0.514300, test/loss=2.157906, test/num_examples=10000, total_duration=31810.733808, train/accuracy=0.707227, train/loss=1.169093, validation/accuracy=0.639120, validation/loss=1.474646, validation/num_examples=50000
I0302 20:24:22.621405 140380326586112 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.3697618246078491, loss=2.1582114696502686
I0302 20:25:06.485220 140380334978816 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.3578341007232666, loss=4.540818214416504
I0302 20:25:51.821421 140380326586112 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.339334487915039, loss=2.1887123584747314
I0302 20:26:37.084929 140380334978816 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.3864164352416992, loss=2.2599081993103027
I0302 20:27:22.158766 140380326586112 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.2499442100524902, loss=2.2510290145874023
I0302 20:28:07.213649 140380334978816 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.0973552465438843, loss=4.684948444366455
I0302 20:28:52.157539 140380326586112 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2110737562179565, loss=2.7995128631591797
I0302 20:29:37.316307 140380334978816 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.4284557104110718, loss=2.0371947288513184
I0302 20:30:22.233143 140380326586112 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.4836971759796143, loss=2.3936944007873535
I0302 20:30:55.154881 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:31:05.904766 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:31:28.958857 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:31:30.638679 140575196817216 submission_runner.py:411] Time since start: 32266.24s, 	Step: 67275, 	{'train/accuracy': 0.6863867044448853, 'train/loss': 1.2691597938537598, 'validation/accuracy': 0.6340999603271484, 'validation/loss': 1.5110490322113037, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.1513500213623047, 'test/num_examples': 10000, 'score': 29872.238432645798, 'total_duration': 32266.239609479904, 'accumulated_submission_time': 29872.238432645798, 'accumulated_eval_time': 2387.653300523758, 'accumulated_logging_time': 2.787407875061035}
I0302 20:31:30.668367 140380334978816 logging_writer.py:48] [67275] accumulated_eval_time=2387.653301, accumulated_logging_time=2.787408, accumulated_submission_time=29872.238433, global_step=67275, preemption_count=0, score=29872.238433, test/accuracy=0.515900, test/loss=2.151350, test/num_examples=10000, total_duration=32266.239609, train/accuracy=0.686387, train/loss=1.269160, validation/accuracy=0.634100, validation/loss=1.511049, validation/num_examples=50000
I0302 20:31:41.067036 140380326586112 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1805827617645264, loss=3.219973564147949
I0302 20:32:22.525973 140380334978816 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.177436351776123, loss=3.2508621215820312
I0302 20:33:07.004194 140380326586112 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.2778651714324951, loss=2.6146113872528076
I0302 20:33:52.649333 140380334978816 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3638560771942139, loss=2.268458366394043
I0302 20:34:38.371502 140380326586112 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.681239366531372, loss=2.7048234939575195
I0302 20:35:23.253049 140380334978816 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.1758148670196533, loss=3.489147424697876
I0302 20:36:08.556953 140380326586112 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.1692743301391602, loss=4.688965797424316
I0302 20:36:53.734587 140380334978816 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.17778480052948, loss=3.348897933959961
I0302 20:37:38.804803 140380326586112 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.4017213582992554, loss=2.450145721435547
I0302 20:38:24.002349 140380334978816 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.2221726179122925, loss=2.3272945880889893
I0302 20:38:30.840721 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:38:41.459336 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:39:03.137030 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:39:04.814022 140575196817216 submission_runner.py:411] Time since start: 32720.42s, 	Step: 68217, 	{'train/accuracy': 0.7003710865974426, 'train/loss': 1.183992624282837, 'validation/accuracy': 0.6473000049591064, 'validation/loss': 1.4416313171386719, 'validation/num_examples': 50000, 'test/accuracy': 0.5285000205039978, 'test/loss': 2.097337245941162, 'test/num_examples': 10000, 'score': 30292.351333141327, 'total_duration': 32720.415112257004, 'accumulated_submission_time': 30292.351333141327, 'accumulated_eval_time': 2421.625636100769, 'accumulated_logging_time': 2.82780122756958}
I0302 20:39:04.840489 140380326586112 logging_writer.py:48] [68217] accumulated_eval_time=2421.625636, accumulated_logging_time=2.827801, accumulated_submission_time=30292.351333, global_step=68217, preemption_count=0, score=30292.351333, test/accuracy=0.528500, test/loss=2.097337, test/num_examples=10000, total_duration=32720.415112, train/accuracy=0.700371, train/loss=1.183993, validation/accuracy=0.647300, validation/loss=1.441631, validation/num_examples=50000
I0302 20:39:38.261511 140380334978816 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.339938759803772, loss=2.100329875946045
I0302 20:40:22.848393 140380326586112 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2839032411575317, loss=2.2258877754211426
I0302 20:41:08.117327 140380334978816 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3947445154190063, loss=2.4128050804138184
I0302 20:41:52.960455 140380326586112 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.1687654256820679, loss=2.640958786010742
I0302 20:42:37.981848 140380334978816 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1087015867233276, loss=2.9672470092773438
I0302 20:43:23.332474 140380326586112 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.3311870098114014, loss=2.1905784606933594
I0302 20:44:08.569759 140380334978816 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.4258619546890259, loss=2.2650179862976074
I0302 20:44:53.575729 140380326586112 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3131473064422607, loss=2.4349400997161865
I0302 20:45:38.706419 140380334978816 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3529093265533447, loss=2.194603204727173
I0302 20:46:04.898218 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:46:15.918762 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:46:36.903358 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:46:38.562738 140575196817216 submission_runner.py:411] Time since start: 33174.16s, 	Step: 69160, 	{'train/accuracy': 0.70570307970047, 'train/loss': 1.190212607383728, 'validation/accuracy': 0.644819974899292, 'validation/loss': 1.4619231224060059, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.136876106262207, 'test/num_examples': 10000, 'score': 30712.349891662598, 'total_duration': 33174.16375398636, 'accumulated_submission_time': 30712.349891662598, 'accumulated_eval_time': 2455.2891149520874, 'accumulated_logging_time': 2.8639655113220215}
I0302 20:46:38.594413 140380326586112 logging_writer.py:48] [69160] accumulated_eval_time=2455.289115, accumulated_logging_time=2.863966, accumulated_submission_time=30712.349892, global_step=69160, preemption_count=0, score=30712.349892, test/accuracy=0.517300, test/loss=2.136876, test/num_examples=10000, total_duration=33174.163754, train/accuracy=0.705703, train/loss=1.190213, validation/accuracy=0.644820, validation/loss=1.461923, validation/num_examples=50000
I0302 20:46:55.535663 140380334978816 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.2903434038162231, loss=2.3037946224212646
I0302 20:47:38.241346 140380326586112 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2098064422607422, loss=2.2673838138580322
I0302 20:48:23.460829 140380334978816 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.3759888410568237, loss=2.311760425567627
I0302 20:49:08.557474 140380326586112 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.3666280508041382, loss=2.2735934257507324
I0302 20:49:53.956560 140380334978816 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.351006031036377, loss=4.781241416931152
I0302 20:50:39.157491 140380326586112 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.5095977783203125, loss=2.236402988433838
I0302 20:51:24.411882 140380334978816 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.4992324113845825, loss=2.191667318344116
I0302 20:52:09.638161 140380326586112 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.1577311754226685, loss=3.395735740661621
I0302 20:52:54.764259 140380334978816 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2149630784988403, loss=3.2372310161590576
I0302 20:53:38.835822 140575196817216 spec.py:321] Evaluating on the training split.
I0302 20:53:49.509180 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 20:54:11.642075 140575196817216 spec.py:349] Evaluating on the test split.
I0302 20:54:13.305664 140575196817216 submission_runner.py:411] Time since start: 33628.91s, 	Step: 70099, 	{'train/accuracy': 0.7290429472923279, 'train/loss': 1.0740240812301636, 'validation/accuracy': 0.6439999938011169, 'validation/loss': 1.463181734085083, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.132868766784668, 'test/num_examples': 10000, 'score': 31131.891610860825, 'total_duration': 33628.90654087067, 'accumulated_submission_time': 31131.891610860825, 'accumulated_eval_time': 2489.757776260376, 'accumulated_logging_time': 3.5462582111358643}
I0302 20:54:13.340501 140380326586112 logging_writer.py:48] [70099] accumulated_eval_time=2489.757776, accumulated_logging_time=3.546258, accumulated_submission_time=31131.891611, global_step=70099, preemption_count=0, score=31131.891611, test/accuracy=0.518400, test/loss=2.132869, test/num_examples=10000, total_duration=33628.906541, train/accuracy=0.729043, train/loss=1.074024, validation/accuracy=0.644000, validation/loss=1.463182, validation/num_examples=50000
I0302 20:54:14.151993 140380334978816 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.3002080917358398, loss=2.2918198108673096
I0302 20:54:53.940035 140380326586112 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.155529260635376, loss=4.141844749450684
I0302 20:55:38.662918 140380334978816 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.30025053024292, loss=2.192842960357666
I0302 20:56:24.018033 140380326586112 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.3243111371994019, loss=2.223163604736328
I0302 20:57:08.945885 140380334978816 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.3061944246292114, loss=2.2931759357452393
I0302 20:57:54.229906 140380326586112 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3633949756622314, loss=2.3953990936279297
I0302 20:58:39.228299 140380334978816 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1671867370605469, loss=3.929136276245117
I0302 20:59:24.226188 140380326586112 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.139415979385376, loss=4.192952632904053
I0302 21:00:09.071522 140380334978816 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.3007086515426636, loss=2.2500815391540527
I0302 21:00:54.096420 140380326586112 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3208603858947754, loss=2.300689220428467
I0302 21:01:13.663461 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:01:24.244589 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:01:47.055628 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:01:48.701768 140575196817216 submission_runner.py:411] Time since start: 34084.30s, 	Step: 71045, 	{'train/accuracy': 0.6943163871765137, 'train/loss': 1.2107747793197632, 'validation/accuracy': 0.6439999938011169, 'validation/loss': 1.4579524993896484, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.1252052783966064, 'test/num_examples': 10000, 'score': 31552.1533575058, 'total_duration': 34084.3028922081, 'accumulated_submission_time': 31552.1533575058, 'accumulated_eval_time': 2524.795145511627, 'accumulated_logging_time': 3.592628002166748}
I0302 21:01:48.730770 140380334978816 logging_writer.py:48] [71045] accumulated_eval_time=2524.795146, accumulated_logging_time=3.592628, accumulated_submission_time=31552.153358, global_step=71045, preemption_count=0, score=31552.153358, test/accuracy=0.523600, test/loss=2.125205, test/num_examples=10000, total_duration=34084.302892, train/accuracy=0.694316, train/loss=1.210775, validation/accuracy=0.644000, validation/loss=1.457952, validation/num_examples=50000
I0302 21:02:11.002644 140380326586112 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2558927536010742, loss=4.6439008712768555
I0302 21:02:53.595640 140380334978816 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1007733345031738, loss=3.020266532897949
I0302 21:03:38.486381 140380326586112 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.2847980260849, loss=4.693661689758301
I0302 21:04:23.959038 140380334978816 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.3357362747192383, loss=2.2302327156066895
I0302 21:05:08.912850 140380326586112 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2598302364349365, loss=2.648859739303589
I0302 21:05:54.138811 140380334978816 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.1151020526885986, loss=4.031233310699463
I0302 21:06:39.137999 140380326586112 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1256866455078125, loss=3.2806167602539062
I0302 21:07:24.687503 140380334978816 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.2265865802764893, loss=2.034456968307495
I0302 21:08:09.807284 140380326586112 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.2911829948425293, loss=2.3222174644470215
I0302 21:08:48.955562 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:08:59.621309 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:09:20.904053 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:09:22.566605 140575196817216 submission_runner.py:411] Time since start: 34538.17s, 	Step: 71989, 	{'train/accuracy': 0.7100585699081421, 'train/loss': 1.1660125255584717, 'validation/accuracy': 0.6522799730300903, 'validation/loss': 1.4295166730880737, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.0866498947143555, 'test/num_examples': 10000, 'score': 31972.31904554367, 'total_duration': 34538.16773843765, 'accumulated_submission_time': 31972.31904554367, 'accumulated_eval_time': 2558.4052596092224, 'accumulated_logging_time': 3.631521224975586}
I0302 21:09:22.596807 140380334978816 logging_writer.py:48] [71989] accumulated_eval_time=2558.405260, accumulated_logging_time=3.631521, accumulated_submission_time=31972.319046, global_step=71989, preemption_count=0, score=31972.319046, test/accuracy=0.524500, test/loss=2.086650, test/num_examples=10000, total_duration=34538.167738, train/accuracy=0.710059, train/loss=1.166013, validation/accuracy=0.652280, validation/loss=1.429517, validation/num_examples=50000
I0302 21:09:27.389354 140380326586112 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4052568674087524, loss=2.371232748031616
I0302 21:10:08.575466 140380334978816 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3106640577316284, loss=2.1427292823791504
I0302 21:10:53.664419 140380326586112 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.293246865272522, loss=2.669504165649414
I0302 21:11:39.246541 140380334978816 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.342756986618042, loss=4.504256248474121
I0302 21:12:24.104313 140380326586112 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.4212331771850586, loss=2.307986259460449
I0302 21:13:08.950495 140380334978816 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.284543514251709, loss=3.6086812019348145
I0302 21:13:54.252229 140380326586112 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.169514775276184, loss=3.214770555496216
I0302 21:14:39.618716 140380334978816 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.432111144065857, loss=2.514233112335205
I0302 21:15:24.561244 140380326586112 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.1829789876937866, loss=2.769331693649292
I0302 21:16:09.677191 140380334978816 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.1373807191848755, loss=3.707308769226074
I0302 21:16:22.584680 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:16:33.358196 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:16:55.804646 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:16:57.455776 140575196817216 submission_runner.py:411] Time since start: 34993.06s, 	Step: 72930, 	{'train/accuracy': 0.7144140601158142, 'train/loss': 1.1511657238006592, 'validation/accuracy': 0.6480000019073486, 'validation/loss': 1.4559029340744019, 'validation/num_examples': 50000, 'test/accuracy': 0.5224000215530396, 'test/loss': 2.14300537109375, 'test/num_examples': 10000, 'score': 32392.246742248535, 'total_duration': 34993.05684757233, 'accumulated_submission_time': 32392.246742248535, 'accumulated_eval_time': 2593.2753612995148, 'accumulated_logging_time': 3.672046422958374}
I0302 21:16:57.482097 140380326586112 logging_writer.py:48] [72930] accumulated_eval_time=2593.275361, accumulated_logging_time=3.672046, accumulated_submission_time=32392.246742, global_step=72930, preemption_count=0, score=32392.246742, test/accuracy=0.522400, test/loss=2.143005, test/num_examples=10000, total_duration=34993.056848, train/accuracy=0.714414, train/loss=1.151166, validation/accuracy=0.648000, validation/loss=1.455903, validation/num_examples=50000
I0302 21:17:25.723982 140380334978816 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.6301697492599487, loss=2.6022024154663086
I0302 21:18:09.504613 140380326586112 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.436740756034851, loss=2.076101064682007
I0302 21:18:54.604523 140380334978816 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.2918211221694946, loss=3.935210704803467
I0302 21:19:39.455128 140380326586112 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3433136940002441, loss=2.227786064147949
I0302 21:20:24.316849 140380334978816 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4064104557037354, loss=2.16756272315979
I0302 21:21:09.282380 140380326586112 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.1650598049163818, loss=4.508522033691406
I0302 21:21:54.414787 140380334978816 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.365444540977478, loss=2.191927671432495
I0302 21:22:39.262878 140380326586112 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3565034866333008, loss=2.297591209411621
I0302 21:23:24.343476 140380334978816 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.4525514841079712, loss=2.200821876525879
I0302 21:23:57.768101 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:24:08.453130 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:24:31.205390 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:24:32.867133 140575196817216 submission_runner.py:411] Time since start: 35448.47s, 	Step: 73876, 	{'train/accuracy': 0.6960546970367432, 'train/loss': 1.2367867231369019, 'validation/accuracy': 0.64656001329422, 'validation/loss': 1.4624232053756714, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.137120246887207, 'test/num_examples': 10000, 'score': 32812.47220945358, 'total_duration': 35448.46819233894, 'accumulated_submission_time': 32812.47220945358, 'accumulated_eval_time': 2628.373400449753, 'accumulated_logging_time': 3.7095766067504883}
I0302 21:24:32.893975 140380326586112 logging_writer.py:48] [73876] accumulated_eval_time=2628.373400, accumulated_logging_time=3.709577, accumulated_submission_time=32812.472209, global_step=73876, preemption_count=0, score=32812.472209, test/accuracy=0.522500, test/loss=2.137120, test/num_examples=10000, total_duration=35448.468192, train/accuracy=0.696055, train/loss=1.236787, validation/accuracy=0.646560, validation/loss=1.462423, validation/num_examples=50000
I0302 21:24:42.853023 140380334978816 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.2225730419158936, loss=3.782700538635254
I0302 21:25:24.149533 140380326586112 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3889471292495728, loss=2.045612335205078
I0302 21:26:09.273586 140380334978816 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3873682022094727, loss=2.3278069496154785
I0302 21:26:54.615689 140380326586112 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1997417211532593, loss=3.7194948196411133
I0302 21:27:39.823606 140380334978816 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.2447021007537842, loss=3.704591751098633
I0302 21:28:25.049894 140380326586112 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.2793586254119873, loss=3.9641616344451904
I0302 21:29:10.066644 140380334978816 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3993737697601318, loss=2.2127935886383057
I0302 21:29:55.216753 140380326586112 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.168033242225647, loss=3.3960442543029785
I0302 21:30:40.231858 140380334978816 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.1917134523391724, loss=2.9589807987213135
I0302 21:31:25.350843 140380326586112 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2169135808944702, loss=2.7890563011169434
I0302 21:31:33.233571 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:31:43.919516 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:32:07.552600 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:32:09.198155 140575196817216 submission_runner.py:411] Time since start: 35904.80s, 	Step: 74819, 	{'train/accuracy': 0.7019921541213989, 'train/loss': 1.2093700170516968, 'validation/accuracy': 0.65447998046875, 'validation/loss': 1.4443110227584839, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.1143290996551514, 'test/num_examples': 10000, 'score': 33232.751855134964, 'total_duration': 35904.79923796654, 'accumulated_submission_time': 33232.751855134964, 'accumulated_eval_time': 2664.336992740631, 'accumulated_logging_time': 3.747407913208008}
I0302 21:32:09.224966 140380334978816 logging_writer.py:48] [74819] accumulated_eval_time=2664.336993, accumulated_logging_time=3.747408, accumulated_submission_time=33232.751855, global_step=74819, preemption_count=0, score=33232.751855, test/accuracy=0.527200, test/loss=2.114329, test/num_examples=10000, total_duration=35904.799238, train/accuracy=0.701992, train/loss=1.209370, validation/accuracy=0.654480, validation/loss=1.444311, validation/num_examples=50000
I0302 21:32:41.864777 140380326586112 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.3086662292480469, loss=2.4902336597442627
I0302 21:33:25.699841 140380334978816 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.289139747619629, loss=2.192445755004883
I0302 21:34:11.629894 140380326586112 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.4289932250976562, loss=2.1275291442871094
I0302 21:34:57.414238 140380334978816 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4918169975280762, loss=2.2660622596740723
I0302 21:35:42.444987 140380326586112 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.1673026084899902, loss=4.437707424163818
I0302 21:36:27.929960 140380334978816 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.4381365776062012, loss=2.2598841190338135
I0302 21:37:12.848155 140380326586112 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.2420740127563477, loss=3.002370834350586
I0302 21:37:58.000416 140380334978816 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.278539776802063, loss=2.535984754562378
I0302 21:38:43.059867 140380326586112 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3176181316375732, loss=2.4059031009674072
I0302 21:39:09.389285 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:39:20.078444 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:39:41.543311 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:39:43.215722 140575196817216 submission_runner.py:411] Time since start: 36358.82s, 	Step: 75760, 	{'train/accuracy': 0.710156261920929, 'train/loss': 1.1536434888839722, 'validation/accuracy': 0.6520400047302246, 'validation/loss': 1.4173551797866821, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.0840115547180176, 'test/num_examples': 10000, 'score': 33652.85684943199, 'total_duration': 36358.816739320755, 'accumulated_submission_time': 33652.85684943199, 'accumulated_eval_time': 2698.1623861789703, 'accumulated_logging_time': 3.784353256225586}
I0302 21:39:43.245089 140380334978816 logging_writer.py:48] [75760] accumulated_eval_time=2698.162386, accumulated_logging_time=3.784353, accumulated_submission_time=33652.856849, global_step=75760, preemption_count=0, score=33652.856849, test/accuracy=0.524300, test/loss=2.084012, test/num_examples=10000, total_duration=36358.816739, train/accuracy=0.710156, train/loss=1.153643, validation/accuracy=0.652040, validation/loss=1.417355, validation/num_examples=50000
I0302 21:39:59.546391 140380326586112 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3348994255065918, loss=2.231994152069092
I0302 21:40:42.100748 140380334978816 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3936882019042969, loss=2.075186252593994
I0302 21:41:27.215610 140380326586112 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3532572984695435, loss=3.1542065143585205
I0302 21:42:12.621480 140380334978816 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.3703789710998535, loss=2.1820273399353027
I0302 21:42:57.665221 140380326586112 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3645778894424438, loss=2.4252731800079346
I0302 21:43:42.492439 140380334978816 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.2475651502609253, loss=4.50769567489624
I0302 21:44:28.571015 140380326586112 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.1738004684448242, loss=4.088547229766846
I0302 21:45:13.531482 140380334978816 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.3110284805297852, loss=2.1468770503997803
I0302 21:45:58.906940 140380326586112 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.4379678964614868, loss=2.1230554580688477
I0302 21:46:43.463065 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:46:54.186648 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:47:14.829150 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:47:16.490513 140575196817216 submission_runner.py:411] Time since start: 36812.09s, 	Step: 76700, 	{'train/accuracy': 0.7394140362739563, 'train/loss': 1.0564640760421753, 'validation/accuracy': 0.6574400067329407, 'validation/loss': 1.422579288482666, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.086050271987915, 'test/num_examples': 10000, 'score': 34073.014280080795, 'total_duration': 36812.09155678749, 'accumulated_submission_time': 34073.014280080795, 'accumulated_eval_time': 2731.1888189315796, 'accumulated_logging_time': 3.8260109424591064}
I0302 21:47:16.521863 140380334978816 logging_writer.py:48] [76700] accumulated_eval_time=2731.188819, accumulated_logging_time=3.826011, accumulated_submission_time=34073.014280, global_step=76700, preemption_count=0, score=34073.014280, test/accuracy=0.531400, test/loss=2.086050, test/num_examples=10000, total_duration=36812.091557, train/accuracy=0.739414, train/loss=1.056464, validation/accuracy=0.657440, validation/loss=1.422579, validation/num_examples=50000
I0302 21:47:16.922656 140380326586112 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.2250727415084839, loss=2.512305498123169
I0302 21:47:56.814716 140380334978816 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.559575080871582, loss=4.655350685119629
I0302 21:48:41.927101 140380326586112 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1933813095092773, loss=3.562889575958252
I0302 21:49:27.474203 140380334978816 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4469820261001587, loss=2.1114842891693115
I0302 21:50:12.225250 140380326586112 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.2848351001739502, loss=2.1719906330108643
I0302 21:50:57.240850 140380334978816 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2148619890213013, loss=4.678985118865967
I0302 21:51:42.026601 140380326586112 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.3924880027770996, loss=2.324146270751953
I0302 21:52:27.172394 140380334978816 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4607295989990234, loss=2.2827298641204834
I0302 21:53:12.308007 140380326586112 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.3619771003723145, loss=2.2084734439849854
I0302 21:53:57.526078 140380334978816 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.4859511852264404, loss=2.3131587505340576
I0302 21:54:16.605100 140575196817216 spec.py:321] Evaluating on the training split.
I0302 21:54:27.356223 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 21:54:51.041971 140575196817216 spec.py:349] Evaluating on the test split.
I0302 21:54:52.703499 140575196817216 submission_runner.py:411] Time since start: 37268.30s, 	Step: 77644, 	{'train/accuracy': 0.7078710794448853, 'train/loss': 1.1789730787277222, 'validation/accuracy': 0.6548399925231934, 'validation/loss': 1.4233994483947754, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.083815813064575, 'test/num_examples': 10000, 'score': 34493.03785729408, 'total_duration': 37268.304604291916, 'accumulated_submission_time': 34493.03785729408, 'accumulated_eval_time': 2767.2862479686737, 'accumulated_logging_time': 3.867687463760376}
I0302 21:54:52.733538 140380326586112 logging_writer.py:48] [77644] accumulated_eval_time=2767.286248, accumulated_logging_time=3.867687, accumulated_submission_time=34493.037857, global_step=77644, preemption_count=0, score=34493.037857, test/accuracy=0.533700, test/loss=2.083816, test/num_examples=10000, total_duration=37268.304604, train/accuracy=0.707871, train/loss=1.178973, validation/accuracy=0.654840, validation/loss=1.423399, validation/num_examples=50000
I0302 21:55:15.412616 140380334978816 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.3025068044662476, loss=2.2516770362854004
I0302 21:55:58.058539 140380326586112 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.536700963973999, loss=2.1555261611938477
I0302 21:56:43.486493 140380334978816 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.3851521015167236, loss=4.67185115814209
I0302 21:57:28.664436 140380326586112 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.4592186212539673, loss=2.229215621948242
I0302 21:58:13.782584 140380334978816 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3132030963897705, loss=2.1368043422698975
I0302 21:58:59.042715 140380326586112 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3953578472137451, loss=2.2031588554382324
I0302 21:59:44.269508 140380334978816 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1300889253616333, loss=3.7306535243988037
I0302 22:00:29.423635 140380326586112 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.496351957321167, loss=2.5013904571533203
I0302 22:01:14.590117 140380334978816 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.3442137241363525, loss=2.642613410949707
I0302 22:01:53.045422 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:02:03.965272 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:02:28.439707 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:02:30.091695 140575196817216 submission_runner.py:411] Time since start: 37725.69s, 	Step: 78587, 	{'train/accuracy': 0.7059569954872131, 'train/loss': 1.1735212802886963, 'validation/accuracy': 0.6548999547958374, 'validation/loss': 1.41805100440979, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.07688570022583, 'test/num_examples': 10000, 'score': 34913.28986310959, 'total_duration': 37725.69277334213, 'accumulated_submission_time': 34913.28986310959, 'accumulated_eval_time': 2804.3315374851227, 'accumulated_logging_time': 3.908069133758545}
I0302 22:02:30.125937 140380326586112 logging_writer.py:48] [78587] accumulated_eval_time=2804.331537, accumulated_logging_time=3.908069, accumulated_submission_time=34913.289863, global_step=78587, preemption_count=0, score=34913.289863, test/accuracy=0.536700, test/loss=2.076886, test/num_examples=10000, total_duration=37725.692773, train/accuracy=0.705957, train/loss=1.173521, validation/accuracy=0.654900, validation/loss=1.418051, validation/num_examples=50000
I0302 22:02:35.692925 140380334978816 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.33661687374115, loss=2.3111979961395264
I0302 22:03:16.618848 140380326586112 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.3396332263946533, loss=2.1062021255493164
I0302 22:04:00.955054 140380334978816 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.4365346431732178, loss=2.321192979812622
I0302 22:04:46.333489 140380326586112 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.2333260774612427, loss=2.579846143722534
I0302 22:05:31.543963 140380334978816 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.3451659679412842, loss=2.259812593460083
I0302 22:06:16.568191 140380326586112 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.3016587495803833, loss=3.1877756118774414
I0302 22:07:01.512472 140380334978816 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.2948346138000488, loss=2.598890542984009
I0302 22:07:46.475936 140380326586112 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.6109789609909058, loss=2.1867446899414062
I0302 22:08:31.669732 140380334978816 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.204068899154663, loss=3.59688138961792
I0302 22:09:16.786166 140380326586112 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.3724098205566406, loss=2.162993907928467
I0302 22:09:30.378062 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:09:41.001850 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:10:04.069166 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:10:05.725128 140575196817216 submission_runner.py:411] Time since start: 38181.33s, 	Step: 79532, 	{'train/accuracy': 0.72802734375, 'train/loss': 1.105783462524414, 'validation/accuracy': 0.6602999567985535, 'validation/loss': 1.4155524969100952, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.0830442905426025, 'test/num_examples': 10000, 'score': 35333.480610609055, 'total_duration': 38181.326036691666, 'accumulated_submission_time': 35333.480610609055, 'accumulated_eval_time': 2839.6774485111237, 'accumulated_logging_time': 3.9546329975128174}
I0302 22:10:05.755052 140380334978816 logging_writer.py:48] [79532] accumulated_eval_time=2839.677449, accumulated_logging_time=3.954633, accumulated_submission_time=35333.480611, global_step=79532, preemption_count=0, score=35333.480611, test/accuracy=0.529700, test/loss=2.083044, test/num_examples=10000, total_duration=38181.326037, train/accuracy=0.728027, train/loss=1.105783, validation/accuracy=0.660300, validation/loss=1.415552, validation/num_examples=50000
I0302 22:10:33.340106 140380326586112 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.492401123046875, loss=2.1935534477233887
I0302 22:11:16.875026 140380334978816 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.3632729053497314, loss=2.160912036895752
I0302 22:12:01.808222 140380326586112 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.2243324518203735, loss=2.9617693424224854
I0302 22:12:46.811483 140380334978816 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.3577234745025635, loss=2.093641757965088
I0302 22:13:31.637023 140380326586112 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.3080576658248901, loss=2.180030345916748
I0302 22:14:16.757579 140380334978816 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.3544920682907104, loss=2.1147775650024414
I0302 22:15:01.828050 140380326586112 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.3525948524475098, loss=2.5528504848480225
I0302 22:15:46.885625 140380334978816 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.2332708835601807, loss=2.0957987308502197
I0302 22:16:32.134863 140380326586112 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4617044925689697, loss=2.302427053451538
I0302 22:17:06.037463 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:17:16.653898 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:17:40.305168 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:17:41.957511 140575196817216 submission_runner.py:411] Time since start: 38637.56s, 	Step: 80477, 	{'train/accuracy': 0.71240234375, 'train/loss': 1.1545332670211792, 'validation/accuracy': 0.6611999869346619, 'validation/loss': 1.3902668952941895, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.055467367172241, 'test/num_examples': 10000, 'score': 35753.704422950745, 'total_duration': 38637.558312892914, 'accumulated_submission_time': 35753.704422950745, 'accumulated_eval_time': 2875.596264362335, 'accumulated_logging_time': 3.994290590286255}
I0302 22:17:41.993656 140380334978816 logging_writer.py:48] [80477] accumulated_eval_time=2875.596264, accumulated_logging_time=3.994291, accumulated_submission_time=35753.704423, global_step=80477, preemption_count=0, score=35753.704423, test/accuracy=0.530800, test/loss=2.055467, test/num_examples=10000, total_duration=38637.558313, train/accuracy=0.712402, train/loss=1.154533, validation/accuracy=0.661200, validation/loss=1.390267, validation/num_examples=50000
I0302 22:17:51.537272 140380326586112 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3659875392913818, loss=2.3360161781311035
I0302 22:18:32.937283 140380334978816 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.4485886096954346, loss=2.2119951248168945
I0302 22:19:18.057184 140380326586112 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.311140537261963, loss=4.573569297790527
I0302 22:20:03.401432 140380334978816 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.33721923828125, loss=2.6025941371917725
I0302 22:20:48.365655 140380326586112 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.317750096321106, loss=2.431978702545166
I0302 22:21:33.355360 140380334978816 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.228790283203125, loss=2.976492404937744
I0302 22:22:18.347037 140380326586112 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.3763413429260254, loss=4.027026653289795
I0302 22:23:03.449033 140380334978816 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.3394602537155151, loss=2.1586906909942627
I0302 22:23:48.258385 140380326586112 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.2204030752182007, loss=3.105877637863159
I0302 22:24:33.675588 140380334978816 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.5640151500701904, loss=2.2056984901428223
I0302 22:24:42.390153 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:24:53.324831 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:25:14.828128 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:25:16.483677 140575196817216 submission_runner.py:411] Time since start: 39092.08s, 	Step: 81421, 	{'train/accuracy': 0.7125195264816284, 'train/loss': 1.1445448398590088, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.3969004154205322, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.045471668243408, 'test/num_examples': 10000, 'score': 36174.04013347626, 'total_duration': 39092.08462238312, 'accumulated_submission_time': 36174.04013347626, 'accumulated_eval_time': 2909.6886727809906, 'accumulated_logging_time': 4.0421226024627686}
I0302 22:25:16.514161 140380326586112 logging_writer.py:48] [81421] accumulated_eval_time=2909.688673, accumulated_logging_time=4.042123, accumulated_submission_time=36174.040133, global_step=81421, preemption_count=0, score=36174.040133, test/accuracy=0.535100, test/loss=2.045472, test/num_examples=10000, total_duration=39092.084622, train/accuracy=0.712520, train/loss=1.144545, validation/accuracy=0.657660, validation/loss=1.396900, validation/num_examples=50000
I0302 22:25:48.345667 140380334978816 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.275538682937622, loss=3.208329200744629
I0302 22:26:32.793358 140380326586112 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.2460929155349731, loss=3.323436737060547
I0302 22:27:17.677269 140380334978816 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.2935689687728882, loss=4.1800713539123535
I0302 22:28:02.611277 140380326586112 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.1644738912582397, loss=2.9297547340393066
I0302 22:28:47.575659 140380334978816 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.3332710266113281, loss=2.0899741649627686
I0302 22:29:32.303122 140380326586112 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.310243844985962, loss=2.4600367546081543
I0302 22:30:17.388229 140380334978816 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.4312505722045898, loss=2.0407304763793945
I0302 22:31:02.362535 140380326586112 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.3688026666641235, loss=2.091801166534424
I0302 22:31:47.483952 140380334978816 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3884270191192627, loss=2.103288173675537
I0302 22:32:16.898379 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:32:27.566352 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:32:50.906115 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:32:52.565142 140575196817216 submission_runner.py:411] Time since start: 39548.17s, 	Step: 82367, 	{'train/accuracy': 0.7277148365974426, 'train/loss': 1.075430154800415, 'validation/accuracy': 0.6658799648284912, 'validation/loss': 1.359333872795105, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.011723279953003, 'test/num_examples': 10000, 'score': 36594.36502742767, 'total_duration': 39548.166163921356, 'accumulated_submission_time': 36594.36502742767, 'accumulated_eval_time': 2945.35439991951, 'accumulated_logging_time': 4.082128286361694}
I0302 22:32:52.596555 140380326586112 logging_writer.py:48] [82367] accumulated_eval_time=2945.354400, accumulated_logging_time=4.082128, accumulated_submission_time=36594.365027, global_step=82367, preemption_count=0, score=36594.365027, test/accuracy=0.544200, test/loss=2.011723, test/num_examples=10000, total_duration=39548.166164, train/accuracy=0.727715, train/loss=1.075430, validation/accuracy=0.665880, validation/loss=1.359334, validation/num_examples=50000
I0302 22:33:06.129728 140380334978816 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.1759870052337646, loss=2.8110742568969727
I0302 22:33:47.700559 140380326586112 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.37953782081604, loss=2.037956953048706
I0302 22:34:32.549460 140380334978816 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.304918885231018, loss=2.1912338733673096
I0302 22:35:17.497970 140380326586112 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.4212208986282349, loss=2.305119752883911
I0302 22:36:02.679516 140380334978816 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.4390171766281128, loss=2.0734593868255615
I0302 22:36:47.708469 140380326586112 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2052925825119019, loss=4.058277606964111
I0302 22:37:32.512363 140380334978816 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.4674034118652344, loss=2.0158002376556396
I0302 22:38:17.608416 140380326586112 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.2670061588287354, loss=4.590040683746338
I0302 22:39:02.610414 140380334978816 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2568906545639038, loss=3.6124775409698486
I0302 22:39:47.441268 140380326586112 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.5229742527008057, loss=2.187727928161621
I0302 22:39:52.619016 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:40:03.424924 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:40:26.217914 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:40:27.877064 140575196817216 submission_runner.py:411] Time since start: 40003.48s, 	Step: 83313, 	{'train/accuracy': 0.7518945336341858, 'train/loss': 0.9783536195755005, 'validation/accuracy': 0.6640399694442749, 'validation/loss': 1.3719205856323242, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.03928279876709, 'test/num_examples': 10000, 'score': 37014.328892469406, 'total_duration': 40003.478034973145, 'accumulated_submission_time': 37014.328892469406, 'accumulated_eval_time': 2980.6113533973694, 'accumulated_logging_time': 4.123176574707031}
I0302 22:40:27.908065 140380334978816 logging_writer.py:48] [83313] accumulated_eval_time=2980.611353, accumulated_logging_time=4.123177, accumulated_submission_time=37014.328892, global_step=83313, preemption_count=0, score=37014.328892, test/accuracy=0.537300, test/loss=2.039283, test/num_examples=10000, total_duration=40003.478035, train/accuracy=0.751895, train/loss=0.978354, validation/accuracy=0.664040, validation/loss=1.371921, validation/num_examples=50000
I0302 22:41:02.900239 140380326586112 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.4508029222488403, loss=2.2038958072662354
I0302 22:41:47.436913 140380334978816 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2398488521575928, loss=3.8599839210510254
I0302 22:42:32.684857 140380326586112 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.397076964378357, loss=1.946624517440796
I0302 22:43:17.521695 140380334978816 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.3081754446029663, loss=4.3048095703125
I0302 22:44:02.219702 140380326586112 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3496609926223755, loss=2.061460018157959
I0302 22:44:47.628922 140380334978816 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.4329860210418701, loss=2.122629404067993
I0302 22:45:32.395487 140380326586112 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.3050469160079956, loss=2.7285430431365967
I0302 22:46:17.758459 140380334978816 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.3462523221969604, loss=2.0049614906311035
I0302 22:47:02.912169 140380326586112 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.4892702102661133, loss=2.122129440307617
I0302 22:47:28.123301 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:47:38.794323 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:48:02.000821 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:48:03.671671 140575196817216 submission_runner.py:411] Time since start: 40459.27s, 	Step: 84258, 	{'train/accuracy': 0.7216015458106995, 'train/loss': 1.1113862991333008, 'validation/accuracy': 0.6678599715232849, 'validation/loss': 1.3596880435943604, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.0086333751678467, 'test/num_examples': 10000, 'score': 37434.48281121254, 'total_duration': 40459.272706747055, 'accumulated_submission_time': 37434.48281121254, 'accumulated_eval_time': 3016.158703804016, 'accumulated_logging_time': 4.166561603546143}
I0302 22:48:03.717400 140380334978816 logging_writer.py:48] [84258] accumulated_eval_time=3016.158704, accumulated_logging_time=4.166562, accumulated_submission_time=37434.482811, global_step=84258, preemption_count=0, score=37434.482811, test/accuracy=0.543800, test/loss=2.008633, test/num_examples=10000, total_duration=40459.272707, train/accuracy=0.721602, train/loss=1.111386, validation/accuracy=0.667860, validation/loss=1.359688, validation/num_examples=50000
I0302 22:48:20.797316 140380326586112 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.1884989738464355, loss=3.407193422317505
I0302 22:49:03.174282 140380334978816 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.3847635984420776, loss=3.0111024379730225
I0302 22:49:48.068406 140380326586112 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.5167454481124878, loss=2.1580536365509033
I0302 22:50:33.080651 140380334978816 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.4101810455322266, loss=4.476996898651123
I0302 22:51:18.344578 140380326586112 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.467841386795044, loss=2.610431671142578
I0302 22:52:03.548217 140380334978816 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.3963404893875122, loss=1.9880938529968262
I0302 22:52:48.503866 140380326586112 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.428968071937561, loss=2.107499122619629
I0302 22:53:33.733417 140380334978816 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.2565606832504272, loss=4.199130058288574
I0302 22:54:19.073833 140380326586112 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.2790031433105469, loss=2.2555017471313477
I0302 22:55:04.241875 140380334978816 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.353703498840332, loss=2.2596209049224854
I0302 22:55:04.256105 140575196817216 spec.py:321] Evaluating on the training split.
I0302 22:55:14.837363 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 22:55:37.761383 140575196817216 spec.py:349] Evaluating on the test split.
I0302 22:55:39.429976 140575196817216 submission_runner.py:411] Time since start: 40915.03s, 	Step: 85201, 	{'train/accuracy': 0.7228124737739563, 'train/loss': 1.1117151975631714, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.3800417184829712, 'validation/num_examples': 50000, 'test/accuracy': 0.5369000434875488, 'test/loss': 2.038984775543213, 'test/num_examples': 10000, 'score': 37854.96192789078, 'total_duration': 40915.03071784973, 'accumulated_submission_time': 37854.96192789078, 'accumulated_eval_time': 3051.331242084503, 'accumulated_logging_time': 4.223360538482666}
I0302 22:55:39.466529 140380326586112 logging_writer.py:48] [85201] accumulated_eval_time=3051.331242, accumulated_logging_time=4.223361, accumulated_submission_time=37854.961928, global_step=85201, preemption_count=0, score=37854.961928, test/accuracy=0.536900, test/loss=2.038985, test/num_examples=10000, total_duration=40915.030718, train/accuracy=0.722812, train/loss=1.111715, validation/accuracy=0.665760, validation/loss=1.380042, validation/num_examples=50000
I0302 22:56:19.901359 140380334978816 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.2600489854812622, loss=4.40450382232666
I0302 22:57:04.865990 140380326586112 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.5046643018722534, loss=1.9658488035202026
I0302 22:57:50.060939 140380334978816 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.3848391771316528, loss=4.376484394073486
I0302 22:58:34.950804 140380326586112 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.4840604066848755, loss=2.1014931201934814
I0302 22:59:20.121999 140380334978816 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.4509251117706299, loss=2.094388723373413
I0302 23:00:04.899588 140380326586112 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.2884522676467896, loss=2.1897151470184326
I0302 23:00:49.990919 140380334978816 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2860381603240967, loss=4.478394508361816
I0302 23:01:35.323209 140380326586112 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.3560203313827515, loss=2.746593475341797
I0302 23:02:20.411303 140380334978816 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.336305856704712, loss=2.385737180709839
I0302 23:02:39.842272 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:02:50.588001 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:03:13.077126 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:03:14.741854 140575196817216 submission_runner.py:411] Time since start: 41370.34s, 	Step: 86145, 	{'train/accuracy': 0.7410351634025574, 'train/loss': 1.0317023992538452, 'validation/accuracy': 0.6699399948120117, 'validation/loss': 1.3505462408065796, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 1.9952341318130493, 'test/num_examples': 10000, 'score': 38275.277834415436, 'total_duration': 41370.34265089035, 'accumulated_submission_time': 38275.277834415436, 'accumulated_eval_time': 3086.22958111763, 'accumulated_logging_time': 4.271142959594727}
I0302 23:03:14.773651 140380326586112 logging_writer.py:48] [86145] accumulated_eval_time=3086.229581, accumulated_logging_time=4.271143, accumulated_submission_time=38275.277834, global_step=86145, preemption_count=0, score=38275.277834, test/accuracy=0.549400, test/loss=1.995234, test/num_examples=10000, total_duration=41370.342651, train/accuracy=0.741035, train/loss=1.031702, validation/accuracy=0.669940, validation/loss=1.350546, validation/num_examples=50000
I0302 23:03:37.052977 140380334978816 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.33671236038208, loss=1.9376519918441772
I0302 23:04:19.673032 140380326586112 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.5483200550079346, loss=2.1587750911712646
I0302 23:05:04.964543 140380334978816 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.4529619216918945, loss=2.047102689743042
I0302 23:05:49.948562 140380326586112 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.2238584756851196, loss=3.0135104656219482
I0302 23:06:35.096561 140380334978816 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2555948495864868, loss=3.2108304500579834
I0302 23:07:20.223776 140380326586112 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.3665728569030762, loss=4.105510711669922
I0302 23:08:05.235772 140380334978816 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.28630793094635, loss=3.0488498210906982
I0302 23:08:50.232033 140380326586112 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.361450433731079, loss=2.5760605335235596
I0302 23:09:35.457047 140380334978816 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.279137372970581, loss=2.5488362312316895
I0302 23:10:14.827972 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:10:25.556033 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:10:49.038506 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:10:50.695443 140575196817216 submission_runner.py:411] Time since start: 41826.30s, 	Step: 87089, 	{'train/accuracy': 0.722949206829071, 'train/loss': 1.0942314863204956, 'validation/accuracy': 0.6686999797821045, 'validation/loss': 1.3394672870635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 1.9972219467163086, 'test/num_examples': 10000, 'score': 38695.271587610245, 'total_duration': 41826.296382427216, 'accumulated_submission_time': 38695.271587610245, 'accumulated_eval_time': 3122.0959231853485, 'accumulated_logging_time': 4.3140411376953125}
I0302 23:10:50.731364 140380326586112 logging_writer.py:48] [87089] accumulated_eval_time=3122.095923, accumulated_logging_time=4.314041, accumulated_submission_time=38695.271588, global_step=87089, preemption_count=0, score=38695.271588, test/accuracy=0.544800, test/loss=1.997222, test/num_examples=10000, total_duration=41826.296382, train/accuracy=0.722949, train/loss=1.094231, validation/accuracy=0.668700, validation/loss=1.339467, validation/num_examples=50000
I0302 23:10:55.497202 140380334978816 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.4570330381393433, loss=2.1704823970794678
I0302 23:11:36.390599 140380326586112 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.3785974979400635, loss=4.628934860229492
I0302 23:12:21.337798 140380334978816 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.4103024005889893, loss=2.1789770126342773
I0302 23:13:06.656914 140380326586112 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3580788373947144, loss=2.127732515335083
I0302 23:13:51.140233 140380334978816 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.2664328813552856, loss=3.9316604137420654
I0302 23:14:36.600613 140380326586112 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.193719744682312, loss=3.6969990730285645
I0302 23:15:21.661937 140380334978816 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2906321287155151, loss=2.0897586345672607
I0302 23:16:06.564994 140380326586112 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.3451933860778809, loss=3.1459388732910156
I0302 23:16:52.009353 140380334978816 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.4088389873504639, loss=2.0329301357269287
I0302 23:17:37.091754 140380326586112 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.237886905670166, loss=3.3488357067108154
I0302 23:17:50.774302 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:18:01.470083 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:18:24.102599 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:18:25.753239 140575196817216 submission_runner.py:411] Time since start: 42281.35s, 	Step: 88032, 	{'train/accuracy': 0.7299609184265137, 'train/loss': 1.0719990730285645, 'validation/accuracy': 0.6747599840164185, 'validation/loss': 1.3282451629638672, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 1.986189603805542, 'test/num_examples': 10000, 'score': 39115.2553396225, 'total_duration': 42281.354194402695, 'accumulated_submission_time': 39115.2553396225, 'accumulated_eval_time': 3157.0737657546997, 'accumulated_logging_time': 4.3602800369262695}
I0302 23:18:25.782764 140380334978816 logging_writer.py:48] [88032] accumulated_eval_time=3157.073766, accumulated_logging_time=4.360280, accumulated_submission_time=39115.255340, global_step=88032, preemption_count=0, score=39115.255340, test/accuracy=0.549700, test/loss=1.986190, test/num_examples=10000, total_duration=42281.354194, train/accuracy=0.729961, train/loss=1.071999, validation/accuracy=0.674760, validation/loss=1.328245, validation/num_examples=50000
I0302 23:18:53.237879 140380326586112 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.2860866785049438, loss=3.2610116004943848
I0302 23:19:36.686329 140380334978816 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.4215632677078247, loss=1.9845335483551025
I0302 23:20:21.721212 140380326586112 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.5338053703308105, loss=2.041686534881592
I0302 23:21:06.698563 140380334978816 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.6259651184082031, loss=1.8927887678146362
I0302 23:21:51.975898 140380326586112 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.4932174682617188, loss=2.0217361450195312
I0302 23:22:37.014845 140380334978816 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.4281405210494995, loss=2.029102325439453
I0302 23:23:22.110648 140380326586112 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.3409173488616943, loss=2.4051055908203125
I0302 23:24:06.903550 140380334978816 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3417680263519287, loss=1.988638997077942
I0302 23:24:52.103126 140380326586112 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.3023937940597534, loss=2.6200475692749023
I0302 23:25:25.975658 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:25:36.752808 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:26:00.458505 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:26:02.125343 140575196817216 submission_runner.py:411] Time since start: 42737.73s, 	Step: 88977, 	{'train/accuracy': 0.7388085722923279, 'train/loss': 1.0372790098190308, 'validation/accuracy': 0.6735000014305115, 'validation/loss': 1.3296064138412476, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.0001771450042725, 'test/num_examples': 10000, 'score': 39535.3887925148, 'total_duration': 42737.726367235184, 'accumulated_submission_time': 39535.3887925148, 'accumulated_eval_time': 3193.2224068641663, 'accumulated_logging_time': 4.400323152542114}
I0302 23:26:02.158051 140380334978816 logging_writer.py:48] [88977] accumulated_eval_time=3193.222407, accumulated_logging_time=4.400323, accumulated_submission_time=39535.388793, global_step=88977, preemption_count=0, score=39535.388793, test/accuracy=0.547100, test/loss=2.000177, test/num_examples=10000, total_duration=42737.726367, train/accuracy=0.738809, train/loss=1.037279, validation/accuracy=0.673500, validation/loss=1.329606, validation/num_examples=50000
I0302 23:26:11.710174 140380326586112 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.3803726434707642, loss=2.385929584503174
I0302 23:26:53.030469 140380334978816 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.3512784242630005, loss=4.26847505569458
I0302 23:27:38.150360 140380326586112 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.3464001417160034, loss=3.9197640419006348
I0302 23:28:23.261682 140380334978816 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.3472548723220825, loss=2.8472390174865723
I0302 23:29:07.960559 140380326586112 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.1731170415878296, loss=3.636723041534424
I0302 23:29:52.813590 140380334978816 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.5204254388809204, loss=2.0468554496765137
I0302 23:30:37.711667 140380326586112 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.3377066850662231, loss=4.1507062911987305
I0302 23:31:22.822898 140380334978816 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.4793392419815063, loss=2.0293569564819336
I0302 23:32:07.869627 140380326586112 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.3736631870269775, loss=2.26254940032959
I0302 23:32:52.899851 140380334978816 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.4143791198730469, loss=2.056652069091797
I0302 23:33:02.441703 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:33:13.260718 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:33:36.436137 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:33:38.078520 140575196817216 submission_runner.py:411] Time since start: 43193.68s, 	Step: 89923, 	{'train/accuracy': 0.7599999904632568, 'train/loss': 0.9553410410881042, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.3442224264144897, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 1.9942545890808105, 'test/num_examples': 10000, 'score': 39955.61276316643, 'total_duration': 43193.67957997322, 'accumulated_submission_time': 39955.61276316643, 'accumulated_eval_time': 3228.858241558075, 'accumulated_logging_time': 4.443213939666748}
I0302 23:33:38.107606 140380326586112 logging_writer.py:48] [89923] accumulated_eval_time=3228.858242, accumulated_logging_time=4.443214, accumulated_submission_time=39955.612763, global_step=89923, preemption_count=0, score=39955.612763, test/accuracy=0.546600, test/loss=1.994255, test/num_examples=10000, total_duration=43193.679580, train/accuracy=0.760000, train/loss=0.955341, validation/accuracy=0.669760, validation/loss=1.344222, validation/num_examples=50000
I0302 23:34:09.140816 140380334978816 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.327441692352295, loss=2.04198956489563
I0302 23:34:53.237902 140380326586112 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.40302312374115, loss=2.053009033203125
I0302 23:35:38.097301 140380334978816 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2418042421340942, loss=2.985646963119507
I0302 23:36:23.574787 140380326586112 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.466956615447998, loss=2.2819275856018066
I0302 23:37:08.422507 140380334978816 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.5357894897460938, loss=2.1294610500335693
I0302 23:37:53.506975 140380326586112 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.2936427593231201, loss=4.610142230987549
I0302 23:38:38.594393 140380334978816 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.3758026361465454, loss=2.0369505882263184
I0302 23:39:23.693662 140380326586112 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.3834609985351562, loss=1.9362883567810059
I0302 23:40:08.703052 140380334978816 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.3887128829956055, loss=1.9847499132156372
I0302 23:40:38.358104 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:40:48.836568 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:41:11.473234 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:41:13.138726 140575196817216 submission_runner.py:411] Time since start: 43648.74s, 	Step: 90868, 	{'train/accuracy': 0.7295898199081421, 'train/loss': 1.097119688987732, 'validation/accuracy': 0.6711399555206299, 'validation/loss': 1.3498778343200684, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 2.0084035396575928, 'test/num_examples': 10000, 'score': 40375.80407691002, 'total_duration': 43648.7395863533, 'accumulated_submission_time': 40375.80407691002, 'accumulated_eval_time': 3263.637674331665, 'accumulated_logging_time': 4.48250937461853}
I0302 23:41:13.176535 140380326586112 logging_writer.py:48] [90868] accumulated_eval_time=3263.637674, accumulated_logging_time=4.482509, accumulated_submission_time=40375.804077, global_step=90868, preemption_count=0, score=40375.804077, test/accuracy=0.546300, test/loss=2.008404, test/num_examples=10000, total_duration=43648.739586, train/accuracy=0.729590, train/loss=1.097120, validation/accuracy=0.671140, validation/loss=1.349878, validation/num_examples=50000
I0302 23:41:26.314904 140380334978816 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.387664794921875, loss=2.815723180770874
I0302 23:42:08.177808 140380326586112 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.5695960521697998, loss=2.1429364681243896
I0302 23:42:52.964067 140380334978816 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.5637357234954834, loss=1.980637788772583
I0302 23:43:38.095682 140380326586112 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.325281023979187, loss=4.501181125640869
I0302 23:44:22.885697 140380334978816 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.3834069967269897, loss=3.7152867317199707
I0302 23:45:08.132478 140380326586112 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.548680305480957, loss=1.9926502704620361
I0302 23:45:53.198150 140380334978816 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.40252685546875, loss=4.381402015686035
I0302 23:46:38.204587 140380326586112 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.4590836763381958, loss=2.0901787281036377
I0302 23:47:23.224858 140380334978816 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.4275215864181519, loss=4.543072700500488
I0302 23:48:08.645039 140380326586112 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.3743896484375, loss=4.5460896492004395
I0302 23:48:13.315935 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:48:23.972597 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:48:46.938990 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:48:48.602015 140575196817216 submission_runner.py:411] Time since start: 44104.20s, 	Step: 91812, 	{'train/accuracy': 0.7383593320846558, 'train/loss': 1.0403574705123901, 'validation/accuracy': 0.6772399544715881, 'validation/loss': 1.3202310800552368, 'validation/num_examples': 50000, 'test/accuracy': 0.5585000514984131, 'test/loss': 1.9671977758407593, 'test/num_examples': 10000, 'score': 40795.882858514786, 'total_duration': 44104.20302581787, 'accumulated_submission_time': 40795.882858514786, 'accumulated_eval_time': 3298.9227085113525, 'accumulated_logging_time': 4.531588315963745}
I0302 23:48:48.633255 140380334978816 logging_writer.py:48] [91812] accumulated_eval_time=3298.922709, accumulated_logging_time=4.531588, accumulated_submission_time=40795.882859, global_step=91812, preemption_count=0, score=40795.882859, test/accuracy=0.558500, test/loss=1.967198, test/num_examples=10000, total_duration=44104.203026, train/accuracy=0.738359, train/loss=1.040357, validation/accuracy=0.677240, validation/loss=1.320231, validation/num_examples=50000
I0302 23:49:24.049943 140380326586112 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.3278981447219849, loss=2.5667285919189453
I0302 23:50:08.947861 140380334978816 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.4561725854873657, loss=3.922640323638916
I0302 23:50:54.193643 140380326586112 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.5380076169967651, loss=2.117218494415283
I0302 23:51:39.132716 140380334978816 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.3625060319900513, loss=4.219235897064209
I0302 23:52:24.400629 140380326586112 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.4859145879745483, loss=2.0166587829589844
I0302 23:53:09.501882 140380334978816 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.3629555702209473, loss=2.2479631900787354
I0302 23:53:54.367834 140380326586112 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.2913669347763062, loss=2.8616526126861572
I0302 23:54:39.770316 140380334978816 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.4619046449661255, loss=2.0770952701568604
I0302 23:55:24.694190 140380326586112 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.5624120235443115, loss=2.1882150173187256
I0302 23:55:48.617336 140575196817216 spec.py:321] Evaluating on the training split.
I0302 23:55:59.195986 140575196817216 spec.py:333] Evaluating on the validation split.
I0302 23:56:20.577582 140575196817216 spec.py:349] Evaluating on the test split.
I0302 23:56:22.235603 140575196817216 submission_runner.py:411] Time since start: 44557.84s, 	Step: 92755, 	{'train/accuracy': 0.7471874952316284, 'train/loss': 1.026834487915039, 'validation/accuracy': 0.6764000058174133, 'validation/loss': 1.334289312362671, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.000419855117798, 'test/num_examples': 10000, 'score': 41215.80678701401, 'total_duration': 44557.836663246155, 'accumulated_submission_time': 41215.80678701401, 'accumulated_eval_time': 3332.5399737358093, 'accumulated_logging_time': 4.574221849441528}
I0302 23:56:22.274179 140380334978816 logging_writer.py:48] [92755] accumulated_eval_time=3332.539974, accumulated_logging_time=4.574222, accumulated_submission_time=41215.806787, global_step=92755, preemption_count=0, score=41215.806787, test/accuracy=0.550600, test/loss=2.000420, test/num_examples=10000, total_duration=44557.836663, train/accuracy=0.747187, train/loss=1.026834, validation/accuracy=0.676400, validation/loss=1.334289, validation/num_examples=50000
I0302 23:56:40.589627 140380326586112 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.394161581993103, loss=2.8981502056121826
I0302 23:57:23.726832 140380334978816 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.5408192873001099, loss=2.1018269062042236
I0302 23:58:08.648131 140380326586112 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.4890952110290527, loss=4.3245038986206055
I0302 23:58:54.040499 140380334978816 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.5843653678894043, loss=2.0488474369049072
I0302 23:59:39.095323 140380326586112 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.4131661653518677, loss=2.0639166831970215
I0303 00:00:24.091532 140380334978816 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.41681969165802, loss=2.153080463409424
I0303 00:01:09.031374 140380326586112 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.5395890474319458, loss=2.123905897140503
I0303 00:01:53.926989 140380334978816 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.385365605354309, loss=2.4844229221343994
I0303 00:02:39.099637 140380326586112 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.2443692684173584, loss=3.1207022666931152
I0303 00:03:22.362725 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:03:33.188268 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:03:56.283679 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:03:57.938596 140575196817216 submission_runner.py:411] Time since start: 45013.54s, 	Step: 93698, 	{'train/accuracy': 0.7383398413658142, 'train/loss': 1.0327916145324707, 'validation/accuracy': 0.678119957447052, 'validation/loss': 1.306376576423645, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 1.944562554359436, 'test/num_examples': 10000, 'score': 41635.83515667915, 'total_duration': 45013.53951334953, 'accumulated_submission_time': 41635.83515667915, 'accumulated_eval_time': 3368.114696741104, 'accumulated_logging_time': 4.624174118041992}
I0303 00:03:57.972328 140380334978816 logging_writer.py:48] [93698] accumulated_eval_time=3368.114697, accumulated_logging_time=4.624174, accumulated_submission_time=41635.835157, global_step=93698, preemption_count=0, score=41635.835157, test/accuracy=0.554300, test/loss=1.944563, test/num_examples=10000, total_duration=45013.539513, train/accuracy=0.738340, train/loss=1.032792, validation/accuracy=0.678120, validation/loss=1.306377, validation/num_examples=50000
I0303 00:03:59.176379 140380326586112 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.34502112865448, loss=3.54451847076416
I0303 00:04:39.275015 140380334978816 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.5655664205551147, loss=2.0215530395507812
I0303 00:05:24.205343 140380326586112 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.347143530845642, loss=2.1224122047424316
I0303 00:06:09.174880 140380334978816 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.3627933263778687, loss=2.6650898456573486
I0303 00:06:54.660525 140380326586112 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.3900957107543945, loss=4.025813102722168
I0303 00:07:39.540387 140380334978816 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.4816501140594482, loss=2.405705690383911
I0303 00:08:24.820623 140380326586112 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.393437147140503, loss=1.9757180213928223
I0303 00:09:09.898889 140380334978816 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.4373418092727661, loss=2.0306036472320557
I0303 00:09:54.730971 140380326586112 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.6213431358337402, loss=1.9845905303955078
I0303 00:10:39.836844 140380334978816 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.4663434028625488, loss=3.9414916038513184
I0303 00:10:58.021294 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:11:08.714783 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:11:32.947756 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:11:34.607999 140575196817216 submission_runner.py:411] Time since start: 45470.21s, 	Step: 94642, 	{'train/accuracy': 0.7393554449081421, 'train/loss': 1.033813714981079, 'validation/accuracy': 0.6789999604225159, 'validation/loss': 1.2958242893218994, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 1.9380820989608765, 'test/num_examples': 10000, 'score': 42055.82461929321, 'total_duration': 45470.208986759186, 'accumulated_submission_time': 42055.82461929321, 'accumulated_eval_time': 3404.7003223896027, 'accumulated_logging_time': 4.667530536651611}
I0303 00:11:34.641916 140380326586112 logging_writer.py:48] [94642] accumulated_eval_time=3404.700322, accumulated_logging_time=4.667531, accumulated_submission_time=42055.824619, global_step=94642, preemption_count=0, score=42055.824619, test/accuracy=0.554100, test/loss=1.938082, test/num_examples=10000, total_duration=45470.208987, train/accuracy=0.739355, train/loss=1.033814, validation/accuracy=0.679000, validation/loss=1.295824, validation/num_examples=50000
I0303 00:11:58.109136 140380334978816 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.5249348878860474, loss=4.51827335357666
I0303 00:12:40.771255 140380326586112 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4767011404037476, loss=2.0268425941467285
I0303 00:13:25.745843 140380334978816 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.3939532041549683, loss=3.5857386589050293
I0303 00:14:10.643716 140380326586112 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.3811715841293335, loss=2.9804935455322266
I0303 00:14:55.673214 140380334978816 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.4596744775772095, loss=3.848379373550415
I0303 00:15:40.717051 140380326586112 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.362119436264038, loss=1.969637155532837
I0303 00:16:25.744720 140380334978816 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.4325889348983765, loss=2.0656216144561768
I0303 00:17:10.672286 140380326586112 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.5142496824264526, loss=2.040069341659546
I0303 00:17:55.563393 140380334978816 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.664202332496643, loss=1.9317628145217896
I0303 00:18:34.833668 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:18:45.464450 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:19:08.795966 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:19:10.460317 140575196817216 submission_runner.py:411] Time since start: 45926.06s, 	Step: 95589, 	{'train/accuracy': 0.749804675579071, 'train/loss': 0.9888725280761719, 'validation/accuracy': 0.6826800107955933, 'validation/loss': 1.2907994985580444, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 1.9421786069869995, 'test/num_examples': 10000, 'score': 42475.95677399635, 'total_duration': 45926.06120347977, 'accumulated_submission_time': 42475.95677399635, 'accumulated_eval_time': 3440.3258113861084, 'accumulated_logging_time': 4.7116851806640625}
I0303 00:19:10.498943 140380326586112 logging_writer.py:48] [95589] accumulated_eval_time=3440.325811, accumulated_logging_time=4.711685, accumulated_submission_time=42475.956774, global_step=95589, preemption_count=0, score=42475.956774, test/accuracy=0.552300, test/loss=1.942179, test/num_examples=10000, total_duration=45926.061203, train/accuracy=0.749805, train/loss=0.988873, validation/accuracy=0.682680, validation/loss=1.290799, validation/num_examples=50000
I0303 00:19:15.284126 140380334978816 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.4957307577133179, loss=2.4954090118408203
I0303 00:19:55.904773 140380326586112 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3947340250015259, loss=2.601081371307373
I0303 00:20:40.802648 140380334978816 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.4575169086456299, loss=2.1914565563201904
I0303 00:21:25.761980 140380326586112 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.5550434589385986, loss=2.011101245880127
I0303 00:22:10.922130 140380334978816 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.3168375492095947, loss=2.288011312484741
I0303 00:22:56.235012 140380326586112 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.4055309295654297, loss=2.035538673400879
I0303 00:23:41.358103 140380334978816 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.388979434967041, loss=3.1214191913604736
I0303 00:24:26.400847 140380326586112 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.421761393547058, loss=2.342952251434326
I0303 00:25:11.867483 140380334978816 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.450126051902771, loss=4.414407253265381
I0303 00:25:56.708419 140380326586112 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.6135088205337524, loss=4.478174686431885
I0303 00:26:10.547649 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:26:21.401001 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:26:45.382199 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:26:47.040755 140575196817216 submission_runner.py:411] Time since start: 46382.64s, 	Step: 96532, 	{'train/accuracy': 0.768261730670929, 'train/loss': 0.9298219680786133, 'validation/accuracy': 0.6815999746322632, 'validation/loss': 1.3072997331619263, 'validation/num_examples': 50000, 'test/accuracy': 0.5608000159263611, 'test/loss': 1.9497448205947876, 'test/num_examples': 10000, 'score': 42895.94270777702, 'total_duration': 46382.64171719551, 'accumulated_submission_time': 42895.94270777702, 'accumulated_eval_time': 3476.817836523056, 'accumulated_logging_time': 4.763655424118042}
I0303 00:26:47.074462 140380334978816 logging_writer.py:48] [96532] accumulated_eval_time=3476.817837, accumulated_logging_time=4.763655, accumulated_submission_time=42895.942708, global_step=96532, preemption_count=0, score=42895.942708, test/accuracy=0.560800, test/loss=1.949745, test/num_examples=10000, total_duration=46382.641717, train/accuracy=0.768262, train/loss=0.929822, validation/accuracy=0.681600, validation/loss=1.307300, validation/num_examples=50000
I0303 00:27:14.506603 140380326586112 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.4056094884872437, loss=3.7581872940063477
I0303 00:27:58.515687 140380334978816 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.3442121744155884, loss=2.8510422706604004
I0303 00:28:43.609336 140380326586112 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.4825114011764526, loss=2.0021071434020996
I0303 00:29:28.830940 140380334978816 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.3392027616500854, loss=4.064242839813232
I0303 00:30:13.810818 140380326586112 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.4256892204284668, loss=1.9698765277862549
I0303 00:30:58.793595 140380334978816 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.4006516933441162, loss=2.1085593700408936
I0303 00:31:43.711173 140380326586112 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.5306483507156372, loss=2.2183589935302734
I0303 00:32:28.922696 140380334978816 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.485439419746399, loss=2.642871141433716
I0303 00:33:14.250020 140380326586112 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.3699918985366821, loss=3.092705011367798
I0303 00:33:47.420171 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:33:58.216193 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:34:20.974207 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:34:22.638048 140575196817216 submission_runner.py:411] Time since start: 46838.24s, 	Step: 97475, 	{'train/accuracy': 0.7375390529632568, 'train/loss': 1.017408013343811, 'validation/accuracy': 0.6825000047683716, 'validation/loss': 1.2762969732284546, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 1.9167892932891846, 'test/num_examples': 10000, 'score': 43316.22964668274, 'total_duration': 46838.239077568054, 'accumulated_submission_time': 43316.22964668274, 'accumulated_eval_time': 3512.03467297554, 'accumulated_logging_time': 4.808404445648193}
I0303 00:34:22.675147 140380334978816 logging_writer.py:48] [97475] accumulated_eval_time=3512.034673, accumulated_logging_time=4.808404, accumulated_submission_time=43316.229647, global_step=97475, preemption_count=0, score=43316.229647, test/accuracy=0.559200, test/loss=1.916789, test/num_examples=10000, total_duration=46838.239078, train/accuracy=0.737539, train/loss=1.017408, validation/accuracy=0.682500, validation/loss=1.276297, validation/num_examples=50000
I0303 00:34:33.023044 140380326586112 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.5331958532333374, loss=4.435922622680664
I0303 00:35:14.857015 140380334978816 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.5121980905532837, loss=1.9501291513442993
I0303 00:35:59.693541 140380326586112 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.4221271276474, loss=1.9837967157363892
I0303 00:36:45.156278 140380334978816 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.369576096534729, loss=4.133258819580078
I0303 00:37:30.236203 140380326586112 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.4088767766952515, loss=2.0769643783569336
I0303 00:38:15.314835 140380334978816 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.5186518430709839, loss=2.118607997894287
I0303 00:39:00.550124 140380326586112 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.4826791286468506, loss=2.1813509464263916
I0303 00:39:46.037716 140380334978816 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.4125722646713257, loss=3.8723864555358887
I0303 00:40:31.327718 140380326586112 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.436893105506897, loss=2.710409164428711
I0303 00:41:16.397827 140380334978816 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.3256502151489258, loss=3.415482521057129
I0303 00:41:22.870466 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:41:33.668322 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:41:57.682015 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:41:59.339684 140575196817216 submission_runner.py:411] Time since start: 47294.94s, 	Step: 98416, 	{'train/accuracy': 0.7465234398841858, 'train/loss': 1.0131640434265137, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.2908909320831299, 'validation/num_examples': 50000, 'test/accuracy': 0.5608000159263611, 'test/loss': 1.9235416650772095, 'test/num_examples': 10000, 'score': 43736.36538696289, 'total_duration': 47294.940781116486, 'accumulated_submission_time': 43736.36538696289, 'accumulated_eval_time': 3548.5029113292694, 'accumulated_logging_time': 4.856558799743652}
I0303 00:41:59.372681 140380326586112 logging_writer.py:48] [98416] accumulated_eval_time=3548.502911, accumulated_logging_time=4.856559, accumulated_submission_time=43736.365387, global_step=98416, preemption_count=0, score=43736.365387, test/accuracy=0.560800, test/loss=1.923542, test/num_examples=10000, total_duration=47294.940781, train/accuracy=0.746523, train/loss=1.013164, validation/accuracy=0.684160, validation/loss=1.290891, validation/num_examples=50000
I0303 00:42:33.157832 140380334978816 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.4875504970550537, loss=2.077514410018921
I0303 00:43:18.407426 140380326586112 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.555309772491455, loss=2.0744950771331787
I0303 00:44:03.787373 140380334978816 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.3996539115905762, loss=2.7555761337280273
I0303 00:44:49.073393 140380326586112 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.4047508239746094, loss=2.228227138519287
I0303 00:45:34.374899 140380334978816 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.6434556245803833, loss=1.851033329963684
I0303 00:46:19.758761 140380326586112 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.4779423475265503, loss=1.905880331993103
I0303 00:47:04.775716 140380334978816 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.3030810356140137, loss=3.618166446685791
I0303 00:47:49.791124 140380326586112 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.4200266599655151, loss=2.5278775691986084
I0303 00:48:34.772616 140380334978816 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.690021276473999, loss=3.48777437210083
I0303 00:48:59.612219 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:49:10.354649 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:49:33.782547 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:49:35.445878 140575196817216 submission_runner.py:411] Time since start: 47751.05s, 	Step: 99357, 	{'train/accuracy': 0.755664050579071, 'train/loss': 0.9562978148460388, 'validation/accuracy': 0.6837199926376343, 'validation/loss': 1.2733529806137085, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 1.913759469985962, 'test/num_examples': 10000, 'score': 44156.54634976387, 'total_duration': 47751.04694914818, 'accumulated_submission_time': 44156.54634976387, 'accumulated_eval_time': 3584.3355853557587, 'accumulated_logging_time': 4.8991899490356445}
I0303 00:49:35.475937 140380326586112 logging_writer.py:48] [99357] accumulated_eval_time=3584.335585, accumulated_logging_time=4.899190, accumulated_submission_time=44156.546350, global_step=99357, preemption_count=0, score=44156.546350, test/accuracy=0.559100, test/loss=1.913759, test/num_examples=10000, total_duration=47751.046949, train/accuracy=0.755664, train/loss=0.956298, validation/accuracy=0.683720, validation/loss=1.273353, validation/num_examples=50000
I0303 00:49:52.966956 140380334978816 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.5290449857711792, loss=2.0120885372161865
I0303 00:50:35.202542 140380326586112 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.3073800802230835, loss=2.2210326194763184
I0303 00:51:20.214165 140380334978816 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.4456322193145752, loss=2.339064836502075
I0303 00:52:05.292897 140380326586112 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.4844921827316284, loss=3.2416155338287354
I0303 00:52:50.204754 140380334978816 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.4237394332885742, loss=1.880055546760559
I0303 00:53:35.494245 140380326586112 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.3352196216583252, loss=3.79023814201355
I0303 00:54:20.207254 140380334978816 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.4350969791412354, loss=1.920259714126587
I0303 00:55:05.316330 140380326586112 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.435206413269043, loss=2.3092758655548096
I0303 00:55:50.432418 140380334978816 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.4103657007217407, loss=4.283719062805176
I0303 00:56:35.423125 140380326586112 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.4419857263565063, loss=4.0428643226623535
I0303 00:56:35.561957 140575196817216 spec.py:321] Evaluating on the training split.
I0303 00:56:46.408545 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 00:57:09.832014 140575196817216 spec.py:349] Evaluating on the test split.
I0303 00:57:11.495277 140575196817216 submission_runner.py:411] Time since start: 48207.10s, 	Step: 100302, 	{'train/accuracy': 0.7462109327316284, 'train/loss': 0.9947492480278015, 'validation/accuracy': 0.6892799735069275, 'validation/loss': 1.256700873374939, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9041144847869873, 'test/num_examples': 10000, 'score': 44576.57260489464, 'total_duration': 48207.096264600754, 'accumulated_submission_time': 44576.57260489464, 'accumulated_eval_time': 3620.267826318741, 'accumulated_logging_time': 4.93875527381897}
I0303 00:57:11.533708 140380334978816 logging_writer.py:48] [100302] accumulated_eval_time=3620.267826, accumulated_logging_time=4.938755, accumulated_submission_time=44576.572605, global_step=100302, preemption_count=0, score=44576.572605, test/accuracy=0.566100, test/loss=1.904114, test/num_examples=10000, total_duration=48207.096265, train/accuracy=0.746211, train/loss=0.994749, validation/accuracy=0.689280, validation/loss=1.256701, validation/num_examples=50000
I0303 00:57:51.236893 140380326586112 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.4547473192214966, loss=2.7436652183532715
I0303 00:58:35.886661 140380334978816 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.2766480445861816, loss=3.3809661865234375
I0303 00:59:21.262709 140380326586112 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.4891822338104248, loss=2.810840606689453
I0303 01:00:06.033432 140380334978816 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.5148781538009644, loss=2.0024807453155518
I0303 01:00:51.199805 140380326586112 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.4812390804290771, loss=4.330286502838135
I0303 01:01:36.289603 140380334978816 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3621675968170166, loss=2.9910147190093994
I0303 01:02:21.022016 140380326586112 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.7639590501785278, loss=2.024245262145996
I0303 01:03:06.158775 140380334978816 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.418953537940979, loss=3.098447799682617
I0303 01:03:51.219842 140380326586112 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.450007438659668, loss=2.2100908756256104
I0303 01:04:11.498493 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:04:22.149360 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:04:45.461132 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:04:47.108740 140575196817216 submission_runner.py:411] Time since start: 48662.71s, 	Step: 101247, 	{'train/accuracy': 0.7510156035423279, 'train/loss': 0.9671342372894287, 'validation/accuracy': 0.6897599697113037, 'validation/loss': 1.2461247444152832, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 1.881093978881836, 'test/num_examples': 10000, 'score': 44996.47789621353, 'total_duration': 48662.70986151695, 'accumulated_submission_time': 44996.47789621353, 'accumulated_eval_time': 3655.877145767212, 'accumulated_logging_time': 4.987702131271362}
I0303 01:04:47.143144 140380334978816 logging_writer.py:48] [101247] accumulated_eval_time=3655.877146, accumulated_logging_time=4.987702, accumulated_submission_time=44996.477896, global_step=101247, preemption_count=0, score=44996.477896, test/accuracy=0.568500, test/loss=1.881094, test/num_examples=10000, total_duration=48662.709862, train/accuracy=0.751016, train/loss=0.967134, validation/accuracy=0.689760, validation/loss=1.246125, validation/num_examples=50000
I0303 01:05:08.610633 140380326586112 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.5094066858291626, loss=2.2103097438812256
I0303 01:05:51.843078 140380334978816 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.450601577758789, loss=3.2964425086975098
I0303 01:06:36.758907 140380326586112 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.7210952043533325, loss=1.8431980609893799
I0303 01:07:22.031085 140380334978816 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.712263822555542, loss=2.957087278366089
I0303 01:08:06.943450 140380326586112 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.5262799263000488, loss=1.9567662477493286
I0303 01:08:51.900519 140380334978816 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.4685720205307007, loss=2.4952991008758545
I0303 01:09:36.961631 140380326586112 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.568698525428772, loss=1.9749784469604492
I0303 01:10:21.954307 140380334978816 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.523757815361023, loss=1.9974461793899536
I0303 01:11:07.026416 140380326586112 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.6919538974761963, loss=1.8482650518417358
I0303 01:11:47.304339 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:11:58.117110 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:12:21.993168 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:12:23.661442 140575196817216 submission_runner.py:411] Time since start: 49119.26s, 	Step: 102191, 	{'train/accuracy': 0.7568163871765137, 'train/loss': 0.9741609692573547, 'validation/accuracy': 0.6883999705314636, 'validation/loss': 1.270899772644043, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.915746808052063, 'test/num_examples': 10000, 'score': 45416.581154346466, 'total_duration': 49119.262209415436, 'accumulated_submission_time': 45416.581154346466, 'accumulated_eval_time': 3692.2329466342926, 'accumulated_logging_time': 5.0316221714019775}
I0303 01:12:23.703766 140380334978816 logging_writer.py:48] [102191] accumulated_eval_time=3692.232947, accumulated_logging_time=5.031622, accumulated_submission_time=45416.581154, global_step=102191, preemption_count=0, score=45416.581154, test/accuracy=0.563400, test/loss=1.915747, test/num_examples=10000, total_duration=49119.262209, train/accuracy=0.756816, train/loss=0.974161, validation/accuracy=0.688400, validation/loss=1.270900, validation/num_examples=50000
I0303 01:12:27.686128 140380326586112 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4911465644836426, loss=4.329056739807129
I0303 01:13:08.410536 140380334978816 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.6325838565826416, loss=1.9868537187576294
I0303 01:13:53.292957 140380326586112 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.3594430685043335, loss=3.6668753623962402
I0303 01:14:38.457075 140380334978816 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.5665379762649536, loss=1.8641715049743652
I0303 01:15:23.771316 140380326586112 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.6191402673721313, loss=4.317015171051025
I0303 01:16:08.652174 140380334978816 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.7884584665298462, loss=3.9776999950408936
I0303 01:16:53.851164 140380326586112 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.591838002204895, loss=2.0175461769104004
I0303 01:17:38.705233 140380334978816 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.5440417528152466, loss=4.186242580413818
I0303 01:18:23.793393 140380326586112 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.552472710609436, loss=2.400343418121338
I0303 01:19:08.774086 140380334978816 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.495456576347351, loss=2.203709363937378
I0303 01:19:23.743845 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:19:34.346390 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:19:58.115464 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:19:59.773331 140575196817216 submission_runner.py:411] Time since start: 49575.37s, 	Step: 103135, 	{'train/accuracy': 0.77490234375, 'train/loss': 0.9026257395744324, 'validation/accuracy': 0.6901199817657471, 'validation/loss': 1.262291669845581, 'validation/num_examples': 50000, 'test/accuracy': 0.5682000517845154, 'test/loss': 1.8965471982955933, 'test/num_examples': 10000, 'score': 45836.55875372887, 'total_duration': 49575.37422776222, 'accumulated_submission_time': 45836.55875372887, 'accumulated_eval_time': 3728.261288166046, 'accumulated_logging_time': 5.086937427520752}
I0303 01:19:59.808300 140380326586112 logging_writer.py:48] [103135] accumulated_eval_time=3728.261288, accumulated_logging_time=5.086937, accumulated_submission_time=45836.558754, global_step=103135, preemption_count=0, score=45836.558754, test/accuracy=0.568200, test/loss=1.896547, test/num_examples=10000, total_duration=49575.374228, train/accuracy=0.774902, train/loss=0.902626, validation/accuracy=0.690120, validation/loss=1.262292, validation/num_examples=50000
I0303 01:20:26.065325 140380334978816 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.4860687255859375, loss=1.8073933124542236
I0303 01:21:09.394764 140380326586112 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.6007989645004272, loss=1.9030390977859497
I0303 01:21:54.743771 140380334978816 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.594101905822754, loss=1.8635724782943726
I0303 01:22:39.726345 140380326586112 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.4667975902557373, loss=2.73016095161438
I0303 01:23:24.631601 140380334978816 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.4554229974746704, loss=1.8075261116027832
I0303 01:24:09.928163 140380326586112 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.6524959802627563, loss=2.006624221801758
I0303 01:24:54.541266 140380334978816 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.5944679975509644, loss=3.8793694972991943
I0303 01:25:40.067523 140380326586112 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.4092077016830444, loss=3.0517053604125977
I0303 01:26:25.209490 140380334978816 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.6797176599502563, loss=2.088411331176758
I0303 01:27:00.097634 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:27:10.910031 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:27:35.083754 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:27:36.743301 140575196817216 submission_runner.py:411] Time since start: 50032.34s, 	Step: 104079, 	{'train/accuracy': 0.7558984160423279, 'train/loss': 0.9612823128700256, 'validation/accuracy': 0.6957399845123291, 'validation/loss': 1.2298921346664429, 'validation/num_examples': 50000, 'test/accuracy': 0.5656000375747681, 'test/loss': 1.8726284503936768, 'test/num_examples': 10000, 'score': 46256.78787302971, 'total_duration': 50032.344173669815, 'accumulated_submission_time': 46256.78787302971, 'accumulated_eval_time': 3764.9057574272156, 'accumulated_logging_time': 5.132697582244873}
I0303 01:27:36.781356 140380326586112 logging_writer.py:48] [104079] accumulated_eval_time=3764.905757, accumulated_logging_time=5.132698, accumulated_submission_time=46256.787873, global_step=104079, preemption_count=0, score=46256.787873, test/accuracy=0.565600, test/loss=1.872628, test/num_examples=10000, total_duration=50032.344174, train/accuracy=0.755898, train/loss=0.961282, validation/accuracy=0.695740, validation/loss=1.229892, validation/num_examples=50000
I0303 01:27:45.531590 140380334978816 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.3515700101852417, loss=3.6695761680603027
I0303 01:28:26.731411 140380326586112 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.4234954118728638, loss=2.636733293533325
I0303 01:29:12.367434 140380334978816 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.4302761554718018, loss=2.9456138610839844
I0303 01:29:57.396075 140380326586112 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.584465742111206, loss=1.9516217708587646
I0303 01:30:42.429392 140380334978816 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.6144862174987793, loss=1.9766075611114502
I0303 01:31:27.575383 140380326586112 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.6850123405456543, loss=2.3153882026672363
I0303 01:32:12.580779 140380334978816 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.5172252655029297, loss=2.881401777267456
I0303 01:32:57.672183 140380326586112 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.9541102647781372, loss=1.821879506111145
I0303 01:33:42.498724 140380334978816 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.5125099420547485, loss=2.1130030155181885
I0303 01:34:27.525898 140380326586112 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.5529156923294067, loss=4.38203239440918
I0303 01:34:37.160782 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:34:48.019910 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:35:10.477070 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:35:12.130524 140575196817216 submission_runner.py:411] Time since start: 50487.73s, 	Step: 105023, 	{'train/accuracy': 0.760546863079071, 'train/loss': 0.9362866282463074, 'validation/accuracy': 0.6957399845123291, 'validation/loss': 1.2292063236236572, 'validation/num_examples': 50000, 'test/accuracy': 0.5689000487327576, 'test/loss': 1.8730270862579346, 'test/num_examples': 10000, 'score': 46677.106143713, 'total_duration': 50487.73134660721, 'accumulated_submission_time': 46677.106143713, 'accumulated_eval_time': 3799.874258995056, 'accumulated_logging_time': 5.182331800460815}
I0303 01:35:12.161983 140380334978816 logging_writer.py:48] [105023] accumulated_eval_time=3799.874259, accumulated_logging_time=5.182332, accumulated_submission_time=46677.106144, global_step=105023, preemption_count=0, score=46677.106144, test/accuracy=0.568900, test/loss=1.873027, test/num_examples=10000, total_duration=50487.731347, train/accuracy=0.760547, train/loss=0.936287, validation/accuracy=0.695740, validation/loss=1.229206, validation/num_examples=50000
I0303 01:35:43.183743 140380326586112 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.4472755193710327, loss=4.088320732116699
I0303 01:36:27.304675 140380334978816 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.5671626329421997, loss=1.8164751529693604
I0303 01:37:12.315301 140380326586112 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.5880417823791504, loss=1.9915046691894531
I0303 01:37:57.217137 140380334978816 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.4518967866897583, loss=3.866596221923828
I0303 01:38:42.202960 140380326586112 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.5894371271133423, loss=1.8723595142364502
I0303 01:39:27.263523 140380334978816 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.4330849647521973, loss=3.166733741760254
I0303 01:40:12.215691 140380326586112 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.5880757570266724, loss=2.4685747623443604
I0303 01:40:57.286289 140380334978816 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.5499660968780518, loss=2.0657436847686768
I0303 01:41:42.064664 140380326586112 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.6074519157409668, loss=4.350768089294434
I0303 01:42:12.361953 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:42:22.955154 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:42:46.443965 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:42:48.101187 140575196817216 submission_runner.py:411] Time since start: 50943.70s, 	Step: 105968, 	{'train/accuracy': 0.7692187428474426, 'train/loss': 0.9136697053909302, 'validation/accuracy': 0.6956999897956848, 'validation/loss': 1.2356456518173218, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 1.8811651468276978, 'test/num_examples': 10000, 'score': 47097.24747681618, 'total_duration': 50943.70225930214, 'accumulated_submission_time': 47097.24747681618, 'accumulated_eval_time': 3835.612533569336, 'accumulated_logging_time': 5.22328519821167}
I0303 01:42:48.134676 140380334978816 logging_writer.py:48] [105968] accumulated_eval_time=3835.612534, accumulated_logging_time=5.223285, accumulated_submission_time=47097.247477, global_step=105968, preemption_count=0, score=47097.247477, test/accuracy=0.564800, test/loss=1.881165, test/num_examples=10000, total_duration=50943.702259, train/accuracy=0.769219, train/loss=0.913670, validation/accuracy=0.695700, validation/loss=1.235646, validation/num_examples=50000
I0303 01:43:01.254286 140380326586112 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.8071517944335938, loss=1.938504695892334
I0303 01:43:43.067604 140380334978816 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.558896780014038, loss=1.854812502861023
I0303 01:44:28.072961 140380326586112 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.4525059461593628, loss=2.918994188308716
I0303 01:45:13.036572 140380334978816 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.679538369178772, loss=1.8379385471343994
I0303 01:45:58.344320 140380326586112 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.6036127805709839, loss=1.9614195823669434
I0303 01:46:43.372803 140380334978816 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.6354433298110962, loss=2.686691999435425
I0303 01:47:28.219968 140380326586112 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.5134243965148926, loss=3.870698928833008
I0303 01:48:13.107077 140380334978816 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.3857780694961548, loss=3.3763647079467773
I0303 01:48:57.792290 140380326586112 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.471828818321228, loss=4.317409038543701
I0303 01:49:42.717517 140380334978816 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.6878786087036133, loss=4.0106120109558105
I0303 01:49:48.233701 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:49:58.962024 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:50:22.679536 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:50:24.345593 140575196817216 submission_runner.py:411] Time since start: 51399.95s, 	Step: 106914, 	{'train/accuracy': 0.7616991996765137, 'train/loss': 0.9309342503547668, 'validation/accuracy': 0.6985599994659424, 'validation/loss': 1.2136071920394897, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.8738008737564087, 'test/num_examples': 10000, 'score': 47517.2873609066, 'total_duration': 51399.94680428505, 'accumulated_submission_time': 47517.2873609066, 'accumulated_eval_time': 3871.7235753536224, 'accumulated_logging_time': 5.266687631607056}
I0303 01:50:24.378170 140380326586112 logging_writer.py:48] [106914] accumulated_eval_time=3871.723575, accumulated_logging_time=5.266688, accumulated_submission_time=47517.287361, global_step=106914, preemption_count=0, score=47517.287361, test/accuracy=0.567600, test/loss=1.873801, test/num_examples=10000, total_duration=51399.946804, train/accuracy=0.761699, train/loss=0.930934, validation/accuracy=0.698560, validation/loss=1.213607, validation/num_examples=50000
I0303 01:50:58.974341 140380334978816 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.4613286256790161, loss=3.094849109649658
I0303 01:51:43.430608 140380326586112 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.453051209449768, loss=2.8903579711914062
I0303 01:52:29.007935 140380334978816 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.6592453718185425, loss=2.203860282897949
I0303 01:53:14.028396 140380326586112 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.8146852254867554, loss=1.9048398733139038
I0303 01:53:59.507324 140380334978816 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.3970012664794922, loss=3.0463850498199463
I0303 01:54:44.276077 140380326586112 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.5690598487854004, loss=1.8794987201690674
I0303 01:55:29.529083 140380334978816 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.5718549489974976, loss=1.9998977184295654
I0303 01:56:14.986014 140380326586112 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.3860176801681519, loss=2.5818614959716797
I0303 01:56:59.944077 140380334978816 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.692841649055481, loss=1.908181071281433
I0303 01:57:24.481841 140575196817216 spec.py:321] Evaluating on the training split.
I0303 01:57:35.150176 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 01:57:58.135544 140575196817216 spec.py:349] Evaluating on the test split.
I0303 01:57:59.809916 140575196817216 submission_runner.py:411] Time since start: 51855.41s, 	Step: 107855, 	{'train/accuracy': 0.7624804377555847, 'train/loss': 0.9120814800262451, 'validation/accuracy': 0.7012799978256226, 'validation/loss': 1.1950334310531616, 'validation/num_examples': 50000, 'test/accuracy': 0.5760000348091125, 'test/loss': 1.8427163362503052, 'test/num_examples': 10000, 'score': 47937.33025097847, 'total_duration': 51855.41088676453, 'accumulated_submission_time': 47937.33025097847, 'accumulated_eval_time': 3907.0505611896515, 'accumulated_logging_time': 5.311384916305542}
I0303 01:57:59.848637 140380326586112 logging_writer.py:48] [107855] accumulated_eval_time=3907.050561, accumulated_logging_time=5.311385, accumulated_submission_time=47937.330251, global_step=107855, preemption_count=0, score=47937.330251, test/accuracy=0.576000, test/loss=1.842716, test/num_examples=10000, total_duration=51855.410887, train/accuracy=0.762480, train/loss=0.912081, validation/accuracy=0.701280, validation/loss=1.195033, validation/num_examples=50000
I0303 01:58:18.154156 140380334978816 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.4184913635253906, loss=2.7965621948242188
I0303 01:59:00.628978 140380326586112 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.439376950263977, loss=2.6756651401519775
I0303 01:59:45.865042 140380334978816 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.5418957471847534, loss=1.9821656942367554
I0303 02:00:30.928849 140380326586112 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.4483599662780762, loss=3.067155361175537
I0303 02:01:15.849818 140380334978816 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.584078311920166, loss=2.1917572021484375
I0303 02:02:00.963829 140380326586112 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.6918834447860718, loss=4.247864246368408
I0303 02:02:46.076899 140380334978816 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.7155358791351318, loss=3.781892776489258
I0303 02:03:31.409747 140380326586112 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.5186983346939087, loss=2.4073424339294434
I0303 02:04:16.491842 140380334978816 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.4139323234558105, loss=2.600700616836548
I0303 02:04:59.861299 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:05:10.452505 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:05:33.748353 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:05:35.395778 140575196817216 submission_runner.py:411] Time since start: 52311.00s, 	Step: 108799, 	{'train/accuracy': 0.7684375047683716, 'train/loss': 0.915904700756073, 'validation/accuracy': 0.6987599730491638, 'validation/loss': 1.2196636199951172, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 1.8710874319076538, 'test/num_examples': 10000, 'score': 48357.28145503998, 'total_duration': 52310.99687170982, 'accumulated_submission_time': 48357.28145503998, 'accumulated_eval_time': 3942.584057331085, 'accumulated_logging_time': 5.36137318611145}
I0303 02:05:35.430966 140380326586112 logging_writer.py:48] [108799] accumulated_eval_time=3942.584057, accumulated_logging_time=5.361373, accumulated_submission_time=48357.281455, global_step=108799, preemption_count=0, score=48357.281455, test/accuracy=0.568500, test/loss=1.871087, test/num_examples=10000, total_duration=52310.996872, train/accuracy=0.768438, train/loss=0.915905, validation/accuracy=0.698760, validation/loss=1.219664, validation/num_examples=50000
I0303 02:05:36.233240 140380334978816 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.6402240991592407, loss=4.064413547515869
I0303 02:06:16.741136 140380326586112 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.6261228322982788, loss=2.290144920349121
I0303 02:07:01.331551 140380334978816 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.7715762853622437, loss=4.264340400695801
I0303 02:07:46.412915 140380326586112 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.5270246267318726, loss=2.1596736907958984
I0303 02:08:31.157245 140380334978816 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.3744224309921265, loss=2.601780891418457
I0303 02:09:16.226877 140380326586112 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.6964528560638428, loss=1.9903424978256226
I0303 02:10:00.969737 140380334978816 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.5092307329177856, loss=2.6248204708099365
I0303 02:10:46.269773 140380326586112 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.4431716203689575, loss=3.3695638179779053
I0303 02:11:31.207743 140380334978816 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.6784942150115967, loss=1.8781003952026367
I0303 02:12:16.221939 140380326586112 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.5989985466003418, loss=2.0315561294555664
I0303 02:12:35.823690 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:12:46.500729 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:13:10.722970 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:13:12.374798 140575196817216 submission_runner.py:411] Time since start: 52767.98s, 	Step: 109745, 	{'train/accuracy': 0.7912695407867432, 'train/loss': 0.8051213026046753, 'validation/accuracy': 0.7003399729728699, 'validation/loss': 1.1956088542938232, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.8296006917953491, 'test/num_examples': 10000, 'score': 48777.6155333519, 'total_duration': 52767.975719213486, 'accumulated_submission_time': 48777.6155333519, 'accumulated_eval_time': 3979.1340260505676, 'accumulated_logging_time': 5.4060890674591064}
I0303 02:13:12.406819 140380334978816 logging_writer.py:48] [109745] accumulated_eval_time=3979.134026, accumulated_logging_time=5.406089, accumulated_submission_time=48777.615533, global_step=109745, preemption_count=0, score=48777.615533, test/accuracy=0.575600, test/loss=1.829601, test/num_examples=10000, total_duration=52767.975719, train/accuracy=0.791270, train/loss=0.805121, validation/accuracy=0.700340, validation/loss=1.195609, validation/num_examples=50000
I0303 02:13:34.663053 140380326586112 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.6104613542556763, loss=2.836465835571289
I0303 02:14:17.748220 140380334978816 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.7252271175384521, loss=2.1272199153900146
I0303 02:15:02.805721 140380326586112 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.6309840679168701, loss=1.8231480121612549
I0303 02:15:48.301669 140380334978816 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.441352128982544, loss=2.404060125350952
I0303 02:16:33.546875 140380326586112 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.6036285161972046, loss=2.2162206172943115
I0303 02:17:18.479479 140380334978816 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.5456078052520752, loss=3.5859434604644775
I0303 02:18:03.725277 140380326586112 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.6019662618637085, loss=1.8762292861938477
I0303 02:18:48.600339 140380334978816 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.4480427503585815, loss=2.0307857990264893
I0303 02:19:33.713523 140380326586112 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.513348937034607, loss=1.8087682723999023
I0303 02:20:12.559319 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:20:23.344146 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:20:47.695494 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:20:49.349193 140575196817216 submission_runner.py:411] Time since start: 53224.95s, 	Step: 110688, 	{'train/accuracy': 0.7667773365974426, 'train/loss': 0.9034397602081299, 'validation/accuracy': 0.7044399976730347, 'validation/loss': 1.1917033195495605, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.837497353553772, 'test/num_examples': 10000, 'score': 49197.70868873596, 'total_duration': 53224.950063705444, 'accumulated_submission_time': 49197.70868873596, 'accumulated_eval_time': 4015.9227290153503, 'accumulated_logging_time': 5.449268817901611}
I0303 02:20:49.389296 140380334978816 logging_writer.py:48] [110688] accumulated_eval_time=4015.922729, accumulated_logging_time=5.449269, accumulated_submission_time=49197.708689, global_step=110688, preemption_count=0, score=49197.708689, test/accuracy=0.575100, test/loss=1.837497, test/num_examples=10000, total_duration=53224.950064, train/accuracy=0.766777, train/loss=0.903440, validation/accuracy=0.704440, validation/loss=1.191703, validation/num_examples=50000
I0303 02:20:54.566226 140380326586112 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.4726543426513672, loss=2.1909291744232178
I0303 02:21:35.343733 140380334978816 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.6694988012313843, loss=1.7670581340789795
I0303 02:22:20.286756 140380326586112 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.745906949043274, loss=1.9197673797607422
I0303 02:23:05.424019 140380334978816 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.6897739171981812, loss=1.667732834815979
I0303 02:23:50.433705 140380326586112 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.4818487167358398, loss=3.0266952514648438
I0303 02:24:35.841981 140380334978816 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.572985053062439, loss=2.075632095336914
I0303 02:25:20.443971 140380326586112 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.4126125574111938, loss=2.7883071899414062
I0303 02:26:05.473362 140380334978816 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.680692195892334, loss=1.842524766921997
I0303 02:26:50.561448 140380326586112 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.6332447528839111, loss=1.7857784032821655
I0303 02:27:35.589859 140380334978816 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.6799548864364624, loss=1.9743449687957764
I0303 02:27:49.546146 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:28:00.218936 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:28:23.368573 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:28:25.043766 140575196817216 submission_runner.py:411] Time since start: 53680.64s, 	Step: 111633, 	{'train/accuracy': 0.7742773294448853, 'train/loss': 0.8982342481613159, 'validation/accuracy': 0.7044399976730347, 'validation/loss': 1.1926296949386597, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 1.8268989324569702, 'test/num_examples': 10000, 'score': 49617.80343198776, 'total_duration': 53680.64485859871, 'accumulated_submission_time': 49617.80343198776, 'accumulated_eval_time': 4051.4193789958954, 'accumulated_logging_time': 5.502639532089233}
I0303 02:28:25.082454 140380326586112 logging_writer.py:48] [111633] accumulated_eval_time=4051.419379, accumulated_logging_time=5.502640, accumulated_submission_time=49617.803432, global_step=111633, preemption_count=0, score=49617.803432, test/accuracy=0.576700, test/loss=1.826899, test/num_examples=10000, total_duration=53680.644859, train/accuracy=0.774277, train/loss=0.898234, validation/accuracy=0.704440, validation/loss=1.192630, validation/num_examples=50000
I0303 02:28:52.135066 140380334978816 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.5697152614593506, loss=1.8347725868225098
I0303 02:29:35.840060 140380326586112 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.86553955078125, loss=1.9002654552459717
I0303 02:30:20.901341 140380334978816 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.606535792350769, loss=3.8556885719299316
I0303 02:31:05.822981 140380326586112 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.6809605360031128, loss=2.0346007347106934
I0303 02:31:50.655645 140380334978816 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.5162990093231201, loss=2.6697006225585938
I0303 02:32:35.520074 140380326586112 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.6363365650177002, loss=2.0219919681549072
I0303 02:33:20.592128 140380334978816 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.4811617136001587, loss=2.8891794681549072
I0303 02:34:05.623701 140380326586112 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.790160059928894, loss=3.8280372619628906
I0303 02:34:50.748937 140380334978816 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.5744973421096802, loss=1.7637706995010376
I0303 02:35:25.344237 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:35:36.565099 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:35:58.976713 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:36:00.637995 140575196817216 submission_runner.py:411] Time since start: 54136.24s, 	Step: 112579, 	{'train/accuracy': 0.7817187309265137, 'train/loss': 0.8364842534065247, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.1751327514648438, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.812219262123108, 'test/num_examples': 10000, 'score': 50038.0046517849, 'total_duration': 54136.23913860321, 'accumulated_submission_time': 50038.0046517849, 'accumulated_eval_time': 4086.712213039398, 'accumulated_logging_time': 5.552619695663452}
I0303 02:36:00.671000 140380326586112 logging_writer.py:48] [112579] accumulated_eval_time=4086.712213, accumulated_logging_time=5.552620, accumulated_submission_time=50038.004652, global_step=112579, preemption_count=0, score=50038.004652, test/accuracy=0.581600, test/loss=1.812219, test/num_examples=10000, total_duration=54136.239139, train/accuracy=0.781719, train/loss=0.836484, validation/accuracy=0.705720, validation/loss=1.175133, validation/num_examples=50000
I0303 02:36:09.419175 140380334978816 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.5970216989517212, loss=3.8164517879486084
I0303 02:36:52.365920 140380326586112 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.702907681465149, loss=1.9748262166976929
I0303 02:37:37.427843 140380334978816 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.5176832675933838, loss=2.198683261871338
I0303 02:38:22.702352 140380326586112 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.3964293003082275, loss=2.727041244506836
I0303 02:39:07.797722 140380334978816 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.6051242351531982, loss=4.103235721588135
I0303 02:39:52.808265 140380326586112 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.6752740144729614, loss=4.17104959487915
I0303 02:40:37.721171 140380334978816 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.6314117908477783, loss=2.152566432952881
I0303 02:41:22.724355 140380326586112 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.667362093925476, loss=4.396108627319336
I0303 02:42:07.936568 140380334978816 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.4354420900344849, loss=3.397428035736084
I0303 02:42:52.997928 140380326586112 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.6808547973632812, loss=1.7470797300338745
I0303 02:43:00.745526 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:43:11.415600 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:43:35.515884 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:43:37.172528 140575196817216 submission_runner.py:411] Time since start: 54592.77s, 	Step: 113519, 	{'train/accuracy': 0.7722070217132568, 'train/loss': 0.889786422252655, 'validation/accuracy': 0.7026799917221069, 'validation/loss': 1.1861135959625244, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 1.8333693742752075, 'test/num_examples': 10000, 'score': 50458.02049589157, 'total_duration': 54592.7734439373, 'accumulated_submission_time': 50458.02049589157, 'accumulated_eval_time': 4123.13805603981, 'accumulated_logging_time': 5.594868421554565}
I0303 02:43:37.206442 140380334978816 logging_writer.py:48] [113519] accumulated_eval_time=4123.138056, accumulated_logging_time=5.594868, accumulated_submission_time=50458.020496, global_step=113519, preemption_count=0, score=50458.020496, test/accuracy=0.576600, test/loss=1.833369, test/num_examples=10000, total_duration=54592.773444, train/accuracy=0.772207, train/loss=0.889786, validation/accuracy=0.702680, validation/loss=1.186114, validation/num_examples=50000
I0303 02:44:09.825728 140380326586112 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.5680134296417236, loss=1.742553949356079
I0303 02:44:53.905927 140380334978816 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.6946943998336792, loss=1.691392183303833
I0303 02:45:38.974761 140380326586112 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.7376078367233276, loss=1.8110291957855225
I0303 02:46:24.507850 140380334978816 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.526871919631958, loss=2.621420383453369
I0303 02:47:09.320988 140380326586112 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.8363542556762695, loss=1.8384625911712646
I0303 02:47:54.088214 140380334978816 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.5597615242004395, loss=1.7028557062149048
I0303 02:48:38.990922 140380326586112 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.5392082929611206, loss=1.9363751411437988
I0303 02:49:24.097940 140380334978816 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.6271862983703613, loss=1.806477665901184
I0303 02:50:08.944154 140380326586112 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.4335612058639526, loss=2.5783920288085938
I0303 02:50:37.349172 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:50:47.916727 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:51:11.368269 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:51:13.029354 140575196817216 submission_runner.py:411] Time since start: 55048.63s, 	Step: 114465, 	{'train/accuracy': 0.7795702815055847, 'train/loss': 0.8615785837173462, 'validation/accuracy': 0.7099199891090393, 'validation/loss': 1.1732913255691528, 'validation/num_examples': 50000, 'test/accuracy': 0.588100016117096, 'test/loss': 1.8046170473098755, 'test/num_examples': 10000, 'score': 50878.1044754982, 'total_duration': 55048.6302447319, 'accumulated_submission_time': 50878.1044754982, 'accumulated_eval_time': 4158.8170692920685, 'accumulated_logging_time': 5.638230085372925}
I0303 02:51:13.065663 140380334978816 logging_writer.py:48] [114465] accumulated_eval_time=4158.817069, accumulated_logging_time=5.638230, accumulated_submission_time=50878.104475, global_step=114465, preemption_count=0, score=50878.104475, test/accuracy=0.588100, test/loss=1.804617, test/num_examples=10000, total_duration=55048.630245, train/accuracy=0.779570, train/loss=0.861579, validation/accuracy=0.709920, validation/loss=1.173291, validation/num_examples=50000
I0303 02:51:27.383859 140380326586112 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.4601739645004272, loss=2.2862372398376465
I0303 02:52:09.264984 140380334978816 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.6021612882614136, loss=2.5770626068115234
I0303 02:52:54.017489 140380326586112 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.6738951206207275, loss=2.001314878463745
I0303 02:53:39.161701 140380334978816 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.82148277759552, loss=3.9930174350738525
I0303 02:54:24.223532 140380326586112 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.5757558345794678, loss=2.2287235260009766
I0303 02:55:08.995862 140380334978816 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.6826555728912354, loss=1.9166812896728516
I0303 02:55:53.831771 140380326586112 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.7604037523269653, loss=1.766756296157837
I0303 02:56:38.856339 140380334978816 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.7208434343338013, loss=3.9846315383911133
I0303 02:57:24.075494 140380326586112 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.7023727893829346, loss=1.7178593873977661
I0303 02:58:08.934837 140380334978816 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.6428940296173096, loss=3.2185819149017334
I0303 02:58:13.105863 140575196817216 spec.py:321] Evaluating on the training split.
I0303 02:58:23.884404 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 02:58:46.353211 140575196817216 spec.py:349] Evaluating on the test split.
I0303 02:58:48.016225 140575196817216 submission_runner.py:411] Time since start: 55503.62s, 	Step: 115411, 	{'train/accuracy': 0.7862890362739563, 'train/loss': 0.82613605260849, 'validation/accuracy': 0.709879994392395, 'validation/loss': 1.1572134494781494, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 1.7958065271377563, 'test/num_examples': 10000, 'score': 51298.08326268196, 'total_duration': 55503.61720633507, 'accumulated_submission_time': 51298.08326268196, 'accumulated_eval_time': 4193.726380109787, 'accumulated_logging_time': 5.686317682266235}
I0303 02:58:48.050543 140380326586112 logging_writer.py:48] [115411] accumulated_eval_time=4193.726380, accumulated_logging_time=5.686318, accumulated_submission_time=51298.083263, global_step=115411, preemption_count=0, score=51298.083263, test/accuracy=0.583000, test/loss=1.795807, test/num_examples=10000, total_duration=55503.617206, train/accuracy=0.786289, train/loss=0.826136, validation/accuracy=0.709880, validation/loss=1.157213, validation/num_examples=50000
I0303 02:59:23.850652 140380334978816 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.6384481191635132, loss=2.111842155456543
I0303 03:00:08.702728 140380326586112 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.7522531747817993, loss=1.8462321758270264
I0303 03:00:53.860032 140380334978816 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.7255967855453491, loss=2.193803071975708
I0303 03:01:38.823665 140380326586112 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.7275344133377075, loss=1.8065545558929443
I0303 03:02:23.723726 140380334978816 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.7651852369308472, loss=1.8386831283569336
I0303 03:03:08.636857 140380326586112 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.615679144859314, loss=2.348660469055176
I0303 03:03:53.733381 140380334978816 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.781960368156433, loss=2.6281275749206543
I0303 03:04:38.623521 140380326586112 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.603844404220581, loss=1.6940747499465942
I0303 03:05:23.537106 140380334978816 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.6013115644454956, loss=2.0661048889160156
I0303 03:05:48.163365 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:05:58.924545 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:06:19.912760 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:06:21.579780 140575196817216 submission_runner.py:411] Time since start: 55957.18s, 	Step: 116356, 	{'train/accuracy': 0.8011523485183716, 'train/loss': 0.7779342532157898, 'validation/accuracy': 0.7084999680519104, 'validation/loss': 1.161314606666565, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 1.8103371858596802, 'test/num_examples': 10000, 'score': 51718.136840581894, 'total_duration': 55957.18047308922, 'accumulated_submission_time': 51718.136840581894, 'accumulated_eval_time': 4227.141417503357, 'accumulated_logging_time': 5.730888605117798}
I0303 03:06:21.623933 140380326586112 logging_writer.py:48] [116356] accumulated_eval_time=4227.141418, accumulated_logging_time=5.730889, accumulated_submission_time=51718.136841, global_step=116356, preemption_count=0, score=51718.136841, test/accuracy=0.582200, test/loss=1.810337, test/num_examples=10000, total_duration=55957.180473, train/accuracy=0.801152, train/loss=0.777934, validation/accuracy=0.708500, validation/loss=1.161315, validation/num_examples=50000
I0303 03:06:39.535004 140380334978816 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.8379030227661133, loss=4.130405902862549
I0303 03:07:22.539740 140380326586112 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.6727195978164673, loss=1.8305976390838623
I0303 03:08:07.691055 140380334978816 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.6826704740524292, loss=1.6915338039398193
I0303 03:08:52.704302 140380326586112 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.641874074935913, loss=2.3037936687469482
I0303 03:09:37.897886 140380334978816 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.759379506111145, loss=1.8555223941802979
I0303 03:10:22.999479 140380326586112 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.6220933198928833, loss=2.518054962158203
I0303 03:11:07.917601 140380334978816 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.7673888206481934, loss=1.7868505716323853
I0303 03:11:53.064699 140380326586112 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.4466686248779297, loss=3.053511142730713
I0303 03:12:38.216672 140380334978816 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.64633309841156, loss=2.1224966049194336
I0303 03:13:22.003135 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:13:32.891684 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:13:57.345215 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:13:58.999856 140575196817216 submission_runner.py:411] Time since start: 56414.60s, 	Step: 117299, 	{'train/accuracy': 0.7773827910423279, 'train/loss': 0.8753180503845215, 'validation/accuracy': 0.7099599838256836, 'validation/loss': 1.174333095550537, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.808908462524414, 'test/num_examples': 10000, 'score': 52138.45469069481, 'total_duration': 56414.60079431534, 'accumulated_submission_time': 52138.45469069481, 'accumulated_eval_time': 4264.137006044388, 'accumulated_logging_time': 5.787650108337402}
I0303 03:13:59.044224 140380326586112 logging_writer.py:48] [117299] accumulated_eval_time=4264.137006, accumulated_logging_time=5.787650, accumulated_submission_time=52138.454691, global_step=117299, preemption_count=0, score=52138.454691, test/accuracy=0.587800, test/loss=1.808908, test/num_examples=10000, total_duration=56414.600794, train/accuracy=0.777383, train/loss=0.875318, validation/accuracy=0.709960, validation/loss=1.174333, validation/num_examples=50000
I0303 03:13:59.847979 140380334978816 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.8010811805725098, loss=4.2327070236206055
I0303 03:14:39.863548 140380326586112 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.5393524169921875, loss=3.1693344116210938
I0303 03:15:24.543685 140380334978816 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.9923880100250244, loss=4.060125827789307
I0303 03:16:09.556268 140380326586112 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.6514286994934082, loss=1.7957124710083008
I0303 03:16:54.901498 140380334978816 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.71961510181427, loss=4.214136123657227
I0303 03:17:39.574291 140380326586112 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.7653985023498535, loss=1.8507311344146729
I0303 03:18:24.634940 140380334978816 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.797577977180481, loss=1.8288440704345703
I0303 03:19:09.214825 140380326586112 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.7106735706329346, loss=1.9076249599456787
I0303 03:19:54.107976 140380334978816 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.708802342414856, loss=1.7702927589416504
I0303 03:20:38.863134 140380326586112 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.6614174842834473, loss=2.3086166381835938
I0303 03:20:59.074267 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:21:09.760654 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:21:33.162438 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:21:34.822484 140575196817216 submission_runner.py:411] Time since start: 56870.42s, 	Step: 118247, 	{'train/accuracy': 0.7845507860183716, 'train/loss': 0.8391237258911133, 'validation/accuracy': 0.7131199836730957, 'validation/loss': 1.1501730680465698, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.791690707206726, 'test/num_examples': 10000, 'score': 52558.41911363602, 'total_duration': 56870.423597335815, 'accumulated_submission_time': 52558.41911363602, 'accumulated_eval_time': 4299.884291887283, 'accumulated_logging_time': 5.847143650054932}
I0303 03:21:34.856031 140380334978816 logging_writer.py:48] [118247] accumulated_eval_time=4299.884292, accumulated_logging_time=5.847144, accumulated_submission_time=52558.419114, global_step=118247, preemption_count=0, score=52558.419114, test/accuracy=0.589500, test/loss=1.791691, test/num_examples=10000, total_duration=56870.423597, train/accuracy=0.784551, train/loss=0.839124, validation/accuracy=0.713120, validation/loss=1.150173, validation/num_examples=50000
I0303 03:21:56.326397 140380326586112 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.7453562021255493, loss=1.8415369987487793
I0303 03:22:38.883051 140380334978816 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.7340381145477295, loss=1.795833945274353
I0303 03:23:23.803437 140380326586112 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.629557490348816, loss=3.3926401138305664
I0303 03:24:08.919462 140380334978816 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.4876660108566284, loss=2.939500331878662
I0303 03:24:54.114374 140380326586112 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.5567878484725952, loss=2.414907217025757
I0303 03:25:38.727798 140380334978816 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.93522310256958, loss=1.8047281503677368
I0303 03:26:23.786055 140380326586112 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.5588257312774658, loss=2.266716718673706
I0303 03:27:08.782938 140380334978816 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.7152267694473267, loss=1.9219186305999756
I0303 03:27:53.767225 140380326586112 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.7591642141342163, loss=1.802894949913025
I0303 03:28:35.141163 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:28:45.777903 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:29:10.381277 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:29:12.041284 140575196817216 submission_runner.py:411] Time since start: 57327.64s, 	Step: 119194, 	{'train/accuracy': 0.7907031178474426, 'train/loss': 0.8129280805587769, 'validation/accuracy': 0.712619960308075, 'validation/loss': 1.1554131507873535, 'validation/num_examples': 50000, 'test/accuracy': 0.5840000510215759, 'test/loss': 1.8003008365631104, 'test/num_examples': 10000, 'score': 52978.646104097366, 'total_duration': 57327.64228081703, 'accumulated_submission_time': 52978.646104097366, 'accumulated_eval_time': 4336.783348560333, 'accumulated_logging_time': 5.890512466430664}
I0303 03:29:12.082762 140380334978816 logging_writer.py:48] [119194] accumulated_eval_time=4336.783349, accumulated_logging_time=5.890512, accumulated_submission_time=52978.646104, global_step=119194, preemption_count=0, score=52978.646104, test/accuracy=0.584000, test/loss=1.800301, test/num_examples=10000, total_duration=57327.642281, train/accuracy=0.790703, train/loss=0.812928, validation/accuracy=0.712620, validation/loss=1.155413, validation/num_examples=50000
I0303 03:29:14.871282 140380326586112 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.7116327285766602, loss=1.7497321367263794
I0303 03:29:55.055052 140380334978816 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.101482391357422, loss=3.8600380420684814
I0303 03:30:40.118446 140380326586112 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.7956244945526123, loss=3.3865957260131836
I0303 03:31:25.446533 140380334978816 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.6996581554412842, loss=1.9686999320983887
I0303 03:32:10.599851 140380326586112 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.889677882194519, loss=1.8878190517425537
I0303 03:32:55.363989 140380334978816 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.0054049491882324, loss=4.127956867218018
I0303 03:33:40.232251 140380326586112 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.8159586191177368, loss=1.8976670503616333
I0303 03:34:25.105651 140380334978816 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.755555272102356, loss=2.085033893585205
I0303 03:35:09.932993 140380326586112 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.6755940914154053, loss=3.765613317489624
I0303 03:35:54.948232 140380334978816 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.8658627271652222, loss=4.2028584480285645
I0303 03:36:12.468074 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:36:23.217838 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:36:46.171392 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:36:47.828101 140575196817216 submission_runner.py:411] Time since start: 57783.43s, 	Step: 120140, 	{'train/accuracy': 0.78236323595047, 'train/loss': 0.8431088328361511, 'validation/accuracy': 0.715939998626709, 'validation/loss': 1.1370404958724976, 'validation/num_examples': 50000, 'test/accuracy': 0.5950000286102295, 'test/loss': 1.7669881582260132, 'test/num_examples': 10000, 'score': 53398.970262527466, 'total_duration': 57783.42899656296, 'accumulated_submission_time': 53398.970262527466, 'accumulated_eval_time': 4372.142210245132, 'accumulated_logging_time': 5.944162607192993}
I0303 03:36:47.865549 140380326586112 logging_writer.py:48] [120140] accumulated_eval_time=4372.142210, accumulated_logging_time=5.944163, accumulated_submission_time=53398.970263, global_step=120140, preemption_count=0, score=53398.970263, test/accuracy=0.595000, test/loss=1.766988, test/num_examples=10000, total_duration=57783.428997, train/accuracy=0.782363, train/loss=0.843109, validation/accuracy=0.715940, validation/loss=1.137040, validation/num_examples=50000
I0303 03:37:12.109459 140380334978816 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.790591835975647, loss=3.42233943939209
I0303 03:37:55.479223 140380326586112 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.7164878845214844, loss=2.8766777515411377
I0303 03:38:40.777864 140380334978816 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.707127332687378, loss=4.195669174194336
I0303 03:39:25.805655 140380326586112 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.608293056488037, loss=3.77140474319458
I0303 03:40:10.818102 140380334978816 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.7611134052276611, loss=1.7626498937606812
I0303 03:40:55.690050 140380326586112 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.847957968711853, loss=3.7787671089172363
I0303 03:41:40.711737 140380334978816 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.9296050071716309, loss=1.7091490030288696
I0303 03:42:25.850383 140380326586112 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.6872590780258179, loss=1.7207752466201782
I0303 03:43:11.064031 140380334978816 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.739154577255249, loss=4.213192939758301
I0303 03:43:48.169164 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:43:59.068450 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:44:23.067609 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:44:24.731106 140575196817216 submission_runner.py:411] Time since start: 58240.33s, 	Step: 121084, 	{'train/accuracy': 0.7857617139816284, 'train/loss': 0.8171523809432983, 'validation/accuracy': 0.716759979724884, 'validation/loss': 1.126121163368225, 'validation/num_examples': 50000, 'test/accuracy': 0.59170001745224, 'test/loss': 1.766446590423584, 'test/num_examples': 10000, 'score': 53819.214713573456, 'total_duration': 58240.33206629753, 'accumulated_submission_time': 53819.214713573456, 'accumulated_eval_time': 4408.703050613403, 'accumulated_logging_time': 5.99152398109436}
I0303 03:44:24.769203 140380326586112 logging_writer.py:48] [121084] accumulated_eval_time=4408.703051, accumulated_logging_time=5.991524, accumulated_submission_time=53819.214714, global_step=121084, preemption_count=0, score=53819.214714, test/accuracy=0.591700, test/loss=1.766447, test/num_examples=10000, total_duration=58240.332066, train/accuracy=0.785762, train/loss=0.817152, validation/accuracy=0.716760, validation/loss=1.126121, validation/num_examples=50000
I0303 03:44:31.531569 140380334978816 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.7476307153701782, loss=3.580368757247925
I0303 03:45:12.761644 140380326586112 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.7246161699295044, loss=1.738302230834961
I0303 03:45:57.402044 140380334978816 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.191924810409546, loss=4.167413711547852
I0303 03:46:43.014092 140380326586112 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.8183226585388184, loss=1.7940086126327515
I0303 03:47:28.565002 140380334978816 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.7745318412780762, loss=1.8039429187774658
I0303 03:48:13.485409 140380326586112 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.834349513053894, loss=1.7776148319244385
I0303 03:48:58.528505 140380334978816 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.7042231559753418, loss=2.994443893432617
I0303 03:49:43.421489 140380326586112 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.750546932220459, loss=2.4806747436523438
I0303 03:50:28.621736 140380334978816 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.899715781211853, loss=1.6901280879974365
I0303 03:51:13.687498 140380326586112 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.777620792388916, loss=3.7843685150146484
I0303 03:51:24.904489 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:51:35.487008 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:51:59.002965 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:52:00.664104 140575196817216 submission_runner.py:411] Time since start: 58696.27s, 	Step: 122027, 	{'train/accuracy': 0.7969726324081421, 'train/loss': 0.7908524870872498, 'validation/accuracy': 0.7226200103759766, 'validation/loss': 1.1162563562393188, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.7421303987503052, 'test/num_examples': 10000, 'score': 54239.28956913948, 'total_duration': 58696.265003442764, 'accumulated_submission_time': 54239.28956913948, 'accumulated_eval_time': 4444.461498260498, 'accumulated_logging_time': 6.041364669799805}
I0303 03:52:00.702142 140380334978816 logging_writer.py:48] [122027] accumulated_eval_time=4444.461498, accumulated_logging_time=6.041365, accumulated_submission_time=54239.289569, global_step=122027, preemption_count=0, score=54239.289569, test/accuracy=0.600500, test/loss=1.742130, test/num_examples=10000, total_duration=58696.265003, train/accuracy=0.796973, train/loss=0.790852, validation/accuracy=0.722620, validation/loss=1.116256, validation/num_examples=50000
I0303 03:52:30.122974 140380326586112 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.7839208841323853, loss=1.7604269981384277
I0303 03:53:14.100422 140380334978816 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.942840337753296, loss=4.229824542999268
I0303 03:53:59.369043 140380326586112 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.679244041442871, loss=2.8623669147491455
I0303 03:54:44.564186 140380334978816 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.678432583808899, loss=3.5213334560394287
I0303 03:55:29.605913 140380326586112 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.7931016683578491, loss=1.765276551246643
I0303 03:56:15.389563 140380334978816 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.7303518056869507, loss=2.0771851539611816
I0303 03:57:00.361993 140380326586112 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.9087623357772827, loss=1.6754779815673828
I0303 03:57:45.470788 140380334978816 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.8649128675460815, loss=1.7666590213775635
I0303 03:58:30.393210 140380326586112 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.7171460390090942, loss=1.6799826622009277
I0303 03:59:00.727774 140575196817216 spec.py:321] Evaluating on the training split.
I0303 03:59:11.886380 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 03:59:35.042943 140575196817216 spec.py:349] Evaluating on the test split.
I0303 03:59:36.687809 140575196817216 submission_runner.py:411] Time since start: 59152.29s, 	Step: 122969, 	{'train/accuracy': 0.8081640601158142, 'train/loss': 0.7565469741821289, 'validation/accuracy': 0.7193799614906311, 'validation/loss': 1.1386165618896484, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.7857192754745483, 'test/num_examples': 10000, 'score': 54659.25565576553, 'total_duration': 59152.28856778145, 'accumulated_submission_time': 54659.25565576553, 'accumulated_eval_time': 4480.420238494873, 'accumulated_logging_time': 6.089730262756348}
I0303 03:59:36.726984 140380334978816 logging_writer.py:48] [122969] accumulated_eval_time=4480.420238, accumulated_logging_time=6.089730, accumulated_submission_time=54659.255656, global_step=122969, preemption_count=0, score=54659.255656, test/accuracy=0.587500, test/loss=1.785719, test/num_examples=10000, total_duration=59152.288568, train/accuracy=0.808164, train/loss=0.756547, validation/accuracy=0.719380, validation/loss=1.138617, validation/num_examples=50000
I0303 03:59:49.444527 140380326586112 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.5625942945480347, loss=2.831321954727173
I0303 04:00:31.125513 140380334978816 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.8204485177993774, loss=1.734605312347412
I0303 04:01:16.278995 140380326586112 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.8262660503387451, loss=3.9269607067108154
I0303 04:02:01.394244 140380334978816 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.8354568481445312, loss=1.7479901313781738
I0303 04:02:46.418362 140380326586112 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.6651172637939453, loss=2.359847068786621
I0303 04:03:31.562260 140380334978816 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.7442947626113892, loss=3.338810682296753
I0303 04:04:16.411405 140380326586112 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.8659937381744385, loss=1.7462612390518188
I0303 04:05:01.313570 140380334978816 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.843110203742981, loss=3.869626522064209
I0303 04:05:45.989849 140380326586112 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.039024591445923, loss=1.6914016008377075
I0303 04:06:31.289099 140380334978816 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.8248329162597656, loss=3.3100881576538086
I0303 04:06:36.800457 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:06:47.616988 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:07:10.597199 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:07:12.251365 140575196817216 submission_runner.py:411] Time since start: 59607.85s, 	Step: 123914, 	{'train/accuracy': 0.7901562452316284, 'train/loss': 0.8068869113922119, 'validation/accuracy': 0.7235400080680847, 'validation/loss': 1.1058323383331299, 'validation/num_examples': 50000, 'test/accuracy': 0.5992000102996826, 'test/loss': 1.731168270111084, 'test/num_examples': 10000, 'score': 55079.27008724213, 'total_duration': 59607.85239100456, 'accumulated_submission_time': 55079.27008724213, 'accumulated_eval_time': 4515.870098590851, 'accumulated_logging_time': 6.138895750045776}
I0303 04:07:12.291894 140380326586112 logging_writer.py:48] [123914] accumulated_eval_time=4515.870099, accumulated_logging_time=6.138896, accumulated_submission_time=55079.270087, global_step=123914, preemption_count=0, score=55079.270087, test/accuracy=0.599200, test/loss=1.731168, test/num_examples=10000, total_duration=59607.852391, train/accuracy=0.790156, train/loss=0.806887, validation/accuracy=0.723540, validation/loss=1.105832, validation/num_examples=50000
I0303 04:07:46.884734 140380334978816 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.9264209270477295, loss=1.7196389436721802
I0303 04:08:31.294226 140380326586112 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.8248991966247559, loss=1.7610113620758057
I0303 04:09:16.521988 140380334978816 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.7790241241455078, loss=1.708435297012329
I0303 04:10:01.490168 140380326586112 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.6838953495025635, loss=3.245305061340332
I0303 04:10:46.431847 140380334978816 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.7729518413543701, loss=1.8020102977752686
I0303 04:11:31.222249 140380326586112 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.6703567504882812, loss=2.959993600845337
I0303 04:12:16.273153 140380334978816 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.695951223373413, loss=1.9976571798324585
I0303 04:13:01.176822 140380326586112 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.9178184270858765, loss=1.7317907810211182
I0303 04:13:46.211223 140380334978816 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.9843933582305908, loss=1.7068743705749512
I0303 04:14:12.300343 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:14:23.023928 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:14:46.601618 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:14:48.254347 140575196817216 submission_runner.py:411] Time since start: 60063.86s, 	Step: 124859, 	{'train/accuracy': 0.7969726324081421, 'train/loss': 0.7828193306922913, 'validation/accuracy': 0.721839964389801, 'validation/loss': 1.1123453378677368, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.7349574565887451, 'test/num_examples': 10000, 'score': 55499.21904158592, 'total_duration': 60063.85525274277, 'accumulated_submission_time': 55499.21904158592, 'accumulated_eval_time': 4551.8229367733, 'accumulated_logging_time': 6.189615488052368}
I0303 04:14:48.291306 140380326586112 logging_writer.py:48] [124859] accumulated_eval_time=4551.822937, accumulated_logging_time=6.189615, accumulated_submission_time=55499.219042, global_step=124859, preemption_count=0, score=55499.219042, test/accuracy=0.600800, test/loss=1.734957, test/num_examples=10000, total_duration=60063.855253, train/accuracy=0.796973, train/loss=0.782819, validation/accuracy=0.721840, validation/loss=1.112345, validation/num_examples=50000
I0303 04:15:04.989744 140380334978816 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.7600547075271606, loss=1.772590160369873
I0303 04:15:46.978897 140380326586112 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.8571174144744873, loss=3.3019981384277344
I0303 04:16:31.997496 140380334978816 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.798598051071167, loss=2.820618152618408
I0303 04:17:17.595168 140380326586112 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.7412444353103638, loss=3.3329458236694336
I0303 04:18:02.732933 140380334978816 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.7470910549163818, loss=2.2304248809814453
I0303 04:18:47.719784 140380326586112 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.8990734815597534, loss=1.6183186769485474
I0303 04:19:33.000158 140380334978816 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.172614812850952, loss=4.051015377044678
I0303 04:20:18.037646 140380326586112 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.7839258909225464, loss=1.7184154987335205
I0303 04:21:03.275016 140380334978816 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.0190579891204834, loss=4.079544544219971
I0303 04:21:48.132210 140380326586112 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.8751373291015625, loss=3.95027494430542
I0303 04:21:48.311123 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:21:58.991148 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:22:22.380138 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:22:24.036988 140575196817216 submission_runner.py:411] Time since start: 60519.64s, 	Step: 125802, 	{'train/accuracy': 0.8039257526397705, 'train/loss': 0.764805257320404, 'validation/accuracy': 0.7251600027084351, 'validation/loss': 1.1057837009429932, 'validation/num_examples': 50000, 'test/accuracy': 0.5984000563621521, 'test/loss': 1.7472631931304932, 'test/num_examples': 10000, 'score': 55919.18010044098, 'total_duration': 60519.63783144951, 'accumulated_submission_time': 55919.18010044098, 'accumulated_eval_time': 4587.547564506531, 'accumulated_logging_time': 6.235995531082153}
I0303 04:22:24.071930 140380334978816 logging_writer.py:48] [125802] accumulated_eval_time=4587.547565, accumulated_logging_time=6.235996, accumulated_submission_time=55919.180100, global_step=125802, preemption_count=0, score=55919.180100, test/accuracy=0.598400, test/loss=1.747263, test/num_examples=10000, total_duration=60519.637831, train/accuracy=0.803926, train/loss=0.764805, validation/accuracy=0.725160, validation/loss=1.105784, validation/num_examples=50000
I0303 04:23:04.003587 140380326586112 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.8424391746520996, loss=1.7445589303970337
I0303 04:23:49.019457 140380334978816 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.7424745559692383, loss=1.4920567274093628
I0303 04:24:34.628950 140380326586112 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.9893063306808472, loss=1.778073787689209
I0303 04:25:19.478775 140380334978816 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.8854930400848389, loss=1.6163995265960693
I0303 04:26:04.377804 140380326586112 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.045386552810669, loss=1.668901801109314
I0303 04:26:49.693839 140380334978816 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.9206089973449707, loss=1.6101759672164917
I0303 04:27:34.588127 140380326586112 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.6608635187149048, loss=3.3119513988494873
I0303 04:28:19.669437 140380334978816 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.679465651512146, loss=2.984870433807373
I0303 04:29:04.446277 140380326586112 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.8581451177597046, loss=1.5850141048431396
I0303 04:29:24.266980 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:29:35.073936 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:29:57.313204 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:29:58.975598 140575196817216 submission_runner.py:411] Time since start: 60974.58s, 	Step: 126746, 	{'train/accuracy': 0.7957617044448853, 'train/loss': 0.7915138006210327, 'validation/accuracy': 0.7269200086593628, 'validation/loss': 1.0992271900177002, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7334434986114502, 'test/num_examples': 10000, 'score': 56339.31581926346, 'total_duration': 60974.57660126686, 'accumulated_submission_time': 56339.31581926346, 'accumulated_eval_time': 4622.255147457123, 'accumulated_logging_time': 6.281415939331055}
I0303 04:29:59.015008 140380334978816 logging_writer.py:48] [126746] accumulated_eval_time=4622.255147, accumulated_logging_time=6.281416, accumulated_submission_time=56339.315819, global_step=126746, preemption_count=0, score=56339.315819, test/accuracy=0.601100, test/loss=1.733443, test/num_examples=10000, total_duration=60974.576601, train/accuracy=0.795762, train/loss=0.791514, validation/accuracy=0.726920, validation/loss=1.099227, validation/num_examples=50000
I0303 04:30:20.908350 140380326586112 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.8162540197372437, loss=1.7713470458984375
I0303 04:31:03.873163 140380334978816 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.8080981969833374, loss=1.7271921634674072
I0303 04:31:48.951380 140380326586112 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.7659761905670166, loss=3.3487508296966553
I0303 04:32:34.120474 140380334978816 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.7349145412445068, loss=2.037184953689575
I0303 04:33:19.103682 140380326586112 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.0715584754943848, loss=1.6568608283996582
I0303 04:34:04.270579 140380334978816 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.049692392349243, loss=1.6797904968261719
I0303 04:34:49.339682 140380326586112 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.89559805393219, loss=2.0338668823242188
I0303 04:35:34.411975 140380334978816 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.9400815963745117, loss=1.556026577949524
I0303 04:36:19.577260 140380326586112 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.1188790798187256, loss=1.6537294387817383
I0303 04:36:59.091562 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:37:09.845576 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:37:33.138693 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:37:34.791394 140575196817216 submission_runner.py:411] Time since start: 61430.39s, 	Step: 127689, 	{'train/accuracy': 0.7975195050239563, 'train/loss': 0.7745173573493958, 'validation/accuracy': 0.725600004196167, 'validation/loss': 1.094840168952942, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7361700534820557, 'test/num_examples': 10000, 'score': 56759.32902884483, 'total_duration': 61430.392345905304, 'accumulated_submission_time': 56759.32902884483, 'accumulated_eval_time': 4657.953867673874, 'accumulated_logging_time': 6.33309531211853}
I0303 04:37:34.832469 140380334978816 logging_writer.py:48] [127689] accumulated_eval_time=4657.953868, accumulated_logging_time=6.333095, accumulated_submission_time=56759.329029, global_step=127689, preemption_count=0, score=56759.329029, test/accuracy=0.603800, test/loss=1.736170, test/num_examples=10000, total_duration=61430.392346, train/accuracy=0.797520, train/loss=0.774517, validation/accuracy=0.725600, validation/loss=1.094840, validation/num_examples=50000
I0303 04:37:39.604226 140380326586112 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.8988467454910278, loss=1.6509742736816406
I0303 04:38:20.341716 140380334978816 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.7930245399475098, loss=1.5967577695846558
I0303 04:39:05.451502 140380326586112 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.7644914388656616, loss=2.1928610801696777
I0303 04:39:50.851601 140380334978816 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.108563184738159, loss=3.978172540664673
I0303 04:40:35.901335 140380326586112 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.8161102533340454, loss=1.5464637279510498
I0303 04:41:20.970852 140380334978816 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.922655701637268, loss=2.616966485977173
I0303 04:42:05.950023 140380326586112 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.9512810707092285, loss=3.4364383220672607
I0303 04:42:50.870642 140380334978816 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.9716871976852417, loss=1.7335501909255981
I0303 04:43:35.903225 140380326586112 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.9147469997406006, loss=1.7322123050689697
I0303 04:44:20.763913 140380334978816 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.769242525100708, loss=2.2495200634002686
I0303 04:44:34.804110 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:44:45.464306 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:45:10.053231 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:45:11.702314 140575196817216 submission_runner.py:411] Time since start: 61887.30s, 	Step: 128633, 	{'train/accuracy': 0.8062695264816284, 'train/loss': 0.7585539221763611, 'validation/accuracy': 0.7260199785232544, 'validation/loss': 1.0970200300216675, 'validation/num_examples': 50000, 'test/accuracy': 0.5994000434875488, 'test/loss': 1.735648274421692, 'test/num_examples': 10000, 'score': 57179.240245103836, 'total_duration': 61887.30322051048, 'accumulated_submission_time': 57179.240245103836, 'accumulated_eval_time': 4694.850904941559, 'accumulated_logging_time': 6.385717153549194}
I0303 04:45:11.739901 140380326586112 logging_writer.py:48] [128633] accumulated_eval_time=4694.850905, accumulated_logging_time=6.385717, accumulated_submission_time=57179.240245, global_step=128633, preemption_count=0, score=57179.240245, test/accuracy=0.599400, test/loss=1.735648, test/num_examples=10000, total_duration=61887.303221, train/accuracy=0.806270, train/loss=0.758554, validation/accuracy=0.726020, validation/loss=1.097020, validation/num_examples=50000
I0303 04:45:38.763977 140380334978816 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.1783320903778076, loss=1.759283185005188
I0303 04:46:22.219430 140380326586112 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.8492801189422607, loss=1.5992136001586914
I0303 04:47:07.365083 140380334978816 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.123120069503784, loss=3.8006043434143066
I0303 04:47:52.730590 140380326586112 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.6978919506072998, loss=2.4414222240448
I0303 04:48:37.750663 140380334978816 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.8733872175216675, loss=1.836822509765625
I0303 04:49:22.930959 140380326586112 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.8430914878845215, loss=2.6432101726531982
I0303 04:50:08.023154 140380334978816 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.9335963726043701, loss=2.311328887939453
I0303 04:50:52.971922 140380326586112 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.9204820394515991, loss=3.3630993366241455
I0303 04:51:38.197674 140380334978816 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.9377005100250244, loss=1.798218011856079
I0303 04:52:11.883281 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:52:22.586808 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 04:52:47.080577 140575196817216 spec.py:349] Evaluating on the test split.
I0303 04:52:48.743906 140575196817216 submission_runner.py:411] Time since start: 62344.34s, 	Step: 129577, 	{'train/accuracy': 0.8240624666213989, 'train/loss': 0.6876732707023621, 'validation/accuracy': 0.7297599911689758, 'validation/loss': 1.080593466758728, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7161928415298462, 'test/num_examples': 10000, 'score': 57599.32408952713, 'total_duration': 62344.34480929375, 'accumulated_submission_time': 57599.32408952713, 'accumulated_eval_time': 4731.710379600525, 'accumulated_logging_time': 6.433982849121094}
I0303 04:52:48.784844 140380326586112 logging_writer.py:48] [129577] accumulated_eval_time=4731.710380, accumulated_logging_time=6.433983, accumulated_submission_time=57599.324090, global_step=129577, preemption_count=0, score=57599.324090, test/accuracy=0.600600, test/loss=1.716193, test/num_examples=10000, total_duration=62344.344809, train/accuracy=0.824062, train/loss=0.687673, validation/accuracy=0.729760, validation/loss=1.080593, validation/num_examples=50000
I0303 04:52:58.347008 140380334978816 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.7341686487197876, loss=2.455829381942749
I0303 04:53:39.623595 140380326586112 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.9228334426879883, loss=2.2789723873138428
I0303 04:54:24.432510 140380334978816 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.8971096277236938, loss=3.056866407394409
I0303 04:55:09.887371 140380326586112 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.9377942085266113, loss=3.811779260635376
I0303 04:55:54.689535 140380334978816 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.7203946113586426, loss=2.4351158142089844
I0303 04:56:39.345547 140380326586112 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.9355645179748535, loss=3.076655149459839
I0303 04:57:25.040555 140380334978816 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.9076850414276123, loss=3.473140239715576
I0303 04:58:10.002358 140380326586112 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.8285179138183594, loss=2.867271900177002
I0303 04:58:55.132646 140380334978816 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.9414727687835693, loss=2.1087565422058105
I0303 04:59:40.251070 140380326586112 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.8750430345535278, loss=1.624734878540039
I0303 04:59:48.931568 140575196817216 spec.py:321] Evaluating on the training split.
I0303 04:59:59.847315 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:00:23.079114 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:00:24.736204 140575196817216 submission_runner.py:411] Time since start: 62800.34s, 	Step: 130521, 	{'train/accuracy': 0.8061913847923279, 'train/loss': 0.74146568775177, 'validation/accuracy': 0.731660008430481, 'validation/loss': 1.0785208940505981, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.7314767837524414, 'test/num_examples': 10000, 'score': 58019.40853381157, 'total_duration': 62800.33711600304, 'accumulated_submission_time': 58019.40853381157, 'accumulated_eval_time': 4767.513851881027, 'accumulated_logging_time': 6.48798131942749}
I0303 05:00:24.774187 140380334978816 logging_writer.py:48] [130521] accumulated_eval_time=4767.513852, accumulated_logging_time=6.487981, accumulated_submission_time=58019.408534, global_step=130521, preemption_count=0, score=58019.408534, test/accuracy=0.606600, test/loss=1.731477, test/num_examples=10000, total_duration=62800.337116, train/accuracy=0.806191, train/loss=0.741466, validation/accuracy=0.731660, validation/loss=1.078521, validation/num_examples=50000
I0303 05:00:56.562174 140380326586112 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.9881492853164673, loss=1.7120375633239746
I0303 05:01:40.838236 140380334978816 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.8220926523208618, loss=1.9826891422271729
I0303 05:02:25.923518 140380326586112 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.8873800039291382, loss=1.7015708684921265
I0303 05:03:10.861194 140380334978816 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.06874418258667, loss=3.547236680984497
I0303 05:03:55.756549 140380326586112 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.926370620727539, loss=1.5660102367401123
I0303 05:04:40.593533 140380334978816 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.9773368835449219, loss=1.5990853309631348
I0303 05:05:25.793399 140380326586112 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.7899366617202759, loss=3.0082247257232666
I0303 05:06:10.662534 140380334978816 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.8490113019943237, loss=1.6179990768432617
I0303 05:06:55.737003 140380326586112 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.0633785724639893, loss=3.740635395050049
I0303 05:07:24.916391 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:07:35.668325 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:08:00.353077 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:08:02.030014 140575196817216 submission_runner.py:411] Time since start: 63257.63s, 	Step: 131467, 	{'train/accuracy': 0.8073632717132568, 'train/loss': 0.7516621947288513, 'validation/accuracy': 0.7322799563407898, 'validation/loss': 1.0729314088821411, 'validation/num_examples': 50000, 'test/accuracy': 0.605400025844574, 'test/loss': 1.702867865562439, 'test/num_examples': 10000, 'score': 58439.49217700958, 'total_duration': 63257.630944013596, 'accumulated_submission_time': 58439.49217700958, 'accumulated_eval_time': 4804.62633895874, 'accumulated_logging_time': 6.535409212112427}
I0303 05:08:02.066416 140380334978816 logging_writer.py:48] [131467] accumulated_eval_time=4804.626339, accumulated_logging_time=6.535409, accumulated_submission_time=58439.492177, global_step=131467, preemption_count=0, score=58439.492177, test/accuracy=0.605400, test/loss=1.702868, test/num_examples=10000, total_duration=63257.630944, train/accuracy=0.807363, train/loss=0.751662, validation/accuracy=0.732280, validation/loss=1.072931, validation/num_examples=50000
I0303 05:08:15.601630 140380326586112 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.7897645235061646, loss=2.549093723297119
I0303 05:08:57.187872 140380334978816 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.717163324356079, loss=1.6109297275543213
I0303 05:09:42.099577 140380326586112 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.8914155960083008, loss=1.615975022315979
I0303 05:10:27.312385 140380334978816 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.9957950115203857, loss=1.5244046449661255
I0303 05:11:12.080990 140380326586112 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.950997233390808, loss=2.1520628929138184
I0303 05:11:57.240656 140380334978816 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.897721767425537, loss=2.672748327255249
I0303 05:12:41.908209 140380326586112 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.7595949172973633, loss=1.6565752029418945
I0303 05:13:26.836815 140380334978816 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.0189263820648193, loss=1.7169321775436401
I0303 05:14:11.911519 140380326586112 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.594146728515625, loss=3.5441930294036865
I0303 05:14:57.229537 140380334978816 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.9309923648834229, loss=1.456038236618042
I0303 05:15:02.275536 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:15:13.030121 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:15:36.419562 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:15:38.087045 140575196817216 submission_runner.py:411] Time since start: 63713.69s, 	Step: 132413, 	{'train/accuracy': 0.8190429210662842, 'train/loss': 0.686676025390625, 'validation/accuracy': 0.7339000105857849, 'validation/loss': 1.062727451324463, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7026625871658325, 'test/num_examples': 10000, 'score': 58859.64246606827, 'total_duration': 63713.68802905083, 'accumulated_submission_time': 58859.64246606827, 'accumulated_eval_time': 4840.436786174774, 'accumulated_logging_time': 6.581597805023193}
I0303 05:15:38.125380 140380326586112 logging_writer.py:48] [132413] accumulated_eval_time=4840.436786, accumulated_logging_time=6.581598, accumulated_submission_time=58859.642466, global_step=132413, preemption_count=0, score=58859.642466, test/accuracy=0.610100, test/loss=1.702663, test/num_examples=10000, total_duration=63713.688029, train/accuracy=0.819043, train/loss=0.686676, validation/accuracy=0.733900, validation/loss=1.062727, validation/num_examples=50000
I0303 05:16:13.119663 140380334978816 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.264346122741699, loss=3.9045867919921875
I0303 05:16:57.462569 140380326586112 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.0327723026275635, loss=1.7554384469985962
I0303 05:17:42.663895 140380334978816 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.9671810865402222, loss=2.9359192848205566
I0303 05:18:27.829224 140380326586112 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.890078067779541, loss=1.6294564008712769
I0303 05:19:12.895456 140380334978816 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.012270927429199, loss=1.9431415796279907
I0303 05:19:57.937148 140380326586112 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.9454694986343384, loss=1.566857099533081
I0303 05:20:42.992075 140380334978816 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.0248520374298096, loss=1.6640671491622925
I0303 05:21:27.956684 140380326586112 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.8261016607284546, loss=1.671699047088623
I0303 05:22:12.824354 140380334978816 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.973885416984558, loss=1.7259292602539062
I0303 05:22:38.225341 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:22:48.958055 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:23:11.373641 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:23:13.037825 140575196817216 submission_runner.py:411] Time since start: 64168.64s, 	Step: 133358, 	{'train/accuracy': 0.809863269329071, 'train/loss': 0.7376311421394348, 'validation/accuracy': 0.735539972782135, 'validation/loss': 1.060116171836853, 'validation/num_examples': 50000, 'test/accuracy': 0.6114000082015991, 'test/loss': 1.6973094940185547, 'test/num_examples': 10000, 'score': 59279.68286252022, 'total_duration': 64168.63878440857, 'accumulated_submission_time': 59279.68286252022, 'accumulated_eval_time': 4875.248173952103, 'accumulated_logging_time': 6.630462884902954}
I0303 05:23:13.073486 140380326586112 logging_writer.py:48] [133358] accumulated_eval_time=4875.248174, accumulated_logging_time=6.630463, accumulated_submission_time=59279.682863, global_step=133358, preemption_count=0, score=59279.682863, test/accuracy=0.611400, test/loss=1.697309, test/num_examples=10000, total_duration=64168.638784, train/accuracy=0.809863, train/loss=0.737631, validation/accuracy=0.735540, validation/loss=1.060116, validation/num_examples=50000
I0303 05:23:30.177702 140380334978816 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.0148043632507324, loss=1.6769492626190186
I0303 05:24:12.477506 140380326586112 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.017324686050415, loss=1.6408779621124268
I0303 05:24:57.674548 140380334978816 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.7741307020187378, loss=2.1834256649017334
I0303 05:25:43.379197 140380326586112 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.9988288879394531, loss=1.5423107147216797
I0303 05:26:28.324761 140380334978816 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.9027754068374634, loss=2.465592861175537
I0303 05:27:13.681942 140380326586112 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.9598082304000854, loss=1.6236976385116577
I0303 05:27:58.570256 140380334978816 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.1100192070007324, loss=1.627668857574463
I0303 05:28:43.797565 140380326586112 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.8807053565979004, loss=1.8755440711975098
I0303 05:29:28.770629 140380334978816 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.0104007720947266, loss=1.6750824451446533
I0303 05:30:13.281795 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:30:24.135895 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:30:47.442297 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:30:49.107028 140575196817216 submission_runner.py:411] Time since start: 64624.71s, 	Step: 134300, 	{'train/accuracy': 0.8107226490974426, 'train/loss': 0.7090687155723572, 'validation/accuracy': 0.735539972782135, 'validation/loss': 1.045581340789795, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.6855266094207764, 'test/num_examples': 10000, 'score': 59699.82854485512, 'total_duration': 64624.70807790756, 'accumulated_submission_time': 59699.82854485512, 'accumulated_eval_time': 4911.07240653038, 'accumulated_logging_time': 6.679866790771484}
I0303 05:30:49.148008 140380326586112 logging_writer.py:48] [134300] accumulated_eval_time=4911.072407, accumulated_logging_time=6.679867, accumulated_submission_time=59699.828545, global_step=134300, preemption_count=0, score=59699.828545, test/accuracy=0.610400, test/loss=1.685527, test/num_examples=10000, total_duration=64624.708078, train/accuracy=0.810723, train/loss=0.709069, validation/accuracy=0.735540, validation/loss=1.045581, validation/num_examples=50000
I0303 05:30:49.552382 140380334978816 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.9555093050003052, loss=2.204772710800171
I0303 05:31:29.715829 140380326586112 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.8427610397338867, loss=2.9786128997802734
I0303 05:32:14.677052 140380334978816 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.9926153421401978, loss=3.6033666133880615
I0303 05:32:59.889980 140380326586112 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.9537581205368042, loss=1.5660769939422607
I0303 05:33:44.763935 140380334978816 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.9846384525299072, loss=2.0417556762695312
I0303 05:34:29.677458 140380326586112 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.0029752254486084, loss=1.445809245109558
I0303 05:35:14.711476 140380334978816 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.9100022315979004, loss=1.8676671981811523
I0303 05:35:59.590927 140380326586112 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.2025146484375, loss=3.737884044647217
I0303 05:36:44.671555 140380334978816 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.0144505500793457, loss=1.559260606765747
I0303 05:37:29.756029 140380326586112 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.088289499282837, loss=1.5735899209976196
I0303 05:37:49.522682 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:38:00.163109 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:38:24.125280 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:38:25.784995 140575196817216 submission_runner.py:411] Time since start: 65081.39s, 	Step: 135246, 	{'train/accuracy': 0.8220117092132568, 'train/loss': 0.6820235252380371, 'validation/accuracy': 0.7381199598312378, 'validation/loss': 1.0405586957931519, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.6574739217758179, 'test/num_examples': 10000, 'score': 60120.14081978798, 'total_duration': 65081.38606548309, 'accumulated_submission_time': 60120.14081978798, 'accumulated_eval_time': 4947.333732366562, 'accumulated_logging_time': 6.733776807785034}
I0303 05:38:25.822728 140380334978816 logging_writer.py:48] [135246] accumulated_eval_time=4947.333732, accumulated_logging_time=6.733777, accumulated_submission_time=60120.140820, global_step=135246, preemption_count=0, score=60120.140820, test/accuracy=0.619100, test/loss=1.657474, test/num_examples=10000, total_duration=65081.386065, train/accuracy=0.822012, train/loss=0.682024, validation/accuracy=0.738120, validation/loss=1.040559, validation/num_examples=50000
I0303 05:38:47.677491 140380326586112 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.9653093814849854, loss=1.6487960815429688
I0303 05:39:30.558309 140380334978816 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.0437982082366943, loss=1.4980801343917847
I0303 05:40:15.866111 140380326586112 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.0717592239379883, loss=1.5333163738250732
I0303 05:41:00.883067 140380334978816 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.1317615509033203, loss=2.8953726291656494
I0303 05:41:45.979056 140380326586112 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.98458993434906, loss=1.6830785274505615
I0303 05:42:31.176176 140380334978816 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.1966207027435303, loss=1.6262911558151245
I0303 05:43:16.293962 140380326586112 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.9861693382263184, loss=1.594139575958252
I0303 05:44:01.250334 140380334978816 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.968189001083374, loss=1.507967472076416
I0303 05:44:46.257309 140380326586112 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.081825017929077, loss=1.623038411140442
I0303 05:45:25.971596 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:45:37.104492 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:46:00.926378 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:46:02.600693 140575196817216 submission_runner.py:411] Time since start: 65538.20s, 	Step: 136190, 	{'train/accuracy': 0.833789050579071, 'train/loss': 0.6432842016220093, 'validation/accuracy': 0.7403799891471863, 'validation/loss': 1.0398958921432495, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.6594387292861938, 'test/num_examples': 10000, 'score': 60540.23037648201, 'total_duration': 65538.20174622536, 'accumulated_submission_time': 60540.23037648201, 'accumulated_eval_time': 4983.9618356227875, 'accumulated_logging_time': 6.781572341918945}
I0303 05:46:02.636976 140380334978816 logging_writer.py:48] [136190] accumulated_eval_time=4983.961836, accumulated_logging_time=6.781572, accumulated_submission_time=60540.230376, global_step=136190, preemption_count=0, score=60540.230376, test/accuracy=0.616600, test/loss=1.659439, test/num_examples=10000, total_duration=65538.201746, train/accuracy=0.833789, train/loss=0.643284, validation/accuracy=0.740380, validation/loss=1.039896, validation/num_examples=50000
I0303 05:46:07.014341 140380326586112 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.2964465618133545, loss=3.972701072692871
I0303 05:46:47.741259 140380334978816 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.094007968902588, loss=1.6751551628112793
I0303 05:47:32.565816 140380326586112 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.0942327976226807, loss=1.7086018323898315
I0303 05:48:17.587887 140380334978816 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.2682690620422363, loss=3.7750864028930664
I0303 05:49:02.853362 140380326586112 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.9716659784317017, loss=3.000993013381958
I0303 05:49:47.843899 140380334978816 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.8905386924743652, loss=1.585816740989685
I0303 05:50:33.160193 140380326586112 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.9389584064483643, loss=3.0672006607055664
I0303 05:51:18.247389 140380334978816 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.8484035730361938, loss=2.6337947845458984
I0303 05:52:03.446752 140380326586112 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.0009517669677734, loss=1.6232699155807495
I0303 05:52:48.322475 140380334978816 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.090012550354004, loss=3.712632656097412
I0303 05:53:02.994171 140575196817216 spec.py:321] Evaluating on the training split.
I0303 05:53:13.611479 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 05:53:36.728330 140575196817216 spec.py:349] Evaluating on the test split.
I0303 05:53:38.384657 140575196817216 submission_runner.py:411] Time since start: 65993.99s, 	Step: 137134, 	{'train/accuracy': 0.8225390315055847, 'train/loss': 0.6828349232673645, 'validation/accuracy': 0.7406799793243408, 'validation/loss': 1.026926040649414, 'validation/num_examples': 50000, 'test/accuracy': 0.6152000427246094, 'test/loss': 1.6574362516403198, 'test/num_examples': 10000, 'score': 60960.528388261795, 'total_duration': 65993.98559451103, 'accumulated_submission_time': 60960.528388261795, 'accumulated_eval_time': 5019.351192474365, 'accumulated_logging_time': 6.82807993888855}
I0303 05:53:38.423094 140380326586112 logging_writer.py:48] [137134] accumulated_eval_time=5019.351192, accumulated_logging_time=6.828080, accumulated_submission_time=60960.528388, global_step=137134, preemption_count=0, score=60960.528388, test/accuracy=0.615200, test/loss=1.657436, test/num_examples=10000, total_duration=65993.985595, train/accuracy=0.822539, train/loss=0.682835, validation/accuracy=0.740680, validation/loss=1.026926, validation/num_examples=50000
I0303 05:54:05.037817 140380334978816 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.9790799617767334, loss=3.5101699829101562
I0303 05:54:48.730882 140380326586112 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.9452505111694336, loss=2.3874361515045166
I0303 05:55:34.105652 140380334978816 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.9252007007598877, loss=1.50943922996521
I0303 05:56:19.227180 140380326586112 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.2227330207824707, loss=3.1790874004364014
I0303 05:57:03.930607 140380334978816 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.1787588596343994, loss=1.583271861076355
I0303 05:57:49.208949 140380326586112 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.9569414854049683, loss=1.6237341165542603
I0303 05:58:34.182021 140380334978816 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.9147605895996094, loss=1.947717308998108
I0303 05:59:19.412701 140380326586112 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.2943782806396484, loss=3.924272298812866
I0303 06:00:04.481528 140380334978816 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.119809865951538, loss=1.7223293781280518
I0303 06:00:38.688160 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:00:49.689838 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:01:13.226315 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:01:14.884745 140575196817216 submission_runner.py:411] Time since start: 66450.49s, 	Step: 138078, 	{'train/accuracy': 0.8241210579872131, 'train/loss': 0.6726440787315369, 'validation/accuracy': 0.7392399907112122, 'validation/loss': 1.0355180501937866, 'validation/num_examples': 50000, 'test/accuracy': 0.6131000518798828, 'test/loss': 1.6730371713638306, 'test/num_examples': 10000, 'score': 61380.73367190361, 'total_duration': 66450.48570799828, 'accumulated_submission_time': 61380.73367190361, 'accumulated_eval_time': 5055.546671152115, 'accumulated_logging_time': 6.876504421234131}
I0303 06:01:14.921484 140380326586112 logging_writer.py:48] [138078] accumulated_eval_time=5055.546671, accumulated_logging_time=6.876504, accumulated_submission_time=61380.733672, global_step=138078, preemption_count=0, score=61380.733672, test/accuracy=0.613100, test/loss=1.673037, test/num_examples=10000, total_duration=66450.485708, train/accuracy=0.824121, train/loss=0.672644, validation/accuracy=0.739240, validation/loss=1.035518, validation/num_examples=50000
I0303 06:01:24.072563 140380334978816 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.0238044261932373, loss=1.4530876874923706
I0303 06:02:05.378109 140380326586112 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.013322353363037, loss=2.0165107250213623
I0303 06:02:50.313810 140380334978816 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.117763042449951, loss=1.5265840291976929
I0303 06:03:35.718972 140380326586112 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.016852617263794, loss=1.4914911985397339
I0303 06:04:20.585738 140380334978816 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.182914972305298, loss=1.5651265382766724
I0303 06:05:05.888270 140380326586112 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.9355393648147583, loss=2.8610172271728516
I0303 06:05:50.965517 140380334978816 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.141660213470459, loss=1.5552349090576172
I0303 06:06:35.851253 140380326586112 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.9626673460006714, loss=1.8052303791046143
I0303 06:07:21.162854 140380334978816 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.998888373374939, loss=3.249569892883301
I0303 06:08:06.352582 140380326586112 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.110882520675659, loss=2.8694381713867188
I0303 06:08:15.055760 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:08:25.767102 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:08:48.101070 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:08:49.772221 140575196817216 submission_runner.py:411] Time since start: 66905.37s, 	Step: 139021, 	{'train/accuracy': 0.8309569954872131, 'train/loss': 0.6413344144821167, 'validation/accuracy': 0.7416599988937378, 'validation/loss': 1.0248628854751587, 'validation/num_examples': 50000, 'test/accuracy': 0.6114000082015991, 'test/loss': 1.6480200290679932, 'test/num_examples': 10000, 'score': 61800.80941319466, 'total_duration': 66905.37312984467, 'accumulated_submission_time': 61800.80941319466, 'accumulated_eval_time': 5090.26197385788, 'accumulated_logging_time': 6.923521041870117}
I0303 06:08:49.814594 140380334978816 logging_writer.py:48] [139021] accumulated_eval_time=5090.261974, accumulated_logging_time=6.923521, accumulated_submission_time=61800.809413, global_step=139021, preemption_count=0, score=61800.809413, test/accuracy=0.611400, test/loss=1.648020, test/num_examples=10000, total_duration=66905.373130, train/accuracy=0.830957, train/loss=0.641334, validation/accuracy=0.741660, validation/loss=1.024863, validation/num_examples=50000
I0303 06:09:21.618535 140380326586112 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.0580363273620605, loss=1.5192534923553467
I0303 06:10:05.945535 140380334978816 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.080214500427246, loss=1.521957278251648
I0303 06:10:51.124907 140380326586112 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.0818567276000977, loss=1.538482666015625
I0303 06:11:36.292899 140380334978816 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.1836464405059814, loss=1.4910569190979004
I0303 06:12:21.208730 140380326586112 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.2163987159729004, loss=3.269622564315796
I0303 06:13:06.372975 140380334978816 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.9375953674316406, loss=2.914255142211914
I0303 06:13:51.225857 140380326586112 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.7492825984954834, loss=1.4726554155349731
I0303 06:14:36.428161 140380334978816 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.3626821041107178, loss=3.8335072994232178
I0303 06:15:21.416966 140380326586112 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.221097946166992, loss=1.4860568046569824
I0303 06:15:49.852578 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:16:00.551227 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:16:24.320828 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:16:25.984479 140575196817216 submission_runner.py:411] Time since start: 67361.59s, 	Step: 139965, 	{'train/accuracy': 0.8257421851158142, 'train/loss': 0.6530791521072388, 'validation/accuracy': 0.7451399564743042, 'validation/loss': 1.004848599433899, 'validation/num_examples': 50000, 'test/accuracy': 0.6248000264167786, 'test/loss': 1.6311440467834473, 'test/num_examples': 10000, 'score': 62220.787368535995, 'total_duration': 67361.58550977707, 'accumulated_submission_time': 62220.787368535995, 'accumulated_eval_time': 5126.392836332321, 'accumulated_logging_time': 6.976944208145142}
I0303 06:16:26.021658 140380334978816 logging_writer.py:48] [139965] accumulated_eval_time=5126.392836, accumulated_logging_time=6.976944, accumulated_submission_time=62220.787369, global_step=139965, preemption_count=0, score=62220.787369, test/accuracy=0.624800, test/loss=1.631144, test/num_examples=10000, total_duration=67361.585510, train/accuracy=0.825742, train/loss=0.653079, validation/accuracy=0.745140, validation/loss=1.004849, validation/num_examples=50000
I0303 06:16:40.326331 140380326586112 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.296238660812378, loss=3.6264803409576416
I0303 06:17:22.398716 140380334978816 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.375030755996704, loss=3.7640981674194336
I0303 06:18:07.452857 140380326586112 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.0635933876037598, loss=2.6269662380218506
I0303 06:18:52.589615 140380334978816 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.1024632453918457, loss=1.6439509391784668
I0303 06:19:37.675626 140380326586112 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.611990451812744, loss=3.4583754539489746
I0303 06:20:22.677889 140380334978816 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.0358266830444336, loss=2.8687829971313477
I0303 06:21:07.778023 140380326586112 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.9475656747817993, loss=1.9257073402404785
I0303 06:21:52.621607 140380334978816 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.1167449951171875, loss=2.806643486022949
I0303 06:22:37.981292 140380326586112 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.1377809047698975, loss=1.528258204460144
I0303 06:23:22.968946 140380334978816 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.21258544921875, loss=2.1641573905944824
I0303 06:23:26.286991 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:23:36.867667 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:24:01.110621 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:24:02.768089 140575196817216 submission_runner.py:411] Time since start: 67818.37s, 	Step: 140909, 	{'train/accuracy': 0.8275976181030273, 'train/loss': 0.6520302295684814, 'validation/accuracy': 0.7456799745559692, 'validation/loss': 1.002097725868225, 'validation/num_examples': 50000, 'test/accuracy': 0.6196000576019287, 'test/loss': 1.6410871744155884, 'test/num_examples': 10000, 'score': 62640.990287303925, 'total_duration': 67818.36901831627, 'accumulated_submission_time': 62640.990287303925, 'accumulated_eval_time': 5162.872810840607, 'accumulated_logging_time': 7.0278894901275635}
I0303 06:24:02.808143 140380326586112 logging_writer.py:48] [140909] accumulated_eval_time=5162.872811, accumulated_logging_time=7.027889, accumulated_submission_time=62640.990287, global_step=140909, preemption_count=0, score=62640.990287, test/accuracy=0.619600, test/loss=1.641087, test/num_examples=10000, total_duration=67818.369018, train/accuracy=0.827598, train/loss=0.652030, validation/accuracy=0.745680, validation/loss=1.002098, validation/num_examples=50000
I0303 06:24:39.331621 140380334978816 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.0801026821136475, loss=1.5201929807662964
I0303 06:25:23.835781 140380326586112 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.100412130355835, loss=1.543688416481018
I0303 06:26:09.277856 140380334978816 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.004711866378784, loss=1.4620919227600098
I0303 06:26:54.187955 140380326586112 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.0134739875793457, loss=2.6378021240234375
I0303 06:27:39.342734 140380334978816 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.8460443019866943, loss=2.6365742683410645
I0303 06:28:24.245341 140380326586112 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.1793699264526367, loss=2.30041241645813
I0303 06:29:09.212796 140380334978816 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.997135043144226, loss=2.308561325073242
I0303 06:29:54.153935 140380326586112 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.1070327758789062, loss=1.4531848430633545
I0303 06:30:38.968354 140380334978816 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.3360817432403564, loss=1.5618810653686523
I0303 06:31:02.963424 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:31:13.600064 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:31:36.800493 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:31:38.449019 140575196817216 submission_runner.py:411] Time since start: 68274.05s, 	Step: 141855, 	{'train/accuracy': 0.8324023485183716, 'train/loss': 0.6519719362258911, 'validation/accuracy': 0.7467199563980103, 'validation/loss': 1.0059432983398438, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.6353424787521362, 'test/num_examples': 10000, 'score': 63061.086705207825, 'total_duration': 68274.05000305176, 'accumulated_submission_time': 63061.086705207825, 'accumulated_eval_time': 5198.357325792313, 'accumulated_logging_time': 7.077587604522705}
I0303 06:31:38.491531 140380326586112 logging_writer.py:48] [141855] accumulated_eval_time=5198.357326, accumulated_logging_time=7.077588, accumulated_submission_time=63061.086705, global_step=141855, preemption_count=0, score=63061.086705, test/accuracy=0.623800, test/loss=1.635342, test/num_examples=10000, total_duration=68274.050003, train/accuracy=0.832402, train/loss=0.651972, validation/accuracy=0.746720, validation/loss=1.005943, validation/num_examples=50000
I0303 06:31:56.773192 140380334978816 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.364081382751465, loss=1.5218729972839355
I0303 06:32:38.829011 140380326586112 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.1469154357910156, loss=3.290485382080078
I0303 06:33:23.771390 140380334978816 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.1474528312683105, loss=1.4526710510253906
I0303 06:34:08.730689 140380326586112 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.4497392177581787, loss=3.696619987487793
I0303 06:34:53.542834 140380334978816 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.2001452445983887, loss=1.5516911745071411
I0303 06:35:38.439803 140380326586112 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.0921132564544678, loss=1.9292020797729492
I0303 06:36:23.578964 140380334978816 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.6996910572052, loss=1.3749456405639648
I0303 06:37:08.251965 140380326586112 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.1349315643310547, loss=1.3777320384979248
I0303 06:37:53.398389 140380334978816 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.2670624256134033, loss=1.496310830116272
I0303 06:38:38.316210 140380326586112 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.9218292236328125, loss=2.1619796752929688
I0303 06:38:38.463633 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:38:49.293025 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:39:12.558612 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:39:14.217156 140575196817216 submission_runner.py:411] Time since start: 68729.82s, 	Step: 142802, 	{'train/accuracy': 0.8463085889816284, 'train/loss': 0.5910487174987793, 'validation/accuracy': 0.7475399971008301, 'validation/loss': 1.0001438856124878, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.6294469833374023, 'test/num_examples': 10000, 'score': 63481.00023508072, 'total_duration': 68729.8181681633, 'accumulated_submission_time': 63481.00023508072, 'accumulated_eval_time': 5234.109792232513, 'accumulated_logging_time': 7.129896640777588}
I0303 06:39:14.253451 140380334978816 logging_writer.py:48] [142802] accumulated_eval_time=5234.109792, accumulated_logging_time=7.129897, accumulated_submission_time=63481.000235, global_step=142802, preemption_count=0, score=63481.000235, test/accuracy=0.624100, test/loss=1.629447, test/num_examples=10000, total_duration=68729.818168, train/accuracy=0.846309, train/loss=0.591049, validation/accuracy=0.747540, validation/loss=1.000144, validation/num_examples=50000
I0303 06:39:53.989566 140380326586112 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.224944829940796, loss=2.539095401763916
I0303 06:40:38.678000 140380334978816 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.15771746635437, loss=3.854647159576416
I0303 06:41:23.945451 140380326586112 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.3230082988739014, loss=3.049143075942993
I0303 06:42:08.910520 140380334978816 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.3010191917419434, loss=1.4657363891601562
I0303 06:42:54.107545 140380326586112 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.006425619125366, loss=1.6421387195587158
I0303 06:43:38.943034 140380334978816 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.2067904472351074, loss=1.96923828125
I0303 06:44:23.878320 140380326586112 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.031280279159546, loss=1.599975347518921
I0303 06:45:08.919729 140380334978816 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.1955935955047607, loss=1.8420002460479736
I0303 06:45:53.942114 140380326586112 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.12530779838562, loss=1.73443603515625
I0303 06:46:14.521591 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:46:25.210456 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:46:48.371922 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:46:50.027140 140575196817216 submission_runner.py:411] Time since start: 69185.63s, 	Step: 143747, 	{'train/accuracy': 0.8325781226158142, 'train/loss': 0.6336491703987122, 'validation/accuracy': 0.7473799586296082, 'validation/loss': 0.996917188167572, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.624912142753601, 'test/num_examples': 10000, 'score': 63901.20878171921, 'total_duration': 69185.62810969353, 'accumulated_submission_time': 63901.20878171921, 'accumulated_eval_time': 5269.614250659943, 'accumulated_logging_time': 7.175962209701538}
I0303 06:46:50.063810 140380334978816 logging_writer.py:48] [143747] accumulated_eval_time=5269.614251, accumulated_logging_time=7.175962, accumulated_submission_time=63901.208782, global_step=143747, preemption_count=0, score=63901.208782, test/accuracy=0.623600, test/loss=1.624912, test/num_examples=10000, total_duration=69185.628110, train/accuracy=0.832578, train/loss=0.633649, validation/accuracy=0.747380, validation/loss=0.996917, validation/num_examples=50000
I0303 06:47:11.531482 140380326586112 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.3416624069213867, loss=1.652742624282837
I0303 06:47:54.310504 140380334978816 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.2926933765411377, loss=3.325472831726074
I0303 06:48:39.105290 140380326586112 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.374980926513672, loss=1.845336675643921
I0303 06:49:24.479291 140380334978816 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.466718912124634, loss=3.053581714630127
I0303 06:50:09.267355 140380326586112 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.0231494903564453, loss=1.5389822721481323
I0303 06:50:54.260378 140380334978816 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.2617902755737305, loss=1.4767593145370483
I0303 06:51:39.162240 140380326586112 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.1143486499786377, loss=1.5788798332214355
I0303 06:52:24.176163 140380334978816 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.278963327407837, loss=1.4601225852966309
I0303 06:53:09.161868 140380326586112 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.3091611862182617, loss=1.5680750608444214
I0303 06:53:50.478000 140575196817216 spec.py:321] Evaluating on the training split.
I0303 06:54:01.229865 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 06:54:25.056141 140575196817216 spec.py:349] Evaluating on the test split.
I0303 06:54:26.708736 140575196817216 submission_runner.py:411] Time since start: 69642.31s, 	Step: 144694, 	{'train/accuracy': 0.83607417345047, 'train/loss': 0.6233413219451904, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 0.9957394003868103, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.6167546510696411, 'test/num_examples': 10000, 'score': 64321.56186580658, 'total_duration': 69642.30963206291, 'accumulated_submission_time': 64321.56186580658, 'accumulated_eval_time': 5305.843818902969, 'accumulated_logging_time': 7.224336624145508}
I0303 06:54:26.754213 140380334978816 logging_writer.py:48] [144694] accumulated_eval_time=5305.843819, accumulated_logging_time=7.224337, accumulated_submission_time=64321.561866, global_step=144694, preemption_count=0, score=64321.561866, test/accuracy=0.626400, test/loss=1.616755, test/num_examples=10000, total_duration=69642.309632, train/accuracy=0.836074, train/loss=0.623341, validation/accuracy=0.749520, validation/loss=0.995739, validation/num_examples=50000
I0303 06:54:29.541813 140380326586112 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.954511284828186, loss=1.6632027626037598
I0303 06:55:10.013642 140380334978816 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.1407790184020996, loss=1.3016788959503174
I0303 06:55:54.667774 140380326586112 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.3743209838867188, loss=3.4459898471832275
I0303 06:56:40.026347 140380334978816 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.4044346809387207, loss=1.5086092948913574
I0303 06:57:25.060593 140380326586112 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.2542097568511963, loss=2.6052441596984863
I0303 06:58:10.251988 140380334978816 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.154722213745117, loss=1.443052053451538
I0303 06:58:55.244397 140380326586112 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.227652072906494, loss=1.448094129562378
I0303 06:59:40.321952 140380334978816 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.3726372718811035, loss=1.4388821125030518
I0303 07:00:25.309732 140380326586112 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.351253032684326, loss=2.74599289894104
I0303 07:01:10.257275 140380334978816 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.2086212635040283, loss=2.22347354888916
I0303 07:01:27.007925 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:01:37.689445 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:02:01.075579 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:02:02.727415 140575196817216 submission_runner.py:411] Time since start: 70098.33s, 	Step: 145639, 	{'train/accuracy': 0.8413866758346558, 'train/loss': 0.6079884767532349, 'validation/accuracy': 0.7496599555015564, 'validation/loss': 0.9920935034751892, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.6104060411453247, 'test/num_examples': 10000, 'score': 64741.75490403175, 'total_duration': 70098.32852602005, 'accumulated_submission_time': 64741.75490403175, 'accumulated_eval_time': 5341.5623569488525, 'accumulated_logging_time': 7.281407833099365}
I0303 07:02:02.763964 140380326586112 logging_writer.py:48] [145639] accumulated_eval_time=5341.562357, accumulated_logging_time=7.281408, accumulated_submission_time=64741.754904, global_step=145639, preemption_count=0, score=64741.754904, test/accuracy=0.626900, test/loss=1.610406, test/num_examples=10000, total_duration=70098.328526, train/accuracy=0.841387, train/loss=0.607988, validation/accuracy=0.749660, validation/loss=0.992094, validation/num_examples=50000
I0303 07:02:27.411744 140380334978816 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.132643938064575, loss=3.055277109146118
I0303 07:03:10.628579 140380326586112 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.5614547729492188, loss=1.52003014087677
I0303 07:03:55.764249 140380334978816 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.527651786804199, loss=1.8134952783584595
I0303 07:04:40.741925 140380326586112 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.1915104389190674, loss=1.7538862228393555
I0303 07:05:25.859416 140380334978816 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.201704502105713, loss=1.465340256690979
I0303 07:06:10.926715 140380326586112 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.4274957180023193, loss=1.5203120708465576
I0303 07:06:56.049854 140380334978816 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.1724584102630615, loss=1.4982097148895264
I0303 07:07:41.317376 140380326586112 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.314819097518921, loss=3.0979931354522705
I0303 07:08:26.541692 140380334978816 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.2621726989746094, loss=1.338906168937683
I0303 07:09:03.011391 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:09:13.741541 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:09:37.760308 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:09:39.412158 140575196817216 submission_runner.py:411] Time since start: 70555.01s, 	Step: 146583, 	{'train/accuracy': 0.8368554711341858, 'train/loss': 0.6317271590232849, 'validation/accuracy': 0.7525999546051025, 'validation/loss': 0.9833926558494568, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.6054846048355103, 'test/num_examples': 10000, 'score': 65161.944747686386, 'total_duration': 70555.01319146156, 'accumulated_submission_time': 65161.944747686386, 'accumulated_eval_time': 5377.9620950222015, 'accumulated_logging_time': 7.32740592956543}
I0303 07:09:39.453578 140380326586112 logging_writer.py:48] [146583] accumulated_eval_time=5377.962095, accumulated_logging_time=7.327406, accumulated_submission_time=65161.944748, global_step=146583, preemption_count=0, score=65161.944748, test/accuracy=0.630500, test/loss=1.605485, test/num_examples=10000, total_duration=70555.013191, train/accuracy=0.836855, train/loss=0.631727, validation/accuracy=0.752600, validation/loss=0.983393, validation/num_examples=50000
I0303 07:09:46.596097 140380334978816 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.1151745319366455, loss=1.81380033493042
I0303 07:10:27.497413 140380326586112 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.1855900287628174, loss=1.4849929809570312
I0303 07:11:12.244391 140380334978816 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.261828660964966, loss=3.3296735286712646
I0303 07:11:57.436815 140380326586112 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.633171319961548, loss=3.604992151260376
I0303 07:12:42.241282 140380334978816 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.322277069091797, loss=1.3379788398742676
I0303 07:13:27.230752 140380326586112 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.282780647277832, loss=3.174593687057495
I0303 07:14:11.944761 140380334978816 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.2399325370788574, loss=1.7255420684814453
I0303 07:14:56.843010 140380326586112 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.3030476570129395, loss=2.902533531188965
I0303 07:15:41.791362 140380334978816 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.1453356742858887, loss=1.563055396080017
I0303 07:16:26.712210 140380326586112 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.5592539310455322, loss=1.4190170764923096
I0303 07:16:39.812095 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:16:50.378254 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:17:13.493016 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:17:15.144455 140575196817216 submission_runner.py:411] Time since start: 71010.75s, 	Step: 147531, 	{'train/accuracy': 0.8406640291213989, 'train/loss': 0.6047345399856567, 'validation/accuracy': 0.7533400058746338, 'validation/loss': 0.9802722930908203, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.5938471555709839, 'test/num_examples': 10000, 'score': 65582.24419879913, 'total_duration': 71010.74550557137, 'accumulated_submission_time': 65582.24419879913, 'accumulated_eval_time': 5413.293456554413, 'accumulated_logging_time': 7.379019498825073}
I0303 07:17:15.186609 140380334978816 logging_writer.py:48] [147531] accumulated_eval_time=5413.293457, accumulated_logging_time=7.379019, accumulated_submission_time=65582.244199, global_step=147531, preemption_count=0, score=65582.244199, test/accuracy=0.631900, test/loss=1.593847, test/num_examples=10000, total_duration=71010.745506, train/accuracy=0.840664, train/loss=0.604735, validation/accuracy=0.753340, validation/loss=0.980272, validation/num_examples=50000
I0303 07:17:43.020798 140380326586112 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.3348278999328613, loss=1.3008122444152832
I0303 07:18:27.556766 140380334978816 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.5074355602264404, loss=1.4295748472213745
I0303 07:19:12.630816 140380326586112 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.1640453338623047, loss=2.198681116104126
I0303 07:19:57.721185 140380334978816 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.2338385581970215, loss=1.3435052633285522
I0303 07:20:42.629882 140380326586112 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.343959331512451, loss=1.559950590133667
I0303 07:21:27.742944 140380334978816 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.2769083976745605, loss=1.933759331703186
I0303 07:22:12.620721 140380326586112 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.4860877990722656, loss=2.4780526161193848
I0303 07:22:57.666080 140380334978816 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.2431185245513916, loss=1.6809210777282715
I0303 07:23:42.639793 140380326586112 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.5731732845306396, loss=3.595998764038086
I0303 07:24:15.267450 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:24:25.756576 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:24:49.951288 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:24:51.613690 140575196817216 submission_runner.py:411] Time since start: 71467.21s, 	Step: 148474, 	{'train/accuracy': 0.8462694883346558, 'train/loss': 0.5779913067817688, 'validation/accuracy': 0.7558599710464478, 'validation/loss': 0.9657291173934937, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.5970369577407837, 'test/num_examples': 10000, 'score': 66002.26580381393, 'total_duration': 71467.2146782875, 'accumulated_submission_time': 66002.26580381393, 'accumulated_eval_time': 5449.63863158226, 'accumulated_logging_time': 7.4307496547698975}
I0303 07:24:51.651352 140380334978816 logging_writer.py:48] [148474] accumulated_eval_time=5449.638632, accumulated_logging_time=7.430750, accumulated_submission_time=66002.265804, global_step=148474, preemption_count=0, score=66002.265804, test/accuracy=0.628800, test/loss=1.597037, test/num_examples=10000, total_duration=71467.214678, train/accuracy=0.846269, train/loss=0.577991, validation/accuracy=0.755860, validation/loss=0.965729, validation/num_examples=50000
I0303 07:25:02.382687 140380326586112 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.358483076095581, loss=1.4049774408340454
I0303 07:25:43.679730 140380334978816 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.6416001319885254, loss=3.816357135772705
I0303 07:26:28.956323 140380326586112 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.4441611766815186, loss=1.4125264883041382
I0303 07:27:14.043552 140380334978816 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.9208786487579346, loss=1.5942957401275635
I0303 07:27:59.336309 140380326586112 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.257344961166382, loss=1.7285085916519165
I0303 07:28:44.315635 140380334978816 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.6757240295410156, loss=3.588080644607544
I0303 07:29:29.282269 140380326586112 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.5107784271240234, loss=1.4660613536834717
I0303 07:30:14.112962 140380334978816 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.429218292236328, loss=1.5038371086120605
I0303 07:30:59.141953 140380326586112 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.753157138824463, loss=3.75333309173584
I0303 07:31:44.243643 140380334978816 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.306755542755127, loss=1.446681261062622
I0303 07:31:52.069109 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:32:02.805358 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:32:26.200112 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:32:27.854869 140575196817216 submission_runner.py:411] Time since start: 71923.46s, 	Step: 149419, 	{'train/accuracy': 0.8524804711341858, 'train/loss': 0.5546852946281433, 'validation/accuracy': 0.7554199695587158, 'validation/loss': 0.9629727602005005, 'validation/num_examples': 50000, 'test/accuracy': 0.6330000162124634, 'test/loss': 1.5846474170684814, 'test/num_examples': 10000, 'score': 66422.62491846085, 'total_duration': 71923.45594525337, 'accumulated_submission_time': 66422.62491846085, 'accumulated_eval_time': 5485.423399209976, 'accumulated_logging_time': 7.478578805923462}
I0303 07:32:27.892412 140380326586112 logging_writer.py:48] [149419] accumulated_eval_time=5485.423399, accumulated_logging_time=7.478579, accumulated_submission_time=66422.624918, global_step=149419, preemption_count=0, score=66422.624918, test/accuracy=0.633000, test/loss=1.584647, test/num_examples=10000, total_duration=71923.455945, train/accuracy=0.852480, train/loss=0.554685, validation/accuracy=0.755420, validation/loss=0.962973, validation/num_examples=50000
I0303 07:33:00.491897 140380334978816 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.192929744720459, loss=1.7241804599761963
I0303 07:33:44.539927 140380326586112 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.511565685272217, loss=1.6010701656341553
I0303 07:34:29.684032 140380334978816 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.4375808238983154, loss=1.4454654455184937
I0303 07:35:14.679820 140380326586112 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.3007736206054688, loss=1.3906853199005127
I0303 07:36:00.238326 140380334978816 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.672427177429199, loss=1.481787919998169
I0303 07:36:44.664565 140380326586112 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.3976569175720215, loss=2.3881566524505615
I0303 07:37:29.278538 140380334978816 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.5336761474609375, loss=1.3869727849960327
I0303 07:38:14.762735 140380326586112 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.316758155822754, loss=2.9554593563079834
I0303 07:38:59.803559 140380334978816 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.3954691886901855, loss=1.3427362442016602
I0303 07:39:28.011913 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:39:38.870531 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:40:01.748264 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:40:03.428261 140575196817216 submission_runner.py:411] Time since start: 72379.03s, 	Step: 150364, 	{'train/accuracy': 0.845996081829071, 'train/loss': 0.5830208659172058, 'validation/accuracy': 0.7575799822807312, 'validation/loss': 0.9540197253227234, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.5807474851608276, 'test/num_examples': 10000, 'score': 66842.6845741272, 'total_duration': 72379.02922987938, 'accumulated_submission_time': 66842.6845741272, 'accumulated_eval_time': 5520.838660478592, 'accumulated_logging_time': 7.52655816078186}
I0303 07:40:03.473583 140380326586112 logging_writer.py:48] [150364] accumulated_eval_time=5520.838660, accumulated_logging_time=7.526558, accumulated_submission_time=66842.684574, global_step=150364, preemption_count=0, score=66842.684574, test/accuracy=0.634100, test/loss=1.580747, test/num_examples=10000, total_duration=72379.029230, train/accuracy=0.845996, train/loss=0.583021, validation/accuracy=0.757580, validation/loss=0.954020, validation/num_examples=50000
I0303 07:40:18.188180 140380334978816 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.1479992866516113, loss=1.920900821685791
I0303 07:41:00.070124 140380326586112 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.30867600440979, loss=2.471071243286133
I0303 07:41:45.126118 140380334978816 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.4594295024871826, loss=2.192826271057129
I0303 07:42:30.232718 140380326586112 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.241487979888916, loss=1.591971516609192
I0303 07:43:15.221227 140380334978816 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.543517589569092, loss=1.510359764099121
I0303 07:44:00.150278 140380326586112 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.39304780960083, loss=2.428544759750366
I0303 07:44:45.228145 140380334978816 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.544567108154297, loss=1.3805687427520752
I0303 07:45:30.411372 140380326586112 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.4069244861602783, loss=1.2708042860031128
I0303 07:46:15.537353 140380334978816 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.3623859882354736, loss=1.8820397853851318
I0303 07:47:00.447732 140380326586112 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.316721200942993, loss=1.43012535572052
I0303 07:47:03.769435 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:47:14.358143 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:47:38.313491 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:47:39.976866 140575196817216 submission_runner.py:411] Time since start: 72835.58s, 	Step: 151309, 	{'train/accuracy': 0.8492968678474426, 'train/loss': 0.5629798173904419, 'validation/accuracy': 0.7577599883079529, 'validation/loss': 0.9516875743865967, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.5633233785629272, 'test/num_examples': 10000, 'score': 67262.9191057682, 'total_duration': 72835.57788276672, 'accumulated_submission_time': 67262.9191057682, 'accumulated_eval_time': 5557.04504776001, 'accumulated_logging_time': 7.584197282791138}
I0303 07:47:40.024832 140380334978816 logging_writer.py:48] [151309] accumulated_eval_time=5557.045048, accumulated_logging_time=7.584197, accumulated_submission_time=67262.919106, global_step=151309, preemption_count=0, score=67262.919106, test/accuracy=0.634100, test/loss=1.563323, test/num_examples=10000, total_duration=72835.577883, train/accuracy=0.849297, train/loss=0.562980, validation/accuracy=0.757760, validation/loss=0.951688, validation/num_examples=50000
I0303 07:48:16.949037 140380326586112 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.341742753982544, loss=2.888561487197876
I0303 07:49:01.840930 140380334978816 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.3686089515686035, loss=1.3122578859329224
I0303 07:49:47.065003 140380326586112 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.8344552516937256, loss=3.4699997901916504
I0303 07:50:32.167425 140380334978816 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.596320867538452, loss=2.4259278774261475
I0303 07:51:17.043251 140380326586112 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.4239132404327393, loss=2.349001407623291
I0303 07:52:01.952638 140380334978816 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.363424777984619, loss=2.6043999195098877
I0303 07:52:46.672223 140380326586112 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.4330594539642334, loss=1.3378973007202148
I0303 07:53:31.699377 140380334978816 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.3819189071655273, loss=1.3125649690628052
I0303 07:54:16.302613 140380326586112 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.249051809310913, loss=1.750856876373291
I0303 07:54:40.319818 140575196817216 spec.py:321] Evaluating on the training split.
I0303 07:54:50.865385 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 07:55:14.146145 140575196817216 spec.py:349] Evaluating on the test split.
I0303 07:55:15.800743 140575196817216 submission_runner.py:411] Time since start: 73291.40s, 	Step: 152255, 	{'train/accuracy': 0.8557031154632568, 'train/loss': 0.5337830781936646, 'validation/accuracy': 0.7605400085449219, 'validation/loss': 0.942303478717804, 'validation/num_examples': 50000, 'test/accuracy': 0.636400043964386, 'test/loss': 1.5599946975708008, 'test/num_examples': 10000, 'score': 67683.15299010277, 'total_duration': 73291.40171384811, 'accumulated_submission_time': 67683.15299010277, 'accumulated_eval_time': 5592.5248901844025, 'accumulated_logging_time': 7.643503665924072}
I0303 07:55:15.842598 140380334978816 logging_writer.py:48] [152255] accumulated_eval_time=5592.524890, accumulated_logging_time=7.643504, accumulated_submission_time=67683.152990, global_step=152255, preemption_count=0, score=67683.152990, test/accuracy=0.636400, test/loss=1.559995, test/num_examples=10000, total_duration=73291.401714, train/accuracy=0.855703, train/loss=0.533783, validation/accuracy=0.760540, validation/loss=0.942303, validation/num_examples=50000
I0303 07:55:34.136868 140380326586112 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.5774974822998047, loss=1.2792184352874756
I0303 07:56:16.477086 140380334978816 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.424898147583008, loss=1.322003960609436
I0303 07:57:01.571680 140380326586112 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.5916340351104736, loss=1.3441519737243652
I0303 07:57:46.851667 140380334978816 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.1602418422698975, loss=1.8494648933410645
I0303 07:58:31.982151 140380326586112 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.8479392528533936, loss=3.437047243118286
I0303 07:59:16.920012 140380334978816 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.5034749507904053, loss=1.663490653038025
I0303 08:00:02.013914 140380326586112 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.694766044616699, loss=1.397149682044983
I0303 08:00:46.746208 140380334978816 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.4702484607696533, loss=2.963440179824829
I0303 08:01:31.739860 140380326586112 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.5187747478485107, loss=1.4864435195922852
I0303 08:02:15.831292 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:02:26.347008 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:02:49.862702 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:02:51.521804 140575196817216 submission_runner.py:411] Time since start: 73747.12s, 	Step: 153200, 	{'train/accuracy': 0.8529492020606995, 'train/loss': 0.5705944895744324, 'validation/accuracy': 0.7596399784088135, 'validation/loss': 0.9485294222831726, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.5622645616531372, 'test/num_examples': 10000, 'score': 68103.0815103054, 'total_duration': 73747.12293791771, 'accumulated_submission_time': 68103.0815103054, 'accumulated_eval_time': 5628.214492797852, 'accumulated_logging_time': 7.696725130081177}
I0303 08:02:51.567203 140380334978816 logging_writer.py:48] [153200] accumulated_eval_time=5628.214493, accumulated_logging_time=7.696725, accumulated_submission_time=68103.081510, global_step=153200, preemption_count=0, score=68103.081510, test/accuracy=0.639100, test/loss=1.562265, test/num_examples=10000, total_duration=73747.122938, train/accuracy=0.852949, train/loss=0.570594, validation/accuracy=0.759640, validation/loss=0.948529, validation/num_examples=50000
I0303 08:02:51.972733 140380326586112 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.362126588821411, loss=1.2274726629257202
I0303 08:03:32.142401 140380334978816 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.7316951751708984, loss=2.8313751220703125
I0303 08:04:16.867662 140380326586112 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.5909132957458496, loss=1.2788902521133423
I0303 08:05:01.814682 140380334978816 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.5248334407806396, loss=1.4899033308029175
I0303 08:05:46.965186 140380326586112 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.5620436668395996, loss=1.395207166671753
I0303 08:06:32.081130 140380334978816 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.2946648597717285, loss=2.1930713653564453
I0303 08:07:16.966964 140380326586112 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.367800712585449, loss=1.4554831981658936
I0303 08:08:02.449135 140380334978816 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.895956039428711, loss=3.6064298152923584
I0303 08:08:47.867919 140380326586112 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.5590829849243164, loss=1.6358177661895752
I0303 08:09:33.424511 140380334978816 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.0851433277130127, loss=3.4700305461883545
I0303 08:09:51.566404 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:10:02.347708 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:10:25.956865 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:10:27.601979 140575196817216 submission_runner.py:411] Time since start: 74203.20s, 	Step: 154142, 	{'train/accuracy': 0.8538476228713989, 'train/loss': 0.5499022006988525, 'validation/accuracy': 0.7615799903869629, 'validation/loss': 0.9385902881622314, 'validation/num_examples': 50000, 'test/accuracy': 0.6353000402450562, 'test/loss': 1.5707690715789795, 'test/num_examples': 10000, 'score': 68523.02074098587, 'total_duration': 74203.20285248756, 'accumulated_submission_time': 68523.02074098587, 'accumulated_eval_time': 5664.248873949051, 'accumulated_logging_time': 7.753176212310791}
I0303 08:10:27.643125 140380326586112 logging_writer.py:48] [154142] accumulated_eval_time=5664.248874, accumulated_logging_time=7.753176, accumulated_submission_time=68523.020741, global_step=154142, preemption_count=0, score=68523.020741, test/accuracy=0.635300, test/loss=1.570769, test/num_examples=10000, total_duration=74203.202852, train/accuracy=0.853848, train/loss=0.549902, validation/accuracy=0.761580, validation/loss=0.938590, validation/num_examples=50000
I0303 08:10:51.073600 140380334978816 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.5880167484283447, loss=1.3700823783874512
I0303 08:11:34.381130 140380326586112 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.5172343254089355, loss=1.4441314935684204
I0303 08:12:19.708450 140380334978816 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.598966360092163, loss=1.4203404188156128
I0303 08:13:05.126477 140380326586112 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.3254003524780273, loss=1.322667121887207
I0303 08:13:50.211657 140380334978816 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.273348569869995, loss=3.568195104598999
I0303 08:14:35.376016 140380326586112 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.393218994140625, loss=1.7565816640853882
I0303 08:15:20.384923 140380334978816 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.5691487789154053, loss=1.4314124584197998
I0303 08:16:05.347471 140380326586112 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.3335366249084473, loss=1.4538109302520752
I0303 08:16:50.727788 140380334978816 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.467741012573242, loss=1.2451287508010864
I0303 08:17:27.967331 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:17:38.667607 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:18:03.701601 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:18:05.351107 140575196817216 submission_runner.py:411] Time since start: 74660.95s, 	Step: 155085, 	{'train/accuracy': 0.8555077910423279, 'train/loss': 0.5564358234405518, 'validation/accuracy': 0.759880006313324, 'validation/loss': 0.9469699859619141, 'validation/num_examples': 50000, 'test/accuracy': 0.6333000063896179, 'test/loss': 1.566225528717041, 'test/num_examples': 10000, 'score': 68943.2842502594, 'total_duration': 74660.95222091675, 'accumulated_submission_time': 68943.2842502594, 'accumulated_eval_time': 5701.631700754166, 'accumulated_logging_time': 7.805613040924072}
I0303 08:18:05.395284 140380326586112 logging_writer.py:48] [155085] accumulated_eval_time=5701.631701, accumulated_logging_time=7.805613, accumulated_submission_time=68943.284250, global_step=155085, preemption_count=0, score=68943.284250, test/accuracy=0.633300, test/loss=1.566226, test/num_examples=10000, total_duration=74660.952221, train/accuracy=0.855508, train/loss=0.556436, validation/accuracy=0.759880, validation/loss=0.946970, validation/num_examples=50000
I0303 08:18:11.756470 140380334978816 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.390298843383789, loss=1.3307263851165771
I0303 08:18:52.685641 140380326586112 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.533987283706665, loss=1.575775384902954
I0303 08:19:37.731179 140380334978816 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.6250829696655273, loss=1.4069980382919312
I0303 08:20:22.855713 140380326586112 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.7625434398651123, loss=3.003505229949951
I0303 08:21:07.699176 140380334978816 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.685511350631714, loss=1.4214928150177002
I0303 08:21:52.584623 140380326586112 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.2707338333129883, loss=2.6487436294555664
I0303 08:22:37.264826 140380334978816 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.402759552001953, loss=2.5238747596740723
I0303 08:23:22.377123 140380326586112 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.367278575897217, loss=1.3619307279586792
I0303 08:24:07.331549 140380334978816 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.6551060676574707, loss=1.3757938146591187
I0303 08:24:52.486942 140380326586112 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.906003475189209, loss=1.486573576927185
I0303 08:25:05.601625 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:25:16.137139 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:25:40.138808 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:25:41.790101 140575196817216 submission_runner.py:411] Time since start: 75117.39s, 	Step: 156031, 	{'train/accuracy': 0.8609960675239563, 'train/loss': 0.5209859609603882, 'validation/accuracy': 0.7628200054168701, 'validation/loss': 0.931879997253418, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.5529048442840576, 'test/num_examples': 10000, 'score': 69363.431671381, 'total_duration': 75117.39098858833, 'accumulated_submission_time': 69363.431671381, 'accumulated_eval_time': 5737.8189878463745, 'accumulated_logging_time': 7.8594958782196045}
I0303 08:25:41.839123 140380334978816 logging_writer.py:48] [156031] accumulated_eval_time=5737.818988, accumulated_logging_time=7.859496, accumulated_submission_time=69363.431671, global_step=156031, preemption_count=0, score=69363.431671, test/accuracy=0.637300, test/loss=1.552905, test/num_examples=10000, total_duration=75117.390989, train/accuracy=0.860996, train/loss=0.520986, validation/accuracy=0.762820, validation/loss=0.931880, validation/num_examples=50000
I0303 08:26:09.663247 140380326586112 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.698620319366455, loss=3.0389883518218994
I0303 08:26:53.417328 140380334978816 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.5955586433410645, loss=1.534433364868164
I0303 08:27:38.180474 140380326586112 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.639497756958008, loss=1.4040918350219727
I0303 08:28:24.519588 140380334978816 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.0410072803497314, loss=3.5901222229003906
I0303 08:29:09.537636 140380326586112 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.9657952785491943, loss=1.205240249633789
I0303 08:29:54.766594 140380334978816 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.7786386013031006, loss=3.3779335021972656
I0303 08:30:39.702752 140380326586112 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.3696422576904297, loss=2.1685783863067627
I0303 08:31:24.875038 140380334978816 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.489969253540039, loss=1.3719348907470703
I0303 08:32:09.856996 140380326586112 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.4005720615386963, loss=2.9381089210510254
I0303 08:32:42.148026 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:32:52.924833 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:33:15.200662 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:33:16.856430 140575196817216 submission_runner.py:411] Time since start: 75572.46s, 	Step: 156974, 	{'train/accuracy': 0.85804682970047, 'train/loss': 0.5311560034751892, 'validation/accuracy': 0.764959990978241, 'validation/loss': 0.9247118234634399, 'validation/num_examples': 50000, 'test/accuracy': 0.6371000409126282, 'test/loss': 1.5472129583358765, 'test/num_examples': 10000, 'score': 69783.68109440804, 'total_duration': 75572.45741295815, 'accumulated_submission_time': 69783.68109440804, 'accumulated_eval_time': 5772.52631187439, 'accumulated_logging_time': 7.919899225234985}
I0303 08:33:16.895175 140380334978816 logging_writer.py:48] [156974] accumulated_eval_time=5772.526312, accumulated_logging_time=7.919899, accumulated_submission_time=69783.681094, global_step=156974, preemption_count=0, score=69783.681094, test/accuracy=0.637100, test/loss=1.547213, test/num_examples=10000, total_duration=75572.457413, train/accuracy=0.858047, train/loss=0.531156, validation/accuracy=0.764960, validation/loss=0.924712, validation/num_examples=50000
I0303 08:33:27.635896 140380326586112 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.552619695663452, loss=1.4892657995224
I0303 08:34:09.176602 140380334978816 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.677820920944214, loss=1.3281844854354858
I0303 08:34:53.904548 140380326586112 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.526615858078003, loss=1.7687337398529053
I0303 08:35:39.165366 140380334978816 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.6139450073242188, loss=1.491114616394043
I0303 08:36:24.294066 140380326586112 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.8842036724090576, loss=3.1765666007995605
I0303 08:37:09.407325 140380334978816 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.501797914505005, loss=2.1107661724090576
I0303 08:37:54.150054 140380326586112 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.865001916885376, loss=3.3693084716796875
I0303 08:38:39.252847 140380334978816 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.5860581398010254, loss=1.350479006767273
I0303 08:39:24.210093 140380326586112 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.679311513900757, loss=1.3279629945755005
I0303 08:40:09.245145 140380334978816 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.6438047885894775, loss=1.3171101808547974
I0303 08:40:16.952870 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:40:27.651955 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:40:51.136677 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:40:52.790892 140575196817216 submission_runner.py:411] Time since start: 76028.39s, 	Step: 157919, 	{'train/accuracy': 0.8621679544448853, 'train/loss': 0.5378159880638123, 'validation/accuracy': 0.7645599842071533, 'validation/loss': 0.9374600052833557, 'validation/num_examples': 50000, 'test/accuracy': 0.6377000212669373, 'test/loss': 1.5564005374908447, 'test/num_examples': 10000, 'score': 70203.67983937263, 'total_duration': 76028.39196753502, 'accumulated_submission_time': 70203.67983937263, 'accumulated_eval_time': 5808.363347053528, 'accumulated_logging_time': 7.96866250038147}
I0303 08:40:52.829829 140380326586112 logging_writer.py:48] [157919] accumulated_eval_time=5808.363347, accumulated_logging_time=7.968663, accumulated_submission_time=70203.679839, global_step=157919, preemption_count=0, score=70203.679839, test/accuracy=0.637700, test/loss=1.556401, test/num_examples=10000, total_duration=76028.391968, train/accuracy=0.862168, train/loss=0.537816, validation/accuracy=0.764560, validation/loss=0.937460, validation/num_examples=50000
I0303 08:41:25.434264 140380334978816 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.4590065479278564, loss=1.657212495803833
I0303 08:42:09.558148 140380326586112 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.3797833919525146, loss=1.2607448101043701
I0303 08:42:54.816458 140380334978816 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.8368332386016846, loss=3.137230634689331
I0303 08:43:39.751700 140380326586112 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.5206003189086914, loss=1.4909002780914307
I0303 08:44:24.838705 140380334978816 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.699774980545044, loss=2.5521090030670166
I0303 08:45:09.915917 140380326586112 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.6054317951202393, loss=1.1939388513565063
I0303 08:45:54.859449 140380334978816 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.565253973007202, loss=1.3929243087768555
I0303 08:46:40.032173 140380326586112 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.6463325023651123, loss=1.2399708032608032
I0303 08:47:24.742119 140380334978816 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.55214524269104, loss=1.283644676208496
I0303 08:47:53.068385 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:48:03.870133 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:48:28.469067 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:48:30.125248 140575196817216 submission_runner.py:411] Time since start: 76485.73s, 	Step: 158865, 	{'train/accuracy': 0.8666406273841858, 'train/loss': 0.5006186962127686, 'validation/accuracy': 0.7650799751281738, 'validation/loss': 0.9160201549530029, 'validation/num_examples': 50000, 'test/accuracy': 0.6474000215530396, 'test/loss': 1.5338735580444336, 'test/num_examples': 10000, 'score': 70623.86025309563, 'total_duration': 76485.72631263733, 'accumulated_submission_time': 70623.86025309563, 'accumulated_eval_time': 5845.419222831726, 'accumulated_logging_time': 8.01696515083313}
I0303 08:48:30.166376 140380326586112 logging_writer.py:48] [158865] accumulated_eval_time=5845.419223, accumulated_logging_time=8.016965, accumulated_submission_time=70623.860253, global_step=158865, preemption_count=0, score=70623.860253, test/accuracy=0.647400, test/loss=1.533874, test/num_examples=10000, total_duration=76485.726313, train/accuracy=0.866641, train/loss=0.500619, validation/accuracy=0.765080, validation/loss=0.916020, validation/num_examples=50000
I0303 08:48:44.469091 140380334978816 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.7333147525787354, loss=1.4510852098464966
I0303 08:49:26.364652 140380326586112 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.776750087738037, loss=1.3054718971252441
I0303 08:50:11.336373 140380334978816 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.534526824951172, loss=1.431633710861206
I0303 08:50:56.462345 140380326586112 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.7621960639953613, loss=1.4067944288253784
I0303 08:51:41.593559 140380334978816 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.541383743286133, loss=2.0189011096954346
I0303 08:52:26.930638 140380326586112 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.9866111278533936, loss=3.548051595687866
I0303 08:53:11.948588 140380334978816 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.6395998001098633, loss=1.302970290184021
I0303 08:53:56.964741 140380326586112 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.7976229190826416, loss=1.3041701316833496
I0303 08:54:42.003038 140380334978816 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.4264109134674072, loss=1.240919589996338
I0303 08:55:27.137919 140380326586112 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.6917383670806885, loss=1.3250755071640015
I0303 08:55:30.427691 140575196817216 spec.py:321] Evaluating on the training split.
I0303 08:55:42.381116 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 08:56:05.637279 140575196817216 spec.py:349] Evaluating on the test split.
I0303 08:56:07.290628 140575196817216 submission_runner.py:411] Time since start: 76942.89s, 	Step: 159809, 	{'train/accuracy': 0.862597644329071, 'train/loss': 0.5141065120697021, 'validation/accuracy': 0.7669199705123901, 'validation/loss': 0.9152435064315796, 'validation/num_examples': 50000, 'test/accuracy': 0.6447000503540039, 'test/loss': 1.5281864404678345, 'test/num_examples': 10000, 'score': 71044.06207823753, 'total_duration': 76942.89191150665, 'accumulated_submission_time': 71044.06207823753, 'accumulated_eval_time': 5882.281366825104, 'accumulated_logging_time': 8.068997144699097}
I0303 08:56:07.335388 140380334978816 logging_writer.py:48] [159809] accumulated_eval_time=5882.281367, accumulated_logging_time=8.068997, accumulated_submission_time=71044.062078, global_step=159809, preemption_count=0, score=71044.062078, test/accuracy=0.644700, test/loss=1.528186, test/num_examples=10000, total_duration=76942.891912, train/accuracy=0.862598, train/loss=0.514107, validation/accuracy=0.766920, validation/loss=0.915244, validation/num_examples=50000
I0303 08:56:43.896677 140380326586112 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.1465821266174316, loss=3.3513588905334473
I0303 08:57:28.585433 140380334978816 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.631366014480591, loss=1.284017562866211
I0303 08:58:13.505223 140380326586112 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.8968958854675293, loss=1.3248260021209717
I0303 08:58:59.038197 140380334978816 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.769714117050171, loss=1.174865484237671
I0303 08:59:43.849223 140380326586112 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.517735004425049, loss=3.390052080154419
I0303 09:00:29.148139 140380334978816 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.8104934692382812, loss=2.869513750076294
I0303 09:01:14.102136 140380326586112 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.5801939964294434, loss=1.3751206398010254
I0303 09:01:59.089660 140380334978816 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.842400074005127, loss=2.7653980255126953
I0303 09:02:44.058189 140380326586112 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.750579833984375, loss=1.9063247442245483
I0303 09:03:07.627180 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:03:18.305438 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:03:43.161017 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:03:44.808068 140575196817216 submission_runner.py:411] Time since start: 77400.41s, 	Step: 160754, 	{'train/accuracy': 0.8669140338897705, 'train/loss': 0.5135356187820435, 'validation/accuracy': 0.7686799764633179, 'validation/loss': 0.9221277236938477, 'validation/num_examples': 50000, 'test/accuracy': 0.6433000564575195, 'test/loss': 1.548334002494812, 'test/num_examples': 10000, 'score': 71464.29527115822, 'total_duration': 77400.4092001915, 'accumulated_submission_time': 71464.29527115822, 'accumulated_eval_time': 5919.461312532425, 'accumulated_logging_time': 8.124317407608032}
I0303 09:03:44.855147 140380334978816 logging_writer.py:48] [160754] accumulated_eval_time=5919.461313, accumulated_logging_time=8.124317, accumulated_submission_time=71464.295271, global_step=160754, preemption_count=0, score=71464.295271, test/accuracy=0.643300, test/loss=1.548334, test/num_examples=10000, total_duration=77400.409200, train/accuracy=0.866914, train/loss=0.513536, validation/accuracy=0.768680, validation/loss=0.922128, validation/num_examples=50000
I0303 09:04:03.555145 140380326586112 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.5511064529418945, loss=1.308764100074768
I0303 09:04:45.838358 140380334978816 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.5240442752838135, loss=3.3413491249084473
I0303 09:05:30.923340 140380326586112 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.5599780082702637, loss=2.090188503265381
I0303 09:06:16.287742 140380334978816 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.7969603538513184, loss=1.268789529800415
I0303 09:07:01.454768 140380326586112 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.540492534637451, loss=1.4037890434265137
I0303 09:07:46.400968 140380334978816 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.542219400405884, loss=1.166046380996704
I0303 09:08:31.718739 140380326586112 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.7443809509277344, loss=2.058436632156372
I0303 09:09:16.599466 140380334978816 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.6809794902801514, loss=1.8371320962905884
I0303 09:10:01.337342 140380326586112 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.6451683044433594, loss=2.3228349685668945
I0303 09:10:45.185280 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:10:56.034257 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:11:20.256111 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:11:21.918779 140575196817216 submission_runner.py:411] Time since start: 77857.52s, 	Step: 161699, 	{'train/accuracy': 0.8700976371765137, 'train/loss': 0.5003441572189331, 'validation/accuracy': 0.768619954586029, 'validation/loss': 0.916907787322998, 'validation/num_examples': 50000, 'test/accuracy': 0.648900032043457, 'test/loss': 1.5213313102722168, 'test/num_examples': 10000, 'score': 71884.56301403046, 'total_duration': 77857.51973557472, 'accumulated_submission_time': 71884.56301403046, 'accumulated_eval_time': 5956.193717479706, 'accumulated_logging_time': 8.185054779052734}
I0303 09:11:21.960985 140380334978816 logging_writer.py:48] [161699] accumulated_eval_time=5956.193717, accumulated_logging_time=8.185055, accumulated_submission_time=71884.563014, global_step=161699, preemption_count=0, score=71884.563014, test/accuracy=0.648900, test/loss=1.521331, test/num_examples=10000, total_duration=77857.519736, train/accuracy=0.870098, train/loss=0.500344, validation/accuracy=0.768620, validation/loss=0.916908, validation/num_examples=50000
I0303 09:11:22.766382 140380326586112 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.8180854320526123, loss=1.3824858665466309
I0303 09:12:02.961670 140380334978816 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.973179817199707, loss=1.34706711769104
I0303 09:12:47.685148 140380326586112 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.6474080085754395, loss=1.375661015510559
I0303 09:13:33.072447 140380334978816 logging_writer.py:48] [162000] global_step=162000, grad_norm=3.04878568649292, loss=2.1365814208984375
I0303 09:14:18.246512 140380326586112 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.5735533237457275, loss=2.326108932495117
I0303 09:15:03.214277 140380334978816 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.7836155891418457, loss=2.80599045753479
I0303 09:15:48.277845 140380326586112 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.790098190307617, loss=3.07051944732666
I0303 09:16:33.725080 140380334978816 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.544361114501953, loss=1.35773766040802
I0303 09:17:18.816094 140380326586112 logging_writer.py:48] [162500] global_step=162500, grad_norm=3.2560577392578125, loss=1.265548825263977
I0303 09:18:03.751589 140380334978816 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.628272294998169, loss=1.2702817916870117
I0303 09:18:22.298132 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:18:33.192514 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:18:57.262873 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:18:58.919894 140575196817216 submission_runner.py:411] Time since start: 78314.52s, 	Step: 162642, 	{'train/accuracy': 0.8746874928474426, 'train/loss': 0.46640151739120483, 'validation/accuracy': 0.7695800065994263, 'validation/loss': 0.9045939445495605, 'validation/num_examples': 50000, 'test/accuracy': 0.6458000540733337, 'test/loss': 1.5222994089126587, 'test/num_examples': 10000, 'score': 72304.83881092072, 'total_duration': 78314.5209043026, 'accumulated_submission_time': 72304.83881092072, 'accumulated_eval_time': 5992.814433336258, 'accumulated_logging_time': 8.240005731582642}
I0303 09:18:58.962189 140380326586112 logging_writer.py:48] [162642] accumulated_eval_time=5992.814433, accumulated_logging_time=8.240006, accumulated_submission_time=72304.838811, global_step=162642, preemption_count=0, score=72304.838811, test/accuracy=0.645800, test/loss=1.522299, test/num_examples=10000, total_duration=78314.520904, train/accuracy=0.874687, train/loss=0.466402, validation/accuracy=0.769580, validation/loss=0.904594, validation/num_examples=50000
I0303 09:19:22.423870 140380334978816 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.976879835128784, loss=3.2088987827301025
I0303 09:20:05.658061 140380326586112 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.7798707485198975, loss=1.2211302518844604
I0303 09:20:50.681466 140380334978816 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.6288135051727295, loss=1.3661423921585083
I0303 09:21:35.938236 140380326586112 logging_writer.py:48] [163000] global_step=163000, grad_norm=3.438570737838745, loss=1.8418607711791992
I0303 09:22:20.960161 140380334978816 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.85272216796875, loss=1.26120126247406
I0303 09:23:06.047800 140380326586112 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.7136881351470947, loss=3.519972562789917
I0303 09:23:51.208108 140380334978816 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.5096523761749268, loss=2.140597343444824
I0303 09:24:36.732983 140380326586112 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.7281646728515625, loss=2.0519306659698486
I0303 09:25:21.653617 140380334978816 logging_writer.py:48] [163500] global_step=163500, grad_norm=3.2022299766540527, loss=2.04494309425354
I0303 09:25:59.033561 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:26:09.944061 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:26:34.552992 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:26:36.213190 140575196817216 submission_runner.py:411] Time since start: 78771.81s, 	Step: 163585, 	{'train/accuracy': 0.8719531297683716, 'train/loss': 0.48470789194107056, 'validation/accuracy': 0.7728599905967712, 'validation/loss': 0.8979256749153137, 'validation/num_examples': 50000, 'test/accuracy': 0.6484000086784363, 'test/loss': 1.5181078910827637, 'test/num_examples': 10000, 'score': 72724.85019230843, 'total_duration': 78771.81423521042, 'accumulated_submission_time': 72724.85019230843, 'accumulated_eval_time': 6029.993057489395, 'accumulated_logging_time': 8.293435096740723}
I0303 09:26:36.253943 140380326586112 logging_writer.py:48] [163585] accumulated_eval_time=6029.993057, accumulated_logging_time=8.293435, accumulated_submission_time=72724.850192, global_step=163585, preemption_count=0, score=72724.850192, test/accuracy=0.648400, test/loss=1.518108, test/num_examples=10000, total_duration=78771.814235, train/accuracy=0.871953, train/loss=0.484708, validation/accuracy=0.772860, validation/loss=0.897926, validation/num_examples=50000
I0303 09:26:42.633422 140380334978816 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.6596531867980957, loss=1.2032078504562378
I0303 09:27:23.675554 140380326586112 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.6910624504089355, loss=2.4390997886657715
I0303 09:28:08.497468 140380334978816 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.6337168216705322, loss=1.3617199659347534
I0303 09:28:53.800613 140380326586112 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.492234706878662, loss=1.127063512802124
I0303 09:29:38.856380 140380334978816 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.6825454235076904, loss=1.471473217010498
I0303 09:30:23.754589 140380326586112 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.6418797969818115, loss=2.398851156234741
I0303 09:31:08.988890 140380334978816 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.9250452518463135, loss=2.622506618499756
I0303 09:31:53.860524 140380326586112 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.903306007385254, loss=2.2153329849243164
I0303 09:32:38.881083 140380334978816 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.9044125080108643, loss=1.3706914186477661
I0303 09:33:23.914922 140380326586112 logging_writer.py:48] [164500] global_step=164500, grad_norm=3.4050421714782715, loss=1.3155508041381836
I0303 09:33:36.216460 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:33:47.009919 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:34:10.767592 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:34:12.416367 140575196817216 submission_runner.py:411] Time since start: 79228.02s, 	Step: 164529, 	{'train/accuracy': 0.8699023127555847, 'train/loss': 0.48884978890419006, 'validation/accuracy': 0.7724399566650391, 'validation/loss': 0.8968047499656677, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.5122696161270142, 'test/num_examples': 10000, 'score': 73144.75268936157, 'total_duration': 79228.01746106148, 'accumulated_submission_time': 73144.75268936157, 'accumulated_eval_time': 6066.192003488541, 'accumulated_logging_time': 8.345094919204712}
I0303 09:34:12.459212 140380334978816 logging_writer.py:48] [164529] accumulated_eval_time=6066.192003, accumulated_logging_time=8.345095, accumulated_submission_time=73144.752689, global_step=164529, preemption_count=0, score=73144.752689, test/accuracy=0.649900, test/loss=1.512270, test/num_examples=10000, total_duration=79228.017461, train/accuracy=0.869902, train/loss=0.488850, validation/accuracy=0.772440, validation/loss=0.896805, validation/num_examples=50000
I0303 09:34:41.038384 140380326586112 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.8072948455810547, loss=1.2049624919891357
I0303 09:35:24.919517 140380334978816 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.7972302436828613, loss=2.503976345062256
I0303 09:36:10.395861 140380326586112 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.0499746799468994, loss=1.9334347248077393
I0303 09:36:55.785897 140380334978816 logging_writer.py:48] [164900] global_step=164900, grad_norm=3.3839874267578125, loss=3.406486988067627
I0303 09:37:40.690197 140380326586112 logging_writer.py:48] [165000] global_step=165000, grad_norm=3.152482271194458, loss=1.320235013961792
I0303 09:38:25.905820 140380334978816 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.771465539932251, loss=1.2827463150024414
I0303 09:39:11.295777 140380326586112 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.644470453262329, loss=1.5387487411499023
I0303 09:39:56.189402 140380334978816 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.8469505310058594, loss=1.20157790184021
I0303 09:40:41.309055 140380326586112 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.883592367172241, loss=1.1628694534301758
I0303 09:41:12.500145 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:41:23.181279 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:41:46.379797 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:41:48.030876 140575196817216 submission_runner.py:411] Time since start: 79683.63s, 	Step: 165471, 	{'train/accuracy': 0.8733788728713989, 'train/loss': 0.46757039427757263, 'validation/accuracy': 0.7732999920845032, 'validation/loss': 0.886921226978302, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.5046427249908447, 'test/num_examples': 10000, 'score': 73564.731341362, 'total_duration': 79683.63198709488, 'accumulated_submission_time': 73564.731341362, 'accumulated_eval_time': 6101.721790552139, 'accumulated_logging_time': 8.40129041671753}
I0303 09:41:48.075997 140380334978816 logging_writer.py:48] [165471] accumulated_eval_time=6101.721791, accumulated_logging_time=8.401290, accumulated_submission_time=73564.731341, global_step=165471, preemption_count=0, score=73564.731341, test/accuracy=0.650700, test/loss=1.504643, test/num_examples=10000, total_duration=79683.631987, train/accuracy=0.873379, train/loss=0.467570, validation/accuracy=0.773300, validation/loss=0.886921, validation/num_examples=50000
I0303 09:41:59.997090 140380326586112 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.8368000984191895, loss=1.229426622390747
I0303 09:42:41.702044 140380334978816 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.0398001670837402, loss=3.177855968475342
I0303 09:43:26.789740 140380326586112 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.0393574237823486, loss=1.152070164680481
I0303 09:44:11.866056 140380334978816 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.75667405128479, loss=1.286055088043213
I0303 09:44:56.753811 140380326586112 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.7791640758514404, loss=1.2341914176940918
I0303 09:45:42.068505 140380334978816 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.699021339416504, loss=2.643411159515381
I0303 09:46:27.306156 140380326586112 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.6713168621063232, loss=2.594028949737549
I0303 09:47:12.826543 140380334978816 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.8193516731262207, loss=2.237459421157837
I0303 09:47:57.839342 140380326586112 logging_writer.py:48] [166300] global_step=166300, grad_norm=3.2298946380615234, loss=3.0437211990356445
I0303 09:48:43.314122 140380334978816 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.680833578109741, loss=1.1465907096862793
I0303 09:48:48.373548 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:48:59.052004 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:49:22.516617 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:49:24.164465 140575196817216 submission_runner.py:411] Time since start: 80139.77s, 	Step: 166413, 	{'train/accuracy': 0.8737499713897705, 'train/loss': 0.464886873960495, 'validation/accuracy': 0.7749800086021423, 'validation/loss': 0.881984293460846, 'validation/num_examples': 50000, 'test/accuracy': 0.6540000438690186, 'test/loss': 1.4882982969284058, 'test/num_examples': 10000, 'score': 73984.96862435341, 'total_duration': 80139.76555800438, 'accumulated_submission_time': 73984.96862435341, 'accumulated_eval_time': 6137.511723995209, 'accumulated_logging_time': 8.457269191741943}
I0303 09:49:24.207545 140380326586112 logging_writer.py:48] [166413] accumulated_eval_time=6137.511724, accumulated_logging_time=8.457269, accumulated_submission_time=73984.968624, global_step=166413, preemption_count=0, score=73984.968624, test/accuracy=0.654000, test/loss=1.488298, test/num_examples=10000, total_duration=80139.765558, train/accuracy=0.873750, train/loss=0.464887, validation/accuracy=0.774980, validation/loss=0.881984, validation/num_examples=50000
I0303 09:49:59.167010 140380334978816 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.3360068798065186, loss=1.3008742332458496
I0303 09:50:44.258839 140380326586112 logging_writer.py:48] [166600] global_step=166600, grad_norm=3.1389377117156982, loss=3.1542086601257324
I0303 09:51:29.253918 140380334978816 logging_writer.py:48] [166700] global_step=166700, grad_norm=3.386258840560913, loss=3.3319716453552246
I0303 09:52:14.190921 140380326586112 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.6621510982513428, loss=1.518049716949463
I0303 09:52:59.426119 140380334978816 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.655569553375244, loss=1.9150171279907227
I0303 09:53:44.496147 140380326586112 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.6160173416137695, loss=1.2292182445526123
I0303 09:54:29.639626 140380334978816 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.9050891399383545, loss=1.1886467933654785
I0303 09:55:14.877861 140380326586112 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.907362937927246, loss=1.354830026626587
I0303 09:55:59.694419 140380334978816 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.8214175701141357, loss=1.1535507440567017
I0303 09:56:24.518569 140575196817216 spec.py:321] Evaluating on the training split.
I0303 09:56:35.296059 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 09:56:59.368749 140575196817216 spec.py:349] Evaluating on the test split.
I0303 09:57:01.027603 140575196817216 submission_runner.py:411] Time since start: 80596.63s, 	Step: 167356, 	{'train/accuracy': 0.8771874904632568, 'train/loss': 0.4618673622608185, 'validation/accuracy': 0.7760599851608276, 'validation/loss': 0.8795783519744873, 'validation/num_examples': 50000, 'test/accuracy': 0.6548000574111938, 'test/loss': 1.4894704818725586, 'test/num_examples': 10000, 'score': 74405.22176742554, 'total_duration': 80596.62842297554, 'accumulated_submission_time': 74405.22176742554, 'accumulated_eval_time': 6174.019504547119, 'accumulated_logging_time': 8.509846687316895}
I0303 09:57:01.079069 140380326586112 logging_writer.py:48] [167356] accumulated_eval_time=6174.019505, accumulated_logging_time=8.509847, accumulated_submission_time=74405.221767, global_step=167356, preemption_count=0, score=74405.221767, test/accuracy=0.654800, test/loss=1.489470, test/num_examples=10000, total_duration=80596.628423, train/accuracy=0.877187, train/loss=0.461867, validation/accuracy=0.776060, validation/loss=0.879578, validation/num_examples=50000
I0303 09:57:18.964789 140380334978816 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.1320040225982666, loss=1.2451976537704468
I0303 09:58:01.548429 140380326586112 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.867652654647827, loss=1.1920288801193237
I0303 09:58:46.017963 140380334978816 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.869020700454712, loss=1.2512335777282715
I0303 09:59:31.217708 140380326586112 logging_writer.py:48] [167700] global_step=167700, grad_norm=2.8683688640594482, loss=1.102339744567871
I0303 10:00:16.135725 140380334978816 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.1610119342803955, loss=3.0057835578918457
I0303 10:01:01.012846 140380326586112 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.8677470684051514, loss=2.0886459350585938
I0303 10:01:46.119330 140380334978816 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.9640004634857178, loss=1.1653615236282349
I0303 10:02:30.901539 140380326586112 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.801302671432495, loss=1.1388763189315796
I0303 10:03:15.852691 140380334978816 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.0593433380126953, loss=1.1675575971603394
I0303 10:04:00.737011 140380326586112 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.170284271240234, loss=3.025136947631836
I0303 10:04:01.366091 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:04:12.158036 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:04:35.529508 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:04:37.182552 140575196817216 submission_runner.py:411] Time since start: 81052.78s, 	Step: 168303, 	{'train/accuracy': 0.8756640553474426, 'train/loss': 0.4663519561290741, 'validation/accuracy': 0.7746599912643433, 'validation/loss': 0.8834454417228699, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.4991596937179565, 'test/num_examples': 10000, 'score': 74825.44824123383, 'total_duration': 81052.783826828, 'accumulated_submission_time': 74825.44824123383, 'accumulated_eval_time': 6209.835191726685, 'accumulated_logging_time': 8.572486162185669}
I0303 10:04:37.222960 140380334978816 logging_writer.py:48] [168303] accumulated_eval_time=6209.835192, accumulated_logging_time=8.572486, accumulated_submission_time=74825.448241, global_step=168303, preemption_count=0, score=74825.448241, test/accuracy=0.651900, test/loss=1.499160, test/num_examples=10000, total_duration=81052.783827, train/accuracy=0.875664, train/loss=0.466352, validation/accuracy=0.774660, validation/loss=0.883445, validation/num_examples=50000
I0303 10:05:16.628245 140380326586112 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.204408884048462, loss=1.209884524345398
I0303 10:06:01.133344 140380334978816 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.932863473892212, loss=2.5364863872528076
I0303 10:06:46.369559 140380326586112 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.6389544010162354, loss=1.1133747100830078
I0303 10:07:31.166610 140380334978816 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.869428873062134, loss=1.1444029808044434
I0303 10:08:16.269404 140380326586112 logging_writer.py:48] [168800] global_step=168800, grad_norm=2.9861154556274414, loss=1.2329918146133423
I0303 10:09:01.819133 140380334978816 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.4520342350006104, loss=3.0293872356414795
I0303 10:09:46.797451 140380326586112 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.075319766998291, loss=1.2053030729293823
I0303 10:10:31.969468 140380334978816 logging_writer.py:48] [169100] global_step=169100, grad_norm=3.245213747024536, loss=1.2230638265609741
I0303 10:11:17.023018 140380326586112 logging_writer.py:48] [169200] global_step=169200, grad_norm=3.18989896774292, loss=1.2457942962646484
I0303 10:11:37.346512 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:11:48.088920 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:12:11.203135 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:12:12.850884 140575196817216 submission_runner.py:411] Time since start: 81508.45s, 	Step: 169247, 	{'train/accuracy': 0.8783007860183716, 'train/loss': 0.4513605535030365, 'validation/accuracy': 0.7752000093460083, 'validation/loss': 0.8814749121665955, 'validation/num_examples': 50000, 'test/accuracy': 0.6554000377655029, 'test/loss': 1.493224024772644, 'test/num_examples': 10000, 'score': 75245.51100564003, 'total_duration': 81508.45189023018, 'accumulated_submission_time': 75245.51100564003, 'accumulated_eval_time': 6245.338510274887, 'accumulated_logging_time': 8.62363052368164}
I0303 10:12:12.894366 140380334978816 logging_writer.py:48] [169247] accumulated_eval_time=6245.338510, accumulated_logging_time=8.623631, accumulated_submission_time=75245.511006, global_step=169247, preemption_count=0, score=75245.511006, test/accuracy=0.655400, test/loss=1.493224, test/num_examples=10000, total_duration=81508.451890, train/accuracy=0.878301, train/loss=0.451361, validation/accuracy=0.775200, validation/loss=0.881475, validation/num_examples=50000
I0303 10:12:34.359639 140380326586112 logging_writer.py:48] [169300] global_step=169300, grad_norm=2.956448554992676, loss=1.1380573511123657
I0303 10:13:17.177514 140380334978816 logging_writer.py:48] [169400] global_step=169400, grad_norm=3.660506248474121, loss=3.2655603885650635
I0303 10:14:02.173933 140380326586112 logging_writer.py:48] [169500] global_step=169500, grad_norm=3.6695456504821777, loss=3.1290693283081055
I0303 10:14:47.313297 140380334978816 logging_writer.py:48] [169600] global_step=169600, grad_norm=3.0549747943878174, loss=1.236631155014038
I0303 10:15:32.329170 140380326586112 logging_writer.py:48] [169700] global_step=169700, grad_norm=3.1164000034332275, loss=1.1918963193893433
I0303 10:16:17.647607 140380334978816 logging_writer.py:48] [169800] global_step=169800, grad_norm=2.8300695419311523, loss=1.1776692867279053
I0303 10:17:02.516338 140380326586112 logging_writer.py:48] [169900] global_step=169900, grad_norm=2.8610548973083496, loss=1.1781362295150757
I0303 10:17:47.790119 140380334978816 logging_writer.py:48] [170000] global_step=170000, grad_norm=3.035318374633789, loss=1.14057195186615
I0303 10:18:32.715392 140380326586112 logging_writer.py:48] [170100] global_step=170100, grad_norm=2.7758102416992188, loss=1.779079556465149
I0303 10:19:13.193591 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:19:23.930907 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:19:46.725859 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:19:48.385929 140575196817216 submission_runner.py:411] Time since start: 81963.99s, 	Step: 170191, 	{'train/accuracy': 0.8774804472923279, 'train/loss': 0.4559576213359833, 'validation/accuracy': 0.7778799533843994, 'validation/loss': 0.8701184391975403, 'validation/num_examples': 50000, 'test/accuracy': 0.65420001745224, 'test/loss': 1.4848601818084717, 'test/num_examples': 10000, 'score': 75665.74991869926, 'total_duration': 81963.98689365387, 'accumulated_submission_time': 75665.74991869926, 'accumulated_eval_time': 6280.529743909836, 'accumulated_logging_time': 8.678744554519653}
I0303 10:19:48.432002 140380334978816 logging_writer.py:48] [170191] accumulated_eval_time=6280.529744, accumulated_logging_time=8.678745, accumulated_submission_time=75665.749919, global_step=170191, preemption_count=0, score=75665.749919, test/accuracy=0.654200, test/loss=1.484860, test/num_examples=10000, total_duration=81963.986894, train/accuracy=0.877480, train/loss=0.455958, validation/accuracy=0.777880, validation/loss=0.870118, validation/num_examples=50000
I0303 10:19:52.417436 140380326586112 logging_writer.py:48] [170200] global_step=170200, grad_norm=3.3684256076812744, loss=2.83968448638916
I0303 10:20:32.988067 140380334978816 logging_writer.py:48] [170300] global_step=170300, grad_norm=2.7312207221984863, loss=1.6641901731491089
I0303 10:21:17.830307 140380326586112 logging_writer.py:48] [170400] global_step=170400, grad_norm=3.3335530757904053, loss=1.566098928451538
I0303 10:22:03.260565 140380334978816 logging_writer.py:48] [170500] global_step=170500, grad_norm=2.708742380142212, loss=1.5756652355194092
I0303 10:22:48.213507 140380326586112 logging_writer.py:48] [170600] global_step=170600, grad_norm=3.123896598815918, loss=2.864248514175415
I0303 10:23:33.318685 140380334978816 logging_writer.py:48] [170700] global_step=170700, grad_norm=3.000896453857422, loss=1.1860651969909668
I0303 10:24:18.486792 140380326586112 logging_writer.py:48] [170800] global_step=170800, grad_norm=2.989006280899048, loss=1.1541696786880493
I0303 10:25:03.484755 140380334978816 logging_writer.py:48] [170900] global_step=170900, grad_norm=3.2615370750427246, loss=1.7618155479431152
I0303 10:25:48.614974 140380326586112 logging_writer.py:48] [171000] global_step=171000, grad_norm=3.2742841243743896, loss=1.1970778703689575
I0303 10:26:33.669929 140380334978816 logging_writer.py:48] [171100] global_step=171100, grad_norm=3.024536609649658, loss=1.2764333486557007
I0303 10:26:48.753234 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:26:59.311483 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:27:24.310639 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:27:25.971247 140575196817216 submission_runner.py:411] Time since start: 82421.57s, 	Step: 171135, 	{'train/accuracy': 0.8802539110183716, 'train/loss': 0.44466161727905273, 'validation/accuracy': 0.7779600024223328, 'validation/loss': 0.8712742924690247, 'validation/num_examples': 50000, 'test/accuracy': 0.6574000120162964, 'test/loss': 1.4882923364639282, 'test/num_examples': 10000, 'score': 76086.01024103165, 'total_duration': 82421.57224082947, 'accumulated_submission_time': 76086.01024103165, 'accumulated_eval_time': 6317.746684074402, 'accumulated_logging_time': 8.737242698669434}
I0303 10:27:26.017722 140380326586112 logging_writer.py:48] [171135] accumulated_eval_time=6317.746684, accumulated_logging_time=8.737243, accumulated_submission_time=76086.010241, global_step=171135, preemption_count=0, score=76086.010241, test/accuracy=0.657400, test/loss=1.488292, test/num_examples=10000, total_duration=82421.572241, train/accuracy=0.880254, train/loss=0.444662, validation/accuracy=0.777960, validation/loss=0.871274, validation/num_examples=50000
I0303 10:27:52.245083 140380334978816 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.4201807975769043, loss=3.312419891357422
I0303 10:28:35.552138 140380326586112 logging_writer.py:48] [171300] global_step=171300, grad_norm=3.119602918624878, loss=1.3231390714645386
I0303 10:29:20.403476 140380334978816 logging_writer.py:48] [171400] global_step=171400, grad_norm=3.072709798812866, loss=1.2022380828857422
I0303 10:30:05.507173 140380326586112 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.729872703552246, loss=1.9848382472991943
I0303 10:30:50.634829 140380334978816 logging_writer.py:48] [171600] global_step=171600, grad_norm=3.011242389678955, loss=1.1277053356170654
I0303 10:31:35.566518 140380326586112 logging_writer.py:48] [171700] global_step=171700, grad_norm=3.6146464347839355, loss=3.3464250564575195
I0303 10:32:20.718190 140380334978816 logging_writer.py:48] [171800] global_step=171800, grad_norm=2.9791438579559326, loss=1.8991754055023193
I0303 10:33:05.771935 140380326586112 logging_writer.py:48] [171900] global_step=171900, grad_norm=2.7869560718536377, loss=2.685046434402466
I0303 10:33:50.615287 140380334978816 logging_writer.py:48] [172000] global_step=172000, grad_norm=3.1757984161376953, loss=2.0001492500305176
I0303 10:34:26.164518 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:34:36.930607 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:35:00.574245 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:35:02.235203 140575196817216 submission_runner.py:411] Time since start: 82877.84s, 	Step: 172081, 	{'train/accuracy': 0.8818163871765137, 'train/loss': 0.4375278651714325, 'validation/accuracy': 0.7779200077056885, 'validation/loss': 0.8668556213378906, 'validation/num_examples': 50000, 'test/accuracy': 0.6577000021934509, 'test/loss': 1.4779142141342163, 'test/num_examples': 10000, 'score': 76506.09647703171, 'total_duration': 82877.83606362343, 'accumulated_submission_time': 76506.09647703171, 'accumulated_eval_time': 6353.816187143326, 'accumulated_logging_time': 8.7944974899292}
I0303 10:35:02.285517 140380326586112 logging_writer.py:48] [172081] accumulated_eval_time=6353.816187, accumulated_logging_time=8.794497, accumulated_submission_time=76506.096477, global_step=172081, preemption_count=0, score=76506.096477, test/accuracy=0.657700, test/loss=1.477914, test/num_examples=10000, total_duration=82877.836064, train/accuracy=0.881816, train/loss=0.437528, validation/accuracy=0.777920, validation/loss=0.866856, validation/num_examples=50000
I0303 10:35:10.236297 140380334978816 logging_writer.py:48] [172100] global_step=172100, grad_norm=2.9858686923980713, loss=1.284104585647583
I0303 10:35:51.277127 140380326586112 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.7429351806640625, loss=2.4788851737976074
I0303 10:36:36.374998 140380334978816 logging_writer.py:48] [172300] global_step=172300, grad_norm=3.737485885620117, loss=3.1795754432678223
I0303 10:37:21.566324 140380326586112 logging_writer.py:48] [172400] global_step=172400, grad_norm=3.8493027687072754, loss=3.2263827323913574
I0303 10:38:06.712828 140380334978816 logging_writer.py:48] [172500] global_step=172500, grad_norm=2.7313144207000732, loss=2.3304436206817627
I0303 10:38:51.149139 140380326586112 logging_writer.py:48] [172600] global_step=172600, grad_norm=3.0569839477539062, loss=1.1830838918685913
I0303 10:39:36.589445 140380334978816 logging_writer.py:48] [172700] global_step=172700, grad_norm=2.968550205230713, loss=1.2490524053573608
I0303 10:40:21.560161 140380326586112 logging_writer.py:48] [172800] global_step=172800, grad_norm=2.730410575866699, loss=1.1663613319396973
I0303 10:41:06.576401 140380334978816 logging_writer.py:48] [172900] global_step=172900, grad_norm=3.3409945964813232, loss=2.612884044647217
I0303 10:41:51.581952 140380326586112 logging_writer.py:48] [173000] global_step=173000, grad_norm=3.095897912979126, loss=1.1547690629959106
I0303 10:42:02.518956 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:42:13.309081 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:42:36.543434 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:42:38.211163 140575196817216 submission_runner.py:411] Time since start: 83333.81s, 	Step: 173026, 	{'train/accuracy': 0.8795507550239563, 'train/loss': 0.44713130593299866, 'validation/accuracy': 0.7780799865722656, 'validation/loss': 0.8690222501754761, 'validation/num_examples': 50000, 'test/accuracy': 0.6589000225067139, 'test/loss': 1.478680968284607, 'test/num_examples': 10000, 'score': 76926.27028298378, 'total_duration': 83333.81209087372, 'accumulated_submission_time': 76926.27028298378, 'accumulated_eval_time': 6389.507263660431, 'accumulated_logging_time': 8.855574369430542}
I0303 10:42:38.260001 140380334978816 logging_writer.py:48] [173026] accumulated_eval_time=6389.507264, accumulated_logging_time=8.855574, accumulated_submission_time=76926.270283, global_step=173026, preemption_count=0, score=76926.270283, test/accuracy=0.658900, test/loss=1.478681, test/num_examples=10000, total_duration=83333.812091, train/accuracy=0.879551, train/loss=0.447131, validation/accuracy=0.778080, validation/loss=0.869022, validation/num_examples=50000
I0303 10:43:08.070046 140380326586112 logging_writer.py:48] [173100] global_step=173100, grad_norm=2.59348464012146, loss=1.8086506128311157
I0303 10:43:51.569942 140380334978816 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.101093769073486, loss=3.077134847640991
I0303 10:44:36.478760 140380326586112 logging_writer.py:48] [173300] global_step=173300, grad_norm=3.4965054988861084, loss=2.8888168334960938
I0303 10:45:21.385533 140380334978816 logging_writer.py:48] [173400] global_step=173400, grad_norm=3.5180656909942627, loss=3.1403462886810303
I0303 10:46:06.356320 140380326586112 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.877782821655273, loss=3.0221242904663086
I0303 10:46:51.231853 140380334978816 logging_writer.py:48] [173600] global_step=173600, grad_norm=3.003244400024414, loss=1.2902919054031372
I0303 10:47:36.178920 140380326586112 logging_writer.py:48] [173700] global_step=173700, grad_norm=3.773524522781372, loss=2.6044962406158447
I0303 10:48:21.125936 140380334978816 logging_writer.py:48] [173800] global_step=173800, grad_norm=3.9732139110565186, loss=3.3062903881073
I0303 10:49:06.080378 140380326586112 logging_writer.py:48] [173900] global_step=173900, grad_norm=2.9226057529449463, loss=1.2860568761825562
I0303 10:49:38.250783 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:49:48.914685 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:50:11.727566 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:50:13.389735 140575196817216 submission_runner.py:411] Time since start: 83788.99s, 	Step: 173974, 	{'train/accuracy': 0.8820312023162842, 'train/loss': 0.43814072012901306, 'validation/accuracy': 0.7789199948310852, 'validation/loss': 0.8620977997779846, 'validation/num_examples': 50000, 'test/accuracy': 0.6617000102996826, 'test/loss': 1.4729156494140625, 'test/num_examples': 10000, 'score': 77346.20090389252, 'total_duration': 83788.990837574, 'accumulated_submission_time': 77346.20090389252, 'accumulated_eval_time': 6424.645256996155, 'accumulated_logging_time': 8.914592504501343}
I0303 10:50:13.432978 140380334978816 logging_writer.py:48] [173974] accumulated_eval_time=6424.645257, accumulated_logging_time=8.914593, accumulated_submission_time=77346.200904, global_step=173974, preemption_count=0, score=77346.200904, test/accuracy=0.661700, test/loss=1.472916, test/num_examples=10000, total_duration=83788.990838, train/accuracy=0.882031, train/loss=0.438141, validation/accuracy=0.778920, validation/loss=0.862098, validation/num_examples=50000
I0303 10:50:24.168875 140380326586112 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.018448829650879, loss=3.295886993408203
I0303 10:51:05.791231 140380334978816 logging_writer.py:48] [174100] global_step=174100, grad_norm=2.972683906555176, loss=1.4515061378479004
I0303 10:51:50.957031 140380326586112 logging_writer.py:48] [174200] global_step=174200, grad_norm=3.8179962635040283, loss=3.0487804412841797
I0303 10:52:36.242439 140380334978816 logging_writer.py:48] [174300] global_step=174300, grad_norm=2.962815284729004, loss=1.8551546335220337
I0303 10:53:21.209442 140380326586112 logging_writer.py:48] [174400] global_step=174400, grad_norm=3.4326117038726807, loss=3.080566167831421
I0303 10:54:06.460431 140380334978816 logging_writer.py:48] [174500] global_step=174500, grad_norm=3.299025297164917, loss=2.558228015899658
I0303 10:54:51.438838 140380326586112 logging_writer.py:48] [174600] global_step=174600, grad_norm=3.05423641204834, loss=1.1452993154525757
I0303 10:55:36.681921 140380334978816 logging_writer.py:48] [174700] global_step=174700, grad_norm=2.8884193897247314, loss=1.660902500152588
I0303 10:56:21.902292 140380326586112 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.805638790130615, loss=3.209439516067505
I0303 10:57:07.036958 140380334978816 logging_writer.py:48] [174900] global_step=174900, grad_norm=3.208860158920288, loss=1.2373769283294678
I0303 10:57:13.503342 140575196817216 spec.py:321] Evaluating on the training split.
I0303 10:57:24.257117 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 10:57:48.859628 140575196817216 spec.py:349] Evaluating on the test split.
I0303 10:57:50.509926 140575196817216 submission_runner.py:411] Time since start: 84246.11s, 	Step: 174916, 	{'train/accuracy': 0.8839452862739563, 'train/loss': 0.43050867319107056, 'validation/accuracy': 0.7795400023460388, 'validation/loss': 0.8621959090232849, 'validation/num_examples': 50000, 'test/accuracy': 0.6585000157356262, 'test/loss': 1.4759646654129028, 'test/num_examples': 10000, 'score': 77766.21065545082, 'total_duration': 84246.11060118675, 'accumulated_submission_time': 77766.21065545082, 'accumulated_eval_time': 6461.650452852249, 'accumulated_logging_time': 8.96901798248291}
I0303 10:57:50.556802 140380326586112 logging_writer.py:48] [174916] accumulated_eval_time=6461.650453, accumulated_logging_time=8.969018, accumulated_submission_time=77766.210655, global_step=174916, preemption_count=0, score=77766.210655, test/accuracy=0.658500, test/loss=1.475965, test/num_examples=10000, total_duration=84246.110601, train/accuracy=0.883945, train/loss=0.430509, validation/accuracy=0.779540, validation/loss=0.862196, validation/num_examples=50000
I0303 10:58:24.323172 140380334978816 logging_writer.py:48] [175000] global_step=175000, grad_norm=3.9059255123138428, loss=3.3041248321533203
I0303 10:59:08.484134 140380326586112 logging_writer.py:48] [175100] global_step=175100, grad_norm=3.2366204261779785, loss=1.1475352048873901
I0303 10:59:53.531226 140380334978816 logging_writer.py:48] [175200] global_step=175200, grad_norm=2.6286611557006836, loss=1.6647766828536987
I0303 11:00:38.922027 140380326586112 logging_writer.py:48] [175300] global_step=175300, grad_norm=3.686339855194092, loss=3.1959469318389893
I0303 11:01:23.857159 140380334978816 logging_writer.py:48] [175400] global_step=175400, grad_norm=3.3473422527313232, loss=2.6525661945343018
I0303 11:02:09.338365 140380326586112 logging_writer.py:48] [175500] global_step=175500, grad_norm=2.9346821308135986, loss=1.1511772871017456
I0303 11:02:54.182364 140380334978816 logging_writer.py:48] [175600] global_step=175600, grad_norm=3.3999340534210205, loss=1.0770379304885864
I0303 11:03:39.120252 140380326586112 logging_writer.py:48] [175700] global_step=175700, grad_norm=3.167586326599121, loss=3.085592746734619
I0303 11:04:24.107406 140380334978816 logging_writer.py:48] [175800] global_step=175800, grad_norm=3.0936639308929443, loss=2.5310795307159424
I0303 11:04:50.676711 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:05:01.405140 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:05:24.905725 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:05:26.551149 140575196817216 submission_runner.py:411] Time since start: 84702.15s, 	Step: 175861, 	{'train/accuracy': 0.8856444954872131, 'train/loss': 0.4301820993423462, 'validation/accuracy': 0.7799199819564819, 'validation/loss': 0.8656541705131531, 'validation/num_examples': 50000, 'test/accuracy': 0.6604000329971313, 'test/loss': 1.4743776321411133, 'test/num_examples': 10000, 'score': 78186.27035021782, 'total_duration': 84702.1522629261, 'accumulated_submission_time': 78186.27035021782, 'accumulated_eval_time': 6497.523934841156, 'accumulated_logging_time': 9.027230262756348}
I0303 11:05:26.595989 140380326586112 logging_writer.py:48] [175861] accumulated_eval_time=6497.523935, accumulated_logging_time=9.027230, accumulated_submission_time=78186.270350, global_step=175861, preemption_count=0, score=78186.270350, test/accuracy=0.660400, test/loss=1.474378, test/num_examples=10000, total_duration=84702.152263, train/accuracy=0.885644, train/loss=0.430182, validation/accuracy=0.779920, validation/loss=0.865654, validation/num_examples=50000
I0303 11:05:42.505333 140380334978816 logging_writer.py:48] [175900] global_step=175900, grad_norm=3.22916316986084, loss=1.4763190746307373
I0303 11:06:24.596332 140380326586112 logging_writer.py:48] [176000] global_step=176000, grad_norm=3.293020009994507, loss=1.192297101020813
I0303 11:07:09.759470 140380334978816 logging_writer.py:48] [176100] global_step=176100, grad_norm=3.8402464389801025, loss=2.927539110183716
I0303 11:07:54.950833 140380326586112 logging_writer.py:48] [176200] global_step=176200, grad_norm=3.44462251663208, loss=2.100329637527466
I0303 11:08:39.685424 140380334978816 logging_writer.py:48] [176300] global_step=176300, grad_norm=3.496812582015991, loss=3.1836678981781006
I0303 11:09:24.699728 140380326586112 logging_writer.py:48] [176400] global_step=176400, grad_norm=3.0989990234375, loss=1.1973462104797363
I0303 11:10:10.020319 140380334978816 logging_writer.py:48] [176500] global_step=176500, grad_norm=2.913022994995117, loss=1.345978021621704
I0303 11:10:54.917438 140380326586112 logging_writer.py:48] [176600] global_step=176600, grad_norm=3.298133611679077, loss=2.7900967597961426
I0303 11:11:40.043706 140380334978816 logging_writer.py:48] [176700] global_step=176700, grad_norm=3.165822744369507, loss=1.582993984222412
I0303 11:12:25.055149 140380326586112 logging_writer.py:48] [176800] global_step=176800, grad_norm=3.084728956222534, loss=1.1849491596221924
I0303 11:12:26.967976 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:12:37.684401 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:13:00.574392 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:13:02.233936 140575196817216 submission_runner.py:411] Time since start: 85157.84s, 	Step: 176806, 	{'train/accuracy': 0.88330078125, 'train/loss': 0.44063422083854675, 'validation/accuracy': 0.7821799516677856, 'validation/loss': 0.86235511302948, 'validation/num_examples': 50000, 'test/accuracy': 0.6619000434875488, 'test/loss': 1.4712177515029907, 'test/num_examples': 10000, 'score': 78606.58297896385, 'total_duration': 85157.835013628, 'accumulated_submission_time': 78606.58297896385, 'accumulated_eval_time': 6532.788912773132, 'accumulated_logging_time': 9.082460403442383}
I0303 11:13:02.278299 140380334978816 logging_writer.py:48] [176806] accumulated_eval_time=6532.788913, accumulated_logging_time=9.082460, accumulated_submission_time=78606.582979, global_step=176806, preemption_count=0, score=78606.582979, test/accuracy=0.661900, test/loss=1.471218, test/num_examples=10000, total_duration=85157.835014, train/accuracy=0.883301, train/loss=0.440634, validation/accuracy=0.782180, validation/loss=0.862355, validation/num_examples=50000
I0303 11:13:40.196028 140380326586112 logging_writer.py:48] [176900] global_step=176900, grad_norm=3.5070865154266357, loss=3.2369349002838135
I0303 11:14:25.156249 140380334978816 logging_writer.py:48] [177000] global_step=177000, grad_norm=3.078824758529663, loss=1.1602801084518433
I0303 11:15:10.380743 140380326586112 logging_writer.py:48] [177100] global_step=177100, grad_norm=2.8453807830810547, loss=1.501024842262268
I0303 11:15:55.215997 140380334978816 logging_writer.py:48] [177200] global_step=177200, grad_norm=3.6698832511901855, loss=3.25346040725708
I0303 11:16:40.500124 140380326586112 logging_writer.py:48] [177300] global_step=177300, grad_norm=3.319600820541382, loss=2.8965132236480713
I0303 11:17:25.575101 140380334978816 logging_writer.py:48] [177400] global_step=177400, grad_norm=2.939164876937866, loss=1.426571249961853
I0303 11:18:10.467901 140380326586112 logging_writer.py:48] [177500] global_step=177500, grad_norm=3.8655073642730713, loss=3.076669454574585
I0303 11:18:55.352311 140380334978816 logging_writer.py:48] [177600] global_step=177600, grad_norm=3.554462432861328, loss=1.3375916481018066
I0303 11:19:40.595892 140380326586112 logging_writer.py:48] [177700] global_step=177700, grad_norm=2.9168789386749268, loss=1.6513445377349854
I0303 11:20:02.605358 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:20:13.356414 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:20:36.787250 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:20:38.439600 140575196817216 submission_runner.py:411] Time since start: 85614.04s, 	Step: 177751, 	{'train/accuracy': 0.88427734375, 'train/loss': 0.43039968609809875, 'validation/accuracy': 0.780959963798523, 'validation/loss': 0.8608471155166626, 'validation/num_examples': 50000, 'test/accuracy': 0.6593000292778015, 'test/loss': 1.473184585571289, 'test/num_examples': 10000, 'score': 79026.84996557236, 'total_duration': 85614.04051232338, 'accumulated_submission_time': 79026.84996557236, 'accumulated_eval_time': 6568.6220116615295, 'accumulated_logging_time': 9.137590885162354}
I0303 11:20:38.485072 140380334978816 logging_writer.py:48] [177751] accumulated_eval_time=6568.622012, accumulated_logging_time=9.137591, accumulated_submission_time=79026.849966, global_step=177751, preemption_count=0, score=79026.849966, test/accuracy=0.659300, test/loss=1.473185, test/num_examples=10000, total_duration=85614.040512, train/accuracy=0.884277, train/loss=0.430400, validation/accuracy=0.780960, validation/loss=0.860847, validation/num_examples=50000
I0303 11:20:58.375895 140380326586112 logging_writer.py:48] [177800] global_step=177800, grad_norm=3.223006010055542, loss=1.111901044845581
I0303 11:21:40.995333 140380334978816 logging_writer.py:48] [177900] global_step=177900, grad_norm=3.5571529865264893, loss=3.0112576484680176
I0303 11:22:26.080896 140380326586112 logging_writer.py:48] [178000] global_step=178000, grad_norm=3.0448739528656006, loss=2.4128575325012207
I0303 11:23:11.161841 140380334978816 logging_writer.py:48] [178100] global_step=178100, grad_norm=3.1751301288604736, loss=1.203466534614563
I0303 11:23:56.026928 140380326586112 logging_writer.py:48] [178200] global_step=178200, grad_norm=3.365290880203247, loss=1.1934475898742676
I0303 11:24:41.108731 140380334978816 logging_writer.py:48] [178300] global_step=178300, grad_norm=3.065781593322754, loss=1.0194514989852905
I0303 11:25:25.965423 140380326586112 logging_writer.py:48] [178400] global_step=178400, grad_norm=5.530312538146973, loss=2.852246046066284
I0303 11:26:11.007226 140380334978816 logging_writer.py:48] [178500] global_step=178500, grad_norm=3.231799364089966, loss=1.2938518524169922
I0303 11:26:55.871799 140380326586112 logging_writer.py:48] [178600] global_step=178600, grad_norm=3.478783130645752, loss=2.937366485595703
I0303 11:27:38.495901 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:27:49.260264 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:28:12.459828 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:28:14.109902 140575196817216 submission_runner.py:411] Time since start: 86069.71s, 	Step: 178697, 	{'train/accuracy': 0.8859374523162842, 'train/loss': 0.4259621798992157, 'validation/accuracy': 0.7819199562072754, 'validation/loss': 0.8566552400588989, 'validation/num_examples': 50000, 'test/accuracy': 0.6617000102996826, 'test/loss': 1.4699724912643433, 'test/num_examples': 10000, 'score': 79446.80137014389, 'total_duration': 86069.71082782745, 'accumulated_submission_time': 79446.80137014389, 'accumulated_eval_time': 6604.234881401062, 'accumulated_logging_time': 9.192800283432007}
I0303 11:28:14.155685 140380334978816 logging_writer.py:48] [178697] accumulated_eval_time=6604.234881, accumulated_logging_time=9.192800, accumulated_submission_time=79446.801370, global_step=178697, preemption_count=0, score=79446.801370, test/accuracy=0.661700, test/loss=1.469972, test/num_examples=10000, total_duration=86069.710828, train/accuracy=0.885937, train/loss=0.425962, validation/accuracy=0.781920, validation/loss=0.856655, validation/num_examples=50000
I0303 11:28:15.751084 140380326586112 logging_writer.py:48] [178700] global_step=178700, grad_norm=3.2282228469848633, loss=1.1090580224990845
I0303 11:28:56.071239 140380334978816 logging_writer.py:48] [178800] global_step=178800, grad_norm=3.2294297218322754, loss=2.408949851989746
I0303 11:29:40.666614 140380326586112 logging_writer.py:48] [178900] global_step=178900, grad_norm=3.693927049636841, loss=3.0867276191711426
I0303 11:30:25.341594 140380334978816 logging_writer.py:48] [179000] global_step=179000, grad_norm=3.1932637691497803, loss=1.4454467296600342
I0303 11:31:10.343317 140380326586112 logging_writer.py:48] [179100] global_step=179100, grad_norm=3.740647554397583, loss=2.926393747329712
I0303 11:31:55.069000 140380334978816 logging_writer.py:48] [179200] global_step=179200, grad_norm=2.87088942527771, loss=1.6339480876922607
I0303 11:32:40.008544 140380326586112 logging_writer.py:48] [179300] global_step=179300, grad_norm=3.101627826690674, loss=2.651726722717285
I0303 11:33:24.835406 140380334978816 logging_writer.py:48] [179400] global_step=179400, grad_norm=5.39280366897583, loss=3.1644184589385986
I0303 11:34:09.824717 140380326586112 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.396724700927734, loss=3.2905564308166504
I0303 11:34:54.750706 140380334978816 logging_writer.py:48] [179600] global_step=179600, grad_norm=3.247680902481079, loss=1.1977992057800293
I0303 11:35:14.321856 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:35:25.113608 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:35:50.110058 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:35:51.765490 140575196817216 submission_runner.py:411] Time since start: 86527.37s, 	Step: 179645, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.4268486797809601, 'validation/accuracy': 0.7813599705696106, 'validation/loss': 0.8589540123939514, 'validation/num_examples': 50000, 'test/accuracy': 0.6598000526428223, 'test/loss': 1.470707654953003, 'test/num_examples': 10000, 'score': 79866.9059574604, 'total_duration': 86527.36637544632, 'accumulated_submission_time': 79866.9059574604, 'accumulated_eval_time': 6641.677387714386, 'accumulated_logging_time': 9.251446962356567}
I0303 11:35:51.812654 140380326586112 logging_writer.py:48] [179645] accumulated_eval_time=6641.677388, accumulated_logging_time=9.251447, accumulated_submission_time=79866.905957, global_step=179645, preemption_count=0, score=79866.905957, test/accuracy=0.659800, test/loss=1.470708, test/num_examples=10000, total_duration=86527.366375, train/accuracy=0.887422, train/loss=0.426849, validation/accuracy=0.781360, validation/loss=0.858954, validation/num_examples=50000
I0303 11:36:14.081484 140380334978816 logging_writer.py:48] [179700] global_step=179700, grad_norm=2.950625419616699, loss=1.1747816801071167
I0303 11:36:56.986687 140380326586112 logging_writer.py:48] [179800] global_step=179800, grad_norm=3.0068047046661377, loss=1.1393344402313232
I0303 11:37:42.364368 140380334978816 logging_writer.py:48] [179900] global_step=179900, grad_norm=3.1485159397125244, loss=1.537619948387146
I0303 11:38:27.536040 140380326586112 logging_writer.py:48] [180000] global_step=180000, grad_norm=2.9424777030944824, loss=1.1887141466140747
I0303 11:39:12.284746 140380334978816 logging_writer.py:48] [180100] global_step=180100, grad_norm=3.307812213897705, loss=2.6989645957946777
I0303 11:39:57.502491 140380326586112 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.174014568328857, loss=2.6329081058502197
I0303 11:40:42.584273 140380334978816 logging_writer.py:48] [180300] global_step=180300, grad_norm=2.9957847595214844, loss=1.293105959892273
I0303 11:41:27.708177 140380326586112 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.71291971206665, loss=2.757385730743408
I0303 11:42:13.016149 140380334978816 logging_writer.py:48] [180500] global_step=180500, grad_norm=3.663248062133789, loss=2.7582919597625732
I0303 11:42:52.141812 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:43:02.963509 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:43:24.800606 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:43:26.461914 140575196817216 submission_runner.py:411] Time since start: 86982.06s, 	Step: 180589, 	{'train/accuracy': 0.8857421875, 'train/loss': 0.4221574366092682, 'validation/accuracy': 0.7830399870872498, 'validation/loss': 0.8535976409912109, 'validation/num_examples': 50000, 'test/accuracy': 0.6617000102996826, 'test/loss': 1.4652323722839355, 'test/num_examples': 10000, 'score': 80287.17362117767, 'total_duration': 86982.06272292137, 'accumulated_submission_time': 80287.17362117767, 'accumulated_eval_time': 6675.996264457703, 'accumulated_logging_time': 9.310755252838135}
I0303 11:43:26.504787 140380326586112 logging_writer.py:48] [180589] accumulated_eval_time=6675.996264, accumulated_logging_time=9.310755, accumulated_submission_time=80287.173621, global_step=180589, preemption_count=0, score=80287.173621, test/accuracy=0.661700, test/loss=1.465232, test/num_examples=10000, total_duration=86982.062723, train/accuracy=0.885742, train/loss=0.422157, validation/accuracy=0.783040, validation/loss=0.853598, validation/num_examples=50000
I0303 11:43:31.281116 140380334978816 logging_writer.py:48] [180600] global_step=180600, grad_norm=3.732253313064575, loss=3.2953975200653076
I0303 11:44:12.385064 140380326586112 logging_writer.py:48] [180700] global_step=180700, grad_norm=3.6548075675964355, loss=3.1309149265289307
I0303 11:44:57.506321 140380334978816 logging_writer.py:48] [180800] global_step=180800, grad_norm=3.8367459774017334, loss=1.145625352859497
I0303 11:45:42.903879 140380326586112 logging_writer.py:48] [180900] global_step=180900, grad_norm=2.9095873832702637, loss=1.1003623008728027
I0303 11:46:28.138583 140380334978816 logging_writer.py:48] [181000] global_step=181000, grad_norm=3.1527059078216553, loss=1.6332640647888184
I0303 11:47:13.360054 140380326586112 logging_writer.py:48] [181100] global_step=181100, grad_norm=3.1424403190612793, loss=1.299766182899475
I0303 11:47:58.339333 140380334978816 logging_writer.py:48] [181200] global_step=181200, grad_norm=2.9643211364746094, loss=1.2228586673736572
I0303 11:48:43.540441 140380326586112 logging_writer.py:48] [181300] global_step=181300, grad_norm=3.2105517387390137, loss=1.3724641799926758
I0303 11:49:28.911034 140380334978816 logging_writer.py:48] [181400] global_step=181400, grad_norm=3.2100930213928223, loss=2.7583117485046387
I0303 11:50:13.898026 140380326586112 logging_writer.py:48] [181500] global_step=181500, grad_norm=2.981635808944702, loss=1.8140138387680054
I0303 11:50:26.797411 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:50:37.409857 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:51:00.218844 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:51:01.872431 140575196817216 submission_runner.py:411] Time since start: 87437.47s, 	Step: 181530, 	{'train/accuracy': 0.8855078220367432, 'train/loss': 0.4229528307914734, 'validation/accuracy': 0.782539963722229, 'validation/loss': 0.8535990118980408, 'validation/num_examples': 50000, 'test/accuracy': 0.6627000570297241, 'test/loss': 1.4637311697006226, 'test/num_examples': 10000, 'score': 80707.40728902817, 'total_duration': 87437.47354197502, 'accumulated_submission_time': 80707.40728902817, 'accumulated_eval_time': 6711.070337772369, 'accumulated_logging_time': 9.363951683044434}
I0303 11:51:01.938078 140380334978816 logging_writer.py:48] [181530] accumulated_eval_time=6711.070338, accumulated_logging_time=9.363952, accumulated_submission_time=80707.407289, global_step=181530, preemption_count=0, score=80707.407289, test/accuracy=0.662700, test/loss=1.463731, test/num_examples=10000, total_duration=87437.473542, train/accuracy=0.885508, train/loss=0.422953, validation/accuracy=0.782540, validation/loss=0.853599, validation/num_examples=50000
I0303 11:51:30.173142 140380326586112 logging_writer.py:48] [181600] global_step=181600, grad_norm=3.154494047164917, loss=1.2696748971939087
I0303 11:52:13.514572 140380334978816 logging_writer.py:48] [181700] global_step=181700, grad_norm=3.2892489433288574, loss=1.8248047828674316
I0303 11:52:58.647743 140380326586112 logging_writer.py:48] [181800] global_step=181800, grad_norm=3.6938867568969727, loss=3.175279140472412
I0303 11:53:43.744309 140380334978816 logging_writer.py:48] [181900] global_step=181900, grad_norm=3.1663403511047363, loss=1.1348532438278198
I0303 11:54:28.652111 140380326586112 logging_writer.py:48] [182000] global_step=182000, grad_norm=3.3174469470977783, loss=1.284631371498108
I0303 11:55:13.501446 140380334978816 logging_writer.py:48] [182100] global_step=182100, grad_norm=3.023098945617676, loss=1.165405511856079
I0303 11:55:58.290573 140380326586112 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.117461204528809, loss=2.2272558212280273
I0303 11:56:43.348963 140380334978816 logging_writer.py:48] [182300] global_step=182300, grad_norm=3.2318360805511475, loss=1.0984513759613037
I0303 11:57:28.547344 140380326586112 logging_writer.py:48] [182400] global_step=182400, grad_norm=3.223327159881592, loss=1.2027958631515503
I0303 11:58:02.012324 140575196817216 spec.py:321] Evaluating on the training split.
I0303 11:58:12.606986 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 11:58:36.265940 140575196817216 spec.py:349] Evaluating on the test split.
I0303 11:58:37.935238 140575196817216 submission_runner.py:411] Time since start: 87893.54s, 	Step: 182476, 	{'train/accuracy': 0.8879101276397705, 'train/loss': 0.41795259714126587, 'validation/accuracy': 0.7822999954223633, 'validation/loss': 0.8548969030380249, 'validation/num_examples': 50000, 'test/accuracy': 0.6619000434875488, 'test/loss': 1.4650492668151855, 'test/num_examples': 10000, 'score': 81127.42118930817, 'total_duration': 87893.53619122505, 'accumulated_submission_time': 81127.42118930817, 'accumulated_eval_time': 6746.992145776749, 'accumulated_logging_time': 9.44046401977539}
I0303 11:58:37.986920 140380334978816 logging_writer.py:48] [182476] accumulated_eval_time=6746.992146, accumulated_logging_time=9.440464, accumulated_submission_time=81127.421189, global_step=182476, preemption_count=0, score=81127.421189, test/accuracy=0.661900, test/loss=1.465049, test/num_examples=10000, total_duration=87893.536191, train/accuracy=0.887910, train/loss=0.417953, validation/accuracy=0.782300, validation/loss=0.854897, validation/num_examples=50000
I0303 11:58:47.918880 140380326586112 logging_writer.py:48] [182500] global_step=182500, grad_norm=3.053107261657715, loss=1.0544352531433105
I0303 11:59:29.086022 140380334978816 logging_writer.py:48] [182600] global_step=182600, grad_norm=3.0692737102508545, loss=1.7209508419036865
I0303 12:00:13.988664 140380326586112 logging_writer.py:48] [182700] global_step=182700, grad_norm=3.0009074211120605, loss=1.5144121646881104
I0303 12:00:59.065099 140380334978816 logging_writer.py:48] [182800] global_step=182800, grad_norm=2.977454423904419, loss=1.1091327667236328
I0303 12:01:44.244296 140380326586112 logging_writer.py:48] [182900] global_step=182900, grad_norm=3.5200366973876953, loss=2.797183036804199
I0303 12:02:29.132936 140380334978816 logging_writer.py:48] [183000] global_step=183000, grad_norm=3.139432668685913, loss=2.256227493286133
I0303 12:03:14.014174 140380326586112 logging_writer.py:48] [183100] global_step=183100, grad_norm=3.4263832569122314, loss=2.955209732055664
I0303 12:03:58.821400 140380334978816 logging_writer.py:48] [183200] global_step=183200, grad_norm=3.335808038711548, loss=2.9265999794006348
I0303 12:04:43.896490 140380326586112 logging_writer.py:48] [183300] global_step=183300, grad_norm=3.024632453918457, loss=1.2093074321746826
I0303 12:05:29.028502 140380334978816 logging_writer.py:48] [183400] global_step=183400, grad_norm=3.9984099864959717, loss=2.6913137435913086
I0303 12:05:38.153282 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:05:48.779769 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:06:11.483393 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:06:13.147309 140575196817216 submission_runner.py:411] Time since start: 88348.75s, 	Step: 183422, 	{'train/accuracy': 0.8875195384025574, 'train/loss': 0.41333675384521484, 'validation/accuracy': 0.7827999591827393, 'validation/loss': 0.8514824509620667, 'validation/num_examples': 50000, 'test/accuracy': 0.6612000465393066, 'test/loss': 1.4615788459777832, 'test/num_examples': 10000, 'score': 81547.52734351158, 'total_duration': 88348.74827170372, 'accumulated_submission_time': 81547.52734351158, 'accumulated_eval_time': 6781.985077857971, 'accumulated_logging_time': 9.50280237197876}
I0303 12:06:13.191249 140380326586112 logging_writer.py:48] [183422] accumulated_eval_time=6781.985078, accumulated_logging_time=9.502802, accumulated_submission_time=81547.527344, global_step=183422, preemption_count=0, score=81547.527344, test/accuracy=0.661200, test/loss=1.461579, test/num_examples=10000, total_duration=88348.748272, train/accuracy=0.887520, train/loss=0.413337, validation/accuracy=0.782800, validation/loss=0.851482, validation/num_examples=50000
I0303 12:06:44.586761 140380334978816 logging_writer.py:48] [183500] global_step=183500, grad_norm=3.575164318084717, loss=1.702612042427063
I0303 12:07:28.673648 140380326586112 logging_writer.py:48] [183600] global_step=183600, grad_norm=3.3259308338165283, loss=1.2945250272750854
I0303 12:08:14.032918 140380334978816 logging_writer.py:48] [183700] global_step=183700, grad_norm=2.924220561981201, loss=1.1219329833984375
I0303 12:08:59.038107 140380326586112 logging_writer.py:48] [183800] global_step=183800, grad_norm=3.3623971939086914, loss=1.7513163089752197
I0303 12:09:43.759104 140380334978816 logging_writer.py:48] [183900] global_step=183900, grad_norm=3.1596882343292236, loss=1.158036470413208
I0303 12:10:28.831618 140380326586112 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.893317699432373, loss=2.870767593383789
I0303 12:11:13.506831 140380334978816 logging_writer.py:48] [184100] global_step=184100, grad_norm=3.3841028213500977, loss=2.137615919113159
I0303 12:11:58.284460 140380326586112 logging_writer.py:48] [184200] global_step=184200, grad_norm=3.1055891513824463, loss=2.402876853942871
I0303 12:12:43.057557 140380334978816 logging_writer.py:48] [184300] global_step=184300, grad_norm=3.2603390216827393, loss=1.9594587087631226
I0303 12:13:13.295777 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:13:24.208351 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:13:49.237931 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:13:50.889010 140575196817216 submission_runner.py:411] Time since start: 88806.49s, 	Step: 184369, 	{'train/accuracy': 0.8892773389816284, 'train/loss': 0.4150594472885132, 'validation/accuracy': 0.7831400036811829, 'validation/loss': 0.8529243469238281, 'validation/num_examples': 50000, 'test/accuracy': 0.6614000201225281, 'test/loss': 1.4628207683563232, 'test/num_examples': 10000, 'score': 81967.57227182388, 'total_duration': 88806.49009799957, 'accumulated_submission_time': 81967.57227182388, 'accumulated_eval_time': 6819.577335119247, 'accumulated_logging_time': 9.556721925735474}
I0303 12:13:50.931510 140380326586112 logging_writer.py:48] [184369] accumulated_eval_time=6819.577335, accumulated_logging_time=9.556722, accumulated_submission_time=81967.572272, global_step=184369, preemption_count=0, score=81967.572272, test/accuracy=0.661400, test/loss=1.462821, test/num_examples=10000, total_duration=88806.490098, train/accuracy=0.889277, train/loss=0.415059, validation/accuracy=0.783140, validation/loss=0.852924, validation/num_examples=50000
I0303 12:14:03.657005 140380334978816 logging_writer.py:48] [184400] global_step=184400, grad_norm=2.977308511734009, loss=1.9923604726791382
I0303 12:14:45.429945 140380326586112 logging_writer.py:48] [184500] global_step=184500, grad_norm=3.028995990753174, loss=1.0784289836883545
I0303 12:15:30.221235 140380334978816 logging_writer.py:48] [184600] global_step=184600, grad_norm=3.6772897243499756, loss=3.0317330360412598
I0303 12:16:15.101183 140380326586112 logging_writer.py:48] [184700] global_step=184700, grad_norm=3.070188045501709, loss=1.1810706853866577
I0303 12:16:59.818983 140380334978816 logging_writer.py:48] [184800] global_step=184800, grad_norm=3.854145050048828, loss=3.189744234085083
I0303 12:17:44.732692 140380326586112 logging_writer.py:48] [184900] global_step=184900, grad_norm=3.5007901191711426, loss=1.3014572858810425
I0303 12:18:29.343061 140380334978816 logging_writer.py:48] [185000] global_step=185000, grad_norm=3.1425962448120117, loss=1.1396647691726685
I0303 12:19:14.172469 140380326586112 logging_writer.py:48] [185100] global_step=185100, grad_norm=3.332167625427246, loss=2.579003095626831
I0303 12:19:59.127999 140380334978816 logging_writer.py:48] [185200] global_step=185200, grad_norm=3.4481141567230225, loss=2.8044450283050537
I0303 12:20:44.065895 140380326586112 logging_writer.py:48] [185300] global_step=185300, grad_norm=3.3070640563964844, loss=1.0183155536651611
I0303 12:20:51.267848 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:21:02.033403 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:21:25.853082 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:21:27.504391 140575196817216 submission_runner.py:411] Time since start: 89263.11s, 	Step: 185318, 	{'train/accuracy': 0.8887499570846558, 'train/loss': 0.41334155201911926, 'validation/accuracy': 0.7833399772644043, 'validation/loss': 0.8519114851951599, 'validation/num_examples': 50000, 'test/accuracy': 0.6621000170707703, 'test/loss': 1.4614572525024414, 'test/num_examples': 10000, 'score': 82387.84855127335, 'total_duration': 89263.10540890694, 'accumulated_submission_time': 82387.84855127335, 'accumulated_eval_time': 6855.812837123871, 'accumulated_logging_time': 9.610158681869507}
I0303 12:21:27.554111 140380334978816 logging_writer.py:48] [185318] accumulated_eval_time=6855.812837, accumulated_logging_time=9.610159, accumulated_submission_time=82387.848551, global_step=185318, preemption_count=0, score=82387.848551, test/accuracy=0.662100, test/loss=1.461457, test/num_examples=10000, total_duration=89263.105409, train/accuracy=0.888750, train/loss=0.413342, validation/accuracy=0.783340, validation/loss=0.851911, validation/num_examples=50000
I0303 12:22:00.552170 140380326586112 logging_writer.py:48] [185400] global_step=185400, grad_norm=3.2526662349700928, loss=1.3871127367019653
I0303 12:22:44.806111 140380334978816 logging_writer.py:48] [185500] global_step=185500, grad_norm=3.5616612434387207, loss=2.4164204597473145
I0303 12:23:30.074290 140380326586112 logging_writer.py:48] [185600] global_step=185600, grad_norm=3.1078732013702393, loss=1.1653008460998535
I0303 12:24:14.986051 140380334978816 logging_writer.py:48] [185700] global_step=185700, grad_norm=3.0409302711486816, loss=1.108429193496704
I0303 12:25:00.057375 140380326586112 logging_writer.py:48] [185800] global_step=185800, grad_norm=2.8968167304992676, loss=2.528233289718628
I0303 12:25:45.174245 140380334978816 logging_writer.py:48] [185900] global_step=185900, grad_norm=3.1594467163085938, loss=1.7308390140533447
I0303 12:26:30.274649 140380326586112 logging_writer.py:48] [186000] global_step=186000, grad_norm=2.825279474258423, loss=1.0769224166870117
I0303 12:27:15.332037 140380334978816 logging_writer.py:48] [186100] global_step=186100, grad_norm=3.652216672897339, loss=1.1464934349060059
I0303 12:28:00.235848 140380326586112 logging_writer.py:48] [186200] global_step=186200, grad_norm=3.106128215789795, loss=1.068660020828247
I0303 12:28:27.928558 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:28:38.610629 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:29:02.664988 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:29:04.320255 140575196817216 submission_runner.py:411] Time since start: 89719.92s, 	Step: 186263, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.4150834083557129, 'validation/accuracy': 0.7832399606704712, 'validation/loss': 0.8526034355163574, 'validation/num_examples': 50000, 'test/accuracy': 0.6614000201225281, 'test/loss': 1.4626858234405518, 'test/num_examples': 10000, 'score': 82808.1642510891, 'total_duration': 89719.92125058174, 'accumulated_submission_time': 82808.1642510891, 'accumulated_eval_time': 6892.20347237587, 'accumulated_logging_time': 9.669806718826294}
I0303 12:29:04.369895 140380334978816 logging_writer.py:48] [186263] accumulated_eval_time=6892.203472, accumulated_logging_time=9.669807, accumulated_submission_time=82808.164251, global_step=186263, preemption_count=0, score=82808.164251, test/accuracy=0.661400, test/loss=1.462686, test/num_examples=10000, total_duration=89719.921251, train/accuracy=0.888809, train/loss=0.415083, validation/accuracy=0.783240, validation/loss=0.852603, validation/num_examples=50000
I0303 12:29:19.486315 140380326586112 logging_writer.py:48] [186300] global_step=186300, grad_norm=3.834317684173584, loss=3.10673189163208
I0303 12:30:01.266062 140380334978816 logging_writer.py:48] [186400] global_step=186400, grad_norm=3.1517245769500732, loss=1.0994800329208374
I0303 12:30:46.195120 140380326586112 logging_writer.py:48] [186500] global_step=186500, grad_norm=3.38149094581604, loss=3.0693399906158447
I0303 12:31:31.190743 140380334978816 logging_writer.py:48] [186600] global_step=186600, grad_norm=3.4413657188415527, loss=3.032059669494629
I0303 12:32:16.268165 140380326586112 logging_writer.py:48] [186700] global_step=186700, grad_norm=4.300892353057861, loss=3.164543628692627
I0303 12:33:01.064626 140380334978816 logging_writer.py:48] [186800] global_step=186800, grad_norm=3.268216609954834, loss=2.8051626682281494
I0303 12:33:46.281664 140380326586112 logging_writer.py:48] [186900] global_step=186900, grad_norm=3.114166736602783, loss=1.1229898929595947
I0303 12:34:31.173130 140380334978816 logging_writer.py:48] [187000] global_step=187000, grad_norm=3.697330951690674, loss=3.1739494800567627
I0303 12:35:16.143802 140380326586112 logging_writer.py:48] [187100] global_step=187100, grad_norm=3.6748626232147217, loss=3.1612753868103027
I0303 12:36:01.074791 140380334978816 logging_writer.py:48] [187200] global_step=187200, grad_norm=3.347661256790161, loss=2.4788317680358887
I0303 12:36:04.371552 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:36:15.226619 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:36:39.182410 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:36:40.838690 140575196817216 submission_runner.py:411] Time since start: 90176.44s, 	Step: 187209, 	{'train/accuracy': 0.8882226347923279, 'train/loss': 0.4158450961112976, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 83228.10613918304, 'total_duration': 90176.43953990936, 'accumulated_submission_time': 83228.10613918304, 'accumulated_eval_time': 6928.669387102127, 'accumulated_logging_time': 9.730186939239502}
I0303 12:36:40.882900 140380326586112 logging_writer.py:48] [187209] accumulated_eval_time=6928.669387, accumulated_logging_time=9.730187, accumulated_submission_time=83228.106139, global_step=187209, preemption_count=0, score=83228.106139, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=90176.439540, train/accuracy=0.888223, train/loss=0.415845, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 12:37:17.508900 140380334978816 logging_writer.py:48] [187300] global_step=187300, grad_norm=2.767775058746338, loss=1.3695625066757202
I0303 12:38:02.294569 140380326586112 logging_writer.py:48] [187400] global_step=187400, grad_norm=3.242114543914795, loss=1.0585250854492188
I0303 12:38:47.676318 140380334978816 logging_writer.py:48] [187500] global_step=187500, grad_norm=3.258183002471924, loss=2.3143062591552734
I0303 12:39:32.490067 140380326586112 logging_writer.py:48] [187600] global_step=187600, grad_norm=3.2890589237213135, loss=1.370336651802063
I0303 12:40:17.650768 140380334978816 logging_writer.py:48] [187700] global_step=187700, grad_norm=4.301562786102295, loss=3.263866424560547
I0303 12:41:02.907946 140380326586112 logging_writer.py:48] [187800] global_step=187800, grad_norm=3.078601598739624, loss=2.3648622035980225
I0303 12:41:48.035626 140380334978816 logging_writer.py:48] [187900] global_step=187900, grad_norm=3.953460931777954, loss=3.071880578994751
I0303 12:42:33.011994 140380326586112 logging_writer.py:48] [188000] global_step=188000, grad_norm=2.955712080001831, loss=1.3146822452545166
I0303 12:43:18.094228 140380334978816 logging_writer.py:48] [188100] global_step=188100, grad_norm=3.218675374984741, loss=2.925909996032715
I0303 12:43:41.104123 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:43:51.717171 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:44:15.449805 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:44:17.107865 140575196817216 submission_runner.py:411] Time since start: 90632.71s, 	Step: 188153, 	{'train/accuracy': 0.8866796493530273, 'train/loss': 0.4240669012069702, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 83648.2685251236, 'total_duration': 90632.70875310898, 'accumulated_submission_time': 83648.2685251236, 'accumulated_eval_time': 6964.6719563007355, 'accumulated_logging_time': 9.78453016281128}
I0303 12:44:17.152169 140380326586112 logging_writer.py:48] [188153] accumulated_eval_time=6964.671956, accumulated_logging_time=9.784530, accumulated_submission_time=83648.268525, global_step=188153, preemption_count=0, score=83648.268525, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=90632.708753, train/accuracy=0.886680, train/loss=0.424067, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 12:44:36.235740 140380334978816 logging_writer.py:48] [188200] global_step=188200, grad_norm=3.4203739166259766, loss=2.4311609268188477
I0303 12:45:18.801878 140380326586112 logging_writer.py:48] [188300] global_step=188300, grad_norm=3.974614381790161, loss=3.2129054069519043
I0303 12:46:03.756834 140380334978816 logging_writer.py:48] [188400] global_step=188400, grad_norm=3.1028385162353516, loss=1.0292586088180542
I0303 12:46:48.685090 140380326586112 logging_writer.py:48] [188500] global_step=188500, grad_norm=4.23098087310791, loss=3.265449285507202
I0303 12:47:33.530956 140380334978816 logging_writer.py:48] [188600] global_step=188600, grad_norm=3.17916202545166, loss=1.3795945644378662
I0303 12:48:18.560017 140380326586112 logging_writer.py:48] [188700] global_step=188700, grad_norm=2.9871158599853516, loss=1.686362862586975
I0303 12:49:03.339054 140380334978816 logging_writer.py:48] [188800] global_step=188800, grad_norm=3.081251621246338, loss=1.2057968378067017
I0303 12:49:48.416806 140380326586112 logging_writer.py:48] [188900] global_step=188900, grad_norm=3.314105272293091, loss=2.5148935317993164
I0303 12:50:33.647409 140380334978816 logging_writer.py:48] [189000] global_step=189000, grad_norm=3.1230335235595703, loss=1.1554560661315918
I0303 12:51:17.421739 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:51:28.031733 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:51:53.085555 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:51:54.748839 140575196817216 submission_runner.py:411] Time since start: 91090.35s, 	Step: 189099, 	{'train/accuracy': 0.8877343535423279, 'train/loss': 0.41805213689804077, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 84068.47924923897, 'total_duration': 91090.34984946251, 'accumulated_submission_time': 84068.47924923897, 'accumulated_eval_time': 7001.998011350632, 'accumulated_logging_time': 9.838761329650879}
I0303 12:51:54.798180 140380326586112 logging_writer.py:48] [189099] accumulated_eval_time=7001.998011, accumulated_logging_time=9.838761, accumulated_submission_time=84068.479249, global_step=189099, preemption_count=0, score=84068.479249, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=91090.349849, train/accuracy=0.887734, train/loss=0.418052, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 12:51:55.592953 140380334978816 logging_writer.py:48] [189100] global_step=189100, grad_norm=3.315009117126465, loss=2.115445613861084
I0303 12:52:36.052594 140380326586112 logging_writer.py:48] [189200] global_step=189200, grad_norm=4.260853290557861, loss=3.2581260204315186
I0303 12:53:20.855005 140380334978816 logging_writer.py:48] [189300] global_step=189300, grad_norm=3.0547688007354736, loss=1.0819355249404907
I0303 12:54:05.600857 140380326586112 logging_writer.py:48] [189400] global_step=189400, grad_norm=2.9556987285614014, loss=1.1847968101501465
I0303 12:54:50.445411 140380334978816 logging_writer.py:48] [189500] global_step=189500, grad_norm=4.866337299346924, loss=3.224398374557495
I0303 12:55:35.464910 140380326586112 logging_writer.py:48] [189600] global_step=189600, grad_norm=3.1457712650299072, loss=1.1109095811843872
I0303 12:56:20.474482 140380334978816 logging_writer.py:48] [189700] global_step=189700, grad_norm=2.8710100650787354, loss=1.1531941890716553
I0303 12:57:05.400663 140380326586112 logging_writer.py:48] [189800] global_step=189800, grad_norm=2.8216588497161865, loss=1.0770875215530396
I0303 12:57:50.211486 140380334978816 logging_writer.py:48] [189900] global_step=189900, grad_norm=3.150689125061035, loss=1.1471655368804932
I0303 12:58:35.194392 140380326586112 logging_writer.py:48] [190000] global_step=190000, grad_norm=3.0865862369537354, loss=1.0809189081192017
I0303 12:58:55.061452 140575196817216 spec.py:321] Evaluating on the training split.
I0303 12:59:06.008240 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 12:59:27.652581 140575196817216 spec.py:349] Evaluating on the test split.
I0303 12:59:29.315282 140575196817216 submission_runner.py:411] Time since start: 91544.92s, 	Step: 190046, 	{'train/accuracy': 0.8861523270606995, 'train/loss': 0.42112576961517334, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 84488.68370914459, 'total_duration': 91544.91623663902, 'accumulated_submission_time': 84488.68370914459, 'accumulated_eval_time': 7036.250737190247, 'accumulated_logging_time': 9.89790678024292}
I0303 12:59:29.361901 140380334978816 logging_writer.py:48] [190046] accumulated_eval_time=7036.250737, accumulated_logging_time=9.897907, accumulated_submission_time=84488.683709, global_step=190046, preemption_count=0, score=84488.683709, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=91544.916237, train/accuracy=0.886152, train/loss=0.421126, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 12:59:51.230665 140380326586112 logging_writer.py:48] [190100] global_step=190100, grad_norm=3.097554922103882, loss=1.7377309799194336
I0303 13:00:34.595584 140380334978816 logging_writer.py:48] [190200] global_step=190200, grad_norm=3.8106448650360107, loss=3.2991223335266113
I0303 13:01:19.843837 140380326586112 logging_writer.py:48] [190300] global_step=190300, grad_norm=3.7964582443237305, loss=1.496433973312378
I0303 13:02:05.291148 140380334978816 logging_writer.py:48] [190400] global_step=190400, grad_norm=2.931941032409668, loss=2.0649545192718506
I0303 13:02:50.361185 140380326586112 logging_writer.py:48] [190500] global_step=190500, grad_norm=2.8355536460876465, loss=1.0685224533081055
I0303 13:03:35.560968 140380334978816 logging_writer.py:48] [190600] global_step=190600, grad_norm=3.1758530139923096, loss=1.110891342163086
I0303 13:04:20.567559 140380326586112 logging_writer.py:48] [190700] global_step=190700, grad_norm=3.04323673248291, loss=1.177473545074463
I0303 13:05:05.687051 140380334978816 logging_writer.py:48] [190800] global_step=190800, grad_norm=3.2256293296813965, loss=1.33571195602417
I0303 13:05:50.690740 140380326586112 logging_writer.py:48] [190900] global_step=190900, grad_norm=2.967745065689087, loss=1.9050613641738892
I0303 13:06:29.771921 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:06:40.461667 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:07:03.853631 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:07:05.508080 140575196817216 submission_runner.py:411] Time since start: 92001.11s, 	Step: 190988, 	{'train/accuracy': 0.8909765481948853, 'train/loss': 0.41313475370407104, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 84909.03533935547, 'total_duration': 92001.10895776749, 'accumulated_submission_time': 84909.03533935547, 'accumulated_eval_time': 7071.985706090927, 'accumulated_logging_time': 9.954696655273438}
I0303 13:07:05.557301 140380334978816 logging_writer.py:48] [190988] accumulated_eval_time=7071.985706, accumulated_logging_time=9.954697, accumulated_submission_time=84909.035339, global_step=190988, preemption_count=0, score=84909.035339, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=92001.108958, train/accuracy=0.890977, train/loss=0.413135, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:07:10.725322 140380326586112 logging_writer.py:48] [191000] global_step=191000, grad_norm=3.072605848312378, loss=1.1433748006820679
I0303 13:07:51.347396 140380334978816 logging_writer.py:48] [191100] global_step=191100, grad_norm=2.9164185523986816, loss=1.5110105276107788
I0303 13:08:36.339924 140380326586112 logging_writer.py:48] [191200] global_step=191200, grad_norm=3.201512336730957, loss=1.9398273229599
I0303 13:09:21.501168 140380334978816 logging_writer.py:48] [191300] global_step=191300, grad_norm=3.1409683227539062, loss=1.8182979822158813
I0303 13:10:06.402050 140380326586112 logging_writer.py:48] [191400] global_step=191400, grad_norm=3.1598992347717285, loss=2.2013585567474365
I0303 13:10:51.768648 140380334978816 logging_writer.py:48] [191500] global_step=191500, grad_norm=3.3057165145874023, loss=1.660088062286377
I0303 13:11:36.757031 140380326586112 logging_writer.py:48] [191600] global_step=191600, grad_norm=3.064462661743164, loss=1.1298911571502686
I0303 13:12:21.938262 140380334978816 logging_writer.py:48] [191700] global_step=191700, grad_norm=3.446061611175537, loss=2.7263846397399902
I0303 13:13:07.136762 140380326586112 logging_writer.py:48] [191800] global_step=191800, grad_norm=2.9433681964874268, loss=1.1165106296539307
I0303 13:13:51.729940 140380334978816 logging_writer.py:48] [191900] global_step=191900, grad_norm=3.1761536598205566, loss=1.2863333225250244
I0303 13:14:05.903392 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:14:16.613936 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:14:40.112106 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:14:41.754415 140575196817216 submission_runner.py:411] Time since start: 92457.36s, 	Step: 191933, 	{'train/accuracy': 0.8879101276397705, 'train/loss': 0.4145359992980957, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 85329.32217812538, 'total_duration': 92457.35536766052, 'accumulated_submission_time': 85329.32217812538, 'accumulated_eval_time': 7107.835638523102, 'accumulated_logging_time': 10.013721942901611}
I0303 13:14:41.797873 140380326586112 logging_writer.py:48] [191933] accumulated_eval_time=7107.835639, accumulated_logging_time=10.013722, accumulated_submission_time=85329.322178, global_step=191933, preemption_count=0, score=85329.322178, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=92457.355368, train/accuracy=0.887910, train/loss=0.414536, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:15:08.867496 140380334978816 logging_writer.py:48] [192000] global_step=192000, grad_norm=3.1778836250305176, loss=1.225986123085022
I0303 13:15:52.210305 140380326586112 logging_writer.py:48] [192100] global_step=192100, grad_norm=3.0501959323883057, loss=2.145343542098999
I0303 13:16:37.742499 140380334978816 logging_writer.py:48] [192200] global_step=192200, grad_norm=3.1485836505889893, loss=1.1276960372924805
I0303 13:17:22.787168 140380326586112 logging_writer.py:48] [192300] global_step=192300, grad_norm=3.454089879989624, loss=1.1906157732009888
I0303 13:18:07.722237 140380334978816 logging_writer.py:48] [192400] global_step=192400, grad_norm=3.3732221126556396, loss=1.1794044971466064
I0303 13:18:52.741462 140380326586112 logging_writer.py:48] [192500] global_step=192500, grad_norm=4.437093734741211, loss=3.2172422409057617
I0303 13:19:37.433054 140380334978816 logging_writer.py:48] [192600] global_step=192600, grad_norm=3.47727632522583, loss=1.188002586364746
I0303 13:20:22.431522 140380326586112 logging_writer.py:48] [192700] global_step=192700, grad_norm=3.2139852046966553, loss=2.6357367038726807
I0303 13:21:07.348086 140380334978816 logging_writer.py:48] [192800] global_step=192800, grad_norm=3.2749555110931396, loss=2.030466079711914
I0303 13:21:41.819616 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:21:52.629986 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:22:14.103256 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:22:15.757990 140575196817216 submission_runner.py:411] Time since start: 92911.36s, 	Step: 192879, 	{'train/accuracy': 0.8848242163658142, 'train/loss': 0.4299986958503723, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 85749.28629517555, 'total_duration': 92911.35882663727, 'accumulated_submission_time': 85749.28629517555, 'accumulated_eval_time': 7141.772822141647, 'accumulated_logging_time': 10.066681861877441}
I0303 13:22:15.807938 140380326586112 logging_writer.py:48] [192879] accumulated_eval_time=7141.772822, accumulated_logging_time=10.066682, accumulated_submission_time=85749.286295, global_step=192879, preemption_count=0, score=85749.286295, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=92911.358827, train/accuracy=0.884824, train/loss=0.429999, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:22:24.558247 140380334978816 logging_writer.py:48] [192900] global_step=192900, grad_norm=3.4476583003997803, loss=1.224808931350708
I0303 13:23:05.469268 140380326586112 logging_writer.py:48] [193000] global_step=193000, grad_norm=3.612990379333496, loss=1.0238059759140015
I0303 13:23:50.336104 140380334978816 logging_writer.py:48] [193100] global_step=193100, grad_norm=3.4617669582366943, loss=2.892646074295044
I0303 13:24:35.716383 140380326586112 logging_writer.py:48] [193200] global_step=193200, grad_norm=3.1412501335144043, loss=1.0624034404754639
I0303 13:25:20.420854 140380334978816 logging_writer.py:48] [193300] global_step=193300, grad_norm=3.6068336963653564, loss=1.410976529121399
I0303 13:26:05.506470 140380326586112 logging_writer.py:48] [193400] global_step=193400, grad_norm=2.9228146076202393, loss=1.6929775476455688
I0303 13:26:50.503006 140380334978816 logging_writer.py:48] [193500] global_step=193500, grad_norm=3.1070761680603027, loss=1.5606670379638672
I0303 13:27:35.409363 140380326586112 logging_writer.py:48] [193600] global_step=193600, grad_norm=3.481417179107666, loss=3.157238245010376
I0303 13:28:20.670549 140380334978816 logging_writer.py:48] [193700] global_step=193700, grad_norm=2.896275281906128, loss=1.120765209197998
I0303 13:29:05.520037 140380326586112 logging_writer.py:48] [193800] global_step=193800, grad_norm=2.9346821308135986, loss=1.1424560546875
I0303 13:29:15.859932 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:29:26.525867 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:29:49.364349 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:29:51.030097 140575196817216 submission_runner.py:411] Time since start: 93366.63s, 	Step: 193825, 	{'train/accuracy': 0.8856444954872131, 'train/loss': 0.4275977611541748, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 86169.27835011482, 'total_duration': 93366.63113760948, 'accumulated_submission_time': 86169.27835011482, 'accumulated_eval_time': 7176.942252874374, 'accumulated_logging_time': 10.127224683761597}
I0303 13:29:51.078179 140380334978816 logging_writer.py:48] [193825] accumulated_eval_time=7176.942253, accumulated_logging_time=10.127225, accumulated_submission_time=86169.278350, global_step=193825, preemption_count=0, score=86169.278350, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=93366.631138, train/accuracy=0.885644, train/loss=0.427598, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:30:21.281126 140380326586112 logging_writer.py:48] [193900] global_step=193900, grad_norm=2.9725699424743652, loss=1.416993498802185
I0303 13:31:05.498195 140380334978816 logging_writer.py:48] [194000] global_step=194000, grad_norm=2.9593446254730225, loss=1.4393706321716309
I0303 13:31:50.524462 140380326586112 logging_writer.py:48] [194100] global_step=194100, grad_norm=3.2485673427581787, loss=2.493518829345703
I0303 13:32:35.919925 140380334978816 logging_writer.py:48] [194200] global_step=194200, grad_norm=3.0811526775360107, loss=1.1062874794006348
I0303 13:33:20.918480 140380326586112 logging_writer.py:48] [194300] global_step=194300, grad_norm=3.500032663345337, loss=1.1336596012115479
I0303 13:34:05.930865 140380334978816 logging_writer.py:48] [194400] global_step=194400, grad_norm=2.9254918098449707, loss=2.060621976852417
I0303 13:34:50.764997 140380326586112 logging_writer.py:48] [194500] global_step=194500, grad_norm=3.6637825965881348, loss=2.8710103034973145
I0303 13:35:35.720045 140380334978816 logging_writer.py:48] [194600] global_step=194600, grad_norm=3.9468801021575928, loss=3.1377899646759033
I0303 13:36:20.977945 140380326586112 logging_writer.py:48] [194700] global_step=194700, grad_norm=2.9150502681732178, loss=1.1416258811950684
I0303 13:36:51.183907 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:37:02.054205 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:37:25.789387 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:37:27.447342 140575196817216 submission_runner.py:411] Time since start: 93823.05s, 	Step: 194769, 	{'train/accuracy': 0.8881444931030273, 'train/loss': 0.4168613851070404, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 86589.32567048073, 'total_duration': 93823.04841327667, 'accumulated_submission_time': 86589.32567048073, 'accumulated_eval_time': 7213.204705715179, 'accumulated_logging_time': 10.184736251831055}
I0303 13:37:27.492867 140380334978816 logging_writer.py:48] [194769] accumulated_eval_time=7213.204706, accumulated_logging_time=10.184736, accumulated_submission_time=86589.325670, global_step=194769, preemption_count=0, score=86589.325670, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=93823.048413, train/accuracy=0.888144, train/loss=0.416861, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:37:40.204356 140380326586112 logging_writer.py:48] [194800] global_step=194800, grad_norm=3.2138867378234863, loss=1.127632975578308
I0303 13:38:21.809240 140380334978816 logging_writer.py:48] [194900] global_step=194900, grad_norm=3.3187804222106934, loss=1.9621788263320923
I0303 13:39:07.022324 140380326586112 logging_writer.py:48] [195000] global_step=195000, grad_norm=3.5557374954223633, loss=3.0872552394866943
I0303 13:39:52.206826 140380334978816 logging_writer.py:48] [195100] global_step=195100, grad_norm=3.0842831134796143, loss=1.0673407316207886
I0303 13:40:37.538628 140380326586112 logging_writer.py:48] [195200] global_step=195200, grad_norm=3.1601145267486572, loss=1.201613426208496
I0303 13:41:22.310679 140380334978816 logging_writer.py:48] [195300] global_step=195300, grad_norm=3.4640910625457764, loss=2.6367852687835693
I0303 13:42:07.482542 140380326586112 logging_writer.py:48] [195400] global_step=195400, grad_norm=3.2621514797210693, loss=2.694777250289917
I0303 13:42:52.076024 140380334978816 logging_writer.py:48] [195500] global_step=195500, grad_norm=3.768976926803589, loss=3.2003698348999023
I0303 13:43:36.955584 140380326586112 logging_writer.py:48] [195600] global_step=195600, grad_norm=3.983774423599243, loss=3.035764455795288
I0303 13:44:21.844069 140380334978816 logging_writer.py:48] [195700] global_step=195700, grad_norm=3.2273762226104736, loss=1.2836272716522217
I0303 13:44:27.498867 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:44:38.164833 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:45:01.486089 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:45:03.145788 140575196817216 submission_runner.py:411] Time since start: 94278.75s, 	Step: 195714, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.41984763741493225, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 87009.27286624908, 'total_duration': 94278.746717453, 'accumulated_submission_time': 87009.27286624908, 'accumulated_eval_time': 7248.850476980209, 'accumulated_logging_time': 10.240220785140991}
I0303 13:45:03.200546 140380326586112 logging_writer.py:48] [195714] accumulated_eval_time=7248.850477, accumulated_logging_time=10.240221, accumulated_submission_time=87009.272866, global_step=195714, preemption_count=0, score=87009.272866, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=94278.746717, train/accuracy=0.887207, train/loss=0.419848, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:45:37.781189 140380334978816 logging_writer.py:48] [195800] global_step=195800, grad_norm=3.127167224884033, loss=1.1062242984771729
I0303 13:46:22.508760 140380326586112 logging_writer.py:48] [195900] global_step=195900, grad_norm=3.3840880393981934, loss=2.868746042251587
I0303 13:47:07.578715 140380334978816 logging_writer.py:48] [196000] global_step=196000, grad_norm=3.0778703689575195, loss=1.1516849994659424
I0303 13:47:52.384291 140380326586112 logging_writer.py:48] [196100] global_step=196100, grad_norm=3.5880026817321777, loss=3.0564088821411133
I0303 13:48:37.308512 140380334978816 logging_writer.py:48] [196200] global_step=196200, grad_norm=3.5436322689056396, loss=1.1413942575454712
I0303 13:49:22.172439 140380326586112 logging_writer.py:48] [196300] global_step=196300, grad_norm=5.821463108062744, loss=3.1247875690460205
I0303 13:50:06.569841 140380334978816 logging_writer.py:48] [196400] global_step=196400, grad_norm=3.0057437419891357, loss=1.2323963642120361
I0303 13:50:51.694894 140380326586112 logging_writer.py:48] [196500] global_step=196500, grad_norm=2.8875956535339355, loss=0.9856950640678406
I0303 13:51:36.453327 140380334978816 logging_writer.py:48] [196600] global_step=196600, grad_norm=3.157562255859375, loss=2.597947835922241
I0303 13:52:03.583512 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:52:14.186324 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 13:52:38.067897 140575196817216 spec.py:349] Evaluating on the test split.
I0303 13:52:39.735582 140575196817216 submission_runner.py:411] Time since start: 94735.34s, 	Step: 196662, 	{'train/accuracy': 0.8884570002555847, 'train/loss': 0.4169119894504547, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 87429.595079422, 'total_duration': 94735.3364572525, 'accumulated_submission_time': 87429.595079422, 'accumulated_eval_time': 7285.001372337341, 'accumulated_logging_time': 10.306553363800049}
I0303 13:52:39.783800 140380326586112 logging_writer.py:48] [196662] accumulated_eval_time=7285.001372, accumulated_logging_time=10.306553, accumulated_submission_time=87429.595079, global_step=196662, preemption_count=0, score=87429.595079, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=94735.336457, train/accuracy=0.888457, train/loss=0.416912, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 13:52:55.297159 140380334978816 logging_writer.py:48] [196700] global_step=196700, grad_norm=3.0759334564208984, loss=1.4748941659927368
I0303 13:53:37.047941 140380326586112 logging_writer.py:48] [196800] global_step=196800, grad_norm=3.1550047397613525, loss=1.2670115232467651
I0303 13:54:21.988862 140380334978816 logging_writer.py:48] [196900] global_step=196900, grad_norm=3.1014676094055176, loss=1.3606404066085815
I0303 13:55:07.227959 140380326586112 logging_writer.py:48] [197000] global_step=197000, grad_norm=3.2897109985351562, loss=1.0800529718399048
I0303 13:55:52.044525 140380334978816 logging_writer.py:48] [197100] global_step=197100, grad_norm=3.1887781620025635, loss=1.0761736631393433
I0303 13:56:37.223687 140380326586112 logging_writer.py:48] [197200] global_step=197200, grad_norm=3.053205728530884, loss=1.392246961593628
I0303 13:57:22.271458 140380334978816 logging_writer.py:48] [197300] global_step=197300, grad_norm=4.571800708770752, loss=2.3828985691070557
I0303 13:58:07.228208 140380326586112 logging_writer.py:48] [197400] global_step=197400, grad_norm=3.3205974102020264, loss=1.4972271919250488
I0303 13:58:52.000937 140380334978816 logging_writer.py:48] [197500] global_step=197500, grad_norm=5.123720645904541, loss=2.560696601867676
I0303 13:59:36.862388 140380326586112 logging_writer.py:48] [197600] global_step=197600, grad_norm=3.1869325637817383, loss=2.317169666290283
I0303 13:59:40.109061 140575196817216 spec.py:321] Evaluating on the training split.
I0303 13:59:50.868346 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:00:12.641200 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:00:14.299664 140575196817216 submission_runner.py:411] Time since start: 95189.90s, 	Step: 197609, 	{'train/accuracy': 0.8848632574081421, 'train/loss': 0.426829069852829, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 87849.86155200005, 'total_duration': 95189.90076732635, 'accumulated_submission_time': 87849.86155200005, 'accumulated_eval_time': 7319.1910054683685, 'accumulated_logging_time': 10.364394903182983}
I0303 14:00:14.348052 140380334978816 logging_writer.py:48] [197609] accumulated_eval_time=7319.191005, accumulated_logging_time=10.364395, accumulated_submission_time=87849.861552, global_step=197609, preemption_count=0, score=87849.861552, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=95189.900767, train/accuracy=0.884863, train/loss=0.426829, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:00:50.955153 140380326586112 logging_writer.py:48] [197700] global_step=197700, grad_norm=3.1638689041137695, loss=1.24079430103302
I0303 14:01:35.663069 140380334978816 logging_writer.py:48] [197800] global_step=197800, grad_norm=3.1592133045196533, loss=1.712285041809082
I0303 14:02:20.867903 140380326586112 logging_writer.py:48] [197900] global_step=197900, grad_norm=3.236395835876465, loss=1.1777712106704712
I0303 14:03:05.939994 140380334978816 logging_writer.py:48] [198000] global_step=198000, grad_norm=4.046632766723633, loss=3.1572768688201904
I0303 14:03:50.723264 140380326586112 logging_writer.py:48] [198100] global_step=198100, grad_norm=3.036698341369629, loss=1.3127332925796509
I0303 14:04:35.776898 140380334978816 logging_writer.py:48] [198200] global_step=198200, grad_norm=4.189793586730957, loss=3.265059232711792
I0303 14:05:20.669951 140380326586112 logging_writer.py:48] [198300] global_step=198300, grad_norm=3.2239139080047607, loss=1.1421570777893066
I0303 14:06:05.888529 140380334978816 logging_writer.py:48] [198400] global_step=198400, grad_norm=3.6904842853546143, loss=3.1420280933380127
I0303 14:06:50.625476 140380326586112 logging_writer.py:48] [198500] global_step=198500, grad_norm=4.164149761199951, loss=3.2245681285858154
I0303 14:07:14.566456 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:07:25.057209 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:07:48.830306 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:07:50.480748 140575196817216 submission_runner.py:411] Time since start: 95646.08s, 	Step: 198555, 	{'train/accuracy': 0.8874609470367432, 'train/loss': 0.41238200664520264, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 88270.01970601082, 'total_duration': 95646.08177280426, 'accumulated_submission_time': 88270.01970601082, 'accumulated_eval_time': 7355.1042511463165, 'accumulated_logging_time': 10.42365837097168}
I0303 14:07:50.530495 140380334978816 logging_writer.py:48] [198555] accumulated_eval_time=7355.104251, accumulated_logging_time=10.423658, accumulated_submission_time=88270.019706, global_step=198555, preemption_count=0, score=88270.019706, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=95646.081773, train/accuracy=0.887461, train/loss=0.412382, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:08:08.808371 140380326586112 logging_writer.py:48] [198600] global_step=198600, grad_norm=3.140868902206421, loss=1.0885610580444336
I0303 14:08:51.212808 140380334978816 logging_writer.py:48] [198700] global_step=198700, grad_norm=3.1856486797332764, loss=1.1649560928344727
I0303 14:09:36.305328 140380326586112 logging_writer.py:48] [198800] global_step=198800, grad_norm=3.1711087226867676, loss=1.19002103805542
I0303 14:10:21.316889 140380334978816 logging_writer.py:48] [198900] global_step=198900, grad_norm=3.0020039081573486, loss=2.22598934173584
I0303 14:11:06.663719 140380326586112 logging_writer.py:48] [199000] global_step=199000, grad_norm=3.236164093017578, loss=1.0196961164474487
I0303 14:11:51.508721 140380334978816 logging_writer.py:48] [199100] global_step=199100, grad_norm=3.063866138458252, loss=1.1161444187164307
I0303 14:12:36.636229 140380326586112 logging_writer.py:48] [199200] global_step=199200, grad_norm=3.239272356033325, loss=2.671903133392334
I0303 14:13:21.490210 140380334978816 logging_writer.py:48] [199300] global_step=199300, grad_norm=3.3049540519714355, loss=1.928124189376831
I0303 14:14:06.257651 140380326586112 logging_writer.py:48] [199400] global_step=199400, grad_norm=3.2522895336151123, loss=1.4571800231933594
I0303 14:14:51.199667 140380334978816 logging_writer.py:48] [199500] global_step=199500, grad_norm=4.1078009605407715, loss=3.270859479904175
I0303 14:14:51.213365 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:15:01.915637 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:15:25.187358 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:15:26.844822 140575196817216 submission_runner.py:411] Time since start: 96102.45s, 	Step: 199501, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.41949188709259033, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 88690.64194583893, 'total_duration': 96102.44577240944, 'accumulated_submission_time': 88690.64194583893, 'accumulated_eval_time': 7390.73459148407, 'accumulated_logging_time': 10.484740495681763}
I0303 14:15:26.897733 140380326586112 logging_writer.py:48] [199501] accumulated_eval_time=7390.734591, accumulated_logging_time=10.484740, accumulated_submission_time=88690.641946, global_step=199501, preemption_count=0, score=88690.641946, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=96102.445772, train/accuracy=0.887578, train/loss=0.419492, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:16:06.955164 140380334978816 logging_writer.py:48] [199600] global_step=199600, grad_norm=3.8615410327911377, loss=3.0186774730682373
I0303 14:16:51.908602 140380326586112 logging_writer.py:48] [199700] global_step=199700, grad_norm=3.294607162475586, loss=1.1393659114837646
I0303 14:17:37.039555 140380334978816 logging_writer.py:48] [199800] global_step=199800, grad_norm=2.945739269256592, loss=1.1738840341567993
I0303 14:18:22.016152 140380326586112 logging_writer.py:48] [199900] global_step=199900, grad_norm=3.7293238639831543, loss=2.400662660598755
I0303 14:19:06.781588 140380334978816 logging_writer.py:48] [200000] global_step=200000, grad_norm=2.92606782913208, loss=1.959116816520691
I0303 14:19:52.596117 140380326586112 logging_writer.py:48] [200100] global_step=200100, grad_norm=3.2459867000579834, loss=1.1418378353118896
I0303 14:20:37.607435 140380334978816 logging_writer.py:48] [200200] global_step=200200, grad_norm=3.157987594604492, loss=1.608687162399292
I0303 14:21:22.530417 140380326586112 logging_writer.py:48] [200300] global_step=200300, grad_norm=2.912712812423706, loss=1.3330601453781128
I0303 14:22:07.519334 140380334978816 logging_writer.py:48] [200400] global_step=200400, grad_norm=4.084961891174316, loss=1.301838755607605
I0303 14:22:26.915853 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:22:37.669379 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:23:02.184759 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:23:03.840261 140575196817216 submission_runner.py:411] Time since start: 96559.44s, 	Step: 200445, 	{'train/accuracy': 0.8860741853713989, 'train/loss': 0.42459869384765625, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 89110.59979486465, 'total_duration': 96559.44136738777, 'accumulated_submission_time': 89110.59979486465, 'accumulated_eval_time': 7427.6580538749695, 'accumulated_logging_time': 10.54875898361206}
I0303 14:23:03.886290 140380326586112 logging_writer.py:48] [200445] accumulated_eval_time=7427.658054, accumulated_logging_time=10.548759, accumulated_submission_time=89110.599795, global_step=200445, preemption_count=0, score=89110.599795, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=96559.441367, train/accuracy=0.886074, train/loss=0.424599, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:23:26.136667 140380334978816 logging_writer.py:48] [200500] global_step=200500, grad_norm=2.9067723751068115, loss=1.0805038213729858
I0303 14:24:09.117431 140380326586112 logging_writer.py:48] [200600] global_step=200600, grad_norm=2.9863102436065674, loss=1.0846492052078247
I0303 14:24:54.052241 140380334978816 logging_writer.py:48] [200700] global_step=200700, grad_norm=2.97921085357666, loss=1.166351318359375
I0303 14:25:39.064291 140380326586112 logging_writer.py:48] [200800] global_step=200800, grad_norm=3.2106008529663086, loss=1.1385891437530518
I0303 14:26:24.157130 140380334978816 logging_writer.py:48] [200900] global_step=200900, grad_norm=2.978304862976074, loss=1.8710403442382812
I0303 14:27:09.025244 140380326586112 logging_writer.py:48] [201000] global_step=201000, grad_norm=3.102875232696533, loss=1.1204452514648438
I0303 14:27:53.696174 140380334978816 logging_writer.py:48] [201100] global_step=201100, grad_norm=4.299200057983398, loss=3.1922240257263184
I0303 14:28:38.993634 140380326586112 logging_writer.py:48] [201200] global_step=201200, grad_norm=3.0898187160491943, loss=1.1218510866165161
I0303 14:29:23.898472 140380334978816 logging_writer.py:48] [201300] global_step=201300, grad_norm=2.809488534927368, loss=1.7591512203216553
I0303 14:30:03.856695 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:30:14.539489 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:30:37.011187 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:30:38.671049 140575196817216 submission_runner.py:411] Time since start: 97014.27s, 	Step: 201391, 	{'train/accuracy': 0.8870702981948853, 'train/loss': 0.4246867001056671, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 89530.50890040398, 'total_duration': 97014.27209377289, 'accumulated_submission_time': 89530.50890040398, 'accumulated_eval_time': 7462.471394062042, 'accumulated_logging_time': 10.607412338256836}
I0303 14:30:38.721482 140380326586112 logging_writer.py:48] [201391] accumulated_eval_time=7462.471394, accumulated_logging_time=10.607412, accumulated_submission_time=89530.508900, global_step=201391, preemption_count=0, score=89530.508900, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=97014.272094, train/accuracy=0.887070, train/loss=0.424687, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:30:42.706105 140380334978816 logging_writer.py:48] [201400] global_step=201400, grad_norm=3.10896635055542, loss=2.5928092002868652
I0303 14:31:23.484912 140380326586112 logging_writer.py:48] [201500] global_step=201500, grad_norm=3.246856212615967, loss=1.1056042909622192
I0303 14:32:08.574704 140380334978816 logging_writer.py:48] [201600] global_step=201600, grad_norm=3.201080322265625, loss=1.3995280265808105
I0303 14:32:53.725249 140380326586112 logging_writer.py:48] [201700] global_step=201700, grad_norm=2.810730218887329, loss=1.3041115999221802
I0303 14:33:38.866155 140380334978816 logging_writer.py:48] [201800] global_step=201800, grad_norm=3.007802963256836, loss=1.2560856342315674
I0303 14:34:23.926891 140380326586112 logging_writer.py:48] [201900] global_step=201900, grad_norm=3.178118944168091, loss=1.473190188407898
I0303 14:35:09.093041 140380334978816 logging_writer.py:48] [202000] global_step=202000, grad_norm=4.977898120880127, loss=3.02500057220459
I0303 14:35:54.186794 140380326586112 logging_writer.py:48] [202100] global_step=202100, grad_norm=2.9163243770599365, loss=1.8278182744979858
I0303 14:36:39.364901 140380334978816 logging_writer.py:48] [202200] global_step=202200, grad_norm=3.7742183208465576, loss=1.0655934810638428
I0303 14:37:24.197976 140380326586112 logging_writer.py:48] [202300] global_step=202300, grad_norm=3.088547945022583, loss=1.166843056678772
I0303 14:37:38.935384 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:37:49.414360 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:38:14.132832 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:38:15.798969 140575196817216 submission_runner.py:411] Time since start: 97471.40s, 	Step: 202334, 	{'train/accuracy': 0.8855664134025574, 'train/loss': 0.427189439535141, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 89950.66359424591, 'total_duration': 97471.39986658096, 'accumulated_submission_time': 89950.66359424591, 'accumulated_eval_time': 7499.333806037903, 'accumulated_logging_time': 10.667636156082153}
I0303 14:38:15.862494 140380334978816 logging_writer.py:48] [202334] accumulated_eval_time=7499.333806, accumulated_logging_time=10.667636, accumulated_submission_time=89950.663594, global_step=202334, preemption_count=0, score=89950.663594, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=97471.399867, train/accuracy=0.885566, train/loss=0.427189, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:38:42.485366 140380326586112 logging_writer.py:48] [202400] global_step=202400, grad_norm=3.0061779022216797, loss=1.611475944519043
I0303 14:39:26.477673 140380334978816 logging_writer.py:48] [202500] global_step=202500, grad_norm=3.108701705932617, loss=1.3730850219726562
I0303 14:40:11.952158 140380326586112 logging_writer.py:48] [202600] global_step=202600, grad_norm=2.9870615005493164, loss=1.0405247211456299
I0303 14:40:57.515285 140380334978816 logging_writer.py:48] [202700] global_step=202700, grad_norm=3.3331968784332275, loss=1.044966459274292
I0303 14:41:42.500583 140380326586112 logging_writer.py:48] [202800] global_step=202800, grad_norm=3.024341344833374, loss=1.1684722900390625
I0303 14:42:27.442052 140380334978816 logging_writer.py:48] [202900] global_step=202900, grad_norm=3.156869888305664, loss=1.5830429792404175
I0303 14:43:12.045177 140380326586112 logging_writer.py:48] [203000] global_step=203000, grad_norm=3.655691146850586, loss=2.948007106781006
I0303 14:43:56.972540 140380334978816 logging_writer.py:48] [203100] global_step=203100, grad_norm=3.164842128753662, loss=1.1954482793807983
I0303 14:44:41.884855 140380326586112 logging_writer.py:48] [203200] global_step=203200, grad_norm=5.143209934234619, loss=3.1448004245758057
I0303 14:45:16.124444 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:45:26.791039 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:45:49.818093 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:45:51.470330 140575196817216 submission_runner.py:411] Time since start: 97927.07s, 	Step: 203278, 	{'train/accuracy': 0.8870702981948853, 'train/loss': 0.41948264837265015, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 90370.86471438408, 'total_duration': 97927.07135868073, 'accumulated_submission_time': 90370.86471438408, 'accumulated_eval_time': 7534.678661584854, 'accumulated_logging_time': 10.743087768554688}
I0303 14:45:51.519362 140380334978816 logging_writer.py:48] [203278] accumulated_eval_time=7534.678662, accumulated_logging_time=10.743088, accumulated_submission_time=90370.864714, global_step=203278, preemption_count=0, score=90370.864714, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=97927.071359, train/accuracy=0.887070, train/loss=0.419483, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:46:00.675091 140380326586112 logging_writer.py:48] [203300] global_step=203300, grad_norm=3.0898427963256836, loss=2.4546029567718506
I0303 14:46:41.925681 140380334978816 logging_writer.py:48] [203400] global_step=203400, grad_norm=4.608026504516602, loss=3.255873918533325
I0303 14:47:27.094744 140380326586112 logging_writer.py:48] [203500] global_step=203500, grad_norm=3.6257379055023193, loss=2.168088674545288
I0303 14:48:12.595610 140380334978816 logging_writer.py:48] [203600] global_step=203600, grad_norm=3.163604259490967, loss=1.1013834476470947
I0303 14:48:57.789626 140380326586112 logging_writer.py:48] [203700] global_step=203700, grad_norm=3.3256113529205322, loss=1.6322484016418457
I0303 14:49:43.066036 140380334978816 logging_writer.py:48] [203800] global_step=203800, grad_norm=3.1428987979888916, loss=1.2205742597579956
I0303 14:50:27.919151 140380326586112 logging_writer.py:48] [203900] global_step=203900, grad_norm=3.3012101650238037, loss=1.071165919303894
I0303 14:51:13.114697 140380334978816 logging_writer.py:48] [204000] global_step=204000, grad_norm=3.338446617126465, loss=1.162479281425476
I0303 14:51:58.225886 140380326586112 logging_writer.py:48] [204100] global_step=204100, grad_norm=2.9622304439544678, loss=1.2531667947769165
I0303 14:52:43.002974 140380334978816 logging_writer.py:48] [204200] global_step=204200, grad_norm=3.1799428462982178, loss=1.6402684450149536
I0303 14:52:51.708530 140575196817216 spec.py:321] Evaluating on the training split.
I0303 14:53:02.704250 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 14:53:26.042086 140575196817216 spec.py:349] Evaluating on the test split.
I0303 14:53:27.700586 140575196817216 submission_runner.py:411] Time since start: 98383.30s, 	Step: 204221, 	{'train/accuracy': 0.8884961009025574, 'train/loss': 0.4168415665626526, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 90790.99326586723, 'total_duration': 98383.30153632164, 'accumulated_submission_time': 90790.99326586723, 'accumulated_eval_time': 7570.669605970383, 'accumulated_logging_time': 10.802512645721436}
I0303 14:53:27.746503 140380326586112 logging_writer.py:48] [204221] accumulated_eval_time=7570.669606, accumulated_logging_time=10.802513, accumulated_submission_time=90790.993266, global_step=204221, preemption_count=0, score=90790.993266, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=98383.301536, train/accuracy=0.888496, train/loss=0.416842, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 14:53:59.555396 140380334978816 logging_writer.py:48] [204300] global_step=204300, grad_norm=3.5770761966705322, loss=2.0923538208007812
I0303 14:54:43.696938 140380326586112 logging_writer.py:48] [204400] global_step=204400, grad_norm=3.08982515335083, loss=1.1146248579025269
I0303 14:55:28.929059 140380334978816 logging_writer.py:48] [204500] global_step=204500, grad_norm=2.757451057434082, loss=1.1821620464324951
I0303 14:56:14.071894 140380326586112 logging_writer.py:48] [204600] global_step=204600, grad_norm=3.203218936920166, loss=1.610465407371521
I0303 14:56:59.122666 140380334978816 logging_writer.py:48] [204700] global_step=204700, grad_norm=3.5960116386413574, loss=2.740156412124634
I0303 14:57:44.171777 140380326586112 logging_writer.py:48] [204800] global_step=204800, grad_norm=3.524149179458618, loss=1.19932222366333
I0303 14:58:29.044731 140380334978816 logging_writer.py:48] [204900] global_step=204900, grad_norm=3.583498477935791, loss=1.192469835281372
I0303 14:59:14.276902 140380326586112 logging_writer.py:48] [205000] global_step=205000, grad_norm=3.698146343231201, loss=3.271331310272217
I0303 14:59:59.133623 140380334978816 logging_writer.py:48] [205100] global_step=205100, grad_norm=3.2405834197998047, loss=1.0871235132217407
I0303 15:00:28.000719 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:00:38.408052 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:01:02.727846 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:01:04.388715 140575196817216 submission_runner.py:411] Time since start: 98839.99s, 	Step: 205166, 	{'train/accuracy': 0.8862499594688416, 'train/loss': 0.4210224449634552, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 91211.18811535835, 'total_duration': 98839.98974704742, 'accumulated_submission_time': 91211.18811535835, 'accumulated_eval_time': 7607.0565667152405, 'accumulated_logging_time': 10.85942006111145}
I0303 15:01:04.436178 140380326586112 logging_writer.py:48] [205166] accumulated_eval_time=7607.056567, accumulated_logging_time=10.859420, accumulated_submission_time=91211.188115, global_step=205166, preemption_count=0, score=91211.188115, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=98839.989747, train/accuracy=0.886250, train/loss=0.421022, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:01:18.340694 140380334978816 logging_writer.py:48] [205200] global_step=205200, grad_norm=3.301419496536255, loss=1.9959019422531128
I0303 15:02:00.351664 140380326586112 logging_writer.py:48] [205300] global_step=205300, grad_norm=3.0563952922821045, loss=1.1723324060440063
I0303 15:02:45.201058 140380334978816 logging_writer.py:48] [205400] global_step=205400, grad_norm=3.3087050914764404, loss=1.2611817121505737
I0303 15:03:30.282678 140380326586112 logging_writer.py:48] [205500] global_step=205500, grad_norm=3.02551531791687, loss=1.2390497922897339
I0303 15:04:15.176560 140380334978816 logging_writer.py:48] [205600] global_step=205600, grad_norm=3.001533269882202, loss=1.2995498180389404
I0303 15:05:00.027511 140380326586112 logging_writer.py:48] [205700] global_step=205700, grad_norm=4.164707660675049, loss=3.1410603523254395
I0303 15:05:45.076987 140380334978816 logging_writer.py:48] [205800] global_step=205800, grad_norm=2.8771841526031494, loss=1.8726019859313965
I0303 15:06:29.908588 140380326586112 logging_writer.py:48] [205900] global_step=205900, grad_norm=3.299339532852173, loss=1.2011923789978027
I0303 15:07:14.738850 140380334978816 logging_writer.py:48] [206000] global_step=206000, grad_norm=3.205303192138672, loss=2.071230888366699
I0303 15:07:59.970304 140380326586112 logging_writer.py:48] [206100] global_step=206100, grad_norm=3.141180992126465, loss=2.44760799407959
I0303 15:08:04.562938 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:08:15.390257 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:08:40.499236 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:08:42.163899 140575196817216 submission_runner.py:411] Time since start: 99297.76s, 	Step: 206112, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.4213772118091583, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 91631.25634765625, 'total_duration': 99297.76484441757, 'accumulated_submission_time': 91631.25634765625, 'accumulated_eval_time': 7644.656408786774, 'accumulated_logging_time': 10.91633415222168}
I0303 15:08:42.218886 140380334978816 logging_writer.py:48] [206112] accumulated_eval_time=7644.656409, accumulated_logging_time=10.916334, accumulated_submission_time=91631.256348, global_step=206112, preemption_count=0, score=91631.256348, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=99297.764844, train/accuracy=0.887598, train/loss=0.421377, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:09:17.581261 140380326586112 logging_writer.py:48] [206200] global_step=206200, grad_norm=4.183940887451172, loss=3.2636313438415527
I0303 15:10:02.328207 140380334978816 logging_writer.py:48] [206300] global_step=206300, grad_norm=2.9705252647399902, loss=1.0463104248046875
I0303 15:10:46.834055 140380326586112 logging_writer.py:48] [206400] global_step=206400, grad_norm=3.741410493850708, loss=3.101053237915039
I0303 15:11:32.404811 140380334978816 logging_writer.py:48] [206500] global_step=206500, grad_norm=3.125387668609619, loss=2.0333962440490723
I0303 15:12:17.665811 140380326586112 logging_writer.py:48] [206600] global_step=206600, grad_norm=5.070591449737549, loss=3.2933249473571777
I0303 15:13:02.748994 140380334978816 logging_writer.py:48] [206700] global_step=206700, grad_norm=3.1265735626220703, loss=1.4346024990081787
I0303 15:13:47.747195 140380326586112 logging_writer.py:48] [206800] global_step=206800, grad_norm=3.1418328285217285, loss=1.118497371673584
I0303 15:14:33.027145 140380334978816 logging_writer.py:48] [206900] global_step=206900, grad_norm=3.1518430709838867, loss=1.1591944694519043
I0303 15:15:17.951945 140380326586112 logging_writer.py:48] [207000] global_step=207000, grad_norm=3.107724666595459, loss=1.5858687162399292
I0303 15:15:42.342832 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:15:53.050132 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:16:14.942199 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:16:16.588091 140575196817216 submission_runner.py:411] Time since start: 99752.19s, 	Step: 207056, 	{'train/accuracy': 0.8889257907867432, 'train/loss': 0.4079552888870239, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 92051.31888103485, 'total_duration': 99752.18914794922, 'accumulated_submission_time': 92051.31888103485, 'accumulated_eval_time': 7678.900659561157, 'accumulated_logging_time': 10.983742237091064}
I0303 15:16:16.634890 140380334978816 logging_writer.py:48] [207056] accumulated_eval_time=7678.900660, accumulated_logging_time=10.983742, accumulated_submission_time=92051.318881, global_step=207056, preemption_count=0, score=92051.318881, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=99752.189148, train/accuracy=0.888926, train/loss=0.407955, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:16:34.531772 140380326586112 logging_writer.py:48] [207100] global_step=207100, grad_norm=3.0978705883026123, loss=1.3008240461349487
I0303 15:17:16.773885 140380334978816 logging_writer.py:48] [207200] global_step=207200, grad_norm=3.8286445140838623, loss=3.127243995666504
I0303 15:18:02.066622 140380326586112 logging_writer.py:48] [207300] global_step=207300, grad_norm=3.3084967136383057, loss=1.6010785102844238
I0303 15:18:47.428017 140380334978816 logging_writer.py:48] [207400] global_step=207400, grad_norm=3.0173847675323486, loss=1.0483251810073853
I0303 15:19:32.400051 140380326586112 logging_writer.py:48] [207500] global_step=207500, grad_norm=3.045640468597412, loss=1.2473922967910767
I0303 15:20:17.648462 140380334978816 logging_writer.py:48] [207600] global_step=207600, grad_norm=3.061361074447632, loss=1.3004283905029297
I0303 15:21:02.766951 140380326586112 logging_writer.py:48] [207700] global_step=207700, grad_norm=3.5994720458984375, loss=2.615417957305908
I0303 15:21:47.543342 140380334978816 logging_writer.py:48] [207800] global_step=207800, grad_norm=5.096821308135986, loss=2.2000136375427246
I0303 15:22:32.553226 140380326586112 logging_writer.py:48] [207900] global_step=207900, grad_norm=3.5492072105407715, loss=2.004343271255493
I0303 15:23:17.348267 140380334978816 logging_writer.py:48] [208000] global_step=208000, grad_norm=3.213646173477173, loss=1.144972562789917
I0303 15:23:17.364015 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:23:28.105281 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:23:51.292275 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:23:52.948868 140575196817216 submission_runner.py:411] Time since start: 100208.55s, 	Step: 208001, 	{'train/accuracy': 0.8864452838897705, 'train/loss': 0.425664484500885, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 92471.98997926712, 'total_duration': 100208.54979085922, 'accumulated_submission_time': 92471.98997926712, 'accumulated_eval_time': 7714.484362125397, 'accumulated_logging_time': 11.040274143218994}
I0303 15:23:52.997382 140380326586112 logging_writer.py:48] [208001] accumulated_eval_time=7714.484362, accumulated_logging_time=11.040274, accumulated_submission_time=92471.989979, global_step=208001, preemption_count=0, score=92471.989979, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=100208.549791, train/accuracy=0.886445, train/loss=0.425664, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:24:32.945909 140380334978816 logging_writer.py:48] [208100] global_step=208100, grad_norm=2.921321153640747, loss=1.7082539796829224
I0303 15:25:17.820424 140380326586112 logging_writer.py:48] [208200] global_step=208200, grad_norm=3.3849871158599854, loss=1.7265514135360718
I0303 15:26:02.936389 140380334978816 logging_writer.py:48] [208300] global_step=208300, grad_norm=4.184891223907471, loss=2.772289752960205
I0303 15:26:48.205092 140380326586112 logging_writer.py:48] [208400] global_step=208400, grad_norm=3.5890467166900635, loss=1.171093463897705
I0303 15:27:33.051058 140380334978816 logging_writer.py:48] [208500] global_step=208500, grad_norm=3.2037007808685303, loss=2.2051939964294434
I0303 15:28:17.915844 140380326586112 logging_writer.py:48] [208600] global_step=208600, grad_norm=2.9771530628204346, loss=1.1234945058822632
I0303 15:29:02.946181 140380334978816 logging_writer.py:48] [208700] global_step=208700, grad_norm=3.030451774597168, loss=1.2029378414154053
I0303 15:29:47.857057 140380326586112 logging_writer.py:48] [208800] global_step=208800, grad_norm=3.280902862548828, loss=2.4545187950134277
I0303 15:30:32.505323 140380334978816 logging_writer.py:48] [208900] global_step=208900, grad_norm=3.2445621490478516, loss=2.3710150718688965
I0303 15:30:53.095520 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:31:03.819546 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:31:26.983752 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:31:28.644270 140575196817216 submission_runner.py:411] Time since start: 100664.25s, 	Step: 208947, 	{'train/accuracy': 0.8887109160423279, 'train/loss': 0.41545364260673523, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 92892.02937984467, 'total_duration': 100664.24516439438, 'accumulated_submission_time': 92892.02937984467, 'accumulated_eval_time': 7750.031948566437, 'accumulated_logging_time': 11.0984628200531}
I0303 15:31:28.696256 140380326586112 logging_writer.py:48] [208947] accumulated_eval_time=7750.031949, accumulated_logging_time=11.098463, accumulated_submission_time=92892.029380, global_step=208947, preemption_count=0, score=92892.029380, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=100664.245164, train/accuracy=0.888711, train/loss=0.415454, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:31:50.158620 140380334978816 logging_writer.py:48] [209000] global_step=209000, grad_norm=3.03787899017334, loss=1.1655268669128418
I0303 15:32:32.828505 140380326586112 logging_writer.py:48] [209100] global_step=209100, grad_norm=3.3298606872558594, loss=1.0908902883529663
I0303 15:33:17.815714 140380334978816 logging_writer.py:48] [209200] global_step=209200, grad_norm=2.9819061756134033, loss=1.3737000226974487
I0303 15:34:02.878934 140380326586112 logging_writer.py:48] [209300] global_step=209300, grad_norm=7.439752101898193, loss=2.158538579940796
I0303 15:34:47.632987 140380334978816 logging_writer.py:48] [209400] global_step=209400, grad_norm=3.2398152351379395, loss=1.1737656593322754
I0303 15:35:32.912056 140380326586112 logging_writer.py:48] [209500] global_step=209500, grad_norm=3.408144474029541, loss=3.064899444580078
I0303 15:36:17.984849 140380334978816 logging_writer.py:48] [209600] global_step=209600, grad_norm=10.07597541809082, loss=1.2653343677520752
I0303 15:37:02.667460 140380326586112 logging_writer.py:48] [209700] global_step=209700, grad_norm=3.1034224033355713, loss=1.0834541320800781
I0303 15:37:47.575828 140380334978816 logging_writer.py:48] [209800] global_step=209800, grad_norm=3.2470083236694336, loss=2.6999380588531494
I0303 15:38:28.787148 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:38:39.398132 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:39:03.731332 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:39:05.388602 140575196817216 submission_runner.py:411] Time since start: 101120.99s, 	Step: 209894, 	{'train/accuracy': 0.8918554782867432, 'train/loss': 0.40474674105644226, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 93312.06176161766, 'total_duration': 101120.9896683693, 'accumulated_submission_time': 93312.06176161766, 'accumulated_eval_time': 7786.6324117183685, 'accumulated_logging_time': 11.160149812698364}
I0303 15:39:05.437736 140380326586112 logging_writer.py:48] [209894] accumulated_eval_time=7786.632412, accumulated_logging_time=11.160150, accumulated_submission_time=93312.061762, global_step=209894, preemption_count=0, score=93312.061762, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=101120.989668, train/accuracy=0.891855, train/loss=0.404747, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:39:08.235810 140380334978816 logging_writer.py:48] [209900] global_step=209900, grad_norm=3.1190316677093506, loss=1.2727991342544556
I0303 15:39:48.937847 140380326586112 logging_writer.py:48] [210000] global_step=210000, grad_norm=2.8333511352539062, loss=1.072938323020935
I0303 15:40:33.788714 140380334978816 logging_writer.py:48] [210100] global_step=210100, grad_norm=3.2820677757263184, loss=1.0991541147232056
I0303 15:41:18.844429 140380326586112 logging_writer.py:48] [210200] global_step=210200, grad_norm=3.7179794311523438, loss=1.5053925514221191
I0303 15:42:04.012318 140380334978816 logging_writer.py:48] [210300] global_step=210300, grad_norm=3.138852119445801, loss=1.05147385597229
I0303 15:42:48.571254 140380326586112 logging_writer.py:48] [210400] global_step=210400, grad_norm=4.392946720123291, loss=2.8508615493774414
I0303 15:43:33.636644 140380334978816 logging_writer.py:48] [210500] global_step=210500, grad_norm=3.448490619659424, loss=2.737119436264038
I0303 15:44:18.714991 140380326586112 logging_writer.py:48] [210600] global_step=210600, grad_norm=3.2631845474243164, loss=2.5546300411224365
I0303 15:45:03.688080 140380334978816 logging_writer.py:48] [210700] global_step=210700, grad_norm=2.967052698135376, loss=1.1232119798660278
I0303 15:45:48.695407 140380326586112 logging_writer.py:48] [210800] global_step=210800, grad_norm=4.037497520446777, loss=3.2026824951171875
I0303 15:46:05.660000 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:46:16.582968 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:46:40.581316 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:46:42.238556 140575196817216 submission_runner.py:411] Time since start: 101577.84s, 	Step: 210839, 	{'train/accuracy': 0.8872460722923279, 'train/loss': 0.4177339971065521, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 93732.22330355644, 'total_duration': 101577.8395473957, 'accumulated_submission_time': 93732.22330355644, 'accumulated_eval_time': 7823.209892272949, 'accumulated_logging_time': 11.221330165863037}
I0303 15:46:42.299779 140380334978816 logging_writer.py:48] [210839] accumulated_eval_time=7823.209892, accumulated_logging_time=11.221330, accumulated_submission_time=93732.223304, global_step=210839, preemption_count=0, score=93732.223304, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=101577.839547, train/accuracy=0.887246, train/loss=0.417734, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:47:06.949621 140380326586112 logging_writer.py:48] [210900] global_step=210900, grad_norm=3.114298105239868, loss=1.9356149435043335
I0303 15:47:50.181803 140380334978816 logging_writer.py:48] [211000] global_step=211000, grad_norm=3.0995490550994873, loss=1.0962793827056885
I0303 15:48:35.258362 140380326586112 logging_writer.py:48] [211100] global_step=211100, grad_norm=3.283445358276367, loss=2.4840445518493652
I0303 15:49:20.355594 140380334978816 logging_writer.py:48] [211200] global_step=211200, grad_norm=3.289537191390991, loss=2.5164685249328613
I0303 15:50:05.168259 140380326586112 logging_writer.py:48] [211300] global_step=211300, grad_norm=3.579202175140381, loss=1.8805066347122192
I0303 15:50:50.285394 140380334978816 logging_writer.py:48] [211400] global_step=211400, grad_norm=3.038713216781616, loss=1.776566505432129
I0303 15:51:35.820700 140380326586112 logging_writer.py:48] [211500] global_step=211500, grad_norm=3.175340175628662, loss=2.25577449798584
I0303 15:52:20.616654 140380334978816 logging_writer.py:48] [211600] global_step=211600, grad_norm=3.129868984222412, loss=1.1726614236831665
I0303 15:53:05.483364 140380326586112 logging_writer.py:48] [211700] global_step=211700, grad_norm=3.3048057556152344, loss=1.3888404369354248
I0303 15:53:42.607808 140575196817216 spec.py:321] Evaluating on the training split.
I0303 15:53:53.249386 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 15:54:15.844298 140575196817216 spec.py:349] Evaluating on the test split.
I0303 15:54:17.505333 140575196817216 submission_runner.py:411] Time since start: 102033.11s, 	Step: 211784, 	{'train/accuracy': 0.8871093392372131, 'train/loss': 0.4195844233036041, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 94152.4714114666, 'total_duration': 102033.10613369942, 'accumulated_submission_time': 94152.4714114666, 'accumulated_eval_time': 7858.10614824295, 'accumulated_logging_time': 11.293134450912476}
I0303 15:54:17.556591 140380334978816 logging_writer.py:48] [211784] accumulated_eval_time=7858.106148, accumulated_logging_time=11.293134, accumulated_submission_time=94152.471411, global_step=211784, preemption_count=0, score=94152.471411, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=102033.106134, train/accuracy=0.887109, train/loss=0.419584, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 15:54:24.316164 140380326586112 logging_writer.py:48] [211800] global_step=211800, grad_norm=2.998828887939453, loss=1.5385991334915161
I0303 15:55:05.355395 140380334978816 logging_writer.py:48] [211900] global_step=211900, grad_norm=4.694574356079102, loss=3.2189440727233887
I0303 15:55:50.185760 140380326586112 logging_writer.py:48] [212000] global_step=212000, grad_norm=3.360584259033203, loss=1.1457674503326416
I0303 15:56:35.599202 140380334978816 logging_writer.py:48] [212100] global_step=212100, grad_norm=3.0342867374420166, loss=1.1096627712249756
I0303 15:57:20.374083 140380326586112 logging_writer.py:48] [212200] global_step=212200, grad_norm=3.82574200630188, loss=3.043595790863037
I0303 15:58:05.535267 140380334978816 logging_writer.py:48] [212300] global_step=212300, grad_norm=3.531076669692993, loss=1.2104361057281494
I0303 15:58:50.300896 140380326586112 logging_writer.py:48] [212400] global_step=212400, grad_norm=3.0346906185150146, loss=1.2494677305221558
I0303 15:59:35.358075 140380334978816 logging_writer.py:48] [212500] global_step=212500, grad_norm=3.0907366275787354, loss=1.1427717208862305
I0303 16:00:20.088571 140380326586112 logging_writer.py:48] [212600] global_step=212600, grad_norm=3.0233800411224365, loss=1.251487374305725
I0303 16:01:05.474190 140380334978816 logging_writer.py:48] [212700] global_step=212700, grad_norm=3.783935546875, loss=2.8331878185272217
I0303 16:01:17.693418 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:01:28.388589 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:01:51.616001 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:01:53.263960 140575196817216 submission_runner.py:411] Time since start: 102488.87s, 	Step: 212729, 	{'train/accuracy': 0.8864257335662842, 'train/loss': 0.4255499839782715, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 94572.54891109467, 'total_duration': 102488.86503720284, 'accumulated_submission_time': 94572.54891109467, 'accumulated_eval_time': 7893.675740480423, 'accumulated_logging_time': 11.354552745819092}
I0303 16:01:53.315186 140380326586112 logging_writer.py:48] [212729] accumulated_eval_time=7893.675740, accumulated_logging_time=11.354553, accumulated_submission_time=94572.548911, global_step=212729, preemption_count=0, score=94572.548911, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=102488.865037, train/accuracy=0.886426, train/loss=0.425550, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:02:21.925927 140380334978816 logging_writer.py:48] [212800] global_step=212800, grad_norm=3.3182477951049805, loss=2.8041489124298096
I0303 16:03:05.872293 140380326586112 logging_writer.py:48] [212900] global_step=212900, grad_norm=3.1638002395629883, loss=1.1938586235046387
I0303 16:03:51.576636 140380334978816 logging_writer.py:48] [213000] global_step=213000, grad_norm=2.9796972274780273, loss=1.7682886123657227
I0303 16:04:36.536397 140380326586112 logging_writer.py:48] [213100] global_step=213100, grad_norm=3.19216251373291, loss=1.2138830423355103
I0303 16:05:21.700876 140380334978816 logging_writer.py:48] [213200] global_step=213200, grad_norm=3.150665521621704, loss=2.6285765171051025
I0303 16:06:06.835957 140380326586112 logging_writer.py:48] [213300] global_step=213300, grad_norm=3.370814323425293, loss=2.93778133392334
I0303 16:06:52.017583 140380334978816 logging_writer.py:48] [213400] global_step=213400, grad_norm=3.219907760620117, loss=1.498547911643982
I0303 16:07:37.093399 140380326586112 logging_writer.py:48] [213500] global_step=213500, grad_norm=4.588973522186279, loss=3.1419734954833984
I0303 16:08:21.889385 140380334978816 logging_writer.py:48] [213600] global_step=213600, grad_norm=3.8900365829467773, loss=3.315624713897705
I0303 16:08:53.439816 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:09:04.308130 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:09:28.569150 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:09:30.224600 140575196817216 submission_runner.py:411] Time since start: 102945.83s, 	Step: 213672, 	{'train/accuracy': 0.8885546922683716, 'train/loss': 0.41471216082572937, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 94992.61476254463, 'total_duration': 102945.82554292679, 'accumulated_submission_time': 94992.61476254463, 'accumulated_eval_time': 7930.459398984909, 'accumulated_logging_time': 11.415436506271362}
I0303 16:09:30.272542 140380326586112 logging_writer.py:48] [213672] accumulated_eval_time=7930.459399, accumulated_logging_time=11.415437, accumulated_submission_time=94992.614763, global_step=213672, preemption_count=0, score=94992.614763, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=102945.825543, train/accuracy=0.888555, train/loss=0.414712, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:09:41.803869 140380334978816 logging_writer.py:48] [213700] global_step=213700, grad_norm=2.82452654838562, loss=1.440922498703003
I0303 16:10:23.285506 140380326586112 logging_writer.py:48] [213800] global_step=213800, grad_norm=2.9909515380859375, loss=1.187535047531128
I0303 16:11:07.967284 140380334978816 logging_writer.py:48] [213900] global_step=213900, grad_norm=4.8242292404174805, loss=1.1597192287445068
I0303 16:11:53.484073 140380326586112 logging_writer.py:48] [214000] global_step=214000, grad_norm=3.096024513244629, loss=1.4955042600631714
I0303 16:12:38.677039 140380334978816 logging_writer.py:48] [214100] global_step=214100, grad_norm=3.0812230110168457, loss=2.4230058193206787
I0303 16:13:23.625201 140380326586112 logging_writer.py:48] [214200] global_step=214200, grad_norm=3.0198702812194824, loss=1.554675579071045
I0303 16:14:08.706292 140380334978816 logging_writer.py:48] [214300] global_step=214300, grad_norm=3.718080520629883, loss=2.114722490310669
I0303 16:14:53.668337 140380326586112 logging_writer.py:48] [214400] global_step=214400, grad_norm=3.3950321674346924, loss=2.0590758323669434
I0303 16:15:38.664049 140380334978816 logging_writer.py:48] [214500] global_step=214500, grad_norm=3.1846539974212646, loss=1.1015762090682983
I0303 16:16:23.856825 140380326586112 logging_writer.py:48] [214600] global_step=214600, grad_norm=3.660745143890381, loss=3.1129276752471924
I0303 16:16:30.295596 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:16:41.059983 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:17:04.845543 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:17:06.491539 140575196817216 submission_runner.py:411] Time since start: 103402.09s, 	Step: 214616, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.419483482837677, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 95412.57948756218, 'total_duration': 103402.09250640869, 'accumulated_submission_time': 95412.57948756218, 'accumulated_eval_time': 7966.65424156189, 'accumulated_logging_time': 11.472744464874268}
I0303 16:17:06.542584 140380334978816 logging_writer.py:48] [214616] accumulated_eval_time=7966.654242, accumulated_logging_time=11.472744, accumulated_submission_time=95412.579488, global_step=214616, preemption_count=0, score=95412.579488, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=103402.092506, train/accuracy=0.888516, train/loss=0.419483, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:17:40.341579 140380326586112 logging_writer.py:48] [214700] global_step=214700, grad_norm=3.2383482456207275, loss=1.0646907091140747
I0303 16:18:24.922019 140380334978816 logging_writer.py:48] [214800] global_step=214800, grad_norm=2.8405356407165527, loss=1.0676056146621704
I0303 16:19:10.050247 140380326586112 logging_writer.py:48] [214900] global_step=214900, grad_norm=3.071969747543335, loss=1.1989070177078247
I0303 16:19:54.911387 140380334978816 logging_writer.py:48] [215000] global_step=215000, grad_norm=2.890484571456909, loss=1.0604804754257202
I0303 16:20:39.758115 140380326586112 logging_writer.py:48] [215100] global_step=215100, grad_norm=3.5160481929779053, loss=2.0113115310668945
I0303 16:21:24.733900 140380334978816 logging_writer.py:48] [215200] global_step=215200, grad_norm=4.162991046905518, loss=1.113055944442749
I0303 16:22:09.761214 140380326586112 logging_writer.py:48] [215300] global_step=215300, grad_norm=3.099247932434082, loss=1.1529353857040405
I0303 16:22:54.448714 140380334978816 logging_writer.py:48] [215400] global_step=215400, grad_norm=3.1129252910614014, loss=1.0766644477844238
I0303 16:23:39.551553 140380326586112 logging_writer.py:48] [215500] global_step=215500, grad_norm=3.3674380779266357, loss=1.127750277519226
I0303 16:24:06.579022 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:24:17.260532 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:24:40.952394 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:24:42.602482 140575196817216 submission_runner.py:411] Time since start: 103858.20s, 	Step: 215562, 	{'train/accuracy': 0.8874413967132568, 'train/loss': 0.41763144731521606, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 95832.54670524597, 'total_duration': 103858.20341420174, 'accumulated_submission_time': 95832.54670524597, 'accumulated_eval_time': 8002.676563978195, 'accumulated_logging_time': 11.544119596481323}
I0303 16:24:42.653015 140380334978816 logging_writer.py:48] [215562] accumulated_eval_time=8002.676564, accumulated_logging_time=11.544120, accumulated_submission_time=95832.546705, global_step=215562, preemption_count=0, score=95832.546705, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=103858.203414, train/accuracy=0.887441, train/loss=0.417631, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:24:58.170791 140380326586112 logging_writer.py:48] [215600] global_step=215600, grad_norm=3.02864670753479, loss=1.2278729677200317
I0303 16:25:40.491825 140380334978816 logging_writer.py:48] [215700] global_step=215700, grad_norm=3.18953537940979, loss=1.0923478603363037
I0303 16:26:25.770318 140380326586112 logging_writer.py:48] [215800] global_step=215800, grad_norm=3.119265079498291, loss=1.3869729042053223
I0303 16:27:11.291051 140380334978816 logging_writer.py:48] [215900] global_step=215900, grad_norm=2.8815131187438965, loss=2.201711893081665
I0303 16:27:56.110821 140380326586112 logging_writer.py:48] [216000] global_step=216000, grad_norm=3.0077967643737793, loss=1.0533430576324463
I0303 16:28:41.156262 140380334978816 logging_writer.py:48] [216100] global_step=216100, grad_norm=2.95560884475708, loss=1.4399442672729492
I0303 16:29:26.245380 140380326586112 logging_writer.py:48] [216200] global_step=216200, grad_norm=3.1669600009918213, loss=1.8899562358856201
I0303 16:30:11.177079 140380334978816 logging_writer.py:48] [216300] global_step=216300, grad_norm=3.057664394378662, loss=1.1239073276519775
I0303 16:30:56.187363 140380326586112 logging_writer.py:48] [216400] global_step=216400, grad_norm=3.2643320560455322, loss=1.0802381038665771
I0303 16:31:43.148843 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:31:51.863906 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:32:08.836181 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:32:10.589678 140575196817216 submission_runner.py:411] Time since start: 104306.19s, 	Step: 216482, 	{'train/accuracy': 0.8871679306030273, 'train/loss': 0.41720953583717346, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 96252.98335146904, 'total_duration': 104306.19088602066, 'accumulated_submission_time': 96252.98335146904, 'accumulated_eval_time': 8030.116549730301, 'accumulated_logging_time': 11.604514122009277}
I0303 16:32:10.694120 140380334978816 logging_writer.py:48] [216482] accumulated_eval_time=8030.116550, accumulated_logging_time=11.604514, accumulated_submission_time=96252.983351, global_step=216482, preemption_count=0, score=96252.983351, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=104306.190886, train/accuracy=0.887168, train/loss=0.417210, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:32:18.345577 140380326586112 logging_writer.py:48] [216500] global_step=216500, grad_norm=3.494612455368042, loss=1.8256255388259888
I0303 16:33:03.732341 140380334978816 logging_writer.py:48] [216600] global_step=216600, grad_norm=4.016077041625977, loss=2.668092966079712
I0303 16:33:50.367861 140380326586112 logging_writer.py:48] [216700] global_step=216700, grad_norm=3.581627368927002, loss=2.875001907348633
I0303 16:34:36.877947 140380334978816 logging_writer.py:48] [216800] global_step=216800, grad_norm=3.5656585693359375, loss=3.0635263919830322
I0303 16:35:23.421350 140380326586112 logging_writer.py:48] [216900] global_step=216900, grad_norm=3.0932412147521973, loss=1.1965036392211914
I0303 16:36:10.452893 140380334978816 logging_writer.py:48] [217000] global_step=217000, grad_norm=2.9969475269317627, loss=2.3271071910858154
I0303 16:36:57.329813 140380326586112 logging_writer.py:48] [217100] global_step=217100, grad_norm=3.3002116680145264, loss=2.5912845134735107
I0303 16:37:43.982583 140380334978816 logging_writer.py:48] [217200] global_step=217200, grad_norm=3.4353036880493164, loss=2.6284096240997314
I0303 16:38:30.566105 140380326586112 logging_writer.py:48] [217300] global_step=217300, grad_norm=3.2144322395324707, loss=2.3237547874450684
I0303 16:39:10.862198 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:39:21.268821 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:39:48.651030 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:39:50.308993 140575196817216 submission_runner.py:411] Time since start: 104765.91s, 	Step: 217388, 	{'train/accuracy': 0.8858984112739563, 'train/loss': 0.42883986234664917, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 96673.04321050644, 'total_duration': 104765.90989851952, 'accumulated_submission_time': 96673.04321050644, 'accumulated_eval_time': 8069.562195062637, 'accumulated_logging_time': 11.770461082458496}
I0303 16:39:50.371855 140380334978816 logging_writer.py:48] [217388] accumulated_eval_time=8069.562195, accumulated_logging_time=11.770461, accumulated_submission_time=96673.043211, global_step=217388, preemption_count=0, score=96673.043211, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=104765.909899, train/accuracy=0.885898, train/loss=0.428840, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:39:55.526540 140380326586112 logging_writer.py:48] [217400] global_step=217400, grad_norm=3.2692601680755615, loss=1.7784088850021362
I0303 16:40:36.002714 140380334978816 logging_writer.py:48] [217500] global_step=217500, grad_norm=2.947727918624878, loss=1.5110511779785156
I0303 16:41:20.771928 140380326586112 logging_writer.py:48] [217600] global_step=217600, grad_norm=3.3699729442596436, loss=2.8113319873809814
I0303 16:42:06.214942 140380334978816 logging_writer.py:48] [217700] global_step=217700, grad_norm=3.964466094970703, loss=1.254867434501648
I0303 16:42:51.527287 140380326586112 logging_writer.py:48] [217800] global_step=217800, grad_norm=3.258369207382202, loss=1.1696162223815918
I0303 16:43:36.262239 140380334978816 logging_writer.py:48] [217900] global_step=217900, grad_norm=3.212642192840576, loss=1.2408369779586792
I0303 16:44:21.455796 140380326586112 logging_writer.py:48] [218000] global_step=218000, grad_norm=2.9920079708099365, loss=1.181165337562561
I0303 16:45:06.236129 140380334978816 logging_writer.py:48] [218100] global_step=218100, grad_norm=3.2682430744171143, loss=1.4935358762741089
I0303 16:45:51.251943 140380326586112 logging_writer.py:48] [218200] global_step=218200, grad_norm=3.131645441055298, loss=1.2513220310211182
I0303 16:46:36.262015 140380334978816 logging_writer.py:48] [218300] global_step=218300, grad_norm=3.256014108657837, loss=2.543099880218506
I0303 16:46:50.396149 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:47:00.934566 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:47:26.042476 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:47:27.703880 140575196817216 submission_runner.py:411] Time since start: 105223.30s, 	Step: 218333, 	{'train/accuracy': 0.8865429759025574, 'train/loss': 0.42255812883377075, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 97093.00739145279, 'total_duration': 105223.30492758751, 'accumulated_submission_time': 97093.00739145279, 'accumulated_eval_time': 8106.868925571442, 'accumulated_logging_time': 11.844651222229004}
I0303 16:47:27.755754 140380326586112 logging_writer.py:48] [218333] accumulated_eval_time=8106.868926, accumulated_logging_time=11.844651, accumulated_submission_time=97093.007391, global_step=218333, preemption_count=0, score=97093.007391, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=105223.304928, train/accuracy=0.886543, train/loss=0.422558, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:47:54.778820 140380334978816 logging_writer.py:48] [218400] global_step=218400, grad_norm=2.9650278091430664, loss=1.7288405895233154
I0303 16:48:38.175368 140380326586112 logging_writer.py:48] [218500] global_step=218500, grad_norm=2.9985439777374268, loss=1.0890414714813232
I0303 16:49:23.323470 140380334978816 logging_writer.py:48] [218600] global_step=218600, grad_norm=3.5255985260009766, loss=1.1886838674545288
I0303 16:50:08.204453 140380326586112 logging_writer.py:48] [218700] global_step=218700, grad_norm=3.7857213020324707, loss=3.0989930629730225
I0303 16:50:53.169377 140380334978816 logging_writer.py:48] [218800] global_step=218800, grad_norm=2.8928956985473633, loss=1.00862455368042
I0303 16:51:38.115585 140380326586112 logging_writer.py:48] [218900] global_step=218900, grad_norm=3.934054374694824, loss=3.0624144077301025
I0303 16:52:23.062454 140380334978816 logging_writer.py:48] [219000] global_step=219000, grad_norm=2.829913854598999, loss=1.8015530109405518
I0303 16:53:07.990467 140380326586112 logging_writer.py:48] [219100] global_step=219100, grad_norm=3.372241497039795, loss=1.100932240486145
I0303 16:53:53.108717 140380334978816 logging_writer.py:48] [219200] global_step=219200, grad_norm=3.7251100540161133, loss=1.2270145416259766
I0303 16:54:28.125293 140575196817216 spec.py:321] Evaluating on the training split.
I0303 16:54:38.684554 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 16:55:04.755090 140575196817216 spec.py:349] Evaluating on the test split.
I0303 16:55:06.420790 140575196817216 submission_runner.py:411] Time since start: 105682.02s, 	Step: 219280, 	{'train/accuracy': 0.8845507502555847, 'train/loss': 0.4251365661621094, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 97513.31821966171, 'total_duration': 105682.02164626122, 'accumulated_submission_time': 97513.31821966171, 'accumulated_eval_time': 8145.163213729858, 'accumulated_logging_time': 11.905775547027588}
I0303 16:55:06.485290 140380326586112 logging_writer.py:48] [219280] accumulated_eval_time=8145.163214, accumulated_logging_time=11.905776, accumulated_submission_time=97513.318220, global_step=219280, preemption_count=0, score=97513.318220, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=105682.021646, train/accuracy=0.884551, train/loss=0.425137, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 16:55:14.841160 140380334978816 logging_writer.py:48] [219300] global_step=219300, grad_norm=3.371386766433716, loss=1.4680535793304443
I0303 16:55:55.770042 140380326586112 logging_writer.py:48] [219400] global_step=219400, grad_norm=4.013843059539795, loss=3.164903163909912
I0303 16:56:40.863477 140380334978816 logging_writer.py:48] [219500] global_step=219500, grad_norm=3.100385904312134, loss=1.4555902481079102
I0303 16:57:26.003886 140380326586112 logging_writer.py:48] [219600] global_step=219600, grad_norm=3.618478775024414, loss=3.0918211936950684
I0303 16:58:11.076162 140380334978816 logging_writer.py:48] [219700] global_step=219700, grad_norm=2.860276699066162, loss=0.9435385465621948
I0303 16:58:56.106333 140380326586112 logging_writer.py:48] [219800] global_step=219800, grad_norm=2.9494857788085938, loss=2.289478063583374
I0303 16:59:41.053332 140380334978816 logging_writer.py:48] [219900] global_step=219900, grad_norm=3.4787120819091797, loss=2.20542049407959
I0303 17:00:26.063546 140380326586112 logging_writer.py:48] [220000] global_step=220000, grad_norm=3.820939064025879, loss=2.961876153945923
I0303 17:01:11.131253 140380334978816 logging_writer.py:48] [220100] global_step=220100, grad_norm=3.6861159801483154, loss=2.798771858215332
I0303 17:01:56.310208 140380326586112 logging_writer.py:48] [220200] global_step=220200, grad_norm=3.0953686237335205, loss=1.1337661743164062
I0303 17:02:06.422642 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:02:17.195022 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:02:37.868548 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:02:39.532840 140575196817216 submission_runner.py:411] Time since start: 106135.13s, 	Step: 220224, 	{'train/accuracy': 0.8892187476158142, 'train/loss': 0.41724804043769836, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 97933.19587111473, 'total_duration': 106135.13363027573, 'accumulated_submission_time': 97933.19587111473, 'accumulated_eval_time': 8178.272148132324, 'accumulated_logging_time': 11.980994701385498}
I0303 17:02:39.590198 140380334978816 logging_writer.py:48] [220224] accumulated_eval_time=8178.272148, accumulated_logging_time=11.980995, accumulated_submission_time=97933.195871, global_step=220224, preemption_count=0, score=97933.195871, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=106135.133630, train/accuracy=0.889219, train/loss=0.417248, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:03:10.200090 140380326586112 logging_writer.py:48] [220300] global_step=220300, grad_norm=3.845315456390381, loss=1.1775039434432983
I0303 17:03:54.684672 140380334978816 logging_writer.py:48] [220400] global_step=220400, grad_norm=2.996370792388916, loss=1.188783884048462
I0303 17:04:39.799482 140380326586112 logging_writer.py:48] [220500] global_step=220500, grad_norm=3.02984881401062, loss=1.9851233959197998
I0303 17:05:24.716216 140380334978816 logging_writer.py:48] [220600] global_step=220600, grad_norm=3.698312282562256, loss=3.080263376235962
I0303 17:06:09.951869 140380326586112 logging_writer.py:48] [220700] global_step=220700, grad_norm=3.7732441425323486, loss=2.5883359909057617
I0303 17:06:55.129695 140380334978816 logging_writer.py:48] [220800] global_step=220800, grad_norm=3.191737413406372, loss=1.1891772747039795
I0303 17:07:40.113840 140380326586112 logging_writer.py:48] [220900] global_step=220900, grad_norm=3.567121982574463, loss=3.041550636291504
I0303 17:08:25.269160 140380334978816 logging_writer.py:48] [221000] global_step=221000, grad_norm=3.4651894569396973, loss=2.989624500274658
I0303 17:09:09.994571 140380326586112 logging_writer.py:48] [221100] global_step=221100, grad_norm=2.896824359893799, loss=1.1379525661468506
I0303 17:09:39.814465 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:09:50.490204 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:10:14.094018 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:10:15.742017 140575196817216 submission_runner.py:411] Time since start: 106591.34s, 	Step: 221168, 	{'train/accuracy': 0.8871093392372131, 'train/loss': 0.4200824499130249, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 98353.35867023468, 'total_duration': 106591.34285092354, 'accumulated_submission_time': 98353.35867023468, 'accumulated_eval_time': 8214.198472261429, 'accumulated_logging_time': 12.051104068756104}
I0303 17:10:15.790965 140380334978816 logging_writer.py:48] [221168] accumulated_eval_time=8214.198472, accumulated_logging_time=12.051104, accumulated_submission_time=98353.358670, global_step=221168, preemption_count=0, score=98353.358670, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=106591.342851, train/accuracy=0.887109, train/loss=0.420082, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:10:28.948414 140380326586112 logging_writer.py:48] [221200] global_step=221200, grad_norm=3.2642080783843994, loss=1.2066419124603271
I0303 17:11:10.670312 140380334978816 logging_writer.py:48] [221300] global_step=221300, grad_norm=3.0555124282836914, loss=1.1079407930374146
I0303 17:11:55.039457 140380326586112 logging_writer.py:48] [221400] global_step=221400, grad_norm=2.967099189758301, loss=2.214888334274292
I0303 17:12:40.443991 140380334978816 logging_writer.py:48] [221500] global_step=221500, grad_norm=3.317624568939209, loss=1.8653637170791626
I0303 17:13:25.430857 140380326586112 logging_writer.py:48] [221600] global_step=221600, grad_norm=3.4150893688201904, loss=1.1292484998703003
I0303 17:14:10.486500 140380334978816 logging_writer.py:48] [221700] global_step=221700, grad_norm=2.9484000205993652, loss=1.3997324705123901
I0303 17:14:55.496487 140380326586112 logging_writer.py:48] [221800] global_step=221800, grad_norm=2.847567081451416, loss=1.1637296676635742
I0303 17:15:40.341433 140380334978816 logging_writer.py:48] [221900] global_step=221900, grad_norm=3.180497169494629, loss=1.1628602743148804
I0303 17:16:25.462409 140380326586112 logging_writer.py:48] [222000] global_step=222000, grad_norm=2.97906494140625, loss=1.1370127201080322
I0303 17:17:10.566146 140380334978816 logging_writer.py:48] [222100] global_step=222100, grad_norm=4.4766645431518555, loss=3.2780723571777344
I0303 17:17:16.187841 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:17:26.665134 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:17:50.392289 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:17:52.048975 140575196817216 submission_runner.py:411] Time since start: 107047.65s, 	Step: 222114, 	{'train/accuracy': 0.8859765529632568, 'train/loss': 0.421042799949646, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 98773.69676375389, 'total_duration': 107047.64982414246, 'accumulated_submission_time': 98773.69676375389, 'accumulated_eval_time': 8250.0584192276, 'accumulated_logging_time': 12.109280109405518}
I0303 17:17:52.103398 140380326586112 logging_writer.py:48] [222114] accumulated_eval_time=8250.058419, accumulated_logging_time=12.109280, accumulated_submission_time=98773.696764, global_step=222114, preemption_count=0, score=98773.696764, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=107047.649824, train/accuracy=0.885977, train/loss=0.421043, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:18:26.865055 140380334978816 logging_writer.py:48] [222200] global_step=222200, grad_norm=3.4013500213623047, loss=1.29652738571167
I0303 17:19:11.312983 140380326586112 logging_writer.py:48] [222300] global_step=222300, grad_norm=2.9959027767181396, loss=1.6475991010665894
I0303 17:19:56.559051 140380334978816 logging_writer.py:48] [222400] global_step=222400, grad_norm=4.624861240386963, loss=2.8977041244506836
I0303 17:20:41.483382 140380326586112 logging_writer.py:48] [222500] global_step=222500, grad_norm=3.1358728408813477, loss=2.6617774963378906
I0303 17:21:26.387199 140380334978816 logging_writer.py:48] [222600] global_step=222600, grad_norm=3.2694997787475586, loss=1.0844959020614624
I0303 17:22:11.691930 140380326586112 logging_writer.py:48] [222700] global_step=222700, grad_norm=3.222168445587158, loss=1.1354398727416992
I0303 17:22:56.675855 140380334978816 logging_writer.py:48] [222800] global_step=222800, grad_norm=4.422539234161377, loss=3.141342878341675
I0303 17:23:41.488114 140380326586112 logging_writer.py:48] [222900] global_step=222900, grad_norm=3.170487880706787, loss=2.538055896759033
I0303 17:24:26.710502 140380334978816 logging_writer.py:48] [223000] global_step=223000, grad_norm=3.0128376483917236, loss=1.197244644165039
I0303 17:24:52.405308 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:25:03.055639 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:25:26.538266 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:25:28.181264 140575196817216 submission_runner.py:411] Time since start: 107503.78s, 	Step: 223059, 	{'train/accuracy': 0.8874609470367432, 'train/loss': 0.41645586490631104, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 99193.9378619194, 'total_duration': 107503.78232359886, 'accumulated_submission_time': 99193.9378619194, 'accumulated_eval_time': 8285.833389759064, 'accumulated_logging_time': 12.175068616867065}
I0303 17:25:28.234502 140380326586112 logging_writer.py:48] [223059] accumulated_eval_time=8285.833390, accumulated_logging_time=12.175069, accumulated_submission_time=99193.937862, global_step=223059, preemption_count=0, score=99193.937862, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=107503.782324, train/accuracy=0.887461, train/loss=0.416456, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:25:44.937685 140380334978816 logging_writer.py:48] [223100] global_step=223100, grad_norm=3.3307907581329346, loss=1.147451639175415
I0303 17:26:27.074928 140380326586112 logging_writer.py:48] [223200] global_step=223200, grad_norm=2.9511172771453857, loss=1.6784934997558594
I0303 17:27:12.142194 140380334978816 logging_writer.py:48] [223300] global_step=223300, grad_norm=3.042839288711548, loss=1.2526791095733643
I0303 17:27:57.093517 140380326586112 logging_writer.py:48] [223400] global_step=223400, grad_norm=3.5652878284454346, loss=1.10503089427948
I0303 17:28:41.852644 140380334978816 logging_writer.py:48] [223500] global_step=223500, grad_norm=3.677802562713623, loss=3.169827461242676
I0303 17:29:27.026085 140380326586112 logging_writer.py:48] [223600] global_step=223600, grad_norm=3.0622398853302, loss=1.0177621841430664
I0303 17:30:11.938012 140380334978816 logging_writer.py:48] [223700] global_step=223700, grad_norm=2.964423179626465, loss=1.5404472351074219
I0303 17:30:56.951621 140380326586112 logging_writer.py:48] [223800] global_step=223800, grad_norm=3.1413588523864746, loss=1.1881768703460693
I0303 17:31:41.580701 140380334978816 logging_writer.py:48] [223900] global_step=223900, grad_norm=3.193899393081665, loss=1.094937801361084
I0303 17:32:26.715549 140380326586112 logging_writer.py:48] [224000] global_step=224000, grad_norm=3.410598039627075, loss=2.6864492893218994
I0303 17:32:28.636265 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:32:39.278305 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:33:02.553285 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:33:04.208686 140575196817216 submission_runner.py:411] Time since start: 107959.81s, 	Step: 224006, 	{'train/accuracy': 0.8888476490974426, 'train/loss': 0.4210689067840576, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 99614.28044009209, 'total_duration': 107959.80958008766, 'accumulated_submission_time': 99614.28044009209, 'accumulated_eval_time': 8321.404640674591, 'accumulated_logging_time': 12.237873315811157}
I0303 17:33:04.263794 140380334978816 logging_writer.py:48] [224006] accumulated_eval_time=8321.404641, accumulated_logging_time=12.237873, accumulated_submission_time=99614.280440, global_step=224006, preemption_count=0, score=99614.280440, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=107959.809580, train/accuracy=0.888848, train/loss=0.421069, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:33:42.158756 140380326586112 logging_writer.py:48] [224100] global_step=224100, grad_norm=3.215888261795044, loss=1.9081603288650513
I0303 17:34:27.084842 140380334978816 logging_writer.py:48] [224200] global_step=224200, grad_norm=3.977592706680298, loss=3.22499942779541
I0303 17:35:12.399788 140380326586112 logging_writer.py:48] [224300] global_step=224300, grad_norm=3.036362409591675, loss=2.39623761177063
I0303 17:35:57.439352 140380334978816 logging_writer.py:48] [224400] global_step=224400, grad_norm=5.696098327636719, loss=1.2950199842453003
I0303 17:36:42.641871 140380326586112 logging_writer.py:48] [224500] global_step=224500, grad_norm=2.990757465362549, loss=1.0594645738601685
I0303 17:37:27.701634 140380334978816 logging_writer.py:48] [224600] global_step=224600, grad_norm=3.5819053649902344, loss=1.1420764923095703
I0303 17:38:12.835835 140380326586112 logging_writer.py:48] [224700] global_step=224700, grad_norm=3.5272979736328125, loss=1.1172266006469727
I0303 17:38:57.787049 140380334978816 logging_writer.py:48] [224800] global_step=224800, grad_norm=3.2538368701934814, loss=2.6233081817626953
I0303 17:39:42.619289 140380326586112 logging_writer.py:48] [224900] global_step=224900, grad_norm=3.3861405849456787, loss=1.6003766059875488
I0303 17:40:04.423413 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:40:15.005151 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:40:39.206606 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:40:40.866005 140575196817216 submission_runner.py:411] Time since start: 108416.47s, 	Step: 224950, 	{'train/accuracy': 0.8860937356948853, 'train/loss': 0.4242303669452667, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 100034.37481594086, 'total_duration': 108416.46704983711, 'accumulated_submission_time': 100034.37481594086, 'accumulated_eval_time': 8357.846234321594, 'accumulated_logging_time': 12.309117078781128}
I0303 17:40:40.918979 140380334978816 logging_writer.py:48] [224950] accumulated_eval_time=8357.846234, accumulated_logging_time=12.309117, accumulated_submission_time=100034.374816, global_step=224950, preemption_count=0, score=100034.374816, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=108416.467050, train/accuracy=0.886094, train/loss=0.424230, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:41:01.185089 140380326586112 logging_writer.py:48] [225000] global_step=225000, grad_norm=3.2686991691589355, loss=1.101668119430542
I0303 17:41:43.665998 140380334978816 logging_writer.py:48] [225100] global_step=225100, grad_norm=3.3993301391601562, loss=1.0940194129943848
I0303 17:42:28.716810 140380326586112 logging_writer.py:48] [225200] global_step=225200, grad_norm=3.1823434829711914, loss=1.1942890882492065
I0303 17:43:14.267412 140380334978816 logging_writer.py:48] [225300] global_step=225300, grad_norm=3.6112008094787598, loss=3.131870746612549
I0303 17:43:59.231473 140380326586112 logging_writer.py:48] [225400] global_step=225400, grad_norm=2.7759742736816406, loss=1.850746750831604
I0303 17:44:44.420172 140380334978816 logging_writer.py:48] [225500] global_step=225500, grad_norm=3.646155834197998, loss=1.066835641860962
I0303 17:45:29.561960 140380326586112 logging_writer.py:48] [225600] global_step=225600, grad_norm=2.994788408279419, loss=1.124415636062622
I0303 17:46:14.634110 140380334978816 logging_writer.py:48] [225700] global_step=225700, grad_norm=3.931292772293091, loss=1.0847015380859375
I0303 17:46:59.672061 140380326586112 logging_writer.py:48] [225800] global_step=225800, grad_norm=3.476724863052368, loss=2.3302669525146484
I0303 17:47:41.019780 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:47:51.591784 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:48:14.942439 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:48:16.596148 140575196817216 submission_runner.py:411] Time since start: 108872.20s, 	Step: 225894, 	{'train/accuracy': 0.8856640458106995, 'train/loss': 0.42499351501464844, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 100454.41598153114, 'total_duration': 108872.19716739655, 'accumulated_submission_time': 100454.41598153114, 'accumulated_eval_time': 8393.42155122757, 'accumulated_logging_time': 12.37180781364441}
I0303 17:48:16.650114 140380334978816 logging_writer.py:48] [225894] accumulated_eval_time=8393.421551, accumulated_logging_time=12.371808, accumulated_submission_time=100454.415982, global_step=225894, preemption_count=0, score=100454.415982, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=108872.197167, train/accuracy=0.885664, train/loss=0.424994, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:48:19.440596 140380326586112 logging_writer.py:48] [225900] global_step=225900, grad_norm=3.4094996452331543, loss=1.0771912336349487
I0303 17:48:59.948781 140380334978816 logging_writer.py:48] [226000] global_step=226000, grad_norm=3.0784595012664795, loss=1.0674245357513428
I0303 17:49:44.852558 140380326586112 logging_writer.py:48] [226100] global_step=226100, grad_norm=2.849393606185913, loss=1.0511972904205322
I0303 17:50:30.197208 140380334978816 logging_writer.py:48] [226200] global_step=226200, grad_norm=2.9491961002349854, loss=1.6549160480499268
I0303 17:51:15.216197 140380326586112 logging_writer.py:48] [226300] global_step=226300, grad_norm=3.137916326522827, loss=1.140335202217102
I0303 17:52:00.023326 140380334978816 logging_writer.py:48] [226400] global_step=226400, grad_norm=3.5219738483428955, loss=1.1589086055755615
I0303 17:52:45.442255 140380326586112 logging_writer.py:48] [226500] global_step=226500, grad_norm=2.9303979873657227, loss=2.1000030040740967
I0303 17:53:30.242703 140380334978816 logging_writer.py:48] [226600] global_step=226600, grad_norm=3.197018623352051, loss=1.504023551940918
I0303 17:54:15.295808 140380326586112 logging_writer.py:48] [226700] global_step=226700, grad_norm=3.039046049118042, loss=1.8543100357055664
I0303 17:55:00.291052 140380334978816 logging_writer.py:48] [226800] global_step=226800, grad_norm=2.848534345626831, loss=1.1483709812164307
I0303 17:55:16.651854 140575196817216 spec.py:321] Evaluating on the training split.
I0303 17:55:27.343531 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 17:55:52.452822 140575196817216 spec.py:349] Evaluating on the test split.
I0303 17:55:54.109709 140575196817216 submission_runner.py:411] Time since start: 109329.71s, 	Step: 226838, 	{'train/accuracy': 0.8877148032188416, 'train/loss': 0.41879454255104065, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 100874.35620498657, 'total_duration': 109329.71062541008, 'accumulated_submission_time': 100874.35620498657, 'accumulated_eval_time': 8430.878259420395, 'accumulated_logging_time': 12.438353300094604}
I0303 17:55:54.162291 140380326586112 logging_writer.py:48] [226838] accumulated_eval_time=8430.878259, accumulated_logging_time=12.438353, accumulated_submission_time=100874.356205, global_step=226838, preemption_count=0, score=100874.356205, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=109329.710625, train/accuracy=0.887715, train/loss=0.418795, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 17:56:19.212063 140380334978816 logging_writer.py:48] [226900] global_step=226900, grad_norm=3.4615468978881836, loss=2.2111973762512207
I0303 17:57:02.811609 140380326586112 logging_writer.py:48] [227000] global_step=227000, grad_norm=3.280810832977295, loss=1.2182276248931885
I0303 17:57:48.097291 140380334978816 logging_writer.py:48] [227100] global_step=227100, grad_norm=3.0859451293945312, loss=2.400547742843628
I0303 17:58:33.215740 140380326586112 logging_writer.py:48] [227200] global_step=227200, grad_norm=3.22456693649292, loss=1.1130247116088867
I0303 17:59:18.247747 140380334978816 logging_writer.py:48] [227300] global_step=227300, grad_norm=3.247777223587036, loss=1.1411858797073364
I0303 18:00:03.372987 140380326586112 logging_writer.py:48] [227400] global_step=227400, grad_norm=3.187793731689453, loss=2.592822551727295
I0303 18:00:48.351788 140380334978816 logging_writer.py:48] [227500] global_step=227500, grad_norm=3.4294910430908203, loss=1.152379035949707
I0303 18:01:33.213862 140380326586112 logging_writer.py:48] [227600] global_step=227600, grad_norm=2.917447805404663, loss=1.032299518585205
I0303 18:02:18.083094 140380334978816 logging_writer.py:48] [227700] global_step=227700, grad_norm=3.055617332458496, loss=1.4447568655014038
I0303 18:02:54.190256 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:03:05.161964 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:03:30.538792 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:03:32.192699 140575196817216 submission_runner.py:411] Time since start: 109787.79s, 	Step: 227782, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.4204618036746979, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 101294.32449269295, 'total_duration': 109787.79367494583, 'accumulated_submission_time': 101294.32449269295, 'accumulated_eval_time': 8468.879625320435, 'accumulated_logging_time': 12.502143383026123}
I0303 18:03:32.247107 140380326586112 logging_writer.py:48] [227782] accumulated_eval_time=8468.879625, accumulated_logging_time=12.502143, accumulated_submission_time=101294.324493, global_step=227782, preemption_count=0, score=101294.324493, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=109787.793675, train/accuracy=0.887227, train/loss=0.420462, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:03:39.801631 140380334978816 logging_writer.py:48] [227800] global_step=227800, grad_norm=4.4741058349609375, loss=3.23563551902771
I0303 18:04:21.198652 140380326586112 logging_writer.py:48] [227900] global_step=227900, grad_norm=3.8177168369293213, loss=2.5738492012023926
I0303 18:05:06.305594 140380334978816 logging_writer.py:48] [228000] global_step=228000, grad_norm=3.3471500873565674, loss=2.154924154281616
I0303 18:05:51.662494 140380326586112 logging_writer.py:48] [228100] global_step=228100, grad_norm=3.4454309940338135, loss=2.8990981578826904
I0303 18:06:37.003841 140380334978816 logging_writer.py:48] [228200] global_step=228200, grad_norm=3.1280078887939453, loss=1.2912198305130005
I0303 18:07:22.127110 140380326586112 logging_writer.py:48] [228300] global_step=228300, grad_norm=3.234959840774536, loss=2.690131187438965
I0303 18:08:07.187457 140380334978816 logging_writer.py:48] [228400] global_step=228400, grad_norm=2.975968837738037, loss=1.119239091873169
I0303 18:08:52.363448 140380326586112 logging_writer.py:48] [228500] global_step=228500, grad_norm=3.219527244567871, loss=1.9411606788635254
I0303 18:09:37.422916 140380334978816 logging_writer.py:48] [228600] global_step=228600, grad_norm=3.203737735748291, loss=1.0868507623672485
I0303 18:10:22.508070 140380326586112 logging_writer.py:48] [228700] global_step=228700, grad_norm=3.174764633178711, loss=2.5358996391296387
I0303 18:10:32.578942 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:10:43.413923 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:11:09.684735 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:11:11.352376 140575196817216 submission_runner.py:411] Time since start: 110246.95s, 	Step: 228723, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.4156016409397125, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 101714.59797477722, 'total_duration': 110246.95320534706, 'accumulated_submission_time': 101714.59797477722, 'accumulated_eval_time': 8507.651833295822, 'accumulated_logging_time': 12.566620111465454}
I0303 18:11:11.402714 140380334978816 logging_writer.py:48] [228723] accumulated_eval_time=8507.651833, accumulated_logging_time=12.566620, accumulated_submission_time=101714.597975, global_step=228723, preemption_count=0, score=101714.597975, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=110246.953205, train/accuracy=0.887559, train/loss=0.415602, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:11:42.401223 140380326586112 logging_writer.py:48] [228800] global_step=228800, grad_norm=2.923790216445923, loss=1.0979537963867188
I0303 18:12:26.426295 140380334978816 logging_writer.py:48] [228900] global_step=228900, grad_norm=3.415371894836426, loss=2.0021731853485107
I0303 18:13:11.879006 140380326586112 logging_writer.py:48] [229000] global_step=229000, grad_norm=3.9916839599609375, loss=3.192657470703125
I0303 18:13:57.612163 140380334978816 logging_writer.py:48] [229100] global_step=229100, grad_norm=3.803133726119995, loss=3.112250328063965
I0303 18:14:42.882340 140380326586112 logging_writer.py:48] [229200] global_step=229200, grad_norm=3.011253833770752, loss=1.078893780708313
I0303 18:15:28.366652 140380334978816 logging_writer.py:48] [229300] global_step=229300, grad_norm=3.103076219558716, loss=2.1909337043762207
I0303 18:16:14.281815 140380326586112 logging_writer.py:48] [229400] global_step=229400, grad_norm=3.192852735519409, loss=1.2960646152496338
I0303 18:16:59.580528 140380334978816 logging_writer.py:48] [229500] global_step=229500, grad_norm=2.965439558029175, loss=1.113696575164795
I0303 18:17:44.710547 140380326586112 logging_writer.py:48] [229600] global_step=229600, grad_norm=2.928168773651123, loss=1.6832938194274902
I0303 18:18:11.516135 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:18:22.225497 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:18:47.639328 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:18:49.289324 140575196817216 submission_runner.py:411] Time since start: 110704.89s, 	Step: 229661, 	{'train/accuracy': 0.8867773413658142, 'train/loss': 0.4198257625102997, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 102134.6535089016, 'total_duration': 110704.89049220085, 'accumulated_submission_time': 102134.6535089016, 'accumulated_eval_time': 8545.424136638641, 'accumulated_logging_time': 12.626487016677856}
I0303 18:18:49.339357 140380334978816 logging_writer.py:48] [229661] accumulated_eval_time=8545.424137, accumulated_logging_time=12.626487, accumulated_submission_time=102134.653509, global_step=229661, preemption_count=0, score=102134.653509, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=110704.890492, train/accuracy=0.886777, train/loss=0.419826, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:19:05.237619 140380326586112 logging_writer.py:48] [229700] global_step=229700, grad_norm=3.0228474140167236, loss=1.1366523504257202
I0303 18:19:47.462339 140380334978816 logging_writer.py:48] [229800] global_step=229800, grad_norm=2.9758613109588623, loss=1.0848151445388794
I0303 18:20:32.681204 140380326586112 logging_writer.py:48] [229900] global_step=229900, grad_norm=3.1645703315734863, loss=1.3576838970184326
I0303 18:21:18.221096 140380334978816 logging_writer.py:48] [230000] global_step=230000, grad_norm=3.5512795448303223, loss=1.1693830490112305
I0303 18:22:03.254956 140380326586112 logging_writer.py:48] [230100] global_step=230100, grad_norm=3.0751893520355225, loss=1.0704517364501953
I0303 18:22:48.450932 140380334978816 logging_writer.py:48] [230200] global_step=230200, grad_norm=3.534503698348999, loss=1.13619863986969
I0303 18:23:33.539814 140380326586112 logging_writer.py:48] [230300] global_step=230300, grad_norm=3.319159507751465, loss=3.034635543823242
I0303 18:24:18.280249 140380334978816 logging_writer.py:48] [230400] global_step=230400, grad_norm=3.544018030166626, loss=1.7916200160980225
I0303 18:25:03.343583 140380326586112 logging_writer.py:48] [230500] global_step=230500, grad_norm=6.264343738555908, loss=1.0534034967422485
I0303 18:25:48.383894 140380334978816 logging_writer.py:48] [230600] global_step=230600, grad_norm=3.3784878253936768, loss=1.1399362087249756
I0303 18:25:49.433334 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:26:00.300953 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:26:27.173589 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:26:28.825855 140575196817216 submission_runner.py:411] Time since start: 111164.43s, 	Step: 230604, 	{'train/accuracy': 0.8856054544448853, 'train/loss': 0.4232231080532074, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 102554.6896674633, 'total_duration': 111164.42679929733, 'accumulated_submission_time': 102554.6896674633, 'accumulated_eval_time': 8584.81552362442, 'accumulated_logging_time': 12.685871601104736}
I0303 18:26:28.879883 140380326586112 logging_writer.py:48] [230604] accumulated_eval_time=8584.815524, accumulated_logging_time=12.685872, accumulated_submission_time=102554.689667, global_step=230604, preemption_count=0, score=102554.689667, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=111164.426799, train/accuracy=0.885605, train/loss=0.423223, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:27:07.900559 140380334978816 logging_writer.py:48] [230700] global_step=230700, grad_norm=3.258718252182007, loss=1.4536024332046509
I0303 18:27:52.990700 140380326586112 logging_writer.py:48] [230800] global_step=230800, grad_norm=3.479264974594116, loss=1.202091932296753
I0303 18:28:38.323192 140380334978816 logging_writer.py:48] [230900] global_step=230900, grad_norm=3.3563835620880127, loss=1.1256663799285889
I0303 18:29:23.128983 140380326586112 logging_writer.py:48] [231000] global_step=231000, grad_norm=3.4639220237731934, loss=1.511238932609558
I0303 18:30:08.243094 140380334978816 logging_writer.py:48] [231100] global_step=231100, grad_norm=3.619762897491455, loss=3.005878210067749
I0303 18:30:53.406384 140380326586112 logging_writer.py:48] [231200] global_step=231200, grad_norm=4.22895622253418, loss=3.2253594398498535
I0303 18:31:38.579822 140380334978816 logging_writer.py:48] [231300] global_step=231300, grad_norm=3.2476911544799805, loss=1.206301212310791
I0303 18:32:23.298367 140380326586112 logging_writer.py:48] [231400] global_step=231400, grad_norm=2.936833620071411, loss=1.0634982585906982
I0303 18:33:08.660160 140380334978816 logging_writer.py:48] [231500] global_step=231500, grad_norm=3.4416615962982178, loss=1.0571885108947754
I0303 18:33:28.907937 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:33:39.639991 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:34:03.037844 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:34:04.690167 140575196817216 submission_runner.py:411] Time since start: 111620.29s, 	Step: 231547, 	{'train/accuracy': 0.8893945217132568, 'train/loss': 0.41441580653190613, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 102974.65440821648, 'total_duration': 111620.29118657112, 'accumulated_submission_time': 102974.65440821648, 'accumulated_eval_time': 8620.59669971466, 'accumulated_logging_time': 12.754069089889526}
I0303 18:34:04.740650 140380326586112 logging_writer.py:48] [231547] accumulated_eval_time=8620.596700, accumulated_logging_time=12.754069, accumulated_submission_time=102974.654408, global_step=231547, preemption_count=0, score=102974.654408, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=111620.291187, train/accuracy=0.889395, train/loss=0.414416, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:34:26.203455 140380334978816 logging_writer.py:48] [231600] global_step=231600, grad_norm=4.000918388366699, loss=1.7824751138687134
I0303 18:35:09.082183 140380326586112 logging_writer.py:48] [231700] global_step=231700, grad_norm=3.404005289077759, loss=2.0324366092681885
I0303 18:35:54.270527 140380334978816 logging_writer.py:48] [231800] global_step=231800, grad_norm=3.1856298446655273, loss=1.6138112545013428
I0303 18:36:39.689579 140380326586112 logging_writer.py:48] [231900] global_step=231900, grad_norm=3.0270919799804688, loss=1.0496618747711182
I0303 18:37:24.872285 140380334978816 logging_writer.py:48] [232000] global_step=232000, grad_norm=3.0568912029266357, loss=1.1323591470718384
I0303 18:38:09.901717 140380326586112 logging_writer.py:48] [232100] global_step=232100, grad_norm=3.294797897338867, loss=1.168302297592163
I0303 18:38:54.899685 140380334978816 logging_writer.py:48] [232200] global_step=232200, grad_norm=3.4026598930358887, loss=1.0974364280700684
I0303 18:39:39.872842 140380326586112 logging_writer.py:48] [232300] global_step=232300, grad_norm=3.4055817127227783, loss=1.2155753374099731
I0303 18:40:24.830660 140380334978816 logging_writer.py:48] [232400] global_step=232400, grad_norm=3.577526330947876, loss=1.2732288837432861
I0303 18:41:04.782895 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:41:15.467742 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:41:41.242084 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:41:42.895856 140575196817216 submission_runner.py:411] Time since start: 112078.50s, 	Step: 232490, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.41773721575737, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 103394.6379442215, 'total_duration': 112078.49687552452, 'accumulated_submission_time': 103394.6379442215, 'accumulated_eval_time': 8658.708618879318, 'accumulated_logging_time': 12.814483880996704}
I0303 18:41:42.952739 140380326586112 logging_writer.py:48] [232490] accumulated_eval_time=8658.708619, accumulated_logging_time=12.814484, accumulated_submission_time=103394.637944, global_step=232490, preemption_count=0, score=103394.637944, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=112078.496876, train/accuracy=0.888633, train/loss=0.417737, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:41:47.321509 140380334978816 logging_writer.py:48] [232500] global_step=232500, grad_norm=3.3006770610809326, loss=2.4915781021118164
I0303 18:42:27.900173 140380326586112 logging_writer.py:48] [232600] global_step=232600, grad_norm=2.9338183403015137, loss=1.084803819656372
I0303 18:43:12.627914 140380334978816 logging_writer.py:48] [232700] global_step=232700, grad_norm=3.639470100402832, loss=2.0519180297851562
I0303 18:43:57.985517 140380326586112 logging_writer.py:48] [232800] global_step=232800, grad_norm=4.867812156677246, loss=3.2041192054748535
I0303 18:44:43.047021 140380334978816 logging_writer.py:48] [232900] global_step=232900, grad_norm=3.0166356563568115, loss=2.2292134761810303
I0303 18:45:28.117892 140380326586112 logging_writer.py:48] [233000] global_step=233000, grad_norm=3.1059811115264893, loss=1.107608675956726
I0303 18:46:13.332563 140380334978816 logging_writer.py:48] [233100] global_step=233100, grad_norm=3.4629132747650146, loss=1.2331123352050781
I0303 18:46:58.597289 140380326586112 logging_writer.py:48] [233200] global_step=233200, grad_norm=3.2819206714630127, loss=1.180593729019165
I0303 18:47:43.897689 140380334978816 logging_writer.py:48] [233300] global_step=233300, grad_norm=3.3830559253692627, loss=1.4119967222213745
I0303 18:48:29.270503 140380326586112 logging_writer.py:48] [233400] global_step=233400, grad_norm=3.4165492057800293, loss=2.798219680786133
I0303 18:48:42.952457 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:48:53.808505 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:49:15.160378 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:49:16.810125 140575196817216 submission_runner.py:411] Time since start: 112532.41s, 	Step: 233432, 	{'train/accuracy': 0.8906640410423279, 'train/loss': 0.40751761198043823, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 103814.5766415596, 'total_duration': 112532.41099596024, 'accumulated_submission_time': 103814.5766415596, 'accumulated_eval_time': 8692.565080881119, 'accumulated_logging_time': 12.884013175964355}
I0303 18:49:16.862590 140380334978816 logging_writer.py:48] [233432] accumulated_eval_time=8692.565081, accumulated_logging_time=12.884013, accumulated_submission_time=103814.576642, global_step=233432, preemption_count=0, score=103814.576642, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=112532.410996, train/accuracy=0.890664, train/loss=0.407518, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:49:44.308599 140380326586112 logging_writer.py:48] [233500] global_step=233500, grad_norm=3.025723934173584, loss=1.1047652959823608
I0303 18:50:28.095506 140380334978816 logging_writer.py:48] [233600] global_step=233600, grad_norm=3.332284688949585, loss=1.1772730350494385
I0303 18:51:13.192860 140380326586112 logging_writer.py:48] [233700] global_step=233700, grad_norm=3.429086685180664, loss=1.1676945686340332
I0303 18:51:58.190268 140380334978816 logging_writer.py:48] [233800] global_step=233800, grad_norm=3.7393081188201904, loss=3.1572580337524414
I0303 18:52:43.007105 140380326586112 logging_writer.py:48] [233900] global_step=233900, grad_norm=3.0309183597564697, loss=2.037895441055298
I0303 18:53:28.531142 140380334978816 logging_writer.py:48] [234000] global_step=234000, grad_norm=2.9339003562927246, loss=1.1967097520828247
I0303 18:54:13.797072 140380326586112 logging_writer.py:48] [234100] global_step=234100, grad_norm=3.080277442932129, loss=1.0672158002853394
I0303 18:54:58.589371 140380334978816 logging_writer.py:48] [234200] global_step=234200, grad_norm=3.1690804958343506, loss=1.286149263381958
I0303 18:55:44.127911 140380326586112 logging_writer.py:48] [234300] global_step=234300, grad_norm=3.1956546306610107, loss=2.2196011543273926
I0303 18:56:17.215767 140575196817216 spec.py:321] Evaluating on the training split.
I0303 18:56:27.937637 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 18:56:51.871011 140575196817216 spec.py:349] Evaluating on the test split.
I0303 18:56:53.523266 140575196817216 submission_runner.py:411] Time since start: 112989.12s, 	Step: 234375, 	{'train/accuracy': 0.8890429735183716, 'train/loss': 0.4111610949039459, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 104234.8699195385, 'total_duration': 112989.12427139282, 'accumulated_submission_time': 104234.8699195385, 'accumulated_eval_time': 8728.871542215347, 'accumulated_logging_time': 12.946634531021118}
I0303 18:56:53.574208 140380334978816 logging_writer.py:48] [234375] accumulated_eval_time=8728.871542, accumulated_logging_time=12.946635, accumulated_submission_time=104234.869920, global_step=234375, preemption_count=0, score=104234.869920, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=112989.124271, train/accuracy=0.889043, train/loss=0.411161, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 18:57:03.911680 140380326586112 logging_writer.py:48] [234400] global_step=234400, grad_norm=3.194831132888794, loss=2.39705228805542
I0303 18:57:45.333206 140380334978816 logging_writer.py:48] [234500] global_step=234500, grad_norm=4.02433967590332, loss=3.2167985439300537
I0303 18:58:30.427666 140380326586112 logging_writer.py:48] [234600] global_step=234600, grad_norm=3.5856494903564453, loss=1.154290795326233
I0303 18:59:15.964565 140380334978816 logging_writer.py:48] [234700] global_step=234700, grad_norm=3.146416187286377, loss=1.0787590742111206
I0303 19:00:01.018054 140380326586112 logging_writer.py:48] [234800] global_step=234800, grad_norm=2.9277429580688477, loss=1.224971890449524
I0303 19:00:46.135684 140380334978816 logging_writer.py:48] [234900] global_step=234900, grad_norm=3.3983781337738037, loss=1.0797032117843628
I0303 19:01:30.851805 140380326586112 logging_writer.py:48] [235000] global_step=235000, grad_norm=3.2548460960388184, loss=1.2021749019622803
I0303 19:02:15.983498 140380334978816 logging_writer.py:48] [235100] global_step=235100, grad_norm=3.8305764198303223, loss=3.131150960922241
I0303 19:03:01.265942 140380326586112 logging_writer.py:48] [235200] global_step=235200, grad_norm=3.4842352867126465, loss=1.925931453704834
I0303 19:03:45.984807 140380334978816 logging_writer.py:48] [235300] global_step=235300, grad_norm=3.025106906890869, loss=1.5499815940856934
I0303 19:03:53.798745 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:04:04.536003 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:04:29.430799 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:04:31.083279 140575196817216 submission_runner.py:411] Time since start: 113446.68s, 	Step: 235319, 	{'train/accuracy': 0.8880859017372131, 'train/loss': 0.42078834772109985, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 104655.03619599342, 'total_duration': 113446.68429636955, 'accumulated_submission_time': 104655.03619599342, 'accumulated_eval_time': 8766.155029296875, 'accumulated_logging_time': 13.006984949111938}
I0303 19:04:31.138345 140380326586112 logging_writer.py:48] [235319] accumulated_eval_time=8766.155029, accumulated_logging_time=13.006985, accumulated_submission_time=104655.036196, global_step=235319, preemption_count=0, score=104655.036196, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=113446.684296, train/accuracy=0.888086, train/loss=0.420788, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:05:03.784260 140380334978816 logging_writer.py:48] [235400] global_step=235400, grad_norm=3.09100604057312, loss=1.887308955192566
I0303 19:05:48.033029 140380326586112 logging_writer.py:48] [235500] global_step=235500, grad_norm=2.8876330852508545, loss=1.4555479288101196
I0303 19:06:33.505597 140380334978816 logging_writer.py:48] [235600] global_step=235600, grad_norm=3.2497615814208984, loss=1.8201262950897217
I0303 19:07:18.635200 140380326586112 logging_writer.py:48] [235700] global_step=235700, grad_norm=3.1336910724639893, loss=1.2559270858764648
I0303 19:08:04.006181 140380334978816 logging_writer.py:48] [235800] global_step=235800, grad_norm=3.373018980026245, loss=1.4017595052719116
I0303 19:08:49.106222 140380326586112 logging_writer.py:48] [235900] global_step=235900, grad_norm=3.295452356338501, loss=1.173043966293335
I0303 19:09:33.963570 140380334978816 logging_writer.py:48] [236000] global_step=236000, grad_norm=3.186692714691162, loss=1.9019291400909424
I0303 19:10:19.055320 140380326586112 logging_writer.py:48] [236100] global_step=236100, grad_norm=3.0582387447357178, loss=1.0921827554702759
I0303 19:11:04.266225 140380334978816 logging_writer.py:48] [236200] global_step=236200, grad_norm=3.1612558364868164, loss=1.3495368957519531
I0303 19:11:31.507950 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:11:42.266604 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:12:06.223378 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:12:07.885192 140575196817216 submission_runner.py:411] Time since start: 113903.49s, 	Step: 236262, 	{'train/accuracy': 0.88671875, 'train/loss': 0.42182213068008423, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 105075.3451757431, 'total_duration': 113903.48621582985, 'accumulated_submission_time': 105075.3451757431, 'accumulated_eval_time': 8802.531280994415, 'accumulated_logging_time': 13.073152780532837}
I0303 19:12:07.948665 140380326586112 logging_writer.py:48] [236262] accumulated_eval_time=8802.531281, accumulated_logging_time=13.073153, accumulated_submission_time=105075.345176, global_step=236262, preemption_count=0, score=105075.345176, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=113903.486216, train/accuracy=0.886719, train/loss=0.421822, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:12:23.473921 140380334978816 logging_writer.py:48] [236300] global_step=236300, grad_norm=4.257244110107422, loss=2.855881929397583
I0303 19:13:05.221482 140380326586112 logging_writer.py:48] [236400] global_step=236400, grad_norm=2.941809892654419, loss=1.1311933994293213
I0303 19:13:50.076868 140380334978816 logging_writer.py:48] [236500] global_step=236500, grad_norm=4.272576332092285, loss=3.107354164123535
I0303 19:14:35.041889 140380326586112 logging_writer.py:48] [236600] global_step=236600, grad_norm=3.2969064712524414, loss=2.7190678119659424
I0303 19:15:20.049510 140380334978816 logging_writer.py:48] [236700] global_step=236700, grad_norm=4.159008502960205, loss=3.127197265625
I0303 19:16:05.091315 140380326586112 logging_writer.py:48] [236800] global_step=236800, grad_norm=3.0602598190307617, loss=1.1505482196807861
I0303 19:16:50.286962 140380334978816 logging_writer.py:48] [236900] global_step=236900, grad_norm=3.2164580821990967, loss=1.1584254503250122
I0303 19:17:35.253502 140380326586112 logging_writer.py:48] [237000] global_step=237000, grad_norm=3.1156961917877197, loss=1.4822766780853271
I0303 19:18:20.212144 140380334978816 logging_writer.py:48] [237100] global_step=237100, grad_norm=2.9214892387390137, loss=1.0364952087402344
I0303 19:19:05.106226 140380326586112 logging_writer.py:48] [237200] global_step=237200, grad_norm=3.08394718170166, loss=1.0717270374298096
I0303 19:19:07.934898 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:19:18.714084 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:19:42.533189 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:19:44.180179 140575196817216 submission_runner.py:411] Time since start: 114359.78s, 	Step: 237207, 	{'train/accuracy': 0.8869531154632568, 'train/loss': 0.41949278116226196, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 105495.2727303505, 'total_duration': 114359.78117632866, 'accumulated_submission_time': 105495.2727303505, 'accumulated_eval_time': 8838.775473117828, 'accumulated_logging_time': 13.146722555160522}
I0303 19:19:44.236543 140380334978816 logging_writer.py:48] [237207] accumulated_eval_time=8838.775473, accumulated_logging_time=13.146723, accumulated_submission_time=105495.272730, global_step=237207, preemption_count=0, score=105495.272730, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=114359.781176, train/accuracy=0.886953, train/loss=0.419493, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:20:21.813360 140380326586112 logging_writer.py:48] [237300] global_step=237300, grad_norm=3.615595817565918, loss=2.7277846336364746
I0303 19:21:06.484519 140380334978816 logging_writer.py:48] [237400] global_step=237400, grad_norm=3.0355656147003174, loss=2.005347967147827
I0303 19:21:51.587412 140380326586112 logging_writer.py:48] [237500] global_step=237500, grad_norm=3.3379690647125244, loss=2.744352340698242
I0303 19:22:36.539496 140380334978816 logging_writer.py:48] [237600] global_step=237600, grad_norm=3.343567132949829, loss=1.4019311666488647
I0303 19:23:21.538636 140380326586112 logging_writer.py:48] [237700] global_step=237700, grad_norm=3.275815963745117, loss=1.1060549020767212
I0303 19:24:07.216880 140380334978816 logging_writer.py:48] [237800] global_step=237800, grad_norm=3.0432703495025635, loss=1.1399121284484863
I0303 19:24:52.110429 140380326586112 logging_writer.py:48] [237900] global_step=237900, grad_norm=3.0157554149627686, loss=1.372675895690918
I0303 19:25:37.184871 140380334978816 logging_writer.py:48] [238000] global_step=238000, grad_norm=3.036583423614502, loss=1.0349725484848022
I0303 19:26:22.752255 140380326586112 logging_writer.py:48] [238100] global_step=238100, grad_norm=2.896157741546631, loss=1.592505693435669
I0303 19:26:44.446513 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:26:55.138411 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:27:17.508644 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:27:19.154155 140575196817216 submission_runner.py:411] Time since start: 114814.76s, 	Step: 238150, 	{'train/accuracy': 0.8891406059265137, 'train/loss': 0.4173803925514221, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 105915.4233739376, 'total_duration': 114814.75517678261, 'accumulated_submission_time': 105915.4233739376, 'accumulated_eval_time': 8873.482074022293, 'accumulated_logging_time': 13.212722063064575}
I0303 19:27:19.205032 140380334978816 logging_writer.py:48] [238150] accumulated_eval_time=8873.482074, accumulated_logging_time=13.212722, accumulated_submission_time=105915.423374, global_step=238150, preemption_count=0, score=105915.423374, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=114814.755177, train/accuracy=0.889141, train/loss=0.417380, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:27:39.479923 140380326586112 logging_writer.py:48] [238200] global_step=238200, grad_norm=2.9019718170166016, loss=1.2722930908203125
I0303 19:28:22.447963 140380334978816 logging_writer.py:48] [238300] global_step=238300, grad_norm=5.762229919433594, loss=2.982821226119995
I0303 19:29:07.529556 140380326586112 logging_writer.py:48] [238400] global_step=238400, grad_norm=3.303226947784424, loss=2.158837080001831
I0303 19:29:52.381951 140380334978816 logging_writer.py:48] [238500] global_step=238500, grad_norm=3.1109869480133057, loss=1.6748913526535034
I0303 19:30:37.465422 140380326586112 logging_writer.py:48] [238600] global_step=238600, grad_norm=3.255986213684082, loss=1.7141245603561401
I0303 19:31:22.499043 140380334978816 logging_writer.py:48] [238700] global_step=238700, grad_norm=2.9266042709350586, loss=2.475593328475952
I0303 19:32:07.774741 140380326586112 logging_writer.py:48] [238800] global_step=238800, grad_norm=3.246000051498413, loss=1.1067487001419067
I0303 19:32:52.642112 140380334978816 logging_writer.py:48] [238900] global_step=238900, grad_norm=5.036044597625732, loss=3.1942527294158936
I0303 19:33:38.082053 140380326586112 logging_writer.py:48] [239000] global_step=239000, grad_norm=3.278165578842163, loss=2.7025163173675537
I0303 19:34:19.396822 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:34:30.049168 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:34:53.493323 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:34:55.154366 140575196817216 submission_runner.py:411] Time since start: 115270.76s, 	Step: 239094, 	{'train/accuracy': 0.8861132860183716, 'train/loss': 0.42082512378692627, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 106335.55676198006, 'total_duration': 115270.7553293705, 'accumulated_submission_time': 106335.55676198006, 'accumulated_eval_time': 8909.238534212112, 'accumulated_logging_time': 13.272995471954346}
I0303 19:34:55.209854 140380334978816 logging_writer.py:48] [239094] accumulated_eval_time=8909.238534, accumulated_logging_time=13.272995, accumulated_submission_time=106335.556762, global_step=239094, preemption_count=0, score=106335.556762, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=115270.755329, train/accuracy=0.886113, train/loss=0.420825, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:34:57.999780 140380326586112 logging_writer.py:48] [239100] global_step=239100, grad_norm=3.4683144092559814, loss=2.7455880641937256
I0303 19:35:38.340515 140380334978816 logging_writer.py:48] [239200] global_step=239200, grad_norm=3.392688035964966, loss=2.9554800987243652
I0303 19:36:23.204853 140380326586112 logging_writer.py:48] [239300] global_step=239300, grad_norm=3.141777753829956, loss=2.481199026107788
I0303 19:37:08.254421 140380334978816 logging_writer.py:48] [239400] global_step=239400, grad_norm=3.4471757411956787, loss=1.1972699165344238
I0303 19:37:52.978340 140380326586112 logging_writer.py:48] [239500] global_step=239500, grad_norm=3.1548330783843994, loss=1.3992892503738403
I0303 19:38:38.111212 140380334978816 logging_writer.py:48] [239600] global_step=239600, grad_norm=2.9292449951171875, loss=1.5294673442840576
I0303 19:39:22.899868 140380326586112 logging_writer.py:48] [239700] global_step=239700, grad_norm=3.0367743968963623, loss=1.1769150495529175
I0303 19:40:07.693914 140380334978816 logging_writer.py:48] [239800] global_step=239800, grad_norm=3.2556955814361572, loss=1.1962233781814575
I0303 19:40:52.628179 140380326586112 logging_writer.py:48] [239900] global_step=239900, grad_norm=3.3443851470947266, loss=1.7305206060409546
I0303 19:41:37.772257 140380334978816 logging_writer.py:48] [240000] global_step=240000, grad_norm=3.1957366466522217, loss=1.1308966875076294
I0303 19:41:55.360765 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:42:06.001226 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:42:30.282524 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:42:31.930241 140575196817216 submission_runner.py:411] Time since start: 115727.53s, 	Step: 240041, 	{'train/accuracy': 0.8859374523162842, 'train/loss': 0.42550522089004517, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 106755.64887499809, 'total_duration': 115727.53117990494, 'accumulated_submission_time': 106755.64887499809, 'accumulated_eval_time': 8945.806883335114, 'accumulated_logging_time': 13.338412046432495}
I0303 19:42:31.986494 140380326586112 logging_writer.py:48] [240041] accumulated_eval_time=8945.806883, accumulated_logging_time=13.338412, accumulated_submission_time=106755.648875, global_step=240041, preemption_count=0, score=106755.648875, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=115727.531180, train/accuracy=0.885937, train/loss=0.425505, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:42:55.824379 140380334978816 logging_writer.py:48] [240100] global_step=240100, grad_norm=3.3798141479492188, loss=1.1502128839492798
I0303 19:43:38.756377 140380326586112 logging_writer.py:48] [240200] global_step=240200, grad_norm=3.3325233459472656, loss=2.4576025009155273
I0303 19:44:24.050366 140380334978816 logging_writer.py:48] [240300] global_step=240300, grad_norm=3.2654495239257812, loss=1.0828778743743896
I0303 19:45:09.186162 140380326586112 logging_writer.py:48] [240400] global_step=240400, grad_norm=3.383549690246582, loss=1.1704301834106445
I0303 19:45:54.430415 140380334978816 logging_writer.py:48] [240500] global_step=240500, grad_norm=4.3608479499816895, loss=2.9868242740631104
I0303 19:46:39.900830 140380326586112 logging_writer.py:48] [240600] global_step=240600, grad_norm=3.094219207763672, loss=1.106071949005127
I0303 19:47:24.767016 140380334978816 logging_writer.py:48] [240700] global_step=240700, grad_norm=3.822449207305908, loss=3.194495916366577
I0303 19:48:10.015077 140380326586112 logging_writer.py:48] [240800] global_step=240800, grad_norm=3.286276340484619, loss=1.1389473676681519
I0303 19:48:55.301443 140380334978816 logging_writer.py:48] [240900] global_step=240900, grad_norm=3.0537047386169434, loss=1.6799516677856445
I0303 19:49:32.231283 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:49:42.803176 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:50:06.948043 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:50:08.605287 140575196817216 submission_runner.py:411] Time since start: 116184.21s, 	Step: 240984, 	{'train/accuracy': 0.88685542345047, 'train/loss': 0.42310622334480286, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 107175.83440184593, 'total_duration': 116184.20617771149, 'accumulated_submission_time': 107175.83440184593, 'accumulated_eval_time': 8982.179713249207, 'accumulated_logging_time': 13.405362844467163}
I0303 19:50:08.660632 140380326586112 logging_writer.py:48] [240984] accumulated_eval_time=8982.179713, accumulated_logging_time=13.405363, accumulated_submission_time=107175.834402, global_step=240984, preemption_count=0, score=107175.834402, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=116184.206178, train/accuracy=0.886855, train/loss=0.423106, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:50:15.421975 140380334978816 logging_writer.py:48] [241000] global_step=241000, grad_norm=3.0309135913848877, loss=1.4083847999572754
I0303 19:50:56.333907 140380326586112 logging_writer.py:48] [241100] global_step=241100, grad_norm=3.9778032302856445, loss=3.0265610218048096
I0303 19:51:41.437813 140380334978816 logging_writer.py:48] [241200] global_step=241200, grad_norm=3.311798095703125, loss=1.4649637937545776
I0303 19:52:27.072867 140380326586112 logging_writer.py:48] [241300] global_step=241300, grad_norm=3.016185760498047, loss=1.1082189083099365
I0303 19:53:11.772372 140380334978816 logging_writer.py:48] [241400] global_step=241400, grad_norm=2.8822453022003174, loss=1.2468371391296387
I0303 19:53:56.853416 140380326586112 logging_writer.py:48] [241500] global_step=241500, grad_norm=3.074309825897217, loss=1.126061201095581
I0303 19:54:41.946045 140380334978816 logging_writer.py:48] [241600] global_step=241600, grad_norm=3.2342145442962646, loss=2.7394795417785645
I0303 19:55:27.074683 140380326586112 logging_writer.py:48] [241700] global_step=241700, grad_norm=3.3035120964050293, loss=1.3106027841567993
I0303 19:56:12.229207 140380334978816 logging_writer.py:48] [241800] global_step=241800, grad_norm=3.0992414951324463, loss=1.175893783569336
I0303 19:56:57.250711 140380326586112 logging_writer.py:48] [241900] global_step=241900, grad_norm=3.364284038543701, loss=1.4624311923980713
I0303 19:57:08.730719 140575196817216 spec.py:321] Evaluating on the training split.
I0303 19:57:19.446200 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 19:57:43.810618 140575196817216 spec.py:349] Evaluating on the test split.
I0303 19:57:45.468572 140575196817216 submission_runner.py:411] Time since start: 116641.07s, 	Step: 241927, 	{'train/accuracy': 0.8875195384025574, 'train/loss': 0.41778334975242615, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 107595.84573626518, 'total_duration': 116641.06905651093, 'accumulated_submission_time': 107595.84573626518, 'accumulated_eval_time': 9018.915987968445, 'accumulated_logging_time': 13.470390558242798}
I0303 19:57:45.532823 140380334978816 logging_writer.py:48] [241927] accumulated_eval_time=9018.915988, accumulated_logging_time=13.470391, accumulated_submission_time=107595.845736, global_step=241927, preemption_count=0, score=107595.845736, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=116641.069057, train/accuracy=0.887520, train/loss=0.417783, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 19:58:14.987269 140380326586112 logging_writer.py:48] [242000] global_step=242000, grad_norm=2.8669991493225098, loss=1.2223496437072754
I0303 19:58:58.928348 140380334978816 logging_writer.py:48] [242100] global_step=242100, grad_norm=3.4655730724334717, loss=1.2436339855194092
I0303 19:59:44.004793 140380326586112 logging_writer.py:48] [242200] global_step=242200, grad_norm=3.2077736854553223, loss=2.620464563369751
I0303 20:00:28.909916 140380334978816 logging_writer.py:48] [242300] global_step=242300, grad_norm=2.9840340614318848, loss=2.173123359680176
I0303 20:01:13.845701 140380326586112 logging_writer.py:48] [242400] global_step=242400, grad_norm=3.0263476371765137, loss=1.8084073066711426
I0303 20:01:58.726021 140380334978816 logging_writer.py:48] [242500] global_step=242500, grad_norm=3.02728009223938, loss=1.1792701482772827
I0303 20:02:43.551247 140380326586112 logging_writer.py:48] [242600] global_step=242600, grad_norm=3.053055763244629, loss=2.0788564682006836
I0303 20:03:28.224701 140380334978816 logging_writer.py:48] [242700] global_step=242700, grad_norm=3.1186513900756836, loss=1.8362094163894653
I0303 20:04:13.658083 140380326586112 logging_writer.py:48] [242800] global_step=242800, grad_norm=3.27410626411438, loss=1.1660435199737549
I0303 20:04:45.762376 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:04:56.476516 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:05:19.378116 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:05:21.032271 140575196817216 submission_runner.py:411] Time since start: 117096.63s, 	Step: 242873, 	{'train/accuracy': 0.8873632550239563, 'train/loss': 0.4211326837539673, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 108016.01560282707, 'total_duration': 117096.63333463669, 'accumulated_submission_time': 108016.01560282707, 'accumulated_eval_time': 9054.184888839722, 'accumulated_logging_time': 13.544833660125732}
I0303 20:05:21.088567 140380334978816 logging_writer.py:48] [242873] accumulated_eval_time=9054.184889, accumulated_logging_time=13.544834, accumulated_submission_time=108016.015603, global_step=242873, preemption_count=0, score=108016.015603, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=117096.633335, train/accuracy=0.887363, train/loss=0.421133, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:05:32.216674 140380326586112 logging_writer.py:48] [242900] global_step=242900, grad_norm=3.058385133743286, loss=2.300168991088867
I0303 20:06:13.899261 140380334978816 logging_writer.py:48] [243000] global_step=243000, grad_norm=3.2644481658935547, loss=2.045304536819458
I0303 20:06:58.920642 140380326586112 logging_writer.py:48] [243100] global_step=243100, grad_norm=3.2616536617279053, loss=1.7585623264312744
I0303 20:07:44.020669 140380334978816 logging_writer.py:48] [243200] global_step=243200, grad_norm=2.9430694580078125, loss=1.1931853294372559
I0303 20:08:29.036702 140380326586112 logging_writer.py:48] [243300] global_step=243300, grad_norm=3.2577531337738037, loss=1.0672168731689453
I0303 20:09:13.937884 140380334978816 logging_writer.py:48] [243400] global_step=243400, grad_norm=3.073046922683716, loss=1.0223318338394165
I0303 20:09:58.829653 140380326586112 logging_writer.py:48] [243500] global_step=243500, grad_norm=3.590715169906616, loss=1.3958487510681152
I0303 20:10:43.814774 140380334978816 logging_writer.py:48] [243600] global_step=243600, grad_norm=3.3299612998962402, loss=1.7615423202514648
I0303 20:11:29.809127 140380326586112 logging_writer.py:48] [243700] global_step=243700, grad_norm=3.1649537086486816, loss=1.183807611465454
I0303 20:12:14.921579 140380334978816 logging_writer.py:48] [243800] global_step=243800, grad_norm=2.9619851112365723, loss=1.5621004104614258
I0303 20:12:21.349235 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:12:32.244654 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:12:57.638449 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:12:59.289669 140575196817216 submission_runner.py:411] Time since start: 117554.89s, 	Step: 243816, 	{'train/accuracy': 0.8863866925239563, 'train/loss': 0.4213360548019409, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 108436.21713781357, 'total_duration': 117554.89075517654, 'accumulated_submission_time': 108436.21713781357, 'accumulated_eval_time': 9092.124347448349, 'accumulated_logging_time': 13.611082077026367}
I0303 20:12:59.347292 140380326586112 logging_writer.py:48] [243816] accumulated_eval_time=9092.124347, accumulated_logging_time=13.611082, accumulated_submission_time=108436.217138, global_step=243816, preemption_count=0, score=108436.217138, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=117554.890755, train/accuracy=0.886387, train/loss=0.421336, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:13:33.125612 140380334978816 logging_writer.py:48] [243900] global_step=243900, grad_norm=3.466841697692871, loss=2.1603574752807617
I0303 20:14:17.609880 140380326586112 logging_writer.py:48] [244000] global_step=244000, grad_norm=3.1128122806549072, loss=1.6284087896347046
I0303 20:15:02.669058 140380334978816 logging_writer.py:48] [244100] global_step=244100, grad_norm=3.031723976135254, loss=1.2005581855773926
I0303 20:15:48.033390 140380326586112 logging_writer.py:48] [244200] global_step=244200, grad_norm=3.1564857959747314, loss=1.17902672290802
I0303 20:16:33.152920 140380334978816 logging_writer.py:48] [244300] global_step=244300, grad_norm=3.069913864135742, loss=1.3016942739486694
I0303 20:17:18.209623 140380326586112 logging_writer.py:48] [244400] global_step=244400, grad_norm=3.4281013011932373, loss=1.2982081174850464
I0303 20:18:03.159731 140380334978816 logging_writer.py:48] [244500] global_step=244500, grad_norm=3.3214797973632812, loss=2.466139316558838
I0303 20:18:48.082212 140380326586112 logging_writer.py:48] [244600] global_step=244600, grad_norm=2.9669501781463623, loss=1.4918296337127686
I0303 20:19:33.061070 140380334978816 logging_writer.py:48] [244700] global_step=244700, grad_norm=3.2598345279693604, loss=2.6489593982696533
I0303 20:19:59.309485 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:20:10.218112 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:20:36.329844 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:20:37.986451 140575196817216 submission_runner.py:411] Time since start: 118013.59s, 	Step: 244760, 	{'train/accuracy': 0.8868163824081421, 'train/loss': 0.42089614272117615, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 108856.1207022667, 'total_duration': 118013.58737802505, 'accumulated_submission_time': 108856.1207022667, 'accumulated_eval_time': 9130.800177574158, 'accumulated_logging_time': 13.678319692611694}
I0303 20:20:38.045177 140380326586112 logging_writer.py:48] [244760] accumulated_eval_time=9130.800178, accumulated_logging_time=13.678320, accumulated_submission_time=108856.120702, global_step=244760, preemption_count=0, score=108856.120702, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=118013.587378, train/accuracy=0.886816, train/loss=0.420896, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:20:54.349900 140380334978816 logging_writer.py:48] [244800] global_step=244800, grad_norm=2.95910382270813, loss=1.1375703811645508
I0303 20:21:36.543505 140380326586112 logging_writer.py:48] [244900] global_step=244900, grad_norm=3.2387566566467285, loss=1.2373908758163452
I0303 20:22:21.646576 140380334978816 logging_writer.py:48] [245000] global_step=245000, grad_norm=3.202547550201416, loss=1.168925404548645
I0303 20:23:06.861404 140380326586112 logging_writer.py:48] [245100] global_step=245100, grad_norm=3.8327207565307617, loss=3.1493687629699707
I0303 20:23:51.681558 140380334978816 logging_writer.py:48] [245200] global_step=245200, grad_norm=3.521559476852417, loss=2.9419748783111572
I0303 20:24:37.094542 140380326586112 logging_writer.py:48] [245300] global_step=245300, grad_norm=3.541897773742676, loss=2.9001717567443848
I0303 20:25:22.228355 140380334978816 logging_writer.py:48] [245400] global_step=245400, grad_norm=3.076148271560669, loss=1.05613112449646
I0303 20:26:07.015687 140380326586112 logging_writer.py:48] [245500] global_step=245500, grad_norm=2.965471029281616, loss=1.572513461112976
I0303 20:26:52.104813 140380334978816 logging_writer.py:48] [245600] global_step=245600, grad_norm=3.0608160495758057, loss=1.096254587173462
I0303 20:27:37.066328 140380326586112 logging_writer.py:48] [245700] global_step=245700, grad_norm=4.155783653259277, loss=3.0737357139587402
I0303 20:27:38.042654 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:27:49.163576 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:28:12.281890 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:28:13.931748 140575196817216 submission_runner.py:411] Time since start: 118469.53s, 	Step: 245704, 	{'train/accuracy': 0.8855273127555847, 'train/loss': 0.4221659004688263, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 109276.05662918091, 'total_duration': 118469.53283405304, 'accumulated_submission_time': 109276.05662918091, 'accumulated_eval_time': 9166.688288450241, 'accumulated_logging_time': 13.749892473220825}
I0303 20:28:13.983295 140380334978816 logging_writer.py:48] [245704] accumulated_eval_time=9166.688288, accumulated_logging_time=13.749892, accumulated_submission_time=109276.056629, global_step=245704, preemption_count=0, score=109276.056629, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=118469.532834, train/accuracy=0.885527, train/loss=0.422166, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:28:52.565211 140380326586112 logging_writer.py:48] [245800] global_step=245800, grad_norm=2.895745277404785, loss=1.7173166275024414
I0303 20:29:37.518992 140380334978816 logging_writer.py:48] [245900] global_step=245900, grad_norm=3.1254990100860596, loss=2.098844289779663
I0303 20:30:23.314651 140380326586112 logging_writer.py:48] [246000] global_step=246000, grad_norm=3.5621747970581055, loss=2.937624454498291
I0303 20:31:08.281951 140380334978816 logging_writer.py:48] [246100] global_step=246100, grad_norm=3.4233291149139404, loss=2.977693796157837
I0303 20:31:53.461249 140380326586112 logging_writer.py:48] [246200] global_step=246200, grad_norm=2.995307207107544, loss=2.271801710128784
I0303 20:32:38.437758 140380334978816 logging_writer.py:48] [246300] global_step=246300, grad_norm=3.3325512409210205, loss=1.7516939640045166
I0303 20:33:23.117072 140380326586112 logging_writer.py:48] [246400] global_step=246400, grad_norm=3.08557391166687, loss=1.0955779552459717
I0303 20:34:08.306262 140380334978816 logging_writer.py:48] [246500] global_step=246500, grad_norm=4.83944034576416, loss=3.1996090412139893
I0303 20:34:53.125115 140380326586112 logging_writer.py:48] [246600] global_step=246600, grad_norm=3.402557611465454, loss=1.3575650453567505
I0303 20:35:14.347770 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:35:25.109295 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:35:47.705387 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:35:49.364382 140575196817216 submission_runner.py:411] Time since start: 118924.97s, 	Step: 246648, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.41467592120170593, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 109696.36248373985, 'total_duration': 118924.96544003487, 'accumulated_submission_time': 109696.36248373985, 'accumulated_eval_time': 9201.703897237778, 'accumulated_logging_time': 13.811155080795288}
I0303 20:35:49.419609 140380334978816 logging_writer.py:48] [246648] accumulated_eval_time=9201.703897, accumulated_logging_time=13.811155, accumulated_submission_time=109696.362484, global_step=246648, preemption_count=0, score=109696.362484, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=118924.965440, train/accuracy=0.888105, train/loss=0.414676, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:36:10.474358 140380326586112 logging_writer.py:48] [246700] global_step=246700, grad_norm=3.4321224689483643, loss=2.742354393005371
I0303 20:36:53.461612 140380334978816 logging_writer.py:48] [246800] global_step=246800, grad_norm=2.920363426208496, loss=1.3266627788543701
I0303 20:37:38.282456 140380326586112 logging_writer.py:48] [246900] global_step=246900, grad_norm=3.0812342166900635, loss=1.6138606071472168
I0303 20:38:23.332260 140380334978816 logging_writer.py:48] [247000] global_step=247000, grad_norm=3.7398693561553955, loss=2.176844596862793
I0303 20:39:08.189682 140380326586112 logging_writer.py:48] [247100] global_step=247100, grad_norm=3.17111873626709, loss=2.4006309509277344
I0303 20:39:53.265131 140380334978816 logging_writer.py:48] [247200] global_step=247200, grad_norm=3.3830268383026123, loss=1.2482448816299438
I0303 20:40:38.246028 140380326586112 logging_writer.py:48] [247300] global_step=247300, grad_norm=3.3945822715759277, loss=1.5494129657745361
I0303 20:41:23.117062 140380334978816 logging_writer.py:48] [247400] global_step=247400, grad_norm=3.2468619346618652, loss=2.2334465980529785
I0303 20:42:08.065716 140380326586112 logging_writer.py:48] [247500] global_step=247500, grad_norm=3.1964426040649414, loss=1.5692847967147827
I0303 20:42:49.567620 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:43:00.217332 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:43:24.943894 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:43:26.606687 140575196817216 submission_runner.py:411] Time since start: 119382.21s, 	Step: 247594, 	{'train/accuracy': 0.8883398175239563, 'train/loss': 0.4226095378398895, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 110116.45049238205, 'total_duration': 119382.20756483078, 'accumulated_submission_time': 110116.45049238205, 'accumulated_eval_time': 9238.741772651672, 'accumulated_logging_time': 13.876649856567383}
I0303 20:43:26.668029 140380334978816 logging_writer.py:48] [247594] accumulated_eval_time=9238.741773, accumulated_logging_time=13.876650, accumulated_submission_time=110116.450492, global_step=247594, preemption_count=0, score=110116.450492, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=119382.207565, train/accuracy=0.888340, train/loss=0.422610, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:43:29.461347 140380326586112 logging_writer.py:48] [247600] global_step=247600, grad_norm=2.931265354156494, loss=1.1835684776306152
I0303 20:44:09.874659 140380334978816 logging_writer.py:48] [247700] global_step=247700, grad_norm=2.9775829315185547, loss=1.4176691770553589
I0303 20:44:54.946031 140380326586112 logging_writer.py:48] [247800] global_step=247800, grad_norm=3.410987138748169, loss=2.8523499965667725
I0303 20:45:40.256682 140380334978816 logging_writer.py:48] [247900] global_step=247900, grad_norm=3.0056464672088623, loss=1.0902560949325562
I0303 20:46:25.442089 140380326586112 logging_writer.py:48] [248000] global_step=248000, grad_norm=3.445950508117676, loss=1.1528544425964355
I0303 20:47:10.429410 140380334978816 logging_writer.py:48] [248100] global_step=248100, grad_norm=2.962251663208008, loss=1.2795147895812988
I0303 20:47:55.208815 140380326586112 logging_writer.py:48] [248200] global_step=248200, grad_norm=3.032409191131592, loss=1.0517706871032715
I0303 20:48:40.223590 140380334978816 logging_writer.py:48] [248300] global_step=248300, grad_norm=3.248692035675049, loss=1.4467864036560059
I0303 20:49:25.232211 140380326586112 logging_writer.py:48] [248400] global_step=248400, grad_norm=3.1077191829681396, loss=1.0949394702911377
I0303 20:50:10.246159 140380334978816 logging_writer.py:48] [248500] global_step=248500, grad_norm=4.135268688201904, loss=1.148955225944519
I0303 20:50:27.057356 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:50:37.890697 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:51:02.829179 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:51:04.472377 140575196817216 submission_runner.py:411] Time since start: 119840.07s, 	Step: 248539, 	{'train/accuracy': 0.8856640458106995, 'train/loss': 0.4264017343521118, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 110536.77914857864, 'total_duration': 119840.07347083092, 'accumulated_submission_time': 110536.77914857864, 'accumulated_eval_time': 9276.15583205223, 'accumulated_logging_time': 13.948917627334595}
I0303 20:51:04.528435 140380326586112 logging_writer.py:48] [248539] accumulated_eval_time=9276.155832, accumulated_logging_time=13.948918, accumulated_submission_time=110536.779149, global_step=248539, preemption_count=0, score=110536.779149, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=119840.073471, train/accuracy=0.885664, train/loss=0.426402, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:51:29.168491 140380334978816 logging_writer.py:48] [248600] global_step=248600, grad_norm=3.429154396057129, loss=1.3068914413452148
I0303 20:52:12.434479 140380326586112 logging_writer.py:48] [248700] global_step=248700, grad_norm=3.0161921977996826, loss=1.3297033309936523
I0303 20:52:57.545277 140380334978816 logging_writer.py:48] [248800] global_step=248800, grad_norm=3.714717149734497, loss=2.953843832015991
I0303 20:53:42.794807 140380326586112 logging_writer.py:48] [248900] global_step=248900, grad_norm=3.187901258468628, loss=1.1615455150604248
I0303 20:54:27.989634 140380334978816 logging_writer.py:48] [249000] global_step=249000, grad_norm=3.6756153106689453, loss=1.171595811843872
I0303 20:55:13.185821 140380326586112 logging_writer.py:48] [249100] global_step=249100, grad_norm=3.894190549850464, loss=3.094817876815796
I0303 20:55:58.030418 140380334978816 logging_writer.py:48] [249200] global_step=249200, grad_norm=3.239225387573242, loss=1.2151820659637451
I0303 20:56:43.301677 140380326586112 logging_writer.py:48] [249300] global_step=249300, grad_norm=3.8225150108337402, loss=1.4436472654342651
I0303 20:57:28.576483 140380334978816 logging_writer.py:48] [249400] global_step=249400, grad_norm=3.50567364692688, loss=1.1804652214050293
I0303 20:58:04.641173 140575196817216 spec.py:321] Evaluating on the training split.
I0303 20:58:15.877111 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 20:58:42.697207 140575196817216 spec.py:349] Evaluating on the test split.
I0303 20:58:44.339692 140575196817216 submission_runner.py:411] Time since start: 120299.94s, 	Step: 249482, 	{'train/accuracy': 0.8876757621765137, 'train/loss': 0.41839712858200073, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 110956.83072423935, 'total_duration': 120299.9406888485, 'accumulated_submission_time': 110956.83072423935, 'accumulated_eval_time': 9315.853289604187, 'accumulated_logging_time': 14.01637315750122}
I0303 20:58:44.396554 140380326586112 logging_writer.py:48] [249482] accumulated_eval_time=9315.853290, accumulated_logging_time=14.016373, accumulated_submission_time=110956.830724, global_step=249482, preemption_count=0, score=110956.830724, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=120299.940689, train/accuracy=0.887676, train/loss=0.418397, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 20:58:51.955660 140380334978816 logging_writer.py:48] [249500] global_step=249500, grad_norm=2.91660737991333, loss=1.8940702676773071
I0303 20:59:33.190887 140380326586112 logging_writer.py:48] [249600] global_step=249600, grad_norm=3.6852097511291504, loss=2.8996758460998535
I0303 21:00:18.194020 140380334978816 logging_writer.py:48] [249700] global_step=249700, grad_norm=4.383242130279541, loss=3.244354009628296
I0303 21:01:03.314283 140380326586112 logging_writer.py:48] [249800] global_step=249800, grad_norm=3.26719069480896, loss=1.1775236129760742
I0303 21:01:48.050842 140380334978816 logging_writer.py:48] [249900] global_step=249900, grad_norm=3.258254289627075, loss=2.549051523208618
I0303 21:02:33.439691 140380326586112 logging_writer.py:48] [250000] global_step=250000, grad_norm=3.2972445487976074, loss=2.788393974304199
I0303 21:03:18.393140 140380334978816 logging_writer.py:48] [250100] global_step=250100, grad_norm=2.943377733230591, loss=1.1922736167907715
I0303 21:04:03.033232 140380326586112 logging_writer.py:48] [250200] global_step=250200, grad_norm=3.076796293258667, loss=1.2393460273742676
I0303 21:04:48.496080 140380334978816 logging_writer.py:48] [250300] global_step=250300, grad_norm=3.0295498371124268, loss=1.467453956604004
I0303 21:05:33.609045 140380326586112 logging_writer.py:48] [250400] global_step=250400, grad_norm=3.2333626747131348, loss=2.1520607471466064
I0303 21:05:44.556498 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:05:55.334853 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:06:18.838370 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:06:20.487742 140575196817216 submission_runner.py:411] Time since start: 120756.09s, 	Step: 250426, 	{'train/accuracy': 0.8869531154632568, 'train/loss': 0.4247783422470093, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 111376.93041610718, 'total_duration': 120756.08883309364, 'accumulated_submission_time': 111376.93041610718, 'accumulated_eval_time': 9351.783564567566, 'accumulated_logging_time': 14.083440780639648}
I0303 21:06:20.541631 140380334978816 logging_writer.py:48] [250426] accumulated_eval_time=9351.783565, accumulated_logging_time=14.083441, accumulated_submission_time=111376.930416, global_step=250426, preemption_count=0, score=111376.930416, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=120756.088833, train/accuracy=0.886953, train/loss=0.424778, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:06:50.350725 140380326586112 logging_writer.py:48] [250500] global_step=250500, grad_norm=2.9518206119537354, loss=1.2215317487716675
I0303 21:07:34.487310 140380334978816 logging_writer.py:48] [250600] global_step=250600, grad_norm=3.110548496246338, loss=2.674382448196411
I0303 21:08:19.587829 140380326586112 logging_writer.py:48] [250700] global_step=250700, grad_norm=3.133235454559326, loss=1.1568069458007812
I0303 21:09:04.418557 140380334978816 logging_writer.py:48] [250800] global_step=250800, grad_norm=2.8615822792053223, loss=1.6150788068771362
I0303 21:09:49.243576 140380326586112 logging_writer.py:48] [250900] global_step=250900, grad_norm=3.2745025157928467, loss=1.2495794296264648
I0303 21:10:34.436868 140380334978816 logging_writer.py:48] [251000] global_step=251000, grad_norm=2.8626949787139893, loss=1.6953461170196533
I0303 21:11:19.514587 140380326586112 logging_writer.py:48] [251100] global_step=251100, grad_norm=3.052905797958374, loss=1.765575885772705
I0303 21:12:04.683348 140380334978816 logging_writer.py:48] [251200] global_step=251200, grad_norm=3.463425397872925, loss=2.920095443725586
I0303 21:12:49.861390 140380326586112 logging_writer.py:48] [251300] global_step=251300, grad_norm=3.268298625946045, loss=1.11665940284729
I0303 21:13:20.522902 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:13:31.045696 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:13:56.564263 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:13:58.221630 140575196817216 submission_runner.py:411] Time since start: 121213.82s, 	Step: 251370, 	{'train/accuracy': 0.8869140148162842, 'train/loss': 0.4152418375015259, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 111796.84997391701, 'total_duration': 121213.82272744179, 'accumulated_submission_time': 111796.84997391701, 'accumulated_eval_time': 9389.481314659119, 'accumulated_logging_time': 14.150177240371704}
I0303 21:13:58.284538 140380334978816 logging_writer.py:48] [251370] accumulated_eval_time=9389.481315, accumulated_logging_time=14.150177, accumulated_submission_time=111796.849974, global_step=251370, preemption_count=0, score=111796.849974, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=121213.822727, train/accuracy=0.886914, train/loss=0.415242, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:14:10.606942 140380326586112 logging_writer.py:48] [251400] global_step=251400, grad_norm=3.0751636028289795, loss=1.1015533208847046
I0303 21:14:52.747753 140380334978816 logging_writer.py:48] [251500] global_step=251500, grad_norm=4.174627304077148, loss=2.8785200119018555
I0303 21:15:37.752006 140380326586112 logging_writer.py:48] [251600] global_step=251600, grad_norm=3.278958559036255, loss=1.5024563074111938
I0303 21:16:23.124348 140380334978816 logging_writer.py:48] [251700] global_step=251700, grad_norm=2.7873716354370117, loss=1.8908677101135254
I0303 21:17:08.039838 140380326586112 logging_writer.py:48] [251800] global_step=251800, grad_norm=4.0941925048828125, loss=3.18515944480896
I0303 21:17:52.906345 140380334978816 logging_writer.py:48] [251900] global_step=251900, grad_norm=2.9689693450927734, loss=1.8362929821014404
I0303 21:18:37.924979 140380326586112 logging_writer.py:48] [252000] global_step=252000, grad_norm=3.9425175189971924, loss=1.3662786483764648
I0303 21:19:22.851683 140380334978816 logging_writer.py:48] [252100] global_step=252100, grad_norm=2.9300150871276855, loss=1.1793968677520752
I0303 21:20:07.831373 140380326586112 logging_writer.py:48] [252200] global_step=252200, grad_norm=3.089299440383911, loss=1.1115187406539917
I0303 21:20:52.444820 140380334978816 logging_writer.py:48] [252300] global_step=252300, grad_norm=3.652757167816162, loss=1.945663332939148
I0303 21:20:58.531694 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:21:09.352621 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:21:35.027812 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:21:36.688741 140575196817216 submission_runner.py:411] Time since start: 121672.29s, 	Step: 252315, 	{'train/accuracy': 0.8876757621765137, 'train/loss': 0.4186638295650482, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 112217.03732776642, 'total_duration': 121672.28964018822, 'accumulated_submission_time': 112217.03732776642, 'accumulated_eval_time': 9427.63720202446, 'accumulated_logging_time': 14.223804235458374}
I0303 21:21:36.748393 140380326586112 logging_writer.py:48] [252315] accumulated_eval_time=9427.637202, accumulated_logging_time=14.223804, accumulated_submission_time=112217.037328, global_step=252315, preemption_count=0, score=112217.037328, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=121672.289640, train/accuracy=0.887676, train/loss=0.418664, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:22:10.917473 140380334978816 logging_writer.py:48] [252400] global_step=252400, grad_norm=3.686687707901001, loss=2.7046830654144287
I0303 21:22:55.392096 140380326586112 logging_writer.py:48] [252500] global_step=252500, grad_norm=3.5821218490600586, loss=2.656996250152588
I0303 21:23:40.680542 140380334978816 logging_writer.py:48] [252600] global_step=252600, grad_norm=3.3803622722625732, loss=2.283504009246826
I0303 21:24:26.025232 140380326586112 logging_writer.py:48] [252700] global_step=252700, grad_norm=3.176906108856201, loss=1.86661696434021
I0303 21:25:11.185636 140380334978816 logging_writer.py:48] [252800] global_step=252800, grad_norm=3.4214513301849365, loss=2.865339756011963
I0303 21:25:56.013390 140380326586112 logging_writer.py:48] [252900] global_step=252900, grad_norm=3.2230284214019775, loss=1.7798893451690674
I0303 21:26:41.195672 140380334978816 logging_writer.py:48] [253000] global_step=253000, grad_norm=3.5647943019866943, loss=2.9226598739624023
I0303 21:27:26.109234 140380326586112 logging_writer.py:48] [253100] global_step=253100, grad_norm=4.685165882110596, loss=3.2369818687438965
I0303 21:28:11.161386 140380334978816 logging_writer.py:48] [253200] global_step=253200, grad_norm=3.2382500171661377, loss=1.1452844142913818
I0303 21:28:37.062060 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:28:47.717712 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:29:10.306061 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:29:11.964707 140575196817216 submission_runner.py:411] Time since start: 122127.57s, 	Step: 253259, 	{'train/accuracy': 0.88623046875, 'train/loss': 0.4246796667575836, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 112637.29222202301, 'total_duration': 122127.56577944756, 'accumulated_submission_time': 112637.29222202301, 'accumulated_eval_time': 9462.538872718811, 'accumulated_logging_time': 14.292896032333374}
I0303 21:29:12.018037 140380326586112 logging_writer.py:48] [253259] accumulated_eval_time=9462.538873, accumulated_logging_time=14.292896, accumulated_submission_time=112637.292222, global_step=253259, preemption_count=0, score=112637.292222, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=122127.565779, train/accuracy=0.886230, train/loss=0.424680, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:29:28.716859 140380334978816 logging_writer.py:48] [253300] global_step=253300, grad_norm=3.2895660400390625, loss=1.1440181732177734
I0303 21:30:10.776011 140380326586112 logging_writer.py:48] [253400] global_step=253400, grad_norm=3.3021116256713867, loss=1.1215509176254272
I0303 21:30:55.494230 140380334978816 logging_writer.py:48] [253500] global_step=253500, grad_norm=3.22908091545105, loss=2.542966365814209
I0303 21:31:40.806324 140380326586112 logging_writer.py:48] [253600] global_step=253600, grad_norm=2.953115463256836, loss=1.8682914972305298
I0303 21:32:25.992290 140380334978816 logging_writer.py:48] [253700] global_step=253700, grad_norm=3.2609827518463135, loss=2.132943630218506
I0303 21:33:10.973538 140380326586112 logging_writer.py:48] [253800] global_step=253800, grad_norm=3.5410263538360596, loss=1.0724021196365356
I0303 21:33:55.780595 140380334978816 logging_writer.py:48] [253900] global_step=253900, grad_norm=3.7191035747528076, loss=1.0518836975097656
I0303 21:34:41.036976 140380326586112 logging_writer.py:48] [254000] global_step=254000, grad_norm=3.1391355991363525, loss=1.2102668285369873
I0303 21:35:25.867399 140380334978816 logging_writer.py:48] [254100] global_step=254100, grad_norm=4.082332611083984, loss=1.9762883186340332
I0303 21:36:10.910383 140380326586112 logging_writer.py:48] [254200] global_step=254200, grad_norm=3.3473896980285645, loss=1.1011559963226318
I0303 21:36:12.400134 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:36:23.679588 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:36:48.511136 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:36:50.153841 140575196817216 submission_runner.py:411] Time since start: 122585.75s, 	Step: 254205, 	{'train/accuracy': 0.8882226347923279, 'train/loss': 0.414854496717453, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 113057.61286640167, 'total_duration': 122585.75490498543, 'accumulated_submission_time': 113057.61286640167, 'accumulated_eval_time': 9500.291576862335, 'accumulated_logging_time': 14.358292579650879}
I0303 21:36:50.210421 140380334978816 logging_writer.py:48] [254205] accumulated_eval_time=9500.291577, accumulated_logging_time=14.358293, accumulated_submission_time=113057.612866, global_step=254205, preemption_count=0, score=113057.612866, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=122585.754905, train/accuracy=0.888223, train/loss=0.414854, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:37:28.548480 140380326586112 logging_writer.py:48] [254300] global_step=254300, grad_norm=3.0672616958618164, loss=1.2334413528442383
I0303 21:38:13.240165 140380334978816 logging_writer.py:48] [254400] global_step=254400, grad_norm=3.249737501144409, loss=1.86951744556427
I0303 21:38:58.366857 140380326586112 logging_writer.py:48] [254500] global_step=254500, grad_norm=2.8205809593200684, loss=1.0485527515411377
I0303 21:39:43.280229 140380334978816 logging_writer.py:48] [254600] global_step=254600, grad_norm=3.27687931060791, loss=1.1409509181976318
I0303 21:40:28.248728 140380326586112 logging_writer.py:48] [254700] global_step=254700, grad_norm=3.4085659980773926, loss=2.585447311401367
I0303 21:41:13.259081 140380334978816 logging_writer.py:48] [254800] global_step=254800, grad_norm=3.2738194465637207, loss=1.145171880722046
I0303 21:41:58.024245 140380326586112 logging_writer.py:48] [254900] global_step=254900, grad_norm=3.5248866081237793, loss=3.0527403354644775
I0303 21:42:43.156647 140380334978816 logging_writer.py:48] [255000] global_step=255000, grad_norm=3.054847002029419, loss=1.0291624069213867
I0303 21:43:28.035979 140380326586112 logging_writer.py:48] [255100] global_step=255100, grad_norm=3.1395297050476074, loss=2.3452181816101074
I0303 21:43:50.426096 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:44:01.180702 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:44:27.019921 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:44:28.673327 140575196817216 submission_runner.py:411] Time since start: 123044.27s, 	Step: 255152, 	{'train/accuracy': 0.8879101276397705, 'train/loss': 0.4163959324359894, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 113477.76876091957, 'total_duration': 123044.27444005013, 'accumulated_submission_time': 113477.76876091957, 'accumulated_eval_time': 9538.537852048874, 'accumulated_logging_time': 14.424855947494507}
I0303 21:44:28.730023 140380334978816 logging_writer.py:48] [255152] accumulated_eval_time=9538.537852, accumulated_logging_time=14.424856, accumulated_submission_time=113477.768761, global_step=255152, preemption_count=0, score=113477.768761, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=123044.274440, train/accuracy=0.887910, train/loss=0.416396, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:44:48.212352 140380326586112 logging_writer.py:48] [255200] global_step=255200, grad_norm=4.013402462005615, loss=3.30793833732605
I0303 21:45:31.320128 140380334978816 logging_writer.py:48] [255300] global_step=255300, grad_norm=3.0437169075012207, loss=1.078464150428772
I0303 21:46:16.569399 140380326586112 logging_writer.py:48] [255400] global_step=255400, grad_norm=3.098097085952759, loss=1.3794950246810913
I0303 21:47:01.901923 140380334978816 logging_writer.py:48] [255500] global_step=255500, grad_norm=3.6151998043060303, loss=2.101480722427368
I0303 21:47:46.720506 140380326586112 logging_writer.py:48] [255600] global_step=255600, grad_norm=4.327884674072266, loss=3.1318016052246094
I0303 21:48:31.738685 140380334978816 logging_writer.py:48] [255700] global_step=255700, grad_norm=3.6472079753875732, loss=2.8617987632751465
I0303 21:49:16.564169 140380326586112 logging_writer.py:48] [255800] global_step=255800, grad_norm=3.147132635116577, loss=1.0885252952575684
I0303 21:50:01.536465 140380334978816 logging_writer.py:48] [255900] global_step=255900, grad_norm=3.6320745944976807, loss=2.857123374938965
I0303 21:50:46.487862 140380326586112 logging_writer.py:48] [256000] global_step=256000, grad_norm=2.870893955230713, loss=1.9062080383300781
I0303 21:51:28.880990 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:51:39.619876 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:52:04.457162 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:52:06.116763 140575196817216 submission_runner.py:411] Time since start: 123501.72s, 	Step: 256096, 	{'train/accuracy': 0.8895312547683716, 'train/loss': 0.4140346050262451, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 113897.86142706871, 'total_duration': 123501.7170226574, 'accumulated_submission_time': 113897.86142706871, 'accumulated_eval_time': 9575.771821022034, 'accumulated_logging_time': 14.490861177444458}
I0303 21:52:06.185579 140380334978816 logging_writer.py:48] [256096] accumulated_eval_time=9575.771821, accumulated_logging_time=14.490861, accumulated_submission_time=113897.861427, global_step=256096, preemption_count=0, score=113897.861427, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=123501.717023, train/accuracy=0.889531, train/loss=0.414035, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 21:52:08.177827 140380326586112 logging_writer.py:48] [256100] global_step=256100, grad_norm=3.6228160858154297, loss=2.704568386077881
I0303 21:52:48.490955 140380334978816 logging_writer.py:48] [256200] global_step=256200, grad_norm=3.3675777912139893, loss=1.1695849895477295
I0303 21:53:33.575903 140380326586112 logging_writer.py:48] [256300] global_step=256300, grad_norm=3.3620152473449707, loss=1.103147268295288
I0303 21:54:18.546879 140380334978816 logging_writer.py:48] [256400] global_step=256400, grad_norm=3.1997056007385254, loss=2.274387836456299
I0303 21:55:03.818960 140380326586112 logging_writer.py:48] [256500] global_step=256500, grad_norm=3.4580159187316895, loss=1.1128754615783691
I0303 21:55:48.589379 140380334978816 logging_writer.py:48] [256600] global_step=256600, grad_norm=2.9776313304901123, loss=1.092329502105713
I0303 21:56:33.963847 140380326586112 logging_writer.py:48] [256700] global_step=256700, grad_norm=3.8826441764831543, loss=3.212019920349121
I0303 21:57:19.068729 140380334978816 logging_writer.py:48] [256800] global_step=256800, grad_norm=4.609804630279541, loss=2.9433038234710693
I0303 21:58:03.941048 140380326586112 logging_writer.py:48] [256900] global_step=256900, grad_norm=3.2088468074798584, loss=1.404295563697815
I0303 21:58:48.940822 140380334978816 logging_writer.py:48] [257000] global_step=257000, grad_norm=3.062591552734375, loss=1.2437258958816528
I0303 21:59:06.256485 140575196817216 spec.py:321] Evaluating on the training split.
I0303 21:59:16.915109 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 21:59:41.479723 140575196817216 spec.py:349] Evaluating on the test split.
I0303 21:59:43.140521 140575196817216 submission_runner.py:411] Time since start: 123958.74s, 	Step: 257040, 	{'train/accuracy': 0.8866796493530273, 'train/loss': 0.4149068593978882, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 114317.87077498436, 'total_duration': 123958.74157166481, 'accumulated_submission_time': 114317.87077498436, 'accumulated_eval_time': 9612.654847860336, 'accumulated_logging_time': 14.572320699691772}
I0303 21:59:43.207221 140380326586112 logging_writer.py:48] [257040] accumulated_eval_time=9612.654848, accumulated_logging_time=14.572321, accumulated_submission_time=114317.870775, global_step=257040, preemption_count=0, score=114317.870775, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=123958.741572, train/accuracy=0.886680, train/loss=0.414907, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:00:07.503524 140380334978816 logging_writer.py:48] [257100] global_step=257100, grad_norm=3.015685796737671, loss=1.2922592163085938
I0303 22:00:50.758304 140380326586112 logging_writer.py:48] [257200] global_step=257200, grad_norm=2.920929193496704, loss=1.1516348123550415
I0303 22:01:35.781604 140380334978816 logging_writer.py:48] [257300] global_step=257300, grad_norm=3.130369186401367, loss=1.0737861394882202
I0303 22:02:20.865479 140380326586112 logging_writer.py:48] [257400] global_step=257400, grad_norm=3.118004560470581, loss=2.8587124347686768
I0303 22:03:05.927663 140380334978816 logging_writer.py:48] [257500] global_step=257500, grad_norm=3.0602962970733643, loss=1.148343801498413
I0303 22:03:51.070120 140380326586112 logging_writer.py:48] [257600] global_step=257600, grad_norm=2.930725574493408, loss=1.1380170583724976
I0303 22:04:35.766056 140380334978816 logging_writer.py:48] [257700] global_step=257700, grad_norm=3.2678749561309814, loss=2.763559579849243
I0303 22:05:21.026137 140380326586112 logging_writer.py:48] [257800] global_step=257800, grad_norm=3.3468639850616455, loss=1.6453155279159546
I0303 22:06:05.903194 140380334978816 logging_writer.py:48] [257900] global_step=257900, grad_norm=2.9481892585754395, loss=1.883123755455017
I0303 22:06:43.444441 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:06:54.092734 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:07:17.172073 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:07:18.848167 140575196817216 submission_runner.py:411] Time since start: 124414.45s, 	Step: 257985, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.4164501130580902, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 114738.04656410217, 'total_duration': 124414.44916987419, 'accumulated_submission_time': 114738.04656410217, 'accumulated_eval_time': 9648.057507038116, 'accumulated_logging_time': 14.651172876358032}
I0303 22:07:18.902480 140380326586112 logging_writer.py:48] [257985] accumulated_eval_time=9648.057507, accumulated_logging_time=14.651173, accumulated_submission_time=114738.046564, global_step=257985, preemption_count=0, score=114738.046564, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=124414.449170, train/accuracy=0.888379, train/loss=0.416450, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:07:25.256732 140380334978816 logging_writer.py:48] [258000] global_step=258000, grad_norm=3.8751354217529297, loss=3.0932257175445557
I0303 22:08:06.038401 140380326586112 logging_writer.py:48] [258100] global_step=258100, grad_norm=3.027113199234009, loss=1.1067142486572266
I0303 22:08:50.849634 140380334978816 logging_writer.py:48] [258200] global_step=258200, grad_norm=3.1890487670898438, loss=1.1334190368652344
I0303 22:09:36.016229 140380326586112 logging_writer.py:48] [258300] global_step=258300, grad_norm=3.0225727558135986, loss=1.3944374322891235
I0303 22:10:20.705465 140380334978816 logging_writer.py:48] [258400] global_step=258400, grad_norm=3.7146096229553223, loss=3.0530340671539307
I0303 22:11:06.035896 140380326586112 logging_writer.py:48] [258500] global_step=258500, grad_norm=3.10727596282959, loss=1.9913914203643799
I0303 22:11:51.117744 140380334978816 logging_writer.py:48] [258600] global_step=258600, grad_norm=3.130967855453491, loss=1.2341866493225098
I0303 22:12:35.914749 140380326586112 logging_writer.py:48] [258700] global_step=258700, grad_norm=3.589777946472168, loss=2.297455072402954
I0303 22:13:21.122052 140380334978816 logging_writer.py:48] [258800] global_step=258800, grad_norm=3.335256338119507, loss=2.620323419570923
I0303 22:14:05.859769 140380326586112 logging_writer.py:48] [258900] global_step=258900, grad_norm=3.973660707473755, loss=3.252501964569092
I0303 22:14:19.012110 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:14:30.746118 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:14:56.164965 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:14:57.818668 140575196817216 submission_runner.py:411] Time since start: 124873.42s, 	Step: 258931, 	{'train/accuracy': 0.8895312547683716, 'train/loss': 0.4190482199192047, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 115158.09628129005, 'total_duration': 124873.41975784302, 'accumulated_submission_time': 115158.09628129005, 'accumulated_eval_time': 9686.863124847412, 'accumulated_logging_time': 14.715799331665039}
I0303 22:14:57.874110 140380334978816 logging_writer.py:48] [258931] accumulated_eval_time=9686.863125, accumulated_logging_time=14.715799, accumulated_submission_time=115158.096281, global_step=258931, preemption_count=0, score=115158.096281, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=124873.419758, train/accuracy=0.889531, train/loss=0.419048, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:15:25.675260 140380326586112 logging_writer.py:48] [259000] global_step=259000, grad_norm=2.95782208442688, loss=2.1190881729125977
I0303 22:16:09.336679 140380334978816 logging_writer.py:48] [259100] global_step=259100, grad_norm=3.1549906730651855, loss=1.184491753578186
I0303 22:16:54.318445 140380326586112 logging_writer.py:48] [259200] global_step=259200, grad_norm=3.901657819747925, loss=3.2420098781585693
I0303 22:17:39.564311 140380334978816 logging_writer.py:48] [259300] global_step=259300, grad_norm=4.104782581329346, loss=3.1179299354553223
I0303 22:18:24.596315 140380326586112 logging_writer.py:48] [259400] global_step=259400, grad_norm=3.0820376873016357, loss=1.1634528636932373
I0303 22:19:09.715962 140380334978816 logging_writer.py:48] [259500] global_step=259500, grad_norm=2.9895529747009277, loss=1.3955973386764526
I0303 22:19:54.507161 140380326586112 logging_writer.py:48] [259600] global_step=259600, grad_norm=3.405015230178833, loss=1.4685758352279663
I0303 22:20:39.520898 140380334978816 logging_writer.py:48] [259700] global_step=259700, grad_norm=2.9500038623809814, loss=1.1107234954833984
I0303 22:21:24.305391 140380326586112 logging_writer.py:48] [259800] global_step=259800, grad_norm=2.89117431640625, loss=1.0225372314453125
I0303 22:21:58.069612 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:22:08.779763 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:22:32.930097 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:22:34.589774 140575196817216 submission_runner.py:411] Time since start: 125330.19s, 	Step: 259877, 	{'train/accuracy': 0.8865820169448853, 'train/loss': 0.42024752497673035, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 115578.23216414452, 'total_duration': 125330.19090342522, 'accumulated_submission_time': 115578.23216414452, 'accumulated_eval_time': 9723.382348060608, 'accumulated_logging_time': 14.781482458114624}
I0303 22:22:34.651904 140380334978816 logging_writer.py:48] [259877] accumulated_eval_time=9723.382348, accumulated_logging_time=14.781482, accumulated_submission_time=115578.232164, global_step=259877, preemption_count=0, score=115578.232164, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=125330.190903, train/accuracy=0.886582, train/loss=0.420248, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:22:44.189222 140380326586112 logging_writer.py:48] [259900] global_step=259900, grad_norm=2.8614847660064697, loss=1.9955679178237915
I0303 22:23:25.525714 140380334978816 logging_writer.py:48] [260000] global_step=260000, grad_norm=2.8272287845611572, loss=1.0205307006835938
I0303 22:24:10.516202 140380326586112 logging_writer.py:48] [260100] global_step=260100, grad_norm=3.203447103500366, loss=1.0167843103408813
I0303 22:24:55.181472 140380334978816 logging_writer.py:48] [260200] global_step=260200, grad_norm=3.034677267074585, loss=2.2899954319000244
I0303 22:25:40.533270 140380326586112 logging_writer.py:48] [260300] global_step=260300, grad_norm=3.1795623302459717, loss=2.1025123596191406
I0303 22:26:25.486752 140380334978816 logging_writer.py:48] [260400] global_step=260400, grad_norm=3.0249476432800293, loss=1.64900803565979
I0303 22:27:10.682040 140380326586112 logging_writer.py:48] [260500] global_step=260500, grad_norm=3.197617530822754, loss=2.61246657371521
I0303 22:27:55.536298 140380334978816 logging_writer.py:48] [260600] global_step=260600, grad_norm=3.1001639366149902, loss=1.2283573150634766
I0303 22:28:40.433927 140380326586112 logging_writer.py:48] [260700] global_step=260700, grad_norm=3.4589552879333496, loss=1.3019919395446777
I0303 22:29:25.530385 140380334978816 logging_writer.py:48] [260800] global_step=260800, grad_norm=3.1136679649353027, loss=1.207869529724121
I0303 22:29:34.593371 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:29:45.557332 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:30:08.978054 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:30:10.632802 140575196817216 submission_runner.py:411] Time since start: 125786.23s, 	Step: 260822, 	{'train/accuracy': 0.8883007764816284, 'train/loss': 0.418276309967041, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 115998.11412405968, 'total_duration': 125786.23392367363, 'accumulated_submission_time': 115998.11412405968, 'accumulated_eval_time': 9759.420825719833, 'accumulated_logging_time': 14.854071378707886}
I0303 22:30:10.690192 140380326586112 logging_writer.py:48] [260822] accumulated_eval_time=9759.420826, accumulated_logging_time=14.854071, accumulated_submission_time=115998.114124, global_step=260822, preemption_count=0, score=115998.114124, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=125786.233924, train/accuracy=0.888301, train/loss=0.418276, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:30:42.095239 140380334978816 logging_writer.py:48] [260900] global_step=260900, grad_norm=3.090860605239868, loss=2.584763288497925
I0303 22:31:26.231008 140380326586112 logging_writer.py:48] [261000] global_step=261000, grad_norm=3.0269365310668945, loss=1.1596647500991821
I0303 22:32:11.446136 140380334978816 logging_writer.py:48] [261100] global_step=261100, grad_norm=3.2865188121795654, loss=1.2513840198516846
I0303 22:32:56.361757 140380326586112 logging_writer.py:48] [261200] global_step=261200, grad_norm=3.306131362915039, loss=1.1799283027648926
I0303 22:33:41.345253 140380334978816 logging_writer.py:48] [261300] global_step=261300, grad_norm=3.191932201385498, loss=1.3744069337844849
I0303 22:34:26.299901 140380326586112 logging_writer.py:48] [261400] global_step=261400, grad_norm=2.991929292678833, loss=1.151221513748169
I0303 22:35:11.407438 140380334978816 logging_writer.py:48] [261500] global_step=261500, grad_norm=3.0267276763916016, loss=1.5763049125671387
I0303 22:35:56.301181 140380326586112 logging_writer.py:48] [261600] global_step=261600, grad_norm=3.1155641078948975, loss=1.6964683532714844
I0303 22:36:41.422531 140380334978816 logging_writer.py:48] [261700] global_step=261700, grad_norm=3.4427549839019775, loss=2.7054967880249023
I0303 22:37:10.757920 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:37:21.428507 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:37:46.853521 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:37:48.506173 140575196817216 submission_runner.py:411] Time since start: 126244.11s, 	Step: 261767, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.4182990491390228, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 116418.11951112747, 'total_duration': 126244.10726952553, 'accumulated_submission_time': 116418.11951112747, 'accumulated_eval_time': 9797.168113470078, 'accumulated_logging_time': 14.92531681060791}
I0303 22:37:48.565474 140380326586112 logging_writer.py:48] [261767] accumulated_eval_time=9797.168113, accumulated_logging_time=14.925317, accumulated_submission_time=116418.119511, global_step=261767, preemption_count=0, score=116418.119511, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=126244.107270, train/accuracy=0.888379, train/loss=0.418299, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:38:02.088032 140380334978816 logging_writer.py:48] [261800] global_step=261800, grad_norm=2.967902421951294, loss=1.1244585514068604
I0303 22:38:43.880336 140380326586112 logging_writer.py:48] [261900] global_step=261900, grad_norm=3.1515979766845703, loss=1.1997876167297363
I0303 22:39:29.157088 140380334978816 logging_writer.py:48] [262000] global_step=262000, grad_norm=3.3754470348358154, loss=1.32178795337677
I0303 22:40:14.384053 140380326586112 logging_writer.py:48] [262100] global_step=262100, grad_norm=3.09906268119812, loss=1.2493462562561035
I0303 22:40:59.335436 140380334978816 logging_writer.py:48] [262200] global_step=262200, grad_norm=3.296091318130493, loss=2.8097405433654785
I0303 22:41:44.290512 140380326586112 logging_writer.py:48] [262300] global_step=262300, grad_norm=3.664522647857666, loss=1.8900055885314941
I0303 22:42:29.225771 140380334978816 logging_writer.py:48] [262400] global_step=262400, grad_norm=3.826943874359131, loss=3.159165382385254
I0303 22:43:14.277810 140380326586112 logging_writer.py:48] [262500] global_step=262500, grad_norm=2.9925200939178467, loss=1.0658950805664062
I0303 22:43:59.023147 140380334978816 logging_writer.py:48] [262600] global_step=262600, grad_norm=3.274888753890991, loss=1.1283934116363525
I0303 22:44:44.073030 140380326586112 logging_writer.py:48] [262700] global_step=262700, grad_norm=3.0511460304260254, loss=1.1016615629196167
I0303 22:44:48.641043 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:44:59.017194 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:45:24.237473 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:45:25.896258 140575196817216 submission_runner.py:411] Time since start: 126701.50s, 	Step: 262712, 	{'train/accuracy': 0.8886132836341858, 'train/loss': 0.41737547516822815, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 116838.13629627228, 'total_duration': 126701.49708580971, 'accumulated_submission_time': 116838.13629627228, 'accumulated_eval_time': 9834.422104597092, 'accumulated_logging_time': 14.993987560272217}
I0303 22:45:25.954814 140380334978816 logging_writer.py:48] [262712] accumulated_eval_time=9834.422105, accumulated_logging_time=14.993988, accumulated_submission_time=116838.136296, global_step=262712, preemption_count=0, score=116838.136296, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=126701.497086, train/accuracy=0.888613, train/loss=0.417375, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:46:01.318278 140380326586112 logging_writer.py:48] [262800] global_step=262800, grad_norm=3.00010347366333, loss=1.0993119478225708
I0303 22:46:46.253405 140380334978816 logging_writer.py:48] [262900] global_step=262900, grad_norm=2.9242753982543945, loss=1.7955113649368286
I0303 22:47:31.505891 140380326586112 logging_writer.py:48] [263000] global_step=263000, grad_norm=3.090575695037842, loss=1.7748645544052124
I0303 22:48:16.612542 140380334978816 logging_writer.py:48] [263100] global_step=263100, grad_norm=2.9791665077209473, loss=1.209825038909912
I0303 22:49:01.362684 140380326586112 logging_writer.py:48] [263200] global_step=263200, grad_norm=3.1732940673828125, loss=1.0852223634719849
I0303 22:49:46.487920 140380334978816 logging_writer.py:48] [263300] global_step=263300, grad_norm=3.0759623050689697, loss=1.3837108612060547
I0303 22:50:31.388715 140380326586112 logging_writer.py:48] [263400] global_step=263400, grad_norm=2.8240914344787598, loss=1.9315918684005737
I0303 22:51:16.274453 140380334978816 logging_writer.py:48] [263500] global_step=263500, grad_norm=3.2042734622955322, loss=1.40995454788208
I0303 22:52:01.158089 140380326586112 logging_writer.py:48] [263600] global_step=263600, grad_norm=4.073111057281494, loss=3.251447916030884
I0303 22:52:26.022823 140575196817216 spec.py:321] Evaluating on the training split.
I0303 22:52:36.611444 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 22:53:03.632277 140575196817216 spec.py:349] Evaluating on the test split.
I0303 22:53:05.287606 140575196817216 submission_runner.py:411] Time since start: 127160.89s, 	Step: 263657, 	{'train/accuracy': 0.8840234279632568, 'train/loss': 0.42402347922325134, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 117258.14576864243, 'total_duration': 127160.88830947876, 'accumulated_submission_time': 117258.14576864243, 'accumulated_eval_time': 9873.685550451279, 'accumulated_logging_time': 15.062115669250488}
I0303 22:53:05.346289 140380334978816 logging_writer.py:48] [263657] accumulated_eval_time=9873.685550, accumulated_logging_time=15.062116, accumulated_submission_time=117258.145769, global_step=263657, preemption_count=0, score=117258.145769, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=127160.888309, train/accuracy=0.884023, train/loss=0.424023, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 22:53:22.846135 140380326586112 logging_writer.py:48] [263700] global_step=263700, grad_norm=3.8451719284057617, loss=3.0848774909973145
I0303 22:54:04.955985 140380334978816 logging_writer.py:48] [263800] global_step=263800, grad_norm=3.168834924697876, loss=1.08139967918396
I0303 22:54:49.957214 140380326586112 logging_writer.py:48] [263900] global_step=263900, grad_norm=3.3932156562805176, loss=2.6130011081695557
I0303 22:55:35.388634 140380334978816 logging_writer.py:48] [264000] global_step=264000, grad_norm=3.1674513816833496, loss=1.135816216468811
I0303 22:56:20.688271 140380326586112 logging_writer.py:48] [264100] global_step=264100, grad_norm=3.1224780082702637, loss=1.2519878149032593
I0303 22:57:05.674827 140380334978816 logging_writer.py:48] [264200] global_step=264200, grad_norm=3.3795323371887207, loss=2.713502883911133
I0303 22:57:50.827051 140380326586112 logging_writer.py:48] [264300] global_step=264300, grad_norm=3.360694408416748, loss=2.9601545333862305
I0303 22:58:35.932662 140380334978816 logging_writer.py:48] [264400] global_step=264400, grad_norm=4.0033488273620605, loss=2.923692226409912
I0303 22:59:20.948441 140380326586112 logging_writer.py:48] [264500] global_step=264500, grad_norm=3.136591672897339, loss=1.2971068620681763
I0303 23:00:05.978194 140380334978816 logging_writer.py:48] [264600] global_step=264600, grad_norm=3.22808837890625, loss=1.1571186780929565
I0303 23:00:05.992473 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:00:16.976495 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:00:42.263311 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:00:43.906746 140575196817216 submission_runner.py:411] Time since start: 127619.51s, 	Step: 264601, 	{'train/accuracy': 0.8874804377555847, 'train/loss': 0.4231193959712982, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 117678.73268508911, 'total_duration': 127619.50759148598, 'accumulated_submission_time': 117678.73268508911, 'accumulated_eval_time': 9911.598610162735, 'accumulated_logging_time': 15.131271123886108}
I0303 23:00:43.966027 140380326586112 logging_writer.py:48] [264601] accumulated_eval_time=9911.598610, accumulated_logging_time=15.131271, accumulated_submission_time=117678.732685, global_step=264601, preemption_count=0, score=117678.732685, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=127619.507591, train/accuracy=0.887480, train/loss=0.423119, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:01:24.116969 140380334978816 logging_writer.py:48] [264700] global_step=264700, grad_norm=2.9172606468200684, loss=1.1566879749298096
I0303 23:02:08.974840 140380326586112 logging_writer.py:48] [264800] global_step=264800, grad_norm=3.307433605194092, loss=1.1250343322753906
I0303 23:02:54.019961 140380334978816 logging_writer.py:48] [264900] global_step=264900, grad_norm=3.225877523422241, loss=1.1854603290557861
I0303 23:03:39.188859 140380326586112 logging_writer.py:48] [265000] global_step=265000, grad_norm=3.181842803955078, loss=2.002732276916504
I0303 23:04:23.834530 140380334978816 logging_writer.py:48] [265100] global_step=265100, grad_norm=3.2449142932891846, loss=1.804352045059204
I0303 23:05:08.636748 140380326586112 logging_writer.py:48] [265200] global_step=265200, grad_norm=2.7852959632873535, loss=1.2801151275634766
I0303 23:05:53.615720 140380334978816 logging_writer.py:48] [265300] global_step=265300, grad_norm=3.313274621963501, loss=1.1811785697937012
I0303 23:06:38.586307 140380326586112 logging_writer.py:48] [265400] global_step=265400, grad_norm=3.0504748821258545, loss=1.1813726425170898
I0303 23:07:23.765571 140380334978816 logging_writer.py:48] [265500] global_step=265500, grad_norm=3.1496422290802, loss=1.1805057525634766
I0303 23:07:44.303564 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:07:54.822455 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:08:16.043506 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:08:17.696771 140575196817216 submission_runner.py:411] Time since start: 128073.30s, 	Step: 265547, 	{'train/accuracy': 0.8855078220367432, 'train/loss': 0.42642009258270264, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 118099.01088500023, 'total_duration': 128073.29778432846, 'accumulated_submission_time': 118099.01088500023, 'accumulated_eval_time': 9944.990770578384, 'accumulated_logging_time': 15.20097303390503}
I0303 23:08:17.755239 140380326586112 logging_writer.py:48] [265547] accumulated_eval_time=9944.990771, accumulated_logging_time=15.200973, accumulated_submission_time=118099.010885, global_step=265547, preemption_count=0, score=118099.010885, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=128073.297784, train/accuracy=0.885508, train/loss=0.426420, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:08:39.213760 140380334978816 logging_writer.py:48] [265600] global_step=265600, grad_norm=3.200669288635254, loss=1.2106672525405884
I0303 23:09:22.113293 140380326586112 logging_writer.py:48] [265700] global_step=265700, grad_norm=3.0471420288085938, loss=1.1876851320266724
I0303 23:10:07.280223 140380334978816 logging_writer.py:48] [265800] global_step=265800, grad_norm=3.047410726547241, loss=1.1495252847671509
I0303 23:10:52.579652 140380326586112 logging_writer.py:48] [265900] global_step=265900, grad_norm=4.284726142883301, loss=3.2751829624176025
I0303 23:11:37.634779 140380334978816 logging_writer.py:48] [266000] global_step=266000, grad_norm=3.805111885070801, loss=3.0118796825408936
I0303 23:12:22.772086 140380326586112 logging_writer.py:48] [266100] global_step=266100, grad_norm=3.294898748397827, loss=1.5937654972076416
I0303 23:13:07.959360 140380334978816 logging_writer.py:48] [266200] global_step=266200, grad_norm=3.890957832336426, loss=3.076622724533081
I0303 23:13:52.853643 140380326586112 logging_writer.py:48] [266300] global_step=266300, grad_norm=3.0214600563049316, loss=1.234602451324463
I0303 23:14:37.779683 140380334978816 logging_writer.py:48] [266400] global_step=266400, grad_norm=3.295151472091675, loss=1.139397144317627
I0303 23:15:17.769938 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:15:28.472962 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:15:52.180917 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:15:53.843311 140575196817216 submission_runner.py:411] Time since start: 128529.44s, 	Step: 266490, 	{'train/accuracy': 0.8890234231948853, 'train/loss': 0.4168081283569336, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 118518.96572709084, 'total_duration': 128529.44418168068, 'accumulated_submission_time': 118518.96572709084, 'accumulated_eval_time': 9981.062950611115, 'accumulated_logging_time': 15.270276546478271}
I0303 23:15:53.901958 140380326586112 logging_writer.py:48] [266490] accumulated_eval_time=9981.062951, accumulated_logging_time=15.270277, accumulated_submission_time=118518.965727, global_step=266490, preemption_count=0, score=118518.965727, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=128529.444182, train/accuracy=0.889023, train/loss=0.416808, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:15:58.276892 140380334978816 logging_writer.py:48] [266500] global_step=266500, grad_norm=2.9716994762420654, loss=1.042335033416748
I0303 23:16:39.046260 140380326586112 logging_writer.py:48] [266600] global_step=266600, grad_norm=3.522557020187378, loss=3.031571388244629
I0303 23:17:23.835673 140380334978816 logging_writer.py:48] [266700] global_step=266700, grad_norm=3.1850476264953613, loss=1.4305658340454102
I0303 23:18:09.313209 140380326586112 logging_writer.py:48] [266800] global_step=266800, grad_norm=3.2758255004882812, loss=2.573575496673584
I0303 23:18:54.326346 140380334978816 logging_writer.py:48] [266900] global_step=266900, grad_norm=3.2552435398101807, loss=1.132920742034912
I0303 23:19:39.416632 140380326586112 logging_writer.py:48] [267000] global_step=267000, grad_norm=2.7668616771698, loss=1.9620846509933472
I0303 23:20:24.047129 140380334978816 logging_writer.py:48] [267100] global_step=267100, grad_norm=3.7616324424743652, loss=3.1485683917999268
I0303 23:21:08.999425 140380326586112 logging_writer.py:48] [267200] global_step=267200, grad_norm=3.576920509338379, loss=1.6763924360275269
I0303 23:21:54.041429 140380334978816 logging_writer.py:48] [267300] global_step=267300, grad_norm=3.006908893585205, loss=1.2750927209854126
I0303 23:22:38.889614 140380326586112 logging_writer.py:48] [267400] global_step=267400, grad_norm=2.873748779296875, loss=1.2052369117736816
I0303 23:22:53.916769 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:23:04.649949 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:23:29.233842 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:23:30.885250 140575196817216 submission_runner.py:411] Time since start: 128986.49s, 	Step: 267435, 	{'train/accuracy': 0.8855664134025574, 'train/loss': 0.42335715889930725, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 118938.91685509682, 'total_duration': 128986.48639082909, 'accumulated_submission_time': 118938.91685509682, 'accumulated_eval_time': 10018.03050661087, 'accumulated_logging_time': 15.34412407875061}
I0303 23:23:30.942927 140380334978816 logging_writer.py:48] [267435] accumulated_eval_time=10018.030507, accumulated_logging_time=15.344124, accumulated_submission_time=118938.916855, global_step=267435, preemption_count=0, score=118938.916855, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=128986.486391, train/accuracy=0.885566, train/loss=0.423357, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:23:57.166491 140380326586112 logging_writer.py:48] [267500] global_step=267500, grad_norm=3.6641111373901367, loss=3.030748128890991
I0303 23:24:40.691595 140380334978816 logging_writer.py:48] [267600] global_step=267600, grad_norm=3.0589661598205566, loss=1.0573792457580566
I0303 23:25:25.415577 140380326586112 logging_writer.py:48] [267700] global_step=267700, grad_norm=3.022935152053833, loss=1.094759225845337
I0303 23:26:10.842985 140380334978816 logging_writer.py:48] [267800] global_step=267800, grad_norm=3.216355800628662, loss=1.5480033159255981
I0303 23:26:55.931463 140380326586112 logging_writer.py:48] [267900] global_step=267900, grad_norm=3.2998082637786865, loss=1.1817216873168945
I0303 23:27:40.882757 140380334978816 logging_writer.py:48] [268000] global_step=268000, grad_norm=15.057153701782227, loss=1.2342438697814941
I0303 23:28:25.975648 140380326586112 logging_writer.py:48] [268100] global_step=268100, grad_norm=3.3014161586761475, loss=1.1753140687942505
I0303 23:29:10.701570 140380334978816 logging_writer.py:48] [268200] global_step=268200, grad_norm=3.203312873840332, loss=1.797688603401184
I0303 23:29:55.668874 140380326586112 logging_writer.py:48] [268300] global_step=268300, grad_norm=3.34432053565979, loss=1.2176162004470825
I0303 23:30:31.261765 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:30:42.083326 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:31:06.359733 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:31:08.021586 140575196817216 submission_runner.py:411] Time since start: 129443.62s, 	Step: 268381, 	{'train/accuracy': 0.8865038752555847, 'train/loss': 0.42107051610946655, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 119359.17529034615, 'total_duration': 129443.62229990959, 'accumulated_submission_time': 119359.17529034615, 'accumulated_eval_time': 10054.78897356987, 'accumulated_logging_time': 15.412813425064087}
I0303 23:31:08.080148 140380334978816 logging_writer.py:48] [268381] accumulated_eval_time=10054.788974, accumulated_logging_time=15.412813, accumulated_submission_time=119359.175290, global_step=268381, preemption_count=0, score=119359.175290, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=129443.622300, train/accuracy=0.886504, train/loss=0.421071, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:31:16.033052 140380326586112 logging_writer.py:48] [268400] global_step=268400, grad_norm=2.9962568283081055, loss=1.0863372087478638
I0303 23:31:56.922759 140380334978816 logging_writer.py:48] [268500] global_step=268500, grad_norm=3.0510551929473877, loss=1.1344521045684814
I0303 23:32:41.948002 140380326586112 logging_writer.py:48] [268600] global_step=268600, grad_norm=3.157620906829834, loss=1.6009998321533203
I0303 23:33:27.131367 140380334978816 logging_writer.py:48] [268700] global_step=268700, grad_norm=3.4664504528045654, loss=2.684938669204712
I0303 23:34:12.108257 140380326586112 logging_writer.py:48] [268800] global_step=268800, grad_norm=3.2320196628570557, loss=1.2268701791763306
I0303 23:34:56.894068 140380334978816 logging_writer.py:48] [268900] global_step=268900, grad_norm=3.240360736846924, loss=1.144930362701416
I0303 23:35:41.970090 140380326586112 logging_writer.py:48] [269000] global_step=269000, grad_norm=3.073882579803467, loss=1.0962088108062744
I0303 23:36:26.842631 140380334978816 logging_writer.py:48] [269100] global_step=269100, grad_norm=3.0358033180236816, loss=1.1727204322814941
I0303 23:37:12.010495 140380326586112 logging_writer.py:48] [269200] global_step=269200, grad_norm=3.2374267578125, loss=1.1190799474716187
I0303 23:37:57.168485 140380334978816 logging_writer.py:48] [269300] global_step=269300, grad_norm=3.2768468856811523, loss=2.7908315658569336
I0303 23:38:08.251071 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:38:19.125064 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:38:44.064490 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:38:45.715037 140575196817216 submission_runner.py:411] Time since start: 129901.32s, 	Step: 269326, 	{'train/accuracy': 0.8862109184265137, 'train/loss': 0.4198490381240845, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 119779.28468871117, 'total_duration': 129901.31611633301, 'accumulated_submission_time': 119779.28468871117, 'accumulated_eval_time': 10092.251964330673, 'accumulated_logging_time': 15.483604669570923}
I0303 23:38:45.774199 140380326586112 logging_writer.py:48] [269326] accumulated_eval_time=10092.251964, accumulated_logging_time=15.483605, accumulated_submission_time=119779.284689, global_step=269326, preemption_count=0, score=119779.284689, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=129901.316116, train/accuracy=0.886211, train/loss=0.419849, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:39:15.572817 140380334978816 logging_writer.py:48] [269400] global_step=269400, grad_norm=4.407957077026367, loss=2.974748373031616
I0303 23:39:59.583049 140380326586112 logging_writer.py:48] [269500] global_step=269500, grad_norm=3.2197775840759277, loss=1.2092537879943848
I0303 23:40:44.770037 140380334978816 logging_writer.py:48] [269600] global_step=269600, grad_norm=4.363440990447998, loss=3.2201948165893555
I0303 23:41:30.033736 140380326586112 logging_writer.py:48] [269700] global_step=269700, grad_norm=2.9895291328430176, loss=1.6107209920883179
I0303 23:42:15.144663 140380334978816 logging_writer.py:48] [269800] global_step=269800, grad_norm=2.8770806789398193, loss=1.1933821439743042
I0303 23:42:59.940088 140380326586112 logging_writer.py:48] [269900] global_step=269900, grad_norm=3.178588628768921, loss=1.4673519134521484
I0303 23:43:45.084288 140380334978816 logging_writer.py:48] [270000] global_step=270000, grad_norm=3.9707438945770264, loss=3.022341251373291
I0303 23:44:30.310693 140380326586112 logging_writer.py:48] [270100] global_step=270100, grad_norm=3.1077334880828857, loss=1.1301891803741455
I0303 23:45:15.275291 140380334978816 logging_writer.py:48] [270200] global_step=270200, grad_norm=3.4168331623077393, loss=2.2617220878601074
I0303 23:45:45.737406 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:45:56.652377 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:46:20.261367 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:46:21.929647 140575196817216 submission_runner.py:411] Time since start: 130357.53s, 	Step: 270269, 	{'train/accuracy': 0.8884961009025574, 'train/loss': 0.4142680764198303, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 120199.18902301788, 'total_duration': 130357.5304300785, 'accumulated_submission_time': 120199.18902301788, 'accumulated_eval_time': 10128.442929267883, 'accumulated_logging_time': 15.552424669265747}
I0303 23:46:21.997617 140380326586112 logging_writer.py:48] [270269] accumulated_eval_time=10128.442929, accumulated_logging_time=15.552425, accumulated_submission_time=120199.189023, global_step=270269, preemption_count=0, score=120199.189023, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=130357.530430, train/accuracy=0.888496, train/loss=0.414268, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:46:34.703570 140380334978816 logging_writer.py:48] [270300] global_step=270300, grad_norm=3.1268019676208496, loss=1.170465350151062
I0303 23:47:16.434256 140380326586112 logging_writer.py:48] [270400] global_step=270400, grad_norm=3.0458288192749023, loss=1.138371229171753
I0303 23:48:01.639068 140380334978816 logging_writer.py:48] [270500] global_step=270500, grad_norm=4.007969379425049, loss=2.979996681213379
I0303 23:48:46.752986 140380326586112 logging_writer.py:48] [270600] global_step=270600, grad_norm=3.1559529304504395, loss=1.0965168476104736
I0303 23:49:31.625926 140380334978816 logging_writer.py:48] [270700] global_step=270700, grad_norm=3.023805618286133, loss=2.310861825942993
I0303 23:50:16.579917 140380326586112 logging_writer.py:48] [270800] global_step=270800, grad_norm=3.256761074066162, loss=1.0543278455734253
I0303 23:51:01.446895 140380334978816 logging_writer.py:48] [270900] global_step=270900, grad_norm=2.9106736183166504, loss=1.0599080324172974
I0303 23:51:46.430016 140380326586112 logging_writer.py:48] [271000] global_step=271000, grad_norm=2.8190619945526123, loss=1.0977240800857544
I0303 23:52:31.647006 140380334978816 logging_writer.py:48] [271100] global_step=271100, grad_norm=3.614774227142334, loss=1.4177956581115723
I0303 23:53:16.495392 140380326586112 logging_writer.py:48] [271200] global_step=271200, grad_norm=3.444105386734009, loss=1.1424639225006104
I0303 23:53:21.958892 140575196817216 spec.py:321] Evaluating on the training split.
I0303 23:53:32.782930 140575196817216 spec.py:333] Evaluating on the validation split.
I0303 23:53:59.603521 140575196817216 spec.py:349] Evaluating on the test split.
I0303 23:54:01.269040 140575196817216 submission_runner.py:411] Time since start: 130816.87s, 	Step: 271214, 	{'train/accuracy': 0.8880664110183716, 'train/loss': 0.4235153794288635, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 120619.08997631073, 'total_duration': 130816.87016510963, 'accumulated_submission_time': 120619.08997631073, 'accumulated_eval_time': 10167.75212931633, 'accumulated_logging_time': 15.631080865859985}
I0303 23:54:01.326480 140380334978816 logging_writer.py:48] [271214] accumulated_eval_time=10167.752129, accumulated_logging_time=15.631081, accumulated_submission_time=120619.089976, global_step=271214, preemption_count=0, score=120619.089976, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=130816.870165, train/accuracy=0.888066, train/loss=0.423515, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0303 23:54:35.870193 140380326586112 logging_writer.py:48] [271300] global_step=271300, grad_norm=3.0794904232025146, loss=1.2356128692626953
I0303 23:55:20.512078 140380334978816 logging_writer.py:48] [271400] global_step=271400, grad_norm=3.079533338546753, loss=1.9087538719177246
I0303 23:56:05.867229 140380326586112 logging_writer.py:48] [271500] global_step=271500, grad_norm=9.285059928894043, loss=3.0839264392852783
I0303 23:56:51.252426 140380334978816 logging_writer.py:48] [271600] global_step=271600, grad_norm=2.9174695014953613, loss=1.441392183303833
I0303 23:57:36.247895 140380326586112 logging_writer.py:48] [271700] global_step=271700, grad_norm=2.9696555137634277, loss=1.051809549331665
I0303 23:58:21.352511 140380334978816 logging_writer.py:48] [271800] global_step=271800, grad_norm=3.6483118534088135, loss=1.1444692611694336
I0303 23:59:06.226852 140380326586112 logging_writer.py:48] [271900] global_step=271900, grad_norm=2.9349868297576904, loss=1.6511813402175903
I0303 23:59:51.237240 140380334978816 logging_writer.py:48] [272000] global_step=272000, grad_norm=3.316314458847046, loss=2.4911575317382812
I0304 00:00:36.121380 140380326586112 logging_writer.py:48] [272100] global_step=272100, grad_norm=4.092087745666504, loss=3.1812589168548584
I0304 00:01:01.688554 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:01:12.581746 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:01:37.848163 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:01:39.512586 140575196817216 submission_runner.py:411] Time since start: 131275.11s, 	Step: 272159, 	{'train/accuracy': 0.887011706829071, 'train/loss': 0.4199972152709961, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 121039.39338994026, 'total_duration': 131275.11368703842, 'accumulated_submission_time': 121039.39338994026, 'accumulated_eval_time': 10205.575210809708, 'accumulated_logging_time': 15.698053359985352}
I0304 00:01:39.569425 140380334978816 logging_writer.py:48] [272159] accumulated_eval_time=10205.575211, accumulated_logging_time=15.698053, accumulated_submission_time=121039.393390, global_step=272159, preemption_count=0, score=121039.393390, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=131275.113687, train/accuracy=0.887012, train/loss=0.419997, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:01:56.274868 140380326586112 logging_writer.py:48] [272200] global_step=272200, grad_norm=3.029020071029663, loss=1.0425372123718262
I0304 00:02:38.567031 140380334978816 logging_writer.py:48] [272300] global_step=272300, grad_norm=3.423553943634033, loss=1.0880094766616821
I0304 00:03:23.280917 140380326586112 logging_writer.py:48] [272400] global_step=272400, grad_norm=3.200953722000122, loss=1.2641444206237793
I0304 00:04:08.774412 140380334978816 logging_writer.py:48] [272500] global_step=272500, grad_norm=3.0641252994537354, loss=1.6344842910766602
I0304 00:04:53.556240 140380326586112 logging_writer.py:48] [272600] global_step=272600, grad_norm=3.244133234024048, loss=2.963728904724121
I0304 00:05:38.169741 140380334978816 logging_writer.py:48] [272700] global_step=272700, grad_norm=4.768426895141602, loss=3.240434408187866
I0304 00:06:24.149336 140380326586112 logging_writer.py:48] [272800] global_step=272800, grad_norm=3.2640554904937744, loss=1.162554383277893
I0304 00:07:09.296197 140380334978816 logging_writer.py:48] [272900] global_step=272900, grad_norm=2.902381181716919, loss=2.136784553527832
I0304 00:07:54.506196 140380326586112 logging_writer.py:48] [273000] global_step=273000, grad_norm=2.9506280422210693, loss=1.1613283157348633
I0304 00:08:39.510529 140380334978816 logging_writer.py:48] [273100] global_step=273100, grad_norm=4.647459030151367, loss=3.293621301651001
I0304 00:08:39.524500 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:08:50.357713 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:09:13.295031 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:09:14.947962 140575196817216 submission_runner.py:411] Time since start: 131730.55s, 	Step: 273101, 	{'train/accuracy': 0.8873242139816284, 'train/loss': 0.4234861135482788, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 121459.28456163406, 'total_duration': 131730.5489742756, 'accumulated_submission_time': 121459.28456163406, 'accumulated_eval_time': 10240.997619867325, 'accumulated_logging_time': 15.770262479782104}
I0304 00:09:15.006624 140380326586112 logging_writer.py:48] [273101] accumulated_eval_time=10240.997620, accumulated_logging_time=15.770262, accumulated_submission_time=121459.284562, global_step=273101, preemption_count=0, score=121459.284562, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=131730.548974, train/accuracy=0.887324, train/loss=0.423486, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:09:55.186526 140380334978816 logging_writer.py:48] [273200] global_step=273200, grad_norm=4.497882843017578, loss=3.19816255569458
I0304 00:10:40.133337 140380326586112 logging_writer.py:48] [273300] global_step=273300, grad_norm=4.03823709487915, loss=3.2380175590515137
I0304 00:11:25.714266 140380334978816 logging_writer.py:48] [273400] global_step=273400, grad_norm=3.160989761352539, loss=1.229515552520752
I0304 00:12:10.792450 140380326586112 logging_writer.py:48] [273500] global_step=273500, grad_norm=2.900073528289795, loss=1.0338488817214966
I0304 00:12:56.093371 140380334978816 logging_writer.py:48] [273600] global_step=273600, grad_norm=3.240445613861084, loss=2.6487138271331787
I0304 00:13:41.179716 140380326586112 logging_writer.py:48] [273700] global_step=273700, grad_norm=3.334043502807617, loss=1.2244218587875366
I0304 00:14:26.402389 140380334978816 logging_writer.py:48] [273800] global_step=273800, grad_norm=3.318054437637329, loss=1.153922438621521
I0304 00:15:11.325750 140380326586112 logging_writer.py:48] [273900] global_step=273900, grad_norm=3.637298583984375, loss=3.1355583667755127
I0304 00:15:56.741961 140380334978816 logging_writer.py:48] [274000] global_step=274000, grad_norm=3.9824581146240234, loss=3.2863729000091553
I0304 00:16:15.018856 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:16:25.732441 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:16:49.712165 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:16:51.368525 140575196817216 submission_runner.py:411] Time since start: 132186.97s, 	Step: 274042, 	{'train/accuracy': 0.8865429759025574, 'train/loss': 0.4222384989261627, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 121879.23887968063, 'total_duration': 132186.96950387955, 'accumulated_submission_time': 121879.23887968063, 'accumulated_eval_time': 10277.346205711365, 'accumulated_logging_time': 15.83849048614502}
I0304 00:16:51.434882 140380326586112 logging_writer.py:48] [274042] accumulated_eval_time=10277.346206, accumulated_logging_time=15.838490, accumulated_submission_time=121879.238880, global_step=274042, preemption_count=0, score=121879.238880, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=132186.969504, train/accuracy=0.886543, train/loss=0.422238, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:17:14.873545 140380334978816 logging_writer.py:48] [274100] global_step=274100, grad_norm=3.8260977268218994, loss=3.2142999172210693
I0304 00:17:58.111424 140380326586112 logging_writer.py:48] [274200] global_step=274200, grad_norm=3.3873019218444824, loss=1.2915693521499634
I0304 00:18:43.173876 140380334978816 logging_writer.py:48] [274300] global_step=274300, grad_norm=3.4084575176239014, loss=2.746495008468628
I0304 00:19:28.205412 140380326586112 logging_writer.py:48] [274400] global_step=274400, grad_norm=3.1097347736358643, loss=1.0583510398864746
I0304 00:20:13.260706 140380334978816 logging_writer.py:48] [274500] global_step=274500, grad_norm=3.005072593688965, loss=1.150409460067749
I0304 00:20:58.406294 140380326586112 logging_writer.py:48] [274600] global_step=274600, grad_norm=3.127333402633667, loss=1.2844204902648926
I0304 00:21:43.219011 140380334978816 logging_writer.py:48] [274700] global_step=274700, grad_norm=3.277160882949829, loss=2.7074451446533203
I0304 00:22:28.320296 140380326586112 logging_writer.py:48] [274800] global_step=274800, grad_norm=3.4149351119995117, loss=1.441609263420105
I0304 00:23:13.375128 140380334978816 logging_writer.py:48] [274900] global_step=274900, grad_norm=6.309258460998535, loss=2.2157843112945557
I0304 00:23:51.636811 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:24:02.386674 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:24:27.766937 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:24:29.416793 140575196817216 submission_runner.py:411] Time since start: 132645.02s, 	Step: 274987, 	{'train/accuracy': 0.8861327767372131, 'train/loss': 0.4210696220397949, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 122299.38060212135, 'total_duration': 132645.01781749725, 'accumulated_submission_time': 122299.38060212135, 'accumulated_eval_time': 10315.125161886215, 'accumulated_logging_time': 15.915863037109375}
I0304 00:24:29.475962 140380326586112 logging_writer.py:48] [274987] accumulated_eval_time=10315.125162, accumulated_logging_time=15.915863, accumulated_submission_time=122299.380602, global_step=274987, preemption_count=0, score=122299.380602, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=132645.017817, train/accuracy=0.886133, train/loss=0.421070, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:24:35.050997 140380334978816 logging_writer.py:48] [275000] global_step=275000, grad_norm=3.1212663650512695, loss=1.825514554977417
I0304 00:25:15.895411 140380326586112 logging_writer.py:48] [275100] global_step=275100, grad_norm=3.036165475845337, loss=1.1373240947723389
I0304 00:26:00.228257 140380334978816 logging_writer.py:48] [275200] global_step=275200, grad_norm=3.2114999294281006, loss=1.7895265817642212
I0304 00:26:45.510079 140380326586112 logging_writer.py:48] [275300] global_step=275300, grad_norm=3.0421903133392334, loss=1.8996994495391846
I0304 00:27:30.530376 140380334978816 logging_writer.py:48] [275400] global_step=275400, grad_norm=3.3473708629608154, loss=1.6021254062652588
I0304 00:28:15.441961 140380326586112 logging_writer.py:48] [275500] global_step=275500, grad_norm=7.55602502822876, loss=1.521580457687378
I0304 00:29:00.342932 140380334978816 logging_writer.py:48] [275600] global_step=275600, grad_norm=3.4600658416748047, loss=1.2005860805511475
I0304 00:29:45.305619 140380326586112 logging_writer.py:48] [275700] global_step=275700, grad_norm=3.1652536392211914, loss=1.218406319618225
I0304 00:30:30.443444 140380334978816 logging_writer.py:48] [275800] global_step=275800, grad_norm=3.083259344100952, loss=1.136655569076538
I0304 00:31:15.547196 140380326586112 logging_writer.py:48] [275900] global_step=275900, grad_norm=3.6014814376831055, loss=1.8898954391479492
I0304 00:31:29.596258 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:31:40.164440 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:32:04.670328 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:32:06.329324 140575196817216 submission_runner.py:411] Time since start: 133101.93s, 	Step: 275933, 	{'train/accuracy': 0.8864843845367432, 'train/loss': 0.4235370457172394, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 122719.44203877449, 'total_duration': 133101.93027758598, 'accumulated_submission_time': 122719.44203877449, 'accumulated_eval_time': 10351.85710811615, 'accumulated_logging_time': 15.985002279281616}
I0304 00:32:06.385411 140380334978816 logging_writer.py:48] [275933] accumulated_eval_time=10351.857108, accumulated_logging_time=15.985002, accumulated_submission_time=122719.442039, global_step=275933, preemption_count=0, score=122719.442039, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=133101.930278, train/accuracy=0.886484, train/loss=0.423537, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:32:33.413036 140380326586112 logging_writer.py:48] [276000] global_step=276000, grad_norm=4.347561836242676, loss=3.1364707946777344
I0304 00:33:16.885868 140380334978816 logging_writer.py:48] [276100] global_step=276100, grad_norm=3.335885524749756, loss=1.1691951751708984
I0304 00:34:01.758293 140380326586112 logging_writer.py:48] [276200] global_step=276200, grad_norm=3.2991232872009277, loss=1.1254918575286865
I0304 00:34:46.817339 140380334978816 logging_writer.py:48] [276300] global_step=276300, grad_norm=3.1736626625061035, loss=1.0731278657913208
I0304 00:35:31.751326 140380326586112 logging_writer.py:48] [276400] global_step=276400, grad_norm=3.663360595703125, loss=3.2638819217681885
I0304 00:36:17.186389 140380334978816 logging_writer.py:48] [276500] global_step=276500, grad_norm=3.352261543273926, loss=1.1980193853378296
I0304 00:37:01.960242 140380326586112 logging_writer.py:48] [276600] global_step=276600, grad_norm=3.198305606842041, loss=1.4389294385910034
I0304 00:37:46.888236 140380334978816 logging_writer.py:48] [276700] global_step=276700, grad_norm=3.2069621086120605, loss=1.7152233123779297
I0304 00:38:31.986632 140380326586112 logging_writer.py:48] [276800] global_step=276800, grad_norm=3.337167263031006, loss=1.1663053035736084
I0304 00:39:06.429722 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:39:17.170225 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:39:43.687792 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:39:45.338388 140575196817216 submission_runner.py:411] Time since start: 133560.94s, 	Step: 276878, 	{'train/accuracy': 0.8878905773162842, 'train/loss': 0.4141671657562256, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 123139.42353534698, 'total_duration': 133560.93928790092, 'accumulated_submission_time': 123139.42353534698, 'accumulated_eval_time': 10390.764615297318, 'accumulated_logging_time': 16.054569482803345}
I0304 00:39:45.395196 140380334978816 logging_writer.py:48] [276878] accumulated_eval_time=10390.764615, accumulated_logging_time=16.054569, accumulated_submission_time=123139.423535, global_step=276878, preemption_count=0, score=123139.423535, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=133560.939288, train/accuracy=0.887891, train/loss=0.414167, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:39:54.527785 140380326586112 logging_writer.py:48] [276900] global_step=276900, grad_norm=3.1359736919403076, loss=1.9523591995239258
I0304 00:40:36.088104 140380334978816 logging_writer.py:48] [277000] global_step=277000, grad_norm=2.8898379802703857, loss=1.178875207901001
I0304 00:41:21.070836 140380326586112 logging_writer.py:48] [277100] global_step=277100, grad_norm=4.512502670288086, loss=3.15451979637146
I0304 00:42:06.155714 140380334978816 logging_writer.py:48] [277200] global_step=277200, grad_norm=3.2889668941497803, loss=1.0976243019104004
I0304 00:42:51.017165 140380326586112 logging_writer.py:48] [277300] global_step=277300, grad_norm=3.148198366165161, loss=1.117258906364441
I0304 00:43:35.993492 140380334978816 logging_writer.py:48] [277400] global_step=277400, grad_norm=3.364001512527466, loss=3.0805134773254395
I0304 00:44:20.956364 140380326586112 logging_writer.py:48] [277500] global_step=277500, grad_norm=2.928980827331543, loss=1.025033950805664
I0304 00:45:05.989415 140380334978816 logging_writer.py:48] [277600] global_step=277600, grad_norm=4.350588798522949, loss=3.394131898880005
I0304 00:45:50.863936 140380326586112 logging_writer.py:48] [277700] global_step=277700, grad_norm=3.1391842365264893, loss=1.0780001878738403
I0304 00:46:36.264035 140380334978816 logging_writer.py:48] [277800] global_step=277800, grad_norm=4.153687477111816, loss=3.041471004486084
I0304 00:46:45.398047 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:46:56.012219 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:47:19.695653 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:47:21.340584 140575196817216 submission_runner.py:411] Time since start: 134016.94s, 	Step: 277822, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.4123508036136627, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 123559.36771154404, 'total_duration': 134016.9411432743, 'accumulated_submission_time': 123559.36771154404, 'accumulated_eval_time': 10426.705643177032, 'accumulated_logging_time': 16.121344804763794}
I0304 00:47:21.399500 140380326586112 logging_writer.py:48] [277822] accumulated_eval_time=10426.705643, accumulated_logging_time=16.121345, accumulated_submission_time=123559.367712, global_step=277822, preemption_count=0, score=123559.367712, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=134016.941143, train/accuracy=0.888633, train/loss=0.412351, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:47:52.801616 140380334978816 logging_writer.py:48] [277900] global_step=277900, grad_norm=2.9118540287017822, loss=1.375198483467102
I0304 00:48:36.900947 140380326586112 logging_writer.py:48] [278000] global_step=278000, grad_norm=2.989888906478882, loss=1.251075029373169
I0304 00:49:22.070152 140380334978816 logging_writer.py:48] [278100] global_step=278100, grad_norm=3.142235040664673, loss=1.0948424339294434
I0304 00:50:07.498132 140380326586112 logging_writer.py:48] [278200] global_step=278200, grad_norm=3.126202344894409, loss=1.208714246749878
I0304 00:50:52.473809 140380334978816 logging_writer.py:48] [278300] global_step=278300, grad_norm=3.4538018703460693, loss=1.1247669458389282
I0304 00:51:37.402958 140380326586112 logging_writer.py:48] [278400] global_step=278400, grad_norm=3.066554307937622, loss=2.6447417736053467
I0304 00:52:22.210045 140380334978816 logging_writer.py:48] [278500] global_step=278500, grad_norm=3.159195899963379, loss=1.0833948850631714
I0304 00:53:07.034828 140380326586112 logging_writer.py:48] [278600] global_step=278600, grad_norm=3.2816262245178223, loss=1.1199871301651
I0304 00:53:52.180720 140380334978816 logging_writer.py:48] [278700] global_step=278700, grad_norm=3.1219875812530518, loss=1.0845000743865967
I0304 00:54:22.231578 140575196817216 spec.py:321] Evaluating on the training split.
I0304 00:54:32.952348 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 00:54:55.953432 140575196817216 spec.py:349] Evaluating on the test split.
I0304 00:54:57.608589 140575196817216 submission_runner.py:411] Time since start: 134473.21s, 	Step: 278767, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.42213332653045654, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 123980.14164686203, 'total_duration': 134473.20946788788, 'accumulated_submission_time': 123980.14164686203, 'accumulated_eval_time': 10462.081463813782, 'accumulated_logging_time': 16.189631462097168}
I0304 00:54:57.666602 140380326586112 logging_writer.py:48] [278767] accumulated_eval_time=10462.081464, accumulated_logging_time=16.189631, accumulated_submission_time=123980.141647, global_step=278767, preemption_count=0, score=123980.141647, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=134473.209468, train/accuracy=0.887852, train/loss=0.422133, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 00:55:11.178016 140380334978816 logging_writer.py:48] [278800] global_step=278800, grad_norm=3.16982364654541, loss=1.1730395555496216
I0304 00:55:52.877393 140380326586112 logging_writer.py:48] [278900] global_step=278900, grad_norm=3.3907597064971924, loss=1.1636391878128052
I0304 00:56:37.998698 140380334978816 logging_writer.py:48] [279000] global_step=279000, grad_norm=3.411444902420044, loss=2.2827939987182617
I0304 00:57:23.306525 140380326586112 logging_writer.py:48] [279100] global_step=279100, grad_norm=3.180013418197632, loss=1.3271381855010986
I0304 00:58:08.407788 140380334978816 logging_writer.py:48] [279200] global_step=279200, grad_norm=3.1349692344665527, loss=1.5168704986572266
I0304 00:58:53.540643 140380326586112 logging_writer.py:48] [279300] global_step=279300, grad_norm=3.335175037384033, loss=2.448237895965576
I0304 00:59:38.801090 140380334978816 logging_writer.py:48] [279400] global_step=279400, grad_norm=3.1652474403381348, loss=1.1563760042190552
I0304 01:00:23.694304 140380326586112 logging_writer.py:48] [279500] global_step=279500, grad_norm=3.2398600578308105, loss=1.1789991855621338
I0304 01:01:09.047252 140380334978816 logging_writer.py:48] [279600] global_step=279600, grad_norm=3.741166114807129, loss=1.1357612609863281
I0304 01:01:54.009982 140380326586112 logging_writer.py:48] [279700] global_step=279700, grad_norm=3.3128955364227295, loss=1.1833961009979248
I0304 01:01:57.774772 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:02:08.570636 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:02:32.147310 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:02:33.803968 140575196817216 submission_runner.py:411] Time since start: 134929.40s, 	Step: 279710, 	{'train/accuracy': 0.8892382383346558, 'train/loss': 0.41321295499801636, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 124400.19126391411, 'total_duration': 134929.40495729446, 'accumulated_submission_time': 124400.19126391411, 'accumulated_eval_time': 10498.109572172165, 'accumulated_logging_time': 16.257033348083496}
I0304 01:02:33.874804 140380334978816 logging_writer.py:48] [279710] accumulated_eval_time=10498.109572, accumulated_logging_time=16.257033, accumulated_submission_time=124400.191264, global_step=279710, preemption_count=0, score=124400.191264, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=134929.404957, train/accuracy=0.889238, train/loss=0.413213, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:03:10.084269 140380326586112 logging_writer.py:48] [279800] global_step=279800, grad_norm=3.0740609169006348, loss=2.7146589756011963
I0304 01:03:55.022570 140380334978816 logging_writer.py:48] [279900] global_step=279900, grad_norm=4.022069931030273, loss=1.529999852180481
I0304 01:04:40.410693 140380326586112 logging_writer.py:48] [280000] global_step=280000, grad_norm=3.5781302452087402, loss=3.181459426879883
I0304 01:05:25.435098 140380334978816 logging_writer.py:48] [280100] global_step=280100, grad_norm=4.593883991241455, loss=3.0571494102478027
I0304 01:06:10.269164 140380326586112 logging_writer.py:48] [280200] global_step=280200, grad_norm=3.2831408977508545, loss=1.118091106414795
I0304 01:06:55.795932 140380334978816 logging_writer.py:48] [280300] global_step=280300, grad_norm=3.1856417655944824, loss=1.1245595216751099
I0304 01:07:40.698566 140380326586112 logging_writer.py:48] [280400] global_step=280400, grad_norm=4.43573522567749, loss=3.159790277481079
I0304 01:08:25.790861 140380334978816 logging_writer.py:48] [280500] global_step=280500, grad_norm=3.1695799827575684, loss=1.0252004861831665
I0304 01:09:10.814598 140380326586112 logging_writer.py:48] [280600] global_step=280600, grad_norm=3.092987060546875, loss=1.3607486486434937
I0304 01:09:33.984829 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:09:44.754933 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:10:10.793922 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:10:12.454098 140575196817216 submission_runner.py:411] Time since start: 135388.06s, 	Step: 280653, 	{'train/accuracy': 0.8885741829872131, 'train/loss': 0.4136310815811157, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 124820.24197268486, 'total_duration': 135388.0551865101, 'accumulated_submission_time': 124820.24197268486, 'accumulated_eval_time': 10536.577868461609, 'accumulated_logging_time': 16.33836817741394}
I0304 01:10:12.511468 140380334978816 logging_writer.py:48] [280653] accumulated_eval_time=10536.577868, accumulated_logging_time=16.338368, accumulated_submission_time=124820.241973, global_step=280653, preemption_count=0, score=124820.241973, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=135388.055187, train/accuracy=0.888574, train/loss=0.413631, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:10:31.594089 140380326586112 logging_writer.py:48] [280700] global_step=280700, grad_norm=2.9036483764648438, loss=1.0260562896728516
I0304 01:11:14.115331 140380334978816 logging_writer.py:48] [280800] global_step=280800, grad_norm=2.887171983718872, loss=1.1030036211013794
I0304 01:11:59.318070 140380326586112 logging_writer.py:48] [280900] global_step=280900, grad_norm=3.3656013011932373, loss=1.6078388690948486
I0304 01:12:44.417542 140380334978816 logging_writer.py:48] [281000] global_step=281000, grad_norm=3.182915449142456, loss=1.332576036453247
I0304 01:13:29.661278 140380326586112 logging_writer.py:48] [281100] global_step=281100, grad_norm=3.2608461380004883, loss=1.1519404649734497
I0304 01:14:14.858845 140380334978816 logging_writer.py:48] [281200] global_step=281200, grad_norm=3.231999397277832, loss=1.1977522373199463
I0304 01:15:00.098352 140380326586112 logging_writer.py:48] [281300] global_step=281300, grad_norm=2.9022111892700195, loss=1.271393060684204
I0304 01:15:45.385685 140380334978816 logging_writer.py:48] [281400] global_step=281400, grad_norm=3.144561290740967, loss=1.1544182300567627
I0304 01:16:30.576396 140380326586112 logging_writer.py:48] [281500] global_step=281500, grad_norm=3.211956739425659, loss=1.0661852359771729
I0304 01:17:12.516174 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:17:23.309013 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:17:47.984065 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:17:49.652646 140575196817216 submission_runner.py:411] Time since start: 135845.25s, 	Step: 281595, 	{'train/accuracy': 0.8893163800239563, 'train/loss': 0.4133542776107788, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 125240.18860721588, 'total_duration': 135845.2536330223, 'accumulated_submission_time': 125240.18860721588, 'accumulated_eval_time': 10573.71326136589, 'accumulated_logging_time': 16.405822038650513}
I0304 01:17:49.712138 140380334978816 logging_writer.py:48] [281595] accumulated_eval_time=10573.713261, accumulated_logging_time=16.405822, accumulated_submission_time=125240.188607, global_step=281595, preemption_count=0, score=125240.188607, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=135845.253633, train/accuracy=0.889316, train/loss=0.413354, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:17:52.111679 140380326586112 logging_writer.py:48] [281600] global_step=281600, grad_norm=4.197893142700195, loss=3.1042075157165527
I0304 01:18:32.442472 140380334978816 logging_writer.py:48] [281700] global_step=281700, grad_norm=3.6883022785186768, loss=2.9999730587005615
I0304 01:19:17.435178 140380326586112 logging_writer.py:48] [281800] global_step=281800, grad_norm=3.3765575885772705, loss=1.0936343669891357
I0304 01:20:02.504333 140380334978816 logging_writer.py:48] [281900] global_step=281900, grad_norm=3.1664116382598877, loss=1.422577142715454
I0304 01:20:47.480732 140380326586112 logging_writer.py:48] [282000] global_step=282000, grad_norm=5.2780632972717285, loss=3.3221487998962402
I0304 01:21:32.368188 140380334978816 logging_writer.py:48] [282100] global_step=282100, grad_norm=6.164592266082764, loss=2.9487836360931396
I0304 01:22:17.425039 140380326586112 logging_writer.py:48] [282200] global_step=282200, grad_norm=3.0341315269470215, loss=1.7629449367523193
I0304 01:23:02.177204 140380334978816 logging_writer.py:48] [282300] global_step=282300, grad_norm=3.0047409534454346, loss=1.488173246383667
I0304 01:23:47.163625 140380326586112 logging_writer.py:48] [282400] global_step=282400, grad_norm=3.019195318222046, loss=1.174485206604004
I0304 01:24:31.995406 140380334978816 logging_writer.py:48] [282500] global_step=282500, grad_norm=3.1861495971679688, loss=1.3151729106903076
I0304 01:24:50.085597 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:25:01.688323 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:25:27.469074 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:25:29.125669 140575196817216 submission_runner.py:411] Time since start: 136304.73s, 	Step: 282542, 	{'train/accuracy': 0.8870312571525574, 'train/loss': 0.4205176830291748, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 125660.50177502632, 'total_duration': 136304.7267510891, 'accumulated_submission_time': 125660.50177502632, 'accumulated_eval_time': 10612.752341747284, 'accumulated_logging_time': 16.47682547569275}
I0304 01:25:29.183302 140380326586112 logging_writer.py:48] [282542] accumulated_eval_time=10612.752342, accumulated_logging_time=16.476825, accumulated_submission_time=125660.501775, global_step=282542, preemption_count=0, score=125660.501775, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=136304.726751, train/accuracy=0.887031, train/loss=0.420518, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:25:52.627960 140380334978816 logging_writer.py:48] [282600] global_step=282600, grad_norm=2.9978156089782715, loss=1.0739505290985107
I0304 01:26:35.716041 140380326586112 logging_writer.py:48] [282700] global_step=282700, grad_norm=2.9557220935821533, loss=1.8379178047180176
I0304 01:27:20.828392 140380334978816 logging_writer.py:48] [282800] global_step=282800, grad_norm=3.3699569702148438, loss=1.2504407167434692
I0304 01:28:06.071069 140380326586112 logging_writer.py:48] [282900] global_step=282900, grad_norm=3.3135697841644287, loss=2.0433406829833984
I0304 01:28:50.964615 140380334978816 logging_writer.py:48] [283000] global_step=283000, grad_norm=3.1490275859832764, loss=1.0778425931930542
I0304 01:29:36.017431 140380326586112 logging_writer.py:48] [283100] global_step=283100, grad_norm=3.1238601207733154, loss=2.3802919387817383
I0304 01:30:21.137902 140380334978816 logging_writer.py:48] [283200] global_step=283200, grad_norm=3.507420539855957, loss=1.1127636432647705
I0304 01:31:06.088870 140380326586112 logging_writer.py:48] [283300] global_step=283300, grad_norm=3.1235573291778564, loss=1.836413860321045
I0304 01:31:51.146794 140380334978816 logging_writer.py:48] [283400] global_step=283400, grad_norm=3.3416154384613037, loss=1.3829952478408813
I0304 01:32:29.502499 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:32:40.222687 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:33:05.601130 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:33:07.256275 140575196817216 submission_runner.py:411] Time since start: 136762.86s, 	Step: 283487, 	{'train/accuracy': 0.8861523270606995, 'train/loss': 0.42186465859413147, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 126080.76248979568, 'total_duration': 136762.85723400116, 'accumulated_submission_time': 126080.76248979568, 'accumulated_eval_time': 10650.505030870438, 'accumulated_logging_time': 16.544782161712646}
I0304 01:33:07.315413 140380326586112 logging_writer.py:48] [283487] accumulated_eval_time=10650.505031, accumulated_logging_time=16.544782, accumulated_submission_time=126080.762490, global_step=283487, preemption_count=0, score=126080.762490, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=136762.857234, train/accuracy=0.886152, train/loss=0.421865, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:33:12.883034 140380334978816 logging_writer.py:48] [283500] global_step=283500, grad_norm=2.867363691329956, loss=1.497880458831787
I0304 01:33:54.026856 140380326586112 logging_writer.py:48] [283600] global_step=283600, grad_norm=4.58803653717041, loss=2.96661376953125
I0304 01:34:38.948479 140380334978816 logging_writer.py:48] [283700] global_step=283700, grad_norm=3.4128427505493164, loss=3.106224298477173
I0304 01:35:24.199195 140380326586112 logging_writer.py:48] [283800] global_step=283800, grad_norm=2.7576606273651123, loss=1.5814683437347412
I0304 01:36:09.374412 140380334978816 logging_writer.py:48] [283900] global_step=283900, grad_norm=3.0541815757751465, loss=1.4956153631210327
I0304 01:36:54.543955 140380326586112 logging_writer.py:48] [284000] global_step=284000, grad_norm=3.6278867721557617, loss=1.1894627809524536
I0304 01:37:39.400098 140380334978816 logging_writer.py:48] [284100] global_step=284100, grad_norm=3.1418395042419434, loss=1.4241788387298584
I0304 01:38:24.170539 140380326586112 logging_writer.py:48] [284200] global_step=284200, grad_norm=4.116335391998291, loss=3.1712558269500732
I0304 01:39:09.294549 140380334978816 logging_writer.py:48] [284300] global_step=284300, grad_norm=3.1644585132598877, loss=1.3950728178024292
I0304 01:39:54.313268 140380326586112 logging_writer.py:48] [284400] global_step=284400, grad_norm=3.1613588333129883, loss=1.2062809467315674
I0304 01:40:07.542087 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:40:18.213724 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:40:42.339426 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:40:44.003868 140575196817216 submission_runner.py:411] Time since start: 137219.60s, 	Step: 284431, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.419499933719635, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 126500.92922019958, 'total_duration': 137219.60481786728, 'accumulated_submission_time': 126500.92922019958, 'accumulated_eval_time': 10686.965690135956, 'accumulated_logging_time': 16.61560034751892}
I0304 01:40:44.073528 140380334978816 logging_writer.py:48] [284431] accumulated_eval_time=10686.965690, accumulated_logging_time=16.615600, accumulated_submission_time=126500.929220, global_step=284431, preemption_count=0, score=126500.929220, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=137219.604818, train/accuracy=0.887852, train/loss=0.419500, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:41:11.914088 140380326586112 logging_writer.py:48] [284500] global_step=284500, grad_norm=3.253319263458252, loss=1.1707088947296143
I0304 01:41:55.576724 140380334978816 logging_writer.py:48] [284600] global_step=284600, grad_norm=4.256231784820557, loss=3.2842087745666504
I0304 01:42:40.556988 140380326586112 logging_writer.py:48] [284700] global_step=284700, grad_norm=3.112306594848633, loss=1.3266022205352783
I0304 01:43:25.473952 140380334978816 logging_writer.py:48] [284800] global_step=284800, grad_norm=3.2191765308380127, loss=2.6920576095581055
I0304 01:44:10.377000 140380326586112 logging_writer.py:48] [284900] global_step=284900, grad_norm=3.09566330909729, loss=1.1078615188598633
I0304 01:44:55.506607 140380334978816 logging_writer.py:48] [285000] global_step=285000, grad_norm=3.246534585952759, loss=2.474527359008789
I0304 01:45:40.306447 140380326586112 logging_writer.py:48] [285100] global_step=285100, grad_norm=3.415332555770874, loss=2.3632431030273438
I0304 01:46:25.232092 140380334978816 logging_writer.py:48] [285200] global_step=285200, grad_norm=3.126901388168335, loss=1.3075065612792969
I0304 01:47:10.522912 140380326586112 logging_writer.py:48] [285300] global_step=285300, grad_norm=3.189544200897217, loss=2.446375846862793
I0304 01:47:44.139451 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:47:54.635027 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:48:16.590260 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:48:18.240145 140575196817216 submission_runner.py:411] Time since start: 137673.84s, 	Step: 285377, 	{'train/accuracy': 0.8895312547683716, 'train/loss': 0.4159430265426636, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 126920.93151402473, 'total_duration': 137673.84118700027, 'accumulated_submission_time': 126920.93151402473, 'accumulated_eval_time': 10721.06535768509, 'accumulated_logging_time': 16.69960618019104}
I0304 01:48:18.298544 140380334978816 logging_writer.py:48] [285377] accumulated_eval_time=10721.065358, accumulated_logging_time=16.699606, accumulated_submission_time=126920.931514, global_step=285377, preemption_count=0, score=126920.931514, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=137673.841187, train/accuracy=0.889531, train/loss=0.415943, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:48:27.847472 140380326586112 logging_writer.py:48] [285400] global_step=285400, grad_norm=3.3338875770568848, loss=1.7751542329788208
I0304 01:49:09.233666 140380334978816 logging_writer.py:48] [285500] global_step=285500, grad_norm=3.4259979724884033, loss=1.1456208229064941
I0304 01:49:54.321057 140380326586112 logging_writer.py:48] [285600] global_step=285600, grad_norm=3.0097815990448, loss=1.330331802368164
I0304 01:50:39.618898 140380334978816 logging_writer.py:48] [285700] global_step=285700, grad_norm=2.920297861099243, loss=1.1581635475158691
I0304 01:51:24.487583 140380326586112 logging_writer.py:48] [285800] global_step=285800, grad_norm=2.9221627712249756, loss=1.6391515731811523
I0304 01:52:09.562694 140380334978816 logging_writer.py:48] [285900] global_step=285900, grad_norm=3.0854783058166504, loss=2.0752525329589844
I0304 01:52:54.676769 140380326586112 logging_writer.py:48] [286000] global_step=286000, grad_norm=3.556450366973877, loss=1.3730106353759766
I0304 01:53:39.435441 140380334978816 logging_writer.py:48] [286100] global_step=286100, grad_norm=3.16160249710083, loss=1.1058586835861206
I0304 01:54:24.448356 140380326586112 logging_writer.py:48] [286200] global_step=286200, grad_norm=3.1886699199676514, loss=1.1776982545852661
I0304 01:55:09.542387 140380334978816 logging_writer.py:48] [286300] global_step=286300, grad_norm=3.536761999130249, loss=1.0890990495681763
I0304 01:55:18.249556 140575196817216 spec.py:321] Evaluating on the training split.
I0304 01:55:28.901714 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 01:55:55.637927 140575196817216 spec.py:349] Evaluating on the test split.
I0304 01:55:57.292101 140575196817216 submission_runner.py:411] Time since start: 138132.89s, 	Step: 286321, 	{'train/accuracy': 0.88818359375, 'train/loss': 0.42066046595573425, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 127340.82446813583, 'total_duration': 138132.89316129684, 'accumulated_submission_time': 127340.82446813583, 'accumulated_eval_time': 10760.106906414032, 'accumulated_logging_time': 16.767632246017456}
I0304 01:55:57.349424 140380326586112 logging_writer.py:48] [286321] accumulated_eval_time=10760.106906, accumulated_logging_time=16.767632, accumulated_submission_time=127340.824468, global_step=286321, preemption_count=0, score=127340.824468, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=138132.893161, train/accuracy=0.888184, train/loss=0.420660, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 01:56:29.147505 140380334978816 logging_writer.py:48] [286400] global_step=286400, grad_norm=3.391371726989746, loss=2.7294774055480957
I0304 01:57:13.604577 140380326586112 logging_writer.py:48] [286500] global_step=286500, grad_norm=3.088928699493408, loss=1.3278417587280273
I0304 01:57:58.489113 140380334978816 logging_writer.py:48] [286600] global_step=286600, grad_norm=2.9376022815704346, loss=1.3508235216140747
I0304 01:58:43.738355 140380326586112 logging_writer.py:48] [286700] global_step=286700, grad_norm=2.873917818069458, loss=1.1945704221725464
I0304 01:59:28.747549 140380334978816 logging_writer.py:48] [286800] global_step=286800, grad_norm=3.1287894248962402, loss=2.228559732437134
I0304 02:00:13.827728 140380326586112 logging_writer.py:48] [286900] global_step=286900, grad_norm=3.1653966903686523, loss=1.0314064025878906
I0304 02:00:58.660599 140380334978816 logging_writer.py:48] [287000] global_step=287000, grad_norm=3.1998298168182373, loss=1.449531078338623
I0304 02:01:43.955509 140380326586112 logging_writer.py:48] [287100] global_step=287100, grad_norm=3.60361385345459, loss=2.9505436420440674
I0304 02:02:29.106992 140380334978816 logging_writer.py:48] [287200] global_step=287200, grad_norm=6.106639385223389, loss=3.0561978816986084
I0304 02:02:57.598201 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:03:08.367574 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:03:32.929713 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:03:34.593570 140575196817216 submission_runner.py:411] Time since start: 138590.19s, 	Step: 287265, 	{'train/accuracy': 0.8854101300239563, 'train/loss': 0.42358577251434326, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 127761.01388812065, 'total_duration': 138590.194170475, 'accumulated_submission_time': 127761.01388812065, 'accumulated_eval_time': 10797.100801467896, 'accumulated_logging_time': 16.835665464401245}
I0304 02:03:34.653894 140380326586112 logging_writer.py:48] [287265] accumulated_eval_time=10797.100801, accumulated_logging_time=16.835665, accumulated_submission_time=127761.013888, global_step=287265, preemption_count=0, score=127761.013888, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=138590.194170, train/accuracy=0.885410, train/loss=0.423586, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:03:49.254200 140380334978816 logging_writer.py:48] [287300] global_step=287300, grad_norm=3.192300796508789, loss=1.157373309135437
I0304 02:04:31.062518 140380326586112 logging_writer.py:48] [287400] global_step=287400, grad_norm=3.084209680557251, loss=1.1026233434677124
I0304 02:05:15.968891 140380334978816 logging_writer.py:48] [287500] global_step=287500, grad_norm=2.996899127960205, loss=1.088398814201355
I0304 02:06:01.260193 140380326586112 logging_writer.py:48] [287600] global_step=287600, grad_norm=2.9118852615356445, loss=0.9997897148132324
I0304 02:06:46.039272 140380334978816 logging_writer.py:48] [287700] global_step=287700, grad_norm=3.123131275177002, loss=1.1022961139678955
I0304 02:07:31.484608 140380326586112 logging_writer.py:48] [287800] global_step=287800, grad_norm=3.043006420135498, loss=1.1376036405563354
I0304 02:08:16.762572 140380334978816 logging_writer.py:48] [287900] global_step=287900, grad_norm=3.4989125728607178, loss=1.0004054307937622
I0304 02:09:01.783332 140380326586112 logging_writer.py:48] [288000] global_step=288000, grad_norm=2.9430177211761475, loss=1.3007071018218994
I0304 02:09:46.890713 140380334978816 logging_writer.py:48] [288100] global_step=288100, grad_norm=3.365288734436035, loss=1.2071421146392822
I0304 02:10:32.101469 140380326586112 logging_writer.py:48] [288200] global_step=288200, grad_norm=3.5959312915802, loss=2.9365625381469727
I0304 02:10:34.967753 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:10:45.877010 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:11:11.454290 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:11:13.102121 140575196817216 submission_runner.py:411] Time since start: 139048.70s, 	Step: 288208, 	{'train/accuracy': 0.8864648342132568, 'train/loss': 0.42087045311927795, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 128181.00546360016, 'total_duration': 139048.703029871, 'accumulated_submission_time': 128181.00546360016, 'accumulated_eval_time': 10835.234018087387, 'accumulated_logging_time': 17.169645071029663}
I0304 02:11:13.161350 140380334978816 logging_writer.py:48] [288208] accumulated_eval_time=10835.234018, accumulated_logging_time=17.169645, accumulated_submission_time=128181.005464, global_step=288208, preemption_count=0, score=128181.005464, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=139048.703030, train/accuracy=0.886465, train/loss=0.420870, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:11:50.441521 140380326586112 logging_writer.py:48] [288300] global_step=288300, grad_norm=3.3649182319641113, loss=1.546940803527832
I0304 02:12:35.500938 140380334978816 logging_writer.py:48] [288400] global_step=288400, grad_norm=2.963183879852295, loss=1.0394220352172852
I0304 02:13:20.812732 140380326586112 logging_writer.py:48] [288500] global_step=288500, grad_norm=3.1530489921569824, loss=1.8258854150772095
I0304 02:14:06.332355 140380334978816 logging_writer.py:48] [288600] global_step=288600, grad_norm=2.9342026710510254, loss=1.665895700454712
I0304 02:14:51.255261 140380326586112 logging_writer.py:48] [288700] global_step=288700, grad_norm=3.3600709438323975, loss=1.3579254150390625
I0304 02:15:36.421653 140380334978816 logging_writer.py:48] [288800] global_step=288800, grad_norm=3.3246662616729736, loss=1.2458306550979614
I0304 02:16:21.747651 140380326586112 logging_writer.py:48] [288900] global_step=288900, grad_norm=2.88702130317688, loss=1.1681609153747559
I0304 02:17:07.047936 140380334978816 logging_writer.py:48] [289000] global_step=289000, grad_norm=3.3556201457977295, loss=1.3576288223266602
I0304 02:17:52.501076 140380326586112 logging_writer.py:48] [289100] global_step=289100, grad_norm=3.0311906337738037, loss=2.3660409450531006
I0304 02:18:13.266782 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:18:24.146615 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:18:50.672564 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:18:52.326439 140575196817216 submission_runner.py:411] Time since start: 139507.93s, 	Step: 289147, 	{'train/accuracy': 0.8867773413658142, 'train/loss': 0.4219215214252472, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 128601.05171775818, 'total_duration': 139507.9273519516, 'accumulated_submission_time': 128601.05171775818, 'accumulated_eval_time': 10874.292521953583, 'accumulated_logging_time': 17.23930835723877}
I0304 02:18:52.393328 140380334978816 logging_writer.py:48] [289147] accumulated_eval_time=10874.292522, accumulated_logging_time=17.239308, accumulated_submission_time=128601.051718, global_step=289147, preemption_count=0, score=128601.051718, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=139507.927352, train/accuracy=0.886777, train/loss=0.421922, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:19:13.870900 140380326586112 logging_writer.py:48] [289200] global_step=289200, grad_norm=3.28018856048584, loss=1.139831304550171
I0304 02:19:56.834190 140380334978816 logging_writer.py:48] [289300] global_step=289300, grad_norm=3.3393197059631348, loss=1.1905765533447266
I0304 02:20:42.181529 140380326586112 logging_writer.py:48] [289400] global_step=289400, grad_norm=3.6478166580200195, loss=3.048062801361084
I0304 02:21:27.286877 140380334978816 logging_writer.py:48] [289500] global_step=289500, grad_norm=3.2272047996520996, loss=2.695371627807617
I0304 02:22:12.348040 140380326586112 logging_writer.py:48] [289600] global_step=289600, grad_norm=3.2785391807556152, loss=2.592261791229248
I0304 02:22:57.430233 140380334978816 logging_writer.py:48] [289700] global_step=289700, grad_norm=3.208883762359619, loss=2.9213764667510986
I0304 02:23:42.450154 140380326586112 logging_writer.py:48] [289800] global_step=289800, grad_norm=3.9072306156158447, loss=2.7798032760620117
I0304 02:24:27.634504 140380334978816 logging_writer.py:48] [289900] global_step=289900, grad_norm=2.9990832805633545, loss=1.8910956382751465
I0304 02:25:12.595394 140380326586112 logging_writer.py:48] [290000] global_step=290000, grad_norm=3.560652732849121, loss=2.9082162380218506
I0304 02:25:52.661102 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:26:03.635357 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:26:27.031850 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:26:28.685912 140575196817216 submission_runner.py:411] Time since start: 139964.29s, 	Step: 290090, 	{'train/accuracy': 0.88734370470047, 'train/loss': 0.41239210963249207, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 129021.26099348068, 'total_duration': 139964.28707146645, 'accumulated_submission_time': 129021.26099348068, 'accumulated_eval_time': 10910.316421985626, 'accumulated_logging_time': 17.317003965377808}
I0304 02:26:28.746294 140380334978816 logging_writer.py:48] [290090] accumulated_eval_time=10910.316422, accumulated_logging_time=17.317004, accumulated_submission_time=129021.260993, global_step=290090, preemption_count=0, score=129021.260993, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=139964.287071, train/accuracy=0.887344, train/loss=0.412392, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:26:33.116478 140380326586112 logging_writer.py:48] [290100] global_step=290100, grad_norm=3.8602254390716553, loss=3.236624002456665
I0304 02:27:13.955462 140380334978816 logging_writer.py:48] [290200] global_step=290200, grad_norm=4.085372447967529, loss=3.066222906112671
I0304 02:27:59.014261 140380326586112 logging_writer.py:48] [290300] global_step=290300, grad_norm=3.1782751083374023, loss=1.3687041997909546
I0304 02:28:44.208850 140380334978816 logging_writer.py:48] [290400] global_step=290400, grad_norm=3.148678779602051, loss=1.763068437576294
I0304 02:29:29.690027 140380326586112 logging_writer.py:48] [290500] global_step=290500, grad_norm=2.9287493228912354, loss=1.4998130798339844
I0304 02:30:14.546654 140380334978816 logging_writer.py:48] [290600] global_step=290600, grad_norm=3.246497392654419, loss=1.0642080307006836
I0304 02:30:59.562629 140380326586112 logging_writer.py:48] [290700] global_step=290700, grad_norm=3.092902660369873, loss=1.0648281574249268
I0304 02:31:44.612086 140380334978816 logging_writer.py:48] [290800] global_step=290800, grad_norm=3.474454164505005, loss=2.788576126098633
I0304 02:32:29.823388 140380326586112 logging_writer.py:48] [290900] global_step=290900, grad_norm=3.140935182571411, loss=1.2279160022735596
I0304 02:33:14.828324 140380334978816 logging_writer.py:48] [291000] global_step=291000, grad_norm=3.2056074142456055, loss=2.8663315773010254
I0304 02:33:28.792943 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:33:39.444005 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:34:03.254209 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:34:04.921032 140575196817216 submission_runner.py:411] Time since start: 140420.52s, 	Step: 291033, 	{'train/accuracy': 0.8869140148162842, 'train/loss': 0.4278862178325653, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 129441.24899697304, 'total_duration': 140420.52204823494, 'accumulated_submission_time': 129441.24899697304, 'accumulated_eval_time': 10946.443460464478, 'accumulated_logging_time': 17.38748025894165}
I0304 02:34:04.994524 140380326586112 logging_writer.py:48] [291033] accumulated_eval_time=10946.443460, accumulated_logging_time=17.387480, accumulated_submission_time=129441.248997, global_step=291033, preemption_count=0, score=129441.248997, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=140420.522048, train/accuracy=0.886914, train/loss=0.427886, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:34:32.023920 140380334978816 logging_writer.py:48] [291100] global_step=291100, grad_norm=3.01627779006958, loss=1.1717689037322998
I0304 02:35:15.364355 140380326586112 logging_writer.py:48] [291200] global_step=291200, grad_norm=3.6583220958709717, loss=1.230733871459961
I0304 02:36:00.858894 140380334978816 logging_writer.py:48] [291300] global_step=291300, grad_norm=3.1679210662841797, loss=1.1868976354599
I0304 02:36:46.041554 140380326586112 logging_writer.py:48] [291400] global_step=291400, grad_norm=4.035463333129883, loss=3.1799063682556152
I0304 02:37:31.021233 140380334978816 logging_writer.py:48] [291500] global_step=291500, grad_norm=3.801288366317749, loss=3.103116035461426
I0304 02:38:16.402029 140380326586112 logging_writer.py:48] [291600] global_step=291600, grad_norm=3.046403169631958, loss=1.1889740228652954
I0304 02:39:01.442245 140380334978816 logging_writer.py:48] [291700] global_step=291700, grad_norm=3.1146786212921143, loss=1.8884950876235962
I0304 02:39:46.676169 140380326586112 logging_writer.py:48] [291800] global_step=291800, grad_norm=3.10685396194458, loss=1.7256799936294556
I0304 02:40:31.757419 140380334978816 logging_writer.py:48] [291900] global_step=291900, grad_norm=2.9725770950317383, loss=2.236605644226074
I0304 02:41:04.984589 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:41:15.745166 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:41:40.646532 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:41:42.326254 140575196817216 submission_runner.py:411] Time since start: 140877.93s, 	Step: 291975, 	{'train/accuracy': 0.885058581829071, 'train/loss': 0.42749667167663574, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 129861.1802175045, 'total_duration': 140877.92724633217, 'accumulated_submission_time': 129861.1802175045, 'accumulated_eval_time': 10983.784052610397, 'accumulated_logging_time': 17.47106671333313}
I0304 02:41:42.392585 140380326586112 logging_writer.py:48] [291975] accumulated_eval_time=10983.784053, accumulated_logging_time=17.471067, accumulated_submission_time=129861.180218, global_step=291975, preemption_count=0, score=129861.180218, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=140877.927246, train/accuracy=0.885059, train/loss=0.427497, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:41:52.734117 140380334978816 logging_writer.py:48] [292000] global_step=292000, grad_norm=3.1594038009643555, loss=1.7792589664459229
I0304 02:42:34.180661 140380326586112 logging_writer.py:48] [292100] global_step=292100, grad_norm=3.069551944732666, loss=1.805187463760376
I0304 02:43:19.220129 140380334978816 logging_writer.py:48] [292200] global_step=292200, grad_norm=3.1340434551239014, loss=1.2710394859313965
I0304 02:44:04.751204 140380326586112 logging_writer.py:48] [292300] global_step=292300, grad_norm=2.962684154510498, loss=1.109344720840454
I0304 02:44:49.542371 140380334978816 logging_writer.py:48] [292400] global_step=292400, grad_norm=2.920036554336548, loss=1.5393075942993164
I0304 02:45:34.896520 140380326586112 logging_writer.py:48] [292500] global_step=292500, grad_norm=3.08817720413208, loss=1.8136351108551025
I0304 02:46:19.830797 140380334978816 logging_writer.py:48] [292600] global_step=292600, grad_norm=3.054713010787964, loss=2.423473834991455
I0304 02:47:04.562010 140380326586112 logging_writer.py:48] [292700] global_step=292700, grad_norm=3.137002468109131, loss=1.1093125343322754
I0304 02:47:49.908504 140380334978816 logging_writer.py:48] [292800] global_step=292800, grad_norm=4.340343952178955, loss=3.1863279342651367
I0304 02:48:34.923497 140380326586112 logging_writer.py:48] [292900] global_step=292900, grad_norm=3.197720766067505, loss=1.0869598388671875
I0304 02:48:42.384990 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:48:53.116201 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:49:15.188875 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:49:16.853515 140575196817216 submission_runner.py:411] Time since start: 141332.45s, 	Step: 292918, 	{'train/accuracy': 0.8882812261581421, 'train/loss': 0.414492130279541, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 130281.11374497414, 'total_duration': 141332.45461821556, 'accumulated_submission_time': 130281.11374497414, 'accumulated_eval_time': 11018.251610517502, 'accumulated_logging_time': 17.548232555389404}
I0304 02:49:16.913110 140380334978816 logging_writer.py:48] [292918] accumulated_eval_time=11018.251611, accumulated_logging_time=17.548233, accumulated_submission_time=130281.113745, global_step=292918, preemption_count=0, score=130281.113745, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=141332.454618, train/accuracy=0.888281, train/loss=0.414492, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:49:49.900859 140380326586112 logging_writer.py:48] [293000] global_step=293000, grad_norm=3.1858110427856445, loss=2.432758331298828
I0304 02:50:34.444381 140380334978816 logging_writer.py:48] [293100] global_step=293100, grad_norm=3.2166635990142822, loss=1.00864839553833
I0304 02:51:19.551462 140380326586112 logging_writer.py:48] [293200] global_step=293200, grad_norm=3.0172119140625, loss=1.1400177478790283
I0304 02:52:04.573238 140380334978816 logging_writer.py:48] [293300] global_step=293300, grad_norm=3.860682487487793, loss=3.387068510055542
I0304 02:52:49.351984 140380326586112 logging_writer.py:48] [293400] global_step=293400, grad_norm=2.9774341583251953, loss=1.2120798826217651
I0304 02:53:34.411835 140380334978816 logging_writer.py:48] [293500] global_step=293500, grad_norm=3.2603540420532227, loss=2.8007264137268066
I0304 02:54:19.426755 140380326586112 logging_writer.py:48] [293600] global_step=293600, grad_norm=3.888017177581787, loss=3.2882025241851807
I0304 02:55:04.725332 140380334978816 logging_writer.py:48] [293700] global_step=293700, grad_norm=2.9952809810638428, loss=1.2868480682373047
I0304 02:55:49.726664 140380326586112 logging_writer.py:48] [293800] global_step=293800, grad_norm=3.109929084777832, loss=1.0084260702133179
I0304 02:56:17.212043 140575196817216 spec.py:321] Evaluating on the training split.
I0304 02:56:28.186485 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 02:56:52.537442 140575196817216 spec.py:349] Evaluating on the test split.
I0304 02:56:54.191375 140575196817216 submission_runner.py:411] Time since start: 141789.79s, 	Step: 293862, 	{'train/accuracy': 0.8871679306030273, 'train/loss': 0.4163171350955963, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 130701.35450816154, 'total_duration': 141789.79231905937, 'accumulated_submission_time': 130701.35450816154, 'accumulated_eval_time': 11055.229821681976, 'accumulated_logging_time': 17.617226600646973}
I0304 02:56:54.258715 140380334978816 logging_writer.py:48] [293862] accumulated_eval_time=11055.229822, accumulated_logging_time=17.617227, accumulated_submission_time=130701.354508, global_step=293862, preemption_count=0, score=130701.354508, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=141789.792319, train/accuracy=0.887168, train/loss=0.416317, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 02:57:09.759921 140380326586112 logging_writer.py:48] [293900] global_step=293900, grad_norm=3.2721986770629883, loss=2.6949613094329834
I0304 02:57:52.063678 140380334978816 logging_writer.py:48] [294000] global_step=294000, grad_norm=3.4932475090026855, loss=3.128251075744629
I0304 02:58:36.855263 140380326586112 logging_writer.py:48] [294100] global_step=294100, grad_norm=3.0351529121398926, loss=1.1130802631378174
I0304 02:59:21.802441 140380334978816 logging_writer.py:48] [294200] global_step=294200, grad_norm=3.113662004470825, loss=1.1487540006637573
I0304 03:00:06.912267 140380326586112 logging_writer.py:48] [294300] global_step=294300, grad_norm=3.862715482711792, loss=3.237212657928467
I0304 03:00:51.743095 140380334978816 logging_writer.py:48] [294400] global_step=294400, grad_norm=3.444415807723999, loss=1.1562713384628296
I0304 03:01:36.890035 140380326586112 logging_writer.py:48] [294500] global_step=294500, grad_norm=3.1216065883636475, loss=1.1879112720489502
I0304 03:02:21.975935 140380334978816 logging_writer.py:48] [294600] global_step=294600, grad_norm=2.8378336429595947, loss=1.154520869255066
I0304 03:03:07.122949 140380326586112 logging_writer.py:48] [294700] global_step=294700, grad_norm=3.206042766571045, loss=1.172947645187378
I0304 03:03:52.191654 140380334978816 logging_writer.py:48] [294800] global_step=294800, grad_norm=2.9807791709899902, loss=1.9341275691986084
I0304 03:03:54.589059 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:04:05.312817 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:04:28.969613 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:04:30.626678 140575196817216 submission_runner.py:411] Time since start: 142246.23s, 	Step: 294807, 	{'train/accuracy': 0.8866991996765137, 'train/loss': 0.425510048866272, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 131121.62486243248, 'total_duration': 142246.22771835327, 'accumulated_submission_time': 131121.62486243248, 'accumulated_eval_time': 11091.266440868378, 'accumulated_logging_time': 17.695829153060913}
I0304 03:04:30.685451 140380326586112 logging_writer.py:48] [294807] accumulated_eval_time=11091.266441, accumulated_logging_time=17.695829, accumulated_submission_time=131121.624862, global_step=294807, preemption_count=0, score=131121.624862, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=142246.227718, train/accuracy=0.886699, train/loss=0.425510, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:05:08.170887 140380334978816 logging_writer.py:48] [294900] global_step=294900, grad_norm=3.3787026405334473, loss=2.6478729248046875
I0304 03:05:52.912860 140380326586112 logging_writer.py:48] [295000] global_step=295000, grad_norm=3.0648155212402344, loss=1.6319820880889893
I0304 03:06:38.644727 140380334978816 logging_writer.py:48] [295100] global_step=295100, grad_norm=3.077838182449341, loss=1.61034095287323
I0304 03:07:23.680955 140380326586112 logging_writer.py:48] [295200] global_step=295200, grad_norm=3.057529926300049, loss=1.2427916526794434
I0304 03:08:09.064915 140380334978816 logging_writer.py:48] [295300] global_step=295300, grad_norm=3.3661324977874756, loss=1.3612502813339233
I0304 03:08:54.378961 140380326586112 logging_writer.py:48] [295400] global_step=295400, grad_norm=3.053802967071533, loss=1.2368205785751343
I0304 03:09:39.550531 140380334978816 logging_writer.py:48] [295500] global_step=295500, grad_norm=3.1047887802124023, loss=1.124693751335144
I0304 03:10:24.821177 140380326586112 logging_writer.py:48] [295600] global_step=295600, grad_norm=3.583409309387207, loss=1.3442646265029907
I0304 03:11:09.956584 140380334978816 logging_writer.py:48] [295700] global_step=295700, grad_norm=3.047437906265259, loss=1.6307734251022339
I0304 03:11:30.847115 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:11:41.601074 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:12:04.901223 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:12:06.561999 140575196817216 submission_runner.py:411] Time since start: 142702.16s, 	Step: 295748, 	{'train/accuracy': 0.8882812261581421, 'train/loss': 0.41996628046035767, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 131541.72653746605, 'total_duration': 142702.16307020187, 'accumulated_submission_time': 131541.72653746605, 'accumulated_eval_time': 11126.980338573456, 'accumulated_logging_time': 17.765344858169556}
I0304 03:12:06.625528 140380326586112 logging_writer.py:48] [295748] accumulated_eval_time=11126.980339, accumulated_logging_time=17.765345, accumulated_submission_time=131541.726537, global_step=295748, preemption_count=0, score=131541.726537, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=142702.163070, train/accuracy=0.888281, train/loss=0.419966, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:12:27.659951 140380334978816 logging_writer.py:48] [295800] global_step=295800, grad_norm=3.3725762367248535, loss=1.587095022201538
I0304 03:13:10.662992 140380326586112 logging_writer.py:48] [295900] global_step=295900, grad_norm=3.5223543643951416, loss=2.7611985206604004
I0304 03:13:55.785794 140380334978816 logging_writer.py:48] [296000] global_step=296000, grad_norm=3.179032325744629, loss=1.3129839897155762
I0304 03:14:40.977125 140380326586112 logging_writer.py:48] [296100] global_step=296100, grad_norm=3.004112720489502, loss=1.5380277633666992
I0304 03:15:25.925367 140380334978816 logging_writer.py:48] [296200] global_step=296200, grad_norm=3.105970859527588, loss=1.1084470748901367
I0304 03:16:11.265832 140380326586112 logging_writer.py:48] [296300] global_step=296300, grad_norm=2.860491991043091, loss=1.0945637226104736
I0304 03:16:56.111186 140380334978816 logging_writer.py:48] [296400] global_step=296400, grad_norm=3.573647975921631, loss=3.0350747108459473
I0304 03:17:41.093805 140380326586112 logging_writer.py:48] [296500] global_step=296500, grad_norm=3.3045601844787598, loss=2.02573823928833
I0304 03:18:26.572023 140380334978816 logging_writer.py:48] [296600] global_step=296600, grad_norm=3.064995765686035, loss=1.5392717123031616
I0304 03:19:06.899496 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:19:17.576400 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:19:43.277805 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:19:44.926129 140575196817216 submission_runner.py:411] Time since start: 143160.53s, 	Step: 296690, 	{'train/accuracy': 0.8873828053474426, 'train/loss': 0.4187077581882477, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 131961.94080877304, 'total_duration': 143160.52719926834, 'accumulated_submission_time': 131961.94080877304, 'accumulated_eval_time': 11165.005978107452, 'accumulated_logging_time': 17.840155839920044}
I0304 03:19:44.988692 140380326586112 logging_writer.py:48] [296690] accumulated_eval_time=11165.005978, accumulated_logging_time=17.840156, accumulated_submission_time=131961.940809, global_step=296690, preemption_count=0, score=131961.940809, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=143160.527199, train/accuracy=0.887383, train/loss=0.418708, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:19:49.371046 140380334978816 logging_writer.py:48] [296700] global_step=296700, grad_norm=4.031335830688477, loss=3.1567351818084717
I0304 03:20:29.957005 140380326586112 logging_writer.py:48] [296800] global_step=296800, grad_norm=3.9027576446533203, loss=3.236793041229248
I0304 03:21:14.771008 140380334978816 logging_writer.py:48] [296900] global_step=296900, grad_norm=3.9677534103393555, loss=3.1327195167541504
I0304 03:22:00.034341 140380326586112 logging_writer.py:48] [297000] global_step=297000, grad_norm=2.9397454261779785, loss=1.1097731590270996
I0304 03:22:44.754246 140380334978816 logging_writer.py:48] [297100] global_step=297100, grad_norm=2.974588394165039, loss=1.4467469453811646
I0304 03:23:29.639359 140380326586112 logging_writer.py:48] [297200] global_step=297200, grad_norm=3.2109992504119873, loss=1.894755482673645
I0304 03:24:14.725682 140380334978816 logging_writer.py:48] [297300] global_step=297300, grad_norm=3.0014376640319824, loss=1.1813501119613647
I0304 03:24:59.668045 140380326586112 logging_writer.py:48] [297400] global_step=297400, grad_norm=3.445953845977783, loss=2.647858142852783
I0304 03:25:44.671693 140380334978816 logging_writer.py:48] [297500] global_step=297500, grad_norm=3.143826961517334, loss=2.3352510929107666
I0304 03:26:30.027847 140380326586112 logging_writer.py:48] [297600] global_step=297600, grad_norm=3.1694674491882324, loss=1.164857268333435
I0304 03:26:45.319095 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:26:55.961195 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:27:17.937318 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:27:19.599712 140575196817216 submission_runner.py:411] Time since start: 143615.20s, 	Step: 297636, 	{'train/accuracy': 0.8857226371765137, 'train/loss': 0.42508581280708313, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 132382.2102355957, 'total_duration': 143615.20072078705, 'accumulated_submission_time': 132382.2102355957, 'accumulated_eval_time': 11199.285538434982, 'accumulated_logging_time': 17.914023637771606}
I0304 03:27:19.659996 140380334978816 logging_writer.py:48] [297636] accumulated_eval_time=11199.285538, accumulated_logging_time=17.914024, accumulated_submission_time=132382.210236, global_step=297636, preemption_count=0, score=132382.210236, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=143615.200721, train/accuracy=0.885723, train/loss=0.425086, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:27:45.506071 140380326586112 logging_writer.py:48] [297700] global_step=297700, grad_norm=3.0673458576202393, loss=1.1253408193588257
I0304 03:28:29.190977 140380334978816 logging_writer.py:48] [297800] global_step=297800, grad_norm=4.334130764007568, loss=1.1372997760772705
I0304 03:29:14.271323 140380326586112 logging_writer.py:48] [297900] global_step=297900, grad_norm=3.556642532348633, loss=2.5925512313842773
I0304 03:29:59.560436 140380334978816 logging_writer.py:48] [298000] global_step=298000, grad_norm=3.0705630779266357, loss=1.2095277309417725
I0304 03:30:44.365021 140380326586112 logging_writer.py:48] [298100] global_step=298100, grad_norm=3.1599771976470947, loss=1.6706821918487549
I0304 03:31:29.725875 140380334978816 logging_writer.py:48] [298200] global_step=298200, grad_norm=3.3149986267089844, loss=1.0802571773529053
I0304 03:32:14.754861 140380326586112 logging_writer.py:48] [298300] global_step=298300, grad_norm=3.104890823364258, loss=1.1777012348175049
I0304 03:32:59.751113 140380334978816 logging_writer.py:48] [298400] global_step=298400, grad_norm=3.335034132003784, loss=1.165520429611206
I0304 03:33:44.760996 140380326586112 logging_writer.py:48] [298500] global_step=298500, grad_norm=3.253695487976074, loss=1.3089216947555542
I0304 03:34:20.030970 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:34:30.779992 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:34:57.595868 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:34:59.253006 140575196817216 submission_runner.py:411] Time since start: 144074.85s, 	Step: 298580, 	{'train/accuracy': 0.8873046636581421, 'train/loss': 0.42156919836997986, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 132802.52157902718, 'total_duration': 144074.85414624214, 'accumulated_submission_time': 132802.52157902718, 'accumulated_eval_time': 11238.506650447845, 'accumulated_logging_time': 17.98520803451538}
I0304 03:34:59.313336 140380334978816 logging_writer.py:48] [298580] accumulated_eval_time=11238.506650, accumulated_logging_time=17.985208, accumulated_submission_time=132802.521579, global_step=298580, preemption_count=0, score=132802.521579, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=144074.854146, train/accuracy=0.887305, train/loss=0.421569, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:35:07.652792 140380326586112 logging_writer.py:48] [298600] global_step=298600, grad_norm=3.1767616271972656, loss=1.519988775253296
I0304 03:35:49.027273 140380334978816 logging_writer.py:48] [298700] global_step=298700, grad_norm=3.136342763900757, loss=1.09060800075531
I0304 03:36:34.831638 140380326586112 logging_writer.py:48] [298800] global_step=298800, grad_norm=4.460298538208008, loss=3.2159183025360107
I0304 03:37:20.008285 140380334978816 logging_writer.py:48] [298900] global_step=298900, grad_norm=3.0172691345214844, loss=1.085195541381836
I0304 03:38:05.020604 140380326586112 logging_writer.py:48] [299000] global_step=299000, grad_norm=2.994037628173828, loss=1.0394136905670166
I0304 03:38:50.276735 140380334978816 logging_writer.py:48] [299100] global_step=299100, grad_norm=3.6085081100463867, loss=3.137364625930786
I0304 03:39:35.273049 140380326586112 logging_writer.py:48] [299200] global_step=299200, grad_norm=3.6106579303741455, loss=2.7624752521514893
I0304 03:40:20.391512 140380334978816 logging_writer.py:48] [299300] global_step=299300, grad_norm=3.479584217071533, loss=3.0429418087005615
I0304 03:41:05.570232 140380326586112 logging_writer.py:48] [299400] global_step=299400, grad_norm=2.7891409397125244, loss=2.2017037868499756
I0304 03:41:50.446288 140380334978816 logging_writer.py:48] [299500] global_step=299500, grad_norm=4.463271141052246, loss=3.14585018157959
I0304 03:41:59.596410 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:42:10.299328 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:42:33.229074 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:42:34.882493 140575196817216 submission_runner.py:411] Time since start: 144530.48s, 	Step: 299522, 	{'train/accuracy': 0.8869335651397705, 'train/loss': 0.4194314777851105, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 133222.74796295166, 'total_duration': 144530.4835705757, 'accumulated_submission_time': 133222.74796295166, 'accumulated_eval_time': 11273.791742801666, 'accumulated_logging_time': 18.054905891418457}
I0304 03:42:34.942583 140380326586112 logging_writer.py:48] [299522] accumulated_eval_time=11273.791743, accumulated_logging_time=18.054906, accumulated_submission_time=133222.747963, global_step=299522, preemption_count=0, score=133222.747963, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=144530.483571, train/accuracy=0.886934, train/loss=0.419431, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:43:06.336021 140380334978816 logging_writer.py:48] [299600] global_step=299600, grad_norm=3.2647852897644043, loss=1.8969131708145142
I0304 03:43:50.611666 140380326586112 logging_writer.py:48] [299700] global_step=299700, grad_norm=2.817143201828003, loss=1.378341555595398
I0304 03:44:35.919530 140380334978816 logging_writer.py:48] [299800] global_step=299800, grad_norm=3.0919253826141357, loss=1.9092998504638672
I0304 03:45:20.924837 140380326586112 logging_writer.py:48] [299900] global_step=299900, grad_norm=3.5143511295318604, loss=1.2304346561431885
I0304 03:46:05.801976 140380334978816 logging_writer.py:48] [300000] global_step=300000, grad_norm=2.876549005508423, loss=1.10468327999115
I0304 03:46:50.732087 140380326586112 logging_writer.py:48] [300100] global_step=300100, grad_norm=2.955382823944092, loss=1.1083813905715942
I0304 03:47:35.690988 140380334978816 logging_writer.py:48] [300200] global_step=300200, grad_norm=3.850177764892578, loss=3.1101720333099365
I0304 03:48:20.827019 140380326586112 logging_writer.py:48] [300300] global_step=300300, grad_norm=3.141284465789795, loss=2.2647323608398438
I0304 03:49:05.826224 140380334978816 logging_writer.py:48] [300400] global_step=300400, grad_norm=2.860560894012451, loss=1.0669007301330566
I0304 03:49:35.082673 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:49:45.750974 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:50:10.417700 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:50:12.078323 140575196817216 submission_runner.py:411] Time since start: 144987.68s, 	Step: 300467, 	{'train/accuracy': 0.8865820169448853, 'train/loss': 0.4196713864803314, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 133642.82978200912, 'total_duration': 144987.6792693138, 'accumulated_submission_time': 133642.82978200912, 'accumulated_eval_time': 11310.786287784576, 'accumulated_logging_time': 18.124565362930298}
I0304 03:50:12.140665 140380326586112 logging_writer.py:48] [300467] accumulated_eval_time=11310.786288, accumulated_logging_time=18.124565, accumulated_submission_time=133642.829782, global_step=300467, preemption_count=0, score=133642.829782, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=144987.679269, train/accuracy=0.886582, train/loss=0.419671, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:50:25.646695 140380334978816 logging_writer.py:48] [300500] global_step=300500, grad_norm=3.0941429138183594, loss=1.4338279962539673
I0304 03:51:07.403917 140380326586112 logging_writer.py:48] [300600] global_step=300600, grad_norm=3.357571601867676, loss=2.868273973464966
I0304 03:51:52.333949 140380334978816 logging_writer.py:48] [300700] global_step=300700, grad_norm=3.6468417644500732, loss=2.0833542346954346
I0304 03:52:37.644230 140380326586112 logging_writer.py:48] [300800] global_step=300800, grad_norm=5.063928604125977, loss=3.207645893096924
I0304 03:53:22.478717 140380334978816 logging_writer.py:48] [300900] global_step=300900, grad_norm=3.101835250854492, loss=2.086527109146118
I0304 03:54:07.565727 140380326586112 logging_writer.py:48] [301000] global_step=301000, grad_norm=3.046351432800293, loss=1.3891435861587524
I0304 03:54:52.427575 140380334978816 logging_writer.py:48] [301100] global_step=301100, grad_norm=3.046349287033081, loss=2.649411201477051
I0304 03:55:37.770609 140380326586112 logging_writer.py:48] [301200] global_step=301200, grad_norm=3.8581597805023193, loss=3.1089277267456055
I0304 03:56:22.840867 140380334978816 logging_writer.py:48] [301300] global_step=301300, grad_norm=3.825519561767578, loss=2.947591781616211
I0304 03:57:07.795840 140380326586112 logging_writer.py:48] [301400] global_step=301400, grad_norm=3.303900957107544, loss=1.1354740858078003
I0304 03:57:12.525818 140575196817216 spec.py:321] Evaluating on the training split.
I0304 03:57:23.618281 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 03:57:46.498644 140575196817216 spec.py:349] Evaluating on the test split.
I0304 03:57:48.154254 140575196817216 submission_runner.py:411] Time since start: 145443.76s, 	Step: 301412, 	{'train/accuracy': 0.8876953125, 'train/loss': 0.4157899022102356, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 134063.15568971634, 'total_duration': 145443.7553141117, 'accumulated_submission_time': 134063.15568971634, 'accumulated_eval_time': 11346.413735866547, 'accumulated_logging_time': 18.19742178916931}
I0304 03:57:48.214940 140380334978816 logging_writer.py:48] [301412] accumulated_eval_time=11346.413736, accumulated_logging_time=18.197422, accumulated_submission_time=134063.155690, global_step=301412, preemption_count=0, score=134063.155690, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=145443.755314, train/accuracy=0.887695, train/loss=0.415790, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 03:58:23.653245 140380326586112 logging_writer.py:48] [301500] global_step=301500, grad_norm=2.974069356918335, loss=0.9990354776382446
I0304 03:59:08.562647 140380334978816 logging_writer.py:48] [301600] global_step=301600, grad_norm=3.2290539741516113, loss=1.2471662759780884
I0304 03:59:53.446118 140380326586112 logging_writer.py:48] [301700] global_step=301700, grad_norm=3.3291308879852295, loss=1.1561065912246704
I0304 04:00:38.713816 140380334978816 logging_writer.py:48] [301800] global_step=301800, grad_norm=3.427640676498413, loss=2.7426955699920654
I0304 04:01:23.633746 140380326586112 logging_writer.py:48] [301900] global_step=301900, grad_norm=3.44783878326416, loss=2.9075162410736084
I0304 04:02:08.593979 140380334978816 logging_writer.py:48] [302000] global_step=302000, grad_norm=4.025157451629639, loss=3.1772549152374268
I0304 04:02:53.245305 140380326586112 logging_writer.py:48] [302100] global_step=302100, grad_norm=3.300520896911621, loss=1.3437058925628662
I0304 04:03:38.367673 140380334978816 logging_writer.py:48] [302200] global_step=302200, grad_norm=3.047231674194336, loss=1.1193007230758667
I0304 04:04:23.163722 140380326586112 logging_writer.py:48] [302300] global_step=302300, grad_norm=3.259645700454712, loss=1.513380527496338
I0304 04:04:48.267210 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:04:59.175147 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:05:24.006037 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:05:25.655483 140575196817216 submission_runner.py:411] Time since start: 145901.26s, 	Step: 302358, 	{'train/accuracy': 0.8873242139816284, 'train/loss': 0.4200023412704468, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 134483.15005874634, 'total_duration': 145901.2564651966, 'accumulated_submission_time': 134483.15005874634, 'accumulated_eval_time': 11383.800921678543, 'accumulated_logging_time': 18.267784357070923}
I0304 04:05:25.716104 140380334978816 logging_writer.py:48] [302358] accumulated_eval_time=11383.800922, accumulated_logging_time=18.267784, accumulated_submission_time=134483.150059, global_step=302358, preemption_count=0, score=134483.150059, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=145901.256465, train/accuracy=0.887324, train/loss=0.420002, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:05:42.831417 140380326586112 logging_writer.py:48] [302400] global_step=302400, grad_norm=3.043292284011841, loss=1.014420747756958
I0304 04:06:25.544142 140380334978816 logging_writer.py:48] [302500] global_step=302500, grad_norm=2.956113576889038, loss=1.4029452800750732
I0304 04:07:10.548806 140380326586112 logging_writer.py:48] [302600] global_step=302600, grad_norm=2.7852320671081543, loss=1.228011131286621
I0304 04:07:55.595262 140380334978816 logging_writer.py:48] [302700] global_step=302700, grad_norm=3.1351964473724365, loss=1.056688666343689
I0304 04:08:40.627319 140380326586112 logging_writer.py:48] [302800] global_step=302800, grad_norm=3.161994695663452, loss=1.886064887046814
I0304 04:09:25.479284 140380334978816 logging_writer.py:48] [302900] global_step=302900, grad_norm=2.974323034286499, loss=1.0921728610992432
I0304 04:10:10.506147 140380326586112 logging_writer.py:48] [303000] global_step=303000, grad_norm=3.0082921981811523, loss=1.9521570205688477
I0304 04:10:55.429429 140380334978816 logging_writer.py:48] [303100] global_step=303100, grad_norm=2.966357469558716, loss=1.1273266077041626
I0304 04:11:40.458149 140380326586112 logging_writer.py:48] [303200] global_step=303200, grad_norm=3.230231761932373, loss=1.5158125162124634
I0304 04:12:25.333288 140380334978816 logging_writer.py:48] [303300] global_step=303300, grad_norm=3.454989194869995, loss=2.8917205333709717
I0304 04:12:25.886161 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:12:36.639675 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:13:00.207479 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:13:01.865964 140575196817216 submission_runner.py:411] Time since start: 146357.47s, 	Step: 303303, 	{'train/accuracy': 0.8898828029632568, 'train/loss': 0.4121365249156952, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 134903.26118922234, 'total_duration': 146357.46699857712, 'accumulated_submission_time': 134903.26118922234, 'accumulated_eval_time': 11419.779727935791, 'accumulated_logging_time': 18.338664531707764}
I0304 04:13:01.926955 140380326586112 logging_writer.py:48] [303303] accumulated_eval_time=11419.779728, accumulated_logging_time=18.338665, accumulated_submission_time=134903.261189, global_step=303303, preemption_count=0, score=134903.261189, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=146357.466999, train/accuracy=0.889883, train/loss=0.412137, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:13:41.224077 140380334978816 logging_writer.py:48] [303400] global_step=303400, grad_norm=2.8365893363952637, loss=1.2100679874420166
I0304 04:14:25.766564 140380326586112 logging_writer.py:48] [303500] global_step=303500, grad_norm=3.160789966583252, loss=2.751922130584717
I0304 04:15:10.876978 140380334978816 logging_writer.py:48] [303600] global_step=303600, grad_norm=2.7705116271972656, loss=1.2441070079803467
I0304 04:15:55.904419 140380326586112 logging_writer.py:48] [303700] global_step=303700, grad_norm=3.8167152404785156, loss=1.1417618989944458
I0304 04:16:41.258415 140380334978816 logging_writer.py:48] [303800] global_step=303800, grad_norm=2.860684633255005, loss=1.218416690826416
I0304 04:17:26.152239 140380326586112 logging_writer.py:48] [303900] global_step=303900, grad_norm=3.373117685317993, loss=1.0811073780059814
I0304 04:18:11.127789 140380334978816 logging_writer.py:48] [304000] global_step=304000, grad_norm=3.0774929523468018, loss=1.1266441345214844
I0304 04:18:56.305890 140380326586112 logging_writer.py:48] [304100] global_step=304100, grad_norm=3.146287202835083, loss=2.7733960151672363
I0304 04:19:41.344351 140380334978816 logging_writer.py:48] [304200] global_step=304200, grad_norm=4.926651477813721, loss=3.367396593093872
I0304 04:20:02.119078 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:20:12.799679 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:20:38.894840 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:20:40.556906 140575196817216 submission_runner.py:411] Time since start: 146816.16s, 	Step: 304248, 	{'train/accuracy': 0.8899218440055847, 'train/loss': 0.4108898341655731, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 135323.39578986168, 'total_duration': 146816.15788459778, 'accumulated_submission_time': 135323.39578986168, 'accumulated_eval_time': 11458.216473817825, 'accumulated_logging_time': 18.40903115272522}
I0304 04:20:40.618046 140380326586112 logging_writer.py:48] [304248] accumulated_eval_time=11458.216474, accumulated_logging_time=18.409031, accumulated_submission_time=135323.395790, global_step=304248, preemption_count=0, score=135323.395790, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=146816.157885, train/accuracy=0.889922, train/loss=0.410890, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:21:01.818895 140380334978816 logging_writer.py:48] [304300] global_step=304300, grad_norm=3.293002128601074, loss=1.175737977027893
I0304 04:21:44.699665 140380326586112 logging_writer.py:48] [304400] global_step=304400, grad_norm=3.1873388290405273, loss=1.3033668994903564
I0304 04:22:29.782335 140380334978816 logging_writer.py:48] [304500] global_step=304500, grad_norm=3.272461414337158, loss=1.3058850765228271
I0304 04:23:14.772734 140380326586112 logging_writer.py:48] [304600] global_step=304600, grad_norm=4.705661773681641, loss=2.6338400840759277
I0304 04:23:59.484555 140380334978816 logging_writer.py:48] [304700] global_step=304700, grad_norm=3.2641754150390625, loss=1.1194318532943726
I0304 04:24:44.511055 140380326586112 logging_writer.py:48] [304800] global_step=304800, grad_norm=3.212752103805542, loss=1.2598191499710083
I0304 04:25:29.439885 140380334978816 logging_writer.py:48] [304900] global_step=304900, grad_norm=2.989814519882202, loss=2.3284218311309814
I0304 04:26:14.496018 140380326586112 logging_writer.py:48] [305000] global_step=305000, grad_norm=3.2645795345306396, loss=1.1959062814712524
I0304 04:26:59.514333 140380334978816 logging_writer.py:48] [305100] global_step=305100, grad_norm=3.291327476501465, loss=1.0501452684402466
I0304 04:27:40.607979 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:27:51.311744 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:28:13.426996 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:28:15.077230 140575196817216 submission_runner.py:411] Time since start: 147270.68s, 	Step: 305193, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.41690871119499207, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 135743.32669734955, 'total_duration': 147270.67830872536, 'accumulated_submission_time': 135743.32669734955, 'accumulated_eval_time': 11492.684744119644, 'accumulated_logging_time': 18.48048448562622}
I0304 04:28:15.147444 140380326586112 logging_writer.py:48] [305193] accumulated_eval_time=11492.684744, accumulated_logging_time=18.480484, accumulated_submission_time=135743.326697, global_step=305193, preemption_count=0, score=135743.326697, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=147270.678309, train/accuracy=0.887598, train/loss=0.416909, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:28:18.329059 140380334978816 logging_writer.py:48] [305200] global_step=305200, grad_norm=3.1421878337860107, loss=1.4470893144607544
I0304 04:28:59.033920 140380326586112 logging_writer.py:48] [305300] global_step=305300, grad_norm=2.9951329231262207, loss=1.9932136535644531
I0304 04:29:43.792791 140380334978816 logging_writer.py:48] [305400] global_step=305400, grad_norm=3.5089499950408936, loss=2.9908926486968994
I0304 04:30:28.861189 140380326586112 logging_writer.py:48] [305500] global_step=305500, grad_norm=3.0923333168029785, loss=2.44266414642334
I0304 04:31:13.787256 140380334978816 logging_writer.py:48] [305600] global_step=305600, grad_norm=3.1995418071746826, loss=1.065248727798462
I0304 04:31:58.877691 140380326586112 logging_writer.py:48] [305700] global_step=305700, grad_norm=6.8183088302612305, loss=3.2394087314605713
I0304 04:32:43.944867 140380334978816 logging_writer.py:48] [305800] global_step=305800, grad_norm=3.120826482772827, loss=1.5714833736419678
I0304 04:33:28.892984 140380326586112 logging_writer.py:48] [305900] global_step=305900, grad_norm=3.6182491779327393, loss=1.0105791091918945
I0304 04:34:13.772072 140380334978816 logging_writer.py:48] [306000] global_step=306000, grad_norm=2.9534757137298584, loss=1.0522037744522095
I0304 04:34:58.667879 140380326586112 logging_writer.py:48] [306100] global_step=306100, grad_norm=3.074028491973877, loss=1.1521670818328857
I0304 04:35:15.133464 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:35:26.852809 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:35:50.697522 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:35:52.357319 140575196817216 submission_runner.py:411] Time since start: 147727.96s, 	Step: 306138, 	{'train/accuracy': 0.8876953125, 'train/loss': 0.4212630093097687, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 136163.254529953, 'total_duration': 147727.958391428, 'accumulated_submission_time': 136163.254529953, 'accumulated_eval_time': 11529.90760731697, 'accumulated_logging_time': 18.560980796813965}
I0304 04:35:52.417699 140380334978816 logging_writer.py:48] [306138] accumulated_eval_time=11529.907607, accumulated_logging_time=18.560981, accumulated_submission_time=136163.254530, global_step=306138, preemption_count=0, score=136163.254530, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=147727.958391, train/accuracy=0.887695, train/loss=0.421263, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:36:17.459746 140380326586112 logging_writer.py:48] [306200] global_step=306200, grad_norm=3.2634315490722656, loss=1.1738905906677246
I0304 04:37:00.747740 140380334978816 logging_writer.py:48] [306300] global_step=306300, grad_norm=3.176182270050049, loss=1.5860406160354614
I0304 04:37:45.848485 140380326586112 logging_writer.py:48] [306400] global_step=306400, grad_norm=2.9149692058563232, loss=2.358811855316162
I0304 04:38:31.150100 140380334978816 logging_writer.py:48] [306500] global_step=306500, grad_norm=2.9559266567230225, loss=0.9955441355705261
I0304 04:39:16.622679 140380326586112 logging_writer.py:48] [306600] global_step=306600, grad_norm=3.110208511352539, loss=1.8887557983398438
I0304 04:40:01.711791 140380334978816 logging_writer.py:48] [306700] global_step=306700, grad_norm=3.433459997177124, loss=1.8106807470321655
I0304 04:40:46.871914 140380326586112 logging_writer.py:48] [306800] global_step=306800, grad_norm=3.2174954414367676, loss=1.1228336095809937
I0304 04:41:32.026275 140380334978816 logging_writer.py:48] [306900] global_step=306900, grad_norm=2.9769623279571533, loss=1.015006422996521
I0304 04:42:17.142427 140380326586112 logging_writer.py:48] [307000] global_step=307000, grad_norm=3.884740114212036, loss=3.189988374710083
I0304 04:42:52.788653 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:43:03.582901 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:43:26.391802 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:43:28.036536 140575196817216 submission_runner.py:411] Time since start: 148183.64s, 	Step: 307081, 	{'train/accuracy': 0.8867382407188416, 'train/loss': 0.4187721908092499, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 136583.568918705, 'total_duration': 148183.63747787476, 'accumulated_submission_time': 136583.568918705, 'accumulated_eval_time': 11565.154380083084, 'accumulated_logging_time': 18.630207300186157}
I0304 04:43:28.103217 140380334978816 logging_writer.py:48] [307081] accumulated_eval_time=11565.154380, accumulated_logging_time=18.630207, accumulated_submission_time=136583.568919, global_step=307081, preemption_count=0, score=136583.568919, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=148183.637478, train/accuracy=0.886738, train/loss=0.418772, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:43:36.066174 140380326586112 logging_writer.py:48] [307100] global_step=307100, grad_norm=3.6302177906036377, loss=3.27827787399292
I0304 04:44:17.318109 140380334978816 logging_writer.py:48] [307200] global_step=307200, grad_norm=3.5509305000305176, loss=1.053268313407898
I0304 04:45:01.986884 140380326586112 logging_writer.py:48] [307300] global_step=307300, grad_norm=3.1619765758514404, loss=1.1427791118621826
I0304 04:45:47.119513 140380334978816 logging_writer.py:48] [307400] global_step=307400, grad_norm=2.993281602859497, loss=1.1208547353744507
I0304 04:46:32.307665 140380326586112 logging_writer.py:48] [307500] global_step=307500, grad_norm=3.149190902709961, loss=1.9112507104873657
I0304 04:47:17.594904 140380334978816 logging_writer.py:48] [307600] global_step=307600, grad_norm=3.2606983184814453, loss=1.2146662473678589
I0304 04:48:02.488269 140380326586112 logging_writer.py:48] [307700] global_step=307700, grad_norm=3.3734402656555176, loss=2.8383727073669434
I0304 04:48:47.649370 140380334978816 logging_writer.py:48] [307800] global_step=307800, grad_norm=2.918361186981201, loss=1.0728511810302734
I0304 04:49:32.526119 140380326586112 logging_writer.py:48] [307900] global_step=307900, grad_norm=3.1353585720062256, loss=1.1578714847564697
I0304 04:50:17.323980 140380334978816 logging_writer.py:48] [308000] global_step=308000, grad_norm=3.686893939971924, loss=2.1946909427642822
I0304 04:50:28.416729 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:50:39.158991 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:51:04.038232 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:51:05.687109 140575196817216 submission_runner.py:411] Time since start: 148641.29s, 	Step: 308026, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.4194600582122803, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 137003.82122135162, 'total_duration': 148641.28811311722, 'accumulated_submission_time': 137003.82122135162, 'accumulated_eval_time': 11602.423706293106, 'accumulated_logging_time': 18.709379196166992}
I0304 04:51:05.747946 140380326586112 logging_writer.py:48] [308026] accumulated_eval_time=11602.423706, accumulated_logging_time=18.709379, accumulated_submission_time=137003.821221, global_step=308026, preemption_count=0, score=137003.821221, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=148641.288113, train/accuracy=0.887227, train/loss=0.419460, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:51:35.564744 140380334978816 logging_writer.py:48] [308100] global_step=308100, grad_norm=3.012789726257324, loss=1.4254534244537354
I0304 04:52:19.674057 140380326586112 logging_writer.py:48] [308200] global_step=308200, grad_norm=3.612847328186035, loss=1.0835785865783691
I0304 04:53:04.775855 140380334978816 logging_writer.py:48] [308300] global_step=308300, grad_norm=2.873134136199951, loss=1.0863943099975586
I0304 04:53:49.827224 140380326586112 logging_writer.py:48] [308400] global_step=308400, grad_norm=3.1007378101348877, loss=1.1728826761245728
I0304 04:54:34.839934 140380334978816 logging_writer.py:48] [308500] global_step=308500, grad_norm=2.9285998344421387, loss=1.0342557430267334
I0304 04:55:19.835710 140380326586112 logging_writer.py:48] [308600] global_step=308600, grad_norm=3.092559337615967, loss=1.1920794248580933
I0304 04:56:04.845562 140380334978816 logging_writer.py:48] [308700] global_step=308700, grad_norm=3.2994871139526367, loss=1.0847915410995483
I0304 04:56:50.420836 140380326586112 logging_writer.py:48] [308800] global_step=308800, grad_norm=3.548574447631836, loss=1.1393942832946777
I0304 04:57:35.569589 140380334978816 logging_writer.py:48] [308900] global_step=308900, grad_norm=4.039559364318848, loss=2.7366621494293213
I0304 04:58:05.944431 140575196817216 spec.py:321] Evaluating on the training split.
I0304 04:58:16.607315 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 04:58:39.705101 140575196817216 spec.py:349] Evaluating on the test split.
I0304 04:58:41.358612 140575196817216 submission_runner.py:411] Time since start: 149096.96s, 	Step: 308970, 	{'train/accuracy': 0.8910546898841858, 'train/loss': 0.41306889057159424, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 137423.9591574669, 'total_duration': 149096.95966887474, 'accumulated_submission_time': 137423.9591574669, 'accumulated_eval_time': 11637.836884021759, 'accumulated_logging_time': 18.780176162719727}
I0304 04:58:41.421299 140380326586112 logging_writer.py:48] [308970] accumulated_eval_time=11637.836884, accumulated_logging_time=18.780176, accumulated_submission_time=137423.959157, global_step=308970, preemption_count=0, score=137423.959157, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=149096.959669, train/accuracy=0.891055, train/loss=0.413069, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 04:58:53.743079 140380334978816 logging_writer.py:48] [309000] global_step=309000, grad_norm=3.2809712886810303, loss=1.2274510860443115
I0304 04:59:35.607740 140380326586112 logging_writer.py:48] [309100] global_step=309100, grad_norm=3.37787127494812, loss=1.1726601123809814
I0304 05:00:20.982257 140380334978816 logging_writer.py:48] [309200] global_step=309200, grad_norm=3.5904312133789062, loss=1.0931283235549927
I0304 05:01:06.922143 140380326586112 logging_writer.py:48] [309300] global_step=309300, grad_norm=3.1538925170898438, loss=1.085564374923706
I0304 05:01:52.055893 140380334978816 logging_writer.py:48] [309400] global_step=309400, grad_norm=2.8606460094451904, loss=1.439497709274292
I0304 05:02:37.109853 140380326586112 logging_writer.py:48] [309500] global_step=309500, grad_norm=3.35745906829834, loss=0.9856216311454773
I0304 05:03:22.415375 140380334978816 logging_writer.py:48] [309600] global_step=309600, grad_norm=2.823986053466797, loss=1.3197216987609863
I0304 05:04:07.545358 140380326586112 logging_writer.py:48] [309700] global_step=309700, grad_norm=3.102003574371338, loss=1.1257656812667847
I0304 05:04:52.517249 140380334978816 logging_writer.py:48] [309800] global_step=309800, grad_norm=3.25539493560791, loss=1.1501017808914185
I0304 05:05:37.381482 140380326586112 logging_writer.py:48] [309900] global_step=309900, grad_norm=2.8563311100006104, loss=2.0211305618286133
I0304 05:05:41.535546 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:05:52.343572 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:06:16.023918 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:06:17.674202 140575196817216 submission_runner.py:411] Time since start: 149553.28s, 	Step: 309911, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.4212709665298462, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 137844.01475334167, 'total_duration': 149553.27523708344, 'accumulated_submission_time': 137844.01475334167, 'accumulated_eval_time': 11673.974525928497, 'accumulated_logging_time': 18.853289127349854}
I0304 05:06:17.736154 140380334978816 logging_writer.py:48] [309911] accumulated_eval_time=11673.974526, accumulated_logging_time=18.853289, accumulated_submission_time=137844.014753, global_step=309911, preemption_count=0, score=137844.014753, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=149553.275237, train/accuracy=0.887559, train/loss=0.421271, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:06:53.472923 140380326586112 logging_writer.py:48] [310000] global_step=310000, grad_norm=3.0992817878723145, loss=1.124482274055481
I0304 05:07:38.316894 140380334978816 logging_writer.py:48] [310100] global_step=310100, grad_norm=3.418400287628174, loss=1.6770182847976685
I0304 05:08:23.114537 140380326586112 logging_writer.py:48] [310200] global_step=310200, grad_norm=3.405684471130371, loss=1.5645986795425415
I0304 05:09:08.510450 140380334978816 logging_writer.py:48] [310300] global_step=310300, grad_norm=2.9895129203796387, loss=1.1486481428146362
I0304 05:09:53.357949 140380326586112 logging_writer.py:48] [310400] global_step=310400, grad_norm=3.3452088832855225, loss=1.1280276775360107
I0304 05:10:38.489338 140380334978816 logging_writer.py:48] [310500] global_step=310500, grad_norm=2.941972494125366, loss=1.082129716873169
I0304 05:11:23.461969 140380326586112 logging_writer.py:48] [310600] global_step=310600, grad_norm=3.088841676712036, loss=1.3782477378845215
I0304 05:12:08.140637 140380334978816 logging_writer.py:48] [310700] global_step=310700, grad_norm=3.13576602935791, loss=1.083964228630066
I0304 05:12:53.517033 140380326586112 logging_writer.py:48] [310800] global_step=310800, grad_norm=2.9469082355499268, loss=1.0489649772644043
I0304 05:13:18.036340 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:13:28.905719 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:13:52.402387 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:13:54.048193 140575196817216 submission_runner.py:411] Time since start: 150009.65s, 	Step: 310856, 	{'train/accuracy': 0.8844921588897705, 'train/loss': 0.42967620491981506, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 138264.2561571598, 'total_duration': 150009.64900398254, 'accumulated_submission_time': 138264.2561571598, 'accumulated_eval_time': 11709.985116958618, 'accumulated_logging_time': 18.925705671310425}
I0304 05:13:54.111356 140380334978816 logging_writer.py:48] [310856] accumulated_eval_time=11709.985117, accumulated_logging_time=18.925706, accumulated_submission_time=138264.256157, global_step=310856, preemption_count=0, score=138264.256157, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=150009.649004, train/accuracy=0.884492, train/loss=0.429676, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:14:12.001454 140380326586112 logging_writer.py:48] [310900] global_step=310900, grad_norm=3.0205395221710205, loss=1.2430006265640259
I0304 05:14:54.254332 140380334978816 logging_writer.py:48] [311000] global_step=311000, grad_norm=3.2396442890167236, loss=1.242053747177124
I0304 05:15:39.253149 140380326586112 logging_writer.py:48] [311100] global_step=311100, grad_norm=2.8978891372680664, loss=1.0713775157928467
I0304 05:16:24.594732 140380334978816 logging_writer.py:48] [311200] global_step=311200, grad_norm=3.2812583446502686, loss=1.229263424873352
I0304 05:17:09.823095 140380326586112 logging_writer.py:48] [311300] global_step=311300, grad_norm=3.163640022277832, loss=1.3307924270629883
I0304 05:17:54.827865 140380334978816 logging_writer.py:48] [311400] global_step=311400, grad_norm=3.1295487880706787, loss=1.1403050422668457
I0304 05:18:39.593441 140380326586112 logging_writer.py:48] [311500] global_step=311500, grad_norm=3.271763324737549, loss=1.5572820901870728
I0304 05:19:24.990326 140380334978816 logging_writer.py:48] [311600] global_step=311600, grad_norm=3.0566141605377197, loss=1.660296082496643
I0304 05:20:10.123291 140380326586112 logging_writer.py:48] [311700] global_step=311700, grad_norm=3.3728020191192627, loss=1.2058653831481934
I0304 05:20:54.355345 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:21:05.182907 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:21:31.445974 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:21:33.094906 140575196817216 submission_runner.py:411] Time since start: 150468.70s, 	Step: 311800, 	{'train/accuracy': 0.88685542345047, 'train/loss': 0.41987916827201843, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 138684.43855118752, 'total_duration': 150468.6958978176, 'accumulated_submission_time': 138684.43855118752, 'accumulated_eval_time': 11748.723598957062, 'accumulated_logging_time': 19.001494646072388}
I0304 05:21:33.160959 140380334978816 logging_writer.py:48] [311800] accumulated_eval_time=11748.723599, accumulated_logging_time=19.001495, accumulated_submission_time=138684.438551, global_step=311800, preemption_count=0, score=138684.438551, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=150468.695898, train/accuracy=0.886855, train/loss=0.419879, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:21:33.559176 140380326586112 logging_writer.py:48] [311800] global_step=311800, grad_norm=4.260767459869385, loss=3.2579915523529053
I0304 05:22:14.075437 140380334978816 logging_writer.py:48] [311900] global_step=311900, grad_norm=3.249943971633911, loss=1.0891456604003906
I0304 05:22:59.096771 140380326586112 logging_writer.py:48] [312000] global_step=312000, grad_norm=4.373425483703613, loss=3.206475257873535
I0304 05:23:44.312825 140380334978816 logging_writer.py:48] [312100] global_step=312100, grad_norm=3.1990745067596436, loss=1.0977232456207275
I0304 05:24:29.306116 140380326586112 logging_writer.py:48] [312200] global_step=312200, grad_norm=3.4801924228668213, loss=2.251434326171875
I0304 05:25:14.342451 140380334978816 logging_writer.py:48] [312300] global_step=312300, grad_norm=3.2124640941619873, loss=1.2377899885177612
I0304 05:25:59.418379 140380326586112 logging_writer.py:48] [312400] global_step=312400, grad_norm=4.427265644073486, loss=1.1569217443466187
I0304 05:26:44.734507 140380334978816 logging_writer.py:48] [312500] global_step=312500, grad_norm=3.155245542526245, loss=1.1177324056625366
I0304 05:27:29.624206 140380326586112 logging_writer.py:48] [312600] global_step=312600, grad_norm=2.995462656021118, loss=1.2758878469467163
I0304 05:28:14.691983 140380334978816 logging_writer.py:48] [312700] global_step=312700, grad_norm=3.1722803115844727, loss=1.1547733545303345
I0304 05:28:33.096297 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:28:43.782760 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:29:09.401455 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:29:11.057987 140575196817216 submission_runner.py:411] Time since start: 150926.66s, 	Step: 312743, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.4195569157600403, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 139104.31449127197, 'total_duration': 150926.6590127945, 'accumulated_submission_time': 139104.31449127197, 'accumulated_eval_time': 11786.684258937836, 'accumulated_logging_time': 19.078715562820435}
I0304 05:29:11.120088 140380326586112 logging_writer.py:48] [312743] accumulated_eval_time=11786.684259, accumulated_logging_time=19.078716, accumulated_submission_time=139104.314491, global_step=312743, preemption_count=0, score=139104.314491, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=150926.659013, train/accuracy=0.887227, train/loss=0.419557, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:29:34.176298 140380334978816 logging_writer.py:48] [312800] global_step=312800, grad_norm=2.9915833473205566, loss=1.104856014251709
I0304 05:30:17.299394 140380326586112 logging_writer.py:48] [312900] global_step=312900, grad_norm=3.1163535118103027, loss=0.9887322187423706
I0304 05:31:02.456032 140380334978816 logging_writer.py:48] [313000] global_step=313000, grad_norm=3.1068458557128906, loss=1.7368347644805908
I0304 05:31:47.334966 140380326586112 logging_writer.py:48] [313100] global_step=313100, grad_norm=3.2367753982543945, loss=1.719308614730835
I0304 05:32:32.371574 140380334978816 logging_writer.py:48] [313200] global_step=313200, grad_norm=3.3907766342163086, loss=1.3996065855026245
I0304 05:33:17.392444 140380326586112 logging_writer.py:48] [313300] global_step=313300, grad_norm=3.164595365524292, loss=1.196649193763733
I0304 05:34:02.140187 140380334978816 logging_writer.py:48] [313400] global_step=313400, grad_norm=3.146488904953003, loss=1.7897557020187378
I0304 05:34:47.203407 140380326586112 logging_writer.py:48] [313500] global_step=313500, grad_norm=4.330997943878174, loss=3.3280794620513916
I0304 05:35:32.105138 140380334978816 logging_writer.py:48] [313600] global_step=313600, grad_norm=3.144002914428711, loss=2.476810932159424
I0304 05:36:11.339341 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:36:21.996635 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:36:47.827543 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:36:49.477360 140575196817216 submission_runner.py:411] Time since start: 151385.08s, 	Step: 313689, 	{'train/accuracy': 0.8866406083106995, 'train/loss': 0.42051199078559875, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 139524.47371315956, 'total_duration': 151385.07851743698, 'accumulated_submission_time': 139524.47371315956, 'accumulated_eval_time': 11824.821381092072, 'accumulated_logging_time': 19.152057647705078}
I0304 05:36:49.541622 140380326586112 logging_writer.py:48] [313689] accumulated_eval_time=11824.821381, accumulated_logging_time=19.152058, accumulated_submission_time=139524.473713, global_step=313689, preemption_count=0, score=139524.473713, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=151385.078517, train/accuracy=0.886641, train/loss=0.420512, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:36:54.315914 140380334978816 logging_writer.py:48] [313700] global_step=313700, grad_norm=2.9094138145446777, loss=1.073209285736084
I0304 05:37:34.854693 140380326586112 logging_writer.py:48] [313800] global_step=313800, grad_norm=3.0989081859588623, loss=2.0060031414031982
I0304 05:38:19.896231 140380334978816 logging_writer.py:48] [313900] global_step=313900, grad_norm=3.062718629837036, loss=1.1327329874038696
I0304 05:39:04.977488 140380326586112 logging_writer.py:48] [314000] global_step=314000, grad_norm=3.3072829246520996, loss=2.684098482131958
I0304 05:39:50.292954 140380334978816 logging_writer.py:48] [314100] global_step=314100, grad_norm=3.388418674468994, loss=1.0673748254776
I0304 05:40:35.116034 140380326586112 logging_writer.py:48] [314200] global_step=314200, grad_norm=3.075725555419922, loss=2.3518130779266357
I0304 05:41:20.315382 140380334978816 logging_writer.py:48] [314300] global_step=314300, grad_norm=3.434283494949341, loss=1.0921093225479126
I0304 05:42:05.223844 140380326586112 logging_writer.py:48] [314400] global_step=314400, grad_norm=2.915574312210083, loss=1.5861482620239258
I0304 05:42:50.313921 140380334978816 logging_writer.py:48] [314500] global_step=314500, grad_norm=3.061485528945923, loss=1.1942863464355469
I0304 05:43:35.449894 140380326586112 logging_writer.py:48] [314600] global_step=314600, grad_norm=3.3641653060913086, loss=2.813128709793091
I0304 05:43:49.538471 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:44:00.270879 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:44:24.785013 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:44:26.442803 140575196817216 submission_runner.py:411] Time since start: 151842.04s, 	Step: 314633, 	{'train/accuracy': 0.8891210556030273, 'train/loss': 0.4166163206100464, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 139944.40990495682, 'total_duration': 151842.04384183884, 'accumulated_submission_time': 139944.40990495682, 'accumulated_eval_time': 11861.724688053131, 'accumulated_logging_time': 19.22867512702942}
I0304 05:44:26.513261 140380334978816 logging_writer.py:48] [314633] accumulated_eval_time=11861.724688, accumulated_logging_time=19.228675, accumulated_submission_time=139944.409905, global_step=314633, preemption_count=0, score=139944.409905, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=151842.043842, train/accuracy=0.889121, train/loss=0.416616, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:44:53.550922 140380326586112 logging_writer.py:48] [314700] global_step=314700, grad_norm=3.146069288253784, loss=1.112843632698059
I0304 05:45:36.676242 140380334978816 logging_writer.py:48] [314800] global_step=314800, grad_norm=2.8918159008026123, loss=1.2187705039978027
I0304 05:46:21.844914 140380326586112 logging_writer.py:48] [314900] global_step=314900, grad_norm=3.04640793800354, loss=1.5650498867034912
I0304 05:47:06.887574 140380334978816 logging_writer.py:48] [315000] global_step=315000, grad_norm=3.0758087635040283, loss=1.5733412504196167
I0304 05:47:51.933697 140380326586112 logging_writer.py:48] [315100] global_step=315100, grad_norm=2.971343755722046, loss=1.7872599363327026
I0304 05:48:36.978996 140380334978816 logging_writer.py:48] [315200] global_step=315200, grad_norm=4.057847499847412, loss=1.915643572807312
I0304 05:49:22.110458 140380326586112 logging_writer.py:48] [315300] global_step=315300, grad_norm=3.1965315341949463, loss=2.3186378479003906
I0304 05:50:07.189953 140380334978816 logging_writer.py:48] [315400] global_step=315400, grad_norm=3.4753284454345703, loss=2.72041916847229
I0304 05:50:52.307636 140380326586112 logging_writer.py:48] [315500] global_step=315500, grad_norm=3.6894469261169434, loss=3.2373523712158203
I0304 05:51:26.613310 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:51:37.488210 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:52:01.194535 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:52:02.845282 140575196817216 submission_runner.py:411] Time since start: 152298.45s, 	Step: 315578, 	{'train/accuracy': 0.8840429782867432, 'train/loss': 0.42759600281715393, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 140364.45055365562, 'total_duration': 152298.446028471, 'accumulated_submission_time': 140364.45055365562, 'accumulated_eval_time': 11897.955362796783, 'accumulated_logging_time': 19.309980630874634}
I0304 05:52:02.908097 140380334978816 logging_writer.py:48] [315578] accumulated_eval_time=11897.955363, accumulated_logging_time=19.309981, accumulated_submission_time=140364.450554, global_step=315578, preemption_count=0, score=140364.450554, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=152298.446028, train/accuracy=0.884043, train/loss=0.427596, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 05:52:12.052025 140380326586112 logging_writer.py:48] [315600] global_step=315600, grad_norm=3.126358985900879, loss=0.9871572256088257
I0304 05:52:53.219706 140380334978816 logging_writer.py:48] [315700] global_step=315700, grad_norm=2.994190216064453, loss=1.59312903881073
I0304 05:53:38.414743 140380326586112 logging_writer.py:48] [315800] global_step=315800, grad_norm=3.0423765182495117, loss=1.5813703536987305
I0304 05:54:23.772690 140380334978816 logging_writer.py:48] [315900] global_step=315900, grad_norm=3.0529544353485107, loss=1.162721037864685
I0304 05:55:08.615534 140380326586112 logging_writer.py:48] [316000] global_step=316000, grad_norm=3.0155298709869385, loss=1.4833871126174927
I0304 05:55:53.441740 140380334978816 logging_writer.py:48] [316100] global_step=316100, grad_norm=3.0335354804992676, loss=1.1493465900421143
I0304 05:56:38.540565 140380326586112 logging_writer.py:48] [316200] global_step=316200, grad_norm=3.410860061645508, loss=1.0507324934005737
I0304 05:57:23.626703 140380334978816 logging_writer.py:48] [316300] global_step=316300, grad_norm=3.339876651763916, loss=1.1555001735687256
I0304 05:58:08.606576 140380326586112 logging_writer.py:48] [316400] global_step=316400, grad_norm=3.3062565326690674, loss=1.1471740007400513
I0304 05:58:53.533045 140380334978816 logging_writer.py:48] [316500] global_step=316500, grad_norm=3.019373893737793, loss=1.1893929243087769
I0304 05:59:03.024675 140575196817216 spec.py:321] Evaluating on the training split.
I0304 05:59:13.811591 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 05:59:39.627996 140575196817216 spec.py:349] Evaluating on the test split.
I0304 05:59:41.300919 140575196817216 submission_runner.py:411] Time since start: 152756.90s, 	Step: 316522, 	{'train/accuracy': 0.8870507478713989, 'train/loss': 0.41709575057029724, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 140784.50740408897, 'total_duration': 152756.90191817284, 'accumulated_submission_time': 140784.50740408897, 'accumulated_eval_time': 11936.230577230453, 'accumulated_logging_time': 19.38427710533142}
I0304 05:59:41.366117 140380326586112 logging_writer.py:48] [316522] accumulated_eval_time=11936.230577, accumulated_logging_time=19.384277, accumulated_submission_time=140784.507404, global_step=316522, preemption_count=0, score=140784.507404, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=152756.901918, train/accuracy=0.887051, train/loss=0.417096, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:00:12.757148 140380334978816 logging_writer.py:48] [316600] global_step=316600, grad_norm=3.3302862644195557, loss=1.3644599914550781
I0304 06:00:56.956817 140380326586112 logging_writer.py:48] [316700] global_step=316700, grad_norm=3.5708751678466797, loss=1.1545943021774292
I0304 06:01:41.915683 140380334978816 logging_writer.py:48] [316800] global_step=316800, grad_norm=3.0088889598846436, loss=0.9820778369903564
I0304 06:02:27.082106 140380326586112 logging_writer.py:48] [316900] global_step=316900, grad_norm=3.0376603603363037, loss=1.0395488739013672
I0304 06:03:12.048337 140380334978816 logging_writer.py:48] [317000] global_step=317000, grad_norm=3.1665186882019043, loss=1.0688389539718628
I0304 06:03:56.960881 140380326586112 logging_writer.py:48] [317100] global_step=317100, grad_norm=3.1221728324890137, loss=1.129601001739502
I0304 06:04:41.945890 140380334978816 logging_writer.py:48] [317200] global_step=317200, grad_norm=3.08687162399292, loss=1.0235611200332642
I0304 06:05:26.822351 140380326586112 logging_writer.py:48] [317300] global_step=317300, grad_norm=3.453850030899048, loss=2.455057144165039
I0304 06:06:11.865422 140380334978816 logging_writer.py:48] [317400] global_step=317400, grad_norm=3.1745710372924805, loss=1.6041793823242188
I0304 06:06:41.662717 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:06:52.502320 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:07:14.784203 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:07:16.445574 140575196817216 submission_runner.py:411] Time since start: 153212.05s, 	Step: 317468, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.41616198420524597, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 141204.74587631226, 'total_duration': 153212.0466427803, 'accumulated_submission_time': 141204.74587631226, 'accumulated_eval_time': 11971.0127120018, 'accumulated_logging_time': 19.45900011062622}
I0304 06:07:16.509219 140380326586112 logging_writer.py:48] [317468] accumulated_eval_time=11971.012712, accumulated_logging_time=19.459000, accumulated_submission_time=141204.745876, global_step=317468, preemption_count=0, score=141204.745876, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=153212.046643, train/accuracy=0.887812, train/loss=0.416162, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:07:29.637866 140380334978816 logging_writer.py:48] [317500] global_step=317500, grad_norm=3.6428675651550293, loss=2.289231777191162
I0304 06:08:11.473043 140380326586112 logging_writer.py:48] [317600] global_step=317600, grad_norm=3.874263048171997, loss=1.1713180541992188
I0304 06:08:56.195407 140380334978816 logging_writer.py:48] [317700] global_step=317700, grad_norm=2.9712462425231934, loss=1.081099033355713
I0304 06:09:41.188230 140380326586112 logging_writer.py:48] [317800] global_step=317800, grad_norm=3.2976603507995605, loss=2.374495029449463
I0304 06:10:26.190420 140380334978816 logging_writer.py:48] [317900] global_step=317900, grad_norm=2.9373650550842285, loss=1.136084794998169
I0304 06:11:11.195276 140380326586112 logging_writer.py:48] [318000] global_step=318000, grad_norm=3.071491003036499, loss=1.126865267753601
I0304 06:11:56.382388 140380334978816 logging_writer.py:48] [318100] global_step=318100, grad_norm=3.149354934692383, loss=1.3099048137664795
I0304 06:12:41.320532 140380326586112 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.229933261871338, loss=3.3210690021514893
I0304 06:13:26.494740 140380334978816 logging_writer.py:48] [318300] global_step=318300, grad_norm=2.949502468109131, loss=1.4906938076019287
I0304 06:14:11.292033 140380326586112 logging_writer.py:48] [318400] global_step=318400, grad_norm=2.9576165676116943, loss=1.7232067584991455
I0304 06:14:16.815913 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:14:27.458839 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:14:52.058503 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:14:53.721172 140575196817216 submission_runner.py:411] Time since start: 153669.32s, 	Step: 318414, 	{'train/accuracy': 0.8857226371765137, 'train/loss': 0.42629972100257874, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 141624.99427366257, 'total_duration': 153669.32224822044, 'accumulated_submission_time': 141624.99427366257, 'accumulated_eval_time': 12007.916976690292, 'accumulated_logging_time': 19.532099723815918}
I0304 06:14:53.793750 140380334978816 logging_writer.py:48] [318414] accumulated_eval_time=12007.916977, accumulated_logging_time=19.532100, accumulated_submission_time=141624.994274, global_step=318414, preemption_count=0, score=141624.994274, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=153669.322248, train/accuracy=0.885723, train/loss=0.426300, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:15:28.360925 140380326586112 logging_writer.py:48] [318500] global_step=318500, grad_norm=3.204646348953247, loss=1.1345261335372925
I0304 06:16:12.938889 140380334978816 logging_writer.py:48] [318600] global_step=318600, grad_norm=3.9018771648406982, loss=2.721129894256592
I0304 06:16:58.245990 140380326586112 logging_writer.py:48] [318700] global_step=318700, grad_norm=3.1680033206939697, loss=2.2490718364715576
I0304 06:17:43.480150 140380334978816 logging_writer.py:48] [318800] global_step=318800, grad_norm=3.379871129989624, loss=1.3437657356262207
I0304 06:18:28.099247 140380326586112 logging_writer.py:48] [318900] global_step=318900, grad_norm=3.481358766555786, loss=1.2906227111816406
I0304 06:19:13.032536 140380334978816 logging_writer.py:48] [319000] global_step=319000, grad_norm=3.036802053451538, loss=1.6750731468200684
I0304 06:19:58.180659 140380326586112 logging_writer.py:48] [319100] global_step=319100, grad_norm=3.2288010120391846, loss=1.3968199491500854
I0304 06:20:43.325736 140380334978816 logging_writer.py:48] [319200] global_step=319200, grad_norm=3.1107051372528076, loss=1.2157490253448486
I0304 06:21:28.526921 140380326586112 logging_writer.py:48] [319300] global_step=319300, grad_norm=5.2730326652526855, loss=3.1874518394470215
I0304 06:21:54.180711 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:22:04.947822 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:22:30.734060 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:22:32.394821 140575196817216 submission_runner.py:411] Time since start: 154128.00s, 	Step: 319359, 	{'train/accuracy': 0.8882616758346558, 'train/loss': 0.4221481680870056, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 142045.31981515884, 'total_duration': 154127.99576354027, 'accumulated_submission_time': 142045.31981515884, 'accumulated_eval_time': 12046.129980802536, 'accumulated_logging_time': 19.614728689193726}
I0304 06:22:32.459761 140380334978816 logging_writer.py:48] [319359] accumulated_eval_time=12046.129981, accumulated_logging_time=19.614729, accumulated_submission_time=142045.319815, global_step=319359, preemption_count=0, score=142045.319815, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=154127.995764, train/accuracy=0.888262, train/loss=0.422148, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:22:49.328336 140380326586112 logging_writer.py:48] [319400] global_step=319400, grad_norm=3.043102264404297, loss=1.7992924451828003
I0304 06:23:31.458643 140380334978816 logging_writer.py:48] [319500] global_step=319500, grad_norm=3.4632413387298584, loss=2.07942795753479
I0304 06:24:16.576483 140380326586112 logging_writer.py:48] [319600] global_step=319600, grad_norm=3.094289779663086, loss=1.0426281690597534
I0304 06:25:01.773958 140380334978816 logging_writer.py:48] [319700] global_step=319700, grad_norm=3.159569501876831, loss=2.122028112411499
I0304 06:25:46.768681 140380326586112 logging_writer.py:48] [319800] global_step=319800, grad_norm=2.9160382747650146, loss=1.5136091709136963
I0304 06:26:32.039561 140380334978816 logging_writer.py:48] [319900] global_step=319900, grad_norm=3.0225327014923096, loss=1.302504062652588
I0304 06:27:17.149858 140380326586112 logging_writer.py:48] [320000] global_step=320000, grad_norm=2.889822244644165, loss=1.123800277709961
I0304 06:28:02.443419 140380334978816 logging_writer.py:48] [320100] global_step=320100, grad_norm=2.9054629802703857, loss=1.1175416707992554
I0304 06:28:47.288857 140380326586112 logging_writer.py:48] [320200] global_step=320200, grad_norm=3.1922996044158936, loss=2.4265692234039307
I0304 06:29:32.503392 140380334978816 logging_writer.py:48] [320300] global_step=320300, grad_norm=2.9225571155548096, loss=2.471426486968994
I0304 06:29:32.517096 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:29:43.171694 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:30:06.408476 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:30:08.059308 140575196817216 submission_runner.py:411] Time since start: 154583.66s, 	Step: 320301, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.4201326370239258, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 142465.31846928596, 'total_duration': 154583.66032719612, 'accumulated_submission_time': 142465.31846928596, 'accumulated_eval_time': 12081.67114663124, 'accumulated_logging_time': 19.68934178352356}
I0304 06:30:08.122242 140380326586112 logging_writer.py:48] [320301] accumulated_eval_time=12081.671147, accumulated_logging_time=19.689342, accumulated_submission_time=142465.318469, global_step=320301, preemption_count=0, score=142465.318469, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=154583.660327, train/accuracy=0.887578, train/loss=0.420133, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:30:48.286558 140380334978816 logging_writer.py:48] [320400] global_step=320400, grad_norm=3.2125415802001953, loss=1.17580246925354
I0304 06:31:32.979795 140380326586112 logging_writer.py:48] [320500] global_step=320500, grad_norm=3.032085418701172, loss=1.4069769382476807
I0304 06:32:17.987782 140380334978816 logging_writer.py:48] [320600] global_step=320600, grad_norm=3.3472862243652344, loss=1.3327935934066772
I0304 06:33:02.912567 140380326586112 logging_writer.py:48] [320700] global_step=320700, grad_norm=3.9528870582580566, loss=1.1414391994476318
I0304 06:33:48.075447 140380334978816 logging_writer.py:48] [320800] global_step=320800, grad_norm=3.376563549041748, loss=1.1912927627563477
I0304 06:34:33.223276 140380326586112 logging_writer.py:48] [320900] global_step=320900, grad_norm=3.0669710636138916, loss=1.629096269607544
I0304 06:35:18.156861 140380334978816 logging_writer.py:48] [321000] global_step=321000, grad_norm=3.355994462966919, loss=1.1210381984710693
I0304 06:36:03.149118 140380326586112 logging_writer.py:48] [321100] global_step=321100, grad_norm=3.0621399879455566, loss=1.7863391637802124
I0304 06:36:48.060482 140380334978816 logging_writer.py:48] [321200] global_step=321200, grad_norm=3.9272072315216064, loss=1.7634590864181519
I0304 06:37:08.417256 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:37:19.184388 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:37:42.745638 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:37:44.390598 140575196817216 submission_runner.py:411] Time since start: 155039.99s, 	Step: 321247, 	{'train/accuracy': 0.8852929472923279, 'train/loss': 0.4266625940799713, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 142885.55553865433, 'total_duration': 155039.99154281616, 'accumulated_submission_time': 142885.55553865433, 'accumulated_eval_time': 12117.643364191055, 'accumulated_logging_time': 19.761374711990356}
I0304 06:37:44.454988 140380326586112 logging_writer.py:48] [321247] accumulated_eval_time=12117.643364, accumulated_logging_time=19.761375, accumulated_submission_time=142885.555539, global_step=321247, preemption_count=0, score=142885.555539, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=155039.991543, train/accuracy=0.885293, train/loss=0.426663, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:38:05.911210 140380334978816 logging_writer.py:48] [321300] global_step=321300, grad_norm=3.3421151638031006, loss=2.1867005825042725
I0304 06:38:48.630964 140380326586112 logging_writer.py:48] [321400] global_step=321400, grad_norm=3.0536510944366455, loss=1.0922383069992065
I0304 06:39:33.207174 140380334978816 logging_writer.py:48] [321500] global_step=321500, grad_norm=3.1759989261627197, loss=1.4890669584274292
I0304 06:40:18.544058 140380326586112 logging_writer.py:48] [321600] global_step=321600, grad_norm=3.09710431098938, loss=1.0435547828674316
I0304 06:41:03.487664 140380334978816 logging_writer.py:48] [321700] global_step=321700, grad_norm=3.038054943084717, loss=1.0560013055801392
I0304 06:41:48.277467 140380326586112 logging_writer.py:48] [321800] global_step=321800, grad_norm=3.6673014163970947, loss=3.2052719593048096
I0304 06:42:33.188723 140380334978816 logging_writer.py:48] [321900] global_step=321900, grad_norm=2.9469234943389893, loss=1.095508098602295
I0304 06:43:17.983137 140380326586112 logging_writer.py:48] [322000] global_step=322000, grad_norm=3.7155864238739014, loss=3.143559217453003
I0304 06:44:02.961061 140380334978816 logging_writer.py:48] [322100] global_step=322100, grad_norm=3.2307775020599365, loss=1.6891427040100098
I0304 06:44:44.739571 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:44:55.355544 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:45:19.051436 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:45:20.699593 140575196817216 submission_runner.py:411] Time since start: 155496.30s, 	Step: 322195, 	{'train/accuracy': 0.8888476490974426, 'train/loss': 0.4134816527366638, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 143305.7799217701, 'total_duration': 155496.300719738, 'accumulated_submission_time': 143305.7799217701, 'accumulated_eval_time': 12153.602460861206, 'accumulated_logging_time': 19.836210012435913}
I0304 06:45:20.765192 140380326586112 logging_writer.py:48] [322195] accumulated_eval_time=12153.602461, accumulated_logging_time=19.836210, accumulated_submission_time=143305.779922, global_step=322195, preemption_count=0, score=143305.779922, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=155496.300720, train/accuracy=0.888848, train/loss=0.413482, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:45:23.157397 140380334978816 logging_writer.py:48] [322200] global_step=322200, grad_norm=3.3680505752563477, loss=1.135446310043335
I0304 06:46:03.828454 140380326586112 logging_writer.py:48] [322300] global_step=322300, grad_norm=4.114133834838867, loss=2.691884756088257
I0304 06:46:48.822368 140380334978816 logging_writer.py:48] [322400] global_step=322400, grad_norm=4.449329853057861, loss=1.8862426280975342
I0304 06:47:34.213701 140380326586112 logging_writer.py:48] [322500] global_step=322500, grad_norm=4.2224531173706055, loss=3.166724681854248
I0304 06:48:19.353639 140380334978816 logging_writer.py:48] [322600] global_step=322600, grad_norm=3.000453472137451, loss=1.171104907989502
I0304 06:49:04.369308 140380326586112 logging_writer.py:48] [322700] global_step=322700, grad_norm=3.1431753635406494, loss=1.2004958391189575
I0304 06:49:49.519626 140380334978816 logging_writer.py:48] [322800] global_step=322800, grad_norm=3.140223741531372, loss=1.1475961208343506
I0304 06:50:34.484996 140380326586112 logging_writer.py:48] [322900] global_step=322900, grad_norm=2.9841997623443604, loss=1.1490565538406372
I0304 06:51:19.474894 140380334978816 logging_writer.py:48] [323000] global_step=323000, grad_norm=5.569030284881592, loss=1.283218264579773
I0304 06:52:04.720131 140380326586112 logging_writer.py:48] [323100] global_step=323100, grad_norm=3.01550555229187, loss=1.2653279304504395
I0304 06:52:20.944926 140575196817216 spec.py:321] Evaluating on the training split.
I0304 06:52:31.784711 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 06:52:58.358839 140575196817216 spec.py:349] Evaluating on the test split.
I0304 06:53:00.011811 140575196817216 submission_runner.py:411] Time since start: 155955.61s, 	Step: 323138, 	{'train/accuracy': 0.8867773413658142, 'train/loss': 0.42134973406791687, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 143725.90061926842, 'total_duration': 155955.61283946037, 'accumulated_submission_time': 143725.90061926842, 'accumulated_eval_time': 12192.668316364288, 'accumulated_logging_time': 19.912522077560425}
I0304 06:53:00.076509 140380334978816 logging_writer.py:48] [323138] accumulated_eval_time=12192.668316, accumulated_logging_time=19.912522, accumulated_submission_time=143725.900619, global_step=323138, preemption_count=0, score=143725.900619, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=155955.612839, train/accuracy=0.886777, train/loss=0.421350, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 06:53:25.100074 140380326586112 logging_writer.py:48] [323200] global_step=323200, grad_norm=10.0120267868042, loss=3.1255736351013184
I0304 06:54:08.564382 140380334978816 logging_writer.py:48] [323300] global_step=323300, grad_norm=2.9976534843444824, loss=1.157128930091858
I0304 06:54:53.683439 140380326586112 logging_writer.py:48] [323400] global_step=323400, grad_norm=3.1854586601257324, loss=1.603054165840149
I0304 06:55:38.978218 140380334978816 logging_writer.py:48] [323500] global_step=323500, grad_norm=3.154456853866577, loss=1.175871729850769
I0304 06:56:24.178826 140380326586112 logging_writer.py:48] [323600] global_step=323600, grad_norm=2.8867874145507812, loss=1.4154853820800781
I0304 06:57:09.500341 140380334978816 logging_writer.py:48] [323700] global_step=323700, grad_norm=3.220099687576294, loss=2.419020891189575
I0304 06:57:54.293942 140380326586112 logging_writer.py:48] [323800] global_step=323800, grad_norm=3.287465810775757, loss=2.432375907897949
I0304 06:58:39.338703 140380334978816 logging_writer.py:48] [323900] global_step=323900, grad_norm=2.7709035873413086, loss=1.50506591796875
I0304 06:59:24.293039 140380326586112 logging_writer.py:48] [324000] global_step=324000, grad_norm=3.3735909461975098, loss=1.101366639137268
I0304 07:00:00.143242 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:00:10.904662 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:00:35.489563 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:00:37.147700 140575196817216 submission_runner.py:411] Time since start: 156412.75s, 	Step: 324081, 	{'train/accuracy': 0.8880664110183716, 'train/loss': 0.41950756311416626, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 144145.90799617767, 'total_duration': 156412.74842858315, 'accumulated_submission_time': 144145.90799617767, 'accumulated_eval_time': 12229.671454191208, 'accumulated_logging_time': 19.987329483032227}
I0304 07:00:37.214206 140380334978816 logging_writer.py:48] [324081] accumulated_eval_time=12229.671454, accumulated_logging_time=19.987329, accumulated_submission_time=144145.907996, global_step=324081, preemption_count=0, score=144145.907996, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=156412.748429, train/accuracy=0.888066, train/loss=0.419508, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:00:45.160789 140380326586112 logging_writer.py:48] [324100] global_step=324100, grad_norm=3.062807083129883, loss=1.1154003143310547
I0304 07:01:26.215243 140380334978816 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.238506317138672, loss=2.6301116943359375
I0304 07:02:11.256439 140380326586112 logging_writer.py:48] [324300] global_step=324300, grad_norm=3.4547717571258545, loss=2.674785614013672
I0304 07:02:56.319722 140380334978816 logging_writer.py:48] [324400] global_step=324400, grad_norm=2.9249091148376465, loss=1.0499427318572998
I0304 07:03:41.086961 140380326586112 logging_writer.py:48] [324500] global_step=324500, grad_norm=3.4800751209259033, loss=2.860187292098999
I0304 07:04:26.043393 140380334978816 logging_writer.py:48] [324600] global_step=324600, grad_norm=3.504542827606201, loss=2.153270721435547
I0304 07:05:11.009682 140380326586112 logging_writer.py:48] [324700] global_step=324700, grad_norm=2.960164785385132, loss=2.1728928089141846
I0304 07:05:56.031540 140380334978816 logging_writer.py:48] [324800] global_step=324800, grad_norm=2.862881898880005, loss=1.7247577905654907
I0304 07:06:41.208922 140380326586112 logging_writer.py:48] [324900] global_step=324900, grad_norm=2.9562761783599854, loss=1.2454822063446045
I0304 07:07:26.064942 140380334978816 logging_writer.py:48] [325000] global_step=325000, grad_norm=3.0172064304351807, loss=1.5331189632415771
I0304 07:07:37.454581 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:07:48.209590 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:08:12.408822 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:08:14.070391 140575196817216 submission_runner.py:411] Time since start: 156869.67s, 	Step: 325027, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.41457727551460266, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 144566.0894215107, 'total_duration': 156869.671346426, 'accumulated_submission_time': 144566.0894215107, 'accumulated_eval_time': 12266.286154031754, 'accumulated_logging_time': 20.06376004219055}
I0304 07:08:14.135318 140380326586112 logging_writer.py:48] [325027] accumulated_eval_time=12266.286154, accumulated_logging_time=20.063760, accumulated_submission_time=144566.089422, global_step=325027, preemption_count=0, score=144566.089422, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=156869.671346, train/accuracy=0.887598, train/loss=0.414577, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:08:43.546208 140380334978816 logging_writer.py:48] [325100] global_step=325100, grad_norm=3.1581332683563232, loss=1.4116328954696655
I0304 07:09:27.375926 140380326586112 logging_writer.py:48] [325200] global_step=325200, grad_norm=4.30303955078125, loss=3.3566341400146484
I0304 07:10:12.381096 140380334978816 logging_writer.py:48] [325300] global_step=325300, grad_norm=3.282041549682617, loss=1.3166471719741821
I0304 07:10:57.621218 140380326586112 logging_writer.py:48] [325400] global_step=325400, grad_norm=3.0489351749420166, loss=1.1200127601623535
I0304 07:11:42.545538 140380334978816 logging_writer.py:48] [325500] global_step=325500, grad_norm=3.248903512954712, loss=1.2652974128723145
I0304 07:12:27.643363 140380326586112 logging_writer.py:48] [325600] global_step=325600, grad_norm=3.4247899055480957, loss=1.1137418746948242
I0304 07:13:12.619653 140380334978816 logging_writer.py:48] [325700] global_step=325700, grad_norm=3.1147491931915283, loss=1.1408514976501465
I0304 07:13:57.486109 140380326586112 logging_writer.py:48] [325800] global_step=325800, grad_norm=3.1057841777801514, loss=1.7055959701538086
I0304 07:14:42.675746 140380334978816 logging_writer.py:48] [325900] global_step=325900, grad_norm=3.0014190673828125, loss=1.0901401042938232
I0304 07:15:14.265603 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:15:24.994914 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:15:50.680736 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:15:52.331387 140575196817216 submission_runner.py:411] Time since start: 157327.93s, 	Step: 325972, 	{'train/accuracy': 0.8864648342132568, 'train/loss': 0.42130258679389954, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 144986.1606106758, 'total_duration': 157327.93233895302, 'accumulated_submission_time': 144986.1606106758, 'accumulated_eval_time': 12304.35082411766, 'accumulated_logging_time': 20.138731241226196}
I0304 07:15:52.396020 140380326586112 logging_writer.py:48] [325972] accumulated_eval_time=12304.350824, accumulated_logging_time=20.138731, accumulated_submission_time=144986.160611, global_step=325972, preemption_count=0, score=144986.160611, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=157327.932339, train/accuracy=0.886465, train/loss=0.421303, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:16:03.919795 140380334978816 logging_writer.py:48] [326000] global_step=326000, grad_norm=3.0257208347320557, loss=1.2130882740020752
I0304 07:16:45.547866 140380326586112 logging_writer.py:48] [326100] global_step=326100, grad_norm=3.1780478954315186, loss=1.550432562828064
I0304 07:17:30.796272 140380334978816 logging_writer.py:48] [326200] global_step=326200, grad_norm=3.4813215732574463, loss=1.2333215475082397
I0304 07:18:16.208782 140380326586112 logging_writer.py:48] [326300] global_step=326300, grad_norm=3.5274503231048584, loss=2.6121468544006348
I0304 07:19:01.092618 140380334978816 logging_writer.py:48] [326400] global_step=326400, grad_norm=3.542461395263672, loss=2.3827269077301025
I0304 07:19:46.043578 140380326586112 logging_writer.py:48] [326500] global_step=326500, grad_norm=3.484149217605591, loss=2.927952527999878
I0304 07:20:31.288651 140380334978816 logging_writer.py:48] [326600] global_step=326600, grad_norm=2.855208396911621, loss=1.113864779472351
I0304 07:21:16.432414 140380326586112 logging_writer.py:48] [326700] global_step=326700, grad_norm=3.3815112113952637, loss=2.759718179702759
I0304 07:22:01.964707 140380334978816 logging_writer.py:48] [326800] global_step=326800, grad_norm=3.192667007446289, loss=1.0593715906143188
I0304 07:22:47.001123 140380326586112 logging_writer.py:48] [326900] global_step=326900, grad_norm=3.281907796859741, loss=1.2240577936172485
I0304 07:22:52.645669 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:23:03.577461 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:23:29.737539 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:23:31.387831 140575196817216 submission_runner.py:411] Time since start: 157786.99s, 	Step: 326914, 	{'train/accuracy': 0.8908789157867432, 'train/loss': 0.4091778099536896, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 145406.35031175613, 'total_duration': 157786.98872613907, 'accumulated_submission_time': 145406.35031175613, 'accumulated_eval_time': 12343.091826438904, 'accumulated_logging_time': 20.214343786239624}
I0304 07:23:31.452756 140380334978816 logging_writer.py:48] [326914] accumulated_eval_time=12343.091826, accumulated_logging_time=20.214344, accumulated_submission_time=145406.350312, global_step=326914, preemption_count=0, score=145406.350312, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=157786.988726, train/accuracy=0.890879, train/loss=0.409178, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:24:06.123986 140380326586112 logging_writer.py:48] [327000] global_step=327000, grad_norm=3.0881829261779785, loss=1.2979471683502197
I0304 07:24:50.733905 140380334978816 logging_writer.py:48] [327100] global_step=327100, grad_norm=3.308631658554077, loss=1.1512824296951294
I0304 07:25:36.125994 140380326586112 logging_writer.py:48] [327200] global_step=327200, grad_norm=7.670009613037109, loss=3.215657949447632
I0304 07:26:21.475244 140380334978816 logging_writer.py:48] [327300] global_step=327300, grad_norm=3.2186598777770996, loss=1.7854975461959839
I0304 07:27:06.410237 140380326586112 logging_writer.py:48] [327400] global_step=327400, grad_norm=3.2394192218780518, loss=1.1309350728988647
I0304 07:27:51.515666 140380334978816 logging_writer.py:48] [327500] global_step=327500, grad_norm=2.8598432540893555, loss=1.2447363138198853
I0304 07:28:36.543083 140380326586112 logging_writer.py:48] [327600] global_step=327600, grad_norm=3.158672332763672, loss=1.1833808422088623
I0304 07:29:21.551974 140380334978816 logging_writer.py:48] [327700] global_step=327700, grad_norm=2.963869571685791, loss=1.7521922588348389
I0304 07:30:06.653841 140380326586112 logging_writer.py:48] [327800] global_step=327800, grad_norm=3.1666743755340576, loss=1.1246471405029297
I0304 07:30:31.484461 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:30:42.075768 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:31:08.832933 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:31:10.485664 140575196817216 submission_runner.py:411] Time since start: 158246.09s, 	Step: 327857, 	{'train/accuracy': 0.8892382383346558, 'train/loss': 0.4134601950645447, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 145826.32364201546, 'total_duration': 158246.08665442467, 'accumulated_submission_time': 145826.32364201546, 'accumulated_eval_time': 12382.091963529587, 'accumulated_logging_time': 20.28935670852661}
I0304 07:31:10.562997 140380334978816 logging_writer.py:48] [327857] accumulated_eval_time=12382.091964, accumulated_logging_time=20.289357, accumulated_submission_time=145826.323642, global_step=327857, preemption_count=0, score=145826.323642, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=158246.086654, train/accuracy=0.889238, train/loss=0.413460, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:31:28.050876 140380326586112 logging_writer.py:48] [327900] global_step=327900, grad_norm=5.029110431671143, loss=1.6335842609405518
I0304 07:32:10.336490 140380334978816 logging_writer.py:48] [328000] global_step=328000, grad_norm=3.0695550441741943, loss=1.0765308141708374
I0304 07:32:55.340464 140380326586112 logging_writer.py:48] [328100] global_step=328100, grad_norm=3.4382684230804443, loss=2.4462833404541016
I0304 07:33:40.505709 140380334978816 logging_writer.py:48] [328200] global_step=328200, grad_norm=2.9961674213409424, loss=1.3986912965774536
I0304 07:34:25.484232 140380326586112 logging_writer.py:48] [328300] global_step=328300, grad_norm=3.22048282623291, loss=1.1695077419281006
I0304 07:35:10.664113 140380334978816 logging_writer.py:48] [328400] global_step=328400, grad_norm=2.9993488788604736, loss=1.966521143913269
I0304 07:35:55.558015 140380326586112 logging_writer.py:48] [328500] global_step=328500, grad_norm=6.331289291381836, loss=3.3330347537994385
I0304 07:36:40.509844 140380334978816 logging_writer.py:48] [328600] global_step=328600, grad_norm=3.2510628700256348, loss=1.2981178760528564
I0304 07:37:25.481905 140380326586112 logging_writer.py:48] [328700] global_step=328700, grad_norm=3.2761406898498535, loss=1.4891632795333862
I0304 07:38:10.622724 140380334978816 logging_writer.py:48] [328800] global_step=328800, grad_norm=3.2965285778045654, loss=1.066227912902832
I0304 07:38:10.638191 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:38:21.410621 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:38:45.315603 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:38:46.967275 140575196817216 submission_runner.py:411] Time since start: 158702.57s, 	Step: 328801, 	{'train/accuracy': 0.887499988079071, 'train/loss': 0.41385096311569214, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 146246.33985328674, 'total_duration': 158702.5681695938, 'accumulated_submission_time': 146246.33985328674, 'accumulated_eval_time': 12418.419880151749, 'accumulated_logging_time': 20.377084732055664}
I0304 07:38:47.032356 140380326586112 logging_writer.py:48] [328801] accumulated_eval_time=12418.419880, accumulated_logging_time=20.377085, accumulated_submission_time=146246.339853, global_step=328801, preemption_count=0, score=146246.339853, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=158702.568170, train/accuracy=0.887500, train/loss=0.413851, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:39:27.110856 140380334978816 logging_writer.py:48] [328900] global_step=328900, grad_norm=3.2695298194885254, loss=1.048567533493042
I0304 07:40:11.388143 140380326586112 logging_writer.py:48] [329000] global_step=329000, grad_norm=3.2260842323303223, loss=1.8273887634277344
I0304 07:40:56.852756 140380334978816 logging_writer.py:48] [329100] global_step=329100, grad_norm=3.315624475479126, loss=1.1313313245773315
I0304 07:41:42.364216 140380326586112 logging_writer.py:48] [329200] global_step=329200, grad_norm=3.040367364883423, loss=1.0391168594360352
I0304 07:42:27.429084 140380334978816 logging_writer.py:48] [329300] global_step=329300, grad_norm=3.0517687797546387, loss=2.556767463684082
I0304 07:43:12.656756 140380326586112 logging_writer.py:48] [329400] global_step=329400, grad_norm=3.264948606491089, loss=1.1120015382766724
I0304 07:43:57.500369 140380334978816 logging_writer.py:48] [329500] global_step=329500, grad_norm=3.353358507156372, loss=1.1900089979171753
I0304 07:44:42.576095 140380326586112 logging_writer.py:48] [329600] global_step=329600, grad_norm=4.1945719718933105, loss=1.4740501642227173
I0304 07:45:27.526312 140380334978816 logging_writer.py:48] [329700] global_step=329700, grad_norm=3.421860694885254, loss=1.2563313245773315
I0304 07:45:47.513864 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:45:59.258772 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:46:22.723936 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:46:24.377424 140575196817216 submission_runner.py:411] Time since start: 159159.98s, 	Step: 329746, 	{'train/accuracy': 0.8868359327316284, 'train/loss': 0.42218390107154846, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 146666.76261496544, 'total_duration': 159159.97850561142, 'accumulated_submission_time': 146666.76261496544, 'accumulated_eval_time': 12455.2824716568, 'accumulated_logging_time': 20.451382637023926}
I0304 07:46:24.442772 140380326586112 logging_writer.py:48] [329746] accumulated_eval_time=12455.282472, accumulated_logging_time=20.451383, accumulated_submission_time=146666.762615, global_step=329746, preemption_count=0, score=146666.762615, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=159159.978506, train/accuracy=0.886836, train/loss=0.422184, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:46:46.313988 140380334978816 logging_writer.py:48] [329800] global_step=329800, grad_norm=3.49881911277771, loss=2.6670825481414795
I0304 07:47:29.033318 140380326586112 logging_writer.py:48] [329900] global_step=329900, grad_norm=3.304147243499756, loss=2.067403554916382
I0304 07:48:14.163044 140380334978816 logging_writer.py:48] [330000] global_step=330000, grad_norm=3.0542452335357666, loss=1.0530763864517212
I0304 07:48:59.589608 140380326586112 logging_writer.py:48] [330100] global_step=330100, grad_norm=3.390118360519409, loss=1.2252625226974487
I0304 07:49:44.542703 140380334978816 logging_writer.py:48] [330200] global_step=330200, grad_norm=2.9539647102355957, loss=1.6607743501663208
I0304 07:50:29.756473 140380326586112 logging_writer.py:48] [330300] global_step=330300, grad_norm=3.6604843139648438, loss=1.3801428079605103
I0304 07:51:14.803314 140380334978816 logging_writer.py:48] [330400] global_step=330400, grad_norm=3.003267288208008, loss=1.6251096725463867
I0304 07:51:59.848833 140380326586112 logging_writer.py:48] [330500] global_step=330500, grad_norm=3.2807562351226807, loss=2.6606860160827637
I0304 07:52:44.830287 140380334978816 logging_writer.py:48] [330600] global_step=330600, grad_norm=3.1358413696289062, loss=1.5183641910552979
I0304 07:53:24.666627 140575196817216 spec.py:321] Evaluating on the training split.
I0304 07:53:35.530694 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 07:53:59.224124 140575196817216 spec.py:349] Evaluating on the test split.
I0304 07:54:00.877359 140575196817216 submission_runner.py:411] Time since start: 159616.48s, 	Step: 330690, 	{'train/accuracy': 0.8869531154632568, 'train/loss': 0.4258280098438263, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 147086.9284837246, 'total_duration': 159616.47861504555, 'accumulated_submission_time': 147086.9284837246, 'accumulated_eval_time': 12491.49240732193, 'accumulated_logging_time': 20.52607488632202}
I0304 07:54:00.955642 140380326586112 logging_writer.py:48] [330690] accumulated_eval_time=12491.492407, accumulated_logging_time=20.526075, accumulated_submission_time=147086.928484, global_step=330690, preemption_count=0, score=147086.928484, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=159616.478615, train/accuracy=0.886953, train/loss=0.425828, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 07:54:05.603347 140380334978816 logging_writer.py:48] [330700] global_step=330700, grad_norm=3.2444212436676025, loss=1.2051677703857422
I0304 07:54:46.531240 140380326586112 logging_writer.py:48] [330800] global_step=330800, grad_norm=3.3665354251861572, loss=1.1012006998062134
I0304 07:55:31.419639 140380334978816 logging_writer.py:48] [330900] global_step=330900, grad_norm=3.468369722366333, loss=1.9520317316055298
I0304 07:56:16.738980 140380326586112 logging_writer.py:48] [331000] global_step=331000, grad_norm=3.709254741668701, loss=2.815974473953247
I0304 07:57:01.863705 140380334978816 logging_writer.py:48] [331100] global_step=331100, grad_norm=5.092219352722168, loss=1.2032887935638428
I0304 07:57:46.828576 140380326586112 logging_writer.py:48] [331200] global_step=331200, grad_norm=2.88763689994812, loss=1.1301690340042114
I0304 07:58:32.076043 140380334978816 logging_writer.py:48] [331300] global_step=331300, grad_norm=3.4343795776367188, loss=1.1308566331863403
I0304 07:59:16.960640 140380326586112 logging_writer.py:48] [331400] global_step=331400, grad_norm=3.348353862762451, loss=1.1151187419891357
I0304 08:00:01.924880 140380334978816 logging_writer.py:48] [331500] global_step=331500, grad_norm=3.2980549335479736, loss=2.487119436264038
I0304 08:00:47.071829 140380326586112 logging_writer.py:48] [331600] global_step=331600, grad_norm=3.9274730682373047, loss=3.0995068550109863
I0304 08:01:01.043846 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:01:12.019218 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:01:37.488867 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:01:39.147574 140575196817216 submission_runner.py:411] Time since start: 160074.75s, 	Step: 331633, 	{'train/accuracy': 0.8866601586341858, 'train/loss': 0.42062777280807495, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 147506.69037270546, 'total_duration': 160074.74869942665, 'accumulated_submission_time': 147506.69037270546, 'accumulated_eval_time': 12529.595186471939, 'accumulated_logging_time': 20.882078886032104}
I0304 08:01:39.213000 140380334978816 logging_writer.py:48] [331633] accumulated_eval_time=12529.595186, accumulated_logging_time=20.882079, accumulated_submission_time=147506.690373, global_step=331633, preemption_count=0, score=147506.690373, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=160074.748699, train/accuracy=0.886660, train/loss=0.420628, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:02:06.255990 140380326586112 logging_writer.py:48] [331700] global_step=331700, grad_norm=3.078462839126587, loss=2.030839681625366
I0304 08:02:50.211063 140380334978816 logging_writer.py:48] [331800] global_step=331800, grad_norm=4.039121150970459, loss=1.1606367826461792
I0304 08:03:35.710020 140380326586112 logging_writer.py:48] [331900] global_step=331900, grad_norm=3.2386910915374756, loss=1.5120152235031128
I0304 08:04:21.014615 140380334978816 logging_writer.py:48] [332000] global_step=332000, grad_norm=3.1179516315460205, loss=1.2303447723388672
I0304 08:05:06.421581 140380326586112 logging_writer.py:48] [332100] global_step=332100, grad_norm=3.164034843444824, loss=1.7934930324554443
I0304 08:05:51.640758 140380334978816 logging_writer.py:48] [332200] global_step=332200, grad_norm=3.029104232788086, loss=2.0980417728424072
I0304 08:06:36.826436 140380326586112 logging_writer.py:48] [332300] global_step=332300, grad_norm=3.2161710262298584, loss=1.8874114751815796
I0304 08:07:21.982262 140380334978816 logging_writer.py:48] [332400] global_step=332400, grad_norm=3.0797998905181885, loss=1.3159083127975464
I0304 08:08:07.262759 140380326586112 logging_writer.py:48] [332500] global_step=332500, grad_norm=3.4133989810943604, loss=1.1186809539794922
I0304 08:08:39.437066 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:08:50.127671 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:09:14.090302 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:09:15.762297 140575196817216 submission_runner.py:411] Time since start: 160531.36s, 	Step: 332573, 	{'train/accuracy': 0.8890624642372131, 'train/loss': 0.41694191098213196, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 147926.8549695015, 'total_duration': 160531.36344838142, 'accumulated_submission_time': 147926.8549695015, 'accumulated_eval_time': 12565.919513225555, 'accumulated_logging_time': 20.957940578460693}
I0304 08:09:15.828899 140380334978816 logging_writer.py:48] [332573] accumulated_eval_time=12565.919513, accumulated_logging_time=20.957941, accumulated_submission_time=147926.854970, global_step=332573, preemption_count=0, score=147926.854970, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=160531.363448, train/accuracy=0.889062, train/loss=0.416942, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:09:26.985468 140380326586112 logging_writer.py:48] [332600] global_step=332600, grad_norm=3.0440919399261475, loss=1.0662730932235718
I0304 08:10:08.625474 140380334978816 logging_writer.py:48] [332700] global_step=332700, grad_norm=3.7991039752960205, loss=3.2816131114959717
I0304 08:10:53.520300 140380326586112 logging_writer.py:48] [332800] global_step=332800, grad_norm=3.863060712814331, loss=3.2224156856536865
I0304 08:11:38.602936 140380334978816 logging_writer.py:48] [332900] global_step=332900, grad_norm=3.2668097019195557, loss=1.3018659353256226
I0304 08:12:23.783246 140380326586112 logging_writer.py:48] [333000] global_step=333000, grad_norm=3.2642202377319336, loss=1.480825662612915
I0304 08:13:08.934230 140380334978816 logging_writer.py:48] [333100] global_step=333100, grad_norm=3.0152695178985596, loss=1.0243265628814697
I0304 08:13:54.324797 140380326586112 logging_writer.py:48] [333200] global_step=333200, grad_norm=3.0835399627685547, loss=1.143107533454895
I0304 08:14:39.484408 140380334978816 logging_writer.py:48] [333300] global_step=333300, grad_norm=3.7813942432403564, loss=1.3214256763458252
I0304 08:15:24.471431 140380326586112 logging_writer.py:48] [333400] global_step=333400, grad_norm=3.1008496284484863, loss=1.1751731634140015
I0304 08:16:09.427785 140380334978816 logging_writer.py:48] [333500] global_step=333500, grad_norm=3.123318910598755, loss=1.0750505924224854
I0304 08:16:16.143742 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:16:26.979083 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:16:52.174788 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:16:53.841800 140575196817216 submission_runner.py:411] Time since start: 160989.44s, 	Step: 333516, 	{'train/accuracy': 0.8870702981948853, 'train/loss': 0.4185446500778198, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 148347.11112332344, 'total_duration': 160989.44289064407, 'accumulated_submission_time': 148347.11112332344, 'accumulated_eval_time': 12603.616605520248, 'accumulated_logging_time': 21.034954071044922}
I0304 08:16:53.907537 140380326586112 logging_writer.py:48] [333516] accumulated_eval_time=12603.616606, accumulated_logging_time=21.034954, accumulated_submission_time=148347.111123, global_step=333516, preemption_count=0, score=148347.111123, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=160989.442891, train/accuracy=0.887070, train/loss=0.418545, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:17:27.684537 140380334978816 logging_writer.py:48] [333600] global_step=333600, grad_norm=3.7444822788238525, loss=3.1824703216552734
I0304 08:18:12.296594 140380326586112 logging_writer.py:48] [333700] global_step=333700, grad_norm=4.135127544403076, loss=2.9111437797546387
I0304 08:18:57.461406 140380334978816 logging_writer.py:48] [333800] global_step=333800, grad_norm=3.005802631378174, loss=2.5027377605438232
I0304 08:19:42.705613 140380326586112 logging_writer.py:48] [333900] global_step=333900, grad_norm=3.0985066890716553, loss=1.0246609449386597
I0304 08:20:27.652186 140380334978816 logging_writer.py:48] [334000] global_step=334000, grad_norm=3.1530847549438477, loss=1.0703625679016113
I0304 08:21:13.262013 140380326586112 logging_writer.py:48] [334100] global_step=334100, grad_norm=3.041208505630493, loss=1.1931570768356323
I0304 08:21:58.246648 140380334978816 logging_writer.py:48] [334200] global_step=334200, grad_norm=4.074643611907959, loss=3.1506400108337402
I0304 08:22:43.287534 140380326586112 logging_writer.py:48] [334300] global_step=334300, grad_norm=3.3697078227996826, loss=1.1476500034332275
I0304 08:23:28.300219 140380334978816 logging_writer.py:48] [334400] global_step=334400, grad_norm=3.4960758686065674, loss=2.803104877471924
I0304 08:23:54.099511 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:24:04.897658 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:24:28.251037 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:24:29.910228 140575196817216 submission_runner.py:411] Time since start: 161445.51s, 	Step: 334459, 	{'train/accuracy': 0.8873242139816284, 'train/loss': 0.4183368682861328, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 148767.2456228733, 'total_duration': 161445.51134824753, 'accumulated_submission_time': 148767.2456228733, 'accumulated_eval_time': 12639.42637515068, 'accumulated_logging_time': 21.11026430130005}
I0304 08:24:29.975034 140380326586112 logging_writer.py:48] [334459] accumulated_eval_time=12639.426375, accumulated_logging_time=21.110264, accumulated_submission_time=148767.245623, global_step=334459, preemption_count=0, score=148767.245623, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=161445.511348, train/accuracy=0.887324, train/loss=0.418337, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:24:46.656793 140380334978816 logging_writer.py:48] [334500] global_step=334500, grad_norm=3.472320795059204, loss=2.897242307662964
I0304 08:25:28.996352 140380326586112 logging_writer.py:48] [334600] global_step=334600, grad_norm=3.137453317642212, loss=1.1326173543930054
I0304 08:26:14.108323 140380334978816 logging_writer.py:48] [334700] global_step=334700, grad_norm=3.1487457752227783, loss=1.199759602546692
I0304 08:26:59.440083 140380326586112 logging_writer.py:48] [334800] global_step=334800, grad_norm=3.0762085914611816, loss=1.0995687246322632
I0304 08:27:44.179049 140380334978816 logging_writer.py:48] [334900] global_step=334900, grad_norm=3.048556327819824, loss=1.6569550037384033
I0304 08:28:28.972658 140380326586112 logging_writer.py:48] [335000] global_step=335000, grad_norm=3.141770362854004, loss=2.1325392723083496
I0304 08:29:13.874191 140380334978816 logging_writer.py:48] [335100] global_step=335100, grad_norm=3.0030720233917236, loss=1.7611825466156006
I0304 08:29:58.373076 140380326586112 logging_writer.py:48] [335200] global_step=335200, grad_norm=3.1224589347839355, loss=1.7027984857559204
I0304 08:30:43.474692 140380334978816 logging_writer.py:48] [335300] global_step=335300, grad_norm=3.003523349761963, loss=1.9569660425186157
I0304 08:31:28.870587 140380326586112 logging_writer.py:48] [335400] global_step=335400, grad_norm=3.143406629562378, loss=1.1506335735321045
I0304 08:31:30.037964 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:31:40.626323 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:32:05.058358 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:32:06.713392 140575196817216 submission_runner.py:411] Time since start: 161902.31s, 	Step: 335404, 	{'train/accuracy': 0.8857226371765137, 'train/loss': 0.4270011782646179, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 149187.25118470192, 'total_duration': 161902.31423664093, 'accumulated_submission_time': 149187.25118470192, 'accumulated_eval_time': 12676.100577354431, 'accumulated_logging_time': 21.18437361717224}
I0304 08:32:06.789838 140380334978816 logging_writer.py:48] [335404] accumulated_eval_time=12676.100577, accumulated_logging_time=21.184374, accumulated_submission_time=149187.251185, global_step=335404, preemption_count=0, score=149187.251185, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=161902.314237, train/accuracy=0.885723, train/loss=0.427001, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:32:45.671944 140380326586112 logging_writer.py:48] [335500] global_step=335500, grad_norm=3.729018449783325, loss=3.2824676036834717
I0304 08:33:30.998699 140380334978816 logging_writer.py:48] [335600] global_step=335600, grad_norm=3.476505756378174, loss=1.1646935939788818
I0304 08:34:16.308435 140380326586112 logging_writer.py:48] [335700] global_step=335700, grad_norm=3.2353851795196533, loss=1.4138625860214233
I0304 08:35:01.472223 140380334978816 logging_writer.py:48] [335800] global_step=335800, grad_norm=2.9271628856658936, loss=1.5746315717697144
I0304 08:35:46.894732 140380326586112 logging_writer.py:48] [335900] global_step=335900, grad_norm=3.346112012863159, loss=1.088461995124817
I0304 08:36:32.424294 140380334978816 logging_writer.py:48] [336000] global_step=336000, grad_norm=3.2457120418548584, loss=1.3395309448242188
I0304 08:37:17.681853 140380326586112 logging_writer.py:48] [336100] global_step=336100, grad_norm=2.9334006309509277, loss=0.9849615097045898
I0304 08:38:02.904461 140380334978816 logging_writer.py:48] [336200] global_step=336200, grad_norm=3.5809504985809326, loss=3.1159791946411133
I0304 08:38:48.145110 140380326586112 logging_writer.py:48] [336300] global_step=336300, grad_norm=3.1455636024475098, loss=1.201192021369934
I0304 08:39:06.981985 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:39:17.673829 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:39:43.887995 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:39:45.544015 140575196817216 submission_runner.py:411] Time since start: 162361.14s, 	Step: 336343, 	{'train/accuracy': 0.8882421851158142, 'train/loss': 0.41847455501556396, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 149607.38150835037, 'total_duration': 162361.1449034214, 'accumulated_submission_time': 149607.38150835037, 'accumulated_eval_time': 12714.661440134048, 'accumulated_logging_time': 21.27260971069336}
I0304 08:39:45.612067 140380334978816 logging_writer.py:48] [336343] accumulated_eval_time=12714.661440, accumulated_logging_time=21.272610, accumulated_submission_time=149607.381508, global_step=336343, preemption_count=0, score=149607.381508, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=162361.144903, train/accuracy=0.888242, train/loss=0.418475, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:40:08.666145 140380326586112 logging_writer.py:48] [336400] global_step=336400, grad_norm=3.4859020709991455, loss=1.2562041282653809
I0304 08:40:51.858515 140380334978816 logging_writer.py:48] [336500] global_step=336500, grad_norm=2.972079277038574, loss=2.0496535301208496
I0304 08:41:37.121485 140380326586112 logging_writer.py:48] [336600] global_step=336600, grad_norm=3.0801095962524414, loss=1.2189967632293701
I0304 08:42:22.642356 140380334978816 logging_writer.py:48] [336700] global_step=336700, grad_norm=2.914578914642334, loss=1.1256619691848755
I0304 08:43:07.705429 140380326586112 logging_writer.py:48] [336800] global_step=336800, grad_norm=3.318950653076172, loss=1.2501620054244995
I0304 08:43:52.907909 140380334978816 logging_writer.py:48] [336900] global_step=336900, grad_norm=3.133761167526245, loss=1.119613528251648
I0304 08:44:38.332701 140380326586112 logging_writer.py:48] [337000] global_step=337000, grad_norm=3.5460617542266846, loss=1.162077784538269
I0304 08:45:23.573215 140380334978816 logging_writer.py:48] [337100] global_step=337100, grad_norm=3.3069651126861572, loss=1.047192096710205
I0304 08:46:08.869274 140380326586112 logging_writer.py:48] [337200] global_step=337200, grad_norm=4.067586898803711, loss=1.6322424411773682
I0304 08:46:45.705756 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:46:56.423496 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:47:19.965045 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:47:21.629303 140575196817216 submission_runner.py:411] Time since start: 162817.23s, 	Step: 337283, 	{'train/accuracy': 0.8860155940055847, 'train/loss': 0.4216941297054291, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 150027.41157388687, 'total_duration': 162817.23032855988, 'accumulated_submission_time': 150027.41157388687, 'accumulated_eval_time': 12750.583944559097, 'accumulated_logging_time': 21.355368614196777}
I0304 08:47:21.700865 140380334978816 logging_writer.py:48] [337283] accumulated_eval_time=12750.583945, accumulated_logging_time=21.355369, accumulated_submission_time=150027.411574, global_step=337283, preemption_count=0, score=150027.411574, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=162817.230329, train/accuracy=0.886016, train/loss=0.421694, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:47:28.864156 140380326586112 logging_writer.py:48] [337300] global_step=337300, grad_norm=3.0462822914123535, loss=2.3962697982788086
I0304 08:48:10.173556 140380334978816 logging_writer.py:48] [337400] global_step=337400, grad_norm=3.3334410190582275, loss=2.663606643676758
I0304 08:48:55.387501 140380326586112 logging_writer.py:48] [337500] global_step=337500, grad_norm=2.9799163341522217, loss=1.1618252992630005
I0304 08:49:41.125755 140380334978816 logging_writer.py:48] [337600] global_step=337600, grad_norm=2.7988903522491455, loss=1.1011415719985962
I0304 08:50:26.326236 140380326586112 logging_writer.py:48] [337700] global_step=337700, grad_norm=3.687394857406616, loss=2.7570583820343018
I0304 08:51:11.615237 140380334978816 logging_writer.py:48] [337800] global_step=337800, grad_norm=3.217494487762451, loss=1.5154485702514648
I0304 08:51:56.863518 140380326586112 logging_writer.py:48] [337900] global_step=337900, grad_norm=3.5427091121673584, loss=3.1087021827697754
I0304 08:52:41.764062 140380334978816 logging_writer.py:48] [338000] global_step=338000, grad_norm=3.3616368770599365, loss=1.2324646711349487
I0304 08:53:26.800743 140380326586112 logging_writer.py:48] [338100] global_step=338100, grad_norm=3.501046657562256, loss=2.3804712295532227
I0304 08:54:11.829046 140380334978816 logging_writer.py:48] [338200] global_step=338200, grad_norm=4.682826995849609, loss=3.3369193077087402
I0304 08:54:21.920040 140575196817216 spec.py:321] Evaluating on the training split.
I0304 08:54:32.797960 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 08:54:58.992967 140575196817216 spec.py:349] Evaluating on the test split.
I0304 08:55:00.648735 140575196817216 submission_runner.py:411] Time since start: 163276.25s, 	Step: 338224, 	{'train/accuracy': 0.88636714220047, 'train/loss': 0.42081913352012634, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 150447.57310414314, 'total_duration': 163276.24970722198, 'accumulated_submission_time': 150447.57310414314, 'accumulated_eval_time': 12789.311537981033, 'accumulated_logging_time': 21.436509132385254}
I0304 08:55:00.722598 140380326586112 logging_writer.py:48] [338224] accumulated_eval_time=12789.311538, accumulated_logging_time=21.436509, accumulated_submission_time=150447.573104, global_step=338224, preemption_count=0, score=150447.573104, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=163276.249707, train/accuracy=0.886367, train/loss=0.420819, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 08:55:31.323908 140380334978816 logging_writer.py:48] [338300] global_step=338300, grad_norm=3.3372960090637207, loss=1.201285481452942
I0304 08:56:15.607866 140380326586112 logging_writer.py:48] [338400] global_step=338400, grad_norm=3.030330181121826, loss=1.1247406005859375
I0304 08:57:00.747300 140380334978816 logging_writer.py:48] [338500] global_step=338500, grad_norm=3.1348984241485596, loss=1.1596293449401855
I0304 08:57:45.928644 140380326586112 logging_writer.py:48] [338600] global_step=338600, grad_norm=3.1603221893310547, loss=2.5622708797454834
I0304 08:58:30.839471 140380334978816 logging_writer.py:48] [338700] global_step=338700, grad_norm=6.0916571617126465, loss=3.2622745037078857
I0304 08:59:15.798572 140380326586112 logging_writer.py:48] [338800] global_step=338800, grad_norm=3.2356643676757812, loss=1.9762548208236694
I0304 09:00:00.874937 140380334978816 logging_writer.py:48] [338900] global_step=338900, grad_norm=3.8171041011810303, loss=2.032248020172119
I0304 09:00:45.764464 140380326586112 logging_writer.py:48] [339000] global_step=339000, grad_norm=3.2213492393493652, loss=1.4150052070617676
I0304 09:01:31.141149 140380334978816 logging_writer.py:48] [339100] global_step=339100, grad_norm=3.70212984085083, loss=3.132805824279785
I0304 09:02:00.837765 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:02:11.550033 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:02:36.263274 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:02:37.924054 140575196817216 submission_runner.py:411] Time since start: 163733.52s, 	Step: 339168, 	{'train/accuracy': 0.8886523246765137, 'train/loss': 0.4195660650730133, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 150867.62800478935, 'total_duration': 163733.52498984337, 'accumulated_submission_time': 150867.62800478935, 'accumulated_eval_time': 12826.396708726883, 'accumulated_logging_time': 21.521263122558594}
I0304 09:02:37.999228 140380326586112 logging_writer.py:48] [339168] accumulated_eval_time=12826.396709, accumulated_logging_time=21.521263, accumulated_submission_time=150867.628005, global_step=339168, preemption_count=0, score=150867.628005, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=163733.524990, train/accuracy=0.888652, train/loss=0.419566, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:02:51.125680 140380334978816 logging_writer.py:48] [339200] global_step=339200, grad_norm=3.3939132690429688, loss=2.929124593734741
I0304 09:03:32.562326 140380326586112 logging_writer.py:48] [339300] global_step=339300, grad_norm=2.7822988033294678, loss=1.0082000494003296
I0304 09:04:17.576356 140380334978816 logging_writer.py:48] [339400] global_step=339400, grad_norm=3.8756632804870605, loss=1.0855352878570557
I0304 09:05:02.836412 140380326586112 logging_writer.py:48] [339500] global_step=339500, grad_norm=3.057497024536133, loss=1.2284437417984009
I0304 09:05:47.948303 140380334978816 logging_writer.py:48] [339600] global_step=339600, grad_norm=2.89052152633667, loss=1.0274018049240112
I0304 09:06:33.318808 140380326586112 logging_writer.py:48] [339700] global_step=339700, grad_norm=3.1712982654571533, loss=1.295371651649475
I0304 09:07:18.271477 140380334978816 logging_writer.py:48] [339800] global_step=339800, grad_norm=3.2194573879241943, loss=1.1738524436950684
I0304 09:08:03.232919 140380326586112 logging_writer.py:48] [339900] global_step=339900, grad_norm=3.3821775913238525, loss=1.2433325052261353
I0304 09:08:48.136320 140380334978816 logging_writer.py:48] [340000] global_step=340000, grad_norm=3.8927743434906006, loss=2.8888368606567383
I0304 09:09:32.977726 140380326586112 logging_writer.py:48] [340100] global_step=340100, grad_norm=3.3789315223693848, loss=1.1900463104248047
I0304 09:09:37.993363 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:09:48.638164 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:10:12.889639 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:10:14.561229 140575196817216 submission_runner.py:411] Time since start: 164190.16s, 	Step: 340113, 	{'train/accuracy': 0.8841210603713989, 'train/loss': 0.4273837208747864, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 151287.56206703186, 'total_duration': 164190.16235351562, 'accumulated_submission_time': 151287.56206703186, 'accumulated_eval_time': 12862.963638782501, 'accumulated_logging_time': 21.607585906982422}
I0304 09:10:14.627379 140380334978816 logging_writer.py:48] [340113] accumulated_eval_time=12862.963639, accumulated_logging_time=21.607586, accumulated_submission_time=151287.562067, global_step=340113, preemption_count=0, score=151287.562067, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=164190.162354, train/accuracy=0.884121, train/loss=0.427384, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:10:49.614865 140380326586112 logging_writer.py:48] [340200] global_step=340200, grad_norm=4.58866548538208, loss=1.1745601892471313
I0304 09:11:34.167802 140380334978816 logging_writer.py:48] [340300] global_step=340300, grad_norm=3.277553081512451, loss=1.168139934539795
I0304 09:12:18.979559 140380326586112 logging_writer.py:48] [340400] global_step=340400, grad_norm=3.267317295074463, loss=1.204290747642517
I0304 09:13:04.264637 140380334978816 logging_writer.py:48] [340500] global_step=340500, grad_norm=3.107290506362915, loss=1.371085286140442
I0304 09:13:49.058434 140380326586112 logging_writer.py:48] [340600] global_step=340600, grad_norm=3.406310558319092, loss=2.988276720046997
I0304 09:14:33.857378 140380334978816 logging_writer.py:48] [340700] global_step=340700, grad_norm=3.4176599979400635, loss=2.7612133026123047
I0304 09:15:18.793323 140380326586112 logging_writer.py:48] [340800] global_step=340800, grad_norm=3.572793960571289, loss=3.0117571353912354
I0304 09:16:03.873231 140380334978816 logging_writer.py:48] [340900] global_step=340900, grad_norm=3.307030439376831, loss=1.1003618240356445
I0304 09:16:48.993879 140380326586112 logging_writer.py:48] [341000] global_step=341000, grad_norm=3.574678897857666, loss=1.4943047761917114
I0304 09:17:14.659375 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:17:25.470255 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:17:50.182958 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:17:51.830933 140575196817216 submission_runner.py:411] Time since start: 164647.43s, 	Step: 341059, 	{'train/accuracy': 0.8888671398162842, 'train/loss': 0.4130294620990753, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 151707.53594970703, 'total_duration': 164647.4318766594, 'accumulated_submission_time': 151707.53594970703, 'accumulated_eval_time': 12900.13408112526, 'accumulated_logging_time': 21.682916164398193}
I0304 09:17:51.896526 140380334978816 logging_writer.py:48] [341059] accumulated_eval_time=12900.134081, accumulated_logging_time=21.682916, accumulated_submission_time=151707.535950, global_step=341059, preemption_count=0, score=151707.535950, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=164647.431877, train/accuracy=0.888867, train/loss=0.413029, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:18:08.586914 140380326586112 logging_writer.py:48] [341100] global_step=341100, grad_norm=4.092190742492676, loss=3.174159049987793
I0304 09:18:50.890616 140380334978816 logging_writer.py:48] [341200] global_step=341200, grad_norm=2.8782193660736084, loss=1.0682188272476196
I0304 09:19:35.736173 140380326586112 logging_writer.py:48] [341300] global_step=341300, grad_norm=3.372823476791382, loss=2.406222343444824
I0304 09:20:21.144517 140380334978816 logging_writer.py:48] [341400] global_step=341400, grad_norm=3.9783527851104736, loss=3.2081246376037598
I0304 09:21:05.878317 140380326586112 logging_writer.py:48] [341500] global_step=341500, grad_norm=3.022954225540161, loss=1.0851991176605225
I0304 09:21:51.447681 140380334978816 logging_writer.py:48] [341600] global_step=341600, grad_norm=3.0981335639953613, loss=1.1052546501159668
I0304 09:22:36.457520 140380326586112 logging_writer.py:48] [341700] global_step=341700, grad_norm=2.808908462524414, loss=2.2496795654296875
I0304 09:23:21.543367 140380334978816 logging_writer.py:48] [341800] global_step=341800, grad_norm=2.974538803100586, loss=1.1566386222839355
I0304 09:24:06.691451 140380326586112 logging_writer.py:48] [341900] global_step=341900, grad_norm=2.9239296913146973, loss=1.9940681457519531
I0304 09:24:51.587165 140380334978816 logging_writer.py:48] [342000] global_step=342000, grad_norm=4.0895538330078125, loss=3.341735601425171
I0304 09:24:52.145927 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:25:02.889547 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:25:27.766885 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:25:29.428791 140575196817216 submission_runner.py:411] Time since start: 165105.03s, 	Step: 342003, 	{'train/accuracy': 0.8862890601158142, 'train/loss': 0.4243661165237427, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 152127.7273261547, 'total_duration': 165105.02981305122, 'accumulated_submission_time': 152127.7273261547, 'accumulated_eval_time': 12937.41590332985, 'accumulated_logging_time': 21.75795602798462}
I0304 09:25:29.497417 140380326586112 logging_writer.py:48] [342003] accumulated_eval_time=12937.415903, accumulated_logging_time=21.757956, accumulated_submission_time=152127.727326, global_step=342003, preemption_count=0, score=152127.727326, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=165105.029813, train/accuracy=0.886289, train/loss=0.424366, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:26:08.774118 140380334978816 logging_writer.py:48] [342100] global_step=342100, grad_norm=3.1139302253723145, loss=1.189516544342041
I0304 09:26:53.669774 140380326586112 logging_writer.py:48] [342200] global_step=342200, grad_norm=3.8154594898223877, loss=2.8607394695281982
I0304 09:27:38.883052 140380334978816 logging_writer.py:48] [342300] global_step=342300, grad_norm=2.933763027191162, loss=1.023181438446045
I0304 09:28:23.755711 140380326586112 logging_writer.py:48] [342400] global_step=342400, grad_norm=3.537921905517578, loss=2.9357612133026123
I0304 09:29:08.798897 140380334978816 logging_writer.py:48] [342500] global_step=342500, grad_norm=3.0119788646698, loss=2.1214683055877686
I0304 09:29:53.771153 140380326586112 logging_writer.py:48] [342600] global_step=342600, grad_norm=3.3874659538269043, loss=1.1384702920913696
I0304 09:30:38.861588 140380334978816 logging_writer.py:48] [342700] global_step=342700, grad_norm=3.705066680908203, loss=3.255673408508301
I0304 09:31:23.779853 140380326586112 logging_writer.py:48] [342800] global_step=342800, grad_norm=3.1372780799865723, loss=1.099778175354004
I0304 09:32:09.211143 140380334978816 logging_writer.py:48] [342900] global_step=342900, grad_norm=2.986267328262329, loss=1.2457046508789062
I0304 09:32:29.823383 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:32:40.604360 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:33:05.448972 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:33:07.102397 140575196817216 submission_runner.py:411] Time since start: 165562.70s, 	Step: 342947, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.42216014862060547, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 152547.99410772324, 'total_duration': 165562.70320606232, 'accumulated_submission_time': 152547.99410772324, 'accumulated_eval_time': 12974.693664312363, 'accumulated_logging_time': 21.836976051330566}
I0304 09:33:07.168894 140380326586112 logging_writer.py:48] [342947] accumulated_eval_time=12974.693664, accumulated_logging_time=21.836976, accumulated_submission_time=152547.994108, global_step=342947, preemption_count=0, score=152547.994108, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=165562.703206, train/accuracy=0.887227, train/loss=0.422160, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:33:28.635958 140380334978816 logging_writer.py:48] [343000] global_step=343000, grad_norm=2.966590404510498, loss=2.157642364501953
I0304 09:34:11.597985 140380326586112 logging_writer.py:48] [343100] global_step=343100, grad_norm=3.0055856704711914, loss=1.1734970808029175
I0304 09:34:56.461291 140380334978816 logging_writer.py:48] [343200] global_step=343200, grad_norm=3.135981321334839, loss=1.367851972579956
I0304 09:35:41.580157 140380326586112 logging_writer.py:48] [343300] global_step=343300, grad_norm=3.0258705615997314, loss=1.2114554643630981
I0304 09:36:26.839327 140380334978816 logging_writer.py:48] [343400] global_step=343400, grad_norm=3.265625, loss=2.8026058673858643
I0304 09:37:12.127646 140380326586112 logging_writer.py:48] [343500] global_step=343500, grad_norm=3.732759714126587, loss=2.151029586791992
I0304 09:37:57.193125 140380334978816 logging_writer.py:48] [343600] global_step=343600, grad_norm=3.0238754749298096, loss=1.0855995416641235
I0304 09:38:42.102653 140380326586112 logging_writer.py:48] [343700] global_step=343700, grad_norm=4.247276306152344, loss=3.2936208248138428
I0304 09:39:26.978785 140380334978816 logging_writer.py:48] [343800] global_step=343800, grad_norm=2.9449009895324707, loss=1.1183291673660278
I0304 09:40:07.241499 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:40:17.934066 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:40:43.063556 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:40:44.719292 140575196817216 submission_runner.py:411] Time since start: 166020.32s, 	Step: 343891, 	{'train/accuracy': 0.8874413967132568, 'train/loss': 0.4182123839855194, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 152968.00593590736, 'total_duration': 166020.32029771805, 'accumulated_submission_time': 152968.00593590736, 'accumulated_eval_time': 13012.170394659042, 'accumulated_logging_time': 21.914891004562378}
I0304 09:40:44.795739 140380326586112 logging_writer.py:48] [343891] accumulated_eval_time=13012.170395, accumulated_logging_time=21.914891, accumulated_submission_time=152968.005936, global_step=343891, preemption_count=0, score=152968.005936, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=166020.320298, train/accuracy=0.887441, train/loss=0.418212, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:40:48.785461 140380334978816 logging_writer.py:48] [343900] global_step=343900, grad_norm=2.962472438812256, loss=1.102844476699829
I0304 09:41:29.372495 140380326586112 logging_writer.py:48] [344000] global_step=344000, grad_norm=4.096612453460693, loss=3.0674519538879395
I0304 09:42:14.496739 140380334978816 logging_writer.py:48] [344100] global_step=344100, grad_norm=3.9400599002838135, loss=2.9569685459136963
I0304 09:42:59.692912 140380326586112 logging_writer.py:48] [344200] global_step=344200, grad_norm=3.127333402633667, loss=1.1183817386627197
I0304 09:43:44.938752 140380334978816 logging_writer.py:48] [344300] global_step=344300, grad_norm=3.7234392166137695, loss=2.8848776817321777
I0304 09:44:29.771486 140380326586112 logging_writer.py:48] [344400] global_step=344400, grad_norm=3.1541695594787598, loss=1.4340265989303589
I0304 09:45:14.885197 140380334978816 logging_writer.py:48] [344500] global_step=344500, grad_norm=3.279331922531128, loss=2.7573580741882324
I0304 09:45:59.699703 140380326586112 logging_writer.py:48] [344600] global_step=344600, grad_norm=2.908198118209839, loss=1.3346327543258667
I0304 09:46:44.813083 140380334978816 logging_writer.py:48] [344700] global_step=344700, grad_norm=3.0670323371887207, loss=1.3773092031478882
I0304 09:47:29.747684 140380326586112 logging_writer.py:48] [344800] global_step=344800, grad_norm=3.2132556438446045, loss=1.14836585521698
I0304 09:47:44.872833 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:47:55.498853 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:48:20.261256 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:48:21.914356 140575196817216 submission_runner.py:411] Time since start: 166477.52s, 	Step: 344835, 	{'train/accuracy': 0.8876562118530273, 'train/loss': 0.42350637912750244, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 153388.0217819214, 'total_duration': 166477.5153517723, 'accumulated_submission_time': 153388.0217819214, 'accumulated_eval_time': 13049.210852384567, 'accumulated_logging_time': 22.003353595733643}
I0304 09:48:21.985432 140380334978816 logging_writer.py:48] [344835] accumulated_eval_time=13049.210852, accumulated_logging_time=22.003354, accumulated_submission_time=153388.021782, global_step=344835, preemption_count=0, score=153388.021782, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=166477.515352, train/accuracy=0.887656, train/loss=0.423506, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:48:48.224176 140380326586112 logging_writer.py:48] [344900] global_step=344900, grad_norm=3.708266258239746, loss=2.876462697982788
I0304 09:49:31.815142 140380334978816 logging_writer.py:48] [345000] global_step=345000, grad_norm=2.8006577491760254, loss=1.8856385946273804
I0304 09:50:17.060803 140380326586112 logging_writer.py:48] [345100] global_step=345100, grad_norm=2.9109668731689453, loss=1.1926220655441284
I0304 09:51:02.350431 140380334978816 logging_writer.py:48] [345200] global_step=345200, grad_norm=2.828768491744995, loss=1.127158284187317
I0304 09:51:47.374150 140380326586112 logging_writer.py:48] [345300] global_step=345300, grad_norm=2.9221861362457275, loss=1.0328927040100098
I0304 09:52:32.673220 140380334978816 logging_writer.py:48] [345400] global_step=345400, grad_norm=3.4431755542755127, loss=2.8729989528656006
I0304 09:53:17.900152 140380326586112 logging_writer.py:48] [345500] global_step=345500, grad_norm=3.5136256217956543, loss=1.0800280570983887
I0304 09:54:03.002540 140380334978816 logging_writer.py:48] [345600] global_step=345600, grad_norm=3.177825689315796, loss=2.3715643882751465
I0304 09:54:48.144690 140380326586112 logging_writer.py:48] [345700] global_step=345700, grad_norm=3.0593652725219727, loss=2.3060529232025146
I0304 09:55:22.130409 140575196817216 spec.py:321] Evaluating on the training split.
I0304 09:55:32.833297 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 09:55:58.569556 140575196817216 spec.py:349] Evaluating on the test split.
I0304 09:56:00.224819 140575196817216 submission_runner.py:411] Time since start: 166935.83s, 	Step: 345777, 	{'train/accuracy': 0.8870507478713989, 'train/loss': 0.41853460669517517, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 153808.1084370613, 'total_duration': 166935.82588744164, 'accumulated_submission_time': 153808.1084370613, 'accumulated_eval_time': 13087.304275989532, 'accumulated_logging_time': 22.083884239196777}
I0304 09:56:00.292523 140380334978816 logging_writer.py:48] [345777] accumulated_eval_time=13087.304276, accumulated_logging_time=22.083884, accumulated_submission_time=153808.108437, global_step=345777, preemption_count=0, score=153808.108437, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=166935.825887, train/accuracy=0.887051, train/loss=0.418535, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 09:56:09.841680 140380326586112 logging_writer.py:48] [345800] global_step=345800, grad_norm=3.198606491088867, loss=1.3091089725494385
I0304 09:56:51.147903 140380334978816 logging_writer.py:48] [345900] global_step=345900, grad_norm=3.269244432449341, loss=1.5761945247650146
I0304 09:57:36.268564 140380326586112 logging_writer.py:48] [346000] global_step=346000, grad_norm=3.5880017280578613, loss=3.1010584831237793
I0304 09:58:21.453967 140380334978816 logging_writer.py:48] [346100] global_step=346100, grad_norm=2.9951233863830566, loss=1.2277843952178955
I0304 09:59:06.556783 140380326586112 logging_writer.py:48] [346200] global_step=346200, grad_norm=2.989055633544922, loss=1.3141701221466064
I0304 09:59:51.868509 140380334978816 logging_writer.py:48] [346300] global_step=346300, grad_norm=3.1931326389312744, loss=1.1664283275604248
I0304 10:00:36.852771 140380326586112 logging_writer.py:48] [346400] global_step=346400, grad_norm=3.4028444290161133, loss=1.1315110921859741
I0304 10:01:21.731007 140380334978816 logging_writer.py:48] [346500] global_step=346500, grad_norm=2.8916263580322266, loss=1.6354750394821167
I0304 10:02:06.992815 140380326586112 logging_writer.py:48] [346600] global_step=346600, grad_norm=3.3708949089050293, loss=2.8220407962799072
I0304 10:02:52.041127 140380334978816 logging_writer.py:48] [346700] global_step=346700, grad_norm=3.0116913318634033, loss=1.579047441482544
I0304 10:03:00.695761 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:03:11.455123 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:03:37.503543 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:03:39.162775 140575196817216 submission_runner.py:411] Time since start: 167394.76s, 	Step: 346721, 	{'train/accuracy': 0.8866601586341858, 'train/loss': 0.4186669886112213, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 154228.45319128036, 'total_duration': 167394.76393675804, 'accumulated_submission_time': 154228.45319128036, 'accumulated_eval_time': 13125.770390748978, 'accumulated_logging_time': 22.161634922027588}
I0304 10:03:39.229062 140380326586112 logging_writer.py:48] [346721] accumulated_eval_time=13125.770391, accumulated_logging_time=22.161635, accumulated_submission_time=154228.453191, global_step=346721, preemption_count=0, score=154228.453191, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=167394.763937, train/accuracy=0.886660, train/loss=0.418667, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:04:11.036526 140380334978816 logging_writer.py:48] [346800] global_step=346800, grad_norm=3.467663526535034, loss=2.0938000679016113
I0304 10:04:55.737973 140380326586112 logging_writer.py:48] [346900] global_step=346900, grad_norm=3.3838160037994385, loss=1.145508885383606
I0304 10:05:41.081092 140380334978816 logging_writer.py:48] [347000] global_step=347000, grad_norm=3.153597116470337, loss=2.4939463138580322
I0304 10:06:26.440977 140380326586112 logging_writer.py:48] [347100] global_step=347100, grad_norm=3.424267530441284, loss=2.9185070991516113
I0304 10:07:11.565127 140380334978816 logging_writer.py:48] [347200] global_step=347200, grad_norm=3.017423391342163, loss=1.2246551513671875
I0304 10:07:56.618121 140380326586112 logging_writer.py:48] [347300] global_step=347300, grad_norm=3.3546578884124756, loss=1.3691141605377197
I0304 10:08:41.619513 140380334978816 logging_writer.py:48] [347400] global_step=347400, grad_norm=3.3021180629730225, loss=1.6526685953140259
I0304 10:09:26.644636 140380326586112 logging_writer.py:48] [347500] global_step=347500, grad_norm=2.879769802093506, loss=1.1369976997375488
I0304 10:10:11.777909 140380334978816 logging_writer.py:48] [347600] global_step=347600, grad_norm=3.8905131816864014, loss=3.1063363552093506
I0304 10:10:39.427894 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:10:50.032049 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:11:16.510752 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:11:18.163372 140575196817216 submission_runner.py:411] Time since start: 167853.76s, 	Step: 347663, 	{'train/accuracy': 0.887011706829071, 'train/loss': 0.4225160777568817, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 154648.5946586132, 'total_duration': 167853.7642493248, 'accumulated_submission_time': 154648.5946586132, 'accumulated_eval_time': 13164.504692316055, 'accumulated_logging_time': 22.23713183403015}
I0304 10:11:18.231911 140380326586112 logging_writer.py:48] [347663] accumulated_eval_time=13164.504692, accumulated_logging_time=22.237132, accumulated_submission_time=154648.594659, global_step=347663, preemption_count=0, score=154648.594659, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=167853.764249, train/accuracy=0.887012, train/loss=0.422516, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:11:33.334945 140380334978816 logging_writer.py:48] [347700] global_step=347700, grad_norm=3.3359107971191406, loss=2.3035759925842285
I0304 10:12:15.797817 140380326586112 logging_writer.py:48] [347800] global_step=347800, grad_norm=3.156952381134033, loss=2.671804666519165
I0304 10:13:00.522155 140380334978816 logging_writer.py:48] [347900] global_step=347900, grad_norm=3.393321990966797, loss=2.7689268589019775
I0304 10:13:45.702960 140380326586112 logging_writer.py:48] [348000] global_step=348000, grad_norm=3.676086902618408, loss=3.2115750312805176
I0304 10:14:30.885478 140380334978816 logging_writer.py:48] [348100] global_step=348100, grad_norm=3.3872528076171875, loss=2.7366750240325928
I0304 10:15:15.999729 140380326586112 logging_writer.py:48] [348200] global_step=348200, grad_norm=3.763988971710205, loss=3.0275096893310547
I0304 10:16:00.950769 140380334978816 logging_writer.py:48] [348300] global_step=348300, grad_norm=2.8317155838012695, loss=1.2585786581039429
I0304 10:16:45.986066 140380326586112 logging_writer.py:48] [348400] global_step=348400, grad_norm=3.5041775703430176, loss=1.2094860076904297
I0304 10:17:31.290159 140380334978816 logging_writer.py:48] [348500] global_step=348500, grad_norm=3.1783502101898193, loss=2.5921292304992676
I0304 10:18:16.316097 140380326586112 logging_writer.py:48] [348600] global_step=348600, grad_norm=3.0235795974731445, loss=1.1235800981521606
I0304 10:18:18.213344 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:18:28.832167 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:18:55.702245 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:18:57.355225 140575196817216 submission_runner.py:411] Time since start: 168312.96s, 	Step: 348606, 	{'train/accuracy': 0.8873828053474426, 'train/loss': 0.41590505838394165, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 155068.51655745506, 'total_duration': 168312.95635533333, 'accumulated_submission_time': 155068.51655745506, 'accumulated_eval_time': 13203.645637273788, 'accumulated_logging_time': 22.316974401474}
I0304 10:18:57.422628 140380334978816 logging_writer.py:48] [348606] accumulated_eval_time=13203.645637, accumulated_logging_time=22.316974, accumulated_submission_time=155068.516557, global_step=348606, preemption_count=0, score=155068.516557, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=168312.956355, train/accuracy=0.887383, train/loss=0.415905, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:19:35.406438 140380326586112 logging_writer.py:48] [348700] global_step=348700, grad_norm=3.4149675369262695, loss=1.3394651412963867
I0304 10:20:20.004233 140380334978816 logging_writer.py:48] [348800] global_step=348800, grad_norm=2.878558397293091, loss=1.27118718624115
I0304 10:21:05.380335 140380326586112 logging_writer.py:48] [348900] global_step=348900, grad_norm=3.496676206588745, loss=1.4299321174621582
I0304 10:21:50.219008 140380334978816 logging_writer.py:48] [349000] global_step=349000, grad_norm=3.089282274246216, loss=1.2051771879196167
I0304 10:22:35.445803 140380326586112 logging_writer.py:48] [349100] global_step=349100, grad_norm=3.4427106380462646, loss=1.1652969121932983
I0304 10:23:20.571459 140380334978816 logging_writer.py:48] [349200] global_step=349200, grad_norm=3.3126285076141357, loss=1.306612253189087
I0304 10:24:05.483026 140380326586112 logging_writer.py:48] [349300] global_step=349300, grad_norm=3.7638802528381348, loss=3.112525701522827
I0304 10:24:50.523162 140380334978816 logging_writer.py:48] [349400] global_step=349400, grad_norm=2.876539945602417, loss=1.0754789113998413
I0304 10:25:35.523317 140380326586112 logging_writer.py:48] [349500] global_step=349500, grad_norm=3.0646886825561523, loss=1.1265915632247925
I0304 10:25:57.540535 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:26:08.411538 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:26:36.674707 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:26:38.330333 140575196817216 submission_runner.py:411] Time since start: 168773.93s, 	Step: 349551, 	{'train/accuracy': 0.8894140720367432, 'train/loss': 0.4155358672142029, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 155488.57597899437, 'total_duration': 168773.9312326908, 'accumulated_submission_time': 155488.57597899437, 'accumulated_eval_time': 13244.434262275696, 'accumulated_logging_time': 22.393721342086792}
I0304 10:26:38.398255 140380334978816 logging_writer.py:48] [349551] accumulated_eval_time=13244.434262, accumulated_logging_time=22.393721, accumulated_submission_time=155488.575979, global_step=349551, preemption_count=0, score=155488.575979, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=168773.931233, train/accuracy=0.889414, train/loss=0.415536, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:26:58.254581 140380326586112 logging_writer.py:48] [349600] global_step=349600, grad_norm=3.249802350997925, loss=1.2198493480682373
I0304 10:27:40.862037 140380334978816 logging_writer.py:48] [349700] global_step=349700, grad_norm=2.9541170597076416, loss=2.061008930206299
I0304 10:28:25.780978 140380326586112 logging_writer.py:48] [349800] global_step=349800, grad_norm=3.9495835304260254, loss=3.1487185955047607
I0304 10:29:10.906923 140380334978816 logging_writer.py:48] [349900] global_step=349900, grad_norm=3.1919829845428467, loss=2.079441785812378
I0304 10:29:55.512413 140380326586112 logging_writer.py:48] [350000] global_step=350000, grad_norm=3.327986478805542, loss=1.1673493385314941
I0304 10:30:40.494410 140380334978816 logging_writer.py:48] [350100] global_step=350100, grad_norm=2.956460952758789, loss=1.4609417915344238
I0304 10:31:25.355435 140380326586112 logging_writer.py:48] [350200] global_step=350200, grad_norm=3.4347198009490967, loss=1.2366386651992798
I0304 10:32:10.058398 140380334978816 logging_writer.py:48] [350300] global_step=350300, grad_norm=2.985326051712036, loss=1.113090991973877
I0304 10:32:55.602856 140380326586112 logging_writer.py:48] [350400] global_step=350400, grad_norm=3.5384340286254883, loss=1.3675799369812012
I0304 10:33:38.668715 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:33:49.360505 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:34:15.715932 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:34:17.364996 140575196817216 submission_runner.py:411] Time since start: 169232.97s, 	Step: 350497, 	{'train/accuracy': 0.8883593678474426, 'train/loss': 0.4154931306838989, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 155908.78848218918, 'total_duration': 169232.9659898281, 'accumulated_submission_time': 155908.78848218918, 'accumulated_eval_time': 13283.12947845459, 'accumulated_logging_time': 22.471112489700317}
I0304 10:34:17.438280 140380334978816 logging_writer.py:48] [350497] accumulated_eval_time=13283.129478, accumulated_logging_time=22.471112, accumulated_submission_time=155908.788482, global_step=350497, preemption_count=0, score=155908.788482, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=169232.965990, train/accuracy=0.888359, train/loss=0.415493, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:34:19.029391 140380326586112 logging_writer.py:48] [350500] global_step=350500, grad_norm=4.114740371704102, loss=3.310720443725586
I0304 10:34:59.555924 140380334978816 logging_writer.py:48] [350600] global_step=350600, grad_norm=3.5407614707946777, loss=1.7733713388442993
I0304 10:35:44.455407 140380326586112 logging_writer.py:48] [350700] global_step=350700, grad_norm=2.951000690460205, loss=1.1897162199020386
I0304 10:36:29.719486 140380334978816 logging_writer.py:48] [350800] global_step=350800, grad_norm=3.031816244125366, loss=2.6148741245269775
I0304 10:37:14.812038 140380326586112 logging_writer.py:48] [350900] global_step=350900, grad_norm=3.1597306728363037, loss=1.1816565990447998
I0304 10:37:59.578440 140380334978816 logging_writer.py:48] [351000] global_step=351000, grad_norm=2.824481248855591, loss=1.2755999565124512
I0304 10:38:44.665634 140380326586112 logging_writer.py:48] [351100] global_step=351100, grad_norm=2.796804189682007, loss=1.7965725660324097
I0304 10:39:29.550232 140380334978816 logging_writer.py:48] [351200] global_step=351200, grad_norm=3.0989372730255127, loss=2.5502710342407227
I0304 10:40:14.854873 140380326586112 logging_writer.py:48] [351300] global_step=351300, grad_norm=2.8846893310546875, loss=1.5487130880355835
I0304 10:40:59.940921 140380334978816 logging_writer.py:48] [351400] global_step=351400, grad_norm=3.1824421882629395, loss=1.1671466827392578
I0304 10:41:17.536581 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:41:28.225645 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:41:54.332290 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:41:55.985073 140575196817216 submission_runner.py:411] Time since start: 169691.59s, 	Step: 351441, 	{'train/accuracy': 0.8911327719688416, 'train/loss': 0.4061800241470337, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 156328.82681155205, 'total_duration': 169691.5860159397, 'accumulated_submission_time': 156328.82681155205, 'accumulated_eval_time': 13321.576848506927, 'accumulated_logging_time': 22.55529522895813}
I0304 10:41:56.054378 140380326586112 logging_writer.py:48] [351441] accumulated_eval_time=13321.576849, accumulated_logging_time=22.555295, accumulated_submission_time=156328.826812, global_step=351441, preemption_count=0, score=156328.826812, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=169691.586016, train/accuracy=0.891133, train/loss=0.406180, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:42:19.907945 140380334978816 logging_writer.py:48] [351500] global_step=351500, grad_norm=3.542325019836426, loss=2.986581802368164
I0304 10:43:03.137988 140380326586112 logging_writer.py:48] [351600] global_step=351600, grad_norm=3.1462225914001465, loss=2.624098062515259
I0304 10:43:47.941481 140380334978816 logging_writer.py:48] [351700] global_step=351700, grad_norm=3.15503191947937, loss=1.0502333641052246
I0304 10:44:33.013215 140380326586112 logging_writer.py:48] [351800] global_step=351800, grad_norm=3.682239055633545, loss=2.416621446609497
I0304 10:45:18.126725 140380334978816 logging_writer.py:48] [351900] global_step=351900, grad_norm=3.069976806640625, loss=2.301891803741455
I0304 10:46:02.947243 140380326586112 logging_writer.py:48] [352000] global_step=352000, grad_norm=3.3967435359954834, loss=1.5721427202224731
I0304 10:46:48.021136 140380334978816 logging_writer.py:48] [352100] global_step=352100, grad_norm=3.230032444000244, loss=1.151188611984253
I0304 10:47:32.963481 140380326586112 logging_writer.py:48] [352200] global_step=352200, grad_norm=3.155200481414795, loss=1.0967096090316772
I0304 10:48:17.864795 140380334978816 logging_writer.py:48] [352300] global_step=352300, grad_norm=3.031719923019409, loss=1.0875823497772217
I0304 10:48:56.106037 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:49:06.971176 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:49:33.744301 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:49:35.389851 140575196817216 submission_runner.py:411] Time since start: 170150.99s, 	Step: 352387, 	{'train/accuracy': 0.8876367211341858, 'train/loss': 0.4141596555709839, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 156748.8176636696, 'total_duration': 170150.9909837246, 'accumulated_submission_time': 156748.8176636696, 'accumulated_eval_time': 13360.859747171402, 'accumulated_logging_time': 22.63696026802063}
I0304 10:49:35.461499 140380326586112 logging_writer.py:48] [352387] accumulated_eval_time=13360.859747, accumulated_logging_time=22.636960, accumulated_submission_time=156748.817664, global_step=352387, preemption_count=0, score=156748.817664, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=170150.990984, train/accuracy=0.887637, train/loss=0.414160, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:49:41.031970 140380334978816 logging_writer.py:48] [352400] global_step=352400, grad_norm=3.18978214263916, loss=2.389756917953491
I0304 10:50:21.999560 140380326586112 logging_writer.py:48] [352500] global_step=352500, grad_norm=3.045928478240967, loss=1.0827555656433105
I0304 10:51:07.151483 140380334978816 logging_writer.py:48] [352600] global_step=352600, grad_norm=2.9048917293548584, loss=1.1437102556228638
I0304 10:51:52.385907 140380326586112 logging_writer.py:48] [352700] global_step=352700, grad_norm=3.356633186340332, loss=1.1916213035583496
I0304 10:52:37.518038 140380334978816 logging_writer.py:48] [352800] global_step=352800, grad_norm=3.814113140106201, loss=2.1954684257507324
I0304 10:53:22.698743 140380326586112 logging_writer.py:48] [352900] global_step=352900, grad_norm=3.0044186115264893, loss=1.1433427333831787
I0304 10:54:07.882035 140380334978816 logging_writer.py:48] [353000] global_step=353000, grad_norm=3.3159263134002686, loss=2.8249948024749756
I0304 10:54:53.013289 140380326586112 logging_writer.py:48] [353100] global_step=353100, grad_norm=3.2800095081329346, loss=2.814589262008667
I0304 10:55:38.359054 140380334978816 logging_writer.py:48] [353200] global_step=353200, grad_norm=3.095738410949707, loss=1.0858445167541504
I0304 10:56:23.626528 140380326586112 logging_writer.py:48] [353300] global_step=353300, grad_norm=3.0903477668762207, loss=1.1223822832107544
I0304 10:56:35.431593 140575196817216 spec.py:321] Evaluating on the training split.
I0304 10:56:46.779332 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 10:57:10.422807 140575196817216 spec.py:349] Evaluating on the test split.
I0304 10:57:12.078491 140575196817216 submission_runner.py:411] Time since start: 170607.68s, 	Step: 353328, 	{'train/accuracy': 0.8882421851158142, 'train/loss': 0.42242830991744995, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 157168.728931427, 'total_duration': 170607.67958974838, 'accumulated_submission_time': 157168.728931427, 'accumulated_eval_time': 13397.505673408508, 'accumulated_logging_time': 22.719033241271973}
I0304 10:57:12.148422 140380334978816 logging_writer.py:48] [353328] accumulated_eval_time=13397.505673, accumulated_logging_time=22.719033, accumulated_submission_time=157168.728931, global_step=353328, preemption_count=0, score=157168.728931, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=170607.679590, train/accuracy=0.888242, train/loss=0.422428, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 10:57:41.152342 140380326586112 logging_writer.py:48] [353400] global_step=353400, grad_norm=3.148864269256592, loss=1.107640266418457
I0304 10:58:24.969010 140380334978816 logging_writer.py:48] [353500] global_step=353500, grad_norm=4.672269344329834, loss=2.986966609954834
I0304 10:59:10.170532 140380326586112 logging_writer.py:48] [353600] global_step=353600, grad_norm=2.9939606189727783, loss=1.2241184711456299
I0304 10:59:55.315985 140380334978816 logging_writer.py:48] [353700] global_step=353700, grad_norm=3.151627779006958, loss=1.125839352607727
I0304 11:00:40.123669 140380326586112 logging_writer.py:48] [353800] global_step=353800, grad_norm=3.172194004058838, loss=1.2969106435775757
I0304 11:01:25.313669 140380334978816 logging_writer.py:48] [353900] global_step=353900, grad_norm=3.2602806091308594, loss=2.0218331813812256
I0304 11:02:10.187610 140380326586112 logging_writer.py:48] [354000] global_step=354000, grad_norm=3.2548069953918457, loss=1.1482200622558594
I0304 11:02:55.139099 140380334978816 logging_writer.py:48] [354100] global_step=354100, grad_norm=3.182689666748047, loss=1.1436432600021362
I0304 11:03:40.099316 140380326586112 logging_writer.py:48] [354200] global_step=354200, grad_norm=4.797748565673828, loss=3.258786678314209
I0304 11:04:12.209696 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:04:22.963866 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:04:49.406691 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:04:51.068313 140575196817216 submission_runner.py:411] Time since start: 171066.67s, 	Step: 354273, 	{'train/accuracy': 0.8841992020606995, 'train/loss': 0.4266878366470337, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 157588.73220658302, 'total_duration': 171066.66933846474, 'accumulated_submission_time': 157588.73220658302, 'accumulated_eval_time': 13436.36325597763, 'accumulated_logging_time': 22.79829692840576}
I0304 11:04:51.148657 140380334978816 logging_writer.py:48] [354273] accumulated_eval_time=13436.363256, accumulated_logging_time=22.798297, accumulated_submission_time=157588.732207, global_step=354273, preemption_count=0, score=157588.732207, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=171066.669338, train/accuracy=0.884199, train/loss=0.426688, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:05:02.274675 140380326586112 logging_writer.py:48] [354300] global_step=354300, grad_norm=2.8629708290100098, loss=1.3066258430480957
I0304 11:05:43.974825 140380334978816 logging_writer.py:48] [354400] global_step=354400, grad_norm=3.0516319274902344, loss=1.1572874784469604
I0304 11:06:29.131414 140380326586112 logging_writer.py:48] [354500] global_step=354500, grad_norm=3.134819269180298, loss=2.4846243858337402
I0304 11:07:14.413800 140380334978816 logging_writer.py:48] [354600] global_step=354600, grad_norm=3.0227417945861816, loss=1.0838543176651
I0304 11:07:59.356646 140380326586112 logging_writer.py:48] [354700] global_step=354700, grad_norm=3.0303549766540527, loss=1.209525227546692
I0304 11:08:44.425548 140380334978816 logging_writer.py:48] [354800] global_step=354800, grad_norm=3.0503392219543457, loss=1.0655368566513062
I0304 11:09:29.720423 140380326586112 logging_writer.py:48] [354900] global_step=354900, grad_norm=3.6348447799682617, loss=2.5360829830169678
I0304 11:10:14.507315 140380334978816 logging_writer.py:48] [355000] global_step=355000, grad_norm=3.0620861053466797, loss=2.6173410415649414
I0304 11:10:59.520132 140380326586112 logging_writer.py:48] [355100] global_step=355100, grad_norm=3.2962684631347656, loss=1.4920262098312378
I0304 11:11:44.691461 140380334978816 logging_writer.py:48] [355200] global_step=355200, grad_norm=6.418583393096924, loss=1.6996089220046997
I0304 11:11:51.447028 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:12:02.155528 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:12:27.333440 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:12:28.994364 140575196817216 submission_runner.py:411] Time since start: 171524.60s, 	Step: 355217, 	{'train/accuracy': 0.8879101276397705, 'train/loss': 0.4218570291996002, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 158008.9703757763, 'total_duration': 171524.59533452988, 'accumulated_submission_time': 158008.9703757763, 'accumulated_eval_time': 13473.909487962723, 'accumulated_logging_time': 22.889212369918823}
I0304 11:12:29.077699 140380326586112 logging_writer.py:48] [355217] accumulated_eval_time=13473.909488, accumulated_logging_time=22.889212, accumulated_submission_time=158008.970376, global_step=355217, preemption_count=0, score=158008.970376, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=171524.595335, train/accuracy=0.887910, train/loss=0.421857, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:13:02.465122 140380334978816 logging_writer.py:48] [355300] global_step=355300, grad_norm=3.1603469848632812, loss=1.1762819290161133
I0304 11:13:47.301596 140380326586112 logging_writer.py:48] [355400] global_step=355400, grad_norm=3.551431894302368, loss=2.335552453994751
I0304 11:14:32.112282 140380334978816 logging_writer.py:48] [355500] global_step=355500, grad_norm=2.9348256587982178, loss=1.679103136062622
I0304 11:15:17.648017 140380326586112 logging_writer.py:48] [355600] global_step=355600, grad_norm=2.9947493076324463, loss=1.1824591159820557
I0304 11:16:02.512611 140380334978816 logging_writer.py:48] [355700] global_step=355700, grad_norm=3.0694854259490967, loss=1.2192720174789429
I0304 11:16:47.793288 140380326586112 logging_writer.py:48] [355800] global_step=355800, grad_norm=3.1074881553649902, loss=1.1559598445892334
I0304 11:17:32.704499 140380334978816 logging_writer.py:48] [355900] global_step=355900, grad_norm=3.527742624282837, loss=2.593412160873413
I0304 11:18:17.768599 140380326586112 logging_writer.py:48] [356000] global_step=356000, grad_norm=3.086785078048706, loss=2.3160934448242188
I0304 11:19:02.784501 140380334978816 logging_writer.py:48] [356100] global_step=356100, grad_norm=3.0306575298309326, loss=1.0240986347198486
I0304 11:19:29.418073 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:19:40.088312 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:20:06.407248 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:20:08.061703 140575196817216 submission_runner.py:411] Time since start: 171983.66s, 	Step: 356161, 	{'train/accuracy': 0.8890234231948853, 'train/loss': 0.4148597717285156, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 158429.252024889, 'total_duration': 171983.66263103485, 'accumulated_submission_time': 158429.252024889, 'accumulated_eval_time': 13512.551969051361, 'accumulated_logging_time': 22.982855796813965}
I0304 11:20:08.132337 140380326586112 logging_writer.py:48] [356161] accumulated_eval_time=13512.551969, accumulated_logging_time=22.982856, accumulated_submission_time=158429.252025, global_step=356161, preemption_count=0, score=158429.252025, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=171983.662631, train/accuracy=0.889023, train/loss=0.414860, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:20:24.046702 140380334978816 logging_writer.py:48] [356200] global_step=356200, grad_norm=3.9260616302490234, loss=3.2049238681793213
I0304 11:21:06.589474 140380326586112 logging_writer.py:48] [356300] global_step=356300, grad_norm=3.3141605854034424, loss=1.6744741201400757
I0304 11:21:51.554503 140380334978816 logging_writer.py:48] [356400] global_step=356400, grad_norm=3.874285936355591, loss=3.1870546340942383
I0304 11:22:36.997860 140380326586112 logging_writer.py:48] [356500] global_step=356500, grad_norm=3.746803045272827, loss=2.7516489028930664
I0304 11:23:22.329312 140380334978816 logging_writer.py:48] [356600] global_step=356600, grad_norm=3.0286877155303955, loss=1.0377180576324463
I0304 11:24:07.455903 140380326586112 logging_writer.py:48] [356700] global_step=356700, grad_norm=3.09061861038208, loss=1.64888334274292
I0304 11:24:52.489473 140380334978816 logging_writer.py:48] [356800] global_step=356800, grad_norm=3.164590835571289, loss=1.5933551788330078
I0304 11:25:37.702064 140380326586112 logging_writer.py:48] [356900] global_step=356900, grad_norm=3.063558340072632, loss=1.8135464191436768
I0304 11:26:22.985024 140380334978816 logging_writer.py:48] [357000] global_step=357000, grad_norm=3.4975743293762207, loss=1.5794470310211182
I0304 11:27:08.081557 140380326586112 logging_writer.py:48] [357100] global_step=357100, grad_norm=3.728888750076294, loss=3.089423894882202
I0304 11:27:08.096785 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:27:18.826828 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:27:47.394129 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:27:49.059361 140575196817216 submission_runner.py:411] Time since start: 172444.66s, 	Step: 357101, 	{'train/accuracy': 0.8884570002555847, 'train/loss': 0.4165292978286743, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 158849.1560485363, 'total_duration': 172444.66031646729, 'accumulated_submission_time': 158849.1560485363, 'accumulated_eval_time': 13553.5134370327, 'accumulated_logging_time': 23.065964221954346}
I0304 11:27:49.137042 140380334978816 logging_writer.py:48] [357101] accumulated_eval_time=13553.513437, accumulated_logging_time=23.065964, accumulated_submission_time=158849.156049, global_step=357101, preemption_count=0, score=158849.156049, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=172444.660316, train/accuracy=0.888457, train/loss=0.416529, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:28:29.295323 140380326586112 logging_writer.py:48] [357200] global_step=357200, grad_norm=3.900852918624878, loss=3.103792667388916
I0304 11:29:14.368466 140380334978816 logging_writer.py:48] [357300] global_step=357300, grad_norm=5.126272201538086, loss=3.2302517890930176
I0304 11:29:59.333494 140380326586112 logging_writer.py:48] [357400] global_step=357400, grad_norm=3.609860897064209, loss=1.121572494506836
I0304 11:30:44.345042 140380334978816 logging_writer.py:48] [357500] global_step=357500, grad_norm=3.0729146003723145, loss=1.5709551572799683
I0304 11:31:29.462410 140380326586112 logging_writer.py:48] [357600] global_step=357600, grad_norm=3.269012451171875, loss=1.5801105499267578
I0304 11:32:14.481643 140380334978816 logging_writer.py:48] [357700] global_step=357700, grad_norm=3.068774461746216, loss=1.9362616539001465
I0304 11:32:58.888705 140380326586112 logging_writer.py:48] [357800] global_step=357800, grad_norm=3.5421905517578125, loss=1.4312777519226074
I0304 11:33:44.654062 140380334978816 logging_writer.py:48] [357900] global_step=357900, grad_norm=6.690581798553467, loss=1.1761555671691895
I0304 11:34:29.781576 140380326586112 logging_writer.py:48] [358000] global_step=358000, grad_norm=3.208885908126831, loss=2.4812426567077637
I0304 11:34:49.176371 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:34:59.839477 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:35:23.573295 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:35:25.224210 140575196817216 submission_runner.py:411] Time since start: 172900.83s, 	Step: 358045, 	{'train/accuracy': 0.8863476514816284, 'train/loss': 0.4233624041080475, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 159269.13534212112, 'total_duration': 172900.82520484924, 'accumulated_submission_time': 159269.13534212112, 'accumulated_eval_time': 13589.560196638107, 'accumulated_logging_time': 23.15531325340271}
I0304 11:35:25.295159 140380334978816 logging_writer.py:48] [358045] accumulated_eval_time=13589.560197, accumulated_logging_time=23.155313, accumulated_submission_time=159269.135342, global_step=358045, preemption_count=0, score=159269.135342, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=172900.825205, train/accuracy=0.886348, train/loss=0.423362, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:35:47.546607 140380326586112 logging_writer.py:48] [358100] global_step=358100, grad_norm=3.5175108909606934, loss=1.423801302909851
I0304 11:36:30.795253 140380334978816 logging_writer.py:48] [358200] global_step=358200, grad_norm=3.14041805267334, loss=1.7292735576629639
I0304 11:37:15.822647 140380326586112 logging_writer.py:48] [358300] global_step=358300, grad_norm=3.3133928775787354, loss=1.1996718645095825
I0304 11:38:01.008665 140380334978816 logging_writer.py:48] [358400] global_step=358400, grad_norm=3.3887076377868652, loss=1.1141141653060913
I0304 11:38:46.028635 140380326586112 logging_writer.py:48] [358500] global_step=358500, grad_norm=3.0276987552642822, loss=1.2328424453735352
I0304 11:39:31.075706 140380334978816 logging_writer.py:48] [358600] global_step=358600, grad_norm=2.940048933029175, loss=1.9009989500045776
I0304 11:40:15.956773 140380326586112 logging_writer.py:48] [358700] global_step=358700, grad_norm=3.3930063247680664, loss=1.542979121208191
I0304 11:41:01.114233 140380334978816 logging_writer.py:48] [358800] global_step=358800, grad_norm=3.22495436668396, loss=1.6812896728515625
I0304 11:41:46.384838 140380326586112 logging_writer.py:48] [358900] global_step=358900, grad_norm=3.3881492614746094, loss=1.174945592880249
I0304 11:42:25.272715 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:42:36.197388 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:43:00.222353 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:43:01.876075 140575196817216 submission_runner.py:411] Time since start: 173357.48s, 	Step: 358988, 	{'train/accuracy': 0.8848632574081421, 'train/loss': 0.427352637052536, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 159689.05473470688, 'total_duration': 173357.47729110718, 'accumulated_submission_time': 159689.05473470688, 'accumulated_eval_time': 13626.16271162033, 'accumulated_logging_time': 23.235952138900757}
I0304 11:43:01.981616 140380334978816 logging_writer.py:48] [358988] accumulated_eval_time=13626.162712, accumulated_logging_time=23.235952, accumulated_submission_time=159689.054735, global_step=358988, preemption_count=0, score=159689.054735, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=173357.477291, train/accuracy=0.884863, train/loss=0.427353, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:43:07.167101 140380326586112 logging_writer.py:48] [359000] global_step=359000, grad_norm=3.2474968433380127, loss=1.0591169595718384
I0304 11:43:48.137632 140380334978816 logging_writer.py:48] [359100] global_step=359100, grad_norm=3.3520126342773438, loss=2.7103707790374756
I0304 11:44:33.117669 140380326586112 logging_writer.py:48] [359200] global_step=359200, grad_norm=3.1076674461364746, loss=1.835992693901062
I0304 11:45:18.220715 140380334978816 logging_writer.py:48] [359300] global_step=359300, grad_norm=2.6978092193603516, loss=1.338165044784546
I0304 11:46:03.268229 140380326586112 logging_writer.py:48] [359400] global_step=359400, grad_norm=2.9110400676727295, loss=1.1439954042434692
I0304 11:46:48.392061 140380334978816 logging_writer.py:48] [359500] global_step=359500, grad_norm=3.143460273742676, loss=1.0905749797821045
I0304 11:47:33.414525 140380326586112 logging_writer.py:48] [359600] global_step=359600, grad_norm=3.1400249004364014, loss=1.1459498405456543
I0304 11:48:18.277732 140380334978816 logging_writer.py:48] [359700] global_step=359700, grad_norm=3.104630708694458, loss=1.2425322532653809
I0304 11:49:03.450754 140380326586112 logging_writer.py:48] [359800] global_step=359800, grad_norm=3.4124104976654053, loss=2.864241361618042
I0304 11:49:48.100057 140380334978816 logging_writer.py:48] [359900] global_step=359900, grad_norm=3.0645222663879395, loss=1.180678367614746
I0304 11:50:02.252964 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:50:13.146611 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:50:37.466853 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:50:39.116996 140575196817216 submission_runner.py:411] Time since start: 173814.72s, 	Step: 359933, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.4193789064884186, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 160109.2667543888, 'total_duration': 173814.71801424026, 'accumulated_submission_time': 160109.2667543888, 'accumulated_eval_time': 13663.025684833527, 'accumulated_logging_time': 23.352552890777588}
I0304 11:50:39.191680 140380326586112 logging_writer.py:48] [359933] accumulated_eval_time=13663.025685, accumulated_logging_time=23.352553, accumulated_submission_time=160109.266754, global_step=359933, preemption_count=0, score=160109.266754, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=173814.718014, train/accuracy=0.887578, train/loss=0.419379, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:51:06.210301 140380334978816 logging_writer.py:48] [360000] global_step=360000, grad_norm=3.282304525375366, loss=1.4017772674560547
I0304 11:51:49.967351 140380326586112 logging_writer.py:48] [360100] global_step=360100, grad_norm=3.1229732036590576, loss=1.559981107711792
I0304 11:52:34.851834 140380334978816 logging_writer.py:48] [360200] global_step=360200, grad_norm=3.262906312942505, loss=2.3650989532470703
I0304 11:53:20.212435 140380326586112 logging_writer.py:48] [360300] global_step=360300, grad_norm=2.976226568222046, loss=1.1156203746795654
I0304 11:54:05.358499 140380334978816 logging_writer.py:48] [360400] global_step=360400, grad_norm=3.4339144229888916, loss=1.1605861186981201
I0304 11:54:50.233304 140380326586112 logging_writer.py:48] [360500] global_step=360500, grad_norm=3.5617151260375977, loss=1.2817292213439941
I0304 11:55:35.177448 140380334978816 logging_writer.py:48] [360600] global_step=360600, grad_norm=3.0484561920166016, loss=1.2748719453811646
I0304 11:56:20.336124 140380326586112 logging_writer.py:48] [360700] global_step=360700, grad_norm=2.973400115966797, loss=1.8190579414367676
I0304 11:57:05.358331 140380334978816 logging_writer.py:48] [360800] global_step=360800, grad_norm=3.283712863922119, loss=2.8347909450531006
I0304 11:57:39.197189 140575196817216 spec.py:321] Evaluating on the training split.
I0304 11:57:49.873946 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 11:58:13.552976 140575196817216 spec.py:349] Evaluating on the test split.
I0304 11:58:15.216161 140575196817216 submission_runner.py:411] Time since start: 174270.82s, 	Step: 360877, 	{'train/accuracy': 0.8887499570846558, 'train/loss': 0.41471391916275024, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 160529.21381664276, 'total_duration': 174270.8171491623, 'accumulated_submission_time': 160529.21381664276, 'accumulated_eval_time': 13699.04360795021, 'accumulated_logging_time': 23.43653154373169}
I0304 11:58:15.295221 140380326586112 logging_writer.py:48] [360877] accumulated_eval_time=13699.043608, accumulated_logging_time=23.436532, accumulated_submission_time=160529.213817, global_step=360877, preemption_count=0, score=160529.213817, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=174270.817149, train/accuracy=0.888750, train/loss=0.414714, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 11:58:24.852209 140380334978816 logging_writer.py:48] [360900] global_step=360900, grad_norm=4.071895599365234, loss=1.0988199710845947
I0304 11:59:06.296161 140380326586112 logging_writer.py:48] [361000] global_step=361000, grad_norm=3.7003250122070312, loss=2.7017736434936523
I0304 11:59:51.165673 140380334978816 logging_writer.py:48] [361100] global_step=361100, grad_norm=3.0807530879974365, loss=1.2054457664489746
I0304 12:00:36.489311 140380326586112 logging_writer.py:48] [361200] global_step=361200, grad_norm=3.426650285720825, loss=1.1716647148132324
I0304 12:01:21.348836 140380334978816 logging_writer.py:48] [361300] global_step=361300, grad_norm=3.360250473022461, loss=1.115047812461853
I0304 12:02:06.484152 140380326586112 logging_writer.py:48] [361400] global_step=361400, grad_norm=3.333818197250366, loss=1.1529370546340942
I0304 12:02:51.472121 140380334978816 logging_writer.py:48] [361500] global_step=361500, grad_norm=3.3676531314849854, loss=2.004026174545288
I0304 12:03:36.654628 140380326586112 logging_writer.py:48] [361600] global_step=361600, grad_norm=3.354719638824463, loss=1.820969581604004
I0304 12:04:21.754473 140380334978816 logging_writer.py:48] [361700] global_step=361700, grad_norm=3.3271000385284424, loss=1.1552191972732544
I0304 12:05:06.772371 140380326586112 logging_writer.py:48] [361800] global_step=361800, grad_norm=3.681598424911499, loss=3.017390251159668
I0304 12:05:15.461050 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:05:26.068691 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:05:52.397385 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:05:54.055188 140575196817216 submission_runner.py:411] Time since start: 174729.66s, 	Step: 361821, 	{'train/accuracy': 0.8864648342132568, 'train/loss': 0.42359521985054016, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 160949.31827545166, 'total_duration': 174729.65630316734, 'accumulated_submission_time': 160949.31827545166, 'accumulated_eval_time': 13737.636802196503, 'accumulated_logging_time': 23.527692556381226}
I0304 12:05:54.125889 140380334978816 logging_writer.py:48] [361821] accumulated_eval_time=13737.636802, accumulated_logging_time=23.527693, accumulated_submission_time=160949.318275, global_step=361821, preemption_count=0, score=160949.318275, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=174729.656303, train/accuracy=0.886465, train/loss=0.423595, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:06:25.901545 140380326586112 logging_writer.py:48] [361900] global_step=361900, grad_norm=4.422975540161133, loss=2.628256320953369
I0304 12:07:10.482687 140380334978816 logging_writer.py:48] [362000] global_step=362000, grad_norm=3.1462302207946777, loss=1.205195665359497
I0304 12:07:55.382468 140380326586112 logging_writer.py:48] [362100] global_step=362100, grad_norm=3.280874252319336, loss=2.615281581878662
I0304 12:08:40.259600 140380334978816 logging_writer.py:48] [362200] global_step=362200, grad_norm=2.954519510269165, loss=1.3690648078918457
I0304 12:09:25.330198 140380326586112 logging_writer.py:48] [362300] global_step=362300, grad_norm=3.8499293327331543, loss=2.9868783950805664
I0304 12:10:10.195560 140380334978816 logging_writer.py:48] [362400] global_step=362400, grad_norm=3.3304104804992676, loss=1.0818939208984375
I0304 12:10:55.017122 140380326586112 logging_writer.py:48] [362500] global_step=362500, grad_norm=3.4990012645721436, loss=1.2420216798782349
I0304 12:11:39.948378 140380334978816 logging_writer.py:48] [362600] global_step=362600, grad_norm=3.189007043838501, loss=2.4045276641845703
I0304 12:12:24.814155 140380326586112 logging_writer.py:48] [362700] global_step=362700, grad_norm=3.273618698120117, loss=1.2394133806228638
I0304 12:12:54.470539 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:13:05.205031 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:13:31.962902 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:13:33.628743 140575196817216 submission_runner.py:411] Time since start: 175189.23s, 	Step: 362768, 	{'train/accuracy': 0.8859570026397705, 'train/loss': 0.4194192886352539, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 161369.6048526764, 'total_duration': 175189.22968149185, 'accumulated_submission_time': 161369.6048526764, 'accumulated_eval_time': 13776.793885946274, 'accumulated_logging_time': 23.607590436935425}
I0304 12:13:33.699100 140380334978816 logging_writer.py:48] [362768] accumulated_eval_time=13776.793886, accumulated_logging_time=23.607590, accumulated_submission_time=161369.604853, global_step=362768, preemption_count=0, score=161369.604853, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=175189.229681, train/accuracy=0.885957, train/loss=0.419419, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:13:46.838286 140380326586112 logging_writer.py:48] [362800] global_step=362800, grad_norm=4.418361663818359, loss=1.1005727052688599
I0304 12:14:29.073520 140380334978816 logging_writer.py:48] [362900] global_step=362900, grad_norm=2.966670274734497, loss=1.1779111623764038
I0304 12:15:14.005346 140380326586112 logging_writer.py:48] [363000] global_step=363000, grad_norm=3.1518304347991943, loss=1.1452600955963135
I0304 12:15:59.086014 140380334978816 logging_writer.py:48] [363100] global_step=363100, grad_norm=3.203169822692871, loss=1.221003770828247
I0304 12:16:44.198754 140380326586112 logging_writer.py:48] [363200] global_step=363200, grad_norm=3.545377492904663, loss=1.8387086391448975
I0304 12:17:29.400206 140380334978816 logging_writer.py:48] [363300] global_step=363300, grad_norm=3.466769218444824, loss=1.0341136455535889
I0304 12:18:14.416078 140380326586112 logging_writer.py:48] [363400] global_step=363400, grad_norm=3.146540880203247, loss=1.1138677597045898
I0304 12:18:59.395860 140380334978816 logging_writer.py:48] [363500] global_step=363500, grad_norm=3.0193989276885986, loss=1.2945704460144043
I0304 12:19:44.274132 140380326586112 logging_writer.py:48] [363600] global_step=363600, grad_norm=3.049995183944702, loss=1.242629051208496
I0304 12:20:29.420632 140380334978816 logging_writer.py:48] [363700] global_step=363700, grad_norm=2.8969671726226807, loss=1.0413025617599487
I0304 12:20:34.017724 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:20:44.683487 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:21:09.325856 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:21:10.983299 140575196817216 submission_runner.py:411] Time since start: 175646.58s, 	Step: 363712, 	{'train/accuracy': 0.8870702981948853, 'train/loss': 0.4185977876186371, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 161789.86298680305, 'total_duration': 175646.58432078362, 'accumulated_submission_time': 161789.86298680305, 'accumulated_eval_time': 13813.758406877518, 'accumulated_logging_time': 23.688162088394165}
I0304 12:21:11.052922 140380326586112 logging_writer.py:48] [363712] accumulated_eval_time=13813.758407, accumulated_logging_time=23.688162, accumulated_submission_time=161789.862987, global_step=363712, preemption_count=0, score=161789.862987, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=175646.584321, train/accuracy=0.887070, train/loss=0.418598, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:21:46.389771 140380334978816 logging_writer.py:48] [363800] global_step=363800, grad_norm=2.9575283527374268, loss=1.1101329326629639
I0304 12:22:31.218607 140380326586112 logging_writer.py:48] [363900] global_step=363900, grad_norm=3.3994791507720947, loss=2.0853796005249023
I0304 12:23:16.169747 140380334978816 logging_writer.py:48] [364000] global_step=364000, grad_norm=3.2290422916412354, loss=2.198035955429077
I0304 12:24:01.794185 140380326586112 logging_writer.py:48] [364100] global_step=364100, grad_norm=2.9917547702789307, loss=1.1137020587921143
I0304 12:24:46.660480 140380334978816 logging_writer.py:48] [364200] global_step=364200, grad_norm=3.520707845687866, loss=2.862147808074951
I0304 12:25:32.220843 140380326586112 logging_writer.py:48] [364300] global_step=364300, grad_norm=3.1577398777008057, loss=1.2731428146362305
I0304 12:26:17.328165 140380334978816 logging_writer.py:48] [364400] global_step=364400, grad_norm=3.080935478210449, loss=1.0326734781265259
I0304 12:27:02.305625 140380326586112 logging_writer.py:48] [364500] global_step=364500, grad_norm=3.3697054386138916, loss=2.964123487472534
I0304 12:27:47.295383 140380334978816 logging_writer.py:48] [364600] global_step=364600, grad_norm=3.2845544815063477, loss=1.0649335384368896
I0304 12:28:11.331562 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:28:21.961241 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:28:46.029303 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:28:47.681738 140575196817216 submission_runner.py:411] Time since start: 176103.28s, 	Step: 364655, 	{'train/accuracy': 0.8883007764816284, 'train/loss': 0.41668641567230225, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 162210.0809226036, 'total_duration': 176103.28262138367, 'accumulated_submission_time': 162210.0809226036, 'accumulated_eval_time': 13850.107394695282, 'accumulated_logging_time': 23.76880979537964}
I0304 12:28:47.753350 140380326586112 logging_writer.py:48] [364655] accumulated_eval_time=13850.107395, accumulated_logging_time=23.768810, accumulated_submission_time=162210.080923, global_step=364655, preemption_count=0, score=162210.080923, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=176103.282621, train/accuracy=0.888301, train/loss=0.416686, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:29:06.063317 140380334978816 logging_writer.py:48] [364700] global_step=364700, grad_norm=3.1363766193389893, loss=1.1553864479064941
I0304 12:29:48.637567 140380326586112 logging_writer.py:48] [364800] global_step=364800, grad_norm=3.182027816772461, loss=1.2202434539794922
I0304 12:30:33.725698 140380334978816 logging_writer.py:48] [364900] global_step=364900, grad_norm=2.956270217895508, loss=1.2662739753723145
I0304 12:31:18.850909 140380326586112 logging_writer.py:48] [365000] global_step=365000, grad_norm=2.92661452293396, loss=1.1813552379608154
I0304 12:32:03.826488 140380334978816 logging_writer.py:48] [365100] global_step=365100, grad_norm=3.161499261856079, loss=1.8274891376495361
I0304 12:32:48.808149 140380326586112 logging_writer.py:48] [365200] global_step=365200, grad_norm=3.062220811843872, loss=1.1448739767074585
I0304 12:33:33.812461 140380334978816 logging_writer.py:48] [365300] global_step=365300, grad_norm=3.2697386741638184, loss=1.1440861225128174
I0304 12:34:18.977849 140380326586112 logging_writer.py:48] [365400] global_step=365400, grad_norm=3.1030960083007812, loss=2.8604061603546143
I0304 12:35:03.816901 140380334978816 logging_writer.py:48] [365500] global_step=365500, grad_norm=3.1336774826049805, loss=1.1535260677337646
I0304 12:35:47.859403 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:35:58.534420 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:36:23.134536 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:36:24.790620 140575196817216 submission_runner.py:411] Time since start: 176560.39s, 	Step: 365599, 	{'train/accuracy': 0.8848828077316284, 'train/loss': 0.4277326166629791, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 162630.12792491913, 'total_duration': 176560.39166498184, 'accumulated_submission_time': 162630.12792491913, 'accumulated_eval_time': 13887.037593126297, 'accumulated_logging_time': 23.85041332244873}
I0304 12:36:24.862561 140380326586112 logging_writer.py:48] [365599] accumulated_eval_time=13887.037593, accumulated_logging_time=23.850413, accumulated_submission_time=162630.127925, global_step=365599, preemption_count=0, score=162630.127925, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=176560.391665, train/accuracy=0.884883, train/loss=0.427733, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:36:25.663621 140380334978816 logging_writer.py:48] [365600] global_step=365600, grad_norm=3.9901249408721924, loss=2.99878191947937
I0304 12:37:06.036093 140380326586112 logging_writer.py:48] [365700] global_step=365700, grad_norm=3.2509267330169678, loss=1.4458191394805908
I0304 12:37:50.891958 140380334978816 logging_writer.py:48] [365800] global_step=365800, grad_norm=4.240398406982422, loss=3.148477792739868
I0304 12:38:36.321091 140380326586112 logging_writer.py:48] [365900] global_step=365900, grad_norm=3.3082456588745117, loss=1.0830153226852417
I0304 12:39:21.254808 140380334978816 logging_writer.py:48] [366000] global_step=366000, grad_norm=3.0579185485839844, loss=1.049086332321167
I0304 12:40:06.417589 140380326586112 logging_writer.py:48] [366100] global_step=366100, grad_norm=3.0513954162597656, loss=1.178497314453125
I0304 12:40:51.409670 140380334978816 logging_writer.py:48] [366200] global_step=366200, grad_norm=3.275334358215332, loss=1.268526554107666
I0304 12:41:36.462975 140380326586112 logging_writer.py:48] [366300] global_step=366300, grad_norm=3.1282553672790527, loss=1.4902266263961792
I0304 12:42:21.657603 140380334978816 logging_writer.py:48] [366400] global_step=366400, grad_norm=3.916344404220581, loss=3.2650275230407715
I0304 12:43:06.661470 140380326586112 logging_writer.py:48] [366500] global_step=366500, grad_norm=3.111572027206421, loss=1.0628619194030762
I0304 12:43:25.136713 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:43:35.808199 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:44:02.120272 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:44:03.770480 140575196817216 submission_runner.py:411] Time since start: 177019.37s, 	Step: 366543, 	{'train/accuracy': 0.8892773389816284, 'train/loss': 0.4170738458633423, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 163050.34325742722, 'total_duration': 177019.37155103683, 'accumulated_submission_time': 163050.34325742722, 'accumulated_eval_time': 13925.670383930206, 'accumulated_logging_time': 23.932437658309937}
I0304 12:44:03.841477 140380334978816 logging_writer.py:48] [366543] accumulated_eval_time=13925.670384, accumulated_logging_time=23.932438, accumulated_submission_time=163050.343257, global_step=366543, preemption_count=0, score=163050.343257, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=177019.371551, train/accuracy=0.889277, train/loss=0.417074, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:44:26.895190 140380326586112 logging_writer.py:48] [366600] global_step=366600, grad_norm=4.4839582443237305, loss=3.336247444152832
I0304 12:45:10.030050 140380334978816 logging_writer.py:48] [366700] global_step=366700, grad_norm=3.9624176025390625, loss=3.1987390518188477
I0304 12:45:55.157744 140380326586112 logging_writer.py:48] [366800] global_step=366800, grad_norm=3.487107276916504, loss=1.0936676263809204
I0304 12:46:40.674870 140380334978816 logging_writer.py:48] [366900] global_step=366900, grad_norm=2.941559314727783, loss=1.4651134014129639
I0304 12:47:25.735318 140380326586112 logging_writer.py:48] [367000] global_step=367000, grad_norm=3.5934174060821533, loss=2.8122971057891846
I0304 12:48:11.012385 140380334978816 logging_writer.py:48] [367100] global_step=367100, grad_norm=3.5920932292938232, loss=1.3597462177276611
I0304 12:48:55.854101 140380326586112 logging_writer.py:48] [367200] global_step=367200, grad_norm=2.9411568641662598, loss=1.2434823513031006
I0304 12:49:40.910140 140380334978816 logging_writer.py:48] [367300] global_step=367300, grad_norm=3.2736823558807373, loss=1.983056664466858
I0304 12:50:25.754925 140380326586112 logging_writer.py:48] [367400] global_step=367400, grad_norm=3.064216136932373, loss=1.1191316843032837
I0304 12:51:03.853156 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:51:14.537981 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:51:40.265725 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:51:41.926141 140575196817216 submission_runner.py:411] Time since start: 177477.53s, 	Step: 367486, 	{'train/accuracy': 0.8848046660423279, 'train/loss': 0.4290616512298584, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 163470.2967107296, 'total_duration': 177477.52720284462, 'accumulated_submission_time': 163470.2967107296, 'accumulated_eval_time': 13963.74238228798, 'accumulated_logging_time': 24.012521743774414}
I0304 12:51:42.000295 140380334978816 logging_writer.py:48] [367486] accumulated_eval_time=13963.742382, accumulated_logging_time=24.012522, accumulated_submission_time=163470.296711, global_step=367486, preemption_count=0, score=163470.296711, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=177477.527203, train/accuracy=0.884805, train/loss=0.429062, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:51:47.969560 140380326586112 logging_writer.py:48] [367500] global_step=367500, grad_norm=3.3587002754211426, loss=2.6777920722961426
I0304 12:52:29.099607 140380334978816 logging_writer.py:48] [367600] global_step=367600, grad_norm=3.104959726333618, loss=1.0834264755249023
I0304 12:53:13.941462 140380326586112 logging_writer.py:48] [367700] global_step=367700, grad_norm=2.8570847511291504, loss=1.948499321937561
I0304 12:53:58.985453 140380334978816 logging_writer.py:48] [367800] global_step=367800, grad_norm=3.072009325027466, loss=1.138897180557251
I0304 12:54:44.432369 140380326586112 logging_writer.py:48] [367900] global_step=367900, grad_norm=4.074785232543945, loss=3.18564772605896
I0304 12:55:29.510170 140380334978816 logging_writer.py:48] [368000] global_step=368000, grad_norm=4.218353271484375, loss=3.304527521133423
I0304 12:56:14.526632 140380326586112 logging_writer.py:48] [368100] global_step=368100, grad_norm=2.9987292289733887, loss=1.1175339221954346
I0304 12:56:59.439930 140380334978816 logging_writer.py:48] [368200] global_step=368200, grad_norm=3.139582395553589, loss=1.7161993980407715
I0304 12:57:44.593305 140380326586112 logging_writer.py:48] [368300] global_step=368300, grad_norm=4.265965938568115, loss=3.313117027282715
I0304 12:58:29.647828 140380334978816 logging_writer.py:48] [368400] global_step=368400, grad_norm=3.351322650909424, loss=1.1374181509017944
I0304 12:58:41.967902 140575196817216 spec.py:321] Evaluating on the training split.
I0304 12:58:52.653721 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 12:59:17.919556 140575196817216 spec.py:349] Evaluating on the test split.
I0304 12:59:19.569504 140575196817216 submission_runner.py:411] Time since start: 177935.17s, 	Step: 368429, 	{'train/accuracy': 0.8879687190055847, 'train/loss': 0.420084685087204, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 163890.20581889153, 'total_duration': 177935.1705136299, 'accumulated_submission_time': 163890.20581889153, 'accumulated_eval_time': 14001.34293437004, 'accumulated_logging_time': 24.096700191497803}
I0304 12:59:19.640863 140380326586112 logging_writer.py:48] [368429] accumulated_eval_time=14001.342934, accumulated_logging_time=24.096700, accumulated_submission_time=163890.205819, global_step=368429, preemption_count=0, score=163890.205819, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=177935.170514, train/accuracy=0.887969, train/loss=0.420085, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 12:59:48.262516 140380334978816 logging_writer.py:48] [368500] global_step=368500, grad_norm=3.0140297412872314, loss=1.1540268659591675
I0304 13:00:32.166051 140380326586112 logging_writer.py:48] [368600] global_step=368600, grad_norm=2.863903284072876, loss=1.8658313751220703
I0304 13:01:17.377589 140380334978816 logging_writer.py:48] [368700] global_step=368700, grad_norm=3.0481643676757812, loss=1.113683819770813
I0304 13:02:02.403219 140380326586112 logging_writer.py:48] [368800] global_step=368800, grad_norm=3.789804697036743, loss=2.977841854095459
I0304 13:02:47.439856 140380334978816 logging_writer.py:48] [368900] global_step=368900, grad_norm=4.391620635986328, loss=3.2090702056884766
I0304 13:03:32.528410 140380326586112 logging_writer.py:48] [369000] global_step=369000, grad_norm=3.9433982372283936, loss=3.1756927967071533
I0304 13:04:17.838903 140380334978816 logging_writer.py:48] [369100] global_step=369100, grad_norm=3.201639413833618, loss=2.576626777648926
I0304 13:05:02.956454 140380326586112 logging_writer.py:48] [369200] global_step=369200, grad_norm=3.166783332824707, loss=1.131394386291504
I0304 13:05:47.977022 140380334978816 logging_writer.py:48] [369300] global_step=369300, grad_norm=3.2404072284698486, loss=1.1511505842208862
I0304 13:06:19.860209 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:06:30.459362 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:06:58.622389 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:07:00.278116 140575196817216 submission_runner.py:411] Time since start: 178395.88s, 	Step: 369372, 	{'train/accuracy': 0.8879687190055847, 'train/loss': 0.42086976766586304, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 164310.3669514656, 'total_duration': 178395.87911200523, 'accumulated_submission_time': 164310.3669514656, 'accumulated_eval_time': 14041.759779453278, 'accumulated_logging_time': 24.177711248397827}
I0304 13:07:00.350438 140380326586112 logging_writer.py:48] [369372] accumulated_eval_time=14041.759779, accumulated_logging_time=24.177711, accumulated_submission_time=164310.366951, global_step=369372, preemption_count=0, score=164310.366951, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=178395.879112, train/accuracy=0.887969, train/loss=0.420870, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:07:11.873410 140380334978816 logging_writer.py:48] [369400] global_step=369400, grad_norm=2.903656482696533, loss=2.081451892852783
I0304 13:07:53.638148 140380326586112 logging_writer.py:48] [369500] global_step=369500, grad_norm=3.475863218307495, loss=1.120082139968872
I0304 13:08:38.676213 140380334978816 logging_writer.py:48] [369600] global_step=369600, grad_norm=3.2427291870117188, loss=2.236051082611084
I0304 13:09:23.920549 140380326586112 logging_writer.py:48] [369700] global_step=369700, grad_norm=2.995194435119629, loss=1.1364073753356934
I0304 13:10:08.620525 140380334978816 logging_writer.py:48] [369800] global_step=369800, grad_norm=3.165748119354248, loss=2.1365089416503906
I0304 13:10:53.394813 140380326586112 logging_writer.py:48] [369900] global_step=369900, grad_norm=3.0008456707000732, loss=1.1803672313690186
I0304 13:11:38.456401 140380334978816 logging_writer.py:48] [370000] global_step=370000, grad_norm=3.270630121231079, loss=1.2810814380645752
I0304 13:12:23.697502 140380326586112 logging_writer.py:48] [370100] global_step=370100, grad_norm=3.2614023685455322, loss=1.5629514455795288
I0304 13:13:08.584511 140380334978816 logging_writer.py:48] [370200] global_step=370200, grad_norm=3.061314821243286, loss=1.3956271409988403
I0304 13:13:53.123262 140380326586112 logging_writer.py:48] [370300] global_step=370300, grad_norm=3.0777337551116943, loss=1.139011025428772
I0304 13:14:00.483631 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:14:11.112109 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:14:38.017493 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:14:39.670673 140575196817216 submission_runner.py:411] Time since start: 178855.27s, 	Step: 370318, 	{'train/accuracy': 0.88587886095047, 'train/loss': 0.42093217372894287, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 164730.44117760658, 'total_duration': 178855.2717988491, 'accumulated_submission_time': 164730.44117760658, 'accumulated_eval_time': 14080.945871591568, 'accumulated_logging_time': 24.259997367858887}
I0304 13:14:39.741985 140380334978816 logging_writer.py:48] [370318] accumulated_eval_time=14080.945872, accumulated_logging_time=24.259997, accumulated_submission_time=164730.441178, global_step=370318, preemption_count=0, score=164730.441178, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=178855.271799, train/accuracy=0.885879, train/loss=0.420932, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:15:12.710711 140380326586112 logging_writer.py:48] [370400] global_step=370400, grad_norm=4.052849769592285, loss=3.030768871307373
I0304 13:15:56.812777 140380334978816 logging_writer.py:48] [370500] global_step=370500, grad_norm=2.8068721294403076, loss=1.8174231052398682
I0304 13:16:41.974490 140380326586112 logging_writer.py:48] [370600] global_step=370600, grad_norm=3.4214210510253906, loss=1.1508787870407104
I0304 13:17:27.016060 140380334978816 logging_writer.py:48] [370700] global_step=370700, grad_norm=3.174555778503418, loss=1.115709662437439
I0304 13:18:11.850087 140380326586112 logging_writer.py:48] [370800] global_step=370800, grad_norm=2.9434962272644043, loss=1.662406325340271
I0304 13:18:56.834010 140380334978816 logging_writer.py:48] [370900] global_step=370900, grad_norm=3.1437623500823975, loss=1.1725976467132568
I0304 13:19:42.220191 140380326586112 logging_writer.py:48] [371000] global_step=371000, grad_norm=3.794969320297241, loss=1.1315362453460693
I0304 13:20:27.195251 140380334978816 logging_writer.py:48] [371100] global_step=371100, grad_norm=3.5880773067474365, loss=3.174342155456543
I0304 13:21:12.216299 140380326586112 logging_writer.py:48] [371200] global_step=371200, grad_norm=3.0287890434265137, loss=1.3385193347930908
I0304 13:21:39.749548 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:21:50.525850 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:22:16.666093 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:22:18.313972 140575196817216 submission_runner.py:411] Time since start: 179313.92s, 	Step: 371263, 	{'train/accuracy': 0.8868359327316284, 'train/loss': 0.419025719165802, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 165150.38989567757, 'total_duration': 179313.91506695747, 'accumulated_submission_time': 165150.38989567757, 'accumulated_eval_time': 14119.509327411652, 'accumulated_logging_time': 24.341216564178467}
I0304 13:22:18.385504 140380334978816 logging_writer.py:48] [371263] accumulated_eval_time=14119.509327, accumulated_logging_time=24.341217, accumulated_submission_time=165150.389896, global_step=371263, preemption_count=0, score=165150.389896, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=179313.915067, train/accuracy=0.886836, train/loss=0.419026, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:22:33.500482 140380326586112 logging_writer.py:48] [371300] global_step=371300, grad_norm=3.426165819168091, loss=1.0578358173370361
I0304 13:23:15.545605 140380334978816 logging_writer.py:48] [371400] global_step=371400, grad_norm=2.9627175331115723, loss=1.4906837940216064
I0304 13:24:00.647976 140380326586112 logging_writer.py:48] [371500] global_step=371500, grad_norm=2.788815498352051, loss=1.3197191953659058
I0304 13:24:46.101134 140380334978816 logging_writer.py:48] [371600] global_step=371600, grad_norm=3.1228413581848145, loss=1.1077117919921875
I0304 13:25:31.026973 140380326586112 logging_writer.py:48] [371700] global_step=371700, grad_norm=3.4669675827026367, loss=2.6293022632598877
I0304 13:26:16.117130 140380334978816 logging_writer.py:48] [371800] global_step=371800, grad_norm=2.8909101486206055, loss=1.1641422510147095
I0304 13:27:01.205180 140380326586112 logging_writer.py:48] [371900] global_step=371900, grad_norm=3.1714346408843994, loss=1.996116280555725
I0304 13:27:46.289765 140380334978816 logging_writer.py:48] [372000] global_step=372000, grad_norm=3.191493511199951, loss=1.2329193353652954
I0304 13:28:31.539045 140380326586112 logging_writer.py:48] [372100] global_step=372100, grad_norm=3.0689728260040283, loss=1.1376198530197144
I0304 13:29:16.537978 140380334978816 logging_writer.py:48] [372200] global_step=372200, grad_norm=3.3301191329956055, loss=1.1794321537017822
I0304 13:29:18.458011 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:29:29.145948 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:29:55.208418 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:29:56.867679 140575196817216 submission_runner.py:411] Time since start: 179772.47s, 	Step: 372206, 	{'train/accuracy': 0.8878905773162842, 'train/loss': 0.4138197898864746, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 165570.40331053734, 'total_duration': 179772.46859931946, 'accumulated_submission_time': 165570.40331053734, 'accumulated_eval_time': 14157.917885541916, 'accumulated_logging_time': 24.423150539398193}
I0304 13:29:56.937821 140380326586112 logging_writer.py:48] [372206] accumulated_eval_time=14157.917886, accumulated_logging_time=24.423151, accumulated_submission_time=165570.403311, global_step=372206, preemption_count=0, score=165570.403311, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=179772.468599, train/accuracy=0.887891, train/loss=0.413820, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:30:35.274982 140380334978816 logging_writer.py:48] [372300] global_step=372300, grad_norm=3.1061110496520996, loss=1.0751783847808838
I0304 13:31:19.991112 140380326586112 logging_writer.py:48] [372400] global_step=372400, grad_norm=3.6187071800231934, loss=2.568129539489746
I0304 13:32:05.144042 140380334978816 logging_writer.py:48] [372500] global_step=372500, grad_norm=3.2920827865600586, loss=2.426424741744995
I0304 13:32:50.124243 140380326586112 logging_writer.py:48] [372600] global_step=372600, grad_norm=3.2027032375335693, loss=1.28952157497406
I0304 13:33:35.029536 140380334978816 logging_writer.py:48] [372700] global_step=372700, grad_norm=3.6967740058898926, loss=2.784019708633423
I0304 13:34:19.844277 140380326586112 logging_writer.py:48] [372800] global_step=372800, grad_norm=3.3752386569976807, loss=1.728429913520813
I0304 13:35:04.996579 140380334978816 logging_writer.py:48] [372900] global_step=372900, grad_norm=4.201700687408447, loss=3.2000303268432617
I0304 13:35:49.991028 140380326586112 logging_writer.py:48] [373000] global_step=373000, grad_norm=3.1686954498291016, loss=1.1902583837509155
I0304 13:36:35.255492 140380334978816 logging_writer.py:48] [373100] global_step=373100, grad_norm=2.9558651447296143, loss=1.9872989654541016
I0304 13:36:57.140965 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:37:07.829270 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:37:34.469849 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:37:36.120376 140575196817216 submission_runner.py:411] Time since start: 180231.72s, 	Step: 373151, 	{'train/accuracy': 0.8874022960662842, 'train/loss': 0.42186304926872253, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 165990.54666543007, 'total_duration': 180231.72131824493, 'accumulated_submission_time': 165990.54666543007, 'accumulated_eval_time': 14196.896178245544, 'accumulated_logging_time': 24.503934383392334}
I0304 13:37:36.190638 140380326586112 logging_writer.py:48] [373151] accumulated_eval_time=14196.896178, accumulated_logging_time=24.503934, accumulated_submission_time=165990.546665, global_step=373151, preemption_count=0, score=165990.546665, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=180231.721318, train/accuracy=0.887402, train/loss=0.421863, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:37:56.077292 140380334978816 logging_writer.py:48] [373200] global_step=373200, grad_norm=3.498224973678589, loss=1.2343499660491943
I0304 13:38:38.819006 140380326586112 logging_writer.py:48] [373300] global_step=373300, grad_norm=3.0596811771392822, loss=1.3019105195999146
I0304 13:39:23.983587 140380334978816 logging_writer.py:48] [373400] global_step=373400, grad_norm=3.348423957824707, loss=1.115034580230713
I0304 13:40:09.300294 140380326586112 logging_writer.py:48] [373500] global_step=373500, grad_norm=2.982700824737549, loss=0.9959391355514526
I0304 13:40:54.451339 140380334978816 logging_writer.py:48] [373600] global_step=373600, grad_norm=3.939981698989868, loss=1.3263142108917236
I0304 13:41:39.534142 140380326586112 logging_writer.py:48] [373700] global_step=373700, grad_norm=3.541736125946045, loss=1.364231824874878
I0304 13:42:24.491983 140380334978816 logging_writer.py:48] [373800] global_step=373800, grad_norm=3.12078595161438, loss=1.9098336696624756
I0304 13:43:09.662110 140380326586112 logging_writer.py:48] [373900] global_step=373900, grad_norm=2.842841625213623, loss=1.1950554847717285
I0304 13:43:54.490661 140380334978816 logging_writer.py:48] [374000] global_step=374000, grad_norm=3.165527820587158, loss=2.176297664642334
I0304 13:44:36.182640 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:44:46.897734 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:45:10.602596 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:45:12.242766 140575196817216 submission_runner.py:411] Time since start: 180687.84s, 	Step: 374094, 	{'train/accuracy': 0.8924023509025574, 'train/loss': 0.4078778326511383, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 166410.48005771637, 'total_duration': 180687.84383511543, 'accumulated_submission_time': 166410.48005771637, 'accumulated_eval_time': 14232.955313920975, 'accumulated_logging_time': 24.584126710891724}
I0304 13:45:12.316133 140380326586112 logging_writer.py:48] [374094] accumulated_eval_time=14232.955314, accumulated_logging_time=24.584127, accumulated_submission_time=166410.480058, global_step=374094, preemption_count=0, score=166410.480058, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=180687.843835, train/accuracy=0.892402, train/loss=0.407878, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:45:15.099407 140380334978816 logging_writer.py:48] [374100] global_step=374100, grad_norm=3.1260266304016113, loss=1.1440064907073975
I0304 13:45:56.405515 140380326586112 logging_writer.py:48] [374200] global_step=374200, grad_norm=2.8474252223968506, loss=1.0620758533477783
I0304 13:46:41.524655 140380334978816 logging_writer.py:48] [374300] global_step=374300, grad_norm=3.05263352394104, loss=1.5981088876724243
I0304 13:47:26.643837 140380326586112 logging_writer.py:48] [374400] global_step=374400, grad_norm=3.4356517791748047, loss=1.1687743663787842
I0304 13:48:11.574361 140380334978816 logging_writer.py:48] [374500] global_step=374500, grad_norm=3.3257784843444824, loss=2.892352819442749
I0304 13:48:56.705541 140380326586112 logging_writer.py:48] [374600] global_step=374600, grad_norm=3.4033405780792236, loss=1.0525621175765991
I0304 13:49:41.604295 140380334978816 logging_writer.py:48] [374700] global_step=374700, grad_norm=3.148740291595459, loss=1.1512795686721802
I0304 13:50:26.524487 140380326586112 logging_writer.py:48] [374800] global_step=374800, grad_norm=3.228015899658203, loss=2.637416362762451
I0304 13:51:11.393205 140380334978816 logging_writer.py:48] [374900] global_step=374900, grad_norm=4.051299571990967, loss=3.1128947734832764
I0304 13:51:56.515329 140380326586112 logging_writer.py:48] [375000] global_step=375000, grad_norm=3.3179805278778076, loss=1.958667516708374
I0304 13:52:12.395116 140575196817216 spec.py:321] Evaluating on the training split.
I0304 13:52:23.284355 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 13:52:48.638077 140575196817216 spec.py:349] Evaluating on the test split.
I0304 13:52:50.300371 140575196817216 submission_runner.py:411] Time since start: 181145.90s, 	Step: 375037, 	{'train/accuracy': 0.8862890601158142, 'train/loss': 0.4168686270713806, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 166830.50103092194, 'total_duration': 181145.90125513077, 'accumulated_submission_time': 166830.50103092194, 'accumulated_eval_time': 14270.859405994415, 'accumulated_logging_time': 24.667348384857178}
I0304 13:52:50.384781 140380334978816 logging_writer.py:48] [375037] accumulated_eval_time=14270.859406, accumulated_logging_time=24.667348, accumulated_submission_time=166830.501031, global_step=375037, preemption_count=0, score=166830.501031, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=181145.901255, train/accuracy=0.886289, train/loss=0.416869, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 13:53:15.845698 140380326586112 logging_writer.py:48] [375100] global_step=375100, grad_norm=3.6895289421081543, loss=2.8675012588500977
I0304 13:53:59.399176 140380334978816 logging_writer.py:48] [375200] global_step=375200, grad_norm=3.691805839538574, loss=3.212721109390259
I0304 13:54:44.215849 140380326586112 logging_writer.py:48] [375300] global_step=375300, grad_norm=3.8594906330108643, loss=3.304641008377075
I0304 13:55:29.806403 140380334978816 logging_writer.py:48] [375400] global_step=375400, grad_norm=4.085318565368652, loss=3.266679286956787
I0304 13:56:14.989193 140380326586112 logging_writer.py:48] [375500] global_step=375500, grad_norm=3.0832509994506836, loss=2.5541672706604004
I0304 13:56:59.981492 140380334978816 logging_writer.py:48] [375600] global_step=375600, grad_norm=3.5172505378723145, loss=2.9198286533355713
I0304 13:57:44.953886 140380326586112 logging_writer.py:48] [375700] global_step=375700, grad_norm=2.952676296234131, loss=1.191341519355774
I0304 13:58:29.871204 140380334978816 logging_writer.py:48] [375800] global_step=375800, grad_norm=3.980442762374878, loss=3.1079421043395996
I0304 13:59:15.025515 140380326586112 logging_writer.py:48] [375900] global_step=375900, grad_norm=3.4486916065216064, loss=2.6665616035461426
I0304 13:59:50.669100 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:00:01.409551 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:00:29.060369 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:00:30.715657 140575196817216 submission_runner.py:411] Time since start: 181606.32s, 	Step: 375981, 	{'train/accuracy': 0.8896484375, 'train/loss': 0.4130140542984009, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 167250.72504115105, 'total_duration': 181606.3165895939, 'accumulated_submission_time': 167250.72504115105, 'accumulated_eval_time': 14310.904839277267, 'accumulated_logging_time': 24.76238989830017}
I0304 14:00:30.805140 140380334978816 logging_writer.py:48] [375981] accumulated_eval_time=14310.904839, accumulated_logging_time=24.762390, accumulated_submission_time=167250.725041, global_step=375981, preemption_count=0, score=167250.725041, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=181606.316590, train/accuracy=0.889648, train/loss=0.413014, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:00:38.746454 140380326586112 logging_writer.py:48] [376000] global_step=376000, grad_norm=3.083610773086548, loss=2.5332913398742676
I0304 14:01:19.988377 140380334978816 logging_writer.py:48] [376100] global_step=376100, grad_norm=4.488944053649902, loss=3.2584331035614014
I0304 14:02:04.804710 140380326586112 logging_writer.py:48] [376200] global_step=376200, grad_norm=3.0776560306549072, loss=1.2068434953689575
I0304 14:02:49.894638 140380334978816 logging_writer.py:48] [376300] global_step=376300, grad_norm=3.2918999195098877, loss=1.0635554790496826
I0304 14:03:35.069602 140380326586112 logging_writer.py:48] [376400] global_step=376400, grad_norm=3.2185518741607666, loss=2.057786464691162
I0304 14:04:19.769797 140380334978816 logging_writer.py:48] [376500] global_step=376500, grad_norm=3.0181992053985596, loss=1.2142210006713867
I0304 14:05:05.339342 140380326586112 logging_writer.py:48] [376600] global_step=376600, grad_norm=3.1004981994628906, loss=1.1701594591140747
I0304 14:05:50.725405 140380334978816 logging_writer.py:48] [376700] global_step=376700, grad_norm=3.2204184532165527, loss=2.696187973022461
I0304 14:06:36.092714 140380326586112 logging_writer.py:48] [376800] global_step=376800, grad_norm=2.741119861602783, loss=1.5735467672348022
I0304 14:07:21.065991 140380334978816 logging_writer.py:48] [376900] global_step=376900, grad_norm=3.2323718070983887, loss=1.386259913444519
I0304 14:07:31.091156 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:07:42.578342 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:08:08.558439 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:08:10.226146 140575196817216 submission_runner.py:411] Time since start: 182065.83s, 	Step: 376924, 	{'train/accuracy': 0.8883007764816284, 'train/loss': 0.41643446683883667, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 167670.94829511642, 'total_duration': 182065.82713150978, 'accumulated_submission_time': 167670.94829511642, 'accumulated_eval_time': 14350.03876209259, 'accumulated_logging_time': 24.86554718017578}
I0304 14:08:10.301922 140380326586112 logging_writer.py:48] [376924] accumulated_eval_time=14350.038762, accumulated_logging_time=24.865547, accumulated_submission_time=167670.948295, global_step=376924, preemption_count=0, score=167670.948295, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=182065.827132, train/accuracy=0.888301, train/loss=0.416434, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:08:40.900467 140380334978816 logging_writer.py:48] [377000] global_step=377000, grad_norm=3.386666774749756, loss=1.175620198249817
I0304 14:09:24.896235 140380326586112 logging_writer.py:48] [377100] global_step=377100, grad_norm=3.047421932220459, loss=1.3121540546417236
I0304 14:10:10.178851 140380334978816 logging_writer.py:48] [377200] global_step=377200, grad_norm=3.029907703399658, loss=1.2941629886627197
I0304 14:10:55.468970 140380326586112 logging_writer.py:48] [377300] global_step=377300, grad_norm=3.1268341541290283, loss=1.034492015838623
I0304 14:11:40.440650 140380334978816 logging_writer.py:48] [377400] global_step=377400, grad_norm=2.9123964309692383, loss=1.0849006175994873
I0304 14:12:25.488066 140380326586112 logging_writer.py:48] [377500] global_step=377500, grad_norm=2.9589591026306152, loss=1.0354784727096558
I0304 14:13:10.570899 140380334978816 logging_writer.py:48] [377600] global_step=377600, grad_norm=3.0605063438415527, loss=1.1280243396759033
I0304 14:13:55.798915 140380326586112 logging_writer.py:48] [377700] global_step=377700, grad_norm=3.849954128265381, loss=3.1196351051330566
I0304 14:14:40.776183 140380334978816 logging_writer.py:48] [377800] global_step=377800, grad_norm=3.3284029960632324, loss=1.0917949676513672
I0304 14:15:10.355513 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:15:21.085323 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:15:46.570521 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:15:48.218137 140575196817216 submission_runner.py:411] Time since start: 182523.82s, 	Step: 377867, 	{'train/accuracy': 0.8856835961341858, 'train/loss': 0.4257451891899109, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 168090.943703413, 'total_duration': 182523.81906175613, 'accumulated_submission_time': 168090.943703413, 'accumulated_eval_time': 14387.900235891342, 'accumulated_logging_time': 24.95109248161316}
I0304 14:15:48.290654 140380326586112 logging_writer.py:48] [377867] accumulated_eval_time=14387.900236, accumulated_logging_time=24.951092, accumulated_submission_time=168090.943703, global_step=377867, preemption_count=0, score=168090.943703, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=182523.819062, train/accuracy=0.885684, train/loss=0.425745, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:16:01.795716 140380334978816 logging_writer.py:48] [377900] global_step=377900, grad_norm=3.354778528213501, loss=1.1015129089355469
I0304 14:16:43.717840 140380326586112 logging_writer.py:48] [378000] global_step=378000, grad_norm=4.061462879180908, loss=3.220299243927002
I0304 14:17:28.963605 140380334978816 logging_writer.py:48] [378100] global_step=378100, grad_norm=3.5927181243896484, loss=3.111835479736328
I0304 14:18:14.287128 140380326586112 logging_writer.py:48] [378200] global_step=378200, grad_norm=3.329587936401367, loss=1.0979149341583252
I0304 14:18:59.394376 140380334978816 logging_writer.py:48] [378300] global_step=378300, grad_norm=2.887407064437866, loss=1.0550289154052734
I0304 14:19:44.639390 140380326586112 logging_writer.py:48] [378400] global_step=378400, grad_norm=3.332932710647583, loss=2.3868706226348877
I0304 14:20:29.701229 140380334978816 logging_writer.py:48] [378500] global_step=378500, grad_norm=4.074762344360352, loss=1.134113073348999
I0304 14:21:14.890495 140380326586112 logging_writer.py:48] [378600] global_step=378600, grad_norm=2.820493459701538, loss=1.431838870048523
I0304 14:21:59.968809 140380334978816 logging_writer.py:48] [378700] global_step=378700, grad_norm=3.077120304107666, loss=1.1278822422027588
I0304 14:22:45.136728 140380326586112 logging_writer.py:48] [378800] global_step=378800, grad_norm=3.196211814880371, loss=1.002825379371643
I0304 14:22:48.359428 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:22:59.080040 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:23:26.402090 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:23:28.057666 140575196817216 submission_runner.py:411] Time since start: 182983.66s, 	Step: 378809, 	{'train/accuracy': 0.8865624666213989, 'train/loss': 0.42228034138679504, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 168510.95400238037, 'total_duration': 182983.65863513947, 'accumulated_submission_time': 168510.95400238037, 'accumulated_eval_time': 14427.597375631332, 'accumulated_logging_time': 25.032990217208862}
I0304 14:23:28.130533 140380334978816 logging_writer.py:48] [378809] accumulated_eval_time=14427.597376, accumulated_logging_time=25.032990, accumulated_submission_time=168510.954002, global_step=378809, preemption_count=0, score=168510.954002, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=182983.658635, train/accuracy=0.886562, train/loss=0.422280, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:24:05.037151 140380326586112 logging_writer.py:48] [378900] global_step=378900, grad_norm=3.0471036434173584, loss=1.9039748907089233
I0304 14:24:49.740154 140380334978816 logging_writer.py:48] [379000] global_step=379000, grad_norm=3.356907606124878, loss=2.795393943786621
I0304 14:25:34.900763 140380326586112 logging_writer.py:48] [379100] global_step=379100, grad_norm=3.4213359355926514, loss=1.2071070671081543
I0304 14:26:20.571857 140380334978816 logging_writer.py:48] [379200] global_step=379200, grad_norm=3.22029972076416, loss=1.1726219654083252
I0304 14:27:05.526796 140380326586112 logging_writer.py:48] [379300] global_step=379300, grad_norm=3.526154041290283, loss=3.0594894886016846
I0304 14:27:50.436329 140380334978816 logging_writer.py:48] [379400] global_step=379400, grad_norm=3.415825366973877, loss=1.0437982082366943
I0304 14:28:35.581225 140380326586112 logging_writer.py:48] [379500] global_step=379500, grad_norm=3.3246712684631348, loss=1.1208349466323853
I0304 14:29:20.496101 140380334978816 logging_writer.py:48] [379600] global_step=379600, grad_norm=3.2498936653137207, loss=1.6927318572998047
I0304 14:30:05.561042 140380326586112 logging_writer.py:48] [379700] global_step=379700, grad_norm=3.1853086948394775, loss=1.084057331085205
I0304 14:30:28.111652 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:30:39.298425 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:31:06.845929 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:31:08.504740 140575196817216 submission_runner.py:411] Time since start: 183444.11s, 	Step: 379752, 	{'train/accuracy': 0.89013671875, 'train/loss': 0.41571661829948425, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 168930.87727046013, 'total_duration': 183444.10567331314, 'accumulated_submission_time': 168930.87727046013, 'accumulated_eval_time': 14467.989336252213, 'accumulated_logging_time': 25.115357160568237}
I0304 14:31:08.577404 140380334978816 logging_writer.py:48] [379752] accumulated_eval_time=14467.989336, accumulated_logging_time=25.115357, accumulated_submission_time=168930.877270, global_step=379752, preemption_count=0, score=168930.877270, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=183444.105673, train/accuracy=0.890137, train/loss=0.415717, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:31:28.057186 140380326586112 logging_writer.py:48] [379800] global_step=379800, grad_norm=2.998321056365967, loss=1.9559173583984375
I0304 14:32:10.647718 140380334978816 logging_writer.py:48] [379900] global_step=379900, grad_norm=2.975684642791748, loss=1.0286428928375244
I0304 14:32:55.492367 140380326586112 logging_writer.py:48] [380000] global_step=380000, grad_norm=3.2803304195404053, loss=1.9299309253692627
I0304 14:33:40.834681 140380334978816 logging_writer.py:48] [380100] global_step=380100, grad_norm=2.867572069168091, loss=1.1003987789154053
I0304 14:34:25.800086 140380326586112 logging_writer.py:48] [380200] global_step=380200, grad_norm=3.6267361640930176, loss=2.5577328205108643
I0304 14:35:10.571271 140380334978816 logging_writer.py:48] [380300] global_step=380300, grad_norm=2.8589634895324707, loss=1.1448935270309448
I0304 14:35:55.940242 140380326586112 logging_writer.py:48] [380400] global_step=380400, grad_norm=3.3191189765930176, loss=1.1321805715560913
I0304 14:36:41.238149 140380334978816 logging_writer.py:48] [380500] global_step=380500, grad_norm=3.7287158966064453, loss=3.1742348670959473
I0304 14:37:26.493348 140380326586112 logging_writer.py:48] [380600] global_step=380600, grad_norm=3.4220876693725586, loss=2.4936695098876953
I0304 14:38:08.612162 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:38:19.480958 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:38:44.416785 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:38:46.080686 140575196817216 submission_runner.py:411] Time since start: 183901.68s, 	Step: 380695, 	{'train/accuracy': 0.8870312571525574, 'train/loss': 0.41996413469314575, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 169350.8524608612, 'total_duration': 183901.6815738678, 'accumulated_submission_time': 169350.8524608612, 'accumulated_eval_time': 14505.456683397293, 'accumulated_logging_time': 25.19770884513855}
I0304 14:38:46.163846 140380334978816 logging_writer.py:48] [380695] accumulated_eval_time=14505.456683, accumulated_logging_time=25.197709, accumulated_submission_time=169350.852461, global_step=380695, preemption_count=0, score=169350.852461, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=183901.681574, train/accuracy=0.887031, train/loss=0.419964, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:38:48.554762 140380326586112 logging_writer.py:48] [380700] global_step=380700, grad_norm=3.25288724899292, loss=1.1084043979644775
I0304 14:39:28.941006 140380334978816 logging_writer.py:48] [380800] global_step=380800, grad_norm=3.318817377090454, loss=2.553753137588501
I0304 14:40:13.850396 140380326586112 logging_writer.py:48] [380900] global_step=380900, grad_norm=3.069697141647339, loss=1.0470097064971924
I0304 14:40:59.190697 140380334978816 logging_writer.py:48] [381000] global_step=381000, grad_norm=3.3010473251342773, loss=1.9688986539840698
I0304 14:41:44.130342 140380326586112 logging_writer.py:48] [381100] global_step=381100, grad_norm=3.3088040351867676, loss=2.667255401611328
I0304 14:42:29.286715 140380334978816 logging_writer.py:48] [381200] global_step=381200, grad_norm=3.2255563735961914, loss=1.0506852865219116
I0304 14:43:14.334406 140380326586112 logging_writer.py:48] [381300] global_step=381300, grad_norm=2.982290267944336, loss=1.0934113264083862
I0304 14:43:59.418421 140380334978816 logging_writer.py:48] [381400] global_step=381400, grad_norm=3.22117018699646, loss=1.340358018875122
I0304 14:44:44.428502 140380326586112 logging_writer.py:48] [381500] global_step=381500, grad_norm=3.23901104927063, loss=1.151732325553894
I0304 14:45:29.582248 140380334978816 logging_writer.py:48] [381600] global_step=381600, grad_norm=3.2433457374572754, loss=2.1218771934509277
I0304 14:45:46.443777 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:45:57.173259 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:46:20.491627 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:46:22.144559 140575196817216 submission_runner.py:411] Time since start: 184357.75s, 	Step: 381639, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.4152979850769043, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 169771.07170915604, 'total_duration': 184357.7455892563, 'accumulated_submission_time': 169771.07170915604, 'accumulated_eval_time': 14541.156434774399, 'accumulated_logging_time': 25.292958974838257}
I0304 14:46:22.217047 140380326586112 logging_writer.py:48] [381639] accumulated_eval_time=14541.156435, accumulated_logging_time=25.292959, accumulated_submission_time=169771.071709, global_step=381639, preemption_count=0, score=169771.071709, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=184357.745589, train/accuracy=0.887578, train/loss=0.415298, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:46:46.870425 140380334978816 logging_writer.py:48] [381700] global_step=381700, grad_norm=3.208519697189331, loss=1.1335340738296509
I0304 14:47:30.271479 140380326586112 logging_writer.py:48] [381800] global_step=381800, grad_norm=3.8830528259277344, loss=3.14184308052063
I0304 14:48:15.683203 140380334978816 logging_writer.py:48] [381900] global_step=381900, grad_norm=2.976426124572754, loss=1.1339991092681885
I0304 14:49:00.704532 140380326586112 logging_writer.py:48] [382000] global_step=382000, grad_norm=3.507854461669922, loss=2.0158238410949707
I0304 14:49:45.562668 140380334978816 logging_writer.py:48] [382100] global_step=382100, grad_norm=3.079777479171753, loss=1.058542251586914
I0304 14:50:30.731569 140380326586112 logging_writer.py:48] [382200] global_step=382200, grad_norm=3.6738877296447754, loss=2.200058698654175
I0304 14:51:15.850063 140380334978816 logging_writer.py:48] [382300] global_step=382300, grad_norm=3.059298038482666, loss=2.6143996715545654
I0304 14:52:00.731301 140380326586112 logging_writer.py:48] [382400] global_step=382400, grad_norm=3.766162395477295, loss=1.0153403282165527
I0304 14:52:46.053982 140380334978816 logging_writer.py:48] [382500] global_step=382500, grad_norm=2.861927032470703, loss=2.1383471488952637
I0304 14:53:22.345225 140575196817216 spec.py:321] Evaluating on the training split.
I0304 14:53:33.179675 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 14:54:02.449190 140575196817216 spec.py:349] Evaluating on the test split.
I0304 14:54:04.108914 140575196817216 submission_runner.py:411] Time since start: 184819.71s, 	Step: 382582, 	{'train/accuracy': 0.8846093416213989, 'train/loss': 0.4293917417526245, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 170191.14010548592, 'total_duration': 184819.70988583565, 'accumulated_submission_time': 170191.14010548592, 'accumulated_eval_time': 14582.919032096863, 'accumulated_logging_time': 25.37623953819275}
I0304 14:54:04.197991 140380326586112 logging_writer.py:48] [382582] accumulated_eval_time=14582.919032, accumulated_logging_time=25.376240, accumulated_submission_time=170191.140105, global_step=382582, preemption_count=0, score=170191.140105, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=184819.709886, train/accuracy=0.884609, train/loss=0.429392, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 14:54:11.759770 140380334978816 logging_writer.py:48] [382600] global_step=382600, grad_norm=3.2185137271881104, loss=1.1962687969207764
I0304 14:54:52.999102 140380326586112 logging_writer.py:48] [382700] global_step=382700, grad_norm=2.8743951320648193, loss=2.053856372833252
I0304 14:55:37.463557 140380334978816 logging_writer.py:48] [382800] global_step=382800, grad_norm=3.227874994277954, loss=1.2383219003677368
I0304 14:56:22.939727 140380326586112 logging_writer.py:48] [382900] global_step=382900, grad_norm=3.231126070022583, loss=1.0437637567520142
I0304 14:57:08.206521 140380334978816 logging_writer.py:48] [383000] global_step=383000, grad_norm=3.6925454139709473, loss=1.1045100688934326
I0304 14:57:53.225712 140380326586112 logging_writer.py:48] [383100] global_step=383100, grad_norm=3.431290626525879, loss=1.4578102827072144
I0304 14:58:38.487006 140380334978816 logging_writer.py:48] [383200] global_step=383200, grad_norm=2.8888356685638428, loss=1.8636959791183472
I0304 14:59:23.575366 140380326586112 logging_writer.py:48] [383300] global_step=383300, grad_norm=2.9616429805755615, loss=1.2984732389450073
I0304 15:00:08.627093 140380334978816 logging_writer.py:48] [383400] global_step=383400, grad_norm=3.024129867553711, loss=1.4318002462387085
I0304 15:00:53.580741 140380326586112 logging_writer.py:48] [383500] global_step=383500, grad_norm=3.2044317722320557, loss=2.3229222297668457
I0304 15:01:04.124754 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:01:14.782227 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:01:41.986434 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:01:43.639592 140575196817216 submission_runner.py:411] Time since start: 185279.24s, 	Step: 383525, 	{'train/accuracy': 0.8859961032867432, 'train/loss': 0.4245673716068268, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 170611.00712823868, 'total_duration': 185279.24055552483, 'accumulated_submission_time': 170611.00712823868, 'accumulated_eval_time': 14622.43280339241, 'accumulated_logging_time': 25.47563648223877}
I0304 15:01:43.711308 140380334978816 logging_writer.py:48] [383525] accumulated_eval_time=14622.432803, accumulated_logging_time=25.475636, accumulated_submission_time=170611.007128, global_step=383525, preemption_count=0, score=170611.007128, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=185279.240556, train/accuracy=0.885996, train/loss=0.424567, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:02:13.925252 140380326586112 logging_writer.py:48] [383600] global_step=383600, grad_norm=3.1933794021606445, loss=2.1899337768554688
I0304 15:02:58.146435 140380334978816 logging_writer.py:48] [383700] global_step=383700, grad_norm=2.835728168487549, loss=1.4692294597625732
I0304 15:03:43.193704 140380326586112 logging_writer.py:48] [383800] global_step=383800, grad_norm=3.046180486679077, loss=1.976080060005188
I0304 15:04:28.174999 140380334978816 logging_writer.py:48] [383900] global_step=383900, grad_norm=3.241239309310913, loss=1.0856270790100098
I0304 15:05:13.070891 140380326586112 logging_writer.py:48] [384000] global_step=384000, grad_norm=2.8996145725250244, loss=1.1113841533660889
I0304 15:05:58.499749 140380334978816 logging_writer.py:48] [384100] global_step=384100, grad_norm=3.0094168186187744, loss=1.5459082126617432
I0304 15:06:43.614284 140380326586112 logging_writer.py:48] [384200] global_step=384200, grad_norm=3.76728892326355, loss=2.882469892501831
I0304 15:07:28.607814 140380334978816 logging_writer.py:48] [384300] global_step=384300, grad_norm=3.1088790893554688, loss=1.143439531326294
I0304 15:08:13.708063 140380326586112 logging_writer.py:48] [384400] global_step=384400, grad_norm=3.3685357570648193, loss=1.1001564264297485
I0304 15:08:44.028021 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:08:54.676947 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:09:16.196252 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:09:17.856207 140575196817216 submission_runner.py:411] Time since start: 185733.46s, 	Step: 384469, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.4146858751773834, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 171031.26492524147, 'total_duration': 185733.45716404915, 'accumulated_submission_time': 171031.26492524147, 'accumulated_eval_time': 14656.259887695312, 'accumulated_logging_time': 25.55725598335266}
I0304 15:09:17.942357 140380334978816 logging_writer.py:48] [384469] accumulated_eval_time=14656.259888, accumulated_logging_time=25.557256, accumulated_submission_time=171031.264925, global_step=384469, preemption_count=0, score=171031.264925, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=185733.457164, train/accuracy=0.887207, train/loss=0.414686, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:09:30.676724 140380326586112 logging_writer.py:48] [384500] global_step=384500, grad_norm=3.143294095993042, loss=2.4584550857543945
I0304 15:10:12.511934 140380334978816 logging_writer.py:48] [384600] global_step=384600, grad_norm=3.236638307571411, loss=2.6217756271362305
I0304 15:10:57.962304 140380326586112 logging_writer.py:48] [384700] global_step=384700, grad_norm=3.3302338123321533, loss=1.1807273626327515
I0304 15:11:43.195981 140380334978816 logging_writer.py:48] [384800] global_step=384800, grad_norm=3.1130831241607666, loss=2.647531747817993
I0304 15:12:27.872807 140380326586112 logging_writer.py:48] [384900] global_step=384900, grad_norm=3.1157753467559814, loss=1.054719090461731
I0304 15:13:12.917408 140380334978816 logging_writer.py:48] [385000] global_step=385000, grad_norm=3.100381851196289, loss=2.076796293258667
I0304 15:13:57.758653 140380326586112 logging_writer.py:48] [385100] global_step=385100, grad_norm=3.090693712234497, loss=1.0961567163467407
I0304 15:14:42.737870 140380334978816 logging_writer.py:48] [385200] global_step=385200, grad_norm=3.4971511363983154, loss=2.0964248180389404
I0304 15:15:27.502807 140380326586112 logging_writer.py:48] [385300] global_step=385300, grad_norm=3.2658145427703857, loss=1.1555089950561523
I0304 15:16:12.852595 140380334978816 logging_writer.py:48] [385400] global_step=385400, grad_norm=3.3110101222991943, loss=1.3678005933761597
I0304 15:16:18.027433 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:16:28.637022 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:16:57.604110 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:16:59.256827 140575196817216 submission_runner.py:411] Time since start: 186194.86s, 	Step: 385413, 	{'train/accuracy': 0.8879492282867432, 'train/loss': 0.4205496907234192, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 171451.29096341133, 'total_duration': 186194.85792946815, 'accumulated_submission_time': 171451.29096341133, 'accumulated_eval_time': 14697.488318920135, 'accumulated_logging_time': 25.653950452804565}
I0304 15:16:59.330403 140380326586112 logging_writer.py:48] [385413] accumulated_eval_time=14697.488319, accumulated_logging_time=25.653950, accumulated_submission_time=171451.290963, global_step=385413, preemption_count=0, score=171451.290963, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=186194.857929, train/accuracy=0.887949, train/loss=0.420550, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:17:34.303901 140380334978816 logging_writer.py:48] [385500] global_step=385500, grad_norm=3.0540361404418945, loss=1.6405601501464844
I0304 15:18:19.184862 140380326586112 logging_writer.py:48] [385600] global_step=385600, grad_norm=4.610433101654053, loss=2.9178225994110107
I0304 15:19:04.491296 140380334978816 logging_writer.py:48] [385700] global_step=385700, grad_norm=3.1627094745635986, loss=1.2080703973770142
I0304 15:19:49.643959 140380326586112 logging_writer.py:48] [385800] global_step=385800, grad_norm=3.26825213432312, loss=1.1229158639907837
I0304 15:20:34.663998 140380334978816 logging_writer.py:48] [385900] global_step=385900, grad_norm=2.935974597930908, loss=1.160983920097351
I0304 15:21:19.734110 140380326586112 logging_writer.py:48] [386000] global_step=386000, grad_norm=3.1847281455993652, loss=2.125387668609619
I0304 15:22:04.665966 140380334978816 logging_writer.py:48] [386100] global_step=386100, grad_norm=2.9244751930236816, loss=1.1761536598205566
I0304 15:22:49.683204 140380326586112 logging_writer.py:48] [386200] global_step=386200, grad_norm=3.1487560272216797, loss=1.060675024986267
I0304 15:23:34.915518 140380334978816 logging_writer.py:48] [386300] global_step=386300, grad_norm=2.9507246017456055, loss=1.770204782485962
I0304 15:23:59.312841 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:24:10.189811 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:24:36.203519 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:24:37.858910 140575196817216 submission_runner.py:411] Time since start: 186653.46s, 	Step: 386356, 	{'train/accuracy': 0.887011706829071, 'train/loss': 0.42353522777557373, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 171871.21469521523, 'total_duration': 186653.45989465714, 'accumulated_submission_time': 171871.21469521523, 'accumulated_eval_time': 14736.033299446106, 'accumulated_logging_time': 25.737998723983765}
I0304 15:24:37.933044 140380326586112 logging_writer.py:48] [386356] accumulated_eval_time=14736.033299, accumulated_logging_time=25.737999, accumulated_submission_time=171871.214695, global_step=386356, preemption_count=0, score=171871.214695, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=186653.459895, train/accuracy=0.887012, train/loss=0.423535, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:24:55.826458 140380334978816 logging_writer.py:48] [386400] global_step=386400, grad_norm=3.254504442214966, loss=1.1800050735473633
I0304 15:25:38.405711 140380326586112 logging_writer.py:48] [386500] global_step=386500, grad_norm=3.297929525375366, loss=1.5224653482437134
I0304 15:26:23.403445 140380334978816 logging_writer.py:48] [386600] global_step=386600, grad_norm=3.223285436630249, loss=1.3032416105270386
I0304 15:27:08.729580 140380326586112 logging_writer.py:48] [386700] global_step=386700, grad_norm=3.3870463371276855, loss=1.4935897588729858
I0304 15:27:53.669423 140380334978816 logging_writer.py:48] [386800] global_step=386800, grad_norm=2.924520254135132, loss=1.313704013824463
I0304 15:28:39.124571 140380326586112 logging_writer.py:48] [386900] global_step=386900, grad_norm=2.982572555541992, loss=1.0576688051223755
I0304 15:29:24.187413 140380334978816 logging_writer.py:48] [387000] global_step=387000, grad_norm=3.216611623764038, loss=1.1434942483901978
I0304 15:30:09.096990 140380326586112 logging_writer.py:48] [387100] global_step=387100, grad_norm=3.066222667694092, loss=1.0937646627426147
I0304 15:30:54.198647 140380334978816 logging_writer.py:48] [387200] global_step=387200, grad_norm=3.14176869392395, loss=1.0895532369613647
I0304 15:31:38.216452 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:31:49.069584 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:32:15.762499 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:32:17.448578 140575196817216 submission_runner.py:411] Time since start: 187113.05s, 	Step: 387300, 	{'train/accuracy': 0.8868163824081421, 'train/loss': 0.41732481122016907, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 172291.44046711922, 'total_duration': 187113.0497689247, 'accumulated_submission_time': 172291.44046711922, 'accumulated_eval_time': 14775.26454615593, 'accumulated_logging_time': 25.821584701538086}
I0304 15:32:17.523178 140380326586112 logging_writer.py:48] [387300] accumulated_eval_time=14775.264546, accumulated_logging_time=25.821585, accumulated_submission_time=172291.440467, global_step=387300, preemption_count=0, score=172291.440467, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=187113.049769, train/accuracy=0.886816, train/loss=0.417325, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:32:17.930050 140380334978816 logging_writer.py:48] [387300] global_step=387300, grad_norm=3.7045037746429443, loss=2.90586256980896
I0304 15:32:58.481825 140380326586112 logging_writer.py:48] [387400] global_step=387400, grad_norm=4.14068603515625, loss=3.066527843475342
I0304 15:33:43.271972 140380334978816 logging_writer.py:48] [387500] global_step=387500, grad_norm=3.239621877670288, loss=2.336953639984131
I0304 15:34:28.671936 140380326586112 logging_writer.py:48] [387600] global_step=387600, grad_norm=3.0358009338378906, loss=1.4795117378234863
I0304 15:35:13.728171 140380334978816 logging_writer.py:48] [387700] global_step=387700, grad_norm=3.7814228534698486, loss=3.2692646980285645
I0304 15:35:58.343314 140380326586112 logging_writer.py:48] [387800] global_step=387800, grad_norm=3.101304292678833, loss=1.914722204208374
I0304 15:36:43.673409 140380334978816 logging_writer.py:48] [387900] global_step=387900, grad_norm=3.3402013778686523, loss=2.8663558959960938
I0304 15:37:28.623645 140380326586112 logging_writer.py:48] [388000] global_step=388000, grad_norm=4.6634111404418945, loss=1.125497579574585
I0304 15:38:13.615699 140380334978816 logging_writer.py:48] [388100] global_step=388100, grad_norm=3.5972869396209717, loss=1.1694552898406982
I0304 15:38:58.572439 140380326586112 logging_writer.py:48] [388200] global_step=388200, grad_norm=3.007627248764038, loss=1.6278270483016968
I0304 15:39:17.477401 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:39:28.214774 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:39:55.019018 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:39:56.676310 140575196817216 submission_runner.py:411] Time since start: 187572.28s, 	Step: 388244, 	{'train/accuracy': 0.8858398199081421, 'train/loss': 0.4219137728214264, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 172711.33534526825, 'total_duration': 187572.27738046646, 'accumulated_submission_time': 172711.33534526825, 'accumulated_eval_time': 14814.46245265007, 'accumulated_logging_time': 25.906278610229492}
I0304 15:39:56.754014 140380334978816 logging_writer.py:48] [388244] accumulated_eval_time=14814.462453, accumulated_logging_time=25.906279, accumulated_submission_time=172711.335345, global_step=388244, preemption_count=0, score=172711.335345, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=187572.277380, train/accuracy=0.885840, train/loss=0.421914, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:40:19.406673 140380326586112 logging_writer.py:48] [388300] global_step=388300, grad_norm=3.250751256942749, loss=1.4460629224777222
I0304 15:41:02.692226 140380334978816 logging_writer.py:48] [388400] global_step=388400, grad_norm=3.3097023963928223, loss=2.336043357849121
I0304 15:41:48.169831 140380326586112 logging_writer.py:48] [388500] global_step=388500, grad_norm=5.1815595626831055, loss=3.150343418121338
I0304 15:42:33.585811 140380334978816 logging_writer.py:48] [388600] global_step=388600, grad_norm=3.044215202331543, loss=1.2256600856781006
I0304 15:43:18.492050 140380326586112 logging_writer.py:48] [388700] global_step=388700, grad_norm=3.1959023475646973, loss=1.1798847913742065
I0304 15:44:03.665394 140380334978816 logging_writer.py:48] [388800] global_step=388800, grad_norm=3.057823896408081, loss=1.3161481618881226
I0304 15:44:48.776478 140380326586112 logging_writer.py:48] [388900] global_step=388900, grad_norm=3.1423916816711426, loss=1.4303218126296997
I0304 15:45:33.948211 140380334978816 logging_writer.py:48] [389000] global_step=389000, grad_norm=4.170454502105713, loss=2.9921085834503174
I0304 15:46:19.683851 140380326586112 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.486984729766846, loss=3.321270704269409
I0304 15:46:56.844812 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:47:07.691641 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:47:33.746516 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:47:35.413805 140575196817216 submission_runner.py:411] Time since start: 188031.01s, 	Step: 389184, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.4197618365287781, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 173131.36773204803, 'total_duration': 188031.01471281052, 'accumulated_submission_time': 173131.36773204803, 'accumulated_eval_time': 14853.030284166336, 'accumulated_logging_time': 25.993929386138916}
I0304 15:47:35.499822 140380334978816 logging_writer.py:48] [389184] accumulated_eval_time=14853.030284, accumulated_logging_time=25.993929, accumulated_submission_time=173131.367732, global_step=389184, preemption_count=0, score=173131.367732, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=188031.014713, train/accuracy=0.887852, train/loss=0.419762, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:47:42.267415 140380326586112 logging_writer.py:48] [389200] global_step=389200, grad_norm=3.232381820678711, loss=0.963084876537323
I0304 15:48:23.503434 140380334978816 logging_writer.py:48] [389300] global_step=389300, grad_norm=3.402142286300659, loss=2.1590280532836914
I0304 15:49:08.308938 140380326586112 logging_writer.py:48] [389400] global_step=389400, grad_norm=3.1908929347991943, loss=1.328137755393982
I0304 15:49:53.446756 140380334978816 logging_writer.py:48] [389500] global_step=389500, grad_norm=2.9863698482513428, loss=2.0532994270324707
I0304 15:50:38.353034 140380326586112 logging_writer.py:48] [389600] global_step=389600, grad_norm=3.0558629035949707, loss=1.1449334621429443
I0304 15:51:23.507647 140380334978816 logging_writer.py:48] [389700] global_step=389700, grad_norm=3.0901732444763184, loss=1.13187575340271
I0304 15:52:08.142073 140380326586112 logging_writer.py:48] [389800] global_step=389800, grad_norm=2.886129140853882, loss=1.423954963684082
I0304 15:52:53.076241 140380334978816 logging_writer.py:48] [389900] global_step=389900, grad_norm=3.47060489654541, loss=1.1479953527450562
I0304 15:53:38.067176 140380326586112 logging_writer.py:48] [390000] global_step=390000, grad_norm=3.887812852859497, loss=3.201007127761841
I0304 15:54:23.058186 140380334978816 logging_writer.py:48] [390100] global_step=390100, grad_norm=3.0324554443359375, loss=1.4024096727371216
I0304 15:54:35.817585 140575196817216 spec.py:321] Evaluating on the training split.
I0304 15:54:46.572587 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 15:55:11.008814 140575196817216 spec.py:349] Evaluating on the test split.
I0304 15:55:12.669374 140575196817216 submission_runner.py:411] Time since start: 188488.27s, 	Step: 390130, 	{'train/accuracy': 0.8890624642372131, 'train/loss': 0.42126619815826416, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 173551.6240181923, 'total_duration': 188488.27039361, 'accumulated_submission_time': 173551.6240181923, 'accumulated_eval_time': 14889.88103055954, 'accumulated_logging_time': 26.092432737350464}
I0304 15:55:12.744398 140380326586112 logging_writer.py:48] [390130] accumulated_eval_time=14889.881031, accumulated_logging_time=26.092433, accumulated_submission_time=173551.624018, global_step=390130, preemption_count=0, score=173551.624018, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=188488.270394, train/accuracy=0.889062, train/loss=0.421266, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 15:55:40.968828 140380334978816 logging_writer.py:48] [390200] global_step=390200, grad_norm=3.9046902656555176, loss=2.1619956493377686
I0304 15:56:24.790164 140380326586112 logging_writer.py:48] [390300] global_step=390300, grad_norm=2.9800987243652344, loss=2.0715715885162354
I0304 15:57:09.922031 140380334978816 logging_writer.py:48] [390400] global_step=390400, grad_norm=3.212761640548706, loss=1.5992531776428223
I0304 15:57:55.046844 140380326586112 logging_writer.py:48] [390500] global_step=390500, grad_norm=2.94291353225708, loss=1.6344372034072876
I0304 15:58:39.891236 140380334978816 logging_writer.py:48] [390600] global_step=390600, grad_norm=3.6387410163879395, loss=2.41452693939209
I0304 15:59:24.979910 140380326586112 logging_writer.py:48] [390700] global_step=390700, grad_norm=4.092118740081787, loss=3.192746639251709
I0304 16:00:10.235578 140380334978816 logging_writer.py:48] [390800] global_step=390800, grad_norm=3.1767101287841797, loss=1.5674307346343994
I0304 16:00:55.403483 140380326586112 logging_writer.py:48] [390900] global_step=390900, grad_norm=3.0682308673858643, loss=1.2510863542556763
I0304 16:01:40.663483 140380334978816 logging_writer.py:48] [391000] global_step=391000, grad_norm=3.382625102996826, loss=2.706385850906372
I0304 16:02:12.682212 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:02:23.286121 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:02:48.376603 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:02:50.028839 140575196817216 submission_runner.py:411] Time since start: 188945.63s, 	Step: 391073, 	{'train/accuracy': 0.88525390625, 'train/loss': 0.4267793893814087, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 173971.50109505653, 'total_duration': 188945.62973308563, 'accumulated_submission_time': 173971.50109505653, 'accumulated_eval_time': 14927.226495981216, 'accumulated_logging_time': 26.179418087005615}
I0304 16:02:50.113857 140380326586112 logging_writer.py:48] [391073] accumulated_eval_time=14927.226496, accumulated_logging_time=26.179418, accumulated_submission_time=173971.501095, global_step=391073, preemption_count=0, score=173971.501095, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=188945.629733, train/accuracy=0.885254, train/loss=0.426779, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:03:01.260323 140380334978816 logging_writer.py:48] [391100] global_step=391100, grad_norm=3.0402798652648926, loss=2.1360437870025635
I0304 16:03:42.797362 140380326586112 logging_writer.py:48] [391200] global_step=391200, grad_norm=3.07021164894104, loss=1.4918100833892822
I0304 16:04:27.773392 140380334978816 logging_writer.py:48] [391300] global_step=391300, grad_norm=2.947272777557373, loss=1.3234165906906128
I0304 16:05:13.097870 140380326586112 logging_writer.py:48] [391400] global_step=391400, grad_norm=2.7748727798461914, loss=2.060328483581543
I0304 16:05:57.790462 140380334978816 logging_writer.py:48] [391500] global_step=391500, grad_norm=2.9401895999908447, loss=1.6195236444473267
I0304 16:06:43.081269 140380326586112 logging_writer.py:48] [391600] global_step=391600, grad_norm=3.077489137649536, loss=1.7796502113342285
I0304 16:07:28.191329 140380334978816 logging_writer.py:48] [391700] global_step=391700, grad_norm=3.7471954822540283, loss=1.1417409181594849
I0304 16:08:13.140625 140380326586112 logging_writer.py:48] [391800] global_step=391800, grad_norm=3.842766284942627, loss=1.220937967300415
I0304 16:08:58.128000 140380334978816 logging_writer.py:48] [391900] global_step=391900, grad_norm=3.177903890609741, loss=1.3343801498413086
I0304 16:09:43.335244 140380326586112 logging_writer.py:48] [392000] global_step=392000, grad_norm=2.9762589931488037, loss=1.2769818305969238
I0304 16:09:50.227289 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:10:00.989007 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:10:27.634058 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:10:29.278427 140575196817216 submission_runner.py:411] Time since start: 189404.88s, 	Step: 392017, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.4180474281311035, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 174391.5544939041, 'total_duration': 189404.8793940544, 'accumulated_submission_time': 174391.5544939041, 'accumulated_eval_time': 14966.276545286179, 'accumulated_logging_time': 26.275421142578125}
I0304 16:10:29.352791 140380334978816 logging_writer.py:48] [392017] accumulated_eval_time=14966.276545, accumulated_logging_time=26.275421, accumulated_submission_time=174391.554494, global_step=392017, preemption_count=0, score=174391.554494, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=189404.879394, train/accuracy=0.887207, train/loss=0.418047, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:11:02.744024 140380326586112 logging_writer.py:48] [392100] global_step=392100, grad_norm=3.184983730316162, loss=1.1872341632843018
I0304 16:11:47.187588 140380334978816 logging_writer.py:48] [392200] global_step=392200, grad_norm=3.211557626724243, loss=1.1497247219085693
I0304 16:12:32.445171 140380326586112 logging_writer.py:48] [392300] global_step=392300, grad_norm=3.1851119995117188, loss=1.6947358846664429
I0304 16:13:17.632439 140380334978816 logging_writer.py:48] [392400] global_step=392400, grad_norm=3.63035249710083, loss=3.098379373550415
I0304 16:14:02.486002 140380326586112 logging_writer.py:48] [392500] global_step=392500, grad_norm=3.313603639602661, loss=1.2681217193603516
I0304 16:14:47.527826 140380334978816 logging_writer.py:48] [392600] global_step=392600, grad_norm=3.215291738510132, loss=1.1263453960418701
I0304 16:15:32.457177 140380326586112 logging_writer.py:48] [392700] global_step=392700, grad_norm=3.0357704162597656, loss=1.7809998989105225
I0304 16:16:17.410025 140380334978816 logging_writer.py:48] [392800] global_step=392800, grad_norm=3.834681272506714, loss=3.0583696365356445
I0304 16:17:02.525942 140380326586112 logging_writer.py:48] [392900] global_step=392900, grad_norm=2.9708001613616943, loss=1.2790987491607666
I0304 16:17:29.591189 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:17:40.319761 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:18:09.952384 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:18:11.606887 140575196817216 submission_runner.py:411] Time since start: 189867.21s, 	Step: 392962, 	{'train/accuracy': 0.8881640434265137, 'train/loss': 0.4164069592952728, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 174811.7327439785, 'total_duration': 189867.20791006088, 'accumulated_submission_time': 174811.7327439785, 'accumulated_eval_time': 15008.291206359863, 'accumulated_logging_time': 26.361035346984863}
I0304 16:18:11.698393 140380334978816 logging_writer.py:48] [392962] accumulated_eval_time=15008.291206, accumulated_logging_time=26.361035, accumulated_submission_time=174811.732744, global_step=392962, preemption_count=0, score=174811.732744, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=189867.207910, train/accuracy=0.888164, train/loss=0.416407, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:18:27.200961 140380326586112 logging_writer.py:48] [393000] global_step=393000, grad_norm=4.072858810424805, loss=3.1568336486816406
I0304 16:19:09.578588 140380334978816 logging_writer.py:48] [393100] global_step=393100, grad_norm=2.789592742919922, loss=1.1689268350601196
I0304 16:19:54.829072 140380326586112 logging_writer.py:48] [393200] global_step=393200, grad_norm=3.061049699783325, loss=2.0868828296661377
I0304 16:20:39.971011 140380334978816 logging_writer.py:48] [393300] global_step=393300, grad_norm=3.1499392986297607, loss=1.260902762413025
I0304 16:21:25.005091 140380326586112 logging_writer.py:48] [393400] global_step=393400, grad_norm=3.9527623653411865, loss=3.2439377307891846
I0304 16:22:09.958285 140380334978816 logging_writer.py:48] [393500] global_step=393500, grad_norm=3.0790185928344727, loss=1.6552269458770752
I0304 16:22:55.013333 140380326586112 logging_writer.py:48] [393600] global_step=393600, grad_norm=3.2523622512817383, loss=2.5883591175079346
I0304 16:23:40.014852 140380334978816 logging_writer.py:48] [393700] global_step=393700, grad_norm=3.112046241760254, loss=1.2472726106643677
I0304 16:24:25.055240 140380326586112 logging_writer.py:48] [393800] global_step=393800, grad_norm=3.294731855392456, loss=2.797930955886841
I0304 16:25:10.071178 140380334978816 logging_writer.py:48] [393900] global_step=393900, grad_norm=3.0729873180389404, loss=1.205819010734558
I0304 16:25:12.001682 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:25:22.701522 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:25:51.054437 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:25:52.718538 140575196817216 submission_runner.py:411] Time since start: 190328.32s, 	Step: 393906, 	{'train/accuracy': 0.88623046875, 'train/loss': 0.4230261445045471, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 175231.97394490242, 'total_duration': 190328.31834244728, 'accumulated_submission_time': 175231.97394490242, 'accumulated_eval_time': 15049.005800247192, 'accumulated_logging_time': 26.46606206893921}
I0304 16:25:52.805968 140380326586112 logging_writer.py:48] [393906] accumulated_eval_time=15049.005800, accumulated_logging_time=26.466062, accumulated_submission_time=175231.973945, global_step=393906, preemption_count=0, score=175231.973945, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=190328.318342, train/accuracy=0.886230, train/loss=0.423026, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:26:30.989145 140380334978816 logging_writer.py:48] [394000] global_step=394000, grad_norm=4.276260852813721, loss=3.173436164855957
I0304 16:27:15.935120 140380326586112 logging_writer.py:48] [394100] global_step=394100, grad_norm=3.056044101715088, loss=1.516811728477478
I0304 16:28:00.796882 140380334978816 logging_writer.py:48] [394200] global_step=394200, grad_norm=3.1550140380859375, loss=1.0942420959472656
I0304 16:28:46.024920 140380326586112 logging_writer.py:48] [394300] global_step=394300, grad_norm=3.1025564670562744, loss=1.1269094944000244
I0304 16:29:30.916766 140380334978816 logging_writer.py:48] [394400] global_step=394400, grad_norm=2.965989112854004, loss=1.1988048553466797
I0304 16:30:16.035174 140380326586112 logging_writer.py:48] [394500] global_step=394500, grad_norm=2.9425244331359863, loss=2.2632932662963867
I0304 16:31:01.181130 140380334978816 logging_writer.py:48] [394600] global_step=394600, grad_norm=3.7313737869262695, loss=3.0079801082611084
I0304 16:31:45.968513 140380326586112 logging_writer.py:48] [394700] global_step=394700, grad_norm=3.1891348361968994, loss=2.392885208129883
I0304 16:32:30.910208 140380334978816 logging_writer.py:48] [394800] global_step=394800, grad_norm=3.1034796237945557, loss=1.1881499290466309
I0304 16:32:53.051434 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:33:04.039262 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:33:31.573544 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:33:33.220978 140575196817216 submission_runner.py:411] Time since start: 190788.82s, 	Step: 394851, 	{'train/accuracy': 0.8873632550239563, 'train/loss': 0.4190618693828583, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 175652.15966057777, 'total_duration': 190788.822149992, 'accumulated_submission_time': 175652.15966057777, 'accumulated_eval_time': 15089.174458265305, 'accumulated_logging_time': 26.564687252044678}
I0304 16:33:33.294830 140380326586112 logging_writer.py:48] [394851] accumulated_eval_time=15089.174458, accumulated_logging_time=26.564687, accumulated_submission_time=175652.159661, global_step=394851, preemption_count=0, score=175652.159661, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=190788.822150, train/accuracy=0.887363, train/loss=0.419062, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:33:53.176505 140380334978816 logging_writer.py:48] [394900] global_step=394900, grad_norm=3.497009038925171, loss=2.189837694168091
I0304 16:34:35.867461 140380326586112 logging_writer.py:48] [395000] global_step=395000, grad_norm=3.0669097900390625, loss=1.7530877590179443
I0304 16:35:21.177226 140380334978816 logging_writer.py:48] [395100] global_step=395100, grad_norm=3.228163480758667, loss=1.2083593606948853
I0304 16:36:06.553918 140380326586112 logging_writer.py:48] [395200] global_step=395200, grad_norm=2.9923882484436035, loss=1.0626040697097778
I0304 16:36:51.288167 140380334978816 logging_writer.py:48] [395300] global_step=395300, grad_norm=4.368005752563477, loss=1.4135854244232178
I0304 16:37:36.500804 140380326586112 logging_writer.py:48] [395400] global_step=395400, grad_norm=3.0585033893585205, loss=1.1542938947677612
I0304 16:38:21.755331 140380334978816 logging_writer.py:48] [395500] global_step=395500, grad_norm=3.174948215484619, loss=1.0760689973831177
I0304 16:39:06.663779 140380326586112 logging_writer.py:48] [395600] global_step=395600, grad_norm=3.2949962615966797, loss=2.9515609741210938
I0304 16:39:51.896606 140380334978816 logging_writer.py:48] [395700] global_step=395700, grad_norm=3.7049505710601807, loss=1.2500436305999756
I0304 16:40:33.569132 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:40:44.443325 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:41:11.855041 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:41:13.505772 140575196817216 submission_runner.py:411] Time since start: 191249.11s, 	Step: 395794, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.41606977581977844, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 176072.37421774864, 'total_duration': 191249.1066968441, 'accumulated_submission_time': 176072.37421774864, 'accumulated_eval_time': 15129.109971046448, 'accumulated_logging_time': 26.64938521385193}
I0304 16:41:13.592869 140380326586112 logging_writer.py:48] [395794] accumulated_eval_time=15129.109971, accumulated_logging_time=26.649385, accumulated_submission_time=176072.374218, global_step=395794, preemption_count=0, score=176072.374218, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=191249.106697, train/accuracy=0.888379, train/loss=0.416070, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:41:16.375258 140380334978816 logging_writer.py:48] [395800] global_step=395800, grad_norm=3.1013827323913574, loss=1.047634482383728
I0304 16:41:56.956232 140380326586112 logging_writer.py:48] [395900] global_step=395900, grad_norm=3.0820648670196533, loss=1.8275214433670044
I0304 16:42:41.958687 140380334978816 logging_writer.py:48] [396000] global_step=396000, grad_norm=2.9032528400421143, loss=1.3679779767990112
I0304 16:43:27.209196 140380326586112 logging_writer.py:48] [396100] global_step=396100, grad_norm=3.342580556869507, loss=2.534450054168701
I0304 16:44:11.918737 140380334978816 logging_writer.py:48] [396200] global_step=396200, grad_norm=3.0523014068603516, loss=2.1812644004821777
I0304 16:44:56.979833 140380326586112 logging_writer.py:48] [396300] global_step=396300, grad_norm=2.923532247543335, loss=1.9894829988479614
I0304 16:45:42.241530 140380334978816 logging_writer.py:48] [396400] global_step=396400, grad_norm=2.799355983734131, loss=1.3961503505706787
I0304 16:46:27.442986 140380326586112 logging_writer.py:48] [396500] global_step=396500, grad_norm=3.2084689140319824, loss=1.1540813446044922
I0304 16:47:12.744144 140380334978816 logging_writer.py:48] [396600] global_step=396600, grad_norm=2.982142925262451, loss=1.566066026687622
I0304 16:47:57.558867 140380326586112 logging_writer.py:48] [396700] global_step=396700, grad_norm=3.99214243888855, loss=3.377791404724121
I0304 16:48:13.919442 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:48:24.811309 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:48:51.029670 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:48:52.683565 140575196817216 submission_runner.py:411] Time since start: 191708.28s, 	Step: 396738, 	{'train/accuracy': 0.8874413967132568, 'train/loss': 0.41682761907577515, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 176492.64163136482, 'total_duration': 191708.28460383415, 'accumulated_submission_time': 176492.64163136482, 'accumulated_eval_time': 15167.873075723648, 'accumulated_logging_time': 26.746363401412964}
I0304 16:48:52.757969 140380334978816 logging_writer.py:48] [396738] accumulated_eval_time=15167.873076, accumulated_logging_time=26.746363, accumulated_submission_time=176492.641631, global_step=396738, preemption_count=0, score=176492.641631, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=191708.284604, train/accuracy=0.887441, train/loss=0.416828, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:49:17.799965 140380326586112 logging_writer.py:48] [396800] global_step=396800, grad_norm=3.887025833129883, loss=3.0265307426452637
I0304 16:50:01.139606 140380334978816 logging_writer.py:48] [396900] global_step=396900, grad_norm=3.123016834259033, loss=2.0020949840545654
I0304 16:50:46.296383 140380326586112 logging_writer.py:48] [397000] global_step=397000, grad_norm=2.961463451385498, loss=1.1120643615722656
I0304 16:51:31.669490 140380334978816 logging_writer.py:48] [397100] global_step=397100, grad_norm=3.006939172744751, loss=1.0431407690048218
I0304 16:52:16.645068 140380326586112 logging_writer.py:48] [397200] global_step=397200, grad_norm=3.056351900100708, loss=2.154383420944214
I0304 16:53:01.579726 140380334978816 logging_writer.py:48] [397300] global_step=397300, grad_norm=3.0563511848449707, loss=1.0960643291473389
I0304 16:53:46.427620 140380326586112 logging_writer.py:48] [397400] global_step=397400, grad_norm=3.7470948696136475, loss=3.008251667022705
I0304 16:54:31.256792 140380334978816 logging_writer.py:48] [397500] global_step=397500, grad_norm=3.022346019744873, loss=1.204775333404541
I0304 16:55:16.509545 140380326586112 logging_writer.py:48] [397600] global_step=397600, grad_norm=2.924896717071533, loss=1.1372979879379272
I0304 16:55:53.063567 140575196817216 spec.py:321] Evaluating on the training split.
I0304 16:56:03.852410 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 16:56:31.475360 140575196817216 spec.py:349] Evaluating on the test split.
I0304 16:56:33.137341 140575196817216 submission_runner.py:411] Time since start: 192168.74s, 	Step: 397683, 	{'train/accuracy': 0.8902539014816284, 'train/loss': 0.4140291213989258, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 176912.88610959053, 'total_duration': 192168.73845505714, 'accumulated_submission_time': 176912.88610959053, 'accumulated_eval_time': 15207.94590306282, 'accumulated_logging_time': 26.832626581192017}
I0304 16:56:33.236802 140380334978816 logging_writer.py:48] [397683] accumulated_eval_time=15207.945903, accumulated_logging_time=26.832627, accumulated_submission_time=176912.886110, global_step=397683, preemption_count=0, score=176912.886110, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=192168.738455, train/accuracy=0.890254, train/loss=0.414029, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 16:56:40.399077 140380326586112 logging_writer.py:48] [397700] global_step=397700, grad_norm=3.1792612075805664, loss=2.083979606628418
I0304 16:57:21.554849 140380334978816 logging_writer.py:48] [397800] global_step=397800, grad_norm=3.0788519382476807, loss=1.197920322418213
I0304 16:58:06.556706 140380326586112 logging_writer.py:48] [397900] global_step=397900, grad_norm=2.94757080078125, loss=1.0499296188354492
I0304 16:58:51.682513 140380334978816 logging_writer.py:48] [398000] global_step=398000, grad_norm=3.251774787902832, loss=1.0807816982269287
I0304 16:59:36.694870 140380326586112 logging_writer.py:48] [398100] global_step=398100, grad_norm=3.462918281555176, loss=1.2379677295684814
I0304 17:00:21.650710 140380334978816 logging_writer.py:48] [398200] global_step=398200, grad_norm=3.1630797386169434, loss=1.1486302614212036
I0304 17:01:06.811476 140380326586112 logging_writer.py:48] [398300] global_step=398300, grad_norm=3.085301637649536, loss=1.1604260206222534
I0304 17:01:51.684377 140380334978816 logging_writer.py:48] [398400] global_step=398400, grad_norm=2.9856202602386475, loss=1.1994847059249878
I0304 17:02:36.532253 140380326586112 logging_writer.py:48] [398500] global_step=398500, grad_norm=3.044743061065674, loss=1.091050148010254
I0304 17:03:21.549274 140380334978816 logging_writer.py:48] [398600] global_step=398600, grad_norm=3.374549627304077, loss=1.7373247146606445
I0304 17:03:33.345094 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:03:44.108625 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:04:12.632275 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:04:14.290428 140575196817216 submission_runner.py:411] Time since start: 192629.89s, 	Step: 398628, 	{'train/accuracy': 0.88818359375, 'train/loss': 0.4120750427246094, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 177332.93433642387, 'total_duration': 192629.89152240753, 'accumulated_submission_time': 177332.93433642387, 'accumulated_eval_time': 15248.890265226364, 'accumulated_logging_time': 26.943554639816284}
I0304 17:04:14.369689 140380326586112 logging_writer.py:48] [398628] accumulated_eval_time=15248.890265, accumulated_logging_time=26.943555, accumulated_submission_time=177332.934336, global_step=398628, preemption_count=0, score=177332.934336, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=192629.891522, train/accuracy=0.888184, train/loss=0.412075, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:04:43.404670 140380334978816 logging_writer.py:48] [398700] global_step=398700, grad_norm=3.263739824295044, loss=1.2446800470352173
I0304 17:05:27.342804 140380326586112 logging_writer.py:48] [398800] global_step=398800, grad_norm=3.974473714828491, loss=3.373744010925293
I0304 17:06:12.743066 140380334978816 logging_writer.py:48] [398900] global_step=398900, grad_norm=3.2494168281555176, loss=2.6645376682281494
I0304 17:06:58.140038 140380326586112 logging_writer.py:48] [399000] global_step=399000, grad_norm=2.9530389308929443, loss=1.137727975845337
I0304 17:07:43.130378 140380334978816 logging_writer.py:48] [399100] global_step=399100, grad_norm=3.1145479679107666, loss=2.490936517715454
I0304 17:08:28.245423 140380326586112 logging_writer.py:48] [399200] global_step=399200, grad_norm=3.356992721557617, loss=2.2480626106262207
I0304 17:09:13.193721 140380334978816 logging_writer.py:48] [399300] global_step=399300, grad_norm=3.6083011627197266, loss=3.2713937759399414
I0304 17:09:58.342336 140380326586112 logging_writer.py:48] [399400] global_step=399400, grad_norm=3.7854702472686768, loss=2.582395315170288
I0304 17:10:43.616820 140380334978816 logging_writer.py:48] [399500] global_step=399500, grad_norm=3.713381052017212, loss=2.983680009841919
I0304 17:11:14.454092 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:11:25.131850 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:11:52.841106 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:11:54.501497 140575196817216 submission_runner.py:411] Time since start: 193090.10s, 	Step: 399570, 	{'train/accuracy': 0.8872656226158142, 'train/loss': 0.41962558031082153, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 177752.96119642258, 'total_duration': 193090.10246396065, 'accumulated_submission_time': 177752.96119642258, 'accumulated_eval_time': 15288.936572790146, 'accumulated_logging_time': 27.032135725021362}
I0304 17:11:54.577769 140380326586112 logging_writer.py:48] [399570] accumulated_eval_time=15288.936573, accumulated_logging_time=27.032136, accumulated_submission_time=177752.961196, global_step=399570, preemption_count=0, score=177752.961196, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=193090.102464, train/accuracy=0.887266, train/loss=0.419626, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:12:06.921692 140380334978816 logging_writer.py:48] [399600] global_step=399600, grad_norm=3.182011365890503, loss=2.55808424949646
I0304 17:12:48.514617 140380326586112 logging_writer.py:48] [399700] global_step=399700, grad_norm=3.5996475219726562, loss=2.887233257293701
I0304 17:13:33.919032 140380334978816 logging_writer.py:48] [399800] global_step=399800, grad_norm=3.05907940864563, loss=1.9192347526550293
I0304 17:14:19.271601 140380326586112 logging_writer.py:48] [399900] global_step=399900, grad_norm=4.887520790100098, loss=3.20027494430542
I0304 17:15:04.309976 140380334978816 logging_writer.py:48] [400000] global_step=400000, grad_norm=3.030797004699707, loss=1.1243854761123657
I0304 17:15:49.526860 140380326586112 logging_writer.py:48] [400100] global_step=400100, grad_norm=3.348482370376587, loss=1.131946086883545
I0304 17:16:34.776007 140380334978816 logging_writer.py:48] [400200] global_step=400200, grad_norm=2.9622981548309326, loss=1.18996262550354
I0304 17:17:19.989278 140380326586112 logging_writer.py:48] [400300] global_step=400300, grad_norm=3.8565149307250977, loss=3.2014691829681396
I0304 17:18:05.140281 140380334978816 logging_writer.py:48] [400400] global_step=400400, grad_norm=4.0434441566467285, loss=1.1329832077026367
I0304 17:18:50.140948 140380326586112 logging_writer.py:48] [400500] global_step=400500, grad_norm=3.2516517639160156, loss=2.680413007736206
I0304 17:18:54.855991 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:19:06.458232 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:19:31.843757 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:19:33.496037 140575196817216 submission_runner.py:411] Time since start: 193549.10s, 	Step: 400512, 	{'train/accuracy': 0.8886913657188416, 'train/loss': 0.41380205750465393, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 178173.17875623703, 'total_duration': 193549.0971210003, 'accumulated_submission_time': 178173.17875623703, 'accumulated_eval_time': 15327.57566690445, 'accumulated_logging_time': 27.120450258255005}
I0304 17:19:33.571196 140380334978816 logging_writer.py:48] [400512] accumulated_eval_time=15327.575667, accumulated_logging_time=27.120450, accumulated_submission_time=178173.178756, global_step=400512, preemption_count=0, score=178173.178756, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=193549.097121, train/accuracy=0.888691, train/loss=0.413802, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:20:08.943653 140380326586112 logging_writer.py:48] [400600] global_step=400600, grad_norm=2.8200762271881104, loss=1.579946756362915
I0304 17:20:53.431849 140380334978816 logging_writer.py:48] [400700] global_step=400700, grad_norm=3.768629789352417, loss=3.1614861488342285
I0304 17:21:38.836488 140380326586112 logging_writer.py:48] [400800] global_step=400800, grad_norm=2.9375088214874268, loss=1.2270931005477905
I0304 17:22:23.952467 140380334978816 logging_writer.py:48] [400900] global_step=400900, grad_norm=3.001946210861206, loss=1.9471046924591064
I0304 17:23:08.856930 140380326586112 logging_writer.py:48] [401000] global_step=401000, grad_norm=2.986846685409546, loss=1.7885650396347046
I0304 17:23:53.882435 140380334978816 logging_writer.py:48] [401100] global_step=401100, grad_norm=3.660414457321167, loss=1.2493038177490234
I0304 17:24:38.751459 140380326586112 logging_writer.py:48] [401200] global_step=401200, grad_norm=4.329843521118164, loss=3.352944850921631
I0304 17:25:23.770529 140380334978816 logging_writer.py:48] [401300] global_step=401300, grad_norm=3.2289059162139893, loss=1.1243704557418823
I0304 17:26:08.806556 140380326586112 logging_writer.py:48] [401400] global_step=401400, grad_norm=3.0752909183502197, loss=1.248140573501587
I0304 17:26:33.690366 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:26:44.416421 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:27:10.011010 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:27:11.658005 140575196817216 submission_runner.py:411] Time since start: 194007.26s, 	Step: 401457, 	{'train/accuracy': 0.8862695097923279, 'train/loss': 0.42301321029663086, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 178593.23889565468, 'total_duration': 194007.2590417862, 'accumulated_submission_time': 178593.23889565468, 'accumulated_eval_time': 15365.542269706726, 'accumulated_logging_time': 27.2052583694458}
I0304 17:27:11.733583 140380334978816 logging_writer.py:48] [401457] accumulated_eval_time=15365.542270, accumulated_logging_time=27.205258, accumulated_submission_time=178593.238896, global_step=401457, preemption_count=0, score=178593.238896, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=194007.259042, train/accuracy=0.886270, train/loss=0.423013, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:27:29.486505 140380326586112 logging_writer.py:48] [401500] global_step=401500, grad_norm=3.075974702835083, loss=2.489959239959717
I0304 17:28:12.123973 140380334978816 logging_writer.py:48] [401600] global_step=401600, grad_norm=3.0459139347076416, loss=1.1261959075927734
I0304 17:28:56.805230 140380326586112 logging_writer.py:48] [401700] global_step=401700, grad_norm=3.284257173538208, loss=1.1540393829345703
I0304 17:29:42.416944 140380334978816 logging_writer.py:48] [401800] global_step=401800, grad_norm=3.381499767303467, loss=2.7744414806365967
I0304 17:30:27.656248 140380326586112 logging_writer.py:48] [401900] global_step=401900, grad_norm=3.1527183055877686, loss=1.6949145793914795
I0304 17:31:12.584113 140380334978816 logging_writer.py:48] [402000] global_step=402000, grad_norm=3.426118850708008, loss=1.1239286661148071
I0304 17:31:57.762840 140380326586112 logging_writer.py:48] [402100] global_step=402100, grad_norm=3.9016757011413574, loss=2.8484511375427246
I0304 17:32:42.563081 140380334978816 logging_writer.py:48] [402200] global_step=402200, grad_norm=3.0984342098236084, loss=2.0838706493377686
I0304 17:33:27.709082 140380326586112 logging_writer.py:48] [402300] global_step=402300, grad_norm=3.074086904525757, loss=1.1157978773117065
I0304 17:34:11.786287 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:34:22.570073 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:34:46.239154 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:34:47.886441 140575196817216 submission_runner.py:411] Time since start: 194463.49s, 	Step: 402400, 	{'train/accuracy': 0.88685542345047, 'train/loss': 0.4238695502281189, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 179013.23162341118, 'total_duration': 194463.48729109764, 'accumulated_submission_time': 179013.23162341118, 'accumulated_eval_time': 15401.641217947006, 'accumulated_logging_time': 27.29204750061035}
I0304 17:34:47.967180 140380334978816 logging_writer.py:48] [402400] accumulated_eval_time=15401.641218, accumulated_logging_time=27.292048, accumulated_submission_time=179013.231623, global_step=402400, preemption_count=0, score=179013.231623, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=194463.487291, train/accuracy=0.886855, train/loss=0.423870, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:34:48.376566 140380326586112 logging_writer.py:48] [402400] global_step=402400, grad_norm=3.163729667663574, loss=1.0939056873321533
I0304 17:35:28.597425 140380334978816 logging_writer.py:48] [402500] global_step=402500, grad_norm=3.7070841789245605, loss=1.7843763828277588
I0304 17:36:13.393425 140380326586112 logging_writer.py:48] [402600] global_step=402600, grad_norm=2.885908842086792, loss=1.04656982421875
I0304 17:36:58.382063 140380334978816 logging_writer.py:48] [402700] global_step=402700, grad_norm=2.997192144393921, loss=1.1599539518356323
I0304 17:37:43.071489 140380326586112 logging_writer.py:48] [402800] global_step=402800, grad_norm=3.4466214179992676, loss=2.6944429874420166
I0304 17:38:28.629881 140380334978816 logging_writer.py:48] [402900] global_step=402900, grad_norm=2.948740005493164, loss=1.9501601457595825
I0304 17:39:13.769351 140380326586112 logging_writer.py:48] [403000] global_step=403000, grad_norm=3.300656795501709, loss=1.0667762756347656
I0304 17:39:58.604471 140380334978816 logging_writer.py:48] [403100] global_step=403100, grad_norm=2.9688870906829834, loss=1.139509916305542
I0304 17:40:43.630797 140380326586112 logging_writer.py:48] [403200] global_step=403200, grad_norm=4.687534809112549, loss=2.724705934524536
I0304 17:41:28.831620 140380334978816 logging_writer.py:48] [403300] global_step=403300, grad_norm=4.027048110961914, loss=3.1235907077789307
I0304 17:41:48.214888 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:41:59.004702 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:42:25.805114 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:42:27.471160 140575196817216 submission_runner.py:411] Time since start: 194923.07s, 	Step: 403345, 	{'train/accuracy': 0.8892382383346558, 'train/loss': 0.4164697527885437, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 179433.4183971882, 'total_duration': 194923.0720796585, 'accumulated_submission_time': 179433.4183971882, 'accumulated_eval_time': 15440.896358013153, 'accumulated_logging_time': 27.38421607017517}
I0304 17:42:27.556524 140380326586112 logging_writer.py:48] [403345] accumulated_eval_time=15440.896358, accumulated_logging_time=27.384216, accumulated_submission_time=179433.418397, global_step=403345, preemption_count=0, score=179433.418397, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=194923.072080, train/accuracy=0.889238, train/loss=0.416470, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:42:49.806730 140380334978816 logging_writer.py:48] [403400] global_step=403400, grad_norm=3.2208619117736816, loss=1.2477946281433105
I0304 17:43:33.070842 140380326586112 logging_writer.py:48] [403500] global_step=403500, grad_norm=3.7883975505828857, loss=3.218303680419922
I0304 17:44:18.366222 140380334978816 logging_writer.py:48] [403600] global_step=403600, grad_norm=2.9322993755340576, loss=1.6154427528381348
I0304 17:45:03.505295 140380326586112 logging_writer.py:48] [403700] global_step=403700, grad_norm=2.9611542224884033, loss=1.3058063983917236
I0304 17:45:48.284786 140380334978816 logging_writer.py:48] [403800] global_step=403800, grad_norm=3.775425910949707, loss=2.6908791065216064
I0304 17:46:33.539137 140380326586112 logging_writer.py:48] [403900] global_step=403900, grad_norm=3.0957984924316406, loss=1.5038156509399414
I0304 17:47:18.544919 140380334978816 logging_writer.py:48] [404000] global_step=404000, grad_norm=3.2243616580963135, loss=1.7883739471435547
I0304 17:48:03.324148 140380326586112 logging_writer.py:48] [404100] global_step=404100, grad_norm=3.0286049842834473, loss=1.260473370552063
I0304 17:48:48.812327 140380334978816 logging_writer.py:48] [404200] global_step=404200, grad_norm=3.3233015537261963, loss=2.5908944606781006
I0304 17:49:27.755296 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:49:38.341911 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:50:05.066372 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:50:06.717352 140575196817216 submission_runner.py:411] Time since start: 195382.32s, 	Step: 404288, 	{'train/accuracy': 0.88623046875, 'train/loss': 0.42118391394615173, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 179853.55819368362, 'total_duration': 195382.3181631565, 'accumulated_submission_time': 179853.55819368362, 'accumulated_eval_time': 15479.857174158096, 'accumulated_logging_time': 27.4800283908844}
I0304 17:50:06.794507 140380326586112 logging_writer.py:48] [404288] accumulated_eval_time=15479.857174, accumulated_logging_time=27.480028, accumulated_submission_time=179853.558194, global_step=404288, preemption_count=0, score=179853.558194, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=195382.318163, train/accuracy=0.886230, train/loss=0.421184, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:50:11.982049 140380334978816 logging_writer.py:48] [404300] global_step=404300, grad_norm=4.021744251251221, loss=3.145413398742676
I0304 17:50:52.988969 140380326586112 logging_writer.py:48] [404400] global_step=404400, grad_norm=3.201390504837036, loss=2.2990856170654297
I0304 17:51:37.828926 140380334978816 logging_writer.py:48] [404500] global_step=404500, grad_norm=2.8918159008026123, loss=1.0415245294570923
I0304 17:52:23.019770 140380326586112 logging_writer.py:48] [404600] global_step=404600, grad_norm=3.0781376361846924, loss=2.3959193229675293
I0304 17:53:07.887347 140380334978816 logging_writer.py:48] [404700] global_step=404700, grad_norm=3.1211555004119873, loss=1.5899523496627808
I0304 17:53:52.930886 140380326586112 logging_writer.py:48] [404800] global_step=404800, grad_norm=3.4686737060546875, loss=2.8097102642059326
I0304 17:54:38.049313 140380334978816 logging_writer.py:48] [404900] global_step=404900, grad_norm=3.1881303787231445, loss=1.216871738433838
I0304 17:55:22.992645 140380326586112 logging_writer.py:48] [405000] global_step=405000, grad_norm=3.0288095474243164, loss=1.2130696773529053
I0304 17:56:08.021059 140380334978816 logging_writer.py:48] [405100] global_step=405100, grad_norm=3.19826078414917, loss=1.0813519954681396
I0304 17:56:53.276421 140380326586112 logging_writer.py:48] [405200] global_step=405200, grad_norm=2.9575676918029785, loss=1.0183933973312378
I0304 17:57:06.940119 140575196817216 spec.py:321] Evaluating on the training split.
I0304 17:57:17.576339 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 17:57:45.448442 140575196817216 spec.py:349] Evaluating on the test split.
I0304 17:57:47.109000 140575196817216 submission_runner.py:411] Time since start: 195842.71s, 	Step: 405232, 	{'train/accuracy': 0.8868359327316284, 'train/loss': 0.42134520411491394, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 180273.64124321938, 'total_duration': 195842.70994138718, 'accumulated_submission_time': 180273.64124321938, 'accumulated_eval_time': 15520.024940729141, 'accumulated_logging_time': 27.570369720458984}
I0304 17:57:47.200356 140380334978816 logging_writer.py:48] [405232] accumulated_eval_time=15520.024941, accumulated_logging_time=27.570370, accumulated_submission_time=180273.641243, global_step=405232, preemption_count=0, score=180273.641243, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=195842.709941, train/accuracy=0.886836, train/loss=0.421345, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 17:58:14.624183 140380326586112 logging_writer.py:48] [405300] global_step=405300, grad_norm=3.2468643188476562, loss=1.1993783712387085
I0304 17:58:58.573742 140380334978816 logging_writer.py:48] [405400] global_step=405400, grad_norm=3.3623428344726562, loss=1.1316078901290894
I0304 17:59:43.465968 140380326586112 logging_writer.py:48] [405500] global_step=405500, grad_norm=3.223557949066162, loss=1.0384434461593628
I0304 18:00:28.675130 140380334978816 logging_writer.py:48] [405600] global_step=405600, grad_norm=3.121899366378784, loss=1.1837825775146484
I0304 18:01:13.621798 140380326586112 logging_writer.py:48] [405700] global_step=405700, grad_norm=3.043999671936035, loss=2.1280088424682617
I0304 18:01:59.022671 140380334978816 logging_writer.py:48] [405800] global_step=405800, grad_norm=3.7874562740325928, loss=3.1810710430145264
I0304 18:02:43.811820 140380326586112 logging_writer.py:48] [405900] global_step=405900, grad_norm=2.9936118125915527, loss=1.4090431928634644
I0304 18:03:28.682419 140380334978816 logging_writer.py:48] [406000] global_step=406000, grad_norm=2.9243545532226562, loss=1.731169581413269
I0304 18:04:13.724603 140380326586112 logging_writer.py:48] [406100] global_step=406100, grad_norm=3.01995587348938, loss=1.1194796562194824
I0304 18:04:47.481729 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:04:58.315798 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:05:25.736529 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:05:27.380515 140575196817216 submission_runner.py:411] Time since start: 196302.98s, 	Step: 406177, 	{'train/accuracy': 0.8876171708106995, 'train/loss': 0.4252983629703522, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 180693.8637099266, 'total_duration': 196302.9815299511, 'accumulated_submission_time': 180693.8637099266, 'accumulated_eval_time': 15559.92268037796, 'accumulated_logging_time': 27.672037363052368}
I0304 18:05:27.456276 140380334978816 logging_writer.py:48] [406177] accumulated_eval_time=15559.922680, accumulated_logging_time=27.672037, accumulated_submission_time=180693.863710, global_step=406177, preemption_count=0, score=180693.863710, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=196302.981530, train/accuracy=0.887617, train/loss=0.425298, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:05:37.011236 140380326586112 logging_writer.py:48] [406200] global_step=406200, grad_norm=3.1233785152435303, loss=1.1604019403457642
I0304 18:06:18.654381 140380334978816 logging_writer.py:48] [406300] global_step=406300, grad_norm=3.0823209285736084, loss=1.0741231441497803
I0304 18:07:03.595025 140380326586112 logging_writer.py:48] [406400] global_step=406400, grad_norm=3.2939584255218506, loss=1.2482534646987915
I0304 18:07:48.905548 140380334978816 logging_writer.py:48] [406500] global_step=406500, grad_norm=3.1059927940368652, loss=1.10039222240448
I0304 18:08:34.116365 140380326586112 logging_writer.py:48] [406600] global_step=406600, grad_norm=3.488133192062378, loss=1.1264132261276245
I0304 18:09:19.208218 140380334978816 logging_writer.py:48] [406700] global_step=406700, grad_norm=2.9901933670043945, loss=1.813388466835022
I0304 18:10:04.301171 140380326586112 logging_writer.py:48] [406800] global_step=406800, grad_norm=3.310476541519165, loss=1.143942952156067
I0304 18:10:49.184082 140380334978816 logging_writer.py:48] [406900] global_step=406900, grad_norm=4.767090320587158, loss=3.1704280376434326
I0304 18:11:34.062497 140380326586112 logging_writer.py:48] [407000] global_step=407000, grad_norm=3.8887736797332764, loss=3.1333231925964355
I0304 18:12:19.008417 140380334978816 logging_writer.py:48] [407100] global_step=407100, grad_norm=2.9901845455169678, loss=2.063912868499756
I0304 18:12:27.671240 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:12:38.396400 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:13:05.681809 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:13:07.336557 140575196817216 submission_runner.py:411] Time since start: 196762.94s, 	Step: 407121, 	{'train/accuracy': 0.8858984112739563, 'train/loss': 0.42193904519081116, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 181114.0154056549, 'total_duration': 196762.93761825562, 'accumulated_submission_time': 181114.0154056549, 'accumulated_eval_time': 15599.586994171143, 'accumulated_logging_time': 27.76237177848816}
I0304 18:13:07.412405 140380326586112 logging_writer.py:48] [407121] accumulated_eval_time=15599.586994, accumulated_logging_time=27.762372, accumulated_submission_time=181114.015406, global_step=407121, preemption_count=0, score=181114.015406, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=196762.937618, train/accuracy=0.885898, train/loss=0.421939, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:13:39.226069 140380334978816 logging_writer.py:48] [407200] global_step=407200, grad_norm=3.2443366050720215, loss=1.179179072380066
I0304 18:14:23.347659 140380326586112 logging_writer.py:48] [407300] global_step=407300, grad_norm=3.6286802291870117, loss=2.9946742057800293
I0304 18:15:08.407449 140380334978816 logging_writer.py:48] [407400] global_step=407400, grad_norm=3.415191650390625, loss=1.5778812170028687
I0304 18:15:53.272333 140380326586112 logging_writer.py:48] [407500] global_step=407500, grad_norm=3.2067551612854004, loss=2.7949767112731934
I0304 18:16:38.419674 140380334978816 logging_writer.py:48] [407600] global_step=407600, grad_norm=3.1321234703063965, loss=2.6288061141967773
I0304 18:17:23.411225 140380326586112 logging_writer.py:48] [407700] global_step=407700, grad_norm=3.2053043842315674, loss=1.6139671802520752
I0304 18:18:08.140020 140380334978816 logging_writer.py:48] [407800] global_step=407800, grad_norm=3.0138823986053467, loss=1.8361687660217285
I0304 18:18:53.374820 140380326586112 logging_writer.py:48] [407900] global_step=407900, grad_norm=3.0058090686798096, loss=1.0543010234832764
I0304 18:19:38.415689 140380334978816 logging_writer.py:48] [408000] global_step=408000, grad_norm=3.0706210136413574, loss=1.130724310874939
I0304 18:20:07.362654 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:20:18.313677 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:20:47.008373 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:20:48.663488 140575196817216 submission_runner.py:411] Time since start: 197224.26s, 	Step: 408066, 	{'train/accuracy': 0.8882030844688416, 'train/loss': 0.4163236916065216, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 181533.90558075905, 'total_duration': 197224.26424860954, 'accumulated_submission_time': 181533.90558075905, 'accumulated_eval_time': 15640.886532783508, 'accumulated_logging_time': 27.849211931228638}
I0304 18:20:48.739576 140380326586112 logging_writer.py:48] [408066] accumulated_eval_time=15640.886533, accumulated_logging_time=27.849212, accumulated_submission_time=181533.905581, global_step=408066, preemption_count=0, score=181533.905581, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=197224.264249, train/accuracy=0.888203, train/loss=0.416324, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:21:02.669650 140380334978816 logging_writer.py:48] [408100] global_step=408100, grad_norm=2.978762149810791, loss=1.0756319761276245
I0304 18:21:44.690841 140380326586112 logging_writer.py:48] [408200] global_step=408200, grad_norm=3.1971733570098877, loss=2.0841939449310303
I0304 18:22:29.936361 140380334978816 logging_writer.py:48] [408300] global_step=408300, grad_norm=3.235383987426758, loss=1.0637786388397217
I0304 18:23:15.312687 140380326586112 logging_writer.py:48] [408400] global_step=408400, grad_norm=4.197038650512695, loss=3.2327754497528076
I0304 18:24:00.315237 140380334978816 logging_writer.py:48] [408500] global_step=408500, grad_norm=3.6201789379119873, loss=2.491126298904419
I0304 18:24:45.589104 140380326586112 logging_writer.py:48] [408600] global_step=408600, grad_norm=3.264387607574463, loss=1.0944055318832397
I0304 18:25:30.429039 140380334978816 logging_writer.py:48] [408700] global_step=408700, grad_norm=4.594274044036865, loss=3.235283851623535
I0304 18:26:15.579518 140380326586112 logging_writer.py:48] [408800] global_step=408800, grad_norm=2.9635114669799805, loss=1.5930153131484985
I0304 18:27:00.602170 140380334978816 logging_writer.py:48] [408900] global_step=408900, grad_norm=3.0491840839385986, loss=1.1502294540405273
I0304 18:27:45.468076 140380326586112 logging_writer.py:48] [409000] global_step=409000, grad_norm=3.221633195877075, loss=1.0824148654937744
I0304 18:27:48.801304 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:27:59.627370 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:28:28.127842 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:28:29.786908 140575196817216 submission_runner.py:411] Time since start: 197685.39s, 	Step: 409009, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.4183976352214813, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 181953.90784978867, 'total_duration': 197685.3879377842, 'accumulated_submission_time': 181953.90784978867, 'accumulated_eval_time': 15681.871098041534, 'accumulated_logging_time': 27.936203956604004}
I0304 18:28:29.868916 140380334978816 logging_writer.py:48] [409009] accumulated_eval_time=15681.871098, accumulated_logging_time=27.936204, accumulated_submission_time=181953.907850, global_step=409009, preemption_count=0, score=181953.907850, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=197685.387938, train/accuracy=0.887207, train/loss=0.418398, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:29:06.578334 140380326586112 logging_writer.py:48] [409100] global_step=409100, grad_norm=3.6331636905670166, loss=3.098759412765503
I0304 18:29:51.524186 140380334978816 logging_writer.py:48] [409200] global_step=409200, grad_norm=3.441436290740967, loss=2.1928722858428955
I0304 18:30:36.581656 140380326586112 logging_writer.py:48] [409300] global_step=409300, grad_norm=3.1624915599823, loss=2.747375249862671
I0304 18:31:21.706652 140380334978816 logging_writer.py:48] [409400] global_step=409400, grad_norm=3.015183448791504, loss=2.501349449157715
I0304 18:32:06.683563 140380326586112 logging_writer.py:48] [409500] global_step=409500, grad_norm=3.148003339767456, loss=1.213678002357483
I0304 18:32:51.945180 140380334978816 logging_writer.py:48] [409600] global_step=409600, grad_norm=12.14148998260498, loss=1.7249341011047363
I0304 18:33:36.925282 140380326586112 logging_writer.py:48] [409700] global_step=409700, grad_norm=4.769092559814453, loss=3.121107816696167
I0304 18:34:22.026464 140380334978816 logging_writer.py:48] [409800] global_step=409800, grad_norm=3.173940420150757, loss=1.400988221168518
I0304 18:35:07.109021 140380326586112 logging_writer.py:48] [409900] global_step=409900, grad_norm=3.1927409172058105, loss=1.1896430253982544
I0304 18:35:30.141004 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:35:40.959235 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:36:08.122423 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:36:09.779559 140575196817216 submission_runner.py:411] Time since start: 198145.38s, 	Step: 409953, 	{'train/accuracy': 0.8857616782188416, 'train/loss': 0.4226754307746887, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 182374.12273025513, 'total_duration': 198145.38085389137, 'accumulated_submission_time': 182374.12273025513, 'accumulated_eval_time': 15721.50890302658, 'accumulated_logging_time': 28.027443408966064}
I0304 18:36:09.857344 140380334978816 logging_writer.py:48] [409953] accumulated_eval_time=15721.508903, accumulated_logging_time=28.027443, accumulated_submission_time=182374.122730, global_step=409953, preemption_count=0, score=182374.122730, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=198145.380854, train/accuracy=0.885762, train/loss=0.422675, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:36:28.945900 140380326586112 logging_writer.py:48] [410000] global_step=410000, grad_norm=3.275444746017456, loss=1.3564915657043457
I0304 18:37:11.629756 140380334978816 logging_writer.py:48] [410100] global_step=410100, grad_norm=3.723803758621216, loss=3.17169451713562
I0304 18:37:56.488887 140380326586112 logging_writer.py:48] [410200] global_step=410200, grad_norm=2.9816741943359375, loss=1.1953184604644775
I0304 18:38:41.850263 140380334978816 logging_writer.py:48] [410300] global_step=410300, grad_norm=3.908081293106079, loss=3.1168630123138428
I0304 18:39:27.146692 140380326586112 logging_writer.py:48] [410400] global_step=410400, grad_norm=3.2011728286743164, loss=2.4414589405059814
I0304 18:40:12.089752 140380334978816 logging_writer.py:48] [410500] global_step=410500, grad_norm=3.0323002338409424, loss=2.364224910736084
I0304 18:40:57.420335 140380326586112 logging_writer.py:48] [410600] global_step=410600, grad_norm=3.241342306137085, loss=1.150891900062561
I0304 18:41:42.288318 140380334978816 logging_writer.py:48] [410700] global_step=410700, grad_norm=3.034393310546875, loss=1.849278211593628
I0304 18:42:27.469175 140380326586112 logging_writer.py:48] [410800] global_step=410800, grad_norm=2.9277219772338867, loss=1.540427565574646
I0304 18:43:09.884147 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:43:20.525698 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:43:45.893877 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:43:47.558672 140575196817216 submission_runner.py:411] Time since start: 198603.16s, 	Step: 410896, 	{'train/accuracy': 0.8866015672683716, 'train/loss': 0.4220174551010132, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 182794.09159350395, 'total_duration': 198603.15967917442, 'accumulated_submission_time': 182794.09159350395, 'accumulated_eval_time': 15759.182371139526, 'accumulated_logging_time': 28.114766120910645}
I0304 18:43:47.655175 140380334978816 logging_writer.py:48] [410896] accumulated_eval_time=15759.182371, accumulated_logging_time=28.114766, accumulated_submission_time=182794.091594, global_step=410896, preemption_count=0, score=182794.091594, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=198603.159679, train/accuracy=0.886602, train/loss=0.422017, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:43:49.649080 140380326586112 logging_writer.py:48] [410900] global_step=410900, grad_norm=3.5853898525238037, loss=3.0829341411590576
I0304 18:44:30.140979 140380334978816 logging_writer.py:48] [411000] global_step=411000, grad_norm=3.6339848041534424, loss=1.223639726638794
I0304 18:45:14.898403 140380326586112 logging_writer.py:48] [411100] global_step=411100, grad_norm=3.209217071533203, loss=1.7348064184188843
I0304 18:46:00.115022 140380334978816 logging_writer.py:48] [411200] global_step=411200, grad_norm=3.217233180999756, loss=1.281813383102417
I0304 18:46:45.302304 140380326586112 logging_writer.py:48] [411300] global_step=411300, grad_norm=3.032341241836548, loss=1.3403279781341553
I0304 18:47:30.517423 140380334978816 logging_writer.py:48] [411400] global_step=411400, grad_norm=3.118616819381714, loss=1.1412513256072998
I0304 18:48:15.512495 140380326586112 logging_writer.py:48] [411500] global_step=411500, grad_norm=3.221496820449829, loss=2.323345184326172
I0304 18:49:00.124252 140380334978816 logging_writer.py:48] [411600] global_step=411600, grad_norm=3.104421615600586, loss=1.0924196243286133
I0304 18:49:45.546321 140380326586112 logging_writer.py:48] [411700] global_step=411700, grad_norm=3.019458055496216, loss=1.0588042736053467
I0304 18:50:30.750403 140380334978816 logging_writer.py:48] [411800] global_step=411800, grad_norm=3.086793899536133, loss=1.2303059101104736
I0304 18:50:47.920931 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:50:58.823613 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:51:25.205412 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:51:26.859907 140575196817216 submission_runner.py:411] Time since start: 199062.46s, 	Step: 411840, 	{'train/accuracy': 0.8879101276397705, 'train/loss': 0.41601622104644775, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 183214.29858469963, 'total_duration': 199062.46124649048, 'accumulated_submission_time': 183214.29858469963, 'accumulated_eval_time': 15798.120640039444, 'accumulated_logging_time': 28.22186017036438}
I0304 18:51:26.938846 140380326586112 logging_writer.py:48] [411840] accumulated_eval_time=15798.120640, accumulated_logging_time=28.221860, accumulated_submission_time=183214.298585, global_step=411840, preemption_count=0, score=183214.298585, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=199062.461246, train/accuracy=0.887910, train/loss=0.416016, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:51:51.181185 140380334978816 logging_writer.py:48] [411900] global_step=411900, grad_norm=3.0860772132873535, loss=1.1381263732910156
I0304 18:52:34.665924 140380326586112 logging_writer.py:48] [412000] global_step=412000, grad_norm=3.347846508026123, loss=1.272984266281128
I0304 18:53:19.690231 140380334978816 logging_writer.py:48] [412100] global_step=412100, grad_norm=2.9879610538482666, loss=1.0495744943618774
I0304 18:54:04.825443 140380326586112 logging_writer.py:48] [412200] global_step=412200, grad_norm=3.1702983379364014, loss=1.1762135028839111
I0304 18:54:49.933632 140380334978816 logging_writer.py:48] [412300] global_step=412300, grad_norm=3.3313446044921875, loss=1.1976885795593262
I0304 18:55:34.998577 140380326586112 logging_writer.py:48] [412400] global_step=412400, grad_norm=2.9877817630767822, loss=1.9594197273254395
I0304 18:56:20.185100 140380334978816 logging_writer.py:48] [412500] global_step=412500, grad_norm=2.8861494064331055, loss=1.2394016981124878
I0304 18:57:05.052373 140380326586112 logging_writer.py:48] [412600] global_step=412600, grad_norm=3.5391602516174316, loss=1.3020668029785156
I0304 18:57:50.061963 140380334978816 logging_writer.py:48] [412700] global_step=412700, grad_norm=3.636937379837036, loss=2.607062578201294
I0304 18:58:27.242208 140575196817216 spec.py:321] Evaluating on the training split.
I0304 18:58:37.987768 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 18:59:02.596170 140575196817216 spec.py:349] Evaluating on the test split.
I0304 18:59:04.238628 140575196817216 submission_runner.py:411] Time since start: 199519.84s, 	Step: 412784, 	{'train/accuracy': 0.8880078196525574, 'train/loss': 0.42055341601371765, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 183634.53731536865, 'total_duration': 199519.83964657784, 'accumulated_submission_time': 183634.53731536865, 'accumulated_eval_time': 15835.116010665894, 'accumulated_logging_time': 28.31593894958496}
I0304 18:59:04.318599 140380326586112 logging_writer.py:48] [412784] accumulated_eval_time=15835.116011, accumulated_logging_time=28.315939, accumulated_submission_time=183634.537315, global_step=412784, preemption_count=0, score=183634.537315, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=199519.839647, train/accuracy=0.888008, train/loss=0.420553, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 18:59:11.076121 140380334978816 logging_writer.py:48] [412800] global_step=412800, grad_norm=2.996084213256836, loss=1.1264656782150269
I0304 18:59:52.283615 140380326586112 logging_writer.py:48] [412900] global_step=412900, grad_norm=3.4669981002807617, loss=2.6056127548217773
I0304 19:00:37.095634 140380334978816 logging_writer.py:48] [413000] global_step=413000, grad_norm=4.086853504180908, loss=2.9572577476501465
I0304 19:01:22.288476 140380326586112 logging_writer.py:48] [413100] global_step=413100, grad_norm=3.121166944503784, loss=1.0898675918579102
I0304 19:02:07.464453 140380334978816 logging_writer.py:48] [413200] global_step=413200, grad_norm=3.6807684898376465, loss=1.9866193532943726
I0304 19:02:52.363630 140380326586112 logging_writer.py:48] [413300] global_step=413300, grad_norm=2.9536287784576416, loss=1.6319549083709717
I0304 19:03:37.466411 140380334978816 logging_writer.py:48] [413400] global_step=413400, grad_norm=3.210414409637451, loss=2.361633777618408
I0304 19:04:22.240818 140380326586112 logging_writer.py:48] [413500] global_step=413500, grad_norm=3.763794183731079, loss=3.2784876823425293
I0304 19:05:07.452595 140380334978816 logging_writer.py:48] [413600] global_step=413600, grad_norm=3.1139283180236816, loss=1.204971432685852
I0304 19:05:52.434087 140380326586112 logging_writer.py:48] [413700] global_step=413700, grad_norm=3.0469250679016113, loss=1.1632840633392334
I0304 19:06:04.363327 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:06:15.377978 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:06:40.945621 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:06:42.596624 140575196817216 submission_runner.py:411] Time since start: 199978.20s, 	Step: 413728, 	{'train/accuracy': 0.8871679306030273, 'train/loss': 0.41991445422172546, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 184054.52427721024, 'total_duration': 199978.19762659073, 'accumulated_submission_time': 184054.52427721024, 'accumulated_eval_time': 15873.348242282867, 'accumulated_logging_time': 28.40547013282776}
I0304 19:06:42.675327 140380334978816 logging_writer.py:48] [413728] accumulated_eval_time=15873.348242, accumulated_logging_time=28.405470, accumulated_submission_time=184054.524277, global_step=413728, preemption_count=0, score=184054.524277, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=199978.197627, train/accuracy=0.887168, train/loss=0.419914, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:07:11.698217 140380326586112 logging_writer.py:48] [413800] global_step=413800, grad_norm=3.132798194885254, loss=1.8541319370269775
I0304 19:07:55.584307 140380334978816 logging_writer.py:48] [413900] global_step=413900, grad_norm=3.2284328937530518, loss=2.559528350830078
I0304 19:08:40.529802 140380326586112 logging_writer.py:48] [414000] global_step=414000, grad_norm=3.161647081375122, loss=1.1269336938858032
I0304 19:09:25.636706 140380334978816 logging_writer.py:48] [414100] global_step=414100, grad_norm=3.368511915206909, loss=1.187177062034607
I0304 19:10:10.975417 140380326586112 logging_writer.py:48] [414200] global_step=414200, grad_norm=3.2768313884735107, loss=1.2261724472045898
I0304 19:10:55.853593 140380334978816 logging_writer.py:48] [414300] global_step=414300, grad_norm=5.0037760734558105, loss=3.2307724952697754
I0304 19:11:41.172345 140380326586112 logging_writer.py:48] [414400] global_step=414400, grad_norm=2.9643020629882812, loss=1.0831050872802734
I0304 19:12:26.136169 140380334978816 logging_writer.py:48] [414500] global_step=414500, grad_norm=3.156874656677246, loss=2.6271920204162598
I0304 19:13:11.174314 140380326586112 logging_writer.py:48] [414600] global_step=414600, grad_norm=4.739357948303223, loss=3.2497096061706543
I0304 19:13:42.779778 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:13:53.795966 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:14:18.815427 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:14:20.478682 140575196817216 submission_runner.py:411] Time since start: 200436.08s, 	Step: 414672, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.4232935905456543, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 184474.57068252563, 'total_duration': 200436.07974553108, 'accumulated_submission_time': 184474.57068252563, 'accumulated_eval_time': 15911.04614019394, 'accumulated_logging_time': 28.493316650390625}
I0304 19:14:20.556558 140380334978816 logging_writer.py:48] [414672] accumulated_eval_time=15911.046140, accumulated_logging_time=28.493317, accumulated_submission_time=184474.570683, global_step=414672, preemption_count=0, score=184474.570683, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=200436.079746, train/accuracy=0.887207, train/loss=0.423294, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:14:32.094676 140380326586112 logging_writer.py:48] [414700] global_step=414700, grad_norm=3.2100822925567627, loss=1.201339840888977
I0304 19:15:13.884458 140380334978816 logging_writer.py:48] [414800] global_step=414800, grad_norm=3.6670124530792236, loss=2.934887647628784
I0304 19:15:58.811562 140380326586112 logging_writer.py:48] [414900] global_step=414900, grad_norm=3.1402316093444824, loss=1.1398626565933228
I0304 19:16:44.278953 140380334978816 logging_writer.py:48] [415000] global_step=415000, grad_norm=3.0960004329681396, loss=1.0971462726593018
I0304 19:17:29.167658 140380326586112 logging_writer.py:48] [415100] global_step=415100, grad_norm=2.976745128631592, loss=1.2252858877182007
I0304 19:18:14.198842 140380334978816 logging_writer.py:48] [415200] global_step=415200, grad_norm=3.1323726177215576, loss=2.40364408493042
I0304 19:18:58.999151 140380326586112 logging_writer.py:48] [415300] global_step=415300, grad_norm=3.7016091346740723, loss=3.0604565143585205
I0304 19:19:44.152220 140380334978816 logging_writer.py:48] [415400] global_step=415400, grad_norm=3.072370767593384, loss=1.9508602619171143
I0304 19:20:29.072792 140380326586112 logging_writer.py:48] [415500] global_step=415500, grad_norm=2.9223432540893555, loss=1.1996066570281982
I0304 19:21:14.029121 140380334978816 logging_writer.py:48] [415600] global_step=415600, grad_norm=3.9356610774993896, loss=2.851834774017334
I0304 19:21:20.932711 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:21:31.535633 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:22:01.138661 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:22:02.806123 140575196817216 submission_runner.py:411] Time since start: 200898.41s, 	Step: 415617, 	{'train/accuracy': 0.8864062428474426, 'train/loss': 0.42240461707115173, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 184894.88948082924, 'total_duration': 200898.40726947784, 'accumulated_submission_time': 184894.88948082924, 'accumulated_eval_time': 15952.918644666672, 'accumulated_logging_time': 28.580711126327515}
I0304 19:22:02.885004 140380326586112 logging_writer.py:48] [415617] accumulated_eval_time=15952.918645, accumulated_logging_time=28.580711, accumulated_submission_time=184894.889481, global_step=415617, preemption_count=0, score=184894.889481, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=200898.407269, train/accuracy=0.886406, train/loss=0.422405, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:22:36.272308 140380334978816 logging_writer.py:48] [415700] global_step=415700, grad_norm=2.8525991439819336, loss=1.917339563369751
I0304 19:23:20.620020 140380326586112 logging_writer.py:48] [415800] global_step=415800, grad_norm=3.006596565246582, loss=1.009425401687622
I0304 19:24:05.863476 140380334978816 logging_writer.py:48] [415900] global_step=415900, grad_norm=3.500319480895996, loss=2.614189386367798
I0304 19:24:50.777337 140380326586112 logging_writer.py:48] [416000] global_step=416000, grad_norm=3.525895118713379, loss=1.2021183967590332
I0304 19:25:35.810822 140380334978816 logging_writer.py:48] [416100] global_step=416100, grad_norm=3.682053804397583, loss=3.048877716064453
I0304 19:26:20.872510 140380326586112 logging_writer.py:48] [416200] global_step=416200, grad_norm=3.0901966094970703, loss=1.2323246002197266
I0304 19:27:05.977650 140380334978816 logging_writer.py:48] [416300] global_step=416300, grad_norm=2.9174320697784424, loss=1.2334253787994385
I0304 19:27:51.211153 140380326586112 logging_writer.py:48] [416400] global_step=416400, grad_norm=3.133354663848877, loss=1.2010786533355713
I0304 19:28:35.972252 140380334978816 logging_writer.py:48] [416500] global_step=416500, grad_norm=3.4681334495544434, loss=1.3549251556396484
I0304 19:29:03.038679 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:29:13.585741 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:29:39.466842 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:29:41.126474 140575196817216 submission_runner.py:411] Time since start: 201356.73s, 	Step: 416562, 	{'train/accuracy': 0.8877148032188416, 'train/loss': 0.4189739227294922, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 185314.9847421646, 'total_duration': 201356.72749471664, 'accumulated_submission_time': 185314.9847421646, 'accumulated_eval_time': 15991.00540137291, 'accumulated_logging_time': 28.669321537017822}
I0304 19:29:41.219350 140380326586112 logging_writer.py:48] [416562] accumulated_eval_time=15991.005401, accumulated_logging_time=28.669322, accumulated_submission_time=185314.984742, global_step=416562, preemption_count=0, score=185314.984742, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=201356.727495, train/accuracy=0.887715, train/loss=0.418974, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:29:56.723005 140380334978816 logging_writer.py:48] [416600] global_step=416600, grad_norm=3.487936496734619, loss=2.9276046752929688
I0304 19:30:39.126042 140380326586112 logging_writer.py:48] [416700] global_step=416700, grad_norm=3.9785714149475098, loss=3.1491434574127197
I0304 19:31:23.909929 140380334978816 logging_writer.py:48] [416800] global_step=416800, grad_norm=3.0253870487213135, loss=2.6259591579437256
I0304 19:32:09.100976 140380326586112 logging_writer.py:48] [416900] global_step=416900, grad_norm=3.308645009994507, loss=1.0963950157165527
I0304 19:32:53.904902 140380334978816 logging_writer.py:48] [417000] global_step=417000, grad_norm=3.0489554405212402, loss=1.0901978015899658
I0304 19:33:39.008450 140380326586112 logging_writer.py:48] [417100] global_step=417100, grad_norm=3.4395267963409424, loss=2.9549076557159424
I0304 19:34:23.841382 140380334978816 logging_writer.py:48] [417200] global_step=417200, grad_norm=2.95651912689209, loss=1.8706167936325073
I0304 19:35:08.914987 140380326586112 logging_writer.py:48] [417300] global_step=417300, grad_norm=2.9121642112731934, loss=1.9993518590927124
I0304 19:35:53.943944 140380334978816 logging_writer.py:48] [417400] global_step=417400, grad_norm=4.394533634185791, loss=3.0652871131896973
I0304 19:36:38.958070 140380326586112 logging_writer.py:48] [417500] global_step=417500, grad_norm=3.168259382247925, loss=1.184787392616272
I0304 19:36:41.303057 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:36:52.042035 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:37:18.927272 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:37:20.569882 140575196817216 submission_runner.py:411] Time since start: 201816.17s, 	Step: 417507, 	{'train/accuracy': 0.8869140148162842, 'train/loss': 0.41903385519981384, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 185735.00822615623, 'total_duration': 201816.17091417313, 'accumulated_submission_time': 185735.00822615623, 'accumulated_eval_time': 16030.271182060242, 'accumulated_logging_time': 28.7737877368927}
I0304 19:37:20.649252 140380334978816 logging_writer.py:48] [417507] accumulated_eval_time=16030.271182, accumulated_logging_time=28.773788, accumulated_submission_time=185735.008226, global_step=417507, preemption_count=0, score=185735.008226, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=201816.170914, train/accuracy=0.886914, train/loss=0.419034, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:37:58.160394 140380326586112 logging_writer.py:48] [417600] global_step=417600, grad_norm=3.338489294052124, loss=2.333130359649658
I0304 19:38:42.975026 140380334978816 logging_writer.py:48] [417700] global_step=417700, grad_norm=3.024690866470337, loss=1.4191476106643677
I0304 19:39:28.086414 140380326586112 logging_writer.py:48] [417800] global_step=417800, grad_norm=3.098888397216797, loss=1.1904877424240112
I0304 19:40:14.590432 140380334978816 logging_writer.py:48] [417900] global_step=417900, grad_norm=3.164480447769165, loss=1.1172517538070679
I0304 19:40:59.447665 140380326586112 logging_writer.py:48] [418000] global_step=418000, grad_norm=3.291429042816162, loss=1.144078254699707
I0304 19:41:44.572731 140380334978816 logging_writer.py:48] [418100] global_step=418100, grad_norm=3.0977518558502197, loss=1.1261725425720215
I0304 19:42:29.546549 140380326586112 logging_writer.py:48] [418200] global_step=418200, grad_norm=3.5828447341918945, loss=1.1113545894622803
I0304 19:43:14.381933 140380334978816 logging_writer.py:48] [418300] global_step=418300, grad_norm=3.252105236053467, loss=1.2797707319259644
I0304 19:43:58.987360 140380326586112 logging_writer.py:48] [418400] global_step=418400, grad_norm=2.922790765762329, loss=1.150355577468872
I0304 19:44:20.738789 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:44:31.556990 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:44:59.822791 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:45:01.478389 140575196817216 submission_runner.py:411] Time since start: 202277.08s, 	Step: 418450, 	{'train/accuracy': 0.8861523270606995, 'train/loss': 0.425559937953949, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 186155.03788423538, 'total_duration': 202277.07932949066, 'accumulated_submission_time': 186155.03788423538, 'accumulated_eval_time': 16071.009669065475, 'accumulated_logging_time': 28.864509344100952}
I0304 19:45:01.594542 140380334978816 logging_writer.py:48] [418450] accumulated_eval_time=16071.009669, accumulated_logging_time=28.864509, accumulated_submission_time=186155.037884, global_step=418450, preemption_count=0, score=186155.037884, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=202277.079329, train/accuracy=0.886152, train/loss=0.425560, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:45:21.885176 140380326586112 logging_writer.py:48] [418500] global_step=418500, grad_norm=3.245612382888794, loss=1.022944688796997
I0304 19:46:04.709814 140380334978816 logging_writer.py:48] [418600] global_step=418600, grad_norm=2.9661264419555664, loss=1.4738022089004517
I0304 19:46:50.035526 140380326586112 logging_writer.py:48] [418700] global_step=418700, grad_norm=3.7257843017578125, loss=3.1648974418640137
I0304 19:47:35.125561 140380334978816 logging_writer.py:48] [418800] global_step=418800, grad_norm=3.4946625232696533, loss=1.1271857023239136
I0304 19:48:19.977373 140380326586112 logging_writer.py:48] [418900] global_step=418900, grad_norm=4.405236721038818, loss=3.284607410430908
I0304 19:49:05.200934 140380334978816 logging_writer.py:48] [419000] global_step=419000, grad_norm=3.3065543174743652, loss=2.229731559753418
I0304 19:49:50.050470 140380326586112 logging_writer.py:48] [419100] global_step=419100, grad_norm=2.942660093307495, loss=1.2226026058197021
I0304 19:50:35.126754 140380334978816 logging_writer.py:48] [419200] global_step=419200, grad_norm=3.1622300148010254, loss=1.1679174900054932
I0304 19:51:20.109011 140380326586112 logging_writer.py:48] [419300] global_step=419300, grad_norm=3.073843479156494, loss=1.1271525621414185
I0304 19:52:01.673080 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:52:12.359126 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 19:52:40.098475 140575196817216 spec.py:349] Evaluating on the test split.
I0304 19:52:41.754848 140575196817216 submission_runner.py:411] Time since start: 202737.36s, 	Step: 419394, 	{'train/accuracy': 0.8866015672683716, 'train/loss': 0.4160337746143341, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 186575.0562889576, 'total_duration': 202737.35576033592, 'accumulated_submission_time': 186575.0562889576, 'accumulated_eval_time': 16111.09059047699, 'accumulated_logging_time': 28.99151039123535}
I0304 19:52:41.835302 140380334978816 logging_writer.py:48] [419394] accumulated_eval_time=16111.090590, accumulated_logging_time=28.991510, accumulated_submission_time=186575.056289, global_step=419394, preemption_count=0, score=186575.056289, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=202737.355760, train/accuracy=0.886602, train/loss=0.416034, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 19:52:44.627785 140380326586112 logging_writer.py:48] [419400] global_step=419400, grad_norm=3.487555980682373, loss=2.744356393814087
I0304 19:53:25.411682 140380334978816 logging_writer.py:48] [419500] global_step=419500, grad_norm=2.8038933277130127, loss=1.3615504503250122
I0304 19:54:10.327885 140380326586112 logging_writer.py:48] [419600] global_step=419600, grad_norm=3.3320701122283936, loss=2.333146572113037
I0304 19:54:55.423476 140380334978816 logging_writer.py:48] [419700] global_step=419700, grad_norm=3.0675647258758545, loss=1.1686172485351562
I0304 19:55:40.339210 140380326586112 logging_writer.py:48] [419800] global_step=419800, grad_norm=3.5385658740997314, loss=1.060213327407837
I0304 19:56:25.475373 140380334978816 logging_writer.py:48] [419900] global_step=419900, grad_norm=3.2803077697753906, loss=1.1844465732574463
I0304 19:57:10.607058 140380326586112 logging_writer.py:48] [420000] global_step=420000, grad_norm=3.1790413856506348, loss=1.1244490146636963
I0304 19:57:55.631474 140380334978816 logging_writer.py:48] [420100] global_step=420100, grad_norm=3.0671746730804443, loss=1.3269988298416138
I0304 19:58:40.637541 140380326586112 logging_writer.py:48] [420200] global_step=420200, grad_norm=3.1545767784118652, loss=1.0079119205474854
I0304 19:59:25.390556 140380334978816 logging_writer.py:48] [420300] global_step=420300, grad_norm=3.537616729736328, loss=3.0311312675476074
I0304 19:59:41.981233 140575196817216 spec.py:321] Evaluating on the training split.
I0304 19:59:52.681416 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:00:18.541517 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:00:20.192658 140575196817216 submission_runner.py:411] Time since start: 203195.79s, 	Step: 420339, 	{'train/accuracy': 0.8887109160423279, 'train/loss': 0.4114374816417694, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 186995.14073944092, 'total_duration': 203195.7936155796, 'accumulated_submission_time': 186995.14073944092, 'accumulated_eval_time': 16149.300919055939, 'accumulated_logging_time': 29.085197925567627}
I0304 20:00:20.275001 140380326586112 logging_writer.py:48] [420339] accumulated_eval_time=16149.300919, accumulated_logging_time=29.085198, accumulated_submission_time=186995.140739, global_step=420339, preemption_count=0, score=186995.140739, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=203195.793616, train/accuracy=0.888711, train/loss=0.411437, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:00:44.929947 140380334978816 logging_writer.py:48] [420400] global_step=420400, grad_norm=2.8565871715545654, loss=1.0788078308105469
I0304 20:01:28.512474 140380326586112 logging_writer.py:48] [420500] global_step=420500, grad_norm=3.4464471340179443, loss=1.1334928274154663
I0304 20:02:13.302728 140380334978816 logging_writer.py:48] [420600] global_step=420600, grad_norm=3.1701300144195557, loss=2.7317304611206055
I0304 20:02:58.388986 140380326586112 logging_writer.py:48] [420700] global_step=420700, grad_norm=3.501880407333374, loss=2.9569671154022217
I0304 20:03:43.500983 140380334978816 logging_writer.py:48] [420800] global_step=420800, grad_norm=3.2919909954071045, loss=1.313524842262268
I0304 20:04:28.804541 140380326586112 logging_writer.py:48] [420900] global_step=420900, grad_norm=3.109605073928833, loss=2.425305128097534
I0304 20:05:13.392529 140380334978816 logging_writer.py:48] [421000] global_step=421000, grad_norm=3.7940542697906494, loss=3.0428569316864014
I0304 20:05:58.160257 140380326586112 logging_writer.py:48] [421100] global_step=421100, grad_norm=3.3193583488464355, loss=2.3260955810546875
I0304 20:06:43.357572 140380334978816 logging_writer.py:48] [421200] global_step=421200, grad_norm=2.9556350708007812, loss=2.4456427097320557
I0304 20:07:20.524333 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:07:31.126135 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:07:56.558811 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:07:58.218816 140575196817216 submission_runner.py:411] Time since start: 203653.82s, 	Step: 421284, 	{'train/accuracy': 0.88978511095047, 'train/loss': 0.41785284876823425, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 187415.33083963394, 'total_duration': 203653.81957530975, 'accumulated_submission_time': 187415.33083963394, 'accumulated_eval_time': 16186.994090795517, 'accumulated_logging_time': 29.1780104637146}
I0304 20:07:58.310755 140380326586112 logging_writer.py:48] [421284] accumulated_eval_time=16186.994091, accumulated_logging_time=29.178010, accumulated_submission_time=187415.330840, global_step=421284, preemption_count=0, score=187415.330840, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=203653.819575, train/accuracy=0.889785, train/loss=0.417853, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:08:05.081115 140380334978816 logging_writer.py:48] [421300] global_step=421300, grad_norm=3.2532427310943604, loss=1.089479684829712
I0304 20:08:45.980726 140380326586112 logging_writer.py:48] [421400] global_step=421400, grad_norm=3.0813729763031006, loss=1.5227797031402588
I0304 20:09:30.907333 140380334978816 logging_writer.py:48] [421500] global_step=421500, grad_norm=2.8122849464416504, loss=1.0827975273132324
I0304 20:10:15.981386 140380326586112 logging_writer.py:48] [421600] global_step=421600, grad_norm=3.172571897506714, loss=1.865839958190918
I0304 20:11:01.473098 140380334978816 logging_writer.py:48] [421700] global_step=421700, grad_norm=3.070679187774658, loss=1.1323994398117065
I0304 20:11:46.420136 140380326586112 logging_writer.py:48] [421800] global_step=421800, grad_norm=3.074200391769409, loss=1.5096460580825806
I0304 20:12:31.582067 140380334978816 logging_writer.py:48] [421900] global_step=421900, grad_norm=3.2421553134918213, loss=1.2220124006271362
I0304 20:13:16.637169 140380326586112 logging_writer.py:48] [422000] global_step=422000, grad_norm=3.4487099647521973, loss=1.4255009889602661
I0304 20:14:01.562364 140380334978816 logging_writer.py:48] [422100] global_step=422100, grad_norm=3.4434444904327393, loss=1.1705100536346436
I0304 20:14:46.528278 140380326586112 logging_writer.py:48] [422200] global_step=422200, grad_norm=3.057286500930786, loss=1.1825135946273804
I0304 20:14:58.419543 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:15:09.207659 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:15:33.592075 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:15:35.248249 140575196817216 submission_runner.py:411] Time since start: 204110.85s, 	Step: 422228, 	{'train/accuracy': 0.8898242115974426, 'train/loss': 0.41243815422058105, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 187835.38063955307, 'total_duration': 204110.84920215607, 'accumulated_submission_time': 187835.38063955307, 'accumulated_eval_time': 16223.821682929993, 'accumulated_logging_time': 29.28029179573059}
I0304 20:15:35.327397 140380334978816 logging_writer.py:48] [422228] accumulated_eval_time=16223.821683, accumulated_logging_time=29.280292, accumulated_submission_time=187835.380640, global_step=422228, preemption_count=0, score=187835.380640, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=204110.849202, train/accuracy=0.889824, train/loss=0.412438, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:16:04.362061 140380326586112 logging_writer.py:48] [422300] global_step=422300, grad_norm=3.427661895751953, loss=3.0183310508728027
I0304 20:16:48.370037 140380334978816 logging_writer.py:48] [422400] global_step=422400, grad_norm=3.5967915058135986, loss=2.5907247066497803
I0304 20:17:33.678518 140380326586112 logging_writer.py:48] [422500] global_step=422500, grad_norm=2.765043258666992, loss=0.9744287729263306
I0304 20:18:18.778451 140380334978816 logging_writer.py:48] [422600] global_step=422600, grad_norm=3.2818257808685303, loss=1.2193864583969116
I0304 20:19:03.645542 140380326586112 logging_writer.py:48] [422700] global_step=422700, grad_norm=4.797597885131836, loss=3.118312358856201
I0304 20:19:48.586031 140380334978816 logging_writer.py:48] [422800] global_step=422800, grad_norm=3.3115227222442627, loss=1.0875111818313599
I0304 20:20:33.806009 140380326586112 logging_writer.py:48] [422900] global_step=422900, grad_norm=3.349210262298584, loss=2.205761432647705
I0304 20:21:18.622884 140380334978816 logging_writer.py:48] [423000] global_step=423000, grad_norm=3.494824171066284, loss=1.2223174571990967
I0304 20:22:03.783019 140380326586112 logging_writer.py:48] [423100] global_step=423100, grad_norm=4.132699489593506, loss=3.268495559692383
I0304 20:22:35.410529 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:22:46.221527 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:23:14.545516 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:23:16.203489 140575196817216 submission_runner.py:411] Time since start: 204571.80s, 	Step: 423172, 	{'train/accuracy': 0.8860546946525574, 'train/loss': 0.4179321825504303, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 188255.40531373024, 'total_duration': 204571.80442857742, 'accumulated_submission_time': 188255.40531373024, 'accumulated_eval_time': 16264.61352443695, 'accumulated_logging_time': 29.368701219558716}
I0304 20:23:16.296211 140380334978816 logging_writer.py:48] [423172] accumulated_eval_time=16264.613524, accumulated_logging_time=29.368701, accumulated_submission_time=188255.405314, global_step=423172, preemption_count=0, score=188255.405314, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=204571.804429, train/accuracy=0.886055, train/loss=0.417932, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:23:27.830803 140380326586112 logging_writer.py:48] [423200] global_step=423200, grad_norm=4.256368637084961, loss=3.3478264808654785
I0304 20:24:09.592070 140380334978816 logging_writer.py:48] [423300] global_step=423300, grad_norm=3.189513683319092, loss=2.124584197998047
I0304 20:24:54.668129 140380326586112 logging_writer.py:48] [423400] global_step=423400, grad_norm=2.855273485183716, loss=1.3619225025177002
I0304 20:25:40.109695 140380334978816 logging_writer.py:48] [423500] global_step=423500, grad_norm=3.233720541000366, loss=1.1792734861373901
I0304 20:26:25.169808 140380326586112 logging_writer.py:48] [423600] global_step=423600, grad_norm=4.305968284606934, loss=1.1752619743347168
I0304 20:27:10.330483 140380334978816 logging_writer.py:48] [423700] global_step=423700, grad_norm=3.7370285987854004, loss=1.223669171333313
I0304 20:27:55.149926 140380326586112 logging_writer.py:48] [423800] global_step=423800, grad_norm=3.174896478652954, loss=1.6919937133789062
I0304 20:28:40.203342 140380334978816 logging_writer.py:48] [423900] global_step=423900, grad_norm=3.500666379928589, loss=1.991729497909546
I0304 20:29:25.441498 140380326586112 logging_writer.py:48] [424000] global_step=424000, grad_norm=3.206559181213379, loss=2.9026730060577393
I0304 20:30:10.371848 140380334978816 logging_writer.py:48] [424100] global_step=424100, grad_norm=3.833540201187134, loss=2.7048399448394775
I0304 20:30:16.321754 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:30:27.359036 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:30:56.165984 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:30:57.813510 140575196817216 submission_runner.py:411] Time since start: 205033.41s, 	Step: 424115, 	{'train/accuracy': 0.8898437023162842, 'train/loss': 0.41486114263534546, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 188675.36987829208, 'total_duration': 205033.4146347046, 'accumulated_submission_time': 188675.36987829208, 'accumulated_eval_time': 16306.104352474213, 'accumulated_logging_time': 29.47391629219055}
I0304 20:30:57.895899 140380326586112 logging_writer.py:48] [424115] accumulated_eval_time=16306.104352, accumulated_logging_time=29.473916, accumulated_submission_time=188675.369878, global_step=424115, preemption_count=0, score=188675.369878, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=205033.414635, train/accuracy=0.889844, train/loss=0.414861, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:31:32.217097 140380334978816 logging_writer.py:48] [424200] global_step=424200, grad_norm=3.764662504196167, loss=3.2303309440612793
I0304 20:32:16.624258 140380326586112 logging_writer.py:48] [424300] global_step=424300, grad_norm=3.1764285564422607, loss=1.1667817831039429
I0304 20:33:01.773047 140380334978816 logging_writer.py:48] [424400] global_step=424400, grad_norm=2.86411190032959, loss=1.0990862846374512
I0304 20:33:47.092314 140380326586112 logging_writer.py:48] [424500] global_step=424500, grad_norm=3.5507707595825195, loss=3.0598301887512207
I0304 20:34:32.030817 140380334978816 logging_writer.py:48] [424600] global_step=424600, grad_norm=3.1773288249969482, loss=1.5359039306640625
I0304 20:35:17.357670 140380326586112 logging_writer.py:48] [424700] global_step=424700, grad_norm=3.0459349155426025, loss=1.1336743831634521
I0304 20:36:02.341421 140380334978816 logging_writer.py:48] [424800] global_step=424800, grad_norm=4.235486030578613, loss=3.0178093910217285
I0304 20:36:47.524091 140380326586112 logging_writer.py:48] [424900] global_step=424900, grad_norm=3.270548105239868, loss=1.0567715167999268
I0304 20:37:32.548979 140380334978816 logging_writer.py:48] [425000] global_step=425000, grad_norm=3.908097743988037, loss=3.0308051109313965
I0304 20:37:58.076032 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:38:08.840266 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:38:36.852088 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:38:38.502495 140575196817216 submission_runner.py:411] Time since start: 205494.10s, 	Step: 425058, 	{'train/accuracy': 0.8862499594688416, 'train/loss': 0.42175033688545227, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 189095.49055552483, 'total_duration': 205494.10353779793, 'accumulated_submission_time': 189095.49055552483, 'accumulated_eval_time': 16346.529787540436, 'accumulated_logging_time': 29.567564010620117}
I0304 20:38:38.594919 140380326586112 logging_writer.py:48] [425058] accumulated_eval_time=16346.529788, accumulated_logging_time=29.567564, accumulated_submission_time=189095.490556, global_step=425058, preemption_count=0, score=189095.490556, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=205494.103538, train/accuracy=0.886250, train/loss=0.421750, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:38:55.680362 140380334978816 logging_writer.py:48] [425100] global_step=425100, grad_norm=4.006513595581055, loss=3.360274076461792
I0304 20:39:38.295888 140380326586112 logging_writer.py:48] [425200] global_step=425200, grad_norm=3.176439046859741, loss=1.173060417175293
I0304 20:40:23.252064 140380334978816 logging_writer.py:48] [425300] global_step=425300, grad_norm=2.9904255867004395, loss=1.0105592012405396
I0304 20:41:08.770530 140380326586112 logging_writer.py:48] [425400] global_step=425400, grad_norm=2.9176108837127686, loss=1.1148232221603394
I0304 20:41:54.017950 140380334978816 logging_writer.py:48] [425500] global_step=425500, grad_norm=3.2689290046691895, loss=2.763158082962036
I0304 20:42:39.173774 140380326586112 logging_writer.py:48] [425600] global_step=425600, grad_norm=3.2375550270080566, loss=1.2488240003585815
I0304 20:43:24.207399 140380334978816 logging_writer.py:48] [425700] global_step=425700, grad_norm=3.0967156887054443, loss=1.1676177978515625
I0304 20:44:09.120055 140380326586112 logging_writer.py:48] [425800] global_step=425800, grad_norm=3.080444574356079, loss=2.3006160259246826
I0304 20:44:54.284229 140380334978816 logging_writer.py:48] [425900] global_step=425900, grad_norm=3.399667501449585, loss=1.1574127674102783
I0304 20:45:38.519505 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:45:49.138724 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:46:15.571330 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:46:17.232397 140575196817216 submission_runner.py:411] Time since start: 205952.83s, 	Step: 426000, 	{'train/accuracy': 0.88818359375, 'train/loss': 0.42022064328193665, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 189515.35760688782, 'total_duration': 205952.8334183693, 'accumulated_submission_time': 189515.35760688782, 'accumulated_eval_time': 16385.24165081978, 'accumulated_logging_time': 29.66967749595642}
I0304 20:46:17.322559 140380326586112 logging_writer.py:48] [426000] accumulated_eval_time=16385.241651, accumulated_logging_time=29.669677, accumulated_submission_time=189515.357607, global_step=426000, preemption_count=0, score=189515.357607, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=205952.833418, train/accuracy=0.888184, train/loss=0.420221, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:46:17.729599 140380334978816 logging_writer.py:48] [426000] global_step=426000, grad_norm=3.444736957550049, loss=1.2499780654907227
I0304 20:46:58.133972 140380326586112 logging_writer.py:48] [426100] global_step=426100, grad_norm=3.527076482772827, loss=2.26639986038208
I0304 20:47:42.913740 140380334978816 logging_writer.py:48] [426200] global_step=426200, grad_norm=3.1616616249084473, loss=1.5992790460586548
I0304 20:48:27.952235 140380326586112 logging_writer.py:48] [426300] global_step=426300, grad_norm=2.961374521255493, loss=1.5293400287628174
I0304 20:49:12.727447 140380334978816 logging_writer.py:48] [426400] global_step=426400, grad_norm=2.9660825729370117, loss=1.144863247871399
I0304 20:49:58.019810 140380326586112 logging_writer.py:48] [426500] global_step=426500, grad_norm=2.9050228595733643, loss=1.6495611667633057
I0304 20:50:43.035940 140380334978816 logging_writer.py:48] [426600] global_step=426600, grad_norm=3.3000247478485107, loss=1.205077886581421
I0304 20:51:28.287821 140380326586112 logging_writer.py:48] [426700] global_step=426700, grad_norm=3.1969425678253174, loss=1.0790917873382568
I0304 20:52:13.395266 140380334978816 logging_writer.py:48] [426800] global_step=426800, grad_norm=2.836710214614868, loss=1.1206715106964111
I0304 20:52:58.372810 140380326586112 logging_writer.py:48] [426900] global_step=426900, grad_norm=3.352451801300049, loss=1.2043336629867554
I0304 20:53:17.429246 140575196817216 spec.py:321] Evaluating on the training split.
I0304 20:53:28.189065 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 20:53:53.161602 140575196817216 spec.py:349] Evaluating on the test split.
I0304 20:53:54.820353 140575196817216 submission_runner.py:411] Time since start: 206410.42s, 	Step: 426944, 	{'train/accuracy': 0.8882812261581421, 'train/loss': 0.41747844219207764, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 189935.40386390686, 'total_duration': 206410.42110538483, 'accumulated_submission_time': 189935.40386390686, 'accumulated_eval_time': 16422.631457805634, 'accumulated_logging_time': 29.771843910217285}
I0304 20:53:54.902699 140380334978816 logging_writer.py:48] [426944] accumulated_eval_time=16422.631458, accumulated_logging_time=29.771844, accumulated_submission_time=189935.403864, global_step=426944, preemption_count=0, score=189935.403864, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=206410.421105, train/accuracy=0.888281, train/loss=0.417478, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 20:54:17.548451 140380326586112 logging_writer.py:48] [427000] global_step=427000, grad_norm=3.1106619834899902, loss=1.1788662672042847
I0304 20:55:00.579564 140380334978816 logging_writer.py:48] [427100] global_step=427100, grad_norm=3.4226937294006348, loss=2.707339286804199
I0304 20:55:46.027832 140380326586112 logging_writer.py:48] [427200] global_step=427200, grad_norm=3.1383135318756104, loss=2.587461471557617
I0304 20:56:31.266797 140380334978816 logging_writer.py:48] [427300] global_step=427300, grad_norm=2.971524477005005, loss=1.0813095569610596
I0304 20:57:16.197967 140380326586112 logging_writer.py:48] [427400] global_step=427400, grad_norm=3.085463285446167, loss=1.065752387046814
I0304 20:58:01.348967 140380334978816 logging_writer.py:48] [427500] global_step=427500, grad_norm=3.5877110958099365, loss=1.6560460329055786
I0304 20:58:46.295657 140380326586112 logging_writer.py:48] [427600] global_step=427600, grad_norm=3.25809383392334, loss=1.098313570022583
I0304 20:59:31.868010 140380334978816 logging_writer.py:48] [427700] global_step=427700, grad_norm=3.257481575012207, loss=1.13992440700531
I0304 21:00:17.077609 140380326586112 logging_writer.py:48] [427800] global_step=427800, grad_norm=3.2101194858551025, loss=1.1426947116851807
I0304 21:00:55.211548 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:01:05.946479 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:01:33.004919 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:01:34.662323 140575196817216 submission_runner.py:411] Time since start: 206870.26s, 	Step: 427886, 	{'train/accuracy': 0.8866015672683716, 'train/loss': 0.42252814769744873, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 190355.65117049217, 'total_duration': 206870.26315903664, 'accumulated_submission_time': 190355.65117049217, 'accumulated_eval_time': 16462.080998659134, 'accumulated_logging_time': 29.867147207260132}
I0304 21:01:34.753715 140380334978816 logging_writer.py:48] [427886] accumulated_eval_time=16462.080999, accumulated_logging_time=29.867147, accumulated_submission_time=190355.651170, global_step=427886, preemption_count=0, score=190355.651170, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=206870.263159, train/accuracy=0.886602, train/loss=0.422528, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:01:40.711126 140380326586112 logging_writer.py:48] [427900] global_step=427900, grad_norm=3.3107802867889404, loss=1.0854480266571045
I0304 21:02:21.805611 140380334978816 logging_writer.py:48] [428000] global_step=428000, grad_norm=2.8181443214416504, loss=1.5742652416229248
I0304 21:03:06.550736 140380326586112 logging_writer.py:48] [428100] global_step=428100, grad_norm=3.258197546005249, loss=1.207625389099121
I0304 21:03:51.774640 140380334978816 logging_writer.py:48] [428200] global_step=428200, grad_norm=3.00417160987854, loss=1.1734064817428589
I0304 21:04:36.729594 140380326586112 logging_writer.py:48] [428300] global_step=428300, grad_norm=2.993391513824463, loss=1.0815571546554565
I0304 21:05:22.065935 140380334978816 logging_writer.py:48] [428400] global_step=428400, grad_norm=2.9383621215820312, loss=1.107545256614685
I0304 21:06:07.096808 140380326586112 logging_writer.py:48] [428500] global_step=428500, grad_norm=3.2922894954681396, loss=2.7467474937438965
I0304 21:06:52.250022 140380334978816 logging_writer.py:48] [428600] global_step=428600, grad_norm=3.0977234840393066, loss=1.179730772972107
I0304 21:07:37.258499 140380326586112 logging_writer.py:48] [428700] global_step=428700, grad_norm=3.0216054916381836, loss=1.9881203174591064
I0304 21:08:22.307338 140380334978816 logging_writer.py:48] [428800] global_step=428800, grad_norm=4.265048980712891, loss=3.135361433029175
I0304 21:08:35.079294 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:08:45.683052 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:09:14.066400 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:09:15.710783 140575196817216 submission_runner.py:411] Time since start: 207331.31s, 	Step: 428830, 	{'train/accuracy': 0.8874022960662842, 'train/loss': 0.41887399554252625, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 190775.91886878014, 'total_duration': 207331.3117365837, 'accumulated_submission_time': 190775.91886878014, 'accumulated_eval_time': 16502.711376667023, 'accumulated_logging_time': 29.968742847442627}
I0304 21:09:15.792145 140380326586112 logging_writer.py:48] [428830] accumulated_eval_time=16502.711377, accumulated_logging_time=29.968743, accumulated_submission_time=190775.918869, global_step=428830, preemption_count=0, score=190775.918869, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=207331.311737, train/accuracy=0.887402, train/loss=0.418874, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:09:44.013272 140380334978816 logging_writer.py:48] [428900] global_step=428900, grad_norm=3.0733203887939453, loss=1.0460681915283203
I0304 21:10:28.006588 140380326586112 logging_writer.py:48] [429000] global_step=429000, grad_norm=3.24281644821167, loss=1.9813953638076782
I0304 21:11:12.757901 140380334978816 logging_writer.py:48] [429100] global_step=429100, grad_norm=3.4105098247528076, loss=1.184338927268982
I0304 21:11:58.056939 140380326586112 logging_writer.py:48] [429200] global_step=429200, grad_norm=3.981844663619995, loss=2.7374393939971924
I0304 21:12:43.017361 140380334978816 logging_writer.py:48] [429300] global_step=429300, grad_norm=3.3704516887664795, loss=2.651251792907715
I0304 21:13:28.124413 140380326586112 logging_writer.py:48] [429400] global_step=429400, grad_norm=3.317213535308838, loss=1.1255041360855103
I0304 21:14:13.147459 140380334978816 logging_writer.py:48] [429500] global_step=429500, grad_norm=3.0690064430236816, loss=1.6473990678787231
I0304 21:14:58.175320 140380326586112 logging_writer.py:48] [429600] global_step=429600, grad_norm=2.9498541355133057, loss=1.217405080795288
I0304 21:15:43.114122 140380334978816 logging_writer.py:48] [429700] global_step=429700, grad_norm=2.865985155105591, loss=1.6795850992202759
I0304 21:16:15.767178 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:16:26.509207 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:16:53.557055 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:16:55.212699 140575196817216 submission_runner.py:411] Time since start: 207790.81s, 	Step: 429774, 	{'train/accuracy': 0.8849608898162842, 'train/loss': 0.4287216365337372, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 191195.83547329903, 'total_duration': 207790.8137485981, 'accumulated_submission_time': 191195.83547329903, 'accumulated_eval_time': 16542.15592098236, 'accumulated_logging_time': 30.059664249420166}
I0304 21:16:55.305985 140380326586112 logging_writer.py:48] [429774] accumulated_eval_time=16542.155921, accumulated_logging_time=30.059664, accumulated_submission_time=191195.835473, global_step=429774, preemption_count=0, score=191195.835473, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=207790.813749, train/accuracy=0.884961, train/loss=0.428722, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:17:06.042559 140380334978816 logging_writer.py:48] [429800] global_step=429800, grad_norm=3.1446266174316406, loss=1.0699392557144165
I0304 21:17:47.777504 140380326586112 logging_writer.py:48] [429900] global_step=429900, grad_norm=2.9522182941436768, loss=1.9420528411865234
I0304 21:18:32.577082 140380334978816 logging_writer.py:48] [430000] global_step=430000, grad_norm=3.2040326595306396, loss=1.1815567016601562
I0304 21:19:17.876392 140380326586112 logging_writer.py:48] [430100] global_step=430100, grad_norm=2.861393928527832, loss=0.9912264347076416
I0304 21:20:02.735535 140380334978816 logging_writer.py:48] [430200] global_step=430200, grad_norm=3.4725403785705566, loss=3.033099889755249
I0304 21:20:47.563384 140380326586112 logging_writer.py:48] [430300] global_step=430300, grad_norm=2.998565196990967, loss=2.0528664588928223
I0304 21:21:33.110658 140380334978816 logging_writer.py:48] [430400] global_step=430400, grad_norm=3.6686289310455322, loss=2.9572746753692627
I0304 21:22:18.303940 140380326586112 logging_writer.py:48] [430500] global_step=430500, grad_norm=3.1903076171875, loss=1.0786356925964355
I0304 21:23:03.197514 140380334978816 logging_writer.py:48] [430600] global_step=430600, grad_norm=3.0209105014801025, loss=1.2690865993499756
I0304 21:23:48.228467 140380326586112 logging_writer.py:48] [430700] global_step=430700, grad_norm=3.17564058303833, loss=1.13063383102417
I0304 21:23:55.521148 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:24:06.255999 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:24:32.988289 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:24:34.653991 140575196817216 submission_runner.py:411] Time since start: 208250.25s, 	Step: 430718, 	{'train/accuracy': 0.8863085508346558, 'train/loss': 0.4251331090927124, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 191615.99197554588, 'total_duration': 208250.25497484207, 'accumulated_submission_time': 191615.99197554588, 'accumulated_eval_time': 16581.287677764893, 'accumulated_logging_time': 30.16299033164978}
I0304 21:24:34.751650 140380334978816 logging_writer.py:48] [430718] accumulated_eval_time=16581.287678, accumulated_logging_time=30.162990, accumulated_submission_time=191615.991976, global_step=430718, preemption_count=0, score=191615.991976, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=208250.254975, train/accuracy=0.886309, train/loss=0.425133, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:25:07.746257 140380326586112 logging_writer.py:48] [430800] global_step=430800, grad_norm=3.268343687057495, loss=1.1478335857391357
I0304 21:25:52.245457 140380334978816 logging_writer.py:48] [430900] global_step=430900, grad_norm=4.978825092315674, loss=1.6665763854980469
I0304 21:26:37.604914 140380326586112 logging_writer.py:48] [431000] global_step=431000, grad_norm=3.0167081356048584, loss=1.445208191871643
I0304 21:27:22.712513 140380334978816 logging_writer.py:48] [431100] global_step=431100, grad_norm=2.948357582092285, loss=1.0451819896697998
I0304 21:28:07.955727 140380326586112 logging_writer.py:48] [431200] global_step=431200, grad_norm=3.0726797580718994, loss=1.152823567390442
I0304 21:28:52.899496 140380334978816 logging_writer.py:48] [431300] global_step=431300, grad_norm=3.2030208110809326, loss=2.5288867950439453
I0304 21:29:38.041100 140380326586112 logging_writer.py:48] [431400] global_step=431400, grad_norm=2.865277051925659, loss=2.4514644145965576
I0304 21:30:23.010484 140380334978816 logging_writer.py:48] [431500] global_step=431500, grad_norm=4.0448079109191895, loss=3.3167479038238525
I0304 21:31:08.182548 140380326586112 logging_writer.py:48] [431600] global_step=431600, grad_norm=3.0193605422973633, loss=1.2944929599761963
I0304 21:31:35.047601 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:31:45.658835 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:32:15.704710 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:32:17.359566 140575196817216 submission_runner.py:411] Time since start: 208712.96s, 	Step: 431661, 	{'train/accuracy': 0.8882616758346558, 'train/loss': 0.4132554531097412, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 192036.22898197174, 'total_duration': 208712.96063542366, 'accumulated_submission_time': 192036.22898197174, 'accumulated_eval_time': 16623.598644971848, 'accumulated_logging_time': 30.271098613739014}
I0304 21:32:17.440996 140380334978816 logging_writer.py:48] [431661] accumulated_eval_time=16623.598645, accumulated_logging_time=30.271099, accumulated_submission_time=192036.228982, global_step=431661, preemption_count=0, score=192036.228982, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=208712.960635, train/accuracy=0.888262, train/loss=0.413255, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:32:33.344585 140380326586112 logging_writer.py:48] [431700] global_step=431700, grad_norm=3.5529634952545166, loss=2.989760637283325
I0304 21:33:15.605707 140380334978816 logging_writer.py:48] [431800] global_step=431800, grad_norm=3.0726170539855957, loss=2.7387022972106934
I0304 21:34:00.617407 140380326586112 logging_writer.py:48] [431900] global_step=431900, grad_norm=3.495729684829712, loss=1.7116979360580444
I0304 21:34:46.040286 140380334978816 logging_writer.py:48] [432000] global_step=432000, grad_norm=3.527728796005249, loss=2.7680258750915527
I0304 21:35:31.112324 140380326586112 logging_writer.py:48] [432100] global_step=432100, grad_norm=3.133504867553711, loss=1.136484980583191
I0304 21:36:16.253106 140380334978816 logging_writer.py:48] [432200] global_step=432200, grad_norm=3.9134349822998047, loss=3.149794340133667
I0304 21:37:01.569313 140380326586112 logging_writer.py:48] [432300] global_step=432300, grad_norm=3.8858330249786377, loss=1.7210477590560913
I0304 21:37:46.679440 140380334978816 logging_writer.py:48] [432400] global_step=432400, grad_norm=3.3399932384490967, loss=2.0840559005737305
I0304 21:38:31.764693 140380326586112 logging_writer.py:48] [432500] global_step=432500, grad_norm=2.996760845184326, loss=1.6382737159729004
I0304 21:39:16.955950 140380334978816 logging_writer.py:48] [432600] global_step=432600, grad_norm=2.909219741821289, loss=1.3334667682647705
I0304 21:39:17.555366 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:39:28.442717 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:39:56.564233 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:39:58.205854 140575196817216 submission_runner.py:411] Time since start: 209173.81s, 	Step: 432603, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.41813334822654724, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 192456.2843079567, 'total_duration': 209173.80654668808, 'accumulated_submission_time': 192456.2843079567, 'accumulated_eval_time': 16664.247750520706, 'accumulated_logging_time': 30.363117218017578}
I0304 21:39:58.287492 140380326586112 logging_writer.py:48] [432603] accumulated_eval_time=16664.247751, accumulated_logging_time=30.363117, accumulated_submission_time=192456.284308, global_step=432603, preemption_count=0, score=192456.284308, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=209173.806547, train/accuracy=0.888516, train/loss=0.418133, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:40:37.711596 140380334978816 logging_writer.py:48] [432700] global_step=432700, grad_norm=3.0411767959594727, loss=1.2033270597457886
I0304 21:41:22.525225 140380326586112 logging_writer.py:48] [432800] global_step=432800, grad_norm=2.908700466156006, loss=1.2355624437332153
I0304 21:42:07.760125 140380334978816 logging_writer.py:48] [432900] global_step=432900, grad_norm=3.1199991703033447, loss=1.882172703742981
I0304 21:42:52.811038 140380326586112 logging_writer.py:48] [433000] global_step=433000, grad_norm=3.9128921031951904, loss=3.225473642349243
I0304 21:43:37.599570 140380334978816 logging_writer.py:48] [433100] global_step=433100, grad_norm=3.3343381881713867, loss=1.4501583576202393
I0304 21:44:22.591665 140380326586112 logging_writer.py:48] [433200] global_step=433200, grad_norm=2.852184534072876, loss=1.5120458602905273
I0304 21:45:07.503238 140380334978816 logging_writer.py:48] [433300] global_step=433300, grad_norm=3.24181866645813, loss=1.077811360359192
I0304 21:45:52.450058 140380326586112 logging_writer.py:48] [433400] global_step=433400, grad_norm=3.357891321182251, loss=1.1623082160949707
I0304 21:46:37.649019 140380334978816 logging_writer.py:48] [433500] global_step=433500, grad_norm=4.051286220550537, loss=1.1048191785812378
I0304 21:46:58.389191 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:47:09.030956 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:47:35.853482 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:47:37.521369 140575196817216 submission_runner.py:411] Time since start: 209633.12s, 	Step: 433548, 	{'train/accuracy': 0.8854687213897705, 'train/loss': 0.42374882102012634, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 192876.3270497322, 'total_duration': 209633.1222922802, 'accumulated_submission_time': 192876.3270497322, 'accumulated_eval_time': 16703.37880063057, 'accumulated_logging_time': 30.455291032791138}
I0304 21:47:37.613201 140380326586112 logging_writer.py:48] [433548] accumulated_eval_time=16703.378801, accumulated_logging_time=30.455291, accumulated_submission_time=192876.327050, global_step=433548, preemption_count=0, score=192876.327050, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=209633.122292, train/accuracy=0.885469, train/loss=0.423749, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:47:58.679879 140380334978816 logging_writer.py:48] [433600] global_step=433600, grad_norm=4.034083843231201, loss=3.2632360458374023
I0304 21:48:41.494013 140380326586112 logging_writer.py:48] [433700] global_step=433700, grad_norm=2.8517379760742188, loss=2.225442886352539
I0304 21:49:26.574021 140380334978816 logging_writer.py:48] [433800] global_step=433800, grad_norm=3.5015735626220703, loss=1.6978271007537842
I0304 21:50:11.880778 140380326586112 logging_writer.py:48] [433900] global_step=433900, grad_norm=3.300936698913574, loss=1.3901910781860352
I0304 21:50:56.658599 140380334978816 logging_writer.py:48] [434000] global_step=434000, grad_norm=3.0689873695373535, loss=1.483923077583313
I0304 21:51:41.545408 140380326586112 logging_writer.py:48] [434100] global_step=434100, grad_norm=2.837986946105957, loss=1.0284560918807983
I0304 21:52:26.811490 140380334978816 logging_writer.py:48] [434200] global_step=434200, grad_norm=3.182804584503174, loss=1.0350432395935059
I0304 21:53:11.654263 140380326586112 logging_writer.py:48] [434300] global_step=434300, grad_norm=3.068608045578003, loss=1.1843392848968506
I0304 21:53:56.766001 140380334978816 logging_writer.py:48] [434400] global_step=434400, grad_norm=3.3092849254608154, loss=3.036569595336914
I0304 21:54:37.897763 140575196817216 spec.py:321] Evaluating on the training split.
I0304 21:54:48.679689 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 21:55:16.441868 140575196817216 spec.py:349] Evaluating on the test split.
I0304 21:55:18.098237 140575196817216 submission_runner.py:411] Time since start: 210093.70s, 	Step: 434493, 	{'train/accuracy': 0.8869531154632568, 'train/loss': 0.41741934418678284, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 193296.5520606041, 'total_duration': 210093.69893980026, 'accumulated_submission_time': 193296.5520606041, 'accumulated_eval_time': 16743.577922344208, 'accumulated_logging_time': 30.5573947429657}
I0304 21:55:18.183500 140380326586112 logging_writer.py:48] [434493] accumulated_eval_time=16743.577922, accumulated_logging_time=30.557395, accumulated_submission_time=193296.552061, global_step=434493, preemption_count=0, score=193296.552061, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=210093.698940, train/accuracy=0.886953, train/loss=0.417419, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 21:55:21.368091 140380334978816 logging_writer.py:48] [434500] global_step=434500, grad_norm=2.9488444328308105, loss=1.0255441665649414
I0304 21:56:02.036296 140380326586112 logging_writer.py:48] [434600] global_step=434600, grad_norm=3.334749698638916, loss=1.739474892616272
I0304 21:56:47.208276 140380334978816 logging_writer.py:48] [434700] global_step=434700, grad_norm=2.938275098800659, loss=1.9684044122695923
I0304 21:57:32.508064 140380326586112 logging_writer.py:48] [434800] global_step=434800, grad_norm=3.1116385459899902, loss=1.0585300922393799
I0304 21:58:17.576076 140380334978816 logging_writer.py:48] [434900] global_step=434900, grad_norm=3.2092130184173584, loss=1.1827696561813354
I0304 21:59:02.740429 140380326586112 logging_writer.py:48] [435000] global_step=435000, grad_norm=3.5545053482055664, loss=2.954904079437256
I0304 21:59:47.660386 140380334978816 logging_writer.py:48] [435100] global_step=435100, grad_norm=3.503204107284546, loss=2.641066551208496
I0304 22:00:32.652338 140380326586112 logging_writer.py:48] [435200] global_step=435200, grad_norm=3.6556057929992676, loss=2.955636501312256
I0304 22:01:17.750862 140380334978816 logging_writer.py:48] [435300] global_step=435300, grad_norm=3.1083481311798096, loss=2.4348514080047607
I0304 22:02:02.974706 140380326586112 logging_writer.py:48] [435400] global_step=435400, grad_norm=3.417935371398926, loss=1.053494930267334
I0304 22:02:18.112095 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:02:29.299509 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:02:57.532933 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:02:59.178076 140575196817216 submission_runner.py:411] Time since start: 210554.78s, 	Step: 435435, 	{'train/accuracy': 0.8883007764816284, 'train/loss': 0.4161953330039978, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 193716.42161488533, 'total_duration': 210554.77907276154, 'accumulated_submission_time': 193716.42161488533, 'accumulated_eval_time': 16784.6428296566, 'accumulated_logging_time': 30.652455806732178}
I0304 22:02:59.268993 140380334978816 logging_writer.py:48] [435435] accumulated_eval_time=16784.642830, accumulated_logging_time=30.652456, accumulated_submission_time=193716.421615, global_step=435435, preemption_count=0, score=193716.421615, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=210554.779073, train/accuracy=0.888301, train/loss=0.416195, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:03:25.481511 140380326586112 logging_writer.py:48] [435500] global_step=435500, grad_norm=4.023340225219727, loss=2.5938851833343506
I0304 22:04:09.389304 140380334978816 logging_writer.py:48] [435600] global_step=435600, grad_norm=4.138628005981445, loss=3.007276773452759
I0304 22:04:54.393690 140380326586112 logging_writer.py:48] [435700] global_step=435700, grad_norm=3.307333469390869, loss=1.2266919612884521
I0304 22:05:39.557242 140380334978816 logging_writer.py:48] [435800] global_step=435800, grad_norm=4.7329277992248535, loss=3.3418641090393066
I0304 22:06:24.683391 140380326586112 logging_writer.py:48] [435900] global_step=435900, grad_norm=3.2129898071289062, loss=1.1445392370224
I0304 22:07:09.916118 140380334978816 logging_writer.py:48] [436000] global_step=436000, grad_norm=3.2670533657073975, loss=1.2483412027359009
I0304 22:07:54.988358 140380326586112 logging_writer.py:48] [436100] global_step=436100, grad_norm=3.194023609161377, loss=2.7009332180023193
I0304 22:08:39.941093 140380334978816 logging_writer.py:48] [436200] global_step=436200, grad_norm=2.8972959518432617, loss=1.133606195449829
I0304 22:09:25.119984 140380326586112 logging_writer.py:48] [436300] global_step=436300, grad_norm=3.1288180351257324, loss=1.0807791948318481
I0304 22:09:59.555164 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:10:10.396902 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:10:42.567333 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:10:44.221762 140575196817216 submission_runner.py:411] Time since start: 211019.82s, 	Step: 436378, 	{'train/accuracy': 0.8877539038658142, 'train/loss': 0.42314401268959045, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 194136.64938545227, 'total_duration': 211019.82277274132, 'accumulated_submission_time': 194136.64938545227, 'accumulated_eval_time': 16829.308379650116, 'accumulated_logging_time': 30.753374576568604}
I0304 22:10:44.313234 140380334978816 logging_writer.py:48] [436378] accumulated_eval_time=16829.308380, accumulated_logging_time=30.753375, accumulated_submission_time=194136.649385, global_step=436378, preemption_count=0, score=194136.649385, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=211019.822773, train/accuracy=0.887754, train/loss=0.423144, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:10:53.461565 140380326586112 logging_writer.py:48] [436400] global_step=436400, grad_norm=3.1817219257354736, loss=2.6490962505340576
I0304 22:11:35.131985 140380334978816 logging_writer.py:48] [436500] global_step=436500, grad_norm=3.710664987564087, loss=3.1280298233032227
I0304 22:12:19.939347 140380326586112 logging_writer.py:48] [436600] global_step=436600, grad_norm=3.2619681358337402, loss=1.1417841911315918
I0304 22:13:05.178284 140380334978816 logging_writer.py:48] [436700] global_step=436700, grad_norm=3.8364052772521973, loss=3.2446720600128174
I0304 22:13:50.461594 140380326586112 logging_writer.py:48] [436800] global_step=436800, grad_norm=3.101884603500366, loss=1.0927923917770386
I0304 22:14:35.180725 140380334978816 logging_writer.py:48] [436900] global_step=436900, grad_norm=2.9853856563568115, loss=1.144405722618103
I0304 22:15:20.384044 140380326586112 logging_writer.py:48] [437000] global_step=437000, grad_norm=3.5924289226531982, loss=2.0376763343811035
I0304 22:16:05.390048 140380334978816 logging_writer.py:48] [437100] global_step=437100, grad_norm=3.0047054290771484, loss=2.0812931060791016
I0304 22:16:50.334646 140380326586112 logging_writer.py:48] [437200] global_step=437200, grad_norm=4.2372331619262695, loss=3.174072027206421
I0304 22:17:35.411095 140380334978816 logging_writer.py:48] [437300] global_step=437300, grad_norm=2.8426554203033447, loss=1.3176733255386353
I0304 22:17:44.636882 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:17:55.363911 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:18:19.362605 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:18:21.015268 140575196817216 submission_runner.py:411] Time since start: 211476.62s, 	Step: 437322, 	{'train/accuracy': 0.8863866925239563, 'train/loss': 0.4232368469238281, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 194556.91301631927, 'total_duration': 211476.6163315773, 'accumulated_submission_time': 194556.91301631927, 'accumulated_eval_time': 16865.685774326324, 'accumulated_logging_time': 30.85575246810913}
I0304 22:18:21.097870 140380326586112 logging_writer.py:48] [437322] accumulated_eval_time=16865.685774, accumulated_logging_time=30.855752, accumulated_submission_time=194556.913016, global_step=437322, preemption_count=0, score=194556.913016, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=211476.616332, train/accuracy=0.886387, train/loss=0.423237, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:18:52.491600 140380334978816 logging_writer.py:48] [437400] global_step=437400, grad_norm=3.17712140083313, loss=1.8075180053710938
I0304 22:19:36.472341 140380326586112 logging_writer.py:48] [437500] global_step=437500, grad_norm=3.2264328002929688, loss=1.350555658340454
I0304 22:20:21.292493 140380334978816 logging_writer.py:48] [437600] global_step=437600, grad_norm=3.5694546699523926, loss=2.8154473304748535
I0304 22:21:06.265696 140380326586112 logging_writer.py:48] [437700] global_step=437700, grad_norm=2.9601688385009766, loss=1.0731340646743774
I0304 22:21:50.766474 140380334978816 logging_writer.py:48] [437800] global_step=437800, grad_norm=3.645259141921997, loss=3.3183953762054443
I0304 22:22:36.163572 140380326586112 logging_writer.py:48] [437900] global_step=437900, grad_norm=3.734382390975952, loss=2.764326572418213
I0304 22:23:21.268476 140380334978816 logging_writer.py:48] [438000] global_step=438000, grad_norm=3.417772054672241, loss=2.780141592025757
I0304 22:24:06.408439 140380326586112 logging_writer.py:48] [438100] global_step=438100, grad_norm=3.3390841484069824, loss=1.1682167053222656
I0304 22:24:51.644062 140380334978816 logging_writer.py:48] [438200] global_step=438200, grad_norm=3.1103320121765137, loss=1.112839937210083
I0304 22:25:21.286399 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:25:31.870992 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:25:58.774773 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:26:00.430381 140575196817216 submission_runner.py:411] Time since start: 211936.03s, 	Step: 438268, 	{'train/accuracy': 0.8855273127555847, 'train/loss': 0.42439004778862, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 194977.04292821884, 'total_duration': 211936.03138518333, 'accumulated_submission_time': 194977.04292821884, 'accumulated_eval_time': 16904.828684091568, 'accumulated_logging_time': 30.9488582611084}
I0304 22:26:00.512182 140380326586112 logging_writer.py:48] [438268] accumulated_eval_time=16904.828684, accumulated_logging_time=30.948858, accumulated_submission_time=194977.042928, global_step=438268, preemption_count=0, score=194977.042928, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=211936.031385, train/accuracy=0.885527, train/loss=0.424390, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:26:13.626402 140380334978816 logging_writer.py:48] [438300] global_step=438300, grad_norm=3.1110448837280273, loss=1.8106257915496826
I0304 22:26:55.619666 140380326586112 logging_writer.py:48] [438400] global_step=438400, grad_norm=3.3648264408111572, loss=2.216545820236206
I0304 22:27:40.496409 140380334978816 logging_writer.py:48] [438500] global_step=438500, grad_norm=3.565403699874878, loss=2.0793709754943848
I0304 22:28:25.696384 140380326586112 logging_writer.py:48] [438600] global_step=438600, grad_norm=2.9184863567352295, loss=1.0734089612960815
I0304 22:29:10.324132 140380334978816 logging_writer.py:48] [438700] global_step=438700, grad_norm=3.4035565853118896, loss=1.1558408737182617
I0304 22:29:55.454595 140380326586112 logging_writer.py:48] [438800] global_step=438800, grad_norm=3.88382887840271, loss=3.103597402572632
I0304 22:30:40.443986 140380334978816 logging_writer.py:48] [438900] global_step=438900, grad_norm=3.482576608657837, loss=2.685746669769287
I0304 22:31:25.629352 140380326586112 logging_writer.py:48] [439000] global_step=439000, grad_norm=4.036092758178711, loss=1.4958422183990479
I0304 22:32:10.892021 140380334978816 logging_writer.py:48] [439100] global_step=439100, grad_norm=3.372561454772949, loss=2.015089273452759
I0304 22:32:56.042630 140380326586112 logging_writer.py:48] [439200] global_step=439200, grad_norm=3.090871572494507, loss=2.571397304534912
I0304 22:33:00.698198 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:33:11.414770 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:33:38.551022 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:33:40.206058 140575196817216 submission_runner.py:411] Time since start: 212395.81s, 	Step: 439212, 	{'train/accuracy': 0.8867773413658142, 'train/loss': 0.4227740466594696, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 195397.16938996315, 'total_duration': 212395.8071053028, 'accumulated_submission_time': 195397.16938996315, 'accumulated_eval_time': 16944.33552479744, 'accumulated_logging_time': 31.040956497192383}
I0304 22:33:40.299573 140380334978816 logging_writer.py:48] [439212] accumulated_eval_time=16944.335525, accumulated_logging_time=31.040956, accumulated_submission_time=195397.169390, global_step=439212, preemption_count=0, score=195397.169390, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=212395.807105, train/accuracy=0.886777, train/loss=0.422774, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:34:15.815333 140380326586112 logging_writer.py:48] [439300] global_step=439300, grad_norm=3.0826239585876465, loss=1.4599320888519287
I0304 22:35:00.261593 140380334978816 logging_writer.py:48] [439400] global_step=439400, grad_norm=8.598316192626953, loss=2.02441668510437
I0304 22:35:45.424488 140380326586112 logging_writer.py:48] [439500] global_step=439500, grad_norm=4.41245698928833, loss=3.091325521469116
I0304 22:36:30.606170 140380334978816 logging_writer.py:48] [439600] global_step=439600, grad_norm=3.0115275382995605, loss=1.0703171491622925
I0304 22:37:15.426552 140380326586112 logging_writer.py:48] [439700] global_step=439700, grad_norm=3.44500994682312, loss=2.6066763401031494
I0304 22:38:00.477077 140380334978816 logging_writer.py:48] [439800] global_step=439800, grad_norm=2.9732866287231445, loss=1.6442866325378418
I0304 22:38:45.288772 140380326586112 logging_writer.py:48] [439900] global_step=439900, grad_norm=3.220907688140869, loss=1.1934142112731934
I0304 22:39:30.554156 140380334978816 logging_writer.py:48] [440000] global_step=440000, grad_norm=3.343461751937866, loss=2.6341800689697266
I0304 22:40:15.658337 140380326586112 logging_writer.py:48] [440100] global_step=440100, grad_norm=3.313962697982788, loss=1.2859617471694946
I0304 22:40:40.579501 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:40:51.281206 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:41:20.072941 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:41:21.737131 140575196817216 submission_runner.py:411] Time since start: 212857.34s, 	Step: 440157, 	{'train/accuracy': 0.8872460722923279, 'train/loss': 0.41813284158706665, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 195817.39040994644, 'total_duration': 212857.33811616898, 'accumulated_submission_time': 195817.39040994644, 'accumulated_eval_time': 16985.492069721222, 'accumulated_logging_time': 31.14467477798462}
I0304 22:41:21.828485 140380334978816 logging_writer.py:48] [440157] accumulated_eval_time=16985.492070, accumulated_logging_time=31.144675, accumulated_submission_time=195817.390410, global_step=440157, preemption_count=0, score=195817.390410, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=212857.338116, train/accuracy=0.887246, train/loss=0.418133, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:41:39.305904 140380326586112 logging_writer.py:48] [440200] global_step=440200, grad_norm=2.7392494678497314, loss=1.3153512477874756
I0304 22:42:22.152102 140380334978816 logging_writer.py:48] [440300] global_step=440300, grad_norm=3.3922760486602783, loss=1.1192810535430908
I0304 22:43:07.123956 140380326586112 logging_writer.py:48] [440400] global_step=440400, grad_norm=3.3953771591186523, loss=2.981687307357788
I0304 22:43:52.497628 140380334978816 logging_writer.py:48] [440500] global_step=440500, grad_norm=3.3432347774505615, loss=1.3572295904159546
I0304 22:44:37.850758 140380326586112 logging_writer.py:48] [440600] global_step=440600, grad_norm=4.728646755218506, loss=3.0789663791656494
I0304 22:45:22.969530 140380334978816 logging_writer.py:48] [440700] global_step=440700, grad_norm=2.9519906044006348, loss=1.3615365028381348
I0304 22:46:08.248944 140380326586112 logging_writer.py:48] [440800] global_step=440800, grad_norm=3.001326084136963, loss=2.039551258087158
I0304 22:46:53.159816 140380334978816 logging_writer.py:48] [440900] global_step=440900, grad_norm=3.190615177154541, loss=1.1382954120635986
I0304 22:47:38.108373 140380326586112 logging_writer.py:48] [441000] global_step=441000, grad_norm=3.2305526733398438, loss=2.910109758377075
I0304 22:48:21.866744 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:48:32.642444 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:49:01.772409 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:49:03.450911 140575196817216 submission_runner.py:411] Time since start: 213319.05s, 	Step: 441099, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.4193216562271118, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 196237.36933875084, 'total_duration': 213319.05184864998, 'accumulated_submission_time': 196237.36933875084, 'accumulated_eval_time': 17027.075122833252, 'accumulated_logging_time': 31.246669054031372}
I0304 22:49:03.535767 140380334978816 logging_writer.py:48] [441099] accumulated_eval_time=17027.075123, accumulated_logging_time=31.246669, accumulated_submission_time=196237.369339, global_step=441099, preemption_count=0, score=196237.369339, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=213319.051849, train/accuracy=0.887812, train/loss=0.419322, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:49:04.337750 140380326586112 logging_writer.py:48] [441100] global_step=441100, grad_norm=3.6392064094543457, loss=2.8614108562469482
I0304 22:49:44.542115 140380334978816 logging_writer.py:48] [441200] global_step=441200, grad_norm=3.2727887630462646, loss=2.5749478340148926
I0304 22:50:29.486732 140380326586112 logging_writer.py:48] [441300] global_step=441300, grad_norm=3.0950942039489746, loss=1.905346155166626
I0304 22:51:14.666409 140380334978816 logging_writer.py:48] [441400] global_step=441400, grad_norm=2.8836264610290527, loss=1.1622644662857056
I0304 22:51:59.678162 140380326586112 logging_writer.py:48] [441500] global_step=441500, grad_norm=2.972210645675659, loss=1.0330407619476318
I0304 22:52:44.451745 140380334978816 logging_writer.py:48] [441600] global_step=441600, grad_norm=3.0688750743865967, loss=1.4737589359283447
I0304 22:53:29.818264 140380326586112 logging_writer.py:48] [441700] global_step=441700, grad_norm=4.100543022155762, loss=3.270089626312256
I0304 22:54:14.872534 140380334978816 logging_writer.py:48] [441800] global_step=441800, grad_norm=3.170936346054077, loss=1.4173343181610107
I0304 22:54:59.767934 140380326586112 logging_writer.py:48] [441900] global_step=441900, grad_norm=4.901282787322998, loss=3.312314033508301
I0304 22:55:44.912576 140380334978816 logging_writer.py:48] [442000] global_step=442000, grad_norm=4.070342063903809, loss=3.2653231620788574
I0304 22:56:03.564149 140575196817216 spec.py:321] Evaluating on the training split.
I0304 22:56:14.427143 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 22:56:43.491985 140575196817216 spec.py:349] Evaluating on the test split.
I0304 22:56:45.148297 140575196817216 submission_runner.py:411] Time since start: 213780.75s, 	Step: 442043, 	{'train/accuracy': 0.8869140148162842, 'train/loss': 0.41694721579551697, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 196657.33681559563, 'total_duration': 213780.74925494194, 'accumulated_submission_time': 196657.33681559563, 'accumulated_eval_time': 17068.658164978027, 'accumulated_logging_time': 31.343939542770386}
I0304 22:56:45.229369 140380326586112 logging_writer.py:48] [442043] accumulated_eval_time=17068.658165, accumulated_logging_time=31.343940, accumulated_submission_time=196657.336816, global_step=442043, preemption_count=0, score=196657.336816, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=213780.749255, train/accuracy=0.886914, train/loss=0.416947, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 22:57:08.261479 140380334978816 logging_writer.py:48] [442100] global_step=442100, grad_norm=2.7488632202148438, loss=1.4462754726409912
I0304 22:57:51.296904 140380326586112 logging_writer.py:48] [442200] global_step=442200, grad_norm=3.6718761920928955, loss=3.01294207572937
I0304 22:58:36.154786 140380334978816 logging_writer.py:48] [442300] global_step=442300, grad_norm=3.1927177906036377, loss=1.5892987251281738
I0304 22:59:21.563169 140380326586112 logging_writer.py:48] [442400] global_step=442400, grad_norm=3.280996084213257, loss=1.1692731380462646
I0304 23:00:06.323629 140380334978816 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.3481035232543945, loss=2.675477981567383
I0304 23:00:51.210414 140380326586112 logging_writer.py:48] [442600] global_step=442600, grad_norm=4.75245475769043, loss=2.9645023345947266
I0304 23:01:36.164577 140380334978816 logging_writer.py:48] [442700] global_step=442700, grad_norm=4.253670692443848, loss=3.092726230621338
I0304 23:02:21.254294 140380326586112 logging_writer.py:48] [442800] global_step=442800, grad_norm=3.245615005493164, loss=1.3146525621414185
I0304 23:03:06.384960 140380334978816 logging_writer.py:48] [442900] global_step=442900, grad_norm=3.023003578186035, loss=1.7972158193588257
I0304 23:03:45.480177 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:03:56.580744 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:04:19.188423 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:04:20.832611 140575196817216 submission_runner.py:411] Time since start: 214236.43s, 	Step: 442989, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.4157116115093231, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 197077.52906560898, 'total_duration': 214236.43349838257, 'accumulated_submission_time': 197077.52906560898, 'accumulated_eval_time': 17104.009438753128, 'accumulated_logging_time': 31.43527889251709}
I0304 23:04:20.917038 140380326586112 logging_writer.py:48] [442989] accumulated_eval_time=17104.009439, accumulated_logging_time=31.435279, accumulated_submission_time=197077.529066, global_step=442989, preemption_count=0, score=197077.529066, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=214236.433498, train/accuracy=0.888516, train/loss=0.415712, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:04:25.690637 140380334978816 logging_writer.py:48] [443000] global_step=443000, grad_norm=3.020324945449829, loss=1.2001466751098633
I0304 23:05:06.506964 140380326586112 logging_writer.py:48] [443100] global_step=443100, grad_norm=3.0995242595672607, loss=1.1483197212219238
I0304 23:05:51.470860 140380334978816 logging_writer.py:48] [443200] global_step=443200, grad_norm=3.287846088409424, loss=1.323586106300354
I0304 23:06:36.903496 140380326586112 logging_writer.py:48] [443300] global_step=443300, grad_norm=3.3700616359710693, loss=1.0855947732925415
I0304 23:07:21.900679 140380334978816 logging_writer.py:48] [443400] global_step=443400, grad_norm=2.9026291370391846, loss=1.0505247116088867
I0304 23:08:06.940636 140380326586112 logging_writer.py:48] [443500] global_step=443500, grad_norm=3.062173366546631, loss=1.1158126592636108
I0304 23:08:51.935422 140380334978816 logging_writer.py:48] [443600] global_step=443600, grad_norm=3.3209187984466553, loss=1.2044987678527832
I0304 23:09:36.877200 140380326586112 logging_writer.py:48] [443700] global_step=443700, grad_norm=3.3579163551330566, loss=1.1680302619934082
I0304 23:10:21.935247 140380334978816 logging_writer.py:48] [443800] global_step=443800, grad_norm=3.165517807006836, loss=1.1671156883239746
I0304 23:11:06.652346 140380326586112 logging_writer.py:48] [443900] global_step=443900, grad_norm=3.432680368423462, loss=2.294715166091919
I0304 23:11:21.210543 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:11:31.858324 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:12:03.456383 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:12:05.109330 140575196817216 submission_runner.py:411] Time since start: 214700.71s, 	Step: 443934, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.42413055896759033, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 197497.7634282112, 'total_duration': 214700.71033906937, 'accumulated_submission_time': 197497.7634282112, 'accumulated_eval_time': 17147.90717315674, 'accumulated_logging_time': 31.5306293964386}
I0304 23:12:05.205887 140380334978816 logging_writer.py:48] [443934] accumulated_eval_time=17147.907173, accumulated_logging_time=31.530629, accumulated_submission_time=197497.763428, global_step=443934, preemption_count=0, score=197497.763428, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=214700.710339, train/accuracy=0.887129, train/loss=0.424131, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:12:31.817594 140380326586112 logging_writer.py:48] [444000] global_step=444000, grad_norm=3.176830291748047, loss=1.8236640691757202
I0304 23:13:15.462000 140380334978816 logging_writer.py:48] [444100] global_step=444100, grad_norm=3.4800710678100586, loss=1.1203222274780273
I0304 23:14:01.435841 140380326586112 logging_writer.py:48] [444200] global_step=444200, grad_norm=3.6932365894317627, loss=1.1774895191192627
I0304 23:14:47.308573 140380334978816 logging_writer.py:48] [444300] global_step=444300, grad_norm=3.4336936473846436, loss=2.374310255050659
I0304 23:15:32.665560 140380326586112 logging_writer.py:48] [444400] global_step=444400, grad_norm=3.425663709640503, loss=1.0314465761184692
I0304 23:16:18.024908 140380334978816 logging_writer.py:48] [444500] global_step=444500, grad_norm=3.10906982421875, loss=1.0316977500915527
I0304 23:17:03.470795 140380326586112 logging_writer.py:48] [444600] global_step=444600, grad_norm=2.9382479190826416, loss=1.3209726810455322
I0304 23:17:48.544708 140380334978816 logging_writer.py:48] [444700] global_step=444700, grad_norm=3.151771068572998, loss=1.322266697883606
I0304 23:18:34.073115 140380326586112 logging_writer.py:48] [444800] global_step=444800, grad_norm=3.0181467533111572, loss=1.0870115756988525
I0304 23:19:05.120148 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:19:15.852463 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:19:43.434232 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:19:45.086996 140575196817216 submission_runner.py:411] Time since start: 215160.69s, 	Step: 444870, 	{'train/accuracy': 0.8898242115974426, 'train/loss': 0.41144275665283203, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 197917.61884331703, 'total_duration': 215160.68771243095, 'accumulated_submission_time': 197917.61884331703, 'accumulated_eval_time': 17187.872669696808, 'accumulated_logging_time': 31.63857674598694}
I0304 23:19:45.170108 140380334978816 logging_writer.py:48] [444870] accumulated_eval_time=17187.872670, accumulated_logging_time=31.638577, accumulated_submission_time=197917.618843, global_step=444870, preemption_count=0, score=197917.618843, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=215160.687712, train/accuracy=0.889824, train/loss=0.411443, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:19:57.490995 140380326586112 logging_writer.py:48] [444900] global_step=444900, grad_norm=2.9530420303344727, loss=1.1926788091659546
I0304 23:20:39.473374 140380334978816 logging_writer.py:48] [445000] global_step=445000, grad_norm=3.0913710594177246, loss=1.1213014125823975
I0304 23:21:24.635129 140380326586112 logging_writer.py:48] [445100] global_step=445100, grad_norm=3.332615613937378, loss=2.982243299484253
I0304 23:22:10.124269 140380334978816 logging_writer.py:48] [445200] global_step=445200, grad_norm=4.171835422515869, loss=3.19272518157959
I0304 23:22:55.007610 140380326586112 logging_writer.py:48] [445300] global_step=445300, grad_norm=3.741609811782837, loss=1.1894975900650024
I0304 23:23:40.095966 140380334978816 logging_writer.py:48] [445400] global_step=445400, grad_norm=3.336791515350342, loss=2.070507287979126
I0304 23:24:25.653068 140380326586112 logging_writer.py:48] [445500] global_step=445500, grad_norm=3.142557144165039, loss=1.177302360534668
I0304 23:25:10.896069 140380334978816 logging_writer.py:48] [445600] global_step=445600, grad_norm=3.0475704669952393, loss=1.0951905250549316
I0304 23:25:55.942697 140380326586112 logging_writer.py:48] [445700] global_step=445700, grad_norm=3.1897518634796143, loss=1.0473837852478027
I0304 23:26:41.245710 140380334978816 logging_writer.py:48] [445800] global_step=445800, grad_norm=3.105551242828369, loss=1.162609338760376
I0304 23:26:45.432800 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:26:56.217284 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:27:18.764228 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:27:20.419298 140575196817216 submission_runner.py:411] Time since start: 215616.02s, 	Step: 445811, 	{'train/accuracy': 0.8890820145606995, 'train/loss': 0.4095396399497986, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 198337.8225800991, 'total_duration': 215616.0204000473, 'accumulated_submission_time': 198337.8225800991, 'accumulated_eval_time': 17222.858201503754, 'accumulated_logging_time': 31.731759071350098}
I0304 23:27:20.503828 140380326586112 logging_writer.py:48] [445811] accumulated_eval_time=17222.858202, accumulated_logging_time=31.731759, accumulated_submission_time=198337.822580, global_step=445811, preemption_count=0, score=198337.822580, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=215616.020400, train/accuracy=0.889082, train/loss=0.409540, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:27:56.272402 140380334978816 logging_writer.py:48] [445900] global_step=445900, grad_norm=3.868999719619751, loss=3.3315653800964355
I0304 23:28:41.061587 140380326586112 logging_writer.py:48] [446000] global_step=446000, grad_norm=3.301967144012451, loss=1.028792381286621
I0304 23:29:26.527963 140380334978816 logging_writer.py:48] [446100] global_step=446100, grad_norm=3.174960136413574, loss=1.2007784843444824
I0304 23:30:11.816363 140380326586112 logging_writer.py:48] [446200] global_step=446200, grad_norm=3.719365119934082, loss=3.0350444316864014
I0304 23:30:56.876912 140380334978816 logging_writer.py:48] [446300] global_step=446300, grad_norm=3.263923406600952, loss=1.3262523412704468
I0304 23:31:42.084463 140380326586112 logging_writer.py:48] [446400] global_step=446400, grad_norm=3.029613494873047, loss=2.8217782974243164
I0304 23:32:27.257473 140380334978816 logging_writer.py:48] [446500] global_step=446500, grad_norm=3.221233606338501, loss=1.6744698286056519
I0304 23:33:12.591254 140380326586112 logging_writer.py:48] [446600] global_step=446600, grad_norm=2.9537553787231445, loss=1.2183119058609009
I0304 23:33:58.051895 140380334978816 logging_writer.py:48] [446700] global_step=446700, grad_norm=3.2305891513824463, loss=1.0961225032806396
I0304 23:34:20.752552 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:34:31.604166 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:35:00.801034 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:35:02.466349 140575196817216 submission_runner.py:411] Time since start: 216078.07s, 	Step: 446752, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.41881701350212097, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 198758.01270985603, 'total_duration': 216078.06741833687, 'accumulated_submission_time': 198758.01270985603, 'accumulated_eval_time': 17264.571006774902, 'accumulated_logging_time': 31.82603144645691}
I0304 23:35:02.563903 140380326586112 logging_writer.py:48] [446752] accumulated_eval_time=17264.571007, accumulated_logging_time=31.826031, accumulated_submission_time=198758.012710, global_step=446752, preemption_count=0, score=198758.012710, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=216078.067418, train/accuracy=0.887598, train/loss=0.418817, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:35:22.036387 140380334978816 logging_writer.py:48] [446800] global_step=446800, grad_norm=3.2725367546081543, loss=1.2102738618850708
I0304 23:36:04.911593 140380326586112 logging_writer.py:48] [446900] global_step=446900, grad_norm=3.442072868347168, loss=2.763843536376953
I0304 23:36:50.180107 140380334978816 logging_writer.py:48] [447000] global_step=447000, grad_norm=2.937727928161621, loss=1.1480863094329834
I0304 23:37:35.407132 140380326586112 logging_writer.py:48] [447100] global_step=447100, grad_norm=4.019305229187012, loss=3.327021598815918
I0304 23:38:20.443878 140380334978816 logging_writer.py:48] [447200] global_step=447200, grad_norm=3.4704480171203613, loss=1.158229112625122
I0304 23:39:05.697301 140380326586112 logging_writer.py:48] [447300] global_step=447300, grad_norm=3.447831869125366, loss=3.058516263961792
I0304 23:39:50.594332 140380334978816 logging_writer.py:48] [447400] global_step=447400, grad_norm=3.41251540184021, loss=1.2374767065048218
I0304 23:40:35.735431 140380326586112 logging_writer.py:48] [447500] global_step=447500, grad_norm=3.1530187129974365, loss=1.1818135976791382
I0304 23:41:20.806209 140380334978816 logging_writer.py:48] [447600] global_step=447600, grad_norm=3.131298303604126, loss=1.3599145412445068
I0304 23:42:02.740233 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:42:13.961167 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:42:39.819949 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:42:41.476682 140575196817216 submission_runner.py:411] Time since start: 216537.08s, 	Step: 447695, 	{'train/accuracy': 0.8876367211341858, 'train/loss': 0.42010965943336487, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 199178.1304731369, 'total_duration': 216537.0774514675, 'accumulated_submission_time': 199178.1304731369, 'accumulated_eval_time': 17303.306176424026, 'accumulated_logging_time': 31.934014320373535}
I0304 23:42:41.562018 140380326586112 logging_writer.py:48] [447695] accumulated_eval_time=17303.306176, accumulated_logging_time=31.934014, accumulated_submission_time=199178.130473, global_step=447695, preemption_count=0, score=199178.130473, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=216537.077451, train/accuracy=0.887637, train/loss=0.420110, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:42:43.952695 140380334978816 logging_writer.py:48] [447700] global_step=447700, grad_norm=3.850121259689331, loss=2.8661296367645264
I0304 23:43:24.246614 140380326586112 logging_writer.py:48] [447800] global_step=447800, grad_norm=3.1450719833374023, loss=1.177361249923706
I0304 23:44:09.217843 140380334978816 logging_writer.py:48] [447900] global_step=447900, grad_norm=3.7469685077667236, loss=2.9498863220214844
I0304 23:44:54.544887 140380326586112 logging_writer.py:48] [448000] global_step=448000, grad_norm=2.9421770572662354, loss=1.2994964122772217
I0304 23:45:40.542305 140380334978816 logging_writer.py:48] [448100] global_step=448100, grad_norm=3.0008974075317383, loss=1.0815380811691284
I0304 23:46:25.611021 140380326586112 logging_writer.py:48] [448200] global_step=448200, grad_norm=3.3200571537017822, loss=2.780374050140381
I0304 23:47:10.716743 140380334978816 logging_writer.py:48] [448300] global_step=448300, grad_norm=3.4859182834625244, loss=3.1593074798583984
I0304 23:47:55.924530 140380326586112 logging_writer.py:48] [448400] global_step=448400, grad_norm=2.999037981033325, loss=1.063364028930664
I0304 23:48:41.113593 140380334978816 logging_writer.py:48] [448500] global_step=448500, grad_norm=3.4180822372436523, loss=1.2701832056045532
I0304 23:49:26.160413 140380326586112 logging_writer.py:48] [448600] global_step=448600, grad_norm=2.870272636413574, loss=1.8071807622909546
I0304 23:49:41.624463 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:49:52.616583 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:50:19.294872 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:50:20.940198 140575196817216 submission_runner.py:411] Time since start: 216996.54s, 	Step: 448636, 	{'train/accuracy': 0.8866015672683716, 'train/loss': 0.421945184469223, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 199598.13326835632, 'total_duration': 216996.54115891457, 'accumulated_submission_time': 199598.13326835632, 'accumulated_eval_time': 17342.620814323425, 'accumulated_logging_time': 32.0307891368866}
I0304 23:50:21.022647 140380334978816 logging_writer.py:48] [448636] accumulated_eval_time=17342.620814, accumulated_logging_time=32.030789, accumulated_submission_time=199598.133268, global_step=448636, preemption_count=0, score=199598.133268, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=216996.541159, train/accuracy=0.886602, train/loss=0.421945, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:50:46.846098 140380326586112 logging_writer.py:48] [448700] global_step=448700, grad_norm=3.6027796268463135, loss=1.0044846534729004
I0304 23:51:30.526367 140380334978816 logging_writer.py:48] [448800] global_step=448800, grad_norm=3.0315287113189697, loss=2.119966745376587
I0304 23:52:15.493081 140380326586112 logging_writer.py:48] [448900] global_step=448900, grad_norm=3.305621385574341, loss=1.9856371879577637
I0304 23:53:00.603226 140380334978816 logging_writer.py:48] [449000] global_step=449000, grad_norm=3.230708122253418, loss=1.186050534248352
I0304 23:53:45.369881 140380326586112 logging_writer.py:48] [449100] global_step=449100, grad_norm=3.2146010398864746, loss=1.2243859767913818
I0304 23:54:30.932240 140380334978816 logging_writer.py:48] [449200] global_step=449200, grad_norm=3.187268018722534, loss=1.095344066619873
I0304 23:55:16.296961 140380326586112 logging_writer.py:48] [449300] global_step=449300, grad_norm=3.1654911041259766, loss=1.0738003253936768
I0304 23:56:01.441699 140380334978816 logging_writer.py:48] [449400] global_step=449400, grad_norm=3.0891923904418945, loss=1.1244587898254395
I0304 23:56:47.190967 140380326586112 logging_writer.py:48] [449500] global_step=449500, grad_norm=3.2754693031311035, loss=1.2775105237960815
I0304 23:57:21.216870 140575196817216 spec.py:321] Evaluating on the training split.
I0304 23:57:31.828339 140575196817216 spec.py:333] Evaluating on the validation split.
I0304 23:57:58.919587 140575196817216 spec.py:349] Evaluating on the test split.
I0304 23:58:00.569152 140575196817216 submission_runner.py:411] Time since start: 217456.17s, 	Step: 449577, 	{'train/accuracy': 0.8864648342132568, 'train/loss': 0.41946879029273987, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 200018.26824116707, 'total_duration': 217456.16999864578, 'accumulated_submission_time': 200018.26824116707, 'accumulated_eval_time': 17381.971882104874, 'accumulated_logging_time': 32.12383246421814}
I0304 23:58:00.672418 140380334978816 logging_writer.py:48] [449577] accumulated_eval_time=17381.971882, accumulated_logging_time=32.123832, accumulated_submission_time=200018.268241, global_step=449577, preemption_count=0, score=200018.268241, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=217456.169999, train/accuracy=0.886465, train/loss=0.419469, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0304 23:58:10.200247 140380326586112 logging_writer.py:48] [449600] global_step=449600, grad_norm=3.328070640563965, loss=2.2766051292419434
I0304 23:58:51.595743 140380334978816 logging_writer.py:48] [449700] global_step=449700, grad_norm=3.0763437747955322, loss=2.293060541152954
I0304 23:59:37.169838 140380326586112 logging_writer.py:48] [449800] global_step=449800, grad_norm=5.56785249710083, loss=3.276205539703369
I0305 00:00:22.922633 140380334978816 logging_writer.py:48] [449900] global_step=449900, grad_norm=3.0644214153289795, loss=1.4015932083129883
I0305 00:01:07.970379 140380326586112 logging_writer.py:48] [450000] global_step=450000, grad_norm=4.264673709869385, loss=2.7128570079803467
I0305 00:01:53.002766 140380334978816 logging_writer.py:48] [450100] global_step=450100, grad_norm=3.4283223152160645, loss=1.3757246732711792
I0305 00:02:38.131213 140380326586112 logging_writer.py:48] [450200] global_step=450200, grad_norm=3.5874598026275635, loss=1.0624964237213135
I0305 00:03:23.194509 140380334978816 logging_writer.py:48] [450300] global_step=450300, grad_norm=2.8607096672058105, loss=1.5123194456100464
I0305 00:04:08.312622 140380326586112 logging_writer.py:48] [450400] global_step=450400, grad_norm=3.32627272605896, loss=2.539480686187744
I0305 00:04:53.335347 140380334978816 logging_writer.py:48] [450500] global_step=450500, grad_norm=4.278940677642822, loss=3.1918187141418457
I0305 00:05:00.706912 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:05:11.494020 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:05:40.889538 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:05:42.542821 140575196817216 submission_runner.py:411] Time since start: 217918.14s, 	Step: 450518, 	{'train/accuracy': 0.89013671875, 'train/loss': 0.41641756892204285, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 200438.24313998222, 'total_duration': 217918.1436984539, 'accumulated_submission_time': 200438.24313998222, 'accumulated_eval_time': 17423.806594848633, 'accumulated_logging_time': 32.23769760131836}
I0305 00:05:42.626771 140380326586112 logging_writer.py:48] [450518] accumulated_eval_time=17423.806595, accumulated_logging_time=32.237698, accumulated_submission_time=200438.243140, global_step=450518, preemption_count=0, score=200438.243140, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=217918.143698, train/accuracy=0.890137, train/loss=0.416418, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:06:15.634574 140380334978816 logging_writer.py:48] [450600] global_step=450600, grad_norm=3.213358163833618, loss=1.2443432807922363
I0305 00:07:00.444224 140380326586112 logging_writer.py:48] [450700] global_step=450700, grad_norm=3.077373743057251, loss=1.5359764099121094
I0305 00:07:45.696408 140380334978816 logging_writer.py:48] [450800] global_step=450800, grad_norm=2.9770398139953613, loss=1.110391616821289
I0305 00:08:30.848142 140380326586112 logging_writer.py:48] [450900] global_step=450900, grad_norm=3.1776509284973145, loss=1.1687794923782349
I0305 00:09:15.826241 140380334978816 logging_writer.py:48] [451000] global_step=451000, grad_norm=2.973825216293335, loss=2.2318882942199707
I0305 00:10:00.895042 140380326586112 logging_writer.py:48] [451100] global_step=451100, grad_norm=2.9194223880767822, loss=1.1775548458099365
I0305 00:10:45.889892 140380334978816 logging_writer.py:48] [451200] global_step=451200, grad_norm=3.2521779537200928, loss=1.1498639583587646
I0305 00:11:31.005020 140380326586112 logging_writer.py:48] [451300] global_step=451300, grad_norm=3.4433374404907227, loss=2.689952850341797
I0305 00:12:16.127553 140380334978816 logging_writer.py:48] [451400] global_step=451400, grad_norm=3.0940370559692383, loss=1.1397215127944946
I0305 00:12:42.843004 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:12:53.590506 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:13:22.547070 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:13:24.199295 140575196817216 submission_runner.py:411] Time since start: 218379.80s, 	Step: 451461, 	{'train/accuracy': 0.8877539038658142, 'train/loss': 0.42140790820121765, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 200858.3983180523, 'total_duration': 218379.80024075508, 'accumulated_submission_time': 200858.3983180523, 'accumulated_eval_time': 17465.161774635315, 'accumulated_logging_time': 32.33136487007141}
I0305 00:13:24.284690 140380326586112 logging_writer.py:48] [451461] accumulated_eval_time=17465.161775, accumulated_logging_time=32.331365, accumulated_submission_time=200858.398318, global_step=451461, preemption_count=0, score=200858.398318, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=218379.800241, train/accuracy=0.887754, train/loss=0.421408, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:13:40.187687 140380334978816 logging_writer.py:48] [451500] global_step=451500, grad_norm=3.2792117595672607, loss=2.3644046783447266
I0305 00:14:22.637856 140380326586112 logging_writer.py:48] [451600] global_step=451600, grad_norm=2.888766050338745, loss=1.0094282627105713
I0305 00:15:07.660710 140380334978816 logging_writer.py:48] [451700] global_step=451700, grad_norm=2.982468605041504, loss=1.0980851650238037
I0305 00:15:52.718535 140380326586112 logging_writer.py:48] [451800] global_step=451800, grad_norm=3.2798380851745605, loss=1.1190017461776733
I0305 00:16:37.866642 140380334978816 logging_writer.py:48] [451900] global_step=451900, grad_norm=3.1598317623138428, loss=1.6315586566925049
I0305 00:17:22.790709 140380326586112 logging_writer.py:48] [452000] global_step=452000, grad_norm=3.2136895656585693, loss=1.020225167274475
I0305 00:18:07.702241 140380334978816 logging_writer.py:48] [452100] global_step=452100, grad_norm=3.129063844680786, loss=1.1548850536346436
I0305 00:18:52.471611 140380326586112 logging_writer.py:48] [452200] global_step=452200, grad_norm=3.03554630279541, loss=1.674163579940796
I0305 00:19:37.666080 140380334978816 logging_writer.py:48] [452300] global_step=452300, grad_norm=3.4354135990142822, loss=2.546513795852661
I0305 00:20:22.844405 140380326586112 logging_writer.py:48] [452400] global_step=452400, grad_norm=3.0713770389556885, loss=1.1531660556793213
I0305 00:20:24.221660 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:20:34.828503 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:21:04.270627 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:21:05.931033 140575196817216 submission_runner.py:411] Time since start: 218841.53s, 	Step: 452405, 	{'train/accuracy': 0.8856835961341858, 'train/loss': 0.4210818409919739, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 201278.27744483948, 'total_duration': 218841.53184318542, 'accumulated_submission_time': 201278.27744483948, 'accumulated_eval_time': 17506.869884729385, 'accumulated_logging_time': 32.42599129676819}
I0305 00:21:06.020919 140380334978816 logging_writer.py:48] [452405] accumulated_eval_time=17506.869885, accumulated_logging_time=32.425991, accumulated_submission_time=201278.277445, global_step=452405, preemption_count=0, score=201278.277445, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=218841.531843, train/accuracy=0.885684, train/loss=0.421082, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:21:44.508025 140380326586112 logging_writer.py:48] [452500] global_step=452500, grad_norm=4.0647735595703125, loss=1.302874207496643
I0305 00:22:29.420406 140380334978816 logging_writer.py:48] [452600] global_step=452600, grad_norm=3.8732776641845703, loss=3.059098482131958
I0305 00:23:14.642647 140380326586112 logging_writer.py:48] [452700] global_step=452700, grad_norm=3.448861837387085, loss=2.784285306930542
I0305 00:23:59.590866 140380334978816 logging_writer.py:48] [452800] global_step=452800, grad_norm=3.1284899711608887, loss=1.493324875831604
I0305 00:24:44.734316 140380326586112 logging_writer.py:48] [452900] global_step=452900, grad_norm=3.1237969398498535, loss=1.1401841640472412
I0305 00:25:29.907039 140380334978816 logging_writer.py:48] [453000] global_step=453000, grad_norm=3.2451987266540527, loss=1.2022651433944702
I0305 00:26:15.186933 140380326586112 logging_writer.py:48] [453100] global_step=453100, grad_norm=3.3139491081237793, loss=1.108918309211731
I0305 00:27:00.354376 140380334978816 logging_writer.py:48] [453200] global_step=453200, grad_norm=3.728767156600952, loss=3.0463573932647705
I0305 00:27:45.519937 140380326586112 logging_writer.py:48] [453300] global_step=453300, grad_norm=3.1554172039031982, loss=2.3147730827331543
I0305 00:28:06.324956 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:28:17.094905 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:28:44.812807 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:28:46.468413 140575196817216 submission_runner.py:411] Time since start: 219302.07s, 	Step: 453348, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.4244422912597656, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 201698.52309155464, 'total_duration': 219302.0693335533, 'accumulated_submission_time': 201698.52309155464, 'accumulated_eval_time': 17547.012185573578, 'accumulated_logging_time': 32.526304721832275}
I0305 00:28:46.564035 140380334978816 logging_writer.py:48] [453348] accumulated_eval_time=17547.012186, accumulated_logging_time=32.526305, accumulated_submission_time=201698.523092, global_step=453348, preemption_count=0, score=201698.523092, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=219302.069334, train/accuracy=0.887207, train/loss=0.424442, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:29:07.607105 140380326586112 logging_writer.py:48] [453400] global_step=453400, grad_norm=3.2169461250305176, loss=2.7538087368011475
I0305 00:29:50.629267 140380334978816 logging_writer.py:48] [453500] global_step=453500, grad_norm=3.742527961730957, loss=2.7721381187438965
I0305 00:30:35.824269 140380326586112 logging_writer.py:48] [453600] global_step=453600, grad_norm=3.4029603004455566, loss=2.2901430130004883
I0305 00:31:21.221548 140380334978816 logging_writer.py:48] [453700] global_step=453700, grad_norm=3.135972023010254, loss=1.1181309223175049
I0305 00:32:06.153428 140380326586112 logging_writer.py:48] [453800] global_step=453800, grad_norm=3.371403217315674, loss=1.2475570440292358
I0305 00:32:51.305353 140380334978816 logging_writer.py:48] [453900] global_step=453900, grad_norm=2.9205501079559326, loss=1.2364939451217651
I0305 00:33:36.533160 140380326586112 logging_writer.py:48] [454000] global_step=454000, grad_norm=3.130977153778076, loss=1.238366723060608
I0305 00:34:21.497416 140380334978816 logging_writer.py:48] [454100] global_step=454100, grad_norm=3.177189350128174, loss=0.989877462387085
I0305 00:35:06.834321 140380326586112 logging_writer.py:48] [454200] global_step=454200, grad_norm=3.361862897872925, loss=1.5022311210632324
I0305 00:35:46.580582 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:35:57.353965 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:36:21.014366 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:36:22.665083 140575196817216 submission_runner.py:411] Time since start: 219758.27s, 	Step: 454290, 	{'train/accuracy': 0.8869140148162842, 'train/loss': 0.421396404504776, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 202118.48056221008, 'total_duration': 219758.2660264969, 'accumulated_submission_time': 202118.48056221008, 'accumulated_eval_time': 17583.095573425293, 'accumulated_logging_time': 32.6318883895874}
I0305 00:36:22.749073 140380334978816 logging_writer.py:48] [454290] accumulated_eval_time=17583.095573, accumulated_logging_time=32.631888, accumulated_submission_time=202118.480562, global_step=454290, preemption_count=0, score=202118.480562, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=219758.266026, train/accuracy=0.886914, train/loss=0.421396, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:36:27.136157 140380326586112 logging_writer.py:48] [454300] global_step=454300, grad_norm=3.113416910171509, loss=2.1205458641052246
I0305 00:37:08.119030 140380334978816 logging_writer.py:48] [454400] global_step=454400, grad_norm=3.333810329437256, loss=2.3334438800811768
I0305 00:37:52.987131 140380326586112 logging_writer.py:48] [454500] global_step=454500, grad_norm=3.236973524093628, loss=1.8476773500442505
I0305 00:38:37.975210 140380334978816 logging_writer.py:48] [454600] global_step=454600, grad_norm=3.298875570297241, loss=1.1454037427902222
I0305 00:39:23.035405 140380326586112 logging_writer.py:48] [454700] global_step=454700, grad_norm=2.996103286743164, loss=1.2050362825393677
I0305 00:40:08.080568 140380334978816 logging_writer.py:48] [454800] global_step=454800, grad_norm=3.7577261924743652, loss=3.179757833480835
I0305 00:40:52.982697 140380326586112 logging_writer.py:48] [454900] global_step=454900, grad_norm=3.3355531692504883, loss=2.7053165435791016
I0305 00:41:38.007242 140380334978816 logging_writer.py:48] [455000] global_step=455000, grad_norm=2.9773457050323486, loss=1.6473355293273926
I0305 00:42:23.209003 140380326586112 logging_writer.py:48] [455100] global_step=455100, grad_norm=3.9000234603881836, loss=3.113048553466797
I0305 00:43:08.011009 140380334978816 logging_writer.py:48] [455200] global_step=455200, grad_norm=2.9045095443725586, loss=1.2269245386123657
I0305 00:43:23.017077 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:43:33.772099 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:44:02.410763 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:44:04.060281 140575196817216 submission_runner.py:411] Time since start: 220219.66s, 	Step: 455235, 	{'train/accuracy': 0.8871093392372131, 'train/loss': 0.41671934723854065, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 202538.690376997, 'total_duration': 220219.66129660606, 'accumulated_submission_time': 202538.690376997, 'accumulated_eval_time': 17624.13773369789, 'accumulated_logging_time': 32.72527503967285}
I0305 00:44:04.144234 140380326586112 logging_writer.py:48] [455235] accumulated_eval_time=17624.137734, accumulated_logging_time=32.725275, accumulated_submission_time=202538.690377, global_step=455235, preemption_count=0, score=202538.690377, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=220219.661297, train/accuracy=0.887109, train/loss=0.416719, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:44:30.378269 140380334978816 logging_writer.py:48] [455300] global_step=455300, grad_norm=3.134610652923584, loss=2.44331431388855
I0305 00:45:14.320905 140380326586112 logging_writer.py:48] [455400] global_step=455400, grad_norm=3.168469190597534, loss=1.3024901151657104
I0305 00:45:59.126957 140380334978816 logging_writer.py:48] [455500] global_step=455500, grad_norm=3.1688616275787354, loss=2.065765619277954
I0305 00:46:44.474757 140380326586112 logging_writer.py:48] [455600] global_step=455600, grad_norm=3.0489144325256348, loss=1.18502938747406
I0305 00:47:29.617390 140380334978816 logging_writer.py:48] [455700] global_step=455700, grad_norm=3.1141715049743652, loss=1.1160681247711182
I0305 00:48:14.736312 140380326586112 logging_writer.py:48] [455800] global_step=455800, grad_norm=3.2490642070770264, loss=1.2684006690979004
I0305 00:48:59.890082 140380334978816 logging_writer.py:48] [455900] global_step=455900, grad_norm=2.9353020191192627, loss=1.0765646696090698
I0305 00:49:44.749464 140380326586112 logging_writer.py:48] [456000] global_step=456000, grad_norm=3.0663788318634033, loss=1.1314327716827393
I0305 00:50:29.849139 140380334978816 logging_writer.py:48] [456100] global_step=456100, grad_norm=3.19061541557312, loss=1.423324465751648
I0305 00:51:04.318300 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:51:15.098692 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:51:44.088191 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:51:45.734613 140575196817216 submission_runner.py:411] Time since start: 220681.34s, 	Step: 456178, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.41891157627105713, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 202958.80642414093, 'total_duration': 220681.3356461525, 'accumulated_submission_time': 202958.80642414093, 'accumulated_eval_time': 17665.55302143097, 'accumulated_logging_time': 32.81936073303223}
I0305 00:51:45.819747 140380326586112 logging_writer.py:48] [456178] accumulated_eval_time=17665.553021, accumulated_logging_time=32.819361, accumulated_submission_time=202958.806424, global_step=456178, preemption_count=0, score=202958.806424, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=220681.335646, train/accuracy=0.887559, train/loss=0.418912, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:51:54.972485 140380334978816 logging_writer.py:48] [456200] global_step=456200, grad_norm=3.68291974067688, loss=3.0795786380767822
I0305 00:52:36.636127 140380326586112 logging_writer.py:48] [456300] global_step=456300, grad_norm=3.46272873878479, loss=2.341080665588379
I0305 00:53:21.392441 140380334978816 logging_writer.py:48] [456400] global_step=456400, grad_norm=3.4522242546081543, loss=1.136265754699707
I0305 00:54:06.613711 140380326586112 logging_writer.py:48] [456500] global_step=456500, grad_norm=3.3877766132354736, loss=1.5040653944015503
I0305 00:54:51.523731 140380334978816 logging_writer.py:48] [456600] global_step=456600, grad_norm=3.019991397857666, loss=1.1188373565673828
I0305 00:55:36.598062 140380326586112 logging_writer.py:48] [456700] global_step=456700, grad_norm=2.7470529079437256, loss=1.616323471069336
I0305 00:56:21.894684 140380334978816 logging_writer.py:48] [456800] global_step=456800, grad_norm=3.4033284187316895, loss=1.1168503761291504
I0305 00:57:06.809532 140380326586112 logging_writer.py:48] [456900] global_step=456900, grad_norm=3.3079235553741455, loss=1.16139554977417
I0305 00:57:52.084630 140380334978816 logging_writer.py:48] [457000] global_step=457000, grad_norm=3.293743133544922, loss=2.831629514694214
I0305 00:58:37.070655 140380326586112 logging_writer.py:48] [457100] global_step=457100, grad_norm=3.348480463027954, loss=2.632460355758667
I0305 00:58:46.174793 140575196817216 spec.py:321] Evaluating on the training split.
I0305 00:58:56.887039 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 00:59:19.071322 140575196817216 spec.py:349] Evaluating on the test split.
I0305 00:59:20.733764 140575196817216 submission_runner.py:411] Time since start: 221136.33s, 	Step: 457122, 	{'train/accuracy': 0.8851562142372131, 'train/loss': 0.42684152722358704, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 203379.1025440693, 'total_duration': 221136.3347158432, 'accumulated_submission_time': 203379.1025440693, 'accumulated_eval_time': 17700.11089038849, 'accumulated_logging_time': 32.914966344833374}
I0305 00:59:20.818540 140380334978816 logging_writer.py:48] [457122] accumulated_eval_time=17700.110890, accumulated_logging_time=32.914966, accumulated_submission_time=203379.102544, global_step=457122, preemption_count=0, score=203379.102544, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=221136.334716, train/accuracy=0.885156, train/loss=0.426842, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 00:59:52.237630 140380326586112 logging_writer.py:48] [457200] global_step=457200, grad_norm=3.313020944595337, loss=1.1448348760604858
I0305 01:00:36.279350 140380334978816 logging_writer.py:48] [457300] global_step=457300, grad_norm=2.8014917373657227, loss=1.115255355834961
I0305 01:01:21.582954 140380326586112 logging_writer.py:48] [457400] global_step=457400, grad_norm=3.0268025398254395, loss=1.1571248769760132
I0305 01:02:06.686976 140380334978816 logging_writer.py:48] [457500] global_step=457500, grad_norm=3.5284242630004883, loss=2.9276647567749023
I0305 01:02:51.702075 140380326586112 logging_writer.py:48] [457600] global_step=457600, grad_norm=3.1733293533325195, loss=1.0824724435806274
I0305 01:03:37.389184 140380334978816 logging_writer.py:48] [457700] global_step=457700, grad_norm=2.9622342586517334, loss=1.7176424264907837
I0305 01:04:22.197225 140380326586112 logging_writer.py:48] [457800] global_step=457800, grad_norm=3.0901548862457275, loss=1.0775277614593506
I0305 01:05:07.354416 140380334978816 logging_writer.py:48] [457900] global_step=457900, grad_norm=4.175940036773682, loss=1.198263168334961
I0305 01:05:52.573184 140380326586112 logging_writer.py:48] [458000] global_step=458000, grad_norm=3.544032335281372, loss=2.5701346397399902
I0305 01:06:21.161966 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:06:31.988845 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:07:02.988800 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:07:04.641652 140575196817216 submission_runner.py:411] Time since start: 221600.24s, 	Step: 458065, 	{'train/accuracy': 0.8870898485183716, 'train/loss': 0.41761600971221924, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 203799.38678312302, 'total_duration': 221600.24261379242, 'accumulated_submission_time': 203799.38678312302, 'accumulated_eval_time': 17743.58947658539, 'accumulated_logging_time': 33.01036882400513}
I0305 01:07:04.736289 140380334978816 logging_writer.py:48] [458065] accumulated_eval_time=17743.589477, accumulated_logging_time=33.010369, accumulated_submission_time=203799.386783, global_step=458065, preemption_count=0, score=203799.386783, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=221600.242614, train/accuracy=0.887090, train/loss=0.417616, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:07:19.032933 140380326586112 logging_writer.py:48] [458100] global_step=458100, grad_norm=3.4952075481414795, loss=1.6574304103851318
I0305 01:08:01.479390 140380334978816 logging_writer.py:48] [458200] global_step=458200, grad_norm=3.158445119857788, loss=1.1457505226135254
I0305 01:08:46.545459 140380326586112 logging_writer.py:48] [458300] global_step=458300, grad_norm=3.166125535964966, loss=1.972607135772705
I0305 01:09:31.716344 140380334978816 logging_writer.py:48] [458400] global_step=458400, grad_norm=3.153198480606079, loss=1.6368863582611084
I0305 01:10:16.661839 140380326586112 logging_writer.py:48] [458500] global_step=458500, grad_norm=3.010960340499878, loss=1.211003303527832
I0305 01:11:02.029877 140380334978816 logging_writer.py:48] [458600] global_step=458600, grad_norm=3.1628806591033936, loss=1.2274776697158813
I0305 01:11:47.059030 140380326586112 logging_writer.py:48] [458700] global_step=458700, grad_norm=3.504108428955078, loss=1.2567805051803589
I0305 01:12:32.205559 140380334978816 logging_writer.py:48] [458800] global_step=458800, grad_norm=3.22419810295105, loss=2.606067180633545
I0305 01:13:17.579467 140380326586112 logging_writer.py:48] [458900] global_step=458900, grad_norm=3.2910757064819336, loss=1.5636383295059204
I0305 01:14:02.822515 140380334978816 logging_writer.py:48] [459000] global_step=459000, grad_norm=3.489912748336792, loss=1.6476942300796509
I0305 01:14:04.803818 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:14:15.647282 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:14:39.299574 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:14:40.950007 140575196817216 submission_runner.py:411] Time since start: 222056.55s, 	Step: 459006, 	{'train/accuracy': 0.8886132836341858, 'train/loss': 0.41353991627693176, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 204219.39708042145, 'total_duration': 222056.5509133339, 'accumulated_submission_time': 204219.39708042145, 'accumulated_eval_time': 17779.73451113701, 'accumulated_logging_time': 33.1145339012146}
I0305 01:14:41.041996 140380326586112 logging_writer.py:48] [459006] accumulated_eval_time=17779.734511, accumulated_logging_time=33.114534, accumulated_submission_time=204219.397080, global_step=459006, preemption_count=0, score=204219.397080, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=222056.550913, train/accuracy=0.888613, train/loss=0.413540, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:15:19.003178 140380334978816 logging_writer.py:48] [459100] global_step=459100, grad_norm=3.4494385719299316, loss=2.4697675704956055
I0305 01:16:03.761576 140380326586112 logging_writer.py:48] [459200] global_step=459200, grad_norm=3.1867668628692627, loss=2.5641419887542725
I0305 01:16:48.896975 140380334978816 logging_writer.py:48] [459300] global_step=459300, grad_norm=3.229762554168701, loss=1.625307559967041
I0305 01:17:34.324932 140380326586112 logging_writer.py:48] [459400] global_step=459400, grad_norm=3.242053985595703, loss=1.1229772567749023
I0305 01:18:19.201629 140380334978816 logging_writer.py:48] [459500] global_step=459500, grad_norm=3.2563834190368652, loss=1.7636836767196655
I0305 01:19:04.286591 140380326586112 logging_writer.py:48] [459600] global_step=459600, grad_norm=3.3591537475585938, loss=1.1852788925170898
I0305 01:19:49.161547 140380334978816 logging_writer.py:48] [459700] global_step=459700, grad_norm=4.008877277374268, loss=3.3077094554901123
I0305 01:20:34.378080 140380326586112 logging_writer.py:48] [459800] global_step=459800, grad_norm=3.0191001892089844, loss=2.0546364784240723
I0305 01:21:19.544391 140380334978816 logging_writer.py:48] [459900] global_step=459900, grad_norm=3.2944374084472656, loss=2.309640407562256
I0305 01:21:41.165281 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:21:51.913585 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:22:19.889828 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:22:21.539907 140575196817216 submission_runner.py:411] Time since start: 222517.14s, 	Step: 459950, 	{'train/accuracy': 0.8851171731948853, 'train/loss': 0.4251580834388733, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 204639.460521698, 'total_duration': 222517.1409971714, 'accumulated_submission_time': 204639.460521698, 'accumulated_eval_time': 17820.108170747757, 'accumulated_logging_time': 33.21669864654541}
I0305 01:22:21.626405 140380326586112 logging_writer.py:48] [459950] accumulated_eval_time=17820.108171, accumulated_logging_time=33.216699, accumulated_submission_time=204639.460522, global_step=459950, preemption_count=0, score=204639.460522, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=222517.140997, train/accuracy=0.885117, train/loss=0.425158, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:22:41.899879 140380334978816 logging_writer.py:48] [460000] global_step=460000, grad_norm=3.1245055198669434, loss=1.0659347772598267
I0305 01:23:24.570659 140380326586112 logging_writer.py:48] [460100] global_step=460100, grad_norm=3.8233907222747803, loss=3.207648992538452
I0305 01:24:10.135404 140380334978816 logging_writer.py:48] [460200] global_step=460200, grad_norm=3.1911425590515137, loss=1.1715985536575317
I0305 01:24:55.443207 140380326586112 logging_writer.py:48] [460300] global_step=460300, grad_norm=3.011058807373047, loss=1.8130642175674438
I0305 01:25:40.123867 140380334978816 logging_writer.py:48] [460400] global_step=460400, grad_norm=3.0162692070007324, loss=1.1880263090133667
I0305 01:26:25.983438 140380326586112 logging_writer.py:48] [460500] global_step=460500, grad_norm=2.9597904682159424, loss=1.1477413177490234
I0305 01:27:11.117305 140380334978816 logging_writer.py:48] [460600] global_step=460600, grad_norm=2.9975123405456543, loss=1.07783043384552
I0305 01:27:55.900353 140380326586112 logging_writer.py:48] [460700] global_step=460700, grad_norm=3.353394031524658, loss=2.076022148132324
I0305 01:28:41.094363 140380334978816 logging_writer.py:48] [460800] global_step=460800, grad_norm=6.341415882110596, loss=3.0388472080230713
I0305 01:29:22.321632 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:29:33.125942 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:29:56.561370 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:29:58.209166 140575196817216 submission_runner.py:411] Time since start: 222973.81s, 	Step: 460892, 	{'train/accuracy': 0.8870507478713989, 'train/loss': 0.42386263608932495, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 205060.09689879417, 'total_duration': 222973.8101565838, 'accumulated_submission_time': 205060.09689879417, 'accumulated_eval_time': 17855.99463057518, 'accumulated_logging_time': 33.313735246658325}
I0305 01:29:58.296273 140380326586112 logging_writer.py:48] [460892] accumulated_eval_time=17855.994631, accumulated_logging_time=33.313735, accumulated_submission_time=205060.096899, global_step=460892, preemption_count=0, score=205060.096899, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=222973.810157, train/accuracy=0.887051, train/loss=0.423863, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:30:01.885330 140380334978816 logging_writer.py:48] [460900] global_step=460900, grad_norm=3.1119918823242188, loss=2.0697600841522217
I0305 01:30:42.561208 140380326586112 logging_writer.py:48] [461000] global_step=461000, grad_norm=3.1942965984344482, loss=1.2324497699737549
I0305 01:31:27.427073 140380334978816 logging_writer.py:48] [461100] global_step=461100, grad_norm=3.1055474281311035, loss=1.0542056560516357
I0305 01:32:13.016833 140380326586112 logging_writer.py:48] [461200] global_step=461200, grad_norm=2.9497361183166504, loss=1.1230111122131348
I0305 01:32:57.908891 140380334978816 logging_writer.py:48] [461300] global_step=461300, grad_norm=3.991068124771118, loss=1.6561381816864014
I0305 01:33:43.238398 140380326586112 logging_writer.py:48] [461400] global_step=461400, grad_norm=4.419278621673584, loss=3.147977113723755
I0305 01:34:28.529971 140380334978816 logging_writer.py:48] [461500] global_step=461500, grad_norm=3.5775306224823, loss=1.1239690780639648
I0305 01:35:14.617702 140380326586112 logging_writer.py:48] [461600] global_step=461600, grad_norm=3.502000331878662, loss=2.2835333347320557
I0305 01:35:59.921458 140380334978816 logging_writer.py:48] [461700] global_step=461700, grad_norm=4.114886283874512, loss=2.3225598335266113
I0305 01:36:45.196911 140380326586112 logging_writer.py:48] [461800] global_step=461800, grad_norm=3.009268283843994, loss=1.1538060903549194
I0305 01:36:58.356977 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:37:09.203327 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:37:36.923811 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:37:38.577051 140575196817216 submission_runner.py:411] Time since start: 223434.18s, 	Step: 461831, 	{'train/accuracy': 0.8887695074081421, 'train/loss': 0.41987350583076477, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 205480.0987341404, 'total_duration': 223434.17816066742, 'accumulated_submission_time': 205480.0987341404, 'accumulated_eval_time': 17896.213749408722, 'accumulated_logging_time': 33.410961866378784}
I0305 01:37:38.663118 140380334978816 logging_writer.py:48] [461831] accumulated_eval_time=17896.213749, accumulated_logging_time=33.410962, accumulated_submission_time=205480.098734, global_step=461831, preemption_count=0, score=205480.098734, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=223434.178161, train/accuracy=0.888770, train/loss=0.419874, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:38:06.489943 140380326586112 logging_writer.py:48] [461900] global_step=461900, grad_norm=4.079675197601318, loss=2.8505771160125732
I0305 01:38:50.346069 140380334978816 logging_writer.py:48] [462000] global_step=462000, grad_norm=3.113283157348633, loss=1.1279456615447998
I0305 01:39:35.659859 140380326586112 logging_writer.py:48] [462100] global_step=462100, grad_norm=3.1057288646698, loss=1.323890209197998
I0305 01:40:20.831934 140380334978816 logging_writer.py:48] [462200] global_step=462200, grad_norm=3.664199113845825, loss=3.159407615661621
I0305 01:41:05.662739 140380326586112 logging_writer.py:48] [462300] global_step=462300, grad_norm=3.2324626445770264, loss=1.8060274124145508
I0305 01:41:50.509596 140380334978816 logging_writer.py:48] [462400] global_step=462400, grad_norm=3.229252815246582, loss=1.2084569931030273
I0305 01:42:35.385763 140380326586112 logging_writer.py:48] [462500] global_step=462500, grad_norm=3.2329864501953125, loss=0.9966414570808411
I0305 01:43:20.483349 140380334978816 logging_writer.py:48] [462600] global_step=462600, grad_norm=3.1540281772613525, loss=1.251391887664795
I0305 01:44:05.303606 140380326586112 logging_writer.py:48] [462700] global_step=462700, grad_norm=3.2705554962158203, loss=1.0960021018981934
I0305 01:44:38.714780 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:44:48.203740 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:45:16.199297 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:45:17.845381 140575196817216 submission_runner.py:411] Time since start: 223893.45s, 	Step: 462776, 	{'train/accuracy': 0.8857616782188416, 'train/loss': 0.42403894662857056, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 205900.09218215942, 'total_duration': 223893.4460091591, 'accumulated_submission_time': 205900.09218215942, 'accumulated_eval_time': 17935.342923879623, 'accumulated_logging_time': 33.50675344467163}
I0305 01:45:17.933029 140380334978816 logging_writer.py:48] [462776] accumulated_eval_time=17935.342924, accumulated_logging_time=33.506753, accumulated_submission_time=205900.092182, global_step=462776, preemption_count=0, score=205900.092182, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=223893.446009, train/accuracy=0.885762, train/loss=0.424039, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:45:27.866501 140380326586112 logging_writer.py:48] [462800] global_step=462800, grad_norm=3.078077554702759, loss=1.1059808731079102
I0305 01:46:09.389632 140380334978816 logging_writer.py:48] [462900] global_step=462900, grad_norm=3.172114372253418, loss=2.256629467010498
I0305 01:46:54.338270 140380326586112 logging_writer.py:48] [463000] global_step=463000, grad_norm=3.92132568359375, loss=3.146538257598877
I0305 01:47:39.472270 140380334978816 logging_writer.py:48] [463100] global_step=463100, grad_norm=3.474318504333496, loss=1.244098424911499
I0305 01:48:24.698587 140380326586112 logging_writer.py:48] [463200] global_step=463200, grad_norm=3.0477802753448486, loss=1.1746821403503418
I0305 01:49:09.417492 140380334978816 logging_writer.py:48] [463300] global_step=463300, grad_norm=4.312489032745361, loss=3.026578664779663
I0305 01:49:54.379367 140380326586112 logging_writer.py:48] [463400] global_step=463400, grad_norm=2.875762939453125, loss=1.0420432090759277
I0305 01:50:39.395960 140380334978816 logging_writer.py:48] [463500] global_step=463500, grad_norm=3.551304340362549, loss=1.1829710006713867
I0305 01:51:24.392186 140380326586112 logging_writer.py:48] [463600] global_step=463600, grad_norm=2.9317803382873535, loss=1.1895692348480225
I0305 01:52:09.202236 140380334978816 logging_writer.py:48] [463700] global_step=463700, grad_norm=3.2871367931365967, loss=1.1769826412200928
I0305 01:52:18.231255 140575196817216 spec.py:321] Evaluating on the training split.
I0305 01:52:28.014774 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 01:52:51.331510 140575196817216 spec.py:349] Evaluating on the test split.
I0305 01:52:52.990399 140575196817216 submission_runner.py:411] Time since start: 224348.59s, 	Step: 463722, 	{'train/accuracy': 0.8868945240974426, 'train/loss': 0.4198920726776123, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 206320.33250784874, 'total_duration': 224348.5914018154, 'accumulated_submission_time': 206320.33250784874, 'accumulated_eval_time': 17970.10102057457, 'accumulated_logging_time': 33.603896617889404}
I0305 01:52:53.075594 140380326586112 logging_writer.py:48] [463722] accumulated_eval_time=17970.101021, accumulated_logging_time=33.603897, accumulated_submission_time=206320.332508, global_step=463722, preemption_count=0, score=206320.332508, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=224348.591402, train/accuracy=0.886895, train/loss=0.419892, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 01:53:24.486929 140380334978816 logging_writer.py:48] [463800] global_step=463800, grad_norm=2.999427556991577, loss=1.2447620630264282
I0305 01:54:08.360345 140380326586112 logging_writer.py:48] [463900] global_step=463900, grad_norm=3.110128402709961, loss=1.0470722913742065
I0305 01:54:53.501075 140380334978816 logging_writer.py:48] [464000] global_step=464000, grad_norm=3.3290395736694336, loss=1.438185214996338
I0305 01:55:38.510394 140380326586112 logging_writer.py:48] [464100] global_step=464100, grad_norm=3.0892488956451416, loss=1.173102617263794
I0305 01:56:24.242731 140380334978816 logging_writer.py:48] [464200] global_step=464200, grad_norm=2.9003424644470215, loss=1.1081444025039673
I0305 01:57:09.719135 140380326586112 logging_writer.py:48] [464300] global_step=464300, grad_norm=3.9018051624298096, loss=3.0608584880828857
I0305 01:57:54.821205 140380334978816 logging_writer.py:48] [464400] global_step=464400, grad_norm=3.063878059387207, loss=1.0832208395004272
I0305 01:58:40.226066 140380326586112 logging_writer.py:48] [464500] global_step=464500, grad_norm=3.223024368286133, loss=1.0222973823547363
I0305 01:59:25.323161 140380334978816 logging_writer.py:48] [464600] global_step=464600, grad_norm=3.5524556636810303, loss=2.793980121612549
I0305 01:59:53.393758 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:00:03.338252 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:00:28.253050 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:00:29.906658 140575196817216 submission_runner.py:411] Time since start: 224805.51s, 	Step: 464664, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.4207744896411896, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 206740.59101843834, 'total_duration': 224805.5076739788, 'accumulated_submission_time': 206740.59101843834, 'accumulated_eval_time': 18006.612882614136, 'accumulated_logging_time': 33.70057702064514}
I0305 02:00:29.992605 140380326586112 logging_writer.py:48] [464664] accumulated_eval_time=18006.612883, accumulated_logging_time=33.700577, accumulated_submission_time=206740.591018, global_step=464664, preemption_count=0, score=206740.591018, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=224805.507674, train/accuracy=0.887559, train/loss=0.420774, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:00:44.694036 140380334978816 logging_writer.py:48] [464700] global_step=464700, grad_norm=3.0796539783477783, loss=2.2132480144500732
I0305 02:01:26.594877 140380326586112 logging_writer.py:48] [464800] global_step=464800, grad_norm=3.253885269165039, loss=2.664804697036743
I0305 02:02:11.746964 140380334978816 logging_writer.py:48] [464900] global_step=464900, grad_norm=3.932032346725464, loss=3.177896022796631
I0305 02:02:56.808747 140380326586112 logging_writer.py:48] [465000] global_step=465000, grad_norm=3.225799322128296, loss=1.0696660280227661
I0305 02:03:41.934769 140380334978816 logging_writer.py:48] [465100] global_step=465100, grad_norm=3.339385509490967, loss=2.682638645172119
I0305 02:04:27.340057 140380326586112 logging_writer.py:48] [465200] global_step=465200, grad_norm=3.0124623775482178, loss=1.1627197265625
I0305 02:05:12.256390 140380334978816 logging_writer.py:48] [465300] global_step=465300, grad_norm=6.004457473754883, loss=2.5918774604797363
I0305 02:05:56.903393 140380326586112 logging_writer.py:48] [465400] global_step=465400, grad_norm=4.224632740020752, loss=3.0901694297790527
I0305 02:06:42.597429 140380334978816 logging_writer.py:48] [465500] global_step=465500, grad_norm=3.12776255607605, loss=1.3770707845687866
I0305 02:07:27.629975 140380326586112 logging_writer.py:48] [465600] global_step=465600, grad_norm=3.181821823120117, loss=1.8703620433807373
I0305 02:07:30.103179 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:07:40.165056 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:08:05.775326 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:08:07.425076 140575196817216 submission_runner.py:411] Time since start: 225263.03s, 	Step: 465607, 	{'train/accuracy': 0.8863866925239563, 'train/loss': 0.4194602966308594, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 207160.64344906807, 'total_duration': 225263.02623271942, 'accumulated_submission_time': 207160.64344906807, 'accumulated_eval_time': 18043.933857917786, 'accumulated_logging_time': 33.79648423194885}
I0305 02:08:07.511142 140380334978816 logging_writer.py:48] [465607] accumulated_eval_time=18043.933858, accumulated_logging_time=33.796484, accumulated_submission_time=207160.643449, global_step=465607, preemption_count=0, score=207160.643449, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=225263.026233, train/accuracy=0.886387, train/loss=0.419460, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:08:45.140139 140380326586112 logging_writer.py:48] [465700] global_step=465700, grad_norm=3.052227020263672, loss=2.54959774017334
I0305 02:09:29.743865 140380334978816 logging_writer.py:48] [465800] global_step=465800, grad_norm=3.196932077407837, loss=1.5033314228057861
I0305 02:10:14.863089 140380326586112 logging_writer.py:48] [465900] global_step=465900, grad_norm=3.1906862258911133, loss=1.1738090515136719
I0305 02:10:59.607152 140380334978816 logging_writer.py:48] [466000] global_step=466000, grad_norm=3.1886978149414062, loss=2.3410050868988037
I0305 02:11:44.558868 140380326586112 logging_writer.py:48] [466100] global_step=466100, grad_norm=3.17330002784729, loss=1.088628888130188
I0305 02:12:29.466393 140380334978816 logging_writer.py:48] [466200] global_step=466200, grad_norm=3.3768627643585205, loss=2.739598512649536
I0305 02:13:14.405350 140380326586112 logging_writer.py:48] [466300] global_step=466300, grad_norm=3.2685723304748535, loss=2.8474912643432617
I0305 02:13:59.479499 140380334978816 logging_writer.py:48] [466400] global_step=466400, grad_norm=3.3204240798950195, loss=1.3621952533721924
I0305 02:14:44.497991 140380326586112 logging_writer.py:48] [466500] global_step=466500, grad_norm=2.9858617782592773, loss=1.1319583654403687
I0305 02:15:07.773928 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:15:17.676985 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:15:44.090144 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:15:45.755492 140575196817216 submission_runner.py:411] Time since start: 225721.36s, 	Step: 466553, 	{'train/accuracy': 0.88783198595047, 'train/loss': 0.41383716464042664, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 207580.84765076637, 'total_duration': 225721.3563463688, 'accumulated_submission_time': 207580.84765076637, 'accumulated_eval_time': 18081.914219379425, 'accumulated_logging_time': 33.89208626747131}
I0305 02:15:45.842485 140380334978816 logging_writer.py:48] [466553] accumulated_eval_time=18081.914219, accumulated_logging_time=33.892086, accumulated_submission_time=207580.847651, global_step=466553, preemption_count=0, score=207580.847651, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=225721.356346, train/accuracy=0.887832, train/loss=0.413837, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:16:04.928534 140380326586112 logging_writer.py:48] [466600] global_step=466600, grad_norm=3.1381473541259766, loss=1.3241018056869507
I0305 02:16:47.876751 140380334978816 logging_writer.py:48] [466700] global_step=466700, grad_norm=3.14475154876709, loss=1.5735275745391846
I0305 02:17:32.497558 140380326586112 logging_writer.py:48] [466800] global_step=466800, grad_norm=3.495190143585205, loss=1.9723289012908936
I0305 02:18:17.610030 140380334978816 logging_writer.py:48] [466900] global_step=466900, grad_norm=3.300992488861084, loss=1.2639436721801758
I0305 02:19:02.488571 140380326586112 logging_writer.py:48] [467000] global_step=467000, grad_norm=3.1615357398986816, loss=1.1028867959976196
I0305 02:19:47.295879 140380334978816 logging_writer.py:48] [467100] global_step=467100, grad_norm=3.1444828510284424, loss=1.1498836278915405
I0305 02:20:32.407168 140380326586112 logging_writer.py:48] [467200] global_step=467200, grad_norm=3.3683907985687256, loss=1.3135120868682861
I0305 02:21:17.372891 140380334978816 logging_writer.py:48] [467300] global_step=467300, grad_norm=2.933804512023926, loss=1.1664124727249146
I0305 02:22:02.653225 140380326586112 logging_writer.py:48] [467400] global_step=467400, grad_norm=3.0615310668945312, loss=2.3753132820129395
I0305 02:22:45.833847 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:22:55.607221 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:23:20.580956 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:23:22.237921 140575196817216 submission_runner.py:411] Time since start: 226177.84s, 	Step: 467498, 	{'train/accuracy': 0.8881640434265137, 'train/loss': 0.4183202385902405, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 208000.78075790405, 'total_duration': 226177.83900737762, 'accumulated_submission_time': 208000.78075790405, 'accumulated_eval_time': 18118.31734418869, 'accumulated_logging_time': 33.98908615112305}
I0305 02:23:22.327116 140380334978816 logging_writer.py:48] [467498] accumulated_eval_time=18118.317344, accumulated_logging_time=33.989086, accumulated_submission_time=208000.780758, global_step=467498, preemption_count=0, score=208000.780758, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=226177.839007, train/accuracy=0.888164, train/loss=0.418320, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:23:23.520118 140380326586112 logging_writer.py:48] [467500] global_step=467500, grad_norm=3.112574338912964, loss=2.48598575592041
I0305 02:24:04.067170 140380334978816 logging_writer.py:48] [467600] global_step=467600, grad_norm=3.2460215091705322, loss=1.1776163578033447
I0305 02:24:48.604882 140380326586112 logging_writer.py:48] [467700] global_step=467700, grad_norm=2.971470594406128, loss=1.1767480373382568
I0305 02:25:33.783058 140380334978816 logging_writer.py:48] [467800] global_step=467800, grad_norm=3.000312566757202, loss=1.1387012004852295
I0305 02:26:19.028940 140380326586112 logging_writer.py:48] [467900] global_step=467900, grad_norm=3.1160500049591064, loss=1.1602336168289185
I0305 02:27:03.792395 140380334978816 logging_writer.py:48] [468000] global_step=468000, grad_norm=3.391765594482422, loss=1.1893454790115356
I0305 02:27:48.504466 140380326586112 logging_writer.py:48] [468100] global_step=468100, grad_norm=3.4319539070129395, loss=1.3323695659637451
I0305 02:28:33.543677 140380334978816 logging_writer.py:48] [468200] global_step=468200, grad_norm=2.836771011352539, loss=1.8078628778457642
I0305 02:29:18.635045 140380326586112 logging_writer.py:48] [468300] global_step=468300, grad_norm=3.260565996170044, loss=1.1125239133834839
I0305 02:30:03.511070 140380334978816 logging_writer.py:48] [468400] global_step=468400, grad_norm=3.2419791221618652, loss=1.8121618032455444
I0305 02:30:22.562171 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:30:32.522424 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:31:02.081627 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:31:03.727389 140575196817216 submission_runner.py:411] Time since start: 226639.33s, 	Step: 468444, 	{'train/accuracy': 0.8898242115974426, 'train/loss': 0.41509810090065, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 208420.95669198036, 'total_duration': 226639.32850503922, 'accumulated_submission_time': 208420.95669198036, 'accumulated_eval_time': 18159.481626987457, 'accumulated_logging_time': 34.088141679763794}
I0305 02:31:03.813767 140380326586112 logging_writer.py:48] [468444] accumulated_eval_time=18159.481627, accumulated_logging_time=34.088142, accumulated_submission_time=208420.956692, global_step=468444, preemption_count=0, score=208420.956692, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=226639.328505, train/accuracy=0.889824, train/loss=0.415098, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:31:26.555929 140380334978816 logging_writer.py:48] [468500] global_step=468500, grad_norm=3.2548272609710693, loss=2.5609045028686523
I0305 02:32:09.669080 140380326586112 logging_writer.py:48] [468600] global_step=468600, grad_norm=3.482839345932007, loss=1.1296560764312744
I0305 02:32:54.609520 140380334978816 logging_writer.py:48] [468700] global_step=468700, grad_norm=3.2227206230163574, loss=2.7965762615203857
I0305 02:33:39.746424 140380326586112 logging_writer.py:48] [468800] global_step=468800, grad_norm=3.481595993041992, loss=2.8245253562927246
I0305 02:34:24.767807 140380334978816 logging_writer.py:48] [468900] global_step=468900, grad_norm=3.331968069076538, loss=3.0611040592193604
I0305 02:35:10.157193 140380326586112 logging_writer.py:48] [469000] global_step=469000, grad_norm=3.05228590965271, loss=1.0906522274017334
I0305 02:35:54.949888 140380334978816 logging_writer.py:48] [469100] global_step=469100, grad_norm=2.9330954551696777, loss=0.979131281375885
I0305 02:36:40.100948 140380326586112 logging_writer.py:48] [469200] global_step=469200, grad_norm=3.623861789703369, loss=3.050088405609131
I0305 02:37:25.093867 140380334978816 logging_writer.py:48] [469300] global_step=469300, grad_norm=3.076443910598755, loss=1.1053773164749146
I0305 02:38:04.096508 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:38:13.818319 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:38:37.880565 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:38:39.530248 140575196817216 submission_runner.py:411] Time since start: 227095.13s, 	Step: 469389, 	{'train/accuracy': 0.8898828029632568, 'train/loss': 0.40898746252059937, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 208841.18082380295, 'total_duration': 227095.1313097477, 'accumulated_submission_time': 208841.18082380295, 'accumulated_eval_time': 18194.914368391037, 'accumulated_logging_time': 34.18496608734131}
I0305 02:38:39.617054 140380326586112 logging_writer.py:48] [469389] accumulated_eval_time=18194.914368, accumulated_logging_time=34.184966, accumulated_submission_time=208841.180824, global_step=469389, preemption_count=0, score=208841.180824, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=227095.131310, train/accuracy=0.889883, train/loss=0.408987, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:38:44.389759 140380334978816 logging_writer.py:48] [469400] global_step=469400, grad_norm=3.1383094787597656, loss=1.1389552354812622
I0305 02:39:25.103823 140380326586112 logging_writer.py:48] [469500] global_step=469500, grad_norm=2.8888869285583496, loss=1.137800693511963
I0305 02:40:10.014779 140380334978816 logging_writer.py:48] [469600] global_step=469600, grad_norm=3.4280853271484375, loss=2.5155892372131348
I0305 02:40:55.109057 140380326586112 logging_writer.py:48] [469700] global_step=469700, grad_norm=3.025409460067749, loss=1.9540197849273682
I0305 02:41:40.054739 140380334978816 logging_writer.py:48] [469800] global_step=469800, grad_norm=3.0069031715393066, loss=1.7807543277740479
I0305 02:42:25.080609 140380326586112 logging_writer.py:48] [469900] global_step=469900, grad_norm=3.1415281295776367, loss=1.108068823814392
I0305 02:43:09.883360 140380334978816 logging_writer.py:48] [470000] global_step=470000, grad_norm=3.990769147872925, loss=2.586275815963745
I0305 02:43:54.608370 140380326586112 logging_writer.py:48] [470100] global_step=470100, grad_norm=2.9285528659820557, loss=1.8704144954681396
I0305 02:44:39.588330 140380334978816 logging_writer.py:48] [470200] global_step=470200, grad_norm=3.1632802486419678, loss=1.1722099781036377
I0305 02:45:24.289294 140380326586112 logging_writer.py:48] [470300] global_step=470300, grad_norm=3.687978744506836, loss=1.6849818229675293
I0305 02:45:39.606981 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:45:49.582863 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:46:15.001171 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:46:16.657604 140575196817216 submission_runner.py:411] Time since start: 227552.26s, 	Step: 470336, 	{'train/accuracy': 0.8884375095367432, 'train/loss': 0.4139959514141083, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 209261.11147785187, 'total_duration': 227552.25866436958, 'accumulated_submission_time': 209261.11147785187, 'accumulated_eval_time': 18231.963979959488, 'accumulated_logging_time': 34.282028675079346}
I0305 02:46:16.744861 140380334978816 logging_writer.py:48] [470336] accumulated_eval_time=18231.963980, accumulated_logging_time=34.282029, accumulated_submission_time=209261.111478, global_step=470336, preemption_count=0, score=209261.111478, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=227552.258664, train/accuracy=0.888438, train/loss=0.413996, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:46:42.584061 140380326586112 logging_writer.py:48] [470400] global_step=470400, grad_norm=4.2876811027526855, loss=2.6631405353546143
I0305 02:47:26.179233 140380334978816 logging_writer.py:48] [470500] global_step=470500, grad_norm=3.043787717819214, loss=1.5661239624023438
I0305 02:48:11.151901 140380326586112 logging_writer.py:48] [470600] global_step=470600, grad_norm=3.175079345703125, loss=1.1015115976333618
I0305 02:48:56.407469 140380334978816 logging_writer.py:48] [470700] global_step=470700, grad_norm=3.553281784057617, loss=1.1858052015304565
I0305 02:49:41.226625 140380326586112 logging_writer.py:48] [470800] global_step=470800, grad_norm=2.7727017402648926, loss=1.4304550886154175
I0305 02:50:26.260992 140380334978816 logging_writer.py:48] [470900] global_step=470900, grad_norm=3.634711980819702, loss=3.140324592590332
I0305 02:51:11.248260 140380326586112 logging_writer.py:48] [471000] global_step=471000, grad_norm=3.0800867080688477, loss=1.0713543891906738
I0305 02:51:56.152065 140380334978816 logging_writer.py:48] [471100] global_step=471100, grad_norm=3.5284459590911865, loss=1.222495198249817
I0305 02:52:41.190644 140380326586112 logging_writer.py:48] [471200] global_step=471200, grad_norm=3.1618235111236572, loss=1.221722960472107
I0305 02:53:16.873692 140575196817216 spec.py:321] Evaluating on the training split.
I0305 02:53:27.554096 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 02:53:53.081294 140575196817216 spec.py:349] Evaluating on the test split.
I0305 02:53:54.735683 140575196817216 submission_runner.py:411] Time since start: 228010.34s, 	Step: 471281, 	{'train/accuracy': 0.8876171708106995, 'train/loss': 0.42104074358940125, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 209681.18304800987, 'total_duration': 228010.33667635918, 'accumulated_submission_time': 209681.18304800987, 'accumulated_eval_time': 18269.824891090393, 'accumulated_logging_time': 34.37890291213989}
I0305 02:53:54.824831 140380334978816 logging_writer.py:48] [471281] accumulated_eval_time=18269.824891, accumulated_logging_time=34.378903, accumulated_submission_time=209681.183048, global_step=471281, preemption_count=0, score=209681.183048, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=228010.336676, train/accuracy=0.887617, train/loss=0.421041, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 02:54:02.779373 140380326586112 logging_writer.py:48] [471300] global_step=471300, grad_norm=3.0576088428497314, loss=1.7287839651107788
I0305 02:54:43.804318 140380334978816 logging_writer.py:48] [471400] global_step=471400, grad_norm=3.276956558227539, loss=1.1262158155441284
I0305 02:55:28.732026 140380326586112 logging_writer.py:48] [471500] global_step=471500, grad_norm=3.06683611869812, loss=1.170907974243164
I0305 02:56:14.232116 140380334978816 logging_writer.py:48] [471600] global_step=471600, grad_norm=3.0361087322235107, loss=1.4450989961624146
I0305 02:56:59.744257 140380326586112 logging_writer.py:48] [471700] global_step=471700, grad_norm=3.2373218536376953, loss=1.0936797857284546
I0305 02:57:44.400245 140380334978816 logging_writer.py:48] [471800] global_step=471800, grad_norm=2.9999942779541016, loss=1.7073196172714233
I0305 02:58:29.650325 140380326586112 logging_writer.py:48] [471900] global_step=471900, grad_norm=3.283552646636963, loss=1.2877553701400757
I0305 02:59:14.458981 140380334978816 logging_writer.py:48] [472000] global_step=472000, grad_norm=3.073352098464966, loss=1.7985764741897583
I0305 02:59:59.555436 140380326586112 logging_writer.py:48] [472100] global_step=472100, grad_norm=3.0132250785827637, loss=1.3344124555587769
I0305 03:00:44.458795 140380334978816 logging_writer.py:48] [472200] global_step=472200, grad_norm=3.2296435832977295, loss=1.1535841226577759
I0305 03:00:54.971715 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:01:04.710070 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:01:31.570137 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:01:33.216288 140575196817216 submission_runner.py:411] Time since start: 228468.82s, 	Step: 472225, 	{'train/accuracy': 0.8863866925239563, 'train/loss': 0.4219321608543396, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 210101.27172207832, 'total_duration': 228468.8173315525, 'accumulated_submission_time': 210101.27172207832, 'accumulated_eval_time': 18308.068465471268, 'accumulated_logging_time': 34.47755169868469}
I0305 03:01:33.303817 140380326586112 logging_writer.py:48] [472225] accumulated_eval_time=18308.068465, accumulated_logging_time=34.477552, accumulated_submission_time=210101.271722, global_step=472225, preemption_count=0, score=210101.271722, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=228468.817332, train/accuracy=0.886387, train/loss=0.421932, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:02:03.504352 140380334978816 logging_writer.py:48] [472300] global_step=472300, grad_norm=3.1064536571502686, loss=1.929896593093872
I0305 03:02:47.633069 140380326586112 logging_writer.py:48] [472400] global_step=472400, grad_norm=3.2802698612213135, loss=2.1034278869628906
I0305 03:03:32.815736 140380334978816 logging_writer.py:48] [472500] global_step=472500, grad_norm=3.3450398445129395, loss=1.1803970336914062
I0305 03:04:17.798310 140380326586112 logging_writer.py:48] [472600] global_step=472600, grad_norm=3.415696382522583, loss=1.150364637374878
I0305 03:05:02.502502 140380334978816 logging_writer.py:48] [472700] global_step=472700, grad_norm=2.948305368423462, loss=1.1152551174163818
I0305 03:05:47.446285 140380326586112 logging_writer.py:48] [472800] global_step=472800, grad_norm=3.054273843765259, loss=2.2981085777282715
I0305 03:06:32.568928 140380334978816 logging_writer.py:48] [472900] global_step=472900, grad_norm=3.1837711334228516, loss=1.2230888605117798
I0305 03:07:17.772054 140380326586112 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.121070384979248, loss=3.2156710624694824
I0305 03:08:02.863606 140380334978816 logging_writer.py:48] [473100] global_step=473100, grad_norm=2.9180727005004883, loss=1.089253544807434
I0305 03:08:33.278086 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:08:42.829271 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:09:10.339159 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:09:11.994023 140575196817216 submission_runner.py:411] Time since start: 228927.59s, 	Step: 473169, 	{'train/accuracy': 0.8877539038658142, 'train/loss': 0.41849520802497864, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 210521.1846988201, 'total_duration': 228927.59491848946, 'accumulated_submission_time': 210521.1846988201, 'accumulated_eval_time': 18346.783236026764, 'accumulated_logging_time': 34.57780885696411}
I0305 03:09:12.096457 140380326586112 logging_writer.py:48] [473169] accumulated_eval_time=18346.783236, accumulated_logging_time=34.577809, accumulated_submission_time=210521.184699, global_step=473169, preemption_count=0, score=210521.184699, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=228927.594918, train/accuracy=0.887754, train/loss=0.418495, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:09:24.821593 140380334978816 logging_writer.py:48] [473200] global_step=473200, grad_norm=3.301651954650879, loss=1.2383434772491455
I0305 03:10:06.845393 140380326586112 logging_writer.py:48] [473300] global_step=473300, grad_norm=3.0266735553741455, loss=1.3596833944320679
I0305 03:10:51.869109 140380334978816 logging_writer.py:48] [473400] global_step=473400, grad_norm=4.020298957824707, loss=2.247011423110962
I0305 03:11:37.072939 140380326586112 logging_writer.py:48] [473500] global_step=473500, grad_norm=3.2875709533691406, loss=1.5566329956054688
I0305 03:12:21.951617 140380334978816 logging_writer.py:48] [473600] global_step=473600, grad_norm=3.240272283554077, loss=1.1213923692703247
I0305 03:13:06.625267 140380326586112 logging_writer.py:48] [473700] global_step=473700, grad_norm=3.342810869216919, loss=1.2345519065856934
I0305 03:13:51.497575 140380334978816 logging_writer.py:48] [473800] global_step=473800, grad_norm=2.950674057006836, loss=1.469260573387146
I0305 03:14:36.514301 140380326586112 logging_writer.py:48] [473900] global_step=473900, grad_norm=3.0771961212158203, loss=1.1001527309417725
I0305 03:15:21.497930 140380334978816 logging_writer.py:48] [474000] global_step=474000, grad_norm=3.289348602294922, loss=1.0503863096237183
I0305 03:16:06.657140 140380326586112 logging_writer.py:48] [474100] global_step=474100, grad_norm=3.5229547023773193, loss=2.8135993480682373
I0305 03:16:12.252267 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:16:22.034409 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:16:53.082437 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:16:54.743007 140575196817216 submission_runner.py:411] Time since start: 229390.34s, 	Step: 474114, 	{'train/accuracy': 0.8903319835662842, 'train/loss': 0.4145033657550812, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 210941.28046417236, 'total_duration': 229390.34396600723, 'accumulated_submission_time': 210941.28046417236, 'accumulated_eval_time': 18389.272876262665, 'accumulated_logging_time': 34.690468072891235}
I0305 03:16:54.829581 140380334978816 logging_writer.py:48] [474114] accumulated_eval_time=18389.272876, accumulated_logging_time=34.690468, accumulated_submission_time=210941.280464, global_step=474114, preemption_count=0, score=210941.280464, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=229390.343966, train/accuracy=0.890332, train/loss=0.414503, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:17:29.642107 140380326586112 logging_writer.py:48] [474200] global_step=474200, grad_norm=3.415475606918335, loss=1.0786584615707397
I0305 03:18:14.210391 140380334978816 logging_writer.py:48] [474300] global_step=474300, grad_norm=3.0108978748321533, loss=1.30167818069458
I0305 03:18:59.459227 140380326586112 logging_writer.py:48] [474400] global_step=474400, grad_norm=3.2395694255828857, loss=1.3565617799758911
I0305 03:19:44.602117 140380334978816 logging_writer.py:48] [474500] global_step=474500, grad_norm=2.9393012523651123, loss=1.1810330152511597
I0305 03:20:29.388363 140380326586112 logging_writer.py:48] [474600] global_step=474600, grad_norm=3.226971387863159, loss=2.2830307483673096
I0305 03:21:14.193212 140380334978816 logging_writer.py:48] [474700] global_step=474700, grad_norm=3.2448084354400635, loss=1.3606821298599243
I0305 03:21:59.033124 140380326586112 logging_writer.py:48] [474800] global_step=474800, grad_norm=3.007368564605713, loss=1.1729423999786377
I0305 03:22:44.228088 140380334978816 logging_writer.py:48] [474900] global_step=474900, grad_norm=3.3313183784484863, loss=1.076155185699463
I0305 03:23:29.231758 140380326586112 logging_writer.py:48] [475000] global_step=475000, grad_norm=3.0705246925354004, loss=1.2107781171798706
I0305 03:23:54.801478 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:24:04.661545 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:24:30.407150 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:24:32.075875 140575196817216 submission_runner.py:411] Time since start: 229847.68s, 	Step: 475059, 	{'train/accuracy': 0.8855859041213989, 'train/loss': 0.4250911474227905, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 211361.19549322128, 'total_duration': 229847.67683005333, 'accumulated_submission_time': 211361.19549322128, 'accumulated_eval_time': 18426.546167850494, 'accumulated_logging_time': 34.78611469268799}
I0305 03:24:32.179961 140380334978816 logging_writer.py:48] [475059] accumulated_eval_time=18426.546168, accumulated_logging_time=34.786115, accumulated_submission_time=211361.195493, global_step=475059, preemption_count=0, score=211361.195493, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=229847.676830, train/accuracy=0.885586, train/loss=0.425091, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:24:48.864784 140380326586112 logging_writer.py:48] [475100] global_step=475100, grad_norm=3.1901872158050537, loss=1.1273261308670044
I0305 03:25:30.811944 140380334978816 logging_writer.py:48] [475200] global_step=475200, grad_norm=3.380734920501709, loss=1.6660817861557007
I0305 03:26:16.063637 140380326586112 logging_writer.py:48] [475300] global_step=475300, grad_norm=3.283679723739624, loss=1.1687647104263306
I0305 03:27:01.133465 140380334978816 logging_writer.py:48] [475400] global_step=475400, grad_norm=3.4833319187164307, loss=2.760676622390747
I0305 03:27:46.426948 140380326586112 logging_writer.py:48] [475500] global_step=475500, grad_norm=3.1156790256500244, loss=1.0995954275131226
I0305 03:28:31.493534 140380334978816 logging_writer.py:48] [475600] global_step=475600, grad_norm=3.2108147144317627, loss=1.4551849365234375
I0305 03:29:16.871553 140380326586112 logging_writer.py:48] [475700] global_step=475700, grad_norm=2.897916555404663, loss=1.6002519130706787
I0305 03:30:01.954255 140380334978816 logging_writer.py:48] [475800] global_step=475800, grad_norm=3.2129814624786377, loss=1.1107536554336548
I0305 03:30:47.208707 140380326586112 logging_writer.py:48] [475900] global_step=475900, grad_norm=2.902628183364868, loss=1.8614249229431152
I0305 03:31:32.213433 140380334978816 logging_writer.py:48] [476000] global_step=476000, grad_norm=3.191894292831421, loss=1.1590496301651
I0305 03:31:32.225880 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:31:42.035852 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:32:10.118746 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:32:11.783280 140575196817216 submission_runner.py:411] Time since start: 230307.38s, 	Step: 476001, 	{'train/accuracy': 0.8862109184265137, 'train/loss': 0.42048144340515137, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 211781.18303704262, 'total_duration': 230307.38406157494, 'accumulated_submission_time': 211781.18303704262, 'accumulated_eval_time': 18466.102340698242, 'accumulated_logging_time': 34.90059804916382}
I0305 03:32:11.871066 140380326586112 logging_writer.py:48] [476001] accumulated_eval_time=18466.102341, accumulated_logging_time=34.900598, accumulated_submission_time=211781.183037, global_step=476001, preemption_count=0, score=211781.183037, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=230307.384062, train/accuracy=0.886211, train/loss=0.420481, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:32:52.133686 140380334978816 logging_writer.py:48] [476100] global_step=476100, grad_norm=3.4793038368225098, loss=2.5116422176361084
I0305 03:33:36.982873 140380326586112 logging_writer.py:48] [476200] global_step=476200, grad_norm=3.1508679389953613, loss=1.5807303190231323
I0305 03:34:22.190585 140380334978816 logging_writer.py:48] [476300] global_step=476300, grad_norm=3.104038715362549, loss=1.5057587623596191
I0305 03:35:07.099074 140380326586112 logging_writer.py:48] [476400] global_step=476400, grad_norm=3.118222951889038, loss=1.9667819738388062
I0305 03:35:52.226603 140380334978816 logging_writer.py:48] [476500] global_step=476500, grad_norm=3.281928062438965, loss=1.1379867792129517
I0305 03:36:37.506087 140380326586112 logging_writer.py:48] [476600] global_step=476600, grad_norm=3.084988594055176, loss=1.3106391429901123
I0305 03:37:22.551520 140380334978816 logging_writer.py:48] [476700] global_step=476700, grad_norm=3.300311326980591, loss=1.0867643356323242
I0305 03:38:07.549623 140380326586112 logging_writer.py:48] [476800] global_step=476800, grad_norm=2.952631711959839, loss=1.924912929534912
I0305 03:38:52.677664 140380334978816 logging_writer.py:48] [476900] global_step=476900, grad_norm=3.4248106479644775, loss=2.679234027862549
I0305 03:39:11.824201 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:39:21.344221 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:39:48.534320 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:39:50.190747 140575196817216 submission_runner.py:411] Time since start: 230765.79s, 	Step: 476944, 	{'train/accuracy': 0.8863866925239563, 'train/loss': 0.4251146614551544, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 212201.07793593407, 'total_duration': 230765.79165744781, 'accumulated_submission_time': 212201.07793593407, 'accumulated_eval_time': 18504.467737436295, 'accumulated_logging_time': 34.99848484992981}
I0305 03:39:50.278250 140380326586112 logging_writer.py:48] [476944] accumulated_eval_time=18504.467737, accumulated_logging_time=34.998485, accumulated_submission_time=212201.077936, global_step=476944, preemption_count=0, score=212201.077936, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=230765.791657, train/accuracy=0.886387, train/loss=0.425115, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:40:12.929700 140380334978816 logging_writer.py:48] [477000] global_step=477000, grad_norm=3.09877347946167, loss=1.6260621547698975
I0305 03:40:55.887314 140380326586112 logging_writer.py:48] [477100] global_step=477100, grad_norm=3.5435640811920166, loss=1.0179409980773926
I0305 03:41:40.882276 140380334978816 logging_writer.py:48] [477200] global_step=477200, grad_norm=3.1200456619262695, loss=1.279655933380127
I0305 03:42:25.915216 140380326586112 logging_writer.py:48] [477300] global_step=477300, grad_norm=3.161694049835205, loss=2.25687313079834
I0305 03:43:10.478098 140380334978816 logging_writer.py:48] [477400] global_step=477400, grad_norm=3.1883301734924316, loss=1.174913763999939
I0305 03:43:55.458146 140380326586112 logging_writer.py:48] [477500] global_step=477500, grad_norm=2.932459831237793, loss=1.083184003829956
I0305 03:44:40.279364 140380334978816 logging_writer.py:48] [477600] global_step=477600, grad_norm=2.932652711868286, loss=1.2133055925369263
I0305 03:45:25.146600 140380326586112 logging_writer.py:48] [477700] global_step=477700, grad_norm=3.55399751663208, loss=2.868626594543457
I0305 03:46:09.954562 140380334978816 logging_writer.py:48] [477800] global_step=477800, grad_norm=4.561391353607178, loss=3.2413387298583984
I0305 03:46:50.320206 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:46:59.817157 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:47:28.495675 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:47:30.145869 140575196817216 submission_runner.py:411] Time since start: 231225.75s, 	Step: 477892, 	{'train/accuracy': 0.88671875, 'train/loss': 0.4219008982181549, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 212621.0614106655, 'total_duration': 231225.74681663513, 'accumulated_submission_time': 212621.0614106655, 'accumulated_eval_time': 18544.29231786728, 'accumulated_logging_time': 35.0954225063324}
I0305 03:47:30.233091 140380326586112 logging_writer.py:48] [477892] accumulated_eval_time=18544.292318, accumulated_logging_time=35.095423, accumulated_submission_time=212621.061411, global_step=477892, preemption_count=0, score=212621.061411, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=231225.746817, train/accuracy=0.886719, train/loss=0.421901, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:47:33.815056 140380334978816 logging_writer.py:48] [477900] global_step=477900, grad_norm=3.0245542526245117, loss=1.1660449504852295
I0305 03:48:14.840842 140380326586112 logging_writer.py:48] [478000] global_step=478000, grad_norm=3.3983120918273926, loss=1.233575701713562
I0305 03:48:59.812653 140380334978816 logging_writer.py:48] [478100] global_step=478100, grad_norm=3.1439075469970703, loss=1.0896886587142944
I0305 03:49:45.111137 140380326586112 logging_writer.py:48] [478200] global_step=478200, grad_norm=3.330711603164673, loss=2.7390894889831543
I0305 03:50:30.023276 140380334978816 logging_writer.py:48] [478300] global_step=478300, grad_norm=2.767381429672241, loss=1.6548365354537964
I0305 03:51:15.062407 140380326586112 logging_writer.py:48] [478400] global_step=478400, grad_norm=3.727843761444092, loss=3.1214048862457275
I0305 03:52:00.081677 140380334978816 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.048079013824463, loss=3.089724063873291
I0305 03:52:45.008786 140380326586112 logging_writer.py:48] [478600] global_step=478600, grad_norm=4.458642959594727, loss=3.2419650554656982
I0305 03:53:30.043619 140380334978816 logging_writer.py:48] [478700] global_step=478700, grad_norm=3.169248104095459, loss=1.135591983795166
I0305 03:54:15.004655 140380326586112 logging_writer.py:48] [478800] global_step=478800, grad_norm=3.478630304336548, loss=2.4999780654907227
I0305 03:54:30.409654 140575196817216 spec.py:321] Evaluating on the training split.
I0305 03:54:40.119355 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 03:55:07.938193 140575196817216 spec.py:349] Evaluating on the test split.
I0305 03:55:09.591951 140575196817216 submission_runner.py:411] Time since start: 231685.19s, 	Step: 478836, 	{'train/accuracy': 0.8879687190055847, 'train/loss': 0.41775888204574585, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 213041.17976403236, 'total_duration': 231685.19267487526, 'accumulated_submission_time': 213041.17976403236, 'accumulated_eval_time': 18583.473279476166, 'accumulated_logging_time': 35.19288158416748}
I0305 03:55:09.692281 140380334978816 logging_writer.py:48] [478836] accumulated_eval_time=18583.473279, accumulated_logging_time=35.192882, accumulated_submission_time=213041.179764, global_step=478836, preemption_count=0, score=213041.179764, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=231685.192675, train/accuracy=0.887969, train/loss=0.417759, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 03:55:35.520642 140380326586112 logging_writer.py:48] [478900] global_step=478900, grad_norm=3.1718695163726807, loss=1.1274741888046265
I0305 03:56:19.292972 140380334978816 logging_writer.py:48] [479000] global_step=479000, grad_norm=3.0760600566864014, loss=2.032207489013672
I0305 03:57:04.300720 140380326586112 logging_writer.py:48] [479100] global_step=479100, grad_norm=2.969691514968872, loss=1.1323610544204712
I0305 03:57:49.270165 140380334978816 logging_writer.py:48] [479200] global_step=479200, grad_norm=2.967719316482544, loss=2.3118386268615723
I0305 03:58:34.248143 140380326586112 logging_writer.py:48] [479300] global_step=479300, grad_norm=2.9033493995666504, loss=1.251419186592102
I0305 03:59:19.526113 140380334978816 logging_writer.py:48] [479400] global_step=479400, grad_norm=3.1332385540008545, loss=1.1360291242599487
I0305 04:00:04.381197 140380326586112 logging_writer.py:48] [479500] global_step=479500, grad_norm=2.9838333129882812, loss=1.063921570777893
I0305 04:00:49.264959 140380334978816 logging_writer.py:48] [479600] global_step=479600, grad_norm=3.0389211177825928, loss=1.722650170326233
I0305 04:01:34.455842 140380326586112 logging_writer.py:48] [479700] global_step=479700, grad_norm=4.247897624969482, loss=3.2425193786621094
I0305 04:02:09.672942 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:02:19.461408 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:02:47.553289 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:02:49.204941 140575196817216 submission_runner.py:411] Time since start: 232144.81s, 	Step: 479780, 	{'train/accuracy': 0.8868749737739563, 'train/loss': 0.4182649850845337, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 213461.09774923325, 'total_duration': 232144.80581593513, 'accumulated_submission_time': 213461.09774923325, 'accumulated_eval_time': 18623.00409412384, 'accumulated_logging_time': 35.30354833602905}
I0305 04:02:49.294337 140380334978816 logging_writer.py:48] [479780] accumulated_eval_time=18623.004094, accumulated_logging_time=35.303548, accumulated_submission_time=213461.097749, global_step=479780, preemption_count=0, score=213461.097749, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=232144.805816, train/accuracy=0.886875, train/loss=0.418265, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:02:57.642244 140380326586112 logging_writer.py:48] [479800] global_step=479800, grad_norm=2.8376097679138184, loss=1.0308979749679565
I0305 04:03:38.817970 140380334978816 logging_writer.py:48] [479900] global_step=479900, grad_norm=3.0283102989196777, loss=1.6723484992980957
I0305 04:04:23.953201 140380326586112 logging_writer.py:48] [480000] global_step=480000, grad_norm=3.3535051345825195, loss=2.697380781173706
I0305 04:05:08.883460 140380334978816 logging_writer.py:48] [480100] global_step=480100, grad_norm=3.3293020725250244, loss=1.1093419790267944
I0305 04:05:53.879504 140380326586112 logging_writer.py:48] [480200] global_step=480200, grad_norm=2.9612395763397217, loss=1.1457175016403198
I0305 04:06:39.401061 140380334978816 logging_writer.py:48] [480300] global_step=480300, grad_norm=3.293029546737671, loss=1.794264793395996
I0305 04:07:24.477140 140380326586112 logging_writer.py:48] [480400] global_step=480400, grad_norm=2.974482297897339, loss=2.3380181789398193
I0305 04:08:10.020153 140380334978816 logging_writer.py:48] [480500] global_step=480500, grad_norm=2.945859909057617, loss=1.0408540964126587
I0305 04:08:55.255992 140380326586112 logging_writer.py:48] [480600] global_step=480600, grad_norm=3.043607234954834, loss=1.1523141860961914
I0305 04:09:40.496068 140380334978816 logging_writer.py:48] [480700] global_step=480700, grad_norm=3.6421782970428467, loss=1.7525734901428223
I0305 04:09:49.628351 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:09:59.266788 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:10:22.915227 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:10:24.571099 140575196817216 submission_runner.py:411] Time since start: 232600.17s, 	Step: 480722, 	{'train/accuracy': 0.8855273127555847, 'train/loss': 0.4250691831111908, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 213881.37053871155, 'total_duration': 232600.1719095707, 'accumulated_submission_time': 213881.37053871155, 'accumulated_eval_time': 18657.945585012436, 'accumulated_logging_time': 35.4053909778595}
I0305 04:10:24.659189 140380326586112 logging_writer.py:48] [480722] accumulated_eval_time=18657.945585, accumulated_logging_time=35.405391, accumulated_submission_time=213881.370539, global_step=480722, preemption_count=0, score=213881.370539, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=232600.171910, train/accuracy=0.885527, train/loss=0.425069, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:10:56.084831 140380334978816 logging_writer.py:48] [480800] global_step=480800, grad_norm=3.2911293506622314, loss=2.363856554031372
I0305 04:11:40.628492 140380326586112 logging_writer.py:48] [480900] global_step=480900, grad_norm=2.8297064304351807, loss=1.348652958869934
I0305 04:12:26.279948 140380334978816 logging_writer.py:48] [481000] global_step=481000, grad_norm=3.1337642669677734, loss=1.185634970664978
I0305 04:13:11.594072 140380326586112 logging_writer.py:48] [481100] global_step=481100, grad_norm=3.1006195545196533, loss=1.364539623260498
I0305 04:13:56.378618 140380334978816 logging_writer.py:48] [481200] global_step=481200, grad_norm=3.0015032291412354, loss=1.0881407260894775
I0305 04:14:41.335875 140380326586112 logging_writer.py:48] [481300] global_step=481300, grad_norm=3.9528141021728516, loss=3.2763171195983887
I0305 04:15:26.440899 140380334978816 logging_writer.py:48] [481400] global_step=481400, grad_norm=3.1677935123443604, loss=2.0215229988098145
I0305 04:16:11.308287 140380326586112 logging_writer.py:48] [481500] global_step=481500, grad_norm=3.5798141956329346, loss=1.1217222213745117
I0305 04:16:56.182828 140380334978816 logging_writer.py:48] [481600] global_step=481600, grad_norm=3.905930995941162, loss=3.3453593254089355
I0305 04:17:24.666695 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:17:34.437385 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:18:01.680967 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:18:03.335260 140575196817216 submission_runner.py:411] Time since start: 233058.94s, 	Step: 481665, 	{'train/accuracy': 0.8876171708106995, 'train/loss': 0.4181188941001892, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 214301.31644773483, 'total_duration': 233058.9361166954, 'accumulated_submission_time': 214301.31644773483, 'accumulated_eval_time': 18696.6129488945, 'accumulated_logging_time': 35.506397008895874}
I0305 04:18:03.422966 140380326586112 logging_writer.py:48] [481665] accumulated_eval_time=18696.612949, accumulated_logging_time=35.506397, accumulated_submission_time=214301.316448, global_step=481665, preemption_count=0, score=214301.316448, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=233058.936117, train/accuracy=0.887617, train/loss=0.418119, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:18:17.730887 140380334978816 logging_writer.py:48] [481700] global_step=481700, grad_norm=3.0221729278564453, loss=1.7576664686203003
I0305 04:18:59.853750 140380326586112 logging_writer.py:48] [481800] global_step=481800, grad_norm=4.899204730987549, loss=3.2387921810150146
I0305 04:19:44.888134 140380334978816 logging_writer.py:48] [481900] global_step=481900, grad_norm=3.2271804809570312, loss=1.0537118911743164
I0305 04:20:30.232934 140380326586112 logging_writer.py:48] [482000] global_step=482000, grad_norm=3.068206548690796, loss=1.4045671224594116
I0305 04:21:15.145333 140380334978816 logging_writer.py:48] [482100] global_step=482100, grad_norm=3.0511670112609863, loss=1.023528814315796
I0305 04:22:00.168962 140380326586112 logging_writer.py:48] [482200] global_step=482200, grad_norm=2.9824771881103516, loss=1.0955784320831299
I0305 04:22:45.070990 140380334978816 logging_writer.py:48] [482300] global_step=482300, grad_norm=3.8307464122772217, loss=1.388235092163086
I0305 04:23:30.197800 140380326586112 logging_writer.py:48] [482400] global_step=482400, grad_norm=3.4394891262054443, loss=2.883721113204956
I0305 04:24:15.643399 140380334978816 logging_writer.py:48] [482500] global_step=482500, grad_norm=3.1166141033172607, loss=1.136400818824768
I0305 04:25:00.399070 140380326586112 logging_writer.py:48] [482600] global_step=482600, grad_norm=3.999436140060425, loss=2.775801181793213
I0305 04:25:03.757508 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:25:13.756060 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:25:41.896083 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:25:43.555172 140575196817216 submission_runner.py:411] Time since start: 233519.16s, 	Step: 482609, 	{'train/accuracy': 0.8867382407188416, 'train/loss': 0.41965851187705994, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 214721.59282803535, 'total_duration': 233519.15626096725, 'accumulated_submission_time': 214721.59282803535, 'accumulated_eval_time': 18736.40962123871, 'accumulated_logging_time': 35.604432344436646}
I0305 04:25:43.644457 140380334978816 logging_writer.py:48] [482609] accumulated_eval_time=18736.409621, accumulated_logging_time=35.604432, accumulated_submission_time=214721.592828, global_step=482609, preemption_count=0, score=214721.592828, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=233519.156261, train/accuracy=0.886738, train/loss=0.419659, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:26:20.639879 140380326586112 logging_writer.py:48] [482700] global_step=482700, grad_norm=2.7662270069122314, loss=0.9696796536445618
I0305 04:27:05.530500 140380334978816 logging_writer.py:48] [482800] global_step=482800, grad_norm=2.9831185340881348, loss=1.0031555891036987
I0305 04:27:50.185240 140380326586112 logging_writer.py:48] [482900] global_step=482900, grad_norm=2.9290285110473633, loss=1.1698194742202759
I0305 04:28:35.793882 140380334978816 logging_writer.py:48] [483000] global_step=483000, grad_norm=3.3352479934692383, loss=2.110335111618042
I0305 04:29:20.880722 140380326586112 logging_writer.py:48] [483100] global_step=483100, grad_norm=3.2005648612976074, loss=1.765028476715088
I0305 04:30:05.855137 140380334978816 logging_writer.py:48] [483200] global_step=483200, grad_norm=3.523932933807373, loss=1.8981614112854004
I0305 04:30:51.029171 140380326586112 logging_writer.py:48] [483300] global_step=483300, grad_norm=3.3498830795288086, loss=2.6277878284454346
I0305 04:31:36.172460 140380334978816 logging_writer.py:48] [483400] global_step=483400, grad_norm=3.342763662338257, loss=2.6682560443878174
I0305 04:32:21.165878 140380326586112 logging_writer.py:48] [483500] global_step=483500, grad_norm=3.141251802444458, loss=1.3121833801269531
I0305 04:32:43.743476 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:32:53.566397 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:33:18.970976 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:33:20.622997 140575196817216 submission_runner.py:411] Time since start: 233976.22s, 	Step: 483552, 	{'train/accuracy': 0.8869335651397705, 'train/loss': 0.4238941967487335, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 215141.62749814987, 'total_duration': 233976.2240343094, 'accumulated_submission_time': 215141.62749814987, 'accumulated_eval_time': 18773.288115262985, 'accumulated_logging_time': 35.703930616378784}
I0305 04:33:20.711674 140380334978816 logging_writer.py:48] [483552] accumulated_eval_time=18773.288115, accumulated_logging_time=35.703931, accumulated_submission_time=215141.627498, global_step=483552, preemption_count=0, score=215141.627498, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=233976.224034, train/accuracy=0.886934, train/loss=0.423894, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:33:40.296899 140380326586112 logging_writer.py:48] [483600] global_step=483600, grad_norm=7.229862689971924, loss=3.2508230209350586
I0305 04:34:23.005907 140380334978816 logging_writer.py:48] [483700] global_step=483700, grad_norm=3.7380974292755127, loss=1.86443030834198
I0305 04:35:08.242435 140380326586112 logging_writer.py:48] [483800] global_step=483800, grad_norm=4.350558280944824, loss=3.211275577545166
I0305 04:35:53.542201 140380334978816 logging_writer.py:48] [483900] global_step=483900, grad_norm=3.0716280937194824, loss=1.1366143226623535
I0305 04:36:38.419031 140380326586112 logging_writer.py:48] [484000] global_step=484000, grad_norm=3.011056423187256, loss=1.8976645469665527
I0305 04:37:23.238252 140380334978816 logging_writer.py:48] [484100] global_step=484100, grad_norm=3.20042085647583, loss=1.633023738861084
I0305 04:38:08.581659 140380326586112 logging_writer.py:48] [484200] global_step=484200, grad_norm=3.4165425300598145, loss=1.2074997425079346
I0305 04:38:53.285844 140380334978816 logging_writer.py:48] [484300] global_step=484300, grad_norm=3.4070422649383545, loss=2.871988534927368
I0305 04:39:38.152137 140380326586112 logging_writer.py:48] [484400] global_step=484400, grad_norm=4.174971103668213, loss=3.3070931434631348
I0305 04:40:20.626611 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:40:30.259376 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:40:56.335618 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:40:57.996713 140575196817216 submission_runner.py:411] Time since start: 234433.60s, 	Step: 484496, 	{'train/accuracy': 0.8873242139816284, 'train/loss': 0.42273107171058655, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 215561.4826450348, 'total_duration': 234433.59770202637, 'accumulated_submission_time': 215561.4826450348, 'accumulated_eval_time': 18810.65714740753, 'accumulated_logging_time': 35.80401062965393}
I0305 04:40:58.088590 140380334978816 logging_writer.py:48] [484496] accumulated_eval_time=18810.657147, accumulated_logging_time=35.804011, accumulated_submission_time=215561.482645, global_step=484496, preemption_count=0, score=215561.482645, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=234433.597702, train/accuracy=0.887324, train/loss=0.422731, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:41:00.082124 140380326586112 logging_writer.py:48] [484500] global_step=484500, grad_norm=2.8831863403320312, loss=1.1886622905731201
I0305 04:41:40.658016 140380334978816 logging_writer.py:48] [484600] global_step=484600, grad_norm=2.9477362632751465, loss=1.3382022380828857
I0305 04:42:25.865183 140380326586112 logging_writer.py:48] [484700] global_step=484700, grad_norm=3.3866517543792725, loss=1.184160590171814
I0305 04:43:11.114908 140380334978816 logging_writer.py:48] [484800] global_step=484800, grad_norm=3.1166422367095947, loss=1.3068755865097046
I0305 04:43:56.012108 140380326586112 logging_writer.py:48] [484900] global_step=484900, grad_norm=3.0428402423858643, loss=1.5131391286849976
I0305 04:44:41.025473 140380334978816 logging_writer.py:48] [485000] global_step=485000, grad_norm=3.3929338455200195, loss=2.7591896057128906
I0305 04:45:26.032242 140380326586112 logging_writer.py:48] [485100] global_step=485100, grad_norm=3.1200037002563477, loss=1.84483003616333
I0305 04:46:11.143681 140380334978816 logging_writer.py:48] [485200] global_step=485200, grad_norm=3.2911548614501953, loss=1.3473339080810547
I0305 04:46:56.327173 140380326586112 logging_writer.py:48] [485300] global_step=485300, grad_norm=2.953575611114502, loss=1.0461996793746948
I0305 04:47:41.255811 140380334978816 logging_writer.py:48] [485400] global_step=485400, grad_norm=3.177476167678833, loss=1.0293834209442139
I0305 04:47:58.219802 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:48:08.094272 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:48:35.194081 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:48:36.859956 140575196817216 submission_runner.py:411] Time since start: 234892.46s, 	Step: 485439, 	{'train/accuracy': 0.8871093392372131, 'train/loss': 0.421786367893219, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 215981.5551698208, 'total_duration': 234892.46096420288, 'accumulated_submission_time': 215981.5551698208, 'accumulated_eval_time': 18849.296257972717, 'accumulated_logging_time': 35.90559959411621}
I0305 04:48:36.960185 140380326586112 logging_writer.py:48] [485439] accumulated_eval_time=18849.296258, accumulated_logging_time=35.905600, accumulated_submission_time=215981.555170, global_step=485439, preemption_count=0, score=215981.555170, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=234892.460964, train/accuracy=0.887109, train/loss=0.421786, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:49:01.670285 140380334978816 logging_writer.py:48] [485500] global_step=485500, grad_norm=2.922887086868286, loss=1.0779736042022705
I0305 04:49:45.235376 140380326586112 logging_writer.py:48] [485600] global_step=485600, grad_norm=3.3262736797332764, loss=1.6438921689987183
I0305 04:50:30.459822 140380334978816 logging_writer.py:48] [485700] global_step=485700, grad_norm=3.6168289184570312, loss=3.0547289848327637
I0305 04:51:15.648932 140380326586112 logging_writer.py:48] [485800] global_step=485800, grad_norm=3.3205223083496094, loss=1.5581326484680176
I0305 04:52:00.549963 140380334978816 logging_writer.py:48] [485900] global_step=485900, grad_norm=3.222095012664795, loss=1.1842843294143677
I0305 04:52:45.469489 140380326586112 logging_writer.py:48] [486000] global_step=486000, grad_norm=3.2728302478790283, loss=1.057934284210205
I0305 04:53:30.595191 140380334978816 logging_writer.py:48] [486100] global_step=486100, grad_norm=3.319427967071533, loss=1.697310447692871
I0305 04:54:15.398376 140380326586112 logging_writer.py:48] [486200] global_step=486200, grad_norm=4.002261638641357, loss=1.170325517654419
I0305 04:55:00.317619 140380334978816 logging_writer.py:48] [486300] global_step=486300, grad_norm=3.5028250217437744, loss=1.1844786405563354
I0305 04:55:36.952797 140575196817216 spec.py:321] Evaluating on the training split.
I0305 04:55:46.726439 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 04:56:19.186040 140575196817216 spec.py:349] Evaluating on the test split.
I0305 04:56:20.842579 140575196817216 submission_runner.py:411] Time since start: 235356.44s, 	Step: 486383, 	{'train/accuracy': 0.8866601586341858, 'train/loss': 0.4253378212451935, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 216401.4870171547, 'total_duration': 235356.44365882874, 'accumulated_submission_time': 216401.4870171547, 'accumulated_eval_time': 18893.185064554214, 'accumulated_logging_time': 36.01810646057129}
I0305 04:56:20.932155 140380326586112 logging_writer.py:48] [486383] accumulated_eval_time=18893.185065, accumulated_logging_time=36.018106, accumulated_submission_time=216401.487017, global_step=486383, preemption_count=0, score=216401.487017, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=235356.443659, train/accuracy=0.886660, train/loss=0.425338, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 04:56:28.079052 140380334978816 logging_writer.py:48] [486400] global_step=486400, grad_norm=3.5141117572784424, loss=3.1329662799835205
I0305 04:57:09.401236 140380326586112 logging_writer.py:48] [486500] global_step=486500, grad_norm=3.287386894226074, loss=1.0538452863693237
I0305 04:57:54.474324 140380334978816 logging_writer.py:48] [486600] global_step=486600, grad_norm=3.4717695713043213, loss=2.830094337463379
I0305 04:58:39.820427 140380326586112 logging_writer.py:48] [486700] global_step=486700, grad_norm=3.001143217086792, loss=1.093643307685852
I0305 04:59:24.778093 140380334978816 logging_writer.py:48] [486800] global_step=486800, grad_norm=3.3625190258026123, loss=2.3827123641967773
I0305 05:00:09.466620 140380326586112 logging_writer.py:48] [486900] global_step=486900, grad_norm=3.1409904956817627, loss=1.3264514207839966
I0305 05:00:54.414800 140380334978816 logging_writer.py:48] [487000] global_step=487000, grad_norm=3.527491569519043, loss=2.3199362754821777
I0305 05:01:39.367615 140380326586112 logging_writer.py:48] [487100] global_step=487100, grad_norm=3.8876640796661377, loss=3.3137097358703613
I0305 05:02:24.264595 140380334978816 logging_writer.py:48] [487200] global_step=487200, grad_norm=3.11370587348938, loss=1.7607927322387695
I0305 05:03:09.164305 140380326586112 logging_writer.py:48] [487300] global_step=487300, grad_norm=3.1140291690826416, loss=1.3426990509033203
I0305 05:03:20.849456 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:03:30.389822 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:03:58.581391 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:04:00.240607 140575196817216 submission_runner.py:411] Time since start: 235815.84s, 	Step: 487328, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.4159683585166931, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 216821.3461008072, 'total_duration': 235815.8413271904, 'accumulated_submission_time': 216821.3461008072, 'accumulated_eval_time': 18932.574870586395, 'accumulated_logging_time': 36.117307901382446}
I0305 05:04:00.332648 140380334978816 logging_writer.py:48] [487328] accumulated_eval_time=18932.574871, accumulated_logging_time=36.117308, accumulated_submission_time=216821.346101, global_step=487328, preemption_count=0, score=216821.346101, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=235815.841327, train/accuracy=0.887227, train/loss=0.415968, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:04:29.331254 140380326586112 logging_writer.py:48] [487400] global_step=487400, grad_norm=3.1283860206604004, loss=1.1113977432250977
I0305 05:05:13.025246 140380334978816 logging_writer.py:48] [487500] global_step=487500, grad_norm=3.1720383167266846, loss=1.0989857912063599
I0305 05:05:57.974546 140380326586112 logging_writer.py:48] [487600] global_step=487600, grad_norm=3.153212547302246, loss=1.1805869340896606
I0305 05:06:43.292084 140380334978816 logging_writer.py:48] [487700] global_step=487700, grad_norm=3.1707918643951416, loss=1.1827982664108276
I0305 05:07:28.300636 140380326586112 logging_writer.py:48] [487800] global_step=487800, grad_norm=3.016125202178955, loss=2.303618907928467
I0305 05:08:13.135467 140380334978816 logging_writer.py:48] [487900] global_step=487900, grad_norm=4.053055763244629, loss=1.1832160949707031
I0305 05:08:58.254031 140380326586112 logging_writer.py:48] [488000] global_step=488000, grad_norm=2.9923086166381836, loss=2.1580653190612793
I0305 05:09:43.086708 140380334978816 logging_writer.py:48] [488100] global_step=488100, grad_norm=3.4686319828033447, loss=1.8214046955108643
I0305 05:10:28.293079 140380326586112 logging_writer.py:48] [488200] global_step=488200, grad_norm=3.792469024658203, loss=3.2549617290496826
I0305 05:11:00.325651 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:11:10.227385 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:11:36.727013 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:11:38.377674 140575196817216 submission_runner.py:411] Time since start: 236273.98s, 	Step: 488273, 	{'train/accuracy': 0.8877733945846558, 'train/loss': 0.4194367825984955, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 217241.2795341015, 'total_duration': 236273.97872304916, 'accumulated_submission_time': 217241.2795341015, 'accumulated_eval_time': 18970.62588739395, 'accumulated_logging_time': 36.220603704452515}
I0305 05:11:38.470708 140380334978816 logging_writer.py:48] [488273] accumulated_eval_time=18970.625887, accumulated_logging_time=36.220604, accumulated_submission_time=217241.279534, global_step=488273, preemption_count=0, score=217241.279534, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=236273.978723, train/accuracy=0.887773, train/loss=0.419437, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:11:49.609948 140380326586112 logging_writer.py:48] [488300] global_step=488300, grad_norm=3.2071990966796875, loss=2.14160418510437
I0305 05:12:31.024707 140380334978816 logging_writer.py:48] [488400] global_step=488400, grad_norm=3.0038797855377197, loss=2.3301544189453125
I0305 05:13:16.043371 140380326586112 logging_writer.py:48] [488500] global_step=488500, grad_norm=3.0061328411102295, loss=2.577897787094116
I0305 05:14:01.457396 140380334978816 logging_writer.py:48] [488600] global_step=488600, grad_norm=3.226567506790161, loss=1.1302762031555176
I0305 05:14:46.288035 140380326586112 logging_writer.py:48] [488700] global_step=488700, grad_norm=3.540370225906372, loss=2.637993335723877
I0305 05:15:31.335734 140380334978816 logging_writer.py:48] [488800] global_step=488800, grad_norm=2.925032377243042, loss=1.1292750835418701
I0305 05:16:16.594132 140380326586112 logging_writer.py:48] [488900] global_step=488900, grad_norm=3.325925350189209, loss=2.8880016803741455
I0305 05:17:01.513088 140380334978816 logging_writer.py:48] [489000] global_step=489000, grad_norm=3.0826706886291504, loss=1.136545181274414
I0305 05:17:46.634312 140380326586112 logging_writer.py:48] [489100] global_step=489100, grad_norm=3.0490479469299316, loss=1.6429306268692017
I0305 05:18:31.796549 140380334978816 logging_writer.py:48] [489200] global_step=489200, grad_norm=3.8458664417266846, loss=2.991489887237549
I0305 05:18:38.563141 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:18:48.550379 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:19:16.358153 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:19:18.009101 140575196817216 submission_runner.py:411] Time since start: 236733.61s, 	Step: 489217, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.41677945852279663, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 217661.31353735924, 'total_duration': 236733.6101295948, 'accumulated_submission_time': 217661.31353735924, 'accumulated_eval_time': 19010.070831775665, 'accumulated_logging_time': 36.32327747344971}
I0305 05:19:18.110033 140380326586112 logging_writer.py:48] [489217] accumulated_eval_time=19010.070832, accumulated_logging_time=36.323277, accumulated_submission_time=217661.313537, global_step=489217, preemption_count=0, score=217661.313537, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=236733.610130, train/accuracy=0.888105, train/loss=0.416779, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:19:51.483908 140380334978816 logging_writer.py:48] [489300] global_step=489300, grad_norm=3.3250579833984375, loss=2.652862310409546
I0305 05:20:36.031820 140380326586112 logging_writer.py:48] [489400] global_step=489400, grad_norm=3.5620691776275635, loss=2.930804967880249
I0305 05:21:21.268223 140380334978816 logging_writer.py:48] [489500] global_step=489500, grad_norm=3.21086049079895, loss=2.3756072521209717
I0305 05:22:06.134930 140380326586112 logging_writer.py:48] [489600] global_step=489600, grad_norm=3.8075318336486816, loss=3.1739401817321777
I0305 05:22:50.860721 140380334978816 logging_writer.py:48] [489700] global_step=489700, grad_norm=3.3673856258392334, loss=1.2217817306518555
I0305 05:23:36.161205 140380326586112 logging_writer.py:48] [489800] global_step=489800, grad_norm=3.408632278442383, loss=1.1067829132080078
I0305 05:24:21.065896 140380334978816 logging_writer.py:48] [489900] global_step=489900, grad_norm=4.180861473083496, loss=3.199507713317871
I0305 05:25:05.974742 140380326586112 logging_writer.py:48] [490000] global_step=490000, grad_norm=2.965121030807495, loss=1.5844014883041382
I0305 05:25:50.693358 140380334978816 logging_writer.py:48] [490100] global_step=490100, grad_norm=4.189995765686035, loss=1.2040809392929077
I0305 05:26:18.236663 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:26:27.950534 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:26:59.034144 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:27:00.692729 140575196817216 submission_runner.py:411] Time since start: 237196.29s, 	Step: 490163, 	{'train/accuracy': 0.8856640458106995, 'train/loss': 0.4225226938724518, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 218081.38036298752, 'total_duration': 237196.29369330406, 'accumulated_submission_time': 218081.38036298752, 'accumulated_eval_time': 19052.525792360306, 'accumulated_logging_time': 36.43513631820679}
I0305 05:27:00.793012 140380326586112 logging_writer.py:48] [490163] accumulated_eval_time=19052.525792, accumulated_logging_time=36.435136, accumulated_submission_time=218081.380363, global_step=490163, preemption_count=0, score=218081.380363, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=237196.293693, train/accuracy=0.885664, train/loss=0.422523, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:27:15.895823 140380334978816 logging_writer.py:48] [490200] global_step=490200, grad_norm=3.323004961013794, loss=1.0675978660583496
I0305 05:27:58.328522 140380326586112 logging_writer.py:48] [490300] global_step=490300, grad_norm=3.1167330741882324, loss=1.8139773607254028
I0305 05:28:43.236591 140380334978816 logging_writer.py:48] [490400] global_step=490400, grad_norm=3.2567431926727295, loss=1.445920467376709
I0305 05:29:28.564290 140380326586112 logging_writer.py:48] [490500] global_step=490500, grad_norm=3.205192804336548, loss=1.135626196861267
I0305 05:30:13.737594 140380334978816 logging_writer.py:48] [490600] global_step=490600, grad_norm=3.7193892002105713, loss=2.301351547241211
I0305 05:30:58.626100 140380326586112 logging_writer.py:48] [490700] global_step=490700, grad_norm=3.450564384460449, loss=2.6552772521972656
I0305 05:31:43.652888 140380334978816 logging_writer.py:48] [490800] global_step=490800, grad_norm=2.9870498180389404, loss=1.1171140670776367
I0305 05:32:28.521846 140380326586112 logging_writer.py:48] [490900] global_step=490900, grad_norm=3.265071153640747, loss=1.6152756214141846
I0305 05:33:13.459583 140380334978816 logging_writer.py:48] [491000] global_step=491000, grad_norm=3.0238399505615234, loss=1.071243405342102
I0305 05:33:58.424036 140380326586112 logging_writer.py:48] [491100] global_step=491100, grad_norm=3.0029990673065186, loss=1.5939706563949585
I0305 05:34:00.702617 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:34:10.449396 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:34:39.842175 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:34:41.498028 140575196817216 submission_runner.py:411] Time since start: 237657.10s, 	Step: 491107, 	{'train/accuracy': 0.8886913657188416, 'train/loss': 0.413221538066864, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 218501.23131036758, 'total_duration': 237657.09906315804, 'accumulated_submission_time': 218501.23131036758, 'accumulated_eval_time': 19093.32017993927, 'accumulated_logging_time': 36.5455060005188}
I0305 05:34:41.609341 140380334978816 logging_writer.py:48] [491107] accumulated_eval_time=19093.320180, accumulated_logging_time=36.545506, accumulated_submission_time=218501.231310, global_step=491107, preemption_count=0, score=218501.231310, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=237657.099063, train/accuracy=0.888691, train/loss=0.413222, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:35:19.261885 140380326586112 logging_writer.py:48] [491200] global_step=491200, grad_norm=3.848647117614746, loss=2.9502148628234863
I0305 05:36:03.844379 140380334978816 logging_writer.py:48] [491300] global_step=491300, grad_norm=3.293745994567871, loss=1.1316899061203003
I0305 05:36:49.067445 140380326586112 logging_writer.py:48] [491400] global_step=491400, grad_norm=3.000694513320923, loss=2.1278765201568604
I0305 05:37:34.240563 140380334978816 logging_writer.py:48] [491500] global_step=491500, grad_norm=3.2131197452545166, loss=1.5599693059921265
I0305 05:38:19.130645 140380326586112 logging_writer.py:48] [491600] global_step=491600, grad_norm=3.0052268505096436, loss=1.0812275409698486
I0305 05:39:04.635968 140380334978816 logging_writer.py:48] [491700] global_step=491700, grad_norm=3.1429433822631836, loss=1.0859880447387695
I0305 05:39:49.975743 140380326586112 logging_writer.py:48] [491800] global_step=491800, grad_norm=3.876443862915039, loss=3.1491432189941406
I0305 05:40:34.802946 140380334978816 logging_writer.py:48] [491900] global_step=491900, grad_norm=3.094928503036499, loss=2.3856310844421387
I0305 05:41:19.928344 140380326586112 logging_writer.py:48] [492000] global_step=492000, grad_norm=2.799851655960083, loss=1.0924698114395142
I0305 05:41:41.624819 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:41:51.369317 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:42:15.861935 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:42:17.503085 140575196817216 submission_runner.py:411] Time since start: 238113.10s, 	Step: 492050, 	{'train/accuracy': 0.8886913657188416, 'train/loss': 0.41571182012557983, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 218921.18591451645, 'total_duration': 238113.10402703285, 'accumulated_submission_time': 218921.18591451645, 'accumulated_eval_time': 19129.19732618332, 'accumulated_logging_time': 36.66964244842529}
I0305 05:42:17.593683 140380334978816 logging_writer.py:48] [492050] accumulated_eval_time=19129.197326, accumulated_logging_time=36.669642, accumulated_submission_time=218921.185915, global_step=492050, preemption_count=0, score=218921.185915, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=238113.104027, train/accuracy=0.888691, train/loss=0.415712, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:42:37.870907 140380326586112 logging_writer.py:48] [492100] global_step=492100, grad_norm=3.1086983680725098, loss=1.2436168193817139
I0305 05:43:20.494835 140380334978816 logging_writer.py:48] [492200] global_step=492200, grad_norm=3.193413019180298, loss=2.681845188140869
I0305 05:44:05.873951 140380326586112 logging_writer.py:48] [492300] global_step=492300, grad_norm=2.930201530456543, loss=1.1456022262573242
I0305 05:44:51.213861 140380334978816 logging_writer.py:48] [492400] global_step=492400, grad_norm=3.414604425430298, loss=2.7669479846954346
I0305 05:45:36.175485 140380326586112 logging_writer.py:48] [492500] global_step=492500, grad_norm=2.9674296379089355, loss=1.6000813245773315
I0305 05:46:21.414812 140380334978816 logging_writer.py:48] [492600] global_step=492600, grad_norm=3.2452545166015625, loss=1.1006262302398682
I0305 05:47:06.465909 140380326586112 logging_writer.py:48] [492700] global_step=492700, grad_norm=3.500356674194336, loss=2.9952242374420166
I0305 05:47:51.687439 140380334978816 logging_writer.py:48] [492800] global_step=492800, grad_norm=3.80605411529541, loss=1.1233748197555542
I0305 05:48:36.556879 140380326586112 logging_writer.py:48] [492900] global_step=492900, grad_norm=3.3528525829315186, loss=2.5078155994415283
I0305 05:49:17.593069 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:49:27.303100 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:49:53.706129 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:49:55.361316 140575196817216 submission_runner.py:411] Time since start: 238570.96s, 	Step: 492992, 	{'train/accuracy': 0.8909375071525574, 'train/loss': 0.40796688199043274, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 219341.1279244423, 'total_duration': 238570.96243953705, 'accumulated_submission_time': 219341.1279244423, 'accumulated_eval_time': 19166.96463418007, 'accumulated_logging_time': 36.76979398727417}
I0305 05:49:55.453799 140380334978816 logging_writer.py:48] [492992] accumulated_eval_time=19166.964634, accumulated_logging_time=36.769794, accumulated_submission_time=219341.127924, global_step=492992, preemption_count=0, score=219341.127924, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=238570.962440, train/accuracy=0.890938, train/loss=0.407967, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:49:59.039325 140380326586112 logging_writer.py:48] [493000] global_step=493000, grad_norm=3.744304656982422, loss=1.1279895305633545
I0305 05:50:39.957959 140380334978816 logging_writer.py:48] [493100] global_step=493100, grad_norm=3.1641433238983154, loss=1.2197660207748413
I0305 05:51:24.981022 140380326586112 logging_writer.py:48] [493200] global_step=493200, grad_norm=3.1424129009246826, loss=1.6868680715560913
I0305 05:52:10.631994 140380334978816 logging_writer.py:48] [493300] global_step=493300, grad_norm=3.3016886711120605, loss=1.0942871570587158
I0305 05:52:55.642910 140380326586112 logging_writer.py:48] [493400] global_step=493400, grad_norm=3.657571315765381, loss=2.196758985519409
I0305 05:53:40.648003 140380334978816 logging_writer.py:48] [493500] global_step=493500, grad_norm=3.1351544857025146, loss=1.5033892393112183
I0305 05:54:25.844591 140380326586112 logging_writer.py:48] [493600] global_step=493600, grad_norm=3.079514503479004, loss=1.1650705337524414
I0305 05:55:10.836179 140380334978816 logging_writer.py:48] [493700] global_step=493700, grad_norm=3.1205577850341797, loss=1.1630061864852905
I0305 05:55:55.843499 140380326586112 logging_writer.py:48] [493800] global_step=493800, grad_norm=3.160325050354004, loss=1.147570252418518
I0305 05:56:41.085137 140380334978816 logging_writer.py:48] [493900] global_step=493900, grad_norm=3.287135362625122, loss=2.4863433837890625
I0305 05:56:55.600199 140575196817216 spec.py:321] Evaluating on the training split.
I0305 05:57:05.558169 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 05:57:34.330421 140575196817216 spec.py:349] Evaluating on the test split.
I0305 05:57:35.977315 140575196817216 submission_runner.py:411] Time since start: 239031.58s, 	Step: 493934, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.4184964597225189, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 219761.21338629723, 'total_duration': 239031.57810759544, 'accumulated_submission_time': 219761.21338629723, 'accumulated_eval_time': 19207.34047794342, 'accumulated_logging_time': 36.873939514160156}
I0305 05:57:36.067049 140380326586112 logging_writer.py:48] [493934] accumulated_eval_time=19207.340478, accumulated_logging_time=36.873940, accumulated_submission_time=219761.213386, global_step=493934, preemption_count=0, score=219761.213386, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=239031.578108, train/accuracy=0.888105, train/loss=0.418496, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 05:58:02.682565 140380334978816 logging_writer.py:48] [494000] global_step=494000, grad_norm=3.4035346508026123, loss=1.1427124738693237
I0305 05:58:46.529432 140380326586112 logging_writer.py:48] [494100] global_step=494100, grad_norm=3.1235601902008057, loss=1.5003302097320557
I0305 05:59:31.797621 140380334978816 logging_writer.py:48] [494200] global_step=494200, grad_norm=3.5865068435668945, loss=2.105729103088379
I0305 06:00:16.971213 140380326586112 logging_writer.py:48] [494300] global_step=494300, grad_norm=4.015605449676514, loss=3.316514492034912
I0305 06:01:01.958336 140380334978816 logging_writer.py:48] [494400] global_step=494400, grad_norm=3.2766404151916504, loss=1.2179181575775146
I0305 06:01:46.980100 140380326586112 logging_writer.py:48] [494500] global_step=494500, grad_norm=3.834611415863037, loss=1.1294245719909668
I0305 06:02:31.980776 140380334978816 logging_writer.py:48] [494600] global_step=494600, grad_norm=3.255894422531128, loss=2.7046315670013428
I0305 06:03:16.970003 140380326586112 logging_writer.py:48] [494700] global_step=494700, grad_norm=4.955198287963867, loss=2.920182228088379
I0305 06:04:01.854318 140380334978816 logging_writer.py:48] [494800] global_step=494800, grad_norm=3.7569308280944824, loss=3.1365914344787598
I0305 06:04:36.268259 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:04:46.829905 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:05:12.834426 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:05:14.482859 140575196817216 submission_runner.py:411] Time since start: 239490.08s, 	Step: 494878, 	{'train/accuracy': 0.8865429759025574, 'train/loss': 0.41928160190582275, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 220181.35465550423, 'total_duration': 239490.08392715454, 'accumulated_submission_time': 220181.35465550423, 'accumulated_eval_time': 19245.554092168808, 'accumulated_logging_time': 36.974446535110474}
I0305 06:05:14.574365 140380326586112 logging_writer.py:48] [494878] accumulated_eval_time=19245.554092, accumulated_logging_time=36.974447, accumulated_submission_time=220181.354656, global_step=494878, preemption_count=0, score=220181.354656, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=239490.083927, train/accuracy=0.886543, train/loss=0.419282, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:05:23.739673 140380334978816 logging_writer.py:48] [494900] global_step=494900, grad_norm=3.52632999420166, loss=2.9793858528137207
I0305 06:06:04.869844 140380326586112 logging_writer.py:48] [495000] global_step=495000, grad_norm=3.5964250564575195, loss=2.8695929050445557
I0305 06:06:50.122897 140380334978816 logging_writer.py:48] [495100] global_step=495100, grad_norm=6.608510971069336, loss=3.26609206199646
I0305 06:07:35.723047 140380326586112 logging_writer.py:48] [495200] global_step=495200, grad_norm=3.1032986640930176, loss=1.223373293876648
I0305 06:08:20.963083 140380334978816 logging_writer.py:48] [495300] global_step=495300, grad_norm=2.971079111099243, loss=1.8987624645233154
I0305 06:09:05.913902 140380326586112 logging_writer.py:48] [495400] global_step=495400, grad_norm=3.5819790363311768, loss=1.1066174507141113
I0305 06:09:51.391569 140380334978816 logging_writer.py:48] [495500] global_step=495500, grad_norm=3.058823347091675, loss=1.1858081817626953
I0305 06:10:36.581457 140380326586112 logging_writer.py:48] [495600] global_step=495600, grad_norm=3.158071517944336, loss=1.597035527229309
I0305 06:11:21.699661 140380334978816 logging_writer.py:48] [495700] global_step=495700, grad_norm=4.337162971496582, loss=3.107429265975952
I0305 06:12:06.838358 140380326586112 logging_writer.py:48] [495800] global_step=495800, grad_norm=3.674410343170166, loss=2.0211076736450195
I0305 06:12:14.597681 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:12:24.105155 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:12:54.178660 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:12:55.837368 140575196817216 submission_runner.py:411] Time since start: 239951.44s, 	Step: 495819, 	{'train/accuracy': 0.8858202695846558, 'train/loss': 0.4241812825202942, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 220601.31715726852, 'total_duration': 239951.43818807602, 'accumulated_submission_time': 220601.31715726852, 'accumulated_eval_time': 19286.792535066605, 'accumulated_logging_time': 37.07832598686218}
I0305 06:12:55.944861 140380334978816 logging_writer.py:48] [495819] accumulated_eval_time=19286.792535, accumulated_logging_time=37.078326, accumulated_submission_time=220601.317157, global_step=495819, preemption_count=0, score=220601.317157, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=239951.438188, train/accuracy=0.885820, train/loss=0.424181, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:13:28.590917 140380326586112 logging_writer.py:48] [495900] global_step=495900, grad_norm=3.325765609741211, loss=1.3503429889678955
I0305 06:14:13.001269 140380334978816 logging_writer.py:48] [496000] global_step=496000, grad_norm=3.4038584232330322, loss=2.8255372047424316
I0305 06:14:58.294401 140380326586112 logging_writer.py:48] [496100] global_step=496100, grad_norm=2.991264820098877, loss=1.1845178604125977
I0305 06:15:43.269303 140380334978816 logging_writer.py:48] [496200] global_step=496200, grad_norm=4.030908107757568, loss=2.752591848373413
I0305 06:16:28.463550 140380326586112 logging_writer.py:48] [496300] global_step=496300, grad_norm=3.497468948364258, loss=1.895021915435791
I0305 06:17:13.488097 140380334978816 logging_writer.py:48] [496400] global_step=496400, grad_norm=2.973830461502075, loss=1.0998049974441528
I0305 06:17:58.502972 140380326586112 logging_writer.py:48] [496500] global_step=496500, grad_norm=3.1157007217407227, loss=1.3507826328277588
I0305 06:18:43.501297 140380334978816 logging_writer.py:48] [496600] global_step=496600, grad_norm=3.220919609069824, loss=1.875433325767517
I0305 06:19:28.840702 140380326586112 logging_writer.py:48] [496700] global_step=496700, grad_norm=3.5716285705566406, loss=3.04377818107605
I0305 06:19:55.995341 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:20:05.724622 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:20:32.437180 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:20:34.083226 140575196817216 submission_runner.py:411] Time since start: 240409.68s, 	Step: 496762, 	{'train/accuracy': 0.8873046636581421, 'train/loss': 0.4224570691585541, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 221021.30834054947, 'total_duration': 240409.68428444862, 'accumulated_submission_time': 221021.30834054947, 'accumulated_eval_time': 19324.879416704178, 'accumulated_logging_time': 37.19647455215454}
I0305 06:20:34.172698 140380334978816 logging_writer.py:48] [496762] accumulated_eval_time=19324.879417, accumulated_logging_time=37.196475, accumulated_submission_time=221021.308341, global_step=496762, preemption_count=0, score=221021.308341, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=240409.684284, train/accuracy=0.887305, train/loss=0.422457, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:20:49.684681 140380326586112 logging_writer.py:48] [496800] global_step=496800, grad_norm=3.2571206092834473, loss=1.0386066436767578
I0305 06:21:31.830699 140380334978816 logging_writer.py:48] [496900] global_step=496900, grad_norm=2.9621450901031494, loss=1.2748692035675049
I0305 06:22:16.854243 140380326586112 logging_writer.py:48] [497000] global_step=497000, grad_norm=6.352118968963623, loss=1.1394062042236328
I0305 06:23:02.032077 140380334978816 logging_writer.py:48] [497100] global_step=497100, grad_norm=2.8718202114105225, loss=1.1768802404403687
I0305 06:23:46.966404 140380326586112 logging_writer.py:48] [497200] global_step=497200, grad_norm=2.958800792694092, loss=1.2854572534561157
I0305 06:24:32.101406 140380334978816 logging_writer.py:48] [497300] global_step=497300, grad_norm=3.082087755203247, loss=1.5193802118301392
I0305 06:25:17.109960 140380326586112 logging_writer.py:48] [497400] global_step=497400, grad_norm=3.6554296016693115, loss=3.055232286453247
I0305 06:26:02.190836 140380334978816 logging_writer.py:48] [497500] global_step=497500, grad_norm=3.260833740234375, loss=1.183972954750061
I0305 06:26:47.412653 140380326586112 logging_writer.py:48] [497600] global_step=497600, grad_norm=3.059108257293701, loss=1.049198031425476
I0305 06:27:32.422522 140380334978816 logging_writer.py:48] [497700] global_step=497700, grad_norm=3.0127453804016113, loss=1.1169191598892212
I0305 06:27:34.376829 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:27:44.186996 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:28:07.850576 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:28:09.507289 140575196817216 submission_runner.py:411] Time since start: 240865.11s, 	Step: 497706, 	{'train/accuracy': 0.8900781273841858, 'train/loss': 0.41129228472709656, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 221441.4514014721, 'total_duration': 240865.10836744308, 'accumulated_submission_time': 221441.4514014721, 'accumulated_eval_time': 19360.008903741837, 'accumulated_logging_time': 37.29786229133606}
I0305 06:28:09.615516 140380326586112 logging_writer.py:48] [497706] accumulated_eval_time=19360.008904, accumulated_logging_time=37.297862, accumulated_submission_time=221441.451401, global_step=497706, preemption_count=0, score=221441.451401, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=240865.108367, train/accuracy=0.890078, train/loss=0.411292, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:28:47.534940 140380334978816 logging_writer.py:48] [497800] global_step=497800, grad_norm=3.0492045879364014, loss=2.0839405059814453
I0305 06:29:32.116972 140380326586112 logging_writer.py:48] [497900] global_step=497900, grad_norm=3.3522789478302, loss=2.6932380199432373
I0305 06:30:17.369328 140380334978816 logging_writer.py:48] [498000] global_step=498000, grad_norm=3.3014369010925293, loss=2.4324750900268555
I0305 06:31:02.719703 140380326586112 logging_writer.py:48] [498100] global_step=498100, grad_norm=3.013857364654541, loss=1.153008222579956
I0305 06:31:47.790649 140380334978816 logging_writer.py:48] [498200] global_step=498200, grad_norm=3.131603717803955, loss=1.0766521692276
I0305 06:32:32.946019 140380326586112 logging_writer.py:48] [498300] global_step=498300, grad_norm=2.8669936656951904, loss=1.0868291854858398
I0305 06:33:18.104738 140380334978816 logging_writer.py:48] [498400] global_step=498400, grad_norm=3.2466611862182617, loss=1.3640378713607788
I0305 06:34:03.213188 140380326586112 logging_writer.py:48] [498500] global_step=498500, grad_norm=3.6632161140441895, loss=1.1785285472869873
I0305 06:34:48.394820 140380334978816 logging_writer.py:48] [498600] global_step=498600, grad_norm=2.919808864593506, loss=1.3266913890838623
I0305 06:35:09.888458 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:35:19.838567 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:35:50.404436 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:35:52.051793 140575196817216 submission_runner.py:411] Time since start: 241327.65s, 	Step: 498649, 	{'train/accuracy': 0.8861132860183716, 'train/loss': 0.42476779222488403, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 221861.6649634838, 'total_duration': 241327.65272402763, 'accumulated_submission_time': 221861.6649634838, 'accumulated_eval_time': 19402.171106815338, 'accumulated_logging_time': 37.41727375984192}
I0305 06:35:52.142771 140380326586112 logging_writer.py:48] [498649] accumulated_eval_time=19402.171107, accumulated_logging_time=37.417274, accumulated_submission_time=221861.664963, global_step=498649, preemption_count=0, score=221861.664963, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=241327.652724, train/accuracy=0.886113, train/loss=0.424768, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:36:12.809936 140380334978816 logging_writer.py:48] [498700] global_step=498700, grad_norm=4.079738616943359, loss=1.9717577695846558
I0305 06:36:56.032638 140380326586112 logging_writer.py:48] [498800] global_step=498800, grad_norm=3.0329251289367676, loss=1.1811363697052002
I0305 06:37:41.201015 140380334978816 logging_writer.py:48] [498900] global_step=498900, grad_norm=3.5989127159118652, loss=1.5016968250274658
I0305 06:38:26.583045 140380326586112 logging_writer.py:48] [499000] global_step=499000, grad_norm=3.2991600036621094, loss=1.1959971189498901
I0305 06:39:11.492436 140380334978816 logging_writer.py:48] [499100] global_step=499100, grad_norm=2.989854574203491, loss=2.310579776763916
I0305 06:39:56.861626 140380326586112 logging_writer.py:48] [499200] global_step=499200, grad_norm=3.3720953464508057, loss=1.44701087474823
I0305 06:40:42.004591 140380334978816 logging_writer.py:48] [499300] global_step=499300, grad_norm=3.043788194656372, loss=1.12532377243042
I0305 06:41:27.126461 140380326586112 logging_writer.py:48] [499400] global_step=499400, grad_norm=3.309244394302368, loss=1.146268367767334
I0305 06:42:12.397616 140380334978816 logging_writer.py:48] [499500] global_step=499500, grad_norm=3.673941135406494, loss=1.13821542263031
I0305 06:42:52.119601 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:43:01.998288 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:43:33.772637 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:43:35.426100 140575196817216 submission_runner.py:411] Time since start: 241791.03s, 	Step: 499590, 	{'train/accuracy': 0.888476550579071, 'train/loss': 0.4164844751358032, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 222281.58246564865, 'total_duration': 241791.02713799477, 'accumulated_submission_time': 222281.58246564865, 'accumulated_eval_time': 19445.476580142975, 'accumulated_logging_time': 37.518784284591675}
I0305 06:43:35.536219 140380326586112 logging_writer.py:48] [499590] accumulated_eval_time=19445.476580, accumulated_logging_time=37.518784, accumulated_submission_time=222281.582466, global_step=499590, preemption_count=0, score=222281.582466, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=241791.027138, train/accuracy=0.888477, train/loss=0.416484, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:43:39.914823 140380334978816 logging_writer.py:48] [499600] global_step=499600, grad_norm=3.1961848735809326, loss=1.1290217638015747
I0305 06:44:20.961947 140380326586112 logging_writer.py:48] [499700] global_step=499700, grad_norm=3.3721885681152344, loss=1.3775228261947632
I0305 06:45:06.072743 140380334978816 logging_writer.py:48] [499800] global_step=499800, grad_norm=3.1626241207122803, loss=1.177962303161621
I0305 06:45:51.376631 140380326586112 logging_writer.py:48] [499900] global_step=499900, grad_norm=3.2373805046081543, loss=1.2022509574890137
I0305 06:46:36.639117 140380334978816 logging_writer.py:48] [500000] global_step=500000, grad_norm=3.0437822341918945, loss=1.152564287185669
I0305 06:47:21.729335 140380326586112 logging_writer.py:48] [500100] global_step=500100, grad_norm=2.9143779277801514, loss=1.3471004962921143
I0305 06:48:07.011561 140380334978816 logging_writer.py:48] [500200] global_step=500200, grad_norm=2.675750494003296, loss=1.8658177852630615
I0305 06:48:52.161539 140380326586112 logging_writer.py:48] [500300] global_step=500300, grad_norm=3.1092889308929443, loss=2.258983850479126
I0305 06:49:37.018163 140380334978816 logging_writer.py:48] [500400] global_step=500400, grad_norm=3.9101717472076416, loss=1.6657403707504272
I0305 06:50:22.414737 140380326586112 logging_writer.py:48] [500500] global_step=500500, grad_norm=3.0405349731445312, loss=1.1331524848937988
I0305 06:50:35.721198 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:50:45.376542 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:51:09.377147 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:51:11.028380 140575196817216 submission_runner.py:411] Time since start: 242246.63s, 	Step: 500531, 	{'train/accuracy': 0.8866991996765137, 'train/loss': 0.42567360401153564, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 222701.70900726318, 'total_duration': 242246.6289396286, 'accumulated_submission_time': 222701.70900726318, 'accumulated_eval_time': 19480.782245635986, 'accumulated_logging_time': 37.63924813270569}
I0305 06:51:11.120349 140380334978816 logging_writer.py:48] [500531] accumulated_eval_time=19480.782246, accumulated_logging_time=37.639248, accumulated_submission_time=222701.709007, global_step=500531, preemption_count=0, score=222701.709007, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=242246.628940, train/accuracy=0.886699, train/loss=0.425674, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:51:38.939173 140380326586112 logging_writer.py:48] [500600] global_step=500600, grad_norm=3.114028215408325, loss=2.024892807006836
I0305 06:52:22.802686 140380334978816 logging_writer.py:48] [500700] global_step=500700, grad_norm=3.113595962524414, loss=2.0582175254821777
I0305 06:53:07.948106 140380326586112 logging_writer.py:48] [500800] global_step=500800, grad_norm=3.7896668910980225, loss=3.0098912715911865
I0305 06:53:53.018710 140380334978816 logging_writer.py:48] [500900] global_step=500900, grad_norm=4.758550643920898, loss=1.433389663696289
I0305 06:54:38.049188 140380326586112 logging_writer.py:48] [501000] global_step=501000, grad_norm=3.838080883026123, loss=3.0509865283966064
I0305 06:55:23.085541 140380334978816 logging_writer.py:48] [501100] global_step=501100, grad_norm=3.0805177688598633, loss=1.2304807901382446
I0305 06:56:07.910291 140380326586112 logging_writer.py:48] [501200] global_step=501200, grad_norm=3.1618027687072754, loss=1.2607545852661133
I0305 06:56:53.348093 140380334978816 logging_writer.py:48] [501300] global_step=501300, grad_norm=3.2047035694122314, loss=1.1464834213256836
I0305 06:57:38.430335 140380326586112 logging_writer.py:48] [501400] global_step=501400, grad_norm=2.9771926403045654, loss=1.834270715713501
I0305 06:58:11.404098 140575196817216 spec.py:321] Evaluating on the training split.
I0305 06:58:21.240399 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 06:58:52.007209 140575196817216 spec.py:349] Evaluating on the test split.
I0305 06:58:53.670642 140575196817216 submission_runner.py:411] Time since start: 242709.27s, 	Step: 501475, 	{'train/accuracy': 0.8859765529632568, 'train/loss': 0.42276960611343384, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 223121.9326927662, 'total_duration': 242709.27169013023, 'accumulated_submission_time': 223121.9326927662, 'accumulated_eval_time': 19523.047776937485, 'accumulated_logging_time': 37.74262857437134}
I0305 06:58:53.764053 140380334978816 logging_writer.py:48] [501475] accumulated_eval_time=19523.047777, accumulated_logging_time=37.742629, accumulated_submission_time=223121.932693, global_step=501475, preemption_count=0, score=223121.932693, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=242709.271690, train/accuracy=0.885977, train/loss=0.422770, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 06:59:04.106968 140380326586112 logging_writer.py:48] [501500] global_step=501500, grad_norm=3.5590763092041016, loss=1.1639460325241089
I0305 06:59:45.665510 140380334978816 logging_writer.py:48] [501600] global_step=501600, grad_norm=3.3504345417022705, loss=1.307100772857666
I0305 07:00:30.330821 140380326586112 logging_writer.py:48] [501700] global_step=501700, grad_norm=3.1778171062469482, loss=1.0941669940948486
I0305 07:01:15.318571 140380334978816 logging_writer.py:48] [501800] global_step=501800, grad_norm=3.348832607269287, loss=1.1392829418182373
I0305 07:02:00.338828 140380326586112 logging_writer.py:48] [501900] global_step=501900, grad_norm=2.8321356773376465, loss=1.0523053407669067
I0305 07:02:45.330246 140380334978816 logging_writer.py:48] [502000] global_step=502000, grad_norm=2.952049493789673, loss=2.055187702178955
I0305 07:03:30.451945 140380326586112 logging_writer.py:48] [502100] global_step=502100, grad_norm=2.8059439659118652, loss=1.2551521062850952
I0305 07:04:15.287073 140380334978816 logging_writer.py:48] [502200] global_step=502200, grad_norm=3.1172807216644287, loss=1.126326560974121
I0305 07:05:00.339025 140380326586112 logging_writer.py:48] [502300] global_step=502300, grad_norm=3.0950615406036377, loss=1.594857096672058
I0305 07:05:45.370441 140380334978816 logging_writer.py:48] [502400] global_step=502400, grad_norm=3.165073871612549, loss=1.554178237915039
I0305 07:05:53.994382 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:06:04.421274 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:06:32.613614 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:06:34.266480 140575196817216 submission_runner.py:411] Time since start: 243169.87s, 	Step: 502421, 	{'train/accuracy': 0.8878905773162842, 'train/loss': 0.4137028455734253, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 223542.10223674774, 'total_duration': 243169.86743497849, 'accumulated_submission_time': 223542.10223674774, 'accumulated_eval_time': 19563.318788290024, 'accumulated_logging_time': 37.84835863113403}
I0305 07:06:34.361959 140380326586112 logging_writer.py:48] [502421] accumulated_eval_time=19563.318788, accumulated_logging_time=37.848359, accumulated_submission_time=223542.102237, global_step=502421, preemption_count=0, score=223542.102237, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=243169.867435, train/accuracy=0.887891, train/loss=0.413703, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:07:06.153121 140380334978816 logging_writer.py:48] [502500] global_step=502500, grad_norm=3.1996748447418213, loss=2.6640028953552246
I0305 07:07:50.539686 140380326586112 logging_writer.py:48] [502600] global_step=502600, grad_norm=3.3365890979766846, loss=1.4310587644577026
I0305 07:08:36.297295 140380334978816 logging_writer.py:48] [502700] global_step=502700, grad_norm=3.5190062522888184, loss=2.8002638816833496
I0305 07:09:21.075041 140380326586112 logging_writer.py:48] [502800] global_step=502800, grad_norm=3.2477869987487793, loss=2.2419769763946533
I0305 07:10:05.865459 140380334978816 logging_writer.py:48] [502900] global_step=502900, grad_norm=2.9374890327453613, loss=1.052064299583435
I0305 07:10:51.878930 140380326586112 logging_writer.py:48] [503000] global_step=503000, grad_norm=3.028032064437866, loss=2.156447649002075
I0305 07:11:37.516650 140380334978816 logging_writer.py:48] [503100] global_step=503100, grad_norm=3.158022165298462, loss=2.3090200424194336
I0305 07:12:22.733608 140380326586112 logging_writer.py:48] [503200] global_step=503200, grad_norm=3.7863712310791016, loss=1.1892777681350708
I0305 07:13:07.796019 140380334978816 logging_writer.py:48] [503300] global_step=503300, grad_norm=3.3577282428741455, loss=1.1309622526168823
I0305 07:13:34.523651 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:13:44.092148 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:14:11.720617 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:14:13.380714 140575196817216 submission_runner.py:411] Time since start: 243628.98s, 	Step: 503361, 	{'train/accuracy': 0.8865038752555847, 'train/loss': 0.4238264262676239, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 223962.2054350376, 'total_duration': 243628.98177289963, 'accumulated_submission_time': 223962.2054350376, 'accumulated_eval_time': 19602.17483854294, 'accumulated_logging_time': 37.95388746261597}
I0305 07:14:13.475800 140380326586112 logging_writer.py:48] [503361] accumulated_eval_time=19602.174839, accumulated_logging_time=37.953887, accumulated_submission_time=223962.205435, global_step=503361, preemption_count=0, score=223962.205435, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=243628.981773, train/accuracy=0.886504, train/loss=0.423826, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:14:29.373528 140380334978816 logging_writer.py:48] [503400] global_step=503400, grad_norm=3.2814888954162598, loss=2.2815005779266357
I0305 07:15:11.697819 140380326586112 logging_writer.py:48] [503500] global_step=503500, grad_norm=3.0557329654693604, loss=1.64037024974823
I0305 07:15:56.648404 140380334978816 logging_writer.py:48] [503600] global_step=503600, grad_norm=3.4908249378204346, loss=1.4062632322311401
I0305 07:16:42.090248 140380326586112 logging_writer.py:48] [503700] global_step=503700, grad_norm=3.135146141052246, loss=1.0810294151306152
I0305 07:17:27.122523 140380334978816 logging_writer.py:48] [503800] global_step=503800, grad_norm=3.2262675762176514, loss=1.6592975854873657
I0305 07:18:12.017709 140380326586112 logging_writer.py:48] [503900] global_step=503900, grad_norm=3.190556049346924, loss=1.257473111152649
I0305 07:18:57.015746 140380334978816 logging_writer.py:48] [504000] global_step=504000, grad_norm=2.8598897457122803, loss=1.1402666568756104
I0305 07:19:41.743406 140380326586112 logging_writer.py:48] [504100] global_step=504100, grad_norm=2.957380771636963, loss=1.1182266473770142
I0305 07:20:26.966302 140380334978816 logging_writer.py:48] [504200] global_step=504200, grad_norm=3.1120285987854004, loss=1.0268337726593018
I0305 07:21:12.039750 140380326586112 logging_writer.py:48] [504300] global_step=504300, grad_norm=3.1289966106414795, loss=1.803978681564331
I0305 07:21:13.469501 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:21:23.255900 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:21:55.190508 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:21:56.838124 140575196817216 submission_runner.py:411] Time since start: 244092.44s, 	Step: 504305, 	{'train/accuracy': 0.8867577910423279, 'train/loss': 0.4207553267478943, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 224382.14086842537, 'total_duration': 244092.43917560577, 'accumulated_submission_time': 224382.14086842537, 'accumulated_eval_time': 19645.54245853424, 'accumulated_logging_time': 38.05898094177246}
I0305 07:21:56.929804 140380334978816 logging_writer.py:48] [504305] accumulated_eval_time=19645.542459, accumulated_logging_time=38.058981, accumulated_submission_time=224382.140868, global_step=504305, preemption_count=0, score=224382.140868, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=244092.439176, train/accuracy=0.886758, train/loss=0.420755, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:22:35.484707 140380326586112 logging_writer.py:48] [504400] global_step=504400, grad_norm=3.515153408050537, loss=1.1403177976608276
I0305 07:23:20.355747 140380334978816 logging_writer.py:48] [504500] global_step=504500, grad_norm=3.363162040710449, loss=2.4086382389068604
I0305 07:24:05.379877 140380326586112 logging_writer.py:48] [504600] global_step=504600, grad_norm=3.0528762340545654, loss=1.969807744026184
I0305 07:24:50.380050 140380334978816 logging_writer.py:48] [504700] global_step=504700, grad_norm=3.281165361404419, loss=2.500636100769043
I0305 07:25:35.422086 140380326586112 logging_writer.py:48] [504800] global_step=504800, grad_norm=3.039921760559082, loss=1.0517961978912354
I0305 07:26:20.805103 140380334978816 logging_writer.py:48] [504900] global_step=504900, grad_norm=3.1201040744781494, loss=1.0658398866653442
I0305 07:27:05.815399 140380326586112 logging_writer.py:48] [505000] global_step=505000, grad_norm=3.801760196685791, loss=2.957540512084961
I0305 07:27:50.771359 140380334978816 logging_writer.py:48] [505100] global_step=505100, grad_norm=3.195061206817627, loss=2.260899543762207
I0305 07:28:36.571867 140380326586112 logging_writer.py:48] [505200] global_step=505200, grad_norm=4.556488513946533, loss=3.2808938026428223
I0305 07:28:56.972467 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:29:06.853970 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:29:32.648328 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:29:34.301058 140575196817216 submission_runner.py:411] Time since start: 244549.90s, 	Step: 505247, 	{'train/accuracy': 0.8862499594688416, 'train/loss': 0.42336857318878174, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 224802.12478852272, 'total_duration': 244549.90216946602, 'accumulated_submission_time': 224802.12478852272, 'accumulated_eval_time': 19682.870091676712, 'accumulated_logging_time': 38.161217212677}
I0305 07:29:34.395525 140380334978816 logging_writer.py:48] [505247] accumulated_eval_time=19682.870092, accumulated_logging_time=38.161217, accumulated_submission_time=224802.124789, global_step=505247, preemption_count=0, score=224802.124789, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=244549.902169, train/accuracy=0.886250, train/loss=0.423369, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:29:55.876195 140380326586112 logging_writer.py:48] [505300] global_step=505300, grad_norm=3.4370439052581787, loss=1.100667953491211
I0305 07:30:38.622114 140380334978816 logging_writer.py:48] [505400] global_step=505400, grad_norm=3.013237237930298, loss=1.196006417274475
I0305 07:31:23.651058 140380326586112 logging_writer.py:48] [505500] global_step=505500, grad_norm=3.6808717250823975, loss=1.0685336589813232
I0305 07:32:09.095165 140380334978816 logging_writer.py:48] [505600] global_step=505600, grad_norm=4.248199462890625, loss=1.5608265399932861
I0305 07:32:54.265689 140380326586112 logging_writer.py:48] [505700] global_step=505700, grad_norm=3.270427942276001, loss=1.480697751045227
I0305 07:33:39.400339 140380334978816 logging_writer.py:48] [505800] global_step=505800, grad_norm=3.035416841506958, loss=1.5005165338516235
I0305 07:34:24.398067 140380326586112 logging_writer.py:48] [505900] global_step=505900, grad_norm=5.714746952056885, loss=1.0580281019210815
I0305 07:35:09.451265 140380334978816 logging_writer.py:48] [506000] global_step=506000, grad_norm=3.1684489250183105, loss=1.1404831409454346
I0305 07:35:54.481108 140380326586112 logging_writer.py:48] [506100] global_step=506100, grad_norm=3.0583667755126953, loss=1.1718590259552002
I0305 07:36:34.570743 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:36:44.562617 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:37:13.382528 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:37:15.033179 140575196817216 submission_runner.py:411] Time since start: 245010.63s, 	Step: 506190, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.4169977903366089, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 225222.24004030228, 'total_duration': 245010.63395023346, 'accumulated_submission_time': 225222.24004030228, 'accumulated_eval_time': 19723.331513643265, 'accumulated_logging_time': 38.26738238334656}
I0305 07:37:15.141920 140380334978816 logging_writer.py:48] [506190] accumulated_eval_time=19723.331514, accumulated_logging_time=38.267382, accumulated_submission_time=225222.240040, global_step=506190, preemption_count=0, score=225222.240040, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=245010.633950, train/accuracy=0.887422, train/loss=0.416998, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:37:19.523471 140380326586112 logging_writer.py:48] [506200] global_step=506200, grad_norm=2.8769874572753906, loss=1.5553311109542847
I0305 07:38:00.510389 140380334978816 logging_writer.py:48] [506300] global_step=506300, grad_norm=3.180671453475952, loss=1.623704195022583
I0305 07:38:45.099368 140380326586112 logging_writer.py:48] [506400] global_step=506400, grad_norm=3.0793309211730957, loss=1.1240439414978027
I0305 07:39:30.213592 140380334978816 logging_writer.py:48] [506500] global_step=506500, grad_norm=3.23856258392334, loss=1.1028724908828735
I0305 07:40:15.085299 140380326586112 logging_writer.py:48] [506600] global_step=506600, grad_norm=3.25783109664917, loss=1.2611210346221924
I0305 07:41:00.086997 140380334978816 logging_writer.py:48] [506700] global_step=506700, grad_norm=3.221012592315674, loss=1.2518131732940674
I0305 07:41:44.920647 140380326586112 logging_writer.py:48] [506800] global_step=506800, grad_norm=3.2322864532470703, loss=1.4038989543914795
I0305 07:42:29.992808 140380334978816 logging_writer.py:48] [506900] global_step=506900, grad_norm=3.125800371170044, loss=1.4409308433532715
I0305 07:43:15.056190 140380326586112 logging_writer.py:48] [507000] global_step=507000, grad_norm=5.208733081817627, loss=3.1947274208068848
I0305 07:43:59.749642 140380334978816 logging_writer.py:48] [507100] global_step=507100, grad_norm=3.102311372756958, loss=1.1690452098846436
I0305 07:44:15.196173 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:44:25.037384 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:44:51.586547 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:44:53.243253 140575196817216 submission_runner.py:411] Time since start: 245468.84s, 	Step: 507136, 	{'train/accuracy': 0.8875390291213989, 'train/loss': 0.41864198446273804, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 225642.23197722435, 'total_duration': 245468.84431529045, 'accumulated_submission_time': 225642.23197722435, 'accumulated_eval_time': 19761.377603769302, 'accumulated_logging_time': 38.3882851600647}
I0305 07:44:53.335201 140380326586112 logging_writer.py:48] [507136] accumulated_eval_time=19761.377604, accumulated_logging_time=38.388285, accumulated_submission_time=225642.231977, global_step=507136, preemption_count=0, score=225642.231977, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=245468.844315, train/accuracy=0.887539, train/loss=0.418642, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:45:19.186815 140380334978816 logging_writer.py:48] [507200] global_step=507200, grad_norm=3.453368902206421, loss=2.98677134513855
I0305 07:46:02.544037 140380326586112 logging_writer.py:48] [507300] global_step=507300, grad_norm=3.2288331985473633, loss=1.0458301305770874
I0305 07:46:47.667657 140380334978816 logging_writer.py:48] [507400] global_step=507400, grad_norm=3.304849624633789, loss=2.7092368602752686
I0305 07:47:33.004458 140380326586112 logging_writer.py:48] [507500] global_step=507500, grad_norm=2.986016273498535, loss=1.1855642795562744
I0305 07:48:18.092148 140380334978816 logging_writer.py:48] [507600] global_step=507600, grad_norm=3.5366997718811035, loss=1.4779263734817505
I0305 07:49:03.532740 140380326586112 logging_writer.py:48] [507700] global_step=507700, grad_norm=3.3436684608459473, loss=2.7059643268585205
I0305 07:49:48.650596 140380334978816 logging_writer.py:48] [507800] global_step=507800, grad_norm=3.0239241123199463, loss=1.2082583904266357
I0305 07:50:33.547452 140380326586112 logging_writer.py:48] [507900] global_step=507900, grad_norm=2.996398687362671, loss=1.1786121129989624
I0305 07:51:18.731352 140380334978816 logging_writer.py:48] [508000] global_step=508000, grad_norm=3.2086143493652344, loss=2.206012010574341
I0305 07:51:53.376424 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:52:03.186486 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 07:52:31.667208 140575196817216 spec.py:349] Evaluating on the test split.
I0305 07:52:33.322020 140575196817216 submission_runner.py:411] Time since start: 245928.92s, 	Step: 508079, 	{'train/accuracy': 0.8869531154632568, 'train/loss': 0.42582082748413086, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 226062.21477413177, 'total_duration': 245928.9227323532, 'accumulated_submission_time': 226062.21477413177, 'accumulated_eval_time': 19801.3218562603, 'accumulated_logging_time': 38.48969912528992}
I0305 07:52:33.421020 140380326586112 logging_writer.py:48] [508079] accumulated_eval_time=19801.321856, accumulated_logging_time=38.489699, accumulated_submission_time=226062.214774, global_step=508079, preemption_count=0, score=226062.214774, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=245928.922732, train/accuracy=0.886953, train/loss=0.425821, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 07:52:42.165851 140380334978816 logging_writer.py:48] [508100] global_step=508100, grad_norm=2.9532477855682373, loss=1.03240966796875
I0305 07:53:23.805352 140380326586112 logging_writer.py:48] [508200] global_step=508200, grad_norm=3.1987078189849854, loss=1.4412591457366943
I0305 07:54:08.527512 140380334978816 logging_writer.py:48] [508300] global_step=508300, grad_norm=2.9880917072296143, loss=1.6148951053619385
I0305 07:54:53.606493 140380326586112 logging_writer.py:48] [508400] global_step=508400, grad_norm=2.9874792098999023, loss=1.0893564224243164
I0305 07:55:38.733450 140380334978816 logging_writer.py:48] [508500] global_step=508500, grad_norm=4.138394832611084, loss=2.6437861919403076
I0305 07:56:23.728526 140380326586112 logging_writer.py:48] [508600] global_step=508600, grad_norm=2.950650215148926, loss=1.057280421257019
I0305 07:57:08.495715 140380334978816 logging_writer.py:48] [508700] global_step=508700, grad_norm=2.8510053157806396, loss=1.2119548320770264
I0305 07:57:53.427803 140380326586112 logging_writer.py:48] [508800] global_step=508800, grad_norm=3.133854627609253, loss=1.159454107284546
I0305 07:58:38.398284 140380334978816 logging_writer.py:48] [508900] global_step=508900, grad_norm=3.5329809188842773, loss=1.1463347673416138
I0305 07:59:23.357970 140380326586112 logging_writer.py:48] [509000] global_step=509000, grad_norm=3.44138503074646, loss=1.1483601331710815
I0305 07:59:33.333137 140575196817216 spec.py:321] Evaluating on the training split.
I0305 07:59:42.983797 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:00:15.964294 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:00:17.616770 140575196817216 submission_runner.py:411] Time since start: 246393.22s, 	Step: 509024, 	{'train/accuracy': 0.8847460746765137, 'train/loss': 0.4278537333011627, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 226482.06759667397, 'total_duration': 246393.21773838997, 'accumulated_submission_time': 226482.06759667397, 'accumulated_eval_time': 19845.604430675507, 'accumulated_logging_time': 38.599026679992676}
I0305 08:00:17.710221 140380334978816 logging_writer.py:48] [509024] accumulated_eval_time=19845.604431, accumulated_logging_time=38.599027, accumulated_submission_time=226482.067597, global_step=509024, preemption_count=0, score=226482.067597, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=246393.217738, train/accuracy=0.884746, train/loss=0.427854, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:00:48.299326 140380326586112 logging_writer.py:48] [509100] global_step=509100, grad_norm=3.0674092769622803, loss=1.1407039165496826
I0305 08:01:32.731030 140380334978816 logging_writer.py:48] [509200] global_step=509200, grad_norm=2.9890317916870117, loss=2.2862961292266846
I0305 08:02:17.812121 140380326586112 logging_writer.py:48] [509300] global_step=509300, grad_norm=3.0512945652008057, loss=1.252375841140747
I0305 08:03:03.277865 140380334978816 logging_writer.py:48] [509400] global_step=509400, grad_norm=3.1462502479553223, loss=1.4183088541030884
I0305 08:03:48.302911 140380326586112 logging_writer.py:48] [509500] global_step=509500, grad_norm=2.848649501800537, loss=2.1333584785461426
I0305 08:04:33.597111 140380334978816 logging_writer.py:48] [509600] global_step=509600, grad_norm=3.1007277965545654, loss=2.8806166648864746
I0305 08:05:18.729043 140380326586112 logging_writer.py:48] [509700] global_step=509700, grad_norm=4.978574752807617, loss=3.3016762733459473
I0305 08:06:03.828148 140380334978816 logging_writer.py:48] [509800] global_step=509800, grad_norm=3.5890021324157715, loss=3.0903379917144775
I0305 08:06:49.249503 140380326586112 logging_writer.py:48] [509900] global_step=509900, grad_norm=3.4809625148773193, loss=2.899211883544922
I0305 08:07:17.811535 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:07:27.698755 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:07:55.698565 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:07:57.352121 140575196817216 submission_runner.py:411] Time since start: 246852.95s, 	Step: 509965, 	{'train/accuracy': 0.8874022960662842, 'train/loss': 0.4216749966144562, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 226902.1107466221, 'total_duration': 246852.95292115211, 'accumulated_submission_time': 226902.1107466221, 'accumulated_eval_time': 19885.14374423027, 'accumulated_logging_time': 38.70250916481018}
I0305 08:07:57.456365 140380334978816 logging_writer.py:48] [509965] accumulated_eval_time=19885.143744, accumulated_logging_time=38.702509, accumulated_submission_time=226902.110747, global_step=509965, preemption_count=0, score=226902.110747, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=246852.952921, train/accuracy=0.887402, train/loss=0.421675, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:08:11.760660 140380326586112 logging_writer.py:48] [510000] global_step=510000, grad_norm=3.287249803543091, loss=2.564110279083252
I0305 08:08:53.905735 140380334978816 logging_writer.py:48] [510100] global_step=510100, grad_norm=3.3786964416503906, loss=2.341313362121582
I0305 08:09:38.872275 140380326586112 logging_writer.py:48] [510200] global_step=510200, grad_norm=3.4756550788879395, loss=1.1363242864608765
I0305 08:10:24.352113 140380334978816 logging_writer.py:48] [510300] global_step=510300, grad_norm=4.3277812004089355, loss=3.156933546066284
I0305 08:11:09.192744 140380326586112 logging_writer.py:48] [510400] global_step=510400, grad_norm=3.1654393672943115, loss=1.1147162914276123
I0305 08:11:54.417759 140380334978816 logging_writer.py:48] [510500] global_step=510500, grad_norm=3.3673410415649414, loss=1.20787513256073
I0305 08:12:39.667023 140380326586112 logging_writer.py:48] [510600] global_step=510600, grad_norm=4.170389175415039, loss=3.236891984939575
I0305 08:13:24.384416 140380334978816 logging_writer.py:48] [510700] global_step=510700, grad_norm=3.043822765350342, loss=1.1514912843704224
I0305 08:14:09.578498 140380326586112 logging_writer.py:48] [510800] global_step=510800, grad_norm=3.1007802486419678, loss=1.2919321060180664
I0305 08:14:54.468937 140380334978816 logging_writer.py:48] [510900] global_step=510900, grad_norm=3.6265857219696045, loss=2.7463784217834473
I0305 08:14:57.741635 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:15:07.438988 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:15:36.873013 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:15:38.524405 140575196817216 submission_runner.py:411] Time since start: 247314.13s, 	Step: 510909, 	{'train/accuracy': 0.8890234231948853, 'train/loss': 0.4123329520225525, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 227322.3340280056, 'total_duration': 247314.1253759861, 'accumulated_submission_time': 227322.3340280056, 'accumulated_eval_time': 19925.925420999527, 'accumulated_logging_time': 38.819873094558716}
I0305 08:15:38.615890 140380326586112 logging_writer.py:48] [510909] accumulated_eval_time=19925.925421, accumulated_logging_time=38.819873, accumulated_submission_time=227322.334028, global_step=510909, preemption_count=0, score=227322.334028, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=247314.125376, train/accuracy=0.889023, train/loss=0.412333, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:16:15.507623 140380334978816 logging_writer.py:48] [511000] global_step=511000, grad_norm=3.1910297870635986, loss=1.1206578016281128
I0305 08:17:00.159430 140380326586112 logging_writer.py:48] [511100] global_step=511100, grad_norm=3.0128860473632812, loss=1.0431870222091675
I0305 08:17:45.285223 140380334978816 logging_writer.py:48] [511200] global_step=511200, grad_norm=4.423829555511475, loss=3.295969009399414
I0305 08:18:30.351449 140380326586112 logging_writer.py:48] [511300] global_step=511300, grad_norm=2.977689266204834, loss=1.1437829732894897
I0305 08:19:15.277371 140380334978816 logging_writer.py:48] [511400] global_step=511400, grad_norm=3.1422674655914307, loss=1.4391071796417236
I0305 08:20:00.384594 140380326586112 logging_writer.py:48] [511500] global_step=511500, grad_norm=2.9682164192199707, loss=1.392479658126831
I0305 08:20:45.466047 140380334978816 logging_writer.py:48] [511600] global_step=511600, grad_norm=3.0912346839904785, loss=1.1163427829742432
I0305 08:21:30.576460 140380326586112 logging_writer.py:48] [511700] global_step=511700, grad_norm=3.057673692703247, loss=1.5767744779586792
I0305 08:22:15.689865 140380334978816 logging_writer.py:48] [511800] global_step=511800, grad_norm=3.18550705909729, loss=1.2764484882354736
I0305 08:22:38.745727 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:22:48.667763 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:23:20.720445 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:23:22.371232 140575196817216 submission_runner.py:411] Time since start: 247777.97s, 	Step: 511853, 	{'train/accuracy': 0.8871679306030273, 'train/loss': 0.42327311635017395, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 227742.40439462662, 'total_duration': 247777.97219228745, 'accumulated_submission_time': 227742.40439462662, 'accumulated_eval_time': 19969.549832820892, 'accumulated_logging_time': 38.92194437980652}
I0305 08:23:22.468138 140380326586112 logging_writer.py:48] [511853] accumulated_eval_time=19969.549833, accumulated_logging_time=38.921944, accumulated_submission_time=227742.404395, global_step=511853, preemption_count=0, score=227742.404395, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=247777.972192, train/accuracy=0.887168, train/loss=0.423273, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:23:41.540260 140380334978816 logging_writer.py:48] [511900] global_step=511900, grad_norm=3.1515161991119385, loss=1.7279146909713745
I0305 08:24:24.426771 140380326586112 logging_writer.py:48] [512000] global_step=512000, grad_norm=3.469209671020508, loss=3.0248007774353027
I0305 08:25:09.490785 140380334978816 logging_writer.py:48] [512100] global_step=512100, grad_norm=3.0955841541290283, loss=2.1338977813720703
I0305 08:25:54.793241 140380326586112 logging_writer.py:48] [512200] global_step=512200, grad_norm=3.7776591777801514, loss=3.3373866081237793
I0305 08:26:39.872661 140380334978816 logging_writer.py:48] [512300] global_step=512300, grad_norm=3.0787482261657715, loss=1.2067859172821045
I0305 08:27:25.027907 140380326586112 logging_writer.py:48] [512400] global_step=512400, grad_norm=3.1167662143707275, loss=1.2099179029464722
I0305 08:28:10.014672 140380334978816 logging_writer.py:48] [512500] global_step=512500, grad_norm=5.255035877227783, loss=1.1694400310516357
I0305 08:28:54.923233 140380326586112 logging_writer.py:48] [512600] global_step=512600, grad_norm=3.6437249183654785, loss=1.7317087650299072
I0305 08:29:40.145196 140380334978816 logging_writer.py:48] [512700] global_step=512700, grad_norm=4.015667915344238, loss=1.1744319200515747
I0305 08:30:22.472307 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:30:32.290078 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:30:59.891082 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:31:01.536971 140575196817216 submission_runner.py:411] Time since start: 248237.14s, 	Step: 512796, 	{'train/accuracy': 0.8870507478713989, 'train/loss': 0.4156704545021057, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 228162.349899292, 'total_duration': 248237.13788104057, 'accumulated_submission_time': 228162.349899292, 'accumulated_eval_time': 20008.613343715668, 'accumulated_logging_time': 39.02891778945923}
I0305 08:31:01.632661 140380326586112 logging_writer.py:48] [512796] accumulated_eval_time=20008.613344, accumulated_logging_time=39.028918, accumulated_submission_time=228162.349899, global_step=512796, preemption_count=0, score=228162.349899, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=248237.137881, train/accuracy=0.887051, train/loss=0.415670, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:31:03.632718 140380334978816 logging_writer.py:48] [512800] global_step=512800, grad_norm=3.044678211212158, loss=1.135171890258789
I0305 08:31:44.417315 140380326586112 logging_writer.py:48] [512900] global_step=512900, grad_norm=3.028265953063965, loss=1.1731674671173096
I0305 08:32:29.262257 140380334978816 logging_writer.py:48] [513000] global_step=513000, grad_norm=2.969123125076294, loss=1.5664924383163452
I0305 08:33:14.189010 140380326586112 logging_writer.py:48] [513100] global_step=513100, grad_norm=3.3896219730377197, loss=2.2668185234069824
I0305 08:33:59.443543 140380334978816 logging_writer.py:48] [513200] global_step=513200, grad_norm=3.1532247066497803, loss=1.9041247367858887
I0305 08:34:44.341554 140380326586112 logging_writer.py:48] [513300] global_step=513300, grad_norm=3.6098949909210205, loss=1.290844202041626
I0305 08:35:29.699776 140380334978816 logging_writer.py:48] [513400] global_step=513400, grad_norm=2.9966821670532227, loss=1.2487651109695435
I0305 08:36:14.750043 140380326586112 logging_writer.py:48] [513500] global_step=513500, grad_norm=3.0701792240142822, loss=2.726837635040283
I0305 08:36:59.565409 140380334978816 logging_writer.py:48] [513600] global_step=513600, grad_norm=3.144684314727783, loss=1.1446270942687988
I0305 08:37:44.808581 140380326586112 logging_writer.py:48] [513700] global_step=513700, grad_norm=3.2437551021575928, loss=1.6181857585906982
I0305 08:38:01.577188 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:38:11.403578 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:38:39.996764 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:38:41.648574 140575196817216 submission_runner.py:411] Time since start: 248697.25s, 	Step: 513739, 	{'train/accuracy': 0.8882030844688416, 'train/loss': 0.4158047139644623, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 228582.2359097004, 'total_duration': 248697.24965715408, 'accumulated_submission_time': 228582.2359097004, 'accumulated_eval_time': 20048.68375325203, 'accumulated_logging_time': 39.13458871841431}
I0305 08:38:41.746219 140380334978816 logging_writer.py:48] [513739] accumulated_eval_time=20048.683753, accumulated_logging_time=39.134589, accumulated_submission_time=228582.235910, global_step=513739, preemption_count=0, score=228582.235910, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=248697.249657, train/accuracy=0.888203, train/loss=0.415805, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:39:06.394584 140380326586112 logging_writer.py:48] [513800] global_step=513800, grad_norm=3.0610873699188232, loss=1.192387580871582
I0305 08:39:50.214690 140380334978816 logging_writer.py:48] [513900] global_step=513900, grad_norm=3.0293076038360596, loss=1.7314351797103882
I0305 08:40:35.463467 140380326586112 logging_writer.py:48] [514000] global_step=514000, grad_norm=2.8882477283477783, loss=1.069413661956787
I0305 08:41:20.732747 140380334978816 logging_writer.py:48] [514100] global_step=514100, grad_norm=8.075239181518555, loss=1.02631676197052
I0305 08:42:05.509564 140380326586112 logging_writer.py:48] [514200] global_step=514200, grad_norm=4.029218673706055, loss=3.1959781646728516
I0305 08:42:50.863916 140380334978816 logging_writer.py:48] [514300] global_step=514300, grad_norm=3.723149299621582, loss=3.181461811065674
I0305 08:43:35.911587 140380326586112 logging_writer.py:48] [514400] global_step=514400, grad_norm=5.051924228668213, loss=2.848792791366577
I0305 08:44:20.934675 140380334978816 logging_writer.py:48] [514500] global_step=514500, grad_norm=2.9701619148254395, loss=1.4813263416290283
I0305 08:45:06.045028 140380326586112 logging_writer.py:48] [514600] global_step=514600, grad_norm=4.532557964324951, loss=2.9889285564422607
I0305 08:45:42.052412 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:45:52.403165 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:46:21.115428 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:46:22.764771 140575196817216 submission_runner.py:411] Time since start: 249158.37s, 	Step: 514682, 	{'train/accuracy': 0.8873828053474426, 'train/loss': 0.4168609082698822, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 229002.4842107296, 'total_duration': 249158.36589884758, 'accumulated_submission_time': 229002.4842107296, 'accumulated_eval_time': 20089.395189762115, 'accumulated_logging_time': 39.24177360534668}
I0305 08:46:22.858406 140380334978816 logging_writer.py:48] [514682] accumulated_eval_time=20089.395190, accumulated_logging_time=39.241774, accumulated_submission_time=229002.484211, global_step=514682, preemption_count=0, score=229002.484211, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=249158.365899, train/accuracy=0.887383, train/loss=0.416861, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:46:30.429667 140380326586112 logging_writer.py:48] [514700] global_step=514700, grad_norm=3.1945769786834717, loss=1.1021915674209595
I0305 08:47:11.754653 140380334978816 logging_writer.py:48] [514800] global_step=514800, grad_norm=3.5126562118530273, loss=2.7829766273498535
I0305 08:47:56.605008 140380326586112 logging_writer.py:48] [514900] global_step=514900, grad_norm=3.9108850955963135, loss=3.0841522216796875
I0305 08:48:42.099846 140380334978816 logging_writer.py:48] [515000] global_step=515000, grad_norm=3.0801565647125244, loss=1.0911576747894287
I0305 08:49:27.072038 140380326586112 logging_writer.py:48] [515100] global_step=515100, grad_norm=5.128689289093018, loss=2.36045503616333
I0305 08:50:12.289247 140380334978816 logging_writer.py:48] [515200] global_step=515200, grad_norm=3.0377776622772217, loss=1.1105315685272217
I0305 08:50:57.433233 140380326586112 logging_writer.py:48] [515300] global_step=515300, grad_norm=3.2796072959899902, loss=1.8904922008514404
I0305 08:51:42.462169 140380334978816 logging_writer.py:48] [515400] global_step=515400, grad_norm=3.1510000228881836, loss=1.150811791419983
I0305 08:52:28.093832 140380326586112 logging_writer.py:48] [515500] global_step=515500, grad_norm=3.1200342178344727, loss=1.42999267578125
I0305 08:53:13.331431 140380334978816 logging_writer.py:48] [515600] global_step=515600, grad_norm=4.965326309204102, loss=3.111130714416504
I0305 08:53:23.011892 140575196817216 spec.py:321] Evaluating on the training split.
I0305 08:53:32.906125 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 08:54:00.912291 140575196817216 spec.py:349] Evaluating on the test split.
I0305 08:54:02.581320 140575196817216 submission_runner.py:411] Time since start: 249618.18s, 	Step: 515623, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.42157629132270813, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 229422.58079338074, 'total_duration': 249618.18248152733, 'accumulated_submission_time': 229422.58079338074, 'accumulated_eval_time': 20128.963701486588, 'accumulated_logging_time': 39.34479546546936}
I0305 08:54:02.673464 140380326586112 logging_writer.py:48] [515623] accumulated_eval_time=20128.963701, accumulated_logging_time=39.344795, accumulated_submission_time=229422.580793, global_step=515623, preemption_count=0, score=229422.580793, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=249618.182482, train/accuracy=0.887598, train/loss=0.421576, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 08:54:33.656303 140380334978816 logging_writer.py:48] [515700] global_step=515700, grad_norm=3.02135968208313, loss=1.0525336265563965
I0305 08:55:17.856357 140380326586112 logging_writer.py:48] [515800] global_step=515800, grad_norm=3.027966022491455, loss=1.1984730958938599
I0305 08:56:03.332879 140380334978816 logging_writer.py:48] [515900] global_step=515900, grad_norm=3.6929473876953125, loss=3.160360097885132
I0305 08:56:48.670268 140380326586112 logging_writer.py:48] [516000] global_step=516000, grad_norm=3.250755548477173, loss=1.8615288734436035
I0305 08:57:33.437564 140380334978816 logging_writer.py:48] [516100] global_step=516100, grad_norm=3.269606590270996, loss=1.2318158149719238
I0305 08:58:18.421538 140380326586112 logging_writer.py:48] [516200] global_step=516200, grad_norm=3.6792478561401367, loss=3.084606170654297
I0305 08:59:03.245214 140380334978816 logging_writer.py:48] [516300] global_step=516300, grad_norm=4.464971542358398, loss=3.1289427280426025
I0305 08:59:48.345713 140380326586112 logging_writer.py:48] [516400] global_step=516400, grad_norm=3.00986385345459, loss=1.154172420501709
I0305 09:00:33.471492 140380334978816 logging_writer.py:48] [516500] global_step=516500, grad_norm=2.8599283695220947, loss=1.0324385166168213
I0305 09:01:02.597822 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:01:12.378960 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:01:38.646163 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:01:40.307310 140575196817216 submission_runner.py:411] Time since start: 250075.91s, 	Step: 516566, 	{'train/accuracy': 0.8914648294448853, 'train/loss': 0.4060322046279907, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 229842.44540429115, 'total_duration': 250075.90839600563, 'accumulated_submission_time': 229842.44540429115, 'accumulated_eval_time': 20166.67224431038, 'accumulated_logging_time': 39.44740009307861}
I0305 09:01:40.423918 140380326586112 logging_writer.py:48] [516566] accumulated_eval_time=20166.672244, accumulated_logging_time=39.447400, accumulated_submission_time=229842.445404, global_step=516566, preemption_count=0, score=229842.445404, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=250075.908396, train/accuracy=0.891465, train/loss=0.406032, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:01:54.345023 140380334978816 logging_writer.py:48] [516600] global_step=516600, grad_norm=3.0109870433807373, loss=1.0268652439117432
I0305 09:02:36.134255 140380326586112 logging_writer.py:48] [516700] global_step=516700, grad_norm=3.0223476886749268, loss=1.202852725982666
I0305 09:03:21.279669 140380334978816 logging_writer.py:48] [516800] global_step=516800, grad_norm=3.582968235015869, loss=2.921217679977417
I0305 09:04:06.389266 140380326586112 logging_writer.py:48] [516900] global_step=516900, grad_norm=3.6270253658294678, loss=2.930243730545044
I0305 09:04:51.574078 140380334978816 logging_writer.py:48] [517000] global_step=517000, grad_norm=3.3300933837890625, loss=1.5680298805236816
I0305 09:05:36.571385 140380326586112 logging_writer.py:48] [517100] global_step=517100, grad_norm=3.208559989929199, loss=1.0636008977890015
I0305 09:06:21.847967 140380334978816 logging_writer.py:48] [517200] global_step=517200, grad_norm=4.122161865234375, loss=3.2192511558532715
I0305 09:07:06.950164 140380326586112 logging_writer.py:48] [517300] global_step=517300, grad_norm=2.8751580715179443, loss=2.0403695106506348
I0305 09:07:51.794191 140380334978816 logging_writer.py:48] [517400] global_step=517400, grad_norm=3.083881139755249, loss=1.1782416105270386
I0305 09:08:36.720964 140380326586112 logging_writer.py:48] [517500] global_step=517500, grad_norm=3.0920724868774414, loss=1.237630844116211
I0305 09:08:40.424679 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:08:50.255130 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:09:14.920908 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:09:16.572706 140575196817216 submission_runner.py:411] Time since start: 250532.17s, 	Step: 517510, 	{'train/accuracy': 0.8875195384025574, 'train/loss': 0.41872453689575195, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 230262.38619685173, 'total_duration': 250532.1736881733, 'accumulated_submission_time': 230262.38619685173, 'accumulated_eval_time': 20202.819187641144, 'accumulated_logging_time': 39.574952125549316}
I0305 09:09:16.678869 140380334978816 logging_writer.py:48] [517510] accumulated_eval_time=20202.819188, accumulated_logging_time=39.574952, accumulated_submission_time=230262.386197, global_step=517510, preemption_count=0, score=230262.386197, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=250532.173688, train/accuracy=0.887520, train/loss=0.418725, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:09:52.874096 140380326586112 logging_writer.py:48] [517600] global_step=517600, grad_norm=3.164182186126709, loss=2.671298027038574
I0305 09:10:37.336491 140380334978816 logging_writer.py:48] [517700] global_step=517700, grad_norm=3.15313982963562, loss=2.3421742916107178
I0305 09:11:22.629440 140380326586112 logging_writer.py:48] [517800] global_step=517800, grad_norm=3.3354477882385254, loss=2.66852068901062
I0305 09:12:07.803714 140380334978816 logging_writer.py:48] [517900] global_step=517900, grad_norm=3.4989469051361084, loss=1.0901516675949097
I0305 09:12:52.886233 140380326586112 logging_writer.py:48] [518000] global_step=518000, grad_norm=3.6331562995910645, loss=3.2663254737854004
I0305 09:13:38.033029 140380334978816 logging_writer.py:48] [518100] global_step=518100, grad_norm=3.5004329681396484, loss=1.4868477582931519
I0305 09:14:22.907402 140380326586112 logging_writer.py:48] [518200] global_step=518200, grad_norm=3.440898895263672, loss=2.1089608669281006
I0305 09:15:07.895954 140380334978816 logging_writer.py:48] [518300] global_step=518300, grad_norm=3.352109432220459, loss=2.705554723739624
I0305 09:15:52.754368 140380326586112 logging_writer.py:48] [518400] global_step=518400, grad_norm=3.2197771072387695, loss=1.1361585855484009
I0305 09:16:16.878282 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:16:27.247617 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:16:54.217345 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:16:55.873803 140575196817216 submission_runner.py:411] Time since start: 250991.47s, 	Step: 518455, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.417012482881546, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 230682.52629995346, 'total_duration': 250991.47455906868, 'accumulated_submission_time': 230682.52629995346, 'accumulated_eval_time': 20241.813415050507, 'accumulated_logging_time': 39.69140291213989}
I0305 09:16:55.968353 140380334978816 logging_writer.py:48] [518455] accumulated_eval_time=20241.813415, accumulated_logging_time=39.691403, accumulated_submission_time=230682.526300, global_step=518455, preemption_count=0, score=230682.526300, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=250991.474559, train/accuracy=0.887852, train/loss=0.417012, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:17:14.242127 140380326586112 logging_writer.py:48] [518500] global_step=518500, grad_norm=2.9996471405029297, loss=1.0739413499832153
I0305 09:17:56.450099 140380334978816 logging_writer.py:48] [518600] global_step=518600, grad_norm=3.096085548400879, loss=1.0341124534606934
I0305 09:18:41.343134 140380326586112 logging_writer.py:48] [518700] global_step=518700, grad_norm=2.9870893955230713, loss=1.2580565214157104
I0305 09:19:26.437978 140380334978816 logging_writer.py:48] [518800] global_step=518800, grad_norm=2.9695112705230713, loss=1.1347270011901855
I0305 09:20:11.207754 140380326586112 logging_writer.py:48] [518900] global_step=518900, grad_norm=2.945422887802124, loss=1.351815938949585
I0305 09:20:56.059180 140380334978816 logging_writer.py:48] [519000] global_step=519000, grad_norm=3.8971030712127686, loss=2.980614423751831
I0305 09:21:41.059318 140380326586112 logging_writer.py:48] [519100] global_step=519100, grad_norm=3.027496099472046, loss=1.5380529165267944
I0305 09:22:25.953594 140380334978816 logging_writer.py:48] [519200] global_step=519200, grad_norm=3.2095580101013184, loss=1.1714260578155518
I0305 09:23:11.182383 140380326586112 logging_writer.py:48] [519300] global_step=519300, grad_norm=3.4178709983825684, loss=3.0042331218719482
I0305 09:23:56.122769 140380334978816 logging_writer.py:48] [519400] global_step=519400, grad_norm=3.0032105445861816, loss=2.588041305541992
I0305 09:23:56.134769 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:24:06.332388 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:24:30.351006 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:24:32.001230 140575196817216 submission_runner.py:411] Time since start: 251447.60s, 	Step: 519401, 	{'train/accuracy': 0.8869335651397705, 'train/loss': 0.4184837341308594, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 231102.6332271099, 'total_duration': 251447.6019744873, 'accumulated_submission_time': 231102.6332271099, 'accumulated_eval_time': 20277.67854499817, 'accumulated_logging_time': 39.79632830619812}
I0305 09:24:32.097811 140380326586112 logging_writer.py:48] [519401] accumulated_eval_time=20277.678545, accumulated_logging_time=39.796328, accumulated_submission_time=231102.633227, global_step=519401, preemption_count=0, score=231102.633227, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=251447.601974, train/accuracy=0.886934, train/loss=0.418484, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:25:12.381689 140380334978816 logging_writer.py:48] [519500] global_step=519500, grad_norm=3.4111392498016357, loss=2.9973533153533936
I0305 09:25:57.190436 140380326586112 logging_writer.py:48] [519600] global_step=519600, grad_norm=4.073899745941162, loss=1.7689173221588135
I0305 09:26:42.795664 140380334978816 logging_writer.py:48] [519700] global_step=519700, grad_norm=3.077568292617798, loss=1.0924643278121948
I0305 09:27:27.718549 140380326586112 logging_writer.py:48] [519800] global_step=519800, grad_norm=2.7701892852783203, loss=1.0035629272460938
I0305 09:28:12.622824 140380334978816 logging_writer.py:48] [519900] global_step=519900, grad_norm=3.284106492996216, loss=2.754850387573242
I0305 09:28:57.715285 140380326586112 logging_writer.py:48] [520000] global_step=520000, grad_norm=4.544189453125, loss=3.2477903366088867
I0305 09:29:42.627733 140380334978816 logging_writer.py:48] [520100] global_step=520100, grad_norm=3.847046136856079, loss=3.185453414916992
I0305 09:30:27.625977 140380326586112 logging_writer.py:48] [520200] global_step=520200, grad_norm=3.1848247051239014, loss=1.471086025238037
I0305 09:31:12.373928 140380334978816 logging_writer.py:48] [520300] global_step=520300, grad_norm=3.68241024017334, loss=1.1513681411743164
I0305 09:31:32.152558 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:31:41.976927 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:32:09.579984 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:32:11.237147 140575196817216 submission_runner.py:411] Time since start: 251906.84s, 	Step: 520346, 	{'train/accuracy': 0.88685542345047, 'train/loss': 0.4225926995277405, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 231522.62950658798, 'total_duration': 251906.83761620522, 'accumulated_submission_time': 231522.62950658798, 'accumulated_eval_time': 20316.76153421402, 'accumulated_logging_time': 39.9029598236084}
I0305 09:32:11.330332 140380326586112 logging_writer.py:48] [520346] accumulated_eval_time=20316.761534, accumulated_logging_time=39.902960, accumulated_submission_time=231522.629507, global_step=520346, preemption_count=0, score=231522.629507, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=251906.837616, train/accuracy=0.886855, train/loss=0.422593, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:32:33.182350 140380334978816 logging_writer.py:48] [520400] global_step=520400, grad_norm=3.2360565662384033, loss=1.1311864852905273
I0305 09:33:16.695257 140380326586112 logging_writer.py:48] [520500] global_step=520500, grad_norm=3.3530006408691406, loss=1.3757647275924683
I0305 09:34:01.429997 140380334978816 logging_writer.py:48] [520600] global_step=520600, grad_norm=3.211507558822632, loss=1.1476922035217285
I0305 09:34:46.584743 140380326586112 logging_writer.py:48] [520700] global_step=520700, grad_norm=3.178924322128296, loss=1.7807278633117676
I0305 09:35:31.794509 140380334978816 logging_writer.py:48] [520800] global_step=520800, grad_norm=3.111133575439453, loss=1.392974853515625
I0305 09:36:17.154245 140380326586112 logging_writer.py:48] [520900] global_step=520900, grad_norm=4.627511501312256, loss=1.0740363597869873
I0305 09:37:02.701190 140380334978816 logging_writer.py:48] [521000] global_step=521000, grad_norm=3.2721471786499023, loss=2.6919350624084473
I0305 09:37:47.590053 140380326586112 logging_writer.py:48] [521100] global_step=521100, grad_norm=3.1499180793762207, loss=2.0149753093719482
I0305 09:38:32.629740 140380334978816 logging_writer.py:48] [521200] global_step=521200, grad_norm=2.9826087951660156, loss=1.1113203763961792
I0305 09:39:11.405997 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:39:21.453228 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:39:51.864296 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:39:53.524897 140575196817216 submission_runner.py:411] Time since start: 252369.13s, 	Step: 521288, 	{'train/accuracy': 0.8891991972923279, 'train/loss': 0.4170268177986145, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 231942.6417376995, 'total_duration': 252369.12612867355, 'accumulated_submission_time': 231942.6417376995, 'accumulated_eval_time': 20358.879606485367, 'accumulated_logging_time': 40.01094198226929}
I0305 09:39:53.619077 140380326586112 logging_writer.py:48] [521288] accumulated_eval_time=20358.879606, accumulated_logging_time=40.010942, accumulated_submission_time=231942.641738, global_step=521288, preemption_count=0, score=231942.641738, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=252369.126129, train/accuracy=0.889199, train/loss=0.417027, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:39:58.789516 140380334978816 logging_writer.py:48] [521300] global_step=521300, grad_norm=4.418087959289551, loss=3.1827073097229004
I0305 09:40:39.839909 140380326586112 logging_writer.py:48] [521400] global_step=521400, grad_norm=3.295273780822754, loss=1.086963176727295
I0305 09:41:25.305336 140380334978816 logging_writer.py:48] [521500] global_step=521500, grad_norm=2.9070305824279785, loss=1.0106513500213623
I0305 09:42:10.063817 140380326586112 logging_writer.py:48] [521600] global_step=521600, grad_norm=4.036867618560791, loss=3.162907361984253
I0305 09:42:54.877986 140380334978816 logging_writer.py:48] [521700] global_step=521700, grad_norm=3.4271438121795654, loss=1.101209282875061
I0305 09:43:40.107631 140380326586112 logging_writer.py:48] [521800] global_step=521800, grad_norm=3.0328900814056396, loss=1.1027350425720215
I0305 09:44:25.123975 140380334978816 logging_writer.py:48] [521900] global_step=521900, grad_norm=3.4056525230407715, loss=1.1304738521575928
I0305 09:45:10.029675 140380326586112 logging_writer.py:48] [522000] global_step=522000, grad_norm=4.02522087097168, loss=2.868532180786133
I0305 09:45:54.851784 140380334978816 logging_writer.py:48] [522100] global_step=522100, grad_norm=3.2511472702026367, loss=1.1875768899917603
I0305 09:46:40.122715 140380326586112 logging_writer.py:48] [522200] global_step=522200, grad_norm=3.223048210144043, loss=1.0481077432632446
I0305 09:46:53.728196 140575196817216 spec.py:321] Evaluating on the training split.
I0305 09:47:03.437636 140575196817216 spec.py:333] Evaluating on the validation split.
I0305 09:47:29.808830 140575196817216 spec.py:349] Evaluating on the test split.
I0305 09:47:31.475751 140575196817216 submission_runner.py:411] Time since start: 252827.08s, 	Step: 522232, 	{'train/accuracy': 0.8890429735183716, 'train/loss': 0.4175160527229309, 'validation/accuracy': 0.7831799983978271, 'validation/loss': 0.8526236414909363, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4626646041870117, 'test/num_examples': 10000, 'score': 232362.6910688877, 'total_duration': 252827.0764052868, 'accumulated_submission_time': 232362.6910688877, 'accumulated_eval_time': 20396.625748872757, 'accumulated_logging_time': 40.11666750907898}
I0305 09:47:31.590087 140380334978816 logging_writer.py:48] [522232] accumulated_eval_time=20396.625749, accumulated_logging_time=40.116668, accumulated_submission_time=232362.691069, global_step=522232, preemption_count=0, score=232362.691069, test/accuracy=0.661500, test/loss=1.462665, test/num_examples=10000, total_duration=252827.076405, train/accuracy=0.889043, train/loss=0.417516, validation/accuracy=0.783180, validation/loss=0.852624, validation/num_examples=50000
I0305 09:47:59.019732 140380326586112 logging_writer.py:48] [522300] global_step=522300, grad_norm=3.048407554626465, loss=1.0835556983947754
I0305 09:48:42.595200 140380334978816 logging_writer.py:48] [522400] global_step=522400, grad_norm=3.211350440979004, loss=1.0510540008544922
I0305 09:49:27.802996 140380326586112 logging_writer.py:48] [522500] global_step=522500, grad_norm=4.383608341217041, loss=2.1253676414489746
I0305 09:50:12.969296 140380334978816 logging_writer.py:48] [522600] global_step=522600, grad_norm=3.3288254737854004, loss=2.8051586151123047
I0305 09:50:49.372157 140380326586112 logging_writer.py:48] [522683] global_step=522683, preemption_count=0, score=232560.308783
I0305 09:50:50.001029 140575196817216 checkpoints.py:490] Saving checkpoint at step: 522683
I0305 09:50:51.394610 140575196817216 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax/trial_1/checkpoint_522683
I0305 09:50:51.420831 140575196817216 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_0/imagenet_vit_jax/trial_1/checkpoint_522683.
I0305 09:50:52.465858 140575196817216 submission_runner.py:676] Final imagenet_vit score: 232560.30878305435
