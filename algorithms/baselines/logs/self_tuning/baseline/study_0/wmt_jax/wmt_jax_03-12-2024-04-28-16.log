python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2966904671 --max_global_steps=399999 --tuning_ruleset=self 2>&1 | tee -a /logs/wmt_jax_03-12-2024-04-28-16.log
I0312 04:28:37.962763 139865354262336 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax because --overwrite was set.
I0312 04:28:37.968518 139865354262336 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax.
I0312 04:28:39.044510 139865354262336 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0312 04:28:39.045295 139865354262336 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0312 04:28:39.045440 139865354262336 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0312 04:28:39.888496 139865354262336 submission_runner.py:612] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax/trial_1.
I0312 04:28:40.091812 139865354262336 submission_runner.py:209] Initializing dataset.
I0312 04:28:40.103271 139865354262336 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:28:40.107744 139865354262336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:28:40.256313 139865354262336 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:28:42.256057 139865354262336 submission_runner.py:220] Initializing model.
I0312 04:28:51.672146 139865354262336 submission_runner.py:262] Initializing optimizer.
I0312 04:28:52.810925 139865354262336 submission_runner.py:269] Initializing metrics bundle.
I0312 04:28:52.811147 139865354262336 submission_runner.py:287] Initializing checkpoint and logger.
I0312 04:28:52.812024 139865354262336 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax/trial_1 with prefix checkpoint_
I0312 04:28:52.812181 139865354262336 submission_runner.py:307] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax/trial_1/meta_data_0.json.
I0312 04:28:52.812391 139865354262336 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0312 04:28:52.812451 139865354262336 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0312 04:28:53.157237 139865354262336 logger_utils.py:220] Unable to record git information. Continuing without it.
I0312 04:28:53.470195 139865354262336 submission_runner.py:311] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax/trial_1/flags_0.json.
I0312 04:28:53.480319 139865354262336 submission_runner.py:321] Starting training loop.
I0312 04:29:33.960096 139699851097856 logging_writer.py:48] [0] global_step=0, grad_norm=4.871642112731934, loss=11.08438777923584
I0312 04:29:33.976539 139865354262336 spec.py:321] Evaluating on the training split.
I0312 04:29:33.980101 139865354262336 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:29:33.982982 139865354262336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:29:34.021885 139865354262336 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:29:42.032772 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 04:34:38.566934 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 04:34:38.570562 139865354262336 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:34:38.574276 139865354262336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:34:38.611736 139865354262336 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:34:46.114366 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 04:39:33.281901 139865354262336 spec.py:349] Evaluating on the test split.
I0312 04:39:33.284811 139865354262336 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:39:33.288152 139865354262336 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:39:33.323978 139865354262336 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:39:36.154726 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 04:44:21.841741 139865354262336 submission_runner.py:420] Time since start: 928.36s, 	Step: 1, 	{'train/accuracy': 0.0006029167561791837, 'train/loss': 11.115453720092773, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.067522048950195, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.072319030761719, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 40.496176958084106, 'total_duration': 928.3613443374634, 'accumulated_submission_time': 40.496176958084106, 'accumulated_eval_time': 887.8651258945465, 'accumulated_logging_time': 0}
I0312 04:44:21.859144 139695672514304 logging_writer.py:48] [1] accumulated_eval_time=887.865126, accumulated_logging_time=0, accumulated_submission_time=40.496177, global_step=1, preemption_count=0, score=40.496177, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.072319, test/num_examples=3003, total_duration=928.361344, train/accuracy=0.000603, train/bleu=0.000000, train/loss=11.115454, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.067522, validation/num_examples=3000
I0312 04:44:57.557914 139695664121600 logging_writer.py:48] [100] global_step=100, grad_norm=0.17043174803256989, loss=8.234813690185547
I0312 04:45:33.272350 139695672514304 logging_writer.py:48] [200] global_step=200, grad_norm=0.36992356181144714, loss=7.4768218994140625
I0312 04:46:09.077057 139695664121600 logging_writer.py:48] [300] global_step=300, grad_norm=0.5909030437469482, loss=6.92075777053833
I0312 04:46:44.884439 139695672514304 logging_writer.py:48] [400] global_step=400, grad_norm=0.42627936601638794, loss=6.313277244567871
I0312 04:47:20.692869 139695664121600 logging_writer.py:48] [500] global_step=500, grad_norm=0.66011643409729, loss=5.908672332763672
I0312 04:47:56.496650 139695672514304 logging_writer.py:48] [600] global_step=600, grad_norm=0.6959498524665833, loss=5.575500011444092
I0312 04:48:32.289032 139695664121600 logging_writer.py:48] [700] global_step=700, grad_norm=0.47828713059425354, loss=5.307307243347168
I0312 04:49:08.101832 139695672514304 logging_writer.py:48] [800] global_step=800, grad_norm=0.49879249930381775, loss=5.0541300773620605
I0312 04:49:43.943347 139695664121600 logging_writer.py:48] [900] global_step=900, grad_norm=0.5246273875236511, loss=4.7763848304748535
I0312 04:50:19.739305 139695672514304 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6963558793067932, loss=4.593554496765137
I0312 04:50:55.532922 139695664121600 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5349801182746887, loss=4.3089599609375
I0312 04:51:31.314320 139695672514304 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6685380339622498, loss=3.9363350868225098
I0312 04:52:07.134482 139695664121600 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5409310460090637, loss=3.968926429748535
I0312 04:52:42.913515 139695672514304 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7600168585777283, loss=3.897111654281616
I0312 04:53:18.722407 139695664121600 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6630371809005737, loss=3.6431491374969482
I0312 04:53:54.505395 139695672514304 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.4342021644115448, loss=3.4055368900299072
I0312 04:54:30.318335 139695664121600 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6127119660377502, loss=3.4474399089813232
I0312 04:55:06.122331 139695672514304 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.4534154534339905, loss=3.259424924850464
I0312 04:55:41.919723 139695664121600 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.47587278485298157, loss=3.3005826473236084
I0312 04:56:17.734432 139695672514304 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.41190648078918457, loss=3.1808712482452393
I0312 04:56:53.524612 139695664121600 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.363496333360672, loss=3.0636343955993652
I0312 04:57:29.313941 139695672514304 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.38977906107902527, loss=3.145314931869507
I0312 04:58:05.099836 139695664121600 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4255653917789459, loss=2.967214584350586
I0312 04:58:21.998190 139865354262336 spec.py:321] Evaluating on the training split.
I0312 04:58:24.995889 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:01:19.941541 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 05:01:22.664957 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:04:03.571772 139865354262336 spec.py:349] Evaluating on the test split.
I0312 05:04:06.292960 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:06:30.831621 139865354262336 submission_runner.py:420] Time since start: 2257.35s, 	Step: 2349, 	{'train/accuracy': 0.5111194849014282, 'train/loss': 2.8539931774139404, 'train/bleu': 21.577162944601955, 'validation/accuracy': 0.5105206370353699, 'validation/loss': 2.8509256839752197, 'validation/bleu': 17.973378801733833, 'validation/num_examples': 3000, 'test/accuracy': 0.5071175694465637, 'test/loss': 2.903294801712036, 'test/bleu': 16.284499320835064, 'test/num_examples': 3003, 'score': 880.5481684207916, 'total_duration': 2257.3512320518494, 'accumulated_submission_time': 880.5481684207916, 'accumulated_eval_time': 1376.6985096931458, 'accumulated_logging_time': 0.027498245239257812}
I0312 05:06:30.845853 139695672514304 logging_writer.py:48] [2349] accumulated_eval_time=1376.698510, accumulated_logging_time=0.027498, accumulated_submission_time=880.548168, global_step=2349, preemption_count=0, score=880.548168, test/accuracy=0.507118, test/bleu=16.284499, test/loss=2.903295, test/num_examples=3003, total_duration=2257.351232, train/accuracy=0.511119, train/bleu=21.577163, train/loss=2.853993, validation/accuracy=0.510521, validation/bleu=17.973379, validation/loss=2.850926, validation/num_examples=3000
I0312 05:06:49.371735 139695664121600 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3411429226398468, loss=2.9230968952178955
I0312 05:07:25.017369 139695672514304 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.29442298412323, loss=2.879563093185425
I0312 05:08:00.804967 139695664121600 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.42155367136001587, loss=2.9021356105804443
I0312 05:08:36.567892 139695672514304 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.2885046899318695, loss=2.7706782817840576
I0312 05:09:12.307118 139695664121600 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.3387594521045685, loss=2.758845090866089
I0312 05:09:48.064452 139695672514304 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.24510934948921204, loss=2.769352436065674
I0312 05:10:23.821484 139695664121600 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.26784732937812805, loss=2.715029239654541
I0312 05:10:59.602179 139695672514304 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.4033052921295166, loss=2.7644715309143066
I0312 05:11:35.375506 139695664121600 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.24468639492988586, loss=2.6246726512908936
I0312 05:12:11.139607 139695672514304 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.25147777795791626, loss=2.5559966564178467
I0312 05:12:46.896196 139695664121600 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2764465808868408, loss=2.5973715782165527
I0312 05:13:22.667350 139695672514304 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.21489500999450684, loss=2.429964065551758
I0312 05:13:58.418919 139695664121600 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.20848454535007477, loss=2.540961265563965
I0312 05:14:34.178050 139695672514304 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.21754218637943268, loss=2.5053045749664307
I0312 05:15:09.989515 139695664121600 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.19472983479499817, loss=2.4640491008758545
I0312 05:15:45.763679 139695672514304 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.2439148873090744, loss=2.5004355907440186
I0312 05:16:21.578169 139695664121600 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.22071047127246857, loss=2.5593864917755127
I0312 05:16:57.342040 139695672514304 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.1993662416934967, loss=2.39536190032959
I0312 05:17:33.096471 139695664121600 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.1904498189687729, loss=2.3744289875030518
I0312 05:18:08.856343 139695672514304 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.17594532668590546, loss=2.3399336338043213
I0312 05:18:44.602994 139695664121600 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.19504214823246002, loss=2.3342206478118896
I0312 05:19:20.367326 139695672514304 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.1604040265083313, loss=2.305321455001831
I0312 05:19:56.165101 139695664121600 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.16710808873176575, loss=2.348890781402588
I0312 05:20:30.947241 139865354262336 spec.py:321] Evaluating on the training split.
I0312 05:20:33.948375 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:23:00.246126 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 05:23:02.968572 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:25:29.621799 139865354262336 spec.py:349] Evaluating on the test split.
I0312 05:25:32.323993 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:27:47.870515 139865354262336 submission_runner.py:420] Time since start: 3534.39s, 	Step: 4699, 	{'train/accuracy': 0.5772063732147217, 'train/loss': 2.269517183303833, 'train/bleu': 27.272062123291107, 'validation/accuracy': 0.5889449715614319, 'validation/loss': 2.158039093017578, 'validation/bleu': 23.188244077476853, 'validation/num_examples': 3000, 'test/accuracy': 0.5924118161201477, 'test/loss': 2.12496280670166, 'test/bleu': 22.337677996314792, 'test/num_examples': 3003, 'score': 1720.5627517700195, 'total_duration': 3534.390073299408, 'accumulated_submission_time': 1720.5627517700195, 'accumulated_eval_time': 1813.6216864585876, 'accumulated_logging_time': 0.05165576934814453}
I0312 05:27:47.888381 139695672514304 logging_writer.py:48] [4699] accumulated_eval_time=1813.621686, accumulated_logging_time=0.051656, accumulated_submission_time=1720.562752, global_step=4699, preemption_count=0, score=1720.562752, test/accuracy=0.592412, test/bleu=22.337678, test/loss=2.124963, test/num_examples=3003, total_duration=3534.390073, train/accuracy=0.577206, train/bleu=27.272062, train/loss=2.269517, validation/accuracy=0.588945, validation/bleu=23.188244, validation/loss=2.158039, validation/num_examples=3000
I0312 05:27:48.625465 139695664121600 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15740080177783966, loss=2.265355348587036
I0312 05:28:24.394067 139695672514304 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1544823944568634, loss=2.4066543579101562
I0312 05:29:00.177324 139695664121600 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.1490069031715393, loss=2.2723217010498047
I0312 05:29:35.942658 139695672514304 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1568017154932022, loss=2.257399320602417
I0312 05:30:11.693143 139695664121600 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.18558110296726227, loss=2.3523483276367188
I0312 05:30:47.484652 139695672514304 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.16942192614078522, loss=2.3561618328094482
I0312 05:31:23.233557 139695664121600 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.1749323606491089, loss=2.2296652793884277
I0312 05:31:58.984092 139695672514304 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1901700496673584, loss=2.320704221725464
I0312 05:32:34.754923 139695664121600 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.15424300730228424, loss=2.3607311248779297
I0312 05:33:10.527013 139695672514304 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.1501401960849762, loss=2.273099422454834
I0312 05:33:46.279108 139695664121600 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.15037481486797333, loss=2.299819231033325
I0312 05:34:22.061738 139695672514304 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.1766985058784485, loss=2.1871986389160156
I0312 05:34:57.802110 139695664121600 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.1450493037700653, loss=2.209622859954834
I0312 05:35:33.575397 139695672514304 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.16599982976913452, loss=2.1832900047302246
I0312 05:36:09.342146 139695664121600 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.14180774986743927, loss=2.178986072540283
I0312 05:36:45.132742 139695672514304 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.16075797379016876, loss=2.2165520191192627
I0312 05:37:20.915210 139695664121600 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.14836877584457397, loss=2.173936367034912
I0312 05:37:56.697740 139695672514304 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.1841973215341568, loss=2.2545974254608154
I0312 05:38:32.460632 139695664121600 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1459251493215561, loss=2.14982008934021
I0312 05:39:08.272097 139695672514304 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.18575716018676758, loss=2.2022287845611572
I0312 05:39:44.027410 139695664121600 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.17038677632808685, loss=2.1613941192626953
I0312 05:40:19.809255 139695672514304 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.14371521770954132, loss=2.1425018310546875
I0312 05:40:55.578177 139695664121600 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.14947766065597534, loss=2.1992485523223877
I0312 05:41:31.355208 139695672514304 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.17418430745601654, loss=2.1075329780578613
I0312 05:41:47.884819 139865354262336 spec.py:321] Evaluating on the training split.
I0312 05:41:50.874134 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:44:23.692640 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 05:44:26.392791 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:46:48.810580 139865354262336 spec.py:349] Evaluating on the test split.
I0312 05:46:51.508908 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 05:48:56.981456 139865354262336 submission_runner.py:420] Time since start: 4803.50s, 	Step: 7048, 	{'train/accuracy': 0.6080343127250671, 'train/loss': 1.9982719421386719, 'train/bleu': 29.0606718918695, 'validation/accuracy': 0.6168801188468933, 'validation/loss': 1.9311338663101196, 'validation/bleu': 25.284735116509545, 'validation/num_examples': 3000, 'test/accuracy': 0.6206844449043274, 'test/loss': 1.8931884765625, 'test/bleu': 23.973413980430863, 'test/num_examples': 3003, 'score': 2560.4681203365326, 'total_duration': 4803.5010669231415, 'accumulated_submission_time': 2560.4681203365326, 'accumulated_eval_time': 2242.718279838562, 'accumulated_logging_time': 0.08126235008239746}
I0312 05:48:56.996625 139695664121600 logging_writer.py:48] [7048] accumulated_eval_time=2242.718280, accumulated_logging_time=0.081262, accumulated_submission_time=2560.468120, global_step=7048, preemption_count=0, score=2560.468120, test/accuracy=0.620684, test/bleu=23.973414, test/loss=1.893188, test/num_examples=3003, total_duration=4803.501067, train/accuracy=0.608034, train/bleu=29.060672, train/loss=1.998272, validation/accuracy=0.616880, validation/bleu=25.284735, validation/loss=1.931134, validation/num_examples=3000
I0312 05:49:15.897284 139695672514304 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.16471362113952637, loss=2.1832242012023926
I0312 05:49:51.543217 139695664121600 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.18529833853244781, loss=2.140087366104126
I0312 05:50:27.293933 139695672514304 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.14453591406345367, loss=2.017866611480713
I0312 05:51:03.101642 139695664121600 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.18043728172779083, loss=2.124831438064575
I0312 05:51:38.873315 139695672514304 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.22244909405708313, loss=2.1043405532836914
I0312 05:52:14.633691 139695664121600 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.19439992308616638, loss=2.157912492752075
I0312 05:52:50.406656 139695672514304 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.17834682762622833, loss=2.1019952297210693
I0312 05:53:26.164143 139695664121600 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.15339328348636627, loss=2.10571026802063
I0312 05:54:01.949117 139695672514304 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.15744788944721222, loss=2.068631649017334
I0312 05:54:37.719227 139695664121600 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.15323801338672638, loss=2.0843517780303955
I0312 05:55:13.482074 139695672514304 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.18623566627502441, loss=2.1376757621765137
I0312 05:55:49.236788 139695664121600 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.16376075148582458, loss=2.0317158699035645
I0312 05:56:25.027207 139695672514304 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.16002504527568817, loss=2.0635619163513184
I0312 05:57:00.872992 139695664121600 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.16527633368968964, loss=2.082937717437744
I0312 05:57:36.671072 139695672514304 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1494690328836441, loss=2.080540180206299
I0312 05:58:12.469372 139695664121600 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.14317966997623444, loss=2.0254530906677246
I0312 05:58:48.281846 139695672514304 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.20701150596141815, loss=2.0968825817108154
I0312 05:59:24.150377 139695664121600 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.17728006839752197, loss=2.0550098419189453
I0312 05:59:59.966591 139695672514304 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.19367901980876923, loss=2.1178054809570312
I0312 06:00:35.825803 139695664121600 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1763347089290619, loss=2.0903406143188477
I0312 06:01:11.639793 139695672514304 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.16015800833702087, loss=2.0609934329986572
I0312 06:01:47.427978 139695664121600 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1625598669052124, loss=2.038304328918457
I0312 06:02:23.185451 139695672514304 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.17007745802402496, loss=1.929042100906372
I0312 06:02:57.281250 139865354262336 spec.py:321] Evaluating on the training split.
I0312 06:03:00.304570 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:05:44.438040 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 06:05:47.137137 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:08:17.595846 139865354262336 spec.py:349] Evaluating on the test split.
I0312 06:08:20.304482 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:10:31.183779 139865354262336 submission_runner.py:420] Time since start: 6097.70s, 	Step: 9397, 	{'train/accuracy': 0.6122574210166931, 'train/loss': 1.960111141204834, 'train/bleu': 29.22745206440884, 'validation/accuracy': 0.6298496127128601, 'validation/loss': 1.819329023361206, 'validation/bleu': 26.101145601908474, 'validation/num_examples': 3000, 'test/accuracy': 0.6367904543876648, 'test/loss': 1.7636173963546753, 'test/bleu': 24.995813239162366, 'test/num_examples': 3003, 'score': 3400.660268306732, 'total_duration': 6097.70339179039, 'accumulated_submission_time': 3400.660268306732, 'accumulated_eval_time': 2696.6207807064056, 'accumulated_logging_time': 0.10751605033874512}
I0312 06:10:31.201565 139695664121600 logging_writer.py:48] [9397] accumulated_eval_time=2696.620781, accumulated_logging_time=0.107516, accumulated_submission_time=3400.660268, global_step=9397, preemption_count=0, score=3400.660268, test/accuracy=0.636790, test/bleu=24.995813, test/loss=1.763617, test/num_examples=3003, total_duration=6097.703392, train/accuracy=0.612257, train/bleu=29.227452, train/loss=1.960111, validation/accuracy=0.629850, validation/bleu=26.101146, validation/loss=1.819329, validation/num_examples=3000
I0312 06:10:32.643267 139695672514304 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.19805504381656647, loss=2.031440258026123
I0312 06:11:08.251073 139695664121600 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.17961980402469635, loss=2.0048890113830566
I0312 06:11:43.912394 139695672514304 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.16123846173286438, loss=2.037738800048828
I0312 06:12:19.648208 139695664121600 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.19447052478790283, loss=2.028428554534912
I0312 06:12:55.470501 139695672514304 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1686672568321228, loss=1.9566278457641602
I0312 06:13:31.248192 139695664121600 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.20978358387947083, loss=2.007793664932251
I0312 06:14:07.010029 139695672514304 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.17582198977470398, loss=1.9939574003219604
I0312 06:14:42.761053 139695664121600 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.21804006397724152, loss=2.0820729732513428
I0312 06:15:18.549427 139695672514304 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.18269643187522888, loss=1.9818192720413208
I0312 06:15:54.281450 139695664121600 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.16426117718219757, loss=1.960930347442627
I0312 06:16:30.053157 139695672514304 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.2489401251077652, loss=1.9358224868774414
I0312 06:17:05.803875 139695664121600 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.20749962329864502, loss=2.057103157043457
I0312 06:17:41.569387 139695672514304 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.18555103242397308, loss=1.9922699928283691
I0312 06:18:17.330531 139695664121600 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.15982747077941895, loss=2.117063522338867
I0312 06:18:53.092789 139695672514304 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.20668785274028778, loss=2.001070499420166
I0312 06:19:28.903383 139695664121600 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.18552863597869873, loss=2.0240726470947266
I0312 06:20:04.719169 139695672514304 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.19181033968925476, loss=2.042955160140991
I0312 06:20:40.485346 139695664121600 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.17766527831554413, loss=1.91989004611969
I0312 06:21:16.264816 139695672514304 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.1614508181810379, loss=1.9395724534988403
I0312 06:21:52.056340 139695664121600 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.15589101612567902, loss=1.9374074935913086
I0312 06:22:27.811133 139695672514304 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.17148344218730927, loss=2.019599676132202
I0312 06:23:03.568983 139695664121600 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17373476922512054, loss=1.9553807973861694
I0312 06:23:39.358627 139695672514304 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1816641390323639, loss=1.8544632196426392
I0312 06:24:15.099190 139695664121600 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.16273167729377747, loss=1.99393630027771
I0312 06:24:31.286977 139865354262336 spec.py:321] Evaluating on the training split.
I0312 06:24:34.295822 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:27:38.544470 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 06:27:41.242815 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:30:09.300502 139865354262336 spec.py:349] Evaluating on the test split.
I0312 06:30:12.002219 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:32:24.879996 139865354262336 submission_runner.py:420] Time since start: 7411.40s, 	Step: 11747, 	{'train/accuracy': 0.6196650862693787, 'train/loss': 1.8994940519332886, 'train/bleu': 30.07848536027601, 'validation/accuracy': 0.6404136419296265, 'validation/loss': 1.751357078552246, 'validation/bleu': 26.887197005780408, 'validation/num_examples': 3000, 'test/accuracy': 0.6459822654724121, 'test/loss': 1.6933776140213013, 'test/bleu': 25.88734468011442, 'test/num_examples': 3003, 'score': 4240.656886100769, 'total_duration': 7411.399604558945, 'accumulated_submission_time': 4240.656886100769, 'accumulated_eval_time': 3170.2137649059296, 'accumulated_logging_time': 0.1367018222808838}
I0312 06:32:24.896306 139695672514304 logging_writer.py:48] [11747] accumulated_eval_time=3170.213765, accumulated_logging_time=0.136702, accumulated_submission_time=4240.656886, global_step=11747, preemption_count=0, score=4240.656886, test/accuracy=0.645982, test/bleu=25.887345, test/loss=1.693378, test/num_examples=3003, total_duration=7411.399605, train/accuracy=0.619665, train/bleu=30.078485, train/loss=1.899494, validation/accuracy=0.640414, validation/bleu=26.887197, validation/loss=1.751357, validation/num_examples=3000
I0312 06:32:44.136563 139695664121600 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.1480439156293869, loss=1.8864234685897827
I0312 06:33:19.772045 139695672514304 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.1622457504272461, loss=1.9991520643234253
I0312 06:33:55.534438 139695664121600 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.17561647295951843, loss=1.959334135055542
I0312 06:34:31.285978 139695672514304 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.21891668438911438, loss=1.9469435214996338
I0312 06:35:07.038616 139695664121600 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.16385483741760254, loss=1.9101804494857788
I0312 06:35:42.800009 139695672514304 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.16918206214904785, loss=2.0044174194335938
I0312 06:36:18.586470 139695664121600 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2899770438671112, loss=1.8688949346542358
I0312 06:36:54.314235 139695672514304 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.22756046056747437, loss=2.031266689300537
I0312 06:37:30.076947 139695664121600 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.18872827291488647, loss=2.047785758972168
I0312 06:38:05.834521 139695672514304 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.19833610951900482, loss=1.9786827564239502
I0312 06:38:41.587598 139695664121600 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1806563138961792, loss=1.9824931621551514
I0312 06:39:17.376052 139695672514304 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.17373067140579224, loss=1.857130765914917
I0312 06:39:53.197621 139695664121600 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.23935359716415405, loss=2.0532538890838623
I0312 06:40:28.975721 139695672514304 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.17989155650138855, loss=1.8967739343643188
I0312 06:41:04.744027 139695664121600 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2301594465970993, loss=1.995629906654358
I0312 06:41:40.505638 139695672514304 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.18974286317825317, loss=1.8898191452026367
I0312 06:42:16.286948 139695664121600 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.24650605022907257, loss=1.9106059074401855
I0312 06:42:52.051288 139695672514304 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.22891275584697723, loss=1.995219111442566
I0312 06:43:27.825340 139695664121600 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.17367023229599, loss=1.949766755104065
I0312 06:44:03.608899 139695672514304 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.16375002264976501, loss=1.9618223905563354
I0312 06:44:39.360004 139695664121600 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2143728882074356, loss=1.9830818176269531
I0312 06:45:15.146302 139695672514304 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.24465623497962952, loss=1.973658800125122
I0312 06:45:50.910642 139695664121600 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.29416608810424805, loss=1.869612693786621
I0312 06:46:24.981813 139865354262336 spec.py:321] Evaluating on the training split.
I0312 06:46:27.984835 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:49:48.056501 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 06:49:50.766750 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:52:26.391240 139865354262336 spec.py:349] Evaluating on the test split.
I0312 06:52:29.120308 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 06:54:42.360625 139865354262336 submission_runner.py:420] Time since start: 8748.88s, 	Step: 14097, 	{'train/accuracy': 0.6269037127494812, 'train/loss': 1.8341394662857056, 'train/bleu': 30.409302554729337, 'validation/accuracy': 0.643438994884491, 'validation/loss': 1.716156244277954, 'validation/bleu': 26.79804227860407, 'validation/num_examples': 3000, 'test/accuracy': 0.6518040895462036, 'test/loss': 1.6483497619628906, 'test/bleu': 25.83268204250817, 'test/num_examples': 3003, 'score': 5080.653621196747, 'total_duration': 8748.880180835724, 'accumulated_submission_time': 5080.653621196747, 'accumulated_eval_time': 3667.5925085544586, 'accumulated_logging_time': 0.16363000869750977}
I0312 06:54:42.380182 139695672514304 logging_writer.py:48] [14097] accumulated_eval_time=3667.592509, accumulated_logging_time=0.163630, accumulated_submission_time=5080.653621, global_step=14097, preemption_count=0, score=5080.653621, test/accuracy=0.651804, test/bleu=25.832682, test/loss=1.648350, test/num_examples=3003, total_duration=8748.880181, train/accuracy=0.626904, train/bleu=30.409303, train/loss=1.834139, validation/accuracy=0.643439, validation/bleu=26.798042, validation/loss=1.716156, validation/num_examples=3000
I0312 06:54:43.826020 139695664121600 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.16061221063137054, loss=1.9467700719833374
I0312 06:55:19.469433 139695672514304 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.16164113581180573, loss=1.9380452632904053
I0312 06:55:55.193377 139695664121600 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.19776852428913116, loss=1.9748286008834839
I0312 06:56:30.972717 139695672514304 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17313389480113983, loss=1.9278843402862549
I0312 06:57:06.729267 139695664121600 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2145881950855255, loss=1.8904156684875488
I0312 06:57:42.618354 139695672514304 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2036939263343811, loss=1.923174262046814
I0312 06:58:18.388184 139695664121600 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.29333940148353577, loss=1.9284639358520508
I0312 06:58:54.138758 139695672514304 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2320069819688797, loss=1.9363943338394165
I0312 06:59:29.893347 139695664121600 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.1782795637845993, loss=1.9248367547988892
I0312 07:00:05.671474 139695672514304 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.19668182730674744, loss=1.9475135803222656
I0312 07:00:41.408955 139695664121600 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.24704842269420624, loss=1.8959072828292847
I0312 07:01:17.169612 139695672514304 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.21007142961025238, loss=1.937064528465271
I0312 07:01:52.956862 139695664121600 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.1960797756910324, loss=1.9127767086029053
I0312 07:02:28.716769 139695672514304 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.17946238815784454, loss=1.8989267349243164
I0312 07:03:04.473526 139695664121600 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.1969505101442337, loss=1.960344672203064
I0312 07:03:40.281054 139695672514304 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.18641585111618042, loss=1.905961036682129
I0312 07:04:16.182612 139695664121600 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.21307037770748138, loss=1.8508344888687134
I0312 07:04:51.952335 139695672514304 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.23641468584537506, loss=1.8186776638031006
I0312 07:05:27.705124 139695664121600 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.17955367267131805, loss=1.9648510217666626
I0312 07:06:03.456191 139695672514304 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.20558913052082062, loss=1.8637787103652954
I0312 07:06:39.223436 139695664121600 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2132655680179596, loss=1.9061115980148315
I0312 07:07:15.051344 139695672514304 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.19898411631584167, loss=1.8578009605407715
I0312 07:07:50.809013 139695664121600 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1842779666185379, loss=1.8984959125518799
I0312 07:08:26.587177 139695672514304 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2365082949399948, loss=1.844626784324646
I0312 07:08:42.404514 139865354262336 spec.py:321] Evaluating on the training split.
I0312 07:08:45.407725 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:11:54.291027 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 07:11:57.006321 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:14:28.816801 139865354262336 spec.py:349] Evaluating on the test split.
I0312 07:14:31.521375 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:16:50.763608 139865354262336 submission_runner.py:420] Time since start: 10077.28s, 	Step: 16446, 	{'train/accuracy': 0.6281946897506714, 'train/loss': 1.814017653465271, 'train/bleu': 30.516623148587534, 'validation/accuracy': 0.646265983581543, 'validation/loss': 1.6841596364974976, 'validation/bleu': 27.324046560476773, 'validation/num_examples': 3000, 'test/accuracy': 0.6574283838272095, 'test/loss': 1.6131547689437866, 'test/bleu': 26.619005671904056, 'test/num_examples': 3003, 'score': 5920.588991165161, 'total_duration': 10077.283208847046, 'accumulated_submission_time': 5920.588991165161, 'accumulated_eval_time': 4155.951548576355, 'accumulated_logging_time': 0.19422245025634766}
I0312 07:16:50.780613 139695664121600 logging_writer.py:48] [16446] accumulated_eval_time=4155.951549, accumulated_logging_time=0.194222, accumulated_submission_time=5920.588991, global_step=16446, preemption_count=0, score=5920.588991, test/accuracy=0.657428, test/bleu=26.619006, test/loss=1.613155, test/num_examples=3003, total_duration=10077.283209, train/accuracy=0.628195, train/bleu=30.516623, train/loss=1.814018, validation/accuracy=0.646266, validation/bleu=27.324047, validation/loss=1.684160, validation/num_examples=3000
I0312 07:17:10.376214 139695672514304 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.22793862223625183, loss=1.865143060684204
I0312 07:17:46.047248 139695664121600 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.18043677508831024, loss=1.9096660614013672
I0312 07:18:21.893535 139695672514304 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.24433349072933197, loss=1.8775408267974854
I0312 07:18:57.709286 139695664121600 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.17758823931217194, loss=1.8646900653839111
I0312 07:19:33.526419 139695672514304 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.19969911873340607, loss=1.930481195449829
I0312 07:20:09.313338 139695664121600 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.1890566498041153, loss=2.0133213996887207
I0312 07:20:45.104136 139695672514304 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.21855895221233368, loss=1.961111307144165
I0312 07:21:20.906217 139695664121600 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.1777234673500061, loss=1.7851271629333496
I0312 07:21:56.697663 139695672514304 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.16327989101409912, loss=1.896131992340088
I0312 07:22:32.445906 139695664121600 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.27873286604881287, loss=1.8965989351272583
I0312 07:23:08.216528 139695672514304 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.1882130205631256, loss=1.8607323169708252
I0312 07:23:43.997494 139695664121600 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.17571814358234406, loss=1.8945465087890625
I0312 07:24:19.788853 139695672514304 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1743723750114441, loss=1.880692958831787
I0312 07:24:55.572929 139695664121600 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.19506308436393738, loss=1.848789095878601
I0312 07:25:31.352836 139695672514304 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.17837943136692047, loss=1.8251153230667114
I0312 07:26:07.105152 139695664121600 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.17717178165912628, loss=1.8641386032104492
I0312 07:26:42.918670 139695672514304 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.2237643152475357, loss=1.896500587463379
I0312 07:27:18.732500 139695664121600 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1841098666191101, loss=1.929325819015503
I0312 07:27:54.555914 139695672514304 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.16663947701454163, loss=1.8761519193649292
I0312 07:28:30.341483 139695664121600 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.20740951597690582, loss=1.8601738214492798
I0312 07:29:06.137376 139695672514304 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.22039571404457092, loss=1.8905178308486938
I0312 07:29:41.909031 139695664121600 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8634312748908997, loss=1.8431035280227661
I0312 07:30:17.664436 139695672514304 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19499796628952026, loss=1.777878761291504
I0312 07:30:51.029354 139865354262336 spec.py:321] Evaluating on the training split.
I0312 07:30:54.034479 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:33:50.355689 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 07:33:53.057221 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:36:22.926527 139865354262336 spec.py:349] Evaluating on the test split.
I0312 07:36:25.620468 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:38:33.769449 139865354262336 submission_runner.py:420] Time since start: 11380.29s, 	Step: 18795, 	{'train/accuracy': 0.6745564937591553, 'train/loss': 1.5200400352478027, 'train/bleu': 33.8394973668327, 'validation/accuracy': 0.6503701210021973, 'validation/loss': 1.6605991125106812, 'validation/bleu': 27.392234079788906, 'validation/num_examples': 3000, 'test/accuracy': 0.6583115458488464, 'test/loss': 1.6000570058822632, 'test/bleu': 26.442234509686312, 'test/num_examples': 3003, 'score': 6760.74355173111, 'total_duration': 11380.289041280746, 'accumulated_submission_time': 6760.74355173111, 'accumulated_eval_time': 4618.6915826797485, 'accumulated_logging_time': 0.2228555679321289}
I0312 07:38:33.786094 139695664121600 logging_writer.py:48] [18795] accumulated_eval_time=4618.691583, accumulated_logging_time=0.222856, accumulated_submission_time=6760.743552, global_step=18795, preemption_count=0, score=6760.743552, test/accuracy=0.658312, test/bleu=26.442235, test/loss=1.600057, test/num_examples=3003, total_duration=11380.289041, train/accuracy=0.674556, train/bleu=33.839497, train/loss=1.520040, validation/accuracy=0.650370, validation/bleu=27.392234, validation/loss=1.660599, validation/num_examples=3000
I0312 07:38:35.946202 139695672514304 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.18092907965183258, loss=1.8733636140823364
I0312 07:39:11.582781 139695664121600 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.19170480966567993, loss=1.8333548307418823
I0312 07:39:47.318736 139695672514304 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.18074648082256317, loss=1.8092085123062134
I0312 07:40:23.085665 139695664121600 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.22174525260925293, loss=1.901010513305664
I0312 07:40:58.855000 139695672514304 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.19994348287582397, loss=1.8035436868667603
I0312 07:41:34.603088 139695664121600 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.19372351467609406, loss=1.8859257698059082
I0312 07:42:10.357651 139695672514304 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.21309645473957062, loss=1.877094030380249
I0312 07:42:46.178621 139695664121600 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.18402116000652313, loss=1.816399097442627
I0312 07:43:22.077609 139695672514304 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17038200795650482, loss=1.9099719524383545
I0312 07:43:57.824543 139695664121600 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.18946586549282074, loss=1.898727297782898
I0312 07:44:33.595129 139695672514304 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17971128225326538, loss=1.8839553594589233
I0312 07:45:09.379240 139695664121600 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.21105888485908508, loss=1.9193899631500244
I0312 07:45:45.165376 139695672514304 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.23456205427646637, loss=1.855476975440979
I0312 07:46:20.949748 139695664121600 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.17622703313827515, loss=1.8500968217849731
I0312 07:46:56.751187 139695672514304 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.23322221636772156, loss=1.8654248714447021
I0312 07:47:32.537818 139695664121600 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.16595610976219177, loss=1.8830207586288452
I0312 07:48:08.316945 139695672514304 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.17252108454704285, loss=1.8775044679641724
I0312 07:48:44.084863 139695664121600 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.18745112419128418, loss=1.8672751188278198
I0312 07:49:19.892477 139695672514304 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.19532884657382965, loss=1.8665701150894165
I0312 07:49:55.678421 139695664121600 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1749662309885025, loss=1.8928260803222656
I0312 07:50:31.453816 139695672514304 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.18868578970432281, loss=1.8760241270065308
I0312 07:51:07.236620 139695664121600 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.17598427832126617, loss=1.7890092134475708
I0312 07:51:43.018781 139695672514304 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.2529749274253845, loss=1.8724867105484009
I0312 07:52:18.798588 139695664121600 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.26180994510650635, loss=1.8684858083724976
I0312 07:52:33.914440 139865354262336 spec.py:321] Evaluating on the training split.
I0312 07:52:36.905682 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:56:06.216575 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 07:56:08.922700 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 07:58:44.762214 139865354262336 spec.py:349] Evaluating on the test split.
I0312 07:58:47.459286 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:01:17.651026 139865354262336 submission_runner.py:420] Time since start: 12744.17s, 	Step: 21144, 	{'train/accuracy': 0.6381985545158386, 'train/loss': 1.7518174648284912, 'train/bleu': 31.213722577422697, 'validation/accuracy': 0.6526267528533936, 'validation/loss': 1.6427363157272339, 'validation/bleu': 27.370917701953303, 'validation/num_examples': 3000, 'test/accuracy': 0.6626808643341064, 'test/loss': 1.5769139528274536, 'test/bleu': 26.793200258777787, 'test/num_examples': 3003, 'score': 7600.783447980881, 'total_duration': 12744.170624256134, 'accumulated_submission_time': 7600.783447980881, 'accumulated_eval_time': 5142.428127288818, 'accumulated_logging_time': 0.2506749629974365}
I0312 08:01:17.668883 139695672514304 logging_writer.py:48] [21144] accumulated_eval_time=5142.428127, accumulated_logging_time=0.250675, accumulated_submission_time=7600.783448, global_step=21144, preemption_count=0, score=7600.783448, test/accuracy=0.662681, test/bleu=26.793200, test/loss=1.576914, test/num_examples=3003, total_duration=12744.170624, train/accuracy=0.638199, train/bleu=31.213723, train/loss=1.751817, validation/accuracy=0.652627, validation/bleu=27.370918, validation/loss=1.642736, validation/num_examples=3000
I0312 08:01:37.953706 139695664121600 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.1951855570077896, loss=1.8752849102020264
I0312 08:02:13.625504 139695672514304 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.35341277718544006, loss=1.8890033960342407
I0312 08:02:49.401607 139695664121600 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.19325672090053558, loss=1.8605709075927734
I0312 08:03:25.224141 139695672514304 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.22952061891555786, loss=1.8472951650619507
I0312 08:04:01.138318 139695664121600 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.2056974172592163, loss=1.8987610340118408
I0312 08:04:36.953154 139695672514304 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.24703426659107208, loss=1.844159483909607
I0312 08:05:12.741111 139695664121600 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.18869490921497345, loss=1.9132070541381836
I0312 08:05:48.495644 139695672514304 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.1953061819076538, loss=1.907850742340088
I0312 08:06:24.278012 139695664121600 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.21687616407871246, loss=1.8936086893081665
I0312 08:07:00.060102 139695672514304 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.19803158938884735, loss=1.9019606113433838
I0312 08:07:35.870559 139695664121600 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.19381676614284515, loss=1.7918697595596313
I0312 08:08:11.625616 139695672514304 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18415863811969757, loss=1.7930359840393066
I0312 08:08:47.390203 139695664121600 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.20650343596935272, loss=1.9374195337295532
I0312 08:09:23.183330 139695672514304 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.1861589550971985, loss=1.875288963317871
I0312 08:09:59.052350 139695664121600 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.21567390859127045, loss=1.7764194011688232
I0312 08:10:34.866667 139695672514304 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.21448417007923126, loss=1.7885065078735352
I0312 08:11:10.626257 139695664121600 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.20429806411266327, loss=1.8401408195495605
I0312 08:11:46.439834 139695672514304 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.174866184592247, loss=1.7868455648422241
I0312 08:12:22.197694 139695664121600 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.23925785720348358, loss=1.8838152885437012
I0312 08:12:57.958623 139695672514304 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.1937991827726364, loss=1.7641748189926147
I0312 08:13:33.772414 139695664121600 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.186422199010849, loss=1.8831958770751953
I0312 08:14:09.572441 139695672514304 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.1712317019701004, loss=1.838928461074829
I0312 08:14:45.337090 139695664121600 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.28336408734321594, loss=1.8143435716629028
I0312 08:15:17.958385 139865354262336 spec.py:321] Evaluating on the training split.
I0312 08:15:20.972966 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:18:55.956645 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 08:18:58.669263 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:21:22.798103 139865354262336 spec.py:349] Evaluating on the test split.
I0312 08:21:25.499261 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:23:38.347980 139865354262336 submission_runner.py:420] Time since start: 14084.87s, 	Step: 23493, 	{'train/accuracy': 0.6324992775917053, 'train/loss': 1.7952932119369507, 'train/bleu': 30.772397688224117, 'validation/accuracy': 0.6544618010520935, 'validation/loss': 1.6285449266433716, 'validation/bleu': 27.839781666594913, 'validation/num_examples': 3000, 'test/accuracy': 0.6649003624916077, 'test/loss': 1.561911702156067, 'test/bleu': 27.014724990420394, 'test/num_examples': 3003, 'score': 8440.980343818665, 'total_duration': 14084.867583990097, 'accumulated_submission_time': 8440.980343818665, 'accumulated_eval_time': 5642.817688941956, 'accumulated_logging_time': 0.2786710262298584}
I0312 08:23:38.369259 139695672514304 logging_writer.py:48] [23493] accumulated_eval_time=5642.817689, accumulated_logging_time=0.278671, accumulated_submission_time=8440.980344, global_step=23493, preemption_count=0, score=8440.980344, test/accuracy=0.664900, test/bleu=27.014725, test/loss=1.561912, test/num_examples=3003, total_duration=14084.867584, train/accuracy=0.632499, train/bleu=30.772398, train/loss=1.795293, validation/accuracy=0.654462, validation/bleu=27.839782, validation/loss=1.628545, validation/num_examples=3000
I0312 08:23:41.246338 139695664121600 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3206503689289093, loss=1.8147852420806885
I0312 08:24:16.860543 139695672514304 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.19968430697917938, loss=1.8880112171173096
I0312 08:24:52.588286 139695664121600 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.18486341834068298, loss=1.8068621158599854
I0312 08:25:28.367013 139695672514304 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.19626300036907196, loss=1.849787712097168
I0312 08:26:04.107710 139695664121600 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.18664075434207916, loss=1.7845314741134644
I0312 08:26:39.911123 139695672514304 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.21334300935268402, loss=1.86941397190094
I0312 08:27:15.669500 139695664121600 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.22530139982700348, loss=1.920632243156433
I0312 08:27:51.439604 139695672514304 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.1787298023700714, loss=1.7844452857971191
I0312 08:28:27.203260 139695664121600 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2552655339241028, loss=1.8751996755599976
I0312 08:29:02.981404 139695672514304 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2861422002315521, loss=1.8053077459335327
I0312 08:29:38.759956 139695664121600 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.2190386801958084, loss=1.7719871997833252
I0312 08:30:14.527671 139695672514304 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.18808028101921082, loss=1.912114143371582
I0312 08:30:50.329760 139695664121600 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.16993467509746552, loss=1.8029484748840332
I0312 08:31:26.110198 139695672514304 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.18460917472839355, loss=1.8123520612716675
I0312 08:32:01.890836 139695664121600 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3784755766391754, loss=1.8664159774780273
I0312 08:32:37.649518 139695672514304 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2471436709165573, loss=1.8562103509902954
I0312 08:33:13.414659 139695664121600 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.35090866684913635, loss=1.863013744354248
I0312 08:33:49.186090 139695672514304 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1915525645017624, loss=1.8667309284210205
I0312 08:34:24.962395 139695664121600 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.21586011350154877, loss=1.8530147075653076
I0312 08:35:00.761639 139695672514304 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.19552946090698242, loss=1.819366455078125
I0312 08:35:36.531465 139695664121600 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.19385264813899994, loss=1.7996604442596436
I0312 08:36:12.328021 139695672514304 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.21658459305763245, loss=1.8664183616638184
I0312 08:36:48.106959 139695664121600 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.18862071633338928, loss=1.7908602952957153
I0312 08:37:23.871767 139695672514304 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.21868452429771423, loss=1.8554648160934448
I0312 08:37:38.603013 139865354262336 spec.py:321] Evaluating on the training split.
I0312 08:37:41.592472 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:40:53.460357 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 08:40:56.153006 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:43:26.668007 139865354262336 spec.py:349] Evaluating on the test split.
I0312 08:43:29.383528 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 08:45:57.101708 139865354262336 submission_runner.py:420] Time since start: 15423.62s, 	Step: 25843, 	{'train/accuracy': 0.646230161190033, 'train/loss': 1.6848127841949463, 'train/bleu': 31.48282830995302, 'validation/accuracy': 0.6560364961624146, 'validation/loss': 1.6232653856277466, 'validation/bleu': 27.66872427113727, 'validation/num_examples': 3000, 'test/accuracy': 0.6663180589675903, 'test/loss': 1.5464180707931519, 'test/bleu': 27.176116060602716, 'test/num_examples': 3003, 'score': 9281.125701904297, 'total_duration': 15423.621300458908, 'accumulated_submission_time': 9281.125701904297, 'accumulated_eval_time': 6141.316317081451, 'accumulated_logging_time': 0.3110058307647705}
I0312 08:45:57.119150 139695664121600 logging_writer.py:48] [25843] accumulated_eval_time=6141.316317, accumulated_logging_time=0.311006, accumulated_submission_time=9281.125702, global_step=25843, preemption_count=0, score=9281.125702, test/accuracy=0.666318, test/bleu=27.176116, test/loss=1.546418, test/num_examples=3003, total_duration=15423.621300, train/accuracy=0.646230, train/bleu=31.482828, train/loss=1.684813, validation/accuracy=0.656036, validation/bleu=27.668724, validation/loss=1.623265, validation/num_examples=3000
I0312 08:46:17.824235 139695672514304 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.21927186846733093, loss=1.8087224960327148
I0312 08:46:53.486487 139695664121600 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.20411242544651031, loss=1.7902947664260864
I0312 08:47:29.252141 139695672514304 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.19112011790275574, loss=1.8643265962600708
I0312 08:48:05.013112 139695664121600 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.20991699397563934, loss=1.7963703870773315
I0312 08:48:40.796768 139695672514304 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.22073937952518463, loss=1.7964611053466797
I0312 08:49:16.554813 139695664121600 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.20511174201965332, loss=1.8357340097427368
I0312 08:49:52.317210 139695672514304 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.18311749398708344, loss=1.8569444417953491
I0312 08:50:28.080345 139695664121600 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2022087126970291, loss=1.8436919450759888
I0312 08:51:03.865556 139695672514304 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2041788548231125, loss=1.7995290756225586
I0312 08:51:39.632323 139695664121600 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.20056754350662231, loss=1.943741798400879
I0312 08:52:15.402144 139695672514304 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.19639557600021362, loss=1.8215981721878052
I0312 08:52:51.193637 139695664121600 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.24158604443073273, loss=1.8541263341903687
I0312 08:53:26.982316 139695672514304 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.23102089762687683, loss=1.8020046949386597
I0312 08:54:02.791919 139695664121600 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.21098309755325317, loss=1.8454835414886475
I0312 08:54:38.548380 139695672514304 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.20983870327472687, loss=1.880642056465149
I0312 08:55:14.328188 139695664121600 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2162211388349533, loss=1.7691937685012817
I0312 08:55:50.089964 139695672514304 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.1912308931350708, loss=1.8458620309829712
I0312 08:56:25.888386 139695664121600 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.24413248896598816, loss=1.7868632078170776
I0312 08:57:01.741613 139695672514304 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.2128189504146576, loss=1.7855433225631714
I0312 08:57:37.517400 139695664121600 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.24883334338665009, loss=1.899423360824585
I0312 08:58:13.287277 139695672514304 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.1994793713092804, loss=1.8359023332595825
I0312 08:58:49.045990 139695664121600 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.21672020852565765, loss=1.8705921173095703
I0312 08:59:24.836564 139695672514304 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.19016845524311066, loss=1.8305861949920654
I0312 08:59:57.110761 139865354262336 spec.py:321] Evaluating on the training split.
I0312 09:00:00.099680 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:02:55.979684 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 09:02:58.681442 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:05:34.051525 139865354262336 spec.py:349] Evaluating on the test split.
I0312 09:05:36.760381 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:08:21.861268 139865354262336 submission_runner.py:420] Time since start: 16768.38s, 	Step: 28192, 	{'train/accuracy': 0.64000004529953, 'train/loss': 1.739559531211853, 'train/bleu': 31.34536810411951, 'validation/accuracy': 0.6591982841491699, 'validation/loss': 1.6082539558410645, 'validation/bleu': 28.11320010201369, 'validation/num_examples': 3000, 'test/accuracy': 0.6693858504295349, 'test/loss': 1.5327434539794922, 'test/bleu': 27.316366215689314, 'test/num_examples': 3003, 'score': 10121.028459310532, 'total_duration': 16768.38085126877, 'accumulated_submission_time': 10121.028459310532, 'accumulated_eval_time': 6646.066769123077, 'accumulated_logging_time': 0.33818817138671875}
I0312 09:08:21.880259 139695664121600 logging_writer.py:48] [28192] accumulated_eval_time=6646.066769, accumulated_logging_time=0.338188, accumulated_submission_time=10121.028459, global_step=28192, preemption_count=0, score=10121.028459, test/accuracy=0.669386, test/bleu=27.316366, test/loss=1.532743, test/num_examples=3003, total_duration=16768.380851, train/accuracy=0.640000, train/bleu=31.345368, train/loss=1.739560, validation/accuracy=0.659198, validation/bleu=28.113200, validation/loss=1.608254, validation/num_examples=3000
I0312 09:08:25.099543 139695672514304 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.3087230622768402, loss=1.8304553031921387
I0312 09:09:00.738249 139695664121600 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2072620540857315, loss=1.7599629163742065
I0312 09:09:36.459757 139695672514304 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.18200308084487915, loss=1.7501821517944336
I0312 09:10:12.250635 139695664121600 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.19593289494514465, loss=1.8209511041641235
I0312 09:10:48.001454 139695672514304 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18803830444812775, loss=1.8510279655456543
I0312 09:11:23.760797 139695664121600 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.23194670677185059, loss=1.8627471923828125
I0312 09:11:59.542543 139695672514304 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.19870419800281525, loss=1.8651496171951294
I0312 09:12:35.284478 139695664121600 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.18153876066207886, loss=1.8445069789886475
I0312 09:13:11.063596 139695672514304 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.18073025345802307, loss=1.823934555053711
I0312 09:13:46.811475 139695664121600 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.20055504143238068, loss=1.823104739189148
I0312 09:14:22.570480 139695672514304 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.1846410483121872, loss=1.8652691841125488
I0312 09:14:58.337504 139695664121600 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.25004926323890686, loss=1.8847570419311523
I0312 09:15:34.074863 139695672514304 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.19725020229816437, loss=1.820021390914917
I0312 09:16:09.834872 139695664121600 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2260037362575531, loss=1.850820779800415
I0312 09:16:45.602128 139695672514304 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.1910233348608017, loss=1.7481273412704468
I0312 09:17:21.355491 139695664121600 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.17898918688297272, loss=1.782466173171997
I0312 09:17:57.114550 139695672514304 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.22489617764949799, loss=1.861273169517517
I0312 09:18:32.873037 139695664121600 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.24982024729251862, loss=1.7258846759796143
I0312 09:19:08.634221 139695672514304 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.19909650087356567, loss=1.7637114524841309
I0312 09:19:44.385762 139695664121600 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.25472018122673035, loss=1.7697306871414185
I0312 09:20:20.151144 139695672514304 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.2424660623073578, loss=1.8841415643692017
I0312 09:20:55.905234 139695664121600 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.18051087856292725, loss=1.7478890419006348
I0312 09:21:31.687060 139695672514304 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.2361731380224228, loss=1.8113088607788086
I0312 09:22:07.452348 139695664121600 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1866680234670639, loss=1.7307686805725098
I0312 09:22:22.201967 139865354262336 spec.py:321] Evaluating on the training split.
I0312 09:22:25.202513 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:25:39.523223 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 09:25:42.210574 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:28:10.191711 139865354262336 spec.py:349] Evaluating on the test split.
I0312 09:28:12.884939 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:30:23.374692 139865354262336 submission_runner.py:420] Time since start: 18089.89s, 	Step: 30543, 	{'train/accuracy': 0.6375179886817932, 'train/loss': 1.7588555812835693, 'train/bleu': 30.77511208561493, 'validation/accuracy': 0.658925473690033, 'validation/loss': 1.6004260778427124, 'validation/bleu': 28.296488039179028, 'validation/num_examples': 3000, 'test/accuracy': 0.6699784994125366, 'test/loss': 1.522635579109192, 'test/bleu': 27.527019800715944, 'test/num_examples': 3003, 'score': 10961.262724161148, 'total_duration': 18089.89426422119, 'accumulated_submission_time': 10961.262724161148, 'accumulated_eval_time': 7127.2394053936005, 'accumulated_logging_time': 0.36673569679260254}
I0312 09:30:23.396870 139695672514304 logging_writer.py:48] [30543] accumulated_eval_time=7127.239405, accumulated_logging_time=0.366736, accumulated_submission_time=10961.262724, global_step=30543, preemption_count=0, score=10961.262724, test/accuracy=0.669978, test/bleu=27.527020, test/loss=1.522636, test/num_examples=3003, total_duration=18089.894264, train/accuracy=0.637518, train/bleu=30.775112, train/loss=1.758856, validation/accuracy=0.658925, validation/bleu=28.296488, validation/loss=1.600426, validation/num_examples=3000
I0312 09:30:44.057134 139695664121600 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.1764518767595291, loss=1.8050806522369385
I0312 09:31:19.707007 139695672514304 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1807638555765152, loss=1.7383311986923218
I0312 09:31:55.441986 139695664121600 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.19392994046211243, loss=1.7362676858901978
I0312 09:32:31.200408 139695672514304 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.1937849074602127, loss=1.7593107223510742
I0312 09:33:06.971877 139695664121600 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.38965991139411926, loss=1.7946865558624268
I0312 09:33:42.746706 139695672514304 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.23149077594280243, loss=1.8065766096115112
I0312 09:34:18.506677 139695664121600 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.19218604266643524, loss=1.798277497291565
I0312 09:34:54.281177 139695672514304 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.18369443714618683, loss=1.8922938108444214
I0312 09:35:30.036398 139695664121600 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20953334867954254, loss=1.8639253377914429
I0312 09:36:05.798058 139695672514304 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.1919405162334442, loss=1.8387742042541504
I0312 09:36:41.573371 139695664121600 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1877673864364624, loss=1.7722117900848389
I0312 09:37:17.347354 139695672514304 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.18688277900218964, loss=1.805991768836975
I0312 09:37:53.104757 139695664121600 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.17560724914073944, loss=1.7904075384140015
I0312 09:38:28.860242 139695672514304 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.19669382274150848, loss=1.8684135675430298
I0312 09:39:04.633147 139695664121600 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.205722376704216, loss=1.791797399520874
I0312 09:39:40.429013 139695672514304 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2699829041957855, loss=1.7560653686523438
I0312 09:40:16.205632 139695664121600 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.22350028157234192, loss=1.8727117776870728
I0312 09:40:51.967385 139695672514304 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.19782374799251556, loss=1.7692303657531738
I0312 09:41:27.722885 139695664121600 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.1971307098865509, loss=1.8766318559646606
I0312 09:42:03.498861 139695672514304 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.2547634243965149, loss=1.7189600467681885
I0312 09:42:39.254431 139695664121600 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.20460158586502075, loss=1.7384452819824219
I0312 09:43:15.058202 139695672514304 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.2526751756668091, loss=1.7547675371170044
I0312 09:43:50.854496 139695664121600 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.25694572925567627, loss=1.8915451765060425
I0312 09:44:23.465902 139865354262336 spec.py:321] Evaluating on the training split.
I0312 09:44:26.461221 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:47:41.834409 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 09:47:44.537236 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:50:10.971995 139865354262336 spec.py:349] Evaluating on the test split.
I0312 09:50:13.688353 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 09:52:31.109352 139865354262336 submission_runner.py:420] Time since start: 19417.63s, 	Step: 32893, 	{'train/accuracy': 0.6432289481163025, 'train/loss': 1.7125086784362793, 'train/bleu': 31.19315213816847, 'validation/accuracy': 0.6582435369491577, 'validation/loss': 1.6062827110290527, 'validation/bleu': 28.121775837299218, 'validation/num_examples': 3000, 'test/accuracy': 0.6708035469055176, 'test/loss': 1.5205928087234497, 'test/bleu': 27.49746230167418, 'test/num_examples': 3003, 'score': 11801.242480754852, 'total_duration': 19417.62895989418, 'accumulated_submission_time': 11801.242480754852, 'accumulated_eval_time': 7614.882809877396, 'accumulated_logging_time': 0.3996870517730713}
I0312 09:52:31.128400 139695672514304 logging_writer.py:48] [32893] accumulated_eval_time=7614.882810, accumulated_logging_time=0.399687, accumulated_submission_time=11801.242481, global_step=32893, preemption_count=0, score=11801.242481, test/accuracy=0.670804, test/bleu=27.497462, test/loss=1.520593, test/num_examples=3003, total_duration=19417.628960, train/accuracy=0.643229, train/bleu=31.193152, train/loss=1.712509, validation/accuracy=0.658244, validation/bleu=28.121776, validation/loss=1.606283, validation/num_examples=3000
I0312 09:52:33.993429 139695664121600 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.0959768295288086, loss=1.825982928276062
I0312 09:53:09.602161 139695672514304 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.21821828186511993, loss=1.8719854354858398
I0312 09:53:45.317820 139695664121600 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.19309855997562408, loss=1.7196147441864014
I0312 09:54:21.044872 139695672514304 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.23363853991031647, loss=1.7555313110351562
I0312 09:54:56.795977 139695664121600 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.22384949028491974, loss=1.7190780639648438
I0312 09:55:32.544868 139695672514304 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2063714861869812, loss=1.8668593168258667
I0312 09:56:08.301705 139695664121600 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.19342240691184998, loss=1.8362078666687012
I0312 09:56:44.069241 139695672514304 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2076544463634491, loss=1.719513177871704
I0312 09:57:19.810199 139695664121600 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.20447437465190887, loss=1.7953801155090332
I0312 09:57:55.559631 139695672514304 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.23213444650173187, loss=1.826193928718567
I0312 09:58:31.357241 139695664121600 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.20638076961040497, loss=1.7673211097717285
I0312 09:59:07.106336 139695672514304 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1957920491695404, loss=1.8207610845565796
I0312 09:59:42.859125 139695664121600 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.2156422734260559, loss=1.8015060424804688
I0312 10:00:18.627725 139695672514304 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.24815766513347626, loss=1.822622299194336
I0312 10:00:54.366611 139695664121600 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.20946787297725677, loss=1.71187424659729
I0312 10:01:30.121953 139695672514304 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.19300426542758942, loss=1.7963491678237915
I0312 10:02:05.868359 139695664121600 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.2339683324098587, loss=1.7827937602996826
I0312 10:02:41.630479 139695672514304 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.23798848688602448, loss=1.7426937818527222
I0312 10:03:17.391140 139695664121600 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.18525303900241852, loss=1.712643027305603
I0312 10:03:53.158726 139695672514304 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.27210602164268494, loss=1.7356420755386353
I0312 10:04:28.941441 139695664121600 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.2121802717447281, loss=1.7203354835510254
I0312 10:05:04.704011 139695672514304 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.20827928185462952, loss=1.827530026435852
I0312 10:05:40.460088 139695664121600 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1949533373117447, loss=1.806686282157898
I0312 10:06:16.253955 139695672514304 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.200834721326828, loss=1.7001440525054932
I0312 10:06:31.352406 139865354262336 spec.py:321] Evaluating on the training split.
I0312 10:06:34.348993 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:10:35.082041 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 10:10:37.777196 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:13:03.553952 139865354262336 spec.py:349] Evaluating on the test split.
I0312 10:13:06.255239 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:15:32.483144 139865354262336 submission_runner.py:420] Time since start: 20799.00s, 	Step: 35244, 	{'train/accuracy': 0.6422494649887085, 'train/loss': 1.720298171043396, 'train/bleu': 31.382447419480027, 'validation/accuracy': 0.6626824140548706, 'validation/loss': 1.5820107460021973, 'validation/bleu': 28.378079676535123, 'validation/num_examples': 3000, 'test/accuracy': 0.6722328662872314, 'test/loss': 1.5019036531448364, 'test/bleu': 27.84060668588333, 'test/num_examples': 3003, 'score': 12641.374759197235, 'total_duration': 20799.002705574036, 'accumulated_submission_time': 12641.374759197235, 'accumulated_eval_time': 8156.013454914093, 'accumulated_logging_time': 0.4326503276824951}
I0312 10:15:32.506380 139695664121600 logging_writer.py:48] [35244] accumulated_eval_time=8156.013455, accumulated_logging_time=0.432650, accumulated_submission_time=12641.374759, global_step=35244, preemption_count=0, score=12641.374759, test/accuracy=0.672233, test/bleu=27.840607, test/loss=1.501904, test/num_examples=3003, total_duration=20799.002706, train/accuracy=0.642249, train/bleu=31.382447, train/loss=1.720298, validation/accuracy=0.662682, validation/bleu=28.378080, validation/loss=1.582011, validation/num_examples=3000
I0312 10:15:52.821991 139695672514304 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.20736315846443176, loss=1.7923628091812134
I0312 10:16:28.468386 139695664121600 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.1828659176826477, loss=1.8062124252319336
I0312 10:17:04.205340 139695672514304 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.20202364027500153, loss=1.8112713098526
I0312 10:17:39.947487 139695664121600 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.23043398559093475, loss=1.6748770475387573
I0312 10:18:15.716529 139695672514304 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2687399387359619, loss=1.77131986618042
I0312 10:18:51.453248 139695664121600 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.19249974191188812, loss=1.739879846572876
I0312 10:19:27.262042 139695672514304 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.21185407042503357, loss=1.7756000757217407
I0312 10:20:03.010075 139695664121600 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.29716405272483826, loss=1.851191759109497
I0312 10:20:38.750487 139695672514304 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.20154696702957153, loss=1.822805404663086
I0312 10:21:14.497421 139695664121600 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.18474453687667847, loss=1.7612669467926025
I0312 10:21:50.349427 139695672514304 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.191279336810112, loss=1.7401927709579468
I0312 10:22:26.138632 139695664121600 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.19595515727996826, loss=1.7452857494354248
I0312 10:23:01.898009 139695672514304 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.24531716108322144, loss=1.7946208715438843
I0312 10:23:37.670804 139695664121600 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.21678364276885986, loss=1.7599568367004395
I0312 10:24:13.428779 139695672514304 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.20625847578048706, loss=1.7790124416351318
I0312 10:24:49.179613 139695664121600 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.20415455102920532, loss=1.7904345989227295
I0312 10:25:24.995707 139695672514304 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.20233936607837677, loss=1.7973893880844116
I0312 10:26:00.783994 139695664121600 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.21121759712696075, loss=1.8114428520202637
I0312 10:26:36.552671 139695672514304 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.23131775856018066, loss=1.7629801034927368
I0312 10:27:12.325677 139695664121600 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.1925259828567505, loss=1.813475489616394
I0312 10:27:48.191233 139695672514304 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.25890740752220154, loss=1.8987376689910889
I0312 10:28:24.050564 139695664121600 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.22814124822616577, loss=1.8456553220748901
I0312 10:28:59.862255 139695672514304 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.23708760738372803, loss=1.8045233488082886
I0312 10:29:32.521512 139865354262336 spec.py:321] Evaluating on the training split.
I0312 10:29:35.512863 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:33:21.336255 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 10:33:24.034937 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:35:56.053395 139865354262336 spec.py:349] Evaluating on the test split.
I0312 10:35:58.748157 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:38:17.892031 139865354262336 submission_runner.py:420] Time since start: 22164.41s, 	Step: 37593, 	{'train/accuracy': 0.6800824999809265, 'train/loss': 1.45755136013031, 'train/bleu': 34.355612800032446, 'validation/accuracy': 0.6637487411499023, 'validation/loss': 1.5757863521575928, 'validation/bleu': 28.429244621089666, 'validation/num_examples': 3000, 'test/accuracy': 0.6723839640617371, 'test/loss': 1.4996920824050903, 'test/bleu': 27.599065186221257, 'test/num_examples': 3003, 'score': 13481.296279668808, 'total_duration': 22164.41160917282, 'accumulated_submission_time': 13481.296279668808, 'accumulated_eval_time': 8681.38389492035, 'accumulated_logging_time': 0.46824193000793457}
I0312 10:38:17.916134 139695664121600 logging_writer.py:48] [37593] accumulated_eval_time=8681.383895, accumulated_logging_time=0.468242, accumulated_submission_time=13481.296280, global_step=37593, preemption_count=0, score=13481.296280, test/accuracy=0.672384, test/bleu=27.599065, test/loss=1.499692, test/num_examples=3003, total_duration=22164.411609, train/accuracy=0.680082, train/bleu=34.355613, train/loss=1.457551, validation/accuracy=0.663749, validation/bleu=28.429245, validation/loss=1.575786, validation/num_examples=3000
I0312 10:38:20.801050 139695672514304 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.183427631855011, loss=1.7775821685791016
I0312 10:38:56.420408 139695664121600 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.19928064942359924, loss=1.771092176437378
I0312 10:39:32.155181 139695672514304 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.19520050287246704, loss=1.772279977798462
I0312 10:40:07.907911 139695664121600 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.20257581770420074, loss=1.8122056722640991
I0312 10:40:43.670349 139695672514304 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.19527216255664825, loss=1.7224926948547363
I0312 10:41:19.425131 139695664121600 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.1964515894651413, loss=1.7314491271972656
I0312 10:41:55.178547 139695672514304 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.18897631764411926, loss=1.7897181510925293
I0312 10:42:30.952654 139695664121600 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.18036764860153198, loss=1.781557559967041
I0312 10:43:06.717849 139695672514304 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.20491871237754822, loss=1.8232697248458862
I0312 10:43:42.486423 139695664121600 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22273671627044678, loss=1.7395813465118408
I0312 10:44:18.257530 139695672514304 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.20985743403434753, loss=1.7847328186035156
I0312 10:44:54.038101 139695664121600 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.22615835070610046, loss=1.7219158411026
I0312 10:45:29.822984 139695672514304 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.19372138381004333, loss=1.7195955514907837
I0312 10:46:05.621851 139695664121600 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.21353501081466675, loss=1.7651786804199219
I0312 10:46:41.409295 139695672514304 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.18484649062156677, loss=1.7849764823913574
I0312 10:47:17.201226 139695664121600 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.19801905751228333, loss=1.7702898979187012
I0312 10:47:52.967937 139695672514304 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.2212519496679306, loss=1.7878985404968262
I0312 10:48:28.742213 139695664121600 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.22155891358852386, loss=1.8224477767944336
I0312 10:49:04.515691 139695672514304 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.3847580552101135, loss=1.8305535316467285
I0312 10:49:40.300383 139695664121600 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2887432873249054, loss=1.786379098892212
I0312 10:50:16.071230 139695672514304 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.19410185515880585, loss=1.7547520399093628
I0312 10:50:51.868144 139695664121600 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.20769467949867249, loss=1.7544132471084595
I0312 10:51:27.686019 139695672514304 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1933256834745407, loss=1.8112266063690186
I0312 10:52:03.462838 139695664121600 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.1938360184431076, loss=1.670116901397705
I0312 10:52:18.203843 139865354262336 spec.py:321] Evaluating on the training split.
I0312 10:52:21.199266 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:56:12.506167 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 10:56:15.199814 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 10:59:25.424438 139865354262336 spec.py:349] Evaluating on the test split.
I0312 10:59:28.116991 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:02:34.488037 139865354262336 submission_runner.py:420] Time since start: 23621.01s, 	Step: 39943, 	{'train/accuracy': 0.6489623785018921, 'train/loss': 1.68193519115448, 'train/bleu': 32.00845543488021, 'validation/accuracy': 0.6647778749465942, 'validation/loss': 1.5651044845581055, 'validation/bleu': 28.787674762573683, 'validation/num_examples': 3000, 'test/accuracy': 0.6742083430290222, 'test/loss': 1.4953006505966187, 'test/bleu': 27.961575161697663, 'test/num_examples': 3003, 'score': 14321.491159915924, 'total_duration': 23621.00763463974, 'accumulated_submission_time': 14321.491159915924, 'accumulated_eval_time': 9297.668025970459, 'accumulated_logging_time': 0.5044949054718018}
I0312 11:02:34.507552 139695672514304 logging_writer.py:48] [39943] accumulated_eval_time=9297.668026, accumulated_logging_time=0.504495, accumulated_submission_time=14321.491160, global_step=39943, preemption_count=0, score=14321.491160, test/accuracy=0.674208, test/bleu=27.961575, test/loss=1.495301, test/num_examples=3003, total_duration=23621.007635, train/accuracy=0.648962, train/bleu=32.008455, train/loss=1.681935, validation/accuracy=0.664778, validation/bleu=28.787675, validation/loss=1.565104, validation/num_examples=3000
I0312 11:02:55.156240 139695664121600 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.1896468847990036, loss=1.6851292848587036
I0312 11:03:30.830598 139695672514304 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.19530348479747772, loss=1.737235426902771
I0312 11:04:06.626976 139695664121600 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.21488799154758453, loss=1.8857650756835938
I0312 11:04:42.424325 139695672514304 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2308466136455536, loss=1.8405616283416748
I0312 11:05:18.184393 139695664121600 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.18849784135818481, loss=1.7481436729431152
I0312 11:05:53.925219 139695672514304 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.19099745154380798, loss=1.8109614849090576
I0312 11:06:29.668931 139695664121600 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.25849300622940063, loss=1.6291900873184204
I0312 11:07:05.382576 139695672514304 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.1981852501630783, loss=1.7818361520767212
I0312 11:07:41.131643 139695664121600 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.17887479066848755, loss=1.7574330568313599
I0312 11:08:16.895645 139695672514304 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.1985672116279602, loss=1.783553957939148
I0312 11:08:52.669881 139695664121600 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.1943875551223755, loss=1.7622627019882202
I0312 11:09:28.424704 139695672514304 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.22889894247055054, loss=1.8268414735794067
I0312 11:10:04.176429 139695664121600 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.2272341400384903, loss=1.7365353107452393
I0312 11:10:39.957930 139695672514304 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.1892198920249939, loss=1.7673492431640625
I0312 11:11:15.722021 139695664121600 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.18495890498161316, loss=1.8038625717163086
I0312 11:11:51.476952 139695672514304 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1860332190990448, loss=1.690839171409607
I0312 11:12:27.236921 139695664121600 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.18979273736476898, loss=1.81206476688385
I0312 11:13:02.991990 139695672514304 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.23123526573181152, loss=1.7692619562149048
I0312 11:13:38.756924 139695664121600 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.19760733842849731, loss=1.7506146430969238
I0312 11:14:14.543161 139695672514304 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.19501946866512299, loss=1.7578781843185425
I0312 11:14:50.315460 139695664121600 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.25702574849128723, loss=1.7543584108352661
I0312 11:15:26.101008 139695672514304 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.22852230072021484, loss=1.8035646677017212
I0312 11:16:01.915488 139695664121600 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.25237226486206055, loss=1.8072113990783691
I0312 11:16:34.630920 139865354262336 spec.py:321] Evaluating on the training split.
I0312 11:16:37.641406 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:20:50.217317 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 11:20:52.934833 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:23:38.481452 139865354262336 spec.py:349] Evaluating on the test split.
I0312 11:23:41.180186 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:26:04.329573 139865354262336 submission_runner.py:420] Time since start: 25030.85s, 	Step: 42293, 	{'train/accuracy': 0.6470581293106079, 'train/loss': 1.6912137269973755, 'train/bleu': 31.773503395043683, 'validation/accuracy': 0.6645422577857971, 'validation/loss': 1.5649478435516357, 'validation/bleu': 28.337935195464578, 'validation/num_examples': 3000, 'test/accuracy': 0.6760095357894897, 'test/loss': 1.48641037940979, 'test/bleu': 27.912674541280314, 'test/num_examples': 3003, 'score': 15161.52368092537, 'total_duration': 25030.849177837372, 'accumulated_submission_time': 15161.52368092537, 'accumulated_eval_time': 9867.36664390564, 'accumulated_logging_time': 0.5345263481140137}
I0312 11:26:04.351234 139695672514304 logging_writer.py:48] [42293] accumulated_eval_time=9867.366644, accumulated_logging_time=0.534526, accumulated_submission_time=15161.523681, global_step=42293, preemption_count=0, score=15161.523681, test/accuracy=0.676010, test/bleu=27.912675, test/loss=1.486410, test/num_examples=3003, total_duration=25030.849178, train/accuracy=0.647058, train/bleu=31.773503, train/loss=1.691214, validation/accuracy=0.664542, validation/bleu=28.337935, validation/loss=1.564948, validation/num_examples=3000
I0312 11:26:07.208762 139695664121600 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.2041671872138977, loss=1.7159024477005005
I0312 11:26:42.797273 139695672514304 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.1778474748134613, loss=1.8147245645523071
I0312 11:27:18.523921 139695664121600 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.20302186906337738, loss=1.7479256391525269
I0312 11:27:54.258056 139695672514304 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.20020140707492828, loss=1.7101938724517822
I0312 11:28:30.056006 139695664121600 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.18950825929641724, loss=1.7072447538375854
I0312 11:29:05.846649 139695672514304 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.2061237245798111, loss=1.7414015531539917
I0312 11:29:41.589419 139695664121600 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.19406481087207794, loss=1.843703031539917
I0312 11:30:17.335750 139695672514304 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.21461772918701172, loss=1.809829831123352
I0312 11:30:53.105266 139695664121600 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.21069858968257904, loss=1.7556266784667969
I0312 11:31:28.876321 139695672514304 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.19332784414291382, loss=1.705715537071228
I0312 11:32:04.617227 139695664121600 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.1943080723285675, loss=1.739916443824768
I0312 11:32:40.376610 139695672514304 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.20086994767189026, loss=1.7891769409179688
I0312 11:33:16.143317 139695664121600 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.188981831073761, loss=1.7199784517288208
I0312 11:33:51.881492 139695672514304 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.20153258740901947, loss=1.7237138748168945
I0312 11:34:27.633390 139695664121600 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.2063588798046112, loss=1.760736346244812
I0312 11:35:03.389628 139695672514304 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.19168227910995483, loss=1.6992641687393188
I0312 11:35:39.170949 139695664121600 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2786961793899536, loss=1.7772947549819946
I0312 11:36:14.966373 139695672514304 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.20306538045406342, loss=1.77040696144104
I0312 11:36:50.697550 139695664121600 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.22240489721298218, loss=1.7640551328659058
I0312 11:37:26.449399 139695672514304 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1775207221508026, loss=1.7274795770645142
I0312 11:38:02.209685 139695664121600 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.18593035638332367, loss=1.8084795475006104
I0312 11:38:37.975386 139695672514304 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.21118566393852234, loss=1.7179044485092163
I0312 11:39:13.732397 139695664121600 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.19865024089813232, loss=1.8607312440872192
I0312 11:39:49.477067 139695672514304 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.19625228643417358, loss=1.7096385955810547
I0312 11:40:04.565093 139865354262336 spec.py:321] Evaluating on the training split.
I0312 11:40:07.556512 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:43:11.875261 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 11:43:14.568174 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:45:38.566435 139865354262336 spec.py:349] Evaluating on the test split.
I0312 11:45:41.264104 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 11:47:52.880877 139865354262336 submission_runner.py:420] Time since start: 26339.40s, 	Step: 44644, 	{'train/accuracy': 0.6550261974334717, 'train/loss': 1.621602177619934, 'train/bleu': 32.423378180989296, 'validation/accuracy': 0.6661665439605713, 'validation/loss': 1.5491173267364502, 'validation/bleu': 28.65202752908857, 'validation/num_examples': 3000, 'test/accuracy': 0.6754517555236816, 'test/loss': 1.4745254516601562, 'test/bleu': 27.637049690288837, 'test/num_examples': 3003, 'score': 16001.64799952507, 'total_duration': 26339.4004714489, 'accumulated_submission_time': 16001.64799952507, 'accumulated_eval_time': 10335.682363271713, 'accumulated_logging_time': 0.5661778450012207}
I0312 11:47:52.901763 139695664121600 logging_writer.py:48] [44644] accumulated_eval_time=10335.682363, accumulated_logging_time=0.566178, accumulated_submission_time=16001.648000, global_step=44644, preemption_count=0, score=16001.648000, test/accuracy=0.675452, test/bleu=27.637050, test/loss=1.474525, test/num_examples=3003, total_duration=26339.400471, train/accuracy=0.655026, train/bleu=32.423378, train/loss=1.621602, validation/accuracy=0.666167, validation/bleu=28.652028, validation/loss=1.549117, validation/num_examples=3000
I0312 11:48:13.219701 139695672514304 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.22143873572349548, loss=1.7343835830688477
I0312 11:48:48.892381 139695664121600 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.19916699826717377, loss=1.7020405530929565
I0312 11:49:24.642685 139695672514304 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2811599671840668, loss=1.8279985189437866
I0312 11:50:00.413515 139695664121600 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.1871105432510376, loss=1.7860100269317627
I0312 11:50:36.156930 139695672514304 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.218760684132576, loss=1.7645838260650635
I0312 11:51:11.927141 139695664121600 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2047441303730011, loss=1.7329859733581543
I0312 11:51:47.662339 139695672514304 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2338012307882309, loss=1.831336498260498
I0312 11:52:23.431901 139695664121600 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.21866437792778015, loss=1.7864201068878174
I0312 11:52:59.202523 139695672514304 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2023027241230011, loss=1.818346619606018
I0312 11:53:34.973957 139695664121600 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.4083509147167206, loss=1.7562596797943115
I0312 11:54:10.719374 139695672514304 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.20511136949062347, loss=1.7189067602157593
I0312 11:54:46.528070 139695664121600 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.200246661901474, loss=1.6868515014648438
I0312 11:55:22.369328 139695672514304 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.22148539125919342, loss=1.841120958328247
I0312 11:55:58.165299 139695664121600 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.21211354434490204, loss=1.8260098695755005
I0312 11:56:33.964915 139695672514304 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.21681103110313416, loss=1.7221927642822266
I0312 11:57:09.821690 139695664121600 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2069321721792221, loss=1.783645749092102
I0312 11:57:45.630730 139695672514304 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2110661417245865, loss=1.762204647064209
I0312 11:58:21.450343 139695664121600 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.2434428483247757, loss=1.7714943885803223
I0312 11:58:57.253760 139695672514304 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.19466908276081085, loss=1.7943848371505737
I0312 11:59:33.062711 139695664121600 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.24222524464130402, loss=1.7690223455429077
I0312 12:00:08.852127 139695672514304 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.20205649733543396, loss=1.8057235479354858
I0312 12:00:44.646251 139695664121600 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.19841182231903076, loss=1.7285150289535522
I0312 12:01:20.415412 139695672514304 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.19318793714046478, loss=1.7317783832550049
I0312 12:01:53.063624 139865354262336 spec.py:321] Evaluating on the training split.
I0312 12:01:56.055800 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:05:48.637719 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 12:05:51.335223 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:08:51.683439 139865354262336 spec.py:349] Evaluating on the test split.
I0312 12:08:54.380228 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:12:04.254914 139865354262336 submission_runner.py:420] Time since start: 27790.77s, 	Step: 46993, 	{'train/accuracy': 0.6464254856109619, 'train/loss': 1.6788039207458496, 'train/bleu': 31.810203588692378, 'validation/accuracy': 0.666340172290802, 'validation/loss': 1.5431653261184692, 'validation/bleu': 28.392401589122812, 'validation/num_examples': 3000, 'test/accuracy': 0.6792981624603271, 'test/loss': 1.4608837366104126, 'test/bleu': 28.031139069077838, 'test/num_examples': 3003, 'score': 16841.717359304428, 'total_duration': 27790.774476528168, 'accumulated_submission_time': 16841.717359304428, 'accumulated_eval_time': 10946.873561382294, 'accumulated_logging_time': 0.5983190536499023}
I0312 12:12:04.279194 139695664121600 logging_writer.py:48] [46993] accumulated_eval_time=10946.873561, accumulated_logging_time=0.598319, accumulated_submission_time=16841.717359, global_step=46993, preemption_count=0, score=16841.717359, test/accuracy=0.679298, test/bleu=28.031139, test/loss=1.460884, test/num_examples=3003, total_duration=27790.774477, train/accuracy=0.646425, train/bleu=31.810204, train/loss=1.678804, validation/accuracy=0.666340, validation/bleu=28.392402, validation/loss=1.543165, validation/num_examples=3000
I0312 12:12:07.156830 139695672514304 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.20174798369407654, loss=1.6922013759613037
I0312 12:12:42.713865 139695664121600 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.18627135455608368, loss=1.7106249332427979
I0312 12:13:18.402595 139695672514304 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2135293185710907, loss=1.7151979207992554
I0312 12:13:54.186627 139695664121600 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.1796201914548874, loss=1.659868597984314
I0312 12:14:29.906495 139695672514304 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20206329226493835, loss=1.7897425889968872
I0312 12:15:05.668820 139695664121600 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.185654416680336, loss=1.6612659692764282
I0312 12:15:41.425225 139695672514304 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.470856249332428, loss=1.7481130361557007
I0312 12:16:17.168389 139695664121600 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19392377138137817, loss=1.7432020902633667
I0312 12:16:52.935025 139695672514304 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.18614912033081055, loss=1.734089732170105
I0312 12:17:28.700696 139695664121600 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1916181594133377, loss=1.75090491771698
I0312 12:18:04.444281 139695672514304 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.19850902259349823, loss=1.7404364347457886
I0312 12:18:40.197664 139695664121600 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.20973718166351318, loss=1.7820720672607422
I0312 12:19:15.950033 139695672514304 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.2064710259437561, loss=1.73725426197052
I0312 12:19:51.680897 139695664121600 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.19873452186584473, loss=1.739917516708374
I0312 12:20:27.444629 139695672514304 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.19092872738838196, loss=1.7323882579803467
I0312 12:21:03.206983 139695664121600 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.20056423544883728, loss=1.7292664051055908
I0312 12:21:38.982467 139695672514304 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.26124492287635803, loss=1.7691365480422974
I0312 12:22:14.761538 139695664121600 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.20607611536979675, loss=1.8181620836257935
I0312 12:22:50.537380 139695672514304 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.20588557422161102, loss=1.6683099269866943
I0312 12:23:26.319628 139695664121600 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.27383023500442505, loss=1.7354960441589355
I0312 12:24:02.122028 139695672514304 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.244035005569458, loss=1.7667123079299927
I0312 12:24:37.891570 139695664121600 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.1910879909992218, loss=1.7439405918121338
I0312 12:25:13.668413 139695672514304 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.1911926120519638, loss=1.7169184684753418
I0312 12:25:49.431307 139695664121600 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.2227708399295807, loss=1.8093451261520386
I0312 12:26:04.521915 139865354262336 spec.py:321] Evaluating on the training split.
I0312 12:26:07.511931 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:29:53.356718 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 12:29:56.054626 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:32:37.897567 139865354262336 spec.py:349] Evaluating on the test split.
I0312 12:32:40.604333 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:35:15.710440 139865354262336 submission_runner.py:420] Time since start: 29182.23s, 	Step: 49344, 	{'train/accuracy': 0.6485030055046082, 'train/loss': 1.6835576295852661, 'train/bleu': 31.90822207781761, 'validation/accuracy': 0.6678528189659119, 'validation/loss': 1.540746808052063, 'validation/bleu': 28.77088171329392, 'validation/num_examples': 3000, 'test/accuracy': 0.6801580786705017, 'test/loss': 1.4569159746170044, 'test/bleu': 28.28024055566884, 'test/num_examples': 3003, 'score': 17681.87114572525, 'total_duration': 29182.23004412651, 'accumulated_submission_time': 17681.87114572525, 'accumulated_eval_time': 11498.062032461166, 'accumulated_logging_time': 0.6335971355438232}
I0312 12:35:15.731554 139695672514304 logging_writer.py:48] [49344] accumulated_eval_time=11498.062032, accumulated_logging_time=0.633597, accumulated_submission_time=17681.871146, global_step=49344, preemption_count=0, score=17681.871146, test/accuracy=0.680158, test/bleu=28.280241, test/loss=1.456916, test/num_examples=3003, total_duration=29182.230044, train/accuracy=0.648503, train/bleu=31.908222, train/loss=1.683558, validation/accuracy=0.667853, validation/bleu=28.770882, validation/loss=1.540747, validation/num_examples=3000
I0312 12:35:36.031515 139695664121600 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1975661963224411, loss=1.6994935274124146
I0312 12:36:11.697160 139695672514304 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.24594366550445557, loss=1.7127909660339355
I0312 12:36:47.421368 139695664121600 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.22099272906780243, loss=1.6989054679870605
I0312 12:37:23.197665 139695672514304 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.19162076711654663, loss=1.630941390991211
I0312 12:37:58.928281 139695664121600 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.23295709490776062, loss=1.7884299755096436
I0312 12:38:34.688047 139695672514304 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.23360446095466614, loss=1.779668927192688
I0312 12:39:10.446913 139695664121600 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.18651102483272552, loss=1.7317014932632446
I0312 12:39:46.211641 139695672514304 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.19917133450508118, loss=1.6807810068130493
I0312 12:40:21.967738 139695664121600 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.19880840182304382, loss=1.8098700046539307
I0312 12:40:57.736792 139695672514304 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.18912848830223083, loss=1.6925458908081055
I0312 12:41:33.518930 139695664121600 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.200741246342659, loss=1.7877120971679688
I0312 12:42:09.296404 139695672514304 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21182513236999512, loss=1.7436180114746094
I0312 12:42:45.062724 139695664121600 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.20094043016433716, loss=1.699664831161499
I0312 12:43:20.798123 139695672514304 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.19306309521198273, loss=1.6527163982391357
I0312 12:43:56.556962 139695664121600 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.25068795680999756, loss=1.70231294631958
I0312 12:44:32.302436 139695672514304 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.19725216925144196, loss=1.728484034538269
I0312 12:45:08.059320 139695664121600 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.22323380410671234, loss=1.7101150751113892
I0312 12:45:43.785288 139695672514304 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.18452641367912292, loss=1.7055646181106567
I0312 12:46:19.525069 139695664121600 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.19781334698200226, loss=1.7249164581298828
I0312 12:46:55.277756 139695672514304 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.19521190226078033, loss=1.7005677223205566
I0312 12:47:31.058031 139695664121600 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.20355388522148132, loss=1.7622982263565063
I0312 12:48:06.821903 139695672514304 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.20771732926368713, loss=1.6461896896362305
I0312 12:48:42.575997 139695664121600 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.22607296705245972, loss=1.716532588005066
I0312 12:49:15.890992 139865354262336 spec.py:321] Evaluating on the training split.
I0312 12:49:18.889754 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:53:11.276656 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 12:53:13.970390 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:56:14.747948 139865354262336 spec.py:349] Evaluating on the test split.
I0312 12:56:17.438764 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 12:59:02.664360 139865354262336 submission_runner.py:420] Time since start: 30609.18s, 	Step: 51695, 	{'train/accuracy': 0.6541595458984375, 'train/loss': 1.6322287321090698, 'train/bleu': 32.13664638188772, 'validation/accuracy': 0.6684479713439941, 'validation/loss': 1.5355753898620605, 'validation/bleu': 28.763730480987636, 'validation/num_examples': 3000, 'test/accuracy': 0.6809133887290955, 'test/loss': 1.451818823814392, 'test/bleu': 28.51707180407588, 'test/num_examples': 3003, 'score': 18521.941950798035, 'total_duration': 30609.1839325428, 'accumulated_submission_time': 18521.941950798035, 'accumulated_eval_time': 12084.83533358574, 'accumulated_logging_time': 0.664344072341919}
I0312 12:59:02.685756 139695672514304 logging_writer.py:48] [51695] accumulated_eval_time=12084.835334, accumulated_logging_time=0.664344, accumulated_submission_time=18521.941951, global_step=51695, preemption_count=0, score=18521.941951, test/accuracy=0.680913, test/bleu=28.517072, test/loss=1.451819, test/num_examples=3003, total_duration=30609.183933, train/accuracy=0.654160, train/bleu=32.136646, train/loss=1.632229, validation/accuracy=0.668448, validation/bleu=28.763730, validation/loss=1.535575, validation/num_examples=3000
I0312 12:59:04.836176 139695664121600 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.19998137652873993, loss=1.7180402278900146
I0312 12:59:40.444914 139695672514304 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.19427721202373505, loss=1.755321741104126
I0312 13:00:16.164767 139695664121600 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.19073334336280823, loss=1.711065411567688
I0312 13:00:51.910388 139695672514304 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.19817356765270233, loss=1.73820161819458
I0312 13:01:27.633965 139695664121600 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.22745394706726074, loss=1.7167534828186035
I0312 13:02:03.407773 139695672514304 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1919683814048767, loss=1.7553261518478394
I0312 13:02:39.147736 139695664121600 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.210354745388031, loss=1.7466567754745483
I0312 13:03:14.958708 139695672514304 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1948367953300476, loss=1.7534393072128296
I0312 13:03:50.766650 139695664121600 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19059529900550842, loss=1.7732917070388794
I0312 13:04:26.546112 139695672514304 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.26915931701660156, loss=1.7888885736465454
I0312 13:05:02.312356 139695664121600 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.19950583577156067, loss=1.8092496395111084
I0312 13:05:38.081311 139695672514304 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.19270724058151245, loss=1.719407320022583
I0312 13:06:13.882918 139695664121600 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.21385963261127472, loss=1.7054526805877686
I0312 13:06:49.674135 139695672514304 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.20062221586704254, loss=1.7022751569747925
I0312 13:07:25.400505 139695664121600 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.19243460893630981, loss=1.7971084117889404
I0312 13:08:01.149387 139695672514304 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.20596370100975037, loss=1.8173309564590454
I0312 13:08:36.928547 139695664121600 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.25128674507141113, loss=1.705605149269104
I0312 13:09:12.688494 139695672514304 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.21138836443424225, loss=1.6932398080825806
I0312 13:09:48.433761 139695664121600 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.19656531512737274, loss=1.7891902923583984
I0312 13:10:24.196516 139695672514304 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.22142523527145386, loss=1.6652570962905884
I0312 13:10:59.938437 139695664121600 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1818336546421051, loss=1.6846039295196533
I0312 13:11:35.697255 139695672514304 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.20884160697460175, loss=1.766265630722046
I0312 13:12:11.447484 139695664121600 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.18871212005615234, loss=1.727465033531189
I0312 13:12:47.228085 139695672514304 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21849939227104187, loss=1.7727024555206299
I0312 13:13:02.745419 139865354262336 spec.py:321] Evaluating on the training split.
I0312 13:13:05.736088 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 13:17:32.576799 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 13:17:35.268426 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 13:20:27.331483 139865354262336 spec.py:349] Evaluating on the test split.
I0312 13:20:30.029969 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 13:23:27.515317 139865354262336 submission_runner.py:420] Time since start: 32074.03s, 	Step: 54045, 	{'train/accuracy': 0.6509293913841248, 'train/loss': 1.6562706232070923, 'train/bleu': 32.05001778447784, 'validation/accuracy': 0.6697251200675964, 'validation/loss': 1.5251288414001465, 'validation/bleu': 28.898027363811607, 'validation/num_examples': 3000, 'test/accuracy': 0.6835047602653503, 'test/loss': 1.4431202411651611, 'test/bleu': 28.570779169095474, 'test/num_examples': 3003, 'score': 19361.910492658615, 'total_duration': 32074.034927845, 'accumulated_submission_time': 19361.910492658615, 'accumulated_eval_time': 12709.605189323425, 'accumulated_logging_time': 0.6956663131713867}
I0312 13:23:27.537321 139695664121600 logging_writer.py:48] [54045] accumulated_eval_time=12709.605189, accumulated_logging_time=0.695666, accumulated_submission_time=19361.910493, global_step=54045, preemption_count=0, score=19361.910493, test/accuracy=0.683505, test/bleu=28.570779, test/loss=1.443120, test/num_examples=3003, total_duration=32074.034928, train/accuracy=0.650929, train/bleu=32.050018, train/loss=1.656271, validation/accuracy=0.669725, validation/bleu=28.898027, validation/loss=1.525129, validation/num_examples=3000
I0312 13:23:47.444415 139695672514304 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.19691574573516846, loss=1.605190396308899
I0312 13:24:23.087406 139695664121600 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.5013156533241272, loss=1.8254483938217163
I0312 13:24:58.831431 139695672514304 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2010248452425003, loss=1.768768548965454
I0312 13:25:34.567581 139695664121600 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19881436228752136, loss=1.7076743841171265
I0312 13:26:10.334834 139695672514304 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.21846546232700348, loss=1.6764987707138062
I0312 13:26:46.116595 139695664121600 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.18755768239498138, loss=1.6688612699508667
I0312 13:27:21.884657 139695672514304 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.20361104607582092, loss=1.7161180973052979
I0312 13:27:57.680320 139695664121600 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.21013589203357697, loss=1.80405592918396
I0312 13:28:33.430686 139695672514304 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.21076051890850067, loss=1.7329148054122925
I0312 13:29:09.185150 139695664121600 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.22126252949237823, loss=1.6747987270355225
I0312 13:29:44.930501 139695672514304 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.19676469266414642, loss=1.6825318336486816
I0312 13:30:20.691543 139695664121600 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.18953683972358704, loss=1.6597729921340942
I0312 13:30:56.479555 139695672514304 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.18120361864566803, loss=1.703967809677124
I0312 13:31:32.267975 139695664121600 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.21523471176624298, loss=1.7631237506866455
I0312 13:32:08.042076 139695672514304 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.20023877918720245, loss=1.774795651435852
I0312 13:32:43.814835 139695664121600 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.20348045229911804, loss=1.6688992977142334
I0312 13:33:19.576701 139695672514304 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1760229915380478, loss=1.672982096672058
I0312 13:33:55.351825 139695664121600 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.19611212611198425, loss=1.685137391090393
I0312 13:34:31.088000 139695672514304 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.1845782846212387, loss=1.724342703819275
I0312 13:35:06.846878 139695664121600 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2002677321434021, loss=1.7079198360443115
I0312 13:35:42.605566 139695672514304 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.19297626614570618, loss=1.6828936338424683
I0312 13:36:18.359006 139695664121600 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.21559478342533112, loss=1.727129340171814
I0312 13:36:54.113484 139695672514304 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1878339797258377, loss=1.6967389583587646
I0312 13:37:27.809601 139865354262336 spec.py:321] Evaluating on the training split.
I0312 13:37:30.810911 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 13:41:04.595305 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 13:41:07.295562 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 13:43:40.242245 139865354262336 spec.py:349] Evaluating on the test split.
I0312 13:43:42.929051 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 13:46:19.910129 139865354262336 submission_runner.py:420] Time since start: 33446.43s, 	Step: 56396, 	{'train/accuracy': 0.6806057691574097, 'train/loss': 1.4595482349395752, 'train/bleu': 34.32327576098257, 'validation/accuracy': 0.6707666516304016, 'validation/loss': 1.515071988105774, 'validation/bleu': 29.178939066208603, 'validation/num_examples': 3000, 'test/accuracy': 0.6842600703239441, 'test/loss': 1.435634970664978, 'test/bleu': 28.68166147587714, 'test/num_examples': 3003, 'score': 20202.092440128326, 'total_duration': 33446.429708480835, 'accumulated_submission_time': 20202.092440128326, 'accumulated_eval_time': 13241.70564031601, 'accumulated_logging_time': 0.7274036407470703}
I0312 13:46:19.932173 139695664121600 logging_writer.py:48] [56396] accumulated_eval_time=13241.705640, accumulated_logging_time=0.727404, accumulated_submission_time=20202.092440, global_step=56396, preemption_count=0, score=20202.092440, test/accuracy=0.684260, test/bleu=28.681661, test/loss=1.435635, test/num_examples=3003, total_duration=33446.429708, train/accuracy=0.680606, train/bleu=34.323276, train/loss=1.459548, validation/accuracy=0.670767, validation/bleu=29.178939, validation/loss=1.515072, validation/num_examples=3000
I0312 13:46:21.738035 139695672514304 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.20858730375766754, loss=1.6982929706573486
I0312 13:46:57.379867 139695664121600 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2747766673564911, loss=1.7313748598098755
I0312 13:47:33.095051 139695672514304 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.18658825755119324, loss=1.8087266683578491
I0312 13:48:08.863470 139695664121600 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.19766193628311157, loss=1.696256160736084
I0312 13:48:44.638592 139695672514304 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.21278004348278046, loss=1.7403037548065186
I0312 13:49:20.398000 139695664121600 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.18674667179584503, loss=1.6285991668701172
I0312 13:49:56.161401 139695672514304 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21338866651058197, loss=1.704909086227417
I0312 13:50:31.932857 139695664121600 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19062377512454987, loss=1.7265132665634155
I0312 13:51:07.721309 139695672514304 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.20752902328968048, loss=1.7468458414077759
I0312 13:51:43.489912 139695664121600 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.1985834389925003, loss=1.7455763816833496
I0312 13:52:19.250180 139695672514304 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.1893959939479828, loss=1.7293586730957031
I0312 13:52:55.016824 139695664121600 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.21001669764518738, loss=1.6516940593719482
I0312 13:53:30.778850 139695672514304 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.1963120996952057, loss=1.7535009384155273
I0312 13:54:06.523061 139695664121600 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.19869837164878845, loss=1.6758731603622437
I0312 13:54:42.250244 139695672514304 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2293384075164795, loss=1.6897355318069458
I0312 13:55:18.023478 139695664121600 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.18449273705482483, loss=1.6371737718582153
I0312 13:55:53.801283 139695672514304 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2162104994058609, loss=1.5926856994628906
I0312 13:56:29.556480 139695664121600 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.193684920668602, loss=1.6747676134109497
I0312 13:57:05.311352 139695672514304 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.1954403817653656, loss=1.6578859090805054
I0312 13:57:41.068530 139695664121600 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2189805507659912, loss=1.6628309488296509
I0312 13:58:16.821339 139695672514304 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.19181765615940094, loss=1.693955898284912
I0312 13:58:52.606119 139695664121600 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.1817876100540161, loss=1.6031906604766846
I0312 13:59:28.382554 139695672514304 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.18931689858436584, loss=1.6712419986724854
I0312 14:00:04.126288 139695664121600 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.20642554759979248, loss=1.7436087131500244
I0312 14:00:19.946061 139865354262336 spec.py:321] Evaluating on the training split.
I0312 14:00:22.942862 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:04:36.179286 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 14:04:38.899472 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:07:37.737674 139865354262336 spec.py:349] Evaluating on the test split.
I0312 14:07:40.438317 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:10:46.015662 139865354262336 submission_runner.py:420] Time since start: 34912.54s, 	Step: 58746, 	{'train/accuracy': 0.653881311416626, 'train/loss': 1.633278727531433, 'train/bleu': 32.14687787371736, 'validation/accuracy': 0.6720437407493591, 'validation/loss': 1.5138766765594482, 'validation/bleu': 28.967526096442047, 'validation/num_examples': 3000, 'test/accuracy': 0.6848527193069458, 'test/loss': 1.4315576553344727, 'test/bleu': 28.62383352483516, 'test/num_examples': 3003, 'score': 21042.018231630325, 'total_duration': 34912.5352704525, 'accumulated_submission_time': 21042.018231630325, 'accumulated_eval_time': 13867.775197029114, 'accumulated_logging_time': 0.7592089176177979}
I0312 14:10:46.039411 139695672514304 logging_writer.py:48] [58746] accumulated_eval_time=13867.775197, accumulated_logging_time=0.759209, accumulated_submission_time=21042.018232, global_step=58746, preemption_count=0, score=21042.018232, test/accuracy=0.684853, test/bleu=28.623834, test/loss=1.431558, test/num_examples=3003, total_duration=34912.535270, train/accuracy=0.653881, train/bleu=32.146878, train/loss=1.633279, validation/accuracy=0.672044, validation/bleu=28.967526, validation/loss=1.513877, validation/num_examples=3000
I0312 14:11:05.593878 139695664121600 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.22445148229599, loss=1.7733067274093628
I0312 14:11:41.239716 139695672514304 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.1881081461906433, loss=1.697646141052246
I0312 14:12:16.976892 139695664121600 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.1994919776916504, loss=1.712442398071289
I0312 14:12:52.728969 139695672514304 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.20624800026416779, loss=1.6830134391784668
I0312 14:13:28.486602 139695664121600 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.20535925030708313, loss=1.6553592681884766
I0312 14:14:04.258011 139695672514304 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.18419329822063446, loss=1.7897385358810425
I0312 14:14:39.993867 139695664121600 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.19539669156074524, loss=1.724219799041748
I0312 14:15:15.740318 139695672514304 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.1998293101787567, loss=1.6609227657318115
I0312 14:15:51.525902 139695664121600 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2038436383008957, loss=1.7029181718826294
I0312 14:16:27.308636 139695672514304 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.18912655115127563, loss=1.6922565698623657
I0312 14:17:03.045764 139695664121600 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.21232640743255615, loss=1.6993021965026855
I0312 14:17:38.792532 139695672514304 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.1883613020181656, loss=1.6853700876235962
I0312 14:18:14.574266 139695664121600 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.20268456637859344, loss=1.7945042848587036
I0312 14:18:50.313761 139695672514304 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.20498166978359222, loss=1.7227253913879395
I0312 14:19:26.068694 139695664121600 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.19353649020195007, loss=1.7371927499771118
I0312 14:20:01.817247 139695672514304 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.18749099969863892, loss=1.7289758920669556
I0312 14:20:37.574955 139695664121600 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20799846947193146, loss=1.728609323501587
I0312 14:21:13.373845 139695672514304 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19860798120498657, loss=1.6954267024993896
I0312 14:21:49.158621 139695664121600 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1978190392255783, loss=1.6814996004104614
I0312 14:22:24.920075 139695672514304 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.1955355703830719, loss=1.7363839149475098
I0312 14:23:00.675190 139695664121600 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.19089855253696442, loss=1.7293143272399902
I0312 14:23:36.442322 139695672514304 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.20901405811309814, loss=1.7175078392028809
I0312 14:24:12.206380 139695664121600 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.22283115983009338, loss=1.7363446950912476
I0312 14:24:46.263363 139865354262336 spec.py:321] Evaluating on the training split.
I0312 14:24:49.271848 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:29:04.776105 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 14:29:07.481408 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:31:55.785262 139865354262336 spec.py:349] Evaluating on the test split.
I0312 14:31:58.477168 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:34:58.243184 139865354262336 submission_runner.py:420] Time since start: 36364.76s, 	Step: 61097, 	{'train/accuracy': 0.6571126580238342, 'train/loss': 1.6194921731948853, 'train/bleu': 32.149596664961024, 'validation/accuracy': 0.6735812425613403, 'validation/loss': 1.5096439123153687, 'validation/bleu': 29.221768396880584, 'validation/num_examples': 3000, 'test/accuracy': 0.6872000694274902, 'test/loss': 1.4211808443069458, 'test/bleu': 28.95892319870933, 'test/num_examples': 3003, 'score': 21882.151026010513, 'total_duration': 36364.76279473305, 'accumulated_submission_time': 21882.151026010513, 'accumulated_eval_time': 14479.754987239838, 'accumulated_logging_time': 0.7943167686462402}
I0312 14:34:58.266911 139695672514304 logging_writer.py:48] [61097] accumulated_eval_time=14479.754987, accumulated_logging_time=0.794317, accumulated_submission_time=21882.151026, global_step=61097, preemption_count=0, score=21882.151026, test/accuracy=0.687200, test/bleu=28.958923, test/loss=1.421181, test/num_examples=3003, total_duration=36364.762795, train/accuracy=0.657113, train/bleu=32.149597, train/loss=1.619492, validation/accuracy=0.673581, validation/bleu=29.221768, validation/loss=1.509644, validation/num_examples=3000
I0312 14:34:59.703559 139695664121600 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.20780256390571594, loss=1.7644968032836914
I0312 14:35:35.317769 139695672514304 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.18733079731464386, loss=1.7108312845230103
I0312 14:36:11.035693 139695664121600 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2045442759990692, loss=1.7202374935150146
I0312 14:36:46.809074 139695672514304 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.20646971464157104, loss=1.7090944051742554
I0312 14:37:22.565132 139695664121600 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19363152980804443, loss=1.713863730430603
I0312 14:37:58.371977 139695672514304 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.20057690143585205, loss=1.667293906211853
I0312 14:38:34.148209 139695664121600 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.2085523009300232, loss=1.6867257356643677
I0312 14:39:09.915789 139695672514304 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.19817449152469635, loss=1.672748327255249
I0312 14:39:45.660416 139695664121600 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.1814226657152176, loss=1.6309103965759277
I0312 14:40:21.420828 139695672514304 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.18621356785297394, loss=1.7804805040359497
I0312 14:40:57.196269 139695664121600 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.19768093526363373, loss=1.6188644170761108
I0312 14:41:32.980134 139695672514304 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19817474484443665, loss=1.7326414585113525
I0312 14:42:08.746518 139695664121600 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.22731837630271912, loss=1.6984596252441406
I0312 14:42:44.503605 139695672514304 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.1880820393562317, loss=1.6691981554031372
I0312 14:43:20.236343 139695664121600 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.19998566806316376, loss=1.7443758249282837
I0312 14:43:56.014584 139695672514304 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.23359890282154083, loss=1.6624720096588135
I0312 14:44:31.768709 139695664121600 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.20945467054843903, loss=1.755197286605835
I0312 14:45:07.488383 139695672514304 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.21502307057380676, loss=1.7867772579193115
I0312 14:45:43.239467 139695664121600 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2378329187631607, loss=1.6895241737365723
I0312 14:46:19.016678 139695672514304 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2055380642414093, loss=1.6972094774246216
I0312 14:46:54.782818 139695664121600 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.2054087221622467, loss=1.6333223581314087
I0312 14:47:30.540364 139695672514304 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.20163123309612274, loss=1.6822457313537598
I0312 14:48:06.306338 139695664121600 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.18931584060192108, loss=1.6743501424789429
I0312 14:48:42.063547 139695672514304 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2072821706533432, loss=1.777207612991333
I0312 14:48:58.252254 139865354262336 spec.py:321] Evaluating on the training split.
I0312 14:49:01.259455 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:53:27.074876 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 14:53:29.761301 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:56:37.978408 139865354262336 spec.py:349] Evaluating on the test split.
I0312 14:56:40.682384 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 14:59:27.680863 139865354262336 submission_runner.py:420] Time since start: 37834.20s, 	Step: 63447, 	{'train/accuracy': 0.6630586385726929, 'train/loss': 1.5684752464294434, 'train/bleu': 32.501644119858895, 'validation/accuracy': 0.6748087406158447, 'validation/loss': 1.494695782661438, 'validation/bleu': 29.088602365219, 'validation/num_examples': 3000, 'test/accuracy': 0.6873278617858887, 'test/loss': 1.4133059978485107, 'test/bleu': 29.131411588174714, 'test/num_examples': 3003, 'score': 22722.046006679535, 'total_duration': 37834.20046401024, 'accumulated_submission_time': 22722.046006679535, 'accumulated_eval_time': 15109.183556556702, 'accumulated_logging_time': 0.8285095691680908}
I0312 14:59:27.704917 139695664121600 logging_writer.py:48] [63447] accumulated_eval_time=15109.183557, accumulated_logging_time=0.828510, accumulated_submission_time=22722.046007, global_step=63447, preemption_count=0, score=22722.046007, test/accuracy=0.687328, test/bleu=29.131412, test/loss=1.413306, test/num_examples=3003, total_duration=37834.200464, train/accuracy=0.663059, train/bleu=32.501644, train/loss=1.568475, validation/accuracy=0.674809, validation/bleu=29.088602, validation/loss=1.494696, validation/num_examples=3000
I0312 14:59:46.929113 139695672514304 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.19355714321136475, loss=1.667659044265747
I0312 15:00:22.569831 139695664121600 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.20939195156097412, loss=1.7235071659088135
I0312 15:00:58.312452 139695672514304 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.18872612714767456, loss=1.6563209295272827
I0312 15:01:34.056809 139695664121600 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.19300051033496857, loss=1.6684890985488892
I0312 15:02:09.835180 139695672514304 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2026485800743103, loss=1.629516839981079
I0312 15:02:45.637529 139695664121600 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.20631974935531616, loss=1.683858871459961
I0312 15:03:21.427400 139695672514304 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.19421903789043427, loss=1.6520739793777466
I0312 15:03:57.201329 139695664121600 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.20689120888710022, loss=1.75533926486969
I0312 15:04:32.972176 139695672514304 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19102570414543152, loss=1.6877555847167969
I0312 15:05:08.726545 139695664121600 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2044392079114914, loss=1.7622826099395752
I0312 15:05:44.513873 139695672514304 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.19524316489696503, loss=1.7849785089492798
I0312 15:06:20.312360 139695664121600 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.22038163244724274, loss=1.67730712890625
I0312 15:06:56.081573 139695672514304 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19688168168067932, loss=1.7275274991989136
I0312 15:07:31.842771 139695664121600 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.19576413929462433, loss=1.7282752990722656
I0312 15:08:07.623639 139695672514304 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20036329329013824, loss=1.7247427701950073
I0312 15:08:43.430445 139695664121600 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.26158589124679565, loss=1.711277961730957
I0312 15:09:19.187375 139695672514304 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.1888759434223175, loss=1.654380202293396
I0312 15:09:54.942721 139695664121600 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.2014080137014389, loss=1.7257790565490723
I0312 15:10:30.705664 139695672514304 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.19350364804267883, loss=1.5978829860687256
I0312 15:11:06.468331 139695664121600 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.1971113383769989, loss=1.651327133178711
I0312 15:11:42.203583 139695672514304 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.20690885186195374, loss=1.6594212055206299
I0312 15:12:17.966618 139695664121600 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.21102294325828552, loss=1.7496589422225952
I0312 15:12:53.720083 139695672514304 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.21977539360523224, loss=1.607858657836914
I0312 15:13:27.763693 139865354262336 spec.py:321] Evaluating on the training split.
I0312 15:13:30.759392 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 15:17:58.218768 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 15:18:00.927509 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 15:21:20.760937 139865354262336 spec.py:349] Evaluating on the test split.
I0312 15:21:23.460264 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 15:24:38.495204 139865354262336 submission_runner.py:420] Time since start: 39345.01s, 	Step: 65797, 	{'train/accuracy': 0.6596559286117554, 'train/loss': 1.6069821119308472, 'train/bleu': 32.72167565803844, 'validation/accuracy': 0.673159658908844, 'validation/loss': 1.5000309944152832, 'validation/bleu': 29.10851777105609, 'validation/num_examples': 3000, 'test/accuracy': 0.687897264957428, 'test/loss': 1.4136604070663452, 'test/bleu': 28.904316917236002, 'test/num_examples': 3003, 'score': 23562.013528585434, 'total_duration': 39345.014755010605, 'accumulated_submission_time': 23562.013528585434, 'accumulated_eval_time': 15779.91496014595, 'accumulated_logging_time': 0.8635752201080322}
I0312 15:24:38.525450 139695664121600 logging_writer.py:48] [65797] accumulated_eval_time=15779.914960, accumulated_logging_time=0.863575, accumulated_submission_time=23562.013529, global_step=65797, preemption_count=0, score=23562.013529, test/accuracy=0.687897, test/bleu=28.904317, test/loss=1.413660, test/num_examples=3003, total_duration=39345.014755, train/accuracy=0.659656, train/bleu=32.721676, train/loss=1.606982, validation/accuracy=0.673160, validation/bleu=29.108518, validation/loss=1.500031, validation/num_examples=3000
I0312 15:24:39.973273 139695672514304 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.22189827263355255, loss=1.669828176498413
I0312 15:25:15.589037 139695664121600 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.19563926756381989, loss=1.6278085708618164
I0312 15:25:51.299065 139695672514304 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2152605503797531, loss=1.6438027620315552
I0312 15:26:27.050844 139695664121600 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.19906286895275116, loss=1.6561729907989502
I0312 15:27:02.822360 139695672514304 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.2116653025150299, loss=1.605947494506836
I0312 15:27:38.622840 139695664121600 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.24890924990177155, loss=1.698407530784607
I0312 15:28:14.371708 139695672514304 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.18810167908668518, loss=1.6480963230133057
I0312 15:28:50.111014 139695664121600 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.19211670756340027, loss=1.6519699096679688
I0312 15:29:25.847698 139695672514304 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.4381251335144043, loss=1.671755075454712
I0312 15:30:01.616588 139695664121600 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.19324912130832672, loss=1.660190463066101
I0312 15:30:37.399548 139695672514304 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.23383833467960358, loss=1.668763518333435
I0312 15:31:13.160435 139695664121600 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.19415932893753052, loss=1.6564443111419678
I0312 15:31:48.958084 139695672514304 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.1949668824672699, loss=1.693779706954956
I0312 15:32:24.760683 139695664121600 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.21092796325683594, loss=1.725579857826233
I0312 15:33:00.566731 139695672514304 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.17633362114429474, loss=1.5862367153167725
I0312 15:33:36.381479 139695664121600 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.2132260799407959, loss=1.680729866027832
I0312 15:34:12.146929 139695672514304 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.22745688259601593, loss=1.7101058959960938
I0312 15:34:47.884021 139695664121600 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.20698827505111694, loss=1.6725411415100098
I0312 15:35:23.656708 139695672514304 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.20885570347309113, loss=1.5702612400054932
I0312 15:35:59.413806 139695664121600 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.19282618165016174, loss=1.6713207960128784
I0312 15:36:35.185875 139695672514304 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.21177253127098083, loss=1.6733660697937012
I0312 15:37:10.952828 139695664121600 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.25132447481155396, loss=1.7196890115737915
I0312 15:37:46.734284 139695672514304 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2005193531513214, loss=1.59529447555542
I0312 15:38:22.585266 139695664121600 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.20789457857608795, loss=1.6567939519882202
I0312 15:38:38.780958 139865354262336 spec.py:321] Evaluating on the training split.
I0312 15:38:41.805757 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 15:42:55.763232 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 15:42:58.454706 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 15:45:26.121215 139865354262336 spec.py:349] Evaluating on the test split.
I0312 15:45:28.815603 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 15:48:13.340994 139865354262336 submission_runner.py:420] Time since start: 40759.86s, 	Step: 68147, 	{'train/accuracy': 0.6588523983955383, 'train/loss': 1.6124247312545776, 'train/bleu': 32.49809723904921, 'validation/accuracy': 0.6761106252670288, 'validation/loss': 1.4879592657089233, 'validation/bleu': 29.24748990886783, 'validation/num_examples': 3000, 'test/accuracy': 0.6913717985153198, 'test/loss': 1.398087501525879, 'test/bleu': 29.46735070037606, 'test/num_examples': 3003, 'score': 24402.17420578003, 'total_duration': 40759.86060547829, 'accumulated_submission_time': 24402.17420578003, 'accumulated_eval_time': 16354.474965810776, 'accumulated_logging_time': 0.905400276184082}
I0312 15:48:13.365605 139695672514304 logging_writer.py:48] [68147] accumulated_eval_time=16354.474966, accumulated_logging_time=0.905400, accumulated_submission_time=24402.174206, global_step=68147, preemption_count=0, score=24402.174206, test/accuracy=0.691372, test/bleu=29.467351, test/loss=1.398088, test/num_examples=3003, total_duration=40759.860605, train/accuracy=0.658852, train/bleu=32.498097, train/loss=1.612425, validation/accuracy=0.676111, validation/bleu=29.247490, validation/loss=1.487959, validation/num_examples=3000
I0312 15:48:32.590004 139695664121600 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.19409796595573425, loss=1.7006131410598755
I0312 15:49:08.257563 139695672514304 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.2002626657485962, loss=1.7399252653121948
I0312 15:49:43.965492 139695664121600 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.22731752693653107, loss=1.5872148275375366
I0312 15:50:19.729475 139695672514304 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.19310155510902405, loss=1.6334593296051025
I0312 15:50:55.474825 139695664121600 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2207631915807724, loss=1.7631475925445557
I0312 15:51:31.253896 139695672514304 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.20255163311958313, loss=1.6743206977844238
I0312 15:52:07.073756 139695664121600 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.19562067091464996, loss=1.68767511844635
I0312 15:52:42.826828 139695672514304 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.18590785562992096, loss=1.6176022291183472
I0312 15:53:18.572170 139695664121600 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.3144706189632416, loss=1.6224027872085571
I0312 15:53:54.303165 139695672514304 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.18871110677719116, loss=1.721470594406128
I0312 15:54:30.076017 139695664121600 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.20576737821102142, loss=1.5717631578445435
I0312 15:55:05.853131 139695672514304 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.20519626140594482, loss=1.700603723526001
I0312 15:55:41.656229 139695664121600 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.19377081096172333, loss=1.7027736902236938
I0312 15:56:17.451396 139695672514304 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.20284798741340637, loss=1.6783931255340576
I0312 15:56:53.182363 139695664121600 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19862417876720428, loss=1.7188247442245483
I0312 15:57:28.943957 139695672514304 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2350033074617386, loss=1.6807912588119507
I0312 15:58:04.729875 139695664121600 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.19576209783554077, loss=1.6238511800765991
I0312 15:58:40.506621 139695672514304 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.20511803030967712, loss=1.6555153131484985
I0312 15:59:16.287851 139695664121600 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.18308161199092865, loss=1.571535348892212
I0312 15:59:52.040748 139695672514304 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.1881418377161026, loss=1.6348315477371216
I0312 16:00:27.790940 139695664121600 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.1818905621767044, loss=1.741802453994751
I0312 16:01:03.591543 139695672514304 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2109009325504303, loss=1.6767388582229614
I0312 16:01:39.352617 139695664121600 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2877110540866852, loss=1.781840443611145
I0312 16:02:13.391766 139865354262336 spec.py:321] Evaluating on the training split.
I0312 16:02:16.394004 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:06:41.653908 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 16:06:44.350660 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:09:19.939248 139865354262336 spec.py:349] Evaluating on the test split.
I0312 16:09:22.644897 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:12:02.612965 139865354262336 submission_runner.py:420] Time since start: 42189.13s, 	Step: 70497, 	{'train/accuracy': 0.6634678840637207, 'train/loss': 1.5741132497787476, 'train/bleu': 33.044339503927496, 'validation/accuracy': 0.6757386922836304, 'validation/loss': 1.4837726354599, 'validation/bleu': 29.433881912093547, 'validation/num_examples': 3000, 'test/accuracy': 0.6908837556838989, 'test/loss': 1.3939534425735474, 'test/bleu': 29.184754428074246, 'test/num_examples': 3003, 'score': 25242.110821008682, 'total_duration': 42189.13254570961, 'accumulated_submission_time': 25242.110821008682, 'accumulated_eval_time': 16943.696088552475, 'accumulated_logging_time': 0.9412546157836914}
I0312 16:12:02.636984 139695672514304 logging_writer.py:48] [70497] accumulated_eval_time=16943.696089, accumulated_logging_time=0.941255, accumulated_submission_time=25242.110821, global_step=70497, preemption_count=0, score=25242.110821, test/accuracy=0.690884, test/bleu=29.184754, test/loss=1.393953, test/num_examples=3003, total_duration=42189.132546, train/accuracy=0.663468, train/bleu=33.044340, train/loss=1.574113, validation/accuracy=0.675739, validation/bleu=29.433882, validation/loss=1.483773, validation/num_examples=3000
I0312 16:12:04.075808 139695664121600 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.21176443994045258, loss=1.7030235528945923
I0312 16:12:39.700604 139695672514304 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.22101107239723206, loss=1.7338535785675049
I0312 16:13:15.427183 139695664121600 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2249038815498352, loss=1.7068148851394653
I0312 16:13:51.180166 139695672514304 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.19548065960407257, loss=1.6887184381484985
I0312 16:14:26.936868 139695664121600 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.45181789994239807, loss=1.5789988040924072
I0312 16:15:02.694651 139695672514304 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20751576125621796, loss=1.6212904453277588
I0312 16:15:38.435485 139695664121600 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.19058746099472046, loss=1.7098133563995361
I0312 16:16:14.185135 139695672514304 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.19497084617614746, loss=1.6784907579421997
I0312 16:16:49.930616 139695664121600 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.20356233417987823, loss=1.6784727573394775
I0312 16:17:25.698225 139695672514304 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.21373258531093597, loss=1.7021337747573853
I0312 16:18:01.449995 139695664121600 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2246653288602829, loss=1.6503815650939941
I0312 16:18:37.205754 139695672514304 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.19236312806606293, loss=1.6232540607452393
I0312 16:19:12.950922 139695664121600 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2098727971315384, loss=1.7835187911987305
I0312 16:19:48.734313 139695672514304 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.21491341292858124, loss=1.6808546781539917
I0312 16:20:24.500376 139695664121600 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.1958923041820526, loss=1.6727102994918823
I0312 16:21:00.297104 139695672514304 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.19380883872509003, loss=1.6458560228347778
I0312 16:21:36.105495 139695664121600 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.21303628385066986, loss=1.6887837648391724
I0312 16:22:11.933512 139695672514304 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.21002429723739624, loss=1.6568681001663208
I0312 16:22:47.678352 139695664121600 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.203760027885437, loss=1.5867024660110474
I0312 16:23:23.445126 139695672514304 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.20006373524665833, loss=1.6933633089065552
I0312 16:23:59.219435 139695664121600 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.18847329914569855, loss=1.6195197105407715
I0312 16:24:34.999917 139695672514304 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.20205256342887878, loss=1.6165066957473755
I0312 16:25:10.766141 139695664121600 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.19590795040130615, loss=1.714547872543335
I0312 16:25:46.513372 139695672514304 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.23271887004375458, loss=1.646523118019104
I0312 16:26:02.669881 139865354262336 spec.py:321] Evaluating on the training split.
I0312 16:26:05.663787 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:30:03.457693 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 16:30:06.151345 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:32:42.608256 139865354262336 spec.py:349] Evaluating on the test split.
I0312 16:32:45.305978 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:35:11.467678 139865354262336 submission_runner.py:420] Time since start: 43577.99s, 	Step: 72847, 	{'train/accuracy': 0.6604095101356506, 'train/loss': 1.5908526182174683, 'train/bleu': 32.75089059937514, 'validation/accuracy': 0.6772513389587402, 'validation/loss': 1.4792447090148926, 'validation/bleu': 29.408424045066894, 'validation/num_examples': 3000, 'test/accuracy': 0.6912323832511902, 'test/loss': 1.3835713863372803, 'test/bleu': 28.924374363232204, 'test/num_examples': 3003, 'score': 26082.05291867256, 'total_duration': 43577.98728346825, 'accumulated_submission_time': 26082.05291867256, 'accumulated_eval_time': 17492.493832349777, 'accumulated_logging_time': 0.9753479957580566}
I0312 16:35:11.491800 139695664121600 logging_writer.py:48] [72847] accumulated_eval_time=17492.493832, accumulated_logging_time=0.975348, accumulated_submission_time=26082.052919, global_step=72847, preemption_count=0, score=26082.052919, test/accuracy=0.691232, test/bleu=28.924374, test/loss=1.383571, test/num_examples=3003, total_duration=43577.987283, train/accuracy=0.660410, train/bleu=32.750891, train/loss=1.590853, validation/accuracy=0.677251, validation/bleu=29.408424, validation/loss=1.479245, validation/num_examples=3000
I0312 16:35:30.739933 139695672514304 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2030184417963028, loss=1.639431357383728
I0312 16:36:06.453494 139695664121600 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2223762720823288, loss=1.7143422365188599
I0312 16:36:42.223728 139695672514304 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.1925114095211029, loss=1.6172231435775757
I0312 16:37:17.977792 139695664121600 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.21456851065158844, loss=1.6628245115280151
I0312 16:37:53.728890 139695672514304 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.20032064616680145, loss=1.6286135911941528
I0312 16:38:29.485003 139695664121600 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.19328758120536804, loss=1.688225507736206
I0312 16:39:05.244790 139695672514304 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.20194970071315765, loss=1.6301661729812622
I0312 16:39:41.095377 139695664121600 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.21330320835113525, loss=1.6358702182769775
I0312 16:40:16.912881 139695672514304 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.1959524154663086, loss=1.6883819103240967
I0312 16:40:52.790738 139695664121600 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.20189569890499115, loss=1.5921604633331299
I0312 16:41:28.578343 139695672514304 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.20613957941532135, loss=1.6638563871383667
I0312 16:42:04.327596 139695664121600 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.21415099501609802, loss=1.679396390914917
I0312 16:42:40.120407 139695672514304 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.1917269378900528, loss=1.6207685470581055
I0312 16:43:15.897150 139695664121600 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.21447816491127014, loss=1.641383171081543
I0312 16:43:51.642730 139695672514304 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.19123688340187073, loss=1.6759761571884155
I0312 16:44:27.399302 139695664121600 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2227894812822342, loss=1.7137632369995117
I0312 16:45:03.180221 139695672514304 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.21441657841205597, loss=1.689858078956604
I0312 16:45:38.939013 139695664121600 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.18920300900936127, loss=1.6036944389343262
I0312 16:46:14.724027 139695672514304 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.1922430694103241, loss=1.6624996662139893
I0312 16:46:50.490231 139695664121600 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.20424072444438934, loss=1.6898577213287354
I0312 16:47:26.246562 139695672514304 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.208495631814003, loss=1.6269241571426392
I0312 16:48:01.992483 139695664121600 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.23309029638767242, loss=1.7092838287353516
I0312 16:48:37.764163 139695672514304 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.20709028840065002, loss=1.6244895458221436
I0312 16:49:11.810098 139865354262336 spec.py:321] Evaluating on the training split.
I0312 16:49:14.803659 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:53:13.758369 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 16:53:16.454639 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:55:47.832241 139865354262336 spec.py:349] Evaluating on the test split.
I0312 16:55:50.532737 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 16:58:14.630117 139865354262336 submission_runner.py:420] Time since start: 44961.15s, 	Step: 75197, 	{'train/accuracy': 0.6884849667549133, 'train/loss': 1.4089932441711426, 'train/bleu': 34.55588996153888, 'validation/accuracy': 0.6790616512298584, 'validation/loss': 1.4695286750793457, 'validation/bleu': 29.559699007521615, 'validation/num_examples': 3000, 'test/accuracy': 0.6939283013343811, 'test/loss': 1.3758292198181152, 'test/bleu': 29.33074891716061, 'test/num_examples': 3003, 'score': 26922.27769780159, 'total_duration': 44961.149689912796, 'accumulated_submission_time': 26922.27769780159, 'accumulated_eval_time': 18035.313774347305, 'accumulated_logging_time': 1.0108327865600586}
I0312 16:58:14.655960 139695664121600 logging_writer.py:48] [75197] accumulated_eval_time=18035.313774, accumulated_logging_time=1.010833, accumulated_submission_time=26922.277698, global_step=75197, preemption_count=0, score=26922.277698, test/accuracy=0.693928, test/bleu=29.330749, test/loss=1.375829, test/num_examples=3003, total_duration=44961.149690, train/accuracy=0.688485, train/bleu=34.555890, train/loss=1.408993, validation/accuracy=0.679062, validation/bleu=29.559699, validation/loss=1.469529, validation/num_examples=3000
I0312 16:58:16.102776 139695672514304 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.2031257599592209, loss=1.7675049304962158
I0312 16:58:51.750590 139695664121600 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.20376907289028168, loss=1.6563864946365356
I0312 16:59:27.520437 139695672514304 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20770184695720673, loss=1.707255482673645
I0312 17:00:03.306368 139695664121600 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.1893138885498047, loss=1.564626693725586
I0312 17:00:39.070023 139695672514304 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.192921444773674, loss=1.6054918766021729
I0312 17:01:14.872431 139695664121600 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.21821489930152893, loss=1.6972171068191528
I0312 17:01:50.672120 139695672514304 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.20079660415649414, loss=1.6559408903121948
I0312 17:02:26.436845 139695664121600 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2142312228679657, loss=1.6566106081008911
I0312 17:03:02.192167 139695672514304 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.21442300081253052, loss=1.6166023015975952
I0312 17:03:37.930638 139695664121600 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.20821647346019745, loss=1.6127346754074097
I0312 17:04:13.702847 139695672514304 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.20582261681556702, loss=1.6236515045166016
I0312 17:04:49.457950 139695664121600 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.19769297540187836, loss=1.6444884538650513
I0312 17:05:25.252159 139695672514304 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.1991342008113861, loss=1.6429651975631714
I0312 17:06:01.028534 139695664121600 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.20990845561027527, loss=1.672461986541748
I0312 17:06:36.810365 139695672514304 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2025100737810135, loss=1.649911880493164
I0312 17:07:12.686475 139695664121600 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.19721639156341553, loss=1.6726795434951782
I0312 17:07:48.469173 139695672514304 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.19623534381389618, loss=1.683548092842102
I0312 17:08:24.263252 139695664121600 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.19399653375148773, loss=1.611697793006897
I0312 17:09:00.057359 139695672514304 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.20396102964878082, loss=1.6547014713287354
I0312 17:09:35.855866 139695664121600 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.21357981860637665, loss=1.6023567914962769
I0312 17:10:11.614040 139695672514304 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.20931118726730347, loss=1.5999305248260498
I0312 17:10:47.393055 139695664121600 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.19957005977630615, loss=1.7210559844970703
I0312 17:11:23.182479 139695672514304 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.2099791318178177, loss=1.6424907445907593
I0312 17:11:58.949308 139695664121600 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.20055456459522247, loss=1.653462529182434
I0312 17:12:14.763085 139865354262336 spec.py:321] Evaluating on the training split.
I0312 17:12:17.759125 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 17:16:27.832916 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 17:16:30.514450 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 17:19:04.601886 139865354262336 spec.py:349] Evaluating on the test split.
I0312 17:19:07.304806 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 17:21:59.707109 139865354262336 submission_runner.py:420] Time since start: 46386.23s, 	Step: 77546, 	{'train/accuracy': 0.6647745966911316, 'train/loss': 1.5595890283584595, 'train/bleu': 33.22219566753689, 'validation/accuracy': 0.6787516474723816, 'validation/loss': 1.46135675907135, 'validation/bleu': 29.467838922686674, 'validation/num_examples': 3000, 'test/accuracy': 0.6951600909233093, 'test/loss': 1.3723912239074707, 'test/bleu': 29.366417158019804, 'test/num_examples': 3003, 'score': 27762.292055606842, 'total_duration': 46386.2266972065, 'accumulated_submission_time': 27762.292055606842, 'accumulated_eval_time': 18620.257729291916, 'accumulated_logging_time': 1.0464684963226318}
I0312 17:21:59.732903 139695672514304 logging_writer.py:48] [77546] accumulated_eval_time=18620.257729, accumulated_logging_time=1.046468, accumulated_submission_time=27762.292056, global_step=77546, preemption_count=0, score=27762.292056, test/accuracy=0.695160, test/bleu=29.366417, test/loss=1.372391, test/num_examples=3003, total_duration=46386.226697, train/accuracy=0.664775, train/bleu=33.222196, train/loss=1.559589, validation/accuracy=0.678752, validation/bleu=29.467839, validation/loss=1.461357, validation/num_examples=3000
I0312 17:22:19.339159 139695664121600 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.19972890615463257, loss=1.6335793733596802
I0312 17:22:55.084884 139695672514304 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.1846182495355606, loss=1.5824863910675049
I0312 17:23:30.861473 139695664121600 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.21695363521575928, loss=1.6596649885177612
I0312 17:24:06.592345 139695672514304 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.20234574377536774, loss=1.6713626384735107
I0312 17:24:42.360822 139695664121600 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.22845026850700378, loss=1.7026195526123047
I0312 17:25:18.109033 139695672514304 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.21326622366905212, loss=1.6571210622787476
I0312 17:25:53.845664 139695664121600 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.20382647216320038, loss=1.6193944215774536
I0312 17:26:29.615398 139695672514304 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2214878350496292, loss=1.6578798294067383
I0312 17:27:05.379110 139695664121600 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2080528289079666, loss=1.5787075757980347
I0312 17:27:41.126053 139695672514304 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.20127995312213898, loss=1.5969634056091309
I0312 17:28:16.877501 139695664121600 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.20205523073673248, loss=1.6389235258102417
I0312 17:28:52.657071 139695672514304 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.21632273495197296, loss=1.656268835067749
I0312 17:29:28.416241 139695664121600 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.19337725639343262, loss=1.6202901601791382
I0312 17:30:04.190978 139695672514304 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.2001747488975525, loss=1.6440110206604004
I0312 17:30:39.940364 139695664121600 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21358701586723328, loss=1.595992922782898
I0312 17:31:15.669013 139695672514304 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.20372779667377472, loss=1.6441890001296997
I0312 17:31:51.411968 139695664121600 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2017425000667572, loss=1.5852900743484497
I0312 17:32:27.169847 139695672514304 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20412912964820862, loss=1.672318935394287
I0312 17:33:02.935009 139695664121600 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.1934841275215149, loss=1.6341626644134521
I0312 17:33:38.695771 139695672514304 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.23801396787166595, loss=1.688995599746704
I0312 17:34:14.476810 139695664121600 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.22223083674907684, loss=1.5969550609588623
I0312 17:34:50.303882 139695672514304 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2010761797428131, loss=1.6424064636230469
I0312 17:35:26.067370 139695664121600 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.20166760683059692, loss=1.6607500314712524
I0312 17:35:59.760188 139865354262336 spec.py:321] Evaluating on the training split.
I0312 17:36:02.763116 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 17:40:20.836591 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 17:40:23.531986 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 17:43:21.735984 139865354262336 spec.py:349] Evaluating on the test split.
I0312 17:43:24.439114 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 17:46:19.474159 139865354262336 submission_runner.py:420] Time since start: 47845.99s, 	Step: 79896, 	{'train/accuracy': 0.6653583645820618, 'train/loss': 1.5624257326126099, 'train/bleu': 32.828529578925284, 'validation/accuracy': 0.6814670562744141, 'validation/loss': 1.453194260597229, 'validation/bleu': 29.776679592171543, 'validation/num_examples': 3000, 'test/accuracy': 0.6968799233436584, 'test/loss': 1.3566237688064575, 'test/bleu': 29.394331373170406, 'test/num_examples': 3003, 'score': 28602.22843670845, 'total_duration': 47845.993671655655, 'accumulated_submission_time': 28602.22843670845, 'accumulated_eval_time': 19239.971554994583, 'accumulated_logging_time': 1.0834178924560547}
I0312 17:46:19.504755 139695672514304 logging_writer.py:48] [79896] accumulated_eval_time=19239.971555, accumulated_logging_time=1.083418, accumulated_submission_time=28602.228437, global_step=79896, preemption_count=0, score=28602.228437, test/accuracy=0.696880, test/bleu=29.394331, test/loss=1.356624, test/num_examples=3003, total_duration=47845.993672, train/accuracy=0.665358, train/bleu=32.828530, train/loss=1.562426, validation/accuracy=0.681467, validation/bleu=29.776680, validation/loss=1.453194, validation/num_examples=3000
I0312 17:46:21.304189 139695664121600 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.20627018809318542, loss=1.5872209072113037
I0312 17:46:56.951720 139695672514304 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.19503772258758545, loss=1.6609827280044556
I0312 17:47:32.676316 139695664121600 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2051360011100769, loss=1.6511496305465698
I0312 17:48:08.412367 139695672514304 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.19054323434829712, loss=1.5711630582809448
I0312 17:48:44.186492 139695664121600 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.19866737723350525, loss=1.6222820281982422
I0312 17:49:19.943792 139695672514304 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20438653230667114, loss=1.6247650384902954
I0312 17:49:55.696203 139695664121600 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.20488318800926208, loss=1.6112865209579468
I0312 17:50:31.475836 139695672514304 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.20107249915599823, loss=1.6368443965911865
I0312 17:51:07.256939 139695664121600 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20089228451251984, loss=1.5763554573059082
I0312 17:51:43.012730 139695672514304 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.20661354064941406, loss=1.6772353649139404
I0312 17:52:18.773274 139695664121600 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.1998220533132553, loss=1.571047306060791
I0312 17:52:54.519389 139695672514304 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.21625538170337677, loss=1.6045233011245728
I0312 17:53:30.291070 139695664121600 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.21304991841316223, loss=1.698525071144104
I0312 17:54:06.035422 139695672514304 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.2026587873697281, loss=1.5809524059295654
I0312 17:54:41.843054 139695664121600 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.1875653713941574, loss=1.637246012687683
I0312 17:55:17.611233 139695672514304 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.21963945031166077, loss=1.637498140335083
I0312 17:55:53.376038 139695664121600 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.20030702650547028, loss=1.6010948419570923
I0312 17:56:29.224277 139695672514304 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.19702014327049255, loss=1.680407166481018
I0312 17:57:04.956252 139695664121600 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2090560495853424, loss=1.5723543167114258
I0312 17:57:40.719982 139695672514304 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.23391929268836975, loss=1.5863062143325806
I0312 17:58:16.491464 139695664121600 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.19728071987628937, loss=1.655143141746521
I0312 17:58:52.285537 139695672514304 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2005155235528946, loss=1.658908724784851
I0312 17:59:28.040151 139695664121600 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.22690127789974213, loss=1.6148146390914917
I0312 18:00:03.778677 139695672514304 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.23051157593727112, loss=1.6236580610275269
I0312 18:00:19.572256 139865354262336 spec.py:321] Evaluating on the training split.
I0312 18:00:22.564091 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:04:37.228682 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 18:04:39.921719 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:08:09.968915 139865354262336 spec.py:349] Evaluating on the test split.
I0312 18:08:12.662214 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:11:44.225705 139865354262336 submission_runner.py:420] Time since start: 49370.75s, 	Step: 82246, 	{'train/accuracy': 0.6765756011009216, 'train/loss': 1.4844000339508057, 'train/bleu': 33.29825089454542, 'validation/accuracy': 0.6817150115966797, 'validation/loss': 1.4443110227584839, 'validation/bleu': 29.627096514149848, 'validation/num_examples': 3000, 'test/accuracy': 0.6970542073249817, 'test/loss': 1.3562180995941162, 'test/bleu': 29.670773861864646, 'test/num_examples': 3003, 'score': 29442.20606327057, 'total_duration': 49370.74531555176, 'accumulated_submission_time': 29442.20606327057, 'accumulated_eval_time': 19924.62496495247, 'accumulated_logging_time': 1.1249215602874756}
I0312 18:11:44.250853 139695664121600 logging_writer.py:48] [82246] accumulated_eval_time=19924.624965, accumulated_logging_time=1.124922, accumulated_submission_time=29442.206063, global_step=82246, preemption_count=0, score=29442.206063, test/accuracy=0.697054, test/bleu=29.670774, test/loss=1.356218, test/num_examples=3003, total_duration=49370.745316, train/accuracy=0.676576, train/bleu=33.298251, train/loss=1.484400, validation/accuracy=0.681715, validation/bleu=29.627097, validation/loss=1.444311, validation/num_examples=3000
I0312 18:12:03.825725 139695672514304 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.20725780725479126, loss=1.6674704551696777
I0312 18:12:39.487224 139695664121600 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.21519184112548828, loss=1.6095619201660156
I0312 18:13:15.229948 139695672514304 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.1868416666984558, loss=1.6242032051086426
I0312 18:13:51.032642 139695664121600 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.21142864227294922, loss=1.6486529111862183
I0312 18:14:26.790606 139695672514304 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.19819828867912292, loss=1.68173086643219
I0312 18:15:02.528865 139695664121600 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.20284169912338257, loss=1.5942548513412476
I0312 18:15:38.283090 139695672514304 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.19838953018188477, loss=1.5833709239959717
I0312 18:16:14.064583 139695664121600 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.19538429379463196, loss=1.5562883615493774
I0312 18:16:49.809320 139695672514304 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.19561995565891266, loss=1.5627316236495972
I0312 18:17:25.564788 139695664121600 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2967835664749146, loss=1.568017601966858
I0312 18:18:01.374970 139695672514304 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.20356741547584534, loss=1.700490951538086
I0312 18:18:37.161372 139695664121600 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.22323700785636902, loss=1.5740598440170288
I0312 18:19:12.924221 139695672514304 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.20578008890151978, loss=1.6745009422302246
I0312 18:19:48.656929 139695664121600 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.21561679244041443, loss=1.6050362586975098
I0312 18:20:24.422999 139695672514304 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.20719456672668457, loss=1.65888249874115
I0312 18:21:00.183511 139695664121600 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.21251259744167328, loss=1.6971114873886108
I0312 18:21:35.929502 139695672514304 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.20336143672466278, loss=1.605078101158142
I0312 18:22:11.701641 139695664121600 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.1719495058059692, loss=1.6131460666656494
I0312 18:22:47.516405 139695672514304 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.21639546751976013, loss=1.60017991065979
I0312 18:23:23.321749 139695664121600 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.19863158464431763, loss=1.6023797988891602
I0312 18:23:59.118046 139695672514304 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.21237891912460327, loss=1.675622582435608
I0312 18:24:34.865252 139695664121600 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.21983234584331512, loss=1.6166291236877441
I0312 18:25:10.653491 139695672514304 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.20503854751586914, loss=1.639326810836792
I0312 18:25:44.334592 139865354262336 spec.py:321] Evaluating on the training split.
I0312 18:25:47.333191 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:29:40.669012 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 18:29:43.367540 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:32:27.188554 139865354262336 spec.py:349] Evaluating on the test split.
I0312 18:32:29.891037 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:35:05.348365 139865354262336 submission_runner.py:420] Time since start: 50771.87s, 	Step: 84596, 	{'train/accuracy': 0.670854926109314, 'train/loss': 1.519789695739746, 'train/bleu': 33.14134605170951, 'validation/accuracy': 0.6831161379814148, 'validation/loss': 1.4388524293899536, 'validation/bleu': 29.692176572418138, 'validation/num_examples': 3000, 'test/accuracy': 0.6992272734642029, 'test/loss': 1.3421218395233154, 'test/bleu': 29.647750123973445, 'test/num_examples': 3003, 'score': 30282.19902396202, 'total_duration': 50771.86794400215, 'accumulated_submission_time': 30282.19902396202, 'accumulated_eval_time': 20485.638658761978, 'accumulated_logging_time': 1.1610476970672607}
I0312 18:35:05.379999 139695664121600 logging_writer.py:48] [84596] accumulated_eval_time=20485.638659, accumulated_logging_time=1.161048, accumulated_submission_time=30282.199024, global_step=84596, preemption_count=0, score=30282.199024, test/accuracy=0.699227, test/bleu=29.647750, test/loss=1.342122, test/num_examples=3003, total_duration=50771.867944, train/accuracy=0.670855, train/bleu=33.141346, train/loss=1.519790, validation/accuracy=0.683116, validation/bleu=29.692177, validation/loss=1.438852, validation/num_examples=3000
I0312 18:35:07.182486 139695672514304 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.19912119209766388, loss=1.6058824062347412
I0312 18:35:42.843505 139695664121600 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.20239317417144775, loss=1.5539474487304688
I0312 18:36:18.561746 139695672514304 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.19902078807353973, loss=1.6109555959701538
I0312 18:36:54.308677 139695664121600 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.38153836131095886, loss=1.610527753829956
I0312 18:37:30.045519 139695672514304 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.19259028136730194, loss=1.56667160987854
I0312 18:38:05.762807 139695664121600 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.20382407307624817, loss=1.613285779953003
I0312 18:38:41.539242 139695672514304 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.198039710521698, loss=1.5652215480804443
I0312 18:39:17.278879 139695664121600 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.21696104109287262, loss=1.7266241312026978
I0312 18:39:53.041411 139695672514304 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.19718703627586365, loss=1.5945484638214111
I0312 18:40:28.794952 139695664121600 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.20126578211784363, loss=1.6184518337249756
I0312 18:41:04.563977 139695672514304 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.21124069392681122, loss=1.58295738697052
I0312 18:41:40.390668 139695664121600 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.21414200961589813, loss=1.5804777145385742
I0312 18:42:16.128283 139695672514304 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2115105390548706, loss=1.6310176849365234
I0312 18:42:51.893323 139695664121600 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.20525570213794708, loss=1.592779517173767
I0312 18:43:27.644232 139695672514304 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.21968543529510498, loss=1.591767430305481
I0312 18:44:03.383084 139695664121600 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.1962289661169052, loss=1.621321439743042
I0312 18:44:39.134140 139695672514304 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.2104761153459549, loss=1.5250848531723022
I0312 18:45:14.903582 139695664121600 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.22703373432159424, loss=1.6492908000946045
I0312 18:45:50.705768 139695672514304 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21246321499347687, loss=1.5687172412872314
I0312 18:46:26.513745 139695664121600 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.21350857615470886, loss=1.639307975769043
I0312 18:47:02.352231 139695672514304 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.21606537699699402, loss=1.6111069917678833
I0312 18:47:38.105623 139695664121600 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.20892873406410217, loss=1.6638797521591187
I0312 18:48:13.845903 139695672514304 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2121100127696991, loss=1.5313076972961426
I0312 18:48:49.574971 139695664121600 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.23274235427379608, loss=1.5908160209655762
I0312 18:49:05.384069 139865354262336 spec.py:321] Evaluating on the training split.
I0312 18:49:08.377323 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:53:07.647002 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 18:53:10.342827 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:55:46.058390 139865354262336 spec.py:349] Evaluating on the test split.
I0312 18:55:48.747414 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 18:58:17.441067 139865354262336 submission_runner.py:420] Time since start: 52163.96s, 	Step: 86946, 	{'train/accuracy': 0.6692231893539429, 'train/loss': 1.5366313457489014, 'train/bleu': 33.56647363891882, 'validation/accuracy': 0.6852735877037048, 'validation/loss': 1.4326162338256836, 'validation/bleu': 30.412507714578492, 'validation/num_examples': 3000, 'test/accuracy': 0.7010865211486816, 'test/loss': 1.3369659185409546, 'test/bleu': 30.055703021706282, 'test/num_examples': 3003, 'score': 31122.1114256382, 'total_duration': 52163.96068120003, 'accumulated_submission_time': 31122.1114256382, 'accumulated_eval_time': 21037.6956114769, 'accumulated_logging_time': 1.203956127166748}
I0312 18:58:17.466605 139695672514304 logging_writer.py:48] [86946] accumulated_eval_time=21037.695611, accumulated_logging_time=1.203956, accumulated_submission_time=31122.111426, global_step=86946, preemption_count=0, score=31122.111426, test/accuracy=0.701087, test/bleu=30.055703, test/loss=1.336966, test/num_examples=3003, total_duration=52163.960681, train/accuracy=0.669223, train/bleu=33.566474, train/loss=1.536631, validation/accuracy=0.685274, validation/bleu=30.412508, validation/loss=1.432616, validation/num_examples=3000
I0312 18:58:37.046499 139695664121600 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.20316970348358154, loss=1.5484524965286255
I0312 18:59:12.786238 139695672514304 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.21063098311424255, loss=1.6812931299209595
I0312 18:59:48.570088 139695664121600 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.19054566323757172, loss=1.6032966375350952
I0312 19:00:24.342967 139695672514304 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.5876010060310364, loss=1.5940518379211426
I0312 19:01:00.080211 139695664121600 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.5468997955322266, loss=1.642451286315918
I0312 19:01:35.845017 139695672514304 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.18968018889427185, loss=1.4805444478988647
I0312 19:02:11.612194 139695664121600 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.21293216943740845, loss=1.5729351043701172
I0312 19:02:47.391548 139695672514304 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.20476597547531128, loss=1.6359429359436035
I0312 19:03:23.149681 139695664121600 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.22814783453941345, loss=1.626846432685852
I0312 19:03:58.899985 139695672514304 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.2027466744184494, loss=1.5643035173416138
I0312 19:04:34.677193 139695664121600 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.20476751029491425, loss=1.619309425354004
I0312 19:05:10.437928 139695672514304 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.20350055396556854, loss=1.6070187091827393
I0312 19:05:46.186058 139695664121600 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.20799601078033447, loss=1.6023316383361816
I0312 19:06:21.966118 139695672514304 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.2172544300556183, loss=1.5416598320007324
I0312 19:06:57.733123 139695664121600 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.22352124750614166, loss=1.7118301391601562
I0312 19:07:33.485399 139695672514304 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.19441749155521393, loss=1.561759352684021
I0312 19:08:09.260906 139695664121600 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.19769008457660675, loss=1.5180697441101074
I0312 19:08:45.044641 139695672514304 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21351386606693268, loss=1.656812310218811
I0312 19:09:20.794363 139695664121600 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.20136699080467224, loss=1.601199746131897
I0312 19:09:56.549322 139695672514304 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2106742411851883, loss=1.546629786491394
I0312 19:10:32.309127 139695664121600 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.218913272023201, loss=1.5291001796722412
I0312 19:11:08.109194 139695672514304 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.214162215590477, loss=1.60590398311615
I0312 19:11:43.854431 139695664121600 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.22495880722999573, loss=1.612991452217102
I0312 19:12:17.542053 139865354262336 spec.py:321] Evaluating on the training split.
I0312 19:12:20.538479 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 19:16:11.892242 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 19:16:14.591029 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 19:18:44.913751 139865354262336 spec.py:349] Evaluating on the test split.
I0312 19:18:47.602325 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 19:21:12.142387 139865354262336 submission_runner.py:420] Time since start: 53538.66s, 	Step: 89296, 	{'train/accuracy': 0.675879180431366, 'train/loss': 1.4836534261703491, 'train/bleu': 34.04868441106427, 'validation/accuracy': 0.6848767995834351, 'validation/loss': 1.427587628364563, 'validation/bleu': 30.374787594004687, 'validation/num_examples': 3000, 'test/accuracy': 0.7005636096000671, 'test/loss': 1.3306442499160767, 'test/bleu': 29.961738819264145, 'test/num_examples': 3003, 'score': 31962.09663248062, 'total_duration': 53538.661982774734, 'accumulated_submission_time': 31962.09663248062, 'accumulated_eval_time': 21572.295882463455, 'accumulated_logging_time': 1.2391314506530762}
I0312 19:21:12.170376 139695672514304 logging_writer.py:48] [89296] accumulated_eval_time=21572.295882, accumulated_logging_time=1.239131, accumulated_submission_time=31962.096632, global_step=89296, preemption_count=0, score=31962.096632, test/accuracy=0.700564, test/bleu=29.961739, test/loss=1.330644, test/num_examples=3003, total_duration=53538.661983, train/accuracy=0.675879, train/bleu=34.048684, train/loss=1.483653, validation/accuracy=0.684877, validation/bleu=30.374788, validation/loss=1.427588, validation/num_examples=3000
I0312 19:21:13.969719 139695664121600 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.20602037012577057, loss=1.6332974433898926
I0312 19:21:49.544100 139695672514304 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.2019633799791336, loss=1.5152493715286255
I0312 19:22:25.229677 139695664121600 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.20931346714496613, loss=1.6522963047027588
I0312 19:23:00.976788 139695672514304 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.21387401223182678, loss=1.6406995058059692
I0312 19:23:36.754550 139695664121600 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.2137393206357956, loss=1.5435177087783813
I0312 19:24:12.519769 139695672514304 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.22503556311130524, loss=1.6173564195632935
I0312 19:24:48.263867 139695664121600 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.20754161477088928, loss=1.5375206470489502
I0312 19:25:24.051701 139695672514304 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.2037971466779709, loss=1.6812292337417603
I0312 19:25:59.813670 139695664121600 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.20831456780433655, loss=1.6689904928207397
I0312 19:26:35.598805 139695672514304 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.20713497698307037, loss=1.631646752357483
I0312 19:27:11.364507 139695664121600 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.19988439977169037, loss=1.6262949705123901
I0312 19:27:47.109679 139695672514304 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.20941105484962463, loss=1.673182725906372
I0312 19:28:22.908055 139695664121600 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.21535736322402954, loss=1.5848300457000732
I0312 19:28:58.686309 139695672514304 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.2118041068315506, loss=1.6779078245162964
I0312 19:29:34.432686 139695664121600 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.22545748949050903, loss=1.6277891397476196
I0312 19:30:10.198807 139695672514304 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.2533738613128662, loss=1.6495718955993652
I0312 19:30:45.959521 139695664121600 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.19513750076293945, loss=1.4863356351852417
I0312 19:31:21.703742 139695672514304 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21674209833145142, loss=1.5893222093582153
I0312 19:31:57.499377 139695664121600 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.20489412546157837, loss=1.5753341913223267
I0312 19:32:33.262086 139695672514304 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.20688387751579285, loss=1.589845895767212
I0312 19:33:09.033332 139695664121600 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.19588351249694824, loss=1.4949198961257935
I0312 19:33:44.790704 139695672514304 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.19885243475437164, loss=1.5162142515182495
I0312 19:34:20.543394 139695664121600 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.21467158198356628, loss=1.570719599723816
I0312 19:34:56.304971 139695672514304 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.20294448733329773, loss=1.6648898124694824
I0312 19:35:12.459346 139865354262336 spec.py:321] Evaluating on the training split.
I0312 19:35:15.453258 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 19:38:52.258345 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 19:38:54.949176 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 19:41:32.789532 139865354262336 spec.py:349] Evaluating on the test split.
I0312 19:41:35.482059 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 19:44:16.407150 139865354262336 submission_runner.py:420] Time since start: 54922.93s, 	Step: 91647, 	{'train/accuracy': 0.6721989512443542, 'train/loss': 1.5158863067626953, 'train/bleu': 33.45237889132655, 'validation/accuracy': 0.6875798106193542, 'validation/loss': 1.4213722944259644, 'validation/bleu': 30.52288304117627, 'validation/num_examples': 3000, 'test/accuracy': 0.7031317353248596, 'test/loss': 1.3237190246582031, 'test/bleu': 30.19090552965387, 'test/num_examples': 3003, 'score': 32802.2980568409, 'total_duration': 54922.926729917526, 'accumulated_submission_time': 32802.2980568409, 'accumulated_eval_time': 22116.24362039566, 'accumulated_logging_time': 1.2771565914154053}
I0312 19:44:16.436592 139695664121600 logging_writer.py:48] [91647] accumulated_eval_time=22116.243620, accumulated_logging_time=1.277157, accumulated_submission_time=32802.298057, global_step=91647, preemption_count=0, score=32802.298057, test/accuracy=0.703132, test/bleu=30.190906, test/loss=1.323719, test/num_examples=3003, total_duration=54922.926730, train/accuracy=0.672199, train/bleu=33.452379, train/loss=1.515886, validation/accuracy=0.687580, validation/bleu=30.522883, validation/loss=1.421372, validation/num_examples=3000
I0312 19:44:35.637904 139695672514304 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.21072079241275787, loss=1.6028558015823364
I0312 19:45:11.286579 139695664121600 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3234250843524933, loss=1.6086763143539429
I0312 19:45:47.016613 139695672514304 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.1989460289478302, loss=1.54460608959198
I0312 19:46:22.823220 139695664121600 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22223305702209473, loss=1.616743564605713
I0312 19:46:58.582178 139695672514304 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.20630547404289246, loss=1.5010168552398682
I0312 19:47:34.348429 139695664121600 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.2092532366514206, loss=1.5267269611358643
I0312 19:48:10.110590 139695672514304 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21316543221473694, loss=1.5339477062225342
I0312 19:48:45.867197 139695664121600 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.20629839599132538, loss=1.5875775814056396
I0312 19:49:21.613640 139695672514304 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22304554283618927, loss=1.536582589149475
I0312 19:49:57.335745 139695664121600 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.20950353145599365, loss=1.5381524562835693
I0312 19:50:33.071446 139695672514304 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.23712967336177826, loss=1.6439745426177979
I0312 19:51:08.870564 139695664121600 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.5771627426147461, loss=1.5542336702346802
I0312 19:51:44.610525 139695672514304 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.20772725343704224, loss=1.6194640398025513
I0312 19:52:20.432439 139695664121600 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.22588509321212769, loss=1.6458070278167725
I0312 19:52:56.174796 139695672514304 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.4538416862487793, loss=1.5068023204803467
I0312 19:53:31.921123 139695664121600 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.21125146746635437, loss=1.562463402748108
I0312 19:54:07.665662 139695672514304 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.22530357539653778, loss=1.5428986549377441
I0312 19:54:43.428773 139695664121600 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21400724351406097, loss=1.6256754398345947
I0312 19:55:19.183664 139695672514304 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.20976503193378448, loss=1.569973349571228
I0312 19:55:54.933998 139695664121600 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.21171694993972778, loss=1.6102484464645386
I0312 19:56:30.712182 139695672514304 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.2292780727148056, loss=1.6743052005767822
I0312 19:57:06.457087 139695664121600 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.22978448867797852, loss=1.560545563697815
I0312 19:57:42.239225 139695672514304 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.21814660727977753, loss=1.6326309442520142
I0312 19:58:16.642241 139865354262336 spec.py:321] Evaluating on the training split.
I0312 19:58:19.657281 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:02:23.373042 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 20:02:26.084236 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:05:08.294469 139865354262336 spec.py:349] Evaluating on the test split.
I0312 20:05:10.997900 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:07:37.325865 139865354262336 submission_runner.py:420] Time since start: 56323.85s, 	Step: 93998, 	{'train/accuracy': 0.6907885074615479, 'train/loss': 1.397413969039917, 'train/bleu': 35.22884127960909, 'validation/accuracy': 0.6862407326698303, 'validation/loss': 1.4174182415008545, 'validation/bleu': 30.26388804807659, 'validation/num_examples': 3000, 'test/accuracy': 0.7028993368148804, 'test/loss': 1.3156377077102661, 'test/bleu': 29.985501215724845, 'test/num_examples': 3003, 'score': 33642.41419363022, 'total_duration': 56323.84543085098, 'accumulated_submission_time': 33642.41419363022, 'accumulated_eval_time': 22676.927169799805, 'accumulated_logging_time': 1.3175652027130127}
I0312 20:07:37.353194 139695664121600 logging_writer.py:48] [93998] accumulated_eval_time=22676.927170, accumulated_logging_time=1.317565, accumulated_submission_time=33642.414194, global_step=93998, preemption_count=0, score=33642.414194, test/accuracy=0.702899, test/bleu=29.985501, test/loss=1.315638, test/num_examples=3003, total_duration=56323.845431, train/accuracy=0.690789, train/bleu=35.228841, train/loss=1.397414, validation/accuracy=0.686241, validation/bleu=30.263888, validation/loss=1.417418, validation/num_examples=3000
I0312 20:07:38.449374 139695672514304 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.1992713063955307, loss=1.4769874811172485
I0312 20:08:14.030557 139695664121600 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.21295221149921417, loss=1.5699058771133423
I0312 20:08:49.781407 139695672514304 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.21451431512832642, loss=1.5802937746047974
I0312 20:09:25.598399 139695664121600 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.23658615350723267, loss=1.5687440633773804
I0312 20:10:01.391789 139695672514304 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.2249635010957718, loss=1.5595736503601074
I0312 20:10:37.148892 139695664121600 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21857167780399323, loss=1.6388587951660156
I0312 20:11:12.917684 139695672514304 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.20279215276241302, loss=1.5195788145065308
I0312 20:11:48.684115 139695664121600 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2193196415901184, loss=1.562891960144043
I0312 20:12:24.443806 139695672514304 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.220475435256958, loss=1.5553154945373535
I0312 20:13:00.199379 139695664121600 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21143656969070435, loss=1.5457510948181152
I0312 20:13:35.975588 139695672514304 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.22051310539245605, loss=1.6534525156021118
I0312 20:14:11.751394 139695664121600 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2206600308418274, loss=1.6157755851745605
I0312 20:14:47.510178 139695672514304 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.21585211157798767, loss=1.6189000606536865
I0312 20:15:23.280297 139695664121600 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21214698255062103, loss=1.593510389328003
I0312 20:15:59.036363 139695672514304 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3542923331260681, loss=1.521220088005066
I0312 20:16:34.809948 139695664121600 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.30022963881492615, loss=1.6222814321517944
I0312 20:17:10.589880 139695672514304 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.2236357182264328, loss=1.5947484970092773
I0312 20:17:46.345649 139695664121600 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.21709279716014862, loss=1.6479651927947998
I0312 20:18:22.100047 139695672514304 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3194156587123871, loss=1.570770263671875
I0312 20:18:57.877445 139695664121600 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.23239146173000336, loss=1.630948543548584
I0312 20:19:33.639526 139695672514304 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.2201164811849594, loss=1.5147281885147095
I0312 20:20:09.408038 139695664121600 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.2280712127685547, loss=1.5982331037521362
I0312 20:20:45.201593 139695672514304 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.4480118155479431, loss=1.5587550401687622
I0312 20:21:20.971732 139695664121600 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.2108394205570221, loss=1.5826561450958252
I0312 20:21:37.488151 139865354262336 spec.py:321] Evaluating on the training split.
I0312 20:21:40.485713 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:25:48.065196 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 20:25:50.754189 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:29:14.611199 139865354262336 spec.py:349] Evaluating on the test split.
I0312 20:29:17.315075 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:32:51.710402 139865354262336 submission_runner.py:420] Time since start: 57838.23s, 	Step: 96348, 	{'train/accuracy': 0.680406928062439, 'train/loss': 1.4625781774520874, 'train/bleu': 33.95429486501357, 'validation/accuracy': 0.6884601712226868, 'validation/loss': 1.409622311592102, 'validation/bleu': 30.50475710065807, 'validation/num_examples': 3000, 'test/accuracy': 0.7055023312568665, 'test/loss': 1.3074795007705688, 'test/bleu': 30.463050020198107, 'test/num_examples': 3003, 'score': 34482.45923447609, 'total_duration': 57838.22999000549, 'accumulated_submission_time': 34482.45923447609, 'accumulated_eval_time': 23351.149356365204, 'accumulated_logging_time': 1.3545808792114258}
I0312 20:32:51.737493 139695672514304 logging_writer.py:48] [96348] accumulated_eval_time=23351.149356, accumulated_logging_time=1.354581, accumulated_submission_time=34482.459234, global_step=96348, preemption_count=0, score=34482.459234, test/accuracy=0.705502, test/bleu=30.463050, test/loss=1.307480, test/num_examples=3003, total_duration=57838.229990, train/accuracy=0.680407, train/bleu=33.954295, train/loss=1.462578, validation/accuracy=0.688460, validation/bleu=30.504757, validation/loss=1.409622, validation/num_examples=3000
I0312 20:33:10.619135 139695664121600 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.2563767433166504, loss=1.5555500984191895
I0312 20:33:46.298059 139695672514304 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22478969395160675, loss=1.572296380996704
I0312 20:34:22.035081 139695664121600 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.24305547773838043, loss=1.604748249053955
I0312 20:34:57.802685 139695672514304 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.20526127517223358, loss=1.6213735342025757
I0312 20:35:33.578214 139695664121600 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.21153396368026733, loss=1.6134158372879028
I0312 20:36:09.376728 139695672514304 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.22508852183818817, loss=1.5564717054367065
I0312 20:36:45.190813 139695664121600 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.20524203777313232, loss=1.5822949409484863
I0312 20:37:20.993668 139695672514304 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.2670609951019287, loss=1.6629291772842407
I0312 20:37:56.783057 139695664121600 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22444233298301697, loss=1.549208641052246
I0312 20:38:32.531338 139695672514304 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.23207399249076843, loss=1.5666247606277466
I0312 20:39:08.290794 139695664121600 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.21465089917182922, loss=1.5867030620574951
I0312 20:39:44.063519 139695672514304 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.2298291176557541, loss=1.5514246225357056
I0312 20:40:19.822189 139695664121600 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21934637427330017, loss=1.556053876876831
I0312 20:40:55.591496 139695672514304 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.2162996083498001, loss=1.5138641595840454
I0312 20:41:31.338402 139695664121600 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.217123880982399, loss=1.6402249336242676
I0312 20:42:07.084037 139695672514304 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21375013887882233, loss=1.5580888986587524
I0312 20:42:42.834628 139695664121600 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.20835794508457184, loss=1.547697901725769
I0312 20:43:18.589442 139695672514304 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.23043905198574066, loss=1.5980165004730225
I0312 20:43:54.359653 139695664121600 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21934452652931213, loss=1.628442406654358
I0312 20:44:30.138988 139695672514304 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.2280726432800293, loss=1.5545704364776611
I0312 20:45:05.897333 139695664121600 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.22777575254440308, loss=1.5047552585601807
I0312 20:45:41.690804 139695672514304 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21855302155017853, loss=1.5319092273712158
I0312 20:46:17.499317 139695664121600 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.22917798161506653, loss=1.540077805519104
I0312 20:46:51.910018 139865354262336 spec.py:321] Evaluating on the training split.
I0312 20:46:54.919942 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:51:17.112447 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 20:51:19.818278 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:54:17.890623 139865354262336 spec.py:349] Evaluating on the test split.
I0312 20:54:20.582272 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 20:56:53.935919 139865354262336 submission_runner.py:420] Time since start: 59280.46s, 	Step: 98698, 	{'train/accuracy': 0.6774348616600037, 'train/loss': 1.4779555797576904, 'train/bleu': 34.15186940985161, 'validation/accuracy': 0.6887453198432922, 'validation/loss': 1.4022163152694702, 'validation/bleu': 30.495876301755462, 'validation/num_examples': 3000, 'test/accuracy': 0.7058973908424377, 'test/loss': 1.300991177558899, 'test/bleu': 30.41957079143748, 'test/num_examples': 3003, 'score': 35322.54058718681, 'total_duration': 59280.455530166626, 'accumulated_submission_time': 35322.54058718681, 'accumulated_eval_time': 23953.17521595955, 'accumulated_logging_time': 1.3932616710662842}
I0312 20:56:53.963604 139695672514304 logging_writer.py:48] [98698] accumulated_eval_time=23953.175216, accumulated_logging_time=1.393262, accumulated_submission_time=35322.540587, global_step=98698, preemption_count=0, score=35322.540587, test/accuracy=0.705897, test/bleu=30.419571, test/loss=1.300991, test/num_examples=3003, total_duration=59280.455530, train/accuracy=0.677435, train/bleu=34.151869, train/loss=1.477956, validation/accuracy=0.688745, validation/bleu=30.495876, validation/loss=1.402216, validation/num_examples=3000
I0312 20:56:55.054540 139695664121600 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.2181103378534317, loss=1.5700130462646484
I0312 20:57:30.661182 139695672514304 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.21967579424381256, loss=1.4712083339691162
I0312 20:58:06.379940 139695664121600 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.21411079168319702, loss=1.544869065284729
I0312 20:58:42.121251 139695672514304 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2130606323480606, loss=1.5018795728683472
I0312 20:59:17.883465 139695664121600 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.20854558050632477, loss=1.491268277168274
I0312 20:59:53.663424 139695672514304 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.22622428834438324, loss=1.5245368480682373
I0312 21:00:29.462069 139695664121600 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21564418077468872, loss=1.5519994497299194
I0312 21:01:05.354496 139695672514304 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.22361144423484802, loss=1.4724942445755005
I0312 21:01:41.139103 139695664121600 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.21123744547367096, loss=1.5339092016220093
I0312 21:02:16.937525 139695672514304 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.24895678460597992, loss=1.4993078708648682
I0312 21:02:52.737424 139695664121600 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22097893059253693, loss=1.555742621421814
I0312 21:03:28.477315 139695672514304 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2248830646276474, loss=1.5704336166381836
I0312 21:04:04.255167 139695664121600 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.21512040495872498, loss=1.451550006866455
I0312 21:04:40.015328 139695672514304 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.23012961447238922, loss=1.5403519868850708
I0312 21:05:15.802180 139695664121600 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2276795506477356, loss=1.5727871656417847
I0312 21:05:51.614324 139695672514304 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.21822190284729004, loss=1.4979525804519653
I0312 21:06:27.431620 139695664121600 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.2071850746870041, loss=1.4992811679840088
I0312 21:07:03.312867 139695672514304 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.2135436236858368, loss=1.5548070669174194
I0312 21:07:39.122616 139695664121600 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.22031860053539276, loss=1.640853762626648
I0312 21:08:14.891499 139695672514304 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.2176620215177536, loss=1.4886152744293213
I0312 21:08:50.645198 139695664121600 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.22368721663951874, loss=1.5682957172393799
I0312 21:09:26.416902 139695672514304 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.22490084171295166, loss=1.5124759674072266
I0312 21:10:02.216073 139695664121600 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.2214874029159546, loss=1.5493507385253906
I0312 21:10:37.979399 139695672514304 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.24614141881465912, loss=1.560823917388916
I0312 21:10:54.153951 139865354262336 spec.py:321] Evaluating on the training split.
I0312 21:10:57.148606 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 21:14:34.185609 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 21:14:36.889079 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 21:17:41.112650 139865354262336 spec.py:349] Evaluating on the test split.
I0312 21:17:43.801446 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 21:20:26.802754 139865354262336 submission_runner.py:420] Time since start: 60693.32s, 	Step: 101047, 	{'train/accuracy': 0.6855158805847168, 'train/loss': 1.433314561843872, 'train/bleu': 34.851935506476764, 'validation/accuracy': 0.6893280744552612, 'validation/loss': 1.3989043235778809, 'validation/bleu': 30.42614686854671, 'validation/num_examples': 3000, 'test/accuracy': 0.7059903740882874, 'test/loss': 1.296967625617981, 'test/bleu': 30.442794792692226, 'test/num_examples': 3003, 'score': 36162.63892054558, 'total_duration': 60693.32236337662, 'accumulated_submission_time': 36162.63892054558, 'accumulated_eval_time': 24525.823969364166, 'accumulated_logging_time': 1.4304945468902588}
I0312 21:20:26.833377 139695664121600 logging_writer.py:48] [101047] accumulated_eval_time=24525.823969, accumulated_logging_time=1.430495, accumulated_submission_time=36162.638921, global_step=101047, preemption_count=0, score=36162.638921, test/accuracy=0.705990, test/bleu=30.442795, test/loss=1.296968, test/num_examples=3003, total_duration=60693.322363, train/accuracy=0.685516, train/bleu=34.851936, train/loss=1.433315, validation/accuracy=0.689328, validation/bleu=30.426147, validation/loss=1.398904, validation/num_examples=3000
I0312 21:20:46.081468 139695672514304 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.21338336169719696, loss=1.5919181108474731
I0312 21:21:21.764022 139695664121600 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.21521274745464325, loss=1.4988664388656616
I0312 21:21:57.521988 139695672514304 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.23579680919647217, loss=1.5349738597869873
I0312 21:22:33.254778 139695664121600 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.21515658497810364, loss=1.567264437675476
I0312 21:23:09.015996 139695672514304 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.22247426211833954, loss=1.5498113632202148
I0312 21:23:44.784081 139695664121600 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22250020503997803, loss=1.5909533500671387
I0312 21:24:20.551398 139695672514304 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.2195064276456833, loss=1.506061315536499
I0312 21:24:56.294896 139695664121600 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.20631423592567444, loss=1.498488187789917
I0312 21:25:32.054626 139695672514304 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22140832245349884, loss=1.58876371383667
I0312 21:26:07.843841 139695664121600 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.2406989336013794, loss=1.5366204977035522
I0312 21:26:43.692431 139695672514304 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2201673537492752, loss=1.4798718690872192
I0312 21:27:19.482482 139695664121600 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.22383667528629303, loss=1.5233314037322998
I0312 21:27:55.345863 139695672514304 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.2230193167924881, loss=1.5273115634918213
I0312 21:28:31.142028 139695664121600 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21760940551757812, loss=1.5250296592712402
I0312 21:29:06.902024 139695672514304 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21988287568092346, loss=1.5326157808303833
I0312 21:29:42.660498 139695664121600 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.22434048354625702, loss=1.5205787420272827
I0312 21:30:18.429071 139695672514304 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.24190039932727814, loss=1.6542280912399292
I0312 21:30:54.173524 139695664121600 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.21059909462928772, loss=1.4374277591705322
I0312 21:31:29.928036 139695672514304 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.2314184606075287, loss=1.5730876922607422
I0312 21:32:05.691372 139695664121600 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2144186943769455, loss=1.4897234439849854
I0312 21:32:41.487574 139695672514304 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.22153683006763458, loss=1.524420976638794
I0312 21:33:17.263137 139695664121600 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.22279563546180725, loss=1.5689442157745361
I0312 21:33:53.053512 139695672514304 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22370652854442596, loss=1.5328094959259033
I0312 21:34:27.081483 139865354262336 spec.py:321] Evaluating on the training split.
I0312 21:34:30.082803 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 21:38:26.436338 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 21:38:29.131639 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 21:41:59.464366 139865354262336 spec.py:349] Evaluating on the test split.
I0312 21:42:02.168781 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 21:45:31.323891 139865354262336 submission_runner.py:420] Time since start: 62197.84s, 	Step: 103397, 	{'train/accuracy': 0.6817169785499573, 'train/loss': 1.4496946334838867, 'train/bleu': 34.91150543285352, 'validation/accuracy': 0.6903820037841797, 'validation/loss': 1.3946393728256226, 'validation/bleu': 30.451516312385404, 'validation/num_examples': 3000, 'test/accuracy': 0.7083725929260254, 'test/loss': 1.2883847951889038, 'test/bleu': 30.640346635700297, 'test/num_examples': 3003, 'score': 37002.79528737068, 'total_duration': 62197.84350180626, 'accumulated_submission_time': 37002.79528737068, 'accumulated_eval_time': 25190.066334486008, 'accumulated_logging_time': 1.4713306427001953}
I0312 21:45:31.353497 139695664121600 logging_writer.py:48] [103397] accumulated_eval_time=25190.066334, accumulated_logging_time=1.471331, accumulated_submission_time=37002.795287, global_step=103397, preemption_count=0, score=37002.795287, test/accuracy=0.708373, test/bleu=30.640347, test/loss=1.288385, test/num_examples=3003, total_duration=62197.843502, train/accuracy=0.681717, train/bleu=34.911505, train/loss=1.449695, validation/accuracy=0.690382, validation/bleu=30.451516, validation/loss=1.394639, validation/num_examples=3000
I0312 21:45:32.795360 139695672514304 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.2163715660572052, loss=1.5390796661376953
I0312 21:46:08.373045 139695664121600 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.21088500320911407, loss=1.4686205387115479
I0312 21:46:44.091191 139695672514304 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.2089388519525528, loss=1.5057531595230103
I0312 21:47:19.864285 139695664121600 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.21605663001537323, loss=1.4602218866348267
I0312 21:47:55.662223 139695672514304 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22758740186691284, loss=1.6128404140472412
I0312 21:48:31.427930 139695664121600 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.21177060902118683, loss=1.5306280851364136
I0312 21:49:07.214534 139695672514304 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.2301999032497406, loss=1.5507475137710571
I0312 21:49:42.971164 139695664121600 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.23091726005077362, loss=1.5087590217590332
I0312 21:50:18.724655 139695672514304 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.235460102558136, loss=1.5791817903518677
I0312 21:50:54.481651 139695664121600 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.21426942944526672, loss=1.5177032947540283
I0312 21:51:30.263862 139695672514304 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.224880188703537, loss=1.5095354318618774
I0312 21:52:06.078624 139695664121600 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.21550709009170532, loss=1.532538652420044
I0312 21:52:41.882760 139695672514304 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22140613198280334, loss=1.5243295431137085
I0312 21:53:17.676034 139695664121600 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.23541392385959625, loss=1.5736141204833984
I0312 21:53:53.443471 139695672514304 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.21947039663791656, loss=1.4615235328674316
I0312 21:54:29.208318 139695664121600 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.21488194167613983, loss=1.5430222749710083
I0312 21:55:05.000273 139695672514304 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.2345787137746811, loss=1.5451847314834595
I0312 21:55:40.761811 139695664121600 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.22230708599090576, loss=1.5193644762039185
I0312 21:56:16.550209 139695672514304 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.23176568746566772, loss=1.554046630859375
I0312 21:56:52.311763 139695664121600 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.21526317298412323, loss=1.526092767715454
I0312 21:57:28.089378 139695672514304 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.21857862174510956, loss=1.4223911762237549
I0312 21:58:03.858541 139695664121600 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.21499556303024292, loss=1.4998252391815186
I0312 21:58:39.628103 139695672514304 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.2309410125017166, loss=1.4787309169769287
I0312 21:59:15.392861 139695664121600 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.22297631204128265, loss=1.5481775999069214
I0312 21:59:31.566051 139865354262336 spec.py:321] Evaluating on the training split.
I0312 21:59:34.566449 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:03:37.246422 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 22:03:39.939534 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:06:23.292635 139865354262336 spec.py:349] Evaluating on the test split.
I0312 22:06:26.002010 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:09:14.241967 139865354262336 submission_runner.py:420] Time since start: 63620.76s, 	Step: 105747, 	{'train/accuracy': 0.683246910572052, 'train/loss': 1.4504034519195557, 'train/bleu': 34.73362591682547, 'validation/accuracy': 0.6902952194213867, 'validation/loss': 1.389479160308838, 'validation/bleu': 30.559631401457906, 'validation/num_examples': 3000, 'test/accuracy': 0.7087560296058655, 'test/loss': 1.285719394683838, 'test/bleu': 30.557939741522667, 'test/num_examples': 3003, 'score': 37842.91463184357, 'total_duration': 63620.761567115784, 'accumulated_submission_time': 37842.91463184357, 'accumulated_eval_time': 25772.742190361023, 'accumulated_logging_time': 1.5127499103546143}
I0312 22:09:14.270829 139695672514304 logging_writer.py:48] [105747] accumulated_eval_time=25772.742190, accumulated_logging_time=1.512750, accumulated_submission_time=37842.914632, global_step=105747, preemption_count=0, score=37842.914632, test/accuracy=0.708756, test/bleu=30.557940, test/loss=1.285719, test/num_examples=3003, total_duration=63620.761567, train/accuracy=0.683247, train/bleu=34.733626, train/loss=1.450403, validation/accuracy=0.690295, validation/bleu=30.559631, validation/loss=1.389479, validation/num_examples=3000
I0312 22:09:33.490599 139695664121600 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.25350573658943176, loss=1.5139119625091553
I0312 22:10:09.140679 139695672514304 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.24127019941806793, loss=1.5689005851745605
I0312 22:10:44.894134 139695664121600 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.23676778376102448, loss=1.5592206716537476
I0312 22:11:20.643837 139695672514304 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.21939170360565186, loss=1.462489366531372
I0312 22:11:56.433132 139695664121600 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.22893761098384857, loss=1.5762312412261963
I0312 22:12:32.180487 139695672514304 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.22119401395320892, loss=1.498363733291626
I0312 22:13:07.994642 139695664121600 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.21756647527217865, loss=1.48698890209198
I0312 22:13:43.776634 139695672514304 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.22778117656707764, loss=1.4944546222686768
I0312 22:14:19.522788 139695664121600 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22456780076026917, loss=1.5074183940887451
I0312 22:14:55.276207 139695672514304 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.2257397472858429, loss=1.5156655311584473
I0312 22:15:31.007694 139695664121600 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22147171199321747, loss=1.4560664892196655
I0312 22:16:06.742599 139695672514304 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.23759859800338745, loss=1.5039722919464111
I0312 22:16:42.554822 139695664121600 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.22887802124023438, loss=1.5411735773086548
I0312 22:17:18.335329 139695672514304 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.214710995554924, loss=1.576798915863037
I0312 22:17:54.100532 139695664121600 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.2647346556186676, loss=1.5579266548156738
I0312 22:18:29.880634 139695672514304 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.24349747598171234, loss=1.437053918838501
I0312 22:19:05.647329 139695664121600 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.2221798300743103, loss=1.5022310018539429
I0312 22:19:41.403901 139695672514304 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.22450198233127594, loss=1.5356907844543457
I0312 22:20:17.178877 139695664121600 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.23352563381195068, loss=1.5865106582641602
I0312 22:20:52.951715 139695672514304 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.22634150087833405, loss=1.4634500741958618
I0312 22:21:28.721606 139695664121600 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.2232280969619751, loss=1.5360631942749023
I0312 22:22:04.496636 139695672514304 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.23074527084827423, loss=1.513559103012085
I0312 22:22:40.260245 139695664121600 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.2187834233045578, loss=1.562543272972107
I0312 22:23:14.312123 139865354262336 spec.py:321] Evaluating on the training split.
I0312 22:23:17.304543 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:26:58.106376 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 22:27:00.810581 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:29:47.675893 139865354262336 spec.py:349] Evaluating on the test split.
I0312 22:29:50.362834 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:32:44.665352 139865354262336 submission_runner.py:420] Time since start: 65031.18s, 	Step: 108097, 	{'train/accuracy': 0.6888919472694397, 'train/loss': 1.4076614379882812, 'train/bleu': 35.10855108546573, 'validation/accuracy': 0.6916466951370239, 'validation/loss': 1.383272647857666, 'validation/bleu': 30.897534597573046, 'validation/num_examples': 3000, 'test/accuracy': 0.7101156115531921, 'test/loss': 1.277177333831787, 'test/bleu': 30.65263189003389, 'test/num_examples': 3003, 'score': 38682.86744952202, 'total_duration': 65031.184963703156, 'accumulated_submission_time': 38682.86744952202, 'accumulated_eval_time': 26343.09537410736, 'accumulated_logging_time': 1.5515015125274658}
I0312 22:32:44.695468 139695672514304 logging_writer.py:48] [108097] accumulated_eval_time=26343.095374, accumulated_logging_time=1.551502, accumulated_submission_time=38682.867450, global_step=108097, preemption_count=0, score=38682.867450, test/accuracy=0.710116, test/bleu=30.652632, test/loss=1.277177, test/num_examples=3003, total_duration=65031.184964, train/accuracy=0.688892, train/bleu=35.108551, train/loss=1.407661, validation/accuracy=0.691647, validation/bleu=30.897535, validation/loss=1.383273, validation/num_examples=3000
I0312 22:32:46.140229 139695664121600 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22623063623905182, loss=1.4923317432403564
I0312 22:33:21.745070 139695672514304 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.22564171254634857, loss=1.5172626972198486
I0312 22:33:57.506995 139695664121600 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.23690229654312134, loss=1.5348812341690063
I0312 22:34:33.278326 139695672514304 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.2231268733739853, loss=1.4607656002044678
I0312 22:35:09.054745 139695664121600 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.21278102695941925, loss=1.464825987815857
I0312 22:35:44.810376 139695672514304 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.22250910103321075, loss=1.5128941535949707
I0312 22:36:20.603537 139695664121600 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.2210635095834732, loss=1.5299853086471558
I0312 22:36:56.414335 139695672514304 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22193223237991333, loss=1.5438275337219238
I0312 22:37:32.161480 139695664121600 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.2462913542985916, loss=1.559786319732666
I0312 22:38:07.915466 139695672514304 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.22492091357707977, loss=1.5197381973266602
I0312 22:38:43.698379 139695664121600 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.23255322873592377, loss=1.4509272575378418
I0312 22:39:19.594182 139695672514304 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.2105349749326706, loss=1.4784497022628784
I0312 22:39:55.415570 139695664121600 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.21419402956962585, loss=1.4153692722320557
I0312 22:40:31.214834 139695672514304 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.22389911115169525, loss=1.45645010471344
I0312 22:41:06.988162 139695664121600 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.23110948503017426, loss=1.5056445598602295
I0312 22:41:42.787939 139695672514304 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.22674556076526642, loss=1.428789496421814
I0312 22:42:18.601624 139695664121600 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.23624199628829956, loss=1.5100996494293213
I0312 22:42:54.399116 139695672514304 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.22901460528373718, loss=1.4347933530807495
I0312 22:43:30.228424 139695664121600 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.23231741786003113, loss=1.5754010677337646
I0312 22:44:05.966803 139695672514304 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.2210303694009781, loss=1.4698210954666138
I0312 22:44:41.752581 139695664121600 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.2295733541250229, loss=1.4898136854171753
I0312 22:45:17.522510 139695672514304 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.21732527017593384, loss=1.4572404623031616
I0312 22:45:53.339109 139695664121600 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.24300257861614227, loss=1.4521677494049072
I0312 22:46:29.105204 139695672514304 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.2315506637096405, loss=1.474399447441101
I0312 22:46:44.908455 139865354262336 spec.py:321] Evaluating on the training split.
I0312 22:46:47.906135 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:50:53.881043 139865354262336 spec.py:333] Evaluating on the validation split.
I0312 22:50:56.572360 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:54:17.504266 139865354262336 spec.py:349] Evaluating on the test split.
I0312 22:54:20.194284 139865354262336 workload.py:181] Translating evaluation dataset.
I0312 22:57:27.162225 139865354262336 submission_runner.py:420] Time since start: 66513.68s, 	Step: 110446, 	{'train/accuracy': 0.6874507069587708, 'train/loss': 1.4143240451812744, 'train/bleu': 34.818386155596556, 'validation/accuracy': 0.6933826208114624, 'validation/loss': 1.3791488409042358, 'validation/bleu': 30.886477140805987, 'validation/num_examples': 3000, 'test/accuracy': 0.7107431292533875, 'test/loss': 1.2718663215637207, 'test/bleu': 30.877531140595956, 'test/num_examples': 3003, 'score': 39522.984773635864, 'total_duration': 66513.68182969093, 'accumulated_submission_time': 39522.984773635864, 'accumulated_eval_time': 26985.349088191986, 'accumulated_logging_time': 1.5927612781524658}
I0312 22:57:27.192307 139695664121600 logging_writer.py:48] [110446] accumulated_eval_time=26985.349088, accumulated_logging_time=1.592761, accumulated_submission_time=39522.984774, global_step=110446, preemption_count=0, score=39522.984774, test/accuracy=0.710743, test/bleu=30.877531, test/loss=1.271866, test/num_examples=3003, total_duration=66513.681830, train/accuracy=0.687451, train/bleu=34.818386, train/loss=1.414324, validation/accuracy=0.693383, validation/bleu=30.886477, validation/loss=1.379149, validation/num_examples=3000
I0312 22:57:27.221713 139695672514304 logging_writer.py:48] [110446] global_step=110446, preemption_count=0, score=39522.984774
I0312 22:57:28.410702 139865354262336 checkpoints.py:490] Saving checkpoint at step: 110446
I0312 22:57:32.408943 139865354262336 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax/trial_1/checkpoint_110446
I0312 22:57:32.413985 139865354262336 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_0/wmt_jax/trial_1/checkpoint_110446.
I0312 22:57:32.465999 139865354262336 submission_runner.py:683] Final wmt score: 39522.984773635864
