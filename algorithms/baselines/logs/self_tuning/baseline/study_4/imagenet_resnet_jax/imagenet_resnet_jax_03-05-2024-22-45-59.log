python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_4 --overwrite=true --save_checkpoints=false --rng_seed=3347380310 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_resnet_jax_03-05-2024-22-45-59.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0305 22:46:20.632915 139951089751872 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax.
I0305 22:46:21.737919 139951089751872 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0305 22:46:21.738990 139951089751872 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0305 22:46:21.739125 139951089751872 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0305 22:46:22.598568 139951089751872 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax/trial_1.
I0305 22:46:22.801538 139951089751872 submission_runner.py:206] Initializing dataset.
I0305 22:46:22.817887 139951089751872 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0305 22:46:22.827924 139951089751872 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0305 22:46:23.206827 139951089751872 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0305 22:46:24.377452 139951089751872 submission_runner.py:213] Initializing model.
I0305 22:46:34.550117 139951089751872 submission_runner.py:255] Initializing optimizer.
I0305 22:46:36.229180 139951089751872 submission_runner.py:262] Initializing metrics bundle.
I0305 22:46:36.229364 139951089751872 submission_runner.py:280] Initializing checkpoint and logger.
I0305 22:46:36.230098 139951089751872 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0305 22:46:36.230237 139951089751872 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0305 22:46:36.512982 139951089751872 logger_utils.py:220] Unable to record git information. Continuing without it.
I0305 22:46:36.778399 139951089751872 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax/trial_1/flags_0.json.
I0305 22:46:36.787617 139951089751872 submission_runner.py:314] Starting training loop.
I0305 22:47:26.325432 139787541403392 logging_writer.py:48] [0] global_step=0, grad_norm=0.6668112874031067, loss=6.91577672958374
I0305 22:47:26.342567 139951089751872 spec.py:321] Evaluating on the training split.
I0305 22:47:27.491302 139951089751872 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0305 22:47:27.500398 139951089751872 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0305 22:47:27.586055 139951089751872 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0305 22:47:40.741312 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 22:47:42.178854 139951089751872 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0305 22:47:42.192604 139951089751872 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0305 22:47:42.264955 139951089751872 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0305 22:47:58.065973 139951089751872 spec.py:349] Evaluating on the test split.
I0305 22:47:58.899167 139951089751872 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0305 22:47:58.905220 139951089751872 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0305 22:47:58.946515 139951089751872 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0305 22:48:02.930425 139951089751872 submission_runner.py:411] Time since start: 86.14s, 	Step: 1, 	{'train/accuracy': 0.0008968430920504034, 'train/loss': 6.913041591644287, 'validation/accuracy': 0.0010999999940395355, 'validation/loss': 6.912304878234863, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911952018737793, 'test/num_examples': 10000, 'score': 49.55485677719116, 'total_duration': 86.1427435874939, 'accumulated_submission_time': 49.55485677719116, 'accumulated_eval_time': 36.58779454231262, 'accumulated_logging_time': 0}
I0305 22:48:02.947170 139766062368512 logging_writer.py:48] [1] accumulated_eval_time=36.587795, accumulated_logging_time=0, accumulated_submission_time=49.554857, global_step=1, preemption_count=0, score=49.554857, test/accuracy=0.001100, test/loss=6.911952, test/num_examples=10000, total_duration=86.142744, train/accuracy=0.000897, train/loss=6.913042, validation/accuracy=0.001100, validation/loss=6.912305, validation/num_examples=50000
I0305 22:48:36.546925 139766053975808 logging_writer.py:48] [100] global_step=100, grad_norm=0.6799252033233643, loss=6.8172078132629395
I0305 22:49:10.246351 139766062368512 logging_writer.py:48] [200] global_step=200, grad_norm=0.8022287487983704, loss=6.524394512176514
I0305 22:49:43.937376 139766053975808 logging_writer.py:48] [300] global_step=300, grad_norm=0.9069234132766724, loss=6.2657575607299805
I0305 22:50:17.697608 139766062368512 logging_writer.py:48] [400] global_step=400, grad_norm=1.4112071990966797, loss=6.002124786376953
I0305 22:50:51.482131 139766053975808 logging_writer.py:48] [500] global_step=500, grad_norm=2.1854312419891357, loss=5.771934509277344
I0305 22:51:25.254059 139766062368512 logging_writer.py:48] [600] global_step=600, grad_norm=2.4046387672424316, loss=5.558484077453613
I0305 22:51:59.036895 139766053975808 logging_writer.py:48] [700] global_step=700, grad_norm=3.5686607360839844, loss=5.374948501586914
I0305 22:52:32.829587 139766062368512 logging_writer.py:48] [800] global_step=800, grad_norm=5.047512531280518, loss=5.243262767791748
I0305 22:53:06.618836 139766053975808 logging_writer.py:48] [900] global_step=900, grad_norm=3.5679996013641357, loss=5.195782661437988
I0305 22:53:40.398061 139766062368512 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.771814823150635, loss=4.960468292236328
I0305 22:54:14.108621 139766053975808 logging_writer.py:48] [1100] global_step=1100, grad_norm=7.645757675170898, loss=5.027572154998779
I0305 22:54:47.915344 139766062368512 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.496709823608398, loss=4.7483930587768555
I0305 22:55:21.679025 139766053975808 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.441456317901611, loss=4.646977424621582
I0305 22:55:55.489284 139766062368512 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.5926852226257324, loss=4.686279296875
I0305 22:56:29.304233 139766053975808 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.763056516647339, loss=4.54282808303833
I0305 22:56:33.104825 139951089751872 spec.py:321] Evaluating on the training split.
I0305 22:56:40.310281 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 22:56:48.656073 139951089751872 spec.py:349] Evaluating on the test split.
I0305 22:56:51.009985 139951089751872 submission_runner.py:411] Time since start: 614.22s, 	Step: 1513, 	{'train/accuracy': 0.169782355427742, 'train/loss': 4.28282356262207, 'validation/accuracy': 0.15041999518871307, 'validation/loss': 4.425992488861084, 'validation/num_examples': 50000, 'test/accuracy': 0.11110000312328339, 'test/loss': 4.882546424865723, 'test/num_examples': 10000, 'score': 559.6508867740631, 'total_duration': 614.2222878932953, 'accumulated_submission_time': 559.6508867740631, 'accumulated_eval_time': 54.492894887924194, 'accumulated_logging_time': 0.025151729583740234}
I0305 22:56:51.029670 139766070761216 logging_writer.py:48] [1513] accumulated_eval_time=54.492895, accumulated_logging_time=0.025152, accumulated_submission_time=559.650887, global_step=1513, preemption_count=0, score=559.650887, test/accuracy=0.111100, test/loss=4.882546, test/num_examples=10000, total_duration=614.222288, train/accuracy=0.169782, train/loss=4.282824, validation/accuracy=0.150420, validation/loss=4.425992, validation/num_examples=50000
I0305 22:57:20.790872 139766079153920 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.732925891876221, loss=4.446077346801758
I0305 22:57:54.558582 139766070761216 logging_writer.py:48] [1700] global_step=1700, grad_norm=7.474422931671143, loss=4.290468215942383
I0305 22:58:28.338727 139766079153920 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.332449436187744, loss=4.323224067687988
I0305 22:59:02.128216 139766070761216 logging_writer.py:48] [1900] global_step=1900, grad_norm=5.218601226806641, loss=4.142845153808594
I0305 22:59:36.018023 139766079153920 logging_writer.py:48] [2000] global_step=2000, grad_norm=10.894929885864258, loss=4.11348295211792
I0305 23:00:09.786244 139766070761216 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.404383659362793, loss=3.9140069484710693
I0305 23:00:43.567614 139766079153920 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.3156578540802, loss=3.8448686599731445
I0305 23:01:17.311625 139766070761216 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.517317295074463, loss=3.93976092338562
I0305 23:01:51.072053 139766079153920 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.618124008178711, loss=3.860882043838501
I0305 23:02:24.834455 139766070761216 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.582026481628418, loss=3.7191824913024902
I0305 23:02:58.582847 139766079153920 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.201915740966797, loss=3.5607874393463135
I0305 23:03:32.343459 139766070761216 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.9575588703155518, loss=3.5690078735351562
I0305 23:04:06.091066 139766079153920 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.9765331745147705, loss=3.583214282989502
I0305 23:04:39.836563 139766070761216 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.694415330886841, loss=3.4705607891082764
I0305 23:05:13.581426 139766079153920 logging_writer.py:48] [3000] global_step=3000, grad_norm=6.841592311859131, loss=3.380422592163086
I0305 23:05:21.153888 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:05:28.515844 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:05:36.880348 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:05:39.170358 139951089751872 submission_runner.py:411] Time since start: 1142.38s, 	Step: 3024, 	{'train/accuracy': 0.33428332209587097, 'train/loss': 3.0926454067230225, 'validation/accuracy': 0.3082599937915802, 'validation/loss': 3.276564359664917, 'validation/num_examples': 50000, 'test/accuracy': 0.23360000550746918, 'test/loss': 3.9567458629608154, 'test/num_examples': 10000, 'score': 1069.7121422290802, 'total_duration': 1142.3826808929443, 'accumulated_submission_time': 1069.7121422290802, 'accumulated_eval_time': 72.50932359695435, 'accumulated_logging_time': 0.05398416519165039}
I0305 23:05:39.186967 139788032145152 logging_writer.py:48] [3024] accumulated_eval_time=72.509324, accumulated_logging_time=0.053984, accumulated_submission_time=1069.712142, global_step=3024, preemption_count=0, score=1069.712142, test/accuracy=0.233600, test/loss=3.956746, test/num_examples=10000, total_duration=1142.382681, train/accuracy=0.334283, train/loss=3.092645, validation/accuracy=0.308260, validation/loss=3.276564, validation/num_examples=50000
I0305 23:06:05.176295 139788116006656 logging_writer.py:48] [3100] global_step=3100, grad_norm=7.291007995605469, loss=3.480152130126953
I0305 23:06:38.955587 139788032145152 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.9174110889434814, loss=3.332648754119873
I0305 23:07:12.662889 139788116006656 logging_writer.py:48] [3300] global_step=3300, grad_norm=6.3364362716674805, loss=3.2767510414123535
I0305 23:07:46.397829 139788032145152 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.5249433517456055, loss=3.2839717864990234
I0305 23:08:20.056006 139788116006656 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.604926824569702, loss=3.161642074584961
I0305 23:08:53.753419 139788032145152 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.791529655456543, loss=3.1550614833831787
I0305 23:09:27.468683 139788116006656 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.251210927963257, loss=3.1007871627807617
I0305 23:10:01.229168 139788032145152 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.9023046493530273, loss=3.0339765548706055
I0305 23:10:34.926093 139788116006656 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.155221939086914, loss=3.0064122676849365
I0305 23:11:08.709791 139788032145152 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.8478801250457764, loss=3.0676279067993164
I0305 23:11:42.439305 139788116006656 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.929701328277588, loss=2.938626289367676
I0305 23:12:16.151766 139788032145152 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.412095069885254, loss=2.8662290573120117
I0305 23:12:49.876961 139788116006656 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.853321075439453, loss=3.006706714630127
I0305 23:13:23.567252 139788032145152 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.3938660621643066, loss=2.909255027770996
I0305 23:13:57.298835 139788116006656 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.417107105255127, loss=2.8512589931488037
I0305 23:14:09.191113 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:14:16.517688 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:14:24.971128 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:14:27.252303 139951089751872 submission_runner.py:411] Time since start: 1670.46s, 	Step: 4537, 	{'train/accuracy': 0.45352357625961304, 'train/loss': 2.3977432250976562, 'validation/accuracy': 0.42075997591018677, 'validation/loss': 2.597283363342285, 'validation/num_examples': 50000, 'test/accuracy': 0.3188000023365021, 'test/loss': 3.3225812911987305, 'test/num_examples': 10000, 'score': 1579.652539730072, 'total_duration': 1670.4646260738373, 'accumulated_submission_time': 1579.652539730072, 'accumulated_eval_time': 90.5704882144928, 'accumulated_logging_time': 0.08067846298217773}
I0305 23:14:27.269137 139787998574336 logging_writer.py:48] [4537] accumulated_eval_time=90.570488, accumulated_logging_time=0.080678, accumulated_submission_time=1579.652540, global_step=4537, preemption_count=0, score=1579.652540, test/accuracy=0.318800, test/loss=3.322581, test/num_examples=10000, total_duration=1670.464626, train/accuracy=0.453524, train/loss=2.397743, validation/accuracy=0.420760, validation/loss=2.597283, validation/num_examples=50000
I0305 23:14:48.837477 139788006967040 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.1102523803710938, loss=2.9203402996063232
I0305 23:15:22.512732 139787998574336 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.1499879360198975, loss=2.664633274078369
I0305 23:15:56.147049 139788006967040 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.403630256652832, loss=2.696096897125244
I0305 23:16:29.827732 139787998574336 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.2313544750213623, loss=2.665339946746826
I0305 23:17:03.579201 139788006967040 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.292703151702881, loss=2.5670993328094482
I0305 23:17:37.338958 139787998574336 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.068887948989868, loss=2.719172716140747
I0305 23:18:11.012145 139788006967040 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.5230231285095215, loss=2.495941638946533
I0305 23:18:44.704107 139787998574336 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.83343768119812, loss=2.602485179901123
I0305 23:19:18.426949 139788006967040 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.052090167999268, loss=2.6410093307495117
I0305 23:19:52.083129 139787998574336 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.9529188871383667, loss=2.5673017501831055
I0305 23:20:25.758050 139788006967040 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.251945972442627, loss=2.5225510597229004
I0305 23:20:59.491353 139787998574336 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.305929183959961, loss=2.462538242340088
I0305 23:21:33.173531 139788006967040 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.12646484375, loss=2.3602957725524902
I0305 23:22:06.908764 139787998574336 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.368910074234009, loss=2.5464396476745605
I0305 23:22:40.566987 139788006967040 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.6624054908752441, loss=2.5582053661346436
I0305 23:22:57.498068 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:23:04.657301 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:23:13.238149 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:23:15.467211 139951089751872 submission_runner.py:411] Time since start: 2198.68s, 	Step: 6052, 	{'train/accuracy': 0.5290178656578064, 'train/loss': 2.0105631351470947, 'validation/accuracy': 0.49111998081207275, 'validation/loss': 2.2131128311157227, 'validation/num_examples': 50000, 'test/accuracy': 0.3797000050544739, 'test/loss': 2.925008773803711, 'test/num_examples': 10000, 'score': 2089.819321870804, 'total_duration': 2198.679522037506, 'accumulated_submission_time': 2089.819321870804, 'accumulated_eval_time': 108.53957986831665, 'accumulated_logging_time': 0.10659360885620117}
I0305 23:23:15.483751 139789542045440 logging_writer.py:48] [6052] accumulated_eval_time=108.539580, accumulated_logging_time=0.106594, accumulated_submission_time=2089.819322, global_step=6052, preemption_count=0, score=2089.819322, test/accuracy=0.379700, test/loss=2.925009, test/num_examples=10000, total_duration=2198.679522, train/accuracy=0.529018, train/loss=2.010563, validation/accuracy=0.491120, validation/loss=2.213113, validation/num_examples=50000
I0305 23:23:31.999110 139789550438144 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.1090667247772217, loss=2.4190115928649902
I0305 23:24:05.659489 139789542045440 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.3347816467285156, loss=2.415982961654663
I0305 23:24:39.315718 139789550438144 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.202364921569824, loss=2.298733711242676
I0305 23:25:13.016644 139789542045440 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.6427526473999023, loss=2.4672799110412598
I0305 23:25:46.656546 139789550438144 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.2308173179626465, loss=2.412574291229248
I0305 23:26:20.346906 139789542045440 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.9141690731048584, loss=2.3754496574401855
I0305 23:26:54.088920 139789550438144 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.5591979026794434, loss=2.294919013977051
I0305 23:27:27.804969 139789542045440 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.977879285812378, loss=2.450092077255249
I0305 23:28:01.495946 139789550438144 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.7914040088653564, loss=2.431110382080078
I0305 23:28:35.163504 139789542045440 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.5579378604888916, loss=2.3791675567626953
I0305 23:29:08.857156 139789550438144 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.54550838470459, loss=2.333282232284546
I0305 23:29:42.508322 139789542045440 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.708599090576172, loss=2.3054771423339844
I0305 23:30:16.195116 139789550438144 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.5543482303619385, loss=2.25749135017395
I0305 23:30:49.822721 139789542045440 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.906897783279419, loss=2.232459306716919
I0305 23:31:23.492883 139789550438144 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.0982444286346436, loss=2.1856305599212646
I0305 23:31:45.788558 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:31:52.961024 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:32:01.411913 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:32:03.730674 139951089751872 submission_runner.py:411] Time since start: 2726.94s, 	Step: 7568, 	{'train/accuracy': 0.5653499364852905, 'train/loss': 1.8284964561462402, 'validation/accuracy': 0.5282999873161316, 'validation/loss': 2.031683921813965, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.7944133281707764, 'test/num_examples': 10000, 'score': 2600.062970161438, 'total_duration': 2726.9429802894592, 'accumulated_submission_time': 2600.062970161438, 'accumulated_eval_time': 126.48164319992065, 'accumulated_logging_time': 0.13203048706054688}
I0305 23:32:03.748898 139789533652736 logging_writer.py:48] [7568] accumulated_eval_time=126.481643, accumulated_logging_time=0.132030, accumulated_submission_time=2600.062970, global_step=7568, preemption_count=0, score=2600.062970, test/accuracy=0.404700, test/loss=2.794413, test/num_examples=10000, total_duration=2726.942980, train/accuracy=0.565350, train/loss=1.828496, validation/accuracy=0.528300, validation/loss=2.031684, validation/num_examples=50000
I0305 23:32:14.863715 139789558830848 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.8570836782455444, loss=2.1960079669952393
I0305 23:32:48.499415 139789533652736 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.3946783542633057, loss=2.267563581466675
I0305 23:33:22.145531 139789558830848 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.9010062217712402, loss=2.1764116287231445
I0305 23:33:55.861848 139789533652736 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.2236502170562744, loss=2.246772527694702
I0305 23:34:29.524684 139789558830848 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.3417909145355225, loss=2.1116158962249756
I0305 23:35:03.182542 139789533652736 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.7547428607940674, loss=2.311584949493408
I0305 23:35:36.927957 139789558830848 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2662264108657837, loss=1.9668697118759155
I0305 23:36:10.602783 139789533652736 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.9721412658691406, loss=2.0911080837249756
I0305 23:36:44.279143 139789558830848 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.05718731880188, loss=2.2470414638519287
I0305 23:37:17.980411 139789533652736 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.6586331129074097, loss=2.235431671142578
I0305 23:37:51.646444 139789558830848 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.8006746768951416, loss=2.1644084453582764
I0305 23:38:25.299601 139789533652736 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.016350269317627, loss=2.234260082244873
I0305 23:38:58.976718 139789558830848 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.2632131576538086, loss=2.061893939971924
I0305 23:39:32.679026 139789533652736 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.6713907718658447, loss=2.0277812480926514
I0305 23:40:06.350957 139789558830848 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.8626872301101685, loss=2.125415802001953
I0305 23:40:34.046866 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:40:41.275253 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:40:49.588709 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:40:51.802431 139951089751872 submission_runner.py:411] Time since start: 3255.01s, 	Step: 9084, 	{'train/accuracy': 0.6339883208274841, 'train/loss': 1.5135927200317383, 'validation/accuracy': 0.5559399724006653, 'validation/loss': 1.890720248222351, 'validation/num_examples': 50000, 'test/accuracy': 0.4335000216960907, 'test/loss': 2.639951467514038, 'test/num_examples': 10000, 'score': 3110.2991478443146, 'total_duration': 3255.0147392749786, 'accumulated_submission_time': 3110.2991478443146, 'accumulated_eval_time': 144.2371587753296, 'accumulated_logging_time': 0.1595776081085205}
I0305 23:40:51.820074 139789542045440 logging_writer.py:48] [9084] accumulated_eval_time=144.237159, accumulated_logging_time=0.159578, accumulated_submission_time=3110.299148, global_step=9084, preemption_count=0, score=3110.299148, test/accuracy=0.433500, test/loss=2.639951, test/num_examples=10000, total_duration=3255.014739, train/accuracy=0.633988, train/loss=1.513593, validation/accuracy=0.555940, validation/loss=1.890720, validation/num_examples=50000
I0305 23:40:57.548796 139789550438144 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.262300729751587, loss=2.0744805335998535
I0305 23:41:31.267637 139789542045440 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.0713109970092773, loss=1.9791641235351562
I0305 23:42:04.898823 139789550438144 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.5496970415115356, loss=2.1568353176116943
I0305 23:42:38.542235 139789542045440 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.39400577545166, loss=2.1358792781829834
I0305 23:43:12.255665 139789550438144 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.8961260318756104, loss=1.9853066205978394
I0305 23:43:45.985155 139789542045440 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8664023876190186, loss=2.0948569774627686
I0305 23:44:19.622837 139789550438144 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.327799081802368, loss=2.0775582790374756
I0305 23:44:53.274507 139789542045440 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.781044840812683, loss=2.0321998596191406
I0305 23:45:26.937269 139789550438144 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6554772853851318, loss=2.0321059226989746
I0305 23:46:00.656395 139789542045440 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.640589714050293, loss=2.2310473918914795
I0305 23:46:34.297498 139789550438144 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.8376669883728027, loss=1.9483721256256104
I0305 23:47:07.961825 139789542045440 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.070261001586914, loss=2.119541883468628
I0305 23:47:41.612458 139789550438144 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.6778626441955566, loss=2.0272374153137207
I0305 23:48:15.329932 139789542045440 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.8824925422668457, loss=2.0297842025756836
I0305 23:48:48.977827 139789550438144 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.6526275873184204, loss=2.028308629989624
I0305 23:49:22.069295 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:49:29.258695 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:49:37.892395 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:49:40.171798 139951089751872 submission_runner.py:411] Time since start: 3783.38s, 	Step: 10600, 	{'train/accuracy': 0.6399075388908386, 'train/loss': 1.445544958114624, 'validation/accuracy': 0.5717999935150146, 'validation/loss': 1.8170902729034424, 'validation/num_examples': 50000, 'test/accuracy': 0.4480000138282776, 'test/loss': 2.5838935375213623, 'test/num_examples': 10000, 'score': 3620.4855065345764, 'total_duration': 3783.384105682373, 'accumulated_submission_time': 3620.4855065345764, 'accumulated_eval_time': 162.33961367607117, 'accumulated_logging_time': 0.1865520477294922}
I0305 23:49:40.199334 139789030385408 logging_writer.py:48] [10600] accumulated_eval_time=162.339614, accumulated_logging_time=0.186552, accumulated_submission_time=3620.485507, global_step=10600, preemption_count=0, score=3620.485507, test/accuracy=0.448000, test/loss=2.583894, test/num_examples=10000, total_duration=3783.384106, train/accuracy=0.639908, train/loss=1.445545, validation/accuracy=0.571800, validation/loss=1.817090, validation/num_examples=50000
I0305 23:49:40.560578 139789038778112 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.5975286960601807, loss=2.078235626220703
I0305 23:50:14.256519 139789030385408 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.5276670455932617, loss=2.1841514110565186
I0305 23:50:47.929384 139789038778112 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.4757548570632935, loss=1.9522359371185303
I0305 23:51:21.574407 139789030385408 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.8792803287506104, loss=1.904861330986023
I0305 23:51:55.228549 139789038778112 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9995406866073608, loss=1.9771239757537842
I0305 23:52:28.954733 139789030385408 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.8075242042541504, loss=1.9716311693191528
I0305 23:53:02.614370 139789038778112 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.8987727165222168, loss=2.0760445594787598
I0305 23:53:36.255435 139789030385408 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.409341812133789, loss=1.9654220342636108
I0305 23:54:10.034947 139789038778112 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.6042896509170532, loss=1.9911353588104248
I0305 23:54:43.721557 139789030385408 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.7817885875701904, loss=2.0179758071899414
I0305 23:55:17.373899 139789038778112 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.6297906637191772, loss=1.9423823356628418
I0305 23:55:51.050012 139789030385408 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.6181249618530273, loss=1.9344375133514404
I0305 23:56:24.693207 139789038778112 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.9074701070785522, loss=1.884061336517334
I0305 23:56:58.406014 139789030385408 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.7048102617263794, loss=2.015955686569214
I0305 23:57:32.056179 139789038778112 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.8096626996994019, loss=1.9541168212890625
I0305 23:58:05.703724 139789030385408 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.5890388488769531, loss=1.9911829233169556
I0305 23:58:10.181219 139951089751872 spec.py:321] Evaluating on the training split.
I0305 23:58:17.483551 139951089751872 spec.py:333] Evaluating on the validation split.
I0305 23:58:26.001394 139951089751872 spec.py:349] Evaluating on the test split.
I0305 23:58:28.286667 139951089751872 submission_runner.py:411] Time since start: 4311.50s, 	Step: 12115, 	{'train/accuracy': 0.6345862150192261, 'train/loss': 1.4771161079406738, 'validation/accuracy': 0.5790799856185913, 'validation/loss': 1.7761107683181763, 'validation/num_examples': 50000, 'test/accuracy': 0.45980003476142883, 'test/loss': 2.4967310428619385, 'test/num_examples': 10000, 'score': 4130.400605678558, 'total_duration': 4311.498994588852, 'accumulated_submission_time': 4130.400605678558, 'accumulated_eval_time': 180.44503736495972, 'accumulated_logging_time': 0.22774243354797363}
I0305 23:58:28.304508 139789550438144 logging_writer.py:48] [12115] accumulated_eval_time=180.445037, accumulated_logging_time=0.227742, accumulated_submission_time=4130.400606, global_step=12115, preemption_count=0, score=4130.400606, test/accuracy=0.459800, test/loss=2.496731, test/num_examples=10000, total_duration=4311.498995, train/accuracy=0.634586, train/loss=1.477116, validation/accuracy=0.579080, validation/loss=1.776111, validation/num_examples=50000
I0305 23:58:57.283902 139789558830848 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6412360668182373, loss=1.9905027151107788
I0305 23:59:30.941494 139789550438144 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.2899938821792603, loss=1.9649274349212646
I0306 00:00:04.578995 139789558830848 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.7826251983642578, loss=1.9767041206359863
I0306 00:00:38.253020 139789550438144 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.5792862176895142, loss=1.9498990774154663
I0306 00:01:11.940605 139789558830848 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.5333276987075806, loss=1.9769936800003052
I0306 00:01:45.658175 139789550438144 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.6856378316879272, loss=1.9985556602478027
I0306 00:02:19.308500 139789558830848 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.409611701965332, loss=1.967281699180603
I0306 00:02:52.974143 139789550438144 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6159917116165161, loss=1.9841464757919312
I0306 00:03:26.642417 139789558830848 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.0814201831817627, loss=1.9422407150268555
I0306 00:04:00.303621 139789550438144 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.9608056545257568, loss=2.013439178466797
I0306 00:04:33.976680 139789558830848 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.2954626083374023, loss=1.9071124792099
I0306 00:05:07.623079 139789550438144 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.491736888885498, loss=1.9662816524505615
I0306 00:05:41.244717 139789558830848 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.640659213066101, loss=1.9419608116149902
I0306 00:06:14.911673 139789550438144 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.5759028196334839, loss=1.9771060943603516
I0306 00:06:48.549916 139789558830848 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6271661520004272, loss=1.8176777362823486
I0306 00:06:58.416889 139951089751872 spec.py:321] Evaluating on the training split.
I0306 00:07:05.797448 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 00:07:15.365812 139951089751872 spec.py:349] Evaluating on the test split.
I0306 00:07:17.658212 139951089751872 submission_runner.py:411] Time since start: 4840.87s, 	Step: 13631, 	{'train/accuracy': 0.6399673223495483, 'train/loss': 1.4503324031829834, 'validation/accuracy': 0.5899400115013123, 'validation/loss': 1.7300362586975098, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.5264737606048584, 'test/num_examples': 10000, 'score': 4640.448156118393, 'total_duration': 4840.869259595871, 'accumulated_submission_time': 4640.448156118393, 'accumulated_eval_time': 199.68504357337952, 'accumulated_logging_time': 0.2561922073364258}
I0306 00:07:17.678309 139789021992704 logging_writer.py:48] [13631] accumulated_eval_time=199.685044, accumulated_logging_time=0.256192, accumulated_submission_time=4640.448156, global_step=13631, preemption_count=0, score=4640.448156, test/accuracy=0.456900, test/loss=2.526474, test/num_examples=10000, total_duration=4840.869260, train/accuracy=0.639967, train/loss=1.450332, validation/accuracy=0.589940, validation/loss=1.730036, validation/num_examples=50000
I0306 00:07:41.225138 139789030385408 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.8010550737380981, loss=1.8433254957199097
I0306 00:08:14.928790 139789021992704 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.4145597219467163, loss=1.9904522895812988
I0306 00:08:48.568253 139789030385408 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4619327783584595, loss=1.8514771461486816
I0306 00:09:22.260052 139789021992704 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6815977096557617, loss=2.0132193565368652
I0306 00:09:55.886131 139789030385408 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.489401936531067, loss=1.973745584487915
I0306 00:10:29.549542 139789021992704 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.9107156991958618, loss=2.0285890102386475
I0306 00:11:03.186246 139789030385408 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.0515735149383545, loss=1.9282236099243164
I0306 00:11:36.860871 139789021992704 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.4327805042266846, loss=1.9628949165344238
I0306 00:12:10.561703 139789030385408 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.7654634714126587, loss=1.931032657623291
I0306 00:12:44.254250 139789021992704 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.5753908157348633, loss=1.9669498205184937
I0306 00:13:17.875277 139789030385408 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.0654871463775635, loss=1.9859991073608398
I0306 00:13:51.531756 139789021992704 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.668170690536499, loss=1.91655695438385
I0306 00:14:25.155098 139789030385408 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.8525400161743164, loss=1.7570326328277588
I0306 00:14:58.791124 139789021992704 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.712815284729004, loss=1.920650601387024
I0306 00:15:32.403414 139789030385408 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.164269208908081, loss=1.9372872114181519
I0306 00:15:47.973673 139951089751872 spec.py:321] Evaluating on the training split.
I0306 00:15:55.968253 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 00:16:06.519912 139951089751872 spec.py:349] Evaluating on the test split.
I0306 00:16:08.790750 139951089751872 submission_runner.py:411] Time since start: 5372.00s, 	Step: 15148, 	{'train/accuracy': 0.6467235088348389, 'train/loss': 1.4315388202667236, 'validation/accuracy': 0.5961399674415588, 'validation/loss': 1.6884781122207642, 'validation/num_examples': 50000, 'test/accuracy': 0.4661000072956085, 'test/loss': 2.474865436553955, 'test/num_examples': 10000, 'score': 5150.681735038757, 'total_duration': 5372.002974510193, 'accumulated_submission_time': 5150.681735038757, 'accumulated_eval_time': 220.5019817352295, 'accumulated_logging_time': 0.2849607467651367}
I0306 00:16:08.822840 139789030385408 logging_writer.py:48] [15148] accumulated_eval_time=220.501982, accumulated_logging_time=0.284961, accumulated_submission_time=5150.681735, global_step=15148, preemption_count=0, score=5150.681735, test/accuracy=0.466100, test/loss=2.474865, test/num_examples=10000, total_duration=5372.002975, train/accuracy=0.646724, train/loss=1.431539, validation/accuracy=0.596140, validation/loss=1.688478, validation/num_examples=50000
I0306 00:16:26.637910 139789038778112 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.661861538887024, loss=1.9629528522491455
I0306 00:17:00.260090 139789030385408 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.922724723815918, loss=2.002335548400879
I0306 00:17:33.899966 139789038778112 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.9158835411071777, loss=1.9019349813461304
I0306 00:18:07.546733 139789030385408 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.4703104496002197, loss=1.9400067329406738
I0306 00:18:41.245257 139789038778112 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.2237768173217773, loss=1.9370187520980835
I0306 00:19:14.885175 139789030385408 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.6459580659866333, loss=1.8437726497650146
I0306 00:19:48.532175 139789038778112 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.5127031803131104, loss=2.0017614364624023
I0306 00:20:22.177366 139789030385408 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.6495921611785889, loss=1.847139596939087
I0306 00:20:55.891092 139789038778112 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.1082072257995605, loss=1.9315788745880127
I0306 00:21:29.542297 139789030385408 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.656378984451294, loss=1.8981826305389404
I0306 00:22:03.201185 139789038778112 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.7143049240112305, loss=1.958441972732544
I0306 00:22:36.928876 139789030385408 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.8262478113174438, loss=1.882071852684021
I0306 00:23:10.593649 139789038778112 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.5918139219284058, loss=1.860875129699707
I0306 00:23:44.184331 139789030385408 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.7118932008743286, loss=1.9893014430999756
I0306 00:24:17.807479 139789038778112 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.5762381553649902, loss=1.85965096950531
I0306 00:24:39.096006 139951089751872 spec.py:321] Evaluating on the training split.
I0306 00:24:46.684002 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 00:24:57.747037 139951089751872 spec.py:349] Evaluating on the test split.
I0306 00:24:59.966797 139951089751872 submission_runner.py:411] Time since start: 5903.18s, 	Step: 16665, 	{'train/accuracy': 0.6599370241165161, 'train/loss': 1.3714268207550049, 'validation/accuracy': 0.6070199608802795, 'validation/loss': 1.642867922782898, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3955681324005127, 'test/num_examples': 10000, 'score': 5660.8894102573395, 'total_duration': 5903.179100036621, 'accumulated_submission_time': 5660.8894102573395, 'accumulated_eval_time': 241.37271213531494, 'accumulated_logging_time': 0.3264124393463135}
I0306 00:24:59.990825 139789021992704 logging_writer.py:48] [16665] accumulated_eval_time=241.372712, accumulated_logging_time=0.326412, accumulated_submission_time=5660.889410, global_step=16665, preemption_count=0, score=5660.889410, test/accuracy=0.482600, test/loss=2.395568, test/num_examples=10000, total_duration=5903.179100, train/accuracy=0.659937, train/loss=1.371427, validation/accuracy=0.607020, validation/loss=1.642868, validation/num_examples=50000
I0306 00:25:12.118296 139789030385408 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.8068116903305054, loss=1.9257783889770508
I0306 00:25:45.764529 139789021992704 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.8831435441970825, loss=1.9248067140579224
I0306 00:26:19.407806 139789030385408 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.5035481452941895, loss=1.797444462776184
I0306 00:26:53.049027 139789021992704 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.4582890272140503, loss=1.9002140760421753
I0306 00:27:26.738756 139789030385408 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.7305771112442017, loss=1.729035496711731
I0306 00:28:00.377565 139789021992704 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.7441052198410034, loss=1.8751031160354614
I0306 00:28:34.039458 139789030385408 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.7912793159484863, loss=1.8899773359298706
I0306 00:29:07.674360 139789021992704 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.7563343048095703, loss=1.908595085144043
I0306 00:29:41.341142 139789030385408 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6981890201568604, loss=1.7560899257659912
I0306 00:30:14.971116 139789021992704 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.1761250495910645, loss=1.9337615966796875
I0306 00:30:48.627390 139789030385408 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.331173300743103, loss=1.8161407709121704
I0306 00:31:22.234791 139789021992704 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.739378571510315, loss=1.8166768550872803
I0306 00:31:55.936792 139789030385408 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.7217670679092407, loss=1.795610785484314
I0306 00:32:29.585556 139789021992704 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.7086752653121948, loss=1.8864752054214478
I0306 00:33:03.242384 139789030385408 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.9022802114486694, loss=1.884318470954895
I0306 00:33:30.250522 139951089751872 spec.py:321] Evaluating on the training split.
I0306 00:33:38.576924 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 00:33:49.545633 139951089751872 spec.py:349] Evaluating on the test split.
I0306 00:33:51.783816 139951089751872 submission_runner.py:411] Time since start: 6435.00s, 	Step: 18182, 	{'train/accuracy': 0.7046794891357422, 'train/loss': 1.1431324481964111, 'validation/accuracy': 0.6042199730873108, 'validation/loss': 1.6550146341323853, 'validation/num_examples': 50000, 'test/accuracy': 0.4863000214099884, 'test/loss': 2.3672258853912354, 'test/num_examples': 10000, 'score': 6171.083922147751, 'total_duration': 6434.996114492416, 'accumulated_submission_time': 6171.083922147751, 'accumulated_eval_time': 262.90595984458923, 'accumulated_logging_time': 0.3627340793609619}
I0306 00:33:51.817732 139789038778112 logging_writer.py:48] [18182] accumulated_eval_time=262.905960, accumulated_logging_time=0.362734, accumulated_submission_time=6171.083922, global_step=18182, preemption_count=0, score=6171.083922, test/accuracy=0.486300, test/loss=2.367226, test/num_examples=10000, total_duration=6434.996114, train/accuracy=0.704679, train/loss=1.143132, validation/accuracy=0.604220, validation/loss=1.655015, validation/num_examples=50000
I0306 00:33:58.232066 139789533652736 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.168553352355957, loss=1.837565302848816
I0306 00:34:31.935634 139789038778112 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.5566706657409668, loss=1.8875104188919067
I0306 00:35:05.621618 139789533652736 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.594387412071228, loss=1.8830671310424805
I0306 00:35:39.240600 139789038778112 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.4630179405212402, loss=1.8225035667419434
I0306 00:36:12.920457 139789533652736 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.2398669719696045, loss=1.8733986616134644
I0306 00:36:46.568467 139789038778112 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.71567702293396, loss=1.762689232826233
I0306 00:37:20.198533 139789533652736 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.3673627376556396, loss=1.6936222314834595
I0306 00:37:53.885350 139789038778112 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.5551040172576904, loss=1.839162826538086
I0306 00:38:27.545075 139789533652736 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.6974451541900635, loss=1.8157148361206055
I0306 00:39:01.169531 139789038778112 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5422699451446533, loss=1.7509067058563232
I0306 00:39:34.805782 139789533652736 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.8438836336135864, loss=1.8535032272338867
I0306 00:40:08.443711 139789038778112 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.874498724937439, loss=1.8906437158584595
I0306 00:40:42.100623 139789533652736 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.4420019388198853, loss=1.767680287361145
I0306 00:41:15.746444 139789038778112 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.3658883571624756, loss=1.6310805082321167
I0306 00:41:49.470772 139789533652736 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.6789970397949219, loss=1.7872475385665894
I0306 00:42:21.862543 139951089751872 spec.py:321] Evaluating on the training split.
I0306 00:42:29.481509 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 00:42:39.316791 139951089751872 spec.py:349] Evaluating on the test split.
I0306 00:42:41.556913 139951089751872 submission_runner.py:411] Time since start: 6964.77s, 	Step: 19698, 	{'train/accuracy': 0.6736288070678711, 'train/loss': 1.300462245941162, 'validation/accuracy': 0.6003400087356567, 'validation/loss': 1.6817909479141235, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.4377734661102295, 'test/num_examples': 10000, 'score': 6681.065488100052, 'total_duration': 6964.769221544266, 'accumulated_submission_time': 6681.065488100052, 'accumulated_eval_time': 282.60028171539307, 'accumulated_logging_time': 0.40625596046447754}
I0306 00:42:41.577589 139789030385408 logging_writer.py:48] [19698] accumulated_eval_time=282.600282, accumulated_logging_time=0.406256, accumulated_submission_time=6681.065488, global_step=19698, preemption_count=0, score=6681.065488, test/accuracy=0.477200, test/loss=2.437773, test/num_examples=10000, total_duration=6964.769222, train/accuracy=0.673629, train/loss=1.300462, validation/accuracy=0.600340, validation/loss=1.681791, validation/num_examples=50000
I0306 00:42:42.597636 139789038778112 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.552571415901184, loss=1.7895907163619995
I0306 00:43:16.249139 139789030385408 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.7353644371032715, loss=1.8293254375457764
I0306 00:43:49.875706 139789038778112 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5404640436172485, loss=1.9306035041809082
I0306 00:44:23.517570 139789030385408 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.8702930212020874, loss=1.9130253791809082
I0306 00:44:57.172759 139789038778112 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.7553585767745972, loss=1.7942469120025635
I0306 00:45:30.832176 139789030385408 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.8241689205169678, loss=1.9434585571289062
I0306 00:46:04.468136 139789038778112 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6809436082839966, loss=1.907547950744629
I0306 00:46:38.089897 139789030385408 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4831297397613525, loss=1.8543713092803955
I0306 00:47:11.721381 139789038778112 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.5255461931228638, loss=1.7199934720993042
I0306 00:47:45.383404 139789030385408 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.7072787284851074, loss=1.6537706851959229
I0306 00:48:19.028640 139789038778112 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.7478512525558472, loss=1.9095782041549683
I0306 00:48:52.670414 139789030385408 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.9298248291015625, loss=1.7724003791809082
I0306 00:49:26.446225 139789038778112 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.7037891149520874, loss=1.7932462692260742
I0306 00:50:00.087938 139789030385408 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.7388955354690552, loss=1.8992639780044556
I0306 00:50:33.732223 139789038778112 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.8335214853286743, loss=1.7731165885925293
I0306 00:51:07.384632 139789030385408 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.1350784301757812, loss=1.9859144687652588
I0306 00:51:11.864216 139951089751872 spec.py:321] Evaluating on the training split.
I0306 00:51:20.017306 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 00:51:28.945423 139951089751872 spec.py:349] Evaluating on the test split.
I0306 00:51:31.203257 139951089751872 submission_runner.py:411] Time since start: 7494.42s, 	Step: 21215, 	{'train/accuracy': 0.6816206574440002, 'train/loss': 1.2650530338287354, 'validation/accuracy': 0.6165199875831604, 'validation/loss': 1.6044803857803345, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.3720510005950928, 'test/num_examples': 10000, 'score': 7191.288413763046, 'total_duration': 7494.415567398071, 'accumulated_submission_time': 7191.288413763046, 'accumulated_eval_time': 301.9392716884613, 'accumulated_logging_time': 0.43721628189086914}
I0306 00:51:31.233386 139789038778112 logging_writer.py:48] [21215] accumulated_eval_time=301.939272, accumulated_logging_time=0.437216, accumulated_submission_time=7191.288414, global_step=21215, preemption_count=0, score=7191.288414, test/accuracy=0.489100, test/loss=2.372051, test/num_examples=10000, total_duration=7494.415567, train/accuracy=0.681621, train/loss=1.265053, validation/accuracy=0.616520, validation/loss=1.604480, validation/num_examples=50000
I0306 00:52:00.169399 139789533652736 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.8037291765213013, loss=1.7867738008499146
I0306 00:52:33.803884 139789038778112 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6873112916946411, loss=1.7875111103057861
I0306 00:53:07.477545 139789533652736 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.4981557130813599, loss=1.8735533952713013
I0306 00:53:41.159679 139789038778112 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.5798579454421997, loss=1.69090735912323
I0306 00:54:14.852646 139789533652736 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.740775227546692, loss=1.849589467048645
I0306 00:54:48.476551 139789038778112 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.5930144786834717, loss=1.791057825088501
I0306 00:55:22.173853 139789533652736 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.799545407295227, loss=1.8348608016967773
I0306 00:55:55.808560 139789038778112 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5722520351409912, loss=1.689974069595337
I0306 00:56:29.506456 139789533652736 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.7708383798599243, loss=1.8663848638534546
I0306 00:57:03.145947 139789038778112 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.8445806503295898, loss=1.7841823101043701
I0306 00:57:36.800075 139789533652736 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.644851565361023, loss=1.711870789527893
I0306 00:58:10.410741 139789038778112 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.7624073028564453, loss=1.7783424854278564
I0306 00:58:44.058711 139789533652736 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.6762546300888062, loss=1.845314621925354
I0306 00:59:17.673713 139789038778112 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.5040614604949951, loss=1.7961221933364868
I0306 00:59:51.333747 139789533652736 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.458536982536316, loss=1.6476328372955322
I0306 01:00:01.540847 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:00:09.242796 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:00:19.273018 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:00:21.503168 139951089751872 submission_runner.py:411] Time since start: 8024.72s, 	Step: 22732, 	{'train/accuracy': 0.6628069281578064, 'train/loss': 1.3463976383209229, 'validation/accuracy': 0.6113799810409546, 'validation/loss': 1.631981372833252, 'validation/num_examples': 50000, 'test/accuracy': 0.4749000370502472, 'test/loss': 2.3722574710845947, 'test/num_examples': 10000, 'score': 7701.532709360123, 'total_duration': 8024.71547794342, 'accumulated_submission_time': 7701.532709360123, 'accumulated_eval_time': 321.9015429019928, 'accumulated_logging_time': 0.4763326644897461}
I0306 01:00:21.524487 139789030385408 logging_writer.py:48] [22732] accumulated_eval_time=321.901543, accumulated_logging_time=0.476333, accumulated_submission_time=7701.532709, global_step=22732, preemption_count=0, score=7701.532709, test/accuracy=0.474900, test/loss=2.372257, test/num_examples=10000, total_duration=8024.715478, train/accuracy=0.662807, train/loss=1.346398, validation/accuracy=0.611380, validation/loss=1.631981, validation/num_examples=50000
I0306 01:00:44.779399 139789038778112 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.5951039791107178, loss=1.7358943223953247
I0306 01:01:18.415165 139789030385408 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.6795032024383545, loss=1.878616452217102
I0306 01:01:52.083928 139789038778112 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.5000479221343994, loss=1.650175929069519
I0306 01:02:25.726201 139789030385408 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7797884941101074, loss=1.6622450351715088
I0306 01:02:59.391327 139789038778112 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.5585105419158936, loss=1.8160099983215332
I0306 01:03:33.024579 139789030385408 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.8020380735397339, loss=1.823674201965332
I0306 01:04:06.682652 139789038778112 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.9586577415466309, loss=1.7402273416519165
I0306 01:04:40.314240 139789030385408 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.7352689504623413, loss=1.8305702209472656
I0306 01:05:14.040982 139789038778112 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.509278416633606, loss=1.8411080837249756
I0306 01:05:47.683312 139789030385408 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.9701844453811646, loss=1.8052397966384888
I0306 01:06:21.354564 139789038778112 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.6783277988433838, loss=1.7495918273925781
I0306 01:06:54.970174 139789030385408 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.7309441566467285, loss=1.7695279121398926
I0306 01:07:28.655941 139789038778112 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.8070642948150635, loss=1.7321099042892456
I0306 01:08:02.262755 139789030385408 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.7333848476409912, loss=1.8103907108306885
I0306 01:08:35.918186 139789038778112 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.6317321062088013, loss=1.780823826789856
I0306 01:08:51.843039 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:08:59.355255 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:09:11.844905 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:09:14.053086 139951089751872 submission_runner.py:411] Time since start: 8557.27s, 	Step: 24249, 	{'train/accuracy': 0.6758809089660645, 'train/loss': 1.2752517461776733, 'validation/accuracy': 0.6176199913024902, 'validation/loss': 1.597264289855957, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.328645706176758, 'test/num_examples': 10000, 'score': 8211.788202762604, 'total_duration': 8557.265342473984, 'accumulated_submission_time': 8211.788202762604, 'accumulated_eval_time': 344.1114830970764, 'accumulated_logging_time': 0.5074851512908936}
I0306 01:09:14.070026 139789021992704 logging_writer.py:48] [24249] accumulated_eval_time=344.111483, accumulated_logging_time=0.507485, accumulated_submission_time=8211.788203, global_step=24249, preemption_count=0, score=8211.788203, test/accuracy=0.492800, test/loss=2.328646, test/num_examples=10000, total_duration=8557.265342, train/accuracy=0.675881, train/loss=1.275252, validation/accuracy=0.617620, validation/loss=1.597264, validation/num_examples=50000
I0306 01:09:31.545136 139789030385408 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.9605523347854614, loss=1.894160509109497
I0306 01:10:05.207489 139789021992704 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.6727972030639648, loss=1.783639907836914
I0306 01:10:38.861763 139789030385408 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6013270616531372, loss=1.625827431678772
I0306 01:11:12.507718 139789021992704 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.8597689867019653, loss=1.9223556518554688
I0306 01:11:46.156646 139789030385408 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.9313058853149414, loss=1.8695645332336426
I0306 01:12:19.826513 139789021992704 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.7088035345077515, loss=1.7302247285842896
I0306 01:12:53.461287 139789030385408 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.7697139978408813, loss=1.8808090686798096
I0306 01:13:27.118458 139789021992704 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.314828872680664, loss=1.6965627670288086
I0306 01:14:00.926413 139789030385408 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.387714147567749, loss=1.7573422193527222
I0306 01:14:34.567044 139789021992704 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.653671383857727, loss=1.6571120023727417
I0306 01:15:08.213560 139789030385408 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.660248041152954, loss=1.7651467323303223
I0306 01:15:41.847466 139789021992704 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8510048389434814, loss=1.7933673858642578
I0306 01:16:15.490703 139789030385408 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.710627555847168, loss=1.8326802253723145
I0306 01:16:49.165958 139789021992704 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.8263229131698608, loss=1.7653930187225342
I0306 01:17:22.817576 139789030385408 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.6942604780197144, loss=1.6254469156265259
I0306 01:17:44.140612 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:17:51.790156 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:18:04.587384 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:18:06.870521 139951089751872 submission_runner.py:411] Time since start: 9090.08s, 	Step: 25765, 	{'train/accuracy': 0.6642617583274841, 'train/loss': 1.3381074666976929, 'validation/accuracy': 0.6139199733734131, 'validation/loss': 1.6178455352783203, 'validation/num_examples': 50000, 'test/accuracy': 0.4853000342845917, 'test/loss': 2.364208459854126, 'test/num_examples': 10000, 'score': 8721.796803951263, 'total_duration': 9090.08283829689, 'accumulated_submission_time': 8721.796803951263, 'accumulated_eval_time': 366.8413505554199, 'accumulated_logging_time': 0.5325994491577148}
I0306 01:18:06.891561 139789558830848 logging_writer.py:48] [25765] accumulated_eval_time=366.841351, accumulated_logging_time=0.532599, accumulated_submission_time=8721.796804, global_step=25765, preemption_count=0, score=8721.796804, test/accuracy=0.485300, test/loss=2.364208, test/num_examples=10000, total_duration=9090.082838, train/accuracy=0.664262, train/loss=1.338107, validation/accuracy=0.613920, validation/loss=1.617846, validation/num_examples=50000
I0306 01:18:18.970405 139789567223552 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.586592197418213, loss=1.7541472911834717
I0306 01:18:52.576872 139789558830848 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.577494740486145, loss=1.819341778755188
I0306 01:19:26.199621 139789567223552 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.5340089797973633, loss=1.7202084064483643
I0306 01:19:59.891967 139789558830848 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7130564451217651, loss=1.7761238813400269
I0306 01:20:33.523890 139789567223552 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6539956331253052, loss=1.6798901557922363
I0306 01:21:07.154636 139789558830848 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7858293056488037, loss=1.8012104034423828
I0306 01:21:40.793421 139789567223552 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.6923171281814575, loss=1.6841926574707031
I0306 01:22:14.409769 139789558830848 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.775497555732727, loss=1.8123387098312378
I0306 01:22:48.096549 139789567223552 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.546648621559143, loss=1.7649160623550415
I0306 01:23:21.759841 139789558830848 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.9402462244033813, loss=1.8385820388793945
I0306 01:23:55.391887 139789567223552 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.8426785469055176, loss=1.7310820817947388
I0306 01:24:29.058411 139789558830848 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.69777512550354, loss=1.8098032474517822
I0306 01:25:02.694033 139789567223552 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.5840753316879272, loss=1.691616177558899
I0306 01:25:36.358082 139789558830848 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.7647181749343872, loss=1.7570457458496094
I0306 01:26:10.002277 139789567223552 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.5776922702789307, loss=1.671365737915039
I0306 01:26:36.881892 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:26:44.654910 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:26:58.928467 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:27:01.043663 139951089751872 submission_runner.py:411] Time since start: 9624.26s, 	Step: 27281, 	{'train/accuracy': 0.7101402878761292, 'train/loss': 1.1184210777282715, 'validation/accuracy': 0.6214799880981445, 'validation/loss': 1.5755999088287354, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.3477091789245605, 'test/num_examples': 10000, 'score': 9231.725385665894, 'total_duration': 9624.255983352661, 'accumulated_submission_time': 9231.725385665894, 'accumulated_eval_time': 391.0030782222748, 'accumulated_logging_time': 0.5623111724853516}
I0306 01:27:01.062577 139789021992704 logging_writer.py:48] [27281] accumulated_eval_time=391.003078, accumulated_logging_time=0.562311, accumulated_submission_time=9231.725386, global_step=27281, preemption_count=0, score=9231.725386, test/accuracy=0.490000, test/loss=2.347709, test/num_examples=10000, total_duration=9624.255983, train/accuracy=0.710140, train/loss=1.118421, validation/accuracy=0.621480, validation/loss=1.575600, validation/num_examples=50000
I0306 01:27:07.792406 139789030385408 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.734787940979004, loss=1.7604703903198242
I0306 01:27:41.445410 139789021992704 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.6853212118148804, loss=1.7282181978225708
I0306 01:28:15.104518 139789030385408 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.817445158958435, loss=1.6910972595214844
I0306 01:28:48.742709 139789021992704 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.9433246850967407, loss=1.7832762002944946
I0306 01:29:22.377884 139789030385408 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.2676990032196045, loss=1.7854419946670532
I0306 01:29:56.015513 139789021992704 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.6667602062225342, loss=1.7880994081497192
I0306 01:30:29.637946 139789030385408 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.928735375404358, loss=1.758253812789917
I0306 01:31:03.255108 139789021992704 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.7078617811203003, loss=1.8075895309448242
I0306 01:31:36.934585 139789030385408 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.7929353713989258, loss=1.8817105293273926
I0306 01:32:10.687099 139789021992704 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.837939977645874, loss=1.7366746664047241
I0306 01:32:44.309795 139789030385408 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.8135000467300415, loss=1.7853553295135498
I0306 01:33:17.965451 139789021992704 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.8343544006347656, loss=1.6929993629455566
I0306 01:33:51.588146 139789030385408 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.039935827255249, loss=1.7768564224243164
I0306 01:34:25.248617 139789021992704 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.7221568822860718, loss=1.7091152667999268
I0306 01:34:58.885373 139789030385408 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6070488691329956, loss=1.7276057004928589
I0306 01:35:31.318115 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:35:39.072728 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:35:52.563911 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:35:54.851455 139951089751872 submission_runner.py:411] Time since start: 10158.06s, 	Step: 28798, 	{'train/accuracy': 0.6959701776504517, 'train/loss': 1.1897066831588745, 'validation/accuracy': 0.6214599609375, 'validation/loss': 1.5625724792480469, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.2749381065368652, 'test/num_examples': 10000, 'score': 9741.918065309525, 'total_duration': 10158.063725471497, 'accumulated_submission_time': 9741.918065309525, 'accumulated_eval_time': 414.53634119033813, 'accumulated_logging_time': 0.5903370380401611}
I0306 01:35:54.873313 139789558830848 logging_writer.py:48] [28798] accumulated_eval_time=414.536341, accumulated_logging_time=0.590337, accumulated_submission_time=9741.918065, global_step=28798, preemption_count=0, score=9741.918065, test/accuracy=0.494000, test/loss=2.274938, test/num_examples=10000, total_duration=10158.063725, train/accuracy=0.695970, train/loss=1.189707, validation/accuracy=0.621460, validation/loss=1.562572, validation/num_examples=50000
I0306 01:35:55.891237 139789567223552 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.1344146728515625, loss=1.7734965085983276
I0306 01:36:29.493708 139789558830848 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.709524393081665, loss=1.6039416790008545
I0306 01:37:03.122481 139789567223552 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6836378574371338, loss=1.790295958518982
I0306 01:37:36.760474 139789558830848 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.5718549489974976, loss=1.673556923866272
I0306 01:38:10.398870 139789567223552 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.7514562606811523, loss=1.6657302379608154
I0306 01:38:44.019595 139789558830848 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.6332591772079468, loss=1.6551588773727417
I0306 01:39:17.618124 139789567223552 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.1339683532714844, loss=1.704748272895813
I0306 01:39:51.237522 139789558830848 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.9677597284317017, loss=1.653668761253357
I0306 01:40:24.893623 139789567223552 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.6880908012390137, loss=1.6968836784362793
I0306 01:40:58.546310 139789558830848 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.766708493232727, loss=1.7115062475204468
I0306 01:41:32.204291 139789567223552 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.710858941078186, loss=1.7255293130874634
I0306 01:42:05.850255 139789558830848 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.704138159751892, loss=1.7364813089370728
I0306 01:42:39.476679 139789567223552 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.973171591758728, loss=1.754732608795166
I0306 01:43:13.125855 139789558830848 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.7864062786102295, loss=1.7972323894500732
I0306 01:43:46.801094 139789567223552 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.65046226978302, loss=1.711471676826477
I0306 01:44:20.452906 139789558830848 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.749808430671692, loss=1.7640106678009033
I0306 01:44:24.953124 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:44:33.022412 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:44:48.667405 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:44:50.763797 139951089751872 submission_runner.py:411] Time since start: 10693.98s, 	Step: 30315, 	{'train/accuracy': 0.6573461294174194, 'train/loss': 1.3810793161392212, 'validation/accuracy': 0.590999960899353, 'validation/loss': 1.7070974111557007, 'validation/num_examples': 50000, 'test/accuracy': 0.47360002994537354, 'test/loss': 2.4069299697875977, 'test/num_examples': 10000, 'score': 10251.934691667557, 'total_duration': 10693.976108551025, 'accumulated_submission_time': 10251.934691667557, 'accumulated_eval_time': 440.34696435928345, 'accumulated_logging_time': 0.6227197647094727}
I0306 01:44:50.782679 139787361040128 logging_writer.py:48] [30315] accumulated_eval_time=440.346964, accumulated_logging_time=0.622720, accumulated_submission_time=10251.934692, global_step=30315, preemption_count=0, score=10251.934692, test/accuracy=0.473600, test/loss=2.406930, test/num_examples=10000, total_duration=10693.976109, train/accuracy=0.657346, train/loss=1.381079, validation/accuracy=0.591000, validation/loss=1.707097, validation/num_examples=50000
I0306 01:45:19.761681 139787369432832 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.231377363204956, loss=1.7059928178787231
I0306 01:45:53.425639 139787361040128 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.936058759689331, loss=1.6984267234802246
I0306 01:46:27.078383 139787369432832 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.692287564277649, loss=1.7655644416809082
I0306 01:47:00.761269 139787361040128 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.7129560708999634, loss=1.7512562274932861
I0306 01:47:34.433427 139787369432832 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.9733383655548096, loss=1.855650782585144
I0306 01:48:08.123674 139787361040128 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.6933674812316895, loss=1.7062764167785645
I0306 01:48:41.731454 139787369432832 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.8254804611206055, loss=1.6675461530685425
I0306 01:49:15.387451 139787361040128 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.957898497581482, loss=1.718301773071289
I0306 01:49:49.016556 139787369432832 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.6710659265518188, loss=1.6004531383514404
I0306 01:50:22.651472 139787361040128 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.7432529926300049, loss=1.6935721635818481
I0306 01:50:56.299829 139787369432832 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6866817474365234, loss=1.8083691596984863
I0306 01:51:29.922763 139787361040128 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.267686128616333, loss=1.7380669116973877
I0306 01:52:03.560530 139787369432832 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.274637460708618, loss=1.7212984561920166
I0306 01:52:37.227661 139787361040128 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.5931740999221802, loss=1.625422477722168
I0306 01:53:10.853595 139787369432832 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7117122411727905, loss=1.6538827419281006
I0306 01:53:21.106579 139951089751872 spec.py:321] Evaluating on the training split.
I0306 01:53:29.695110 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 01:53:43.049113 139951089751872 spec.py:349] Evaluating on the test split.
I0306 01:53:45.370139 139951089751872 submission_runner.py:411] Time since start: 11228.58s, 	Step: 31832, 	{'train/accuracy': 0.6697026491165161, 'train/loss': 1.3070857524871826, 'validation/accuracy': 0.6118599772453308, 'validation/loss': 1.6050184965133667, 'validation/num_examples': 50000, 'test/accuracy': 0.4913000166416168, 'test/loss': 2.338101863861084, 'test/num_examples': 10000, 'score': 10762.190105199814, 'total_duration': 11228.582449913025, 'accumulated_submission_time': 10762.190105199814, 'accumulated_eval_time': 464.6104917526245, 'accumulated_logging_time': 0.6552050113677979}
I0306 01:53:45.393430 139787369432832 logging_writer.py:48] [31832] accumulated_eval_time=464.610492, accumulated_logging_time=0.655205, accumulated_submission_time=10762.190105, global_step=31832, preemption_count=0, score=10762.190105, test/accuracy=0.491300, test/loss=2.338102, test/num_examples=10000, total_duration=11228.582450, train/accuracy=0.669703, train/loss=1.307086, validation/accuracy=0.611860, validation/loss=1.605018, validation/num_examples=50000
I0306 01:54:08.659033 139787562383104 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.8658225536346436, loss=1.781329870223999
I0306 01:54:42.334915 139787369432832 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.6741822957992554, loss=1.7029390335083008
I0306 01:55:15.977378 139787562383104 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.8006846904754639, loss=1.7548707723617554
I0306 01:55:49.634777 139787369432832 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.9029711484909058, loss=1.7501832246780396
I0306 01:56:23.254254 139787562383104 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.8870577812194824, loss=1.678078055381775
I0306 01:56:56.924085 139787369432832 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.6829477548599243, loss=1.6595978736877441
I0306 01:57:30.565855 139787562383104 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.794033169746399, loss=1.5918443202972412
I0306 01:58:04.172794 139787369432832 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.8702229261398315, loss=1.628997802734375
I0306 01:58:37.797017 139787562383104 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.8182648420333862, loss=1.7153841257095337
I0306 01:59:11.469500 139787369432832 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.717564344406128, loss=1.6100549697875977
I0306 01:59:45.129381 139787562383104 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.827339768409729, loss=1.7427771091461182
I0306 02:00:18.765071 139787369432832 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.629897952079773, loss=1.6640758514404297
I0306 02:00:52.446412 139787562383104 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.5910837650299072, loss=1.6353442668914795
I0306 02:01:26.134520 139787369432832 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.8211778402328491, loss=1.779710292816162
I0306 02:01:59.762249 139787562383104 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.896054744720459, loss=1.611129641532898
I0306 02:02:15.721283 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:02:24.272689 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:02:37.375282 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:02:39.610085 139951089751872 submission_runner.py:411] Time since start: 11762.82s, 	Step: 33349, 	{'train/accuracy': 0.6818000674247742, 'train/loss': 1.2354713678359985, 'validation/accuracy': 0.6244399547576904, 'validation/loss': 1.547203540802002, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.3112423419952393, 'test/num_examples': 10000, 'score': 11272.451827764511, 'total_duration': 11762.822406291962, 'accumulated_submission_time': 11272.451827764511, 'accumulated_eval_time': 488.4992530345917, 'accumulated_logging_time': 0.6910066604614258}
I0306 02:02:39.629417 139787369432832 logging_writer.py:48] [33349] accumulated_eval_time=488.499253, accumulated_logging_time=0.691007, accumulated_submission_time=11272.451828, global_step=33349, preemption_count=0, score=11272.451828, test/accuracy=0.495700, test/loss=2.311242, test/num_examples=10000, total_duration=11762.822406, train/accuracy=0.681800, train/loss=1.235471, validation/accuracy=0.624440, validation/loss=1.547204, validation/num_examples=50000
I0306 02:02:57.085787 139787562383104 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.7372428178787231, loss=1.6941274404525757
I0306 02:03:30.701820 139787369432832 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.7647769451141357, loss=1.7455625534057617
I0306 02:04:04.301567 139787562383104 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.7360366582870483, loss=1.6793949604034424
I0306 02:04:37.901532 139787369432832 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.876259684562683, loss=1.668082356452942
I0306 02:05:11.514868 139787562383104 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8455753326416016, loss=1.7197555303573608
I0306 02:05:45.080846 139787369432832 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.6025258302688599, loss=1.7551017999649048
I0306 02:06:18.682581 139787562383104 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7588485479354858, loss=1.7361057996749878
I0306 02:06:52.325256 139787369432832 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.6664289236068726, loss=1.693764567375183
I0306 02:07:25.951652 139787562383104 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.6443241834640503, loss=1.7379201650619507
I0306 02:07:59.575549 139787369432832 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.6617482900619507, loss=1.759363055229187
I0306 02:08:33.184718 139787562383104 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.3674256801605225, loss=1.8088587522506714
I0306 02:09:06.825153 139787369432832 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.7609474658966064, loss=1.7295570373535156
I0306 02:09:40.473574 139787562383104 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.9251716136932373, loss=1.6435991525650024
I0306 02:10:14.133709 139787369432832 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.8197418451309204, loss=1.8146388530731201
I0306 02:10:47.730866 139787562383104 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.7160251140594482, loss=1.6316500902175903
I0306 02:11:09.712947 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:11:18.020917 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:11:32.306367 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:11:34.537055 139951089751872 submission_runner.py:411] Time since start: 12297.75s, 	Step: 34867, 	{'train/accuracy': 0.6680484414100647, 'train/loss': 1.3182141780853271, 'validation/accuracy': 0.6115800142288208, 'validation/loss': 1.6301088333129883, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.4063782691955566, 'test/num_examples': 10000, 'score': 11782.472059488297, 'total_duration': 12297.74937415123, 'accumulated_submission_time': 11782.472059488297, 'accumulated_eval_time': 513.3233184814453, 'accumulated_logging_time': 0.7194736003875732}
I0306 02:11:34.562091 139789005207296 logging_writer.py:48] [34867] accumulated_eval_time=513.323318, accumulated_logging_time=0.719474, accumulated_submission_time=11782.472059, global_step=34867, preemption_count=0, score=11782.472059, test/accuracy=0.484300, test/loss=2.406378, test/num_examples=10000, total_duration=12297.749374, train/accuracy=0.668048, train/loss=1.318214, validation/accuracy=0.611580, validation/loss=1.630109, validation/num_examples=50000
I0306 02:11:45.992823 139789013600000 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.76105535030365, loss=1.7158890962600708
I0306 02:12:19.601253 139789005207296 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.985972285270691, loss=1.694652795791626
I0306 02:12:53.256794 139789013600000 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.8616864681243896, loss=1.6021400690078735
I0306 02:13:26.916761 139789005207296 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7502140998840332, loss=1.7165141105651855
I0306 02:14:00.553678 139789013600000 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.7938536405563354, loss=1.6986064910888672
I0306 02:14:34.208081 139789005207296 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.8817445039749146, loss=1.6641048192977905
I0306 02:15:07.846702 139789013600000 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.8003681898117065, loss=1.683424472808838
I0306 02:15:41.512011 139789005207296 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.073324203491211, loss=1.7539892196655273
I0306 02:16:15.184129 139789013600000 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.8396316766738892, loss=1.6534841060638428
I0306 02:16:48.827822 139789005207296 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.7609038352966309, loss=1.7489537000656128
I0306 02:17:22.446085 139789013600000 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.6641113758087158, loss=1.6759804487228394
I0306 02:17:56.092576 139789005207296 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.044099807739258, loss=1.6988542079925537
I0306 02:18:29.729625 139789013600000 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7581249475479126, loss=1.6888983249664307
I0306 02:19:03.389410 139789005207296 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6677961349487305, loss=1.594181776046753
I0306 02:19:37.037535 139789013600000 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.9764583110809326, loss=1.7856359481811523
I0306 02:20:04.729638 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:20:12.985547 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:20:27.032841 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:20:29.271744 139951089751872 submission_runner.py:411] Time since start: 12832.48s, 	Step: 36384, 	{'train/accuracy': 0.7164580821990967, 'train/loss': 1.0982540845870972, 'validation/accuracy': 0.6311799883842468, 'validation/loss': 1.5274664163589478, 'validation/num_examples': 50000, 'test/accuracy': 0.49650001525878906, 'test/loss': 2.2949490547180176, 'test/num_examples': 10000, 'score': 12292.574988365173, 'total_duration': 12832.48405623436, 'accumulated_submission_time': 12292.574988365173, 'accumulated_eval_time': 537.8653752803802, 'accumulated_logging_time': 0.7547996044158936}
I0306 02:20:29.291196 139787562383104 logging_writer.py:48] [36384] accumulated_eval_time=537.865375, accumulated_logging_time=0.754800, accumulated_submission_time=12292.574988, global_step=36384, preemption_count=0, score=12292.574988, test/accuracy=0.496500, test/loss=2.294949, test/num_examples=10000, total_duration=12832.484056, train/accuracy=0.716458, train/loss=1.098254, validation/accuracy=0.631180, validation/loss=1.527466, validation/num_examples=50000
I0306 02:20:35.000797 139788996814592 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.907835602760315, loss=1.6796525716781616
I0306 02:21:08.632484 139787562383104 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.0024025440216064, loss=1.754064917564392
I0306 02:21:42.291912 139788996814592 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.8004605770111084, loss=1.71792733669281
I0306 02:22:16.117906 139787562383104 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.9008891582489014, loss=1.7730120420455933
I0306 02:22:49.811326 139788996814592 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.6581180095672607, loss=1.5508646965026855
I0306 02:23:23.422819 139787562383104 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.8581440448760986, loss=1.6215652227401733
I0306 02:23:57.100936 139788996814592 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.712140440940857, loss=1.6885933876037598
I0306 02:24:30.703938 139787562383104 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7961419820785522, loss=1.724649429321289
I0306 02:25:04.349825 139788996814592 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.8257746696472168, loss=1.722285270690918
I0306 02:25:37.994559 139787562383104 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.8481628894805908, loss=1.6997301578521729
I0306 02:26:11.599748 139788996814592 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.969327688217163, loss=1.734961748123169
I0306 02:26:45.309859 139787562383104 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.0099942684173584, loss=1.7001267671585083
I0306 02:27:19.000000 139788996814592 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.0651917457580566, loss=1.6584484577178955
I0306 02:27:52.629587 139787562383104 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.637702226638794, loss=1.665519118309021
I0306 02:28:26.448689 139788996814592 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.9057592153549194, loss=1.6712920665740967
I0306 02:28:59.491995 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:29:06.705386 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:29:18.158344 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:29:20.465992 139951089751872 submission_runner.py:411] Time since start: 13363.68s, 	Step: 37900, 	{'train/accuracy': 0.7069514989852905, 'train/loss': 1.1378610134124756, 'validation/accuracy': 0.6352800130844116, 'validation/loss': 1.50535249710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.2614681720733643, 'test/num_examples': 10000, 'score': 12802.7119576931, 'total_duration': 13363.678293943405, 'accumulated_submission_time': 12802.7119576931, 'accumulated_eval_time': 558.8393158912659, 'accumulated_logging_time': 0.7837421894073486}
I0306 02:29:20.488803 139787369432832 logging_writer.py:48] [37900] accumulated_eval_time=558.839316, accumulated_logging_time=0.783742, accumulated_submission_time=12802.711958, global_step=37900, preemption_count=0, score=12802.711958, test/accuracy=0.504300, test/loss=2.261468, test/num_examples=10000, total_duration=13363.678294, train/accuracy=0.706951, train/loss=1.137861, validation/accuracy=0.635280, validation/loss=1.505352, validation/num_examples=50000
I0306 02:29:20.828161 139789005207296 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.2323195934295654, loss=1.7549123764038086
I0306 02:29:54.397976 139787369432832 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.8464710712432861, loss=1.545740008354187
I0306 02:30:27.985417 139789005207296 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.676321268081665, loss=1.5759888887405396
I0306 02:31:01.583935 139787369432832 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.89048433303833, loss=1.6965243816375732
I0306 02:31:35.238615 139789005207296 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.1491615772247314, loss=1.7155451774597168
I0306 02:32:08.857036 139787369432832 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7824000120162964, loss=1.69637131690979
I0306 02:32:42.513643 139789005207296 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.7544642686843872, loss=1.5982835292816162
I0306 02:33:16.153600 139787369432832 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.5459890365600586, loss=1.6361260414123535
I0306 02:33:49.812900 139789005207296 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.540036678314209, loss=1.736303687095642
I0306 02:34:23.531491 139787369432832 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.1927547454833984, loss=1.7155144214630127
I0306 02:34:57.248104 139789005207296 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.884700059890747, loss=1.7695047855377197
I0306 02:35:30.895235 139787369432832 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.0214524269104004, loss=1.694947361946106
I0306 02:36:04.550030 139789005207296 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.8697021007537842, loss=1.621140718460083
I0306 02:36:38.174628 139787369432832 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.9669657945632935, loss=1.7305513620376587
I0306 02:37:11.825327 139789005207296 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.0114994049072266, loss=1.817903995513916
I0306 02:37:45.517026 139787369432832 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.9050334692001343, loss=1.69746994972229
I0306 02:37:50.666731 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:37:57.761690 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:38:11.061150 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:38:13.182329 139951089751872 submission_runner.py:411] Time since start: 13896.39s, 	Step: 39417, 	{'train/accuracy': 0.6930006146430969, 'train/loss': 1.2103509902954102, 'validation/accuracy': 0.6251599788665771, 'validation/loss': 1.5366623401641846, 'validation/num_examples': 50000, 'test/accuracy': 0.4983000159263611, 'test/loss': 2.2956364154815674, 'test/num_examples': 10000, 'score': 13312.825773000717, 'total_duration': 13896.394639730453, 'accumulated_submission_time': 13312.825773000717, 'accumulated_eval_time': 581.3548767566681, 'accumulated_logging_time': 0.8163936138153076}
I0306 02:38:13.202086 139789030385408 logging_writer.py:48] [39417] accumulated_eval_time=581.354877, accumulated_logging_time=0.816394, accumulated_submission_time=13312.825773, global_step=39417, preemption_count=0, score=13312.825773, test/accuracy=0.498300, test/loss=2.295636, test/num_examples=10000, total_duration=13896.394640, train/accuracy=0.693001, train/loss=1.210351, validation/accuracy=0.625160, validation/loss=1.536662, validation/num_examples=50000
I0306 02:38:41.478772 139789038778112 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.7683241367340088, loss=1.6552836894989014
I0306 02:39:15.104267 139789030385408 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7675050497055054, loss=1.6234753131866455
I0306 02:39:48.779847 139789038778112 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.652100920677185, loss=1.6340386867523193
I0306 02:40:22.485604 139789030385408 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.7434176206588745, loss=1.5939522981643677
I0306 02:40:56.128287 139789038778112 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.76497483253479, loss=1.6112803220748901
I0306 02:41:29.785572 139789030385408 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.9006403684616089, loss=1.6116572618484497
I0306 02:42:03.416761 139789038778112 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.903002142906189, loss=1.7368757724761963
I0306 02:42:37.068204 139789030385408 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.7790207862854004, loss=1.701204776763916
I0306 02:43:10.706711 139789038778112 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.8842480182647705, loss=1.7233428955078125
I0306 02:43:44.358414 139789030385408 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.9676910638809204, loss=1.7378565073013306
I0306 02:44:18.024035 139789038778112 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.8748902082443237, loss=1.7169952392578125
I0306 02:44:51.639179 139789030385408 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.708019733428955, loss=1.5908085107803345
I0306 02:45:25.232830 139789038778112 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.7304645776748657, loss=1.7212245464324951
I0306 02:45:58.861756 139789030385408 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.8055917024612427, loss=1.7097766399383545
I0306 02:46:32.523009 139789038778112 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.6534628868103027, loss=1.6584008932113647
I0306 02:46:43.395905 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:46:50.174300 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:47:02.388036 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:47:04.627296 139951089751872 submission_runner.py:411] Time since start: 14427.84s, 	Step: 40934, 	{'train/accuracy': 0.6949737071990967, 'train/loss': 1.1954536437988281, 'validation/accuracy': 0.6284799575805664, 'validation/loss': 1.5238358974456787, 'validation/num_examples': 50000, 'test/accuracy': 0.5011000037193298, 'test/loss': 2.2670748233795166, 'test/num_examples': 10000, 'score': 13822.957331418991, 'total_duration': 14427.839611768723, 'accumulated_submission_time': 13822.957331418991, 'accumulated_eval_time': 602.5862288475037, 'accumulated_logging_time': 0.8450939655303955}
I0306 02:47:04.648938 139787369432832 logging_writer.py:48] [40934] accumulated_eval_time=602.586229, accumulated_logging_time=0.845094, accumulated_submission_time=13822.957331, global_step=40934, preemption_count=0, score=13822.957331, test/accuracy=0.501100, test/loss=2.267075, test/num_examples=10000, total_duration=14427.839612, train/accuracy=0.694974, train/loss=1.195454, validation/accuracy=0.628480, validation/loss=1.523836, validation/num_examples=50000
I0306 02:47:27.148962 139787562383104 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.85647451877594, loss=1.6547328233718872
I0306 02:48:00.687702 139787369432832 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7430026531219482, loss=1.6979484558105469
I0306 02:48:34.326851 139787562383104 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.7592543363571167, loss=1.5714201927185059
I0306 02:49:07.934626 139787369432832 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.8111003637313843, loss=1.6000031232833862
I0306 02:49:41.616252 139787562383104 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.066699743270874, loss=1.5944442749023438
I0306 02:50:15.244499 139787369432832 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.855093240737915, loss=1.69096839427948
I0306 02:50:48.926457 139787562383104 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.7760910987854004, loss=1.62742280960083
I0306 02:51:22.556457 139787369432832 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.8302630186080933, loss=1.5542265176773071
I0306 02:51:56.206104 139787562383104 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.8669705390930176, loss=1.7408385276794434
I0306 02:52:29.857380 139787369432832 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8664226531982422, loss=1.701069951057434
I0306 02:53:03.498364 139787562383104 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.8287112712860107, loss=1.7130745649337769
I0306 02:53:37.199161 139787369432832 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.774645447731018, loss=1.6847553253173828
I0306 02:54:10.892142 139787562383104 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.7696399688720703, loss=1.6243603229522705
I0306 02:54:44.515609 139787369432832 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.798567295074463, loss=1.6782734394073486
I0306 02:55:18.194902 139787562383104 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.8459110260009766, loss=1.612452507019043
I0306 02:55:34.779508 139951089751872 spec.py:321] Evaluating on the training split.
I0306 02:55:41.560499 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 02:55:53.318983 139951089751872 spec.py:349] Evaluating on the test split.
I0306 02:55:55.593975 139951089751872 submission_runner.py:411] Time since start: 14958.81s, 	Step: 42451, 	{'train/accuracy': 0.686543345451355, 'train/loss': 1.2287561893463135, 'validation/accuracy': 0.6284199953079224, 'validation/loss': 1.5451955795288086, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.3081836700439453, 'test/num_examples': 10000, 'score': 14333.02351474762, 'total_duration': 14958.806265830994, 'accumulated_submission_time': 14333.02351474762, 'accumulated_eval_time': 623.4006247520447, 'accumulated_logging_time': 0.8766939640045166}
I0306 02:55:55.621403 139789021992704 logging_writer.py:48] [42451] accumulated_eval_time=623.400625, accumulated_logging_time=0.876694, accumulated_submission_time=14333.023515, global_step=42451, preemption_count=0, score=14333.023515, test/accuracy=0.498600, test/loss=2.308184, test/num_examples=10000, total_duration=14958.806266, train/accuracy=0.686543, train/loss=1.228756, validation/accuracy=0.628420, validation/loss=1.545196, validation/num_examples=50000
I0306 02:56:12.465802 139789030385408 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.6740158796310425, loss=1.6879041194915771
I0306 02:56:46.116343 139789021992704 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.8509601354599, loss=1.5386717319488525
I0306 02:57:19.745921 139789030385408 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.819349765777588, loss=1.7592500448226929
I0306 02:57:53.388493 139789021992704 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.6198136806488037, loss=1.8047364950180054
I0306 02:58:27.112076 139789030385408 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.11380672454834, loss=1.6573238372802734
I0306 02:59:00.778240 139789021992704 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6349966526031494, loss=1.6111094951629639
I0306 02:59:34.380538 139789030385408 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8398454189300537, loss=1.5672905445098877
I0306 03:00:08.001400 139789021992704 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7933661937713623, loss=1.7077232599258423
I0306 03:00:41.724519 139789030385408 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.9973342418670654, loss=1.6371026039123535
I0306 03:01:15.346608 139789021992704 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.8152590990066528, loss=1.649358868598938
I0306 03:01:49.012660 139789030385408 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.1651017665863037, loss=1.6336685419082642
I0306 03:02:22.651451 139789021992704 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.7221206426620483, loss=1.5690126419067383
I0306 03:02:56.311971 139789030385408 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.849823236465454, loss=1.7811074256896973
I0306 03:03:29.938395 139789021992704 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.8363831043243408, loss=1.6298915147781372
I0306 03:04:03.614897 139789030385408 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.1197876930236816, loss=1.6677628755569458
I0306 03:04:25.609760 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:04:32.621859 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:04:41.698936 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:04:43.960789 139951089751872 submission_runner.py:411] Time since start: 15487.17s, 	Step: 43967, 	{'train/accuracy': 0.704500138759613, 'train/loss': 1.1581501960754395, 'validation/accuracy': 0.6357600092887878, 'validation/loss': 1.5145957469940186, 'validation/num_examples': 50000, 'test/accuracy': 0.5024000406265259, 'test/loss': 2.258817195892334, 'test/num_examples': 10000, 'score': 14842.946564435959, 'total_duration': 15487.173084020615, 'accumulated_submission_time': 14842.946564435959, 'accumulated_eval_time': 641.7515881061554, 'accumulated_logging_time': 0.9145431518554688}
I0306 03:04:43.986056 139787369432832 logging_writer.py:48] [43967] accumulated_eval_time=641.751588, accumulated_logging_time=0.914543, accumulated_submission_time=14842.946564, global_step=43967, preemption_count=0, score=14842.946564, test/accuracy=0.502400, test/loss=2.258817, test/num_examples=10000, total_duration=15487.173084, train/accuracy=0.704500, train/loss=1.158150, validation/accuracy=0.635760, validation/loss=1.514596, validation/num_examples=50000
I0306 03:04:55.419568 139787562383104 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.690427303314209, loss=1.5561424493789673
I0306 03:05:29.102859 139787369432832 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6912202835083008, loss=1.6058226823806763
I0306 03:06:02.774867 139787562383104 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.9737310409545898, loss=1.6457266807556152
I0306 03:06:36.460872 139787369432832 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8920683860778809, loss=1.6942216157913208
I0306 03:07:10.098408 139787562383104 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.6377638578414917, loss=1.6095302104949951
I0306 03:07:43.723881 139787369432832 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.88930082321167, loss=1.633776307106018
I0306 03:08:17.364150 139787562383104 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7897080183029175, loss=1.570205807685852
I0306 03:08:51.010737 139787369432832 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.7634934186935425, loss=1.6133220195770264
I0306 03:09:24.650840 139787562383104 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.8569605350494385, loss=1.618253231048584
I0306 03:09:58.298655 139787369432832 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7801291942596436, loss=1.6115503311157227
I0306 03:10:31.949316 139787562383104 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.9128143787384033, loss=1.6155660152435303
I0306 03:11:05.631128 139787369432832 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.9238306283950806, loss=1.7205601930618286
I0306 03:11:39.258125 139787562383104 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.089789867401123, loss=1.726163387298584
I0306 03:12:12.888215 139787369432832 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8584333658218384, loss=1.5757067203521729
I0306 03:12:46.517669 139787562383104 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.5947641134262085, loss=1.6826391220092773
I0306 03:13:14.269149 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:13:21.014699 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:13:29.926341 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:13:32.177629 139951089751872 submission_runner.py:411] Time since start: 16015.39s, 	Step: 45484, 	{'train/accuracy': 0.720723032951355, 'train/loss': 1.0734890699386597, 'validation/accuracy': 0.6385200023651123, 'validation/loss': 1.4788291454315186, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.2426843643188477, 'test/num_examples': 10000, 'score': 15353.166466712952, 'total_duration': 16015.389912128448, 'accumulated_submission_time': 15353.166466712952, 'accumulated_eval_time': 659.6599915027618, 'accumulated_logging_time': 0.949812650680542}
I0306 03:13:32.206291 139789013600000 logging_writer.py:48] [45484] accumulated_eval_time=659.659992, accumulated_logging_time=0.949813, accumulated_submission_time=15353.166467, global_step=45484, preemption_count=0, score=15353.166467, test/accuracy=0.505200, test/loss=2.242684, test/num_examples=10000, total_duration=16015.389912, train/accuracy=0.720723, train/loss=1.073489, validation/accuracy=0.638520, validation/loss=1.478829, validation/num_examples=50000
I0306 03:13:37.920224 139789021992704 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7125811576843262, loss=1.5518162250518799
I0306 03:14:11.504415 139789013600000 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.966760277748108, loss=1.724621057510376
I0306 03:14:45.086472 139789021992704 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5997726917266846, loss=1.5568631887435913
I0306 03:15:18.743945 139789013600000 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.7529772520065308, loss=1.644651174545288
I0306 03:15:52.399350 139789021992704 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.0239388942718506, loss=1.6745593547821045
I0306 03:16:26.056083 139789013600000 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.9126766920089722, loss=1.6573171615600586
I0306 03:16:59.677733 139789021992704 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8734854459762573, loss=1.6016747951507568
I0306 03:17:33.359059 139789013600000 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.791972041130066, loss=1.70262610912323
I0306 03:18:07.003274 139789021992704 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.9215633869171143, loss=1.6645426750183105
I0306 03:18:40.651909 139789013600000 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.8221675157546997, loss=1.6226160526275635
I0306 03:19:14.274447 139789021992704 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.094299793243408, loss=1.6682871580123901
I0306 03:19:47.947674 139789013600000 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.973147988319397, loss=1.6585229635238647
I0306 03:20:21.604400 139789021992704 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.0847420692443848, loss=1.6772021055221558
I0306 03:20:55.249814 139789013600000 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.712733507156372, loss=1.543362021446228
I0306 03:21:28.877877 139789021992704 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8248778581619263, loss=1.7035819292068481
I0306 03:22:02.531122 139789013600000 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.906501293182373, loss=1.6433018445968628
I0306 03:22:02.538915 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:22:09.256749 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:22:18.086356 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:22:20.380370 139951089751872 submission_runner.py:411] Time since start: 16543.59s, 	Step: 47001, 	{'train/accuracy': 0.7143255472183228, 'train/loss': 1.107452392578125, 'validation/accuracy': 0.6450999975204468, 'validation/loss': 1.4719318151474, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.193423271179199, 'test/num_examples': 10000, 'score': 15863.43516588211, 'total_duration': 16543.592682123184, 'accumulated_submission_time': 15863.43516588211, 'accumulated_eval_time': 677.5013723373413, 'accumulated_logging_time': 0.9880750179290771}
I0306 03:22:20.405318 139788996814592 logging_writer.py:48] [47001] accumulated_eval_time=677.501372, accumulated_logging_time=0.988075, accumulated_submission_time=15863.435166, global_step=47001, preemption_count=0, score=15863.435166, test/accuracy=0.517400, test/loss=2.193423, test/num_examples=10000, total_duration=16543.592682, train/accuracy=0.714326, train/loss=1.107452, validation/accuracy=0.645100, validation/loss=1.471932, validation/num_examples=50000
I0306 03:22:54.087844 139789005207296 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.9447541236877441, loss=1.6177923679351807
I0306 03:23:27.758513 139788996814592 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.945076823234558, loss=1.5795930624008179
I0306 03:24:01.385292 139789005207296 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7376102209091187, loss=1.6202343702316284
I0306 03:24:35.021969 139788996814592 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7236762046813965, loss=1.5840966701507568
I0306 03:25:08.671103 139789005207296 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.133557081222534, loss=1.5685099363327026
I0306 03:25:42.312714 139788996814592 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.856514573097229, loss=1.5812408924102783
I0306 03:26:15.964310 139789005207296 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.9337778091430664, loss=1.5741089582443237
I0306 03:26:49.592698 139788996814592 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.1809444427490234, loss=1.7475039958953857
I0306 03:27:23.193264 139789005207296 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.033226251602173, loss=1.6043673753738403
I0306 03:27:56.800293 139788996814592 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.6441115140914917, loss=1.5907248258590698
I0306 03:28:30.464941 139789005207296 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.8505985736846924, loss=1.6364104747772217
I0306 03:29:04.099105 139788996814592 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.928049921989441, loss=1.6397755146026611
I0306 03:29:37.776632 139789005207296 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.9010123014450073, loss=1.6388458013534546
I0306 03:30:11.436526 139788996814592 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.6767504215240479, loss=1.6561203002929688
I0306 03:30:45.137589 139789005207296 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.029923677444458, loss=1.7026640176773071
I0306 03:30:50.651571 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:30:57.394303 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:31:07.328668 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:31:09.571352 139951089751872 submission_runner.py:411] Time since start: 17072.78s, 	Step: 48518, 	{'train/accuracy': 0.7139468789100647, 'train/loss': 1.1055635213851929, 'validation/accuracy': 0.6447799801826477, 'validation/loss': 1.4599003791809082, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.1961843967437744, 'test/num_examples': 10000, 'score': 16373.61761522293, 'total_duration': 17072.783661842346, 'accumulated_submission_time': 16373.61761522293, 'accumulated_eval_time': 696.4210996627808, 'accumulated_logging_time': 1.0227127075195312}
I0306 03:31:09.596666 139789013600000 logging_writer.py:48] [48518] accumulated_eval_time=696.421100, accumulated_logging_time=1.022713, accumulated_submission_time=16373.617615, global_step=48518, preemption_count=0, score=16373.617615, test/accuracy=0.521700, test/loss=2.196184, test/num_examples=10000, total_duration=17072.783662, train/accuracy=0.713947, train/loss=1.105564, validation/accuracy=0.644780, validation/loss=1.459900, validation/num_examples=50000
I0306 03:31:37.573305 139789021992704 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.9774960279464722, loss=1.5277483463287354
I0306 03:32:11.224139 139789013600000 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.9735935926437378, loss=1.6482391357421875
I0306 03:32:44.844609 139789021992704 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.7689380645751953, loss=1.5603468418121338
I0306 03:33:18.514937 139789013600000 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.9471352100372314, loss=1.5317435264587402
I0306 03:33:52.149915 139789021992704 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.100959062576294, loss=1.5927780866622925
I0306 03:34:25.785122 139789013600000 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.8612895011901855, loss=1.6058621406555176
I0306 03:34:59.410760 139789021992704 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.9083991050720215, loss=1.6438754796981812
I0306 03:35:33.019896 139789013600000 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.8780263662338257, loss=1.674525499343872
I0306 03:36:06.699864 139789021992704 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.70639967918396, loss=1.635392427444458
I0306 03:36:40.392654 139789013600000 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8208259344100952, loss=1.5257724523544312
I0306 03:37:14.012792 139789021992704 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.0460572242736816, loss=1.550544261932373
I0306 03:37:47.665753 139789013600000 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.9648284912109375, loss=1.6297943592071533
I0306 03:38:21.280063 139789021992704 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.9616838693618774, loss=1.5400943756103516
I0306 03:38:54.881752 139789013600000 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.7373197078704834, loss=1.4748890399932861
I0306 03:39:28.483706 139789021992704 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.9440460205078125, loss=1.657012701034546
I0306 03:39:39.737486 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:39:46.164119 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:39:55.238065 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:39:57.504748 139951089751872 submission_runner.py:411] Time since start: 17600.72s, 	Step: 50035, 	{'train/accuracy': 0.703523576259613, 'train/loss': 1.156677484512329, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.461700439453125, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.2414681911468506, 'test/num_examples': 10000, 'score': 16883.693967342377, 'total_duration': 17600.717061519623, 'accumulated_submission_time': 16883.693967342377, 'accumulated_eval_time': 714.1883132457733, 'accumulated_logging_time': 1.057837963104248}
I0306 03:39:57.531149 139787562383104 logging_writer.py:48] [50035] accumulated_eval_time=714.188313, accumulated_logging_time=1.057838, accumulated_submission_time=16883.693967, global_step=50035, preemption_count=0, score=16883.693967, test/accuracy=0.510400, test/loss=2.241468, test/num_examples=10000, total_duration=17600.717062, train/accuracy=0.703524, train/loss=1.156677, validation/accuracy=0.641800, validation/loss=1.461700, validation/num_examples=50000
I0306 03:40:19.760621 139788996814592 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8768593072891235, loss=1.6736395359039307
I0306 03:40:53.452838 139787562383104 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.6710858345031738, loss=1.5376663208007812
I0306 03:41:27.067755 139788996814592 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.050082206726074, loss=1.533205270767212
I0306 03:42:00.708244 139787562383104 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.9986636638641357, loss=1.5147373676300049
I0306 03:42:34.321815 139788996814592 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.7850619554519653, loss=1.6044296026229858
I0306 03:43:07.986457 139787562383104 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.1011204719543457, loss=1.7216789722442627
I0306 03:43:41.622731 139788996814592 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.9873937368392944, loss=1.6660248041152954
I0306 03:44:15.255741 139787562383104 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8999378681182861, loss=1.5591976642608643
I0306 03:44:48.884148 139788996814592 logging_writer.py:48] [50900] global_step=50900, grad_norm=2.083068609237671, loss=1.618128776550293
I0306 03:45:22.542896 139787562383104 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.849364995956421, loss=1.5976016521453857
I0306 03:45:56.168715 139788996814592 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7101200819015503, loss=1.6051198244094849
I0306 03:46:29.831734 139787562383104 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.0081255435943604, loss=1.6905936002731323
I0306 03:47:03.496513 139788996814592 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.8426460027694702, loss=1.700662612915039
I0306 03:47:37.154308 139787562383104 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7418944835662842, loss=1.689299464225769
I0306 03:48:10.797156 139788996814592 logging_writer.py:48] [51500] global_step=51500, grad_norm=2.0781822204589844, loss=1.5869035720825195
I0306 03:48:27.721755 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:48:34.215564 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:48:43.697175 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:48:45.937035 139951089751872 submission_runner.py:411] Time since start: 18129.15s, 	Step: 51552, 	{'train/accuracy': 0.7147839665412903, 'train/loss': 1.1116281747817993, 'validation/accuracy': 0.6556400060653687, 'validation/loss': 1.4142948389053345, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.136894941329956, 'test/num_examples': 10000, 'score': 17393.82016658783, 'total_duration': 18129.149336099625, 'accumulated_submission_time': 17393.82016658783, 'accumulated_eval_time': 732.4035475254059, 'accumulated_logging_time': 1.0941922664642334}
I0306 03:48:45.963114 139789021992704 logging_writer.py:48] [51552] accumulated_eval_time=732.403548, accumulated_logging_time=1.094192, accumulated_submission_time=17393.820167, global_step=51552, preemption_count=0, score=17393.820167, test/accuracy=0.524900, test/loss=2.136895, test/num_examples=10000, total_duration=18129.149336, train/accuracy=0.714784, train/loss=1.111628, validation/accuracy=0.655640, validation/loss=1.414295, validation/num_examples=50000
I0306 03:49:02.405824 139789030385408 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.001342296600342, loss=1.6480519771575928
I0306 03:49:36.003873 139789021992704 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8567739725112915, loss=1.622339129447937
I0306 03:50:09.686293 139789030385408 logging_writer.py:48] [51800] global_step=51800, grad_norm=2.040666103363037, loss=1.5684776306152344
I0306 03:50:43.301644 139789021992704 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.101513147354126, loss=1.6943895816802979
I0306 03:51:16.980962 139789030385408 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.21449613571167, loss=1.6978110074996948
I0306 03:51:50.599417 139789021992704 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.7645906209945679, loss=1.6352345943450928
I0306 03:52:24.266496 139789030385408 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7036492824554443, loss=1.6331788301467896
I0306 03:52:57.894489 139789021992704 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.043893337249756, loss=1.6819204092025757
I0306 03:53:31.488992 139789030385408 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.9491889476776123, loss=1.5532504320144653
I0306 03:54:05.176304 139789021992704 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8872795104980469, loss=1.5566720962524414
I0306 03:54:38.861897 139789030385408 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9111130237579346, loss=1.7302637100219727
I0306 03:55:12.489191 139789021992704 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8173693418502808, loss=1.529601812362671
I0306 03:55:46.152520 139789030385408 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8319978713989258, loss=1.6130802631378174
I0306 03:56:19.754930 139789021992704 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8576197624206543, loss=1.5553699731826782
I0306 03:56:53.425446 139789030385408 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.7384074926376343, loss=1.5447009801864624
I0306 03:57:16.065929 139951089751872 spec.py:321] Evaluating on the training split.
I0306 03:57:22.438829 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 03:57:31.497821 139951089751872 spec.py:349] Evaluating on the test split.
I0306 03:57:33.745556 139951089751872 submission_runner.py:411] Time since start: 18656.96s, 	Step: 53069, 	{'train/accuracy': 0.7700493931770325, 'train/loss': 0.8872178196907043, 'validation/accuracy': 0.6549599766731262, 'validation/loss': 1.4167771339416504, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.1360621452331543, 'test/num_examples': 10000, 'score': 17903.856494903564, 'total_duration': 18656.95786833763, 'accumulated_submission_time': 17903.856494903564, 'accumulated_eval_time': 750.0831353664398, 'accumulated_logging_time': 1.131983757019043}
I0306 03:57:33.771548 139789005207296 logging_writer.py:48] [53069] accumulated_eval_time=750.083135, accumulated_logging_time=1.131984, accumulated_submission_time=17903.856495, global_step=53069, preemption_count=0, score=17903.856495, test/accuracy=0.527000, test/loss=2.136062, test/num_examples=10000, total_duration=18656.957868, train/accuracy=0.770049, train/loss=0.887218, validation/accuracy=0.654960, validation/loss=1.416777, validation/num_examples=50000
I0306 03:57:44.516523 139789013600000 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.7090044021606445, loss=1.4792249202728271
I0306 03:58:18.108167 139789005207296 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.677230954170227, loss=1.5296165943145752
I0306 03:58:51.744534 139789013600000 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.9942271709442139, loss=1.6827367544174194
I0306 03:59:25.406182 139789005207296 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.677491307258606, loss=1.5033241510391235
I0306 03:59:59.057881 139789013600000 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.853890061378479, loss=1.6284269094467163
I0306 04:00:32.657702 139789005207296 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8482682704925537, loss=1.6466437578201294
I0306 04:01:06.250101 139789013600000 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.1418421268463135, loss=1.599459171295166
I0306 04:01:39.862677 139789005207296 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.9258830547332764, loss=1.6633665561676025
I0306 04:02:13.513589 139789013600000 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.9754170179367065, loss=1.5799638032913208
I0306 04:02:47.146461 139789005207296 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.961453914642334, loss=1.6688337326049805
I0306 04:03:20.791883 139789013600000 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8941720724105835, loss=1.5834983587265015
I0306 04:03:54.419367 139789005207296 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8378411531448364, loss=1.5357178449630737
I0306 04:04:28.068361 139789013600000 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7887076139450073, loss=1.4506137371063232
I0306 04:05:01.692239 139789005207296 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.775509238243103, loss=1.619073510169983
I0306 04:05:35.335317 139789013600000 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9980559349060059, loss=1.665441632270813
I0306 04:06:04.060390 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:06:10.942212 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:06:22.463391 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:06:24.709484 139951089751872 submission_runner.py:411] Time since start: 19187.92s, 	Step: 54587, 	{'train/accuracy': 0.7238121628761292, 'train/loss': 1.0641529560089111, 'validation/accuracy': 0.6489999890327454, 'validation/loss': 1.4466161727905273, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.1638076305389404, 'test/num_examples': 10000, 'score': 18414.08018231392, 'total_duration': 19187.921802043915, 'accumulated_submission_time': 18414.08018231392, 'accumulated_eval_time': 770.7322072982788, 'accumulated_logging_time': 1.1675801277160645}
I0306 04:06:24.735251 139788996814592 logging_writer.py:48] [54587] accumulated_eval_time=770.732207, accumulated_logging_time=1.167580, accumulated_submission_time=18414.080182, global_step=54587, preemption_count=0, score=18414.080182, test/accuracy=0.520000, test/loss=2.163808, test/num_examples=10000, total_duration=19187.921802, train/accuracy=0.723812, train/loss=1.064153, validation/accuracy=0.649000, validation/loss=1.446616, validation/num_examples=50000
I0306 04:06:29.437927 139789005207296 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.0196447372436523, loss=1.7121365070343018
I0306 04:07:03.097626 139788996814592 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.8611630201339722, loss=1.6966924667358398
I0306 04:07:36.744527 139789005207296 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8243205547332764, loss=1.6526144742965698
I0306 04:08:10.404863 139788996814592 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.8334177732467651, loss=1.5270812511444092
I0306 04:08:44.021245 139789005207296 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.9167661666870117, loss=1.6919540166854858
I0306 04:09:17.682610 139788996814592 logging_writer.py:48] [55100] global_step=55100, grad_norm=2.11035418510437, loss=1.6670498847961426
I0306 04:09:51.324740 139789005207296 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.6726948022842407, loss=1.602410078048706
I0306 04:10:24.993680 139788996814592 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.8842440843582153, loss=1.6480880975723267
I0306 04:10:58.629319 139789005207296 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.931958794593811, loss=1.6122782230377197
I0306 04:11:32.266513 139788996814592 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.9487130641937256, loss=1.666704773902893
I0306 04:12:05.926190 139789005207296 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.830392837524414, loss=1.6519755125045776
I0306 04:12:39.697584 139788996814592 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.876053810119629, loss=1.6947523355484009
I0306 04:13:13.333286 139789005207296 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.0048539638519287, loss=1.6029548645019531
I0306 04:13:47.000517 139788996814592 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.8344180583953857, loss=1.64399254322052
I0306 04:14:20.623906 139789005207296 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.018547296524048, loss=1.5741922855377197
I0306 04:14:54.299046 139788996814592 logging_writer.py:48] [56100] global_step=56100, grad_norm=2.1092355251312256, loss=1.4868654012680054
I0306 04:14:54.779263 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:15:01.129631 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:15:10.718556 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:15:13.002130 139951089751872 submission_runner.py:411] Time since start: 19716.21s, 	Step: 56103, 	{'train/accuracy': 0.7228754758834839, 'train/loss': 1.0718671083450317, 'validation/accuracy': 0.6495999693870544, 'validation/loss': 1.4355239868164062, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.1802403926849365, 'test/num_examples': 10000, 'score': 18924.0583422184, 'total_duration': 19716.214436769485, 'accumulated_submission_time': 18924.0583422184, 'accumulated_eval_time': 788.9550342559814, 'accumulated_logging_time': 1.20243501663208}
I0306 04:15:13.031678 139788996814592 logging_writer.py:48] [56103] accumulated_eval_time=788.955034, accumulated_logging_time=1.202435, accumulated_submission_time=18924.058342, global_step=56103, preemption_count=0, score=18924.058342, test/accuracy=0.517500, test/loss=2.180240, test/num_examples=10000, total_duration=19716.214437, train/accuracy=0.722875, train/loss=1.071867, validation/accuracy=0.649600, validation/loss=1.435524, validation/num_examples=50000
I0306 04:15:46.058053 139789030385408 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8227593898773193, loss=1.6283966302871704
I0306 04:16:19.691442 139788996814592 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8275011777877808, loss=1.4912984371185303
I0306 04:16:53.294645 139789030385408 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.8525463342666626, loss=1.626160740852356
I0306 04:17:26.894616 139788996814592 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.880492925643921, loss=1.612548828125
I0306 04:18:00.483674 139789030385408 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.0085055828094482, loss=1.4881510734558105
I0306 04:18:34.122046 139788996814592 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.7845076322555542, loss=1.5917061567306519
I0306 04:19:07.821995 139789030385408 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8126145601272583, loss=1.62901771068573
I0306 04:19:41.477193 139788996814592 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.9367742538452148, loss=1.4910554885864258
I0306 04:20:15.105357 139789030385408 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.6939901113510132, loss=1.4788485765457153
I0306 04:20:48.689693 139788996814592 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8693325519561768, loss=1.597406268119812
I0306 04:21:22.262218 139789030385408 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.9470947980880737, loss=1.6021931171417236
I0306 04:21:55.848965 139788996814592 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.017282485961914, loss=1.5303242206573486
I0306 04:22:29.497614 139789030385408 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9539365768432617, loss=1.5225622653961182
I0306 04:23:03.118778 139788996814592 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.0553195476531982, loss=1.5441298484802246
I0306 04:23:36.777092 139789030385408 logging_writer.py:48] [57600] global_step=57600, grad_norm=2.1244356632232666, loss=1.569276213645935
I0306 04:23:43.310858 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:23:49.602128 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:24:01.329749 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:24:03.580869 139951089751872 submission_runner.py:411] Time since start: 20246.79s, 	Step: 57621, 	{'train/accuracy': 0.7300701141357422, 'train/loss': 1.035422444343567, 'validation/accuracy': 0.6582599878311157, 'validation/loss': 1.3901373147964478, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.1339540481567383, 'test/num_examples': 10000, 'score': 19434.272382974625, 'total_duration': 20246.793174266815, 'accumulated_submission_time': 19434.272382974625, 'accumulated_eval_time': 809.2250037193298, 'accumulated_logging_time': 1.2412221431732178}
I0306 04:24:03.607532 139787562383104 logging_writer.py:48] [57621] accumulated_eval_time=809.225004, accumulated_logging_time=1.241222, accumulated_submission_time=19434.272383, global_step=57621, preemption_count=0, score=19434.272383, test/accuracy=0.530000, test/loss=2.133954, test/num_examples=10000, total_duration=20246.793174, train/accuracy=0.730070, train/loss=1.035422, validation/accuracy=0.658260, validation/loss=1.390137, validation/num_examples=50000
I0306 04:24:30.524161 139788996814592 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.736495018005371, loss=1.457503318786621
I0306 04:25:04.204032 139787562383104 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8780728578567505, loss=1.6313925981521606
I0306 04:25:37.877704 139788996814592 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.7533397674560547, loss=1.659628987312317
I0306 04:26:11.516034 139787562383104 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.883970856666565, loss=1.5130454301834106
I0306 04:26:45.210825 139788996814592 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.328613042831421, loss=1.6721112728118896
I0306 04:27:18.853881 139787562383104 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.7698960304260254, loss=1.6016125679016113
I0306 04:27:52.494120 139788996814592 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.0138120651245117, loss=1.4564334154129028
I0306 04:28:26.135460 139787562383104 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.8175311088562012, loss=1.3697855472564697
I0306 04:28:59.790178 139788996814592 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8413347005844116, loss=1.6421408653259277
I0306 04:29:33.442007 139787562383104 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9268203973770142, loss=1.6128233671188354
I0306 04:30:07.077005 139788996814592 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.1554932594299316, loss=1.4960156679153442
I0306 04:30:40.742799 139787562383104 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.7241909503936768, loss=1.480401873588562
I0306 04:31:14.431444 139788996814592 logging_writer.py:48] [58900] global_step=58900, grad_norm=2.051163673400879, loss=1.5542197227478027
I0306 04:31:48.041751 139787562383104 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9604058265686035, loss=1.5150924921035767
I0306 04:32:21.688236 139788996814592 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.842989444732666, loss=1.6590622663497925
I0306 04:32:33.618842 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:32:39.852841 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:32:48.704138 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:32:50.949314 139951089751872 submission_runner.py:411] Time since start: 20774.16s, 	Step: 59137, 	{'train/accuracy': 0.7110969424247742, 'train/loss': 1.1127877235412598, 'validation/accuracy': 0.6474999785423279, 'validation/loss': 1.4427329301834106, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.223588466644287, 'test/num_examples': 10000, 'score': 19944.219733953476, 'total_duration': 20774.161609888077, 'accumulated_submission_time': 19944.219733953476, 'accumulated_eval_time': 826.5554084777832, 'accumulated_logging_time': 1.2777154445648193}
I0306 04:32:50.976416 139789030385408 logging_writer.py:48] [59137] accumulated_eval_time=826.555408, accumulated_logging_time=1.277715, accumulated_submission_time=19944.219734, global_step=59137, preemption_count=0, score=19944.219734, test/accuracy=0.512200, test/loss=2.223588, test/num_examples=10000, total_duration=20774.161610, train/accuracy=0.711097, train/loss=1.112788, validation/accuracy=0.647500, validation/loss=1.442733, validation/num_examples=50000
I0306 04:33:12.465504 139789038778112 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9094233512878418, loss=1.5169247388839722
I0306 04:33:46.103933 139789030385408 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.7574923038482666, loss=1.4775062799453735
I0306 04:34:19.719945 139789038778112 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.9195293188095093, loss=1.615950107574463
I0306 04:34:53.375713 139789030385408 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.8851450681686401, loss=1.513697624206543
I0306 04:35:27.003342 139789038778112 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.8899567127227783, loss=1.602948546409607
I0306 04:36:00.613497 139789030385408 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.0218629837036133, loss=1.5202107429504395
I0306 04:36:34.217404 139789038778112 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.885602355003357, loss=1.6170034408569336
I0306 04:37:07.872804 139789030385408 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.1463537216186523, loss=1.4670618772506714
I0306 04:37:41.493956 139789038778112 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.944555640220642, loss=1.5623403787612915
I0306 04:38:15.074673 139789030385408 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.7915295362472534, loss=1.5547600984573364
I0306 04:38:48.678164 139789038778112 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.1001956462860107, loss=1.630289912223816
I0306 04:39:22.235692 139789030385408 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.1631898880004883, loss=1.7037750482559204
I0306 04:39:55.835822 139789038778112 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9551924467086792, loss=1.639064908027649
I0306 04:40:29.519289 139789030385408 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.958451747894287, loss=1.6139827966690063
I0306 04:41:03.138449 139789038778112 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.09600567817688, loss=1.6062736511230469
I0306 04:41:21.125695 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:41:27.419074 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:41:36.654213 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:41:38.952196 139951089751872 submission_runner.py:411] Time since start: 21302.16s, 	Step: 60655, 	{'train/accuracy': 0.7175143361091614, 'train/loss': 1.090225100517273, 'validation/accuracy': 0.6553599834442139, 'validation/loss': 1.408957600593567, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.151703119277954, 'test/num_examples': 10000, 'score': 20454.302026748657, 'total_duration': 21302.164501667023, 'accumulated_submission_time': 20454.302026748657, 'accumulated_eval_time': 844.3818547725677, 'accumulated_logging_time': 1.3164253234863281}
I0306 04:41:38.980723 139787361040128 logging_writer.py:48] [60655] accumulated_eval_time=844.381855, accumulated_logging_time=1.316425, accumulated_submission_time=20454.302027, global_step=60655, preemption_count=0, score=20454.302027, test/accuracy=0.523400, test/loss=2.151703, test/num_examples=10000, total_duration=21302.164502, train/accuracy=0.717514, train/loss=1.090225, validation/accuracy=0.655360, validation/loss=1.408958, validation/num_examples=50000
I0306 04:41:54.410302 139787369432832 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9909169673919678, loss=1.5608599185943604
I0306 04:42:28.025160 139787361040128 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.1513144969940186, loss=1.64662504196167
I0306 04:43:01.696280 139787369432832 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.7494494915008545, loss=1.5870801210403442
I0306 04:43:35.278717 139787361040128 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.7715518474578857, loss=1.5102667808532715
I0306 04:44:08.854713 139787369432832 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.7726858854293823, loss=1.5650981664657593
I0306 04:44:42.493040 139787361040128 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.9103838205337524, loss=1.519188642501831
I0306 04:45:16.145401 139787369432832 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9116767644882202, loss=1.5526764392852783
I0306 04:45:49.791755 139787361040128 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.101208209991455, loss=1.7271875143051147
I0306 04:46:23.461965 139787369432832 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.7512867450714111, loss=1.6504579782485962
I0306 04:46:57.075086 139787361040128 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.9158108234405518, loss=1.6508868932724
I0306 04:47:30.656056 139787369432832 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.8550407886505127, loss=1.556187391281128
I0306 04:48:04.212782 139787361040128 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9178296327590942, loss=1.5829105377197266
I0306 04:48:37.785972 139787369432832 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.7696751356124878, loss=1.5451382398605347
I0306 04:49:11.435282 139787361040128 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.997982382774353, loss=1.5434677600860596
I0306 04:49:45.029969 139787369432832 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.8226243257522583, loss=1.51914381980896
I0306 04:50:09.050313 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:50:15.329159 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:50:24.559670 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:50:26.858723 139951089751872 submission_runner.py:411] Time since start: 21830.07s, 	Step: 62173, 	{'train/accuracy': 0.7569156289100647, 'train/loss': 0.9074265360832214, 'validation/accuracy': 0.6525200009346008, 'validation/loss': 1.410874843597412, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.190683364868164, 'test/num_examples': 10000, 'score': 20964.30534338951, 'total_duration': 21830.071031332016, 'accumulated_submission_time': 20964.30534338951, 'accumulated_eval_time': 862.1902160644531, 'accumulated_logging_time': 1.356015682220459}
I0306 04:50:26.889575 139789013600000 logging_writer.py:48] [62173] accumulated_eval_time=862.190216, accumulated_logging_time=1.356016, accumulated_submission_time=20964.305343, global_step=62173, preemption_count=0, score=20964.305343, test/accuracy=0.519100, test/loss=2.190683, test/num_examples=10000, total_duration=21830.071031, train/accuracy=0.756916, train/loss=0.907427, validation/accuracy=0.652520, validation/loss=1.410875, validation/num_examples=50000
I0306 04:50:36.313478 139789021992704 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.3295910358428955, loss=1.532301902770996
I0306 04:51:09.918809 139789013600000 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.1760079860687256, loss=1.502071738243103
I0306 04:51:43.576198 139789021992704 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.267554998397827, loss=1.5060933828353882
I0306 04:52:17.181519 139789013600000 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.895774483680725, loss=1.5017271041870117
I0306 04:52:50.844656 139789021992704 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.8266699314117432, loss=1.4977596998214722
I0306 04:53:24.460833 139789013600000 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.0391533374786377, loss=1.5589587688446045
I0306 04:53:58.109566 139789021992704 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.8321675062179565, loss=1.4750057458877563
I0306 04:54:31.736051 139789013600000 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.9811824560165405, loss=1.503233790397644
I0306 04:55:05.361230 139789021992704 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.8624054193496704, loss=1.477804183959961
I0306 04:55:39.023987 139789013600000 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.248509168624878, loss=1.5945382118225098
I0306 04:56:12.613988 139789021992704 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.1723926067352295, loss=1.6059201955795288
I0306 04:56:46.211583 139789013600000 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.048919200897217, loss=1.5135409832000732
I0306 04:57:19.768264 139789021992704 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.0031001567840576, loss=1.6230710744857788
I0306 04:57:53.360003 139789013600000 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.974435806274414, loss=1.4759985208511353
I0306 04:58:27.017651 139789021992704 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.929591178894043, loss=1.5438209772109985
I0306 04:58:57.079118 139951089751872 spec.py:321] Evaluating on the training split.
I0306 04:59:03.325619 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 04:59:14.058385 139951089751872 spec.py:349] Evaluating on the test split.
I0306 04:59:16.271320 139951089751872 submission_runner.py:411] Time since start: 22359.48s, 	Step: 63691, 	{'train/accuracy': 0.7365473508834839, 'train/loss': 1.001426100730896, 'validation/accuracy': 0.6548999547958374, 'validation/loss': 1.4198815822601318, 'validation/num_examples': 50000, 'test/accuracy': 0.5295000076293945, 'test/loss': 2.1336960792541504, 'test/num_examples': 10000, 'score': 21474.428068876266, 'total_duration': 22359.483619451523, 'accumulated_submission_time': 21474.428068876266, 'accumulated_eval_time': 881.3823571205139, 'accumulated_logging_time': 1.397477388381958}
I0306 04:59:16.298642 139787369432832 logging_writer.py:48] [63691] accumulated_eval_time=881.382357, accumulated_logging_time=1.397477, accumulated_submission_time=21474.428069, global_step=63691, preemption_count=0, score=21474.428069, test/accuracy=0.529500, test/loss=2.133696, test/num_examples=10000, total_duration=22359.483619, train/accuracy=0.736547, train/loss=1.001426, validation/accuracy=0.654900, validation/loss=1.419882, validation/num_examples=50000
I0306 04:59:19.660882 139787562383104 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.0818843841552734, loss=1.553908348083496
I0306 04:59:53.250763 139787369432832 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.818657636642456, loss=1.4762650728225708
I0306 05:00:26.821104 139787562383104 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9343098402023315, loss=1.6413027048110962
I0306 05:01:00.382019 139787369432832 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.0635852813720703, loss=1.66022789478302
I0306 05:01:34.050694 139787562383104 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.1431751251220703, loss=1.5470157861709595
I0306 05:02:07.655266 139787369432832 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9481875896453857, loss=1.6499481201171875
I0306 05:02:41.246760 139787562383104 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.2164008617401123, loss=1.629494309425354
I0306 05:03:14.829805 139787369432832 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.9817357063293457, loss=1.5269262790679932
I0306 05:03:48.460167 139787562383104 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.953205943107605, loss=1.5679314136505127
I0306 05:04:22.101519 139787369432832 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.920283555984497, loss=1.4890096187591553
I0306 05:04:55.736639 139787562383104 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.0289645195007324, loss=1.5074734687805176
I0306 05:05:29.395983 139787369432832 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.9765456914901733, loss=1.6104280948638916
I0306 05:06:03.029556 139787562383104 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.463592529296875, loss=1.472273349761963
I0306 05:06:36.660946 139787369432832 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.2005863189697266, loss=1.5531938076019287
I0306 05:07:10.322582 139787562383104 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.9649896621704102, loss=1.5454949140548706
I0306 05:07:43.979152 139787369432832 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.221769332885742, loss=1.541083574295044
I0306 05:07:46.471642 139951089751872 spec.py:321] Evaluating on the training split.
I0306 05:07:52.604109 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 05:08:02.393753 139951089751872 spec.py:349] Evaluating on the test split.
I0306 05:08:04.631748 139951089751872 submission_runner.py:411] Time since start: 22887.84s, 	Step: 65209, 	{'train/accuracy': 0.7339963316917419, 'train/loss': 1.0098152160644531, 'validation/accuracy': 0.6584599614143372, 'validation/loss': 1.3867906332015991, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.088006019592285, 'test/num_examples': 10000, 'score': 21984.534370660782, 'total_duration': 22887.84405231476, 'accumulated_submission_time': 21984.534370660782, 'accumulated_eval_time': 899.5424103736877, 'accumulated_logging_time': 1.4358513355255127}
I0306 05:08:04.660266 139787369432832 logging_writer.py:48] [65209] accumulated_eval_time=899.542410, accumulated_logging_time=1.435851, accumulated_submission_time=21984.534371, global_step=65209, preemption_count=0, score=21984.534371, test/accuracy=0.537700, test/loss=2.088006, test/num_examples=10000, total_duration=22887.844052, train/accuracy=0.733996, train/loss=1.009815, validation/accuracy=0.658460, validation/loss=1.386791, validation/num_examples=50000
I0306 05:08:35.576582 139789005207296 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.7857862710952759, loss=1.4110431671142578
I0306 05:09:09.192660 139787369432832 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.949959397315979, loss=1.5696587562561035
I0306 05:09:42.852134 139789005207296 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.2631030082702637, loss=1.5319490432739258
I0306 05:10:16.462281 139787369432832 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.9559401273727417, loss=1.506314754486084
I0306 05:10:50.109175 139789005207296 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.075974464416504, loss=1.5503171682357788
I0306 05:11:23.722027 139787369432832 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.9837673902511597, loss=1.5236644744873047
I0306 05:11:57.367543 139789005207296 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.1889352798461914, loss=1.4605804681777954
I0306 05:12:30.995469 139787369432832 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.2014729976654053, loss=1.6302294731140137
I0306 05:13:04.646008 139789005207296 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.150631904602051, loss=1.4752047061920166
I0306 05:13:38.428656 139787369432832 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.8094271421432495, loss=1.5225011110305786
I0306 05:14:12.079867 139789005207296 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.9443303346633911, loss=1.5543489456176758
I0306 05:14:45.711149 139787369432832 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.7733404636383057, loss=1.4734866619110107
I0306 05:15:19.378824 139789005207296 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.063152551651001, loss=1.440453052520752
I0306 05:15:52.996886 139787369432832 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.0531156063079834, loss=1.5761443376541138
I0306 05:16:26.681762 139789005207296 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.8207863569259644, loss=1.4370403289794922
I0306 05:16:34.885185 139951089751872 spec.py:321] Evaluating on the training split.
I0306 05:16:41.166110 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 05:16:49.919925 139951089751872 spec.py:349] Evaluating on the test split.
I0306 05:16:52.179131 139951089751872 submission_runner.py:411] Time since start: 23415.39s, 	Step: 66726, 	{'train/accuracy': 0.7247887253761292, 'train/loss': 1.0592175722122192, 'validation/accuracy': 0.6513000130653381, 'validation/loss': 1.425256371498108, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.1146953105926514, 'test/num_examples': 10000, 'score': 22494.692160129547, 'total_duration': 23415.391426324844, 'accumulated_submission_time': 22494.692160129547, 'accumulated_eval_time': 916.8363063335419, 'accumulated_logging_time': 1.4768211841583252}
I0306 05:16:52.212068 139787369432832 logging_writer.py:48] [66726] accumulated_eval_time=916.836306, accumulated_logging_time=1.476821, accumulated_submission_time=22494.692160, global_step=66726, preemption_count=0, score=22494.692160, test/accuracy=0.529100, test/loss=2.114695, test/num_examples=10000, total_duration=23415.391426, train/accuracy=0.724789, train/loss=1.059218, validation/accuracy=0.651300, validation/loss=1.425256, validation/num_examples=50000
I0306 05:17:17.446506 139787562383104 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.121206283569336, loss=1.5621106624603271
I0306 05:17:51.051234 139787369432832 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.356029748916626, loss=1.5157537460327148
I0306 05:18:24.690242 139787562383104 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.9620996713638306, loss=1.558110237121582
I0306 05:18:58.309259 139787369432832 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.327110767364502, loss=1.6445565223693848
I0306 05:19:31.969212 139787562383104 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.067786693572998, loss=1.4726756811141968
I0306 05:20:05.654425 139787369432832 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.993643879890442, loss=1.5211067199707031
I0306 05:20:39.290947 139787562383104 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.9467989206314087, loss=1.4859511852264404
I0306 05:21:12.908360 139787369432832 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.268672466278076, loss=1.4566236734390259
I0306 05:21:46.601856 139787562383104 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.848204493522644, loss=1.4033364057540894
I0306 05:22:20.255876 139787369432832 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.294109344482422, loss=1.5953619480133057
I0306 05:22:53.912769 139787562383104 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.995197057723999, loss=1.5278911590576172
I0306 05:23:27.570340 139787369432832 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.105677604675293, loss=1.5287232398986816
I0306 05:24:01.249882 139787562383104 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.9928823709487915, loss=1.5368107557296753
I0306 05:24:34.881880 139787369432832 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.1175920963287354, loss=1.5337233543395996
I0306 05:25:08.546247 139787562383104 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.1675899028778076, loss=1.4793028831481934
I0306 05:25:22.476369 139951089751872 spec.py:321] Evaluating on the training split.
I0306 05:25:28.680729 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 05:25:37.540349 139951089751872 spec.py:349] Evaluating on the test split.
I0306 05:25:39.807231 139951089751872 submission_runner.py:411] Time since start: 23943.02s, 	Step: 68243, 	{'train/accuracy': 0.7330994606018066, 'train/loss': 1.0156515836715698, 'validation/accuracy': 0.6629999876022339, 'validation/loss': 1.369211196899414, 'validation/num_examples': 50000, 'test/accuracy': 0.531000018119812, 'test/loss': 2.121507167816162, 'test/num_examples': 10000, 'score': 23004.892677783966, 'total_duration': 23943.019522428513, 'accumulated_submission_time': 23004.892677783966, 'accumulated_eval_time': 934.1671011447906, 'accumulated_logging_time': 1.5193002223968506}
I0306 05:25:39.837935 139789030385408 logging_writer.py:48] [68243] accumulated_eval_time=934.167101, accumulated_logging_time=1.519300, accumulated_submission_time=23004.892678, global_step=68243, preemption_count=0, score=23004.892678, test/accuracy=0.531000, test/loss=2.121507, test/num_examples=10000, total_duration=23943.019522, train/accuracy=0.733099, train/loss=1.015652, validation/accuracy=0.663000, validation/loss=1.369211, validation/num_examples=50000
I0306 05:25:59.460798 139789038778112 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.9257044792175293, loss=1.4807300567626953
I0306 05:26:33.067521 139789030385408 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.150254249572754, loss=1.4902498722076416
I0306 05:27:06.680848 139789038778112 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.8955762386322021, loss=1.5374021530151367
I0306 05:27:40.315989 139789030385408 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.9843279123306274, loss=1.5346729755401611
I0306 05:28:13.961442 139789038778112 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.050718307495117, loss=1.5087484121322632
I0306 05:28:47.616240 139789030385408 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.031928539276123, loss=1.4966404438018799
I0306 05:29:21.265526 139789038778112 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0296130180358887, loss=1.5163991451263428
I0306 05:29:54.896141 139789030385408 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.012328624725342, loss=1.5689480304718018
I0306 05:30:28.538122 139789038778112 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.0985937118530273, loss=1.5442231893539429
I0306 05:31:02.177234 139789030385408 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.8847346305847168, loss=1.4841077327728271
I0306 05:31:35.807955 139789038778112 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.9344815015792847, loss=1.3997523784637451
I0306 05:32:09.466001 139789030385408 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.377387523651123, loss=1.6642787456512451
I0306 05:32:43.087463 139789038778112 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.259598970413208, loss=1.5320792198181152
I0306 05:33:16.721949 139789030385408 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.1756742000579834, loss=1.5848398208618164
I0306 05:33:50.341445 139789038778112 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.0322344303131104, loss=1.5558927059173584
I0306 05:34:10.003078 139951089751872 spec.py:321] Evaluating on the training split.
I0306 05:34:16.126334 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 05:34:25.747295 139951089751872 spec.py:349] Evaluating on the test split.
I0306 05:34:28.032260 139951089751872 submission_runner.py:411] Time since start: 24471.24s, 	Step: 69760, 	{'train/accuracy': 0.7288743257522583, 'train/loss': 1.038947582244873, 'validation/accuracy': 0.6620599627494812, 'validation/loss': 1.3821091651916504, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.0804221630096436, 'test/num_examples': 10000, 'score': 23514.99297809601, 'total_duration': 24471.2444729805, 'accumulated_submission_time': 23514.99297809601, 'accumulated_eval_time': 952.1961376667023, 'accumulated_logging_time': 1.5602679252624512}
I0306 05:34:28.073686 139787562383104 logging_writer.py:48] [69760] accumulated_eval_time=952.196138, accumulated_logging_time=1.560268, accumulated_submission_time=23514.992978, global_step=69760, preemption_count=0, score=23514.992978, test/accuracy=0.532400, test/loss=2.080422, test/num_examples=10000, total_duration=24471.244473, train/accuracy=0.728874, train/loss=1.038948, validation/accuracy=0.662060, validation/loss=1.382109, validation/num_examples=50000
I0306 05:34:41.832597 139788996814592 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.9875922203063965, loss=1.566630244255066
I0306 05:35:15.416972 139787562383104 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.0986714363098145, loss=1.574758529663086
I0306 05:35:49.000677 139788996814592 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.9189947843551636, loss=1.5716516971588135
I0306 05:36:22.648495 139787562383104 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.9245818853378296, loss=1.4996784925460815
I0306 05:36:56.299660 139788996814592 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.1008832454681396, loss=1.5447742938995361
I0306 05:37:29.975433 139787562383104 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.0048985481262207, loss=1.5129458904266357
I0306 05:38:03.784254 139788996814592 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.1058802604675293, loss=1.467692494392395
I0306 05:38:37.452193 139787562383104 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.180260181427002, loss=1.695258617401123
I0306 05:39:11.085884 139788996814592 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.427061080932617, loss=1.6849688291549683
I0306 05:39:44.738904 139787562383104 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.0944902896881104, loss=1.534043312072754
I0306 05:40:18.383569 139788996814592 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.818827748298645, loss=1.4714818000793457
I0306 05:40:52.035417 139787562383104 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.043484926223755, loss=1.517706036567688
I0306 05:41:25.666140 139788996814592 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.1669344902038574, loss=1.6403357982635498
I0306 05:41:59.325294 139787562383104 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.8951354026794434, loss=1.409029483795166
I0306 05:42:32.927322 139788996814592 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.386415958404541, loss=1.576361060142517
I0306 05:42:58.322281 139951089751872 spec.py:321] Evaluating on the training split.
I0306 05:43:04.389211 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 05:43:13.252144 139951089751872 spec.py:349] Evaluating on the test split.
I0306 05:43:15.474034 139951089751872 submission_runner.py:411] Time since start: 24998.69s, 	Step: 71277, 	{'train/accuracy': 0.7565967440605164, 'train/loss': 0.9106816649436951, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.408387303352356, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.185938835144043, 'test/num_examples': 10000, 'score': 24025.17360162735, 'total_duration': 24998.686302900314, 'accumulated_submission_time': 24025.17360162735, 'accumulated_eval_time': 969.347799539566, 'accumulated_logging_time': 1.6115102767944336}
I0306 05:43:15.501881 139787369432832 logging_writer.py:48] [71277] accumulated_eval_time=969.347800, accumulated_logging_time=1.611510, accumulated_submission_time=24025.173602, global_step=71277, preemption_count=0, score=24025.173602, test/accuracy=0.527300, test/loss=2.185939, test/num_examples=10000, total_duration=24998.686303, train/accuracy=0.756597, train/loss=0.910682, validation/accuracy=0.657640, validation/loss=1.408387, validation/num_examples=50000
I0306 05:43:23.557187 139787562383104 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.055307388305664, loss=1.6028615236282349
I0306 05:43:57.184907 139787369432832 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.1181623935699463, loss=1.4468438625335693
I0306 05:44:30.864416 139787562383104 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.289858102798462, loss=1.565028190612793
I0306 05:45:04.456248 139787369432832 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.9795550107955933, loss=1.5228679180145264
I0306 05:45:38.038997 139787562383104 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.0674662590026855, loss=1.4432381391525269
I0306 05:46:11.725674 139787369432832 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.8980642557144165, loss=1.4292300939559937
I0306 05:46:45.387281 139787562383104 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.21659517288208, loss=1.5338220596313477
I0306 05:47:19.012772 139787369432832 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.9734619855880737, loss=1.5363885164260864
I0306 05:47:52.640290 139787562383104 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.012824058532715, loss=1.4474899768829346
I0306 05:48:26.271271 139787369432832 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.0026206970214844, loss=1.5442081689834595
I0306 05:48:59.947928 139787562383104 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.2676703929901123, loss=1.4867299795150757
I0306 05:49:33.577425 139787369432832 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.8877158164978027, loss=1.4314682483673096
I0306 05:50:07.258794 139787562383104 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.9821826219558716, loss=1.49759840965271
I0306 05:50:40.822946 139787369432832 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.116917371749878, loss=1.4900901317596436
I0306 05:51:14.419027 139787562383104 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.283214569091797, loss=1.5669952630996704
I0306 05:51:45.492623 139951089751872 spec.py:321] Evaluating on the training split.
I0306 05:51:51.579706 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 05:52:00.478346 139951089751872 spec.py:349] Evaluating on the test split.
I0306 05:52:02.749017 139951089751872 submission_runner.py:411] Time since start: 25525.96s, 	Step: 72794, 	{'train/accuracy': 0.7460139989852905, 'train/loss': 0.9631520509719849, 'validation/accuracy': 0.6609399914741516, 'validation/loss': 1.3794595003128052, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.1230790615081787, 'test/num_examples': 10000, 'score': 24535.09615302086, 'total_duration': 25525.9612326622, 'accumulated_submission_time': 24535.09615302086, 'accumulated_eval_time': 986.6040508747101, 'accumulated_logging_time': 1.6504340171813965}
I0306 05:52:02.778259 139787361040128 logging_writer.py:48] [72794] accumulated_eval_time=986.604051, accumulated_logging_time=1.650434, accumulated_submission_time=24535.096153, global_step=72794, preemption_count=0, score=24535.096153, test/accuracy=0.529800, test/loss=2.123079, test/num_examples=10000, total_duration=25525.961233, train/accuracy=0.746014, train/loss=0.963152, validation/accuracy=0.660940, validation/loss=1.379460, validation/num_examples=50000
I0306 05:52:05.133498 139787369432832 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9701087474822998, loss=1.5315977334976196
I0306 05:52:38.807052 139787361040128 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.0177767276763916, loss=1.43793523311615
I0306 05:53:12.495371 139787369432832 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.0732903480529785, loss=1.6195440292358398
I0306 05:53:46.136899 139787361040128 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.0037448406219482, loss=1.6943734884262085
I0306 05:54:19.827076 139787369432832 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.2547495365142822, loss=1.5738422870635986
I0306 05:54:53.470911 139787361040128 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.812217116355896, loss=1.432299256324768
I0306 05:55:27.148319 139787369432832 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.828635811805725, loss=1.3562122583389282
I0306 05:56:00.802421 139787361040128 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.2192323207855225, loss=1.550140142440796
I0306 05:56:34.488916 139787369432832 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.055938482284546, loss=1.4547395706176758
I0306 05:57:08.103760 139787361040128 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.078852653503418, loss=1.4750977754592896
I0306 05:57:41.756181 139787369432832 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.0006773471832275, loss=1.46148681640625
I0306 05:58:15.366630 139787361040128 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.2164156436920166, loss=1.473770022392273
I0306 05:58:49.019520 139787369432832 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.1333587169647217, loss=1.5714280605316162
I0306 05:59:22.630225 139787361040128 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.2053778171539307, loss=1.565942645072937
I0306 05:59:56.291942 139787369432832 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.0062336921691895, loss=1.5292627811431885
I0306 06:00:29.907111 139787361040128 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.09478759765625, loss=1.4058760404586792
I0306 06:00:33.078588 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:00:39.148162 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:00:49.192597 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:00:51.395926 139951089751872 submission_runner.py:411] Time since start: 26054.61s, 	Step: 74311, 	{'train/accuracy': 0.7478475570678711, 'train/loss': 0.971817135810852, 'validation/accuracy': 0.6683200001716614, 'validation/loss': 1.3521226644515991, 'validation/num_examples': 50000, 'test/accuracy': 0.5426000356674194, 'test/loss': 2.0722763538360596, 'test/num_examples': 10000, 'score': 25045.33080291748, 'total_duration': 26054.60822916031, 'accumulated_submission_time': 25045.33080291748, 'accumulated_eval_time': 1004.921329498291, 'accumulated_logging_time': 1.6894316673278809}
I0306 06:00:51.429367 139789005207296 logging_writer.py:48] [74311] accumulated_eval_time=1004.921329, accumulated_logging_time=1.689432, accumulated_submission_time=25045.330803, global_step=74311, preemption_count=0, score=25045.330803, test/accuracy=0.542600, test/loss=2.072276, test/num_examples=10000, total_duration=26054.608229, train/accuracy=0.747848, train/loss=0.971817, validation/accuracy=0.668320, validation/loss=1.352123, validation/num_examples=50000
I0306 06:01:21.670107 139789013600000 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.0506858825683594, loss=1.625953197479248
I0306 06:01:55.297705 139789005207296 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.842123031616211, loss=1.3833911418914795
I0306 06:02:28.964564 139789013600000 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.1177520751953125, loss=1.5622698068618774
I0306 06:03:02.542733 139789005207296 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.2257585525512695, loss=1.5067938566207886
I0306 06:03:36.150816 139789013600000 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.997912883758545, loss=1.3860474824905396
I0306 06:04:09.777788 139789005207296 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.1626648902893066, loss=1.5661141872406006
I0306 06:04:43.431571 139789013600000 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.3035616874694824, loss=1.4863085746765137
I0306 06:05:17.069844 139789005207296 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.8611316680908203, loss=1.4937130212783813
I0306 06:05:50.702401 139789013600000 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.130382537841797, loss=1.6028647422790527
I0306 06:06:24.336957 139789005207296 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.0306973457336426, loss=1.5502657890319824
I0306 06:06:57.956583 139789013600000 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.2696211338043213, loss=1.419755220413208
I0306 06:07:31.593475 139789005207296 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.4003958702087402, loss=1.484309434890747
I0306 06:08:05.225063 139789013600000 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.078195095062256, loss=1.4777909517288208
I0306 06:08:38.887032 139789005207296 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.340846538543701, loss=1.5594813823699951
I0306 06:09:12.476530 139789013600000 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.2001101970672607, loss=1.538602590560913
I0306 06:09:21.698612 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:09:28.473496 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:09:38.127251 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:09:40.381397 139951089751872 submission_runner.py:411] Time since start: 26583.59s, 	Step: 75829, 	{'train/accuracy': 0.7421077489852905, 'train/loss': 0.9846024513244629, 'validation/accuracy': 0.6684799790382385, 'validation/loss': 1.3483023643493652, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.0896012783050537, 'test/num_examples': 10000, 'score': 25555.535472393036, 'total_duration': 26583.593682527542, 'accumulated_submission_time': 25555.535472393036, 'accumulated_eval_time': 1023.604052066803, 'accumulated_logging_time': 1.732506513595581}
I0306 06:09:40.412198 139787562383104 logging_writer.py:48] [75829] accumulated_eval_time=1023.604052, accumulated_logging_time=1.732507, accumulated_submission_time=25555.535472, global_step=75829, preemption_count=0, score=25555.535472, test/accuracy=0.538800, test/loss=2.089601, test/num_examples=10000, total_duration=26583.593683, train/accuracy=0.742108, train/loss=0.984602, validation/accuracy=0.668480, validation/loss=1.348302, validation/num_examples=50000
I0306 06:10:04.592589 139788996814592 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.1728107929229736, loss=1.4058494567871094
I0306 06:10:38.187423 139787562383104 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.069209575653076, loss=1.4408533573150635
I0306 06:11:11.855983 139788996814592 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.1365437507629395, loss=1.421667218208313
I0306 06:11:45.498332 139787562383104 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.004316568374634, loss=1.4288725852966309
I0306 06:12:19.166763 139788996814592 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1379401683807373, loss=1.361459493637085
I0306 06:12:52.832000 139787562383104 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.1342966556549072, loss=1.5229891538619995
I0306 06:13:26.506795 139788996814592 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.167696237564087, loss=1.5579684972763062
I0306 06:14:00.113278 139787562383104 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.2574541568756104, loss=1.543776035308838
I0306 06:14:33.787023 139788996814592 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.9771243333816528, loss=1.5365188121795654
I0306 06:15:07.556200 139787562383104 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.235771656036377, loss=1.5888901948928833
I0306 06:15:41.208901 139788996814592 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.0381031036376953, loss=1.4876132011413574
I0306 06:16:14.827019 139787562383104 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.3308756351470947, loss=1.437631607055664
I0306 06:16:48.491642 139788996814592 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.0616390705108643, loss=1.4932717084884644
I0306 06:17:22.099729 139787562383104 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.000135898590088, loss=1.448482871055603
I0306 06:17:55.750950 139788996814592 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.193901777267456, loss=1.5224038362503052
I0306 06:18:10.681121 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:18:16.692505 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:18:25.915197 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:18:28.164526 139951089751872 submission_runner.py:411] Time since start: 27111.38s, 	Step: 77346, 	{'train/accuracy': 0.7206632494926453, 'train/loss': 1.0757194757461548, 'validation/accuracy': 0.6527599692344666, 'validation/loss': 1.4330989122390747, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.150460720062256, 'test/num_examples': 10000, 'score': 26065.737631559372, 'total_duration': 27111.376794815063, 'accumulated_submission_time': 26065.737631559372, 'accumulated_eval_time': 1041.087366104126, 'accumulated_logging_time': 1.7743992805480957}
I0306 06:18:28.203054 139787361040128 logging_writer.py:48] [77346] accumulated_eval_time=1041.087366, accumulated_logging_time=1.774399, accumulated_submission_time=26065.737632, global_step=77346, preemption_count=0, score=26065.737632, test/accuracy=0.529400, test/loss=2.150461, test/num_examples=10000, total_duration=27111.376795, train/accuracy=0.720663, train/loss=1.075719, validation/accuracy=0.652760, validation/loss=1.433099, validation/num_examples=50000
I0306 06:18:47.534312 139787369432832 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.262763500213623, loss=1.5029536485671997
I0306 06:19:21.093768 139787361040128 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.9921153783798218, loss=1.4746577739715576
I0306 06:19:54.728693 139787369432832 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.030334711074829, loss=1.4557468891143799
I0306 06:20:28.372915 139787361040128 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.210848093032837, loss=1.4816300868988037
I0306 06:21:02.071774 139787369432832 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.9872283935546875, loss=1.3644558191299438
I0306 06:21:35.653764 139787361040128 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.1915767192840576, loss=1.453552007675171
I0306 06:22:09.231171 139787369432832 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.3169217109680176, loss=1.5366724729537964
I0306 06:22:42.801338 139787361040128 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.192291259765625, loss=1.4368889331817627
I0306 06:23:16.465125 139787369432832 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.2748076915740967, loss=1.508191466331482
I0306 06:23:50.100098 139787361040128 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.130208730697632, loss=1.5169014930725098
I0306 06:24:23.753156 139787369432832 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.0567543506622314, loss=1.50713312625885
I0306 06:24:57.382401 139787361040128 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.061147689819336, loss=1.4010549783706665
I0306 06:25:31.028175 139787369432832 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.9413570165634155, loss=1.4247361421585083
I0306 06:26:04.678276 139787361040128 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.0794758796691895, loss=1.512430191040039
I0306 06:26:38.328619 139787369432832 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.307352066040039, loss=1.4737745523452759
I0306 06:26:58.347908 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:27:04.475130 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:27:13.349494 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:27:15.641888 139951089751872 submission_runner.py:411] Time since start: 27638.85s, 	Step: 78861, 	{'train/accuracy': 0.7466318607330322, 'train/loss': 0.9687818288803101, 'validation/accuracy': 0.6676799654960632, 'validation/loss': 1.3544172048568726, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.095369815826416, 'test/num_examples': 10000, 'score': 26574.949008226395, 'total_duration': 27638.854197502136, 'accumulated_submission_time': 26574.949008226395, 'accumulated_eval_time': 1058.3813076019287, 'accumulated_logging_time': 2.6917877197265625}
I0306 06:27:15.675554 139789005207296 logging_writer.py:48] [78861] accumulated_eval_time=1058.381308, accumulated_logging_time=2.691788, accumulated_submission_time=26574.949008, global_step=78861, preemption_count=0, score=26574.949008, test/accuracy=0.544100, test/loss=2.095370, test/num_examples=10000, total_duration=27638.854198, train/accuracy=0.746632, train/loss=0.968782, validation/accuracy=0.667680, validation/loss=1.354417, validation/num_examples=50000
I0306 06:27:29.161652 139789021992704 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.10962176322937, loss=1.3856806755065918
I0306 06:28:02.833627 139789005207296 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.229792356491089, loss=1.5330946445465088
I0306 06:28:36.440333 139789021992704 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.2503647804260254, loss=1.5450170040130615
I0306 06:29:10.086068 139789005207296 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.0212652683258057, loss=1.3788357973098755
I0306 06:29:43.694458 139789021992704 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.124056339263916, loss=1.5303971767425537
I0306 06:30:17.350086 139789005207296 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.9862115383148193, loss=1.5281492471694946
I0306 06:30:50.977441 139789021992704 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.223541259765625, loss=1.43939208984375
I0306 06:31:24.640088 139789005207296 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.2851409912109375, loss=1.4615094661712646
I0306 06:31:58.274022 139789021992704 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.0945804119110107, loss=1.4359474182128906
I0306 06:32:31.916126 139789005207296 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.2958641052246094, loss=1.457042932510376
I0306 06:33:05.580067 139789021992704 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.080251932144165, loss=1.431757926940918
I0306 06:33:39.179734 139789005207296 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.3837404251098633, loss=1.4775508642196655
I0306 06:34:12.768860 139789021992704 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.0617282390594482, loss=1.4984768629074097
I0306 06:34:46.367488 139789005207296 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.163454294204712, loss=1.4454883337020874
I0306 06:35:19.967326 139789021992704 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.2383391857147217, loss=1.4849791526794434
I0306 06:35:45.695863 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:35:51.719071 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:36:00.626837 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:36:02.916678 139951089751872 submission_runner.py:411] Time since start: 28166.13s, 	Step: 80378, 	{'train/accuracy': 0.7697305083274841, 'train/loss': 0.8728119730949402, 'validation/accuracy': 0.6732000112533569, 'validation/loss': 1.3282780647277832, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.056852340698242, 'test/num_examples': 10000, 'score': 27084.90593457222, 'total_duration': 28166.128904104233, 'accumulated_submission_time': 27084.90593457222, 'accumulated_eval_time': 1075.6019878387451, 'accumulated_logging_time': 2.734912633895874}
I0306 06:36:02.952037 139787562383104 logging_writer.py:48] [80378] accumulated_eval_time=1075.601988, accumulated_logging_time=2.734913, accumulated_submission_time=27084.905935, global_step=80378, preemption_count=0, score=27084.905935, test/accuracy=0.543700, test/loss=2.056852, test/num_examples=10000, total_duration=28166.128904, train/accuracy=0.769731, train/loss=0.872812, validation/accuracy=0.673200, validation/loss=1.328278, validation/num_examples=50000
I0306 06:36:10.699997 139788996814592 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.2761151790618896, loss=1.4820473194122314
I0306 06:36:44.380661 139787562383104 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.536043405532837, loss=1.4713484048843384
I0306 06:37:18.029227 139788996814592 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.1705493927001953, loss=1.5229692459106445
I0306 06:37:51.674613 139787562383104 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.9681744575500488, loss=1.5226976871490479
I0306 06:38:25.316637 139788996814592 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.307525634765625, loss=1.4349075555801392
I0306 06:38:59.164486 139787562383104 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.2393763065338135, loss=1.5245420932769775
I0306 06:39:32.813584 139788996814592 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.0690653324127197, loss=1.492692470550537
I0306 06:40:06.466675 139787562383104 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1505401134490967, loss=1.4810349941253662
I0306 06:40:40.104347 139788996814592 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.875380039215088, loss=1.4411931037902832
I0306 06:41:13.751291 139787562383104 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.0227065086364746, loss=1.4094794988632202
I0306 06:41:47.350796 139788996814592 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.3456358909606934, loss=1.4157776832580566
I0306 06:42:21.004204 139787562383104 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.289844512939453, loss=1.4059491157531738
I0306 06:42:54.640532 139788996814592 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.0697038173675537, loss=1.5092828273773193
I0306 06:43:28.286799 139787562383104 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.9838916063308716, loss=1.4423799514770508
I0306 06:44:01.915405 139788996814592 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.2418792247772217, loss=1.4579417705535889
I0306 06:44:33.023304 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:44:39.054929 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:44:47.950979 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:44:50.229131 139951089751872 submission_runner.py:411] Time since start: 28693.44s, 	Step: 81894, 	{'train/accuracy': 0.748066782951355, 'train/loss': 0.9496461153030396, 'validation/accuracy': 0.6683799624443054, 'validation/loss': 1.3535780906677246, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.098299026489258, 'test/num_examples': 10000, 'score': 27594.912347316742, 'total_duration': 28693.44144463539, 'accumulated_submission_time': 27594.912347316742, 'accumulated_eval_time': 1092.8077733516693, 'accumulated_logging_time': 2.7800300121307373}
I0306 06:44:50.259892 139789030385408 logging_writer.py:48] [81894] accumulated_eval_time=1092.807773, accumulated_logging_time=2.780030, accumulated_submission_time=27594.912347, global_step=81894, preemption_count=0, score=27594.912347, test/accuracy=0.542500, test/loss=2.098299, test/num_examples=10000, total_duration=28693.441445, train/accuracy=0.748067, train/loss=0.949646, validation/accuracy=0.668380, validation/loss=1.353578, validation/num_examples=50000
I0306 06:44:52.608817 139789038778112 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.105541706085205, loss=1.3728268146514893
I0306 06:45:26.296973 139789030385408 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.123403549194336, loss=1.4359763860702515
I0306 06:45:59.954600 139789038778112 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.1479735374450684, loss=1.4604136943817139
I0306 06:46:33.614178 139789030385408 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.2152321338653564, loss=1.4800599813461304
I0306 06:47:07.289073 139789038778112 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.368809938430786, loss=1.4922558069229126
I0306 06:47:40.909119 139789030385408 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.2748072147369385, loss=1.449979543685913
I0306 06:48:14.554648 139789038778112 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.1863038539886475, loss=1.4125747680664062
I0306 06:48:48.187580 139789030385408 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.2989461421966553, loss=1.5336716175079346
I0306 06:49:21.827944 139789038778112 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.3403244018554688, loss=1.320198893547058
I0306 06:49:55.460727 139789030385408 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.1181445121765137, loss=1.350281834602356
I0306 06:50:29.108475 139789038778112 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.469280242919922, loss=1.4266729354858398
I0306 06:51:02.731002 139789030385408 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.104907989501953, loss=1.4569802284240723
I0306 06:51:36.357261 139789038778112 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.227308988571167, loss=1.5007350444793701
I0306 06:52:09.943543 139789030385408 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.1305320262908936, loss=1.4069900512695312
I0306 06:52:43.579941 139789038778112 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.152390718460083, loss=1.372194528579712
I0306 06:53:17.218225 139789030385408 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1147055625915527, loss=1.3333632946014404
I0306 06:53:20.391360 139951089751872 spec.py:321] Evaluating on the training split.
I0306 06:53:26.423478 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 06:53:35.464672 139951089751872 spec.py:349] Evaluating on the test split.
I0306 06:53:37.747362 139951089751872 submission_runner.py:411] Time since start: 29220.96s, 	Step: 83411, 	{'train/accuracy': 0.7523118257522583, 'train/loss': 0.942414402961731, 'validation/accuracy': 0.6723600029945374, 'validation/loss': 1.330456018447876, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.059842586517334, 'test/num_examples': 10000, 'score': 28104.9787709713, 'total_duration': 29220.95869898796, 'accumulated_submission_time': 28104.9787709713, 'accumulated_eval_time': 1110.1627497673035, 'accumulated_logging_time': 2.821474552154541}
I0306 06:53:37.782629 139788996814592 logging_writer.py:48] [83411] accumulated_eval_time=1110.162750, accumulated_logging_time=2.821475, accumulated_submission_time=28104.978771, global_step=83411, preemption_count=0, score=28104.978771, test/accuracy=0.542100, test/loss=2.059843, test/num_examples=10000, total_duration=29220.958699, train/accuracy=0.752312, train/loss=0.942414, validation/accuracy=0.672360, validation/loss=1.330456, validation/num_examples=50000
I0306 06:54:08.010332 139789005207296 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2637546062469482, loss=1.384250521659851
I0306 06:54:41.575432 139788996814592 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.3139095306396484, loss=1.4675487279891968
I0306 06:55:15.169431 139789005207296 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.1528754234313965, loss=1.3696341514587402
I0306 06:55:48.799063 139788996814592 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.2126903533935547, loss=1.4494712352752686
I0306 06:56:22.462044 139789005207296 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.4875473976135254, loss=1.4086387157440186
I0306 06:56:56.141608 139788996814592 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.2080721855163574, loss=1.4675254821777344
I0306 06:57:29.915760 139789005207296 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.9967801570892334, loss=1.3292592763900757
I0306 06:58:03.557828 139788996814592 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.081078052520752, loss=1.4124382734298706
I0306 06:58:37.198088 139789005207296 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.0984177589416504, loss=1.343071460723877
I0306 06:59:10.834259 139788996814592 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.133148431777954, loss=1.4168438911437988
I0306 06:59:44.469856 139789005207296 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2478466033935547, loss=1.475022792816162
I0306 07:00:18.120376 139788996814592 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.1195719242095947, loss=1.4066649675369263
I0306 07:00:51.753209 139789005207296 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.2820804119110107, loss=1.5242273807525635
I0306 07:01:25.402298 139788996814592 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.3473241329193115, loss=1.4717700481414795
I0306 07:01:59.017933 139789005207296 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.258079767227173, loss=1.4649097919464111
I0306 07:02:07.907277 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:02:13.929238 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:02:22.975168 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:02:25.232607 139951089751872 submission_runner.py:411] Time since start: 29748.44s, 	Step: 84928, 	{'train/accuracy': 0.7465720772743225, 'train/loss': 0.9657593369483948, 'validation/accuracy': 0.6717000007629395, 'validation/loss': 1.3397146463394165, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.0694806575775146, 'test/num_examples': 10000, 'score': 28615.038668632507, 'total_duration': 29748.444899082184, 'accumulated_submission_time': 28615.038668632507, 'accumulated_eval_time': 1127.4880149364471, 'accumulated_logging_time': 2.8666574954986572}
I0306 07:02:25.277683 139787361040128 logging_writer.py:48] [84928] accumulated_eval_time=1127.488015, accumulated_logging_time=2.866657, accumulated_submission_time=28615.038669, global_step=84928, preemption_count=0, score=28615.038669, test/accuracy=0.540000, test/loss=2.069481, test/num_examples=10000, total_duration=29748.444899, train/accuracy=0.746572, train/loss=0.965759, validation/accuracy=0.671700, validation/loss=1.339715, validation/num_examples=50000
I0306 07:02:49.804879 139787369432832 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.1734731197357178, loss=1.3933645486831665
I0306 07:03:23.376084 139787361040128 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.448009729385376, loss=1.4472475051879883
I0306 07:03:57.112273 139787369432832 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.131366014480591, loss=1.3980419635772705
I0306 07:04:30.733764 139787361040128 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.1842446327209473, loss=1.437005639076233
I0306 07:05:04.394153 139787369432832 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.058265447616577, loss=1.373817801475525
I0306 07:05:38.008471 139787361040128 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.1074984073638916, loss=1.4587002992630005
I0306 07:06:11.666063 139787369432832 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.0665173530578613, loss=1.4396982192993164
I0306 07:06:45.280285 139787361040128 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.135528564453125, loss=1.396226167678833
I0306 07:07:18.935562 139787369432832 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.10192608833313, loss=1.3541741371154785
I0306 07:07:52.591558 139787361040128 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.032792568206787, loss=1.334895133972168
I0306 07:08:26.280531 139787369432832 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.1045918464660645, loss=1.3897603750228882
I0306 07:08:59.953203 139787361040128 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.278568744659424, loss=1.4164875745773315
I0306 07:09:33.772758 139787369432832 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.006666660308838, loss=1.3463494777679443
I0306 07:10:07.466250 139787361040128 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.055833578109741, loss=1.3635910749435425
I0306 07:10:41.129076 139787369432832 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.164203405380249, loss=1.4629204273223877
I0306 07:10:55.399722 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:11:01.678224 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:11:10.689067 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:11:12.916560 139951089751872 submission_runner.py:411] Time since start: 30276.13s, 	Step: 86444, 	{'train/accuracy': 0.7580317258834839, 'train/loss': 0.906674325466156, 'validation/accuracy': 0.6821799874305725, 'validation/loss': 1.3001172542572021, 'validation/num_examples': 50000, 'test/accuracy': 0.5557000041007996, 'test/loss': 2.0219106674194336, 'test/num_examples': 10000, 'score': 29125.096694231033, 'total_duration': 30276.128866910934, 'accumulated_submission_time': 29125.096694231033, 'accumulated_eval_time': 1145.0048153400421, 'accumulated_logging_time': 2.9214494228363037}
I0306 07:11:12.947434 139789021992704 logging_writer.py:48] [86444] accumulated_eval_time=1145.004815, accumulated_logging_time=2.921449, accumulated_submission_time=29125.096694, global_step=86444, preemption_count=0, score=29125.096694, test/accuracy=0.555700, test/loss=2.021911, test/num_examples=10000, total_duration=30276.128867, train/accuracy=0.758032, train/loss=0.906674, validation/accuracy=0.682180, validation/loss=1.300117, validation/num_examples=50000
I0306 07:11:32.114041 139789030385408 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.248384952545166, loss=1.515783429145813
I0306 07:12:05.721222 139789021992704 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.5030581951141357, loss=1.4291785955429077
I0306 07:12:39.384919 139789030385408 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3168246746063232, loss=1.3359348773956299
I0306 07:13:13.000607 139789021992704 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.4699079990386963, loss=1.423467755317688
I0306 07:13:46.652855 139789030385408 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.1723642349243164, loss=1.4208967685699463
I0306 07:14:20.243848 139789021992704 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.36458683013916, loss=1.4625344276428223
I0306 07:14:53.908744 139789030385408 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.4917244911193848, loss=1.4965873956680298
I0306 07:15:27.545266 139789021992704 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.1980977058410645, loss=1.3804646730422974
I0306 07:16:01.251132 139789030385408 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.398782968521118, loss=1.4469504356384277
I0306 07:16:34.842703 139789021992704 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.460615396499634, loss=1.419769525527954
I0306 07:17:08.507692 139789030385408 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.192615509033203, loss=1.4473717212677002
I0306 07:17:42.102848 139789021992704 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.1917011737823486, loss=1.3613927364349365
I0306 07:18:15.764569 139789030385408 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.5268588066101074, loss=1.5467575788497925
I0306 07:18:49.423651 139789021992704 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.1426892280578613, loss=1.3309047222137451
I0306 07:19:23.083583 139789030385408 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.4119479656219482, loss=1.4224439859390259
I0306 07:19:43.094167 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:19:49.096687 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:19:57.819974 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:20:00.073094 139951089751872 submission_runner.py:411] Time since start: 30803.28s, 	Step: 87961, 	{'train/accuracy': 0.7918526530265808, 'train/loss': 0.7671975493431091, 'validation/accuracy': 0.6766999959945679, 'validation/loss': 1.3069652318954468, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.0292537212371826, 'test/num_examples': 10000, 'score': 29635.179485797882, 'total_duration': 30803.284975528717, 'accumulated_submission_time': 29635.179485797882, 'accumulated_eval_time': 1161.9832620620728, 'accumulated_logging_time': 2.962131977081299}
I0306 07:20:00.108328 139788996814592 logging_writer.py:48] [87961] accumulated_eval_time=1161.983262, accumulated_logging_time=2.962132, accumulated_submission_time=29635.179486, global_step=87961, preemption_count=0, score=29635.179486, test/accuracy=0.550400, test/loss=2.029254, test/num_examples=10000, total_duration=30803.284976, train/accuracy=0.791853, train/loss=0.767198, validation/accuracy=0.676700, validation/loss=1.306965, validation/num_examples=50000
I0306 07:20:13.558881 139789005207296 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.0731778144836426, loss=1.412408709526062
I0306 07:20:47.142347 139788996814592 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.284740924835205, loss=1.4629693031311035
I0306 07:21:20.763250 139789005207296 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3923118114471436, loss=1.3716697692871094
I0306 07:21:54.430475 139788996814592 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.4289402961730957, loss=1.3869044780731201
I0306 07:22:28.044913 139789005207296 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.34330415725708, loss=1.3996555805206299
I0306 07:23:01.697594 139788996814592 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.2673468589782715, loss=1.4559028148651123
I0306 07:23:35.350975 139789005207296 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.235238552093506, loss=1.264418125152588
I0306 07:24:09.010039 139788996814592 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.269998550415039, loss=1.472835898399353
I0306 07:24:42.631381 139789005207296 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.2285304069519043, loss=1.3914848566055298
I0306 07:25:16.245143 139788996814592 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.223975419998169, loss=1.317639946937561
I0306 07:25:49.891074 139789005207296 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.4222207069396973, loss=1.4944655895233154
I0306 07:26:23.546027 139788996814592 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3320164680480957, loss=1.4516499042510986
I0306 07:26:57.166057 139789005207296 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.4822051525115967, loss=1.4097957611083984
I0306 07:27:30.813424 139788996814592 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.078218936920166, loss=1.3448045253753662
I0306 07:28:04.485129 139789005207296 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.316345691680908, loss=1.5871624946594238
I0306 07:28:30.168564 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:28:36.213565 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:28:45.044669 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:28:47.317889 139951089751872 submission_runner.py:411] Time since start: 31330.53s, 	Step: 89478, 	{'train/accuracy': 0.7750119566917419, 'train/loss': 0.8389298915863037, 'validation/accuracy': 0.6815599799156189, 'validation/loss': 1.3008902072906494, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.012035369873047, 'test/num_examples': 10000, 'score': 30145.17422771454, 'total_duration': 31330.530110120773, 'accumulated_submission_time': 30145.17422771454, 'accumulated_eval_time': 1179.1324508190155, 'accumulated_logging_time': 3.0069963932037354}
I0306 07:28:47.363054 139787361040128 logging_writer.py:48] [89478] accumulated_eval_time=1179.132451, accumulated_logging_time=3.006996, accumulated_submission_time=30145.174228, global_step=89478, preemption_count=0, score=30145.174228, test/accuracy=0.554700, test/loss=2.012035, test/num_examples=10000, total_duration=31330.530110, train/accuracy=0.775012, train/loss=0.838930, validation/accuracy=0.681560, validation/loss=1.300890, validation/num_examples=50000
I0306 07:28:55.069554 139787369432832 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.1156084537506104, loss=1.3519140481948853
I0306 07:29:28.658383 139787361040128 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.3659512996673584, loss=1.4432849884033203
I0306 07:30:02.259436 139787369432832 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.5199196338653564, loss=1.3725546598434448
I0306 07:30:35.920335 139787361040128 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.385296106338501, loss=1.3466367721557617
I0306 07:31:09.576809 139787369432832 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.6578876972198486, loss=1.386049747467041
I0306 07:31:43.180838 139787361040128 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.2451014518737793, loss=1.4345227479934692
I0306 07:32:16.768235 139787369432832 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.2933740615844727, loss=1.4407249689102173
I0306 07:32:50.352722 139787361040128 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.3373830318450928, loss=1.3754390478134155
I0306 07:33:23.921580 139787369432832 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.192209005355835, loss=1.3250117301940918
I0306 07:33:57.538152 139787361040128 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.143381357192993, loss=1.3357843160629272
I0306 07:34:31.139919 139787369432832 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.3226654529571533, loss=1.413258671760559
I0306 07:35:04.701715 139787361040128 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.337862014770508, loss=1.4243440628051758
I0306 07:35:38.270856 139787369432832 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.352261543273926, loss=1.4302029609680176
I0306 07:36:11.929880 139787361040128 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.181109666824341, loss=1.4063246250152588
I0306 07:36:45.571066 139787369432832 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.39365816116333, loss=1.44269859790802
I0306 07:37:17.357426 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:37:23.436482 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:37:32.368662 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:37:34.643712 139951089751872 submission_runner.py:411] Time since start: 31857.86s, 	Step: 90996, 	{'train/accuracy': 0.7703682780265808, 'train/loss': 0.8547177910804749, 'validation/accuracy': 0.6854400038719177, 'validation/loss': 1.282153844833374, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.0209715366363525, 'test/num_examples': 10000, 'score': 30655.103055000305, 'total_duration': 31857.856000185013, 'accumulated_submission_time': 30655.103055000305, 'accumulated_eval_time': 1196.4186639785767, 'accumulated_logging_time': 3.062368392944336}
I0306 07:37:34.679254 139789013600000 logging_writer.py:48] [90996] accumulated_eval_time=1196.418664, accumulated_logging_time=3.062368, accumulated_submission_time=30655.103055, global_step=90996, preemption_count=0, score=30655.103055, test/accuracy=0.552200, test/loss=2.020972, test/num_examples=10000, total_duration=31857.856000, train/accuracy=0.770368, train/loss=0.854718, validation/accuracy=0.685440, validation/loss=1.282154, validation/num_examples=50000
I0306 07:37:36.361490 139789021992704 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.0701215267181396, loss=1.310324788093567
I0306 07:38:10.006023 139789013600000 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.3215932846069336, loss=1.453677773475647
I0306 07:38:43.662528 139789021992704 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.2948856353759766, loss=1.3876445293426514
I0306 07:39:17.306863 139789013600000 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.9996545314788818, loss=1.329288363456726
I0306 07:39:50.920520 139789021992704 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.19003963470459, loss=1.321455478668213
I0306 07:40:24.584393 139789013600000 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.2449305057525635, loss=1.392727255821228
I0306 07:40:58.215253 139789021992704 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.475527763366699, loss=1.4469372034072876
I0306 07:41:31.840702 139789013600000 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.207531213760376, loss=1.3981413841247559
I0306 07:42:05.485467 139789021992704 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.4464304447174072, loss=1.353473424911499
I0306 07:42:39.109798 139789013600000 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.410020351409912, loss=1.4634472131729126
I0306 07:43:12.751361 139789021992704 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.3176517486572266, loss=1.442784070968628
I0306 07:43:46.390545 139789013600000 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.6083037853240967, loss=1.420139193534851
I0306 07:44:20.024371 139789021992704 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.2195773124694824, loss=1.476388931274414
I0306 07:44:53.654598 139789013600000 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.214057445526123, loss=1.3122942447662354
I0306 07:45:27.305182 139789021992704 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.2344400882720947, loss=1.3319716453552246
I0306 07:46:00.954869 139789013600000 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.3839111328125, loss=1.4209222793579102
I0306 07:46:04.801009 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:46:11.073784 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:46:20.033956 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:46:22.314334 139951089751872 submission_runner.py:411] Time since start: 32385.53s, 	Step: 92513, 	{'train/accuracy': 0.7651067972183228, 'train/loss': 0.8746317625045776, 'validation/accuracy': 0.6829400062561035, 'validation/loss': 1.2891305685043335, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.015486001968384, 'test/num_examples': 10000, 'score': 31165.160599946976, 'total_duration': 32385.52663731575, 'accumulated_submission_time': 31165.160599946976, 'accumulated_eval_time': 1213.9319269657135, 'accumulated_logging_time': 3.1073718070983887}
I0306 07:46:22.350247 139787562383104 logging_writer.py:48] [92513] accumulated_eval_time=1213.931927, accumulated_logging_time=3.107372, accumulated_submission_time=31165.160600, global_step=92513, preemption_count=0, score=31165.160600, test/accuracy=0.558400, test/loss=2.015486, test/num_examples=10000, total_duration=32385.526637, train/accuracy=0.765107, train/loss=0.874632, validation/accuracy=0.682940, validation/loss=1.289131, validation/num_examples=50000
I0306 07:46:51.919417 139788996814592 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.5154929161071777, loss=1.4350173473358154
I0306 07:47:25.501772 139787562383104 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.205847978591919, loss=1.3911445140838623
I0306 07:47:59.083766 139788996814592 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.4502737522125244, loss=1.3700973987579346
I0306 07:48:32.643722 139787562383104 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.522571563720703, loss=1.3458006381988525
I0306 07:49:06.236194 139788996814592 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.584630012512207, loss=1.386582851409912
I0306 07:49:39.906495 139787562383104 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.492096185684204, loss=1.310426950454712
I0306 07:50:13.511036 139788996814592 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.2961034774780273, loss=1.3603554964065552
I0306 07:50:47.171982 139787562383104 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.3806257247924805, loss=1.4076238870620728
I0306 07:51:20.772675 139788996814592 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.623875617980957, loss=1.4553405046463013
I0306 07:51:54.437847 139787562383104 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.2201547622680664, loss=1.3534648418426514
I0306 07:52:28.088077 139788996814592 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.258578062057495, loss=1.411231517791748
I0306 07:53:01.712815 139787562383104 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.247549057006836, loss=1.4377260208129883
I0306 07:53:35.315739 139788996814592 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.4534811973571777, loss=1.4087858200073242
I0306 07:54:08.987854 139787562383104 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.3812668323516846, loss=1.4370386600494385
I0306 07:54:42.629233 139788996814592 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.4354190826416016, loss=1.3638979196548462
I0306 07:54:52.529526 139951089751872 spec.py:321] Evaluating on the training split.
I0306 07:54:58.597548 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 07:55:07.576197 139951089751872 spec.py:349] Evaluating on the test split.
I0306 07:55:09.853183 139951089751872 submission_runner.py:411] Time since start: 32913.07s, 	Step: 94031, 	{'train/accuracy': 0.7631736397743225, 'train/loss': 0.8782700896263123, 'validation/accuracy': 0.6861799955368042, 'validation/loss': 1.2679184675216675, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 1.9711854457855225, 'test/num_examples': 10000, 'score': 31675.272496938705, 'total_duration': 32913.0654964447, 'accumulated_submission_time': 31675.272496938705, 'accumulated_eval_time': 1231.2555334568024, 'accumulated_logging_time': 3.1558098793029785}
I0306 07:55:09.885373 139787369432832 logging_writer.py:48] [94031] accumulated_eval_time=1231.255533, accumulated_logging_time=3.155810, accumulated_submission_time=31675.272497, global_step=94031, preemption_count=0, score=31675.272497, test/accuracy=0.562600, test/loss=1.971185, test/num_examples=10000, total_duration=32913.065496, train/accuracy=0.763174, train/loss=0.878270, validation/accuracy=0.686180, validation/loss=1.267918, validation/num_examples=50000
I0306 07:55:33.381056 139787562383104 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.307222843170166, loss=1.349796175956726
I0306 07:56:06.987246 139787369432832 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.3215577602386475, loss=1.3745810985565186
I0306 07:56:40.640486 139787562383104 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.197700023651123, loss=1.2391178607940674
I0306 07:57:14.313607 139787369432832 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.230670928955078, loss=1.3243210315704346
I0306 07:57:47.966031 139787562383104 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.247892141342163, loss=1.334599256515503
I0306 07:58:21.758654 139787369432832 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.48486328125, loss=1.441344141960144
I0306 07:58:55.429179 139787562383104 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.5882463455200195, loss=1.334646224975586
I0306 07:59:29.064419 139787369432832 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.650728702545166, loss=1.435356855392456
I0306 08:00:02.684799 139787562383104 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.5707881450653076, loss=1.3726662397384644
I0306 08:00:36.362345 139787369432832 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.205498695373535, loss=1.382560133934021
I0306 08:01:10.031467 139787562383104 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.494678258895874, loss=1.3147790431976318
I0306 08:01:43.669702 139787369432832 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.690876007080078, loss=1.4058207273483276
I0306 08:02:17.300635 139787562383104 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3613667488098145, loss=1.4198602437973022
I0306 08:02:50.925771 139787369432832 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.3586318492889404, loss=1.4247347116470337
I0306 08:03:24.550234 139787562383104 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.5822856426239014, loss=1.2956244945526123
I0306 08:03:40.161103 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:03:46.126676 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:03:55.127365 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:03:57.385016 139951089751872 submission_runner.py:411] Time since start: 33440.60s, 	Step: 95548, 	{'train/accuracy': 0.7469507455825806, 'train/loss': 0.9530366063117981, 'validation/accuracy': 0.6697399616241455, 'validation/loss': 1.3450031280517578, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.0591471195220947, 'test/num_examples': 10000, 'score': 32185.48177075386, 'total_duration': 33440.59732437134, 'accumulated_submission_time': 32185.48177075386, 'accumulated_eval_time': 1248.4793949127197, 'accumulated_logging_time': 3.1990935802459717}
I0306 08:03:57.424186 139787361040128 logging_writer.py:48] [95548] accumulated_eval_time=1248.479395, accumulated_logging_time=3.199094, accumulated_submission_time=32185.481771, global_step=95548, preemption_count=0, score=32185.481771, test/accuracy=0.552700, test/loss=2.059147, test/num_examples=10000, total_duration=33440.597324, train/accuracy=0.746951, train/loss=0.953037, validation/accuracy=0.669740, validation/loss=1.345003, validation/num_examples=50000
I0306 08:04:15.302098 139787369432832 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.5156068801879883, loss=1.4336638450622559
I0306 08:04:48.951986 139787361040128 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.3655924797058105, loss=1.3181627988815308
I0306 08:05:22.541398 139787369432832 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.0987157821655273, loss=1.2888455390930176
I0306 08:05:56.132729 139787361040128 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.6900389194488525, loss=1.4111593961715698
I0306 08:06:29.778607 139787369432832 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.4100890159606934, loss=1.400397539138794
I0306 08:07:03.417100 139787361040128 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.363389015197754, loss=1.326636552810669
I0306 08:07:37.090873 139787369432832 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.2893195152282715, loss=1.3594361543655396
I0306 08:08:10.744911 139787361040128 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.466320514678955, loss=1.3857585191726685
I0306 08:08:44.415627 139787369432832 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.5341086387634277, loss=1.4070733785629272
I0306 08:09:18.036891 139787361040128 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.5579187870025635, loss=1.3979063034057617
I0306 08:09:51.704374 139787369432832 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4611384868621826, loss=1.4518979787826538
I0306 08:10:25.342319 139787361040128 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.587994337081909, loss=1.3834538459777832
I0306 08:10:58.951034 139787369432832 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.437694787979126, loss=1.4295247793197632
I0306 08:11:32.566570 139787361040128 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.303969144821167, loss=1.2244385480880737
I0306 08:12:06.245983 139787369432832 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.3385682106018066, loss=1.3425402641296387
I0306 08:12:27.587395 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:12:33.610615 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:12:42.589608 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:12:44.866668 139951089751872 submission_runner.py:411] Time since start: 33968.08s, 	Step: 97065, 	{'train/accuracy': 0.8135961294174194, 'train/loss': 0.6818169951438904, 'validation/accuracy': 0.6930800080299377, 'validation/loss': 1.2403595447540283, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 1.9498519897460938, 'test/num_examples': 10000, 'score': 32695.579294204712, 'total_duration': 33968.07897615433, 'accumulated_submission_time': 32695.579294204712, 'accumulated_eval_time': 1265.758617401123, 'accumulated_logging_time': 3.2494664192199707}
I0306 08:12:44.901685 139787361040128 logging_writer.py:48] [97065] accumulated_eval_time=1265.758617, accumulated_logging_time=3.249466, accumulated_submission_time=32695.579294, global_step=97065, preemption_count=0, score=32695.579294, test/accuracy=0.566600, test/loss=1.949852, test/num_examples=10000, total_duration=33968.078976, train/accuracy=0.813596, train/loss=0.681817, validation/accuracy=0.693080, validation/loss=1.240360, validation/num_examples=50000
I0306 08:12:57.032131 139787369432832 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.4163055419921875, loss=1.311762809753418
I0306 08:13:30.659789 139787361040128 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.747056245803833, loss=1.42852783203125
I0306 08:14:04.336465 139787369432832 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.5086424350738525, loss=1.3588061332702637
I0306 08:14:38.007623 139787361040128 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.5874040126800537, loss=1.402625322341919
I0306 08:15:11.682951 139787369432832 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.2276148796081543, loss=1.3327200412750244
I0306 08:15:45.297814 139787361040128 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.466860294342041, loss=1.4092323780059814
I0306 08:16:18.944495 139787369432832 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.626241445541382, loss=1.401989221572876
I0306 08:16:52.747647 139787361040128 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.1711716651916504, loss=1.3272626399993896
I0306 08:17:26.406581 139787369432832 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.56912899017334, loss=1.4985994100570679
I0306 08:18:00.036911 139787361040128 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.3479244709014893, loss=1.3972830772399902
I0306 08:18:33.663624 139787369432832 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.826897382736206, loss=1.4387739896774292
I0306 08:19:07.280132 139787361040128 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.2650859355926514, loss=1.2673006057739258
I0306 08:19:40.931352 139787369432832 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.395531415939331, loss=1.3343158960342407
I0306 08:20:14.580182 139787361040128 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.3321480751037598, loss=1.4250285625457764
I0306 08:20:48.245285 139787369432832 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.4479503631591797, loss=1.3934056758880615
I0306 08:21:14.974283 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:21:21.056642 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:21:29.856252 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:21:32.110110 139951089751872 submission_runner.py:411] Time since start: 34495.32s, 	Step: 98581, 	{'train/accuracy': 0.7823262214660645, 'train/loss': 0.8076447248458862, 'validation/accuracy': 0.6801799535751343, 'validation/loss': 1.2974464893341064, 'validation/num_examples': 50000, 'test/accuracy': 0.5488000512123108, 'test/loss': 2.0415008068084717, 'test/num_examples': 10000, 'score': 33205.586966753006, 'total_duration': 34495.32242035866, 'accumulated_submission_time': 33205.586966753006, 'accumulated_eval_time': 1282.8943948745728, 'accumulated_logging_time': 3.295815944671631}
I0306 08:21:32.147073 139789013600000 logging_writer.py:48] [98581] accumulated_eval_time=1282.894395, accumulated_logging_time=3.295816, accumulated_submission_time=33205.586967, global_step=98581, preemption_count=0, score=33205.586967, test/accuracy=0.548800, test/loss=2.041501, test/num_examples=10000, total_duration=34495.322420, train/accuracy=0.782326, train/loss=0.807645, validation/accuracy=0.680180, validation/loss=1.297446, validation/num_examples=50000
I0306 08:21:38.857654 139789021992704 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.3881676197052, loss=1.3044203519821167
I0306 08:22:12.487092 139789013600000 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.716224193572998, loss=1.3913400173187256
I0306 08:22:46.136909 139789021992704 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.502950429916382, loss=1.3687846660614014
I0306 08:23:19.702701 139789013600000 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.379019260406494, loss=1.3144677877426147
I0306 08:23:53.295528 139789021992704 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.3411505222320557, loss=1.2834372520446777
I0306 08:24:26.954851 139789013600000 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.395080327987671, loss=1.4080586433410645
I0306 08:25:00.560425 139789021992704 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.549718141555786, loss=1.4428367614746094
I0306 08:25:34.240114 139789013600000 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.4439523220062256, loss=1.3431048393249512
I0306 08:26:07.873876 139789021992704 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.6202330589294434, loss=1.3997145891189575
I0306 08:26:50.030871 139789013600000 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.734431266784668, loss=1.361081838607788
I0306 08:27:41.984642 139789021992704 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.5158193111419678, loss=1.4424558877944946
I0306 08:28:15.660537 139789013600000 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.446026563644409, loss=1.3501863479614258
I0306 08:28:49.336815 139789021992704 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.283142566680908, loss=1.259245753288269
I0306 08:29:23.001961 139789013600000 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.2352585792541504, loss=1.2349238395690918
I0306 08:29:56.587557 139789021992704 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.386582612991333, loss=1.41705322265625
I0306 08:30:02.436458 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:30:08.430419 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:30:17.163990 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:30:19.424919 139951089751872 submission_runner.py:411] Time since start: 35022.64s, 	Step: 100019, 	{'train/accuracy': 0.7859135866165161, 'train/loss': 0.7940120697021484, 'validation/accuracy': 0.6896599531173706, 'validation/loss': 1.2514312267303467, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 1.9564324617385864, 'test/num_examples': 10000, 'score': 33715.81254816055, 'total_duration': 35022.6372282505, 'accumulated_submission_time': 33715.81254816055, 'accumulated_eval_time': 1299.8828177452087, 'accumulated_logging_time': 3.3442134857177734}
I0306 08:30:19.459271 139787369432832 logging_writer.py:48] [100019] accumulated_eval_time=1299.882818, accumulated_logging_time=3.344213, accumulated_submission_time=33715.812548, global_step=100019, preemption_count=0, score=33715.812548, test/accuracy=0.560200, test/loss=1.956432, test/num_examples=10000, total_duration=35022.637228, train/accuracy=0.785914, train/loss=0.794012, validation/accuracy=0.689660, validation/loss=1.251431, validation/num_examples=50000
I0306 08:30:47.010947 139787562383104 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.478386163711548, loss=1.3529560565948486
I0306 08:31:20.575072 139787369432832 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.4455456733703613, loss=1.2719887495040894
I0306 08:31:54.191082 139787562383104 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.6344592571258545, loss=1.4020761251449585
I0306 08:32:27.821376 139787369432832 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.602818489074707, loss=1.327346920967102
I0306 08:33:01.444729 139787562383104 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.6848909854888916, loss=1.3414340019226074
I0306 08:33:35.060944 139787369432832 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.45271635055542, loss=1.3062522411346436
I0306 08:34:08.700428 139787562383104 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.388563632965088, loss=1.329371690750122
I0306 08:34:42.356471 139787369432832 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.265866994857788, loss=1.212148904800415
I0306 08:35:16.042183 139787562383104 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.2268261909484863, loss=1.2859768867492676
I0306 08:35:49.620620 139787369432832 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.6272356510162354, loss=1.3556991815567017
I0306 08:36:23.264452 139787562383104 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.3092541694641113, loss=1.294894814491272
I0306 08:36:56.925485 139787369432832 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.6006717681884766, loss=1.326177716255188
I0306 08:37:30.547257 139787562383104 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.433546304702759, loss=1.3515326976776123
I0306 08:38:04.195027 139787369432832 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.578117847442627, loss=1.3390185832977295
I0306 08:38:37.850373 139787562383104 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.417492628097534, loss=1.3571869134902954
I0306 08:38:49.445224 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:38:55.576785 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:39:04.558215 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:39:06.848441 139951089751872 submission_runner.py:411] Time since start: 35550.06s, 	Step: 101536, 	{'train/accuracy': 0.7773038744926453, 'train/loss': 0.815521776676178, 'validation/accuracy': 0.6916799545288086, 'validation/loss': 1.2453638315200806, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9738190174102783, 'test/num_examples': 10000, 'score': 34225.73194885254, 'total_duration': 35550.06074118614, 'accumulated_submission_time': 34225.73194885254, 'accumulated_eval_time': 1317.2859869003296, 'accumulated_logging_time': 3.389284610748291}
I0306 08:39:06.876508 139789013600000 logging_writer.py:48] [101536] accumulated_eval_time=1317.285987, accumulated_logging_time=3.389285, accumulated_submission_time=34225.731949, global_step=101536, preemption_count=0, score=34225.731949, test/accuracy=0.562100, test/loss=1.973819, test/num_examples=10000, total_duration=35550.060741, train/accuracy=0.777304, train/loss=0.815522, validation/accuracy=0.691680, validation/loss=1.245364, validation/num_examples=50000
I0306 08:39:28.777583 139789021992704 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.5393407344818115, loss=1.4214733839035034
I0306 08:40:02.379650 139789013600000 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.5347371101379395, loss=1.2715576887130737
I0306 08:40:35.969890 139789021992704 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.511152744293213, loss=1.2867517471313477
I0306 08:41:09.543288 139789013600000 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.5119316577911377, loss=1.243175983428955
I0306 08:41:43.295204 139789021992704 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.566359758377075, loss=1.3582614660263062
I0306 08:42:16.866291 139789013600000 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.382474422454834, loss=1.250591516494751
I0306 08:42:50.459743 139789021992704 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.8742334842681885, loss=1.385297179222107
I0306 08:43:24.129731 139789013600000 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.6564724445343018, loss=1.2978028059005737
I0306 08:43:57.746635 139789021992704 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.4814324378967285, loss=1.200241208076477
I0306 08:44:31.426879 139789013600000 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.46346378326416, loss=1.4192888736724854
I0306 08:45:05.037926 139789021992704 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.563061237335205, loss=1.4292891025543213
I0306 08:45:38.677490 139789013600000 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.2928566932678223, loss=1.2610552310943604
I0306 08:46:12.313116 139789021992704 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.6125752925872803, loss=1.2976847887039185
I0306 08:46:45.991823 139789013600000 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.7177860736846924, loss=1.4133353233337402
I0306 08:47:19.638939 139789021992704 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.5301175117492676, loss=1.3504317998886108
I0306 08:47:37.018529 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:47:43.115073 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:47:51.936200 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:47:54.140058 139951089751872 submission_runner.py:411] Time since start: 36077.35s, 	Step: 103053, 	{'train/accuracy': 0.7824656963348389, 'train/loss': 0.8034330010414124, 'validation/accuracy': 0.6937800049781799, 'validation/loss': 1.2375240325927734, 'validation/num_examples': 50000, 'test/accuracy': 0.5625, 'test/loss': 1.970668077468872, 'test/num_examples': 10000, 'score': 34735.81096434593, 'total_duration': 36077.352365493774, 'accumulated_submission_time': 34735.81096434593, 'accumulated_eval_time': 1334.4074614048004, 'accumulated_logging_time': 3.4263105392456055}
I0306 08:47:54.174956 139788996814592 logging_writer.py:48] [103053] accumulated_eval_time=1334.407461, accumulated_logging_time=3.426311, accumulated_submission_time=34735.810964, global_step=103053, preemption_count=0, score=34735.810964, test/accuracy=0.562500, test/loss=1.970668, test/num_examples=10000, total_duration=36077.352365, train/accuracy=0.782466, train/loss=0.803433, validation/accuracy=0.693780, validation/loss=1.237524, validation/num_examples=50000
I0306 08:48:10.301068 139789005207296 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.5356881618499756, loss=1.29398775100708
I0306 08:48:43.873168 139788996814592 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.343843698501587, loss=1.3314332962036133
I0306 08:49:17.452780 139789005207296 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.52561354637146, loss=1.373024344444275
I0306 08:49:51.082698 139788996814592 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.6601855754852295, loss=1.3877655267715454
I0306 08:50:24.732120 139789005207296 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.6068639755249023, loss=1.4678431749343872
I0306 08:50:58.408476 139788996814592 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.486078977584839, loss=1.3483293056488037
I0306 08:51:32.032970 139789005207296 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.5017809867858887, loss=1.3932167291641235
I0306 08:52:05.683716 139788996814592 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.4884495735168457, loss=1.2781027555465698
I0306 08:52:39.317680 139789005207296 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.6290855407714844, loss=1.3621017932891846
I0306 08:53:12.948658 139788996814592 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.5090386867523193, loss=1.4376308917999268
I0306 08:53:46.624122 139789005207296 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.43487286567688, loss=1.1935946941375732
I0306 08:54:20.256056 139788996814592 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.5515923500061035, loss=1.3352947235107422
I0306 08:54:53.872331 139789005207296 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.6003894805908203, loss=1.3457601070404053
I0306 08:55:27.513594 139788996814592 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.7452032566070557, loss=1.3035836219787598
I0306 08:56:01.162156 139789005207296 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.4770548343658447, loss=1.291709065437317
I0306 08:56:24.170135 139951089751872 spec.py:321] Evaluating on the training split.
I0306 08:56:30.231472 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 08:56:39.170147 139951089751872 spec.py:349] Evaluating on the test split.
I0306 08:56:41.441709 139951089751872 submission_runner.py:411] Time since start: 36604.65s, 	Step: 104570, 	{'train/accuracy': 0.7754902839660645, 'train/loss': 0.8326342701911926, 'validation/accuracy': 0.6926199793815613, 'validation/loss': 1.252827763557434, 'validation/num_examples': 50000, 'test/accuracy': 0.5625, 'test/loss': 2.004796266555786, 'test/num_examples': 10000, 'score': 35245.739324092865, 'total_duration': 36604.654005527496, 'accumulated_submission_time': 35245.739324092865, 'accumulated_eval_time': 1351.678982257843, 'accumulated_logging_time': 3.4716062545776367}
I0306 08:56:41.477103 139787369432832 logging_writer.py:48] [104570] accumulated_eval_time=1351.678982, accumulated_logging_time=3.471606, accumulated_submission_time=35245.739324, global_step=104570, preemption_count=0, score=35245.739324, test/accuracy=0.562500, test/loss=2.004796, test/num_examples=10000, total_duration=36604.654006, train/accuracy=0.775490, train/loss=0.832634, validation/accuracy=0.692620, validation/loss=1.252828, validation/num_examples=50000
I0306 08:56:51.898927 139787562383104 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.492734670639038, loss=1.3067177534103394
I0306 08:57:25.479151 139787369432832 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.6998958587646484, loss=1.3504515886306763
I0306 08:57:59.139215 139787562383104 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.4991323947906494, loss=1.2643990516662598
I0306 08:58:32.759045 139787369432832 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.8593244552612305, loss=1.3063198328018188
I0306 08:59:06.395234 139787562383104 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.931547164916992, loss=1.4310319423675537
I0306 08:59:40.086005 139787369432832 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.504321813583374, loss=1.300718903541565
I0306 09:00:13.681696 139787562383104 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.659297466278076, loss=1.3610559701919556
I0306 09:00:47.307699 139787369432832 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.4350156784057617, loss=1.2962809801101685
I0306 09:01:20.982802 139787562383104 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.8731887340545654, loss=1.2794914245605469
I0306 09:01:54.587879 139787369432832 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.518921375274658, loss=1.2771917581558228
I0306 09:02:28.215029 139787562383104 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.5812630653381348, loss=1.286612629890442
I0306 09:03:01.825699 139787369432832 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.595094680786133, loss=1.3083504438400269
I0306 09:03:35.464276 139787562383104 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.5590193271636963, loss=1.3847906589508057
I0306 09:04:09.075632 139787369432832 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.461474895477295, loss=1.2700340747833252
I0306 09:04:42.717911 139787562383104 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.5181925296783447, loss=1.1786773204803467
I0306 09:05:11.459357 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:05:17.583380 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:05:26.601276 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:05:28.874216 139951089751872 submission_runner.py:411] Time since start: 37132.09s, 	Step: 106087, 	{'train/accuracy': 0.8234414458274841, 'train/loss': 0.6515009999275208, 'validation/accuracy': 0.6938199996948242, 'validation/loss': 1.2419836521148682, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9858746528625488, 'test/num_examples': 10000, 'score': 35755.65688419342, 'total_duration': 37132.086525440216, 'accumulated_submission_time': 35755.65688419342, 'accumulated_eval_time': 1369.0937926769257, 'accumulated_logging_time': 3.5167410373687744}
I0306 09:05:28.910605 139789021992704 logging_writer.py:48] [106087] accumulated_eval_time=1369.093793, accumulated_logging_time=3.516741, accumulated_submission_time=35755.656884, global_step=106087, preemption_count=0, score=35755.656884, test/accuracy=0.561200, test/loss=1.985875, test/num_examples=10000, total_duration=37132.086525, train/accuracy=0.823441, train/loss=0.651501, validation/accuracy=0.693820, validation/loss=1.241984, validation/num_examples=50000
I0306 09:05:33.618770 139789030385408 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.4147491455078125, loss=1.384596824645996
I0306 09:06:07.278385 139789021992704 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.519927740097046, loss=1.3294893503189087
I0306 09:06:40.890021 139789030385408 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.9400548934936523, loss=1.3056228160858154
I0306 09:07:14.549095 139789021992704 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.5261409282684326, loss=1.3371094465255737
I0306 09:07:48.195714 139789030385408 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.6030750274658203, loss=1.307051181793213
I0306 09:08:21.840170 139789021992704 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.7591257095336914, loss=1.3308569192886353
I0306 09:08:55.490187 139789030385408 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.9213831424713135, loss=1.301950454711914
I0306 09:09:29.139222 139789021992704 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.4658985137939453, loss=1.254775047302246
I0306 09:10:02.790960 139789030385408 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.759510040283203, loss=1.3454731702804565
I0306 09:10:36.429086 139789021992704 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.5006585121154785, loss=1.246699333190918
I0306 09:11:10.071715 139789030385408 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.5347344875335693, loss=1.198932409286499
I0306 09:11:43.706838 139789021992704 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.59436297416687, loss=1.3871095180511475
I0306 09:12:17.371059 139789030385408 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.945932388305664, loss=1.301552653312683
I0306 09:12:51.020371 139789021992704 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.740494728088379, loss=1.376694917678833
I0306 09:13:24.707893 139789030385408 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.756615161895752, loss=1.2797460556030273
I0306 09:13:58.361437 139789021992704 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.8114612102508545, loss=1.3364349603652954
I0306 09:13:59.177713 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:14:05.223996 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:14:14.021140 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:14:16.279893 139951089751872 submission_runner.py:411] Time since start: 37659.49s, 	Step: 107604, 	{'train/accuracy': 0.8073580861091614, 'train/loss': 0.7141561508178711, 'validation/accuracy': 0.6963199973106384, 'validation/loss': 1.2244770526885986, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.948879361152649, 'test/num_examples': 10000, 'score': 36265.858674287796, 'total_duration': 37659.49220252037, 'accumulated_submission_time': 36265.858674287796, 'accumulated_eval_time': 1386.1959176063538, 'accumulated_logging_time': 3.563281774520874}
I0306 09:14:16.318970 139787361040128 logging_writer.py:48] [107604] accumulated_eval_time=1386.195918, accumulated_logging_time=3.563282, accumulated_submission_time=36265.858674, global_step=107604, preemption_count=0, score=36265.858674, test/accuracy=0.569900, test/loss=1.948879, test/num_examples=10000, total_duration=37659.492203, train/accuracy=0.807358, train/loss=0.714156, validation/accuracy=0.696320, validation/loss=1.224477, validation/num_examples=50000
I0306 09:14:48.934157 139787369432832 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.695408821105957, loss=1.2968257665634155
I0306 09:15:22.604804 139787361040128 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.478919744491577, loss=1.2256888151168823
I0306 09:15:56.284440 139787369432832 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.757070779800415, loss=1.379705548286438
I0306 09:16:29.903998 139787361040128 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.8351008892059326, loss=1.4643642902374268
I0306 09:17:03.570639 139787369432832 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.5140514373779297, loss=1.2331537008285522
I0306 09:17:37.186382 139787361040128 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.7209842205047607, loss=1.3428784608840942
I0306 09:18:10.885421 139787369432832 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.867734909057617, loss=1.2432174682617188
I0306 09:18:44.473640 139787361040128 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.6218390464782715, loss=1.1578258275985718
I0306 09:19:18.129088 139787369432832 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.6546385288238525, loss=1.2370555400848389
I0306 09:19:51.747046 139787361040128 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.7628533840179443, loss=1.25814950466156
I0306 09:20:25.392865 139787369432832 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.78760027885437, loss=1.3515218496322632
I0306 09:20:58.994626 139787361040128 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.589071035385132, loss=1.3396382331848145
I0306 09:21:32.645390 139787369432832 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.8235628604888916, loss=1.246748685836792
I0306 09:22:06.260332 139787361040128 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.6411666870117188, loss=1.2023085355758667
I0306 09:22:39.909952 139787369432832 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.8548576831817627, loss=1.4002275466918945
I0306 09:22:46.452696 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:22:52.496157 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:23:01.279166 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:23:03.534816 139951089751872 submission_runner.py:411] Time since start: 38186.75s, 	Step: 109121, 	{'train/accuracy': 0.7997050285339355, 'train/loss': 0.7437049746513367, 'validation/accuracy': 0.6982399821281433, 'validation/loss': 1.220030665397644, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.927659273147583, 'test/num_examples': 10000, 'score': 36775.92725396156, 'total_duration': 38186.74701857567, 'accumulated_submission_time': 36775.92725396156, 'accumulated_eval_time': 1403.2778811454773, 'accumulated_logging_time': 3.6121723651885986}
I0306 09:23:03.574856 139789013600000 logging_writer.py:48] [109121] accumulated_eval_time=1403.277881, accumulated_logging_time=3.612172, accumulated_submission_time=36775.927254, global_step=109121, preemption_count=0, score=36775.927254, test/accuracy=0.573300, test/loss=1.927659, test/num_examples=10000, total_duration=38186.747019, train/accuracy=0.799705, train/loss=0.743705, validation/accuracy=0.698240, validation/loss=1.220031, validation/num_examples=50000
I0306 09:23:30.503057 139789021992704 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.5818567276000977, loss=1.2014689445495605
I0306 09:24:04.144548 139789013600000 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.639573812484741, loss=1.280014157295227
I0306 09:24:37.801562 139789021992704 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.633293390274048, loss=1.31851327419281
I0306 09:25:11.412988 139789013600000 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.5277814865112305, loss=1.2961981296539307
I0306 09:25:45.036451 139789021992704 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.504692554473877, loss=1.2073891162872314
I0306 09:26:18.657841 139789013600000 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.7372899055480957, loss=1.3051249980926514
I0306 09:26:52.289621 139789021992704 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.8543620109558105, loss=1.2353777885437012
I0306 09:27:25.941503 139789013600000 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.607865571975708, loss=1.3489360809326172
I0306 09:27:59.615222 139789021992704 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.504567861557007, loss=1.273855209350586
I0306 09:28:33.260126 139789013600000 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.86659836769104, loss=1.3469254970550537
I0306 09:29:06.890091 139789021992704 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.664790391921997, loss=1.347306728363037
I0306 09:29:40.502510 139789013600000 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.9923837184906006, loss=1.3346116542816162
I0306 09:30:14.157190 139789021992704 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.8279008865356445, loss=1.345190167427063
I0306 09:30:47.787060 139789013600000 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.767346143722534, loss=1.258095622062683
I0306 09:31:21.384248 139789021992704 logging_writer.py:48] [110600] global_step=110600, grad_norm=3.0686113834381104, loss=1.3607900142669678
I0306 09:31:33.638062 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:31:39.639394 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:31:48.652065 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:31:50.959169 139951089751872 submission_runner.py:411] Time since start: 38714.17s, 	Step: 110638, 	{'train/accuracy': 0.7970942258834839, 'train/loss': 0.739464282989502, 'validation/accuracy': 0.7014200091362, 'validation/loss': 1.2130403518676758, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.961884617805481, 'test/num_examples': 10000, 'score': 37285.923748254776, 'total_duration': 38714.17147755623, 'accumulated_submission_time': 37285.923748254776, 'accumulated_eval_time': 1420.5989344120026, 'accumulated_logging_time': 3.6651365756988525}
I0306 09:31:50.997886 139787562383104 logging_writer.py:48] [110638] accumulated_eval_time=1420.598934, accumulated_logging_time=3.665137, accumulated_submission_time=37285.923748, global_step=110638, preemption_count=0, score=37285.923748, test/accuracy=0.570800, test/loss=1.961885, test/num_examples=10000, total_duration=38714.171478, train/accuracy=0.797094, train/loss=0.739464, validation/accuracy=0.701420, validation/loss=1.213040, validation/num_examples=50000
I0306 09:32:12.186255 139788996814592 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.801032781600952, loss=1.3120609521865845
I0306 09:32:45.867434 139787562383104 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.5293970108032227, loss=1.2352230548858643
I0306 09:33:19.509775 139788996814592 logging_writer.py:48] [110900] global_step=110900, grad_norm=3.071655035018921, loss=1.3040157556533813
I0306 09:33:53.076644 139787562383104 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.836895227432251, loss=1.2386325597763062
I0306 09:34:26.663125 139788996814592 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.6017091274261475, loss=1.3362659215927124
I0306 09:35:00.237268 139787562383104 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.674356460571289, loss=1.2331321239471436
I0306 09:35:33.834608 139788996814592 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.8865463733673096, loss=1.2549458742141724
I0306 09:36:07.484224 139787562383104 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.5420846939086914, loss=1.243066668510437
I0306 09:36:41.135497 139788996814592 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.7643189430236816, loss=1.3001223802566528
I0306 09:37:14.735606 139787562383104 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.813493251800537, loss=1.2558881044387817
I0306 09:37:48.346948 139788996814592 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.5364129543304443, loss=1.2146472930908203
I0306 09:38:21.999860 139787562383104 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.506681442260742, loss=1.2564036846160889
I0306 09:38:55.618959 139788996814592 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.627795934677124, loss=1.2330423593521118
I0306 09:39:29.273558 139787562383104 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.8733835220336914, loss=1.2873027324676514
I0306 09:40:02.915352 139788996814592 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.9277548789978027, loss=1.1990077495574951
I0306 09:40:21.234621 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:40:27.343710 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:40:36.262403 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:40:38.541328 139951089751872 submission_runner.py:411] Time since start: 39241.75s, 	Step: 112156, 	{'train/accuracy': 0.7895607352256775, 'train/loss': 0.7663513422012329, 'validation/accuracy': 0.6975199580192566, 'validation/loss': 1.2365078926086426, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9826350212097168, 'test/num_examples': 10000, 'score': 37796.095804691315, 'total_duration': 39241.753627061844, 'accumulated_submission_time': 37796.095804691315, 'accumulated_eval_time': 1437.9055798053741, 'accumulated_logging_time': 3.7136876583099365}
I0306 09:40:38.576367 139789021992704 logging_writer.py:48] [112156] accumulated_eval_time=1437.905580, accumulated_logging_time=3.713688, accumulated_submission_time=37796.095805, global_step=112156, preemption_count=0, score=37796.095805, test/accuracy=0.566100, test/loss=1.982635, test/num_examples=10000, total_duration=39241.753627, train/accuracy=0.789561, train/loss=0.766351, validation/accuracy=0.697520, validation/loss=1.236508, validation/num_examples=50000
I0306 09:40:53.695913 139789030385408 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.578742742538452, loss=1.2474439144134521
I0306 09:41:27.327953 139789021992704 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.600817918777466, loss=1.1874659061431885
I0306 09:42:00.950849 139789030385408 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.836669683456421, loss=1.2832667827606201
I0306 09:42:34.619728 139789021992704 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.5317182540893555, loss=1.2422146797180176
I0306 09:43:08.202615 139789030385408 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.790510892868042, loss=1.2525439262390137
I0306 09:43:41.844688 139789021992704 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.579740047454834, loss=1.2461951971054077
I0306 09:44:15.477712 139789030385408 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.7145256996154785, loss=1.2008540630340576
I0306 09:44:49.108178 139789021992704 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.680488109588623, loss=1.2885255813598633
I0306 09:45:22.746489 139789030385408 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.830941915512085, loss=1.3022359609603882
I0306 09:45:56.352031 139789021992704 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.7039923667907715, loss=1.2510228157043457
I0306 09:46:29.916926 139789030385408 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.696510076522827, loss=1.2887893915176392
I0306 09:47:03.478712 139789021992704 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.7453083992004395, loss=1.1994819641113281
I0306 09:47:37.026584 139789030385408 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.6423487663269043, loss=1.2207796573638916
I0306 09:48:10.657053 139789021992704 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.534588575363159, loss=1.153721570968628
I0306 09:48:44.299028 139789030385408 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8129160404205322, loss=1.2790135145187378
I0306 09:49:08.600472 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:49:14.648906 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:49:23.515763 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:49:25.827766 139951089751872 submission_runner.py:411] Time since start: 39769.04s, 	Step: 113674, 	{'train/accuracy': 0.7941246628761292, 'train/loss': 0.7522966861724854, 'validation/accuracy': 0.6987999677658081, 'validation/loss': 1.2218286991119385, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.9535249471664429, 'test/num_examples': 10000, 'score': 38306.05176615715, 'total_duration': 39769.040078401566, 'accumulated_submission_time': 38306.05176615715, 'accumulated_eval_time': 1455.1328389644623, 'accumulated_logging_time': 3.761552095413208}
I0306 09:49:25.868024 139787562383104 logging_writer.py:48] [113674] accumulated_eval_time=1455.132839, accumulated_logging_time=3.761552, accumulated_submission_time=38306.051766, global_step=113674, preemption_count=0, score=38306.051766, test/accuracy=0.570900, test/loss=1.953525, test/num_examples=10000, total_duration=39769.040078, train/accuracy=0.794125, train/loss=0.752297, validation/accuracy=0.698800, validation/loss=1.221829, validation/num_examples=50000
I0306 09:49:34.948167 139788996814592 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.8905465602874756, loss=1.2980307340621948
I0306 09:50:08.592221 139787562383104 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.583620548248291, loss=1.1534147262573242
I0306 09:50:42.252336 139788996814592 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.7073416709899902, loss=1.3019099235534668
I0306 09:51:15.895021 139787562383104 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.775224447250366, loss=1.2401949167251587
I0306 09:51:49.473412 139788996814592 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.8163795471191406, loss=1.2387741804122925
I0306 09:52:23.036747 139787562383104 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.7133638858795166, loss=1.2586578130722046
I0306 09:52:56.610493 139788996814592 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.8810360431671143, loss=1.3046376705169678
I0306 09:53:30.213221 139787562383104 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.953265428543091, loss=1.2706913948059082
I0306 09:54:03.858650 139788996814592 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.5910332202911377, loss=1.2616002559661865
I0306 09:54:37.512547 139787562383104 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.757262945175171, loss=1.2960950136184692
I0306 09:55:11.131888 139788996814592 logging_writer.py:48] [114700] global_step=114700, grad_norm=3.083514928817749, loss=1.2437509298324585
I0306 09:55:44.731777 139787562383104 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.6040730476379395, loss=1.2069000005722046
I0306 09:56:18.361973 139788996814592 logging_writer.py:48] [114900] global_step=114900, grad_norm=3.1258230209350586, loss=1.2219352722167969
I0306 09:56:51.973846 139787562383104 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.8560547828674316, loss=1.2647521495819092
I0306 09:57:25.632610 139788996814592 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.538938283920288, loss=1.2093913555145264
I0306 09:57:56.013535 139951089751872 spec.py:321] Evaluating on the training split.
I0306 09:58:02.103334 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 09:58:10.798159 139951089751872 spec.py:349] Evaluating on the test split.
I0306 09:58:13.073069 139951089751872 submission_runner.py:411] Time since start: 40296.29s, 	Step: 115192, 	{'train/accuracy': 0.832051157951355, 'train/loss': 0.6108251214027405, 'validation/accuracy': 0.700659990310669, 'validation/loss': 1.2239474058151245, 'validation/num_examples': 50000, 'test/accuracy': 0.5716000199317932, 'test/loss': 1.930112361907959, 'test/num_examples': 10000, 'score': 38816.13313245773, 'total_duration': 40296.28511285782, 'accumulated_submission_time': 38816.13313245773, 'accumulated_eval_time': 1472.192055463791, 'accumulated_logging_time': 3.811495065689087}
I0306 09:58:13.110124 139789013600000 logging_writer.py:48] [115192] accumulated_eval_time=1472.192055, accumulated_logging_time=3.811495, accumulated_submission_time=38816.133132, global_step=115192, preemption_count=0, score=38816.133132, test/accuracy=0.571600, test/loss=1.930112, test/num_examples=10000, total_duration=40296.285113, train/accuracy=0.832051, train/loss=0.610825, validation/accuracy=0.700660, validation/loss=1.223947, validation/num_examples=50000
I0306 09:58:16.121972 139789021992704 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.9452219009399414, loss=1.1899937391281128
I0306 09:58:49.701713 139789013600000 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.7073400020599365, loss=1.241557002067566
I0306 09:59:23.246093 139789021992704 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.108614444732666, loss=1.2548255920410156
I0306 09:59:56.811177 139789013600000 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.747436046600342, loss=1.0503185987472534
I0306 10:00:30.444091 139789021992704 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.885089159011841, loss=1.271345853805542
I0306 10:01:04.097776 139789013600000 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.591783285140991, loss=1.2232122421264648
I0306 10:01:37.650559 139789021992704 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.8658769130706787, loss=1.270404577255249
I0306 10:02:11.233951 139789013600000 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.818185567855835, loss=1.2296842336654663
I0306 10:02:44.874301 139789021992704 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.889770984649658, loss=1.218231201171875
I0306 10:03:18.507920 139789013600000 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.952051877975464, loss=1.2166130542755127
I0306 10:03:52.158501 139789021992704 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.827807664871216, loss=1.233698844909668
I0306 10:04:25.803622 139789013600000 logging_writer.py:48] [116300] global_step=116300, grad_norm=3.1944265365600586, loss=1.280588150024414
I0306 10:04:59.465124 139789021992704 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.980926275253296, loss=1.2419012784957886
I0306 10:05:33.083166 139789013600000 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.7002246379852295, loss=1.1466858386993408
I0306 10:06:06.710730 139789021992704 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.875070333480835, loss=1.1680506467819214
I0306 10:06:40.353625 139789013600000 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.950558662414551, loss=1.2729586362838745
I0306 10:06:43.201426 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:06:49.220204 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:06:58.300308 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:07:00.617060 139951089751872 submission_runner.py:411] Time since start: 40823.83s, 	Step: 116710, 	{'train/accuracy': 0.8093311190605164, 'train/loss': 0.6926111578941345, 'validation/accuracy': 0.7005800008773804, 'validation/loss': 1.2249276638031006, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.9686000347137451, 'test/num_examples': 10000, 'score': 39326.16049218178, 'total_duration': 40823.829357624054, 'accumulated_submission_time': 39326.16049218178, 'accumulated_eval_time': 1489.6076259613037, 'accumulated_logging_time': 3.858402729034424}
I0306 10:07:00.654312 139787562383104 logging_writer.py:48] [116710] accumulated_eval_time=1489.607626, accumulated_logging_time=3.858403, accumulated_submission_time=39326.160492, global_step=116710, preemption_count=0, score=39326.160492, test/accuracy=0.571900, test/loss=1.968600, test/num_examples=10000, total_duration=40823.829358, train/accuracy=0.809331, train/loss=0.692611, validation/accuracy=0.700580, validation/loss=1.224928, validation/num_examples=50000
I0306 10:07:31.209985 139788996814592 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.9226796627044678, loss=1.1955629587173462
I0306 10:08:04.774613 139787562383104 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.790191888809204, loss=1.3167569637298584
I0306 10:08:38.343462 139788996814592 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9619359970092773, loss=1.214508295059204
I0306 10:09:11.934110 139787562383104 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.838064193725586, loss=1.2077651023864746
I0306 10:09:45.578972 139788996814592 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.753695487976074, loss=1.1184228658676147
I0306 10:10:19.210629 139787562383104 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.8459572792053223, loss=1.2134596109390259
I0306 10:10:52.851557 139788996814592 logging_writer.py:48] [117400] global_step=117400, grad_norm=3.1578752994537354, loss=1.2006447315216064
I0306 10:11:26.516208 139787562383104 logging_writer.py:48] [117500] global_step=117500, grad_norm=3.034501791000366, loss=1.203627347946167
I0306 10:12:00.173152 139788996814592 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.7812814712524414, loss=1.1594840288162231
I0306 10:12:33.813290 139787562383104 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.5432658195495605, loss=1.165185570716858
I0306 10:13:07.509360 139788996814592 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.963665723800659, loss=1.319549798965454
I0306 10:13:41.099303 139787562383104 logging_writer.py:48] [117900] global_step=117900, grad_norm=3.0884315967559814, loss=1.3321430683135986
I0306 10:14:14.700042 139788996814592 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.0979244709014893, loss=1.2429708242416382
I0306 10:14:48.291106 139787562383104 logging_writer.py:48] [118100] global_step=118100, grad_norm=3.1502468585968018, loss=1.2805237770080566
I0306 10:15:21.829481 139788996814592 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8376550674438477, loss=1.2574002742767334
I0306 10:15:30.690348 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:15:36.744625 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:15:45.647620 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:15:47.921781 139951089751872 submission_runner.py:411] Time since start: 41351.13s, 	Step: 118228, 	{'train/accuracy': 0.8143933415412903, 'train/loss': 0.6682717204093933, 'validation/accuracy': 0.7066400051116943, 'validation/loss': 1.1898187398910522, 'validation/num_examples': 50000, 'test/accuracy': 0.5803000330924988, 'test/loss': 1.9361289739608765, 'test/num_examples': 10000, 'score': 39836.12970209122, 'total_duration': 41351.13400554657, 'accumulated_submission_time': 39836.12970209122, 'accumulated_eval_time': 1506.8389210700989, 'accumulated_logging_time': 3.907601833343506}
I0306 10:15:47.959038 139787369432832 logging_writer.py:48] [118228] accumulated_eval_time=1506.838921, accumulated_logging_time=3.907602, accumulated_submission_time=39836.129702, global_step=118228, preemption_count=0, score=39836.129702, test/accuracy=0.580300, test/loss=1.936129, test/num_examples=10000, total_duration=41351.134006, train/accuracy=0.814393, train/loss=0.668272, validation/accuracy=0.706640, validation/loss=1.189819, validation/num_examples=50000
I0306 10:16:12.478088 139789013600000 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.758885383605957, loss=1.117693305015564
I0306 10:16:46.071338 139787369432832 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.891824960708618, loss=1.239009141921997
I0306 10:17:19.634310 139789013600000 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.422873020172119, loss=1.1890546083450317
I0306 10:17:53.215671 139787369432832 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.179666042327881, loss=1.2837774753570557
I0306 10:18:26.849085 139789013600000 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7866907119750977, loss=1.1490527391433716
I0306 10:19:00.507051 139787369432832 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.803008794784546, loss=1.1789827346801758
I0306 10:19:34.286128 139789013600000 logging_writer.py:48] [118900] global_step=118900, grad_norm=3.1001815795898438, loss=1.2504082918167114
I0306 10:20:07.867738 139787369432832 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.1133673191070557, loss=1.2202987670898438
I0306 10:20:41.437619 139789013600000 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.061917781829834, loss=1.182295799255371
I0306 10:21:15.011132 139787369432832 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.9781079292297363, loss=1.1267914772033691
I0306 10:21:48.651571 139789013600000 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.7946619987487793, loss=1.131070852279663
I0306 10:22:22.273239 139787369432832 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.9143025875091553, loss=1.2570011615753174
I0306 10:22:55.931426 139789013600000 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.9582467079162598, loss=1.1233667135238647
I0306 10:23:29.572775 139787369432832 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.941899538040161, loss=1.1814919710159302
I0306 10:24:03.234311 139789013600000 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.924386739730835, loss=1.2043957710266113
I0306 10:24:18.196549 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:24:24.164589 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:24:33.056484 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:24:35.328156 139951089751872 submission_runner.py:411] Time since start: 41878.54s, 	Step: 119746, 	{'train/accuracy': 0.8092713356018066, 'train/loss': 0.6910978555679321, 'validation/accuracy': 0.7084599733352661, 'validation/loss': 1.1896438598632812, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.9084508419036865, 'test/num_examples': 10000, 'score': 40346.30208206177, 'total_duration': 41878.540466308594, 'accumulated_submission_time': 40346.30208206177, 'accumulated_eval_time': 1523.9704895019531, 'accumulated_logging_time': 3.954399585723877}
I0306 10:24:35.368507 139788996814592 logging_writer.py:48] [119746] accumulated_eval_time=1523.970490, accumulated_logging_time=3.954400, accumulated_submission_time=40346.302082, global_step=119746, preemption_count=0, score=40346.302082, test/accuracy=0.577000, test/loss=1.908451, test/num_examples=10000, total_duration=41878.540466, train/accuracy=0.809271, train/loss=0.691098, validation/accuracy=0.708460, validation/loss=1.189644, validation/num_examples=50000
I0306 10:24:53.856344 139789005207296 logging_writer.py:48] [119800] global_step=119800, grad_norm=3.1459062099456787, loss=1.259784460067749
I0306 10:25:27.667117 139788996814592 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.1631860733032227, loss=1.1400749683380127
I0306 10:26:01.315605 139789005207296 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.163536548614502, loss=1.1861045360565186
I0306 10:26:34.907660 139788996814592 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.765721082687378, loss=1.0310355424880981
I0306 10:27:08.606142 139789005207296 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.637704849243164, loss=1.1288906335830688
I0306 10:27:42.275868 139788996814592 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.0298538208007812, loss=1.2343268394470215
I0306 10:28:15.928995 139789005207296 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.2388687133789062, loss=1.2159757614135742
I0306 10:28:49.491956 139788996814592 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.8876395225524902, loss=1.1996196508407593
I0306 10:29:23.054827 139789005207296 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.128084659576416, loss=1.133959412574768
I0306 10:29:56.637145 139788996814592 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.932101011276245, loss=1.1609092950820923
I0306 10:30:30.257822 139789005207296 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.9682154655456543, loss=1.1924388408660889
I0306 10:31:03.892165 139788996814592 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.845968246459961, loss=1.1661432981491089
I0306 10:31:37.665777 139789005207296 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.977386713027954, loss=1.1106781959533691
I0306 10:32:11.320417 139788996814592 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.962618112564087, loss=1.249765396118164
I0306 10:32:44.956941 139789005207296 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.8491904735565186, loss=1.040457010269165
I0306 10:33:05.633733 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:33:11.770699 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:33:20.586198 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:33:22.824204 139951089751872 submission_runner.py:411] Time since start: 42406.04s, 	Step: 121263, 	{'train/accuracy': 0.8119817972183228, 'train/loss': 0.6661557555198669, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.1816290616989136, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.903845191001892, 'test/num_examples': 10000, 'score': 40856.50037384033, 'total_duration': 42406.036387205124, 'accumulated_submission_time': 40856.50037384033, 'accumulated_eval_time': 1541.1607983112335, 'accumulated_logging_time': 4.005815267562866}
I0306 10:33:22.860786 139787562383104 logging_writer.py:48] [121263] accumulated_eval_time=1541.160798, accumulated_logging_time=4.005815, accumulated_submission_time=40856.500374, global_step=121263, preemption_count=0, score=40856.500374, test/accuracy=0.584800, test/loss=1.903845, test/num_examples=10000, total_duration=42406.036387, train/accuracy=0.811982, train/loss=0.666156, validation/accuracy=0.709080, validation/loss=1.181629, validation/num_examples=50000
I0306 10:33:35.618538 139789013600000 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.9672305583953857, loss=1.068224310874939
I0306 10:34:09.194178 139787562383104 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.1056811809539795, loss=1.1303293704986572
I0306 10:34:42.796097 139789013600000 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.8490307331085205, loss=1.0553085803985596
I0306 10:35:16.446735 139787562383104 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.869598388671875, loss=1.203583002090454
I0306 10:35:50.059121 139789013600000 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.861398696899414, loss=1.0857000350952148
I0306 10:36:23.713535 139787562383104 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.9707956314086914, loss=1.087756633758545
I0306 10:36:57.354152 139789013600000 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.1188197135925293, loss=1.2551615238189697
I0306 10:37:31.169170 139787562383104 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.0085952281951904, loss=1.127040982246399
I0306 10:38:04.832331 139789013600000 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.0962281227111816, loss=1.0936224460601807
I0306 10:38:38.486809 139787562383104 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.029864549636841, loss=1.1649636030197144
I0306 10:39:12.122012 139789013600000 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.1475329399108887, loss=1.195568561553955
I0306 10:39:45.756875 139787562383104 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.06874680519104, loss=1.1027464866638184
I0306 10:40:19.374020 139789013600000 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.7661845684051514, loss=1.1832990646362305
I0306 10:40:53.018541 139787562383104 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.242666244506836, loss=1.1018880605697632
I0306 10:41:26.637551 139789013600000 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.306527853012085, loss=1.188981533050537
I0306 10:41:53.027339 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:41:59.139527 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:42:07.832489 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:42:10.105582 139951089751872 submission_runner.py:411] Time since start: 42933.32s, 	Step: 122780, 	{'train/accuracy': 0.8156090378761292, 'train/loss': 0.6675543189048767, 'validation/accuracy': 0.7113800048828125, 'validation/loss': 1.1680610179901123, 'validation/num_examples': 50000, 'test/accuracy': 0.5860000252723694, 'test/loss': 1.884270191192627, 'test/num_examples': 10000, 'score': 41366.60287451744, 'total_duration': 42933.317843437195, 'accumulated_submission_time': 41366.60287451744, 'accumulated_eval_time': 1558.2389419078827, 'accumulated_logging_time': 4.051869869232178}
I0306 10:42:10.143608 139788996814592 logging_writer.py:48] [122780] accumulated_eval_time=1558.238942, accumulated_logging_time=4.051870, accumulated_submission_time=41366.602875, global_step=122780, preemption_count=0, score=41366.602875, test/accuracy=0.586000, test/loss=1.884270, test/num_examples=10000, total_duration=42933.317843, train/accuracy=0.815609, train/loss=0.667554, validation/accuracy=0.711380, validation/loss=1.168061, validation/num_examples=50000
I0306 10:42:17.200051 139789005207296 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.172287940979004, loss=1.2019271850585938
I0306 10:42:50.877263 139788996814592 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.0042500495910645, loss=1.1123931407928467
I0306 10:43:24.531594 139789005207296 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.120797872543335, loss=1.1990678310394287
I0306 10:43:58.180968 139788996814592 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.068486452102661, loss=1.1506006717681885
I0306 10:44:31.745128 139789005207296 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.0560662746429443, loss=1.0733014345169067
I0306 10:45:05.389170 139788996814592 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.924440383911133, loss=1.2196933031082153
I0306 10:45:39.015443 139789005207296 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.0130598545074463, loss=1.1551971435546875
I0306 10:46:12.607170 139788996814592 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.9855895042419434, loss=1.1272497177124023
I0306 10:46:46.250919 139789005207296 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.057452917098999, loss=1.1623504161834717
I0306 10:47:19.898632 139788996814592 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.200392961502075, loss=1.2361544370651245
I0306 10:47:53.526592 139789005207296 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.755234956741333, loss=1.1313450336456299
I0306 10:48:27.135260 139788996814592 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.878847599029541, loss=1.0932323932647705
I0306 10:49:00.789907 139789005207296 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.2512943744659424, loss=1.1534234285354614
I0306 10:49:34.584372 139788996814592 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.2038662433624268, loss=1.2186028957366943
I0306 10:50:08.253269 139789005207296 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.8957931995391846, loss=1.0662014484405518
I0306 10:50:40.366833 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:50:46.477094 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:50:55.330730 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:50:57.585404 139951089751872 submission_runner.py:411] Time since start: 43460.80s, 	Step: 124297, 	{'train/accuracy': 0.8449258208274841, 'train/loss': 0.5565405488014221, 'validation/accuracy': 0.710099995136261, 'validation/loss': 1.1687265634536743, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8822606801986694, 'test/num_examples': 10000, 'score': 41876.761585474014, 'total_duration': 43460.797714948654, 'accumulated_submission_time': 41876.761585474014, 'accumulated_eval_time': 1575.4574618339539, 'accumulated_logging_time': 4.100099802017212}
I0306 10:50:57.621670 139787562383104 logging_writer.py:48] [124297] accumulated_eval_time=1575.457462, accumulated_logging_time=4.100100, accumulated_submission_time=41876.761585, global_step=124297, preemption_count=0, score=41876.761585, test/accuracy=0.591000, test/loss=1.882261, test/num_examples=10000, total_duration=43460.797715, train/accuracy=0.844926, train/loss=0.556541, validation/accuracy=0.710100, validation/loss=1.168727, validation/num_examples=50000
I0306 10:50:58.979928 139789013600000 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.039968729019165, loss=1.1082111597061157
I0306 10:51:32.658410 139787562383104 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.893519401550293, loss=1.1342222690582275
I0306 10:52:06.254303 139789013600000 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.3444125652313232, loss=1.184197187423706
I0306 10:52:39.840154 139787562383104 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.376476764678955, loss=1.2135652303695679
I0306 10:53:13.406167 139789013600000 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.132075309753418, loss=1.2354108095169067
I0306 10:53:47.009630 139787562383104 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.1270270347595215, loss=1.1079530715942383
I0306 10:54:20.663689 139789013600000 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.0578625202178955, loss=1.1098843812942505
I0306 10:54:54.294839 139787562383104 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.0710787773132324, loss=1.1765332221984863
I0306 10:55:27.944460 139789013600000 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.1109869480133057, loss=1.1603131294250488
I0306 10:56:01.673195 139787562383104 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.9188103675842285, loss=1.203979730606079
I0306 10:56:35.366305 139789013600000 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.522336483001709, loss=1.2350375652313232
I0306 10:57:08.990728 139787562383104 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.8908958435058594, loss=1.05203115940094
I0306 10:57:42.649150 139789013600000 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.1589789390563965, loss=1.229745626449585
I0306 10:58:16.286147 139787562383104 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.288874387741089, loss=1.1591682434082031
I0306 10:58:49.949873 139789013600000 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.9848744869232178, loss=1.035567283630371
I0306 10:59:23.571412 139787562383104 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.132542371749878, loss=1.0625907182693481
I0306 10:59:27.751070 139951089751872 spec.py:321] Evaluating on the training split.
I0306 10:59:33.887827 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 10:59:42.527096 139951089751872 spec.py:349] Evaluating on the test split.
I0306 10:59:44.811371 139951089751872 submission_runner.py:411] Time since start: 43988.02s, 	Step: 125814, 	{'train/accuracy': 0.8313934803009033, 'train/loss': 0.5971071124076843, 'validation/accuracy': 0.714139997959137, 'validation/loss': 1.1716444492340088, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.9235954284667969, 'test/num_examples': 10000, 'score': 42386.82617163658, 'total_duration': 43988.02368068695, 'accumulated_submission_time': 42386.82617163658, 'accumulated_eval_time': 1592.5177104473114, 'accumulated_logging_time': 4.146549224853516}
I0306 10:59:44.855906 139788996814592 logging_writer.py:48] [125814] accumulated_eval_time=1592.517710, accumulated_logging_time=4.146549, accumulated_submission_time=42386.826172, global_step=125814, preemption_count=0, score=42386.826172, test/accuracy=0.578100, test/loss=1.923595, test/num_examples=10000, total_duration=43988.023681, train/accuracy=0.831393, train/loss=0.597107, validation/accuracy=0.714140, validation/loss=1.171644, validation/num_examples=50000
I0306 11:00:14.053926 139789005207296 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.077141284942627, loss=1.196986198425293
I0306 11:00:47.616213 139788996814592 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.882643938064575, loss=1.0461664199829102
I0306 11:01:21.193329 139789005207296 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.434232234954834, loss=1.131736397743225
I0306 11:01:54.837580 139788996814592 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.35044002532959, loss=1.208773136138916
I0306 11:02:28.406975 139789005207296 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.1503846645355225, loss=1.1379351615905762
I0306 11:03:01.993745 139788996814592 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.1594958305358887, loss=1.0570056438446045
I0306 11:03:35.624307 139789005207296 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.3471972942352295, loss=1.2448593378067017
I0306 11:04:09.237320 139788996814592 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.12261962890625, loss=1.1323256492614746
I0306 11:04:42.878585 139789005207296 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.192233085632324, loss=1.177208662033081
I0306 11:05:16.518823 139788996814592 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.138270139694214, loss=1.1911120414733887
I0306 11:05:50.147470 139789005207296 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.136615037918091, loss=1.1775141954421997
I0306 11:06:23.786844 139788996814592 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.552684783935547, loss=1.2476201057434082
I0306 11:06:57.437960 139789005207296 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.0620946884155273, loss=1.0612702369689941
I0306 11:07:31.087265 139788996814592 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.042682647705078, loss=1.1094692945480347
I0306 11:08:04.781953 139789005207296 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.2037205696105957, loss=1.1339235305786133
I0306 11:08:14.993709 139951089751872 spec.py:321] Evaluating on the training split.
I0306 11:08:21.018115 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 11:08:29.979341 139951089751872 spec.py:349] Evaluating on the test split.
I0306 11:08:32.249778 139951089751872 submission_runner.py:411] Time since start: 44515.46s, 	Step: 127332, 	{'train/accuracy': 0.8355787396430969, 'train/loss': 0.5848914980888367, 'validation/accuracy': 0.7186799645423889, 'validation/loss': 1.14486563205719, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.8843411207199097, 'test/num_examples': 10000, 'score': 42896.90095162392, 'total_duration': 44515.461646556854, 'accumulated_submission_time': 42896.90095162392, 'accumulated_eval_time': 1609.7732861042023, 'accumulated_logging_time': 4.200785160064697}
I0306 11:08:32.287766 139787562383104 logging_writer.py:48] [127332] accumulated_eval_time=1609.773286, accumulated_logging_time=4.200785, accumulated_submission_time=42896.900952, global_step=127332, preemption_count=0, score=42896.900952, test/accuracy=0.587500, test/loss=1.884341, test/num_examples=10000, total_duration=44515.461647, train/accuracy=0.835579, train/loss=0.584891, validation/accuracy=0.718680, validation/loss=1.144866, validation/num_examples=50000
I0306 11:08:56.170936 139789013600000 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.905749559402466, loss=1.056719183921814
I0306 11:09:29.728600 139787562383104 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.172914505004883, loss=1.0650807619094849
I0306 11:10:03.299703 139789013600000 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.022655963897705, loss=1.1322731971740723
I0306 11:10:36.881485 139787562383104 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.473689079284668, loss=1.1354944705963135
I0306 11:11:10.533383 139789013600000 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.1778359413146973, loss=1.1038916110992432
I0306 11:11:44.144690 139787562383104 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.3213932514190674, loss=1.0974574089050293
I0306 11:12:17.800597 139789013600000 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.036271572113037, loss=1.1065385341644287
I0306 11:12:51.407242 139787562383104 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.8873119354248047, loss=1.0337295532226562
I0306 11:13:25.071873 139789013600000 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.877312421798706, loss=1.0839120149612427
I0306 11:13:58.690947 139787562383104 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.049870252609253, loss=1.0966113805770874
I0306 11:14:32.347502 139789013600000 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.0913777351379395, loss=1.1243342161178589
I0306 11:15:05.950409 139787562383104 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.03532338142395, loss=1.1471149921417236
I0306 11:15:39.509147 139789013600000 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.195727825164795, loss=1.1616564989089966
I0306 11:16:13.087468 139787562383104 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.3793528079986572, loss=1.1923943758010864
I0306 11:16:46.647952 139789013600000 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.4821720123291016, loss=1.210106611251831
I0306 11:17:02.538103 139951089751872 spec.py:321] Evaluating on the training split.
I0306 11:17:08.777165 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 11:17:17.631916 139951089751872 spec.py:349] Evaluating on the test split.
I0306 11:17:19.877698 139951089751872 submission_runner.py:411] Time since start: 45043.09s, 	Step: 128849, 	{'train/accuracy': 0.8219467401504517, 'train/loss': 0.6358543634414673, 'validation/accuracy': 0.7104799747467041, 'validation/loss': 1.1965482234954834, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.9271339178085327, 'test/num_examples': 10000, 'score': 43406.36694979668, 'total_duration': 45043.08939242363, 'accumulated_submission_time': 43406.36694979668, 'accumulated_eval_time': 1627.1122126579285, 'accumulated_logging_time': 4.968811750411987}
I0306 11:17:19.923582 139787562383104 logging_writer.py:48] [128849] accumulated_eval_time=1627.112213, accumulated_logging_time=4.968812, accumulated_submission_time=43406.366950, global_step=128849, preemption_count=0, score=43406.366950, test/accuracy=0.590000, test/loss=1.927134, test/num_examples=10000, total_duration=45043.089392, train/accuracy=0.821947, train/loss=0.635854, validation/accuracy=0.710480, validation/loss=1.196548, validation/num_examples=50000
I0306 11:17:37.420357 139788996814592 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.1140596866607666, loss=0.9785807132720947
I0306 11:18:11.005669 139787562383104 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.238300085067749, loss=1.0712509155273438
I0306 11:18:44.688352 139788996814592 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.9771289825439453, loss=1.0771584510803223
I0306 11:19:18.356662 139787562383104 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.1232049465179443, loss=1.0640448331832886
I0306 11:19:51.979385 139788996814592 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.917144775390625, loss=1.0387623310089111
I0306 11:20:25.630896 139787562383104 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.0962975025177, loss=1.076997995376587
I0306 11:20:59.309462 139788996814592 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.156702995300293, loss=1.1063047647476196
I0306 11:21:32.969050 139787562383104 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.9727187156677246, loss=0.9952481985092163
I0306 11:22:06.605315 139788996814592 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.2830300331115723, loss=1.0712591409683228
I0306 11:22:40.255890 139787562383104 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.1976213455200195, loss=1.1882624626159668
I0306 11:23:13.904913 139788996814592 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.3214712142944336, loss=1.1830453872680664
I0306 11:23:47.542729 139787562383104 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.216270923614502, loss=1.1505811214447021
I0306 11:24:21.189000 139788996814592 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.081894874572754, loss=1.1882474422454834
I0306 11:24:54.856201 139787562383104 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.2913010120391846, loss=1.1300854682922363
I0306 11:25:28.494668 139788996814592 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.310675859451294, loss=1.0210597515106201
I0306 11:25:50.166122 139951089751872 spec.py:321] Evaluating on the training split.
I0306 11:25:56.202390 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 11:26:05.107005 139951089751872 spec.py:349] Evaluating on the test split.
I0306 11:26:07.667516 139951089751872 submission_runner.py:411] Time since start: 45570.88s, 	Step: 130366, 	{'train/accuracy': 0.8296396732330322, 'train/loss': 0.599907636642456, 'validation/accuracy': 0.718559980392456, 'validation/loss': 1.1586591005325317, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.9023849964141846, 'test/num_examples': 10000, 'score': 43916.54048705101, 'total_duration': 45570.87974739075, 'accumulated_submission_time': 43916.54048705101, 'accumulated_eval_time': 1644.6134796142578, 'accumulated_logging_time': 5.027925252914429}
I0306 11:26:07.708161 139789021992704 logging_writer.py:48] [130366] accumulated_eval_time=1644.613480, accumulated_logging_time=5.027925, accumulated_submission_time=43916.540487, global_step=130366, preemption_count=0, score=43916.540487, test/accuracy=0.589200, test/loss=1.902385, test/num_examples=10000, total_duration=45570.879747, train/accuracy=0.829640, train/loss=0.599908, validation/accuracy=0.718560, validation/loss=1.158659, validation/num_examples=50000
I0306 11:26:19.521020 139789030385408 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.019275665283203, loss=1.0233526229858398
I0306 11:26:53.104140 139789021992704 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.394202947616577, loss=1.1989620923995972
I0306 11:27:26.648933 139789030385408 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.540524482727051, loss=1.2080609798431396
I0306 11:28:00.235480 139789021992704 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.5034890174865723, loss=1.1103341579437256
I0306 11:28:33.813526 139789030385408 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.3583109378814697, loss=1.1134792566299438
I0306 11:29:07.460490 139789021992704 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.193910837173462, loss=1.054667353630066
I0306 11:29:41.073920 139789030385408 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.277488946914673, loss=1.0764377117156982
I0306 11:30:14.733894 139789021992704 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.398510217666626, loss=1.1531729698181152
I0306 11:30:48.343226 139789030385408 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.2220544815063477, loss=1.0659754276275635
I0306 11:31:21.998588 139789021992704 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.3651795387268066, loss=1.1626499891281128
I0306 11:31:55.609629 139789030385408 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.3075437545776367, loss=1.1461923122406006
I0306 11:32:29.277594 139789021992704 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.618036985397339, loss=1.1553699970245361
I0306 11:33:02.851204 139789030385408 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.2443759441375732, loss=1.1148213148117065
I0306 11:33:36.454543 139789021992704 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.365682601928711, loss=1.1596975326538086
I0306 11:34:10.040473 139789030385408 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.2581374645233154, loss=1.0793685913085938
I0306 11:34:37.791260 139951089751872 spec.py:321] Evaluating on the training split.
I0306 11:34:43.932517 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 11:34:52.913203 139951089751872 spec.py:349] Evaluating on the test split.
I0306 11:34:55.194025 139951089751872 submission_runner.py:411] Time since start: 46098.41s, 	Step: 131884, 	{'train/accuracy': 0.8519012928009033, 'train/loss': 0.5267131328582764, 'validation/accuracy': 0.7257999777793884, 'validation/loss': 1.1247061491012573, 'validation/num_examples': 50000, 'test/accuracy': 0.6004000306129456, 'test/loss': 1.8547765016555786, 'test/num_examples': 10000, 'score': 44426.55810856819, 'total_duration': 46098.40629863739, 'accumulated_submission_time': 44426.55810856819, 'accumulated_eval_time': 1662.0161790847778, 'accumulated_logging_time': 5.078296184539795}
I0306 11:34:55.232540 139787361040128 logging_writer.py:48] [131884] accumulated_eval_time=1662.016179, accumulated_logging_time=5.078296, accumulated_submission_time=44426.558109, global_step=131884, preemption_count=0, score=44426.558109, test/accuracy=0.600400, test/loss=1.854777, test/num_examples=10000, total_duration=46098.406299, train/accuracy=0.851901, train/loss=0.526713, validation/accuracy=0.725800, validation/loss=1.124706, validation/num_examples=50000
I0306 11:35:00.954710 139787369432832 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.3926444053649902, loss=1.1638221740722656
I0306 11:35:34.625433 139787361040128 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.9968888759613037, loss=1.143265724182129
I0306 11:36:08.293690 139787369432832 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.283602714538574, loss=1.1400806903839111
I0306 11:36:41.975747 139787361040128 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.2851738929748535, loss=1.0553412437438965
I0306 11:37:15.636584 139787369432832 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.1743476390838623, loss=0.9560579657554626
I0306 11:37:49.282653 139787361040128 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.397810459136963, loss=1.0694619417190552
I0306 11:38:22.913243 139787369432832 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.5329349040985107, loss=1.138839840888977
I0306 11:38:56.552747 139787361040128 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.228846311569214, loss=0.957737147808075
I0306 11:39:30.182484 139787369432832 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.543069362640381, loss=1.0155057907104492
I0306 11:40:03.894454 139787361040128 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.310214042663574, loss=1.025137186050415
I0306 11:40:37.538931 139787369432832 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.1106865406036377, loss=1.0397400856018066
I0306 11:41:11.177650 139787361040128 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.3843917846679688, loss=1.0177969932556152
I0306 11:41:44.835469 139787369432832 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.302398681640625, loss=0.9806401133537292
I0306 11:42:18.482245 139787361040128 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.5041801929473877, loss=1.0230090618133545
I0306 11:42:52.097280 139787369432832 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.47277569770813, loss=1.0850930213928223
I0306 11:43:25.197422 139951089751872 spec.py:321] Evaluating on the training split.
I0306 11:43:31.224023 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 11:43:40.243127 139951089751872 spec.py:349] Evaluating on the test split.
I0306 11:43:42.466536 139951089751872 submission_runner.py:411] Time since start: 46625.68s, 	Step: 133400, 	{'train/accuracy': 0.8651546239852905, 'train/loss': 0.4772387146949768, 'validation/accuracy': 0.7245199680328369, 'validation/loss': 1.1348223686218262, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.873829960823059, 'test/num_examples': 10000, 'score': 44936.45907497406, 'total_duration': 46625.67883968353, 'accumulated_submission_time': 44936.45907497406, 'accumulated_eval_time': 1679.2852370738983, 'accumulated_logging_time': 5.126734733581543}
I0306 11:43:42.511412 139789013600000 logging_writer.py:48] [133400] accumulated_eval_time=1679.285237, accumulated_logging_time=5.126735, accumulated_submission_time=44936.459075, global_step=133400, preemption_count=0, score=44936.459075, test/accuracy=0.594800, test/loss=1.873830, test/num_examples=10000, total_duration=46625.678840, train/accuracy=0.865155, train/loss=0.477239, validation/accuracy=0.724520, validation/loss=1.134822, validation/num_examples=50000
I0306 11:43:42.858820 139789021992704 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.137759208679199, loss=1.1082916259765625
I0306 11:44:16.419970 139789013600000 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3930301666259766, loss=1.1633487939834595
I0306 11:44:50.229679 139789021992704 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.4149351119995117, loss=1.039911150932312
I0306 11:45:23.864487 139789013600000 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.3465187549591064, loss=1.0645008087158203
I0306 11:45:57.547717 139789021992704 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.268012762069702, loss=1.0645800828933716
I0306 11:46:31.150515 139789013600000 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.496638059616089, loss=1.0784090757369995
I0306 11:47:04.815085 139789021992704 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.3534204959869385, loss=0.9922195076942444
I0306 11:47:38.449592 139789013600000 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.1812775135040283, loss=1.0145814418792725
I0306 11:48:12.059209 139789021992704 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.2863261699676514, loss=0.9976330399513245
I0306 11:48:45.658514 139789013600000 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.398714542388916, loss=1.0081478357315063
I0306 11:49:19.193165 139789021992704 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.5332629680633545, loss=1.0738693475723267
I0306 11:49:52.766851 139789013600000 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.230116128921509, loss=1.005975604057312
I0306 11:50:26.433341 139789021992704 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.493356943130493, loss=1.0494542121887207
I0306 11:51:00.101563 139789013600000 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.589973211288452, loss=1.0289344787597656
I0306 11:51:33.675084 139789021992704 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.510608196258545, loss=1.0692856311798096
I0306 11:52:07.264776 139789013600000 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.6146416664123535, loss=1.0255354642868042
I0306 11:52:12.790163 139951089751872 spec.py:321] Evaluating on the training split.
I0306 11:52:18.788454 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 11:52:27.656089 139951089751872 spec.py:349] Evaluating on the test split.
I0306 11:52:30.002554 139951089751872 submission_runner.py:411] Time since start: 47153.21s, 	Step: 134918, 	{'train/accuracy': 0.8583386540412903, 'train/loss': 0.5006170272827148, 'validation/accuracy': 0.7210999727249146, 'validation/loss': 1.1423414945602417, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.9024995565414429, 'test/num_examples': 10000, 'score': 45446.67143511772, 'total_duration': 47153.214833021164, 'accumulated_submission_time': 45446.67143511772, 'accumulated_eval_time': 1696.4975454807281, 'accumulated_logging_time': 5.18248438835144}
I0306 11:52:30.056073 139788996814592 logging_writer.py:48] [134918] accumulated_eval_time=1696.497545, accumulated_logging_time=5.182484, accumulated_submission_time=45446.671435, global_step=134918, preemption_count=0, score=45446.671435, test/accuracy=0.596900, test/loss=1.902500, test/num_examples=10000, total_duration=47153.214833, train/accuracy=0.858339, train/loss=0.500617, validation/accuracy=0.721100, validation/loss=1.142341, validation/num_examples=50000
I0306 11:52:57.907542 139789005207296 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.5100960731506348, loss=1.1145991086959839
I0306 11:53:31.457456 139788996814592 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.655327558517456, loss=1.0496702194213867
I0306 11:54:05.034777 139789005207296 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.6058475971221924, loss=1.0775806903839111
I0306 11:54:38.686508 139788996814592 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.5954339504241943, loss=1.1018564701080322
I0306 11:55:12.317119 139789005207296 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.346139430999756, loss=1.056220293045044
I0306 11:55:45.965331 139788996814592 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.4241557121276855, loss=1.1051788330078125
I0306 11:56:19.606485 139789005207296 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.270745277404785, loss=1.0004962682724
I0306 11:56:53.301696 139788996814592 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.315908670425415, loss=0.9355969429016113
I0306 11:57:26.884847 139789005207296 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.625864028930664, loss=1.1055498123168945
I0306 11:58:00.534912 139788996814592 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.607774019241333, loss=0.9968999028205872
I0306 11:58:34.180700 139789005207296 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.604543447494507, loss=1.0019649267196655
I0306 11:59:07.816540 139788996814592 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.5520401000976562, loss=1.0347867012023926
I0306 11:59:41.466116 139789005207296 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.435147762298584, loss=0.9949744343757629
I0306 12:00:15.123563 139788996814592 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.7388689517974854, loss=1.009653091430664
I0306 12:00:48.747322 139789005207296 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.5097947120666504, loss=1.0433975458145142
I0306 12:01:00.310104 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:01:06.362983 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:01:15.361635 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:01:17.621696 139951089751872 submission_runner.py:411] Time since start: 47680.83s, 	Step: 136436, 	{'train/accuracy': 0.852937638759613, 'train/loss': 0.5173604488372803, 'validation/accuracy': 0.7234799861907959, 'validation/loss': 1.1310100555419922, 'validation/num_examples': 50000, 'test/accuracy': 0.5994000434875488, 'test/loss': 1.866504430770874, 'test/num_examples': 10000, 'score': 45956.86107802391, 'total_duration': 47680.83400058746, 'accumulated_submission_time': 45956.86107802391, 'accumulated_eval_time': 1713.8090782165527, 'accumulated_logging_time': 5.246311902999878}
I0306 12:01:17.661602 139787562383104 logging_writer.py:48] [136436] accumulated_eval_time=1713.809078, accumulated_logging_time=5.246312, accumulated_submission_time=45956.861078, global_step=136436, preemption_count=0, score=45956.861078, test/accuracy=0.599400, test/loss=1.866504, test/num_examples=10000, total_duration=47680.834001, train/accuracy=0.852938, train/loss=0.517360, validation/accuracy=0.723480, validation/loss=1.131010, validation/num_examples=50000
I0306 12:01:39.480489 139788996814592 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.3442485332489014, loss=0.943385899066925
I0306 12:02:13.069884 139787562383104 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.8612678050994873, loss=1.1060734987258911
I0306 12:02:46.726295 139788996814592 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.975527286529541, loss=0.9947189092636108
I0306 12:03:20.389352 139787562383104 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.307432174682617, loss=0.9864184260368347
I0306 12:03:54.038463 139788996814592 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.516545534133911, loss=1.1036033630371094
I0306 12:04:27.665969 139787562383104 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.3582763671875, loss=1.0093530416488647
I0306 12:05:01.322464 139788996814592 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.551133394241333, loss=1.1171530485153198
I0306 12:05:34.941061 139787562383104 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.7408041954040527, loss=1.0783600807189941
I0306 12:06:08.607593 139788996814592 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.2486464977264404, loss=0.9852784872055054
I0306 12:06:42.198083 139787562383104 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.837662935256958, loss=1.0848288536071777
I0306 12:07:15.834276 139788996814592 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.545797824859619, loss=1.0276296138763428
I0306 12:07:49.443724 139787562383104 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.4704978466033936, loss=1.0301878452301025
I0306 12:08:23.110618 139788996814592 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.669463634490967, loss=0.9909061789512634
I0306 12:08:56.869908 139787562383104 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.4731197357177734, loss=0.9945400953292847
I0306 12:09:30.525717 139788996814592 logging_writer.py:48] [137900] global_step=137900, grad_norm=4.063604831695557, loss=1.1308178901672363
I0306 12:09:47.813086 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:09:53.930159 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:10:02.908995 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:10:05.182644 139951089751872 submission_runner.py:411] Time since start: 48208.39s, 	Step: 137953, 	{'train/accuracy': 0.8542529940605164, 'train/loss': 0.5037813782691956, 'validation/accuracy': 0.7277799844741821, 'validation/loss': 1.1227219104766846, 'validation/num_examples': 50000, 'test/accuracy': 0.6015000343322754, 'test/loss': 1.8601411581039429, 'test/num_examples': 10000, 'score': 46466.94680929184, 'total_duration': 48208.394939661026, 'accumulated_submission_time': 46466.94680929184, 'accumulated_eval_time': 1731.1785836219788, 'accumulated_logging_time': 5.296331167221069}
I0306 12:10:05.223332 139787369432832 logging_writer.py:48] [137953] accumulated_eval_time=1731.178584, accumulated_logging_time=5.296331, accumulated_submission_time=46466.946809, global_step=137953, preemption_count=0, score=46466.946809, test/accuracy=0.601500, test/loss=1.860141, test/num_examples=10000, total_duration=48208.394940, train/accuracy=0.854253, train/loss=0.503781, validation/accuracy=0.727780, validation/loss=1.122722, validation/num_examples=50000
I0306 12:10:21.319498 139787562383104 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.642904281616211, loss=1.123359203338623
I0306 12:10:54.869971 139787369432832 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.6362617015838623, loss=0.9480756521224976
I0306 12:11:28.457815 139787562383104 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.8083534240722656, loss=1.0750761032104492
I0306 12:12:02.036132 139787369432832 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.72031307220459, loss=1.1063225269317627
I0306 12:12:35.685546 139787562383104 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.4236130714416504, loss=0.9937034845352173
I0306 12:13:09.313875 139787369432832 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.5965821743011475, loss=0.9996985197067261
I0306 12:13:42.980621 139787562383104 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.3970067501068115, loss=0.9287809133529663
I0306 12:14:16.615143 139787369432832 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.5989131927490234, loss=0.9517767429351807
I0306 12:14:50.261704 139787562383104 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.8158986568450928, loss=1.0435773134231567
I0306 12:15:23.950343 139787369432832 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.6651673316955566, loss=0.9724412560462952
I0306 12:15:57.521454 139787562383104 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.6629390716552734, loss=1.0602144002914429
I0306 12:16:31.120099 139787369432832 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.224233865737915, loss=0.9966100454330444
I0306 12:17:04.767688 139787562383104 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.5845561027526855, loss=1.000107765197754
I0306 12:17:38.415625 139787369432832 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.8239388465881348, loss=0.9710007905960083
I0306 12:18:12.096678 139787562383104 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.738755941390991, loss=0.9779128432273865
I0306 12:18:35.460544 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:18:41.415524 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:18:50.187879 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:18:52.450505 139951089751872 submission_runner.py:411] Time since start: 48735.66s, 	Step: 139471, 	{'train/accuracy': 0.8598333597183228, 'train/loss': 0.4867278039455414, 'validation/accuracy': 0.7307999730110168, 'validation/loss': 1.1039236783981323, 'validation/num_examples': 50000, 'test/accuracy': 0.5992000102996826, 'test/loss': 1.8634774684906006, 'test/num_examples': 10000, 'score': 46977.11908912659, 'total_duration': 48735.66280722618, 'accumulated_submission_time': 46977.11908912659, 'accumulated_eval_time': 1748.1684920787811, 'accumulated_logging_time': 5.347053289413452}
I0306 12:18:52.491573 139787369432832 logging_writer.py:48] [139471] accumulated_eval_time=1748.168492, accumulated_logging_time=5.347053, accumulated_submission_time=46977.119089, global_step=139471, preemption_count=0, score=46977.119089, test/accuracy=0.599200, test/loss=1.863477, test/num_examples=10000, total_duration=48735.662807, train/accuracy=0.859833, train/loss=0.486728, validation/accuracy=0.730800, validation/loss=1.103924, validation/num_examples=50000
I0306 12:19:02.568598 139789021992704 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.2680306434631348, loss=0.8752789497375488
I0306 12:19:36.162551 139787369432832 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.8273074626922607, loss=1.0220088958740234
I0306 12:20:09.764708 139789021992704 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.4104626178741455, loss=0.9153014421463013
I0306 12:20:43.428708 139787369432832 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.2714080810546875, loss=0.91098952293396
I0306 12:21:17.136745 139789021992704 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.7640860080718994, loss=0.9887017607688904
I0306 12:21:50.794807 139787369432832 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.5422778129577637, loss=0.9649864435195923
I0306 12:22:24.449037 139789021992704 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.4561970233917236, loss=1.0008013248443604
I0306 12:22:58.099720 139787369432832 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.642784595489502, loss=1.0373361110687256
I0306 12:23:31.716906 139789021992704 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.7631702423095703, loss=0.9539915323257446
I0306 12:24:05.371059 139787369432832 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.7262864112854004, loss=1.0868728160858154
I0306 12:24:39.010272 139789021992704 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.6543142795562744, loss=1.0392274856567383
I0306 12:25:12.673451 139787369432832 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.4519431591033936, loss=1.0037283897399902
I0306 12:25:46.326582 139789021992704 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.37052321434021, loss=0.9300845265388489
I0306 12:26:19.995765 139787369432832 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.5431153774261475, loss=0.9308459758758545
I0306 12:26:53.643799 139789021992704 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.83699369430542, loss=1.0757033824920654
I0306 12:27:22.771809 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:27:28.797186 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:27:37.656904 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:27:39.897053 139951089751872 submission_runner.py:411] Time since start: 49263.11s, 	Step: 140988, 	{'train/accuracy': 0.8991350531578064, 'train/loss': 0.36057475209236145, 'validation/accuracy': 0.7303000092506409, 'validation/loss': 1.114681601524353, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.8561393022537231, 'test/num_examples': 10000, 'score': 47487.33484148979, 'total_duration': 49263.10889315605, 'accumulated_submission_time': 47487.33484148979, 'accumulated_eval_time': 1765.2932143211365, 'accumulated_logging_time': 5.399057388305664}
I0306 12:27:39.937154 139787562383104 logging_writer.py:48] [140988] accumulated_eval_time=1765.293214, accumulated_logging_time=5.399057, accumulated_submission_time=47487.334841, global_step=140988, preemption_count=0, score=47487.334841, test/accuracy=0.603400, test/loss=1.856139, test/num_examples=10000, total_duration=49263.108893, train/accuracy=0.899135, train/loss=0.360575, validation/accuracy=0.730300, validation/loss=1.114682, validation/num_examples=50000
I0306 12:27:44.297965 139788996814592 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.6520683765411377, loss=1.0948883295059204
I0306 12:28:17.918350 139787562383104 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.5930275917053223, loss=1.016991376876831
I0306 12:28:51.565398 139788996814592 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.564192771911621, loss=1.043205738067627
I0306 12:29:25.178920 139787562383104 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.6408050060272217, loss=0.9072761535644531
I0306 12:29:58.758008 139788996814592 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.394473075866699, loss=0.9884246587753296
I0306 12:30:32.312139 139787562383104 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.5368499755859375, loss=0.9389028549194336
I0306 12:31:05.866588 139788996814592 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.5802009105682373, loss=0.9496762752532959
I0306 12:31:39.449156 139787562383104 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.8187570571899414, loss=0.9819347858428955
I0306 12:32:13.033752 139788996814592 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.5830421447753906, loss=1.0179399251937866
I0306 12:32:46.695615 139787562383104 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.896850347518921, loss=0.9740363359451294
I0306 12:33:20.302104 139788996814592 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.698073387145996, loss=0.9975287318229675
I0306 12:33:53.962033 139787562383104 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.962602376937866, loss=1.0756808519363403
I0306 12:34:27.563490 139788996814592 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.8935556411743164, loss=0.9711319208145142
I0306 12:35:01.206988 139787562383104 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.581430196762085, loss=0.9422715902328491
I0306 12:35:34.842787 139788996814592 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.612276315689087, loss=0.918753981590271
I0306 12:36:08.501510 139787562383104 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.790684461593628, loss=0.9156424403190613
I0306 12:36:09.988881 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:36:16.066061 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:36:24.925824 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:36:27.156765 139951089751872 submission_runner.py:411] Time since start: 49790.37s, 	Step: 142506, 	{'train/accuracy': 0.8854033350944519, 'train/loss': 0.39159104228019714, 'validation/accuracy': 0.7321400046348572, 'validation/loss': 1.103589653968811, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.8697199821472168, 'test/num_examples': 10000, 'score': 47997.32077693939, 'total_duration': 49790.369067668915, 'accumulated_submission_time': 47997.32077693939, 'accumulated_eval_time': 1782.4610340595245, 'accumulated_logging_time': 5.4506518840789795}
I0306 12:36:27.202465 139787369432832 logging_writer.py:48] [142506] accumulated_eval_time=1782.461034, accumulated_logging_time=5.450652, accumulated_submission_time=47997.320777, global_step=142506, preemption_count=0, score=47997.320777, test/accuracy=0.601000, test/loss=1.869720, test/num_examples=10000, total_duration=49790.369068, train/accuracy=0.885403, train/loss=0.391591, validation/accuracy=0.732140, validation/loss=1.103590, validation/num_examples=50000
I0306 12:36:59.067974 139787562383104 logging_writer.py:48] [142600] global_step=142600, grad_norm=4.085809707641602, loss=1.0457217693328857
I0306 12:37:32.707033 139787369432832 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.7571542263031006, loss=0.9694631695747375
I0306 12:38:06.329099 139787562383104 logging_writer.py:48] [142800] global_step=142800, grad_norm=4.013781547546387, loss=1.0820727348327637
I0306 12:38:39.976110 139787369432832 logging_writer.py:48] [142900] global_step=142900, grad_norm=4.044877052307129, loss=0.9383905529975891
I0306 12:39:13.591910 139787562383104 logging_writer.py:48] [143000] global_step=143000, grad_norm=4.220685958862305, loss=0.9468499422073364
I0306 12:39:47.255529 139787369432832 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.7794666290283203, loss=0.9384891390800476
I0306 12:40:20.869818 139787562383104 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.901839256286621, loss=0.9735541939735413
I0306 12:40:54.521537 139787369432832 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.525357484817505, loss=0.9196376204490662
I0306 12:41:28.142014 139787562383104 logging_writer.py:48] [143400] global_step=143400, grad_norm=4.252892971038818, loss=0.9382217526435852
I0306 12:42:01.790284 139787369432832 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.964755058288574, loss=0.999748170375824
I0306 12:42:35.401620 139787562383104 logging_writer.py:48] [143600] global_step=143600, grad_norm=4.045661449432373, loss=1.0352762937545776
I0306 12:43:09.064931 139787369432832 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.9855527877807617, loss=0.9543197751045227
I0306 12:43:42.682694 139787562383104 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.711451292037964, loss=0.9163261651992798
I0306 12:44:16.335551 139787369432832 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.9082846641540527, loss=1.0010089874267578
I0306 12:44:49.986189 139787562383104 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.454479455947876, loss=0.8929802775382996
I0306 12:44:57.178426 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:45:03.249487 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:45:12.174040 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:45:14.432873 139951089751872 submission_runner.py:411] Time since start: 50317.65s, 	Step: 144023, 	{'train/accuracy': 0.8768733739852905, 'train/loss': 0.4285891652107239, 'validation/accuracy': 0.7296800017356873, 'validation/loss': 1.1146568059921265, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.8817957639694214, 'test/num_examples': 10000, 'score': 48507.233662605286, 'total_duration': 50317.64517068863, 'accumulated_submission_time': 48507.233662605286, 'accumulated_eval_time': 1799.7154169082642, 'accumulated_logging_time': 5.505875587463379}
I0306 12:45:14.471961 139787361040128 logging_writer.py:48] [144023] accumulated_eval_time=1799.715417, accumulated_logging_time=5.505876, accumulated_submission_time=48507.233663, global_step=144023, preemption_count=0, score=48507.233663, test/accuracy=0.602200, test/loss=1.881796, test/num_examples=10000, total_duration=50317.645171, train/accuracy=0.876873, train/loss=0.428589, validation/accuracy=0.729680, validation/loss=1.114657, validation/num_examples=50000
I0306 12:45:40.749289 139787369432832 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.9467391967773438, loss=1.041068196296692
I0306 12:46:14.393766 139787361040128 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.7796828746795654, loss=0.9483640789985657
I0306 12:46:48.012024 139787369432832 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.7101478576660156, loss=0.9604664444923401
I0306 12:47:21.655412 139787361040128 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.6425960063934326, loss=0.9181637167930603
I0306 12:47:55.262297 139787369432832 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.5575032234191895, loss=0.8554673194885254
I0306 12:48:28.906841 139787361040128 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.9806928634643555, loss=0.9740988612174988
I0306 12:49:02.529985 139787369432832 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.6714208126068115, loss=0.9543002843856812
I0306 12:49:36.183478 139787361040128 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.7988810539245605, loss=0.8989740610122681
I0306 12:50:09.779026 139787369432832 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.8095650672912598, loss=0.8849315047264099
I0306 12:50:43.426906 139787361040128 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.909128427505493, loss=0.9267356991767883
I0306 12:51:17.050395 139787369432832 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.6704559326171875, loss=0.9541979432106018
I0306 12:51:50.738735 139787361040128 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.912260055541992, loss=1.0260324478149414
I0306 12:52:24.285389 139787369432832 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.9424290657043457, loss=1.0225354433059692
I0306 12:52:57.910436 139787361040128 logging_writer.py:48] [145400] global_step=145400, grad_norm=4.047133445739746, loss=0.9160985946655273
I0306 12:53:31.533236 139787369432832 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.7430903911590576, loss=0.9165892601013184
I0306 12:53:44.447315 139951089751872 spec.py:321] Evaluating on the training split.
I0306 12:53:50.478699 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 12:53:59.438601 139951089751872 spec.py:349] Evaluating on the test split.
I0306 12:54:01.714119 139951089751872 submission_runner.py:411] Time since start: 50844.93s, 	Step: 145540, 	{'train/accuracy': 0.8807597160339355, 'train/loss': 0.41349780559539795, 'validation/accuracy': 0.7312399744987488, 'validation/loss': 1.1134945154190063, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.8748246431350708, 'test/num_examples': 10000, 'score': 49017.14409446716, 'total_duration': 50844.926412820816, 'accumulated_submission_time': 49017.14409446716, 'accumulated_eval_time': 1816.9821512699127, 'accumulated_logging_time': 5.555546760559082}
I0306 12:54:01.755299 139788996814592 logging_writer.py:48] [145540] accumulated_eval_time=1816.982151, accumulated_logging_time=5.555547, accumulated_submission_time=49017.144094, global_step=145540, preemption_count=0, score=49017.144094, test/accuracy=0.602800, test/loss=1.874825, test/num_examples=10000, total_duration=50844.926413, train/accuracy=0.880760, train/loss=0.413498, validation/accuracy=0.731240, validation/loss=1.113495, validation/num_examples=50000
I0306 12:54:22.283401 139789021992704 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.658891201019287, loss=0.9494917392730713
I0306 12:54:55.932175 139788996814592 logging_writer.py:48] [145700] global_step=145700, grad_norm=4.022152423858643, loss=0.9130529761314392
I0306 12:55:29.505984 139789021992704 logging_writer.py:48] [145800] global_step=145800, grad_norm=4.041654586791992, loss=0.9081602692604065
I0306 12:56:03.096503 139788996814592 logging_writer.py:48] [145900] global_step=145900, grad_norm=4.197285175323486, loss=0.9920880794525146
I0306 12:56:36.679498 139789021992704 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.5796515941619873, loss=0.8634668588638306
I0306 12:57:10.293481 139788996814592 logging_writer.py:48] [146100] global_step=146100, grad_norm=4.002761363983154, loss=0.8890193104743958
I0306 12:57:43.950235 139789021992704 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.6610641479492188, loss=0.954484760761261
I0306 12:58:17.592003 139788996814592 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.5958938598632812, loss=0.8798010349273682
I0306 12:58:51.248369 139789021992704 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.9500319957733154, loss=0.910628616809845
I0306 12:59:24.869716 139788996814592 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.089791774749756, loss=0.8912708163261414
I0306 12:59:58.533616 139789021992704 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.9374654293060303, loss=0.9115675091743469
I0306 13:00:32.151632 139788996814592 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.751023054122925, loss=0.8646634817123413
I0306 13:01:05.776688 139789021992704 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.5939409732818604, loss=0.8110945224761963
I0306 13:01:39.393973 139788996814592 logging_writer.py:48] [146900] global_step=146900, grad_norm=4.06488561630249, loss=0.8946858644485474
I0306 13:02:13.023139 139789021992704 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.5447232723236084, loss=0.9252223968505859
I0306 13:02:31.985397 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:02:38.073699 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:02:46.866544 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:02:49.203062 139951089751872 submission_runner.py:411] Time since start: 51372.42s, 	Step: 147058, 	{'train/accuracy': 0.8866389989852905, 'train/loss': 0.3935048282146454, 'validation/accuracy': 0.7372199892997742, 'validation/loss': 1.0860953330993652, 'validation/num_examples': 50000, 'test/accuracy': 0.6084000468254089, 'test/loss': 1.8452149629592896, 'test/num_examples': 10000, 'score': 49527.31033730507, 'total_duration': 51372.41536259651, 'accumulated_submission_time': 49527.31033730507, 'accumulated_eval_time': 1834.1997547149658, 'accumulated_logging_time': 5.6061952114105225}
I0306 13:02:49.245072 139789013600000 logging_writer.py:48] [147058] accumulated_eval_time=1834.199755, accumulated_logging_time=5.606195, accumulated_submission_time=49527.310337, global_step=147058, preemption_count=0, score=49527.310337, test/accuracy=0.608400, test/loss=1.845215, test/num_examples=10000, total_duration=51372.415363, train/accuracy=0.886639, train/loss=0.393505, validation/accuracy=0.737220, validation/loss=1.086095, validation/num_examples=50000
I0306 13:03:03.668759 139789021992704 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.019000053405762, loss=0.9366123080253601
I0306 13:03:37.244299 139789013600000 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.67386794090271, loss=0.8278647661209106
I0306 13:04:10.976172 139789021992704 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.5213401317596436, loss=0.8962560892105103
I0306 13:04:44.573832 139789013600000 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.768004894256592, loss=0.9062818884849548
I0306 13:05:18.213422 139789021992704 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.7206759452819824, loss=0.8211005330085754
I0306 13:05:51.852140 139789013600000 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.997701406478882, loss=0.9393419027328491
I0306 13:06:25.527201 139789021992704 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.7885351181030273, loss=0.936042308807373
I0306 13:06:59.172092 139789013600000 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.064764976501465, loss=0.846739649772644
I0306 13:07:32.831681 139789021992704 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.1627044677734375, loss=0.8662563562393188
I0306 13:08:06.488821 139789013600000 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.9381954669952393, loss=0.8383066058158875
I0306 13:08:40.082849 139789021992704 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.9728469848632812, loss=0.8434916138648987
I0306 13:09:13.663021 139789013600000 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.2281622886657715, loss=0.8499971032142639
I0306 13:09:47.209895 139789021992704 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.9505393505096436, loss=0.884605884552002
I0306 13:10:20.913446 139789013600000 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.105828762054443, loss=0.8234856128692627
I0306 13:10:54.515115 139789021992704 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.9557082653045654, loss=0.8865318894386292
I0306 13:11:19.509993 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:11:25.559240 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:11:34.347116 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:11:36.596330 139951089751872 submission_runner.py:411] Time since start: 51899.81s, 	Step: 148576, 	{'train/accuracy': 0.886738657951355, 'train/loss': 0.3839896619319916, 'validation/accuracy': 0.7354399561882019, 'validation/loss': 1.1088933944702148, 'validation/num_examples': 50000, 'test/accuracy': 0.6083000302314758, 'test/loss': 1.8626391887664795, 'test/num_examples': 10000, 'score': 50037.51057219505, 'total_duration': 51899.808640003204, 'accumulated_submission_time': 50037.51057219505, 'accumulated_eval_time': 1851.2860395908356, 'accumulated_logging_time': 5.658465147018433}
I0306 13:11:36.637843 139788996814592 logging_writer.py:48] [148576] accumulated_eval_time=1851.286040, accumulated_logging_time=5.658465, accumulated_submission_time=50037.510572, global_step=148576, preemption_count=0, score=50037.510572, test/accuracy=0.608300, test/loss=1.862639, test/num_examples=10000, total_duration=51899.808640, train/accuracy=0.886739, train/loss=0.383990, validation/accuracy=0.735440, validation/loss=1.108893, validation/num_examples=50000
I0306 13:11:45.034056 139789005207296 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.066795825958252, loss=0.858152449131012
I0306 13:12:18.614541 139788996814592 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.6631510257720947, loss=0.8985131978988647
I0306 13:12:52.223875 139789005207296 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.816509962081909, loss=0.9270186424255371
I0306 13:13:25.882160 139788996814592 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.7010326385498047, loss=0.864675760269165
I0306 13:13:59.546797 139789005207296 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8195996284484863, loss=0.8801285028457642
I0306 13:14:33.192025 139788996814592 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.56705904006958, loss=0.79168301820755
I0306 13:15:06.882721 139789005207296 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.807072639465332, loss=0.8271263837814331
I0306 13:15:40.526835 139788996814592 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.5779998302459717, loss=0.7787210941314697
I0306 13:16:14.164604 139789005207296 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.046013832092285, loss=0.9056070446968079
I0306 13:16:47.794345 139788996814592 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.022343635559082, loss=0.9086819291114807
I0306 13:17:21.364877 139789005207296 logging_writer.py:48] [149600] global_step=149600, grad_norm=4.301487922668457, loss=0.8911421298980713
I0306 13:17:54.949328 139788996814592 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.7308948040008545, loss=0.8796713352203369
I0306 13:18:28.569957 139789005207296 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.9975154399871826, loss=0.8611354231834412
I0306 13:19:02.216744 139788996814592 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.605308771133423, loss=0.807621419429779
I0306 13:19:35.817190 139789005207296 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.6961870193481445, loss=0.9242705702781677
I0306 13:20:06.918608 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:20:12.906062 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:20:21.824858 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:20:24.154456 139951089751872 submission_runner.py:411] Time since start: 52427.37s, 	Step: 150094, 	{'train/accuracy': 0.9194435477256775, 'train/loss': 0.27869871258735657, 'validation/accuracy': 0.7404599785804749, 'validation/loss': 1.088493824005127, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.8475301265716553, 'test/num_examples': 10000, 'score': 50547.72637343407, 'total_duration': 52427.36676931381, 'accumulated_submission_time': 50547.72637343407, 'accumulated_eval_time': 1868.5218431949615, 'accumulated_logging_time': 5.710692644119263}
I0306 13:20:24.197647 139787562383104 logging_writer.py:48] [150094] accumulated_eval_time=1868.521843, accumulated_logging_time=5.710693, accumulated_submission_time=50547.726373, global_step=150094, preemption_count=0, score=50547.726373, test/accuracy=0.610700, test/loss=1.847530, test/num_examples=10000, total_duration=52427.366769, train/accuracy=0.919444, train/loss=0.278699, validation/accuracy=0.740460, validation/loss=1.088494, validation/num_examples=50000
I0306 13:20:26.553497 139789013600000 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.074772834777832, loss=0.9491490721702576
I0306 13:21:00.128931 139787562383104 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.811051845550537, loss=0.8569444417953491
I0306 13:21:33.686612 139789013600000 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.257632732391357, loss=0.87195885181427
I0306 13:22:07.270997 139787562383104 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.4602766036987305, loss=0.9861505627632141
I0306 13:22:40.899996 139789013600000 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.8889882564544678, loss=0.8416597247123718
I0306 13:23:14.467547 139787562383104 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.362363815307617, loss=0.8969738483428955
I0306 13:23:48.042921 139789013600000 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.068126678466797, loss=0.8471922874450684
I0306 13:24:21.662513 139787562383104 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.063825607299805, loss=0.8769969940185547
I0306 13:24:55.259677 139789013600000 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.260385036468506, loss=0.9070478081703186
I0306 13:25:28.924044 139787562383104 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.429352283477783, loss=0.884319543838501
I0306 13:26:02.542301 139789013600000 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.9930291175842285, loss=0.9580167531967163
I0306 13:26:36.197271 139787562383104 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.7785046100616455, loss=0.8570746779441833
I0306 13:27:09.853503 139789013600000 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.10360050201416, loss=0.9212042689323425
I0306 13:27:43.509310 139787562383104 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.29417085647583, loss=0.8588271141052246
I0306 13:28:17.116242 139789013600000 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.9091122150421143, loss=0.8788754940032959
I0306 13:28:50.784113 139787562383104 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.734290838241577, loss=0.819706380367279
I0306 13:28:54.288010 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:29:00.367174 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:29:09.367451 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:29:11.672617 139951089751872 submission_runner.py:411] Time since start: 52954.88s, 	Step: 151612, 	{'train/accuracy': 0.9063894748687744, 'train/loss': 0.3237541913986206, 'validation/accuracy': 0.7383399605751038, 'validation/loss': 1.0937960147857666, 'validation/num_examples': 50000, 'test/accuracy': 0.6117000579833984, 'test/loss': 1.8623268604278564, 'test/num_examples': 10000, 'score': 51057.75116443634, 'total_duration': 52954.8849272728, 'accumulated_submission_time': 51057.75116443634, 'accumulated_eval_time': 1885.9064099788666, 'accumulated_logging_time': 5.763494491577148}
I0306 13:29:11.720032 139787369432832 logging_writer.py:48] [151612] accumulated_eval_time=1885.906410, accumulated_logging_time=5.763494, accumulated_submission_time=51057.751164, global_step=151612, preemption_count=0, score=51057.751164, test/accuracy=0.611700, test/loss=1.862327, test/num_examples=10000, total_duration=52954.884927, train/accuracy=0.906389, train/loss=0.323754, validation/accuracy=0.738340, validation/loss=1.093796, validation/num_examples=50000
I0306 13:29:41.693942 139787562383104 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.016266345977783, loss=0.8665398955345154
I0306 13:30:15.359935 139787369432832 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.9716949462890625, loss=0.7549721598625183
I0306 13:30:48.990495 139787562383104 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.223696708679199, loss=0.8022786378860474
I0306 13:31:22.625230 139787369432832 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.030259132385254, loss=0.8396294713020325
I0306 13:31:56.222166 139787562383104 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.579047918319702, loss=0.7574003338813782
I0306 13:32:29.825964 139787369432832 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.28705358505249, loss=0.7860223054885864
I0306 13:33:03.467451 139787562383104 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.7567811012268066, loss=0.7199841141700745
I0306 13:33:37.098783 139787369432832 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.047310829162598, loss=0.7956240177154541
I0306 13:34:10.731017 139787562383104 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.8748998641967773, loss=0.7688694000244141
I0306 13:34:44.377128 139787369432832 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.431464672088623, loss=0.9821206331253052
I0306 13:35:17.955498 139787562383104 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.8693432807922363, loss=0.8025880455970764
I0306 13:35:51.590541 139787369432832 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.256213665008545, loss=0.8676406741142273
I0306 13:36:25.230381 139787562383104 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.084877014160156, loss=0.8147485852241516
I0306 13:36:58.877483 139787369432832 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.263280868530273, loss=0.9186764359474182
I0306 13:37:32.522988 139787562383104 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.1503753662109375, loss=0.8607799410820007
I0306 13:37:41.749381 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:37:48.510918 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:37:57.485509 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:37:59.770269 139951089751872 submission_runner.py:411] Time since start: 53482.98s, 	Step: 153129, 	{'train/accuracy': 0.9104551672935486, 'train/loss': 0.31001177430152893, 'validation/accuracy': 0.7428399920463562, 'validation/loss': 1.072890043258667, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.8307033777236938, 'test/num_examples': 10000, 'score': 51567.71314716339, 'total_duration': 53482.98257446289, 'accumulated_submission_time': 51567.71314716339, 'accumulated_eval_time': 1903.927239894867, 'accumulated_logging_time': 5.821335554122925}
I0306 13:37:59.816359 139787361040128 logging_writer.py:48] [153129] accumulated_eval_time=1903.927240, accumulated_logging_time=5.821336, accumulated_submission_time=51567.713147, global_step=153129, preemption_count=0, score=51567.713147, test/accuracy=0.614800, test/loss=1.830703, test/num_examples=10000, total_duration=53482.982574, train/accuracy=0.910455, train/loss=0.310012, validation/accuracy=0.742840, validation/loss=1.072890, validation/num_examples=50000
I0306 13:38:24.069882 139789013600000 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.090632438659668, loss=0.8590879440307617
I0306 13:38:57.680173 139787361040128 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.2884368896484375, loss=0.8840264081954956
I0306 13:39:31.349054 139789013600000 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.8504178524017334, loss=0.8389748930931091
I0306 13:40:05.035240 139787361040128 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.198604583740234, loss=0.8871249556541443
I0306 13:40:38.637765 139789013600000 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.129703044891357, loss=0.8429427146911621
I0306 13:41:12.264863 139787361040128 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.7752034664154053, loss=0.7805182933807373
I0306 13:41:45.865737 139789013600000 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.176696300506592, loss=0.8420493602752686
I0306 13:42:19.529083 139787361040128 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.630139350891113, loss=0.8847166895866394
I0306 13:42:53.147984 139789013600000 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.814305067062378, loss=0.8134434819221497
I0306 13:43:26.812644 139787361040128 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.3181915283203125, loss=0.8676631450653076
I0306 13:44:00.428138 139789013600000 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.9542784690856934, loss=0.8681411743164062
I0306 13:44:34.085155 139787361040128 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.224113464355469, loss=0.7656886577606201
I0306 13:45:07.705746 139789013600000 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.9933557510375977, loss=0.8424355387687683
I0306 13:45:41.365486 139787361040128 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.959409236907959, loss=0.8512100577354431
I0306 13:46:14.981617 139789013600000 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.0160231590271, loss=0.8246971964836121
I0306 13:46:29.941772 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:46:35.926132 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:46:44.758608 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:46:47.366191 139951089751872 submission_runner.py:411] Time since start: 54010.58s, 	Step: 154646, 	{'train/accuracy': 0.9080237150192261, 'train/loss': 0.30529657006263733, 'validation/accuracy': 0.7430599927902222, 'validation/loss': 1.0862489938735962, 'validation/num_examples': 50000, 'test/accuracy': 0.6117000579833984, 'test/loss': 1.857428789138794, 'test/num_examples': 10000, 'score': 52077.77292633057, 'total_duration': 54010.57850217819, 'accumulated_submission_time': 52077.77292633057, 'accumulated_eval_time': 1921.3516061306, 'accumulated_logging_time': 5.878023862838745}
I0306 13:46:47.408469 139789005207296 logging_writer.py:48] [154646] accumulated_eval_time=1921.351606, accumulated_logging_time=5.878024, accumulated_submission_time=52077.772926, global_step=154646, preemption_count=0, score=52077.772926, test/accuracy=0.611700, test/loss=1.857429, test/num_examples=10000, total_duration=54010.578502, train/accuracy=0.908024, train/loss=0.305297, validation/accuracy=0.743060, validation/loss=1.086249, validation/num_examples=50000
I0306 13:47:05.920879 139789021992704 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.648589611053467, loss=0.7630457878112793
I0306 13:47:39.545131 139789005207296 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.8515381813049316, loss=0.6908789873123169
I0306 13:48:13.184124 139789021992704 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.31777286529541, loss=0.7780122756958008
I0306 13:48:46.841893 139789005207296 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.320065021514893, loss=0.8065794706344604
I0306 13:49:20.473891 139789021992704 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.5042948722839355, loss=0.8469467163085938
I0306 13:49:54.106592 139789005207296 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.129917621612549, loss=0.7838650941848755
I0306 13:50:27.733519 139789021992704 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.514237880706787, loss=0.8798127174377441
I0306 13:51:01.383724 139789005207296 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.141041278839111, loss=0.8304486870765686
I0306 13:51:35.016678 139789021992704 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.337401866912842, loss=0.826685905456543
I0306 13:52:08.666596 139789005207296 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.116731643676758, loss=0.820269763469696
I0306 13:52:42.314695 139789021992704 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.9429421424865723, loss=0.8417723774909973
I0306 13:53:16.006356 139789005207296 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.9688315391540527, loss=0.7487130165100098
I0306 13:53:49.659255 139789021992704 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.9306647777557373, loss=0.793301522731781
I0306 13:54:23.277087 139789005207296 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.959106206893921, loss=0.7766211032867432
I0306 13:54:56.931988 139789021992704 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.045374393463135, loss=0.7944954633712769
I0306 13:55:17.620240 139951089751872 spec.py:321] Evaluating on the training split.
I0306 13:55:23.727511 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 13:55:32.408019 139951089751872 spec.py:349] Evaluating on the test split.
I0306 13:55:34.791758 139951089751872 submission_runner.py:411] Time since start: 54538.00s, 	Step: 156163, 	{'train/accuracy': 0.9146803021430969, 'train/loss': 0.29021114110946655, 'validation/accuracy': 0.744879961013794, 'validation/loss': 1.0719773769378662, 'validation/num_examples': 50000, 'test/accuracy': 0.616100013256073, 'test/loss': 1.843456506729126, 'test/num_examples': 10000, 'score': 52587.91961193085, 'total_duration': 54538.00402379036, 'accumulated_submission_time': 52587.91961193085, 'accumulated_eval_time': 1938.523027420044, 'accumulated_logging_time': 5.9307475090026855}
I0306 13:55:34.854481 139787361040128 logging_writer.py:48] [156163] accumulated_eval_time=1938.523027, accumulated_logging_time=5.930748, accumulated_submission_time=52587.919612, global_step=156163, preemption_count=0, score=52587.919612, test/accuracy=0.616100, test/loss=1.843457, test/num_examples=10000, total_duration=54538.004024, train/accuracy=0.914680, train/loss=0.290211, validation/accuracy=0.744880, validation/loss=1.071977, validation/num_examples=50000
I0306 13:55:47.598447 139787562383104 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.220272064208984, loss=0.7565430998802185
I0306 13:56:21.168057 139787361040128 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.293044090270996, loss=0.7474015355110168
I0306 13:56:54.818855 139787562383104 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.203855037689209, loss=0.7703864574432373
I0306 13:57:28.443971 139787361040128 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.519019603729248, loss=0.8966928720474243
I0306 13:58:02.094482 139787562383104 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.664299964904785, loss=0.8217508792877197
I0306 13:58:35.700889 139787361040128 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.751399517059326, loss=0.8385350704193115
I0306 13:59:09.389701 139787562383104 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.475699424743652, loss=0.8950538039207458
I0306 13:59:42.984089 139787361040128 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.105654716491699, loss=0.7467657923698425
I0306 14:00:16.625014 139787562383104 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.634912967681885, loss=0.8144446611404419
I0306 14:00:50.241155 139787361040128 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.200134754180908, loss=0.7557551860809326
I0306 14:01:23.916301 139787562383104 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.626745700836182, loss=0.9069505929946899
I0306 14:01:57.544221 139787361040128 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.071047782897949, loss=0.771804928779602
I0306 14:02:31.206474 139787562383104 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.644692897796631, loss=0.8377178311347961
I0306 14:03:04.852753 139787361040128 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.223487377166748, loss=0.8333271145820618
I0306 14:03:38.529627 139787562383104 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.030279159545898, loss=0.7244792580604553
I0306 14:04:04.922442 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:04:11.079927 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:04:19.928943 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:04:22.203240 139951089751872 submission_runner.py:411] Time since start: 55065.42s, 	Step: 157680, 	{'train/accuracy': 0.9210578799247742, 'train/loss': 0.27307939529418945, 'validation/accuracy': 0.7454400062561035, 'validation/loss': 1.0773038864135742, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.838584065437317, 'test/num_examples': 10000, 'score': 53097.921402692795, 'total_duration': 55065.41555118561, 'accumulated_submission_time': 53097.921402692795, 'accumulated_eval_time': 1955.8037884235382, 'accumulated_logging_time': 6.003688812255859}
I0306 14:04:22.246562 139789013600000 logging_writer.py:48] [157680] accumulated_eval_time=1955.803788, accumulated_logging_time=6.003689, accumulated_submission_time=53097.921403, global_step=157680, preemption_count=0, score=53097.921403, test/accuracy=0.619100, test/loss=1.838584, test/num_examples=10000, total_duration=55065.415551, train/accuracy=0.921058, train/loss=0.273079, validation/accuracy=0.745440, validation/loss=1.077304, validation/num_examples=50000
I0306 14:04:29.322187 139789021992704 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.290791034698486, loss=0.7209654450416565
I0306 14:05:02.976299 139789013600000 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.408308506011963, loss=0.9140334725379944
I0306 14:05:36.654100 139789021992704 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.571987628936768, loss=0.8390320539474487
I0306 14:06:10.275893 139789013600000 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.270369529724121, loss=0.7510031461715698
I0306 14:06:43.916483 139789021992704 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.139189720153809, loss=0.7626195549964905
I0306 14:07:17.547813 139789013600000 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.399022579193115, loss=0.8156285285949707
I0306 14:07:51.193569 139789021992704 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.226709365844727, loss=0.7501358389854431
I0306 14:08:24.834880 139789013600000 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.7224202156066895, loss=0.870632529258728
I0306 14:08:58.500219 139789021992704 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.12908411026001, loss=0.7555203437805176
I0306 14:09:32.130197 139789013600000 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.5585408210754395, loss=0.8315988183021545
I0306 14:10:05.743319 139789021992704 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.368183135986328, loss=0.7670911550521851
I0306 14:10:39.336519 139789013600000 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.026851654052734, loss=0.7462471723556519
I0306 14:11:12.995940 139789021992704 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.2099738121032715, loss=0.7621021866798401
I0306 14:11:46.600295 139789013600000 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.9387145042419434, loss=0.7427141666412354
I0306 14:12:20.247495 139789021992704 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.420048236846924, loss=0.7907353639602661
I0306 14:12:52.339298 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:12:58.338047 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:13:07.371669 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:13:09.580399 139951089751872 submission_runner.py:411] Time since start: 55592.79s, 	Step: 159197, 	{'train/accuracy': 0.9376793503761292, 'train/loss': 0.22130529582500458, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.0691070556640625, 'validation/num_examples': 50000, 'test/accuracy': 0.6219000220298767, 'test/loss': 1.8522708415985107, 'test/num_examples': 10000, 'score': 53607.94852924347, 'total_duration': 55592.79269909859, 'accumulated_submission_time': 53607.94852924347, 'accumulated_eval_time': 1973.0448276996613, 'accumulated_logging_time': 6.057266712188721}
I0306 14:13:09.628183 139787369432832 logging_writer.py:48] [159197] accumulated_eval_time=1973.044828, accumulated_logging_time=6.057267, accumulated_submission_time=53607.948529, global_step=159197, preemption_count=0, score=53607.948529, test/accuracy=0.621900, test/loss=1.852271, test/num_examples=10000, total_duration=55592.792699, train/accuracy=0.937679, train/loss=0.221305, validation/accuracy=0.747060, validation/loss=1.069107, validation/num_examples=50000
I0306 14:13:10.967331 139787562383104 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.9761016368865967, loss=0.7783371210098267
I0306 14:13:44.641216 139787369432832 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.487054824829102, loss=0.8829272389411926
I0306 14:14:18.314839 139787562383104 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.545840740203857, loss=0.7620157599449158
I0306 14:14:51.996300 139787369432832 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.438836574554443, loss=0.8273859024047852
I0306 14:15:25.623780 139787562383104 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.45449161529541, loss=0.8043437600135803
I0306 14:15:59.287052 139787369432832 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.419694423675537, loss=0.7196052074432373
I0306 14:16:32.908912 139787562383104 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.161487579345703, loss=0.7400329113006592
I0306 14:17:06.555577 139787369432832 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.380301475524902, loss=0.7170937061309814
I0306 14:17:40.318213 139787562383104 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.858769178390503, loss=0.6780715584754944
I0306 14:18:13.974501 139787369432832 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.673785209655762, loss=0.7463828921318054
I0306 14:18:47.598991 139787562383104 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.803611755371094, loss=0.7171489596366882
I0306 14:19:21.262670 139787369432832 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.308850288391113, loss=0.7602238059043884
I0306 14:19:54.927385 139787562383104 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.399336814880371, loss=0.7894997596740723
I0306 14:20:28.555437 139787369432832 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.02504301071167, loss=0.7201964855194092
I0306 14:21:02.240065 139787562383104 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.292219638824463, loss=0.8016600608825684
I0306 14:21:35.908106 139787369432832 logging_writer.py:48] [160700] global_step=160700, grad_norm=3.6927003860473633, loss=0.6638039350509644
I0306 14:21:39.747784 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:21:45.740087 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:21:54.681566 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:21:56.911974 139951089751872 submission_runner.py:411] Time since start: 56120.12s, 	Step: 160713, 	{'train/accuracy': 0.9351084232330322, 'train/loss': 0.22827468812465668, 'validation/accuracy': 0.747439980506897, 'validation/loss': 1.0624006986618042, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.8397870063781738, 'test/num_examples': 10000, 'score': 54118.00081586838, 'total_duration': 56120.12428307533, 'accumulated_submission_time': 54118.00081586838, 'accumulated_eval_time': 1990.2089619636536, 'accumulated_logging_time': 6.115020275115967}
I0306 14:21:56.958559 139787369432832 logging_writer.py:48] [160713] accumulated_eval_time=1990.208962, accumulated_logging_time=6.115020, accumulated_submission_time=54118.000816, global_step=160713, preemption_count=0, score=54118.000816, test/accuracy=0.620600, test/loss=1.839787, test/num_examples=10000, total_duration=56120.124283, train/accuracy=0.935108, train/loss=0.228275, validation/accuracy=0.747440, validation/loss=1.062401, validation/num_examples=50000
I0306 14:22:26.498813 139787562383104 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.325589656829834, loss=0.8164681196212769
I0306 14:23:00.042522 139787369432832 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.281222343444824, loss=0.7219722270965576
I0306 14:23:33.733487 139787562383104 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.057490825653076, loss=0.7354360818862915
I0306 14:24:07.398289 139787369432832 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.84805965423584, loss=0.8501814007759094
I0306 14:24:41.085150 139787562383104 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.189350128173828, loss=0.7001674771308899
I0306 14:25:14.723339 139787369432832 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.99344801902771, loss=0.7301057577133179
I0306 14:25:48.366885 139787562383104 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.11446475982666, loss=0.7602694034576416
I0306 14:26:21.988121 139787369432832 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.168142795562744, loss=0.7942252159118652
I0306 14:26:55.629113 139787562383104 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.270177841186523, loss=0.8099344968795776
I0306 14:27:29.271468 139787369432832 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.2898969650268555, loss=0.7881285548210144
I0306 14:28:02.923057 139787562383104 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.020923614501953, loss=0.7206924557685852
I0306 14:28:36.533823 139787369432832 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.887694835662842, loss=0.8541203737258911
I0306 14:29:10.189645 139787562383104 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.037782192230225, loss=0.7648396492004395
I0306 14:29:43.857762 139787369432832 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.323625564575195, loss=0.734613835811615
I0306 14:30:17.447818 139787562383104 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.719619274139404, loss=0.84261554479599
I0306 14:30:26.992706 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:30:32.993977 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:30:41.989650 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:30:44.242311 139951089751872 submission_runner.py:411] Time since start: 56647.45s, 	Step: 162230, 	{'train/accuracy': 0.9322983026504517, 'train/loss': 0.23329292237758636, 'validation/accuracy': 0.7475199699401855, 'validation/loss': 1.0625967979431152, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.8488606214523315, 'test/num_examples': 10000, 'score': 54627.96944499016, 'total_duration': 56647.45462155342, 'accumulated_submission_time': 54627.96944499016, 'accumulated_eval_time': 2007.458515882492, 'accumulated_logging_time': 6.172126054763794}
I0306 14:30:44.286778 139787361040128 logging_writer.py:48] [162230] accumulated_eval_time=2007.458516, accumulated_logging_time=6.172126, accumulated_submission_time=54627.969445, global_step=162230, preemption_count=0, score=54627.969445, test/accuracy=0.620300, test/loss=1.848861, test/num_examples=10000, total_duration=56647.454622, train/accuracy=0.932298, train/loss=0.233293, validation/accuracy=0.747520, validation/loss=1.062597, validation/num_examples=50000
I0306 14:31:08.146740 139787369432832 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.260111331939697, loss=0.7404227256774902
I0306 14:31:41.787723 139787361040128 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.6938700675964355, loss=0.7366117238998413
I0306 14:32:15.451720 139787369432832 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.437682628631592, loss=0.862080454826355
I0306 14:32:49.107519 139787361040128 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.285313606262207, loss=0.6860484480857849
I0306 14:33:22.742204 139787369432832 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.498309135437012, loss=0.7260392904281616
I0306 14:33:56.382754 139787361040128 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.632325172424316, loss=0.7567307353019714
I0306 14:34:29.993303 139787369432832 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.258388042449951, loss=0.7750784754753113
I0306 14:35:03.660151 139787361040128 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.184141635894775, loss=0.699019193649292
I0306 14:35:37.447810 139787369432832 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.520359039306641, loss=0.7476825714111328
I0306 14:36:11.107437 139787361040128 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.761111259460449, loss=0.7337018847465515
I0306 14:36:44.752203 139787369432832 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.559817314147949, loss=0.7543811202049255
I0306 14:37:18.425325 139787361040128 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.7345170974731445, loss=0.7709630131721497
I0306 14:37:52.076602 139787369432832 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.911900997161865, loss=0.7510713338851929
I0306 14:38:25.716357 139787361040128 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.601016998291016, loss=0.8142113089561462
I0306 14:38:59.339548 139787369432832 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.842850923538208, loss=0.6623308658599854
I0306 14:39:14.295146 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:39:20.387216 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:39:29.064937 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:39:31.351973 139951089751872 submission_runner.py:411] Time since start: 57174.56s, 	Step: 163746, 	{'train/accuracy': 0.9357860088348389, 'train/loss': 0.22098106145858765, 'validation/accuracy': 0.7492600083351135, 'validation/loss': 1.0612397193908691, 'validation/num_examples': 50000, 'test/accuracy': 0.6248000264167786, 'test/loss': 1.8343461751937866, 'test/num_examples': 10000, 'score': 55137.909828186035, 'total_duration': 57174.564274311066, 'accumulated_submission_time': 55137.909828186035, 'accumulated_eval_time': 2024.5153052806854, 'accumulated_logging_time': 6.228562831878662}
I0306 14:39:31.394107 139787369432832 logging_writer.py:48] [163746] accumulated_eval_time=2024.515305, accumulated_logging_time=6.228563, accumulated_submission_time=55137.909828, global_step=163746, preemption_count=0, score=55137.909828, test/accuracy=0.624800, test/loss=1.834346, test/num_examples=10000, total_duration=57174.564274, train/accuracy=0.935786, train/loss=0.220981, validation/accuracy=0.749260, validation/loss=1.061240, validation/num_examples=50000
I0306 14:39:49.851088 139789013600000 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.606236457824707, loss=0.7195723056793213
I0306 14:40:23.415732 139787369432832 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.8448545932769775, loss=0.5683627128601074
I0306 14:40:56.998306 139789013600000 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.180814266204834, loss=0.6815455555915833
I0306 14:41:30.644947 139787369432832 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.355939865112305, loss=0.7362903952598572
I0306 14:42:04.353724 139789013600000 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.492430210113525, loss=0.6402412056922913
I0306 14:42:38.069878 139787369432832 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.288255214691162, loss=0.7111819982528687
I0306 14:43:11.663057 139789013600000 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.089719295501709, loss=0.7473372220993042
I0306 14:43:45.313646 139787369432832 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.231624603271484, loss=0.7040778398513794
I0306 14:44:18.932912 139789013600000 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.089806079864502, loss=0.6820712089538574
I0306 14:44:52.581136 139787369432832 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.579049110412598, loss=0.721477746963501
I0306 14:45:26.200379 139789013600000 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.4711761474609375, loss=0.7952321171760559
I0306 14:45:59.856345 139787369432832 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.595986843109131, loss=0.6487253904342651
I0306 14:46:33.500315 139789013600000 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.040143013000488, loss=0.6431089043617249
I0306 14:47:07.138606 139787369432832 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.437164783477783, loss=0.6940171122550964
I0306 14:47:40.768619 139789013600000 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.56356143951416, loss=0.7167181968688965
I0306 14:48:01.471709 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:48:07.562639 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:48:16.259744 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:48:18.490563 139951089751872 submission_runner.py:411] Time since start: 57701.70s, 	Step: 165263, 	{'train/accuracy': 0.9393334984779358, 'train/loss': 0.21111615002155304, 'validation/accuracy': 0.7497999668121338, 'validation/loss': 1.0586296319961548, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.8307875394821167, 'test/num_examples': 10000, 'score': 55647.922043800354, 'total_duration': 57701.702865600586, 'accumulated_submission_time': 55647.922043800354, 'accumulated_eval_time': 2041.534103155136, 'accumulated_logging_time': 6.281407117843628}
I0306 14:48:18.536929 139788996814592 logging_writer.py:48] [165263] accumulated_eval_time=2041.534103, accumulated_logging_time=6.281407, accumulated_submission_time=55647.922044, global_step=165263, preemption_count=0, score=55647.922044, test/accuracy=0.624900, test/loss=1.830788, test/num_examples=10000, total_duration=57701.702866, train/accuracy=0.939333, train/loss=0.211116, validation/accuracy=0.749800, validation/loss=1.058630, validation/num_examples=50000
I0306 14:48:31.325952 139789005207296 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.164350509643555, loss=0.6722708940505981
I0306 14:49:04.926868 139788996814592 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.162836074829102, loss=0.6876708269119263
I0306 14:49:38.578653 139789005207296 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.608104705810547, loss=0.7473461627960205
I0306 14:50:12.199255 139788996814592 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.509157657623291, loss=0.712212085723877
I0306 14:50:45.841664 139789005207296 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.815654277801514, loss=0.7923644781112671
I0306 14:51:19.454251 139788996814592 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.534177303314209, loss=0.7172428369522095
I0306 14:51:53.110522 139789005207296 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.189958095550537, loss=0.6194655895233154
I0306 14:52:26.735317 139788996814592 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.138401985168457, loss=0.7644286751747131
I0306 14:53:00.406122 139789005207296 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.697748184204102, loss=0.7918578386306763
I0306 14:53:34.022475 139788996814592 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.388156890869141, loss=0.6709377765655518
I0306 14:54:07.709328 139789005207296 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.851522922515869, loss=0.7215663194656372
I0306 14:54:41.307944 139788996814592 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.021765232086182, loss=0.6102946400642395
I0306 14:55:14.920654 139789005207296 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.462896823883057, loss=0.6696515679359436
I0306 14:55:48.592787 139788996814592 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.673603534698486, loss=0.7526407241821289
I0306 14:56:22.239188 139789005207296 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.182372093200684, loss=0.6708332896232605
I0306 14:56:48.588050 139951089751872 spec.py:321] Evaluating on the training split.
I0306 14:56:54.615220 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 14:57:03.309303 139951089751872 spec.py:349] Evaluating on the test split.
I0306 14:57:05.575017 139951089751872 submission_runner.py:411] Time since start: 58228.79s, 	Step: 166780, 	{'train/accuracy': 0.9436383843421936, 'train/loss': 0.20118504762649536, 'validation/accuracy': 0.751800000667572, 'validation/loss': 1.0534090995788574, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8349944353103638, 'test/num_examples': 10000, 'score': 56157.90705728531, 'total_duration': 58228.787316560745, 'accumulated_submission_time': 56157.90705728531, 'accumulated_eval_time': 2058.5210251808167, 'accumulated_logging_time': 6.337911128997803}
I0306 14:57:05.620643 139787369432832 logging_writer.py:48] [166780] accumulated_eval_time=2058.521025, accumulated_logging_time=6.337911, accumulated_submission_time=56157.907057, global_step=166780, preemption_count=0, score=56157.907057, test/accuracy=0.629000, test/loss=1.834994, test/num_examples=10000, total_duration=58228.787317, train/accuracy=0.943638, train/loss=0.201185, validation/accuracy=0.751800, validation/loss=1.053409, validation/num_examples=50000
I0306 14:57:12.662038 139787562383104 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.223649501800537, loss=0.6587406992912292
I0306 14:57:46.249643 139787369432832 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.448538780212402, loss=0.7194628715515137
I0306 14:58:19.905695 139787562383104 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.152710437774658, loss=0.6811680197715759
I0306 14:58:53.533514 139787369432832 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.788288593292236, loss=0.7361662983894348
I0306 14:59:27.194613 139787562383104 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.096818447113037, loss=0.6317853331565857
I0306 15:00:00.853633 139787369432832 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.16096830368042, loss=0.6887995600700378
I0306 15:00:34.435820 139787562383104 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.36826229095459, loss=0.7100090980529785
I0306 15:01:08.022885 139787369432832 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.286730766296387, loss=0.6915209293365479
I0306 15:01:41.668082 139787562383104 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.299708843231201, loss=0.6951064467430115
I0306 15:02:15.285129 139787369432832 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.9834518432617188, loss=0.6471009254455566
I0306 15:02:48.932905 139787562383104 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.478326797485352, loss=0.6386499404907227
I0306 15:03:22.550257 139787369432832 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.21051025390625, loss=0.7052631974220276
I0306 15:03:56.205947 139787562383104 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.4661126136779785, loss=0.7404465675354004
I0306 15:04:29.828198 139787369432832 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.865055561065674, loss=0.6879889369010925
I0306 15:05:03.473685 139787562383104 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.680960655212402, loss=0.7239314317703247
I0306 15:05:35.895290 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:05:41.986186 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:05:50.762337 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:05:53.068629 139951089751872 submission_runner.py:411] Time since start: 58756.28s, 	Step: 168298, 	{'train/accuracy': 0.9491389989852905, 'train/loss': 0.17798037827014923, 'validation/accuracy': 0.752079963684082, 'validation/loss': 1.0596641302108765, 'validation/num_examples': 50000, 'test/accuracy': 0.6277000308036804, 'test/loss': 1.8354393243789673, 'test/num_examples': 10000, 'score': 56668.114616155624, 'total_duration': 58756.28093838692, 'accumulated_submission_time': 56668.114616155624, 'accumulated_eval_time': 2075.694316625595, 'accumulated_logging_time': 6.394445896148682}
I0306 15:05:53.118567 139789005207296 logging_writer.py:48] [168298] accumulated_eval_time=2075.694317, accumulated_logging_time=6.394446, accumulated_submission_time=56668.114616, global_step=168298, preemption_count=0, score=56668.114616, test/accuracy=0.627700, test/loss=1.835439, test/num_examples=10000, total_duration=58756.280938, train/accuracy=0.949139, train/loss=0.177980, validation/accuracy=0.752080, validation/loss=1.059664, validation/num_examples=50000
I0306 15:05:54.142068 139789021992704 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.340155601501465, loss=0.6514183282852173
I0306 15:06:27.831518 139789005207296 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.802175521850586, loss=0.7340832948684692
I0306 15:07:01.463912 139789021992704 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.329780578613281, loss=0.693673849105835
I0306 15:07:35.126873 139789005207296 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.333589553833008, loss=0.6639464497566223
I0306 15:08:08.780913 139789021992704 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.790181636810303, loss=0.5889176726341248
I0306 15:08:42.440894 139789005207296 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.300069332122803, loss=0.694989800453186
I0306 15:09:16.098116 139789021992704 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.538064002990723, loss=0.7371376156806946
I0306 15:09:49.761938 139789005207296 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.043064117431641, loss=0.6471887230873108
I0306 15:10:23.379200 139789021992704 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.35737419128418, loss=0.6212223172187805
I0306 15:10:57.040339 139789005207296 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.754946231842041, loss=0.7751783728599548
I0306 15:11:30.678724 139789021992704 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.486801624298096, loss=0.6596627831459045
I0306 15:12:04.325289 139789005207296 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.174869060516357, loss=0.7083407640457153
I0306 15:12:37.967156 139789021992704 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.001976490020752, loss=0.6559925079345703
I0306 15:13:11.567655 139789005207296 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.449947834014893, loss=0.6815810203552246
I0306 15:13:45.179438 139789021992704 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.53085470199585, loss=0.7895525097846985
I0306 15:14:18.811038 139789005207296 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.5425848960876465, loss=0.7060091495513916
I0306 15:14:23.341207 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:14:29.353508 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:14:38.277019 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:14:40.559918 139951089751872 submission_runner.py:411] Time since start: 59283.77s, 	Step: 169815, 	{'train/accuracy': 0.950215220451355, 'train/loss': 0.17708361148834229, 'validation/accuracy': 0.7531399726867676, 'validation/loss': 1.052578091621399, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8301844596862793, 'test/num_examples': 10000, 'score': 57178.27260637283, 'total_duration': 59283.77220726013, 'accumulated_submission_time': 57178.27260637283, 'accumulated_eval_time': 2092.9129543304443, 'accumulated_logging_time': 6.455153465270996}
I0306 15:14:40.605510 139787369432832 logging_writer.py:48] [169815] accumulated_eval_time=2092.912954, accumulated_logging_time=6.455153, accumulated_submission_time=57178.272606, global_step=169815, preemption_count=0, score=57178.272606, test/accuracy=0.627400, test/loss=1.830184, test/num_examples=10000, total_duration=59283.772207, train/accuracy=0.950215, train/loss=0.177084, validation/accuracy=0.753140, validation/loss=1.052578, validation/num_examples=50000
I0306 15:15:09.493950 139787562383104 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.613107681274414, loss=0.6555808782577515
I0306 15:15:43.097687 139787369432832 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.655808448791504, loss=0.6869388818740845
I0306 15:16:16.754711 139787562383104 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.574489593505859, loss=0.696824312210083
I0306 15:16:50.351849 139787369432832 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.514096736907959, loss=0.6770509481430054
I0306 15:17:24.021743 139787562383104 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.315632343292236, loss=0.655164897441864
I0306 15:17:57.623005 139787369432832 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.692208766937256, loss=0.7225574254989624
I0306 15:18:31.307109 139787562383104 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.111306667327881, loss=0.6949795484542847
I0306 15:19:04.876032 139787369432832 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.598660469055176, loss=0.6923828125
I0306 15:19:38.461123 139787562383104 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.599649429321289, loss=0.68528151512146
I0306 15:20:12.015666 139787369432832 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.3922271728515625, loss=0.6630587577819824
I0306 15:20:45.676125 139787562383104 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.712545871734619, loss=0.737141489982605
I0306 15:21:19.331781 139787369432832 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.062395095825195, loss=0.6443525552749634
I0306 15:21:52.985796 139787562383104 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.308781623840332, loss=0.6329838037490845
I0306 15:22:26.597954 139787369432832 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.646877288818359, loss=0.6897554993629456
I0306 15:23:00.243520 139787562383104 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.409356117248535, loss=0.6529070734977722
I0306 15:23:10.828511 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:23:16.834144 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:23:25.645164 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:23:27.926744 139951089751872 submission_runner.py:411] Time since start: 59811.14s, 	Step: 171333, 	{'train/accuracy': 0.9529256820678711, 'train/loss': 0.17174378037452698, 'validation/accuracy': 0.7522000074386597, 'validation/loss': 1.0531929731369019, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8310190439224243, 'test/num_examples': 10000, 'score': 57688.4286673069, 'total_duration': 59811.13905739784, 'accumulated_submission_time': 57688.4286673069, 'accumulated_eval_time': 2110.01118683815, 'accumulated_logging_time': 6.511188507080078}
I0306 15:23:27.974548 139787369432832 logging_writer.py:48] [171333] accumulated_eval_time=2110.011187, accumulated_logging_time=6.511189, accumulated_submission_time=57688.428667, global_step=171333, preemption_count=0, score=57688.428667, test/accuracy=0.627500, test/loss=1.831019, test/num_examples=10000, total_duration=59811.139057, train/accuracy=0.952926, train/loss=0.171744, validation/accuracy=0.752200, validation/loss=1.053193, validation/num_examples=50000
I0306 15:23:50.852533 139789005207296 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.746054649353027, loss=0.6813715100288391
I0306 15:24:24.493261 139787369432832 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.604828834533691, loss=0.704487144947052
I0306 15:24:58.133018 139789005207296 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.230567455291748, loss=0.5889787673950195
I0306 15:25:31.734024 139787369432832 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.435445308685303, loss=0.685234785079956
I0306 15:26:05.351424 139789005207296 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.521946430206299, loss=0.6959601044654846
I0306 15:26:39.020584 139787369432832 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.32188081741333, loss=0.6520504951477051
I0306 15:27:12.642112 139789005207296 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.651143550872803, loss=0.6966261863708496
I0306 15:27:46.303049 139787369432832 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.440347194671631, loss=0.5966061353683472
I0306 15:28:19.959424 139789005207296 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.597376346588135, loss=0.6372089385986328
I0306 15:28:53.610645 139787369432832 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.570616245269775, loss=0.6721635460853577
I0306 15:29:27.236929 139789005207296 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.893979549407959, loss=0.7100486755371094
I0306 15:30:00.879209 139787369432832 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.277021408081055, loss=0.6072971820831299
I0306 15:30:34.548677 139789005207296 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.835946083068848, loss=0.7050373554229736
I0306 15:31:08.158766 139787369432832 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.530682563781738, loss=0.7077723145484924
I0306 15:31:41.786791 139789005207296 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.613267421722412, loss=0.7066237926483154
I0306 15:31:58.071932 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:32:04.150713 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:32:12.886786 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:32:15.161608 139951089751872 submission_runner.py:411] Time since start: 60338.37s, 	Step: 172850, 	{'train/accuracy': 0.9522879123687744, 'train/loss': 0.17572997510433197, 'validation/accuracy': 0.7527799606323242, 'validation/loss': 1.0505002737045288, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8338404893875122, 'test/num_examples': 10000, 'score': 58198.459025382996, 'total_duration': 60338.37391376495, 'accumulated_submission_time': 58198.459025382996, 'accumulated_eval_time': 2127.100840330124, 'accumulated_logging_time': 6.570757627487183}
I0306 15:32:15.211130 139787361040128 logging_writer.py:48] [172850] accumulated_eval_time=2127.100840, accumulated_logging_time=6.570758, accumulated_submission_time=58198.459025, global_step=172850, preemption_count=0, score=58198.459025, test/accuracy=0.627100, test/loss=1.833840, test/num_examples=10000, total_duration=60338.373914, train/accuracy=0.952288, train/loss=0.175730, validation/accuracy=0.752780, validation/loss=1.050500, validation/num_examples=50000
I0306 15:32:32.351367 139787369432832 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.502607822418213, loss=0.6920658946037292
I0306 15:33:06.002501 139787361040128 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.530186653137207, loss=0.6551142930984497
I0306 15:33:39.614176 139787369432832 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.887266159057617, loss=0.7785748243331909
I0306 15:34:13.273571 139787361040128 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.232407569885254, loss=0.6397925019264221
I0306 15:34:46.929924 139787369432832 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.355343341827393, loss=0.6306074261665344
I0306 15:35:20.583486 139787361040128 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.529667377471924, loss=0.6255230903625488
I0306 15:35:54.194845 139787369432832 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.9691877365112305, loss=0.7019169330596924
I0306 15:36:27.839515 139787361040128 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.269891262054443, loss=0.6326978802680969
I0306 15:37:01.486911 139787369432832 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.057598114013672, loss=0.6169460415840149
I0306 15:37:35.103533 139787361040128 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.2811713218688965, loss=0.5988293290138245
I0306 15:38:08.717087 139787369432832 logging_writer.py:48] [173900] global_step=173900, grad_norm=5.195074081420898, loss=0.761487603187561
I0306 15:38:42.279148 139787361040128 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.047781944274902, loss=0.6142768859863281
I0306 15:39:15.833347 139787369432832 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.302829742431641, loss=0.6206827163696289
I0306 15:39:49.473444 139787361040128 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.426362037658691, loss=0.6046608686447144
I0306 15:40:23.101156 139787369432832 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.407124042510986, loss=0.6194567680358887
I0306 15:40:45.479752 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:40:51.514666 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:41:00.179826 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:41:02.485655 139951089751872 submission_runner.py:411] Time since start: 60865.70s, 	Step: 174368, 	{'train/accuracy': 0.9563336968421936, 'train/loss': 0.16504782438278198, 'validation/accuracy': 0.7552399635314941, 'validation/loss': 1.0452936887741089, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8263959884643555, 'test/num_examples': 10000, 'score': 58708.66118121147, 'total_duration': 60865.69796657562, 'accumulated_submission_time': 58708.66118121147, 'accumulated_eval_time': 2144.1066920757294, 'accumulated_logging_time': 6.630687713623047}
I0306 15:41:02.533290 139787361040128 logging_writer.py:48] [174368] accumulated_eval_time=2144.106692, accumulated_logging_time=6.630688, accumulated_submission_time=58708.661181, global_step=174368, preemption_count=0, score=58708.661181, test/accuracy=0.629000, test/loss=1.826396, test/num_examples=10000, total_duration=60865.697967, train/accuracy=0.956334, train/loss=0.165048, validation/accuracy=0.755240, validation/loss=1.045294, validation/num_examples=50000
I0306 15:41:13.607343 139787369432832 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.273116588592529, loss=0.6211971044540405
I0306 15:41:47.175143 139787361040128 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.557989120483398, loss=0.6124234199523926
I0306 15:42:20.755535 139787369432832 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.484363079071045, loss=0.693909227848053
I0306 15:42:54.411230 139787361040128 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.745513916015625, loss=0.7444570064544678
I0306 15:43:28.015633 139787369432832 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.28679895401001, loss=0.63053297996521
I0306 15:44:01.656378 139787361040128 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.138859272003174, loss=0.6362399458885193
I0306 15:44:35.294808 139787369432832 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.234570503234863, loss=0.6808339953422546
I0306 15:45:08.930469 139787361040128 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.2600908279418945, loss=0.6152204275131226
I0306 15:45:42.563787 139787369432832 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.101162910461426, loss=0.6814678311347961
I0306 15:46:16.191830 139787361040128 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.648719310760498, loss=0.6501429677009583
I0306 15:46:49.815975 139787369432832 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.239543914794922, loss=0.6616830825805664
I0306 15:47:23.456732 139787361040128 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.240112781524658, loss=0.6465181112289429
I0306 15:47:57.108311 139787369432832 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.559476375579834, loss=0.7073453664779663
I0306 15:48:30.765694 139787361040128 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.711307525634766, loss=0.7132271528244019
I0306 15:49:04.424495 139787369432832 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.4050679206848145, loss=0.673221230506897
I0306 15:49:32.796468 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:49:38.847376 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:49:47.631795 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:49:49.904546 139951089751872 submission_runner.py:411] Time since start: 61393.12s, 	Step: 175886, 	{'train/accuracy': 0.9587850570678711, 'train/loss': 0.15112841129302979, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0456360578536987, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.824371576309204, 'test/num_examples': 10000, 'score': 59218.857328653336, 'total_duration': 61393.116857767105, 'accumulated_submission_time': 59218.857328653336, 'accumulated_eval_time': 2161.2147500514984, 'accumulated_logging_time': 6.689452409744263}
I0306 15:49:49.953749 139789030385408 logging_writer.py:48] [175886] accumulated_eval_time=2161.214750, accumulated_logging_time=6.689452, accumulated_submission_time=59218.857329, global_step=175886, preemption_count=0, score=59218.857329, test/accuracy=0.631600, test/loss=1.824372, test/num_examples=10000, total_duration=61393.116858, train/accuracy=0.958785, train/loss=0.151128, validation/accuracy=0.754960, validation/loss=1.045636, validation/num_examples=50000
I0306 15:49:55.001471 139789038778112 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.766068458557129, loss=0.7104997038841248
I0306 15:50:28.598554 139789030385408 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.757514953613281, loss=0.7074370980262756
I0306 15:51:02.165072 139789038778112 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.858423233032227, loss=0.6368308663368225
I0306 15:51:35.761463 139789030385408 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.403669357299805, loss=0.5848618149757385
I0306 15:52:09.386504 139789038778112 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.066736698150635, loss=0.6552053689956665
I0306 15:52:42.991841 139789030385408 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.895312309265137, loss=0.6631379127502441
I0306 15:53:16.641226 139789038778112 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.508110046386719, loss=0.674872636795044
I0306 15:53:50.262221 139789030385408 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.852585792541504, loss=0.6185457706451416
I0306 15:54:23.919342 139789038778112 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.577991485595703, loss=0.6649206280708313
I0306 15:54:57.547796 139789030385408 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.375728130340576, loss=0.6755333542823792
I0306 15:55:31.196763 139789038778112 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.58548641204834, loss=0.7598065733909607
I0306 15:56:04.790857 139789030385408 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.216256618499756, loss=0.622822642326355
I0306 15:56:38.350176 139789038778112 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.150231838226318, loss=0.5859125256538391
I0306 15:57:11.948297 139789030385408 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.864552974700928, loss=0.6448671817779541
I0306 15:57:45.617225 139789038778112 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.45040225982666, loss=0.6211497187614441
I0306 15:58:19.239555 139789030385408 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.873785972595215, loss=0.6843626499176025
I0306 15:58:20.060878 139951089751872 spec.py:321] Evaluating on the training split.
I0306 15:58:26.059918 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 15:58:34.746052 139951089751872 spec.py:349] Evaluating on the test split.
I0306 15:58:37.004790 139951089751872 submission_runner.py:411] Time since start: 61920.22s, 	Step: 177404, 	{'train/accuracy': 0.9591039419174194, 'train/loss': 0.1507154107093811, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.0450918674468994, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8281220197677612, 'test/num_examples': 10000, 'score': 59728.900113105774, 'total_duration': 61920.21709442139, 'accumulated_submission_time': 59728.900113105774, 'accumulated_eval_time': 2178.15860247612, 'accumulated_logging_time': 6.749515771865845}
I0306 15:58:37.051160 139788996814592 logging_writer.py:48] [177404] accumulated_eval_time=2178.158602, accumulated_logging_time=6.749516, accumulated_submission_time=59728.900113, global_step=177404, preemption_count=0, score=59728.900113, test/accuracy=0.629100, test/loss=1.828122, test/num_examples=10000, total_duration=61920.217094, train/accuracy=0.959104, train/loss=0.150715, validation/accuracy=0.755940, validation/loss=1.045092, validation/num_examples=50000
I0306 15:59:10.514640 139789005207296 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.860576629638672, loss=0.5637730360031128
I0306 15:59:44.158307 139788996814592 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.589564800262451, loss=0.6647849082946777
I0306 16:00:17.767838 139789005207296 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.451799392700195, loss=0.6057014465332031
I0306 16:00:51.413843 139788996814592 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.23477840423584, loss=0.6735313534736633
I0306 16:01:25.205599 139789005207296 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.294084548950195, loss=0.6506054997444153
I0306 16:01:58.846857 139788996814592 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.7209014892578125, loss=0.6439703702926636
I0306 16:02:32.494662 139789005207296 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.339685440063477, loss=0.5609304308891296
I0306 16:03:06.137249 139788996814592 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.524350166320801, loss=0.6836329102516174
I0306 16:03:39.774468 139789005207296 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.375124931335449, loss=0.6179327964782715
I0306 16:04:13.422943 139788996814592 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.318995475769043, loss=0.6373697519302368
I0306 16:04:47.036597 139789005207296 logging_writer.py:48] [178500] global_step=178500, grad_norm=5.053985595703125, loss=0.6412231922149658
I0306 16:05:20.683477 139788996814592 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.265022277832031, loss=0.6784070730209351
I0306 16:05:54.316202 139789005207296 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.576754093170166, loss=0.6171467304229736
I0306 16:06:27.965508 139788996814592 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.896008491516113, loss=0.6839867234230042
I0306 16:07:01.608315 139789005207296 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.035431861877441, loss=0.5842989087104797
I0306 16:07:07.123600 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:07:13.421109 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 16:07:22.291340 139951089751872 spec.py:349] Evaluating on the test split.
I0306 16:07:24.647176 139951089751872 submission_runner.py:411] Time since start: 62447.86s, 	Step: 178918, 	{'train/accuracy': 0.959004282951355, 'train/loss': 0.15064071118831635, 'validation/accuracy': 0.756659984588623, 'validation/loss': 1.0407747030258179, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.8209997415542603, 'test/num_examples': 10000, 'score': 60238.09110379219, 'total_duration': 62447.85934686661, 'accumulated_submission_time': 60238.09110379219, 'accumulated_eval_time': 2195.6819846630096, 'accumulated_logging_time': 7.62337064743042}
I0306 16:07:24.712636 139787562383104 logging_writer.py:48] [178918] accumulated_eval_time=2195.681985, accumulated_logging_time=7.623371, accumulated_submission_time=60238.091104, global_step=178918, preemption_count=0, score=60238.091104, test/accuracy=0.629300, test/loss=1.821000, test/num_examples=10000, total_duration=62447.859347, train/accuracy=0.959004, train/loss=0.150641, validation/accuracy=0.756660, validation/loss=1.040775, validation/num_examples=50000
I0306 16:07:52.590191 139788996814592 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.425363063812256, loss=0.6580245494842529
I0306 16:08:26.158986 139787562383104 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.643955230712891, loss=0.6015711426734924
I0306 16:08:59.728939 139788996814592 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.200957298278809, loss=0.5784266591072083
I0306 16:09:33.311789 139787562383104 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.092440605163574, loss=0.5513057708740234
I0306 16:10:06.980011 139788996814592 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.830183506011963, loss=0.6325252652168274
I0306 16:10:40.605427 139787562383104 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.373007297515869, loss=0.6294363737106323
I0306 16:11:14.257762 139788996814592 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.731067180633545, loss=0.6289268732070923
I0306 16:11:47.873090 139787562383104 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.373697280883789, loss=0.6394579410552979
I0306 16:12:21.541173 139788996814592 logging_writer.py:48] [179800] global_step=179800, grad_norm=5.1109490394592285, loss=0.7447992563247681
I0306 16:12:55.153373 139787562383104 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.675313472747803, loss=0.7084500789642334
I0306 16:13:28.840454 139788996814592 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.712124347686768, loss=0.6331239342689514
I0306 16:14:02.425522 139787562383104 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.175457954406738, loss=0.5891640782356262
I0306 16:14:36.097673 139788996814592 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.641501426696777, loss=0.5996439456939697
I0306 16:15:09.751827 139787562383104 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.3649444580078125, loss=0.5744001269340515
I0306 16:15:43.442549 139788996814592 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.851457595825195, loss=0.6096940636634827
I0306 16:15:54.689475 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:16:00.686145 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 16:16:09.639128 139951089751872 spec.py:349] Evaluating on the test split.
I0306 16:16:11.899047 139951089751872 submission_runner.py:411] Time since start: 62975.11s, 	Step: 180435, 	{'train/accuracy': 0.9593430757522583, 'train/loss': 0.148685485124588, 'validation/accuracy': 0.7560799717903137, 'validation/loss': 1.0443583726882935, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8259391784667969, 'test/num_examples': 10000, 'score': 60748.00309252739, 'total_duration': 62975.11134815216, 'accumulated_submission_time': 60748.00309252739, 'accumulated_eval_time': 2212.8914959430695, 'accumulated_logging_time': 7.698556661605835}
I0306 16:16:11.945871 139787361040128 logging_writer.py:48] [180435] accumulated_eval_time=2212.891496, accumulated_logging_time=7.698557, accumulated_submission_time=60748.003093, global_step=180435, preemption_count=0, score=60748.003093, test/accuracy=0.630900, test/loss=1.825939, test/num_examples=10000, total_duration=62975.111348, train/accuracy=0.959343, train/loss=0.148685, validation/accuracy=0.756080, validation/loss=1.044358, validation/num_examples=50000
I0306 16:16:34.076399 139787369432832 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.198507308959961, loss=0.6588484048843384
I0306 16:17:07.625972 139787361040128 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.384159564971924, loss=0.6183746457099915
I0306 16:17:41.244777 139787369432832 logging_writer.py:48] [180700] global_step=180700, grad_norm=5.007326602935791, loss=0.6350582838058472
I0306 16:18:14.895155 139787361040128 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.83735990524292, loss=0.6681674122810364
I0306 16:18:48.516664 139787369432832 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.689386367797852, loss=0.67888343334198
I0306 16:19:22.086724 139787361040128 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.734248161315918, loss=0.6930294036865234
I0306 16:19:55.767872 139787369432832 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.801522731781006, loss=0.6520991325378418
I0306 16:20:29.362502 139787361040128 logging_writer.py:48] [181200] global_step=181200, grad_norm=3.7865443229675293, loss=0.5548113584518433
I0306 16:21:02.920789 139787369432832 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.698576927185059, loss=0.6577704548835754
I0306 16:21:36.489956 139787361040128 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.509392261505127, loss=0.5742022395133972
I0306 16:22:10.074275 139787369432832 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.275447368621826, loss=0.58980792760849
I0306 16:22:43.651683 139787361040128 logging_writer.py:48] [181600] global_step=181600, grad_norm=5.106584072113037, loss=0.6672234535217285
I0306 16:23:17.216923 139787369432832 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.543517589569092, loss=0.5960473418235779
I0306 16:23:50.780102 139787361040128 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.499985218048096, loss=0.6377473473548889
I0306 16:24:24.405572 139787369432832 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.714456558227539, loss=0.6163052320480347
I0306 16:24:42.035120 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:24:48.085124 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 16:24:56.841154 139951089751872 spec.py:349] Evaluating on the test split.
I0306 16:24:59.095837 139951089751872 submission_runner.py:411] Time since start: 63502.31s, 	Step: 181954, 	{'train/accuracy': 0.9592036008834839, 'train/loss': 0.14881004393100739, 'validation/accuracy': 0.7558799982070923, 'validation/loss': 1.0424973964691162, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8252125978469849, 'test/num_examples': 10000, 'score': 61258.025297403336, 'total_duration': 63502.30787181854, 'accumulated_submission_time': 61258.025297403336, 'accumulated_eval_time': 2229.9518847465515, 'accumulated_logging_time': 7.7558934688568115}
I0306 16:24:59.147903 139787361040128 logging_writer.py:48] [181954] accumulated_eval_time=2229.951885, accumulated_logging_time=7.755893, accumulated_submission_time=61258.025297, global_step=181954, preemption_count=0, score=61258.025297, test/accuracy=0.630900, test/loss=1.825213, test/num_examples=10000, total_duration=63502.307872, train/accuracy=0.959204, train/loss=0.148810, validation/accuracy=0.755880, validation/loss=1.042497, validation/num_examples=50000
I0306 16:25:14.962944 139787369432832 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.330479621887207, loss=0.5573306679725647
I0306 16:25:48.614026 139787361040128 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.525339126586914, loss=0.6181989312171936
I0306 16:26:22.268236 139787369432832 logging_writer.py:48] [182200] global_step=182200, grad_norm=5.0236735343933105, loss=0.6759865283966064
I0306 16:26:55.945755 139787361040128 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.6628217697143555, loss=0.6098655462265015
I0306 16:27:29.581779 139787369432832 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.8707427978515625, loss=0.6561617851257324
I0306 16:28:03.253564 139787361040128 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.628950119018555, loss=0.6385288834571838
I0306 16:28:36.913546 139787369432832 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.478233814239502, loss=0.6391910314559937
I0306 16:29:10.576319 139787361040128 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.22099494934082, loss=0.5725083351135254
I0306 16:29:44.229043 139787369432832 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.97139835357666, loss=0.7119671106338501
I0306 16:30:17.890569 139787361040128 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.375274181365967, loss=0.5518862009048462
I0306 16:30:51.510175 139787369432832 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.546804904937744, loss=0.6547650694847107
I0306 16:31:25.173938 139787361040128 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.4905290603637695, loss=0.5774344801902771
I0306 16:31:58.849105 139787369432832 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.140154838562012, loss=0.5962115526199341
I0306 16:32:32.504273 139787361040128 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.636960983276367, loss=0.6026403307914734
I0306 16:33:06.095801 139787369432832 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.543205738067627, loss=0.6054939031600952
I0306 16:33:29.133600 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:33:35.191467 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 16:33:43.880098 139951089751872 spec.py:349] Evaluating on the test split.
I0306 16:33:46.185482 139951089751872 submission_runner.py:411] Time since start: 64029.40s, 	Step: 183470, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14815087616443634, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0403326749801636, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8215793371200562, 'test/num_examples': 10000, 'score': 61767.9468460083, 'total_duration': 64029.397710323334, 'accumulated_submission_time': 61767.9468460083, 'accumulated_eval_time': 2247.0036346912384, 'accumulated_logging_time': 7.818108081817627}
I0306 16:33:46.232377 139789030385408 logging_writer.py:48] [183470] accumulated_eval_time=2247.003635, accumulated_logging_time=7.818108, accumulated_submission_time=61767.946846, global_step=183470, preemption_count=0, score=61767.946846, test/accuracy=0.632000, test/loss=1.821579, test/num_examples=10000, total_duration=64029.397710, train/accuracy=0.961097, train/loss=0.148151, validation/accuracy=0.757180, validation/loss=1.040333, validation/num_examples=50000
I0306 16:33:56.618334 139789038778112 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.185567378997803, loss=0.6146794557571411
I0306 16:34:30.206857 139789030385408 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.884378910064697, loss=0.6715782880783081
I0306 16:35:03.839625 139789038778112 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.37534236907959, loss=0.5962055325508118
I0306 16:35:37.513694 139789030385408 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.698288440704346, loss=0.6338198781013489
I0306 16:36:11.165915 139789038778112 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.513884544372559, loss=0.634003758430481
I0306 16:36:44.844555 139789030385408 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.434213161468506, loss=0.627362072467804
I0306 16:37:18.497561 139789038778112 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.315783977508545, loss=0.6349995136260986
I0306 16:37:52.218292 139789030385408 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.36915397644043, loss=0.6698668003082275
I0306 16:38:25.800074 139789038778112 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.75537633895874, loss=0.6345990896224976
I0306 16:38:59.436869 139789030385408 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.512570858001709, loss=0.6557419300079346
I0306 16:39:33.055534 139789038778112 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.734874248504639, loss=0.6796475648880005
I0306 16:40:06.705517 139789030385408 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.681228160858154, loss=0.6542987823486328
I0306 16:40:40.345391 139789038778112 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.361146450042725, loss=0.6405308246612549
I0306 16:41:13.999764 139789030385408 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.466474533081055, loss=0.6989689469337463
I0306 16:41:47.651842 139789038778112 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.309665679931641, loss=0.5982201099395752
I0306 16:42:16.403538 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:42:22.494355 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 16:42:31.329099 139951089751872 spec.py:349] Evaluating on the test split.
I0306 16:42:33.576979 139951089751872 submission_runner.py:411] Time since start: 64556.79s, 	Step: 184987, 	{'train/accuracy': 0.9597815275192261, 'train/loss': 0.14894135296344757, 'validation/accuracy': 0.7575399875640869, 'validation/loss': 1.0411063432693481, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8240126371383667, 'test/num_examples': 10000, 'score': 62278.05148458481, 'total_duration': 64556.78928041458, 'accumulated_submission_time': 62278.05148458481, 'accumulated_eval_time': 2264.1770162582397, 'accumulated_logging_time': 7.874583959579468}
I0306 16:42:33.624341 139787361040128 logging_writer.py:48] [184987] accumulated_eval_time=2264.177016, accumulated_logging_time=7.874584, accumulated_submission_time=62278.051485, global_step=184987, preemption_count=0, score=62278.051485, test/accuracy=0.631700, test/loss=1.824013, test/num_examples=10000, total_duration=64556.789280, train/accuracy=0.959782, train/loss=0.148941, validation/accuracy=0.757540, validation/loss=1.041106, validation/num_examples=50000
I0306 16:42:38.310535 139787369432832 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.421029567718506, loss=0.5799379348754883
I0306 16:43:11.898464 139787361040128 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.206313133239746, loss=0.5985183715820312
I0306 16:43:45.570294 139787369432832 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.322052001953125, loss=0.6400462985038757
I0306 16:44:19.335148 139787361040128 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.163147926330566, loss=0.6319336891174316
I0306 16:44:53.001209 139787369432832 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.685605049133301, loss=0.6871531009674072
I0306 16:45:26.630007 139787361040128 logging_writer.py:48] [185500] global_step=185500, grad_norm=3.9327309131622314, loss=0.599711537361145
I0306 16:46:00.270697 139787369432832 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.367128849029541, loss=0.5849242806434631
I0306 16:46:33.880563 139787361040128 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.16916561126709, loss=0.5904296636581421
I0306 16:47:07.560734 139787369432832 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.383889198303223, loss=0.6300259828567505
I0306 16:47:41.238388 139787361040128 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.440088748931885, loss=0.6923369765281677
I0306 16:48:14.837077 139787369432832 logging_writer.py:48] [186000] global_step=186000, grad_norm=5.064493179321289, loss=0.6647149324417114
I0306 16:48:48.525623 139787361040128 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.234847068786621, loss=0.5742954611778259
I0306 16:49:22.183470 139787369432832 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.505438327789307, loss=0.6882972717285156
I0306 16:49:55.813463 139787361040128 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.168869972229004, loss=0.5906082987785339
I0306 16:50:29.497545 139787369432832 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.868382453918457, loss=0.6406826972961426
I0306 16:51:03.103513 139787361040128 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.609765529632568, loss=0.6380261778831482
I0306 16:51:03.580161 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:51:09.687637 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 16:51:18.368246 139951089751872 spec.py:349] Evaluating on the test split.
I0306 16:51:20.597268 139951089751872 submission_runner.py:411] Time since start: 65083.81s, 	Step: 186503, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14516466856002808, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.0415334701538086, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8241097927093506, 'test/num_examples': 10000, 'score': 62787.941890478134, 'total_duration': 65083.8095471859, 'accumulated_submission_time': 62787.941890478134, 'accumulated_eval_time': 2281.1940383911133, 'accumulated_logging_time': 7.932438135147095}
I0306 16:51:20.645745 139789013600000 logging_writer.py:48] [186503] accumulated_eval_time=2281.194038, accumulated_logging_time=7.932438, accumulated_submission_time=62787.941890, global_step=186503, preemption_count=0, score=62787.941890, test/accuracy=0.632000, test/loss=1.824110, test/num_examples=10000, total_duration=65083.809547, train/accuracy=0.960519, train/loss=0.145165, validation/accuracy=0.757360, validation/loss=1.041533, validation/num_examples=50000
I0306 16:51:53.614702 139789021992704 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.666328430175781, loss=0.6719987392425537
I0306 16:52:27.257333 139789013600000 logging_writer.py:48] [186700] global_step=186700, grad_norm=4.199197769165039, loss=0.6076720952987671
I0306 16:53:00.897839 139789021992704 logging_writer.py:48] [186800] global_step=186800, grad_norm=4.528817176818848, loss=0.5608845949172974
I0306 16:53:34.549846 139789013600000 logging_writer.py:48] [186900] global_step=186900, grad_norm=4.031922817230225, loss=0.5614423751831055
I0306 16:54:08.178514 139789021992704 logging_writer.py:48] [187000] global_step=187000, grad_norm=4.789815902709961, loss=0.6834651827812195
I0306 16:54:41.800646 139789013600000 logging_writer.py:48] [187100] global_step=187100, grad_norm=4.270689964294434, loss=0.6375364065170288
I0306 16:55:15.387633 139789021992704 logging_writer.py:48] [187200] global_step=187200, grad_norm=4.970500469207764, loss=0.6523874402046204
I0306 16:55:49.045639 139789013600000 logging_writer.py:48] [187300] global_step=187300, grad_norm=4.293942451477051, loss=0.6068573594093323
I0306 16:56:22.702443 139789021992704 logging_writer.py:48] [187400] global_step=187400, grad_norm=4.471582412719727, loss=0.6128048896789551
I0306 16:56:56.310332 139789013600000 logging_writer.py:48] [187500] global_step=187500, grad_norm=4.385431289672852, loss=0.6136350631713867
I0306 16:57:29.971954 139789021992704 logging_writer.py:48] [187600] global_step=187600, grad_norm=4.579204082489014, loss=0.5934206247329712
I0306 16:58:03.627548 139789013600000 logging_writer.py:48] [187700] global_step=187700, grad_norm=4.722378730773926, loss=0.6002106666564941
I0306 16:58:37.250476 139789021992704 logging_writer.py:48] [187800] global_step=187800, grad_norm=4.410287857055664, loss=0.580304741859436
I0306 16:59:10.899006 139789013600000 logging_writer.py:48] [187900] global_step=187900, grad_norm=4.273620128631592, loss=0.63909512758255
I0306 16:59:44.524338 139789021992704 logging_writer.py:48] [188000] global_step=188000, grad_norm=4.21271276473999, loss=0.5844150185585022
I0306 16:59:50.719919 139951089751872 spec.py:321] Evaluating on the training split.
I0306 16:59:56.755234 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:00:05.451681 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:00:08.182015 139951089751872 submission_runner.py:411] Time since start: 65611.39s, 	Step: 188020, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.14389528334140778, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0408782958984375, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8217973709106445, 'test/num_examples': 10000, 'score': 63297.95154929161, 'total_duration': 65611.39433121681, 'accumulated_submission_time': 63297.95154929161, 'accumulated_eval_time': 2298.656086206436, 'accumulated_logging_time': 7.991012096405029}
I0306 17:00:08.223874 139787369432832 logging_writer.py:48] [188020] accumulated_eval_time=2298.656086, accumulated_logging_time=7.991012, accumulated_submission_time=63297.951549, global_step=188020, preemption_count=0, score=63297.951549, test/accuracy=0.631700, test/loss=1.821797, test/num_examples=10000, total_duration=65611.394331, train/accuracy=0.961376, train/loss=0.143895, validation/accuracy=0.757260, validation/loss=1.040878, validation/num_examples=50000
I0306 17:00:35.434401 139787562383104 logging_writer.py:48] [188100] global_step=188100, grad_norm=4.879216194152832, loss=0.6458501219749451
I0306 17:01:09.023932 139787369432832 logging_writer.py:48] [188200] global_step=188200, grad_norm=4.6189680099487305, loss=0.6479659676551819
I0306 17:01:42.700035 139787562383104 logging_writer.py:48] [188300] global_step=188300, grad_norm=4.7018914222717285, loss=0.649140477180481
I0306 17:02:16.381617 139787369432832 logging_writer.py:48] [188400] global_step=188400, grad_norm=4.530540466308594, loss=0.6446781158447266
I0306 17:02:50.008205 139787562383104 logging_writer.py:48] [188500] global_step=188500, grad_norm=4.801657676696777, loss=0.6269202828407288
I0306 17:03:23.624654 139787369432832 logging_writer.py:48] [188600] global_step=188600, grad_norm=4.780620574951172, loss=0.5955901145935059
I0306 17:03:57.292090 139787562383104 logging_writer.py:48] [188700] global_step=188700, grad_norm=4.541135787963867, loss=0.6521085500717163
I0306 17:04:30.919937 139787369432832 logging_writer.py:48] [188800] global_step=188800, grad_norm=4.624691963195801, loss=0.7265940308570862
I0306 17:05:04.574983 139787562383104 logging_writer.py:48] [188900] global_step=188900, grad_norm=4.507078647613525, loss=0.6281433701515198
I0306 17:05:38.216456 139787369432832 logging_writer.py:48] [189000] global_step=189000, grad_norm=4.301331520080566, loss=0.5733761191368103
I0306 17:06:11.854791 139787562383104 logging_writer.py:48] [189100] global_step=189100, grad_norm=4.2436933517456055, loss=0.5924762487411499
I0306 17:06:45.490463 139787369432832 logging_writer.py:48] [189200] global_step=189200, grad_norm=4.693027973175049, loss=0.667769730091095
I0306 17:07:19.122311 139787562383104 logging_writer.py:48] [189300] global_step=189300, grad_norm=4.936339855194092, loss=0.6941242218017578
I0306 17:07:52.749114 139787369432832 logging_writer.py:48] [189400] global_step=189400, grad_norm=4.491170406341553, loss=0.6014275550842285
I0306 17:08:26.442902 139787562383104 logging_writer.py:48] [189500] global_step=189500, grad_norm=5.008755207061768, loss=0.6963861584663391
I0306 17:08:38.348607 139951089751872 spec.py:321] Evaluating on the training split.
I0306 17:08:44.345618 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:08:53.197993 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:08:55.469695 139951089751872 submission_runner.py:411] Time since start: 66138.68s, 	Step: 189537, 	{'train/accuracy': 0.962332546710968, 'train/loss': 0.14211790263652802, 'validation/accuracy': 0.757420003414154, 'validation/loss': 1.0418941974639893, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8226947784423828, 'test/num_examples': 10000, 'score': 63808.01502132416, 'total_duration': 66138.68200182915, 'accumulated_submission_time': 63808.01502132416, 'accumulated_eval_time': 2315.7771377563477, 'accumulated_logging_time': 8.041492938995361}
I0306 17:08:55.524835 139787369432832 logging_writer.py:48] [189537] accumulated_eval_time=2315.777138, accumulated_logging_time=8.041493, accumulated_submission_time=63808.015021, global_step=189537, preemption_count=0, score=63808.015021, test/accuracy=0.631100, test/loss=1.822695, test/num_examples=10000, total_duration=66138.682002, train/accuracy=0.962333, train/loss=0.142118, validation/accuracy=0.757420, validation/loss=1.041894, validation/num_examples=50000
I0306 17:09:17.014478 139789021992704 logging_writer.py:48] [189600] global_step=189600, grad_norm=4.260819911956787, loss=0.5562846660614014
I0306 17:09:50.611735 139787369432832 logging_writer.py:48] [189700] global_step=189700, grad_norm=4.4436469078063965, loss=0.6333290934562683
I0306 17:10:24.190410 139789021992704 logging_writer.py:48] [189800] global_step=189800, grad_norm=4.453659534454346, loss=0.5899525880813599
I0306 17:10:57.803296 139787369432832 logging_writer.py:48] [189900] global_step=189900, grad_norm=4.909684658050537, loss=0.6098600625991821
I0306 17:11:31.521117 139789021992704 logging_writer.py:48] [190000] global_step=190000, grad_norm=4.558692932128906, loss=0.6515190601348877
I0306 17:12:05.137964 139787369432832 logging_writer.py:48] [190100] global_step=190100, grad_norm=4.273699760437012, loss=0.6234391331672668
I0306 17:12:38.740521 139789021992704 logging_writer.py:48] [190200] global_step=190200, grad_norm=4.194492816925049, loss=0.576095461845398
I0306 17:13:12.313896 139787369432832 logging_writer.py:48] [190300] global_step=190300, grad_norm=4.930967330932617, loss=0.646429181098938
I0306 17:13:45.875244 139789021992704 logging_writer.py:48] [190400] global_step=190400, grad_norm=4.503645896911621, loss=0.621332585811615
I0306 17:14:19.422749 139787369432832 logging_writer.py:48] [190500] global_step=190500, grad_norm=4.405221462249756, loss=0.645477294921875
I0306 17:14:53.054679 139789021992704 logging_writer.py:48] [190600] global_step=190600, grad_norm=4.5393967628479, loss=0.6011065244674683
I0306 17:15:26.627169 139787369432832 logging_writer.py:48] [190700] global_step=190700, grad_norm=4.196723461151123, loss=0.5841244459152222
I0306 17:16:00.282127 139789021992704 logging_writer.py:48] [190800] global_step=190800, grad_norm=4.317891597747803, loss=0.6690694093704224
I0306 17:16:33.928755 139787369432832 logging_writer.py:48] [190900] global_step=190900, grad_norm=4.1490373611450195, loss=0.525155782699585
I0306 17:17:07.584844 139789021992704 logging_writer.py:48] [191000] global_step=191000, grad_norm=4.503394603729248, loss=0.6465805768966675
I0306 17:17:25.568498 139951089751872 spec.py:321] Evaluating on the training split.
I0306 17:17:31.667963 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:17:40.460931 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:17:42.731146 139951089751872 submission_runner.py:411] Time since start: 66665.94s, 	Step: 191055, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14542309939861298, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0415695905685425, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8221133947372437, 'test/num_examples': 10000, 'score': 64317.99254322052, 'total_duration': 66665.94344830513, 'accumulated_submission_time': 64317.99254322052, 'accumulated_eval_time': 2332.939744949341, 'accumulated_logging_time': 8.108099699020386}
I0306 17:17:42.780394 139787369432832 logging_writer.py:48] [191055] accumulated_eval_time=2332.939745, accumulated_logging_time=8.108100, accumulated_submission_time=64317.992543, global_step=191055, preemption_count=0, score=64317.992543, test/accuracy=0.632100, test/loss=1.822113, test/num_examples=10000, total_duration=66665.943448, train/accuracy=0.961735, train/loss=0.145423, validation/accuracy=0.757340, validation/loss=1.041570, validation/num_examples=50000
I0306 17:17:58.226183 139788996814592 logging_writer.py:48] [191100] global_step=191100, grad_norm=3.892422914505005, loss=0.5611867904663086
I0306 17:18:31.839954 139787369432832 logging_writer.py:48] [191200] global_step=191200, grad_norm=4.502206325531006, loss=0.6776198148727417
I0306 17:19:05.475936 139788996814592 logging_writer.py:48] [191300] global_step=191300, grad_norm=4.496975421905518, loss=0.6568962931632996
I0306 17:19:39.127422 139787369432832 logging_writer.py:48] [191400] global_step=191400, grad_norm=4.869845390319824, loss=0.6532805562019348
I0306 17:20:12.743146 139788996814592 logging_writer.py:48] [191500] global_step=191500, grad_norm=4.797563552856445, loss=0.6042253971099854
I0306 17:20:46.430939 139787369432832 logging_writer.py:48] [191600] global_step=191600, grad_norm=4.373255729675293, loss=0.5767728090286255
I0306 17:21:20.044406 139788996814592 logging_writer.py:48] [191700] global_step=191700, grad_norm=4.338179588317871, loss=0.6302127838134766
I0306 17:21:53.717859 139787369432832 logging_writer.py:48] [191800] global_step=191800, grad_norm=5.270642280578613, loss=0.6417670845985413
I0306 17:22:27.363222 139788996814592 logging_writer.py:48] [191900] global_step=191900, grad_norm=4.6106061935424805, loss=0.6080031991004944
I0306 17:23:00.990904 139787369432832 logging_writer.py:48] [192000] global_step=192000, grad_norm=4.327613353729248, loss=0.526168704032898
I0306 17:23:34.607719 139788996814592 logging_writer.py:48] [192100] global_step=192100, grad_norm=4.268264293670654, loss=0.6333372592926025
I0306 17:24:08.263751 139787369432832 logging_writer.py:48] [192200] global_step=192200, grad_norm=4.728182792663574, loss=0.579555094242096
I0306 17:24:41.884310 139788996814592 logging_writer.py:48] [192300] global_step=192300, grad_norm=3.9215195178985596, loss=0.5793007612228394
I0306 17:25:15.550198 139787369432832 logging_writer.py:48] [192400] global_step=192400, grad_norm=4.5547261238098145, loss=0.6691796779632568
I0306 17:25:49.166589 139788996814592 logging_writer.py:48] [192500] global_step=192500, grad_norm=4.662262439727783, loss=0.5948213338851929
I0306 17:26:12.859433 139951089751872 spec.py:321] Evaluating on the training split.
I0306 17:26:19.567272 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:26:28.469269 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:26:30.766682 139951089751872 submission_runner.py:411] Time since start: 67193.98s, 	Step: 192572, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.14771416783332825, 'validation/accuracy': 0.757319986820221, 'validation/loss': 1.040766954421997, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8228449821472168, 'test/num_examples': 10000, 'score': 64828.0078830719, 'total_duration': 67193.97899508476, 'accumulated_submission_time': 64828.0078830719, 'accumulated_eval_time': 2350.8469479084015, 'accumulated_logging_time': 8.166432857513428}
I0306 17:26:30.816715 139789021992704 logging_writer.py:48] [192572] accumulated_eval_time=2350.846948, accumulated_logging_time=8.166433, accumulated_submission_time=64828.007883, global_step=192572, preemption_count=0, score=64828.007883, test/accuracy=0.632400, test/loss=1.822845, test/num_examples=10000, total_duration=67193.978995, train/accuracy=0.959742, train/loss=0.147714, validation/accuracy=0.757320, validation/loss=1.040767, validation/num_examples=50000
I0306 17:26:40.577283 139789030385408 logging_writer.py:48] [192600] global_step=192600, grad_norm=4.678347587585449, loss=0.678954541683197
I0306 17:27:14.368760 139789021992704 logging_writer.py:48] [192700] global_step=192700, grad_norm=4.551419258117676, loss=0.5931499600410461
I0306 17:27:48.032116 139789030385408 logging_writer.py:48] [192800] global_step=192800, grad_norm=4.754791736602783, loss=0.614720344543457
I0306 17:28:21.688008 139789021992704 logging_writer.py:48] [192900] global_step=192900, grad_norm=4.74762487411499, loss=0.6374919414520264
I0306 17:28:55.296735 139789030385408 logging_writer.py:48] [193000] global_step=193000, grad_norm=4.397665977478027, loss=0.5950692892074585
I0306 17:29:28.952936 139789021992704 logging_writer.py:48] [193100] global_step=193100, grad_norm=3.9065780639648438, loss=0.5482076406478882
I0306 17:30:02.585459 139789030385408 logging_writer.py:48] [193200] global_step=193200, grad_norm=4.875696182250977, loss=0.7223661541938782
I0306 17:30:36.232233 139789021992704 logging_writer.py:48] [193300] global_step=193300, grad_norm=4.597513675689697, loss=0.6661196947097778
I0306 17:31:09.864676 139789030385408 logging_writer.py:48] [193400] global_step=193400, grad_norm=4.419443607330322, loss=0.5956250429153442
I0306 17:31:43.538379 139789021992704 logging_writer.py:48] [193500] global_step=193500, grad_norm=4.667120456695557, loss=0.6279516816139221
I0306 17:32:17.167367 139789030385408 logging_writer.py:48] [193600] global_step=193600, grad_norm=4.779073715209961, loss=0.6632573008537292
I0306 17:32:50.924591 139789021992704 logging_writer.py:48] [193700] global_step=193700, grad_norm=4.173657417297363, loss=0.6089386940002441
I0306 17:33:24.533132 139789030385408 logging_writer.py:48] [193800] global_step=193800, grad_norm=4.504079818725586, loss=0.6002544164657593
I0306 17:33:58.118936 139789021992704 logging_writer.py:48] [193900] global_step=193900, grad_norm=4.129500865936279, loss=0.619248628616333
I0306 17:34:31.707934 139789030385408 logging_writer.py:48] [194000] global_step=194000, grad_norm=4.587490558624268, loss=0.6377826929092407
I0306 17:35:01.061015 139951089751872 spec.py:321] Evaluating on the training split.
I0306 17:35:07.083395 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:35:15.747286 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:35:18.059782 139951089751872 submission_runner.py:411] Time since start: 67721.27s, 	Step: 194089, 	{'train/accuracy': 0.9594626426696777, 'train/loss': 0.14820446074008942, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0418704748153687, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8231639862060547, 'test/num_examples': 10000, 'score': 65338.185666799545, 'total_duration': 67721.27209353447, 'accumulated_submission_time': 65338.185666799545, 'accumulated_eval_time': 2367.845665693283, 'accumulated_logging_time': 8.226392269134521}
I0306 17:35:18.108984 139787562383104 logging_writer.py:48] [194089] accumulated_eval_time=2367.845666, accumulated_logging_time=8.226392, accumulated_submission_time=65338.185667, global_step=194089, preemption_count=0, score=65338.185667, test/accuracy=0.631200, test/loss=1.823164, test/num_examples=10000, total_duration=67721.272094, train/accuracy=0.959463, train/loss=0.148204, validation/accuracy=0.757160, validation/loss=1.041870, validation/num_examples=50000
I0306 17:35:22.136643 139788996814592 logging_writer.py:48] [194100] global_step=194100, grad_norm=4.700103282928467, loss=0.6507903933525085
I0306 17:35:55.724996 139787562383104 logging_writer.py:48] [194200] global_step=194200, grad_norm=4.2366414070129395, loss=0.6251652836799622
I0306 17:36:29.285989 139788996814592 logging_writer.py:48] [194300] global_step=194300, grad_norm=4.465762615203857, loss=0.6034067869186401
I0306 17:37:02.896739 139787562383104 logging_writer.py:48] [194400] global_step=194400, grad_norm=4.862202167510986, loss=0.6724011898040771
I0306 17:37:36.556726 139788996814592 logging_writer.py:48] [194500] global_step=194500, grad_norm=4.35269021987915, loss=0.618354856967926
I0306 17:38:10.193911 139787562383104 logging_writer.py:48] [194600] global_step=194600, grad_norm=4.841200351715088, loss=0.5894597768783569
I0306 17:38:43.819615 139788996814592 logging_writer.py:48] [194700] global_step=194700, grad_norm=5.066755771636963, loss=0.7600924372673035
I0306 17:39:17.627679 139787562383104 logging_writer.py:48] [194800] global_step=194800, grad_norm=4.532132625579834, loss=0.6405469179153442
I0306 17:39:51.285091 139788996814592 logging_writer.py:48] [194900] global_step=194900, grad_norm=4.722146987915039, loss=0.6618853211402893
I0306 17:40:24.930538 139787562383104 logging_writer.py:48] [195000] global_step=195000, grad_norm=4.684426307678223, loss=0.6753895878791809
I0306 17:40:58.591174 139788996814592 logging_writer.py:48] [195100] global_step=195100, grad_norm=4.092667579650879, loss=0.5603010058403015
I0306 17:41:32.249785 139787562383104 logging_writer.py:48] [195200] global_step=195200, grad_norm=4.450497150421143, loss=0.5908617377281189
I0306 17:42:05.916735 139788996814592 logging_writer.py:48] [195300] global_step=195300, grad_norm=4.603338718414307, loss=0.6273155212402344
I0306 17:42:39.583077 139787562383104 logging_writer.py:48] [195400] global_step=195400, grad_norm=4.654613018035889, loss=0.6410156488418579
I0306 17:43:13.199525 139788996814592 logging_writer.py:48] [195500] global_step=195500, grad_norm=5.155791282653809, loss=0.629031777381897
I0306 17:43:46.841727 139787562383104 logging_writer.py:48] [195600] global_step=195600, grad_norm=4.370257377624512, loss=0.5634977221488953
I0306 17:43:48.340736 139951089751872 spec.py:321] Evaluating on the training split.
I0306 17:43:54.433254 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:44:03.233836 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:44:05.514663 139951089751872 submission_runner.py:411] Time since start: 68248.73s, 	Step: 195606, 	{'train/accuracy': 0.9624919891357422, 'train/loss': 0.14241808652877808, 'validation/accuracy': 0.7576199769973755, 'validation/loss': 1.0406326055526733, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.822136402130127, 'test/num_examples': 10000, 'score': 65848.35327005386, 'total_duration': 68248.72687864304, 'accumulated_submission_time': 65848.35327005386, 'accumulated_eval_time': 2385.0194478034973, 'accumulated_logging_time': 8.285529613494873}
I0306 17:44:05.565084 139789021992704 logging_writer.py:48] [195606] accumulated_eval_time=2385.019448, accumulated_logging_time=8.285530, accumulated_submission_time=65848.353270, global_step=195606, preemption_count=0, score=65848.353270, test/accuracy=0.631400, test/loss=1.822136, test/num_examples=10000, total_duration=68248.726879, train/accuracy=0.962492, train/loss=0.142418, validation/accuracy=0.757620, validation/loss=1.040633, validation/num_examples=50000
I0306 17:44:37.481217 139789030385408 logging_writer.py:48] [195700] global_step=195700, grad_norm=4.703187465667725, loss=0.6765279173851013
I0306 17:45:11.145637 139789021992704 logging_writer.py:48] [195800] global_step=195800, grad_norm=4.335363864898682, loss=0.6624062657356262
I0306 17:45:44.810755 139789030385408 logging_writer.py:48] [195900] global_step=195900, grad_norm=4.631819248199463, loss=0.6437135934829712
I0306 17:46:18.474046 139789021992704 logging_writer.py:48] [196000] global_step=196000, grad_norm=4.213554859161377, loss=0.6261779069900513
I0306 17:46:52.116485 139789030385408 logging_writer.py:48] [196100] global_step=196100, grad_norm=4.20526647567749, loss=0.5962631106376648
I0306 17:47:25.784445 139789021992704 logging_writer.py:48] [196200] global_step=196200, grad_norm=4.744263172149658, loss=0.6398135423660278
I0306 17:47:59.384386 139789030385408 logging_writer.py:48] [196300] global_step=196300, grad_norm=4.349789619445801, loss=0.6061397194862366
I0306 17:48:33.034295 139789021992704 logging_writer.py:48] [196400] global_step=196400, grad_norm=4.202744483947754, loss=0.5718002915382385
I0306 17:49:06.687529 139789030385408 logging_writer.py:48] [196500] global_step=196500, grad_norm=4.911699295043945, loss=0.6689329147338867
I0306 17:49:40.333886 139789021992704 logging_writer.py:48] [196600] global_step=196600, grad_norm=4.848742961883545, loss=0.6548104286193848
I0306 17:50:13.986553 139789030385408 logging_writer.py:48] [196700] global_step=196700, grad_norm=5.047308921813965, loss=0.6416670083999634
I0306 17:50:47.641010 139789021992704 logging_writer.py:48] [196800] global_step=196800, grad_norm=4.792555332183838, loss=0.5908514261245728
I0306 17:51:21.312233 139789030385408 logging_writer.py:48] [196900] global_step=196900, grad_norm=4.955470085144043, loss=0.5813994407653809
I0306 17:51:54.933483 139789021992704 logging_writer.py:48] [197000] global_step=197000, grad_norm=4.624053001403809, loss=0.6295531988143921
I0306 17:52:28.612302 139789030385408 logging_writer.py:48] [197100] global_step=197100, grad_norm=4.539216041564941, loss=0.6595526933670044
I0306 17:52:35.827605 139951089751872 spec.py:321] Evaluating on the training split.
I0306 17:52:41.814799 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 17:52:50.555440 139951089751872 spec.py:349] Evaluating on the test split.
I0306 17:52:52.802598 139951089751872 submission_runner.py:411] Time since start: 68776.01s, 	Step: 197123, 	{'train/accuracy': 0.9610969424247742, 'train/loss': 0.14622968435287476, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.039520263671875, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.821480393409729, 'test/num_examples': 10000, 'score': 66358.55031299591, 'total_duration': 68776.01489758492, 'accumulated_submission_time': 66358.55031299591, 'accumulated_eval_time': 2401.994375705719, 'accumulated_logging_time': 8.346335649490356}
I0306 17:52:52.857285 139788996814592 logging_writer.py:48] [197123] accumulated_eval_time=2401.994376, accumulated_logging_time=8.346336, accumulated_submission_time=66358.550313, global_step=197123, preemption_count=0, score=66358.550313, test/accuracy=0.632800, test/loss=1.821480, test/num_examples=10000, total_duration=68776.014898, train/accuracy=0.961097, train/loss=0.146230, validation/accuracy=0.757140, validation/loss=1.039520, validation/num_examples=50000
I0306 17:53:19.068344 139789005207296 logging_writer.py:48] [197200] global_step=197200, grad_norm=4.299296855926514, loss=0.6082066893577576
I0306 17:53:52.680118 139788996814592 logging_writer.py:48] [197300] global_step=197300, grad_norm=4.471226692199707, loss=0.635948657989502
I0306 17:54:26.318034 139789005207296 logging_writer.py:48] [197400] global_step=197400, grad_norm=4.040372371673584, loss=0.5724152326583862
I0306 17:54:59.946935 139788996814592 logging_writer.py:48] [197500] global_step=197500, grad_norm=4.356869697570801, loss=0.6322708129882812
I0306 17:55:33.582495 139789005207296 logging_writer.py:48] [197600] global_step=197600, grad_norm=4.979971408843994, loss=0.650657057762146
I0306 17:56:07.201508 139788996814592 logging_writer.py:48] [197700] global_step=197700, grad_norm=4.434178829193115, loss=0.6015135645866394
I0306 17:56:40.838027 139789005207296 logging_writer.py:48] [197800] global_step=197800, grad_norm=4.480257987976074, loss=0.6289452314376831
I0306 17:57:14.452718 139788996814592 logging_writer.py:48] [197900] global_step=197900, grad_norm=4.791382789611816, loss=0.6343418955802917
I0306 17:57:48.126695 139789005207296 logging_writer.py:48] [198000] global_step=198000, grad_norm=4.323897838592529, loss=0.5948051810264587
I0306 17:58:21.762156 139788996814592 logging_writer.py:48] [198100] global_step=198100, grad_norm=4.659165859222412, loss=0.5892991423606873
I0306 17:58:55.384640 139789005207296 logging_writer.py:48] [198200] global_step=198200, grad_norm=4.521701335906982, loss=0.6169078946113586
I0306 17:59:29.023826 139788996814592 logging_writer.py:48] [198300] global_step=198300, grad_norm=4.418821334838867, loss=0.6632566452026367
I0306 18:00:02.664143 139789005207296 logging_writer.py:48] [198400] global_step=198400, grad_norm=4.458913326263428, loss=0.6421630382537842
I0306 18:00:36.306661 139788996814592 logging_writer.py:48] [198500] global_step=198500, grad_norm=4.337887763977051, loss=0.6316317319869995
I0306 18:01:09.939958 139789005207296 logging_writer.py:48] [198600] global_step=198600, grad_norm=4.821333408355713, loss=0.6434122323989868
I0306 18:01:22.846432 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:01:28.935615 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:01:37.630535 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:01:39.923622 139951089751872 submission_runner.py:411] Time since start: 69303.14s, 	Step: 198640, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14489157497882843, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.041074275970459, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8206541538238525, 'test/num_examples': 10000, 'score': 66868.47186207771, 'total_duration': 69303.13593435287, 'accumulated_submission_time': 66868.47186207771, 'accumulated_eval_time': 2419.071531057358, 'accumulated_logging_time': 8.412399053573608}
I0306 18:01:39.974029 139787562383104 logging_writer.py:48] [198640] accumulated_eval_time=2419.071531, accumulated_logging_time=8.412399, accumulated_submission_time=66868.471862, global_step=198640, preemption_count=0, score=66868.471862, test/accuracy=0.631700, test/loss=1.820654, test/num_examples=10000, total_duration=69303.135934, train/accuracy=0.961834, train/loss=0.144892, validation/accuracy=0.757000, validation/loss=1.041074, validation/num_examples=50000
I0306 18:02:00.520356 139788996814592 logging_writer.py:48] [198700] global_step=198700, grad_norm=4.946349620819092, loss=0.6806546449661255
I0306 18:02:34.138868 139787562383104 logging_writer.py:48] [198800] global_step=198800, grad_norm=4.686394214630127, loss=0.5912542939186096
I0306 18:03:07.802303 139788996814592 logging_writer.py:48] [198900] global_step=198900, grad_norm=4.528977870941162, loss=0.711312472820282
I0306 18:03:41.462208 139787562383104 logging_writer.py:48] [199000] global_step=199000, grad_norm=4.518031120300293, loss=0.6158417463302612
I0306 18:04:15.099108 139788996814592 logging_writer.py:48] [199100] global_step=199100, grad_norm=4.619333267211914, loss=0.6010013222694397
I0306 18:04:48.785744 139787562383104 logging_writer.py:48] [199200] global_step=199200, grad_norm=4.782042503356934, loss=0.6588196754455566
I0306 18:05:22.465801 139788996814592 logging_writer.py:48] [199300] global_step=199300, grad_norm=4.6458234786987305, loss=0.6837254762649536
I0306 18:05:56.073548 139787562383104 logging_writer.py:48] [199400] global_step=199400, grad_norm=5.087934494018555, loss=0.6273447871208191
I0306 18:06:29.735630 139788996814592 logging_writer.py:48] [199500] global_step=199500, grad_norm=4.543509483337402, loss=0.60621577501297
I0306 18:07:03.352357 139787562383104 logging_writer.py:48] [199600] global_step=199600, grad_norm=4.6636962890625, loss=0.6835124492645264
I0306 18:07:37.010808 139788996814592 logging_writer.py:48] [199700] global_step=199700, grad_norm=4.510126113891602, loss=0.6262620687484741
I0306 18:08:10.658983 139787562383104 logging_writer.py:48] [199800] global_step=199800, grad_norm=4.282905101776123, loss=0.6089902520179749
I0306 18:08:44.346182 139788996814592 logging_writer.py:48] [199900] global_step=199900, grad_norm=4.121244430541992, loss=0.5487044453620911
I0306 18:09:17.984788 139787562383104 logging_writer.py:48] [200000] global_step=200000, grad_norm=4.579374313354492, loss=0.6377307176589966
I0306 18:09:51.658975 139788996814592 logging_writer.py:48] [200100] global_step=200100, grad_norm=4.53228759765625, loss=0.6828960180282593
I0306 18:10:09.963005 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:10:16.172114 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:10:24.947919 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:10:27.221861 139951089751872 submission_runner.py:411] Time since start: 69830.43s, 	Step: 200156, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14856016635894775, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.0412520170211792, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8226794004440308, 'test/num_examples': 10000, 'score': 67378.39338731766, 'total_duration': 69830.4341545105, 'accumulated_submission_time': 67378.39338731766, 'accumulated_eval_time': 2436.330323457718, 'accumulated_logging_time': 8.475186109542847}
I0306 18:10:27.271250 139789021992704 logging_writer.py:48] [200156] accumulated_eval_time=2436.330323, accumulated_logging_time=8.475186, accumulated_submission_time=67378.393387, global_step=200156, preemption_count=0, score=67378.393387, test/accuracy=0.631900, test/loss=1.822679, test/num_examples=10000, total_duration=69830.434155, train/accuracy=0.960180, train/loss=0.148560, validation/accuracy=0.757220, validation/loss=1.041252, validation/num_examples=50000
I0306 18:10:42.411606 139789030385408 logging_writer.py:48] [200200] global_step=200200, grad_norm=4.400723457336426, loss=0.5862115621566772
I0306 18:11:16.041240 139789021992704 logging_writer.py:48] [200300] global_step=200300, grad_norm=4.557697296142578, loss=0.5874006152153015
I0306 18:11:49.712574 139789030385408 logging_writer.py:48] [200400] global_step=200400, grad_norm=4.112825870513916, loss=0.5313237905502319
I0306 18:12:23.325537 139789021992704 logging_writer.py:48] [200500] global_step=200500, grad_norm=3.9635636806488037, loss=0.5304555296897888
I0306 18:12:56.957274 139789030385408 logging_writer.py:48] [200600] global_step=200600, grad_norm=4.175587177276611, loss=0.5232566595077515
I0306 18:13:30.589544 139789021992704 logging_writer.py:48] [200700] global_step=200700, grad_norm=4.933189868927002, loss=0.6837959885597229
I0306 18:14:04.218278 139789030385408 logging_writer.py:48] [200800] global_step=200800, grad_norm=4.85771369934082, loss=0.6554726958274841
I0306 18:14:37.849391 139789021992704 logging_writer.py:48] [200900] global_step=200900, grad_norm=4.776770114898682, loss=0.6580843925476074
I0306 18:15:11.493792 139789030385408 logging_writer.py:48] [201000] global_step=201000, grad_norm=5.091707229614258, loss=0.6493957042694092
I0306 18:15:45.174587 139789021992704 logging_writer.py:48] [201100] global_step=201100, grad_norm=4.673862457275391, loss=0.6243746876716614
I0306 18:16:18.752819 139789030385408 logging_writer.py:48] [201200] global_step=201200, grad_norm=4.9004807472229, loss=0.6455985307693481
I0306 18:16:52.329619 139789021992704 logging_writer.py:48] [201300] global_step=201300, grad_norm=4.697362899780273, loss=0.6505156755447388
I0306 18:17:25.947338 139789030385408 logging_writer.py:48] [201400] global_step=201400, grad_norm=4.040958404541016, loss=0.5503239631652832
I0306 18:17:59.598463 139789021992704 logging_writer.py:48] [201500] global_step=201500, grad_norm=4.5142035484313965, loss=0.5943336486816406
I0306 18:18:33.226651 139789030385408 logging_writer.py:48] [201600] global_step=201600, grad_norm=4.076246738433838, loss=0.6017911434173584
I0306 18:18:57.269131 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:19:03.291168 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:19:12.216267 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:19:14.487206 139951089751872 submission_runner.py:411] Time since start: 70357.70s, 	Step: 201673, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14740224182605743, 'validation/accuracy': 0.7573800086975098, 'validation/loss': 1.0413967370986938, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.823739767074585, 'test/num_examples': 10000, 'score': 67888.32476067543, 'total_duration': 70357.69950819016, 'accumulated_submission_time': 67888.32476067543, 'accumulated_eval_time': 2453.5483391284943, 'accumulated_logging_time': 8.535441637039185}
I0306 18:19:14.537690 139787562383104 logging_writer.py:48] [201673] accumulated_eval_time=2453.548339, accumulated_logging_time=8.535442, accumulated_submission_time=67888.324761, global_step=201673, preemption_count=0, score=67888.324761, test/accuracy=0.630900, test/loss=1.823740, test/num_examples=10000, total_duration=70357.699508, train/accuracy=0.960459, train/loss=0.147402, validation/accuracy=0.757380, validation/loss=1.041397, validation/num_examples=50000
I0306 18:19:23.950923 139788996814592 logging_writer.py:48] [201700] global_step=201700, grad_norm=4.282077789306641, loss=0.5627285242080688
I0306 18:19:57.506723 139787562383104 logging_writer.py:48] [201800] global_step=201800, grad_norm=4.47985315322876, loss=0.6284154057502747
I0306 18:20:31.103381 139788996814592 logging_writer.py:48] [201900] global_step=201900, grad_norm=4.624650001525879, loss=0.6635196805000305
I0306 18:21:04.666096 139787562383104 logging_writer.py:48] [202000] global_step=202000, grad_norm=4.895102024078369, loss=0.6327107548713684
I0306 18:21:38.246085 139788996814592 logging_writer.py:48] [202100] global_step=202100, grad_norm=4.193146228790283, loss=0.5438927412033081
I0306 18:22:11.898740 139787562383104 logging_writer.py:48] [202200] global_step=202200, grad_norm=4.426466941833496, loss=0.6705761551856995
I0306 18:22:45.509448 139788996814592 logging_writer.py:48] [202300] global_step=202300, grad_norm=4.669634819030762, loss=0.6248602867126465
I0306 18:23:19.152306 139787562383104 logging_writer.py:48] [202400] global_step=202400, grad_norm=4.224006175994873, loss=0.6282410025596619
I0306 18:23:52.767986 139788996814592 logging_writer.py:48] [202500] global_step=202500, grad_norm=5.008212089538574, loss=0.5579015016555786
I0306 18:24:26.444068 139787562383104 logging_writer.py:48] [202600] global_step=202600, grad_norm=4.395299911499023, loss=0.6736574172973633
I0306 18:25:00.053283 139788996814592 logging_writer.py:48] [202700] global_step=202700, grad_norm=4.754604339599609, loss=0.6210139393806458
I0306 18:25:33.711976 139787562383104 logging_writer.py:48] [202800] global_step=202800, grad_norm=4.609078884124756, loss=0.633266806602478
I0306 18:26:07.339736 139788996814592 logging_writer.py:48] [202900] global_step=202900, grad_norm=4.552217483520508, loss=0.6476830244064331
I0306 18:26:41.005274 139787562383104 logging_writer.py:48] [203000] global_step=203000, grad_norm=4.398805618286133, loss=0.6281797885894775
I0306 18:27:14.609452 139788996814592 logging_writer.py:48] [203100] global_step=203100, grad_norm=4.400607109069824, loss=0.6584601402282715
I0306 18:27:44.704064 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:27:51.013909 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:27:59.976889 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:28:02.217416 139951089751872 submission_runner.py:411] Time since start: 70885.43s, 	Step: 203191, 	{'train/accuracy': 0.9622727632522583, 'train/loss': 0.14330536127090454, 'validation/accuracy': 0.7568999528884888, 'validation/loss': 1.0419265031814575, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.822848916053772, 'test/num_examples': 10000, 'score': 68398.4266242981, 'total_duration': 70885.42971634865, 'accumulated_submission_time': 68398.4266242981, 'accumulated_eval_time': 2471.0616297721863, 'accumulated_logging_time': 8.596054077148438}
I0306 18:28:02.267479 139787562383104 logging_writer.py:48] [203191] accumulated_eval_time=2471.061630, accumulated_logging_time=8.596054, accumulated_submission_time=68398.426624, global_step=203191, preemption_count=0, score=68398.426624, test/accuracy=0.631100, test/loss=1.822849, test/num_examples=10000, total_duration=70885.429716, train/accuracy=0.962273, train/loss=0.143305, validation/accuracy=0.756900, validation/loss=1.041927, validation/num_examples=50000
I0306 18:28:05.632917 139789021992704 logging_writer.py:48] [203200] global_step=203200, grad_norm=4.735774040222168, loss=0.6774920225143433
I0306 18:28:39.354170 139787562383104 logging_writer.py:48] [203300] global_step=203300, grad_norm=4.310593128204346, loss=0.6151114106178284
I0306 18:29:12.993762 139789021992704 logging_writer.py:48] [203400] global_step=203400, grad_norm=4.409888744354248, loss=0.619105875492096
I0306 18:29:46.623945 139787562383104 logging_writer.py:48] [203500] global_step=203500, grad_norm=4.974442005157471, loss=0.6405256986618042
I0306 18:30:20.265444 139789021992704 logging_writer.py:48] [203600] global_step=203600, grad_norm=4.5519633293151855, loss=0.618606448173523
I0306 18:30:53.909123 139787562383104 logging_writer.py:48] [203700] global_step=203700, grad_norm=4.424905776977539, loss=0.5914093852043152
I0306 18:31:27.554784 139789021992704 logging_writer.py:48] [203800] global_step=203800, grad_norm=4.804458141326904, loss=0.5957983732223511
I0306 18:32:01.211365 139787562383104 logging_writer.py:48] [203900] global_step=203900, grad_norm=4.425354957580566, loss=0.5669811367988586
I0306 18:32:34.835307 139789021992704 logging_writer.py:48] [204000] global_step=204000, grad_norm=4.804056167602539, loss=0.6583574414253235
I0306 18:33:08.473664 139787562383104 logging_writer.py:48] [204100] global_step=204100, grad_norm=4.560881614685059, loss=0.6432477235794067
I0306 18:33:42.140814 139789021992704 logging_writer.py:48] [204200] global_step=204200, grad_norm=4.454297065734863, loss=0.6399807333946228
I0306 18:34:15.801038 139787562383104 logging_writer.py:48] [204300] global_step=204300, grad_norm=4.373913288116455, loss=0.6810857653617859
I0306 18:34:49.416946 139789021992704 logging_writer.py:48] [204400] global_step=204400, grad_norm=4.559650897979736, loss=0.6341405510902405
I0306 18:35:23.020403 139787562383104 logging_writer.py:48] [204500] global_step=204500, grad_norm=4.788188934326172, loss=0.6428061723709106
I0306 18:35:56.624214 139789021992704 logging_writer.py:48] [204600] global_step=204600, grad_norm=4.453649997711182, loss=0.589012861251831
I0306 18:36:30.256947 139787562383104 logging_writer.py:48] [204700] global_step=204700, grad_norm=4.871193885803223, loss=0.6578304767608643
I0306 18:36:32.427900 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:36:38.441780 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:36:47.108011 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:36:49.361858 139951089751872 submission_runner.py:411] Time since start: 71412.57s, 	Step: 204708, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14481258392333984, 'validation/accuracy': 0.7568399906158447, 'validation/loss': 1.0413434505462646, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8237817287445068, 'test/num_examples': 10000, 'score': 68908.5199649334, 'total_duration': 71412.57393550873, 'accumulated_submission_time': 68908.5199649334, 'accumulated_eval_time': 2487.9953002929688, 'accumulated_logging_time': 8.656528234481812}
I0306 18:36:49.415291 139787562383104 logging_writer.py:48] [204708] accumulated_eval_time=2487.995300, accumulated_logging_time=8.656528, accumulated_submission_time=68908.519965, global_step=204708, preemption_count=0, score=68908.519965, test/accuracy=0.631400, test/loss=1.823782, test/num_examples=10000, total_duration=71412.573936, train/accuracy=0.960619, train/loss=0.144813, validation/accuracy=0.756840, validation/loss=1.041343, validation/num_examples=50000
I0306 18:37:20.681987 139788996814592 logging_writer.py:48] [204800] global_step=204800, grad_norm=4.215668201446533, loss=0.6053123474121094
I0306 18:37:54.349651 139787562383104 logging_writer.py:48] [204900] global_step=204900, grad_norm=4.753578186035156, loss=0.6646773815155029
I0306 18:38:27.992783 139788996814592 logging_writer.py:48] [205000] global_step=205000, grad_norm=4.4754743576049805, loss=0.6160805821418762
I0306 18:39:01.644104 139787562383104 logging_writer.py:48] [205100] global_step=205100, grad_norm=4.460229396820068, loss=0.5950850248336792
I0306 18:39:35.265381 139788996814592 logging_writer.py:48] [205200] global_step=205200, grad_norm=4.400260925292969, loss=0.577303409576416
I0306 18:40:08.966587 139787562383104 logging_writer.py:48] [205300] global_step=205300, grad_norm=4.286229133605957, loss=0.6431640982627869
I0306 18:40:42.554015 139788996814592 logging_writer.py:48] [205400] global_step=205400, grad_norm=4.365218639373779, loss=0.5964341759681702
I0306 18:41:16.173858 139787562383104 logging_writer.py:48] [205500] global_step=205500, grad_norm=4.549557209014893, loss=0.6349461078643799
I0306 18:41:49.784300 139788996814592 logging_writer.py:48] [205600] global_step=205600, grad_norm=4.21589994430542, loss=0.5497685074806213
I0306 18:42:23.449536 139787562383104 logging_writer.py:48] [205700] global_step=205700, grad_norm=4.662951469421387, loss=0.5358798503875732
I0306 18:42:57.061342 139788996814592 logging_writer.py:48] [205800] global_step=205800, grad_norm=4.621572017669678, loss=0.6223947405815125
I0306 18:43:30.694199 139787562383104 logging_writer.py:48] [205900] global_step=205900, grad_norm=4.912394046783447, loss=0.6520399451255798
I0306 18:44:04.307458 139788996814592 logging_writer.py:48] [206000] global_step=206000, grad_norm=4.734635829925537, loss=0.5924755334854126
I0306 18:44:37.970073 139787562383104 logging_writer.py:48] [206100] global_step=206100, grad_norm=3.9802842140197754, loss=0.587553083896637
I0306 18:45:11.596361 139788996814592 logging_writer.py:48] [206200] global_step=206200, grad_norm=4.192661762237549, loss=0.5394573211669922
I0306 18:45:19.492736 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:45:25.542884 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:45:34.197785 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:45:36.441026 139951089751872 submission_runner.py:411] Time since start: 71939.65s, 	Step: 206225, 	{'train/accuracy': 0.9596420526504517, 'train/loss': 0.15033118426799774, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.041097640991211, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.822572112083435, 'test/num_examples': 10000, 'score': 69418.53223371506, 'total_duration': 71939.65333914757, 'accumulated_submission_time': 69418.53223371506, 'accumulated_eval_time': 2504.9435436725616, 'accumulated_logging_time': 8.719544410705566}
I0306 18:45:36.493413 139787562383104 logging_writer.py:48] [206225] accumulated_eval_time=2504.943544, accumulated_logging_time=8.719544, accumulated_submission_time=69418.532234, global_step=206225, preemption_count=0, score=69418.532234, test/accuracy=0.631300, test/loss=1.822572, test/num_examples=10000, total_duration=71939.653339, train/accuracy=0.959642, train/loss=0.150331, validation/accuracy=0.757280, validation/loss=1.041098, validation/num_examples=50000
I0306 18:46:02.030184 139789021992704 logging_writer.py:48] [206300] global_step=206300, grad_norm=4.289121627807617, loss=0.5665535926818848
I0306 18:46:35.656218 139787562383104 logging_writer.py:48] [206400] global_step=206400, grad_norm=4.288694858551025, loss=0.6256356239318848
I0306 18:47:09.241470 139789021992704 logging_writer.py:48] [206500] global_step=206500, grad_norm=5.095545291900635, loss=0.6618086695671082
I0306 18:47:42.860227 139787562383104 logging_writer.py:48] [206600] global_step=206600, grad_norm=4.78513765335083, loss=0.6181623935699463
I0306 18:48:16.508074 139789021992704 logging_writer.py:48] [206700] global_step=206700, grad_norm=4.322982311248779, loss=0.5934950709342957
I0306 18:48:50.155652 139787562383104 logging_writer.py:48] [206800] global_step=206800, grad_norm=4.2951555252075195, loss=0.6222965717315674
I0306 18:49:23.822869 139789021992704 logging_writer.py:48] [206900] global_step=206900, grad_norm=4.694767475128174, loss=0.6426632404327393
I0306 18:49:57.472463 139787562383104 logging_writer.py:48] [207000] global_step=207000, grad_norm=4.577027320861816, loss=0.6552555561065674
I0306 18:50:31.058482 139789021992704 logging_writer.py:48] [207100] global_step=207100, grad_norm=4.14824104309082, loss=0.5381097793579102
I0306 18:51:04.608530 139787562383104 logging_writer.py:48] [207200] global_step=207200, grad_norm=5.24891471862793, loss=0.670330286026001
I0306 18:51:38.195906 139789021992704 logging_writer.py:48] [207300] global_step=207300, grad_norm=4.783016204833984, loss=0.6242742538452148
I0306 18:52:11.842144 139787562383104 logging_writer.py:48] [207400] global_step=207400, grad_norm=4.157910346984863, loss=0.5718684792518616
I0306 18:52:45.505277 139789021992704 logging_writer.py:48] [207500] global_step=207500, grad_norm=4.235097885131836, loss=0.5768961310386658
I0306 18:53:19.082934 139787562383104 logging_writer.py:48] [207600] global_step=207600, grad_norm=4.012209415435791, loss=0.5696583986282349
I0306 18:53:52.671671 139789021992704 logging_writer.py:48] [207700] global_step=207700, grad_norm=4.319887161254883, loss=0.5751227140426636
I0306 18:54:06.602031 139951089751872 spec.py:321] Evaluating on the training split.
I0306 18:54:12.603084 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 18:54:21.336229 139951089751872 spec.py:349] Evaluating on the test split.
I0306 18:54:23.664750 139951089751872 submission_runner.py:411] Time since start: 72466.88s, 	Step: 207743, 	{'train/accuracy': 0.9592633843421936, 'train/loss': 0.14812898635864258, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.0401948690414429, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8207805156707764, 'test/num_examples': 10000, 'score': 69928.57495713234, 'total_duration': 72466.876288414, 'accumulated_submission_time': 69928.57495713234, 'accumulated_eval_time': 2522.005439043045, 'accumulated_logging_time': 8.781482458114624}
I0306 18:54:23.716783 139787369432832 logging_writer.py:48] [207743] accumulated_eval_time=2522.005439, accumulated_logging_time=8.781482, accumulated_submission_time=69928.574957, global_step=207743, preemption_count=0, score=69928.574957, test/accuracy=0.631900, test/loss=1.820781, test/num_examples=10000, total_duration=72466.876288, train/accuracy=0.959263, train/loss=0.148129, validation/accuracy=0.757360, validation/loss=1.040195, validation/num_examples=50000
I0306 18:54:43.256826 139788996814592 logging_writer.py:48] [207800] global_step=207800, grad_norm=4.875828742980957, loss=0.6969497799873352
I0306 18:55:16.873162 139787369432832 logging_writer.py:48] [207900] global_step=207900, grad_norm=4.241809844970703, loss=0.5722172856330872
I0306 18:55:50.534144 139788996814592 logging_writer.py:48] [208000] global_step=208000, grad_norm=4.4423322677612305, loss=0.6435788869857788
I0306 18:56:24.164092 139787369432832 logging_writer.py:48] [208100] global_step=208100, grad_norm=4.851632595062256, loss=0.7246302366256714
I0306 18:56:57.835977 139788996814592 logging_writer.py:48] [208200] global_step=208200, grad_norm=4.443962574005127, loss=0.6027699708938599
I0306 18:57:31.448652 139787369432832 logging_writer.py:48] [208300] global_step=208300, grad_norm=3.983729362487793, loss=0.6122473478317261
I0306 18:58:05.120528 139788996814592 logging_writer.py:48] [208400] global_step=208400, grad_norm=4.308846950531006, loss=0.5980516672134399
I0306 18:58:38.788094 139787369432832 logging_writer.py:48] [208500] global_step=208500, grad_norm=4.3214826583862305, loss=0.6315830945968628
I0306 18:59:12.430290 139788996814592 logging_writer.py:48] [208600] global_step=208600, grad_norm=4.352844715118408, loss=0.5824546813964844
I0306 18:59:46.048049 139787369432832 logging_writer.py:48] [208700] global_step=208700, grad_norm=4.4765191078186035, loss=0.6187460422515869
I0306 19:00:19.663982 139788996814592 logging_writer.py:48] [208800] global_step=208800, grad_norm=4.261870384216309, loss=0.631680965423584
I0306 19:00:53.270183 139787369432832 logging_writer.py:48] [208900] global_step=208900, grad_norm=4.653801441192627, loss=0.5590718984603882
I0306 19:01:26.924541 139788996814592 logging_writer.py:48] [209000] global_step=209000, grad_norm=4.2235918045043945, loss=0.5433252453804016
I0306 19:02:00.553259 139787369432832 logging_writer.py:48] [209100] global_step=209100, grad_norm=3.988527297973633, loss=0.6207009553909302
I0306 19:02:34.214932 139788996814592 logging_writer.py:48] [209200] global_step=209200, grad_norm=4.4350128173828125, loss=0.6944687366485596
I0306 19:02:53.859363 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:02:59.859285 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:03:08.690114 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:03:11.045300 139951089751872 submission_runner.py:411] Time since start: 72994.26s, 	Step: 209260, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.1483916938304901, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.0417641401290894, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.822595238685608, 'test/num_examples': 10000, 'score': 70438.65213561058, 'total_duration': 72994.25761318207, 'accumulated_submission_time': 70438.65213561058, 'accumulated_eval_time': 2539.191326379776, 'accumulated_logging_time': 8.84352707862854}
I0306 19:03:11.101397 139787369432832 logging_writer.py:48] [209260] accumulated_eval_time=2539.191326, accumulated_logging_time=8.843527, accumulated_submission_time=70438.652136, global_step=209260, preemption_count=0, score=70438.652136, test/accuracy=0.630400, test/loss=1.822595, test/num_examples=10000, total_duration=72994.257613, train/accuracy=0.960858, train/loss=0.148392, validation/accuracy=0.756920, validation/loss=1.041764, validation/num_examples=50000
I0306 19:03:24.854416 139789021992704 logging_writer.py:48] [209300] global_step=209300, grad_norm=4.120601177215576, loss=0.5917009115219116
I0306 19:03:58.444508 139787369432832 logging_writer.py:48] [209400] global_step=209400, grad_norm=4.351805210113525, loss=0.6482828855514526
I0306 19:04:32.227004 139789021992704 logging_writer.py:48] [209500] global_step=209500, grad_norm=4.5519256591796875, loss=0.6130386590957642
I0306 19:05:05.853470 139787369432832 logging_writer.py:48] [209600] global_step=209600, grad_norm=4.353477478027344, loss=0.6133641600608826
I0306 19:05:39.454300 139789021992704 logging_writer.py:48] [209700] global_step=209700, grad_norm=4.554055213928223, loss=0.6369209885597229
I0306 19:06:13.070527 139787369432832 logging_writer.py:48] [209800] global_step=209800, grad_norm=4.926812171936035, loss=0.669975757598877
I0306 19:06:46.771357 139789021992704 logging_writer.py:48] [209900] global_step=209900, grad_norm=4.419607162475586, loss=0.6187243461608887
I0306 19:07:20.395162 139787369432832 logging_writer.py:48] [210000] global_step=210000, grad_norm=4.432760715484619, loss=0.6334274411201477
I0306 19:07:54.022740 139789021992704 logging_writer.py:48] [210100] global_step=210100, grad_norm=4.148643493652344, loss=0.584186315536499
I0306 19:08:27.670588 139787369432832 logging_writer.py:48] [210200] global_step=210200, grad_norm=4.67802619934082, loss=0.6506393551826477
I0306 19:09:01.305318 139789021992704 logging_writer.py:48] [210300] global_step=210300, grad_norm=4.476957321166992, loss=0.651340663433075
I0306 19:09:34.919950 139787369432832 logging_writer.py:48] [210400] global_step=210400, grad_norm=4.714409351348877, loss=0.5970655679702759
I0306 19:10:08.566869 139789021992704 logging_writer.py:48] [210500] global_step=210500, grad_norm=4.631381034851074, loss=0.5802081823348999
I0306 19:10:42.308058 139787369432832 logging_writer.py:48] [210600] global_step=210600, grad_norm=5.133985996246338, loss=0.6047099828720093
I0306 19:11:15.945006 139789021992704 logging_writer.py:48] [210700] global_step=210700, grad_norm=4.817276477813721, loss=0.7045296430587769
I0306 19:11:41.277583 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:11:47.348069 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:11:56.211675 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:11:58.477613 139951089751872 submission_runner.py:411] Time since start: 73521.69s, 	Step: 210777, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14493539929389954, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.041481375694275, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8224416971206665, 'test/num_examples': 10000, 'score': 70948.76120257378, 'total_duration': 73521.68989634514, 'accumulated_submission_time': 70948.76120257378, 'accumulated_eval_time': 2556.391278028488, 'accumulated_logging_time': 8.910342454910278}
I0306 19:11:58.534175 139787361040128 logging_writer.py:48] [210777] accumulated_eval_time=2556.391278, accumulated_logging_time=8.910342, accumulated_submission_time=70948.761203, global_step=210777, preemption_count=0, score=70948.761203, test/accuracy=0.632100, test/loss=1.822442, test/num_examples=10000, total_duration=73521.689896, train/accuracy=0.961595, train/loss=0.144935, validation/accuracy=0.757040, validation/loss=1.041481, validation/num_examples=50000
I0306 19:12:06.593524 139787369432832 logging_writer.py:48] [210800] global_step=210800, grad_norm=4.290486812591553, loss=0.6496356725692749
I0306 19:12:40.241261 139787361040128 logging_writer.py:48] [210900] global_step=210900, grad_norm=4.570291996002197, loss=0.620531439781189
I0306 19:13:13.888046 139787369432832 logging_writer.py:48] [211000] global_step=211000, grad_norm=4.312327861785889, loss=0.5352456569671631
I0306 19:13:47.452249 139787361040128 logging_writer.py:48] [211100] global_step=211100, grad_norm=4.571422576904297, loss=0.6197060942649841
I0306 19:14:21.023569 139787369432832 logging_writer.py:48] [211200] global_step=211200, grad_norm=4.639113426208496, loss=0.6626294255256653
I0306 19:14:54.599829 139787361040128 logging_writer.py:48] [211300] global_step=211300, grad_norm=4.28024959564209, loss=0.6214503049850464
I0306 19:15:28.216620 139787369432832 logging_writer.py:48] [211400] global_step=211400, grad_norm=4.160195350646973, loss=0.6045424342155457
I0306 19:16:01.841894 139787361040128 logging_writer.py:48] [211500] global_step=211500, grad_norm=4.328742980957031, loss=0.6167284250259399
I0306 19:16:35.443315 139787369432832 logging_writer.py:48] [211600] global_step=211600, grad_norm=4.522574424743652, loss=0.6191903352737427
I0306 19:17:09.109409 139787361040128 logging_writer.py:48] [211700] global_step=211700, grad_norm=4.595987319946289, loss=0.6221362352371216
I0306 19:17:42.722136 139787369432832 logging_writer.py:48] [211800] global_step=211800, grad_norm=4.597570896148682, loss=0.6077601909637451
I0306 19:18:16.394060 139787361040128 logging_writer.py:48] [211900] global_step=211900, grad_norm=4.446411609649658, loss=0.5976172685623169
I0306 19:18:50.019850 139787369432832 logging_writer.py:48] [212000] global_step=212000, grad_norm=4.765361309051514, loss=0.6810271739959717
I0306 19:19:23.682522 139787361040128 logging_writer.py:48] [212100] global_step=212100, grad_norm=4.587305545806885, loss=0.6322965621948242
I0306 19:19:57.296893 139787369432832 logging_writer.py:48] [212200] global_step=212200, grad_norm=4.4990644454956055, loss=0.6351714730262756
I0306 19:20:28.707543 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:20:34.763578 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:20:43.642948 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:20:45.911173 139951089751872 submission_runner.py:411] Time since start: 74049.12s, 	Step: 212295, 	{'train/accuracy': 0.9602199792861938, 'train/loss': 0.14977553486824036, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0418096780776978, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8232481479644775, 'test/num_examples': 10000, 'score': 71458.87047481537, 'total_duration': 74049.12346696854, 'accumulated_submission_time': 71458.87047481537, 'accumulated_eval_time': 2573.594845056534, 'accumulated_logging_time': 8.976428747177124}
I0306 19:20:45.966630 139787369432832 logging_writer.py:48] [212295] accumulated_eval_time=2573.594845, accumulated_logging_time=8.976429, accumulated_submission_time=71458.870475, global_step=212295, preemption_count=0, score=71458.870475, test/accuracy=0.631500, test/loss=1.823248, test/num_examples=10000, total_duration=74049.123467, train/accuracy=0.960220, train/loss=0.149776, validation/accuracy=0.757020, validation/loss=1.041810, validation/num_examples=50000
I0306 19:20:47.993913 139789021992704 logging_writer.py:48] [212300] global_step=212300, grad_norm=4.294889450073242, loss=0.6304029226303101
I0306 19:21:21.605313 139787369432832 logging_writer.py:48] [212400] global_step=212400, grad_norm=4.522552490234375, loss=0.6059105396270752
I0306 19:21:55.182567 139789021992704 logging_writer.py:48] [212500] global_step=212500, grad_norm=4.743154525756836, loss=0.5676892399787903
I0306 19:22:28.763002 139787369432832 logging_writer.py:48] [212600] global_step=212600, grad_norm=4.414246559143066, loss=0.6260161995887756
I0306 19:23:02.411898 139789021992704 logging_writer.py:48] [212700] global_step=212700, grad_norm=4.507225513458252, loss=0.679848849773407
I0306 19:23:36.049383 139787369432832 logging_writer.py:48] [212800] global_step=212800, grad_norm=4.2061896324157715, loss=0.5908229947090149
I0306 19:24:09.724536 139789021992704 logging_writer.py:48] [212900] global_step=212900, grad_norm=4.439111709594727, loss=0.6735978722572327
I0306 19:24:43.389436 139787369432832 logging_writer.py:48] [213000] global_step=213000, grad_norm=4.5920586585998535, loss=0.5892822742462158
I0306 19:25:17.014967 139789021992704 logging_writer.py:48] [213100] global_step=213100, grad_norm=4.501110076904297, loss=0.6545587182044983
I0306 19:25:50.641263 139787369432832 logging_writer.py:48] [213200] global_step=213200, grad_norm=4.359307765960693, loss=0.5496899485588074
I0306 19:26:24.292041 139789021992704 logging_writer.py:48] [213300] global_step=213300, grad_norm=4.454893112182617, loss=0.6304833889007568
I0306 19:26:57.938824 139787369432832 logging_writer.py:48] [213400] global_step=213400, grad_norm=4.710578441619873, loss=0.6989598274230957
I0306 19:27:31.588456 139789021992704 logging_writer.py:48] [213500] global_step=213500, grad_norm=4.472674369812012, loss=0.6381593346595764
I0306 19:28:05.248729 139787369432832 logging_writer.py:48] [213600] global_step=213600, grad_norm=4.65173864364624, loss=0.6191427707672119
I0306 19:28:38.887296 139789021992704 logging_writer.py:48] [213700] global_step=213700, grad_norm=4.171408653259277, loss=0.5768014788627625
I0306 19:29:12.553279 139787369432832 logging_writer.py:48] [213800] global_step=213800, grad_norm=4.726386070251465, loss=0.6145535111427307
I0306 19:29:16.062307 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:29:22.113005 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:29:30.747870 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:29:33.066998 139951089751872 submission_runner.py:411] Time since start: 74576.28s, 	Step: 213812, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.14492282271385193, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0410807132720947, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.823547601699829, 'test/num_examples': 10000, 'score': 71968.89846563339, 'total_duration': 74576.27930808067, 'accumulated_submission_time': 71968.89846563339, 'accumulated_eval_time': 2590.599480867386, 'accumulated_logging_time': 9.042627096176147}
I0306 19:29:33.121452 139788996814592 logging_writer.py:48] [213812] accumulated_eval_time=2590.599481, accumulated_logging_time=9.042627, accumulated_submission_time=71968.898466, global_step=213812, preemption_count=0, score=71968.898466, test/accuracy=0.631900, test/loss=1.823548, test/num_examples=10000, total_duration=74576.279308, train/accuracy=0.961396, train/loss=0.144923, validation/accuracy=0.757300, validation/loss=1.041081, validation/num_examples=50000
I0306 19:30:03.084162 139789005207296 logging_writer.py:48] [213900] global_step=213900, grad_norm=4.896105766296387, loss=0.5944198966026306
I0306 19:30:36.753977 139788996814592 logging_writer.py:48] [214000] global_step=214000, grad_norm=4.339391708374023, loss=0.6912794709205627
I0306 19:31:10.376361 139789005207296 logging_writer.py:48] [214100] global_step=214100, grad_norm=4.507819175720215, loss=0.6506730914115906
I0306 19:31:43.987076 139788996814592 logging_writer.py:48] [214200] global_step=214200, grad_norm=4.732147216796875, loss=0.6513972878456116
I0306 19:32:17.609831 139789005207296 logging_writer.py:48] [214300] global_step=214300, grad_norm=4.22617244720459, loss=0.629244327545166
I0306 19:32:51.258491 139788996814592 logging_writer.py:48] [214400] global_step=214400, grad_norm=4.763628005981445, loss=0.7154302597045898
I0306 19:33:24.865239 139789005207296 logging_writer.py:48] [214500] global_step=214500, grad_norm=4.503739356994629, loss=0.6027287244796753
I0306 19:33:58.537641 139788996814592 logging_writer.py:48] [214600] global_step=214600, grad_norm=4.753577709197998, loss=0.6396347284317017
I0306 19:34:32.184316 139789005207296 logging_writer.py:48] [214700] global_step=214700, grad_norm=4.477721214294434, loss=0.6244872808456421
I0306 19:35:05.886312 139788996814592 logging_writer.py:48] [214800] global_step=214800, grad_norm=4.281008720397949, loss=0.5917525291442871
I0306 19:35:39.492207 139789005207296 logging_writer.py:48] [214900] global_step=214900, grad_norm=5.076778888702393, loss=0.648555338382721
I0306 19:36:13.142295 139788996814592 logging_writer.py:48] [215000] global_step=215000, grad_norm=4.7776055335998535, loss=0.6358885765075684
I0306 19:36:46.816128 139789005207296 logging_writer.py:48] [215100] global_step=215100, grad_norm=4.482683181762695, loss=0.6214741468429565
I0306 19:37:20.442959 139788996814592 logging_writer.py:48] [215200] global_step=215200, grad_norm=4.745725154876709, loss=0.648958683013916
I0306 19:37:54.138893 139789005207296 logging_writer.py:48] [215300] global_step=215300, grad_norm=4.337217807769775, loss=0.6468244791030884
I0306 19:38:03.388770 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:38:09.439172 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:38:18.137840 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:38:20.449303 139951089751872 submission_runner.py:411] Time since start: 75103.66s, 	Step: 215329, 	{'train/accuracy': 0.9591438174247742, 'train/loss': 0.1498548686504364, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0407342910766602, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.822715163230896, 'test/num_examples': 10000, 'score': 72479.10156702995, 'total_duration': 75103.66161513329, 'accumulated_submission_time': 72479.10156702995, 'accumulated_eval_time': 2607.659962415695, 'accumulated_logging_time': 9.106708526611328}
I0306 19:38:20.502005 139787629491968 logging_writer.py:48] [215329] accumulated_eval_time=2607.659962, accumulated_logging_time=9.106709, accumulated_submission_time=72479.101567, global_step=215329, preemption_count=0, score=72479.101567, test/accuracy=0.631600, test/loss=1.822715, test/num_examples=10000, total_duration=75103.661615, train/accuracy=0.959144, train/loss=0.149855, validation/accuracy=0.757300, validation/loss=1.040734, validation/num_examples=50000
I0306 19:38:44.746253 139789013600000 logging_writer.py:48] [215400] global_step=215400, grad_norm=4.4357123374938965, loss=0.6058835983276367
I0306 19:39:18.393212 139787629491968 logging_writer.py:48] [215500] global_step=215500, grad_norm=4.441239356994629, loss=0.6436901092529297
I0306 19:39:52.042146 139789013600000 logging_writer.py:48] [215600] global_step=215600, grad_norm=4.69219446182251, loss=0.6195996403694153
I0306 19:40:25.727098 139787629491968 logging_writer.py:48] [215700] global_step=215700, grad_norm=4.403578281402588, loss=0.610096275806427
I0306 19:40:59.337228 139789013600000 logging_writer.py:48] [215800] global_step=215800, grad_norm=4.974468231201172, loss=0.6948771476745605
I0306 19:41:33.164837 139787629491968 logging_writer.py:48] [215900] global_step=215900, grad_norm=4.306150913238525, loss=0.6137219667434692
I0306 19:42:06.790427 139789013600000 logging_writer.py:48] [216000] global_step=216000, grad_norm=4.696133136749268, loss=0.6200445294380188
I0306 19:42:40.435955 139787629491968 logging_writer.py:48] [216100] global_step=216100, grad_norm=4.368676662445068, loss=0.5414825677871704
I0306 19:43:14.090679 139789013600000 logging_writer.py:48] [216200] global_step=216200, grad_norm=4.303380012512207, loss=0.5655307173728943
I0306 19:43:47.716112 139787629491968 logging_writer.py:48] [216300] global_step=216300, grad_norm=4.108484268188477, loss=0.5754132866859436
I0306 19:44:21.337127 139789013600000 logging_writer.py:48] [216400] global_step=216400, grad_norm=4.262723922729492, loss=0.5912657976150513
I0306 19:44:54.990734 139787629491968 logging_writer.py:48] [216500] global_step=216500, grad_norm=4.948434352874756, loss=0.6541352272033691
I0306 19:45:28.616130 139789013600000 logging_writer.py:48] [216600] global_step=216600, grad_norm=4.468025207519531, loss=0.6722303032875061
I0306 19:46:02.259255 139787629491968 logging_writer.py:48] [216700] global_step=216700, grad_norm=4.306460857391357, loss=0.6269212365150452
I0306 19:46:35.880197 139789013600000 logging_writer.py:48] [216800] global_step=216800, grad_norm=4.667450428009033, loss=0.6158442497253418
I0306 19:46:50.488216 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:46:56.655870 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:47:05.391568 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:47:07.928058 139951089751872 submission_runner.py:411] Time since start: 75631.14s, 	Step: 216845, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.14571017026901245, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.041617512702942, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8236967325210571, 'test/num_examples': 10000, 'score': 72989.02272677422, 'total_duration': 75631.14029312134, 'accumulated_submission_time': 72989.02272677422, 'accumulated_eval_time': 2625.0996906757355, 'accumulated_logging_time': 9.168915510177612}
I0306 19:47:07.975340 139789005207296 logging_writer.py:48] [216845] accumulated_eval_time=2625.099691, accumulated_logging_time=9.168916, accumulated_submission_time=72989.022727, global_step=216845, preemption_count=0, score=72989.022727, test/accuracy=0.631800, test/loss=1.823697, test/num_examples=10000, total_duration=75631.140293, train/accuracy=0.960878, train/loss=0.145710, validation/accuracy=0.757300, validation/loss=1.041618, validation/num_examples=50000
I0306 19:47:26.947872 139789038778112 logging_writer.py:48] [216900] global_step=216900, grad_norm=4.5963215827941895, loss=0.6720190048217773
I0306 19:48:00.586770 139789005207296 logging_writer.py:48] [217000] global_step=217000, grad_norm=4.425619602203369, loss=0.663244903087616
I0306 19:48:34.230732 139789038778112 logging_writer.py:48] [217100] global_step=217100, grad_norm=4.5926666259765625, loss=0.6705150008201599
I0306 19:49:07.873687 139789005207296 logging_writer.py:48] [217200] global_step=217200, grad_norm=4.5639495849609375, loss=0.6372777819633484
I0306 19:49:41.546232 139789038778112 logging_writer.py:48] [217300] global_step=217300, grad_norm=4.8025922775268555, loss=0.6464798450469971
I0306 19:50:15.169988 139789005207296 logging_writer.py:48] [217400] global_step=217400, grad_norm=4.680747985839844, loss=0.6770586371421814
I0306 19:50:48.827753 139789038778112 logging_writer.py:48] [217500] global_step=217500, grad_norm=4.447537422180176, loss=0.6178761720657349
I0306 19:51:22.448880 139789005207296 logging_writer.py:48] [217600] global_step=217600, grad_norm=4.061549663543701, loss=0.5650182962417603
I0306 19:51:56.089420 139789038778112 logging_writer.py:48] [217700] global_step=217700, grad_norm=3.8934566974639893, loss=0.5772925019264221
I0306 19:52:29.710321 139789005207296 logging_writer.py:48] [217800] global_step=217800, grad_norm=4.27068567276001, loss=0.6811072826385498
I0306 19:53:03.359608 139789038778112 logging_writer.py:48] [217900] global_step=217900, grad_norm=4.377296447753906, loss=0.6101580262184143
I0306 19:53:37.018223 139789005207296 logging_writer.py:48] [218000] global_step=218000, grad_norm=4.2019429206848145, loss=0.6304239630699158
I0306 19:54:10.660721 139789038778112 logging_writer.py:48] [218100] global_step=218100, grad_norm=4.269636154174805, loss=0.5564246773719788
I0306 19:54:44.354836 139789005207296 logging_writer.py:48] [218200] global_step=218200, grad_norm=4.5096964836120605, loss=0.6166602373123169
I0306 19:55:18.010475 139789038778112 logging_writer.py:48] [218300] global_step=218300, grad_norm=4.783189296722412, loss=0.6472691297531128
I0306 19:55:38.004633 139951089751872 spec.py:321] Evaluating on the training split.
I0306 19:55:44.077683 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 19:55:52.725704 139951089751872 spec.py:349] Evaluating on the test split.
I0306 19:55:54.960521 139951089751872 submission_runner.py:411] Time since start: 76158.17s, 	Step: 218361, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.14628644287586212, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0405405759811401, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.821586012840271, 'test/num_examples': 10000, 'score': 73498.98733758926, 'total_duration': 76158.17280721664, 'accumulated_submission_time': 73498.98733758926, 'accumulated_eval_time': 2642.055506706238, 'accumulated_logging_time': 9.225011825561523}
I0306 19:55:55.015819 139787621099264 logging_writer.py:48] [218361] accumulated_eval_time=2642.055507, accumulated_logging_time=9.225012, accumulated_submission_time=73498.987338, global_step=218361, preemption_count=0, score=73498.987338, test/accuracy=0.631600, test/loss=1.821586, test/num_examples=10000, total_duration=76158.172807, train/accuracy=0.960360, train/loss=0.146286, validation/accuracy=0.757340, validation/loss=1.040541, validation/num_examples=50000
I0306 19:56:08.489512 139787629491968 logging_writer.py:48] [218400] global_step=218400, grad_norm=4.315430164337158, loss=0.5858789682388306
I0306 19:56:42.147347 139787621099264 logging_writer.py:48] [218500] global_step=218500, grad_norm=4.340555191040039, loss=0.6422950029373169
I0306 19:57:15.843676 139787629491968 logging_writer.py:48] [218600] global_step=218600, grad_norm=4.721069812774658, loss=0.6499662399291992
I0306 19:57:49.484839 139787621099264 logging_writer.py:48] [218700] global_step=218700, grad_norm=4.6057820320129395, loss=0.6546234488487244
I0306 19:58:23.092146 139787629491968 logging_writer.py:48] [218800] global_step=218800, grad_norm=4.543538570404053, loss=0.6734614968299866
I0306 19:58:56.739782 139787621099264 logging_writer.py:48] [218900] global_step=218900, grad_norm=4.441170692443848, loss=0.6630116701126099
I0306 19:59:30.454653 139787629491968 logging_writer.py:48] [219000] global_step=219000, grad_norm=4.860660076141357, loss=0.649474024772644
I0306 20:00:04.042447 139787621099264 logging_writer.py:48] [219100] global_step=219100, grad_norm=4.892728328704834, loss=0.684053361415863
I0306 20:00:37.706671 139787629491968 logging_writer.py:48] [219200] global_step=219200, grad_norm=4.87025785446167, loss=0.7118459939956665
I0306 20:01:11.321853 139787621099264 logging_writer.py:48] [219300] global_step=219300, grad_norm=4.493138313293457, loss=0.6130719184875488
I0306 20:01:44.971561 139787629491968 logging_writer.py:48] [219400] global_step=219400, grad_norm=4.4029951095581055, loss=0.6299381852149963
I0306 20:02:18.567228 139787621099264 logging_writer.py:48] [219500] global_step=219500, grad_norm=4.8342604637146, loss=0.588819146156311
I0306 20:02:52.213217 139787629491968 logging_writer.py:48] [219600] global_step=219600, grad_norm=4.463945388793945, loss=0.6644138097763062
I0306 20:03:25.846190 139787621099264 logging_writer.py:48] [219700] global_step=219700, grad_norm=4.449869632720947, loss=0.5603473782539368
I0306 20:03:59.506833 139787629491968 logging_writer.py:48] [219800] global_step=219800, grad_norm=4.656780242919922, loss=0.6512500047683716
I0306 20:04:25.186924 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:04:31.280016 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:04:40.115687 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:04:42.370297 139951089751872 submission_runner.py:411] Time since start: 76685.58s, 	Step: 219878, 	{'train/accuracy': 0.9594826102256775, 'train/loss': 0.14756397902965546, 'validation/accuracy': 0.7575199604034424, 'validation/loss': 1.040326476097107, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8223992586135864, 'test/num_examples': 10000, 'score': 74009.09196305275, 'total_duration': 76685.58260345459, 'accumulated_submission_time': 74009.09196305275, 'accumulated_eval_time': 2659.2388412952423, 'accumulated_logging_time': 9.289936542510986}
I0306 20:04:42.423601 139787621099264 logging_writer.py:48] [219878] accumulated_eval_time=2659.238841, accumulated_logging_time=9.289937, accumulated_submission_time=74009.091963, global_step=219878, preemption_count=0, score=74009.091963, test/accuracy=0.631800, test/loss=1.822399, test/num_examples=10000, total_duration=76685.582603, train/accuracy=0.959483, train/loss=0.147564, validation/accuracy=0.757520, validation/loss=1.040326, validation/num_examples=50000
I0306 20:04:50.153251 139789021992704 logging_writer.py:48] [219900] global_step=219900, grad_norm=4.352346420288086, loss=0.5880141854286194
I0306 20:05:23.740325 139787621099264 logging_writer.py:48] [220000] global_step=220000, grad_norm=4.376964569091797, loss=0.6213776469230652
I0306 20:05:57.411716 139789021992704 logging_writer.py:48] [220100] global_step=220100, grad_norm=4.619864463806152, loss=0.5865782499313354
I0306 20:06:30.989228 139787621099264 logging_writer.py:48] [220200] global_step=220200, grad_norm=3.989419937133789, loss=0.5857813358306885
I0306 20:07:04.549590 139789021992704 logging_writer.py:48] [220300] global_step=220300, grad_norm=4.243715763092041, loss=0.626060426235199
I0306 20:07:38.115653 139787621099264 logging_writer.py:48] [220400] global_step=220400, grad_norm=4.824401378631592, loss=0.6071310639381409
I0306 20:08:11.759938 139789021992704 logging_writer.py:48] [220500] global_step=220500, grad_norm=4.24437952041626, loss=0.6372888088226318
I0306 20:08:45.388959 139787621099264 logging_writer.py:48] [220600] global_step=220600, grad_norm=4.796516418457031, loss=0.6349956393241882
I0306 20:09:19.007481 139789021992704 logging_writer.py:48] [220700] global_step=220700, grad_norm=4.644981861114502, loss=0.6516371965408325
I0306 20:09:52.643104 139787621099264 logging_writer.py:48] [220800] global_step=220800, grad_norm=4.455135345458984, loss=0.5982551574707031
I0306 20:10:26.259143 139789021992704 logging_writer.py:48] [220900] global_step=220900, grad_norm=4.751896858215332, loss=0.6676945090293884
I0306 20:10:59.920966 139787621099264 logging_writer.py:48] [221000] global_step=221000, grad_norm=4.258425712585449, loss=0.5806859731674194
I0306 20:11:33.566616 139789021992704 logging_writer.py:48] [221100] global_step=221100, grad_norm=4.518571853637695, loss=0.6214726567268372
I0306 20:12:07.201170 139787621099264 logging_writer.py:48] [221200] global_step=221200, grad_norm=4.5086798667907715, loss=0.6225783824920654
I0306 20:12:40.773395 139789021992704 logging_writer.py:48] [221300] global_step=221300, grad_norm=4.426218509674072, loss=0.6759549975395203
I0306 20:13:12.496757 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:13:18.536535 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:13:27.334900 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:13:29.624218 139951089751872 submission_runner.py:411] Time since start: 77212.84s, 	Step: 221396, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.14820456504821777, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.0406891107559204, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8241325616836548, 'test/num_examples': 10000, 'score': 74519.10008716583, 'total_duration': 77212.83652758598, 'accumulated_submission_time': 74519.10008716583, 'accumulated_eval_time': 2676.3662524223328, 'accumulated_logging_time': 9.35293436050415}
I0306 20:13:29.678242 139787612706560 logging_writer.py:48] [221396] accumulated_eval_time=2676.366252, accumulated_logging_time=9.352934, accumulated_submission_time=74519.100087, global_step=221396, preemption_count=0, score=74519.100087, test/accuracy=0.632300, test/loss=1.824133, test/num_examples=10000, total_duration=77212.836528, train/accuracy=0.959682, train/loss=0.148205, validation/accuracy=0.757500, validation/loss=1.040689, validation/num_examples=50000
I0306 20:13:31.368940 139787621099264 logging_writer.py:48] [221400] global_step=221400, grad_norm=4.8373637199401855, loss=0.6115930080413818
I0306 20:14:04.974604 139787612706560 logging_writer.py:48] [221500] global_step=221500, grad_norm=4.756992816925049, loss=0.6133633852005005
I0306 20:14:38.601298 139787621099264 logging_writer.py:48] [221600] global_step=221600, grad_norm=4.2444047927856445, loss=0.5627208948135376
I0306 20:15:12.268632 139787612706560 logging_writer.py:48] [221700] global_step=221700, grad_norm=4.517577648162842, loss=0.5966556072235107
I0306 20:15:45.875805 139787621099264 logging_writer.py:48] [221800] global_step=221800, grad_norm=4.702019691467285, loss=0.577491044998169
I0306 20:16:19.538164 139787612706560 logging_writer.py:48] [221900] global_step=221900, grad_norm=4.632261276245117, loss=0.7013476490974426
I0306 20:16:53.190311 139787621099264 logging_writer.py:48] [222000] global_step=222000, grad_norm=4.465085506439209, loss=0.6318945288658142
I0306 20:17:26.845648 139787612706560 logging_writer.py:48] [222100] global_step=222100, grad_norm=4.137118339538574, loss=0.6451385021209717
I0306 20:18:00.659972 139787621099264 logging_writer.py:48] [222200] global_step=222200, grad_norm=5.010176181793213, loss=0.5916596055030823
I0306 20:18:34.327290 139787612706560 logging_writer.py:48] [222300] global_step=222300, grad_norm=4.240306854248047, loss=0.615885853767395
I0306 20:19:07.980701 139787621099264 logging_writer.py:48] [222400] global_step=222400, grad_norm=4.646978378295898, loss=0.67071932554245
I0306 20:19:41.671317 139787612706560 logging_writer.py:48] [222500] global_step=222500, grad_norm=4.322449684143066, loss=0.6402270793914795
I0306 20:20:15.270317 139787621099264 logging_writer.py:48] [222600] global_step=222600, grad_norm=4.28128719329834, loss=0.6274893283843994
I0306 20:20:48.921444 139787612706560 logging_writer.py:48] [222700] global_step=222700, grad_norm=4.781731128692627, loss=0.6498624086380005
I0306 20:21:22.522073 139787621099264 logging_writer.py:48] [222800] global_step=222800, grad_norm=4.5040740966796875, loss=0.6240896582603455
I0306 20:21:56.177315 139787612706560 logging_writer.py:48] [222900] global_step=222900, grad_norm=4.623751640319824, loss=0.6089347004890442
I0306 20:21:59.684906 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:22:05.746507 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:22:14.642300 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:22:16.967597 139951089751872 submission_runner.py:411] Time since start: 77740.18s, 	Step: 222912, 	{'train/accuracy': 0.9620137214660645, 'train/loss': 0.14403626322746277, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.0408563613891602, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.822832465171814, 'test/num_examples': 10000, 'score': 75029.03829264641, 'total_duration': 77740.1799068451, 'accumulated_submission_time': 75029.03829264641, 'accumulated_eval_time': 2693.648894548416, 'accumulated_logging_time': 9.417216300964355}
I0306 20:22:17.022308 139787612706560 logging_writer.py:48] [222912] accumulated_eval_time=2693.648895, accumulated_logging_time=9.417216, accumulated_submission_time=75029.038293, global_step=222912, preemption_count=0, score=75029.038293, test/accuracy=0.631400, test/loss=1.822832, test/num_examples=10000, total_duration=77740.179907, train/accuracy=0.962014, train/loss=0.144036, validation/accuracy=0.757500, validation/loss=1.040856, validation/num_examples=50000
I0306 20:22:46.955590 139789005207296 logging_writer.py:48] [223000] global_step=223000, grad_norm=5.018405437469482, loss=0.6319060325622559
I0306 20:23:20.613337 139787612706560 logging_writer.py:48] [223100] global_step=223100, grad_norm=4.356752395629883, loss=0.6632843017578125
I0306 20:23:54.380658 139789005207296 logging_writer.py:48] [223200] global_step=223200, grad_norm=5.0930585861206055, loss=0.6678400039672852
I0306 20:24:28.047303 139787612706560 logging_writer.py:48] [223300] global_step=223300, grad_norm=4.308708190917969, loss=0.6440814137458801
I0306 20:25:01.722229 139789005207296 logging_writer.py:48] [223400] global_step=223400, grad_norm=4.421872615814209, loss=0.6674649715423584
I0306 20:25:35.373293 139787612706560 logging_writer.py:48] [223500] global_step=223500, grad_norm=4.539779186248779, loss=0.6091525554656982
I0306 20:26:09.050629 139789005207296 logging_writer.py:48] [223600] global_step=223600, grad_norm=3.9790046215057373, loss=0.6268019676208496
I0306 20:26:42.704957 139787612706560 logging_writer.py:48] [223700] global_step=223700, grad_norm=4.565459728240967, loss=0.6774581670761108
I0306 20:27:16.338256 139789005207296 logging_writer.py:48] [223800] global_step=223800, grad_norm=4.074317932128906, loss=0.5882269740104675
I0306 20:27:49.989833 139787612706560 logging_writer.py:48] [223900] global_step=223900, grad_norm=4.352591514587402, loss=0.603175699710846
I0306 20:28:23.649880 139789005207296 logging_writer.py:48] [224000] global_step=224000, grad_norm=4.557614326477051, loss=0.7295886874198914
I0306 20:28:57.313277 139787612706560 logging_writer.py:48] [224100] global_step=224100, grad_norm=4.301772117614746, loss=0.561794638633728
I0306 20:29:30.958712 139789005207296 logging_writer.py:48] [224200] global_step=224200, grad_norm=4.24055290222168, loss=0.5411509871482849
I0306 20:30:04.779077 139787612706560 logging_writer.py:48] [224300] global_step=224300, grad_norm=4.650312423706055, loss=0.6496777534484863
I0306 20:30:38.455715 139789005207296 logging_writer.py:48] [224400] global_step=224400, grad_norm=4.603603839874268, loss=0.6230259537696838
I0306 20:30:47.020495 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:30:53.049335 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:31:01.714731 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:31:03.965415 139951089751872 submission_runner.py:411] Time since start: 78267.18s, 	Step: 224427, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.14808064699172974, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.0420169830322266, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8239521980285645, 'test/num_examples': 10000, 'score': 75538.97178125381, 'total_duration': 78267.17767620087, 'accumulated_submission_time': 75538.97178125381, 'accumulated_eval_time': 2710.5937311649323, 'accumulated_logging_time': 9.481563091278076}
I0306 20:31:04.023198 139788996814592 logging_writer.py:48] [224427] accumulated_eval_time=2710.593731, accumulated_logging_time=9.481563, accumulated_submission_time=75538.971781, global_step=224427, preemption_count=0, score=75538.971781, test/accuracy=0.631000, test/loss=1.823952, test/num_examples=10000, total_duration=78267.177676, train/accuracy=0.960240, train/loss=0.148081, validation/accuracy=0.757000, validation/loss=1.042017, validation/num_examples=50000
I0306 20:31:28.882706 139789013600000 logging_writer.py:48] [224500] global_step=224500, grad_norm=4.45711612701416, loss=0.5871384143829346
I0306 20:32:02.489675 139788996814592 logging_writer.py:48] [224600] global_step=224600, grad_norm=4.116832256317139, loss=0.5724365711212158
I0306 20:32:36.156904 139789013600000 logging_writer.py:48] [224700] global_step=224700, grad_norm=4.446425437927246, loss=0.6018511056900024
I0306 20:33:09.822027 139788996814592 logging_writer.py:48] [224800] global_step=224800, grad_norm=4.338514804840088, loss=0.6251181364059448
I0306 20:33:43.465603 139789013600000 logging_writer.py:48] [224900] global_step=224900, grad_norm=4.591790676116943, loss=0.6288546323776245
I0306 20:34:17.123022 139788996814592 logging_writer.py:48] [225000] global_step=225000, grad_norm=4.178377628326416, loss=0.5537643432617188
I0306 20:34:50.768846 139789013600000 logging_writer.py:48] [225100] global_step=225100, grad_norm=3.988690137863159, loss=0.5679733753204346
I0306 20:35:24.425852 139788996814592 logging_writer.py:48] [225200] global_step=225200, grad_norm=4.676844120025635, loss=0.6249551773071289
I0306 20:35:58.086374 139789013600000 logging_writer.py:48] [225300] global_step=225300, grad_norm=4.764778137207031, loss=0.6417995691299438
I0306 20:36:31.784609 139788996814592 logging_writer.py:48] [225400] global_step=225400, grad_norm=4.46481466293335, loss=0.6173475980758667
I0306 20:37:05.376325 139789013600000 logging_writer.py:48] [225500] global_step=225500, grad_norm=4.097624778747559, loss=0.5754468441009521
I0306 20:37:39.027580 139788996814592 logging_writer.py:48] [225600] global_step=225600, grad_norm=4.8146562576293945, loss=0.6507607698440552
I0306 20:38:12.638807 139789013600000 logging_writer.py:48] [225700] global_step=225700, grad_norm=4.309972763061523, loss=0.591880202293396
I0306 20:38:46.275104 139788996814592 logging_writer.py:48] [225800] global_step=225800, grad_norm=4.487363815307617, loss=0.6341201663017273
I0306 20:39:19.879761 139789013600000 logging_writer.py:48] [225900] global_step=225900, grad_norm=4.185077667236328, loss=0.5568028688430786
I0306 20:39:34.157183 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:39:40.219687 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:39:49.102289 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:39:51.387804 139951089751872 submission_runner.py:411] Time since start: 78794.60s, 	Step: 225944, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14447066187858582, 'validation/accuracy': 0.7567799687385559, 'validation/loss': 1.0402354001998901, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8221471309661865, 'test/num_examples': 10000, 'score': 76049.03899121284, 'total_duration': 78794.60010194778, 'accumulated_submission_time': 76049.03899121284, 'accumulated_eval_time': 2727.824291229248, 'accumulated_logging_time': 9.551263332366943}
I0306 20:39:51.446665 139787621099264 logging_writer.py:48] [225944] accumulated_eval_time=2727.824291, accumulated_logging_time=9.551263, accumulated_submission_time=76049.038991, global_step=225944, preemption_count=0, score=76049.038991, test/accuracy=0.631400, test/loss=1.822147, test/num_examples=10000, total_duration=78794.600102, train/accuracy=0.960798, train/loss=0.144471, validation/accuracy=0.756780, validation/loss=1.040235, validation/num_examples=50000
I0306 20:40:10.643746 139787629491968 logging_writer.py:48] [226000] global_step=226000, grad_norm=4.59562873840332, loss=0.6063692569732666
I0306 20:40:44.267361 139787621099264 logging_writer.py:48] [226100] global_step=226100, grad_norm=4.307374477386475, loss=0.6017643213272095
I0306 20:41:17.870204 139787629491968 logging_writer.py:48] [226200] global_step=226200, grad_norm=4.603095531463623, loss=0.6343812942504883
I0306 20:41:51.446104 139787621099264 logging_writer.py:48] [226300] global_step=226300, grad_norm=4.220223426818848, loss=0.5949552655220032
I0306 20:42:25.075353 139787629491968 logging_writer.py:48] [226400] global_step=226400, grad_norm=4.06382942199707, loss=0.5882558822631836
I0306 20:42:58.671800 139787621099264 logging_writer.py:48] [226500] global_step=226500, grad_norm=4.754323482513428, loss=0.638594388961792
I0306 20:43:32.290406 139787629491968 logging_writer.py:48] [226600] global_step=226600, grad_norm=4.335072994232178, loss=0.5925631523132324
I0306 20:44:05.997342 139787621099264 logging_writer.py:48] [226700] global_step=226700, grad_norm=4.392290115356445, loss=0.6300159692764282
I0306 20:44:39.649079 139787629491968 logging_writer.py:48] [226800] global_step=226800, grad_norm=4.2189178466796875, loss=0.5972923636436462
I0306 20:45:13.327189 139787621099264 logging_writer.py:48] [226900] global_step=226900, grad_norm=4.485537528991699, loss=0.648525595664978
I0306 20:45:46.995994 139787629491968 logging_writer.py:48] [227000] global_step=227000, grad_norm=4.761630535125732, loss=0.7029315829277039
I0306 20:46:20.657463 139787621099264 logging_writer.py:48] [227100] global_step=227100, grad_norm=4.6780686378479, loss=0.6485382914543152
I0306 20:46:54.279439 139787629491968 logging_writer.py:48] [227200] global_step=227200, grad_norm=4.003835678100586, loss=0.559436559677124
I0306 20:47:27.909865 139787621099264 logging_writer.py:48] [227300] global_step=227300, grad_norm=4.417421340942383, loss=0.5884653925895691
I0306 20:48:01.573616 139787629491968 logging_writer.py:48] [227400] global_step=227400, grad_norm=4.247025489807129, loss=0.5776821970939636
I0306 20:48:21.393589 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:48:27.448031 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:48:36.139357 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:48:38.395181 139951089751872 submission_runner.py:411] Time since start: 79321.61s, 	Step: 227460, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14311087131500244, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.040708303451538, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.820837140083313, 'test/num_examples': 10000, 'score': 76558.91853904724, 'total_duration': 79321.60742902756, 'accumulated_submission_time': 76558.91853904724, 'accumulated_eval_time': 2744.825801372528, 'accumulated_logging_time': 9.621059656143188}
I0306 20:48:38.456392 139789013600000 logging_writer.py:48] [227460] accumulated_eval_time=2744.825801, accumulated_logging_time=9.621060, accumulated_submission_time=76558.918539, global_step=227460, preemption_count=0, score=76558.918539, test/accuracy=0.631200, test/loss=1.820837, test/num_examples=10000, total_duration=79321.607429, train/accuracy=0.961834, train/loss=0.143111, validation/accuracy=0.757200, validation/loss=1.040708, validation/num_examples=50000
I0306 20:48:53.038774 139789021992704 logging_writer.py:48] [227500] global_step=227500, grad_norm=5.013036727905273, loss=0.6836335062980652
I0306 20:49:26.631229 139789013600000 logging_writer.py:48] [227600] global_step=227600, grad_norm=4.624908447265625, loss=0.5850415229797363
I0306 20:50:00.252872 139789021992704 logging_writer.py:48] [227700] global_step=227700, grad_norm=4.671591281890869, loss=0.625124990940094
I0306 20:50:33.850862 139789013600000 logging_writer.py:48] [227800] global_step=227800, grad_norm=4.991086483001709, loss=0.6001614332199097
I0306 20:51:07.493702 139789021992704 logging_writer.py:48] [227900] global_step=227900, grad_norm=4.468875408172607, loss=0.6275635361671448
I0306 20:51:41.104140 139789013600000 logging_writer.py:48] [228000] global_step=228000, grad_norm=4.559443950653076, loss=0.5953187346458435
I0306 20:52:14.766009 139789021992704 logging_writer.py:48] [228100] global_step=228100, grad_norm=4.262248992919922, loss=0.625076949596405
I0306 20:52:48.406946 139789013600000 logging_writer.py:48] [228200] global_step=228200, grad_norm=4.409844398498535, loss=0.6743901968002319
I0306 20:53:22.068803 139789021992704 logging_writer.py:48] [228300] global_step=228300, grad_norm=4.933679103851318, loss=0.6456494331359863
I0306 20:53:55.674973 139789013600000 logging_writer.py:48] [228400] global_step=228400, grad_norm=4.720917224884033, loss=0.6919137239456177
I0306 20:54:29.415766 139789021992704 logging_writer.py:48] [228500] global_step=228500, grad_norm=4.340677738189697, loss=0.6467316150665283
I0306 20:55:03.000450 139789013600000 logging_writer.py:48] [228600] global_step=228600, grad_norm=4.6169562339782715, loss=0.6330011487007141
I0306 20:55:36.574155 139789021992704 logging_writer.py:48] [228700] global_step=228700, grad_norm=4.563540935516357, loss=0.5986215472221375
I0306 20:56:10.175283 139789013600000 logging_writer.py:48] [228800] global_step=228800, grad_norm=4.238921642303467, loss=0.5650572776794434
I0306 20:56:43.839967 139789021992704 logging_writer.py:48] [228900] global_step=228900, grad_norm=4.826303005218506, loss=0.6480423212051392
I0306 20:57:08.553795 139951089751872 spec.py:321] Evaluating on the training split.
I0306 20:57:14.585179 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 20:57:23.429393 139951089751872 spec.py:349] Evaluating on the test split.
I0306 20:57:25.733412 139951089751872 submission_runner.py:411] Time since start: 79848.95s, 	Step: 228975, 	{'train/accuracy': 0.9617745280265808, 'train/loss': 0.14527709782123566, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.039811372756958, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8225457668304443, 'test/num_examples': 10000, 'score': 77068.11691904068, 'total_duration': 79848.94569897652, 'accumulated_submission_time': 77068.11691904068, 'accumulated_eval_time': 2762.005347251892, 'accumulated_logging_time': 10.524330615997314}
I0306 20:57:25.796156 139787621099264 logging_writer.py:48] [228975] accumulated_eval_time=2762.005347, accumulated_logging_time=10.524331, accumulated_submission_time=77068.116919, global_step=228975, preemption_count=0, score=77068.116919, test/accuracy=0.630900, test/loss=1.822546, test/num_examples=10000, total_duration=79848.945699, train/accuracy=0.961775, train/loss=0.145277, validation/accuracy=0.757080, validation/loss=1.039811, validation/num_examples=50000
I0306 20:57:34.523157 139787629491968 logging_writer.py:48] [229000] global_step=229000, grad_norm=4.44647741317749, loss=0.6439394950866699
I0306 20:58:08.133419 139787621099264 logging_writer.py:48] [229100] global_step=229100, grad_norm=4.61572790145874, loss=0.6792832016944885
I0306 20:58:41.703919 139787629491968 logging_writer.py:48] [229200] global_step=229200, grad_norm=4.379512786865234, loss=0.6044309139251709
I0306 20:59:15.297321 139787621099264 logging_writer.py:48] [229300] global_step=229300, grad_norm=4.546127796173096, loss=0.7355314493179321
I0306 20:59:48.943690 139787629491968 logging_writer.py:48] [229400] global_step=229400, grad_norm=4.4344258308410645, loss=0.601081132888794
I0306 21:00:22.567390 139787621099264 logging_writer.py:48] [229500] global_step=229500, grad_norm=4.563037395477295, loss=0.6451858282089233
I0306 21:00:56.225007 139787629491968 logging_writer.py:48] [229600] global_step=229600, grad_norm=4.461813449859619, loss=0.6490833759307861
I0306 21:01:29.862353 139787621099264 logging_writer.py:48] [229700] global_step=229700, grad_norm=4.419673919677734, loss=0.5743987560272217
I0306 21:02:03.574008 139787629491968 logging_writer.py:48] [229800] global_step=229800, grad_norm=4.150529384613037, loss=0.6203181743621826
I0306 21:02:37.198597 139787621099264 logging_writer.py:48] [229900] global_step=229900, grad_norm=4.195569038391113, loss=0.5804027318954468
I0306 21:03:10.833306 139787629491968 logging_writer.py:48] [230000] global_step=230000, grad_norm=4.0836052894592285, loss=0.6231869459152222
I0306 21:03:44.464737 139787621099264 logging_writer.py:48] [230100] global_step=230100, grad_norm=4.493719577789307, loss=0.6283779144287109
I0306 21:04:18.107956 139787629491968 logging_writer.py:48] [230200] global_step=230200, grad_norm=4.068549156188965, loss=0.5625097155570984
I0306 21:04:51.751679 139787621099264 logging_writer.py:48] [230300] global_step=230300, grad_norm=4.591585636138916, loss=0.5517822504043579
I0306 21:05:25.384033 139787629491968 logging_writer.py:48] [230400] global_step=230400, grad_norm=4.2593488693237305, loss=0.6297516226768494
I0306 21:05:55.807895 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:06:02.487835 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:06:11.262233 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:06:13.520762 139951089751872 submission_runner.py:411] Time since start: 80376.73s, 	Step: 230492, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.1454789787530899, 'validation/accuracy': 0.7569800019264221, 'validation/loss': 1.0405935049057007, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8216196298599243, 'test/num_examples': 10000, 'score': 77578.0630581379, 'total_duration': 80376.73307228088, 'accumulated_submission_time': 77578.0630581379, 'accumulated_eval_time': 2779.7181718349457, 'accumulated_logging_time': 10.597296953201294}
I0306 21:06:13.576632 139787612706560 logging_writer.py:48] [230492] accumulated_eval_time=2779.718172, accumulated_logging_time=10.597297, accumulated_submission_time=77578.063058, global_step=230492, preemption_count=0, score=77578.063058, test/accuracy=0.630900, test/loss=1.821620, test/num_examples=10000, total_duration=80376.733072, train/accuracy=0.960957, train/loss=0.145479, validation/accuracy=0.756980, validation/loss=1.040594, validation/num_examples=50000
I0306 21:06:16.612512 139787621099264 logging_writer.py:48] [230500] global_step=230500, grad_norm=4.923334121704102, loss=0.7401648759841919
I0306 21:06:50.281811 139787612706560 logging_writer.py:48] [230600] global_step=230600, grad_norm=3.972947120666504, loss=0.5664031505584717
I0306 21:07:23.882093 139787621099264 logging_writer.py:48] [230700] global_step=230700, grad_norm=5.142667770385742, loss=0.5672837495803833
I0306 21:07:57.549667 139787612706560 logging_writer.py:48] [230800] global_step=230800, grad_norm=4.606079578399658, loss=0.671259880065918
I0306 21:08:31.184687 139787621099264 logging_writer.py:48] [230900] global_step=230900, grad_norm=4.526014804840088, loss=0.6771247982978821
I0306 21:09:04.833139 139787612706560 logging_writer.py:48] [231000] global_step=231000, grad_norm=4.363704204559326, loss=0.6349213123321533
I0306 21:09:38.433837 139787621099264 logging_writer.py:48] [231100] global_step=231100, grad_norm=4.250037670135498, loss=0.6626484990119934
I0306 21:10:12.112298 139787612706560 logging_writer.py:48] [231200] global_step=231200, grad_norm=4.550189971923828, loss=0.6063253879547119
I0306 21:10:45.737264 139787621099264 logging_writer.py:48] [231300] global_step=231300, grad_norm=4.342054843902588, loss=0.5903398394584656
I0306 21:11:19.385249 139787612706560 logging_writer.py:48] [231400] global_step=231400, grad_norm=5.110219955444336, loss=0.6480040550231934
I0306 21:11:52.986428 139787621099264 logging_writer.py:48] [231500] global_step=231500, grad_norm=3.9606401920318604, loss=0.5546146035194397
I0306 21:12:26.663706 139787612706560 logging_writer.py:48] [231600] global_step=231600, grad_norm=4.324130058288574, loss=0.6156296133995056
I0306 21:13:00.310777 139787621099264 logging_writer.py:48] [231700] global_step=231700, grad_norm=4.68755578994751, loss=0.6096833348274231
I0306 21:13:33.892625 139787612706560 logging_writer.py:48] [231800] global_step=231800, grad_norm=4.784286022186279, loss=0.645433783531189
I0306 21:14:07.491991 139787621099264 logging_writer.py:48] [231900] global_step=231900, grad_norm=4.390735149383545, loss=0.5936465263366699
I0306 21:14:41.128731 139787612706560 logging_writer.py:48] [232000] global_step=232000, grad_norm=4.640419006347656, loss=0.6299857497215271
I0306 21:14:43.635255 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:14:49.716563 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:14:58.489182 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:15:00.748988 139951089751872 submission_runner.py:411] Time since start: 80903.96s, 	Step: 232009, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14939545094966888, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.041401743888855, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8230193853378296, 'test/num_examples': 10000, 'score': 78088.05710411072, 'total_duration': 80903.96130156517, 'accumulated_submission_time': 78088.05710411072, 'accumulated_eval_time': 2796.831855535507, 'accumulated_logging_time': 10.66297459602356}
I0306 21:15:00.804954 139787612706560 logging_writer.py:48] [232009] accumulated_eval_time=2796.831856, accumulated_logging_time=10.662975, accumulated_submission_time=78088.057104, global_step=232009, preemption_count=0, score=78088.057104, test/accuracy=0.631700, test/loss=1.823019, test/num_examples=10000, total_duration=80903.961302, train/accuracy=0.960001, train/loss=0.149395, validation/accuracy=0.757020, validation/loss=1.041402, validation/num_examples=50000
I0306 21:15:31.715712 139788996814592 logging_writer.py:48] [232100] global_step=232100, grad_norm=4.474042892456055, loss=0.6565227508544922
I0306 21:16:05.295640 139787612706560 logging_writer.py:48] [232200] global_step=232200, grad_norm=4.2603607177734375, loss=0.6194486618041992
I0306 21:16:38.941262 139788996814592 logging_writer.py:48] [232300] global_step=232300, grad_norm=4.646121025085449, loss=0.6575382947921753
I0306 21:17:12.593042 139787612706560 logging_writer.py:48] [232400] global_step=232400, grad_norm=4.502184867858887, loss=0.6272701621055603
I0306 21:17:46.228585 139788996814592 logging_writer.py:48] [232500] global_step=232500, grad_norm=4.909065246582031, loss=0.6217541694641113
I0306 21:18:19.878334 139787612706560 logging_writer.py:48] [232600] global_step=232600, grad_norm=4.5110063552856445, loss=0.6565280556678772
I0306 21:18:53.533131 139788996814592 logging_writer.py:48] [232700] global_step=232700, grad_norm=5.026137828826904, loss=0.6385418176651001
I0306 21:19:27.179179 139787612706560 logging_writer.py:48] [232800] global_step=232800, grad_norm=4.760828971862793, loss=0.6882299780845642
I0306 21:20:00.825927 139788996814592 logging_writer.py:48] [232900] global_step=232900, grad_norm=4.943516731262207, loss=0.6642303466796875
I0306 21:20:34.435456 139787612706560 logging_writer.py:48] [233000] global_step=233000, grad_norm=5.145168781280518, loss=0.7584372758865356
I0306 21:21:08.067324 139788996814592 logging_writer.py:48] [233100] global_step=233100, grad_norm=4.634242534637451, loss=0.6338674426078796
I0306 21:21:41.676963 139787612706560 logging_writer.py:48] [233200] global_step=233200, grad_norm=4.464913368225098, loss=0.6180732846260071
I0306 21:22:15.323213 139788996814592 logging_writer.py:48] [233300] global_step=233300, grad_norm=4.479150295257568, loss=0.6619035005569458
I0306 21:22:48.950929 139787612706560 logging_writer.py:48] [233400] global_step=233400, grad_norm=4.774259567260742, loss=0.6380698084831238
I0306 21:23:22.583276 139788996814592 logging_writer.py:48] [233500] global_step=233500, grad_norm=4.386314868927002, loss=0.6290889978408813
I0306 21:23:30.802435 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:23:36.785059 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:23:45.614554 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:23:47.905184 139951089751872 submission_runner.py:411] Time since start: 81431.12s, 	Step: 233526, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14444713294506073, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.0412191152572632, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8219051361083984, 'test/num_examples': 10000, 'score': 78597.98800444603, 'total_duration': 81431.11747145653, 'accumulated_submission_time': 78597.98800444603, 'accumulated_eval_time': 2813.9345309734344, 'accumulated_logging_time': 10.729990720748901}
I0306 21:23:47.973532 139789013600000 logging_writer.py:48] [233526] accumulated_eval_time=2813.934531, accumulated_logging_time=10.729991, accumulated_submission_time=78597.988004, global_step=233526, preemption_count=0, score=78597.988004, test/accuracy=0.631300, test/loss=1.821905, test/num_examples=10000, total_duration=81431.117471, train/accuracy=0.960818, train/loss=0.144447, validation/accuracy=0.757000, validation/loss=1.041219, validation/num_examples=50000
I0306 21:24:13.161023 139789021992704 logging_writer.py:48] [233600] global_step=233600, grad_norm=4.466038703918457, loss=0.6149537563323975
I0306 21:24:46.734047 139789013600000 logging_writer.py:48] [233700] global_step=233700, grad_norm=4.786866664886475, loss=0.7023121118545532
I0306 21:25:20.365231 139789021992704 logging_writer.py:48] [233800] global_step=233800, grad_norm=4.301166534423828, loss=0.6291793584823608
I0306 21:25:53.971088 139789013600000 logging_writer.py:48] [233900] global_step=233900, grad_norm=4.779651641845703, loss=0.6624470949172974
I0306 21:26:27.548679 139789021992704 logging_writer.py:48] [234000] global_step=234000, grad_norm=4.423828125, loss=0.5253819227218628
I0306 21:27:01.209871 139789013600000 logging_writer.py:48] [234100] global_step=234100, grad_norm=4.7285919189453125, loss=0.6492577195167542
I0306 21:27:34.809612 139789021992704 logging_writer.py:48] [234200] global_step=234200, grad_norm=4.195314884185791, loss=0.5611148476600647
I0306 21:28:08.471565 139789013600000 logging_writer.py:48] [234300] global_step=234300, grad_norm=4.511541366577148, loss=0.6588013172149658
I0306 21:28:42.119027 139789021992704 logging_writer.py:48] [234400] global_step=234400, grad_norm=4.6368865966796875, loss=0.6279754042625427
I0306 21:29:15.770075 139789013600000 logging_writer.py:48] [234500] global_step=234500, grad_norm=4.678023815155029, loss=0.6143105030059814
I0306 21:29:49.385685 139789021992704 logging_writer.py:48] [234600] global_step=234600, grad_norm=4.969226837158203, loss=0.6464428305625916
I0306 21:30:23.042935 139789013600000 logging_writer.py:48] [234700] global_step=234700, grad_norm=4.731362819671631, loss=0.5867161154747009
I0306 21:30:56.657290 139789021992704 logging_writer.py:48] [234800] global_step=234800, grad_norm=5.096251964569092, loss=0.5704343318939209
I0306 21:31:30.319912 139789013600000 logging_writer.py:48] [234900] global_step=234900, grad_norm=4.597326755523682, loss=0.6597260236740112
I0306 21:32:03.921337 139789021992704 logging_writer.py:48] [235000] global_step=235000, grad_norm=4.401731967926025, loss=0.583168625831604
I0306 21:32:18.203054 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:32:24.239774 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:32:32.986618 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:32:35.257079 139951089751872 submission_runner.py:411] Time since start: 81958.47s, 	Step: 235044, 	{'train/accuracy': 0.9616948366165161, 'train/loss': 0.14319097995758057, 'validation/accuracy': 0.7575399875640869, 'validation/loss': 1.041340708732605, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8218598365783691, 'test/num_examples': 10000, 'score': 79108.1516199112, 'total_duration': 81958.46935725212, 'accumulated_submission_time': 79108.1516199112, 'accumulated_eval_time': 2830.9884712696075, 'accumulated_logging_time': 10.809279441833496}
I0306 21:32:35.314825 139787621099264 logging_writer.py:48] [235044] accumulated_eval_time=2830.988471, accumulated_logging_time=10.809279, accumulated_submission_time=79108.151620, global_step=235044, preemption_count=0, score=79108.151620, test/accuracy=0.631700, test/loss=1.821860, test/num_examples=10000, total_duration=81958.469357, train/accuracy=0.961695, train/loss=0.143191, validation/accuracy=0.757540, validation/loss=1.041341, validation/num_examples=50000
I0306 21:32:54.519155 139787629491968 logging_writer.py:48] [235100] global_step=235100, grad_norm=4.556327819824219, loss=0.6963678002357483
I0306 21:33:28.129579 139787621099264 logging_writer.py:48] [235200] global_step=235200, grad_norm=4.656925201416016, loss=0.7563099265098572
I0306 21:34:01.677848 139787629491968 logging_writer.py:48] [235300] global_step=235300, grad_norm=4.256581783294678, loss=0.6180797815322876
I0306 21:34:35.237498 139787621099264 logging_writer.py:48] [235400] global_step=235400, grad_norm=4.287562847137451, loss=0.6311615109443665
I0306 21:35:08.832656 139787629491968 logging_writer.py:48] [235500] global_step=235500, grad_norm=4.41379976272583, loss=0.6322621703147888
I0306 21:35:42.463079 139787621099264 logging_writer.py:48] [235600] global_step=235600, grad_norm=4.631585121154785, loss=0.6564149260520935
I0306 21:36:16.120368 139787629491968 logging_writer.py:48] [235700] global_step=235700, grad_norm=4.865594863891602, loss=0.6486354470252991
I0306 21:36:49.751722 139787621099264 logging_writer.py:48] [235800] global_step=235800, grad_norm=4.625136375427246, loss=0.6307381987571716
I0306 21:37:23.446455 139787629491968 logging_writer.py:48] [235900] global_step=235900, grad_norm=4.5760955810546875, loss=0.5688994526863098
I0306 21:37:57.056719 139787621099264 logging_writer.py:48] [236000] global_step=236000, grad_norm=4.32780647277832, loss=0.565430760383606
I0306 21:38:30.718488 139787629491968 logging_writer.py:48] [236100] global_step=236100, grad_norm=4.5420355796813965, loss=0.6975178718566895
I0306 21:39:04.374971 139787621099264 logging_writer.py:48] [236200] global_step=236200, grad_norm=4.349685192108154, loss=0.6592461466789246
I0306 21:39:38.023587 139787629491968 logging_writer.py:48] [236300] global_step=236300, grad_norm=4.321075916290283, loss=0.5964021682739258
I0306 21:40:11.684115 139787621099264 logging_writer.py:48] [236400] global_step=236400, grad_norm=4.337491989135742, loss=0.6001039743423462
I0306 21:40:45.339396 139787629491968 logging_writer.py:48] [236500] global_step=236500, grad_norm=4.524994850158691, loss=0.6393048167228699
I0306 21:41:05.362204 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:41:11.367825 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:41:20.210460 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:41:22.440233 139951089751872 submission_runner.py:411] Time since start: 82485.65s, 	Step: 236561, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1462259292602539, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0412096977233887, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8224035501480103, 'test/num_examples': 10000, 'score': 79618.13293385506, 'total_duration': 82485.65254235268, 'accumulated_submission_time': 79618.13293385506, 'accumulated_eval_time': 2848.0664477348328, 'accumulated_logging_time': 10.878455400466919}
I0306 21:41:22.498876 139789013600000 logging_writer.py:48] [236561] accumulated_eval_time=2848.066448, accumulated_logging_time=10.878455, accumulated_submission_time=79618.132934, global_step=236561, preemption_count=0, score=79618.132934, test/accuracy=0.630400, test/loss=1.822404, test/num_examples=10000, total_duration=82485.652542, train/accuracy=0.961217, train/loss=0.146226, validation/accuracy=0.757180, validation/loss=1.041210, validation/num_examples=50000
I0306 21:41:35.962392 139789021992704 logging_writer.py:48] [236600] global_step=236600, grad_norm=4.844936370849609, loss=0.6178564429283142
I0306 21:42:09.622633 139789013600000 logging_writer.py:48] [236700] global_step=236700, grad_norm=4.437962532043457, loss=0.6469350457191467
I0306 21:42:43.187238 139789021992704 logging_writer.py:48] [236800] global_step=236800, grad_norm=4.302552223205566, loss=0.538550078868866
I0306 21:43:16.805263 139789013600000 logging_writer.py:48] [236900] global_step=236900, grad_norm=4.695225715637207, loss=0.6576470732688904
I0306 21:43:50.515290 139789021992704 logging_writer.py:48] [237000] global_step=237000, grad_norm=5.059706687927246, loss=0.6803297996520996
I0306 21:44:24.126894 139789013600000 logging_writer.py:48] [237100] global_step=237100, grad_norm=4.077402114868164, loss=0.5470179915428162
I0306 21:44:57.796406 139789021992704 logging_writer.py:48] [237200] global_step=237200, grad_norm=4.296308994293213, loss=0.6426212191581726
I0306 21:45:31.423515 139789013600000 logging_writer.py:48] [237300] global_step=237300, grad_norm=4.3389201164245605, loss=0.6284632682800293
I0306 21:46:05.077224 139789021992704 logging_writer.py:48] [237400] global_step=237400, grad_norm=4.25038480758667, loss=0.640555202960968
I0306 21:46:38.697898 139789013600000 logging_writer.py:48] [237500] global_step=237500, grad_norm=4.350698947906494, loss=0.681601881980896
I0306 21:47:12.341656 139789021992704 logging_writer.py:48] [237600] global_step=237600, grad_norm=4.68330192565918, loss=0.6534982323646545
I0306 21:47:45.971469 139789013600000 logging_writer.py:48] [237700] global_step=237700, grad_norm=5.060793399810791, loss=0.6710900068283081
I0306 21:48:19.618139 139789021992704 logging_writer.py:48] [237800] global_step=237800, grad_norm=4.442963123321533, loss=0.6775810718536377
I0306 21:48:53.242607 139789013600000 logging_writer.py:48] [237900] global_step=237900, grad_norm=4.33992862701416, loss=0.5625833868980408
I0306 21:49:26.899886 139789021992704 logging_writer.py:48] [238000] global_step=238000, grad_norm=4.73578405380249, loss=0.69804447889328
I0306 21:49:52.649373 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:49:58.672602 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:50:07.493023 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:50:09.731137 139951089751872 submission_runner.py:411] Time since start: 83012.94s, 	Step: 238078, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14642657339572906, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0422078371047974, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8252544403076172, 'test/num_examples': 10000, 'score': 80128.2179043293, 'total_duration': 83012.94344353676, 'accumulated_submission_time': 80128.2179043293, 'accumulated_eval_time': 2865.148155927658, 'accumulated_logging_time': 10.948329448699951}
I0306 21:50:09.792477 139787612706560 logging_writer.py:48] [238078] accumulated_eval_time=2865.148156, accumulated_logging_time=10.948329, accumulated_submission_time=80128.217904, global_step=238078, preemption_count=0, score=80128.217904, test/accuracy=0.630900, test/loss=1.825254, test/num_examples=10000, total_duration=83012.943444, train/accuracy=0.961416, train/loss=0.146427, validation/accuracy=0.757160, validation/loss=1.042208, validation/num_examples=50000
I0306 21:50:17.504576 139787621099264 logging_writer.py:48] [238100] global_step=238100, grad_norm=4.280961036682129, loss=0.5828117728233337
I0306 21:50:51.099053 139787612706560 logging_writer.py:48] [238200] global_step=238200, grad_norm=4.610694408416748, loss=0.6112012267112732
I0306 21:51:24.718662 139787621099264 logging_writer.py:48] [238300] global_step=238300, grad_norm=4.407872200012207, loss=0.6302694082260132
I0306 21:51:58.362114 139787612706560 logging_writer.py:48] [238400] global_step=238400, grad_norm=4.390584468841553, loss=0.6541438102722168
I0306 21:52:31.995810 139787621099264 logging_writer.py:48] [238500] global_step=238500, grad_norm=4.785810470581055, loss=0.6369728446006775
I0306 21:53:05.648151 139787612706560 logging_writer.py:48] [238600] global_step=238600, grad_norm=4.943087577819824, loss=0.7000389695167542
I0306 21:53:39.303188 139787621099264 logging_writer.py:48] [238700] global_step=238700, grad_norm=4.403387069702148, loss=0.6320185661315918
I0306 21:54:12.912777 139787612706560 logging_writer.py:48] [238800] global_step=238800, grad_norm=4.646498203277588, loss=0.6722857356071472
I0306 21:54:46.559768 139787621099264 logging_writer.py:48] [238900] global_step=238900, grad_norm=4.752071380615234, loss=0.5998559594154358
I0306 21:55:20.186509 139787612706560 logging_writer.py:48] [239000] global_step=239000, grad_norm=4.401443004608154, loss=0.549998939037323
I0306 21:55:53.856694 139787621099264 logging_writer.py:48] [239100] global_step=239100, grad_norm=4.640887260437012, loss=0.6043664216995239
I0306 21:56:27.453290 139787612706560 logging_writer.py:48] [239200] global_step=239200, grad_norm=4.37834358215332, loss=0.6270683407783508
I0306 21:57:01.121649 139787621099264 logging_writer.py:48] [239300] global_step=239300, grad_norm=4.670915126800537, loss=0.6019698977470398
I0306 21:57:34.741595 139787612706560 logging_writer.py:48] [239400] global_step=239400, grad_norm=4.426171779632568, loss=0.6268272995948792
I0306 21:58:08.391777 139787621099264 logging_writer.py:48] [239500] global_step=239500, grad_norm=4.328908443450928, loss=0.6440972089767456
I0306 21:58:39.807691 139951089751872 spec.py:321] Evaluating on the training split.
I0306 21:58:45.806072 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 21:58:54.517156 139951089751872 spec.py:349] Evaluating on the test split.
I0306 21:58:56.833161 139951089751872 submission_runner.py:411] Time since start: 83540.05s, 	Step: 239595, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.1480749398469925, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0406923294067383, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8220387697219849, 'test/num_examples': 10000, 'score': 80638.16833734512, 'total_duration': 83540.04543042183, 'accumulated_submission_time': 80638.16833734512, 'accumulated_eval_time': 2882.1735339164734, 'accumulated_logging_time': 11.02028512954712}
I0306 21:58:56.907080 139787629491968 logging_writer.py:48] [239595] accumulated_eval_time=2882.173534, accumulated_logging_time=11.020285, accumulated_submission_time=80638.168337, global_step=239595, preemption_count=0, score=80638.168337, test/accuracy=0.632400, test/loss=1.822039, test/num_examples=10000, total_duration=83540.045430, train/accuracy=0.959682, train/loss=0.148075, validation/accuracy=0.757020, validation/loss=1.040692, validation/num_examples=50000
I0306 21:58:58.928420 139789013600000 logging_writer.py:48] [239600] global_step=239600, grad_norm=4.506078243255615, loss=0.6082800626754761
I0306 21:59:32.590329 139787629491968 logging_writer.py:48] [239700] global_step=239700, grad_norm=4.605515956878662, loss=0.6216357946395874
I0306 22:00:06.253741 139789013600000 logging_writer.py:48] [239800] global_step=239800, grad_norm=4.3952765464782715, loss=0.6453946232795715
I0306 22:00:39.909337 139787629491968 logging_writer.py:48] [239900] global_step=239900, grad_norm=4.319940090179443, loss=0.6028220653533936
I0306 22:01:13.539760 139789013600000 logging_writer.py:48] [240000] global_step=240000, grad_norm=4.526426315307617, loss=0.6635816693305969
I0306 22:01:47.193798 139787629491968 logging_writer.py:48] [240100] global_step=240100, grad_norm=4.0897626876831055, loss=0.5894661545753479
I0306 22:02:20.842589 139789013600000 logging_writer.py:48] [240200] global_step=240200, grad_norm=4.482432842254639, loss=0.5835187435150146
I0306 22:02:54.464693 139787629491968 logging_writer.py:48] [240300] global_step=240300, grad_norm=4.862899303436279, loss=0.6714185476303101
I0306 22:03:28.069406 139789013600000 logging_writer.py:48] [240400] global_step=240400, grad_norm=4.086551189422607, loss=0.5246415138244629
I0306 22:04:01.635206 139787629491968 logging_writer.py:48] [240500] global_step=240500, grad_norm=4.319277286529541, loss=0.6159681081771851
I0306 22:04:35.245872 139789013600000 logging_writer.py:48] [240600] global_step=240600, grad_norm=4.438449859619141, loss=0.5993565320968628
I0306 22:05:08.908961 139787629491968 logging_writer.py:48] [240700] global_step=240700, grad_norm=4.863776683807373, loss=0.6756173968315125
I0306 22:05:42.566437 139789013600000 logging_writer.py:48] [240800] global_step=240800, grad_norm=4.616440773010254, loss=0.6582831144332886
I0306 22:06:16.253792 139787629491968 logging_writer.py:48] [240900] global_step=240900, grad_norm=4.794030666351318, loss=0.6770828366279602
I0306 22:06:49.876241 139789013600000 logging_writer.py:48] [241000] global_step=241000, grad_norm=4.497408866882324, loss=0.6783292293548584
I0306 22:07:23.499828 139787629491968 logging_writer.py:48] [241100] global_step=241100, grad_norm=4.638348579406738, loss=0.5938213467597961
I0306 22:07:27.000384 139951089751872 spec.py:321] Evaluating on the training split.
I0306 22:07:33.216392 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 22:07:42.016750 139951089751872 spec.py:349] Evaluating on the test split.
I0306 22:07:44.349275 139951089751872 submission_runner.py:411] Time since start: 84067.56s, 	Step: 241112, 	{'train/accuracy': 0.9610570669174194, 'train/loss': 0.1477317065000534, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0410159826278687, 'validation/num_examples': 50000, 'test/accuracy': 0.6330000162124634, 'test/loss': 1.8216248750686646, 'test/num_examples': 10000, 'score': 81148.19727015495, 'total_duration': 84067.56158614159, 'accumulated_submission_time': 81148.19727015495, 'accumulated_eval_time': 2899.5223717689514, 'accumulated_logging_time': 11.105278968811035}
I0306 22:07:44.410774 139787629491968 logging_writer.py:48] [241112] accumulated_eval_time=2899.522372, accumulated_logging_time=11.105279, accumulated_submission_time=81148.197270, global_step=241112, preemption_count=0, score=81148.197270, test/accuracy=0.633000, test/loss=1.821625, test/num_examples=10000, total_duration=84067.561586, train/accuracy=0.961057, train/loss=0.147732, validation/accuracy=0.757040, validation/loss=1.041016, validation/num_examples=50000
I0306 22:08:14.335405 139788996814592 logging_writer.py:48] [241200] global_step=241200, grad_norm=4.5446319580078125, loss=0.6936414241790771
I0306 22:08:47.996712 139787629491968 logging_writer.py:48] [241300] global_step=241300, grad_norm=4.681211471557617, loss=0.6723403930664062
I0306 22:09:21.665792 139788996814592 logging_writer.py:48] [241400] global_step=241400, grad_norm=4.682153701782227, loss=0.6292814612388611
I0306 22:09:55.326000 139787629491968 logging_writer.py:48] [241500] global_step=241500, grad_norm=4.865479946136475, loss=0.6652790307998657
I0306 22:10:29.014143 139788996814592 logging_writer.py:48] [241600] global_step=241600, grad_norm=4.4585347175598145, loss=0.5833747386932373
I0306 22:11:02.653932 139787629491968 logging_writer.py:48] [241700] global_step=241700, grad_norm=4.278269290924072, loss=0.574856162071228
I0306 22:11:36.301329 139788996814592 logging_writer.py:48] [241800] global_step=241800, grad_norm=4.109478950500488, loss=0.5762311816215515
I0306 22:12:09.932691 139787629491968 logging_writer.py:48] [241900] global_step=241900, grad_norm=4.662447929382324, loss=0.6233948469161987
I0306 22:12:43.574660 139788996814592 logging_writer.py:48] [242000] global_step=242000, grad_norm=4.503312587738037, loss=0.6045131683349609
I0306 22:13:17.185167 139787629491968 logging_writer.py:48] [242100] global_step=242100, grad_norm=4.2193450927734375, loss=0.5875795483589172
I0306 22:13:50.839096 139788996814592 logging_writer.py:48] [242200] global_step=242200, grad_norm=4.442196369171143, loss=0.6367353200912476
I0306 22:14:24.483874 139787629491968 logging_writer.py:48] [242300] global_step=242300, grad_norm=4.801569938659668, loss=0.6764285564422607
I0306 22:14:58.089294 139788996814592 logging_writer.py:48] [242400] global_step=242400, grad_norm=4.411600589752197, loss=0.590359628200531
I0306 22:15:31.758961 139787629491968 logging_writer.py:48] [242500] global_step=242500, grad_norm=4.394474506378174, loss=0.5935778617858887
I0306 22:16:05.411068 139788996814592 logging_writer.py:48] [242600] global_step=242600, grad_norm=4.857257843017578, loss=0.6337844729423523
I0306 22:16:14.637781 139951089751872 spec.py:321] Evaluating on the training split.
I0306 22:16:20.743045 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 22:16:29.428304 139951089751872 spec.py:349] Evaluating on the test split.
I0306 22:16:31.749001 139951089751872 submission_runner.py:411] Time since start: 84594.96s, 	Step: 242629, 	{'train/accuracy': 0.9610769748687744, 'train/loss': 0.14381439983844757, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.0407848358154297, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8233078718185425, 'test/num_examples': 10000, 'score': 81658.35938191414, 'total_duration': 84594.96130204201, 'accumulated_submission_time': 81658.35938191414, 'accumulated_eval_time': 2916.6335439682007, 'accumulated_logging_time': 11.176434993743896}
I0306 22:16:31.808187 139789021992704 logging_writer.py:48] [242629] accumulated_eval_time=2916.633544, accumulated_logging_time=11.176435, accumulated_submission_time=81658.359382, global_step=242629, preemption_count=0, score=81658.359382, test/accuracy=0.631100, test/loss=1.823308, test/num_examples=10000, total_duration=84594.961302, train/accuracy=0.961077, train/loss=0.143814, validation/accuracy=0.757000, validation/loss=1.040785, validation/num_examples=50000
I0306 22:16:55.998820 139789030385408 logging_writer.py:48] [242700] global_step=242700, grad_norm=4.518237590789795, loss=0.6158026456832886
I0306 22:17:29.633631 139789021992704 logging_writer.py:48] [242800] global_step=242800, grad_norm=4.5276408195495605, loss=0.6938288807868958
I0306 22:18:03.243745 139789030385408 logging_writer.py:48] [242900] global_step=242900, grad_norm=5.162907600402832, loss=0.6151033639907837
I0306 22:18:36.896691 139789021992704 logging_writer.py:48] [243000] global_step=243000, grad_norm=4.2607903480529785, loss=0.5795629024505615
I0306 22:19:10.516196 139789030385408 logging_writer.py:48] [243100] global_step=243100, grad_norm=4.962490081787109, loss=0.6746355295181274
I0306 22:19:44.167088 139789021992704 logging_writer.py:48] [243200] global_step=243200, grad_norm=4.478485584259033, loss=0.6337609887123108
I0306 22:20:17.828125 139789030385408 logging_writer.py:48] [243300] global_step=243300, grad_norm=4.365242004394531, loss=0.6515096426010132
I0306 22:20:51.447187 139789021992704 logging_writer.py:48] [243400] global_step=243400, grad_norm=4.683555603027344, loss=0.5624175667762756
I0306 22:21:25.056629 139789030385408 logging_writer.py:48] [243500] global_step=243500, grad_norm=5.179762363433838, loss=0.7003644704818726
I0306 22:21:58.707051 139789021992704 logging_writer.py:48] [243600] global_step=243600, grad_norm=4.704651832580566, loss=0.6369435787200928
I0306 22:22:32.348178 139789030385408 logging_writer.py:48] [243700] global_step=243700, grad_norm=4.378269672393799, loss=0.6141380071640015
I0306 22:23:06.010958 139789021992704 logging_writer.py:48] [243800] global_step=243800, grad_norm=4.767956256866455, loss=0.6240447759628296
I0306 22:23:39.631325 139789030385408 logging_writer.py:48] [243900] global_step=243900, grad_norm=4.11650276184082, loss=0.5566492080688477
I0306 22:24:13.255141 139789021992704 logging_writer.py:48] [244000] global_step=244000, grad_norm=4.762301445007324, loss=0.6130208969116211
I0306 22:24:46.869034 139789030385408 logging_writer.py:48] [244100] global_step=244100, grad_norm=4.510297775268555, loss=0.6337586641311646
I0306 22:25:01.821593 139951089751872 spec.py:321] Evaluating on the training split.
I0306 22:25:07.892958 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 22:25:16.703896 139951089751872 spec.py:349] Evaluating on the test split.
I0306 22:25:18.928839 139951089751872 submission_runner.py:411] Time since start: 85122.14s, 	Step: 244146, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14677740633487701, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0413265228271484, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8220953941345215, 'test/num_examples': 10000, 'score': 82168.30734395981, 'total_duration': 85122.14113402367, 'accumulated_submission_time': 82168.30734395981, 'accumulated_eval_time': 2933.7407212257385, 'accumulated_logging_time': 11.245970726013184}
I0306 22:25:18.990223 139788996814592 logging_writer.py:48] [244146] accumulated_eval_time=2933.740721, accumulated_logging_time=11.245971, accumulated_submission_time=82168.307344, global_step=244146, preemption_count=0, score=82168.307344, test/accuracy=0.631700, test/loss=1.822095, test/num_examples=10000, total_duration=85122.141134, train/accuracy=0.961595, train/loss=0.146777, validation/accuracy=0.757300, validation/loss=1.041327, validation/num_examples=50000
I0306 22:25:37.472339 139789005207296 logging_writer.py:48] [244200] global_step=244200, grad_norm=4.334160804748535, loss=0.6332056522369385
I0306 22:26:11.114456 139788996814592 logging_writer.py:48] [244300] global_step=244300, grad_norm=4.286892414093018, loss=0.6021727323532104
I0306 22:26:44.947139 139789005207296 logging_writer.py:48] [244400] global_step=244400, grad_norm=4.663445472717285, loss=0.5979426503181458
I0306 22:27:18.609296 139788996814592 logging_writer.py:48] [244500] global_step=244500, grad_norm=4.286103248596191, loss=0.619515597820282
I0306 22:27:52.268006 139789005207296 logging_writer.py:48] [244600] global_step=244600, grad_norm=4.3608198165893555, loss=0.6123785972595215
I0306 22:28:25.962461 139788996814592 logging_writer.py:48] [244700] global_step=244700, grad_norm=4.680197238922119, loss=0.7011213898658752
I0306 22:28:59.575758 139789005207296 logging_writer.py:48] [244800] global_step=244800, grad_norm=4.6922101974487305, loss=0.6227794289588928
I0306 22:29:33.230633 139788996814592 logging_writer.py:48] [244900] global_step=244900, grad_norm=4.693450450897217, loss=0.599111795425415
I0306 22:30:06.871652 139789005207296 logging_writer.py:48] [245000] global_step=245000, grad_norm=4.3557448387146, loss=0.6126725673675537
I0306 22:30:40.540278 139788996814592 logging_writer.py:48] [245100] global_step=245100, grad_norm=4.469601154327393, loss=0.604945182800293
I0306 22:31:14.160886 139789005207296 logging_writer.py:48] [245200] global_step=245200, grad_norm=4.589510440826416, loss=0.6678610444068909
I0306 22:31:47.826287 139788996814592 logging_writer.py:48] [245300] global_step=245300, grad_norm=4.310330867767334, loss=0.5915673971176147
I0306 22:32:21.448904 139789005207296 logging_writer.py:48] [245400] global_step=245400, grad_norm=4.085568428039551, loss=0.536916971206665
I0306 22:32:55.246707 139788996814592 logging_writer.py:48] [245500] global_step=245500, grad_norm=4.320195198059082, loss=0.6248989105224609
I0306 22:33:28.882971 139789005207296 logging_writer.py:48] [245600] global_step=245600, grad_norm=4.289941787719727, loss=0.5524633526802063
I0306 22:33:49.221929 139951089751872 spec.py:321] Evaluating on the training split.
I0306 22:33:55.265372 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 22:34:04.132552 139951089751872 spec.py:349] Evaluating on the test split.
I0306 22:34:06.400682 139951089751872 submission_runner.py:411] Time since start: 85649.61s, 	Step: 245662, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.1484692096710205, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0395944118499756, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.821165919303894, 'test/num_examples': 10000, 'score': 82678.47346019745, 'total_duration': 85649.61295294762, 'accumulated_submission_time': 82678.47346019745, 'accumulated_eval_time': 2950.9193875789642, 'accumulated_logging_time': 11.318059921264648}
I0306 22:34:06.478749 139789013600000 logging_writer.py:48] [245662] accumulated_eval_time=2950.919388, accumulated_logging_time=11.318060, accumulated_submission_time=82678.473460, global_step=245662, preemption_count=0, score=82678.473460, test/accuracy=0.631600, test/loss=1.821166, test/num_examples=10000, total_duration=85649.612953, train/accuracy=0.959742, train/loss=0.148469, validation/accuracy=0.757260, validation/loss=1.039594, validation/num_examples=50000
I0306 22:34:19.632521 139789021992704 logging_writer.py:48] [245700] global_step=245700, grad_norm=5.058707237243652, loss=0.664046049118042
I0306 22:34:53.295857 139789013600000 logging_writer.py:48] [245800] global_step=245800, grad_norm=4.4851861000061035, loss=0.6447799801826477
I0306 22:35:26.948233 139789021992704 logging_writer.py:48] [245900] global_step=245900, grad_norm=4.2308669090271, loss=0.6145517230033875
I0306 22:36:00.579048 139789013600000 logging_writer.py:48] [246000] global_step=246000, grad_norm=3.976994752883911, loss=0.5793542861938477
I0306 22:36:34.210069 139789021992704 logging_writer.py:48] [246100] global_step=246100, grad_norm=4.505039691925049, loss=0.6606180667877197
I0306 22:37:07.829219 139789013600000 logging_writer.py:48] [246200] global_step=246200, grad_norm=4.3620405197143555, loss=0.5838946104049683
I0306 22:37:41.499868 139789021992704 logging_writer.py:48] [246300] global_step=246300, grad_norm=4.438473224639893, loss=0.5783808827400208
I0306 22:38:15.174516 139789013600000 logging_writer.py:48] [246400] global_step=246400, grad_norm=4.398556232452393, loss=0.6300976872444153
I0306 22:38:48.915413 139789021992704 logging_writer.py:48] [246500] global_step=246500, grad_norm=4.280980587005615, loss=0.6288518309593201
I0306 22:39:22.609520 139789013600000 logging_writer.py:48] [246600] global_step=246600, grad_norm=4.643858432769775, loss=0.6746214032173157
I0306 22:39:56.286901 139789021992704 logging_writer.py:48] [246700] global_step=246700, grad_norm=4.846532821655273, loss=0.71809983253479
I0306 22:40:29.923561 139789013600000 logging_writer.py:48] [246800] global_step=246800, grad_norm=4.152949333190918, loss=0.5709847211837769
I0306 22:41:03.598070 139789021992704 logging_writer.py:48] [246900] global_step=246900, grad_norm=4.732850074768066, loss=0.671842098236084
I0306 22:41:37.216303 139789013600000 logging_writer.py:48] [247000] global_step=247000, grad_norm=4.633940696716309, loss=0.6712504625320435
I0306 22:42:10.849143 139789021992704 logging_writer.py:48] [247100] global_step=247100, grad_norm=4.805763244628906, loss=0.6541755199432373
I0306 22:42:36.539859 139951089751872 spec.py:321] Evaluating on the training split.
I0306 22:42:42.585217 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 22:42:51.406505 139951089751872 spec.py:349] Evaluating on the test split.
I0306 22:42:53.683558 139951089751872 submission_runner.py:411] Time since start: 86176.90s, 	Step: 247178, 	{'train/accuracy': 0.9597217440605164, 'train/loss': 0.14925456047058105, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.0409460067749023, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8228518962860107, 'test/num_examples': 10000, 'score': 83188.46709156036, 'total_duration': 86176.89586758614, 'accumulated_submission_time': 83188.46709156036, 'accumulated_eval_time': 2968.0630457401276, 'accumulated_logging_time': 11.407188892364502}
I0306 22:42:53.741503 139787629491968 logging_writer.py:48] [247178] accumulated_eval_time=2968.063046, accumulated_logging_time=11.407189, accumulated_submission_time=83188.467092, global_step=247178, preemption_count=0, score=83188.467092, test/accuracy=0.631800, test/loss=1.822852, test/num_examples=10000, total_duration=86176.895868, train/accuracy=0.959722, train/loss=0.149255, validation/accuracy=0.756920, validation/loss=1.040946, validation/num_examples=50000
I0306 22:43:01.461096 139788996814592 logging_writer.py:48] [247200] global_step=247200, grad_norm=4.478423595428467, loss=0.6143089532852173
I0306 22:43:35.047852 139787629491968 logging_writer.py:48] [247300] global_step=247300, grad_norm=4.6138081550598145, loss=0.6304401755332947
I0306 22:44:08.596316 139788996814592 logging_writer.py:48] [247400] global_step=247400, grad_norm=4.295422554016113, loss=0.5930057168006897
I0306 22:44:42.235234 139787629491968 logging_writer.py:48] [247500] global_step=247500, grad_norm=4.14460563659668, loss=0.642601490020752
I0306 22:45:15.804234 139788996814592 logging_writer.py:48] [247600] global_step=247600, grad_norm=4.3379225730896, loss=0.5760537981987
I0306 22:45:49.402324 139787629491968 logging_writer.py:48] [247700] global_step=247700, grad_norm=4.267846584320068, loss=0.6373763084411621
I0306 22:46:23.042102 139788996814592 logging_writer.py:48] [247800] global_step=247800, grad_norm=4.814160346984863, loss=0.6434582471847534
I0306 22:46:56.679264 139787629491968 logging_writer.py:48] [247900] global_step=247900, grad_norm=5.122716903686523, loss=0.5828899145126343
I0306 22:47:30.350895 139788996814592 logging_writer.py:48] [248000] global_step=248000, grad_norm=4.760611057281494, loss=0.6504694223403931
I0306 22:48:03.968126 139787629491968 logging_writer.py:48] [248100] global_step=248100, grad_norm=4.713065147399902, loss=0.6879565119743347
I0306 22:48:37.645272 139788996814592 logging_writer.py:48] [248200] global_step=248200, grad_norm=4.215973854064941, loss=0.6376702189445496
I0306 22:49:11.254129 139787629491968 logging_writer.py:48] [248300] global_step=248300, grad_norm=4.3945817947387695, loss=0.6074424982070923
I0306 22:49:44.901227 139788996814592 logging_writer.py:48] [248400] global_step=248400, grad_norm=4.393026828765869, loss=0.5980322360992432
I0306 22:50:18.510262 139787629491968 logging_writer.py:48] [248500] global_step=248500, grad_norm=4.568221569061279, loss=0.6835991144180298
I0306 22:50:52.326884 139788996814592 logging_writer.py:48] [248600] global_step=248600, grad_norm=4.017326831817627, loss=0.5376855134963989
I0306 22:51:23.750452 139951089751872 spec.py:321] Evaluating on the training split.
I0306 22:51:29.784952 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 22:51:38.486603 139951089751872 spec.py:349] Evaluating on the test split.
I0306 22:51:40.787757 139951089751872 submission_runner.py:411] Time since start: 86704.00s, 	Step: 248695, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14717866480350494, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0416258573532104, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8244123458862305, 'test/num_examples': 10000, 'score': 83698.41069197655, 'total_duration': 86704.00005698204, 'accumulated_submission_time': 83698.41069197655, 'accumulated_eval_time': 2985.100293636322, 'accumulated_logging_time': 11.474862098693848}
I0306 22:51:40.844020 139789013600000 logging_writer.py:48] [248695] accumulated_eval_time=2985.100294, accumulated_logging_time=11.474862, accumulated_submission_time=83698.410692, global_step=248695, preemption_count=0, score=83698.410692, test/accuracy=0.630800, test/loss=1.824412, test/num_examples=10000, total_duration=86704.000057, train/accuracy=0.960061, train/loss=0.147179, validation/accuracy=0.757120, validation/loss=1.041626, validation/num_examples=50000
I0306 22:51:42.867832 139789021992704 logging_writer.py:48] [248700] global_step=248700, grad_norm=4.745556354522705, loss=0.659074604511261
I0306 22:52:16.453640 139789013600000 logging_writer.py:48] [248800] global_step=248800, grad_norm=4.653878211975098, loss=0.6365100145339966
I0306 22:52:50.018267 139789021992704 logging_writer.py:48] [248900] global_step=248900, grad_norm=4.730866432189941, loss=0.6497508883476257
I0306 22:53:23.599352 139789013600000 logging_writer.py:48] [249000] global_step=249000, grad_norm=4.22652530670166, loss=0.6292556524276733
I0306 22:53:57.222413 139789021992704 logging_writer.py:48] [249100] global_step=249100, grad_norm=4.343794822692871, loss=0.5874201059341431
I0306 22:54:30.866902 139789013600000 logging_writer.py:48] [249200] global_step=249200, grad_norm=4.791937351226807, loss=0.671458899974823
I0306 22:55:04.503565 139789021992704 logging_writer.py:48] [249300] global_step=249300, grad_norm=4.869169235229492, loss=0.7035484910011292
I0306 22:55:38.121412 139789013600000 logging_writer.py:48] [249400] global_step=249400, grad_norm=4.0974812507629395, loss=0.5601197481155396
I0306 22:56:11.743139 139789021992704 logging_writer.py:48] [249500] global_step=249500, grad_norm=4.763949871063232, loss=0.6561394929885864
I0306 22:56:45.394110 139789013600000 logging_writer.py:48] [249600] global_step=249600, grad_norm=4.577124118804932, loss=0.6171700358390808
I0306 22:57:19.039616 139789021992704 logging_writer.py:48] [249700] global_step=249700, grad_norm=4.319048881530762, loss=0.6514594554901123
I0306 22:57:52.639206 139789013600000 logging_writer.py:48] [249800] global_step=249800, grad_norm=4.19921875, loss=0.6480451226234436
I0306 22:58:26.273168 139789021992704 logging_writer.py:48] [249900] global_step=249900, grad_norm=4.7741217613220215, loss=0.5576995015144348
I0306 22:58:59.888007 139789013600000 logging_writer.py:48] [250000] global_step=250000, grad_norm=4.002055644989014, loss=0.5581728219985962
I0306 22:59:33.534143 139789021992704 logging_writer.py:48] [250100] global_step=250100, grad_norm=5.213342666625977, loss=0.7084140777587891
I0306 23:00:07.169338 139789013600000 logging_writer.py:48] [250200] global_step=250200, grad_norm=4.542383193969727, loss=0.6375388503074646
I0306 23:00:11.009346 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:00:17.052494 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:00:25.806283 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:00:28.155672 139951089751872 submission_runner.py:411] Time since start: 87231.37s, 	Step: 250213, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14675913751125336, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0408724546432495, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8219854831695557, 'test/num_examples': 10000, 'score': 84208.50983929634, 'total_duration': 87231.36796617508, 'accumulated_submission_time': 84208.50983929634, 'accumulated_eval_time': 3002.2465505599976, 'accumulated_logging_time': 11.541433811187744}
I0306 23:00:28.213633 139787612706560 logging_writer.py:48] [250213] accumulated_eval_time=3002.246551, accumulated_logging_time=11.541434, accumulated_submission_time=84208.509839, global_step=250213, preemption_count=0, score=84208.509839, test/accuracy=0.631500, test/loss=1.821985, test/num_examples=10000, total_duration=87231.367966, train/accuracy=0.961635, train/loss=0.146759, validation/accuracy=0.757340, validation/loss=1.040872, validation/num_examples=50000
I0306 23:00:57.767895 139787621099264 logging_writer.py:48] [250300] global_step=250300, grad_norm=4.792405605316162, loss=0.7414219975471497
I0306 23:01:31.350851 139787612706560 logging_writer.py:48] [250400] global_step=250400, grad_norm=4.3193278312683105, loss=0.6117173433303833
I0306 23:02:04.911915 139787621099264 logging_writer.py:48] [250500] global_step=250500, grad_norm=4.8627495765686035, loss=0.6784881353378296
I0306 23:02:38.513230 139787612706560 logging_writer.py:48] [250600] global_step=250600, grad_norm=4.603663444519043, loss=0.641715407371521
I0306 23:03:12.171195 139787621099264 logging_writer.py:48] [250700] global_step=250700, grad_norm=4.744960784912109, loss=0.5800552368164062
I0306 23:03:45.778954 139787612706560 logging_writer.py:48] [250800] global_step=250800, grad_norm=4.140538215637207, loss=0.5998077392578125
I0306 23:04:19.416513 139787621099264 logging_writer.py:48] [250900] global_step=250900, grad_norm=4.570806980133057, loss=0.6113407611846924
I0306 23:04:53.009379 139787612706560 logging_writer.py:48] [251000] global_step=251000, grad_norm=4.6631760597229, loss=0.6638578772544861
I0306 23:05:26.682500 139787621099264 logging_writer.py:48] [251100] global_step=251100, grad_norm=4.528125286102295, loss=0.613244891166687
I0306 23:06:00.304918 139787612706560 logging_writer.py:48] [251200] global_step=251200, grad_norm=5.168393135070801, loss=0.604393720626831
I0306 23:06:33.978160 139787621099264 logging_writer.py:48] [251300] global_step=251300, grad_norm=4.530972003936768, loss=0.6677687168121338
I0306 23:07:07.586243 139787612706560 logging_writer.py:48] [251400] global_step=251400, grad_norm=4.701294422149658, loss=0.5739659070968628
I0306 23:07:41.242909 139787621099264 logging_writer.py:48] [251500] global_step=251500, grad_norm=3.824251174926758, loss=0.5565589666366577
I0306 23:08:14.885381 139787612706560 logging_writer.py:48] [251600] global_step=251600, grad_norm=4.330124378204346, loss=0.6611016392707825
I0306 23:08:48.554224 139787621099264 logging_writer.py:48] [251700] global_step=251700, grad_norm=4.3415045738220215, loss=0.5430207252502441
I0306 23:08:58.460010 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:09:04.704711 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:09:13.350119 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:09:15.621808 139951089751872 submission_runner.py:411] Time since start: 87758.83s, 	Step: 251731, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.1473046839237213, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0428922176361084, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8245079517364502, 'test/num_examples': 10000, 'score': 84718.69186162949, 'total_duration': 87758.83411717415, 'accumulated_submission_time': 84718.69186162949, 'accumulated_eval_time': 3019.4082946777344, 'accumulated_logging_time': 11.609270811080933}
I0306 23:09:15.679018 139789013600000 logging_writer.py:48] [251731] accumulated_eval_time=3019.408295, accumulated_logging_time=11.609271, accumulated_submission_time=84718.691862, global_step=251731, preemption_count=0, score=84718.691862, test/accuracy=0.631200, test/loss=1.824508, test/num_examples=10000, total_duration=87758.834117, train/accuracy=0.960758, train/loss=0.147305, validation/accuracy=0.757080, validation/loss=1.042892, validation/num_examples=50000
I0306 23:09:39.189282 139789021992704 logging_writer.py:48] [251800] global_step=251800, grad_norm=4.368016242980957, loss=0.6335156559944153
I0306 23:10:12.763994 139789013600000 logging_writer.py:48] [251900] global_step=251900, grad_norm=4.438401699066162, loss=0.5915789604187012
I0306 23:10:46.405601 139789021992704 logging_writer.py:48] [252000] global_step=252000, grad_norm=4.186067581176758, loss=0.5876970291137695
I0306 23:11:20.031360 139789013600000 logging_writer.py:48] [252100] global_step=252100, grad_norm=4.8879804611206055, loss=0.6648848652839661
I0306 23:11:53.686346 139789021992704 logging_writer.py:48] [252200] global_step=252200, grad_norm=4.454051971435547, loss=0.5798287987709045
I0306 23:12:27.331884 139789013600000 logging_writer.py:48] [252300] global_step=252300, grad_norm=4.012025356292725, loss=0.5798618793487549
I0306 23:13:00.950462 139789021992704 logging_writer.py:48] [252400] global_step=252400, grad_norm=4.880832195281982, loss=0.6358062624931335
I0306 23:13:34.596673 139789013600000 logging_writer.py:48] [252500] global_step=252500, grad_norm=4.1771368980407715, loss=0.5013710856437683
I0306 23:14:08.258520 139789021992704 logging_writer.py:48] [252600] global_step=252600, grad_norm=4.106251239776611, loss=0.534063458442688
I0306 23:14:41.904689 139789013600000 logging_writer.py:48] [252700] global_step=252700, grad_norm=4.374492168426514, loss=0.5717452764511108
I0306 23:15:15.578822 139789021992704 logging_writer.py:48] [252800] global_step=252800, grad_norm=4.184508323669434, loss=0.5254523158073425
I0306 23:15:49.184575 139789013600000 logging_writer.py:48] [252900] global_step=252900, grad_norm=4.419014930725098, loss=0.5857846736907959
I0306 23:16:22.827199 139789021992704 logging_writer.py:48] [253000] global_step=253000, grad_norm=4.307196140289307, loss=0.5413377285003662
I0306 23:16:56.435320 139789013600000 logging_writer.py:48] [253100] global_step=253100, grad_norm=4.832056045532227, loss=0.6428282260894775
I0306 23:17:30.062545 139789021992704 logging_writer.py:48] [253200] global_step=253200, grad_norm=4.047201156616211, loss=0.5774887800216675
I0306 23:17:45.684798 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:17:51.743758 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:18:00.572884 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:18:02.918398 139951089751872 submission_runner.py:411] Time since start: 88286.13s, 	Step: 253248, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14696697890758514, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0400139093399048, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.820750117301941, 'test/num_examples': 10000, 'score': 85228.6314611435, 'total_duration': 88286.13070821762, 'accumulated_submission_time': 85228.6314611435, 'accumulated_eval_time': 3036.6418414115906, 'accumulated_logging_time': 11.676239252090454}
I0306 23:18:02.981848 139788996814592 logging_writer.py:48] [253248] accumulated_eval_time=3036.641841, accumulated_logging_time=11.676239, accumulated_submission_time=85228.631461, global_step=253248, preemption_count=0, score=85228.631461, test/accuracy=0.631600, test/loss=1.820750, test/num_examples=10000, total_duration=88286.130708, train/accuracy=0.960120, train/loss=0.146967, validation/accuracy=0.757120, validation/loss=1.040014, validation/num_examples=50000
I0306 23:18:20.798013 139789005207296 logging_writer.py:48] [253300] global_step=253300, grad_norm=4.340534687042236, loss=0.5672435760498047
I0306 23:18:54.444035 139788996814592 logging_writer.py:48] [253400] global_step=253400, grad_norm=4.461414813995361, loss=0.5916538238525391
I0306 23:19:28.096666 139789005207296 logging_writer.py:48] [253500] global_step=253500, grad_norm=4.522734642028809, loss=0.6439360976219177
I0306 23:20:01.689832 139788996814592 logging_writer.py:48] [253600] global_step=253600, grad_norm=4.491103649139404, loss=0.603824257850647
I0306 23:20:35.338573 139789005207296 logging_writer.py:48] [253700] global_step=253700, grad_norm=4.25965690612793, loss=0.6342453956604004
I0306 23:21:09.007112 139788996814592 logging_writer.py:48] [253800] global_step=253800, grad_norm=4.1878132820129395, loss=0.6634820103645325
I0306 23:21:42.647808 139789005207296 logging_writer.py:48] [253900] global_step=253900, grad_norm=4.114032745361328, loss=0.5766865015029907
I0306 23:22:16.307359 139788996814592 logging_writer.py:48] [254000] global_step=254000, grad_norm=4.268077850341797, loss=0.6404224634170532
I0306 23:22:49.922153 139789005207296 logging_writer.py:48] [254100] global_step=254100, grad_norm=4.891624450683594, loss=0.7264049053192139
I0306 23:23:23.575824 139788996814592 logging_writer.py:48] [254200] global_step=254200, grad_norm=4.304561614990234, loss=0.6421056985855103
I0306 23:23:57.173067 139789005207296 logging_writer.py:48] [254300] global_step=254300, grad_norm=4.5558905601501465, loss=0.5775975584983826
I0306 23:24:30.836386 139788996814592 logging_writer.py:48] [254400] global_step=254400, grad_norm=5.179047584533691, loss=0.6223170161247253
I0306 23:25:04.448593 139789005207296 logging_writer.py:48] [254500] global_step=254500, grad_norm=4.11979341506958, loss=0.591077983379364
I0306 23:25:38.113636 139788996814592 logging_writer.py:48] [254600] global_step=254600, grad_norm=4.665368556976318, loss=0.7207460403442383
I0306 23:26:11.752090 139789005207296 logging_writer.py:48] [254700] global_step=254700, grad_norm=4.466533184051514, loss=0.6379019618034363
I0306 23:26:33.109934 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:26:39.143408 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:26:47.834083 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:26:50.103483 139951089751872 submission_runner.py:411] Time since start: 88813.32s, 	Step: 254765, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.1476990282535553, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.041129231452942, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.823580026626587, 'test/num_examples': 10000, 'score': 85738.69255518913, 'total_duration': 88813.3157889843, 'accumulated_submission_time': 85738.69255518913, 'accumulated_eval_time': 3053.6353392601013, 'accumulated_logging_time': 11.750758647918701}
I0306 23:26:50.174833 139787621099264 logging_writer.py:48] [254765] accumulated_eval_time=3053.635339, accumulated_logging_time=11.750759, accumulated_submission_time=85738.692555, global_step=254765, preemption_count=0, score=85738.692555, test/accuracy=0.631700, test/loss=1.823580, test/num_examples=10000, total_duration=88813.315789, train/accuracy=0.960100, train/loss=0.147699, validation/accuracy=0.757240, validation/loss=1.041129, validation/num_examples=50000
I0306 23:27:02.266627 139787629491968 logging_writer.py:48] [254800] global_step=254800, grad_norm=5.025822639465332, loss=0.7192407250404358
I0306 23:27:35.892037 139787621099264 logging_writer.py:48] [254900] global_step=254900, grad_norm=4.285430908203125, loss=0.6083877682685852
I0306 23:28:09.480401 139787629491968 logging_writer.py:48] [255000] global_step=255000, grad_norm=4.7028703689575195, loss=0.6291684508323669
I0306 23:28:43.029101 139787621099264 logging_writer.py:48] [255100] global_step=255100, grad_norm=4.438635349273682, loss=0.5969452857971191
I0306 23:29:16.611403 139787629491968 logging_writer.py:48] [255200] global_step=255200, grad_norm=4.363447666168213, loss=0.6021242141723633
I0306 23:29:50.244337 139787621099264 logging_writer.py:48] [255300] global_step=255300, grad_norm=4.909548282623291, loss=0.6034563183784485
I0306 23:30:23.869061 139787629491968 logging_writer.py:48] [255400] global_step=255400, grad_norm=4.702800750732422, loss=0.6984109878540039
I0306 23:30:57.507226 139787621099264 logging_writer.py:48] [255500] global_step=255500, grad_norm=4.773813724517822, loss=0.6537251472473145
I0306 23:31:31.115087 139787629491968 logging_writer.py:48] [255600] global_step=255600, grad_norm=4.053586006164551, loss=0.5419373512268066
I0306 23:32:04.753388 139787621099264 logging_writer.py:48] [255700] global_step=255700, grad_norm=4.397456169128418, loss=0.6664069294929504
I0306 23:32:38.368227 139787629491968 logging_writer.py:48] [255800] global_step=255800, grad_norm=4.516477584838867, loss=0.5755731463432312
I0306 23:33:12.003354 139787621099264 logging_writer.py:48] [255900] global_step=255900, grad_norm=4.825992107391357, loss=0.6619060039520264
I0306 23:33:45.664753 139787629491968 logging_writer.py:48] [256000] global_step=256000, grad_norm=4.153857231140137, loss=0.6059585213661194
I0306 23:34:19.310585 139787621099264 logging_writer.py:48] [256100] global_step=256100, grad_norm=4.5139923095703125, loss=0.6373602747917175
I0306 23:34:52.934540 139787629491968 logging_writer.py:48] [256200] global_step=256200, grad_norm=4.5387864112854, loss=0.6028148531913757
I0306 23:35:20.325354 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:35:26.355044 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:35:35.042402 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:35:37.353572 139951089751872 submission_runner.py:411] Time since start: 89340.57s, 	Step: 256283, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.1457504779100418, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0411179065704346, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8220078945159912, 'test/num_examples': 10000, 'score': 86248.77793478966, 'total_duration': 89340.56578230858, 'accumulated_submission_time': 86248.77793478966, 'accumulated_eval_time': 3070.663406610489, 'accumulated_logging_time': 11.832293510437012}
I0306 23:35:37.412659 139787612706560 logging_writer.py:48] [256283] accumulated_eval_time=3070.663407, accumulated_logging_time=11.832294, accumulated_submission_time=86248.777935, global_step=256283, preemption_count=0, score=86248.777935, test/accuracy=0.631800, test/loss=1.822008, test/num_examples=10000, total_duration=89340.565782, train/accuracy=0.960957, train/loss=0.145750, validation/accuracy=0.757020, validation/loss=1.041118, validation/num_examples=50000
I0306 23:35:43.480878 139787621099264 logging_writer.py:48] [256300] global_step=256300, grad_norm=4.768533706665039, loss=0.6208446025848389
I0306 23:36:17.171213 139787612706560 logging_writer.py:48] [256400] global_step=256400, grad_norm=4.192993640899658, loss=0.6208799481391907
I0306 23:36:50.793587 139787621099264 logging_writer.py:48] [256500] global_step=256500, grad_norm=4.470484733581543, loss=0.630974531173706
I0306 23:37:24.500758 139787612706560 logging_writer.py:48] [256600] global_step=256600, grad_norm=5.107946872711182, loss=0.6388116478919983
I0306 23:37:58.176795 139787621099264 logging_writer.py:48] [256700] global_step=256700, grad_norm=4.4074015617370605, loss=0.6306287050247192
I0306 23:38:31.832284 139787612706560 logging_writer.py:48] [256800] global_step=256800, grad_norm=4.681278228759766, loss=0.6281580924987793
I0306 23:39:05.484673 139787621099264 logging_writer.py:48] [256900] global_step=256900, grad_norm=4.60089111328125, loss=0.5837169885635376
I0306 23:39:39.082662 139787612706560 logging_writer.py:48] [257000] global_step=257000, grad_norm=4.2294087409973145, loss=0.5834245085716248
I0306 23:40:12.747612 139787621099264 logging_writer.py:48] [257100] global_step=257100, grad_norm=4.744231224060059, loss=0.5751705169677734
I0306 23:40:46.364527 139787612706560 logging_writer.py:48] [257200] global_step=257200, grad_norm=5.034208297729492, loss=0.6134709119796753
I0306 23:41:20.022186 139787621099264 logging_writer.py:48] [257300] global_step=257300, grad_norm=4.072270393371582, loss=0.6317143440246582
I0306 23:41:53.636862 139787612706560 logging_writer.py:48] [257400] global_step=257400, grad_norm=4.46600341796875, loss=0.6708340644836426
I0306 23:42:27.307654 139787621099264 logging_writer.py:48] [257500] global_step=257500, grad_norm=4.420756816864014, loss=0.6129662990570068
I0306 23:43:00.964176 139787612706560 logging_writer.py:48] [257600] global_step=257600, grad_norm=4.532690525054932, loss=0.6681240797042847
I0306 23:43:34.546303 139787621099264 logging_writer.py:48] [257700] global_step=257700, grad_norm=4.416360855102539, loss=0.6864516735076904
I0306 23:44:07.596346 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:44:13.592364 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:44:22.364231 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:44:24.643398 139951089751872 submission_runner.py:411] Time since start: 89867.86s, 	Step: 257800, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.14766673743724823, 'validation/accuracy': 0.7569800019264221, 'validation/loss': 1.041002869606018, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8211606740951538, 'test/num_examples': 10000, 'score': 86758.89575958252, 'total_duration': 89867.85570526123, 'accumulated_submission_time': 86758.89575958252, 'accumulated_eval_time': 3087.7104094028473, 'accumulated_logging_time': 11.901975393295288}
I0306 23:44:24.705093 139787612706560 logging_writer.py:48] [257800] accumulated_eval_time=3087.710409, accumulated_logging_time=11.901975, accumulated_submission_time=86758.895760, global_step=257800, preemption_count=0, score=86758.895760, test/accuracy=0.631600, test/loss=1.821161, test/num_examples=10000, total_duration=89867.855705, train/accuracy=0.959742, train/loss=0.147667, validation/accuracy=0.756980, validation/loss=1.041003, validation/num_examples=50000
I0306 23:44:25.048305 139787621099264 logging_writer.py:48] [257800] global_step=257800, grad_norm=4.594853401184082, loss=0.6255453824996948
I0306 23:44:58.644058 139787612706560 logging_writer.py:48] [257900] global_step=257900, grad_norm=4.512646675109863, loss=0.6170797348022461
I0306 23:45:32.276017 139787621099264 logging_writer.py:48] [258000] global_step=258000, grad_norm=4.376391410827637, loss=0.6294755339622498
I0306 23:46:05.935848 139787612706560 logging_writer.py:48] [258100] global_step=258100, grad_norm=4.687632083892822, loss=0.6749808192253113
I0306 23:46:39.564276 139787621099264 logging_writer.py:48] [258200] global_step=258200, grad_norm=4.545665740966797, loss=0.6027410626411438
I0306 23:47:13.205452 139787612706560 logging_writer.py:48] [258300] global_step=258300, grad_norm=4.944382667541504, loss=0.6687353849411011
I0306 23:47:46.840125 139787621099264 logging_writer.py:48] [258400] global_step=258400, grad_norm=4.454010963439941, loss=0.5995578169822693
I0306 23:48:20.490517 139787612706560 logging_writer.py:48] [258500] global_step=258500, grad_norm=4.609959125518799, loss=0.5854383707046509
I0306 23:48:54.192862 139787621099264 logging_writer.py:48] [258600] global_step=258600, grad_norm=4.5658745765686035, loss=0.6744235754013062
I0306 23:49:27.852602 139787612706560 logging_writer.py:48] [258700] global_step=258700, grad_norm=4.159786224365234, loss=0.5670564770698547
I0306 23:50:01.502069 139787621099264 logging_writer.py:48] [258800] global_step=258800, grad_norm=4.668492794036865, loss=0.627321720123291
I0306 23:50:35.127355 139787612706560 logging_writer.py:48] [258900] global_step=258900, grad_norm=4.240265846252441, loss=0.6149332523345947
I0306 23:51:08.769460 139787621099264 logging_writer.py:48] [259000] global_step=259000, grad_norm=4.8217291831970215, loss=0.7109224200248718
I0306 23:51:42.405869 139787612706560 logging_writer.py:48] [259100] global_step=259100, grad_norm=5.0482096672058105, loss=0.7601926922798157
I0306 23:52:16.055187 139787621099264 logging_writer.py:48] [259200] global_step=259200, grad_norm=4.530976295471191, loss=0.6409003138542175
I0306 23:52:49.677093 139787612706560 logging_writer.py:48] [259300] global_step=259300, grad_norm=4.5389723777771, loss=0.6477597951889038
I0306 23:52:54.861361 139951089751872 spec.py:321] Evaluating on the training split.
I0306 23:53:00.894503 139951089751872 spec.py:333] Evaluating on the validation split.
I0306 23:53:09.704909 139951089751872 spec.py:349] Evaluating on the test split.
I0306 23:53:12.111185 139951089751872 submission_runner.py:411] Time since start: 90395.32s, 	Step: 259317, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.1454952508211136, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0412981510162354, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8210585117340088, 'test/num_examples': 10000, 'score': 87268.98562383652, 'total_duration': 90395.32349467278, 'accumulated_submission_time': 87268.98562383652, 'accumulated_eval_time': 3104.9601757526398, 'accumulated_logging_time': 11.974144697189331}
I0306 23:53:12.183722 139787612706560 logging_writer.py:48] [259317] accumulated_eval_time=3104.960176, accumulated_logging_time=11.974145, accumulated_submission_time=87268.985624, global_step=259317, preemption_count=0, score=87268.985624, test/accuracy=0.631300, test/loss=1.821059, test/num_examples=10000, total_duration=90395.323495, train/accuracy=0.960379, train/loss=0.145495, validation/accuracy=0.757160, validation/loss=1.041298, validation/num_examples=50000
I0306 23:53:40.435811 139787621099264 logging_writer.py:48] [259400] global_step=259400, grad_norm=4.32526969909668, loss=0.6560235023498535
I0306 23:54:14.013679 139787612706560 logging_writer.py:48] [259500] global_step=259500, grad_norm=4.874683856964111, loss=0.6229492425918579
I0306 23:54:47.682872 139787621099264 logging_writer.py:48] [259600] global_step=259600, grad_norm=4.563393592834473, loss=0.6565706133842468
I0306 23:55:21.349788 139787612706560 logging_writer.py:48] [259700] global_step=259700, grad_norm=4.343501091003418, loss=0.6423529386520386
I0306 23:55:55.022414 139787621099264 logging_writer.py:48] [259800] global_step=259800, grad_norm=4.6720404624938965, loss=0.6304007768630981
I0306 23:56:28.664638 139787612706560 logging_writer.py:48] [259900] global_step=259900, grad_norm=4.332164287567139, loss=0.6041288375854492
I0306 23:57:02.333495 139787621099264 logging_writer.py:48] [260000] global_step=260000, grad_norm=4.5483245849609375, loss=0.616716742515564
I0306 23:57:35.984148 139787612706560 logging_writer.py:48] [260100] global_step=260100, grad_norm=5.241478443145752, loss=0.6996869444847107
I0306 23:58:09.822125 139787621099264 logging_writer.py:48] [260200] global_step=260200, grad_norm=4.412592887878418, loss=0.656800389289856
I0306 23:58:43.493899 139787612706560 logging_writer.py:48] [260300] global_step=260300, grad_norm=4.638181209564209, loss=0.6767138242721558
I0306 23:59:17.174826 139787621099264 logging_writer.py:48] [260400] global_step=260400, grad_norm=4.452928066253662, loss=0.606651246547699
I0306 23:59:50.832225 139787612706560 logging_writer.py:48] [260500] global_step=260500, grad_norm=4.34282922744751, loss=0.6184825301170349
I0307 00:00:24.505884 139787621099264 logging_writer.py:48] [260600] global_step=260600, grad_norm=4.632379055023193, loss=0.6932703256607056
I0307 00:00:58.157653 139787612706560 logging_writer.py:48] [260700] global_step=260700, grad_norm=4.3877153396606445, loss=0.5768382549285889
I0307 00:01:31.831024 139787621099264 logging_writer.py:48] [260800] global_step=260800, grad_norm=4.67844820022583, loss=0.6354618668556213
I0307 00:01:42.401421 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:01:48.415341 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:01:57.200374 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:01:59.500905 139951089751872 submission_runner.py:411] Time since start: 90922.71s, 	Step: 260833, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14696051180362701, 'validation/accuracy': 0.7566999793052673, 'validation/loss': 1.0429741144180298, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8252462148666382, 'test/num_examples': 10000, 'score': 87779.135191679, 'total_duration': 90922.71321320534, 'accumulated_submission_time': 87779.135191679, 'accumulated_eval_time': 3122.0596079826355, 'accumulated_logging_time': 12.059104919433594}
I0307 00:01:59.563921 139787612706560 logging_writer.py:48] [260833] accumulated_eval_time=3122.059608, accumulated_logging_time=12.059105, accumulated_submission_time=87779.135192, global_step=260833, preemption_count=0, score=87779.135192, test/accuracy=0.631000, test/loss=1.825246, test/num_examples=10000, total_duration=90922.713213, train/accuracy=0.960678, train/loss=0.146961, validation/accuracy=0.756700, validation/loss=1.042974, validation/num_examples=50000
I0307 00:02:22.464084 139787621099264 logging_writer.py:48] [260900] global_step=260900, grad_norm=4.566634178161621, loss=0.6752878427505493
I0307 00:02:56.118408 139787612706560 logging_writer.py:48] [261000] global_step=261000, grad_norm=4.711668968200684, loss=0.6292557716369629
I0307 00:03:29.751190 139787621099264 logging_writer.py:48] [261100] global_step=261100, grad_norm=4.348459243774414, loss=0.6168307065963745
I0307 00:04:03.415929 139787612706560 logging_writer.py:48] [261200] global_step=261200, grad_norm=4.4417572021484375, loss=0.5589362978935242
I0307 00:04:37.106333 139787621099264 logging_writer.py:48] [261300] global_step=261300, grad_norm=4.371111869812012, loss=0.5993413925170898
I0307 00:05:10.736549 139787612706560 logging_writer.py:48] [261400] global_step=261400, grad_norm=4.148159027099609, loss=0.5858734846115112
I0307 00:05:44.317113 139787621099264 logging_writer.py:48] [261500] global_step=261500, grad_norm=4.6781840324401855, loss=0.6537888646125793
I0307 00:06:17.905091 139787612706560 logging_writer.py:48] [261600] global_step=261600, grad_norm=4.591660499572754, loss=0.6101477146148682
I0307 00:06:51.462939 139787621099264 logging_writer.py:48] [261700] global_step=261700, grad_norm=4.609288215637207, loss=0.6700425148010254
I0307 00:07:25.070371 139787612706560 logging_writer.py:48] [261800] global_step=261800, grad_norm=4.2194414138793945, loss=0.5795221328735352
I0307 00:07:58.701043 139787621099264 logging_writer.py:48] [261900] global_step=261900, grad_norm=4.625494480133057, loss=0.6707807183265686
I0307 00:08:32.314407 139787612706560 logging_writer.py:48] [262000] global_step=262000, grad_norm=4.377817153930664, loss=0.6190768480300903
I0307 00:09:05.958184 139787621099264 logging_writer.py:48] [262100] global_step=262100, grad_norm=5.05223274230957, loss=0.7571752667427063
I0307 00:09:39.598359 139787612706560 logging_writer.py:48] [262200] global_step=262200, grad_norm=4.362319469451904, loss=0.5924870371818542
I0307 00:10:13.244923 139787621099264 logging_writer.py:48] [262300] global_step=262300, grad_norm=4.493387222290039, loss=0.5739713907241821
I0307 00:10:29.684839 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:10:35.707170 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:10:44.578458 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:10:46.882354 139951089751872 submission_runner.py:411] Time since start: 91450.09s, 	Step: 262350, 	{'train/accuracy': 0.9612364172935486, 'train/loss': 0.14767929911613464, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 1.0416964292526245, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8224248886108398, 'test/num_examples': 10000, 'score': 88289.18821763992, 'total_duration': 91450.09464883804, 'accumulated_submission_time': 88289.18821763992, 'accumulated_eval_time': 3139.257059574127, 'accumulated_logging_time': 12.131905794143677}
I0307 00:10:46.945065 139789021992704 logging_writer.py:48] [262350] accumulated_eval_time=3139.257060, accumulated_logging_time=12.131906, accumulated_submission_time=88289.188218, global_step=262350, preemption_count=0, score=88289.188218, test/accuracy=0.630900, test/loss=1.822425, test/num_examples=10000, total_duration=91450.094649, train/accuracy=0.961236, train/loss=0.147679, validation/accuracy=0.757400, validation/loss=1.041696, validation/num_examples=50000
I0307 00:11:04.132500 139789030385408 logging_writer.py:48] [262400] global_step=262400, grad_norm=4.84681510925293, loss=0.6863808631896973
I0307 00:11:37.810407 139789021992704 logging_writer.py:48] [262500] global_step=262500, grad_norm=4.736572742462158, loss=0.6187568306922913
I0307 00:12:11.480637 139789030385408 logging_writer.py:48] [262600] global_step=262600, grad_norm=4.341383457183838, loss=0.6149303913116455
I0307 00:12:45.165221 139789021992704 logging_writer.py:48] [262700] global_step=262700, grad_norm=4.695995807647705, loss=0.6301475763320923
I0307 00:13:18.823206 139789030385408 logging_writer.py:48] [262800] global_step=262800, grad_norm=4.930870056152344, loss=0.6710662841796875
I0307 00:13:52.495732 139789021992704 logging_writer.py:48] [262900] global_step=262900, grad_norm=4.417026519775391, loss=0.5768086910247803
I0307 00:14:26.099595 139789030385408 logging_writer.py:48] [263000] global_step=263000, grad_norm=4.659877300262451, loss=0.6918046474456787
I0307 00:14:59.763860 139789021992704 logging_writer.py:48] [263100] global_step=263100, grad_norm=4.465223789215088, loss=0.614406943321228
I0307 00:15:33.423766 139789030385408 logging_writer.py:48] [263200] global_step=263200, grad_norm=4.481985092163086, loss=0.683725118637085
I0307 00:16:07.085128 139789021992704 logging_writer.py:48] [263300] global_step=263300, grad_norm=4.4554338455200195, loss=0.5846390128135681
I0307 00:16:40.792208 139789030385408 logging_writer.py:48] [263400] global_step=263400, grad_norm=4.515546798706055, loss=0.606306254863739
I0307 00:17:14.391645 139789021992704 logging_writer.py:48] [263500] global_step=263500, grad_norm=4.384570121765137, loss=0.6519994139671326
I0307 00:17:48.085164 139789030385408 logging_writer.py:48] [263600] global_step=263600, grad_norm=4.417902946472168, loss=0.6496485471725464
I0307 00:18:21.765138 139789021992704 logging_writer.py:48] [263700] global_step=263700, grad_norm=4.133533000946045, loss=0.6035244464874268
I0307 00:18:55.393857 139789030385408 logging_writer.py:48] [263800] global_step=263800, grad_norm=4.246663570404053, loss=0.5856044292449951
I0307 00:19:17.071809 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:19:23.055524 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:19:31.863466 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:19:34.127574 139951089751872 submission_runner.py:411] Time since start: 91977.34s, 	Step: 263866, 	{'train/accuracy': 0.9598214030265808, 'train/loss': 0.14644008874893188, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0404446125030518, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8234227895736694, 'test/num_examples': 10000, 'score': 88799.24826645851, 'total_duration': 91977.33986473083, 'accumulated_submission_time': 88799.24826645851, 'accumulated_eval_time': 3156.3127546310425, 'accumulated_logging_time': 12.205149173736572}
I0307 00:19:34.190097 139787629491968 logging_writer.py:48] [263866] accumulated_eval_time=3156.312755, accumulated_logging_time=12.205149, accumulated_submission_time=88799.248266, global_step=263866, preemption_count=0, score=88799.248266, test/accuracy=0.632000, test/loss=1.823423, test/num_examples=10000, total_duration=91977.339865, train/accuracy=0.959821, train/loss=0.146440, validation/accuracy=0.757300, validation/loss=1.040445, validation/num_examples=50000
I0307 00:19:45.973227 139788996814592 logging_writer.py:48] [263900] global_step=263900, grad_norm=4.413572311401367, loss=0.6695864200592041
I0307 00:20:19.657701 139787629491968 logging_writer.py:48] [264000] global_step=264000, grad_norm=4.514969825744629, loss=0.5909407138824463
I0307 00:20:53.313339 139788996814592 logging_writer.py:48] [264100] global_step=264100, grad_norm=4.1694135665893555, loss=0.5796403288841248
I0307 00:21:26.945511 139787629491968 logging_writer.py:48] [264200] global_step=264200, grad_norm=4.769203186035156, loss=0.646477222442627
I0307 00:22:00.523910 139788996814592 logging_writer.py:48] [264300] global_step=264300, grad_norm=4.245579719543457, loss=0.6310974955558777
I0307 00:22:34.097374 139787629491968 logging_writer.py:48] [264400] global_step=264400, grad_norm=4.336284637451172, loss=0.5820512771606445
I0307 00:23:07.819764 139788996814592 logging_writer.py:48] [264500] global_step=264500, grad_norm=4.667831897735596, loss=0.6421031355857849
I0307 00:23:41.489921 139787629491968 logging_writer.py:48] [264600] global_step=264600, grad_norm=4.32836389541626, loss=0.5986449122428894
I0307 00:24:15.098171 139788996814592 logging_writer.py:48] [264700] global_step=264700, grad_norm=4.509758949279785, loss=0.650457501411438
I0307 00:24:48.737758 139787629491968 logging_writer.py:48] [264800] global_step=264800, grad_norm=4.740232944488525, loss=0.6823834776878357
I0307 00:25:22.380342 139788996814592 logging_writer.py:48] [264900] global_step=264900, grad_norm=4.339104652404785, loss=0.5754197835922241
I0307 00:25:56.038994 139787629491968 logging_writer.py:48] [265000] global_step=265000, grad_norm=4.556251049041748, loss=0.6426693201065063
I0307 00:26:29.649524 139788996814592 logging_writer.py:48] [265100] global_step=265100, grad_norm=4.40424919128418, loss=0.6670043468475342
I0307 00:27:03.290995 139787629491968 logging_writer.py:48] [265200] global_step=265200, grad_norm=4.304994583129883, loss=0.5872315764427185
I0307 00:27:36.905378 139788996814592 logging_writer.py:48] [265300] global_step=265300, grad_norm=4.497025489807129, loss=0.5779423713684082
I0307 00:28:04.311137 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:28:10.343686 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:28:19.022491 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:28:21.329341 139951089751872 submission_runner.py:411] Time since start: 92504.54s, 	Step: 265383, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.142778679728508, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.040645956993103, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.82268226146698, 'test/num_examples': 10000, 'score': 89309.30315113068, 'total_duration': 92504.5416522026, 'accumulated_submission_time': 89309.30315113068, 'accumulated_eval_time': 3173.3309082984924, 'accumulated_logging_time': 12.277857065200806}
I0307 00:28:21.391271 139789021992704 logging_writer.py:48] [265383] accumulated_eval_time=3173.330908, accumulated_logging_time=12.277857, accumulated_submission_time=89309.303151, global_step=265383, preemption_count=0, score=89309.303151, test/accuracy=0.631500, test/loss=1.822682, test/num_examples=10000, total_duration=92504.541652, train/accuracy=0.961854, train/loss=0.142779, validation/accuracy=0.757140, validation/loss=1.040646, validation/num_examples=50000
I0307 00:28:27.453986 139789525280512 logging_writer.py:48] [265400] global_step=265400, grad_norm=4.694904804229736, loss=0.6289902925491333
I0307 00:29:01.164549 139789021992704 logging_writer.py:48] [265500] global_step=265500, grad_norm=4.243282318115234, loss=0.6022199392318726
I0307 00:29:34.782115 139789525280512 logging_writer.py:48] [265600] global_step=265600, grad_norm=4.3181657791137695, loss=0.6306585073471069
I0307 00:30:08.402550 139789021992704 logging_writer.py:48] [265700] global_step=265700, grad_norm=4.516872406005859, loss=0.6057814955711365
I0307 00:30:42.051676 139789525280512 logging_writer.py:48] [265800] global_step=265800, grad_norm=4.344371795654297, loss=0.6516223549842834
I0307 00:31:15.661830 139789021992704 logging_writer.py:48] [265900] global_step=265900, grad_norm=4.51265811920166, loss=0.6265475749969482
I0307 00:31:49.316285 139789525280512 logging_writer.py:48] [266000] global_step=266000, grad_norm=4.533226013183594, loss=0.608776867389679
I0307 00:32:22.948263 139789021992704 logging_writer.py:48] [266100] global_step=266100, grad_norm=4.933093547821045, loss=0.6262418627738953
I0307 00:32:56.596869 139789525280512 logging_writer.py:48] [266200] global_step=266200, grad_norm=4.598156452178955, loss=0.7121890187263489
I0307 00:33:30.214850 139789021992704 logging_writer.py:48] [266300] global_step=266300, grad_norm=4.549893856048584, loss=0.680321216583252
I0307 00:34:03.864084 139789525280512 logging_writer.py:48] [266400] global_step=266400, grad_norm=4.554095268249512, loss=0.6347839832305908
I0307 00:34:37.476494 139789021992704 logging_writer.py:48] [266500] global_step=266500, grad_norm=4.662151336669922, loss=0.6186641454696655
I0307 00:35:11.128729 139789525280512 logging_writer.py:48] [266600] global_step=266600, grad_norm=4.280725002288818, loss=0.6237955093383789
I0307 00:35:44.697447 139789021992704 logging_writer.py:48] [266700] global_step=266700, grad_norm=4.8679351806640625, loss=0.6717934012413025
I0307 00:36:18.319111 139789525280512 logging_writer.py:48] [266800] global_step=266800, grad_norm=4.300814151763916, loss=0.566136360168457
I0307 00:36:51.416527 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:36:57.515296 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:37:06.292175 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:37:09.202276 139951089751872 submission_runner.py:411] Time since start: 93032.41s, 	Step: 266900, 	{'train/accuracy': 0.9614357352256775, 'train/loss': 0.1437123864889145, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0415598154067993, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.821916103363037, 'test/num_examples': 10000, 'score': 89819.25856900215, 'total_duration': 93032.41458940506, 'accumulated_submission_time': 89819.25856900215, 'accumulated_eval_time': 3191.1166141033173, 'accumulated_logging_time': 12.352385759353638}
I0307 00:37:09.255417 139787629491968 logging_writer.py:48] [266900] accumulated_eval_time=3191.116614, accumulated_logging_time=12.352386, accumulated_submission_time=89819.258569, global_step=266900, preemption_count=0, score=89819.258569, test/accuracy=0.631500, test/loss=1.821916, test/num_examples=10000, total_duration=93032.414589, train/accuracy=0.961436, train/loss=0.143712, validation/accuracy=0.757340, validation/loss=1.041560, validation/num_examples=50000
I0307 00:37:09.603930 139788996814592 logging_writer.py:48] [266900] global_step=266900, grad_norm=4.368839740753174, loss=0.5931061506271362
I0307 00:37:43.197432 139787629491968 logging_writer.py:48] [267000] global_step=267000, grad_norm=4.888307571411133, loss=0.6012226939201355
I0307 00:38:16.825083 139788996814592 logging_writer.py:48] [267100] global_step=267100, grad_norm=4.8071608543396, loss=0.6333314180374146
I0307 00:38:50.439449 139787629491968 logging_writer.py:48] [267200] global_step=267200, grad_norm=4.6150078773498535, loss=0.5765013694763184
I0307 00:39:24.087312 139788996814592 logging_writer.py:48] [267300] global_step=267300, grad_norm=4.849667549133301, loss=0.6406009793281555
I0307 00:39:57.730198 139787629491968 logging_writer.py:48] [267400] global_step=267400, grad_norm=4.353000164031982, loss=0.6467357277870178
I0307 00:40:31.381118 139788996814592 logging_writer.py:48] [267500] global_step=267500, grad_norm=4.3271098136901855, loss=0.5326290726661682
I0307 00:41:05.059583 139787629491968 logging_writer.py:48] [267600] global_step=267600, grad_norm=4.225481033325195, loss=0.5868749618530273
I0307 00:41:38.656556 139788996814592 logging_writer.py:48] [267700] global_step=267700, grad_norm=4.6571197509765625, loss=0.6748502254486084
I0307 00:42:12.262198 139787629491968 logging_writer.py:48] [267800] global_step=267800, grad_norm=5.336413383483887, loss=0.6561840176582336
I0307 00:42:45.917814 139788996814592 logging_writer.py:48] [267900] global_step=267900, grad_norm=4.390798091888428, loss=0.5703754425048828
I0307 00:43:19.533049 139787629491968 logging_writer.py:48] [268000] global_step=268000, grad_norm=4.398344993591309, loss=0.6419658064842224
I0307 00:43:53.187012 139788996814592 logging_writer.py:48] [268100] global_step=268100, grad_norm=4.46876859664917, loss=0.6282145977020264
I0307 00:44:26.847102 139787629491968 logging_writer.py:48] [268200] global_step=268200, grad_norm=4.620763301849365, loss=0.5787384510040283
I0307 00:45:00.525248 139788996814592 logging_writer.py:48] [268300] global_step=268300, grad_norm=4.458981037139893, loss=0.5632956624031067
I0307 00:45:34.181158 139787629491968 logging_writer.py:48] [268400] global_step=268400, grad_norm=4.286862373352051, loss=0.5841521620750427
I0307 00:45:39.373063 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:45:45.399348 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:45:54.144142 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:45:56.444580 139951089751872 submission_runner.py:411] Time since start: 93559.66s, 	Step: 268417, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14577053487300873, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0407840013504028, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.823684811592102, 'test/num_examples': 10000, 'score': 90329.31252336502, 'total_duration': 93559.65687131882, 'accumulated_submission_time': 90329.31252336502, 'accumulated_eval_time': 3208.188056945801, 'accumulated_logging_time': 12.414271593093872}
I0307 00:45:56.505637 139787612706560 logging_writer.py:48] [268417] accumulated_eval_time=3208.188057, accumulated_logging_time=12.414272, accumulated_submission_time=90329.312523, global_step=268417, preemption_count=0, score=90329.312523, test/accuracy=0.631800, test/loss=1.823685, test/num_examples=10000, total_duration=93559.656871, train/accuracy=0.961595, train/loss=0.145771, validation/accuracy=0.757120, validation/loss=1.040784, validation/num_examples=50000
I0307 00:46:24.790792 139787621099264 logging_writer.py:48] [268500] global_step=268500, grad_norm=4.676140308380127, loss=0.6087823510169983
I0307 00:46:58.440557 139787612706560 logging_writer.py:48] [268600] global_step=268600, grad_norm=4.680541038513184, loss=0.6323614120483398
I0307 00:47:32.109464 139787621099264 logging_writer.py:48] [268700] global_step=268700, grad_norm=5.127283573150635, loss=0.7081769108772278
I0307 00:48:05.738731 139787612706560 logging_writer.py:48] [268800] global_step=268800, grad_norm=4.507133960723877, loss=0.6069119572639465
I0307 00:48:39.400390 139787621099264 logging_writer.py:48] [268900] global_step=268900, grad_norm=4.858397483825684, loss=0.5930528044700623
I0307 00:49:13.038285 139787612706560 logging_writer.py:48] [269000] global_step=269000, grad_norm=4.701032638549805, loss=0.6592574715614319
I0307 00:49:46.696573 139787621099264 logging_writer.py:48] [269100] global_step=269100, grad_norm=4.178969860076904, loss=0.6000784039497375
I0307 00:50:20.318075 139787612706560 logging_writer.py:48] [269200] global_step=269200, grad_norm=4.4855055809021, loss=0.6424931883811951
I0307 00:50:53.977312 139787621099264 logging_writer.py:48] [269300] global_step=269300, grad_norm=4.556004524230957, loss=0.5243192911148071
I0307 00:51:27.611886 139787612706560 logging_writer.py:48] [269400] global_step=269400, grad_norm=4.717358589172363, loss=0.5716684460639954
I0307 00:52:01.235892 139787621099264 logging_writer.py:48] [269500] global_step=269500, grad_norm=4.333922386169434, loss=0.6202657222747803
I0307 00:52:34.863776 139787612706560 logging_writer.py:48] [269600] global_step=269600, grad_norm=4.436309814453125, loss=0.6353390216827393
I0307 00:53:08.504722 139787621099264 logging_writer.py:48] [269700] global_step=269700, grad_norm=4.126541614532471, loss=0.6029144525527954
I0307 00:53:42.296955 139787612706560 logging_writer.py:48] [269800] global_step=269800, grad_norm=4.884594917297363, loss=0.6823487877845764
I0307 00:54:15.957397 139787621099264 logging_writer.py:48] [269900] global_step=269900, grad_norm=4.254234313964844, loss=0.6029358506202698
I0307 00:54:26.524671 139951089751872 spec.py:321] Evaluating on the training split.
I0307 00:54:33.181789 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 00:54:41.993463 139951089751872 spec.py:349] Evaluating on the test split.
I0307 00:54:44.271322 139951089751872 submission_runner.py:411] Time since start: 94087.48s, 	Step: 269933, 	{'train/accuracy': 0.9596420526504517, 'train/loss': 0.14781729876995087, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0419752597808838, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8229761123657227, 'test/num_examples': 10000, 'score': 90839.26527810097, 'total_duration': 94087.48363137245, 'accumulated_submission_time': 90839.26527810097, 'accumulated_eval_time': 3225.9346590042114, 'accumulated_logging_time': 12.48537278175354}
I0307 00:54:44.333649 139787621099264 logging_writer.py:48] [269933] accumulated_eval_time=3225.934659, accumulated_logging_time=12.485373, accumulated_submission_time=90839.265278, global_step=269933, preemption_count=0, score=90839.265278, test/accuracy=0.631100, test/loss=1.822976, test/num_examples=10000, total_duration=94087.483631, train/accuracy=0.959642, train/loss=0.147817, validation/accuracy=0.757020, validation/loss=1.041975, validation/num_examples=50000
I0307 00:55:07.227427 139789005207296 logging_writer.py:48] [270000] global_step=270000, grad_norm=4.257288932800293, loss=0.5799696445465088
I0307 00:55:40.815258 139787621099264 logging_writer.py:48] [270100] global_step=270100, grad_norm=3.9026939868927, loss=0.5922210216522217
I0307 00:56:14.448852 139789005207296 logging_writer.py:48] [270200] global_step=270200, grad_norm=3.9551808834075928, loss=0.5330949425697327
I0307 00:56:48.089040 139787621099264 logging_writer.py:48] [270300] global_step=270300, grad_norm=4.641826629638672, loss=0.6089394092559814
I0307 00:57:21.746230 139789005207296 logging_writer.py:48] [270400] global_step=270400, grad_norm=4.493743419647217, loss=0.6276199817657471
I0307 00:57:55.366445 139787621099264 logging_writer.py:48] [270500] global_step=270500, grad_norm=4.772538185119629, loss=0.6346317529678345
I0307 00:58:29.020669 139789005207296 logging_writer.py:48] [270600] global_step=270600, grad_norm=4.718380928039551, loss=0.6279489398002625
I0307 00:59:02.646945 139787621099264 logging_writer.py:48] [270700] global_step=270700, grad_norm=4.394834518432617, loss=0.6337805986404419
I0307 00:59:36.307416 139789005207296 logging_writer.py:48] [270800] global_step=270800, grad_norm=4.377087116241455, loss=0.6234744787216187
I0307 01:00:09.903690 139787621099264 logging_writer.py:48] [270900] global_step=270900, grad_norm=4.238492965698242, loss=0.6446061134338379
I0307 01:00:43.545835 139789005207296 logging_writer.py:48] [271000] global_step=271000, grad_norm=4.990394115447998, loss=0.6977266073226929
I0307 01:01:17.145845 139787621099264 logging_writer.py:48] [271100] global_step=271100, grad_norm=4.7978057861328125, loss=0.6085946559906006
I0307 01:01:50.806203 139789005207296 logging_writer.py:48] [271200] global_step=271200, grad_norm=3.8769967555999756, loss=0.5337865948677063
I0307 01:02:24.422410 139787621099264 logging_writer.py:48] [271300] global_step=271300, grad_norm=4.163140296936035, loss=0.5356310606002808
I0307 01:02:58.075367 139789005207296 logging_writer.py:48] [271400] global_step=271400, grad_norm=4.629561901092529, loss=0.6098856329917908
I0307 01:03:14.350444 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:03:20.388019 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:03:28.971395 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:03:31.339967 139951089751872 submission_runner.py:411] Time since start: 94614.55s, 	Step: 271450, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.14620599150657654, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0409760475158691, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.824073314666748, 'test/num_examples': 10000, 'score': 91349.21398591995, 'total_duration': 94614.55206918716, 'accumulated_submission_time': 91349.21398591995, 'accumulated_eval_time': 3242.923921108246, 'accumulated_logging_time': 12.55991506576538}
I0307 01:03:31.404120 139787612706560 logging_writer.py:48] [271450] accumulated_eval_time=3242.923921, accumulated_logging_time=12.559915, accumulated_submission_time=91349.213986, global_step=271450, preemption_count=0, score=91349.213986, test/accuracy=0.631500, test/loss=1.824073, test/num_examples=10000, total_duration=94614.552069, train/accuracy=0.961137, train/loss=0.146206, validation/accuracy=0.757300, validation/loss=1.040976, validation/num_examples=50000
I0307 01:03:48.546589 139787621099264 logging_writer.py:48] [271500] global_step=271500, grad_norm=4.42553186416626, loss=0.6459375619888306
I0307 01:04:22.116121 139787612706560 logging_writer.py:48] [271600] global_step=271600, grad_norm=4.932106971740723, loss=0.576260507106781
I0307 01:04:55.664597 139787621099264 logging_writer.py:48] [271700] global_step=271700, grad_norm=4.642899513244629, loss=0.6899628639221191
I0307 01:05:29.411430 139787612706560 logging_writer.py:48] [271800] global_step=271800, grad_norm=4.726634502410889, loss=0.6380698084831238
I0307 01:06:03.037469 139787621099264 logging_writer.py:48] [271900] global_step=271900, grad_norm=4.650314807891846, loss=0.6884525418281555
I0307 01:06:36.677567 139787612706560 logging_writer.py:48] [272000] global_step=272000, grad_norm=4.322900772094727, loss=0.6238887310028076
I0307 01:07:10.290259 139787621099264 logging_writer.py:48] [272100] global_step=272100, grad_norm=4.672542572021484, loss=0.6298609375953674
I0307 01:07:43.916379 139787612706560 logging_writer.py:48] [272200] global_step=272200, grad_norm=4.646939277648926, loss=0.5550765991210938
I0307 01:08:17.549447 139787621099264 logging_writer.py:48] [272300] global_step=272300, grad_norm=4.889975547790527, loss=0.7112720012664795
I0307 01:08:51.165927 139787612706560 logging_writer.py:48] [272400] global_step=272400, grad_norm=4.319990158081055, loss=0.5908106565475464
I0307 01:09:24.786364 139787621099264 logging_writer.py:48] [272500] global_step=272500, grad_norm=4.369447231292725, loss=0.6097042560577393
I0307 01:09:58.427585 139787612706560 logging_writer.py:48] [272600] global_step=272600, grad_norm=4.017638206481934, loss=0.545495331287384
I0307 01:10:32.060995 139787621099264 logging_writer.py:48] [272700] global_step=272700, grad_norm=4.511355400085449, loss=0.5316058993339539
I0307 01:11:05.714769 139787612706560 logging_writer.py:48] [272800] global_step=272800, grad_norm=4.359261512756348, loss=0.6599946618080139
I0307 01:11:39.439081 139787621099264 logging_writer.py:48] [272900] global_step=272900, grad_norm=4.2661519050598145, loss=0.562432587146759
I0307 01:12:01.432447 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:12:07.538301 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:12:16.285869 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:12:18.578062 139951089751872 submission_runner.py:411] Time since start: 95141.79s, 	Step: 272967, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14573054015636444, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.041136622428894, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8223321437835693, 'test/num_examples': 10000, 'score': 91859.17581629753, 'total_duration': 95141.79025554657, 'accumulated_submission_time': 91859.17581629753, 'accumulated_eval_time': 3260.069388628006, 'accumulated_logging_time': 12.63509225845337}
I0307 01:12:18.646634 139789005207296 logging_writer.py:48] [272967] accumulated_eval_time=3260.069389, accumulated_logging_time=12.635092, accumulated_submission_time=91859.175816, global_step=272967, preemption_count=0, score=91859.175816, test/accuracy=0.631400, test/loss=1.822332, test/num_examples=10000, total_duration=95141.790256, train/accuracy=0.960818, train/loss=0.145731, validation/accuracy=0.757200, validation/loss=1.041137, validation/num_examples=50000
I0307 01:12:30.074038 139789013600000 logging_writer.py:48] [273000] global_step=273000, grad_norm=4.354787826538086, loss=0.6305707097053528
I0307 01:13:03.731883 139789005207296 logging_writer.py:48] [273100] global_step=273100, grad_norm=4.448992729187012, loss=0.6512851715087891
I0307 01:13:37.378528 139789013600000 logging_writer.py:48] [273200] global_step=273200, grad_norm=5.1873321533203125, loss=0.7632908225059509
I0307 01:14:11.045051 139789005207296 logging_writer.py:48] [273300] global_step=273300, grad_norm=4.567098140716553, loss=0.6798859238624573
I0307 01:14:44.642414 139789013600000 logging_writer.py:48] [273400] global_step=273400, grad_norm=5.27118444442749, loss=0.7568141222000122
I0307 01:15:18.298713 139789005207296 logging_writer.py:48] [273500] global_step=273500, grad_norm=4.48646879196167, loss=0.6465404033660889
I0307 01:15:51.920795 139789013600000 logging_writer.py:48] [273600] global_step=273600, grad_norm=4.362030982971191, loss=0.6190506219863892
I0307 01:16:25.591918 139789005207296 logging_writer.py:48] [273700] global_step=273700, grad_norm=4.685828685760498, loss=0.6652221083641052
I0307 01:16:59.208001 139789013600000 logging_writer.py:48] [273800] global_step=273800, grad_norm=4.569940567016602, loss=0.7047538757324219
I0307 01:17:32.868760 139789005207296 logging_writer.py:48] [273900] global_step=273900, grad_norm=4.367308139801025, loss=0.673978328704834
I0307 01:18:06.516527 139789013600000 logging_writer.py:48] [274000] global_step=274000, grad_norm=4.50153112411499, loss=0.7070014476776123
I0307 01:18:40.148410 139789005207296 logging_writer.py:48] [274100] global_step=274100, grad_norm=4.566812515258789, loss=0.588207483291626
I0307 01:19:13.814599 139789013600000 logging_writer.py:48] [274200] global_step=274200, grad_norm=4.625009059906006, loss=0.6658645272254944
I0307 01:19:47.530591 139789005207296 logging_writer.py:48] [274300] global_step=274300, grad_norm=4.0215535163879395, loss=0.5694177150726318
I0307 01:20:21.144202 139789013600000 logging_writer.py:48] [274400] global_step=274400, grad_norm=4.358298301696777, loss=0.6122149229049683
I0307 01:20:48.887611 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:20:54.872864 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:21:03.621736 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:21:05.827989 139951089751872 submission_runner.py:411] Time since start: 95669.04s, 	Step: 274484, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.1444941759109497, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0405818223953247, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8210731744766235, 'test/num_examples': 10000, 'score': 92369.35075259209, 'total_duration': 95669.04021525383, 'accumulated_submission_time': 92369.35075259209, 'accumulated_eval_time': 3277.009635448456, 'accumulated_logging_time': 12.7144455909729}
I0307 01:21:05.888221 139788996814592 logging_writer.py:48] [274484] accumulated_eval_time=3277.009635, accumulated_logging_time=12.714446, accumulated_submission_time=92369.350753, global_step=274484, preemption_count=0, score=92369.350753, test/accuracy=0.632000, test/loss=1.821073, test/num_examples=10000, total_duration=95669.040215, train/accuracy=0.961137, train/loss=0.144494, validation/accuracy=0.757300, validation/loss=1.040582, validation/num_examples=50000
I0307 01:21:11.591241 139789021992704 logging_writer.py:48] [274500] global_step=274500, grad_norm=4.780895233154297, loss=0.5977954864501953
I0307 01:21:45.194469 139788996814592 logging_writer.py:48] [274600] global_step=274600, grad_norm=4.491494655609131, loss=0.585152268409729
I0307 01:22:18.854278 139789021992704 logging_writer.py:48] [274700] global_step=274700, grad_norm=4.5942559242248535, loss=0.6143216490745544
I0307 01:22:52.531498 139788996814592 logging_writer.py:48] [274800] global_step=274800, grad_norm=4.14816951751709, loss=0.5447300672531128
I0307 01:23:26.185043 139789021992704 logging_writer.py:48] [274900] global_step=274900, grad_norm=4.584615707397461, loss=0.6514582633972168
I0307 01:23:59.875139 139788996814592 logging_writer.py:48] [275000] global_step=275000, grad_norm=4.540388107299805, loss=0.5719478130340576
I0307 01:24:33.491424 139789021992704 logging_writer.py:48] [275100] global_step=275100, grad_norm=4.251744270324707, loss=0.5713937282562256
I0307 01:25:07.125887 139788996814592 logging_writer.py:48] [275200] global_step=275200, grad_norm=4.765773296356201, loss=0.674126148223877
I0307 01:25:40.793335 139789021992704 logging_writer.py:48] [275300] global_step=275300, grad_norm=4.384042739868164, loss=0.6188328266143799
I0307 01:26:14.464163 139788996814592 logging_writer.py:48] [275400] global_step=275400, grad_norm=4.479028701782227, loss=0.6221347451210022
I0307 01:26:48.136659 139789021992704 logging_writer.py:48] [275500] global_step=275500, grad_norm=4.42825174331665, loss=0.6688597202301025
I0307 01:27:21.781060 139788996814592 logging_writer.py:48] [275600] global_step=275600, grad_norm=4.391599178314209, loss=0.6038954257965088
I0307 01:27:55.436322 139789021992704 logging_writer.py:48] [275700] global_step=275700, grad_norm=4.497845649719238, loss=0.6197836399078369
I0307 01:28:29.087932 139788996814592 logging_writer.py:48] [275800] global_step=275800, grad_norm=4.85751485824585, loss=0.6164143085479736
I0307 01:29:02.738500 139789021992704 logging_writer.py:48] [275900] global_step=275900, grad_norm=4.234864711761475, loss=0.5760058164596558
I0307 01:29:35.845624 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:29:42.106401 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:29:50.632921 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:29:52.895183 139951089751872 submission_runner.py:411] Time since start: 96196.11s, 	Step: 276000, 	{'train/accuracy': 0.9621930718421936, 'train/loss': 0.1435670554637909, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.0415067672729492, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8237287998199463, 'test/num_examples': 10000, 'score': 92879.23972916603, 'total_duration': 96196.10747170448, 'accumulated_submission_time': 92879.23972916603, 'accumulated_eval_time': 3294.059141635895, 'accumulated_logging_time': 12.786207437515259}
I0307 01:29:52.960464 139787612706560 logging_writer.py:48] [276000] accumulated_eval_time=3294.059142, accumulated_logging_time=12.786207, accumulated_submission_time=92879.239729, global_step=276000, preemption_count=0, score=92879.239729, test/accuracy=0.631900, test/loss=1.823729, test/num_examples=10000, total_duration=96196.107472, train/accuracy=0.962193, train/loss=0.143567, validation/accuracy=0.757500, validation/loss=1.041507, validation/num_examples=50000
I0307 01:29:53.313921 139787621099264 logging_writer.py:48] [276000] global_step=276000, grad_norm=4.2720561027526855, loss=0.643159031867981
I0307 01:30:26.955684 139787612706560 logging_writer.py:48] [276100] global_step=276100, grad_norm=4.4605326652526855, loss=0.617771565914154
I0307 01:31:00.530664 139787621099264 logging_writer.py:48] [276200] global_step=276200, grad_norm=4.6121015548706055, loss=0.6110610961914062
I0307 01:31:34.101212 139787612706560 logging_writer.py:48] [276300] global_step=276300, grad_norm=4.245400905609131, loss=0.6122164726257324
I0307 01:32:07.764168 139787621099264 logging_writer.py:48] [276400] global_step=276400, grad_norm=4.348729610443115, loss=0.6280628442764282
I0307 01:32:41.374275 139787612706560 logging_writer.py:48] [276500] global_step=276500, grad_norm=4.434859275817871, loss=0.6343204379081726
I0307 01:33:15.023018 139787621099264 logging_writer.py:48] [276600] global_step=276600, grad_norm=4.045760154724121, loss=0.5869312286376953
I0307 01:33:48.625673 139787612706560 logging_writer.py:48] [276700] global_step=276700, grad_norm=4.208967208862305, loss=0.5666736960411072
I0307 01:34:22.291564 139787621099264 logging_writer.py:48] [276800] global_step=276800, grad_norm=4.775726795196533, loss=0.6130830645561218
I0307 01:34:55.907539 139787612706560 logging_writer.py:48] [276900] global_step=276900, grad_norm=4.520890235900879, loss=0.6286039352416992
I0307 01:35:29.568644 139787621099264 logging_writer.py:48] [277000] global_step=277000, grad_norm=4.293696880340576, loss=0.5803778171539307
I0307 01:36:03.244637 139787612706560 logging_writer.py:48] [277100] global_step=277100, grad_norm=4.99053955078125, loss=0.6757660508155823
I0307 01:36:36.849900 139787621099264 logging_writer.py:48] [277200] global_step=277200, grad_norm=4.365700721740723, loss=0.5883091688156128
I0307 01:37:10.491854 139787612706560 logging_writer.py:48] [277300] global_step=277300, grad_norm=4.49996280670166, loss=0.5906208157539368
I0307 01:37:44.106696 139787621099264 logging_writer.py:48] [277400] global_step=277400, grad_norm=4.620162487030029, loss=0.6548525094985962
I0307 01:38:17.693957 139787612706560 logging_writer.py:48] [277500] global_step=277500, grad_norm=4.272085189819336, loss=0.592792272567749
I0307 01:38:23.219744 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:38:29.231476 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:38:37.905293 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:38:40.155638 139951089751872 submission_runner.py:411] Time since start: 96723.37s, 	Step: 277518, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.149187371134758, 'validation/accuracy': 0.7570599913597107, 'validation/loss': 1.0416173934936523, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8231300115585327, 'test/num_examples': 10000, 'score': 93389.43068313599, 'total_duration': 96723.36788201332, 'accumulated_submission_time': 93389.43068313599, 'accumulated_eval_time': 3310.9949176311493, 'accumulated_logging_time': 12.863352537155151}
I0307 01:38:40.218678 139787612706560 logging_writer.py:48] [277518] accumulated_eval_time=3310.994918, accumulated_logging_time=12.863353, accumulated_submission_time=93389.430683, global_step=277518, preemption_count=0, score=93389.430683, test/accuracy=0.631400, test/loss=1.823130, test/num_examples=10000, total_duration=96723.367882, train/accuracy=0.960100, train/loss=0.149187, validation/accuracy=0.757060, validation/loss=1.041617, validation/num_examples=50000
I0307 01:39:08.110555 139787621099264 logging_writer.py:48] [277600] global_step=277600, grad_norm=4.104155540466309, loss=0.5458269715309143
I0307 01:39:41.985022 139787612706560 logging_writer.py:48] [277700] global_step=277700, grad_norm=4.480534076690674, loss=0.6023371815681458
I0307 01:40:15.643675 139787621099264 logging_writer.py:48] [277800] global_step=277800, grad_norm=4.709933280944824, loss=0.6135849952697754
I0307 01:40:49.284148 139787612706560 logging_writer.py:48] [277900] global_step=277900, grad_norm=4.103203773498535, loss=0.5909852385520935
I0307 01:41:22.932559 139787621099264 logging_writer.py:48] [278000] global_step=278000, grad_norm=5.121773719787598, loss=0.6391940712928772
I0307 01:41:56.579947 139787612706560 logging_writer.py:48] [278100] global_step=278100, grad_norm=5.1987786293029785, loss=0.6586993932723999
I0307 01:42:30.230611 139787621099264 logging_writer.py:48] [278200] global_step=278200, grad_norm=4.366944789886475, loss=0.6355005502700806
I0307 01:43:03.842345 139787612706560 logging_writer.py:48] [278300] global_step=278300, grad_norm=4.468437671661377, loss=0.6448060274124146
I0307 01:43:37.463132 139787621099264 logging_writer.py:48] [278400] global_step=278400, grad_norm=4.393139362335205, loss=0.6213172674179077
I0307 01:44:11.171360 139787612706560 logging_writer.py:48] [278500] global_step=278500, grad_norm=4.954208850860596, loss=0.6435856223106384
I0307 01:44:44.843755 139787621099264 logging_writer.py:48] [278600] global_step=278600, grad_norm=4.349488735198975, loss=0.66522616147995
I0307 01:45:18.497180 139787612706560 logging_writer.py:48] [278700] global_step=278700, grad_norm=4.452236652374268, loss=0.6242411136627197
I0307 01:45:52.151786 139787621099264 logging_writer.py:48] [278800] global_step=278800, grad_norm=4.491397857666016, loss=0.6466414928436279
I0307 01:46:25.797274 139787612706560 logging_writer.py:48] [278900] global_step=278900, grad_norm=4.449235439300537, loss=0.624319314956665
I0307 01:46:59.463903 139787621099264 logging_writer.py:48] [279000] global_step=279000, grad_norm=4.327513217926025, loss=0.5274685621261597
I0307 01:47:10.375484 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:47:16.459671 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:47:25.343091 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:47:27.610233 139951089751872 submission_runner.py:411] Time since start: 97250.82s, 	Step: 279034, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.1479669213294983, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0405782461166382, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8226639032363892, 'test/num_examples': 10000, 'score': 93899.51947426796, 'total_duration': 97250.8225440979, 'accumulated_submission_time': 93899.51947426796, 'accumulated_eval_time': 3328.229616165161, 'accumulated_logging_time': 12.936316013336182}
I0307 01:47:27.676020 139787629491968 logging_writer.py:48] [279034] accumulated_eval_time=3328.229616, accumulated_logging_time=12.936316, accumulated_submission_time=93899.519474, global_step=279034, preemption_count=0, score=93899.519474, test/accuracy=0.631100, test/loss=1.822664, test/num_examples=10000, total_duration=97250.822544, train/accuracy=0.961376, train/loss=0.147967, validation/accuracy=0.757040, validation/loss=1.040578, validation/num_examples=50000
I0307 01:47:50.260167 139789005207296 logging_writer.py:48] [279100] global_step=279100, grad_norm=4.722801208496094, loss=0.6123682856559753
I0307 01:48:23.925364 139787629491968 logging_writer.py:48] [279200] global_step=279200, grad_norm=4.608294486999512, loss=0.6224321722984314
I0307 01:48:57.528217 139789005207296 logging_writer.py:48] [279300] global_step=279300, grad_norm=5.118571758270264, loss=0.7188615798950195
I0307 01:49:31.152761 139787629491968 logging_writer.py:48] [279400] global_step=279400, grad_norm=4.398263931274414, loss=0.6171085834503174
I0307 01:50:04.818193 139789005207296 logging_writer.py:48] [279500] global_step=279500, grad_norm=4.2909626960754395, loss=0.5992184281349182
I0307 01:50:38.443245 139787629491968 logging_writer.py:48] [279600] global_step=279600, grad_norm=4.099534034729004, loss=0.5496072173118591
I0307 01:51:12.102683 139789005207296 logging_writer.py:48] [279700] global_step=279700, grad_norm=4.590331554412842, loss=0.6310127973556519
I0307 01:51:45.704695 139787629491968 logging_writer.py:48] [279800] global_step=279800, grad_norm=4.377869606018066, loss=0.6330536603927612
I0307 01:52:19.381424 139789005207296 logging_writer.py:48] [279900] global_step=279900, grad_norm=4.79683256149292, loss=0.6859135031700134
I0307 01:52:53.005953 139787629491968 logging_writer.py:48] [280000] global_step=280000, grad_norm=3.977267026901245, loss=0.6065778732299805
I0307 01:53:26.684918 139789005207296 logging_writer.py:48] [280100] global_step=280100, grad_norm=4.527409076690674, loss=0.6445472240447998
I0307 01:54:00.300905 139787629491968 logging_writer.py:48] [280200] global_step=280200, grad_norm=4.485194683074951, loss=0.6220130324363708
I0307 01:54:34.116339 139789005207296 logging_writer.py:48] [280300] global_step=280300, grad_norm=4.769047260284424, loss=0.6129059791564941
I0307 01:55:07.784778 139787629491968 logging_writer.py:48] [280400] global_step=280400, grad_norm=4.257665157318115, loss=0.6010674238204956
I0307 01:55:41.468400 139789005207296 logging_writer.py:48] [280500] global_step=280500, grad_norm=4.491869926452637, loss=0.6514222621917725
I0307 01:55:57.728467 139951089751872 spec.py:321] Evaluating on the training split.
I0307 01:56:03.769421 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 01:56:12.619974 139951089751872 spec.py:349] Evaluating on the test split.
I0307 01:56:14.827516 139951089751872 submission_runner.py:411] Time since start: 97778.04s, 	Step: 280550, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14436136186122894, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0420703887939453, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8226004838943481, 'test/num_examples': 10000, 'score': 94409.50527763367, 'total_duration': 97778.03981542587, 'accumulated_submission_time': 94409.50527763367, 'accumulated_eval_time': 3345.328625202179, 'accumulated_logging_time': 13.0116868019104}
I0307 01:56:14.894860 139789030385408 logging_writer.py:48] [280550] accumulated_eval_time=3345.328625, accumulated_logging_time=13.011687, accumulated_submission_time=94409.505278, global_step=280550, preemption_count=0, score=94409.505278, test/accuracy=0.631500, test/loss=1.822600, test/num_examples=10000, total_duration=97778.039815, train/accuracy=0.960678, train/loss=0.144361, validation/accuracy=0.757340, validation/loss=1.042070, validation/num_examples=50000
I0307 01:56:32.038269 139789038778112 logging_writer.py:48] [280600] global_step=280600, grad_norm=4.2645463943481445, loss=0.6023901700973511
I0307 01:57:05.645616 139789030385408 logging_writer.py:48] [280700] global_step=280700, grad_norm=4.9840922355651855, loss=0.657996654510498
I0307 01:57:39.268359 139789038778112 logging_writer.py:48] [280800] global_step=280800, grad_norm=4.560242652893066, loss=0.6883463263511658
I0307 01:58:12.894986 139789030385408 logging_writer.py:48] [280900] global_step=280900, grad_norm=4.5045881271362305, loss=0.6474514603614807
I0307 01:58:46.612964 139789038778112 logging_writer.py:48] [281000] global_step=281000, grad_norm=4.205435276031494, loss=0.600021243095398
I0307 01:59:20.228724 139789030385408 logging_writer.py:48] [281100] global_step=281100, grad_norm=4.153668403625488, loss=0.6633806228637695
I0307 01:59:53.870812 139789038778112 logging_writer.py:48] [281200] global_step=281200, grad_norm=4.784725666046143, loss=0.6171994805335999
I0307 02:00:27.504534 139789030385408 logging_writer.py:48] [281300] global_step=281300, grad_norm=5.019192695617676, loss=0.6363531947135925
I0307 02:01:01.311079 139789038778112 logging_writer.py:48] [281400] global_step=281400, grad_norm=4.384760856628418, loss=0.6116076707839966
I0307 02:01:34.963851 139789030385408 logging_writer.py:48] [281500] global_step=281500, grad_norm=4.280579566955566, loss=0.6461712121963501
I0307 02:02:08.638889 139789038778112 logging_writer.py:48] [281600] global_step=281600, grad_norm=4.297664642333984, loss=0.6151576042175293
I0307 02:02:42.267786 139789030385408 logging_writer.py:48] [281700] global_step=281700, grad_norm=4.183349132537842, loss=0.691552996635437
I0307 02:03:15.912042 139789038778112 logging_writer.py:48] [281800] global_step=281800, grad_norm=4.249898910522461, loss=0.5666094422340393
I0307 02:03:49.545978 139789030385408 logging_writer.py:48] [281900] global_step=281900, grad_norm=4.376795768737793, loss=0.6388218998908997
I0307 02:04:23.207129 139789038778112 logging_writer.py:48] [282000] global_step=282000, grad_norm=4.483270645141602, loss=0.6152843832969666
I0307 02:04:44.897925 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:04:50.951312 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:04:59.623248 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:05:01.893757 139951089751872 submission_runner.py:411] Time since start: 98305.11s, 	Step: 282066, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14506347477436066, 'validation/accuracy': 0.7574599981307983, 'validation/loss': 1.0403265953063965, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8230668306350708, 'test/num_examples': 10000, 'score': 94919.44342923164, 'total_duration': 98305.10606789589, 'accumulated_submission_time': 94919.44342923164, 'accumulated_eval_time': 3362.324405670166, 'accumulated_logging_time': 13.08899974822998}
I0307 02:05:01.956080 139787629491968 logging_writer.py:48] [282066] accumulated_eval_time=3362.324406, accumulated_logging_time=13.089000, accumulated_submission_time=94919.443429, global_step=282066, preemption_count=0, score=94919.443429, test/accuracy=0.631800, test/loss=1.823067, test/num_examples=10000, total_duration=98305.106068, train/accuracy=0.960957, train/loss=0.145063, validation/accuracy=0.757460, validation/loss=1.040327, validation/num_examples=50000
I0307 02:05:13.715832 139788996814592 logging_writer.py:48] [282100] global_step=282100, grad_norm=4.360940933227539, loss=0.678500771522522
I0307 02:05:47.263368 139787629491968 logging_writer.py:48] [282200] global_step=282200, grad_norm=4.292849540710449, loss=0.6451776623725891
I0307 02:06:20.869801 139788996814592 logging_writer.py:48] [282300] global_step=282300, grad_norm=4.274602890014648, loss=0.5669238567352295
I0307 02:06:54.526772 139787629491968 logging_writer.py:48] [282400] global_step=282400, grad_norm=4.082711219787598, loss=0.5600674152374268
I0307 02:07:28.118987 139788996814592 logging_writer.py:48] [282500] global_step=282500, grad_norm=4.383365154266357, loss=0.6011677980422974
I0307 02:08:01.781221 139787629491968 logging_writer.py:48] [282600] global_step=282600, grad_norm=4.420780658721924, loss=0.5823466181755066
I0307 02:08:35.404129 139788996814592 logging_writer.py:48] [282700] global_step=282700, grad_norm=4.141229629516602, loss=0.5924920439720154
I0307 02:09:09.061418 139787629491968 logging_writer.py:48] [282800] global_step=282800, grad_norm=4.302730560302734, loss=0.537487268447876
I0307 02:09:42.680944 139788996814592 logging_writer.py:48] [282900] global_step=282900, grad_norm=4.609583854675293, loss=0.6474705934524536
I0307 02:10:16.353201 139787629491968 logging_writer.py:48] [283000] global_step=283000, grad_norm=4.560438632965088, loss=0.6668000817298889
I0307 02:10:49.952080 139788996814592 logging_writer.py:48] [283100] global_step=283100, grad_norm=4.403046131134033, loss=0.5479134917259216
I0307 02:11:23.618332 139787629491968 logging_writer.py:48] [283200] global_step=283200, grad_norm=4.831582069396973, loss=0.7246929407119751
I0307 02:11:57.235439 139788996814592 logging_writer.py:48] [283300] global_step=283300, grad_norm=4.722609996795654, loss=0.699599027633667
I0307 02:12:30.890519 139787629491968 logging_writer.py:48] [283400] global_step=283400, grad_norm=4.146053791046143, loss=0.6259229183197021
I0307 02:13:04.661508 139788996814592 logging_writer.py:48] [283500] global_step=283500, grad_norm=4.363698959350586, loss=0.6454786062240601
I0307 02:13:32.068761 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:13:38.210782 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:13:46.812714 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:13:49.080035 139951089751872 submission_runner.py:411] Time since start: 98832.29s, 	Step: 283583, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14860162138938904, 'validation/accuracy': 0.7568999528884888, 'validation/loss': 1.0414470434188843, 'validation/num_examples': 50000, 'test/accuracy': 0.632900059223175, 'test/loss': 1.8238024711608887, 'test/num_examples': 10000, 'score': 95429.4895875454, 'total_duration': 98832.29233837128, 'accumulated_submission_time': 95429.4895875454, 'accumulated_eval_time': 3379.3356223106384, 'accumulated_logging_time': 13.161826610565186}
I0307 02:13:49.148152 139787629491968 logging_writer.py:48] [283583] accumulated_eval_time=3379.335622, accumulated_logging_time=13.161827, accumulated_submission_time=95429.489588, global_step=283583, preemption_count=0, score=95429.489588, test/accuracy=0.632900, test/loss=1.823802, test/num_examples=10000, total_duration=98832.292338, train/accuracy=0.959881, train/loss=0.148602, validation/accuracy=0.756900, validation/loss=1.041447, validation/num_examples=50000
I0307 02:13:55.213387 139789021992704 logging_writer.py:48] [283600] global_step=283600, grad_norm=4.887729644775391, loss=0.6407855749130249
I0307 02:14:28.829332 139787629491968 logging_writer.py:48] [283700] global_step=283700, grad_norm=4.187840938568115, loss=0.6441133618354797
I0307 02:15:02.398758 139789021992704 logging_writer.py:48] [283800] global_step=283800, grad_norm=4.681454658508301, loss=0.6639150381088257
I0307 02:15:35.960869 139787629491968 logging_writer.py:48] [283900] global_step=283900, grad_norm=4.476820945739746, loss=0.5777679085731506
I0307 02:16:09.533970 139789021992704 logging_writer.py:48] [284000] global_step=284000, grad_norm=4.671131610870361, loss=0.572724461555481
I0307 02:16:43.183272 139787629491968 logging_writer.py:48] [284100] global_step=284100, grad_norm=4.5851006507873535, loss=0.6512450575828552
I0307 02:17:16.832140 139789021992704 logging_writer.py:48] [284200] global_step=284200, grad_norm=4.463210105895996, loss=0.6035546064376831
I0307 02:17:50.496941 139787629491968 logging_writer.py:48] [284300] global_step=284300, grad_norm=4.528252601623535, loss=0.560490608215332
I0307 02:18:24.158969 139789021992704 logging_writer.py:48] [284400] global_step=284400, grad_norm=4.8768110275268555, loss=0.6592628359794617
I0307 02:18:57.848191 139787629491968 logging_writer.py:48] [284500] global_step=284500, grad_norm=4.276656150817871, loss=0.6349676251411438
I0307 02:19:31.426423 139789021992704 logging_writer.py:48] [284600] global_step=284600, grad_norm=4.591350555419922, loss=0.6702663898468018
I0307 02:20:05.004345 139787629491968 logging_writer.py:48] [284700] global_step=284700, grad_norm=4.095932483673096, loss=0.5579683780670166
I0307 02:20:38.597997 139789021992704 logging_writer.py:48] [284800] global_step=284800, grad_norm=4.611328125, loss=0.634940505027771
I0307 02:21:12.243244 139787629491968 logging_writer.py:48] [284900] global_step=284900, grad_norm=4.781695365905762, loss=0.6189846396446228
I0307 02:21:45.877188 139789021992704 logging_writer.py:48] [285000] global_step=285000, grad_norm=4.473758220672607, loss=0.6747373342514038
I0307 02:22:19.532917 139787629491968 logging_writer.py:48] [285100] global_step=285100, grad_norm=4.882164478302002, loss=0.6514148116111755
I0307 02:22:19.539504 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:22:25.524508 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:22:34.337201 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:22:36.622841 139951089751872 submission_runner.py:411] Time since start: 99359.84s, 	Step: 285101, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14797185361385345, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0402021408081055, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8214406967163086, 'test/num_examples': 10000, 'score': 95939.8146879673, 'total_duration': 99359.83514142036, 'accumulated_submission_time': 95939.8146879673, 'accumulated_eval_time': 3396.4188709259033, 'accumulated_logging_time': 13.239661693572998}
I0307 02:22:36.688235 139789005207296 logging_writer.py:48] [285101] accumulated_eval_time=3396.418871, accumulated_logging_time=13.239662, accumulated_submission_time=95939.814688, global_step=285101, preemption_count=0, score=95939.814688, test/accuracy=0.631800, test/loss=1.821441, test/num_examples=10000, total_duration=99359.835141, train/accuracy=0.960061, train/loss=0.147972, validation/accuracy=0.757040, validation/loss=1.040202, validation/num_examples=50000
I0307 02:23:10.285152 139789013600000 logging_writer.py:48] [285200] global_step=285200, grad_norm=4.550444602966309, loss=0.6070451736450195
I0307 02:23:43.946369 139789005207296 logging_writer.py:48] [285300] global_step=285300, grad_norm=4.4847564697265625, loss=0.6451638340950012
I0307 02:24:17.554302 139789013600000 logging_writer.py:48] [285400] global_step=285400, grad_norm=4.397223949432373, loss=0.6242204308509827
I0307 02:24:51.210745 139789005207296 logging_writer.py:48] [285500] global_step=285500, grad_norm=4.799556255340576, loss=0.5975641012191772
I0307 02:25:24.873180 139789013600000 logging_writer.py:48] [285600] global_step=285600, grad_norm=4.700735569000244, loss=0.5972428917884827
I0307 02:25:58.483167 139789005207296 logging_writer.py:48] [285700] global_step=285700, grad_norm=4.292568683624268, loss=0.632088840007782
I0307 02:26:32.188170 139789013600000 logging_writer.py:48] [285800] global_step=285800, grad_norm=4.7498698234558105, loss=0.6850922107696533
I0307 02:27:05.862531 139789005207296 logging_writer.py:48] [285900] global_step=285900, grad_norm=4.801862716674805, loss=0.6034970283508301
I0307 02:27:39.511764 139789013600000 logging_writer.py:48] [286000] global_step=286000, grad_norm=4.66898775100708, loss=0.6014505624771118
I0307 02:28:13.205786 139789005207296 logging_writer.py:48] [286100] global_step=286100, grad_norm=4.634661674499512, loss=0.6212588548660278
I0307 02:28:46.829570 139789013600000 logging_writer.py:48] [286200] global_step=286200, grad_norm=4.153439998626709, loss=0.5866696238517761
I0307 02:29:20.450610 139789005207296 logging_writer.py:48] [286300] global_step=286300, grad_norm=4.34718132019043, loss=0.5954872369766235
I0307 02:29:54.075969 139789013600000 logging_writer.py:48] [286400] global_step=286400, grad_norm=4.665277004241943, loss=0.6234380602836609
I0307 02:30:27.730175 139789005207296 logging_writer.py:48] [286500] global_step=286500, grad_norm=4.6287336349487305, loss=0.6431280374526978
I0307 02:31:01.400917 139789013600000 logging_writer.py:48] [286600] global_step=286600, grad_norm=4.671159744262695, loss=0.6301548480987549
I0307 02:31:06.923152 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:31:12.954670 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:31:21.512140 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:31:23.828960 139951089751872 submission_runner.py:411] Time since start: 99887.04s, 	Step: 286618, 	{'train/accuracy': 0.9597417116165161, 'train/loss': 0.14933490753173828, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.040514349937439, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.822487235069275, 'test/num_examples': 10000, 'score': 96449.9844045639, 'total_duration': 99887.04127049446, 'accumulated_submission_time': 96449.9844045639, 'accumulated_eval_time': 3413.324624300003, 'accumulated_logging_time': 13.314711093902588}
I0307 02:31:23.898023 139788996814592 logging_writer.py:48] [286618] accumulated_eval_time=3413.324624, accumulated_logging_time=13.314711, accumulated_submission_time=96449.984405, global_step=286618, preemption_count=0, score=96449.984405, test/accuracy=0.631200, test/loss=1.822487, test/num_examples=10000, total_duration=99887.041270, train/accuracy=0.959742, train/loss=0.149335, validation/accuracy=0.757120, validation/loss=1.040514, validation/num_examples=50000
I0307 02:31:51.779988 139789021992704 logging_writer.py:48] [286700] global_step=286700, grad_norm=4.460421085357666, loss=0.6104735136032104
I0307 02:32:25.414585 139788996814592 logging_writer.py:48] [286800] global_step=286800, grad_norm=4.4643707275390625, loss=0.604451060295105
I0307 02:33:16.950853 139789021992704 logging_writer.py:48] [286900] global_step=286900, grad_norm=4.465528964996338, loss=0.6383940577507019
I0307 02:33:50.558416 139788996814592 logging_writer.py:48] [287000] global_step=287000, grad_norm=4.606258869171143, loss=0.6020829677581787
I0307 02:34:24.170394 139789021992704 logging_writer.py:48] [287100] global_step=287100, grad_norm=4.745009899139404, loss=0.6087661981582642
I0307 02:34:57.839599 139788996814592 logging_writer.py:48] [287200] global_step=287200, grad_norm=4.739335060119629, loss=0.6594573855400085
I0307 02:35:31.468320 139789021992704 logging_writer.py:48] [287300] global_step=287300, grad_norm=4.274484157562256, loss=0.6123911142349243
I0307 02:36:05.105056 139788996814592 logging_writer.py:48] [287400] global_step=287400, grad_norm=4.449200630187988, loss=0.6482152342796326
I0307 02:36:38.721620 139789021992704 logging_writer.py:48] [287500] global_step=287500, grad_norm=4.407384395599365, loss=0.6827737092971802
I0307 02:37:12.372412 139788996814592 logging_writer.py:48] [287600] global_step=287600, grad_norm=4.244195938110352, loss=0.5326442122459412
I0307 02:37:46.062277 139789021992704 logging_writer.py:48] [287700] global_step=287700, grad_norm=4.305972576141357, loss=0.6189966201782227
I0307 02:38:19.630467 139788996814592 logging_writer.py:48] [287800] global_step=287800, grad_norm=4.989455699920654, loss=0.6496677398681641
I0307 02:38:53.240957 139789021992704 logging_writer.py:48] [287900] global_step=287900, grad_norm=4.310299396514893, loss=0.5826346278190613
I0307 02:39:26.915213 139788996814592 logging_writer.py:48] [288000] global_step=288000, grad_norm=4.801758289337158, loss=0.65401691198349
I0307 02:39:53.973184 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:40:00.058904 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:40:08.833195 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:40:11.042147 139951089751872 submission_runner.py:411] Time since start: 100414.25s, 	Step: 288082, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14409057796001434, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.042284607887268, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8242065906524658, 'test/num_examples': 10000, 'score': 96959.99516606331, 'total_duration': 100414.2544465065, 'accumulated_submission_time': 96959.99516606331, 'accumulated_eval_time': 3430.393528699875, 'accumulated_logging_time': 13.393523693084717}
I0307 02:40:11.103646 139789013600000 logging_writer.py:48] [288082] accumulated_eval_time=3430.393529, accumulated_logging_time=13.393524, accumulated_submission_time=96959.995166, global_step=288082, preemption_count=0, score=96959.995166, test/accuracy=0.632000, test/loss=1.824207, test/num_examples=10000, total_duration=100414.254447, train/accuracy=0.961735, train/loss=0.144091, validation/accuracy=0.757000, validation/loss=1.042285, validation/num_examples=50000
I0307 02:40:17.497414 139789038778112 logging_writer.py:48] [288100] global_step=288100, grad_norm=4.523069858551025, loss=0.6543768048286438
I0307 02:40:51.139022 139789013600000 logging_writer.py:48] [288200] global_step=288200, grad_norm=4.41271448135376, loss=0.6677849292755127
I0307 02:41:24.741588 139789038778112 logging_writer.py:48] [288300] global_step=288300, grad_norm=4.56324577331543, loss=0.6798440217971802
I0307 02:41:58.338798 139789013600000 logging_writer.py:48] [288400] global_step=288400, grad_norm=4.327621936798096, loss=0.610143780708313
I0307 02:42:31.942408 139789038778112 logging_writer.py:48] [288500] global_step=288500, grad_norm=5.057437419891357, loss=0.6417317986488342
I0307 02:43:05.590786 139789013600000 logging_writer.py:48] [288600] global_step=288600, grad_norm=4.634549140930176, loss=0.6728674173355103
I0307 02:43:39.338977 139789038778112 logging_writer.py:48] [288700] global_step=288700, grad_norm=4.723232269287109, loss=0.5798760056495667
I0307 02:44:12.932745 139789013600000 logging_writer.py:48] [288800] global_step=288800, grad_norm=4.777003288269043, loss=0.6554648876190186
I0307 02:44:46.586104 139789038778112 logging_writer.py:48] [288900] global_step=288900, grad_norm=4.468819618225098, loss=0.6337195038795471
I0307 02:45:20.216649 139789013600000 logging_writer.py:48] [289000] global_step=289000, grad_norm=4.314211368560791, loss=0.6401681900024414
I0307 02:45:53.862843 139789038778112 logging_writer.py:48] [289100] global_step=289100, grad_norm=4.178457736968994, loss=0.6162771582603455
I0307 02:46:27.484635 139789013600000 logging_writer.py:48] [289200] global_step=289200, grad_norm=5.535001754760742, loss=0.6017888784408569
I0307 02:47:01.123232 139789038778112 logging_writer.py:48] [289300] global_step=289300, grad_norm=4.355020523071289, loss=0.6345680952072144
I0307 02:47:34.755771 139789013600000 logging_writer.py:48] [289400] global_step=289400, grad_norm=4.459950923919678, loss=0.6182230710983276
I0307 02:48:08.408393 139789038778112 logging_writer.py:48] [289500] global_step=289500, grad_norm=4.495329856872559, loss=0.5927282571792603
I0307 02:48:41.155982 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:48:47.158309 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:48:55.821385 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:48:58.090934 139951089751872 submission_runner.py:411] Time since start: 100941.30s, 	Step: 289599, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.15086126327514648, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0402519702911377, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.821026086807251, 'test/num_examples': 10000, 'score': 97469.9808113575, 'total_duration': 100941.30323529243, 'accumulated_submission_time': 97469.9808113575, 'accumulated_eval_time': 3447.3284227848053, 'accumulated_logging_time': 13.465181112289429}
I0307 02:48:58.155135 139788996814592 logging_writer.py:48] [289599] accumulated_eval_time=3447.328423, accumulated_logging_time=13.465181, accumulated_submission_time=97469.980811, global_step=289599, preemption_count=0, score=97469.980811, test/accuracy=0.631500, test/loss=1.821026, test/num_examples=10000, total_duration=100941.303235, train/accuracy=0.960419, train/loss=0.150861, validation/accuracy=0.757280, validation/loss=1.040252, validation/num_examples=50000
I0307 02:48:58.835346 139789005207296 logging_writer.py:48] [289600] global_step=289600, grad_norm=4.310007572174072, loss=0.6094303131103516
I0307 02:49:32.446704 139788996814592 logging_writer.py:48] [289700] global_step=289700, grad_norm=4.693676471710205, loss=0.6698833703994751
I0307 02:50:06.107248 139789005207296 logging_writer.py:48] [289800] global_step=289800, grad_norm=4.429287433624268, loss=0.5789733529090881
I0307 02:50:39.705043 139788996814592 logging_writer.py:48] [289900] global_step=289900, grad_norm=4.736947059631348, loss=0.6525606513023376
I0307 02:51:13.286545 139789005207296 logging_writer.py:48] [290000] global_step=290000, grad_norm=4.153997421264648, loss=0.554543673992157
I0307 02:51:46.868028 139788996814592 logging_writer.py:48] [290100] global_step=290100, grad_norm=4.347906112670898, loss=0.6623795628547668
I0307 02:52:20.462352 139789005207296 logging_writer.py:48] [290200] global_step=290200, grad_norm=4.624655246734619, loss=0.6273559927940369
I0307 02:52:54.124454 139788996814592 logging_writer.py:48] [290300] global_step=290300, grad_norm=4.971320152282715, loss=0.6539146900177002
I0307 02:53:27.756902 139789005207296 logging_writer.py:48] [290400] global_step=290400, grad_norm=4.517141819000244, loss=0.6159641146659851
I0307 02:54:01.432793 139788996814592 logging_writer.py:48] [290500] global_step=290500, grad_norm=4.382098197937012, loss=0.5868929028511047
I0307 02:54:35.050582 139789005207296 logging_writer.py:48] [290600] global_step=290600, grad_norm=4.511032581329346, loss=0.6421425342559814
I0307 02:55:08.712959 139788996814592 logging_writer.py:48] [290700] global_step=290700, grad_norm=4.237687110900879, loss=0.6456038355827332
I0307 02:55:42.351716 139789005207296 logging_writer.py:48] [290800] global_step=290800, grad_norm=4.080038070678711, loss=0.5749925374984741
I0307 02:56:15.983168 139788996814592 logging_writer.py:48] [290900] global_step=290900, grad_norm=4.454989433288574, loss=0.6756885051727295
I0307 02:56:49.573335 139789005207296 logging_writer.py:48] [291000] global_step=291000, grad_norm=5.264901161193848, loss=0.6701166033744812
I0307 02:57:23.240865 139788996814592 logging_writer.py:48] [291100] global_step=291100, grad_norm=4.309360980987549, loss=0.6112200617790222
I0307 02:57:28.110340 139951089751872 spec.py:321] Evaluating on the training split.
I0307 02:57:34.298825 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 02:57:43.037725 139951089751872 spec.py:349] Evaluating on the test split.
I0307 02:57:45.326184 139951089751872 submission_runner.py:411] Time since start: 101468.54s, 	Step: 291116, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14596088230609894, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0409700870513916, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8246285915374756, 'test/num_examples': 10000, 'score': 97979.8696899414, 'total_duration': 101468.5384926796, 'accumulated_submission_time': 97979.8696899414, 'accumulated_eval_time': 3464.5442264080048, 'accumulated_logging_time': 13.540786027908325}
I0307 02:57:45.390949 139787612706560 logging_writer.py:48] [291116] accumulated_eval_time=3464.544226, accumulated_logging_time=13.540786, accumulated_submission_time=97979.869690, global_step=291116, preemption_count=0, score=97979.869690, test/accuracy=0.631900, test/loss=1.824629, test/num_examples=10000, total_duration=101468.538493, train/accuracy=0.960818, train/loss=0.145961, validation/accuracy=0.757120, validation/loss=1.040970, validation/num_examples=50000
I0307 02:58:14.008663 139787621099264 logging_writer.py:48] [291200] global_step=291200, grad_norm=4.592296600341797, loss=0.7125830054283142
I0307 02:58:47.616539 139787612706560 logging_writer.py:48] [291300] global_step=291300, grad_norm=4.9769816398620605, loss=0.6442224383354187
I0307 02:59:21.263604 139787621099264 logging_writer.py:48] [291400] global_step=291400, grad_norm=4.548187255859375, loss=0.6216845512390137
I0307 02:59:54.873923 139787612706560 logging_writer.py:48] [291500] global_step=291500, grad_norm=4.489994525909424, loss=0.631767749786377
I0307 03:00:28.526164 139787621099264 logging_writer.py:48] [291600] global_step=291600, grad_norm=4.812313556671143, loss=0.6978685259819031
I0307 03:01:02.150023 139787612706560 logging_writer.py:48] [291700] global_step=291700, grad_norm=4.345152854919434, loss=0.6306062936782837
I0307 03:01:35.797224 139787621099264 logging_writer.py:48] [291800] global_step=291800, grad_norm=4.358542442321777, loss=0.603100597858429
I0307 03:02:09.446215 139787612706560 logging_writer.py:48] [291900] global_step=291900, grad_norm=4.623527526855469, loss=0.5670093894004822
I0307 03:02:43.066612 139787621099264 logging_writer.py:48] [292000] global_step=292000, grad_norm=4.288510322570801, loss=0.6212590336799622
I0307 03:03:16.736687 139787612706560 logging_writer.py:48] [292100] global_step=292100, grad_norm=4.856346130371094, loss=0.6891380548477173
I0307 03:03:50.428286 139787621099264 logging_writer.py:48] [292200] global_step=292200, grad_norm=4.6841325759887695, loss=0.7000805139541626
I0307 03:04:24.070685 139787612706560 logging_writer.py:48] [292300] global_step=292300, grad_norm=4.369129180908203, loss=0.5791753530502319
I0307 03:04:57.720992 139787621099264 logging_writer.py:48] [292400] global_step=292400, grad_norm=4.4516215324401855, loss=0.6181124448776245
I0307 03:05:31.363065 139787612706560 logging_writer.py:48] [292500] global_step=292500, grad_norm=4.48817777633667, loss=0.6416904330253601
I0307 03:06:05.016704 139787621099264 logging_writer.py:48] [292600] global_step=292600, grad_norm=4.3176751136779785, loss=0.5963801741600037
I0307 03:06:15.598583 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:06:21.691831 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:06:30.415533 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:06:32.772741 139951089751872 submission_runner.py:411] Time since start: 101995.99s, 	Step: 292633, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14557455480098724, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0424156188964844, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8246649503707886, 'test/num_examples': 10000, 'score': 98490.01154613495, 'total_duration': 101995.98503899574, 'accumulated_submission_time': 98490.01154613495, 'accumulated_eval_time': 3481.718322277069, 'accumulated_logging_time': 13.615017414093018}
I0307 03:06:32.837272 139789005207296 logging_writer.py:48] [292633] accumulated_eval_time=3481.718322, accumulated_logging_time=13.615017, accumulated_submission_time=98490.011546, global_step=292633, preemption_count=0, score=98490.011546, test/accuracy=0.631300, test/loss=1.824665, test/num_examples=10000, total_duration=101995.985039, train/accuracy=0.960639, train/loss=0.145575, validation/accuracy=0.757200, validation/loss=1.042416, validation/num_examples=50000
I0307 03:06:55.675598 139789021992704 logging_writer.py:48] [292700] global_step=292700, grad_norm=4.383730888366699, loss=0.6551785469055176
I0307 03:07:29.225169 139789005207296 logging_writer.py:48] [292800] global_step=292800, grad_norm=3.7927002906799316, loss=0.5527154803276062
I0307 03:08:02.855154 139789021992704 logging_writer.py:48] [292900] global_step=292900, grad_norm=4.698375701904297, loss=0.5962715744972229
I0307 03:08:36.464227 139789005207296 logging_writer.py:48] [293000] global_step=293000, grad_norm=4.17368745803833, loss=0.6002621054649353
I0307 03:09:10.085625 139789021992704 logging_writer.py:48] [293100] global_step=293100, grad_norm=5.286590099334717, loss=0.6946008205413818
I0307 03:09:43.730034 139789005207296 logging_writer.py:48] [293200] global_step=293200, grad_norm=4.412501811981201, loss=0.567523181438446
I0307 03:10:17.360964 139789021992704 logging_writer.py:48] [293300] global_step=293300, grad_norm=4.680639743804932, loss=0.6341869831085205
I0307 03:10:50.981256 139789005207296 logging_writer.py:48] [293400] global_step=293400, grad_norm=4.305919647216797, loss=0.6645952463150024
I0307 03:11:24.615631 139789021992704 logging_writer.py:48] [293500] global_step=293500, grad_norm=4.652797698974609, loss=0.6052753925323486
I0307 03:11:58.249892 139789005207296 logging_writer.py:48] [293600] global_step=293600, grad_norm=4.404393196105957, loss=0.6372926831245422
I0307 03:12:31.878431 139789021992704 logging_writer.py:48] [293700] global_step=293700, grad_norm=4.1746368408203125, loss=0.6469796895980835
I0307 03:13:05.498294 139789005207296 logging_writer.py:48] [293800] global_step=293800, grad_norm=4.7994608879089355, loss=0.6606156229972839
I0307 03:13:39.099794 139789021992704 logging_writer.py:48] [293900] global_step=293900, grad_norm=4.226372241973877, loss=0.6047834157943726
I0307 03:14:12.758181 139789005207296 logging_writer.py:48] [294000] global_step=294000, grad_norm=4.209926128387451, loss=0.5652151107788086
I0307 03:14:46.360858 139789021992704 logging_writer.py:48] [294100] global_step=294100, grad_norm=4.765948295593262, loss=0.610383927822113
I0307 03:15:02.966063 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:15:08.953025 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:15:17.622533 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:15:19.898826 139951089751872 submission_runner.py:411] Time since start: 102523.11s, 	Step: 294151, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.1481228917837143, 'validation/accuracy': 0.756659984588623, 'validation/loss': 1.041306495666504, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.823425531387329, 'test/num_examples': 10000, 'score': 99000.07468652725, 'total_duration': 102523.11113667488, 'accumulated_submission_time': 99000.07468652725, 'accumulated_eval_time': 3498.651032447815, 'accumulated_logging_time': 13.689282655715942}
I0307 03:15:19.964563 139789013600000 logging_writer.py:48] [294151] accumulated_eval_time=3498.651032, accumulated_logging_time=13.689283, accumulated_submission_time=99000.074687, global_step=294151, preemption_count=0, score=99000.074687, test/accuracy=0.630700, test/loss=1.823426, test/num_examples=10000, total_duration=102523.111137, train/accuracy=0.959682, train/loss=0.148123, validation/accuracy=0.756660, validation/loss=1.041306, validation/num_examples=50000
I0307 03:15:36.811158 139789030385408 logging_writer.py:48] [294200] global_step=294200, grad_norm=4.314610004425049, loss=0.6388647556304932
I0307 03:16:10.430216 139789013600000 logging_writer.py:48] [294300] global_step=294300, grad_norm=4.473243236541748, loss=0.6672570705413818
I0307 03:16:44.030027 139789030385408 logging_writer.py:48] [294400] global_step=294400, grad_norm=4.318337440490723, loss=0.5980764627456665
I0307 03:17:17.628457 139789013600000 logging_writer.py:48] [294500] global_step=294500, grad_norm=4.415658473968506, loss=0.6793939471244812
I0307 03:17:51.261422 139789030385408 logging_writer.py:48] [294600] global_step=294600, grad_norm=4.4633378982543945, loss=0.6127889156341553
I0307 03:18:24.953171 139789013600000 logging_writer.py:48] [294700] global_step=294700, grad_norm=4.2617363929748535, loss=0.6319139003753662
I0307 03:18:58.570018 139789030385408 logging_writer.py:48] [294800] global_step=294800, grad_norm=4.665643692016602, loss=0.6246209740638733
I0307 03:19:32.230076 139789013600000 logging_writer.py:48] [294900] global_step=294900, grad_norm=4.143207550048828, loss=0.5870850682258606
I0307 03:20:05.927237 139789030385408 logging_writer.py:48] [295000] global_step=295000, grad_norm=4.053711891174316, loss=0.6224339008331299
I0307 03:20:39.480355 139789013600000 logging_writer.py:48] [295100] global_step=295100, grad_norm=4.416470527648926, loss=0.6098288893699646
I0307 03:21:13.077601 139789030385408 logging_writer.py:48] [295200] global_step=295200, grad_norm=4.965633869171143, loss=0.6750697493553162
I0307 03:21:46.669018 139789013600000 logging_writer.py:48] [295300] global_step=295300, grad_norm=4.456474304199219, loss=0.6550708413124084
I0307 03:22:20.246338 139789030385408 logging_writer.py:48] [295400] global_step=295400, grad_norm=4.882805824279785, loss=0.6484763622283936
I0307 03:22:53.827353 139789013600000 logging_writer.py:48] [295500] global_step=295500, grad_norm=4.177431106567383, loss=0.5780856013298035
I0307 03:23:27.432281 139789030385408 logging_writer.py:48] [295600] global_step=295600, grad_norm=4.5894622802734375, loss=0.6339729428291321
I0307 03:23:50.126627 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:23:56.137525 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:24:04.819259 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:24:07.080862 139951089751872 submission_runner.py:411] Time since start: 103050.29s, 	Step: 295669, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14685218036174774, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.0419366359710693, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8231909275054932, 'test/num_examples': 10000, 'score': 99510.17113494873, 'total_duration': 103050.29315805435, 'accumulated_submission_time': 99510.17113494873, 'accumulated_eval_time': 3515.6052017211914, 'accumulated_logging_time': 13.764830350875854}
I0307 03:24:07.151439 139789005207296 logging_writer.py:48] [295669] accumulated_eval_time=3515.605202, accumulated_logging_time=13.764830, accumulated_submission_time=99510.171135, global_step=295669, preemption_count=0, score=99510.171135, test/accuracy=0.631200, test/loss=1.823191, test/num_examples=10000, total_duration=103050.293158, train/accuracy=0.960459, train/loss=0.146852, validation/accuracy=0.757140, validation/loss=1.041937, validation/num_examples=50000
I0307 03:24:17.890018 139789021992704 logging_writer.py:48] [295700] global_step=295700, grad_norm=4.5189080238342285, loss=0.6594641804695129
I0307 03:24:51.503727 139789005207296 logging_writer.py:48] [295800] global_step=295800, grad_norm=4.641392230987549, loss=0.6481207013130188
I0307 03:25:25.154281 139789021992704 logging_writer.py:48] [295900] global_step=295900, grad_norm=4.2063446044921875, loss=0.6303827166557312
I0307 03:25:58.832679 139789005207296 logging_writer.py:48] [296000] global_step=296000, grad_norm=4.335887908935547, loss=0.5794262886047363
I0307 03:26:32.487527 139789021992704 logging_writer.py:48] [296100] global_step=296100, grad_norm=4.691243648529053, loss=0.6029759049415588
I0307 03:27:06.108902 139789005207296 logging_writer.py:48] [296200] global_step=296200, grad_norm=4.582352161407471, loss=0.6640845537185669
I0307 03:27:39.765473 139789021992704 logging_writer.py:48] [296300] global_step=296300, grad_norm=4.20486307144165, loss=0.5728641748428345
I0307 03:28:13.413202 139789005207296 logging_writer.py:48] [296400] global_step=296400, grad_norm=4.949381351470947, loss=0.6928310394287109
I0307 03:28:47.086910 139789021992704 logging_writer.py:48] [296500] global_step=296500, grad_norm=4.6271891593933105, loss=0.6629109382629395
I0307 03:29:20.756043 139789005207296 logging_writer.py:48] [296600] global_step=296600, grad_norm=4.6970534324646, loss=0.5912123918533325
I0307 03:29:54.392864 139789021992704 logging_writer.py:48] [296700] global_step=296700, grad_norm=4.218509197235107, loss=0.6271121501922607
I0307 03:30:28.040093 139789005207296 logging_writer.py:48] [296800] global_step=296800, grad_norm=4.625411033630371, loss=0.5638065934181213
I0307 03:31:01.689168 139789021992704 logging_writer.py:48] [296900] global_step=296900, grad_norm=4.32174825668335, loss=0.5877233147621155
I0307 03:31:35.325021 139789005207296 logging_writer.py:48] [297000] global_step=297000, grad_norm=4.988763332366943, loss=0.6669527888298035
I0307 03:32:08.957338 139789021992704 logging_writer.py:48] [297100] global_step=297100, grad_norm=4.454010963439941, loss=0.5795155763626099
I0307 03:32:37.404366 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:32:43.406861 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:32:52.035211 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:32:54.313105 139951089751872 submission_runner.py:411] Time since start: 103577.53s, 	Step: 297186, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14682362973690033, 'validation/accuracy': 0.756659984588623, 'validation/loss': 1.0413254499435425, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8235479593276978, 'test/num_examples': 10000, 'score': 100020.35486221313, 'total_duration': 103577.52541542053, 'accumulated_submission_time': 100020.35486221313, 'accumulated_eval_time': 3532.513886451721, 'accumulated_logging_time': 13.849292993545532}
I0307 03:32:54.377144 139787621099264 logging_writer.py:48] [297186] accumulated_eval_time=3532.513886, accumulated_logging_time=13.849293, accumulated_submission_time=100020.354862, global_step=297186, preemption_count=0, score=100020.354862, test/accuracy=0.631800, test/loss=1.823548, test/num_examples=10000, total_duration=103577.525415, train/accuracy=0.959881, train/loss=0.146824, validation/accuracy=0.756660, validation/loss=1.041325, validation/num_examples=50000
I0307 03:32:59.419058 139787629491968 logging_writer.py:48] [297200] global_step=297200, grad_norm=4.240065097808838, loss=0.5929358005523682
I0307 03:33:33.008906 139787621099264 logging_writer.py:48] [297300] global_step=297300, grad_norm=4.747664928436279, loss=0.6748234033584595
I0307 03:34:06.569130 139787629491968 logging_writer.py:48] [297400] global_step=297400, grad_norm=4.0968475341796875, loss=0.5924795269966125
I0307 03:34:40.115947 139787621099264 logging_writer.py:48] [297500] global_step=297500, grad_norm=4.7609944343566895, loss=0.5799161195755005
I0307 03:35:13.770515 139787629491968 logging_writer.py:48] [297600] global_step=297600, grad_norm=4.521650314331055, loss=0.6196673512458801
I0307 03:35:47.385765 139787621099264 logging_writer.py:48] [297700] global_step=297700, grad_norm=4.285434246063232, loss=0.6135735511779785
I0307 03:36:21.036544 139787629491968 logging_writer.py:48] [297800] global_step=297800, grad_norm=4.40386438369751, loss=0.576601505279541
I0307 03:36:54.681621 139787621099264 logging_writer.py:48] [297900] global_step=297900, grad_norm=4.491890907287598, loss=0.6453397870063782
I0307 03:37:28.326561 139787629491968 logging_writer.py:48] [298000] global_step=298000, grad_norm=4.677001476287842, loss=0.6145062446594238
I0307 03:38:01.933491 139787621099264 logging_writer.py:48] [298100] global_step=298100, grad_norm=3.8965723514556885, loss=0.5427751541137695
I0307 03:38:35.612842 139787629491968 logging_writer.py:48] [298200] global_step=298200, grad_norm=4.013339996337891, loss=0.5372458100318909
I0307 03:39:09.195696 139787621099264 logging_writer.py:48] [298300] global_step=298300, grad_norm=5.170351982116699, loss=0.6926642060279846
I0307 03:39:42.808772 139787629491968 logging_writer.py:48] [298400] global_step=298400, grad_norm=4.4835381507873535, loss=0.6211921572685242
I0307 03:40:16.492769 139787621099264 logging_writer.py:48] [298500] global_step=298500, grad_norm=4.024486541748047, loss=0.5599895715713501
I0307 03:40:50.168926 139787629491968 logging_writer.py:48] [298600] global_step=298600, grad_norm=4.251425266265869, loss=0.5665295124053955
I0307 03:41:23.813223 139787621099264 logging_writer.py:48] [298700] global_step=298700, grad_norm=4.656257152557373, loss=0.7148301601409912
I0307 03:41:24.635838 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:41:30.797950 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:41:39.440336 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:41:41.713365 139951089751872 submission_runner.py:411] Time since start: 104104.93s, 	Step: 298704, 	{'train/accuracy': 0.9595623016357422, 'train/loss': 0.14792300760746002, 'validation/accuracy': 0.7567999958992004, 'validation/loss': 1.040809154510498, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8223090171813965, 'test/num_examples': 10000, 'score': 100530.54751634598, 'total_duration': 104104.92567372322, 'accumulated_submission_time': 100530.54751634598, 'accumulated_eval_time': 3549.5913729667664, 'accumulated_logging_time': 13.923123359680176}
I0307 03:41:41.781437 139789013600000 logging_writer.py:48] [298704] accumulated_eval_time=3549.591373, accumulated_logging_time=13.923123, accumulated_submission_time=100530.547516, global_step=298704, preemption_count=0, score=100530.547516, test/accuracy=0.631200, test/loss=1.822309, test/num_examples=10000, total_duration=104104.925674, train/accuracy=0.959562, train/loss=0.147923, validation/accuracy=0.756800, validation/loss=1.040809, validation/num_examples=50000
I0307 03:42:14.399833 139789021992704 logging_writer.py:48] [298800] global_step=298800, grad_norm=4.446104526519775, loss=0.5985921621322632
I0307 03:42:48.020465 139789013600000 logging_writer.py:48] [298900] global_step=298900, grad_norm=4.252952575683594, loss=0.5641907453536987
I0307 03:43:21.681366 139789021992704 logging_writer.py:48] [299000] global_step=299000, grad_norm=4.328787803649902, loss=0.5818703770637512
I0307 03:43:55.371009 139789013600000 logging_writer.py:48] [299100] global_step=299100, grad_norm=4.9228949546813965, loss=0.6557028889656067
I0307 03:44:29.065304 139789021992704 logging_writer.py:48] [299200] global_step=299200, grad_norm=4.497124671936035, loss=0.6188475489616394
I0307 03:45:02.684599 139789013600000 logging_writer.py:48] [299300] global_step=299300, grad_norm=4.089433193206787, loss=0.5128434300422668
I0307 03:45:36.317554 139789021992704 logging_writer.py:48] [299400] global_step=299400, grad_norm=4.561914920806885, loss=0.6730811595916748
I0307 03:46:09.926519 139789013600000 logging_writer.py:48] [299500] global_step=299500, grad_norm=4.844820022583008, loss=0.577347993850708
I0307 03:46:43.522103 139789021992704 logging_writer.py:48] [299600] global_step=299600, grad_norm=4.735956192016602, loss=0.6170878410339355
I0307 03:47:17.152700 139789013600000 logging_writer.py:48] [299700] global_step=299700, grad_norm=4.3991594314575195, loss=0.6173511743545532
I0307 03:47:50.801937 139789021992704 logging_writer.py:48] [299800] global_step=299800, grad_norm=4.982211112976074, loss=0.6396092772483826
I0307 03:48:24.451907 139789013600000 logging_writer.py:48] [299900] global_step=299900, grad_norm=4.941909313201904, loss=0.5709744691848755
I0307 03:48:58.112750 139789021992704 logging_writer.py:48] [300000] global_step=300000, grad_norm=4.927995681762695, loss=0.5741840600967407
I0307 03:49:31.753002 139789013600000 logging_writer.py:48] [300100] global_step=300100, grad_norm=4.370137691497803, loss=0.575669527053833
I0307 03:50:05.371759 139789021992704 logging_writer.py:48] [300200] global_step=300200, grad_norm=4.275057315826416, loss=0.6150846481323242
I0307 03:50:11.904746 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:50:18.243323 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:50:26.719992 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:50:29.011132 139951089751872 submission_runner.py:411] Time since start: 104632.22s, 	Step: 300221, 	{'train/accuracy': 0.962332546710968, 'train/loss': 0.14433689415454865, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0422277450561523, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8250768184661865, 'test/num_examples': 10000, 'score': 101040.60511946678, 'total_duration': 104632.22343468666, 'accumulated_submission_time': 101040.60511946678, 'accumulated_eval_time': 3566.6976997852325, 'accumulated_logging_time': 14.00077509880066}
I0307 03:50:29.081682 139787629491968 logging_writer.py:48] [300221] accumulated_eval_time=3566.697700, accumulated_logging_time=14.000775, accumulated_submission_time=101040.605119, global_step=300221, preemption_count=0, score=101040.605119, test/accuracy=0.631500, test/loss=1.825077, test/num_examples=10000, total_duration=104632.223435, train/accuracy=0.962333, train/loss=0.144337, validation/accuracy=0.757120, validation/loss=1.042228, validation/num_examples=50000
I0307 03:50:55.980921 139788996814592 logging_writer.py:48] [300300] global_step=300300, grad_norm=4.4004669189453125, loss=0.6009119153022766
I0307 03:51:29.574569 139787629491968 logging_writer.py:48] [300400] global_step=300400, grad_norm=4.330902576446533, loss=0.6040937900543213
I0307 03:52:03.137126 139788996814592 logging_writer.py:48] [300500] global_step=300500, grad_norm=4.592392921447754, loss=0.6282100081443787
I0307 03:52:36.687420 139787629491968 logging_writer.py:48] [300600] global_step=300600, grad_norm=5.08798885345459, loss=0.7127217054367065
I0307 03:53:10.307593 139788996814592 logging_writer.py:48] [300700] global_step=300700, grad_norm=4.40883731842041, loss=0.6211003661155701
I0307 03:53:43.897678 139787629491968 logging_writer.py:48] [300800] global_step=300800, grad_norm=4.832557678222656, loss=0.664323091506958
I0307 03:54:17.550337 139788996814592 logging_writer.py:48] [300900] global_step=300900, grad_norm=4.196348667144775, loss=0.5548374652862549
I0307 03:54:51.160645 139787629491968 logging_writer.py:48] [301000] global_step=301000, grad_norm=4.463240146636963, loss=0.6740033626556396
I0307 03:55:24.829092 139788996814592 logging_writer.py:48] [301100] global_step=301100, grad_norm=4.561659812927246, loss=0.6020587682723999
I0307 03:55:58.461789 139787629491968 logging_writer.py:48] [301200] global_step=301200, grad_norm=4.820975303649902, loss=0.6482422351837158
I0307 03:56:32.126004 139788996814592 logging_writer.py:48] [301300] global_step=301300, grad_norm=4.433348655700684, loss=0.6518970131874084
I0307 03:57:05.795586 139787629491968 logging_writer.py:48] [301400] global_step=301400, grad_norm=5.231740474700928, loss=0.558807373046875
I0307 03:57:39.355625 139788996814592 logging_writer.py:48] [301500] global_step=301500, grad_norm=4.433771133422852, loss=0.6196708679199219
I0307 03:58:12.973750 139787629491968 logging_writer.py:48] [301600] global_step=301600, grad_norm=4.350694179534912, loss=0.5800950527191162
I0307 03:58:46.599040 139788996814592 logging_writer.py:48] [301700] global_step=301700, grad_norm=4.944286823272705, loss=0.6450820565223694
I0307 03:58:59.188602 139951089751872 spec.py:321] Evaluating on the training split.
I0307 03:59:05.190167 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 03:59:13.826563 139951089751872 spec.py:349] Evaluating on the test split.
I0307 03:59:16.072135 139951089751872 submission_runner.py:411] Time since start: 105159.28s, 	Step: 301739, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.1479778289794922, 'validation/accuracy': 0.7568999528884888, 'validation/loss': 1.0407817363739014, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8221626281738281, 'test/num_examples': 10000, 'score': 101550.64585661888, 'total_duration': 105159.28444576263, 'accumulated_submission_time': 101550.64585661888, 'accumulated_eval_time': 3583.581182718277, 'accumulated_logging_time': 14.081845045089722}
I0307 03:59:16.140711 139789013600000 logging_writer.py:48] [301739] accumulated_eval_time=3583.581183, accumulated_logging_time=14.081845, accumulated_submission_time=101550.645857, global_step=301739, preemption_count=0, score=101550.645857, test/accuracy=0.631400, test/loss=1.822163, test/num_examples=10000, total_duration=105159.284446, train/accuracy=0.960459, train/loss=0.147978, validation/accuracy=0.756900, validation/loss=1.040782, validation/num_examples=50000
I0307 03:59:36.957477 139789021992704 logging_writer.py:48] [301800] global_step=301800, grad_norm=4.185797214508057, loss=0.5603004693984985
I0307 04:00:10.487442 139789013600000 logging_writer.py:48] [301900] global_step=301900, grad_norm=4.715981960296631, loss=0.6600261330604553
I0307 04:00:44.102365 139789021992704 logging_writer.py:48] [302000] global_step=302000, grad_norm=4.167290210723877, loss=0.5678499341011047
I0307 04:01:17.772032 139789013600000 logging_writer.py:48] [302100] global_step=302100, grad_norm=4.655219554901123, loss=0.641085147857666
I0307 04:01:51.442509 139789021992704 logging_writer.py:48] [302200] global_step=302200, grad_norm=4.435758590698242, loss=0.5648968815803528
I0307 04:02:25.066604 139789013600000 logging_writer.py:48] [302300] global_step=302300, grad_norm=4.359391689300537, loss=0.632298469543457
I0307 04:02:58.733017 139789021992704 logging_writer.py:48] [302400] global_step=302400, grad_norm=4.455148696899414, loss=0.5827776193618774
I0307 04:03:32.412253 139789013600000 logging_writer.py:48] [302500] global_step=302500, grad_norm=4.705312728881836, loss=0.6767956018447876
I0307 04:04:06.088010 139789021992704 logging_writer.py:48] [302600] global_step=302600, grad_norm=4.7378249168396, loss=0.6271100640296936
I0307 04:04:39.732362 139789013600000 logging_writer.py:48] [302700] global_step=302700, grad_norm=4.327764511108398, loss=0.6149963140487671
I0307 04:05:13.381452 139789021992704 logging_writer.py:48] [302800] global_step=302800, grad_norm=4.730541706085205, loss=0.5981481075286865
I0307 04:05:47.030623 139789013600000 logging_writer.py:48] [302900] global_step=302900, grad_norm=5.129807472229004, loss=0.7024240493774414
I0307 04:06:20.662540 139789021992704 logging_writer.py:48] [303000] global_step=303000, grad_norm=4.592194557189941, loss=0.6715697646141052
I0307 04:06:54.322275 139789013600000 logging_writer.py:48] [303100] global_step=303100, grad_norm=4.795401096343994, loss=0.677617073059082
I0307 04:07:27.958677 139789021992704 logging_writer.py:48] [303200] global_step=303200, grad_norm=4.391209125518799, loss=0.6132588386535645
I0307 04:07:46.260399 139951089751872 spec.py:321] Evaluating on the training split.
I0307 04:07:52.481284 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 04:08:01.113036 139951089751872 spec.py:349] Evaluating on the test split.
I0307 04:08:03.348542 139951089751872 submission_runner.py:411] Time since start: 105686.56s, 	Step: 303256, 	{'train/accuracy': 0.9602199792861938, 'train/loss': 0.1464538872241974, 'validation/accuracy': 0.7568199634552002, 'validation/loss': 1.0412921905517578, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8222219944000244, 'test/num_examples': 10000, 'score': 102060.69908952713, 'total_duration': 105686.56081461906, 'accumulated_submission_time': 102060.69908952713, 'accumulated_eval_time': 3600.6692349910736, 'accumulated_logging_time': 14.160232305526733}
I0307 04:08:03.417016 139788996814592 logging_writer.py:48] [303256] accumulated_eval_time=3600.669235, accumulated_logging_time=14.160232, accumulated_submission_time=102060.699090, global_step=303256, preemption_count=0, score=102060.699090, test/accuracy=0.631600, test/loss=1.822222, test/num_examples=10000, total_duration=105686.560815, train/accuracy=0.960220, train/loss=0.146454, validation/accuracy=0.756820, validation/loss=1.041292, validation/num_examples=50000
I0307 04:08:18.509599 139789005207296 logging_writer.py:48] [303300] global_step=303300, grad_norm=4.988462448120117, loss=0.6639593243598938
I0307 04:08:52.212897 139788996814592 logging_writer.py:48] [303400] global_step=303400, grad_norm=4.315258502960205, loss=0.6459047794342041
I0307 04:09:25.778764 139789005207296 logging_writer.py:48] [303500] global_step=303500, grad_norm=4.714025974273682, loss=0.5993378758430481
I0307 04:09:59.456618 139788996814592 logging_writer.py:48] [303600] global_step=303600, grad_norm=4.31929874420166, loss=0.6105921864509583
I0307 04:10:33.067185 139789005207296 logging_writer.py:48] [303700] global_step=303700, grad_norm=4.6822686195373535, loss=0.6289694309234619
I0307 04:11:06.734210 139788996814592 logging_writer.py:48] [303800] global_step=303800, grad_norm=4.375537872314453, loss=0.5526422262191772
I0307 04:11:40.363008 139789005207296 logging_writer.py:48] [303900] global_step=303900, grad_norm=4.470904350280762, loss=0.5875222682952881
I0307 04:12:14.027096 139788996814592 logging_writer.py:48] [304000] global_step=304000, grad_norm=4.262065410614014, loss=0.5394095778465271
I0307 04:12:47.668937 139789005207296 logging_writer.py:48] [304100] global_step=304100, grad_norm=4.958714485168457, loss=0.5756348371505737
I0307 04:13:21.342096 139788996814592 logging_writer.py:48] [304200] global_step=304200, grad_norm=4.392401695251465, loss=0.5974248647689819
I0307 04:13:54.940848 139789005207296 logging_writer.py:48] [304300] global_step=304300, grad_norm=4.485226154327393, loss=0.6245436072349548
I0307 04:14:28.595737 139788996814592 logging_writer.py:48] [304400] global_step=304400, grad_norm=4.254452705383301, loss=0.617480993270874
I0307 04:15:02.271930 139789005207296 logging_writer.py:48] [304500] global_step=304500, grad_norm=4.477414131164551, loss=0.6712512969970703
I0307 04:15:35.859143 139788996814592 logging_writer.py:48] [304600] global_step=304600, grad_norm=4.61638879776001, loss=0.6110284924507141
I0307 04:16:09.538089 139789005207296 logging_writer.py:48] [304700] global_step=304700, grad_norm=4.846568584442139, loss=0.6810749769210815
I0307 04:16:33.613734 139951089751872 spec.py:321] Evaluating on the training split.
I0307 04:16:39.676650 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 04:16:48.314936 139951089751872 spec.py:349] Evaluating on the test split.
I0307 04:16:50.552301 139951089751872 submission_runner.py:411] Time since start: 106213.76s, 	Step: 304773, 	{'train/accuracy': 0.96195387840271, 'train/loss': 0.1408708095550537, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.0406047105789185, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.823304533958435, 'test/num_examples': 10000, 'score': 102570.83042025566, 'total_duration': 106213.7646138668, 'accumulated_submission_time': 102570.83042025566, 'accumulated_eval_time': 3617.607753753662, 'accumulated_logging_time': 14.239030599594116}
I0307 04:16:50.620080 139789013600000 logging_writer.py:48] [304773] accumulated_eval_time=3617.607754, accumulated_logging_time=14.239031, accumulated_submission_time=102570.830420, global_step=304773, preemption_count=0, score=102570.830420, test/accuracy=0.631500, test/loss=1.823305, test/num_examples=10000, total_duration=106213.764614, train/accuracy=0.961954, train/loss=0.140871, validation/accuracy=0.757220, validation/loss=1.040605, validation/num_examples=50000
I0307 04:17:00.029427 139789021992704 logging_writer.py:48] [304800] global_step=304800, grad_norm=4.329779148101807, loss=0.6864122152328491
I0307 04:17:33.681711 139789013600000 logging_writer.py:48] [304900] global_step=304900, grad_norm=4.607423782348633, loss=0.5694584846496582
I0307 04:18:07.333395 139789021992704 logging_writer.py:48] [305000] global_step=305000, grad_norm=4.702937126159668, loss=0.6050295829772949
I0307 04:18:40.965264 139789013600000 logging_writer.py:48] [305100] global_step=305100, grad_norm=4.227147102355957, loss=0.6025354862213135
I0307 04:19:14.578975 139789021992704 logging_writer.py:48] [305200] global_step=305200, grad_norm=4.547293186187744, loss=0.6288681030273438
I0307 04:19:48.217577 139789013600000 logging_writer.py:48] [305300] global_step=305300, grad_norm=4.31633996963501, loss=0.6258604526519775
I0307 04:20:21.884588 139789021992704 logging_writer.py:48] [305400] global_step=305400, grad_norm=4.532747745513916, loss=0.6432855129241943
I0307 04:20:55.562329 139789013600000 logging_writer.py:48] [305500] global_step=305500, grad_norm=4.86936092376709, loss=0.6786772608757019
I0307 04:21:29.200574 139789021992704 logging_writer.py:48] [305600] global_step=305600, grad_norm=4.606322765350342, loss=0.7117922902107239
I0307 04:22:02.838456 139789013600000 logging_writer.py:48] [305700] global_step=305700, grad_norm=4.7043681144714355, loss=0.6748679280281067
I0307 04:22:36.467009 139789021992704 logging_writer.py:48] [305800] global_step=305800, grad_norm=4.367924690246582, loss=0.5850341320037842
I0307 04:23:10.110776 139789013600000 logging_writer.py:48] [305900] global_step=305900, grad_norm=4.545620441436768, loss=0.6291318535804749
I0307 04:23:43.769010 139789021992704 logging_writer.py:48] [306000] global_step=306000, grad_norm=4.217061519622803, loss=0.633699893951416
I0307 04:24:17.425132 139789013600000 logging_writer.py:48] [306100] global_step=306100, grad_norm=4.313579082489014, loss=0.5545477867126465
I0307 04:24:51.074361 139789021992704 logging_writer.py:48] [306200] global_step=306200, grad_norm=4.7007737159729, loss=0.6163708567619324
I0307 04:25:20.807649 139951089751872 spec.py:321] Evaluating on the training split.
I0307 04:25:26.897868 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 04:25:35.549324 139951089751872 spec.py:349] Evaluating on the test split.
I0307 04:25:37.794226 139951089751872 submission_runner.py:411] Time since start: 106741.01s, 	Step: 306290, 	{'train/accuracy': 0.9622727632522583, 'train/loss': 0.14470048248767853, 'validation/accuracy': 0.7573800086975098, 'validation/loss': 1.0414577722549438, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.824026107788086, 'test/num_examples': 10000, 'score': 103080.9529056549, 'total_duration': 106741.00653576851, 'accumulated_submission_time': 103080.9529056549, 'accumulated_eval_time': 3634.5942788124084, 'accumulated_logging_time': 14.316243886947632}
I0307 04:25:37.862328 139787621099264 logging_writer.py:48] [306290] accumulated_eval_time=3634.594279, accumulated_logging_time=14.316244, accumulated_submission_time=103080.952906, global_step=306290, preemption_count=0, score=103080.952906, test/accuracy=0.631600, test/loss=1.824026, test/num_examples=10000, total_duration=106741.006536, train/accuracy=0.962273, train/loss=0.144700, validation/accuracy=0.757380, validation/loss=1.041458, validation/num_examples=50000
I0307 04:25:41.555469 139787629491968 logging_writer.py:48] [306300] global_step=306300, grad_norm=4.158471584320068, loss=0.5823866724967957
I0307 04:26:15.159618 139787621099264 logging_writer.py:48] [306400] global_step=306400, grad_norm=3.830085039138794, loss=0.47765231132507324
I0307 04:26:48.827456 139787629491968 logging_writer.py:48] [306500] global_step=306500, grad_norm=4.292701244354248, loss=0.6178435683250427
I0307 04:27:22.494810 139787621099264 logging_writer.py:48] [306600] global_step=306600, grad_norm=4.361202239990234, loss=0.6564871668815613
I0307 04:27:56.087123 139787629491968 logging_writer.py:48] [306700] global_step=306700, grad_norm=4.525302410125732, loss=0.6053267121315002
I0307 04:28:29.694882 139787621099264 logging_writer.py:48] [306800] global_step=306800, grad_norm=4.75548791885376, loss=0.6006061434745789
I0307 04:29:03.353838 139787629491968 logging_writer.py:48] [306900] global_step=306900, grad_norm=4.580182075500488, loss=0.6917420625686646
I0307 04:29:36.954378 139787621099264 logging_writer.py:48] [307000] global_step=307000, grad_norm=4.562047004699707, loss=0.6435920596122742
I0307 04:30:10.626276 139787629491968 logging_writer.py:48] [307100] global_step=307100, grad_norm=4.497592449188232, loss=0.6572704315185547
I0307 04:30:44.240460 139787621099264 logging_writer.py:48] [307200] global_step=307200, grad_norm=4.323529243469238, loss=0.6253867745399475
I0307 04:31:17.905887 139787629491968 logging_writer.py:48] [307300] global_step=307300, grad_norm=4.836192607879639, loss=0.6590730547904968
I0307 04:31:51.551106 139787621099264 logging_writer.py:48] [307400] global_step=307400, grad_norm=4.651861190795898, loss=0.6433246731758118
I0307 04:32:25.194573 139787629491968 logging_writer.py:48] [307500] global_step=307500, grad_norm=4.363920211791992, loss=0.5683106184005737
I0307 04:32:58.798833 139787621099264 logging_writer.py:48] [307600] global_step=307600, grad_norm=4.600511074066162, loss=0.6239254474639893
I0307 04:33:32.461316 139787629491968 logging_writer.py:48] [307700] global_step=307700, grad_norm=4.7848801612854, loss=0.6494618654251099
I0307 04:34:06.065583 139787621099264 logging_writer.py:48] [307800] global_step=307800, grad_norm=4.480116844177246, loss=0.6223119497299194
I0307 04:34:07.893079 139951089751872 spec.py:321] Evaluating on the training split.
I0307 04:34:14.572521 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 04:34:23.334191 139951089751872 spec.py:349] Evaluating on the test split.
I0307 04:34:25.619300 139951089751872 submission_runner.py:411] Time since start: 107268.83s, 	Step: 307807, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14633195102214813, 'validation/accuracy': 0.7570599913597107, 'validation/loss': 1.0414164066314697, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8238807916641235, 'test/num_examples': 10000, 'score': 103590.91629076004, 'total_duration': 107268.83128428459, 'accumulated_submission_time': 103590.91629076004, 'accumulated_eval_time': 3652.3201220035553, 'accumulated_logging_time': 14.395713567733765}
I0307 04:34:25.689106 139787621099264 logging_writer.py:48] [307807] accumulated_eval_time=3652.320122, accumulated_logging_time=14.395714, accumulated_submission_time=103590.916291, global_step=307807, preemption_count=0, score=103590.916291, test/accuracy=0.632000, test/loss=1.823881, test/num_examples=10000, total_duration=107268.831284, train/accuracy=0.960938, train/loss=0.146332, validation/accuracy=0.757060, validation/loss=1.041416, validation/num_examples=50000
I0307 04:34:57.321758 139789013600000 logging_writer.py:48] [307900] global_step=307900, grad_norm=4.296883583068848, loss=0.6027830839157104
I0307 04:35:30.963187 139787621099264 logging_writer.py:48] [308000] global_step=308000, grad_norm=4.45372200012207, loss=0.6461197137832642
I0307 04:36:04.623990 139789013600000 logging_writer.py:48] [308100] global_step=308100, grad_norm=4.1110405921936035, loss=0.591472327709198
I0307 04:36:38.267438 139787621099264 logging_writer.py:48] [308200] global_step=308200, grad_norm=4.754634380340576, loss=0.560321569442749
I0307 04:37:11.894196 139789013600000 logging_writer.py:48] [308300] global_step=308300, grad_norm=4.83854866027832, loss=0.6789472103118896
I0307 04:37:45.520189 139787621099264 logging_writer.py:48] [308400] global_step=308400, grad_norm=4.862965106964111, loss=0.7683492302894592
I0307 04:38:19.136240 139789013600000 logging_writer.py:48] [308500] global_step=308500, grad_norm=4.344104766845703, loss=0.6088874340057373
I0307 04:38:52.773108 139787621099264 logging_writer.py:48] [308600] global_step=308600, grad_norm=4.301673412322998, loss=0.6002732515335083
I0307 04:39:26.474115 139789013600000 logging_writer.py:48] [308700] global_step=308700, grad_norm=4.291726589202881, loss=0.6710102558135986
I0307 04:40:00.057060 139787621099264 logging_writer.py:48] [308800] global_step=308800, grad_norm=4.296980381011963, loss=0.6539500951766968
I0307 04:40:33.663712 139789013600000 logging_writer.py:48] [308900] global_step=308900, grad_norm=4.8485894203186035, loss=0.5960997343063354
I0307 04:41:07.294615 139787621099264 logging_writer.py:48] [309000] global_step=309000, grad_norm=4.673972129821777, loss=0.6270223259925842
I0307 04:41:40.925070 139789013600000 logging_writer.py:48] [309100] global_step=309100, grad_norm=4.667270660400391, loss=0.6873466372489929
I0307 04:42:14.574242 139787621099264 logging_writer.py:48] [309200] global_step=309200, grad_norm=3.970376491546631, loss=0.5697377920150757
I0307 04:42:48.203332 139789013600000 logging_writer.py:48] [309300] global_step=309300, grad_norm=4.194551944732666, loss=0.5546370148658752
I0307 04:42:55.743390 139951089751872 spec.py:321] Evaluating on the training split.
I0307 04:43:01.796476 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 04:43:10.570027 139951089751872 spec.py:349] Evaluating on the test split.
I0307 04:43:12.871960 139951089751872 submission_runner.py:411] Time since start: 107796.08s, 	Step: 309324, 	{'train/accuracy': 0.9601601958274841, 'train/loss': 0.14819782972335815, 'validation/accuracy': 0.7568399906158447, 'validation/loss': 1.0411854982376099, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.8230949640274048, 'test/num_examples': 10000, 'score': 104100.90484285355, 'total_duration': 107796.08420968056, 'accumulated_submission_time': 104100.90484285355, 'accumulated_eval_time': 3669.4485788345337, 'accumulated_logging_time': 14.476394414901733}
I0307 04:43:12.947706 139787629491968 logging_writer.py:48] [309324] accumulated_eval_time=3669.448579, accumulated_logging_time=14.476394, accumulated_submission_time=104100.904843, global_step=309324, preemption_count=0, score=104100.904843, test/accuracy=0.630900, test/loss=1.823095, test/num_examples=10000, total_duration=107796.084210, train/accuracy=0.960160, train/loss=0.148198, validation/accuracy=0.756840, validation/loss=1.041185, validation/num_examples=50000
I0307 04:43:38.827923 139788996814592 logging_writer.py:48] [309400] global_step=309400, grad_norm=4.748831272125244, loss=0.6416757106781006
I0307 04:44:12.435321 139787629491968 logging_writer.py:48] [309500] global_step=309500, grad_norm=4.091573715209961, loss=0.6266878843307495
I0307 04:44:46.085581 139788996814592 logging_writer.py:48] [309600] global_step=309600, grad_norm=4.567521095275879, loss=0.6862208247184753
I0307 04:45:19.708363 139787629491968 logging_writer.py:48] [309700] global_step=309700, grad_norm=4.47183084487915, loss=0.6045453548431396
I0307 04:45:53.369310 139788996814592 logging_writer.py:48] [309800] global_step=309800, grad_norm=4.246160984039307, loss=0.561938464641571
I0307 04:46:26.967242 139787629491968 logging_writer.py:48] [309900] global_step=309900, grad_norm=4.308304786682129, loss=0.5920543670654297
I0307 04:47:00.613880 139788996814592 logging_writer.py:48] [310000] global_step=310000, grad_norm=4.4097418785095215, loss=0.6628231406211853
I0307 04:47:34.226381 139787629491968 logging_writer.py:48] [310100] global_step=310100, grad_norm=4.209294319152832, loss=0.6321169137954712
I0307 04:48:07.882659 139788996814592 logging_writer.py:48] [310200] global_step=310200, grad_norm=4.142210960388184, loss=0.547432005405426
I0307 04:48:41.498266 139787629491968 logging_writer.py:48] [310300] global_step=310300, grad_norm=4.533375263214111, loss=0.6142407059669495
I0307 04:49:15.142311 139788996814592 logging_writer.py:48] [310400] global_step=310400, grad_norm=4.285470008850098, loss=0.6453580856323242
I0307 04:49:48.745959 139787629491968 logging_writer.py:48] [310500] global_step=310500, grad_norm=4.815899848937988, loss=0.6757099628448486
I0307 04:50:22.427534 139788996814592 logging_writer.py:48] [310600] global_step=310600, grad_norm=4.799682140350342, loss=0.5633360743522644
I0307 04:50:56.054248 139787629491968 logging_writer.py:48] [310700] global_step=310700, grad_norm=4.403786659240723, loss=0.6193093061447144
I0307 04:51:29.721281 139788996814592 logging_writer.py:48] [310800] global_step=310800, grad_norm=4.515805244445801, loss=0.6461037993431091
I0307 04:51:43.031310 139951089751872 spec.py:321] Evaluating on the training split.
I0307 04:51:49.000293 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 04:51:57.702381 139951089751872 spec.py:349] Evaluating on the test split.
I0307 04:51:59.905324 139951089751872 submission_runner.py:411] Time since start: 108323.12s, 	Step: 310841, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.1436031311750412, 'validation/accuracy': 0.7575199604034424, 'validation/loss': 1.0405359268188477, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.8219730854034424, 'test/num_examples': 10000, 'score': 104610.92230081558, 'total_duration': 108323.11762619019, 'accumulated_submission_time': 104610.92230081558, 'accumulated_eval_time': 3686.322530031204, 'accumulated_logging_time': 14.564249038696289}
I0307 04:51:59.969600 139789021992704 logging_writer.py:48] [310841] accumulated_eval_time=3686.322530, accumulated_logging_time=14.564249, accumulated_submission_time=104610.922301, global_step=310841, preemption_count=0, score=104610.922301, test/accuracy=0.632600, test/loss=1.821973, test/num_examples=10000, total_duration=108323.117626, train/accuracy=0.960818, train/loss=0.143603, validation/accuracy=0.757520, validation/loss=1.040536, validation/num_examples=50000
I0307 04:52:20.126956 139789030385408 logging_writer.py:48] [310900] global_step=310900, grad_norm=4.255480766296387, loss=0.6044111251831055
I0307 04:52:53.740224 139789021992704 logging_writer.py:48] [311000] global_step=311000, grad_norm=4.9607319831848145, loss=0.6473114490509033
I0307 04:53:27.368051 139789030385408 logging_writer.py:48] [311100] global_step=311100, grad_norm=4.469930648803711, loss=0.6973268985748291
I0307 04:54:01.024747 139789021992704 logging_writer.py:48] [311200] global_step=311200, grad_norm=4.5750932693481445, loss=0.6531615257263184
I0307 04:54:34.711626 139789030385408 logging_writer.py:48] [311300] global_step=311300, grad_norm=4.464875221252441, loss=0.5957893133163452
I0307 04:55:08.324084 139789021992704 logging_writer.py:48] [311400] global_step=311400, grad_norm=4.496009826660156, loss=0.5939309000968933
I0307 04:55:41.969122 139789030385408 logging_writer.py:48] [311500] global_step=311500, grad_norm=4.47369384765625, loss=0.5588727593421936
I0307 04:56:15.603393 139789021992704 logging_writer.py:48] [311600] global_step=311600, grad_norm=4.617110252380371, loss=0.6051398515701294
I0307 04:56:49.241529 139789030385408 logging_writer.py:48] [311700] global_step=311700, grad_norm=4.39246129989624, loss=0.5934070944786072
I0307 04:57:22.851825 139789021992704 logging_writer.py:48] [311800] global_step=311800, grad_norm=4.503178596496582, loss=0.6341308355331421
I0307 04:57:56.513098 139789030385408 logging_writer.py:48] [311900] global_step=311900, grad_norm=4.585371494293213, loss=0.6761229038238525
I0307 04:58:30.126535 139789021992704 logging_writer.py:48] [312000] global_step=312000, grad_norm=4.993289947509766, loss=0.6466924548149109
I0307 04:59:03.781436 139789030385408 logging_writer.py:48] [312100] global_step=312100, grad_norm=4.412635803222656, loss=0.6378758549690247
I0307 04:59:37.401811 139789021992704 logging_writer.py:48] [312200] global_step=312200, grad_norm=4.7171220779418945, loss=0.6784219741821289
I0307 05:00:11.049427 139789030385408 logging_writer.py:48] [312300] global_step=312300, grad_norm=4.2506327629089355, loss=0.5837457180023193
I0307 05:00:30.051150 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:00:36.081902 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:00:44.807812 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:00:47.108224 139951089751872 submission_runner.py:411] Time since start: 108850.32s, 	Step: 312358, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14674478769302368, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.0412235260009766, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8220093250274658, 'test/num_examples': 10000, 'score': 105120.93588781357, 'total_duration': 108850.32050657272, 'accumulated_submission_time': 105120.93588781357, 'accumulated_eval_time': 3703.3795261383057, 'accumulated_logging_time': 14.63990044593811}
I0307 05:00:47.184823 139787629491968 logging_writer.py:48] [312358] accumulated_eval_time=3703.379526, accumulated_logging_time=14.639900, accumulated_submission_time=105120.935888, global_step=312358, preemption_count=0, score=105120.935888, test/accuracy=0.632500, test/loss=1.822009, test/num_examples=10000, total_duration=108850.320507, train/accuracy=0.961416, train/loss=0.146745, validation/accuracy=0.757240, validation/loss=1.041224, validation/num_examples=50000
I0307 05:01:01.652683 139788996814592 logging_writer.py:48] [312400] global_step=312400, grad_norm=4.6562819480896, loss=0.6651479005813599
I0307 05:01:35.235990 139787629491968 logging_writer.py:48] [312500] global_step=312500, grad_norm=4.736710548400879, loss=0.607302188873291
I0307 05:02:08.796699 139788996814592 logging_writer.py:48] [312600] global_step=312600, grad_norm=4.226223468780518, loss=0.6081331968307495
I0307 05:02:42.375351 139787629491968 logging_writer.py:48] [312700] global_step=312700, grad_norm=4.532517433166504, loss=0.6663706302642822
I0307 05:03:15.976671 139788996814592 logging_writer.py:48] [312800] global_step=312800, grad_norm=4.295685768127441, loss=0.6337195038795471
I0307 05:03:49.657924 139787629491968 logging_writer.py:48] [312900] global_step=312900, grad_norm=4.564090728759766, loss=0.6965750455856323
I0307 05:04:23.261175 139788996814592 logging_writer.py:48] [313000] global_step=313000, grad_norm=4.330564022064209, loss=0.6197280287742615
I0307 05:04:56.875463 139787629491968 logging_writer.py:48] [313100] global_step=313100, grad_norm=4.752455234527588, loss=0.6125361323356628
I0307 05:05:30.479091 139788996814592 logging_writer.py:48] [313200] global_step=313200, grad_norm=4.571583271026611, loss=0.6276040077209473
I0307 05:06:04.047689 139787629491968 logging_writer.py:48] [313300] global_step=313300, grad_norm=3.956158399581909, loss=0.5584697127342224
I0307 05:06:37.620084 139788996814592 logging_writer.py:48] [313400] global_step=313400, grad_norm=4.538796424865723, loss=0.6178638935089111
I0307 05:07:11.195101 139787629491968 logging_writer.py:48] [313500] global_step=313500, grad_norm=3.8604602813720703, loss=0.5767736434936523
I0307 05:07:44.808017 139788996814592 logging_writer.py:48] [313600] global_step=313600, grad_norm=4.991469860076904, loss=0.5970149636268616
I0307 05:08:18.435326 139787629491968 logging_writer.py:48] [313700] global_step=313700, grad_norm=4.255378246307373, loss=0.5378269553184509
I0307 05:08:52.023497 139788996814592 logging_writer.py:48] [313800] global_step=313800, grad_norm=4.398648262023926, loss=0.6390551328659058
I0307 05:09:17.428667 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:09:23.437722 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:09:32.174904 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:09:34.463629 139951089751872 submission_runner.py:411] Time since start: 109377.68s, 	Step: 313877, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14489611983299255, 'validation/accuracy': 0.757420003414154, 'validation/loss': 1.0412286520004272, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8241868019104004, 'test/num_examples': 10000, 'score': 105631.11181282997, 'total_duration': 109377.67593812943, 'accumulated_submission_time': 105631.11181282997, 'accumulated_eval_time': 3720.4144394397736, 'accumulated_logging_time': 14.726381301879883}
I0307 05:09:34.533683 139787612706560 logging_writer.py:48] [313877] accumulated_eval_time=3720.414439, accumulated_logging_time=14.726381, accumulated_submission_time=105631.111813, global_step=313877, preemption_count=0, score=105631.111813, test/accuracy=0.632200, test/loss=1.824187, test/num_examples=10000, total_duration=109377.675938, train/accuracy=0.960539, train/loss=0.144896, validation/accuracy=0.757420, validation/loss=1.041229, validation/num_examples=50000
I0307 05:09:42.620741 139787621099264 logging_writer.py:48] [313900] global_step=313900, grad_norm=4.46421480178833, loss=0.5585834980010986
I0307 05:10:16.323959 139787612706560 logging_writer.py:48] [314000] global_step=314000, grad_norm=4.365437984466553, loss=0.5557449460029602
I0307 05:10:49.974491 139787621099264 logging_writer.py:48] [314100] global_step=314100, grad_norm=4.0200347900390625, loss=0.6257290840148926
I0307 05:11:23.657475 139787612706560 logging_writer.py:48] [314200] global_step=314200, grad_norm=4.3587141036987305, loss=0.6601008772850037
I0307 05:11:57.272414 139787621099264 logging_writer.py:48] [314300] global_step=314300, grad_norm=4.806779384613037, loss=0.665066123008728
I0307 05:12:30.921912 139787612706560 logging_writer.py:48] [314400] global_step=314400, grad_norm=4.634603023529053, loss=0.6240077614784241
I0307 05:13:04.523960 139787621099264 logging_writer.py:48] [314500] global_step=314500, grad_norm=4.312605857849121, loss=0.6073724627494812
I0307 05:13:38.181482 139787612706560 logging_writer.py:48] [314600] global_step=314600, grad_norm=4.497339725494385, loss=0.6282199025154114
I0307 05:14:11.832657 139787621099264 logging_writer.py:48] [314700] global_step=314700, grad_norm=4.434552192687988, loss=0.655144989490509
I0307 05:14:45.497809 139787612706560 logging_writer.py:48] [314800] global_step=314800, grad_norm=4.583451271057129, loss=0.63413006067276
I0307 05:15:19.124090 139787621099264 logging_writer.py:48] [314900] global_step=314900, grad_norm=4.145818710327148, loss=0.5955376625061035
I0307 05:15:52.782880 139787612706560 logging_writer.py:48] [315000] global_step=315000, grad_norm=4.365316867828369, loss=0.6304912567138672
I0307 05:16:26.445464 139787621099264 logging_writer.py:48] [315100] global_step=315100, grad_norm=4.4004669189453125, loss=0.669745147228241
I0307 05:17:00.045803 139787612706560 logging_writer.py:48] [315200] global_step=315200, grad_norm=4.825377941131592, loss=0.6236956715583801
I0307 05:17:33.742701 139787621099264 logging_writer.py:48] [315300] global_step=315300, grad_norm=4.2272047996521, loss=0.5711557269096375
I0307 05:18:04.525505 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:18:10.572875 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:18:19.322641 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:18:21.606430 139951089751872 submission_runner.py:411] Time since start: 109904.82s, 	Step: 315393, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.1467013955116272, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0401941537857056, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8226666450500488, 'test/num_examples': 10000, 'score': 106141.03833842278, 'total_duration': 109904.81872224808, 'accumulated_submission_time': 106141.03833842278, 'accumulated_eval_time': 3737.4952976703644, 'accumulated_logging_time': 14.80655813217163}
I0307 05:18:21.675109 139787621099264 logging_writer.py:48] [315393] accumulated_eval_time=3737.495298, accumulated_logging_time=14.806558, accumulated_submission_time=106141.038338, global_step=315393, preemption_count=0, score=106141.038338, test/accuracy=0.631600, test/loss=1.822667, test/num_examples=10000, total_duration=109904.818722, train/accuracy=0.961635, train/loss=0.146701, validation/accuracy=0.757260, validation/loss=1.040194, validation/num_examples=50000
I0307 05:18:24.368601 139787629491968 logging_writer.py:48] [315400] global_step=315400, grad_norm=4.251568794250488, loss=0.636151134967804
I0307 05:18:57.991370 139787621099264 logging_writer.py:48] [315500] global_step=315500, grad_norm=4.941323757171631, loss=0.6372149586677551
I0307 05:19:31.642177 139787629491968 logging_writer.py:48] [315600] global_step=315600, grad_norm=4.218556880950928, loss=0.6257118582725525
I0307 05:20:05.257388 139787621099264 logging_writer.py:48] [315700] global_step=315700, grad_norm=4.613537311553955, loss=0.6470038294792175
I0307 05:20:38.879867 139787629491968 logging_writer.py:48] [315800] global_step=315800, grad_norm=4.50515079498291, loss=0.5505431294441223
I0307 05:21:12.498690 139787621099264 logging_writer.py:48] [315900] global_step=315900, grad_norm=4.489793300628662, loss=0.6578803062438965
I0307 05:21:46.139098 139787629491968 logging_writer.py:48] [316000] global_step=316000, grad_norm=4.324712753295898, loss=0.5725675821304321
I0307 05:22:19.816939 139787621099264 logging_writer.py:48] [316100] global_step=316100, grad_norm=4.645221710205078, loss=0.6357190012931824
I0307 05:22:53.433447 139787629491968 logging_writer.py:48] [316200] global_step=316200, grad_norm=4.5969390869140625, loss=0.6797829270362854
I0307 05:23:27.136602 139787621099264 logging_writer.py:48] [316300] global_step=316300, grad_norm=4.332101345062256, loss=0.6045113205909729
I0307 05:24:00.778012 139787629491968 logging_writer.py:48] [316400] global_step=316400, grad_norm=4.851967811584473, loss=0.6118475198745728
I0307 05:24:34.399083 139787621099264 logging_writer.py:48] [316500] global_step=316500, grad_norm=4.43251895904541, loss=0.6167308688163757
I0307 05:25:08.081421 139787629491968 logging_writer.py:48] [316600] global_step=316600, grad_norm=5.118067264556885, loss=0.6562926769256592
I0307 05:25:41.723932 139787621099264 logging_writer.py:48] [316700] global_step=316700, grad_norm=4.552950859069824, loss=0.6796944737434387
I0307 05:26:15.316781 139787629491968 logging_writer.py:48] [316800] global_step=316800, grad_norm=4.330632209777832, loss=0.578709065914154
I0307 05:26:48.975178 139787621099264 logging_writer.py:48] [316900] global_step=316900, grad_norm=4.650684356689453, loss=0.6506117582321167
I0307 05:26:51.808444 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:26:57.802987 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:27:06.550654 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:27:09.226391 139951089751872 submission_runner.py:411] Time since start: 110432.44s, 	Step: 316910, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14651809632778168, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0433276891708374, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.825588345527649, 'test/num_examples': 10000, 'score': 106651.106498003, 'total_duration': 110432.43870258331, 'accumulated_submission_time': 106651.106498003, 'accumulated_eval_time': 3754.913206577301, 'accumulated_logging_time': 14.88515329360962}
I0307 05:27:09.281558 139787612706560 logging_writer.py:48] [316910] accumulated_eval_time=3754.913207, accumulated_logging_time=14.885153, accumulated_submission_time=106651.106498, global_step=316910, preemption_count=0, score=106651.106498, test/accuracy=0.631700, test/loss=1.825588, test/num_examples=10000, total_duration=110432.438703, train/accuracy=0.961256, train/loss=0.146518, validation/accuracy=0.757120, validation/loss=1.043328, validation/num_examples=50000
I0307 05:27:39.869681 139787621099264 logging_writer.py:48] [317000] global_step=317000, grad_norm=4.229729175567627, loss=0.6256428360939026
I0307 05:28:13.596527 139787612706560 logging_writer.py:48] [317100] global_step=317100, grad_norm=4.486278533935547, loss=0.6713548898696899
I0307 05:28:47.164315 139787621099264 logging_writer.py:48] [317200] global_step=317200, grad_norm=4.588924884796143, loss=0.6039037108421326
I0307 05:29:20.821346 139787612706560 logging_writer.py:48] [317300] global_step=317300, grad_norm=4.270940780639648, loss=0.6000221967697144
I0307 05:29:54.459185 139787621099264 logging_writer.py:48] [317400] global_step=317400, grad_norm=4.385839462280273, loss=0.5865118503570557
I0307 05:30:28.161737 139787612706560 logging_writer.py:48] [317500] global_step=317500, grad_norm=4.551701545715332, loss=0.6443759799003601
I0307 05:31:01.824108 139787621099264 logging_writer.py:48] [317600] global_step=317600, grad_norm=4.195981502532959, loss=0.6392920017242432
I0307 05:31:35.508121 139787612706560 logging_writer.py:48] [317700] global_step=317700, grad_norm=4.010506629943848, loss=0.5726163983345032
I0307 05:32:09.137167 139787621099264 logging_writer.py:48] [317800] global_step=317800, grad_norm=4.683787822723389, loss=0.6444600820541382
I0307 05:32:42.816584 139787612706560 logging_writer.py:48] [317900] global_step=317900, grad_norm=4.477926731109619, loss=0.6648114919662476
I0307 05:33:16.466135 139787621099264 logging_writer.py:48] [318000] global_step=318000, grad_norm=4.426170349121094, loss=0.5398022532463074
I0307 05:33:50.088410 139787612706560 logging_writer.py:48] [318100] global_step=318100, grad_norm=4.215163230895996, loss=0.6037238240242004
I0307 05:34:23.834141 139787621099264 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.458615779876709, loss=0.6339086890220642
I0307 05:34:57.416877 139787612706560 logging_writer.py:48] [318300] global_step=318300, grad_norm=4.265127182006836, loss=0.5690608024597168
I0307 05:35:31.002688 139787621099264 logging_writer.py:48] [318400] global_step=318400, grad_norm=4.597567081451416, loss=0.6322133541107178
I0307 05:35:39.554520 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:35:45.616999 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:35:54.397091 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:35:56.682330 139951089751872 submission_runner.py:411] Time since start: 110959.89s, 	Step: 318427, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14601951837539673, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.0418167114257812, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8240481615066528, 'test/num_examples': 10000, 'score': 107161.31473088264, 'total_duration': 110959.89461374283, 'accumulated_submission_time': 107161.31473088264, 'accumulated_eval_time': 3772.040938138962, 'accumulated_logging_time': 14.948791265487671}
I0307 05:35:56.751361 139789005207296 logging_writer.py:48] [318427] accumulated_eval_time=3772.040938, accumulated_logging_time=14.948791, accumulated_submission_time=107161.314731, global_step=318427, preemption_count=0, score=107161.314731, test/accuracy=0.632500, test/loss=1.824048, test/num_examples=10000, total_duration=110959.894614, train/accuracy=0.960957, train/loss=0.146020, validation/accuracy=0.757220, validation/loss=1.041817, validation/num_examples=50000
I0307 05:36:21.623808 139789013600000 logging_writer.py:48] [318500] global_step=318500, grad_norm=4.318844795227051, loss=0.6367025375366211
I0307 05:36:55.238266 139789005207296 logging_writer.py:48] [318600] global_step=318600, grad_norm=4.51784086227417, loss=0.6551430821418762
I0307 05:37:28.902479 139789013600000 logging_writer.py:48] [318700] global_step=318700, grad_norm=4.57150411605835, loss=0.636505126953125
I0307 05:38:02.536105 139789005207296 logging_writer.py:48] [318800] global_step=318800, grad_norm=4.43271541595459, loss=0.6291888952255249
I0307 05:38:36.185263 139789013600000 logging_writer.py:48] [318900] global_step=318900, grad_norm=4.550991535186768, loss=0.5741026401519775
I0307 05:39:09.801377 139789005207296 logging_writer.py:48] [319000] global_step=319000, grad_norm=4.81947660446167, loss=0.7040480375289917
I0307 05:39:43.434253 139789013600000 logging_writer.py:48] [319100] global_step=319100, grad_norm=4.671611309051514, loss=0.6059952974319458
I0307 05:40:17.055663 139789005207296 logging_writer.py:48] [319200] global_step=319200, grad_norm=5.431036472320557, loss=0.701762855052948
I0307 05:40:50.738071 139789013600000 logging_writer.py:48] [319300] global_step=319300, grad_norm=4.975667953491211, loss=0.6703031063079834
I0307 05:41:24.329658 139789005207296 logging_writer.py:48] [319400] global_step=319400, grad_norm=5.239522933959961, loss=0.7289963960647583
I0307 05:41:57.923170 139789013600000 logging_writer.py:48] [319500] global_step=319500, grad_norm=4.573582649230957, loss=0.6109497547149658
I0307 05:42:31.535635 139789005207296 logging_writer.py:48] [319600] global_step=319600, grad_norm=4.724580764770508, loss=0.6395224332809448
I0307 05:43:05.197724 139789013600000 logging_writer.py:48] [319700] global_step=319700, grad_norm=4.333052635192871, loss=0.6169213652610779
I0307 05:43:38.840191 139789005207296 logging_writer.py:48] [319800] global_step=319800, grad_norm=4.517135143280029, loss=0.621532678604126
I0307 05:44:12.494072 139789013600000 logging_writer.py:48] [319900] global_step=319900, grad_norm=4.609869956970215, loss=0.6467781066894531
I0307 05:44:26.751281 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:44:32.735173 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:44:41.505746 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:44:43.801275 139951089751872 submission_runner.py:411] Time since start: 111487.01s, 	Step: 319944, 	{'train/accuracy': 0.96000075340271, 'train/loss': 0.14638227224349976, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0409986972808838, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8227121829986572, 'test/num_examples': 10000, 'score': 107671.24874210358, 'total_duration': 111487.01357507706, 'accumulated_submission_time': 107671.24874210358, 'accumulated_eval_time': 3789.0908699035645, 'accumulated_logging_time': 15.027590036392212}
I0307 05:44:43.874652 139787629491968 logging_writer.py:48] [319944] accumulated_eval_time=3789.090870, accumulated_logging_time=15.027590, accumulated_submission_time=107671.248742, global_step=319944, preemption_count=0, score=107671.248742, test/accuracy=0.631500, test/loss=1.822712, test/num_examples=10000, total_duration=111487.013575, train/accuracy=0.960001, train/loss=0.146382, validation/accuracy=0.757040, validation/loss=1.040999, validation/num_examples=50000
I0307 05:45:03.070935 139788996814592 logging_writer.py:48] [320000] global_step=320000, grad_norm=4.528107643127441, loss=0.52567458152771
I0307 05:45:36.680721 139787629491968 logging_writer.py:48] [320100] global_step=320100, grad_norm=4.4894490242004395, loss=0.6028345227241516
I0307 05:46:10.342722 139788996814592 logging_writer.py:48] [320200] global_step=320200, grad_norm=4.2427449226379395, loss=0.6049924492835999
I0307 05:46:43.978524 139787629491968 logging_writer.py:48] [320300] global_step=320300, grad_norm=4.500785827636719, loss=0.6371303796768188
I0307 05:47:17.636349 139788996814592 logging_writer.py:48] [320400] global_step=320400, grad_norm=4.621151447296143, loss=0.6947602033615112
I0307 05:47:51.240079 139787629491968 logging_writer.py:48] [320500] global_step=320500, grad_norm=4.2493896484375, loss=0.6186733245849609
I0307 05:48:24.901305 139788996814592 logging_writer.py:48] [320600] global_step=320600, grad_norm=4.27998161315918, loss=0.6283895969390869
I0307 05:48:58.517489 139787629491968 logging_writer.py:48] [320700] global_step=320700, grad_norm=4.646290302276611, loss=0.6530066132545471
I0307 05:49:32.175937 139788996814592 logging_writer.py:48] [320800] global_step=320800, grad_norm=4.3767170906066895, loss=0.612126350402832
I0307 05:50:05.828889 139787629491968 logging_writer.py:48] [320900] global_step=320900, grad_norm=4.8166422843933105, loss=0.637757420539856
I0307 05:50:39.498732 139788996814592 logging_writer.py:48] [321000] global_step=321000, grad_norm=4.47152853012085, loss=0.685195803642273
I0307 05:51:13.127527 139787629491968 logging_writer.py:48] [321100] global_step=321100, grad_norm=4.873676776885986, loss=0.6437311768531799
I0307 05:51:46.781394 139788996814592 logging_writer.py:48] [321200] global_step=321200, grad_norm=4.792718410491943, loss=0.6077017784118652
I0307 05:52:20.384602 139787629491968 logging_writer.py:48] [321300] global_step=321300, grad_norm=4.810072422027588, loss=0.6802918910980225
I0307 05:52:54.055932 139788996814592 logging_writer.py:48] [321400] global_step=321400, grad_norm=4.520855903625488, loss=0.7176985144615173
I0307 05:53:14.034270 139951089751872 spec.py:321] Evaluating on the training split.
I0307 05:53:20.056879 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 05:53:28.818033 139951089751872 spec.py:349] Evaluating on the test split.
I0307 05:53:31.106097 139951089751872 submission_runner.py:411] Time since start: 112014.32s, 	Step: 321461, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.1443527191877365, 'validation/accuracy': 0.7574599981307983, 'validation/loss': 1.0413386821746826, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8240190744400024, 'test/num_examples': 10000, 'score': 108181.3416583538, 'total_duration': 112014.31840515137, 'accumulated_submission_time': 108181.3416583538, 'accumulated_eval_time': 3806.1626420021057, 'accumulated_logging_time': 15.110766649246216}
I0307 05:53:31.179805 139788996814592 logging_writer.py:48] [321461] accumulated_eval_time=3806.162642, accumulated_logging_time=15.110767, accumulated_submission_time=108181.341658, global_step=321461, preemption_count=0, score=108181.341658, test/accuracy=0.632300, test/loss=1.824019, test/num_examples=10000, total_duration=112014.318405, train/accuracy=0.961595, train/loss=0.144353, validation/accuracy=0.757460, validation/loss=1.041339, validation/num_examples=50000
I0307 05:53:44.636277 139789013600000 logging_writer.py:48] [321500] global_step=321500, grad_norm=4.652835369110107, loss=0.6063557267189026
I0307 05:54:18.262621 139788996814592 logging_writer.py:48] [321600] global_step=321600, grad_norm=4.178291320800781, loss=0.6555108428001404
I0307 05:54:51.886994 139789013600000 logging_writer.py:48] [321700] global_step=321700, grad_norm=4.501708507537842, loss=0.6148958802223206
I0307 05:55:25.538460 139788996814592 logging_writer.py:48] [321800] global_step=321800, grad_norm=4.526334762573242, loss=0.6825051307678223
I0307 05:55:59.194793 139789013600000 logging_writer.py:48] [321900] global_step=321900, grad_norm=4.788038730621338, loss=0.651053249835968
I0307 05:56:32.852406 139788996814592 logging_writer.py:48] [322000] global_step=322000, grad_norm=4.658942222595215, loss=0.6496959924697876
I0307 05:57:06.498604 139789013600000 logging_writer.py:48] [322100] global_step=322100, grad_norm=4.205249309539795, loss=0.6064989566802979
I0307 05:57:40.162233 139788996814592 logging_writer.py:48] [322200] global_step=322200, grad_norm=4.562675476074219, loss=0.7214326858520508
I0307 05:58:13.824738 139789013600000 logging_writer.py:48] [322300] global_step=322300, grad_norm=5.788369655609131, loss=0.664365291595459
I0307 05:58:47.501126 139788996814592 logging_writer.py:48] [322400] global_step=322400, grad_norm=5.3351898193359375, loss=0.6473968625068665
I0307 05:59:21.106402 139789013600000 logging_writer.py:48] [322500] global_step=322500, grad_norm=4.600987911224365, loss=0.6264274716377258
I0307 05:59:54.735147 139788996814592 logging_writer.py:48] [322600] global_step=322600, grad_norm=4.606058597564697, loss=0.6854612827301025
I0307 06:00:28.351623 139789013600000 logging_writer.py:48] [322700] global_step=322700, grad_norm=4.38379430770874, loss=0.7075403928756714
I0307 06:01:01.983223 139788996814592 logging_writer.py:48] [322800] global_step=322800, grad_norm=4.720919132232666, loss=0.6552809476852417
I0307 06:01:35.612571 139789013600000 logging_writer.py:48] [322900] global_step=322900, grad_norm=4.149534702301025, loss=0.5961923599243164
I0307 06:02:01.329396 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:02:07.414367 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:02:16.206068 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:02:18.520470 139951089751872 submission_runner.py:411] Time since start: 112541.73s, 	Step: 322978, 	{'train/accuracy': 0.9588049650192261, 'train/loss': 0.15124468505382538, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0406066179275513, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8205403089523315, 'test/num_examples': 10000, 'score': 108691.42335653305, 'total_duration': 112541.7327632904, 'accumulated_submission_time': 108691.42335653305, 'accumulated_eval_time': 3823.353661775589, 'accumulated_logging_time': 15.195438861846924}
I0307 06:02:18.602047 139787621099264 logging_writer.py:48] [322978] accumulated_eval_time=3823.353662, accumulated_logging_time=15.195439, accumulated_submission_time=108691.423357, global_step=322978, preemption_count=0, score=108691.423357, test/accuracy=0.631900, test/loss=1.820540, test/num_examples=10000, total_duration=112541.732763, train/accuracy=0.958805, train/loss=0.151245, validation/accuracy=0.757280, validation/loss=1.040607, validation/num_examples=50000
I0307 06:02:26.353871 139787629491968 logging_writer.py:48] [323000] global_step=323000, grad_norm=4.720273494720459, loss=0.6255858540534973
I0307 06:03:00.044832 139787621099264 logging_writer.py:48] [323100] global_step=323100, grad_norm=5.0740966796875, loss=0.6820623874664307
I0307 06:03:33.648746 139787629491968 logging_writer.py:48] [323200] global_step=323200, grad_norm=4.610571384429932, loss=0.5842020511627197
I0307 06:04:07.321841 139787621099264 logging_writer.py:48] [323300] global_step=323300, grad_norm=4.491215705871582, loss=0.655552864074707
I0307 06:04:40.944891 139787629491968 logging_writer.py:48] [323400] global_step=323400, grad_norm=4.7167863845825195, loss=0.5700410604476929
I0307 06:05:14.605652 139787621099264 logging_writer.py:48] [323500] global_step=323500, grad_norm=4.71620512008667, loss=0.6298036575317383
I0307 06:05:48.206904 139787629491968 logging_writer.py:48] [323600] global_step=323600, grad_norm=5.233517646789551, loss=0.6590084433555603
I0307 06:06:21.873472 139787621099264 logging_writer.py:48] [323700] global_step=323700, grad_norm=4.524529457092285, loss=0.6172479391098022
I0307 06:06:55.512118 139787629491968 logging_writer.py:48] [323800] global_step=323800, grad_norm=5.406999588012695, loss=0.6598954796791077
I0307 06:07:29.185961 139787621099264 logging_writer.py:48] [323900] global_step=323900, grad_norm=4.283975124359131, loss=0.6532906293869019
I0307 06:08:02.791045 139787629491968 logging_writer.py:48] [324000] global_step=324000, grad_norm=4.157207012176514, loss=0.5031824111938477
I0307 06:08:36.438129 139787621099264 logging_writer.py:48] [324100] global_step=324100, grad_norm=4.107400894165039, loss=0.6322383284568787
I0307 06:09:10.032168 139787629491968 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.49384880065918, loss=0.5543240308761597
I0307 06:09:43.684623 139787621099264 logging_writer.py:48] [324300] global_step=324300, grad_norm=4.5433197021484375, loss=0.6271434426307678
I0307 06:10:17.335945 139787629491968 logging_writer.py:48] [324400] global_step=324400, grad_norm=4.303157806396484, loss=0.5539236068725586
I0307 06:10:48.790673 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:10:55.031321 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:11:03.663250 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:11:05.948906 139951089751872 submission_runner.py:411] Time since start: 113069.16s, 	Step: 324495, 	{'train/accuracy': 0.9601801633834839, 'train/loss': 0.14853984117507935, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0408989191055298, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.82335364818573, 'test/num_examples': 10000, 'score': 109201.54509663582, 'total_duration': 113069.16121602058, 'accumulated_submission_time': 109201.54509663582, 'accumulated_eval_time': 3840.5118465423584, 'accumulated_logging_time': 15.287701606750488}
I0307 06:11:06.018770 139787621099264 logging_writer.py:48] [324495] accumulated_eval_time=3840.511847, accumulated_logging_time=15.287702, accumulated_submission_time=109201.545097, global_step=324495, preemption_count=0, score=109201.545097, test/accuracy=0.631300, test/loss=1.823354, test/num_examples=10000, total_duration=113069.161216, train/accuracy=0.960180, train/loss=0.148540, validation/accuracy=0.757180, validation/loss=1.040899, validation/num_examples=50000
I0307 06:11:08.047972 139787629491968 logging_writer.py:48] [324500] global_step=324500, grad_norm=4.6341776847839355, loss=0.653688371181488
I0307 06:11:41.767618 139787621099264 logging_writer.py:48] [324600] global_step=324600, grad_norm=4.412552356719971, loss=0.613219141960144
I0307 06:12:15.406534 139787629491968 logging_writer.py:48] [324700] global_step=324700, grad_norm=4.366888999938965, loss=0.6153049468994141
I0307 06:12:49.033666 139787621099264 logging_writer.py:48] [324800] global_step=324800, grad_norm=4.719429969787598, loss=0.6180410981178284
I0307 06:13:22.695826 139787629491968 logging_writer.py:48] [324900] global_step=324900, grad_norm=4.339898586273193, loss=0.6257300972938538
I0307 06:13:56.320546 139787621099264 logging_writer.py:48] [325000] global_step=325000, grad_norm=4.465030193328857, loss=0.67149418592453
I0307 06:14:29.952964 139787629491968 logging_writer.py:48] [325100] global_step=325100, grad_norm=4.883790016174316, loss=0.5917802453041077
I0307 06:15:03.587940 139787621099264 logging_writer.py:48] [325200] global_step=325200, grad_norm=4.362131118774414, loss=0.5945140719413757
I0307 06:15:37.217290 139787629491968 logging_writer.py:48] [325300] global_step=325300, grad_norm=4.806053638458252, loss=0.5652139186859131
I0307 06:16:10.847141 139787621099264 logging_writer.py:48] [325400] global_step=325400, grad_norm=4.907004356384277, loss=0.5960924029350281
I0307 06:16:44.495370 139787629491968 logging_writer.py:48] [325500] global_step=325500, grad_norm=4.268958568572998, loss=0.6122134327888489
I0307 06:17:18.147027 139787621099264 logging_writer.py:48] [325600] global_step=325600, grad_norm=4.837093830108643, loss=0.6513261198997498
I0307 06:17:51.760683 139787629491968 logging_writer.py:48] [325700] global_step=325700, grad_norm=4.38427734375, loss=0.6048152446746826
I0307 06:18:25.336192 139787621099264 logging_writer.py:48] [325800] global_step=325800, grad_norm=5.255559921264648, loss=0.8027124404907227
I0307 06:18:58.934570 139787629491968 logging_writer.py:48] [325900] global_step=325900, grad_norm=4.350837707519531, loss=0.6155213713645935
I0307 06:19:32.573013 139787621099264 logging_writer.py:48] [326000] global_step=326000, grad_norm=4.923297882080078, loss=0.6841145157814026
I0307 06:19:36.076510 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:19:42.202157 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:19:50.949922 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:19:53.213401 139951089751872 submission_runner.py:411] Time since start: 113596.43s, 	Step: 326012, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.14659945666790009, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.041593074798584, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.822590947151184, 'test/num_examples': 10000, 'score': 109711.53610014915, 'total_duration': 113596.42569184303, 'accumulated_submission_time': 109711.53610014915, 'accumulated_eval_time': 3857.648682117462, 'accumulated_logging_time': 15.368526697158813}
I0307 06:19:53.282417 139788996814592 logging_writer.py:48] [326012] accumulated_eval_time=3857.648682, accumulated_logging_time=15.368527, accumulated_submission_time=109711.536100, global_step=326012, preemption_count=0, score=109711.536100, test/accuracy=0.631600, test/loss=1.822591, test/num_examples=10000, total_duration=113596.425692, train/accuracy=0.960340, train/loss=0.146599, validation/accuracy=0.757000, validation/loss=1.041593, validation/num_examples=50000
I0307 06:20:23.201795 139789005207296 logging_writer.py:48] [326100] global_step=326100, grad_norm=4.257618427276611, loss=0.5458112359046936
I0307 06:20:56.760584 139788996814592 logging_writer.py:48] [326200] global_step=326200, grad_norm=4.6368489265441895, loss=0.6679142117500305
I0307 06:21:30.353648 139789005207296 logging_writer.py:48] [326300] global_step=326300, grad_norm=4.269282817840576, loss=0.556134819984436
I0307 06:22:03.912378 139788996814592 logging_writer.py:48] [326400] global_step=326400, grad_norm=4.635440349578857, loss=0.6556875109672546
I0307 06:22:37.459142 139789005207296 logging_writer.py:48] [326500] global_step=326500, grad_norm=4.655515670776367, loss=0.6559414267539978
I0307 06:23:11.165643 139788996814592 logging_writer.py:48] [326600] global_step=326600, grad_norm=5.155758380889893, loss=0.6636905670166016
I0307 06:23:44.756120 139789005207296 logging_writer.py:48] [326700] global_step=326700, grad_norm=4.834534168243408, loss=0.716228723526001
I0307 06:24:18.325999 139788996814592 logging_writer.py:48] [326800] global_step=326800, grad_norm=5.115108013153076, loss=0.6427832245826721
I0307 06:24:51.916269 139789005207296 logging_writer.py:48] [326900] global_step=326900, grad_norm=4.443683624267578, loss=0.6144460439682007
I0307 06:25:25.494998 139788996814592 logging_writer.py:48] [327000] global_step=327000, grad_norm=4.534280300140381, loss=0.6400600075721741
I0307 06:25:59.103250 139789005207296 logging_writer.py:48] [327100] global_step=327100, grad_norm=4.778561115264893, loss=0.5842326283454895
I0307 06:26:32.671977 139788996814592 logging_writer.py:48] [327200] global_step=327200, grad_norm=4.306354522705078, loss=0.594797670841217
I0307 06:27:06.261345 139789005207296 logging_writer.py:48] [327300] global_step=327300, grad_norm=4.780823707580566, loss=0.5758203268051147
I0307 06:27:39.831025 139788996814592 logging_writer.py:48] [327400] global_step=327400, grad_norm=4.740512371063232, loss=0.6386185884475708
I0307 06:28:13.435493 139789005207296 logging_writer.py:48] [327500] global_step=327500, grad_norm=4.713985919952393, loss=0.6364258527755737
I0307 06:28:23.345741 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:28:29.386331 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:28:38.050793 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:28:40.323431 139951089751872 submission_runner.py:411] Time since start: 114123.54s, 	Step: 327531, 	{'train/accuracy': 0.9618144035339355, 'train/loss': 0.14751899242401123, 'validation/accuracy': 0.7569400072097778, 'validation/loss': 1.0417448282241821, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.8233752250671387, 'test/num_examples': 10000, 'score': 110221.53441381454, 'total_duration': 114123.5357427597, 'accumulated_submission_time': 110221.53441381454, 'accumulated_eval_time': 3874.6263246536255, 'accumulated_logging_time': 15.447299480438232}
I0307 06:28:40.395858 139787621099264 logging_writer.py:48] [327531] accumulated_eval_time=3874.626325, accumulated_logging_time=15.447299, accumulated_submission_time=110221.534414, global_step=327531, preemption_count=0, score=110221.534414, test/accuracy=0.632600, test/loss=1.823375, test/num_examples=10000, total_duration=114123.535743, train/accuracy=0.961814, train/loss=0.147519, validation/accuracy=0.756940, validation/loss=1.041745, validation/num_examples=50000
I0307 06:29:03.900268 139787629491968 logging_writer.py:48] [327600] global_step=327600, grad_norm=4.707309246063232, loss=0.6510148644447327
I0307 06:29:37.564861 139787621099264 logging_writer.py:48] [327700] global_step=327700, grad_norm=4.936964988708496, loss=0.6295949816703796
I0307 06:30:11.379820 139787629491968 logging_writer.py:48] [327800] global_step=327800, grad_norm=4.202662467956543, loss=0.5797675848007202
I0307 06:30:45.007095 139787621099264 logging_writer.py:48] [327900] global_step=327900, grad_norm=4.590494632720947, loss=0.6596583127975464
I0307 06:31:18.638736 139787629491968 logging_writer.py:48] [328000] global_step=328000, grad_norm=4.4228196144104, loss=0.6258386373519897
I0307 06:31:52.265093 139787621099264 logging_writer.py:48] [328100] global_step=328100, grad_norm=4.959202766418457, loss=0.6511130928993225
I0307 06:32:25.912561 139787629491968 logging_writer.py:48] [328200] global_step=328200, grad_norm=4.25628662109375, loss=0.5385945439338684
I0307 06:32:59.548099 139787621099264 logging_writer.py:48] [328300] global_step=328300, grad_norm=4.7221293449401855, loss=0.6639862060546875
I0307 06:33:33.201540 139787629491968 logging_writer.py:48] [328400] global_step=328400, grad_norm=4.156391620635986, loss=0.5621311068534851
I0307 06:34:06.845448 139787621099264 logging_writer.py:48] [328500] global_step=328500, grad_norm=4.085444927215576, loss=0.5630499124526978
I0307 06:34:40.495369 139787629491968 logging_writer.py:48] [328600] global_step=328600, grad_norm=4.563285827636719, loss=0.6227802038192749
I0307 06:35:14.134720 139787621099264 logging_writer.py:48] [328700] global_step=328700, grad_norm=4.077785015106201, loss=0.5617404580116272
I0307 06:35:47.739371 139787629491968 logging_writer.py:48] [328800] global_step=328800, grad_norm=4.708309650421143, loss=0.7085901498794556
I0307 06:36:21.354984 139787621099264 logging_writer.py:48] [328900] global_step=328900, grad_norm=4.426764011383057, loss=0.579378068447113
I0307 06:36:54.973073 139787629491968 logging_writer.py:48] [329000] global_step=329000, grad_norm=4.833879470825195, loss=0.7044276595115662
I0307 06:37:10.577485 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:37:16.741533 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:37:25.340408 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:37:27.602319 139951089751872 submission_runner.py:411] Time since start: 114650.81s, 	Step: 329048, 	{'train/accuracy': 0.9612563848495483, 'train/loss': 0.14570049941539764, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.0415037870407104, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.823715090751648, 'test/num_examples': 10000, 'score': 110731.65055704117, 'total_duration': 114650.81463003159, 'accumulated_submission_time': 110731.65055704117, 'accumulated_eval_time': 3891.651116847992, 'accumulated_logging_time': 15.529188394546509}
I0307 06:37:27.676414 139787621099264 logging_writer.py:48] [329048] accumulated_eval_time=3891.651117, accumulated_logging_time=15.529188, accumulated_submission_time=110731.650557, global_step=329048, preemption_count=0, score=110731.650557, test/accuracy=0.631400, test/loss=1.823715, test/num_examples=10000, total_duration=114650.814630, train/accuracy=0.961256, train/loss=0.145700, validation/accuracy=0.757240, validation/loss=1.041504, validation/num_examples=50000
I0307 06:37:45.492757 139789013600000 logging_writer.py:48] [329100] global_step=329100, grad_norm=4.347101211547852, loss=0.642339825630188
I0307 06:38:19.057349 139787621099264 logging_writer.py:48] [329200] global_step=329200, grad_norm=4.587167739868164, loss=0.6253699660301208
I0307 06:38:52.724513 139789013600000 logging_writer.py:48] [329300] global_step=329300, grad_norm=4.530488967895508, loss=0.5792248249053955
I0307 06:39:26.355792 139787621099264 logging_writer.py:48] [329400] global_step=329400, grad_norm=4.649083614349365, loss=0.6984103322029114
I0307 06:40:00.049925 139789013600000 logging_writer.py:48] [329500] global_step=329500, grad_norm=5.125087261199951, loss=0.6958221793174744
I0307 06:40:33.665547 139787621099264 logging_writer.py:48] [329600] global_step=329600, grad_norm=4.325908660888672, loss=0.5793768763542175
I0307 06:41:07.324941 139789013600000 logging_writer.py:48] [329700] global_step=329700, grad_norm=4.712936878204346, loss=0.625648021697998
I0307 06:41:41.001055 139787621099264 logging_writer.py:48] [329800] global_step=329800, grad_norm=4.601471900939941, loss=0.5872973203659058
I0307 06:42:14.629165 139789013600000 logging_writer.py:48] [329900] global_step=329900, grad_norm=4.265202045440674, loss=0.591715931892395
I0307 06:42:48.315343 139787621099264 logging_writer.py:48] [330000] global_step=330000, grad_norm=3.9322359561920166, loss=0.571736752986908
I0307 06:43:21.993258 139789013600000 logging_writer.py:48] [330100] global_step=330100, grad_norm=4.793388843536377, loss=0.6026791930198669
I0307 06:43:55.617262 139787621099264 logging_writer.py:48] [330200] global_step=330200, grad_norm=4.70517110824585, loss=0.6073322296142578
I0307 06:44:29.278027 139789013600000 logging_writer.py:48] [330300] global_step=330300, grad_norm=4.2203264236450195, loss=0.5771299600601196
I0307 06:45:02.906198 139787621099264 logging_writer.py:48] [330400] global_step=330400, grad_norm=4.251866340637207, loss=0.6201043128967285
I0307 06:45:36.567234 139789013600000 logging_writer.py:48] [330500] global_step=330500, grad_norm=4.41837215423584, loss=0.650139331817627
I0307 06:45:57.880102 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:46:03.992494 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:46:12.703404 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:46:14.989143 139951089751872 submission_runner.py:411] Time since start: 115178.20s, 	Step: 330565, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.146982803940773, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.0409815311431885, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.822548747062683, 'test/num_examples': 10000, 'score': 111241.78953266144, 'total_duration': 115178.20145368576, 'accumulated_submission_time': 111241.78953266144, 'accumulated_eval_time': 3908.7601075172424, 'accumulated_logging_time': 15.613263130187988}
I0307 06:46:15.062970 139787612706560 logging_writer.py:48] [330565] accumulated_eval_time=3908.760108, accumulated_logging_time=15.613263, accumulated_submission_time=111241.789533, global_step=330565, preemption_count=0, score=111241.789533, test/accuracy=0.631900, test/loss=1.822549, test/num_examples=10000, total_duration=115178.201454, train/accuracy=0.959901, train/loss=0.146983, validation/accuracy=0.757500, validation/loss=1.040982, validation/num_examples=50000
I0307 06:46:27.143362 139788996814592 logging_writer.py:48] [330600] global_step=330600, grad_norm=4.273955821990967, loss=0.6692545413970947
I0307 06:47:00.687440 139787612706560 logging_writer.py:48] [330700] global_step=330700, grad_norm=4.760569095611572, loss=0.6876287460327148
I0307 06:47:34.399721 139788996814592 logging_writer.py:48] [330800] global_step=330800, grad_norm=4.705070972442627, loss=0.7062677145004272
I0307 06:48:07.948361 139787612706560 logging_writer.py:48] [330900] global_step=330900, grad_norm=4.43186616897583, loss=0.6094158887863159
I0307 06:48:41.598760 139788996814592 logging_writer.py:48] [331000] global_step=331000, grad_norm=4.549647331237793, loss=0.649920642375946
I0307 06:49:15.240879 139787612706560 logging_writer.py:48] [331100] global_step=331100, grad_norm=4.1318039894104, loss=0.5634483098983765
I0307 06:49:48.878238 139788996814592 logging_writer.py:48] [331200] global_step=331200, grad_norm=4.381081581115723, loss=0.576725959777832
I0307 06:50:22.513597 139787612706560 logging_writer.py:48] [331300] global_step=331300, grad_norm=4.552207946777344, loss=0.5618799924850464
I0307 06:50:56.153127 139788996814592 logging_writer.py:48] [331400] global_step=331400, grad_norm=4.42331600189209, loss=0.6599084138870239
I0307 06:51:29.814091 139787612706560 logging_writer.py:48] [331500] global_step=331500, grad_norm=4.642906665802002, loss=0.634400486946106
I0307 06:52:03.457724 139788996814592 logging_writer.py:48] [331600] global_step=331600, grad_norm=4.33600378036499, loss=0.5952324271202087
I0307 06:52:37.109745 139787612706560 logging_writer.py:48] [331700] global_step=331700, grad_norm=4.5465407371521, loss=0.670987606048584
I0307 06:53:10.746405 139788996814592 logging_writer.py:48] [331800] global_step=331800, grad_norm=4.613829135894775, loss=0.5981190204620361
I0307 06:53:44.410070 139787612706560 logging_writer.py:48] [331900] global_step=331900, grad_norm=4.496275424957275, loss=0.632176399230957
I0307 06:54:18.024706 139788996814592 logging_writer.py:48] [332000] global_step=332000, grad_norm=4.875011920928955, loss=0.6630435585975647
I0307 06:54:45.083095 139951089751872 spec.py:321] Evaluating on the training split.
I0307 06:54:51.163374 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 06:54:59.860734 139951089751872 spec.py:349] Evaluating on the test split.
I0307 06:55:02.146590 139951089751872 submission_runner.py:411] Time since start: 115705.36s, 	Step: 332082, 	{'train/accuracy': 0.959382951259613, 'train/loss': 0.14871381223201752, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0413559675216675, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8225479125976562, 'test/num_examples': 10000, 'score': 111751.74325227737, 'total_duration': 115705.35889673233, 'accumulated_submission_time': 111751.74325227737, 'accumulated_eval_time': 3925.823562145233, 'accumulated_logging_time': 15.69628357887268}
I0307 06:55:02.222613 139787621099264 logging_writer.py:48] [332082] accumulated_eval_time=3925.823562, accumulated_logging_time=15.696284, accumulated_submission_time=111751.743252, global_step=332082, preemption_count=0, score=111751.743252, test/accuracy=0.631800, test/loss=1.822548, test/num_examples=10000, total_duration=115705.358897, train/accuracy=0.959383, train/loss=0.148714, validation/accuracy=0.757260, validation/loss=1.041356, validation/num_examples=50000
I0307 06:55:08.640362 139787629491968 logging_writer.py:48] [332100] global_step=332100, grad_norm=4.9142913818359375, loss=0.6893823146820068
I0307 06:55:42.306498 139787621099264 logging_writer.py:48] [332200] global_step=332200, grad_norm=5.097549915313721, loss=0.6346399784088135
I0307 06:56:15.959654 139787629491968 logging_writer.py:48] [332300] global_step=332300, grad_norm=4.635561943054199, loss=0.6693660616874695
I0307 06:56:49.582685 139787621099264 logging_writer.py:48] [332400] global_step=332400, grad_norm=4.527801036834717, loss=0.6870952248573303
I0307 06:57:23.269972 139787629491968 logging_writer.py:48] [332500] global_step=332500, grad_norm=4.298377990722656, loss=0.599811315536499
I0307 06:57:56.959364 139787621099264 logging_writer.py:48] [332600] global_step=332600, grad_norm=4.392861843109131, loss=0.6647117137908936
I0307 06:58:30.559314 139787629491968 logging_writer.py:48] [332700] global_step=332700, grad_norm=4.441875457763672, loss=0.578761100769043
I0307 06:59:04.243512 139787621099264 logging_writer.py:48] [332800] global_step=332800, grad_norm=4.469151496887207, loss=0.5689963102340698
I0307 06:59:37.859263 139787629491968 logging_writer.py:48] [332900] global_step=332900, grad_norm=4.329289436340332, loss=0.6890616416931152
I0307 07:00:11.513588 139787621099264 logging_writer.py:48] [333000] global_step=333000, grad_norm=4.444401264190674, loss=0.6737163662910461
I0307 07:00:45.140704 139787629491968 logging_writer.py:48] [333100] global_step=333100, grad_norm=4.281890392303467, loss=0.5731923580169678
I0307 07:01:18.802828 139787621099264 logging_writer.py:48] [333200] global_step=333200, grad_norm=4.505910873413086, loss=0.5814352035522461
I0307 07:01:52.429212 139787629491968 logging_writer.py:48] [333300] global_step=333300, grad_norm=4.42115592956543, loss=0.6331566572189331
I0307 07:02:26.111455 139787621099264 logging_writer.py:48] [333400] global_step=333400, grad_norm=4.552231788635254, loss=0.6591216325759888
I0307 07:02:59.711223 139787629491968 logging_writer.py:48] [333500] global_step=333500, grad_norm=4.7118988037109375, loss=0.6484673023223877
I0307 07:03:32.164459 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:03:38.187479 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:03:46.919994 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:03:49.188776 139951089751872 submission_runner.py:411] Time since start: 116232.40s, 	Step: 333598, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.1459372639656067, 'validation/accuracy': 0.7568599581718445, 'validation/loss': 1.0412836074829102, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.822006106376648, 'test/num_examples': 10000, 'score': 112261.62126588821, 'total_duration': 116232.40108180046, 'accumulated_submission_time': 112261.62126588821, 'accumulated_eval_time': 3942.8478474617004, 'accumulated_logging_time': 15.781913757324219}
I0307 07:03:49.266086 139789013600000 logging_writer.py:48] [333598] accumulated_eval_time=3942.847847, accumulated_logging_time=15.781914, accumulated_submission_time=112261.621266, global_step=333598, preemption_count=0, score=112261.621266, test/accuracy=0.631700, test/loss=1.822006, test/num_examples=10000, total_duration=116232.401082, train/accuracy=0.960918, train/loss=0.145937, validation/accuracy=0.756860, validation/loss=1.041284, validation/num_examples=50000
I0307 07:03:50.307487 139789021992704 logging_writer.py:48] [333600] global_step=333600, grad_norm=4.825851917266846, loss=0.6496123671531677
I0307 07:04:23.960268 139789013600000 logging_writer.py:48] [333700] global_step=333700, grad_norm=4.052780628204346, loss=0.5375285744667053
I0307 07:04:57.524975 139789021992704 logging_writer.py:48] [333800] global_step=333800, grad_norm=4.895872116088867, loss=0.6758468151092529
I0307 07:05:31.081574 139789013600000 logging_writer.py:48] [333900] global_step=333900, grad_norm=4.507599830627441, loss=0.6016691327095032
I0307 07:06:04.718379 139789021992704 logging_writer.py:48] [334000] global_step=334000, grad_norm=4.204975605010986, loss=0.5645080208778381
I0307 07:06:38.285422 139789013600000 logging_writer.py:48] [334100] global_step=334100, grad_norm=4.33105993270874, loss=0.577415943145752
I0307 07:07:11.854539 139789021992704 logging_writer.py:48] [334200] global_step=334200, grad_norm=4.506167411804199, loss=0.6612235307693481
I0307 07:07:45.503010 139789013600000 logging_writer.py:48] [334300] global_step=334300, grad_norm=4.412628650665283, loss=0.6072766184806824
I0307 07:08:19.140603 139789021992704 logging_writer.py:48] [334400] global_step=334400, grad_norm=4.4995951652526855, loss=0.6108626127243042
I0307 07:08:52.795368 139789013600000 logging_writer.py:48] [334500] global_step=334500, grad_norm=4.562913417816162, loss=0.6641921997070312
I0307 07:09:26.437479 139789021992704 logging_writer.py:48] [334600] global_step=334600, grad_norm=4.014233589172363, loss=0.6053723692893982
I0307 07:10:00.064172 139789013600000 logging_writer.py:48] [334700] global_step=334700, grad_norm=4.65076208114624, loss=0.6799255609512329
I0307 07:10:33.736336 139789021992704 logging_writer.py:48] [334800] global_step=334800, grad_norm=4.319680690765381, loss=0.5790063142776489
I0307 07:11:07.417480 139789013600000 logging_writer.py:48] [334900] global_step=334900, grad_norm=4.923823833465576, loss=0.6841832399368286
I0307 07:11:41.062459 139789021992704 logging_writer.py:48] [335000] global_step=335000, grad_norm=4.411753177642822, loss=0.6310631036758423
I0307 07:12:14.858197 139789013600000 logging_writer.py:48] [335100] global_step=335100, grad_norm=4.210354328155518, loss=0.5726069808006287
I0307 07:12:19.385181 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:12:25.401753 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:12:34.125868 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:12:36.374248 139951089751872 submission_runner.py:411] Time since start: 116759.59s, 	Step: 335115, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14638200402259827, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0413930416107178, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8224438428878784, 'test/num_examples': 10000, 'score': 112771.67519831657, 'total_duration': 116759.58655571938, 'accumulated_submission_time': 112771.67519831657, 'accumulated_eval_time': 3959.8368589878082, 'accumulated_logging_time': 15.869224786758423}
I0307 07:12:36.447292 139787612706560 logging_writer.py:48] [335115] accumulated_eval_time=3959.836859, accumulated_logging_time=15.869225, accumulated_submission_time=112771.675198, global_step=335115, preemption_count=0, score=112771.675198, test/accuracy=0.631600, test/loss=1.822444, test/num_examples=10000, total_duration=116759.586556, train/accuracy=0.960100, train/loss=0.146382, validation/accuracy=0.757040, validation/loss=1.041393, validation/num_examples=50000
I0307 07:13:05.301263 139787621099264 logging_writer.py:48] [335200] global_step=335200, grad_norm=4.9784770011901855, loss=0.6502354741096497
I0307 07:13:38.893905 139787612706560 logging_writer.py:48] [335300] global_step=335300, grad_norm=4.197803974151611, loss=0.611065149307251
I0307 07:14:12.484478 139787621099264 logging_writer.py:48] [335400] global_step=335400, grad_norm=4.7044358253479, loss=0.6295049786567688
I0307 07:14:46.172267 139787612706560 logging_writer.py:48] [335500] global_step=335500, grad_norm=4.301102638244629, loss=0.612629234790802
I0307 07:15:19.782917 139787621099264 logging_writer.py:48] [335600] global_step=335600, grad_norm=4.116718769073486, loss=0.6546929478645325
I0307 07:15:53.439360 139787612706560 logging_writer.py:48] [335700] global_step=335700, grad_norm=5.223690032958984, loss=0.7641007304191589
I0307 07:16:27.063754 139787621099264 logging_writer.py:48] [335800] global_step=335800, grad_norm=4.650570869445801, loss=0.6625204682350159
I0307 07:17:00.732556 139787612706560 logging_writer.py:48] [335900] global_step=335900, grad_norm=4.338119029998779, loss=0.5793411731719971
I0307 07:17:34.369676 139787621099264 logging_writer.py:48] [336000] global_step=336000, grad_norm=4.40683126449585, loss=0.6226390600204468
I0307 07:18:08.106261 139787612706560 logging_writer.py:48] [336100] global_step=336100, grad_norm=4.303057670593262, loss=0.592191755771637
I0307 07:18:41.675761 139787621099264 logging_writer.py:48] [336200] global_step=336200, grad_norm=4.380520820617676, loss=0.6330083012580872
I0307 07:19:15.232834 139787612706560 logging_writer.py:48] [336300] global_step=336300, grad_norm=4.229429244995117, loss=0.5961428880691528
I0307 07:19:48.832315 139787621099264 logging_writer.py:48] [336400] global_step=336400, grad_norm=4.808919906616211, loss=0.6733827590942383
I0307 07:20:22.519665 139787612706560 logging_writer.py:48] [336500] global_step=336500, grad_norm=4.428406715393066, loss=0.6367729902267456
I0307 07:20:56.160893 139787621099264 logging_writer.py:48] [336600] global_step=336600, grad_norm=4.626983165740967, loss=0.6156090497970581
I0307 07:21:06.383150 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:21:12.394941 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:21:20.954781 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:21:23.245175 139951089751872 submission_runner.py:411] Time since start: 117286.46s, 	Step: 336632, 	{'train/accuracy': 0.9599011540412903, 'train/loss': 0.1479339897632599, 'validation/accuracy': 0.757319986820221, 'validation/loss': 1.0409128665924072, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8232208490371704, 'test/num_examples': 10000, 'score': 113281.5459959507, 'total_duration': 117286.4574854374, 'accumulated_submission_time': 113281.5459959507, 'accumulated_eval_time': 3976.698829650879, 'accumulated_logging_time': 15.951818227767944}
I0307 07:21:23.317792 139789013600000 logging_writer.py:48] [336632] accumulated_eval_time=3976.698830, accumulated_logging_time=15.951818, accumulated_submission_time=113281.545996, global_step=336632, preemption_count=0, score=113281.545996, test/accuracy=0.631000, test/loss=1.823221, test/num_examples=10000, total_duration=117286.457485, train/accuracy=0.959901, train/loss=0.147934, validation/accuracy=0.757320, validation/loss=1.040913, validation/num_examples=50000
I0307 07:21:46.477204 139789021992704 logging_writer.py:48] [336700] global_step=336700, grad_norm=4.8239665031433105, loss=0.6020828485488892
I0307 07:22:20.022711 139789013600000 logging_writer.py:48] [336800] global_step=336800, grad_norm=4.4691853523254395, loss=0.6365512013435364
I0307 07:22:53.618536 139789021992704 logging_writer.py:48] [336900] global_step=336900, grad_norm=5.302677154541016, loss=0.6703283190727234
I0307 07:23:27.271858 139789013600000 logging_writer.py:48] [337000] global_step=337000, grad_norm=5.160008907318115, loss=0.7455524802207947
I0307 07:24:00.920794 139789021992704 logging_writer.py:48] [337100] global_step=337100, grad_norm=4.903475284576416, loss=0.7142633199691772
I0307 07:24:34.574071 139789013600000 logging_writer.py:48] [337200] global_step=337200, grad_norm=4.401350021362305, loss=0.5938893556594849
I0307 07:25:08.178936 139789021992704 logging_writer.py:48] [337300] global_step=337300, grad_norm=4.381902694702148, loss=0.6265616416931152
I0307 07:25:41.815777 139789013600000 logging_writer.py:48] [337400] global_step=337400, grad_norm=4.871097087860107, loss=0.6514739990234375
I0307 07:26:15.437281 139789021992704 logging_writer.py:48] [337500] global_step=337500, grad_norm=4.503043174743652, loss=0.6301437616348267
I0307 07:26:49.083249 139789013600000 logging_writer.py:48] [337600] global_step=337600, grad_norm=4.450716972351074, loss=0.6006447672843933
I0307 07:27:22.705842 139789021992704 logging_writer.py:48] [337700] global_step=337700, grad_norm=4.630455493927002, loss=0.6282123327255249
I0307 07:27:56.364172 139789013600000 logging_writer.py:48] [337800] global_step=337800, grad_norm=4.0520219802856445, loss=0.5208940505981445
I0307 07:28:30.009128 139789021992704 logging_writer.py:48] [337900] global_step=337900, grad_norm=4.181846618652344, loss=0.5951510667800903
I0307 07:29:03.653606 139789013600000 logging_writer.py:48] [338000] global_step=338000, grad_norm=4.407744407653809, loss=0.5645115375518799
I0307 07:29:37.266810 139789021992704 logging_writer.py:48] [338100] global_step=338100, grad_norm=4.921205520629883, loss=0.7367907762527466
I0307 07:29:53.542421 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:29:59.574104 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:30:08.196583 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:30:10.449615 139951089751872 submission_runner.py:411] Time since start: 117813.66s, 	Step: 338150, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14653083682060242, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 1.0408369302749634, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8228672742843628, 'test/num_examples': 10000, 'score': 113791.70448088646, 'total_duration': 117813.6619169712, 'accumulated_submission_time': 113791.70448088646, 'accumulated_eval_time': 3993.6059629917145, 'accumulated_logging_time': 16.03429889678955}
I0307 07:30:10.526489 139789005207296 logging_writer.py:48] [338150] accumulated_eval_time=3993.605963, accumulated_logging_time=16.034299, accumulated_submission_time=113791.704481, global_step=338150, preemption_count=0, score=113791.704481, test/accuracy=0.631700, test/loss=1.822867, test/num_examples=10000, total_duration=117813.661917, train/accuracy=0.960938, train/loss=0.146531, validation/accuracy=0.757400, validation/loss=1.040837, validation/num_examples=50000
I0307 07:30:27.715969 139789038778112 logging_writer.py:48] [338200] global_step=338200, grad_norm=4.3577561378479, loss=0.5540835857391357
I0307 07:31:01.295360 139789005207296 logging_writer.py:48] [338300] global_step=338300, grad_norm=4.4954071044921875, loss=0.5787344574928284
I0307 07:31:34.932353 139789038778112 logging_writer.py:48] [338400] global_step=338400, grad_norm=4.518983364105225, loss=0.5513613224029541
I0307 07:32:08.523184 139789005207296 logging_writer.py:48] [338500] global_step=338500, grad_norm=4.568942546844482, loss=0.7612940073013306
I0307 07:32:42.172623 139789038778112 logging_writer.py:48] [338600] global_step=338600, grad_norm=4.036574363708496, loss=0.5466322898864746
I0307 07:33:15.814028 139789005207296 logging_writer.py:48] [338700] global_step=338700, grad_norm=3.9132089614868164, loss=0.5005015134811401
I0307 07:33:49.484666 139789038778112 logging_writer.py:48] [338800] global_step=338800, grad_norm=4.667507648468018, loss=0.7018222212791443
I0307 07:34:23.087784 139789005207296 logging_writer.py:48] [338900] global_step=338900, grad_norm=4.48636531829834, loss=0.6753473281860352
I0307 07:34:56.746723 139789038778112 logging_writer.py:48] [339000] global_step=339000, grad_norm=4.929353713989258, loss=0.6695278286933899
I0307 07:35:30.355812 139789005207296 logging_writer.py:48] [339100] global_step=339100, grad_norm=4.58836030960083, loss=0.676727831363678
I0307 07:36:04.033299 139789038778112 logging_writer.py:48] [339200] global_step=339200, grad_norm=4.486225605010986, loss=0.6074938774108887
I0307 07:36:37.716182 139789005207296 logging_writer.py:48] [339300] global_step=339300, grad_norm=4.164141654968262, loss=0.5861387252807617
I0307 07:37:11.364107 139789038778112 logging_writer.py:48] [339400] global_step=339400, grad_norm=4.438510894775391, loss=0.5606679320335388
I0307 07:37:45.016632 139789005207296 logging_writer.py:48] [339500] global_step=339500, grad_norm=4.726971626281738, loss=0.5811835527420044
I0307 07:38:18.697487 139789038778112 logging_writer.py:48] [339600] global_step=339600, grad_norm=4.4474921226501465, loss=0.5773338675498962
I0307 07:38:40.717348 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:38:46.994411 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:38:55.554235 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:38:57.828140 139951089751872 submission_runner.py:411] Time since start: 118341.04s, 	Step: 339667, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14656049013137817, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.040977120399475, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8227611780166626, 'test/num_examples': 10000, 'score': 114301.828332901, 'total_duration': 118341.0404188633, 'accumulated_submission_time': 114301.828332901, 'accumulated_eval_time': 4010.716674566269, 'accumulated_logging_time': 16.120765447616577}
I0307 07:38:57.899559 139789013600000 logging_writer.py:48] [339667] accumulated_eval_time=4010.716675, accumulated_logging_time=16.120765, accumulated_submission_time=114301.828333, global_step=339667, preemption_count=0, score=114301.828333, test/accuracy=0.632400, test/loss=1.822761, test/num_examples=10000, total_duration=118341.040419, train/accuracy=0.960938, train/loss=0.146560, validation/accuracy=0.757100, validation/loss=1.040977, validation/num_examples=50000
I0307 07:39:09.327590 139789021992704 logging_writer.py:48] [339700] global_step=339700, grad_norm=4.490233898162842, loss=0.6343193650245667
I0307 07:39:42.931074 139789013600000 logging_writer.py:48] [339800] global_step=339800, grad_norm=4.3444623947143555, loss=0.625087320804596
I0307 07:40:16.567182 139789021992704 logging_writer.py:48] [339900] global_step=339900, grad_norm=4.366107940673828, loss=0.5521699786186218
I0307 07:40:50.219331 139789013600000 logging_writer.py:48] [340000] global_step=340000, grad_norm=4.691341400146484, loss=0.6330492496490479
I0307 07:41:23.880188 139789021992704 logging_writer.py:48] [340100] global_step=340100, grad_norm=4.756426811218262, loss=0.6223383545875549
I0307 07:41:57.511667 139789013600000 logging_writer.py:48] [340200] global_step=340200, grad_norm=4.461640357971191, loss=0.6384652853012085
I0307 07:42:31.173577 139789021992704 logging_writer.py:48] [340300] global_step=340300, grad_norm=4.145267486572266, loss=0.6217512488365173
I0307 07:43:04.757328 139789013600000 logging_writer.py:48] [340400] global_step=340400, grad_norm=4.510809421539307, loss=0.6640740036964417
I0307 07:43:38.398182 139789021992704 logging_writer.py:48] [340500] global_step=340500, grad_norm=4.226703643798828, loss=0.5666715502738953
I0307 07:44:12.010968 139789013600000 logging_writer.py:48] [340600] global_step=340600, grad_norm=4.691697120666504, loss=0.6235142946243286
I0307 07:44:45.653883 139789021992704 logging_writer.py:48] [340700] global_step=340700, grad_norm=4.907474040985107, loss=0.6349591016769409
I0307 07:45:19.294349 139789013600000 logging_writer.py:48] [340800] global_step=340800, grad_norm=4.243797302246094, loss=0.6505160331726074
I0307 07:45:52.910343 139789021992704 logging_writer.py:48] [340900] global_step=340900, grad_norm=4.36646032333374, loss=0.6098394393920898
I0307 07:46:26.597434 139789013600000 logging_writer.py:48] [341000] global_step=341000, grad_norm=4.419618129730225, loss=0.5941574573516846
I0307 07:47:00.292878 139789021992704 logging_writer.py:48] [341100] global_step=341100, grad_norm=4.851293087005615, loss=0.6503856778144836
I0307 07:47:28.037565 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:47:34.162664 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:47:42.793153 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:47:45.057077 139951089751872 submission_runner.py:411] Time since start: 118868.27s, 	Step: 341184, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.14501605927944183, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.0414011478424072, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8225243091583252, 'test/num_examples': 10000, 'score': 114811.8994588852, 'total_duration': 118868.26939034462, 'accumulated_submission_time': 114811.8994588852, 'accumulated_eval_time': 4027.736142396927, 'accumulated_logging_time': 16.20158553123474}
I0307 07:47:45.129074 139788996814592 logging_writer.py:48] [341184] accumulated_eval_time=4027.736142, accumulated_logging_time=16.201586, accumulated_submission_time=114811.899459, global_step=341184, preemption_count=0, score=114811.899459, test/accuracy=0.631600, test/loss=1.822524, test/num_examples=10000, total_duration=118868.269390, train/accuracy=0.960678, train/loss=0.145016, validation/accuracy=0.756920, validation/loss=1.041401, validation/num_examples=50000
I0307 07:47:50.844298 139789005207296 logging_writer.py:48] [341200] global_step=341200, grad_norm=4.751905918121338, loss=0.7023756504058838
I0307 07:48:24.447129 139788996814592 logging_writer.py:48] [341300] global_step=341300, grad_norm=4.331684589385986, loss=0.568597137928009
I0307 07:48:58.138761 139789005207296 logging_writer.py:48] [341400] global_step=341400, grad_norm=4.3802900314331055, loss=0.5242705941200256
I0307 07:49:31.805476 139788996814592 logging_writer.py:48] [341500] global_step=341500, grad_norm=4.702474117279053, loss=0.6493454575538635
I0307 07:50:05.445208 139789005207296 logging_writer.py:48] [341600] global_step=341600, grad_norm=4.22444486618042, loss=0.5752115249633789
I0307 07:50:39.096585 139788996814592 logging_writer.py:48] [341700] global_step=341700, grad_norm=4.3025336265563965, loss=0.5731468200683594
I0307 07:51:12.764607 139789005207296 logging_writer.py:48] [341800] global_step=341800, grad_norm=4.8497748374938965, loss=0.6886302828788757
I0307 07:51:46.418026 139788996814592 logging_writer.py:48] [341900] global_step=341900, grad_norm=4.4959187507629395, loss=0.6318421959877014
I0307 07:52:20.043462 139789005207296 logging_writer.py:48] [342000] global_step=342000, grad_norm=4.645158767700195, loss=0.6220551133155823
I0307 07:52:53.688702 139788996814592 logging_writer.py:48] [342100] global_step=342100, grad_norm=4.4761528968811035, loss=0.645388126373291
I0307 07:53:27.302634 139789005207296 logging_writer.py:48] [342200] global_step=342200, grad_norm=4.835526943206787, loss=0.6485055685043335
I0307 07:54:00.963325 139788996814592 logging_writer.py:48] [342300] global_step=342300, grad_norm=5.178770542144775, loss=0.6372405886650085
I0307 07:54:34.586750 139789005207296 logging_writer.py:48] [342400] global_step=342400, grad_norm=4.674792766571045, loss=0.7008036375045776
I0307 07:55:08.227805 139788996814592 logging_writer.py:48] [342500] global_step=342500, grad_norm=4.628377437591553, loss=0.6684452891349792
I0307 07:55:41.819364 139789005207296 logging_writer.py:48] [342600] global_step=342600, grad_norm=4.892942905426025, loss=0.6118379831314087
I0307 07:56:15.429731 139788996814592 logging_writer.py:48] [342700] global_step=342700, grad_norm=4.586576461791992, loss=0.6828893423080444
I0307 07:56:15.436790 139951089751872 spec.py:321] Evaluating on the training split.
I0307 07:56:21.470458 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 07:56:30.145715 139951089751872 spec.py:349] Evaluating on the test split.
I0307 07:56:32.433583 139951089751872 submission_runner.py:411] Time since start: 119395.65s, 	Step: 342701, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14527294039726257, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.040116548538208, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8221936225891113, 'test/num_examples': 10000, 'score': 115322.14385986328, 'total_duration': 119395.64589619637, 'accumulated_submission_time': 115322.14385986328, 'accumulated_eval_time': 4044.732864379883, 'accumulated_logging_time': 16.282942533493042}
I0307 07:56:32.509997 139787621099264 logging_writer.py:48] [342701] accumulated_eval_time=4044.732864, accumulated_logging_time=16.282943, accumulated_submission_time=115322.143860, global_step=342701, preemption_count=0, score=115322.143860, test/accuracy=0.632100, test/loss=1.822194, test/num_examples=10000, total_duration=119395.645896, train/accuracy=0.960579, train/loss=0.145273, validation/accuracy=0.757180, validation/loss=1.040117, validation/num_examples=50000
I0307 07:57:06.097159 139787629491968 logging_writer.py:48] [342800] global_step=342800, grad_norm=4.967949867248535, loss=0.6775810718536377
I0307 07:57:39.681805 139787621099264 logging_writer.py:48] [342900] global_step=342900, grad_norm=4.625838279724121, loss=0.6026139259338379
I0307 07:58:13.330288 139787629491968 logging_writer.py:48] [343000] global_step=343000, grad_norm=4.221792697906494, loss=0.5848252773284912
I0307 07:58:46.953524 139787621099264 logging_writer.py:48] [343100] global_step=343100, grad_norm=5.008254528045654, loss=0.675180196762085
I0307 07:59:20.604099 139787629491968 logging_writer.py:48] [343200] global_step=343200, grad_norm=4.658102989196777, loss=0.6568862795829773
I0307 07:59:54.261549 139787621099264 logging_writer.py:48] [343300] global_step=343300, grad_norm=4.353634834289551, loss=0.6092313528060913
I0307 08:00:27.875944 139787629491968 logging_writer.py:48] [343400] global_step=343400, grad_norm=4.663940906524658, loss=0.6669838428497314
I0307 08:01:01.567343 139787621099264 logging_writer.py:48] [343500] global_step=343500, grad_norm=4.516563415527344, loss=0.6153159737586975
I0307 08:01:35.244591 139787629491968 logging_writer.py:48] [343600] global_step=343600, grad_norm=4.770010948181152, loss=0.6541870832443237
I0307 08:02:08.860056 139787621099264 logging_writer.py:48] [343700] global_step=343700, grad_norm=4.3456130027771, loss=0.601548433303833
I0307 08:02:42.507397 139787629491968 logging_writer.py:48] [343800] global_step=343800, grad_norm=4.185302257537842, loss=0.5712693929672241
I0307 08:03:16.117423 139787621099264 logging_writer.py:48] [343900] global_step=343900, grad_norm=4.334750652313232, loss=0.6529085040092468
I0307 08:03:49.769314 139787629491968 logging_writer.py:48] [344000] global_step=344000, grad_norm=4.656351566314697, loss=0.6592012047767639
I0307 08:04:23.385115 139787621099264 logging_writer.py:48] [344100] global_step=344100, grad_norm=4.703187465667725, loss=0.5562576055526733
I0307 08:04:57.036080 139787629491968 logging_writer.py:48] [344200] global_step=344200, grad_norm=4.870547771453857, loss=0.6021638512611389
I0307 08:05:02.564989 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:05:08.581195 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:05:17.285733 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:05:19.630680 139951089751872 submission_runner.py:411] Time since start: 119922.84s, 	Step: 344218, 	{'train/accuracy': 0.9624322056770325, 'train/loss': 0.1427851915359497, 'validation/accuracy': 0.7567399740219116, 'validation/loss': 1.042166829109192, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8227174282073975, 'test/num_examples': 10000, 'score': 115832.13288211823, 'total_duration': 119922.84298753738, 'accumulated_submission_time': 115832.13288211823, 'accumulated_eval_time': 4061.798500061035, 'accumulated_logging_time': 16.369632244110107}
I0307 08:05:19.705826 139789013600000 logging_writer.py:48] [344218] accumulated_eval_time=4061.798500, accumulated_logging_time=16.369632, accumulated_submission_time=115832.132882, global_step=344218, preemption_count=0, score=115832.132882, test/accuracy=0.631600, test/loss=1.822717, test/num_examples=10000, total_duration=119922.842988, train/accuracy=0.962432, train/loss=0.142785, validation/accuracy=0.756740, validation/loss=1.042167, validation/num_examples=50000
I0307 08:05:47.585488 139789021992704 logging_writer.py:48] [344300] global_step=344300, grad_norm=4.568362712860107, loss=0.5525760650634766
I0307 08:06:21.247621 139789013600000 logging_writer.py:48] [344400] global_step=344400, grad_norm=4.411599159240723, loss=0.5950607061386108
I0307 08:06:54.958567 139789021992704 logging_writer.py:48] [344500] global_step=344500, grad_norm=4.236253261566162, loss=0.5682620406150818
I0307 08:07:28.546141 139789013600000 logging_writer.py:48] [344600] global_step=344600, grad_norm=4.316979885101318, loss=0.6408389806747437
I0307 08:08:02.151293 139789021992704 logging_writer.py:48] [344700] global_step=344700, grad_norm=4.686363220214844, loss=0.6330016851425171
I0307 08:08:35.797440 139789013600000 logging_writer.py:48] [344800] global_step=344800, grad_norm=4.792813777923584, loss=0.6375502347946167
I0307 08:09:09.408054 139789021992704 logging_writer.py:48] [344900] global_step=344900, grad_norm=4.617079257965088, loss=0.619777500629425
I0307 08:09:43.059416 139789013600000 logging_writer.py:48] [345000] global_step=345000, grad_norm=4.7767181396484375, loss=0.6882554888725281
I0307 08:10:16.685879 139789021992704 logging_writer.py:48] [345100] global_step=345100, grad_norm=4.323151111602783, loss=0.5759828090667725
I0307 08:10:50.332170 139789013600000 logging_writer.py:48] [345200] global_step=345200, grad_norm=4.533967018127441, loss=0.654115617275238
I0307 08:11:23.954450 139789021992704 logging_writer.py:48] [345300] global_step=345300, grad_norm=4.650345802307129, loss=0.6386469602584839
I0307 08:11:57.619956 139789013600000 logging_writer.py:48] [345400] global_step=345400, grad_norm=4.479838848114014, loss=0.623767614364624
I0307 08:12:31.250068 139789021992704 logging_writer.py:48] [345500] global_step=345500, grad_norm=4.209254741668701, loss=0.6421019434928894
I0307 08:13:04.928965 139789013600000 logging_writer.py:48] [345600] global_step=345600, grad_norm=4.3685407638549805, loss=0.6202085614204407
I0307 08:13:38.553903 139789021992704 logging_writer.py:48] [345700] global_step=345700, grad_norm=4.2880072593688965, loss=0.6116238832473755
I0307 08:13:49.814879 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:13:55.841285 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:14:04.491065 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:14:06.785264 139951089751872 submission_runner.py:411] Time since start: 120450.00s, 	Step: 345735, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.14662529528141022, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.0420275926589966, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8223018646240234, 'test/num_examples': 10000, 'score': 116342.17555689812, 'total_duration': 120449.99757409096, 'accumulated_submission_time': 116342.17555689812, 'accumulated_eval_time': 4078.7688570022583, 'accumulated_logging_time': 16.454465627670288}
I0307 08:14:06.859863 139787621099264 logging_writer.py:48] [345735] accumulated_eval_time=4078.768857, accumulated_logging_time=16.454466, accumulated_submission_time=116342.175557, global_step=345735, preemption_count=0, score=116342.175557, test/accuracy=0.632200, test/loss=1.822302, test/num_examples=10000, total_duration=120449.997574, train/accuracy=0.961396, train/loss=0.146625, validation/accuracy=0.757100, validation/loss=1.042028, validation/num_examples=50000
I0307 08:14:29.019448 139787629491968 logging_writer.py:48] [345800] global_step=345800, grad_norm=4.206840515136719, loss=0.6062673330307007
I0307 08:15:02.586558 139787621099264 logging_writer.py:48] [345900] global_step=345900, grad_norm=4.524470806121826, loss=0.6039845943450928
I0307 08:15:36.177733 139787629491968 logging_writer.py:48] [346000] global_step=346000, grad_norm=4.654238224029541, loss=0.6414223909378052
I0307 08:16:09.818960 139787621099264 logging_writer.py:48] [346100] global_step=346100, grad_norm=4.755447864532471, loss=0.6019812822341919
I0307 08:16:43.438272 139787629491968 logging_writer.py:48] [346200] global_step=346200, grad_norm=4.351585865020752, loss=0.6116056442260742
I0307 08:17:17.089681 139787621099264 logging_writer.py:48] [346300] global_step=346300, grad_norm=4.464815139770508, loss=0.6622524857521057
I0307 08:17:50.731776 139787629491968 logging_writer.py:48] [346400] global_step=346400, grad_norm=5.037141799926758, loss=0.6522372961044312
I0307 08:18:24.372910 139787621099264 logging_writer.py:48] [346500] global_step=346500, grad_norm=4.5337815284729, loss=0.6933881044387817
I0307 08:18:58.004537 139787629491968 logging_writer.py:48] [346600] global_step=346600, grad_norm=5.02187967300415, loss=0.6631281971931458
I0307 08:19:31.658390 139787621099264 logging_writer.py:48] [346700] global_step=346700, grad_norm=4.7041497230529785, loss=0.6417216658592224
I0307 08:20:05.262118 139787629491968 logging_writer.py:48] [346800] global_step=346800, grad_norm=4.398040771484375, loss=0.6141577959060669
I0307 08:20:38.922870 139787621099264 logging_writer.py:48] [346900] global_step=346900, grad_norm=4.47707462310791, loss=0.5940722823143005
I0307 08:21:12.560950 139787629491968 logging_writer.py:48] [347000] global_step=347000, grad_norm=4.4932169914245605, loss=0.703708291053772
I0307 08:21:46.216463 139787621099264 logging_writer.py:48] [347100] global_step=347100, grad_norm=4.8717570304870605, loss=0.6176108717918396
I0307 08:22:19.806702 139787629491968 logging_writer.py:48] [347200] global_step=347200, grad_norm=4.442298889160156, loss=0.6028358340263367
I0307 08:22:36.787869 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:22:43.493607 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:22:52.245130 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:22:54.535796 139951089751872 submission_runner.py:411] Time since start: 120977.75s, 	Step: 347252, 	{'train/accuracy': 0.9598612785339355, 'train/loss': 0.14649569988250732, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.039595127105713, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8208379745483398, 'test/num_examples': 10000, 'score': 116852.03779673576, 'total_duration': 120977.74806761742, 'accumulated_submission_time': 116852.03779673576, 'accumulated_eval_time': 4096.516710281372, 'accumulated_logging_time': 16.538807153701782}
I0307 08:22:54.618462 139789021992704 logging_writer.py:48] [347252] accumulated_eval_time=4096.516710, accumulated_logging_time=16.538807, accumulated_submission_time=116852.037797, global_step=347252, preemption_count=0, score=116852.037797, test/accuracy=0.632300, test/loss=1.820838, test/num_examples=10000, total_duration=120977.748068, train/accuracy=0.959861, train/loss=0.146496, validation/accuracy=0.757120, validation/loss=1.039595, validation/num_examples=50000
I0307 08:23:11.088811 139789030385408 logging_writer.py:48] [347300] global_step=347300, grad_norm=4.171248912811279, loss=0.5709583759307861
I0307 08:23:44.706610 139789021992704 logging_writer.py:48] [347400] global_step=347400, grad_norm=4.3352251052856445, loss=0.6219100952148438
I0307 08:24:18.353084 139789030385408 logging_writer.py:48] [347500] global_step=347500, grad_norm=4.762597560882568, loss=0.6345343589782715
I0307 08:24:52.001005 139789021992704 logging_writer.py:48] [347600] global_step=347600, grad_norm=4.34027624130249, loss=0.6074942350387573
I0307 08:25:25.646865 139789030385408 logging_writer.py:48] [347700] global_step=347700, grad_norm=4.938500881195068, loss=0.5961037874221802
I0307 08:25:59.307961 139789021992704 logging_writer.py:48] [347800] global_step=347800, grad_norm=4.288335800170898, loss=0.6148804426193237
I0307 08:26:32.950411 139789030385408 logging_writer.py:48] [347900] global_step=347900, grad_norm=4.594552993774414, loss=0.6022564172744751
I0307 08:27:06.639546 139789021992704 logging_writer.py:48] [348000] global_step=348000, grad_norm=4.52288293838501, loss=0.615035891532898
I0307 08:27:40.298888 139789030385408 logging_writer.py:48] [348100] global_step=348100, grad_norm=4.298863410949707, loss=0.6095343828201294
I0307 08:28:13.921309 139789021992704 logging_writer.py:48] [348200] global_step=348200, grad_norm=4.600478649139404, loss=0.646590530872345
I0307 08:28:47.565052 139789030385408 logging_writer.py:48] [348300] global_step=348300, grad_norm=3.893563747406006, loss=0.5816160440444946
I0307 08:29:21.229766 139789021992704 logging_writer.py:48] [348400] global_step=348400, grad_norm=4.841457843780518, loss=0.6422279477119446
I0307 08:29:54.892477 139789030385408 logging_writer.py:48] [348500] global_step=348500, grad_norm=4.276257514953613, loss=0.5354929566383362
I0307 08:30:28.541794 139789021992704 logging_writer.py:48] [348600] global_step=348600, grad_norm=4.911205291748047, loss=0.5968303084373474
I0307 08:31:02.197172 139789030385408 logging_writer.py:48] [348700] global_step=348700, grad_norm=4.334794044494629, loss=0.6204428672790527
I0307 08:31:24.682427 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:31:30.732329 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:31:39.403166 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:31:41.650175 139951089751872 submission_runner.py:411] Time since start: 121504.86s, 	Step: 348768, 	{'train/accuracy': 0.9602598547935486, 'train/loss': 0.14855141937732697, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.0419752597808838, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.823034405708313, 'test/num_examples': 10000, 'score': 117362.03670334816, 'total_duration': 121504.86248326302, 'accumulated_submission_time': 117362.03670334816, 'accumulated_eval_time': 4113.484414339066, 'accumulated_logging_time': 16.631054878234863}
I0307 08:31:41.725147 139788996814592 logging_writer.py:48] [348768] accumulated_eval_time=4113.484414, accumulated_logging_time=16.631055, accumulated_submission_time=117362.036703, global_step=348768, preemption_count=0, score=117362.036703, test/accuracy=0.631600, test/loss=1.823034, test/num_examples=10000, total_duration=121504.862483, train/accuracy=0.960260, train/loss=0.148551, validation/accuracy=0.756920, validation/loss=1.041975, validation/num_examples=50000
I0307 08:31:52.797121 139789005207296 logging_writer.py:48] [348800] global_step=348800, grad_norm=4.289605617523193, loss=0.661852240562439
I0307 08:32:26.354651 139788996814592 logging_writer.py:48] [348900] global_step=348900, grad_norm=4.678389549255371, loss=0.6179727911949158
I0307 08:32:59.995593 139789005207296 logging_writer.py:48] [349000] global_step=349000, grad_norm=4.86068058013916, loss=0.6249050498008728
I0307 08:33:33.648159 139788996814592 logging_writer.py:48] [349100] global_step=349100, grad_norm=4.8338751792907715, loss=0.6315740346908569
I0307 08:34:07.330500 139789005207296 logging_writer.py:48] [349200] global_step=349200, grad_norm=4.073233127593994, loss=0.5998705625534058
I0307 08:34:40.946974 139788996814592 logging_writer.py:48] [349300] global_step=349300, grad_norm=5.0281081199646, loss=0.6142577528953552
I0307 08:35:14.602634 139789005207296 logging_writer.py:48] [349400] global_step=349400, grad_norm=4.403844833374023, loss=0.654046893119812
I0307 08:35:48.230251 139788996814592 logging_writer.py:48] [349500] global_step=349500, grad_norm=4.684062480926514, loss=0.6453133821487427
I0307 08:36:21.874038 139789005207296 logging_writer.py:48] [349600] global_step=349600, grad_norm=4.356418132781982, loss=0.6226366758346558
I0307 08:36:55.465305 139788996814592 logging_writer.py:48] [349700] global_step=349700, grad_norm=5.068619251251221, loss=0.6641646027565002
I0307 08:37:29.177692 139789005207296 logging_writer.py:48] [349800] global_step=349800, grad_norm=4.771000385284424, loss=0.5817877054214478
I0307 08:38:02.778065 139788996814592 logging_writer.py:48] [349900] global_step=349900, grad_norm=4.188629150390625, loss=0.6069120764732361
I0307 08:38:36.396369 139789005207296 logging_writer.py:48] [350000] global_step=350000, grad_norm=4.870026111602783, loss=0.6729245185852051
I0307 08:39:10.001697 139788996814592 logging_writer.py:48] [350100] global_step=350100, grad_norm=4.143889427185059, loss=0.5203123092651367
I0307 08:39:43.553889 139789005207296 logging_writer.py:48] [350200] global_step=350200, grad_norm=4.535044193267822, loss=0.6271206736564636
I0307 08:40:11.870087 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:40:17.967298 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:40:26.508248 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:40:28.776604 139951089751872 submission_runner.py:411] Time since start: 122031.99s, 	Step: 350286, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14294403791427612, 'validation/accuracy': 0.7569400072097778, 'validation/loss': 1.0426743030548096, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8241838216781616, 'test/num_examples': 10000, 'score': 117872.1169886589, 'total_duration': 122031.98891568184, 'accumulated_submission_time': 117872.1169886589, 'accumulated_eval_time': 4130.390887975693, 'accumulated_logging_time': 16.715901851654053}
I0307 08:40:28.856225 139787621099264 logging_writer.py:48] [350286] accumulated_eval_time=4130.390888, accumulated_logging_time=16.715902, accumulated_submission_time=117872.116989, global_step=350286, preemption_count=0, score=117872.116989, test/accuracy=0.631300, test/loss=1.824184, test/num_examples=10000, total_duration=122031.988916, train/accuracy=0.961735, train/loss=0.142944, validation/accuracy=0.756940, validation/loss=1.042674, validation/num_examples=50000
I0307 08:40:33.898833 139787629491968 logging_writer.py:48] [350300] global_step=350300, grad_norm=4.699379920959473, loss=0.6422405242919922
I0307 08:41:07.541229 139787621099264 logging_writer.py:48] [350400] global_step=350400, grad_norm=4.753194332122803, loss=0.6203144192695618
I0307 08:41:41.161617 139787629491968 logging_writer.py:48] [350500] global_step=350500, grad_norm=4.5294084548950195, loss=0.6295129656791687
I0307 08:42:14.741762 139787621099264 logging_writer.py:48] [350600] global_step=350600, grad_norm=4.96095609664917, loss=0.6239295601844788
I0307 08:42:48.309200 139787629491968 logging_writer.py:48] [350700] global_step=350700, grad_norm=4.765120983123779, loss=0.6637659072875977
I0307 08:43:21.907795 139787621099264 logging_writer.py:48] [350800] global_step=350800, grad_norm=4.549179553985596, loss=0.5816472768783569
I0307 08:43:55.565387 139787629491968 logging_writer.py:48] [350900] global_step=350900, grad_norm=4.850893020629883, loss=0.6648012399673462
I0307 08:44:29.199823 139787621099264 logging_writer.py:48] [351000] global_step=351000, grad_norm=4.860169887542725, loss=0.6339354515075684
I0307 08:45:02.876821 139787629491968 logging_writer.py:48] [351100] global_step=351100, grad_norm=4.533431053161621, loss=0.6369876265525818
I0307 08:45:36.524271 139787621099264 logging_writer.py:48] [351200] global_step=351200, grad_norm=3.977137327194214, loss=0.5646134614944458
I0307 08:46:10.169623 139787629491968 logging_writer.py:48] [351300] global_step=351300, grad_norm=4.305889129638672, loss=0.633022665977478
I0307 08:46:43.853812 139787621099264 logging_writer.py:48] [351400] global_step=351400, grad_norm=4.543262958526611, loss=0.6114792227745056
I0307 08:47:17.493775 139787629491968 logging_writer.py:48] [351500] global_step=351500, grad_norm=4.2789411544799805, loss=0.5940070152282715
I0307 08:47:51.149313 139787621099264 logging_writer.py:48] [351600] global_step=351600, grad_norm=4.486074447631836, loss=0.6522749662399292
I0307 08:48:24.782263 139787629491968 logging_writer.py:48] [351700] global_step=351700, grad_norm=4.235785484313965, loss=0.5877904295921326
I0307 08:48:58.395982 139787621099264 logging_writer.py:48] [351800] global_step=351800, grad_norm=4.443094730377197, loss=0.6703477501869202
I0307 08:48:58.876328 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:49:04.878891 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:49:13.553593 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:49:15.856685 139951089751872 submission_runner.py:411] Time since start: 122559.07s, 	Step: 351803, 	{'train/accuracy': 0.9614955186843872, 'train/loss': 0.14411213994026184, 'validation/accuracy': 0.7574399709701538, 'validation/loss': 1.0409822463989258, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8227815628051758, 'test/num_examples': 10000, 'score': 118382.07088541985, 'total_duration': 122559.0689971447, 'accumulated_submission_time': 118382.07088541985, 'accumulated_eval_time': 4147.371191740036, 'accumulated_logging_time': 16.80562400817871}
I0307 08:49:15.935322 139787629491968 logging_writer.py:48] [351803] accumulated_eval_time=4147.371192, accumulated_logging_time=16.805624, accumulated_submission_time=118382.070885, global_step=351803, preemption_count=0, score=118382.070885, test/accuracy=0.631400, test/loss=1.822782, test/num_examples=10000, total_duration=122559.068997, train/accuracy=0.961496, train/loss=0.144112, validation/accuracy=0.757440, validation/loss=1.040982, validation/num_examples=50000
I0307 08:49:48.904645 139789005207296 logging_writer.py:48] [351900] global_step=351900, grad_norm=4.363326549530029, loss=0.6335831880569458
I0307 08:50:22.473664 139787629491968 logging_writer.py:48] [352000] global_step=352000, grad_norm=4.395987033843994, loss=0.6511427760124207
I0307 08:50:56.119260 139789005207296 logging_writer.py:48] [352100] global_step=352100, grad_norm=4.438274383544922, loss=0.6363276839256287
I0307 08:51:29.772998 139787629491968 logging_writer.py:48] [352200] global_step=352200, grad_norm=4.368178367614746, loss=0.6070293188095093
I0307 08:52:03.382950 139789005207296 logging_writer.py:48] [352300] global_step=352300, grad_norm=4.889970779418945, loss=0.6414788961410522
I0307 08:52:37.052240 139787629491968 logging_writer.py:48] [352400] global_step=352400, grad_norm=4.561161518096924, loss=0.6101405024528503
I0307 08:53:10.717982 139789005207296 logging_writer.py:48] [352500] global_step=352500, grad_norm=4.885732173919678, loss=0.6519009470939636
I0307 08:53:44.361872 139787629491968 logging_writer.py:48] [352600] global_step=352600, grad_norm=4.3841423988342285, loss=0.586484968662262
I0307 08:54:18.019528 139789005207296 logging_writer.py:48] [352700] global_step=352700, grad_norm=4.488987445831299, loss=0.5907743573188782
I0307 08:54:51.676064 139787629491968 logging_writer.py:48] [352800] global_step=352800, grad_norm=4.97226095199585, loss=0.6295319199562073
I0307 08:55:25.293693 139789005207296 logging_writer.py:48] [352900] global_step=352900, grad_norm=4.215470790863037, loss=0.6324459910392761
I0307 08:55:59.031406 139787629491968 logging_writer.py:48] [353000] global_step=353000, grad_norm=4.505054950714111, loss=0.6923014521598816
I0307 08:56:32.597259 139789005207296 logging_writer.py:48] [353100] global_step=353100, grad_norm=4.4134345054626465, loss=0.6702307462692261
I0307 08:57:06.181388 139787629491968 logging_writer.py:48] [353200] global_step=353200, grad_norm=4.53914213180542, loss=0.614702582359314
I0307 08:57:39.842173 139789005207296 logging_writer.py:48] [353300] global_step=353300, grad_norm=4.604518413543701, loss=0.620169997215271
I0307 08:57:46.037118 139951089751872 spec.py:321] Evaluating on the training split.
I0307 08:57:52.117478 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 08:58:00.831092 139951089751872 spec.py:349] Evaluating on the test split.
I0307 08:58:03.151693 139951089751872 submission_runner.py:411] Time since start: 123086.36s, 	Step: 353320, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14671777188777924, 'validation/accuracy': 0.7574399709701538, 'validation/loss': 1.0406513214111328, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8218772411346436, 'test/num_examples': 10000, 'score': 118892.1052172184, 'total_duration': 123086.36362028122, 'accumulated_submission_time': 118892.1052172184, 'accumulated_eval_time': 4164.485343694687, 'accumulated_logging_time': 16.895461559295654}
I0307 08:58:03.230485 139788996814592 logging_writer.py:48] [353320] accumulated_eval_time=4164.485344, accumulated_logging_time=16.895462, accumulated_submission_time=118892.105217, global_step=353320, preemption_count=0, score=118892.105217, test/accuracy=0.631400, test/loss=1.821877, test/num_examples=10000, total_duration=123086.363620, train/accuracy=0.961456, train/loss=0.146718, validation/accuracy=0.757440, validation/loss=1.040651, validation/num_examples=50000
I0307 08:58:30.472399 139789021992704 logging_writer.py:48] [353400] global_step=353400, grad_norm=4.511845588684082, loss=0.6110253930091858
I0307 08:59:04.102431 139788996814592 logging_writer.py:48] [353500] global_step=353500, grad_norm=4.1760573387146, loss=0.5587920546531677
I0307 08:59:37.739060 139789021992704 logging_writer.py:48] [353600] global_step=353600, grad_norm=4.782490253448486, loss=0.6538700461387634
I0307 09:00:11.347848 139788996814592 logging_writer.py:48] [353700] global_step=353700, grad_norm=4.7604899406433105, loss=0.6611056327819824
I0307 09:00:45.045376 139789021992704 logging_writer.py:48] [353800] global_step=353800, grad_norm=5.042887210845947, loss=0.6974864602088928
I0307 09:01:18.717191 139788996814592 logging_writer.py:48] [353900] global_step=353900, grad_norm=4.759893417358398, loss=0.6252874135971069
I0307 09:01:52.450784 139789021992704 logging_writer.py:48] [354000] global_step=354000, grad_norm=4.404972076416016, loss=0.6520520448684692
I0307 09:02:26.033569 139788996814592 logging_writer.py:48] [354100] global_step=354100, grad_norm=4.586202144622803, loss=0.6140688061714172
I0307 09:02:59.601114 139789021992704 logging_writer.py:48] [354200] global_step=354200, grad_norm=4.290414810180664, loss=0.5809844732284546
I0307 09:03:33.186175 139788996814592 logging_writer.py:48] [354300] global_step=354300, grad_norm=4.92210054397583, loss=0.6575568318367004
I0307 09:04:06.808078 139789021992704 logging_writer.py:48] [354400] global_step=354400, grad_norm=4.806263446807861, loss=0.6582956314086914
I0307 09:04:40.473818 139788996814592 logging_writer.py:48] [354500] global_step=354500, grad_norm=4.7035393714904785, loss=0.6784889698028564
I0307 09:05:14.123067 139789021992704 logging_writer.py:48] [354600] global_step=354600, grad_norm=4.213825225830078, loss=0.5949304103851318
I0307 09:05:47.773290 139788996814592 logging_writer.py:48] [354700] global_step=354700, grad_norm=4.4244537353515625, loss=0.5863574147224426
I0307 09:06:21.405417 139789021992704 logging_writer.py:48] [354800] global_step=354800, grad_norm=4.639324188232422, loss=0.6735785007476807
I0307 09:06:33.324495 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:06:39.333760 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:06:47.989849 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:06:50.249828 139951089751872 submission_runner.py:411] Time since start: 123613.46s, 	Step: 354837, 	{'train/accuracy': 0.9608577489852905, 'train/loss': 0.14623570442199707, 'validation/accuracy': 0.7569599747657776, 'validation/loss': 1.042207956314087, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8235318660736084, 'test/num_examples': 10000, 'score': 119402.13184118271, 'total_duration': 123613.46213245392, 'accumulated_submission_time': 119402.13184118271, 'accumulated_eval_time': 4181.4106187820435, 'accumulated_logging_time': 16.984424591064453}
I0307 09:06:50.327224 139787629491968 logging_writer.py:48] [354837] accumulated_eval_time=4181.410619, accumulated_logging_time=16.984425, accumulated_submission_time=119402.131841, global_step=354837, preemption_count=0, score=119402.131841, test/accuracy=0.631500, test/loss=1.823532, test/num_examples=10000, total_duration=123613.462132, train/accuracy=0.960858, train/loss=0.146236, validation/accuracy=0.756960, validation/loss=1.042208, validation/num_examples=50000
I0307 09:07:11.807504 139788996814592 logging_writer.py:48] [354900] global_step=354900, grad_norm=4.619419097900391, loss=0.552415132522583
I0307 09:07:45.367161 139787629491968 logging_writer.py:48] [355000] global_step=355000, grad_norm=4.411269664764404, loss=0.587307333946228
I0307 09:08:19.001029 139788996814592 logging_writer.py:48] [355100] global_step=355100, grad_norm=4.778459548950195, loss=0.6678577661514282
I0307 09:08:52.564510 139787629491968 logging_writer.py:48] [355200] global_step=355200, grad_norm=4.8067731857299805, loss=0.588995635509491
I0307 09:09:26.157420 139788996814592 logging_writer.py:48] [355300] global_step=355300, grad_norm=4.863687515258789, loss=0.614506185054779
I0307 09:09:59.811734 139787629491968 logging_writer.py:48] [355400] global_step=355400, grad_norm=4.204615592956543, loss=0.5708808302879333
I0307 09:10:33.430343 139788996814592 logging_writer.py:48] [355500] global_step=355500, grad_norm=4.465799331665039, loss=0.6662136912345886
I0307 09:11:07.094138 139787629491968 logging_writer.py:48] [355600] global_step=355600, grad_norm=4.373283863067627, loss=0.5910506844520569
I0307 09:11:40.730321 139788996814592 logging_writer.py:48] [355700] global_step=355700, grad_norm=4.398190498352051, loss=0.583207368850708
I0307 09:12:14.384938 139787629491968 logging_writer.py:48] [355800] global_step=355800, grad_norm=4.363219738006592, loss=0.5786710381507874
I0307 09:12:48.001377 139788996814592 logging_writer.py:48] [355900] global_step=355900, grad_norm=4.8358869552612305, loss=0.6500524878501892
I0307 09:13:21.661133 139787629491968 logging_writer.py:48] [356000] global_step=356000, grad_norm=4.84185266494751, loss=0.6764602065086365
I0307 09:13:55.297387 139788996814592 logging_writer.py:48] [356100] global_step=356100, grad_norm=4.689703941345215, loss=0.58026522397995
I0307 09:14:28.996713 139787629491968 logging_writer.py:48] [356200] global_step=356200, grad_norm=4.29344367980957, loss=0.5367022156715393
I0307 09:15:02.624014 139788996814592 logging_writer.py:48] [356300] global_step=356300, grad_norm=5.0455322265625, loss=0.6665202379226685
I0307 09:15:20.284375 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:15:26.357006 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:15:34.830717 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:15:37.118571 139951089751872 submission_runner.py:411] Time since start: 124140.33s, 	Step: 356354, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.1481306254863739, 'validation/accuracy': 0.7575599551200867, 'validation/loss': 1.041216492652893, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8233606815338135, 'test/num_examples': 10000, 'score': 119912.02209234238, 'total_duration': 124140.33087706566, 'accumulated_submission_time': 119912.02209234238, 'accumulated_eval_time': 4198.244757652283, 'accumulated_logging_time': 17.072447061538696}
I0307 09:15:37.196290 139787629491968 logging_writer.py:48] [356354] accumulated_eval_time=4198.244758, accumulated_logging_time=17.072447, accumulated_submission_time=119912.022092, global_step=356354, preemption_count=0, score=119912.022092, test/accuracy=0.632500, test/loss=1.823361, test/num_examples=10000, total_duration=124140.330877, train/accuracy=0.960300, train/loss=0.148131, validation/accuracy=0.757560, validation/loss=1.041216, validation/num_examples=50000
I0307 09:15:52.989237 139788996814592 logging_writer.py:48] [356400] global_step=356400, grad_norm=4.73237943649292, loss=0.5683442950248718
I0307 09:16:26.673806 139787629491968 logging_writer.py:48] [356500] global_step=356500, grad_norm=5.046454429626465, loss=0.5888345837593079
I0307 09:17:00.334105 139788996814592 logging_writer.py:48] [356600] global_step=356600, grad_norm=4.47916841506958, loss=0.6252127289772034
I0307 09:17:33.973369 139787629491968 logging_writer.py:48] [356700] global_step=356700, grad_norm=4.6835103034973145, loss=0.6063508987426758
I0307 09:18:07.541327 139788996814592 logging_writer.py:48] [356800] global_step=356800, grad_norm=4.328561305999756, loss=0.6219979524612427
I0307 09:18:41.090673 139787629491968 logging_writer.py:48] [356900] global_step=356900, grad_norm=4.380377769470215, loss=0.6438397765159607
I0307 09:19:14.699794 139788996814592 logging_writer.py:48] [357000] global_step=357000, grad_norm=4.677824020385742, loss=0.6358333826065063
I0307 09:19:48.326059 139787629491968 logging_writer.py:48] [357100] global_step=357100, grad_norm=4.564598560333252, loss=0.5898140668869019
I0307 09:20:22.029539 139788996814592 logging_writer.py:48] [357200] global_step=357200, grad_norm=4.272851943969727, loss=0.5871990323066711
I0307 09:20:55.645706 139787629491968 logging_writer.py:48] [357300] global_step=357300, grad_norm=4.405766010284424, loss=0.6211109161376953
I0307 09:21:29.263880 139788996814592 logging_writer.py:48] [357400] global_step=357400, grad_norm=4.361452102661133, loss=0.5526331663131714
I0307 09:22:02.868309 139787629491968 logging_writer.py:48] [357500] global_step=357500, grad_norm=4.2773823738098145, loss=0.6097053289413452
I0307 09:22:36.479672 139788996814592 logging_writer.py:48] [357600] global_step=357600, grad_norm=4.168529033660889, loss=0.6117792129516602
I0307 09:23:10.106000 139787629491968 logging_writer.py:48] [357700] global_step=357700, grad_norm=4.210545539855957, loss=0.6006726026535034
I0307 09:23:43.747995 139788996814592 logging_writer.py:48] [357800] global_step=357800, grad_norm=4.320338249206543, loss=0.6235097646713257
I0307 09:24:07.121341 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:24:13.181266 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:24:21.891083 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:24:24.170559 139951089751872 submission_runner.py:411] Time since start: 124667.38s, 	Step: 357871, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14780618250370026, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0402666330337524, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8204973936080933, 'test/num_examples': 10000, 'score': 120421.88165020943, 'total_duration': 124667.38286662102, 'accumulated_submission_time': 120421.88165020943, 'accumulated_eval_time': 4215.293922185898, 'accumulated_logging_time': 17.160407543182373}
I0307 09:24:24.248010 139789013600000 logging_writer.py:48] [357871] accumulated_eval_time=4215.293922, accumulated_logging_time=17.160408, accumulated_submission_time=120421.881650, global_step=357871, preemption_count=0, score=120421.881650, test/accuracy=0.631800, test/loss=1.820497, test/num_examples=10000, total_duration=124667.382867, train/accuracy=0.960539, train/loss=0.147806, validation/accuracy=0.757200, validation/loss=1.040267, validation/num_examples=50000
I0307 09:24:34.315484 139789021992704 logging_writer.py:48] [357900] global_step=357900, grad_norm=4.466629981994629, loss=0.6674860119819641
I0307 09:25:07.907820 139789013600000 logging_writer.py:48] [358000] global_step=358000, grad_norm=4.353758811950684, loss=0.5928145051002502
I0307 09:25:41.465898 139789021992704 logging_writer.py:48] [358100] global_step=358100, grad_norm=4.086899757385254, loss=0.5600562691688538
I0307 09:26:15.054025 139789013600000 logging_writer.py:48] [358200] global_step=358200, grad_norm=4.555656433105469, loss=0.6383076310157776
I0307 09:26:48.671362 139789021992704 logging_writer.py:48] [358300] global_step=358300, grad_norm=4.757747650146484, loss=0.6374273300170898
I0307 09:27:22.300032 139789013600000 logging_writer.py:48] [358400] global_step=358400, grad_norm=5.016016960144043, loss=0.6468505859375
I0307 09:27:55.982561 139789021992704 logging_writer.py:48] [358500] global_step=358500, grad_norm=4.56602144241333, loss=0.6383528709411621
I0307 09:28:29.659057 139789013600000 logging_writer.py:48] [358600] global_step=358600, grad_norm=4.481777191162109, loss=0.6174300909042358
I0307 09:29:03.332895 139789021992704 logging_writer.py:48] [358700] global_step=358700, grad_norm=4.785125255584717, loss=0.6792671084403992
I0307 09:29:36.931163 139789013600000 logging_writer.py:48] [358800] global_step=358800, grad_norm=5.035039901733398, loss=0.7308648824691772
I0307 09:30:10.591007 139789021992704 logging_writer.py:48] [358900] global_step=358900, grad_norm=4.523810863494873, loss=0.6366882920265198
I0307 09:30:44.210074 139789013600000 logging_writer.py:48] [359000] global_step=359000, grad_norm=4.26231050491333, loss=0.6063130497932434
I0307 09:31:17.877320 139789021992704 logging_writer.py:48] [359100] global_step=359100, grad_norm=4.231533527374268, loss=0.6166744232177734
I0307 09:31:51.486536 139789013600000 logging_writer.py:48] [359200] global_step=359200, grad_norm=4.3468918800354, loss=0.5773467421531677
I0307 09:32:25.143633 139789021992704 logging_writer.py:48] [359300] global_step=359300, grad_norm=4.1823225021362305, loss=0.6189064979553223
I0307 09:32:54.240107 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:33:00.275346 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:33:09.044949 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:33:11.304540 139951089751872 submission_runner.py:411] Time since start: 125194.52s, 	Step: 359388, 	{'train/accuracy': 0.9615951776504517, 'train/loss': 0.14287497103214264, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0410480499267578, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8228076696395874, 'test/num_examples': 10000, 'score': 120931.80952954292, 'total_duration': 125194.51679444313, 'accumulated_submission_time': 120931.80952954292, 'accumulated_eval_time': 4232.3582463264465, 'accumulated_logging_time': 17.2476224899292}
I0307 09:33:11.385067 139788996814592 logging_writer.py:48] [359388] accumulated_eval_time=4232.358246, accumulated_logging_time=17.247622, accumulated_submission_time=120931.809530, global_step=359388, preemption_count=0, score=120931.809530, test/accuracy=0.632100, test/loss=1.822808, test/num_examples=10000, total_duration=125194.516794, train/accuracy=0.961595, train/loss=0.142875, validation/accuracy=0.757200, validation/loss=1.041048, validation/num_examples=50000
I0307 09:33:15.748357 139789005207296 logging_writer.py:48] [359400] global_step=359400, grad_norm=4.532376289367676, loss=0.6419382095336914
I0307 09:33:49.351473 139788996814592 logging_writer.py:48] [359500] global_step=359500, grad_norm=4.157548427581787, loss=0.6101880073547363
I0307 09:34:22.954063 139789005207296 logging_writer.py:48] [359600] global_step=359600, grad_norm=4.264355182647705, loss=0.6185328364372253
I0307 09:34:56.606930 139788996814592 logging_writer.py:48] [359700] global_step=359700, grad_norm=4.818313121795654, loss=0.6798808574676514
I0307 09:35:30.291789 139789005207296 logging_writer.py:48] [359800] global_step=359800, grad_norm=4.530745983123779, loss=0.6130692362785339
I0307 09:36:03.950507 139788996814592 logging_writer.py:48] [359900] global_step=359900, grad_norm=4.4722418785095215, loss=0.6436572074890137
I0307 09:36:37.623156 139789005207296 logging_writer.py:48] [360000] global_step=360000, grad_norm=4.689504623413086, loss=0.6201012134552002
I0307 09:37:11.269521 139788996814592 logging_writer.py:48] [360100] global_step=360100, grad_norm=4.630630970001221, loss=0.5839342474937439
I0307 09:37:44.961928 139789005207296 logging_writer.py:48] [360200] global_step=360200, grad_norm=4.539794445037842, loss=0.61488938331604
I0307 09:38:18.601471 139788996814592 logging_writer.py:48] [360300] global_step=360300, grad_norm=4.265014171600342, loss=0.6087629795074463
I0307 09:38:52.270649 139789005207296 logging_writer.py:48] [360400] global_step=360400, grad_norm=3.9177467823028564, loss=0.559898853302002
I0307 09:39:25.857331 139788996814592 logging_writer.py:48] [360500] global_step=360500, grad_norm=5.01296854019165, loss=0.6931766271591187
I0307 09:39:59.485455 139789005207296 logging_writer.py:48] [360600] global_step=360600, grad_norm=4.448968410491943, loss=0.6175487637519836
I0307 09:40:33.105681 139788996814592 logging_writer.py:48] [360700] global_step=360700, grad_norm=5.125072002410889, loss=0.6698429584503174
I0307 09:41:06.746084 139789005207296 logging_writer.py:48] [360800] global_step=360800, grad_norm=4.354044437408447, loss=0.6365328431129456
I0307 09:41:40.395456 139788996814592 logging_writer.py:48] [360900] global_step=360900, grad_norm=3.889740467071533, loss=0.6118139624595642
I0307 09:41:41.546668 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:41:47.611044 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:41:56.153756 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:41:58.447218 139951089751872 submission_runner.py:411] Time since start: 125721.66s, 	Step: 360905, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.15055322647094727, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0411326885223389, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8209383487701416, 'test/num_examples': 10000, 'score': 121441.90519046783, 'total_duration': 125721.65953350067, 'accumulated_submission_time': 121441.90519046783, 'accumulated_eval_time': 4249.258747339249, 'accumulated_logging_time': 17.337990045547485}
I0307 09:41:58.523874 139787621099264 logging_writer.py:48] [360905] accumulated_eval_time=4249.258747, accumulated_logging_time=17.337990, accumulated_submission_time=121441.905190, global_step=360905, preemption_count=0, score=121441.905190, test/accuracy=0.631900, test/loss=1.820938, test/num_examples=10000, total_duration=125721.659534, train/accuracy=0.960081, train/loss=0.150553, validation/accuracy=0.757080, validation/loss=1.041133, validation/num_examples=50000
I0307 09:42:30.825059 139787629491968 logging_writer.py:48] [361000] global_step=361000, grad_norm=4.330551624298096, loss=0.6565159559249878
I0307 09:43:04.440268 139787621099264 logging_writer.py:48] [361100] global_step=361100, grad_norm=4.400028228759766, loss=0.6836628317832947
I0307 09:43:38.107968 139787629491968 logging_writer.py:48] [361200] global_step=361200, grad_norm=4.365859031677246, loss=0.6163797378540039
I0307 09:44:11.731315 139787621099264 logging_writer.py:48] [361300] global_step=361300, grad_norm=5.189681529998779, loss=0.6475027799606323
I0307 09:44:45.402219 139787629491968 logging_writer.py:48] [361400] global_step=361400, grad_norm=4.67344856262207, loss=0.6198003888130188
I0307 09:45:19.213848 139787621099264 logging_writer.py:48] [361500] global_step=361500, grad_norm=4.350050926208496, loss=0.5880837440490723
I0307 09:45:52.885969 139787629491968 logging_writer.py:48] [361600] global_step=361600, grad_norm=4.304722309112549, loss=0.556243896484375
I0307 09:46:26.494155 139787621099264 logging_writer.py:48] [361700] global_step=361700, grad_norm=5.005555152893066, loss=0.5946451425552368
I0307 09:47:00.160069 139787629491968 logging_writer.py:48] [361800] global_step=361800, grad_norm=4.296725749969482, loss=0.5523385405540466
I0307 09:47:33.808143 139787621099264 logging_writer.py:48] [361900] global_step=361900, grad_norm=4.635618209838867, loss=0.6386272311210632
I0307 09:48:07.458524 139787629491968 logging_writer.py:48] [362000] global_step=362000, grad_norm=4.410369873046875, loss=0.6573346853256226
I0307 09:48:41.092026 139787621099264 logging_writer.py:48] [362100] global_step=362100, grad_norm=4.533276557922363, loss=0.5651214718818665
I0307 09:49:14.760572 139787629491968 logging_writer.py:48] [362200] global_step=362200, grad_norm=4.24472188949585, loss=0.5967886447906494
I0307 09:49:48.405892 139787621099264 logging_writer.py:48] [362300] global_step=362300, grad_norm=4.429346084594727, loss=0.7128807902336121
I0307 09:50:22.099807 139787629491968 logging_writer.py:48] [362400] global_step=362400, grad_norm=5.322988510131836, loss=0.665641188621521
I0307 09:50:28.649246 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:50:34.800047 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:50:43.321202 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:50:45.617154 139951089751872 submission_runner.py:411] Time since start: 126248.83s, 	Step: 362421, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.14728204905986786, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0414822101593018, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8226184844970703, 'test/num_examples': 10000, 'score': 121951.9643895626, 'total_duration': 126248.82946372032, 'accumulated_submission_time': 121951.9643895626, 'accumulated_eval_time': 4266.226603746414, 'accumulated_logging_time': 17.42392110824585}
I0307 09:50:45.702001 139787621099264 logging_writer.py:48] [362421] accumulated_eval_time=4266.226604, accumulated_logging_time=17.423921, accumulated_submission_time=121951.964390, global_step=362421, preemption_count=0, score=121951.964390, test/accuracy=0.631800, test/loss=1.822618, test/num_examples=10000, total_duration=126248.829464, train/accuracy=0.960100, train/loss=0.147282, validation/accuracy=0.757200, validation/loss=1.041482, validation/num_examples=50000
I0307 09:51:12.719205 139789013600000 logging_writer.py:48] [362500] global_step=362500, grad_norm=4.553838729858398, loss=0.6478950381278992
I0307 09:51:46.365193 139787621099264 logging_writer.py:48] [362600] global_step=362600, grad_norm=4.59453010559082, loss=0.6532036066055298
I0307 09:52:20.023855 139789013600000 logging_writer.py:48] [362700] global_step=362700, grad_norm=4.6052117347717285, loss=0.6181811690330505
I0307 09:52:53.656331 139787621099264 logging_writer.py:48] [362800] global_step=362800, grad_norm=4.783592700958252, loss=0.603698194026947
I0307 09:53:27.284274 139789013600000 logging_writer.py:48] [362900] global_step=362900, grad_norm=4.671631336212158, loss=0.6499710083007812
I0307 09:54:00.937346 139787621099264 logging_writer.py:48] [363000] global_step=363000, grad_norm=4.537789344787598, loss=0.6349565386772156
I0307 09:54:34.622733 139789013600000 logging_writer.py:48] [363100] global_step=363100, grad_norm=4.573390960693359, loss=0.6107859015464783
I0307 09:55:08.267835 139787621099264 logging_writer.py:48] [363200] global_step=363200, grad_norm=4.945004463195801, loss=0.6245955228805542
I0307 09:55:41.924843 139789013600000 logging_writer.py:48] [363300] global_step=363300, grad_norm=4.580546855926514, loss=0.6501857042312622
I0307 09:56:15.580158 139787621099264 logging_writer.py:48] [363400] global_step=363400, grad_norm=4.886890411376953, loss=0.6340622901916504
I0307 09:56:49.218250 139789013600000 logging_writer.py:48] [363500] global_step=363500, grad_norm=3.8560473918914795, loss=0.5249022841453552
I0307 09:57:22.857362 139787621099264 logging_writer.py:48] [363600] global_step=363600, grad_norm=4.72996711730957, loss=0.6142442226409912
I0307 09:57:56.435002 139789013600000 logging_writer.py:48] [363700] global_step=363700, grad_norm=4.6163716316223145, loss=0.6523628234863281
I0307 09:58:30.025539 139787621099264 logging_writer.py:48] [363800] global_step=363800, grad_norm=4.254278659820557, loss=0.620955765247345
I0307 09:59:03.665957 139789013600000 logging_writer.py:48] [363900] global_step=363900, grad_norm=4.404841423034668, loss=0.6177919507026672
I0307 09:59:15.932328 139951089751872 spec.py:321] Evaluating on the training split.
I0307 09:59:21.923533 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 09:59:30.516190 139951089751872 spec.py:349] Evaluating on the test split.
I0307 09:59:32.784545 139951089751872 submission_runner.py:411] Time since start: 126776.00s, 	Step: 363938, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.14804889261722565, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.0426230430603027, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.823939323425293, 'test/num_examples': 10000, 'score': 122462.1295595169, 'total_duration': 126775.99683475494, 'accumulated_submission_time': 122462.1295595169, 'accumulated_eval_time': 4283.07874751091, 'accumulated_logging_time': 17.518263339996338}
I0307 09:59:32.862441 139789021992704 logging_writer.py:48] [363938] accumulated_eval_time=4283.078748, accumulated_logging_time=17.518263, accumulated_submission_time=122462.129560, global_step=363938, preemption_count=0, score=122462.129560, test/accuracy=0.632400, test/loss=1.823939, test/num_examples=10000, total_duration=126775.996835, train/accuracy=0.960499, train/loss=0.148049, validation/accuracy=0.757000, validation/loss=1.042623, validation/num_examples=50000
I0307 09:59:54.069657 139789030385408 logging_writer.py:48] [364000] global_step=364000, grad_norm=4.752923965454102, loss=0.7032462358474731
I0307 10:00:27.738440 139789021992704 logging_writer.py:48] [364100] global_step=364100, grad_norm=4.579095363616943, loss=0.6835089921951294
I0307 10:01:01.344015 139789030385408 logging_writer.py:48] [364200] global_step=364200, grad_norm=5.118109226226807, loss=0.6331677436828613
I0307 10:01:34.999784 139789021992704 logging_writer.py:48] [364300] global_step=364300, grad_norm=4.408632755279541, loss=0.6366161108016968
I0307 10:02:08.614745 139789030385408 logging_writer.py:48] [364400] global_step=364400, grad_norm=4.605643272399902, loss=0.6557390689849854
I0307 10:02:42.261960 139789021992704 logging_writer.py:48] [364500] global_step=364500, grad_norm=4.729450225830078, loss=0.5986855030059814
I0307 10:03:15.911373 139789030385408 logging_writer.py:48] [364600] global_step=364600, grad_norm=4.487240314483643, loss=0.6344393491744995
I0307 10:03:49.530372 139789021992704 logging_writer.py:48] [364700] global_step=364700, grad_norm=4.427122592926025, loss=0.5941959023475647
I0307 10:04:23.147447 139789030385408 logging_writer.py:48] [364800] global_step=364800, grad_norm=4.343765735626221, loss=0.6508919596672058
I0307 10:04:56.808484 139789021992704 logging_writer.py:48] [364900] global_step=364900, grad_norm=4.650759220123291, loss=0.7414531111717224
I0307 10:05:30.421521 139789030385408 logging_writer.py:48] [365000] global_step=365000, grad_norm=4.573878765106201, loss=0.5883089900016785
I0307 10:06:04.071131 139789021992704 logging_writer.py:48] [365100] global_step=365100, grad_norm=4.4623260498046875, loss=0.6509414315223694
I0307 10:06:37.670148 139789030385408 logging_writer.py:48] [365200] global_step=365200, grad_norm=4.21269416809082, loss=0.6256875395774841
I0307 10:07:11.330943 139789021992704 logging_writer.py:48] [365300] global_step=365300, grad_norm=4.496863842010498, loss=0.6583023071289062
I0307 10:07:44.947419 139789030385408 logging_writer.py:48] [365400] global_step=365400, grad_norm=4.817850112915039, loss=0.6449403762817383
I0307 10:08:02.927575 139951089751872 spec.py:321] Evaluating on the training split.
I0307 10:08:09.004195 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 10:08:17.566361 139951089751872 spec.py:349] Evaluating on the test split.
I0307 10:08:19.837786 139951089751872 submission_runner.py:411] Time since start: 127303.05s, 	Step: 365455, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.14424340426921844, 'validation/accuracy': 0.7568599581718445, 'validation/loss': 1.0414214134216309, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.822780728340149, 'test/num_examples': 10000, 'score': 122972.12771296501, 'total_duration': 127303.0500793457, 'accumulated_submission_time': 122972.12771296501, 'accumulated_eval_time': 4299.988889694214, 'accumulated_logging_time': 17.606799125671387}
I0307 10:08:19.908794 139787612706560 logging_writer.py:48] [365455] accumulated_eval_time=4299.988890, accumulated_logging_time=17.606799, accumulated_submission_time=122972.127713, global_step=365455, preemption_count=0, score=122972.127713, test/accuracy=0.630900, test/loss=1.822781, test/num_examples=10000, total_duration=127303.050079, train/accuracy=0.961476, train/loss=0.144243, validation/accuracy=0.756860, validation/loss=1.041421, validation/num_examples=50000
I0307 10:08:35.373528 139787621099264 logging_writer.py:48] [365500] global_step=365500, grad_norm=4.511894702911377, loss=0.6050009727478027
I0307 10:09:08.970988 139787612706560 logging_writer.py:48] [365600] global_step=365600, grad_norm=5.046798229217529, loss=0.6894380450248718
I0307 10:09:42.665857 139787621099264 logging_writer.py:48] [365700] global_step=365700, grad_norm=4.525790214538574, loss=0.6177136898040771
I0307 10:10:16.340301 139787612706560 logging_writer.py:48] [365800] global_step=365800, grad_norm=4.285128593444824, loss=0.6028459072113037
I0307 10:10:49.968091 139787621099264 logging_writer.py:48] [365900] global_step=365900, grad_norm=4.491271495819092, loss=0.5671350359916687
I0307 10:11:23.602734 139787612706560 logging_writer.py:48] [366000] global_step=366000, grad_norm=4.703037738800049, loss=0.6628494262695312
I0307 10:11:57.249924 139787621099264 logging_writer.py:48] [366100] global_step=366100, grad_norm=4.8246541023254395, loss=0.6012600660324097
I0307 10:12:30.902545 139787612706560 logging_writer.py:48] [366200] global_step=366200, grad_norm=4.859527111053467, loss=0.6406084895133972
I0307 10:13:04.513355 139787621099264 logging_writer.py:48] [366300] global_step=366300, grad_norm=4.23919153213501, loss=0.6048421263694763
I0307 10:13:38.164345 139787612706560 logging_writer.py:48] [366400] global_step=366400, grad_norm=4.616652011871338, loss=0.6157515048980713
I0307 10:14:11.778270 139787621099264 logging_writer.py:48] [366500] global_step=366500, grad_norm=4.58728551864624, loss=0.6165941953659058
I0307 10:14:45.421709 139787612706560 logging_writer.py:48] [366600] global_step=366600, grad_norm=4.514891624450684, loss=0.7292749881744385
I0307 10:15:19.112511 139787621099264 logging_writer.py:48] [366700] global_step=366700, grad_norm=4.278247833251953, loss=0.6124615669250488
I0307 10:15:52.730623 139787612706560 logging_writer.py:48] [366800] global_step=366800, grad_norm=4.136612892150879, loss=0.5684909820556641
I0307 10:16:26.406174 139787621099264 logging_writer.py:48] [366900] global_step=366900, grad_norm=4.896882057189941, loss=0.6791548728942871
I0307 10:16:50.112252 139951089751872 spec.py:321] Evaluating on the training split.
I0307 10:16:56.129848 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 10:17:04.772914 139951089751872 spec.py:349] Evaluating on the test split.
I0307 10:17:07.050001 139951089751872 submission_runner.py:411] Time since start: 127830.26s, 	Step: 366972, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14816734194755554, 'validation/accuracy': 0.7574399709701538, 'validation/loss': 1.0411081314086914, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8231074810028076, 'test/num_examples': 10000, 'score': 123482.26363253593, 'total_duration': 127830.26229286194, 'accumulated_submission_time': 123482.26363253593, 'accumulated_eval_time': 4316.926568746567, 'accumulated_logging_time': 17.68900442123413}
I0307 10:17:07.135206 139787612706560 logging_writer.py:48] [366972] accumulated_eval_time=4316.926569, accumulated_logging_time=17.689004, accumulated_submission_time=123482.263633, global_step=366972, preemption_count=0, score=123482.263633, test/accuracy=0.631300, test/loss=1.823107, test/num_examples=10000, total_duration=127830.262293, train/accuracy=0.960758, train/loss=0.148167, validation/accuracy=0.757440, validation/loss=1.041108, validation/num_examples=50000
I0307 10:17:16.866423 139787621099264 logging_writer.py:48] [367000] global_step=367000, grad_norm=4.31545877456665, loss=0.6594576835632324
I0307 10:17:50.487652 139787612706560 logging_writer.py:48] [367100] global_step=367100, grad_norm=4.8079142570495605, loss=0.6786590814590454
I0307 10:18:24.121209 139787621099264 logging_writer.py:48] [367200] global_step=367200, grad_norm=4.374332904815674, loss=0.6156193017959595
I0307 10:18:57.790964 139787612706560 logging_writer.py:48] [367300] global_step=367300, grad_norm=4.1997389793396, loss=0.5345040559768677
I0307 10:19:31.454519 139787621099264 logging_writer.py:48] [367400] global_step=367400, grad_norm=4.397868633270264, loss=0.6326462030410767
I0307 10:20:05.108607 139787612706560 logging_writer.py:48] [367500] global_step=367500, grad_norm=4.439260005950928, loss=0.6261337399482727
I0307 10:20:38.790818 139787621099264 logging_writer.py:48] [367600] global_step=367600, grad_norm=4.034327030181885, loss=0.5434366464614868
I0307 10:21:12.406628 139787612706560 logging_writer.py:48] [367700] global_step=367700, grad_norm=5.008209705352783, loss=0.5999255776405334
I0307 10:21:46.072060 139787621099264 logging_writer.py:48] [367800] global_step=367800, grad_norm=4.131171703338623, loss=0.617794394493103
I0307 10:22:19.694818 139787612706560 logging_writer.py:48] [367900] global_step=367900, grad_norm=4.4949469566345215, loss=0.6424341797828674
I0307 10:22:53.348827 139787621099264 logging_writer.py:48] [368000] global_step=368000, grad_norm=4.665959358215332, loss=0.624182939529419
I0307 10:23:26.958126 139787612706560 logging_writer.py:48] [368100] global_step=368100, grad_norm=4.493064880371094, loss=0.6005312204360962
I0307 10:24:00.620787 139787621099264 logging_writer.py:48] [368200] global_step=368200, grad_norm=4.793782711029053, loss=0.7043300867080688
I0307 10:24:34.281872 139787612706560 logging_writer.py:48] [368300] global_step=368300, grad_norm=4.443399906158447, loss=0.5925737619400024
I0307 10:25:07.955662 139787621099264 logging_writer.py:48] [368400] global_step=368400, grad_norm=4.4111409187316895, loss=0.6507754325866699
I0307 10:25:37.356797 139951089751872 spec.py:321] Evaluating on the training split.
I0307 10:25:43.348067 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 10:25:52.087056 139951089751872 spec.py:349] Evaluating on the test split.
I0307 10:25:54.336566 139951089751872 submission_runner.py:411] Time since start: 128357.55s, 	Step: 368489, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14713416993618011, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 1.0416234731674194, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8244590759277344, 'test/num_examples': 10000, 'score': 123992.4189324379, 'total_duration': 128357.54884719849, 'accumulated_submission_time': 123992.4189324379, 'accumulated_eval_time': 4333.9062695503235, 'accumulated_logging_time': 17.783448457717896}
I0307 10:25:54.412626 139789030385408 logging_writer.py:48] [368489] accumulated_eval_time=4333.906270, accumulated_logging_time=17.783448, accumulated_submission_time=123992.418932, global_step=368489, preemption_count=0, score=123992.418932, test/accuracy=0.631600, test/loss=1.824459, test/num_examples=10000, total_duration=128357.548847, train/accuracy=0.960698, train/loss=0.147134, validation/accuracy=0.757400, validation/loss=1.041623, validation/num_examples=50000
I0307 10:25:58.450268 139789038778112 logging_writer.py:48] [368500] global_step=368500, grad_norm=4.858392715454102, loss=0.6086286306381226
I0307 10:26:32.037972 139789030385408 logging_writer.py:48] [368600] global_step=368600, grad_norm=4.090558052062988, loss=0.6138004064559937
I0307 10:27:05.648537 139789038778112 logging_writer.py:48] [368700] global_step=368700, grad_norm=4.401532173156738, loss=0.5630846619606018
I0307 10:27:39.351306 139789030385408 logging_writer.py:48] [368800] global_step=368800, grad_norm=4.243534088134766, loss=0.5874439477920532
I0307 10:28:13.006897 139789038778112 logging_writer.py:48] [368900] global_step=368900, grad_norm=4.483774662017822, loss=0.628018319606781
I0307 10:28:46.648157 139789030385408 logging_writer.py:48] [369000] global_step=369000, grad_norm=3.94159197807312, loss=0.5888831615447998
I0307 10:29:20.304352 139789038778112 logging_writer.py:48] [369100] global_step=369100, grad_norm=4.098335266113281, loss=0.5941891074180603
I0307 10:29:53.930693 139789030385408 logging_writer.py:48] [369200] global_step=369200, grad_norm=4.8635172843933105, loss=0.6235223412513733
I0307 10:30:27.564313 139789038778112 logging_writer.py:48] [369300] global_step=369300, grad_norm=4.644631862640381, loss=0.6480631232261658
I0307 10:31:01.223680 139789030385408 logging_writer.py:48] [369400] global_step=369400, grad_norm=4.480321884155273, loss=0.602881669998169
I0307 10:31:34.880399 139789038778112 logging_writer.py:48] [369500] global_step=369500, grad_norm=4.316126346588135, loss=0.6403157114982605
I0307 10:32:08.503508 139789030385408 logging_writer.py:48] [369600] global_step=369600, grad_norm=4.325293064117432, loss=0.6133744120597839
I0307 10:32:42.147897 139789038778112 logging_writer.py:48] [369700] global_step=369700, grad_norm=4.044985771179199, loss=0.5369191765785217
I0307 10:33:15.771847 139789030385408 logging_writer.py:48] [369800] global_step=369800, grad_norm=4.590214729309082, loss=0.6543213129043579
I0307 10:33:49.432105 139789038778112 logging_writer.py:48] [369900] global_step=369900, grad_norm=4.395626544952393, loss=0.6908363103866577
I0307 10:34:23.041628 139789030385408 logging_writer.py:48] [370000] global_step=370000, grad_norm=4.317553997039795, loss=0.5838168859481812
I0307 10:34:24.530359 139951089751872 spec.py:321] Evaluating on the training split.
I0307 10:34:30.599174 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 10:34:39.332880 139951089751872 spec.py:349] Evaluating on the test split.
I0307 10:34:41.659902 139951089751872 submission_runner.py:411] Time since start: 128884.87s, 	Step: 370006, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.148869127035141, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0409268140792847, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8226678371429443, 'test/num_examples': 10000, 'score': 124502.4652018547, 'total_duration': 128884.87219500542, 'accumulated_submission_time': 124502.4652018547, 'accumulated_eval_time': 4351.035741329193, 'accumulated_logging_time': 17.875396251678467}
I0307 10:34:41.736913 139789005207296 logging_writer.py:48] [370006] accumulated_eval_time=4351.035741, accumulated_logging_time=17.875396, accumulated_submission_time=124502.465202, global_step=370006, preemption_count=0, score=124502.465202, test/accuracy=0.632100, test/loss=1.822668, test/num_examples=10000, total_duration=128884.872195, train/accuracy=0.960360, train/loss=0.148869, validation/accuracy=0.757040, validation/loss=1.040927, validation/num_examples=50000
I0307 10:35:13.630373 139789013600000 logging_writer.py:48] [370100] global_step=370100, grad_norm=4.436882495880127, loss=0.6265785694122314
I0307 10:35:47.239941 139789005207296 logging_writer.py:48] [370200] global_step=370200, grad_norm=4.542627334594727, loss=0.5876688957214355
I0307 10:36:20.904635 139789013600000 logging_writer.py:48] [370300] global_step=370300, grad_norm=4.657954216003418, loss=0.6728894710540771
I0307 10:36:54.532264 139789005207296 logging_writer.py:48] [370400] global_step=370400, grad_norm=4.325107574462891, loss=0.6068217158317566
I0307 10:37:28.194263 139789013600000 logging_writer.py:48] [370500] global_step=370500, grad_norm=5.113296031951904, loss=0.6745417714118958
I0307 10:38:01.832456 139789005207296 logging_writer.py:48] [370600] global_step=370600, grad_norm=4.457405090332031, loss=0.6269132494926453
I0307 10:38:35.512066 139789013600000 logging_writer.py:48] [370700] global_step=370700, grad_norm=4.249317646026611, loss=0.5756612420082092
I0307 10:39:09.128545 139789005207296 logging_writer.py:48] [370800] global_step=370800, grad_norm=4.308389186859131, loss=0.6337406635284424
I0307 10:39:42.754518 139789013600000 logging_writer.py:48] [370900] global_step=370900, grad_norm=4.525269508361816, loss=0.6760157942771912
I0307 10:40:16.526107 139789005207296 logging_writer.py:48] [371000] global_step=371000, grad_norm=4.294860363006592, loss=0.5561825037002563
I0307 10:40:50.180165 139789013600000 logging_writer.py:48] [371100] global_step=371100, grad_norm=4.410248756408691, loss=0.6127818822860718
I0307 10:41:23.821181 139789005207296 logging_writer.py:48] [371200] global_step=371200, grad_norm=4.1177978515625, loss=0.5337275266647339
I0307 10:41:57.436410 139789013600000 logging_writer.py:48] [371300] global_step=371300, grad_norm=5.037224769592285, loss=0.7136240005493164
I0307 10:42:31.036712 139789005207296 logging_writer.py:48] [371400] global_step=371400, grad_norm=4.361634731292725, loss=0.630958616733551
I0307 10:43:04.687825 139789013600000 logging_writer.py:48] [371500] global_step=371500, grad_norm=4.373740196228027, loss=0.5872675776481628
I0307 10:43:11.885014 139951089751872 spec.py:321] Evaluating on the training split.
I0307 10:43:17.866363 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 10:43:26.503322 139951089751872 spec.py:349] Evaluating on the test split.
I0307 10:43:28.751209 139951089751872 submission_runner.py:411] Time since start: 129411.96s, 	Step: 371523, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.1471908539533615, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.0407812595367432, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8242459297180176, 'test/num_examples': 10000, 'score': 125012.54879522324, 'total_duration': 129411.96352028847, 'accumulated_submission_time': 125012.54879522324, 'accumulated_eval_time': 4367.901885032654, 'accumulated_logging_time': 17.96213459968567}
I0307 10:43:28.835698 139787629491968 logging_writer.py:48] [371523] accumulated_eval_time=4367.901885, accumulated_logging_time=17.962135, accumulated_submission_time=125012.548795, global_step=371523, preemption_count=0, score=125012.548795, test/accuracy=0.631400, test/loss=1.824246, test/num_examples=10000, total_duration=129411.963520, train/accuracy=0.960041, train/loss=0.147191, validation/accuracy=0.757140, validation/loss=1.040781, validation/num_examples=50000
I0307 10:43:55.100712 139788996814592 logging_writer.py:48] [371600] global_step=371600, grad_norm=4.455299377441406, loss=0.6224985718727112
I0307 10:44:28.751273 139787629491968 logging_writer.py:48] [371700] global_step=371700, grad_norm=4.486221790313721, loss=0.6642471551895142
I0307 10:45:02.414665 139788996814592 logging_writer.py:48] [371800] global_step=371800, grad_norm=4.315465450286865, loss=0.622204065322876
I0307 10:45:36.063083 139787629491968 logging_writer.py:48] [371900] global_step=371900, grad_norm=4.082354545593262, loss=0.565862238407135
I0307 10:46:09.811719 139788996814592 logging_writer.py:48] [372000] global_step=372000, grad_norm=4.212495803833008, loss=0.6078763008117676
I0307 10:46:43.477467 139787629491968 logging_writer.py:48] [372100] global_step=372100, grad_norm=5.508383274078369, loss=0.6840428113937378
I0307 10:47:17.162260 139788996814592 logging_writer.py:48] [372200] global_step=372200, grad_norm=4.718060493469238, loss=0.5868958830833435
I0307 10:47:50.789177 139787629491968 logging_writer.py:48] [372300] global_step=372300, grad_norm=4.144439220428467, loss=0.621369481086731
I0307 10:48:24.448649 139788996814592 logging_writer.py:48] [372400] global_step=372400, grad_norm=4.145800590515137, loss=0.5642090439796448
I0307 10:48:58.067512 139787629491968 logging_writer.py:48] [372500] global_step=372500, grad_norm=4.669958591461182, loss=0.6062724590301514
I0307 10:49:31.725640 139788996814592 logging_writer.py:48] [372600] global_step=372600, grad_norm=4.461186408996582, loss=0.5991449952125549
I0307 10:50:05.334033 139787629491968 logging_writer.py:48] [372700] global_step=372700, grad_norm=4.13871431350708, loss=0.5135239362716675
I0307 10:50:38.997656 139788996814592 logging_writer.py:48] [372800] global_step=372800, grad_norm=4.566201210021973, loss=0.5803031921386719
I0307 10:51:12.648048 139787629491968 logging_writer.py:48] [372900] global_step=372900, grad_norm=4.297752380371094, loss=0.6682430505752563
I0307 10:51:46.305376 139788996814592 logging_writer.py:48] [373000] global_step=373000, grad_norm=5.791295051574707, loss=0.7223253846168518
I0307 10:51:58.954833 139951089751872 spec.py:321] Evaluating on the training split.
I0307 10:52:05.143555 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 10:52:13.771728 139951089751872 spec.py:349] Evaluating on the test split.
I0307 10:52:16.035563 139951089751872 submission_runner.py:411] Time since start: 129939.25s, 	Step: 373039, 	{'train/accuracy': 0.9602399468421936, 'train/loss': 0.14671608805656433, 'validation/accuracy': 0.7579599618911743, 'validation/loss': 1.0410587787628174, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8227859735488892, 'test/num_examples': 10000, 'score': 125522.60182905197, 'total_duration': 129939.24783706665, 'accumulated_submission_time': 125522.60182905197, 'accumulated_eval_time': 4384.982523202896, 'accumulated_logging_time': 18.056538581848145}
I0307 10:52:16.122733 139787612706560 logging_writer.py:48] [373039] accumulated_eval_time=4384.982523, accumulated_logging_time=18.056539, accumulated_submission_time=125522.601829, global_step=373039, preemption_count=0, score=125522.601829, test/accuracy=0.631200, test/loss=1.822786, test/num_examples=10000, total_duration=129939.247837, train/accuracy=0.960240, train/loss=0.146716, validation/accuracy=0.757960, validation/loss=1.041059, validation/num_examples=50000
I0307 10:52:36.964285 139787621099264 logging_writer.py:48] [373100] global_step=373100, grad_norm=4.586470603942871, loss=0.6651977300643921
I0307 10:53:10.534096 139787612706560 logging_writer.py:48] [373200] global_step=373200, grad_norm=4.572772979736328, loss=0.6504468321800232
I0307 10:53:44.131453 139787621099264 logging_writer.py:48] [373300] global_step=373300, grad_norm=4.2448954582214355, loss=0.610650360584259
I0307 10:54:17.805033 139787612706560 logging_writer.py:48] [373400] global_step=373400, grad_norm=3.97747802734375, loss=0.5589908361434937
I0307 10:54:51.416936 139787621099264 logging_writer.py:48] [373500] global_step=373500, grad_norm=4.39349889755249, loss=0.5686324834823608
I0307 10:55:25.066283 139787612706560 logging_writer.py:48] [373600] global_step=373600, grad_norm=4.9349470138549805, loss=0.6006698608398438
I0307 10:55:58.677544 139787621099264 logging_writer.py:48] [373700] global_step=373700, grad_norm=4.585904121398926, loss=0.6763008832931519
I0307 10:56:32.292715 139787612706560 logging_writer.py:48] [373800] global_step=373800, grad_norm=4.756959915161133, loss=0.6330194473266602
I0307 10:57:05.906715 139787621099264 logging_writer.py:48] [373900] global_step=373900, grad_norm=5.061185359954834, loss=0.6734755039215088
I0307 10:57:39.537035 139787612706560 logging_writer.py:48] [374000] global_step=374000, grad_norm=4.434482574462891, loss=0.6309590339660645
I0307 10:58:13.197977 139787621099264 logging_writer.py:48] [374100] global_step=374100, grad_norm=4.336832523345947, loss=0.5903427004814148
I0307 10:58:46.809011 139787612706560 logging_writer.py:48] [374200] global_step=374200, grad_norm=4.485503673553467, loss=0.6394607424736023
I0307 10:59:20.434157 139787621099264 logging_writer.py:48] [374300] global_step=374300, grad_norm=4.440217971801758, loss=0.6873615980148315
I0307 10:59:54.090085 139787612706560 logging_writer.py:48] [374400] global_step=374400, grad_norm=4.306941986083984, loss=0.600454568862915
I0307 11:00:27.762043 139787621099264 logging_writer.py:48] [374500] global_step=374500, grad_norm=4.488382816314697, loss=0.6621392965316772
I0307 11:00:46.081686 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:00:52.069709 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:01:00.635038 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:01:02.913383 139951089751872 submission_runner.py:411] Time since start: 130466.13s, 	Step: 374556, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14545674622058868, 'validation/accuracy': 0.7567200064659119, 'validation/loss': 1.041068434715271, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.823155164718628, 'test/num_examples': 10000, 'score': 126032.49347376823, 'total_duration': 130466.12568640709, 'accumulated_submission_time': 126032.49347376823, 'accumulated_eval_time': 4401.814165115356, 'accumulated_logging_time': 18.15566897392273}
I0307 11:01:02.991987 139789013600000 logging_writer.py:48] [374556] accumulated_eval_time=4401.814165, accumulated_logging_time=18.155669, accumulated_submission_time=126032.493474, global_step=374556, preemption_count=0, score=126032.493474, test/accuracy=0.631500, test/loss=1.823155, test/num_examples=10000, total_duration=130466.125686, train/accuracy=0.960200, train/loss=0.145457, validation/accuracy=0.756720, validation/loss=1.041068, validation/num_examples=50000
I0307 11:01:18.163954 139789021992704 logging_writer.py:48] [374600] global_step=374600, grad_norm=4.599256992340088, loss=0.6216437220573425
I0307 11:01:51.861438 139789013600000 logging_writer.py:48] [374700] global_step=374700, grad_norm=4.402191162109375, loss=0.6137878894805908
I0307 11:02:25.494523 139789021992704 logging_writer.py:48] [374800] global_step=374800, grad_norm=4.579094409942627, loss=0.7270210385322571
I0307 11:02:59.167538 139789013600000 logging_writer.py:48] [374900] global_step=374900, grad_norm=4.681442737579346, loss=0.6851376295089722
I0307 11:03:32.777588 139789021992704 logging_writer.py:48] [375000] global_step=375000, grad_norm=4.153454780578613, loss=0.5627458095550537
I0307 11:04:06.439331 139789013600000 logging_writer.py:48] [375100] global_step=375100, grad_norm=4.63120174407959, loss=0.6623045206069946
I0307 11:04:40.089365 139789021992704 logging_writer.py:48] [375200] global_step=375200, grad_norm=4.4975690841674805, loss=0.6389957070350647
I0307 11:05:13.732003 139789013600000 logging_writer.py:48] [375300] global_step=375300, grad_norm=4.390028476715088, loss=0.5681355595588684
I0307 11:05:47.354510 139789021992704 logging_writer.py:48] [375400] global_step=375400, grad_norm=4.2172112464904785, loss=0.6243377923965454
I0307 11:06:21.015259 139789013600000 logging_writer.py:48] [375500] global_step=375500, grad_norm=4.682629108428955, loss=0.5666088461875916
I0307 11:06:54.659411 139789021992704 logging_writer.py:48] [375600] global_step=375600, grad_norm=4.829431056976318, loss=0.682664692401886
I0307 11:07:28.313057 139789013600000 logging_writer.py:48] [375700] global_step=375700, grad_norm=4.416031837463379, loss=0.6210717558860779
I0307 11:08:01.919158 139789021992704 logging_writer.py:48] [375800] global_step=375800, grad_norm=4.576770782470703, loss=0.673954427242279
I0307 11:08:35.576236 139789013600000 logging_writer.py:48] [375900] global_step=375900, grad_norm=4.864072322845459, loss=0.6640359163284302
I0307 11:09:09.183183 139789021992704 logging_writer.py:48] [376000] global_step=376000, grad_norm=4.307876110076904, loss=0.6078018546104431
I0307 11:09:33.211845 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:09:39.268781 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:09:47.953630 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:09:50.206939 139951089751872 submission_runner.py:411] Time since start: 130993.42s, 	Step: 376073, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.14798428118228912, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0399235486984253, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8223358392715454, 'test/num_examples': 10000, 'score': 126542.64698553085, 'total_duration': 130993.41924715042, 'accumulated_submission_time': 126542.64698553085, 'accumulated_eval_time': 4418.809207677841, 'accumulated_logging_time': 18.24462342262268}
I0307 11:09:50.287689 139787621099264 logging_writer.py:48] [376073] accumulated_eval_time=4418.809208, accumulated_logging_time=18.244623, accumulated_submission_time=126542.646986, global_step=376073, preemption_count=0, score=126542.646986, test/accuracy=0.632100, test/loss=1.822336, test/num_examples=10000, total_duration=130993.419247, train/accuracy=0.959762, train/loss=0.147984, validation/accuracy=0.757200, validation/loss=1.039924, validation/num_examples=50000
I0307 11:09:59.689505 139787629491968 logging_writer.py:48] [376100] global_step=376100, grad_norm=4.450520992279053, loss=0.7095996737480164
I0307 11:10:33.345452 139787621099264 logging_writer.py:48] [376200] global_step=376200, grad_norm=4.48766565322876, loss=0.594704270362854
I0307 11:11:06.918645 139787629491968 logging_writer.py:48] [376300] global_step=376300, grad_norm=4.245385646820068, loss=0.5869652032852173
I0307 11:11:40.499279 139787621099264 logging_writer.py:48] [376400] global_step=376400, grad_norm=4.395947456359863, loss=0.6006209254264832
I0307 11:12:14.078444 139787629491968 logging_writer.py:48] [376500] global_step=376500, grad_norm=4.858429431915283, loss=0.6775360703468323
I0307 11:12:47.674098 139787621099264 logging_writer.py:48] [376600] global_step=376600, grad_norm=4.838940143585205, loss=0.6330671310424805
I0307 11:13:21.221718 139787629491968 logging_writer.py:48] [376700] global_step=376700, grad_norm=4.534322261810303, loss=0.7023634314537048
I0307 11:13:54.780652 139787621099264 logging_writer.py:48] [376800] global_step=376800, grad_norm=4.8926801681518555, loss=0.6132184863090515
I0307 11:14:28.394043 139787629491968 logging_writer.py:48] [376900] global_step=376900, grad_norm=4.518590450286865, loss=0.6334640979766846
I0307 11:15:02.012299 139787621099264 logging_writer.py:48] [377000] global_step=377000, grad_norm=4.87913703918457, loss=0.636056661605835
I0307 11:15:35.679270 139787629491968 logging_writer.py:48] [377100] global_step=377100, grad_norm=4.383447170257568, loss=0.6479551196098328
I0307 11:16:09.343985 139787621099264 logging_writer.py:48] [377200] global_step=377200, grad_norm=4.879714488983154, loss=0.6512875556945801
I0307 11:16:43.016440 139787629491968 logging_writer.py:48] [377300] global_step=377300, grad_norm=4.2838921546936035, loss=0.6888142228126526
I0307 11:17:16.609874 139787621099264 logging_writer.py:48] [377400] global_step=377400, grad_norm=5.470413684844971, loss=0.6349887251853943
I0307 11:17:50.258162 139787629491968 logging_writer.py:48] [377500] global_step=377500, grad_norm=4.166794776916504, loss=0.6114903092384338
I0307 11:18:20.318624 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:18:26.427551 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:18:34.909926 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:18:37.212725 139951089751872 submission_runner.py:411] Time since start: 131520.43s, 	Step: 377591, 	{'train/accuracy': 0.9620535373687744, 'train/loss': 0.14559465646743774, 'validation/accuracy': 0.7574799656867981, 'validation/loss': 1.0399705171585083, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8216814994812012, 'test/num_examples': 10000, 'score': 127052.61329627037, 'total_duration': 131520.42503738403, 'accumulated_submission_time': 127052.61329627037, 'accumulated_eval_time': 4435.703264951706, 'accumulated_logging_time': 18.334900856018066}
I0307 11:18:37.290135 139789030385408 logging_writer.py:48] [377591] accumulated_eval_time=4435.703265, accumulated_logging_time=18.334901, accumulated_submission_time=127052.613296, global_step=377591, preemption_count=0, score=127052.613296, test/accuracy=0.631600, test/loss=1.821681, test/num_examples=10000, total_duration=131520.425037, train/accuracy=0.962054, train/loss=0.145595, validation/accuracy=0.757480, validation/loss=1.039971, validation/num_examples=50000
I0307 11:18:40.647694 139789038778112 logging_writer.py:48] [377600] global_step=377600, grad_norm=4.332989692687988, loss=0.5466653108596802
I0307 11:19:14.314057 139789030385408 logging_writer.py:48] [377700] global_step=377700, grad_norm=4.502145767211914, loss=0.6287032961845398
I0307 11:19:48.009215 139789038778112 logging_writer.py:48] [377800] global_step=377800, grad_norm=4.533227443695068, loss=0.6167933344841003
I0307 11:20:21.618031 139789030385408 logging_writer.py:48] [377900] global_step=377900, grad_norm=4.571978569030762, loss=0.6548628211021423
I0307 11:20:55.514829 139789038778112 logging_writer.py:48] [378000] global_step=378000, grad_norm=4.380724906921387, loss=0.5975161790847778
I0307 11:21:29.196190 139789030385408 logging_writer.py:48] [378100] global_step=378100, grad_norm=4.0713725090026855, loss=0.6241452097892761
I0307 11:22:02.895726 139789038778112 logging_writer.py:48] [378200] global_step=378200, grad_norm=4.825469970703125, loss=0.6076388359069824
I0307 11:22:36.585447 139789030385408 logging_writer.py:48] [378300] global_step=378300, grad_norm=4.81436824798584, loss=0.6468165516853333
I0307 11:23:10.220322 139789038778112 logging_writer.py:48] [378400] global_step=378400, grad_norm=4.519436836242676, loss=0.6587870717048645
I0307 11:23:43.829299 139789030385408 logging_writer.py:48] [378500] global_step=378500, grad_norm=4.718848705291748, loss=0.6586993932723999
I0307 11:24:17.490936 139789038778112 logging_writer.py:48] [378600] global_step=378600, grad_norm=4.1275787353515625, loss=0.5924249291419983
I0307 11:24:51.115711 139789030385408 logging_writer.py:48] [378700] global_step=378700, grad_norm=4.234570503234863, loss=0.6036353707313538
I0307 11:25:24.779204 139789038778112 logging_writer.py:48] [378800] global_step=378800, grad_norm=4.6379475593566895, loss=0.6324206590652466
I0307 11:25:58.415461 139789030385408 logging_writer.py:48] [378900] global_step=378900, grad_norm=4.46609354019165, loss=0.6410571932792664
I0307 11:26:32.112874 139789038778112 logging_writer.py:48] [379000] global_step=379000, grad_norm=4.539422988891602, loss=0.6986047029495239
I0307 11:27:05.763663 139789030385408 logging_writer.py:48] [379100] global_step=379100, grad_norm=4.395674705505371, loss=0.6065574884414673
I0307 11:27:07.254564 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:27:13.294251 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:27:22.015117 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:27:24.226111 139951089751872 submission_runner.py:411] Time since start: 132047.44s, 	Step: 379106, 	{'train/accuracy': 0.9601004123687744, 'train/loss': 0.1481819748878479, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.0412617921829224, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.822556495666504, 'test/num_examples': 10000, 'score': 127562.5142903328, 'total_duration': 132047.43829274178, 'accumulated_submission_time': 127562.5142903328, 'accumulated_eval_time': 4452.67462849617, 'accumulated_logging_time': 18.42171573638916}
I0307 11:27:24.308833 139789005207296 logging_writer.py:48] [379106] accumulated_eval_time=4452.674628, accumulated_logging_time=18.421716, accumulated_submission_time=127562.514290, global_step=379106, preemption_count=0, score=127562.514290, test/accuracy=0.631300, test/loss=1.822556, test/num_examples=10000, total_duration=132047.438293, train/accuracy=0.960100, train/loss=0.148182, validation/accuracy=0.756880, validation/loss=1.041262, validation/num_examples=50000
I0307 11:27:56.219320 139789013600000 logging_writer.py:48] [379200] global_step=379200, grad_norm=4.5228681564331055, loss=0.6294647455215454
I0307 11:28:29.837215 139789005207296 logging_writer.py:48] [379300] global_step=379300, grad_norm=4.48588752746582, loss=0.7064362168312073
I0307 11:29:03.505241 139789013600000 logging_writer.py:48] [379400] global_step=379400, grad_norm=4.408084869384766, loss=0.6237644553184509
I0307 11:29:37.133964 139789005207296 logging_writer.py:48] [379500] global_step=379500, grad_norm=4.3912739753723145, loss=0.6800669431686401
I0307 11:30:10.779423 139789013600000 logging_writer.py:48] [379600] global_step=379600, grad_norm=4.836028575897217, loss=0.6984002590179443
I0307 11:30:44.385418 139789005207296 logging_writer.py:48] [379700] global_step=379700, grad_norm=4.293373107910156, loss=0.6162896156311035
I0307 11:31:18.026853 139789013600000 logging_writer.py:48] [379800] global_step=379800, grad_norm=4.540032386779785, loss=0.643208384513855
I0307 11:31:51.661272 139789005207296 logging_writer.py:48] [379900] global_step=379900, grad_norm=4.832266807556152, loss=0.6481721997261047
I0307 11:32:25.293077 139789013600000 logging_writer.py:48] [380000] global_step=380000, grad_norm=4.176577091217041, loss=0.6144921779632568
I0307 11:32:58.925771 139789005207296 logging_writer.py:48] [380100] global_step=380100, grad_norm=4.398436546325684, loss=0.6271700263023376
I0307 11:33:32.568007 139789013600000 logging_writer.py:48] [380200] global_step=380200, grad_norm=4.563937664031982, loss=0.621751606464386
I0307 11:34:06.194214 139789005207296 logging_writer.py:48] [380300] global_step=380300, grad_norm=4.530553340911865, loss=0.6566668748855591
I0307 11:34:39.837866 139789013600000 logging_writer.py:48] [380400] global_step=380400, grad_norm=4.815236568450928, loss=0.5695656538009644
I0307 11:35:13.500971 139789005207296 logging_writer.py:48] [380500] global_step=380500, grad_norm=4.604548931121826, loss=0.6416839957237244
I0307 11:35:47.133569 139789013600000 logging_writer.py:48] [380600] global_step=380600, grad_norm=4.438776969909668, loss=0.6519507765769958
I0307 11:35:54.363310 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:36:00.376566 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:36:08.934261 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:36:11.223977 139951089751872 submission_runner.py:411] Time since start: 132574.44s, 	Step: 380623, 	{'train/accuracy': 0.9607580900192261, 'train/loss': 0.14613230526447296, 'validation/accuracy': 0.7569599747657776, 'validation/loss': 1.0406873226165771, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8233776092529297, 'test/num_examples': 10000, 'score': 128072.50441098213, 'total_duration': 132574.43628549576, 'accumulated_submission_time': 128072.50441098213, 'accumulated_eval_time': 4469.5352375507355, 'accumulated_logging_time': 18.514261960983276}
I0307 11:36:11.303017 139787621099264 logging_writer.py:48] [380623] accumulated_eval_time=4469.535238, accumulated_logging_time=18.514262, accumulated_submission_time=128072.504411, global_step=380623, preemption_count=0, score=128072.504411, test/accuracy=0.630800, test/loss=1.823378, test/num_examples=10000, total_duration=132574.436285, train/accuracy=0.960758, train/loss=0.146132, validation/accuracy=0.756960, validation/loss=1.040687, validation/num_examples=50000
I0307 11:36:37.466162 139787629491968 logging_writer.py:48] [380700] global_step=380700, grad_norm=4.325506687164307, loss=0.6356169581413269
I0307 11:37:11.056243 139787621099264 logging_writer.py:48] [380800] global_step=380800, grad_norm=4.968963623046875, loss=0.6410956382751465
I0307 11:37:44.664375 139787629491968 logging_writer.py:48] [380900] global_step=380900, grad_norm=4.721634864807129, loss=0.6721432209014893
I0307 11:38:18.338569 139787621099264 logging_writer.py:48] [381000] global_step=381000, grad_norm=4.500946044921875, loss=0.670265257358551
I0307 11:38:51.973553 139787629491968 logging_writer.py:48] [381100] global_step=381100, grad_norm=4.080170154571533, loss=0.5703710317611694
I0307 11:39:25.651280 139787621099264 logging_writer.py:48] [381200] global_step=381200, grad_norm=4.437673568725586, loss=0.5982509851455688
I0307 11:39:59.332911 139787629491968 logging_writer.py:48] [381300] global_step=381300, grad_norm=4.690576076507568, loss=0.579174280166626
I0307 11:40:32.979557 139787621099264 logging_writer.py:48] [381400] global_step=381400, grad_norm=4.383262634277344, loss=0.6223623156547546
I0307 11:41:06.745147 139787629491968 logging_writer.py:48] [381500] global_step=381500, grad_norm=4.285507678985596, loss=0.5719536542892456
I0307 11:41:40.413443 139787621099264 logging_writer.py:48] [381600] global_step=381600, grad_norm=4.4540839195251465, loss=0.5991731286048889
I0307 11:42:14.097866 139787629491968 logging_writer.py:48] [381700] global_step=381700, grad_norm=4.154838562011719, loss=0.56108158826828
I0307 11:42:47.716665 139787621099264 logging_writer.py:48] [381800] global_step=381800, grad_norm=4.1731767654418945, loss=0.5736596584320068
I0307 11:43:21.367257 139787629491968 logging_writer.py:48] [381900] global_step=381900, grad_norm=4.2142157554626465, loss=0.5962017774581909
I0307 11:43:54.983143 139787621099264 logging_writer.py:48] [382000] global_step=382000, grad_norm=4.845616340637207, loss=0.627445638179779
I0307 11:44:28.646297 139787629491968 logging_writer.py:48] [382100] global_step=382100, grad_norm=5.257357120513916, loss=0.575015664100647
I0307 11:44:41.233443 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:44:47.285329 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:44:55.738627 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:44:57.993884 139951089751872 submission_runner.py:411] Time since start: 133101.21s, 	Step: 382139, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.1419612616300583, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.040238618850708, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8214529752731323, 'test/num_examples': 10000, 'score': 128582.37007761002, 'total_duration': 133101.20618534088, 'accumulated_submission_time': 128582.37007761002, 'accumulated_eval_time': 4486.295618534088, 'accumulated_logging_time': 18.603281497955322}
I0307 11:44:58.079097 139789030385408 logging_writer.py:48] [382139] accumulated_eval_time=4486.295619, accumulated_logging_time=18.603281, accumulated_submission_time=128582.370078, global_step=382139, preemption_count=0, score=128582.370078, test/accuracy=0.632400, test/loss=1.821453, test/num_examples=10000, total_duration=133101.206185, train/accuracy=0.961655, train/loss=0.141961, validation/accuracy=0.757340, validation/loss=1.040239, validation/num_examples=50000
I0307 11:45:18.916687 139789038778112 logging_writer.py:48] [382200] global_step=382200, grad_norm=5.003815650939941, loss=0.6740376949310303
I0307 11:45:52.515810 139789030385408 logging_writer.py:48] [382300] global_step=382300, grad_norm=4.761965274810791, loss=0.6858344078063965
I0307 11:46:26.141774 139789038778112 logging_writer.py:48] [382400] global_step=382400, grad_norm=4.689660549163818, loss=0.7034709453582764
I0307 11:46:59.889498 139789030385408 logging_writer.py:48] [382500] global_step=382500, grad_norm=4.090133190155029, loss=0.5853647589683533
I0307 11:47:33.553926 139789038778112 logging_writer.py:48] [382600] global_step=382600, grad_norm=4.92539119720459, loss=0.6405777931213379
I0307 11:48:07.221811 139789030385408 logging_writer.py:48] [382700] global_step=382700, grad_norm=5.071203708648682, loss=0.639723002910614
I0307 11:48:40.875905 139789038778112 logging_writer.py:48] [382800] global_step=382800, grad_norm=4.211239337921143, loss=0.6261403560638428
I0307 11:49:14.508540 139789030385408 logging_writer.py:48] [382900] global_step=382900, grad_norm=4.501565933227539, loss=0.7385886907577515
I0307 11:49:48.141534 139789038778112 logging_writer.py:48] [383000] global_step=383000, grad_norm=4.415719985961914, loss=0.5988622903823853
I0307 11:50:21.814187 139789030385408 logging_writer.py:48] [383100] global_step=383100, grad_norm=4.934779167175293, loss=0.6799033284187317
I0307 11:50:55.455906 139789038778112 logging_writer.py:48] [383200] global_step=383200, grad_norm=4.457272529602051, loss=0.6139264702796936
I0307 11:51:29.071510 139789030385408 logging_writer.py:48] [383300] global_step=383300, grad_norm=5.054582118988037, loss=0.6882022619247437
I0307 11:52:02.776761 139789038778112 logging_writer.py:48] [383400] global_step=383400, grad_norm=4.330043315887451, loss=0.6196372509002686
I0307 11:52:36.419265 139789030385408 logging_writer.py:48] [383500] global_step=383500, grad_norm=5.226133823394775, loss=0.6639895439147949
I0307 11:53:10.067142 139789038778112 logging_writer.py:48] [383600] global_step=383600, grad_norm=4.436588287353516, loss=0.5722883939743042
I0307 11:53:28.021026 139951089751872 spec.py:321] Evaluating on the training split.
I0307 11:53:34.137583 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 11:53:42.857757 139951089751872 spec.py:349] Evaluating on the test split.
I0307 11:53:45.175722 139951089751872 submission_runner.py:411] Time since start: 133628.39s, 	Step: 383655, 	{'train/accuracy': 0.9625119566917419, 'train/loss': 0.14235332608222961, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0416830778121948, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.824738621711731, 'test/num_examples': 10000, 'score': 129092.24616885185, 'total_duration': 133628.38803076744, 'accumulated_submission_time': 129092.24616885185, 'accumulated_eval_time': 4503.450261116028, 'accumulated_logging_time': 18.69795846939087}
I0307 11:53:45.258766 139788996814592 logging_writer.py:48] [383655] accumulated_eval_time=4503.450261, accumulated_logging_time=18.697958, accumulated_submission_time=129092.246169, global_step=383655, preemption_count=0, score=129092.246169, test/accuracy=0.632100, test/loss=1.824739, test/num_examples=10000, total_duration=133628.388031, train/accuracy=0.962512, train/loss=0.142353, validation/accuracy=0.757080, validation/loss=1.041683, validation/num_examples=50000
I0307 11:54:00.762373 139789005207296 logging_writer.py:48] [383700] global_step=383700, grad_norm=4.657458305358887, loss=0.657191812992096
I0307 11:54:34.367796 139788996814592 logging_writer.py:48] [383800] global_step=383800, grad_norm=4.398022174835205, loss=0.5907620787620544
I0307 11:55:07.950799 139789005207296 logging_writer.py:48] [383900] global_step=383900, grad_norm=4.698991298675537, loss=0.6781931519508362
I0307 11:55:41.508500 139788996814592 logging_writer.py:48] [384000] global_step=384000, grad_norm=4.72851037979126, loss=0.5945168137550354
I0307 11:56:15.100949 139789005207296 logging_writer.py:48] [384100] global_step=384100, grad_norm=4.6545305252075195, loss=0.6166149377822876
I0307 11:56:48.739939 139788996814592 logging_writer.py:48] [384200] global_step=384200, grad_norm=4.423248291015625, loss=0.7119054794311523
I0307 11:57:22.403133 139789005207296 logging_writer.py:48] [384300] global_step=384300, grad_norm=4.807227611541748, loss=0.7156792283058167
I0307 11:57:56.068819 139788996814592 logging_writer.py:48] [384400] global_step=384400, grad_norm=4.534689426422119, loss=0.6006014943122864
I0307 11:58:29.671052 139789005207296 logging_writer.py:48] [384500] global_step=384500, grad_norm=4.443143844604492, loss=0.632758378982544
I0307 11:59:03.396375 139788996814592 logging_writer.py:48] [384600] global_step=384600, grad_norm=4.944108486175537, loss=0.646028459072113
I0307 11:59:36.979195 139789005207296 logging_writer.py:48] [384700] global_step=384700, grad_norm=4.394857883453369, loss=0.6838817000389099
I0307 12:00:10.578923 139788996814592 logging_writer.py:48] [384800] global_step=384800, grad_norm=4.613987445831299, loss=0.657707691192627
I0307 12:00:44.248853 139789005207296 logging_writer.py:48] [384900] global_step=384900, grad_norm=4.585763454437256, loss=0.6260532140731812
I0307 12:01:17.930384 139788996814592 logging_writer.py:48] [385000] global_step=385000, grad_norm=4.6268768310546875, loss=0.697151243686676
I0307 12:01:51.587322 139789005207296 logging_writer.py:48] [385100] global_step=385100, grad_norm=4.427382469177246, loss=0.6399307250976562
I0307 12:02:15.304887 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:02:21.950354 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:02:30.480606 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:02:32.758099 139951089751872 submission_runner.py:411] Time since start: 134155.97s, 	Step: 385172, 	{'train/accuracy': 0.9605189561843872, 'train/loss': 0.14763645827770233, 'validation/accuracy': 0.757319986820221, 'validation/loss': 1.0413920879364014, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8220690488815308, 'test/num_examples': 10000, 'score': 129602.22697257996, 'total_duration': 134155.97039675713, 'accumulated_submission_time': 129602.22697257996, 'accumulated_eval_time': 4520.90341758728, 'accumulated_logging_time': 18.79100012779236}
I0307 12:02:32.838733 139789021992704 logging_writer.py:48] [385172] accumulated_eval_time=4520.903418, accumulated_logging_time=18.791000, accumulated_submission_time=129602.226973, global_step=385172, preemption_count=0, score=129602.226973, test/accuracy=0.632500, test/loss=1.822069, test/num_examples=10000, total_duration=134155.970397, train/accuracy=0.960519, train/loss=0.147636, validation/accuracy=0.757320, validation/loss=1.041392, validation/num_examples=50000
I0307 12:02:42.571458 139789030385408 logging_writer.py:48] [385200] global_step=385200, grad_norm=4.576812267303467, loss=0.6885207891464233
I0307 12:03:16.203457 139789021992704 logging_writer.py:48] [385300] global_step=385300, grad_norm=4.618584156036377, loss=0.6079850792884827
I0307 12:03:49.861085 139789030385408 logging_writer.py:48] [385400] global_step=385400, grad_norm=4.205235481262207, loss=0.5506881475448608
I0307 12:04:23.490472 139789021992704 logging_writer.py:48] [385500] global_step=385500, grad_norm=4.495969772338867, loss=0.6699459552764893
I0307 12:04:57.129233 139789030385408 logging_writer.py:48] [385600] global_step=385600, grad_norm=4.538527965545654, loss=0.5933621525764465
I0307 12:05:30.789021 139789021992704 logging_writer.py:48] [385700] global_step=385700, grad_norm=4.95980978012085, loss=0.6952774524688721
I0307 12:06:04.408833 139789030385408 logging_writer.py:48] [385800] global_step=385800, grad_norm=4.306955337524414, loss=0.6464413404464722
I0307 12:06:38.038096 139789021992704 logging_writer.py:48] [385900] global_step=385900, grad_norm=4.838422775268555, loss=0.6134355664253235
I0307 12:07:11.671236 139789030385408 logging_writer.py:48] [386000] global_step=386000, grad_norm=4.209697723388672, loss=0.6354067325592041
I0307 12:07:45.301037 139789021992704 logging_writer.py:48] [386100] global_step=386100, grad_norm=4.618303298950195, loss=0.6591534614562988
I0307 12:08:18.963989 139789030385408 logging_writer.py:48] [386200] global_step=386200, grad_norm=5.2663397789001465, loss=0.6396551728248596
I0307 12:08:52.591161 139789021992704 logging_writer.py:48] [386300] global_step=386300, grad_norm=4.143954277038574, loss=0.5874337553977966
I0307 12:09:26.210515 139789030385408 logging_writer.py:48] [386400] global_step=386400, grad_norm=4.323473930358887, loss=0.6381373405456543
I0307 12:09:59.847010 139789021992704 logging_writer.py:48] [386500] global_step=386500, grad_norm=4.6354498863220215, loss=0.6486160159111023
I0307 12:10:33.477642 139789030385408 logging_writer.py:48] [386600] global_step=386600, grad_norm=4.154407978057861, loss=0.6043595671653748
I0307 12:11:02.858855 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:11:08.859105 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:11:17.409304 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:11:19.677602 139951089751872 submission_runner.py:411] Time since start: 134682.89s, 	Step: 386689, 	{'train/accuracy': 0.9601402878761292, 'train/loss': 0.14751040935516357, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0403294563293457, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8210442066192627, 'test/num_examples': 10000, 'score': 130112.18139767647, 'total_duration': 134682.88991069794, 'accumulated_submission_time': 130112.18139767647, 'accumulated_eval_time': 4537.72211432457, 'accumulated_logging_time': 18.88157558441162}
I0307 12:11:19.757632 139787629491968 logging_writer.py:48] [386689] accumulated_eval_time=4537.722114, accumulated_logging_time=18.881576, accumulated_submission_time=130112.181398, global_step=386689, preemption_count=0, score=130112.181398, test/accuracy=0.631800, test/loss=1.821044, test/num_examples=10000, total_duration=134682.889911, train/accuracy=0.960140, train/loss=0.147510, validation/accuracy=0.757080, validation/loss=1.040329, validation/num_examples=50000
I0307 12:11:23.885573 139788996814592 logging_writer.py:48] [386700] global_step=386700, grad_norm=4.337417125701904, loss=0.5978972315788269
I0307 12:11:57.473087 139787629491968 logging_writer.py:48] [386800] global_step=386800, grad_norm=4.555254936218262, loss=0.6326978802680969
I0307 12:12:31.116914 139788996814592 logging_writer.py:48] [386900] global_step=386900, grad_norm=4.4852728843688965, loss=0.6136484146118164
I0307 12:13:04.756613 139787629491968 logging_writer.py:48] [387000] global_step=387000, grad_norm=4.624579429626465, loss=0.6184143424034119
I0307 12:13:38.437306 139788996814592 logging_writer.py:48] [387100] global_step=387100, grad_norm=4.5624542236328125, loss=0.6666014790534973
I0307 12:14:12.063445 139787629491968 logging_writer.py:48] [387200] global_step=387200, grad_norm=4.285933017730713, loss=0.6266689896583557
I0307 12:14:45.725119 139788996814592 logging_writer.py:48] [387300] global_step=387300, grad_norm=4.499691963195801, loss=0.5880515575408936
I0307 12:15:19.362489 139787629491968 logging_writer.py:48] [387400] global_step=387400, grad_norm=4.275045394897461, loss=0.6319996118545532
I0307 12:15:53.041309 139788996814592 logging_writer.py:48] [387500] global_step=387500, grad_norm=5.012201309204102, loss=0.623405396938324
I0307 12:16:26.664008 139787629491968 logging_writer.py:48] [387600] global_step=387600, grad_norm=4.788638114929199, loss=0.6221106648445129
I0307 12:17:00.307288 139788996814592 logging_writer.py:48] [387700] global_step=387700, grad_norm=3.8576369285583496, loss=0.5525976419448853
I0307 12:17:33.977971 139787629491968 logging_writer.py:48] [387800] global_step=387800, grad_norm=5.073252201080322, loss=0.659360945224762
I0307 12:18:07.562528 139788996814592 logging_writer.py:48] [387900] global_step=387900, grad_norm=5.03234338760376, loss=0.6368347406387329
I0307 12:18:41.185450 139787629491968 logging_writer.py:48] [388000] global_step=388000, grad_norm=4.307270526885986, loss=0.5616273283958435
I0307 12:19:14.754209 139788996814592 logging_writer.py:48] [388100] global_step=388100, grad_norm=4.281149387359619, loss=0.6217321157455444
I0307 12:19:48.347981 139787629491968 logging_writer.py:48] [388200] global_step=388200, grad_norm=4.058823585510254, loss=0.5581052899360657
I0307 12:19:49.834592 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:19:55.874294 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:20:04.585509 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:20:06.862843 139951089751872 submission_runner.py:411] Time since start: 135210.08s, 	Step: 388206, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14631153643131256, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 1.0419403314590454, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8237435817718506, 'test/num_examples': 10000, 'score': 130622.19213318825, 'total_duration': 135210.0751516819, 'accumulated_submission_time': 130622.19213318825, 'accumulated_eval_time': 4554.750309705734, 'accumulated_logging_time': 18.971620559692383}
I0307 12:20:06.947383 139787629491968 logging_writer.py:48] [388206] accumulated_eval_time=4554.750310, accumulated_logging_time=18.971621, accumulated_submission_time=130622.192133, global_step=388206, preemption_count=0, score=130622.192133, test/accuracy=0.631500, test/loss=1.823744, test/num_examples=10000, total_duration=135210.075152, train/accuracy=0.960639, train/loss=0.146312, validation/accuracy=0.757400, validation/loss=1.041940, validation/num_examples=50000
I0307 12:20:38.840060 139789021992704 logging_writer.py:48] [388300] global_step=388300, grad_norm=4.434401512145996, loss=0.6109204292297363
I0307 12:21:12.456781 139787629491968 logging_writer.py:48] [388400] global_step=388400, grad_norm=4.283226013183594, loss=0.5867630243301392
I0307 12:21:46.097935 139789021992704 logging_writer.py:48] [388500] global_step=388500, grad_norm=4.228407859802246, loss=0.6250954866409302
I0307 12:22:19.744092 139787629491968 logging_writer.py:48] [388600] global_step=388600, grad_norm=4.2622551918029785, loss=0.6226856708526611
I0307 12:22:53.419203 139789021992704 logging_writer.py:48] [388700] global_step=388700, grad_norm=4.16782808303833, loss=0.6000530123710632
I0307 12:23:27.061739 139787629491968 logging_writer.py:48] [388800] global_step=388800, grad_norm=4.207749843597412, loss=0.5544480085372925
I0307 12:24:00.697508 139789021992704 logging_writer.py:48] [388900] global_step=388900, grad_norm=4.0574798583984375, loss=0.547243595123291
I0307 12:24:34.296914 139787629491968 logging_writer.py:48] [389000] global_step=389000, grad_norm=4.477108478546143, loss=0.6585233211517334
I0307 12:25:07.912168 139789021992704 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.828597068786621, loss=0.6525290012359619
I0307 12:25:41.554910 139787629491968 logging_writer.py:48] [389200] global_step=389200, grad_norm=4.330092906951904, loss=0.6027755737304688
I0307 12:26:15.210159 139789021992704 logging_writer.py:48] [389300] global_step=389300, grad_norm=4.773013591766357, loss=0.6244598031044006
I0307 12:26:48.855369 139787629491968 logging_writer.py:48] [389400] global_step=389400, grad_norm=4.212429523468018, loss=0.6451758742332458
I0307 12:27:22.457255 139789021992704 logging_writer.py:48] [389500] global_step=389500, grad_norm=4.62609338760376, loss=0.6333562731742859
I0307 12:27:56.105175 139787629491968 logging_writer.py:48] [389600] global_step=389600, grad_norm=4.667762279510498, loss=0.6465578675270081
I0307 12:28:29.729693 139789021992704 logging_writer.py:48] [389700] global_step=389700, grad_norm=4.698163032531738, loss=0.6235130429267883
I0307 12:28:36.923771 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:28:43.085175 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:28:51.651485 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:28:53.942284 139951089751872 submission_runner.py:411] Time since start: 135737.15s, 	Step: 389723, 	{'train/accuracy': 0.9618542790412903, 'train/loss': 0.14340443909168243, 'validation/accuracy': 0.7575399875640869, 'validation/loss': 1.0409173965454102, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8231186866760254, 'test/num_examples': 10000, 'score': 131132.10186076164, 'total_duration': 135737.15456604958, 'accumulated_submission_time': 131132.10186076164, 'accumulated_eval_time': 4571.768753767014, 'accumulated_logging_time': 19.067416191101074}
I0307 12:28:54.024373 139787621099264 logging_writer.py:48] [389723] accumulated_eval_time=4571.768754, accumulated_logging_time=19.067416, accumulated_submission_time=131132.101861, global_step=389723, preemption_count=0, score=131132.101861, test/accuracy=0.631700, test/loss=1.823119, test/num_examples=10000, total_duration=135737.154566, train/accuracy=0.961854, train/loss=0.143404, validation/accuracy=0.757540, validation/loss=1.040917, validation/num_examples=50000
I0307 12:29:20.216409 139787629491968 logging_writer.py:48] [389800] global_step=389800, grad_norm=4.62871789932251, loss=0.6053752899169922
I0307 12:29:53.875582 139787621099264 logging_writer.py:48] [389900] global_step=389900, grad_norm=4.22418737411499, loss=0.602942943572998
I0307 12:30:27.564995 139787629491968 logging_writer.py:48] [390000] global_step=390000, grad_norm=5.014692783355713, loss=0.6385416984558105
I0307 12:31:01.223498 139787621099264 logging_writer.py:48] [390100] global_step=390100, grad_norm=4.27737283706665, loss=0.5521721243858337
I0307 12:31:34.902957 139787629491968 logging_writer.py:48] [390200] global_step=390200, grad_norm=4.161466598510742, loss=0.6084703207015991
I0307 12:32:08.520586 139787621099264 logging_writer.py:48] [390300] global_step=390300, grad_norm=5.019925594329834, loss=0.6495203971862793
I0307 12:32:42.186038 139787629491968 logging_writer.py:48] [390400] global_step=390400, grad_norm=4.506463050842285, loss=0.6733204126358032
I0307 12:33:15.842904 139787621099264 logging_writer.py:48] [390500] global_step=390500, grad_norm=5.012425422668457, loss=0.6360622048377991
I0307 12:33:49.509597 139787629491968 logging_writer.py:48] [390600] global_step=390600, grad_norm=4.597240924835205, loss=0.6081547737121582
I0307 12:34:23.134025 139787621099264 logging_writer.py:48] [390700] global_step=390700, grad_norm=4.7420878410339355, loss=0.6537263989448547
I0307 12:34:56.793672 139787629491968 logging_writer.py:48] [390800] global_step=390800, grad_norm=4.624127388000488, loss=0.663689374923706
I0307 12:35:30.423445 139787621099264 logging_writer.py:48] [390900] global_step=390900, grad_norm=4.127379417419434, loss=0.5932378768920898
I0307 12:36:04.100193 139787629491968 logging_writer.py:48] [391000] global_step=391000, grad_norm=4.849156856536865, loss=0.6229763031005859
I0307 12:36:37.694729 139787621099264 logging_writer.py:48] [391100] global_step=391100, grad_norm=4.068198204040527, loss=0.6382473707199097
I0307 12:37:11.371490 139787629491968 logging_writer.py:48] [391200] global_step=391200, grad_norm=4.4442138671875, loss=0.6940106749534607
I0307 12:37:24.280808 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:37:30.327465 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:37:38.990846 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:37:41.292468 139951089751872 submission_runner.py:411] Time since start: 136264.50s, 	Step: 391240, 	{'train/accuracy': 0.9617346525192261, 'train/loss': 0.14447654783725739, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0409315824508667, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8221389055252075, 'test/num_examples': 10000, 'score': 131642.28998470306, 'total_duration': 136264.5047211647, 'accumulated_submission_time': 131642.28998470306, 'accumulated_eval_time': 4588.780303239822, 'accumulated_logging_time': 19.16048574447632}
I0307 12:37:41.382827 139787612706560 logging_writer.py:48] [391240] accumulated_eval_time=4588.780303, accumulated_logging_time=19.160486, accumulated_submission_time=131642.289985, global_step=391240, preemption_count=0, score=131642.289985, test/accuracy=0.631700, test/loss=1.822139, test/num_examples=10000, total_duration=136264.504721, train/accuracy=0.961735, train/loss=0.144477, validation/accuracy=0.757080, validation/loss=1.040932, validation/num_examples=50000
I0307 12:38:01.926098 139787621099264 logging_writer.py:48] [391300] global_step=391300, grad_norm=5.262248516082764, loss=0.7206324338912964
I0307 12:38:35.588385 139787612706560 logging_writer.py:48] [391400] global_step=391400, grad_norm=5.586097240447998, loss=0.6824835538864136
I0307 12:39:09.229016 139787621099264 logging_writer.py:48] [391500] global_step=391500, grad_norm=4.617273807525635, loss=0.6641795039176941
I0307 12:39:42.887098 139787612706560 logging_writer.py:48] [391600] global_step=391600, grad_norm=4.1108503341674805, loss=0.5675002932548523
I0307 12:40:16.531826 139787621099264 logging_writer.py:48] [391700] global_step=391700, grad_norm=4.198641300201416, loss=0.5616381168365479
I0307 12:40:50.208687 139787612706560 logging_writer.py:48] [391800] global_step=391800, grad_norm=4.20908784866333, loss=0.5788671374320984
I0307 12:41:23.870943 139787621099264 logging_writer.py:48] [391900] global_step=391900, grad_norm=3.829939603805542, loss=0.5609328746795654
I0307 12:41:57.530227 139787612706560 logging_writer.py:48] [392000] global_step=392000, grad_norm=4.358203411102295, loss=0.6288400888442993
I0307 12:42:31.149716 139787621099264 logging_writer.py:48] [392100] global_step=392100, grad_norm=4.402907848358154, loss=0.6654306054115295
I0307 12:43:04.742942 139787612706560 logging_writer.py:48] [392200] global_step=392200, grad_norm=4.101365566253662, loss=0.5917491912841797
I0307 12:43:38.389834 139787621099264 logging_writer.py:48] [392300] global_step=392300, grad_norm=4.297952651977539, loss=0.6933436989784241
I0307 12:44:12.018602 139787612706560 logging_writer.py:48] [392400] global_step=392400, grad_norm=4.81311559677124, loss=0.6567995548248291
I0307 12:44:45.629209 139787621099264 logging_writer.py:48] [392500] global_step=392500, grad_norm=4.201690673828125, loss=0.5401214361190796
I0307 12:45:19.224210 139787612706560 logging_writer.py:48] [392600] global_step=392600, grad_norm=4.338184356689453, loss=0.6368640661239624
I0307 12:45:52.856584 139787621099264 logging_writer.py:48] [392700] global_step=392700, grad_norm=4.554815292358398, loss=0.6147004961967468
I0307 12:46:11.514572 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:46:17.550788 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:46:26.217737 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:46:28.485501 139951089751872 submission_runner.py:411] Time since start: 136791.70s, 	Step: 392757, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.1481156349182129, 'validation/accuracy': 0.7575199604034424, 'validation/loss': 1.040520191192627, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8216640949249268, 'test/num_examples': 10000, 'score': 132152.3566787243, 'total_duration': 136791.69770216942, 'accumulated_submission_time': 132152.3566787243, 'accumulated_eval_time': 4605.751079559326, 'accumulated_logging_time': 19.260599851608276}
I0307 12:46:28.568717 139789021992704 logging_writer.py:48] [392757] accumulated_eval_time=4605.751080, accumulated_logging_time=19.260600, accumulated_submission_time=132152.356679, global_step=392757, preemption_count=0, score=132152.356679, test/accuracy=0.632500, test/loss=1.821664, test/num_examples=10000, total_duration=136791.697702, train/accuracy=0.960559, train/loss=0.148116, validation/accuracy=0.757520, validation/loss=1.040520, validation/num_examples=50000
I0307 12:46:43.371287 139789030385408 logging_writer.py:48] [392800] global_step=392800, grad_norm=4.824373722076416, loss=0.6729963421821594
I0307 12:47:16.903003 139789021992704 logging_writer.py:48] [392900] global_step=392900, grad_norm=4.946966648101807, loss=0.6449160575866699
I0307 12:47:50.504554 139789030385408 logging_writer.py:48] [393000] global_step=393000, grad_norm=4.543809413909912, loss=0.6525552272796631
I0307 12:48:24.167215 139789021992704 logging_writer.py:48] [393100] global_step=393100, grad_norm=4.483788967132568, loss=0.728607177734375
I0307 12:48:57.778896 139789030385408 logging_writer.py:48] [393200] global_step=393200, grad_norm=4.5796027183532715, loss=0.6579403877258301
I0307 12:49:31.437583 139789021992704 logging_writer.py:48] [393300] global_step=393300, grad_norm=4.388485908508301, loss=0.6873550415039062
I0307 12:50:05.045162 139789030385408 logging_writer.py:48] [393400] global_step=393400, grad_norm=4.262869834899902, loss=0.5681973695755005
I0307 12:50:38.710206 139789021992704 logging_writer.py:48] [393500] global_step=393500, grad_norm=4.170892715454102, loss=0.5551714897155762
I0307 12:51:12.321415 139789030385408 logging_writer.py:48] [393600] global_step=393600, grad_norm=4.8189239501953125, loss=0.6389297842979431
I0307 12:51:45.970639 139789021992704 logging_writer.py:48] [393700] global_step=393700, grad_norm=4.329610347747803, loss=0.640540361404419
I0307 12:52:19.569435 139789030385408 logging_writer.py:48] [393800] global_step=393800, grad_norm=4.07444429397583, loss=0.5582270622253418
I0307 12:52:53.231604 139789021992704 logging_writer.py:48] [393900] global_step=393900, grad_norm=4.9110212326049805, loss=0.6965641975402832
I0307 12:53:26.838342 139789030385408 logging_writer.py:48] [394000] global_step=394000, grad_norm=4.478907108306885, loss=0.5565024614334106
I0307 12:54:00.503267 139789021992704 logging_writer.py:48] [394100] global_step=394100, grad_norm=4.243470191955566, loss=0.5863234400749207
I0307 12:54:34.186988 139789030385408 logging_writer.py:48] [394200] global_step=394200, grad_norm=4.097163677215576, loss=0.6331579685211182
I0307 12:54:58.499332 139951089751872 spec.py:321] Evaluating on the training split.
I0307 12:55:04.613158 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 12:55:13.187498 139951089751872 spec.py:349] Evaluating on the test split.
I0307 12:55:15.465520 139951089751872 submission_runner.py:411] Time since start: 137318.68s, 	Step: 394274, 	{'train/accuracy': 0.9601203799247742, 'train/loss': 0.14825664460659027, 'validation/accuracy': 0.7576000094413757, 'validation/loss': 1.0401118993759155, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8207794427871704, 'test/num_examples': 10000, 'score': 132662.2222611904, 'total_duration': 137318.67782998085, 'accumulated_submission_time': 132662.2222611904, 'accumulated_eval_time': 4622.717230796814, 'accumulated_logging_time': 19.354504823684692}
I0307 12:55:15.550196 139787621099264 logging_writer.py:48] [394274] accumulated_eval_time=4622.717231, accumulated_logging_time=19.354505, accumulated_submission_time=132662.222261, global_step=394274, preemption_count=0, score=132662.222261, test/accuracy=0.631900, test/loss=1.820779, test/num_examples=10000, total_duration=137318.677830, train/accuracy=0.960120, train/loss=0.148257, validation/accuracy=0.757600, validation/loss=1.040112, validation/num_examples=50000
I0307 12:55:24.630939 139787629491968 logging_writer.py:48] [394300] global_step=394300, grad_norm=4.727865695953369, loss=0.7046411037445068
I0307 12:55:58.257448 139787621099264 logging_writer.py:48] [394400] global_step=394400, grad_norm=4.629856109619141, loss=0.6254894733428955
I0307 12:56:31.840403 139787629491968 logging_writer.py:48] [394500] global_step=394500, grad_norm=4.800549030303955, loss=0.6831746697425842
I0307 12:57:05.413478 139787621099264 logging_writer.py:48] [394600] global_step=394600, grad_norm=4.94830846786499, loss=0.617015540599823
I0307 12:57:38.983250 139787629491968 logging_writer.py:48] [394700] global_step=394700, grad_norm=4.739063262939453, loss=0.6335533857345581
I0307 12:58:12.619495 139787621099264 logging_writer.py:48] [394800] global_step=394800, grad_norm=4.532470226287842, loss=0.5681527853012085
I0307 12:58:46.238259 139787629491968 logging_writer.py:48] [394900] global_step=394900, grad_norm=4.733969211578369, loss=0.7182916402816772
I0307 12:59:19.877106 139787621099264 logging_writer.py:48] [395000] global_step=395000, grad_norm=4.594399452209473, loss=0.6065959334373474
I0307 12:59:53.502580 139787629491968 logging_writer.py:48] [395100] global_step=395100, grad_norm=4.508482933044434, loss=0.6770496368408203
I0307 13:00:27.186293 139787621099264 logging_writer.py:48] [395200] global_step=395200, grad_norm=5.069217681884766, loss=0.6796389818191528
I0307 13:01:00.761523 139787629491968 logging_writer.py:48] [395300] global_step=395300, grad_norm=4.358581066131592, loss=0.6063499450683594
I0307 13:01:34.407127 139787621099264 logging_writer.py:48] [395400] global_step=395400, grad_norm=4.137524127960205, loss=0.5825740098953247
I0307 13:02:08.054822 139787629491968 logging_writer.py:48] [395500] global_step=395500, grad_norm=4.664340972900391, loss=0.6677370071411133
I0307 13:02:41.703878 139787621099264 logging_writer.py:48] [395600] global_step=395600, grad_norm=4.094205856323242, loss=0.5438801646232605
I0307 13:03:15.313408 139787629491968 logging_writer.py:48] [395700] global_step=395700, grad_norm=4.374218463897705, loss=0.6109247207641602
I0307 13:03:45.721762 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:03:51.825216 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:04:00.465418 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:04:02.751706 139951089751872 submission_runner.py:411] Time since start: 137845.96s, 	Step: 395792, 	{'train/accuracy': 0.9600805044174194, 'train/loss': 0.14803801476955414, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.0424708127975464, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8253854513168335, 'test/num_examples': 10000, 'score': 133172.32787752151, 'total_duration': 137845.96401286125, 'accumulated_submission_time': 133172.32787752151, 'accumulated_eval_time': 4639.747121095657, 'accumulated_logging_time': 19.450233936309814}
I0307 13:04:02.837571 139787612706560 logging_writer.py:48] [395792] accumulated_eval_time=4639.747121, accumulated_logging_time=19.450234, accumulated_submission_time=133172.327878, global_step=395792, preemption_count=0, score=133172.327878, test/accuracy=0.631300, test/loss=1.825385, test/num_examples=10000, total_duration=137845.964013, train/accuracy=0.960081, train/loss=0.148038, validation/accuracy=0.757220, validation/loss=1.042471, validation/num_examples=50000
I0307 13:04:05.891900 139787621099264 logging_writer.py:48] [395800] global_step=395800, grad_norm=4.988860607147217, loss=0.7451584339141846
I0307 13:04:39.564419 139787612706560 logging_writer.py:48] [395900] global_step=395900, grad_norm=4.61509895324707, loss=0.6409133672714233
I0307 13:05:13.247420 139787621099264 logging_writer.py:48] [396000] global_step=396000, grad_norm=4.82179594039917, loss=0.6325321793556213
I0307 13:05:46.898703 139787612706560 logging_writer.py:48] [396100] global_step=396100, grad_norm=4.538991928100586, loss=0.6178831458091736
I0307 13:06:20.579598 139787621099264 logging_writer.py:48] [396200] global_step=396200, grad_norm=4.168580532073975, loss=0.6068013906478882
I0307 13:06:54.267557 139787612706560 logging_writer.py:48] [396300] global_step=396300, grad_norm=4.70027494430542, loss=0.7246840000152588
I0307 13:07:27.895892 139787621099264 logging_writer.py:48] [396400] global_step=396400, grad_norm=4.33158016204834, loss=0.6823514103889465
I0307 13:08:01.571255 139787612706560 logging_writer.py:48] [396500] global_step=396500, grad_norm=4.454472541809082, loss=0.579035758972168
I0307 13:08:35.205645 139787621099264 logging_writer.py:48] [396600] global_step=396600, grad_norm=4.196280002593994, loss=0.6179328560829163
I0307 13:09:08.821495 139787612706560 logging_writer.py:48] [396700] global_step=396700, grad_norm=4.95513391494751, loss=0.7172072529792786
I0307 13:09:42.473054 139787621099264 logging_writer.py:48] [396800] global_step=396800, grad_norm=4.764505386352539, loss=0.6920551061630249
I0307 13:10:16.092585 139787612706560 logging_writer.py:48] [396900] global_step=396900, grad_norm=4.75875997543335, loss=0.5801293849945068
I0307 13:10:49.737642 139787621099264 logging_writer.py:48] [397000] global_step=397000, grad_norm=4.659947872161865, loss=0.5483171939849854
I0307 13:11:23.349876 139787612706560 logging_writer.py:48] [397100] global_step=397100, grad_norm=4.614572525024414, loss=0.6004817485809326
I0307 13:11:56.992355 139787621099264 logging_writer.py:48] [397200] global_step=397200, grad_norm=4.576808929443359, loss=0.610007107257843
I0307 13:12:30.637474 139787612706560 logging_writer.py:48] [397300] global_step=397300, grad_norm=4.8683671951293945, loss=0.6973422765731812
I0307 13:12:32.801481 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:12:38.878749 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:12:47.523033 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:12:49.795994 139951089751872 submission_runner.py:411] Time since start: 138373.01s, 	Step: 397308, 	{'train/accuracy': 0.9609972834587097, 'train/loss': 0.1444547176361084, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.039499282836914, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.821471929550171, 'test/num_examples': 10000, 'score': 133682.22514748573, 'total_duration': 138373.00830101967, 'accumulated_submission_time': 133682.22514748573, 'accumulated_eval_time': 4656.741573810577, 'accumulated_logging_time': 19.547237634658813}
I0307 13:12:49.884330 139789013600000 logging_writer.py:48] [397308] accumulated_eval_time=4656.741574, accumulated_logging_time=19.547238, accumulated_submission_time=133682.225147, global_step=397308, preemption_count=0, score=133682.225147, test/accuracy=0.631500, test/loss=1.821472, test/num_examples=10000, total_duration=138373.008301, train/accuracy=0.960997, train/loss=0.144455, validation/accuracy=0.757160, validation/loss=1.039499, validation/num_examples=50000
I0307 13:13:21.150824 139789021992704 logging_writer.py:48] [397400] global_step=397400, grad_norm=4.849532127380371, loss=0.6663146018981934
I0307 13:13:54.796660 139789013600000 logging_writer.py:48] [397500] global_step=397500, grad_norm=4.5267333984375, loss=0.6185535192489624
I0307 13:14:28.452662 139789021992704 logging_writer.py:48] [397600] global_step=397600, grad_norm=4.581249237060547, loss=0.6714362502098083
I0307 13:15:02.133530 139789013600000 logging_writer.py:48] [397700] global_step=397700, grad_norm=4.355981349945068, loss=0.684948205947876
I0307 13:15:35.778126 139789021992704 logging_writer.py:48] [397800] global_step=397800, grad_norm=4.59308385848999, loss=0.6077577471733093
I0307 13:16:09.425879 139789013600000 logging_writer.py:48] [397900] global_step=397900, grad_norm=4.155633449554443, loss=0.5608145594596863
I0307 13:16:43.040720 139789021992704 logging_writer.py:48] [398000] global_step=398000, grad_norm=4.676907539367676, loss=0.6058878898620605
I0307 13:17:16.684100 139789013600000 logging_writer.py:48] [398100] global_step=398100, grad_norm=5.0452494621276855, loss=0.7111223340034485
I0307 13:17:50.307320 139789021992704 logging_writer.py:48] [398200] global_step=398200, grad_norm=4.48297119140625, loss=0.6186341047286987
I0307 13:18:23.968279 139789013600000 logging_writer.py:48] [398300] global_step=398300, grad_norm=4.937167167663574, loss=0.6771018505096436
I0307 13:18:57.627577 139789021992704 logging_writer.py:48] [398400] global_step=398400, grad_norm=4.362704277038574, loss=0.6451050043106079
I0307 13:19:31.260943 139789013600000 logging_writer.py:48] [398500] global_step=398500, grad_norm=4.5993242263793945, loss=0.6691511273384094
I0307 13:20:04.945895 139789021992704 logging_writer.py:48] [398600] global_step=398600, grad_norm=4.460216999053955, loss=0.6502337455749512
I0307 13:20:38.644331 139789013600000 logging_writer.py:48] [398700] global_step=398700, grad_norm=4.448292255401611, loss=0.6339604258537292
I0307 13:21:12.291303 139789021992704 logging_writer.py:48] [398800] global_step=398800, grad_norm=5.539159774780273, loss=0.6171187162399292
I0307 13:21:19.841674 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:21:25.843856 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:21:34.366260 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:21:36.643728 139951089751872 submission_runner.py:411] Time since start: 138899.86s, 	Step: 398824, 	{'train/accuracy': 0.9614756107330322, 'train/loss': 0.1464393436908722, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.0412877798080444, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8227674961090088, 'test/num_examples': 10000, 'score': 134192.11761021614, 'total_duration': 138899.8560230732, 'accumulated_submission_time': 134192.11761021614, 'accumulated_eval_time': 4673.543560028076, 'accumulated_logging_time': 19.645186185836792}
I0307 13:21:36.728159 139787621099264 logging_writer.py:48] [398824] accumulated_eval_time=4673.543560, accumulated_logging_time=19.645186, accumulated_submission_time=134192.117610, global_step=398824, preemption_count=0, score=134192.117610, test/accuracy=0.632100, test/loss=1.822767, test/num_examples=10000, total_duration=138899.856023, train/accuracy=0.961476, train/loss=0.146439, validation/accuracy=0.757100, validation/loss=1.041288, validation/num_examples=50000
I0307 13:22:02.598379 139787629491968 logging_writer.py:48] [398900] global_step=398900, grad_norm=4.762585163116455, loss=0.6170293688774109
I0307 13:22:36.187716 139787621099264 logging_writer.py:48] [399000] global_step=399000, grad_norm=4.719602108001709, loss=0.6173166036605835
I0307 13:23:09.819965 139787629491968 logging_writer.py:48] [399100] global_step=399100, grad_norm=4.8618059158325195, loss=0.5942258238792419
I0307 13:23:43.405756 139787621099264 logging_writer.py:48] [399200] global_step=399200, grad_norm=4.58490514755249, loss=0.6708532571792603
I0307 13:24:17.037095 139787629491968 logging_writer.py:48] [399300] global_step=399300, grad_norm=4.611720561981201, loss=0.6314762830734253
I0307 13:24:50.649837 139787621099264 logging_writer.py:48] [399400] global_step=399400, grad_norm=4.800714492797852, loss=0.5748668909072876
I0307 13:25:24.284085 139787629491968 logging_writer.py:48] [399500] global_step=399500, grad_norm=4.416792392730713, loss=0.560729444026947
I0307 13:25:57.838569 139787621099264 logging_writer.py:48] [399600] global_step=399600, grad_norm=4.167996406555176, loss=0.6306198835372925
I0307 13:26:31.393991 139787629491968 logging_writer.py:48] [399700] global_step=399700, grad_norm=4.390527248382568, loss=0.5918093323707581
I0307 13:27:04.989646 139787621099264 logging_writer.py:48] [399800] global_step=399800, grad_norm=4.454576015472412, loss=0.5959726572036743
I0307 13:27:38.642231 139787629491968 logging_writer.py:48] [399900] global_step=399900, grad_norm=4.631922721862793, loss=0.6975139379501343
I0307 13:28:12.255831 139787621099264 logging_writer.py:48] [400000] global_step=400000, grad_norm=4.479930877685547, loss=0.6063099503517151
I0307 13:28:45.905948 139787629491968 logging_writer.py:48] [400100] global_step=400100, grad_norm=4.706077575683594, loss=0.5864170789718628
I0307 13:29:19.582433 139787621099264 logging_writer.py:48] [400200] global_step=400200, grad_norm=3.95835280418396, loss=0.5520354509353638
I0307 13:29:53.242884 139787629491968 logging_writer.py:48] [400300] global_step=400300, grad_norm=4.389161586761475, loss=0.6660118103027344
I0307 13:30:06.852185 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:30:12.991786 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:30:21.581863 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:30:23.862906 139951089751872 submission_runner.py:411] Time since start: 139427.08s, 	Step: 400342, 	{'train/accuracy': 0.9597616195678711, 'train/loss': 0.14685043692588806, 'validation/accuracy': 0.7565400004386902, 'validation/loss': 1.0416756868362427, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8230339288711548, 'test/num_examples': 10000, 'score': 134702.17586684227, 'total_duration': 139427.07520484924, 'accumulated_submission_time': 134702.17586684227, 'accumulated_eval_time': 4690.554224252701, 'accumulated_logging_time': 19.73924994468689}
I0307 13:30:23.945708 139787629491968 logging_writer.py:48] [400342] accumulated_eval_time=4690.554224, accumulated_logging_time=19.739250, accumulated_submission_time=134702.175867, global_step=400342, preemption_count=0, score=134702.175867, test/accuracy=0.632100, test/loss=1.823034, test/num_examples=10000, total_duration=139427.075205, train/accuracy=0.959762, train/loss=0.146850, validation/accuracy=0.756540, validation/loss=1.041676, validation/num_examples=50000
I0307 13:30:43.745892 139789013600000 logging_writer.py:48] [400400] global_step=400400, grad_norm=4.609512805938721, loss=0.650505006313324
I0307 13:31:17.417038 139787629491968 logging_writer.py:48] [400500] global_step=400500, grad_norm=4.518190860748291, loss=0.5933119058609009
I0307 13:31:51.102694 139789013600000 logging_writer.py:48] [400600] global_step=400600, grad_norm=4.484684944152832, loss=0.5890744924545288
I0307 13:32:24.746350 139787629491968 logging_writer.py:48] [400700] global_step=400700, grad_norm=4.551462650299072, loss=0.6031280755996704
I0307 13:32:58.446275 139789013600000 logging_writer.py:48] [400800] global_step=400800, grad_norm=4.395407199859619, loss=0.6315515041351318
I0307 13:33:32.086319 139787629491968 logging_writer.py:48] [400900] global_step=400900, grad_norm=4.842571258544922, loss=0.6611133813858032
I0307 13:34:05.737121 139789013600000 logging_writer.py:48] [401000] global_step=401000, grad_norm=4.366607189178467, loss=0.6235882043838501
I0307 13:34:39.345045 139787629491968 logging_writer.py:48] [401100] global_step=401100, grad_norm=4.507231712341309, loss=0.6051979660987854
I0307 13:35:12.992807 139789013600000 logging_writer.py:48] [401200] global_step=401200, grad_norm=4.465688705444336, loss=0.6084648370742798
I0307 13:35:46.595769 139787629491968 logging_writer.py:48] [401300] global_step=401300, grad_norm=4.514805316925049, loss=0.6888043284416199
I0307 13:36:20.239309 139789013600000 logging_writer.py:48] [401400] global_step=401400, grad_norm=5.289588928222656, loss=0.6396051645278931
I0307 13:36:53.860477 139787629491968 logging_writer.py:48] [401500] global_step=401500, grad_norm=4.183608055114746, loss=0.5519007444381714
I0307 13:37:27.679177 139789013600000 logging_writer.py:48] [401600] global_step=401600, grad_norm=4.342827796936035, loss=0.6234673857688904
I0307 13:38:01.300303 139787629491968 logging_writer.py:48] [401700] global_step=401700, grad_norm=4.6045122146606445, loss=0.6700706481933594
I0307 13:38:34.969850 139789013600000 logging_writer.py:48] [401800] global_step=401800, grad_norm=4.591222763061523, loss=0.5542429089546204
I0307 13:38:53.941013 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:38:59.929367 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:39:08.638485 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:39:10.922152 139951089751872 submission_runner.py:411] Time since start: 139954.13s, 	Step: 401858, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14928840100765228, 'validation/accuracy': 0.7573800086975098, 'validation/loss': 1.0411863327026367, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.822576642036438, 'test/num_examples': 10000, 'score': 135212.1051683426, 'total_duration': 139954.13414263725, 'accumulated_submission_time': 135212.1051683426, 'accumulated_eval_time': 4707.534991264343, 'accumulated_logging_time': 19.832281827926636}
I0307 13:39:11.009959 139788996814592 logging_writer.py:48] [401858] accumulated_eval_time=4707.534991, accumulated_logging_time=19.832282, accumulated_submission_time=135212.105168, global_step=401858, preemption_count=0, score=135212.105168, test/accuracy=0.632300, test/loss=1.822577, test/num_examples=10000, total_duration=139954.134143, train/accuracy=0.959881, train/loss=0.149288, validation/accuracy=0.757380, validation/loss=1.041186, validation/num_examples=50000
I0307 13:39:25.452220 139789005207296 logging_writer.py:48] [401900] global_step=401900, grad_norm=4.922813415527344, loss=0.6736322045326233
I0307 13:39:59.074796 139788996814592 logging_writer.py:48] [402000] global_step=402000, grad_norm=4.30723237991333, loss=0.6597300171852112
I0307 13:40:32.768737 139789005207296 logging_writer.py:48] [402100] global_step=402100, grad_norm=4.3989644050598145, loss=0.6855764389038086
I0307 13:41:06.443239 139788996814592 logging_writer.py:48] [402200] global_step=402200, grad_norm=5.087875843048096, loss=0.6840099096298218
I0307 13:41:40.071586 139789005207296 logging_writer.py:48] [402300] global_step=402300, grad_norm=4.015583038330078, loss=0.5712685585021973
I0307 13:42:13.725712 139788996814592 logging_writer.py:48] [402400] global_step=402400, grad_norm=4.785608291625977, loss=0.6748984456062317
I0307 13:42:47.330552 139789005207296 logging_writer.py:48] [402500] global_step=402500, grad_norm=4.490510940551758, loss=0.6289257407188416
I0307 13:43:21.083382 139788996814592 logging_writer.py:48] [402600] global_step=402600, grad_norm=4.7188801765441895, loss=0.5927637815475464
I0307 13:43:54.736531 139789005207296 logging_writer.py:48] [402700] global_step=402700, grad_norm=4.5102314949035645, loss=0.6556835770606995
I0307 13:44:28.346363 139788996814592 logging_writer.py:48] [402800] global_step=402800, grad_norm=4.102448463439941, loss=0.5485959053039551
I0307 13:45:02.050832 139789005207296 logging_writer.py:48] [402900] global_step=402900, grad_norm=4.736208438873291, loss=0.6654939651489258
I0307 13:45:35.705878 139788996814592 logging_writer.py:48] [403000] global_step=403000, grad_norm=4.939635276794434, loss=0.6272857785224915
I0307 13:46:09.337545 139789005207296 logging_writer.py:48] [403100] global_step=403100, grad_norm=4.594277858734131, loss=0.6539587378501892
I0307 13:46:42.992321 139788996814592 logging_writer.py:48] [403200] global_step=403200, grad_norm=4.35028600692749, loss=0.6596012711524963
I0307 13:47:16.606695 139789005207296 logging_writer.py:48] [403300] global_step=403300, grad_norm=4.42434549331665, loss=0.6279480457305908
I0307 13:47:40.985281 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:47:46.998605 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:47:55.699845 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:47:57.919031 139951089751872 submission_runner.py:411] Time since start: 140481.13s, 	Step: 403374, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14611656963825226, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0413422584533691, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.822820782661438, 'test/num_examples': 10000, 'score': 135722.01432061195, 'total_duration': 140481.13131904602, 'accumulated_submission_time': 135722.01432061195, 'accumulated_eval_time': 4724.468670129776, 'accumulated_logging_time': 19.930617094039917}
I0307 13:47:58.003642 139787621099264 logging_writer.py:48] [403374] accumulated_eval_time=4724.468670, accumulated_logging_time=19.930617, accumulated_submission_time=135722.014321, global_step=403374, preemption_count=0, score=135722.014321, test/accuracy=0.632000, test/loss=1.822821, test/num_examples=10000, total_duration=140481.131319, train/accuracy=0.960579, train/loss=0.146117, validation/accuracy=0.757180, validation/loss=1.041342, validation/num_examples=50000
I0307 13:48:07.067327 139787629491968 logging_writer.py:48] [403400] global_step=403400, grad_norm=4.621077537536621, loss=0.7513386011123657
I0307 13:48:40.674074 139787621099264 logging_writer.py:48] [403500] global_step=403500, grad_norm=4.947033405303955, loss=0.7227418422698975
I0307 13:49:14.342633 139787629491968 logging_writer.py:48] [403600] global_step=403600, grad_norm=4.44761323928833, loss=0.607833743095398
I0307 13:49:48.020716 139787621099264 logging_writer.py:48] [403700] global_step=403700, grad_norm=4.33321475982666, loss=0.6077005863189697
I0307 13:50:21.621869 139787629491968 logging_writer.py:48] [403800] global_step=403800, grad_norm=4.3649821281433105, loss=0.5954508781433105
I0307 13:50:55.268950 139787621099264 logging_writer.py:48] [403900] global_step=403900, grad_norm=4.329595565795898, loss=0.5657471418380737
I0307 13:51:28.910032 139787629491968 logging_writer.py:48] [404000] global_step=404000, grad_norm=4.630274295806885, loss=0.6151493787765503
I0307 13:52:02.592304 139787621099264 logging_writer.py:48] [404100] global_step=404100, grad_norm=4.15279483795166, loss=0.6632339954376221
I0307 13:52:36.236156 139787629491968 logging_writer.py:48] [404200] global_step=404200, grad_norm=4.832421779632568, loss=0.6570677757263184
I0307 13:53:09.877740 139787621099264 logging_writer.py:48] [404300] global_step=404300, grad_norm=4.211018085479736, loss=0.5944938659667969
I0307 13:53:43.517368 139787629491968 logging_writer.py:48] [404400] global_step=404400, grad_norm=5.059373378753662, loss=0.6442891359329224
I0307 13:54:17.167610 139787621099264 logging_writer.py:48] [404500] global_step=404500, grad_norm=4.333444595336914, loss=0.6669374108314514
I0307 13:54:50.796854 139787629491968 logging_writer.py:48] [404600] global_step=404600, grad_norm=4.446666717529297, loss=0.5508779287338257
I0307 13:55:24.445621 139787621099264 logging_writer.py:48] [404700] global_step=404700, grad_norm=4.581297874450684, loss=0.7004703283309937
I0307 13:55:58.260218 139787629491968 logging_writer.py:48] [404800] global_step=404800, grad_norm=4.129580974578857, loss=0.6327667832374573
I0307 13:56:28.018799 139951089751872 spec.py:321] Evaluating on the training split.
I0307 13:56:34.047502 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 13:56:42.637397 139951089751872 spec.py:349] Evaluating on the test split.
I0307 13:56:44.989204 139951089751872 submission_runner.py:411] Time since start: 141008.20s, 	Step: 404890, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14855101704597473, 'validation/accuracy': 0.7574799656867981, 'validation/loss': 1.041606068611145, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.824077844619751, 'test/num_examples': 10000, 'score': 136231.96271348, 'total_duration': 141008.20151233673, 'accumulated_submission_time': 136231.96271348, 'accumulated_eval_time': 4741.439030885696, 'accumulated_logging_time': 20.024643182754517}
I0307 13:56:45.072743 139789030385408 logging_writer.py:48] [404890] accumulated_eval_time=4741.439031, accumulated_logging_time=20.024643, accumulated_submission_time=136231.962713, global_step=404890, preemption_count=0, score=136231.962713, test/accuracy=0.631200, test/loss=1.824078, test/num_examples=10000, total_duration=141008.201512, train/accuracy=0.960698, train/loss=0.148551, validation/accuracy=0.757480, validation/loss=1.041606, validation/num_examples=50000
I0307 13:56:48.773805 139789038778112 logging_writer.py:48] [404900] global_step=404900, grad_norm=4.770298480987549, loss=0.6849279999732971
I0307 13:57:22.372224 139789030385408 logging_writer.py:48] [405000] global_step=405000, grad_norm=4.5602288246154785, loss=0.6310524940490723
I0307 13:57:55.989274 139789038778112 logging_writer.py:48] [405100] global_step=405100, grad_norm=4.264434814453125, loss=0.6368248462677002
I0307 13:58:29.584374 139789030385408 logging_writer.py:48] [405200] global_step=405200, grad_norm=4.190762042999268, loss=0.6179636120796204
I0307 13:59:03.140311 139789038778112 logging_writer.py:48] [405300] global_step=405300, grad_norm=4.527924537658691, loss=0.625259280204773
I0307 13:59:36.726523 139789030385408 logging_writer.py:48] [405400] global_step=405400, grad_norm=4.867483615875244, loss=0.6651971936225891
I0307 14:00:10.321877 139789038778112 logging_writer.py:48] [405500] global_step=405500, grad_norm=4.531987190246582, loss=0.6776041984558105
I0307 14:00:43.923579 139789030385408 logging_writer.py:48] [405600] global_step=405600, grad_norm=4.364192485809326, loss=0.5960628390312195
I0307 14:01:17.590915 139789038778112 logging_writer.py:48] [405700] global_step=405700, grad_norm=4.53900671005249, loss=0.5765571594238281
I0307 14:01:51.279502 139789030385408 logging_writer.py:48] [405800] global_step=405800, grad_norm=4.414453983306885, loss=0.5889826416969299
I0307 14:02:24.853680 139789038778112 logging_writer.py:48] [405900] global_step=405900, grad_norm=4.26315975189209, loss=0.6076148152351379
I0307 14:02:58.569032 139789030385408 logging_writer.py:48] [406000] global_step=406000, grad_norm=4.697589874267578, loss=0.6151280999183655
I0307 14:03:32.230576 139789038778112 logging_writer.py:48] [406100] global_step=406100, grad_norm=4.534144401550293, loss=0.6219936609268188
I0307 14:04:05.864122 139789030385408 logging_writer.py:48] [406200] global_step=406200, grad_norm=4.485838413238525, loss=0.6582194566726685
I0307 14:04:39.537448 139789038778112 logging_writer.py:48] [406300] global_step=406300, grad_norm=4.2143354415893555, loss=0.5991007089614868
I0307 14:05:13.178331 139789030385408 logging_writer.py:48] [406400] global_step=406400, grad_norm=4.58053731918335, loss=0.6535996198654175
I0307 14:05:15.008661 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:05:21.073319 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:05:29.594183 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:05:31.878874 139951089751872 submission_runner.py:411] Time since start: 141535.09s, 	Step: 406407, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14861144125461578, 'validation/accuracy': 0.75764000415802, 'validation/loss': 1.0409525632858276, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.8219377994537354, 'test/num_examples': 10000, 'score': 136741.83385276794, 'total_duration': 141535.09117531776, 'accumulated_submission_time': 136741.83385276794, 'accumulated_eval_time': 4758.309185504913, 'accumulated_logging_time': 20.118609189987183}
I0307 14:05:31.962106 139788996814592 logging_writer.py:48] [406407] accumulated_eval_time=4758.309186, accumulated_logging_time=20.118609, accumulated_submission_time=136741.833853, global_step=406407, preemption_count=0, score=136741.833853, test/accuracy=0.632300, test/loss=1.821938, test/num_examples=10000, total_duration=141535.091175, train/accuracy=0.960320, train/loss=0.148611, validation/accuracy=0.757640, validation/loss=1.040953, validation/num_examples=50000
I0307 14:06:03.536954 139789005207296 logging_writer.py:48] [406500] global_step=406500, grad_norm=4.516910076141357, loss=0.6413732171058655
I0307 14:06:37.111642 139788996814592 logging_writer.py:48] [406600] global_step=406600, grad_norm=4.204928874969482, loss=0.57651287317276
I0307 14:07:10.660669 139789005207296 logging_writer.py:48] [406700] global_step=406700, grad_norm=4.299323081970215, loss=0.589043378829956
I0307 14:07:44.256406 139788996814592 logging_writer.py:48] [406800] global_step=406800, grad_norm=4.479727745056152, loss=0.6314204335212708
I0307 14:08:17.929114 139789005207296 logging_writer.py:48] [406900] global_step=406900, grad_norm=4.643029689788818, loss=0.6098789572715759
I0307 14:08:51.590098 139788996814592 logging_writer.py:48] [407000] global_step=407000, grad_norm=4.4316911697387695, loss=0.693640410900116
I0307 14:09:25.236008 139789005207296 logging_writer.py:48] [407100] global_step=407100, grad_norm=4.828555583953857, loss=0.6523528695106506
I0307 14:09:58.903849 139788996814592 logging_writer.py:48] [407200] global_step=407200, grad_norm=4.393161296844482, loss=0.6184242963790894
I0307 14:10:32.502220 139789005207296 logging_writer.py:48] [407300] global_step=407300, grad_norm=4.896208763122559, loss=0.6482723355293274
I0307 14:11:06.146559 139788996814592 logging_writer.py:48] [407400] global_step=407400, grad_norm=4.601390361785889, loss=0.6681197881698608
I0307 14:11:39.759709 139789005207296 logging_writer.py:48] [407500] global_step=407500, grad_norm=3.9464495182037354, loss=0.5762858390808105
I0307 14:12:13.403917 139788996814592 logging_writer.py:48] [407600] global_step=407600, grad_norm=4.2110185623168945, loss=0.6072001457214355
I0307 14:12:47.061473 139789005207296 logging_writer.py:48] [407700] global_step=407700, grad_norm=4.6203203201293945, loss=0.6192142367362976
I0307 14:13:20.721310 139788996814592 logging_writer.py:48] [407800] global_step=407800, grad_norm=4.741894721984863, loss=0.6391109824180603
I0307 14:13:54.484252 139789005207296 logging_writer.py:48] [407900] global_step=407900, grad_norm=5.1671247482299805, loss=0.6848263740539551
I0307 14:14:02.023199 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:14:08.132016 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:14:16.664777 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:14:18.937771 139951089751872 submission_runner.py:411] Time since start: 142062.15s, 	Step: 407924, 	{'train/accuracy': 0.960957407951355, 'train/loss': 0.14471402764320374, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.0417368412017822, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.821706771850586, 'test/num_examples': 10000, 'score': 137251.82893824577, 'total_duration': 142062.15008211136, 'accumulated_submission_time': 137251.82893824577, 'accumulated_eval_time': 4775.223704099655, 'accumulated_logging_time': 20.211474180221558}
I0307 14:14:19.025604 139787629491968 logging_writer.py:48] [407924] accumulated_eval_time=4775.223704, accumulated_logging_time=20.211474, accumulated_submission_time=137251.828938, global_step=407924, preemption_count=0, score=137251.828938, test/accuracy=0.631400, test/loss=1.821707, test/num_examples=10000, total_duration=142062.150082, train/accuracy=0.960957, train/loss=0.144714, validation/accuracy=0.757140, validation/loss=1.041737, validation/num_examples=50000
I0307 14:14:44.901797 139789021992704 logging_writer.py:48] [408000] global_step=408000, grad_norm=4.184648513793945, loss=0.5949844121932983
I0307 14:15:18.526761 139787629491968 logging_writer.py:48] [408100] global_step=408100, grad_norm=4.450110912322998, loss=0.6750620603561401
I0307 14:15:52.183666 139789021992704 logging_writer.py:48] [408200] global_step=408200, grad_norm=4.023621082305908, loss=0.5590037107467651
I0307 14:16:25.833386 139787629491968 logging_writer.py:48] [408300] global_step=408300, grad_norm=4.327586650848389, loss=0.6186522245407104
I0307 14:16:59.483738 139789021992704 logging_writer.py:48] [408400] global_step=408400, grad_norm=4.395380973815918, loss=0.5893208384513855
I0307 14:17:33.132147 139787629491968 logging_writer.py:48] [408500] global_step=408500, grad_norm=4.492475986480713, loss=0.6716236472129822
I0307 14:18:06.769277 139789021992704 logging_writer.py:48] [408600] global_step=408600, grad_norm=4.673183917999268, loss=0.5945520997047424
I0307 14:18:40.398003 139787629491968 logging_writer.py:48] [408700] global_step=408700, grad_norm=4.5881547927856445, loss=0.6124882102012634
I0307 14:19:14.020266 139789021992704 logging_writer.py:48] [408800] global_step=408800, grad_norm=4.459285259246826, loss=0.6369313597679138
I0307 14:19:47.648324 139787629491968 logging_writer.py:48] [408900] global_step=408900, grad_norm=4.621242046356201, loss=0.6223616003990173
I0307 14:20:21.351934 139789021992704 logging_writer.py:48] [409000] global_step=409000, grad_norm=4.368267059326172, loss=0.6462534070014954
I0307 14:20:54.921820 139787629491968 logging_writer.py:48] [409100] global_step=409100, grad_norm=4.157698154449463, loss=0.5842873454093933
I0307 14:21:28.480369 139789021992704 logging_writer.py:48] [409200] global_step=409200, grad_norm=4.789962291717529, loss=0.6391090750694275
I0307 14:22:02.089802 139787629491968 logging_writer.py:48] [409300] global_step=409300, grad_norm=4.412106037139893, loss=0.6026647090911865
I0307 14:22:35.745544 139789021992704 logging_writer.py:48] [409400] global_step=409400, grad_norm=4.914747714996338, loss=0.6588248014450073
I0307 14:22:48.996264 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:22:55.029122 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:23:03.596975 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:23:05.860383 139951089751872 submission_runner.py:411] Time since start: 142589.07s, 	Step: 409441, 	{'train/accuracy': 0.9587651491165161, 'train/loss': 0.14951689541339874, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.041053056716919, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8224622011184692, 'test/num_examples': 10000, 'score': 137761.7340619564, 'total_duration': 142589.07264232635, 'accumulated_submission_time': 137761.7340619564, 'accumulated_eval_time': 4792.087728738785, 'accumulated_logging_time': 20.309407234191895}
I0307 14:23:05.955146 139788996814592 logging_writer.py:48] [409441] accumulated_eval_time=4792.087729, accumulated_logging_time=20.309407, accumulated_submission_time=137761.734062, global_step=409441, preemption_count=0, score=137761.734062, test/accuracy=0.631900, test/loss=1.822462, test/num_examples=10000, total_duration=142589.072642, train/accuracy=0.958765, train/loss=0.149517, validation/accuracy=0.757000, validation/loss=1.041053, validation/num_examples=50000
I0307 14:23:26.155755 139789005207296 logging_writer.py:48] [409500] global_step=409500, grad_norm=4.247847080230713, loss=0.5741796493530273
I0307 14:23:59.783996 139788996814592 logging_writer.py:48] [409600] global_step=409600, grad_norm=4.8497843742370605, loss=0.6472495198249817
I0307 14:24:33.448875 139789005207296 logging_writer.py:48] [409700] global_step=409700, grad_norm=4.5624284744262695, loss=0.6090404391288757
I0307 14:25:07.102324 139788996814592 logging_writer.py:48] [409800] global_step=409800, grad_norm=4.153789520263672, loss=0.6339064836502075
I0307 14:25:40.795079 139789005207296 logging_writer.py:48] [409900] global_step=409900, grad_norm=4.795864582061768, loss=0.6042344570159912
I0307 14:26:14.464762 139788996814592 logging_writer.py:48] [410000] global_step=410000, grad_norm=4.508790016174316, loss=0.5642567873001099
I0307 14:26:48.059764 139789005207296 logging_writer.py:48] [410100] global_step=410100, grad_norm=4.309628009796143, loss=0.6306087970733643
I0307 14:27:21.656496 139788996814592 logging_writer.py:48] [410200] global_step=410200, grad_norm=4.23553991317749, loss=0.579008162021637
I0307 14:27:55.332831 139789005207296 logging_writer.py:48] [410300] global_step=410300, grad_norm=4.66392183303833, loss=0.5943686366081238
I0307 14:28:28.954516 139788996814592 logging_writer.py:48] [410400] global_step=410400, grad_norm=4.695967674255371, loss=0.6866021156311035
I0307 14:29:02.588179 139789005207296 logging_writer.py:48] [410500] global_step=410500, grad_norm=4.650119304656982, loss=0.6778501868247986
I0307 14:29:36.196837 139788996814592 logging_writer.py:48] [410600] global_step=410600, grad_norm=4.168087959289551, loss=0.6136487722396851
I0307 14:30:09.859796 139789005207296 logging_writer.py:48] [410700] global_step=410700, grad_norm=4.216153144836426, loss=0.5100669860839844
I0307 14:30:43.501040 139788996814592 logging_writer.py:48] [410800] global_step=410800, grad_norm=4.339730262756348, loss=0.6434575319290161
I0307 14:31:17.170419 139789005207296 logging_writer.py:48] [410900] global_step=410900, grad_norm=4.064897537231445, loss=0.5886882543563843
I0307 14:31:36.158020 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:31:42.243377 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:31:50.724342 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:31:52.998328 139951089751872 submission_runner.py:411] Time since start: 143116.21s, 	Step: 410958, 	{'train/accuracy': 0.9617944359779358, 'train/loss': 0.14471274614334106, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.0410289764404297, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.821075677871704, 'test/num_examples': 10000, 'score': 138271.87314987183, 'total_duration': 143116.21062779427, 'accumulated_submission_time': 138271.87314987183, 'accumulated_eval_time': 4808.927973031998, 'accumulated_logging_time': 20.41361141204834}
I0307 14:31:53.079923 139787612706560 logging_writer.py:48] [410958] accumulated_eval_time=4808.927973, accumulated_logging_time=20.413611, accumulated_submission_time=138271.873150, global_step=410958, preemption_count=0, score=138271.873150, test/accuracy=0.631000, test/loss=1.821076, test/num_examples=10000, total_duration=143116.210628, train/accuracy=0.961794, train/loss=0.144713, validation/accuracy=0.756920, validation/loss=1.041029, validation/num_examples=50000
I0307 14:32:07.506966 139787621099264 logging_writer.py:48] [411000] global_step=411000, grad_norm=4.298223495483398, loss=0.6112542748451233
I0307 14:32:41.263226 139787612706560 logging_writer.py:48] [411100] global_step=411100, grad_norm=4.379640102386475, loss=0.5913716554641724
I0307 14:33:14.811204 139787621099264 logging_writer.py:48] [411200] global_step=411200, grad_norm=4.652421474456787, loss=0.5751526355743408
I0307 14:33:48.410104 139787612706560 logging_writer.py:48] [411300] global_step=411300, grad_norm=4.464746952056885, loss=0.5745203495025635
I0307 14:34:22.025538 139787621099264 logging_writer.py:48] [411400] global_step=411400, grad_norm=4.908392429351807, loss=0.6548433303833008
I0307 14:34:55.636409 139787612706560 logging_writer.py:48] [411500] global_step=411500, grad_norm=4.863247871398926, loss=0.6051508188247681
I0307 14:35:29.256614 139787621099264 logging_writer.py:48] [411600] global_step=411600, grad_norm=4.5607194900512695, loss=0.6505371332168579
I0307 14:36:02.898685 139787612706560 logging_writer.py:48] [411700] global_step=411700, grad_norm=4.48794412612915, loss=0.5828635096549988
I0307 14:36:36.534628 139787621099264 logging_writer.py:48] [411800] global_step=411800, grad_norm=4.596836566925049, loss=0.666088879108429
I0307 14:37:10.177354 139787612706560 logging_writer.py:48] [411900] global_step=411900, grad_norm=4.039422988891602, loss=0.5837059617042542
I0307 14:37:43.808612 139787621099264 logging_writer.py:48] [412000] global_step=412000, grad_norm=4.866290092468262, loss=0.6602708101272583
I0307 14:38:17.531410 139787612706560 logging_writer.py:48] [412100] global_step=412100, grad_norm=4.427197456359863, loss=0.6170718669891357
I0307 14:38:51.224088 139787621099264 logging_writer.py:48] [412200] global_step=412200, grad_norm=4.572254180908203, loss=0.6608095765113831
I0307 14:39:24.873908 139787612706560 logging_writer.py:48] [412300] global_step=412300, grad_norm=4.503638744354248, loss=0.6430263519287109
I0307 14:39:58.518490 139787621099264 logging_writer.py:48] [412400] global_step=412400, grad_norm=4.551492214202881, loss=0.5958231687545776
I0307 14:40:23.218498 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:40:29.233541 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:40:37.839389 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:40:40.147402 139951089751872 submission_runner.py:411] Time since start: 143643.36s, 	Step: 412475, 	{'train/accuracy': 0.958984375, 'train/loss': 0.14896036684513092, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0413627624511719, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.822407841682434, 'test/num_examples': 10000, 'score': 138781.94454431534, 'total_duration': 143643.35971355438, 'accumulated_submission_time': 138781.94454431534, 'accumulated_eval_time': 4825.856830596924, 'accumulated_logging_time': 20.50609040260315}
I0307 14:40:40.232447 139789013600000 logging_writer.py:48] [412475] accumulated_eval_time=4825.856831, accumulated_logging_time=20.506090, accumulated_submission_time=138781.944544, global_step=412475, preemption_count=0, score=138781.944544, test/accuracy=0.631200, test/loss=1.822408, test/num_examples=10000, total_duration=143643.359714, train/accuracy=0.958984, train/loss=0.148960, validation/accuracy=0.757180, validation/loss=1.041363, validation/num_examples=50000
I0307 14:40:48.953607 139789021992704 logging_writer.py:48] [412500] global_step=412500, grad_norm=4.374770164489746, loss=0.6259216666221619
I0307 14:41:22.579634 139789013600000 logging_writer.py:48] [412600] global_step=412600, grad_norm=4.3688530921936035, loss=0.5610321760177612
I0307 14:41:56.216279 139789021992704 logging_writer.py:48] [412700] global_step=412700, grad_norm=4.342620372772217, loss=0.6227361559867859
I0307 14:42:29.883636 139789013600000 logging_writer.py:48] [412800] global_step=412800, grad_norm=5.131959438323975, loss=0.6167259216308594
I0307 14:43:03.535764 139789021992704 logging_writer.py:48] [412900] global_step=412900, grad_norm=4.41350793838501, loss=0.6643791198730469
I0307 14:43:37.210221 139789013600000 logging_writer.py:48] [413000] global_step=413000, grad_norm=4.267670154571533, loss=0.6334869861602783
I0307 14:44:10.856224 139789021992704 logging_writer.py:48] [413100] global_step=413100, grad_norm=4.416689872741699, loss=0.5886054635047913
I0307 14:44:44.618203 139789013600000 logging_writer.py:48] [413200] global_step=413200, grad_norm=4.5245890617370605, loss=0.6172257661819458
I0307 14:45:18.237132 139789021992704 logging_writer.py:48] [413300] global_step=413300, grad_norm=5.445832252502441, loss=0.650544285774231
I0307 14:45:51.854072 139789013600000 logging_writer.py:48] [413400] global_step=413400, grad_norm=4.450334548950195, loss=0.6576451659202576
I0307 14:46:25.492823 139789021992704 logging_writer.py:48] [413500] global_step=413500, grad_norm=4.4886393547058105, loss=0.6374415159225464
I0307 14:46:59.176759 139789013600000 logging_writer.py:48] [413600] global_step=413600, grad_norm=4.147010803222656, loss=0.567081093788147
I0307 14:47:32.778282 139789021992704 logging_writer.py:48] [413700] global_step=413700, grad_norm=4.2680768966674805, loss=0.562920093536377
I0307 14:48:06.446310 139789013600000 logging_writer.py:48] [413800] global_step=413800, grad_norm=5.038507461547852, loss=0.6087695360183716
I0307 14:48:40.059555 139789021992704 logging_writer.py:48] [413900] global_step=413900, grad_norm=4.600481986999512, loss=0.5823125839233398
I0307 14:49:10.167623 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:49:16.207793 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:49:24.817201 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:49:27.098021 139951089751872 submission_runner.py:411] Time since start: 144170.31s, 	Step: 413991, 	{'train/accuracy': 0.9603396058082581, 'train/loss': 0.1461835503578186, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0407791137695312, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8215856552124023, 'test/num_examples': 10000, 'score': 139291.81418538094, 'total_duration': 144170.310328722, 'accumulated_submission_time': 139291.81418538094, 'accumulated_eval_time': 4842.787178993225, 'accumulated_logging_time': 20.600675106048584}
I0307 14:49:27.184775 139787612706560 logging_writer.py:48] [413991] accumulated_eval_time=4842.787179, accumulated_logging_time=20.600675, accumulated_submission_time=139291.814185, global_step=413991, preemption_count=0, score=139291.814185, test/accuracy=0.632100, test/loss=1.821586, test/num_examples=10000, total_duration=144170.310329, train/accuracy=0.960340, train/loss=0.146184, validation/accuracy=0.757080, validation/loss=1.040779, validation/num_examples=50000
I0307 14:49:30.546309 139787621099264 logging_writer.py:48] [414000] global_step=414000, grad_norm=4.255646228790283, loss=0.5909622311592102
I0307 14:50:04.157241 139787612706560 logging_writer.py:48] [414100] global_step=414100, grad_norm=4.728183746337891, loss=0.6559250354766846
I0307 14:50:37.848618 139787621099264 logging_writer.py:48] [414200] global_step=414200, grad_norm=4.212708473205566, loss=0.6381500959396362
I0307 14:51:11.522373 139787612706560 logging_writer.py:48] [414300] global_step=414300, grad_norm=4.551779270172119, loss=0.6093648672103882
I0307 14:51:45.190055 139787621099264 logging_writer.py:48] [414400] global_step=414400, grad_norm=5.29508638381958, loss=0.651301920413971
I0307 14:52:18.887676 139787612706560 logging_writer.py:48] [414500] global_step=414500, grad_norm=3.8540375232696533, loss=0.5508614182472229
I0307 14:52:52.542541 139787621099264 logging_writer.py:48] [414600] global_step=414600, grad_norm=4.698448181152344, loss=0.5708019733428955
I0307 14:53:26.176402 139787612706560 logging_writer.py:48] [414700] global_step=414700, grad_norm=4.56550931930542, loss=0.6557446718215942
I0307 14:53:59.828837 139787621099264 logging_writer.py:48] [414800] global_step=414800, grad_norm=4.490051746368408, loss=0.6518036127090454
I0307 14:54:33.469632 139787612706560 logging_writer.py:48] [414900] global_step=414900, grad_norm=4.735963821411133, loss=0.7367435693740845
I0307 14:55:07.118482 139787621099264 logging_writer.py:48] [415000] global_step=415000, grad_norm=4.8024210929870605, loss=0.631138801574707
I0307 14:55:40.767808 139787612706560 logging_writer.py:48] [415100] global_step=415100, grad_norm=4.548943996429443, loss=0.6667062640190125
I0307 14:56:14.380214 139787621099264 logging_writer.py:48] [415200] global_step=415200, grad_norm=4.232837200164795, loss=0.5723645091056824
I0307 14:56:48.049214 139787612706560 logging_writer.py:48] [415300] global_step=415300, grad_norm=4.306210041046143, loss=0.6500522494316101
I0307 14:57:21.652397 139787621099264 logging_writer.py:48] [415400] global_step=415400, grad_norm=4.5640153884887695, loss=0.6236727237701416
I0307 14:57:55.284659 139787612706560 logging_writer.py:48] [415500] global_step=415500, grad_norm=4.890556335449219, loss=0.627011775970459
I0307 14:57:57.112227 139951089751872 spec.py:321] Evaluating on the training split.
I0307 14:58:03.137839 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 14:58:11.847480 139951089751872 spec.py:349] Evaluating on the test split.
I0307 14:58:14.144386 139951089751872 submission_runner.py:411] Time since start: 144697.36s, 	Step: 415507, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14661581814289093, 'validation/accuracy': 0.7567999958992004, 'validation/loss': 1.041944146156311, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.823670744895935, 'test/num_examples': 10000, 'score': 139801.6758582592, 'total_duration': 144697.35668969154, 'accumulated_submission_time': 139801.6758582592, 'accumulated_eval_time': 4859.819278478622, 'accumulated_logging_time': 20.697286128997803}
I0307 14:58:14.227309 139787612706560 logging_writer.py:48] [415507] accumulated_eval_time=4859.819278, accumulated_logging_time=20.697286, accumulated_submission_time=139801.675858, global_step=415507, preemption_count=0, score=139801.675858, test/accuracy=0.630700, test/loss=1.823671, test/num_examples=10000, total_duration=144697.356690, train/accuracy=0.960977, train/loss=0.146616, validation/accuracy=0.756800, validation/loss=1.041944, validation/num_examples=50000
I0307 14:58:45.888056 139787621099264 logging_writer.py:48] [415600] global_step=415600, grad_norm=4.394643306732178, loss=0.6105819940567017
I0307 14:59:19.567665 139787612706560 logging_writer.py:48] [415700] global_step=415700, grad_norm=4.777501583099365, loss=0.6302869319915771
I0307 14:59:53.196610 139787621099264 logging_writer.py:48] [415800] global_step=415800, grad_norm=4.636660575866699, loss=0.6591281890869141
I0307 15:00:26.868087 139787612706560 logging_writer.py:48] [415900] global_step=415900, grad_norm=4.395382881164551, loss=0.6277083158493042
I0307 15:01:00.477024 139787621099264 logging_writer.py:48] [416000] global_step=416000, grad_norm=4.272535800933838, loss=0.6193610429763794
I0307 15:01:34.142462 139787612706560 logging_writer.py:48] [416100] global_step=416100, grad_norm=4.030466556549072, loss=0.5783143639564514
I0307 15:02:07.756988 139787621099264 logging_writer.py:48] [416200] global_step=416200, grad_norm=4.304248332977295, loss=0.5511488914489746
I0307 15:02:41.429075 139787612706560 logging_writer.py:48] [416300] global_step=416300, grad_norm=4.047600269317627, loss=0.526581883430481
I0307 15:03:15.087031 139787621099264 logging_writer.py:48] [416400] global_step=416400, grad_norm=4.1692423820495605, loss=0.5526602864265442
I0307 15:03:48.692252 139787612706560 logging_writer.py:48] [416500] global_step=416500, grad_norm=4.461341857910156, loss=0.6271018385887146
I0307 15:04:22.295047 139787621099264 logging_writer.py:48] [416600] global_step=416600, grad_norm=4.465376853942871, loss=0.6230171918869019
I0307 15:04:55.834396 139787612706560 logging_writer.py:48] [416700] global_step=416700, grad_norm=4.654733180999756, loss=0.6206661462783813
I0307 15:05:29.406654 139787621099264 logging_writer.py:48] [416800] global_step=416800, grad_norm=4.28724479675293, loss=0.609261155128479
I0307 15:06:03.053794 139787612706560 logging_writer.py:48] [416900] global_step=416900, grad_norm=4.738636493682861, loss=0.6611590385437012
I0307 15:06:36.655660 139787621099264 logging_writer.py:48] [417000] global_step=417000, grad_norm=4.193386077880859, loss=0.622176468372345
I0307 15:06:44.198679 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:06:50.248881 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:06:58.894408 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:07:01.151396 139951089751872 submission_runner.py:411] Time since start: 145224.36s, 	Step: 417024, 	{'train/accuracy': 0.9611766338348389, 'train/loss': 0.1452847421169281, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.0428640842437744, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8238797187805176, 'test/num_examples': 10000, 'score': 140311.5811650753, 'total_duration': 145224.3636994362, 'accumulated_submission_time': 140311.5811650753, 'accumulated_eval_time': 4876.771936416626, 'accumulated_logging_time': 20.78969407081604}
I0307 15:07:01.234604 139789005207296 logging_writer.py:48] [417024] accumulated_eval_time=4876.771936, accumulated_logging_time=20.789694, accumulated_submission_time=140311.581165, global_step=417024, preemption_count=0, score=140311.581165, test/accuracy=0.631000, test/loss=1.823880, test/num_examples=10000, total_duration=145224.363699, train/accuracy=0.961177, train/loss=0.145285, validation/accuracy=0.756880, validation/loss=1.042864, validation/num_examples=50000
I0307 15:07:27.093479 139789030385408 logging_writer.py:48] [417100] global_step=417100, grad_norm=4.745344161987305, loss=0.5706643462181091
I0307 15:08:00.728760 139789005207296 logging_writer.py:48] [417200] global_step=417200, grad_norm=4.743927001953125, loss=0.5967848896980286
I0307 15:08:34.339807 139789030385408 logging_writer.py:48] [417300] global_step=417300, grad_norm=4.47083044052124, loss=0.699954628944397
I0307 15:09:07.986310 139789005207296 logging_writer.py:48] [417400] global_step=417400, grad_norm=4.530296802520752, loss=0.5825016498565674
I0307 15:09:41.580167 139789030385408 logging_writer.py:48] [417500] global_step=417500, grad_norm=4.373035907745361, loss=0.5953406095504761
I0307 15:10:15.222096 139789005207296 logging_writer.py:48] [417600] global_step=417600, grad_norm=4.569848537445068, loss=0.6147944927215576
I0307 15:10:48.855266 139789030385408 logging_writer.py:48] [417700] global_step=417700, grad_norm=4.532531261444092, loss=0.618690013885498
I0307 15:11:22.492097 139789005207296 logging_writer.py:48] [417800] global_step=417800, grad_norm=4.4778876304626465, loss=0.6874838471412659
I0307 15:11:56.150873 139789030385408 logging_writer.py:48] [417900] global_step=417900, grad_norm=4.691709995269775, loss=0.6218961477279663
I0307 15:12:29.773325 139789005207296 logging_writer.py:48] [418000] global_step=418000, grad_norm=4.464487552642822, loss=0.593305766582489
I0307 15:13:03.434478 139789030385408 logging_writer.py:48] [418100] global_step=418100, grad_norm=4.231074810028076, loss=0.5812400579452515
I0307 15:13:37.091090 139789005207296 logging_writer.py:48] [418200] global_step=418200, grad_norm=4.430598258972168, loss=0.5980117321014404
I0307 15:14:10.750159 139789030385408 logging_writer.py:48] [418300] global_step=418300, grad_norm=4.450723648071289, loss=0.6042165756225586
I0307 15:14:44.448460 139789005207296 logging_writer.py:48] [418400] global_step=418400, grad_norm=4.345853805541992, loss=0.5915822982788086
I0307 15:15:18.095663 139789030385408 logging_writer.py:48] [418500] global_step=418500, grad_norm=4.182600498199463, loss=0.6304391622543335
I0307 15:15:31.332319 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:15:37.317717 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:15:45.948395 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:15:48.261478 139951089751872 submission_runner.py:411] Time since start: 145751.47s, 	Step: 418541, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14593924582004547, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.040714144706726, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8214082717895508, 'test/num_examples': 10000, 'score': 140821.61366438866, 'total_duration': 145751.47378349304, 'accumulated_submission_time': 140821.61366438866, 'accumulated_eval_time': 4893.701037406921, 'accumulated_logging_time': 20.88233780860901}
I0307 15:15:48.348071 139787612706560 logging_writer.py:48] [418541] accumulated_eval_time=4893.701037, accumulated_logging_time=20.882338, accumulated_submission_time=140821.613664, global_step=418541, preemption_count=0, score=140821.613664, test/accuracy=0.631900, test/loss=1.821408, test/num_examples=10000, total_duration=145751.473783, train/accuracy=0.961037, train/loss=0.145939, validation/accuracy=0.757160, validation/loss=1.040714, validation/num_examples=50000
I0307 15:16:08.545186 139787629491968 logging_writer.py:48] [418600] global_step=418600, grad_norm=4.69690465927124, loss=0.5948981642723083
I0307 15:16:42.198389 139787612706560 logging_writer.py:48] [418700] global_step=418700, grad_norm=4.405786514282227, loss=0.6221178770065308
I0307 15:17:15.869354 139787629491968 logging_writer.py:48] [418800] global_step=418800, grad_norm=4.524270057678223, loss=0.6001874208450317
I0307 15:17:49.523116 139787612706560 logging_writer.py:48] [418900] global_step=418900, grad_norm=4.308488368988037, loss=0.6037223935127258
I0307 15:18:23.197807 139787629491968 logging_writer.py:48] [419000] global_step=419000, grad_norm=4.634725570678711, loss=0.6571304202079773
I0307 15:18:56.826185 139787612706560 logging_writer.py:48] [419100] global_step=419100, grad_norm=4.49369478225708, loss=0.5997813940048218
I0307 15:19:30.471304 139787629491968 logging_writer.py:48] [419200] global_step=419200, grad_norm=4.620397090911865, loss=0.615068793296814
I0307 15:20:04.091106 139787612706560 logging_writer.py:48] [419300] global_step=419300, grad_norm=4.605072498321533, loss=0.6223742961883545
I0307 15:20:37.748256 139787629491968 logging_writer.py:48] [419400] global_step=419400, grad_norm=4.562788963317871, loss=0.6028318405151367
I0307 15:21:11.411759 139787612706560 logging_writer.py:48] [419500] global_step=419500, grad_norm=4.587456703186035, loss=0.6448358297348022
I0307 15:21:45.011647 139787629491968 logging_writer.py:48] [419600] global_step=419600, grad_norm=4.393550872802734, loss=0.6205377578735352
I0307 15:22:18.599009 139787612706560 logging_writer.py:48] [419700] global_step=419700, grad_norm=4.87784481048584, loss=0.6192554235458374
I0307 15:22:52.238675 139787629491968 logging_writer.py:48] [419800] global_step=419800, grad_norm=4.900110721588135, loss=0.6668511033058167
I0307 15:23:25.840894 139787612706560 logging_writer.py:48] [419900] global_step=419900, grad_norm=4.527774810791016, loss=0.642288863658905
I0307 15:23:59.501762 139787629491968 logging_writer.py:48] [420000] global_step=420000, grad_norm=4.2343621253967285, loss=0.5973175764083862
I0307 15:24:18.474054 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:24:24.522817 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:24:33.158606 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:24:35.386218 139951089751872 submission_runner.py:411] Time since start: 146278.60s, 	Step: 420058, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.14487868547439575, 'validation/accuracy': 0.7575799822807312, 'validation/loss': 1.0407829284667969, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8214672803878784, 'test/num_examples': 10000, 'score': 141331.67381334305, 'total_duration': 146278.59851312637, 'accumulated_submission_time': 141331.67381334305, 'accumulated_eval_time': 4910.61313700676, 'accumulated_logging_time': 20.978939056396484}
I0307 15:24:35.474834 139789013600000 logging_writer.py:48] [420058] accumulated_eval_time=4910.613137, accumulated_logging_time=20.978939, accumulated_submission_time=141331.673813, global_step=420058, preemption_count=0, score=141331.673813, test/accuracy=0.631400, test/loss=1.821467, test/num_examples=10000, total_duration=146278.598513, train/accuracy=0.961296, train/loss=0.144879, validation/accuracy=0.757580, validation/loss=1.040783, validation/num_examples=50000
I0307 15:24:49.932787 139789021992704 logging_writer.py:48] [420100] global_step=420100, grad_norm=4.730856895446777, loss=0.5884279012680054
I0307 15:25:23.545477 139789013600000 logging_writer.py:48] [420200] global_step=420200, grad_norm=4.802448272705078, loss=0.7212542295455933
I0307 15:25:57.192576 139789021992704 logging_writer.py:48] [420300] global_step=420300, grad_norm=4.446834564208984, loss=0.6456722021102905
I0307 15:26:30.819016 139789013600000 logging_writer.py:48] [420400] global_step=420400, grad_norm=4.944520473480225, loss=0.6108866333961487
I0307 15:27:04.466293 139789021992704 logging_writer.py:48] [420500] global_step=420500, grad_norm=4.660233974456787, loss=0.6263291835784912
I0307 15:27:38.097190 139789013600000 logging_writer.py:48] [420600] global_step=420600, grad_norm=4.736121654510498, loss=0.64902663230896
I0307 15:28:11.687510 139789021992704 logging_writer.py:48] [420700] global_step=420700, grad_norm=4.259200096130371, loss=0.6390313506126404
I0307 15:28:45.295922 139789013600000 logging_writer.py:48] [420800] global_step=420800, grad_norm=5.110647678375244, loss=0.6743437051773071
I0307 15:29:18.943174 139789021992704 logging_writer.py:48] [420900] global_step=420900, grad_norm=4.093512535095215, loss=0.5743862390518188
I0307 15:29:52.563167 139789013600000 logging_writer.py:48] [421000] global_step=421000, grad_norm=4.345672130584717, loss=0.5873807072639465
I0307 15:30:26.198290 139789021992704 logging_writer.py:48] [421100] global_step=421100, grad_norm=4.501672267913818, loss=0.595069408416748
I0307 15:30:59.834928 139789013600000 logging_writer.py:48] [421200] global_step=421200, grad_norm=4.647758483886719, loss=0.6352051496505737
I0307 15:31:33.481929 139789021992704 logging_writer.py:48] [421300] global_step=421300, grad_norm=4.752366065979004, loss=0.7314150333404541
I0307 15:32:07.119777 139789013600000 logging_writer.py:48] [421400] global_step=421400, grad_norm=4.730100154876709, loss=0.611915111541748
I0307 15:32:40.764892 139789021992704 logging_writer.py:48] [421500] global_step=421500, grad_norm=4.614541530609131, loss=0.6563528776168823
I0307 15:33:05.463737 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:33:11.752851 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:33:20.299216 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:33:22.569677 139951089751872 submission_runner.py:411] Time since start: 146805.78s, 	Step: 421575, 	{'train/accuracy': 0.9612364172935486, 'train/loss': 0.1430351883172989, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0402424335479736, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8217582702636719, 'test/num_examples': 10000, 'score': 141841.59809207916, 'total_duration': 146805.78198504448, 'accumulated_submission_time': 141841.59809207916, 'accumulated_eval_time': 4927.719024181366, 'accumulated_logging_time': 21.077617645263672}
I0307 15:33:22.659229 139787629491968 logging_writer.py:48] [421575] accumulated_eval_time=4927.719024, accumulated_logging_time=21.077618, accumulated_submission_time=141841.598092, global_step=421575, preemption_count=0, score=141841.598092, test/accuracy=0.632200, test/loss=1.821758, test/num_examples=10000, total_duration=146805.781985, train/accuracy=0.961236, train/loss=0.143035, validation/accuracy=0.757260, validation/loss=1.040242, validation/num_examples=50000
I0307 15:33:31.378137 139788996814592 logging_writer.py:48] [421600] global_step=421600, grad_norm=4.678322792053223, loss=0.6595561504364014
I0307 15:34:04.982357 139787629491968 logging_writer.py:48] [421700] global_step=421700, grad_norm=4.658395767211914, loss=0.5993280410766602
I0307 15:34:38.590045 139788996814592 logging_writer.py:48] [421800] global_step=421800, grad_norm=4.796326160430908, loss=0.616399884223938
I0307 15:35:12.253654 139787629491968 logging_writer.py:48] [421900] global_step=421900, grad_norm=4.27269983291626, loss=0.5585760474205017
I0307 15:35:45.896183 139788996814592 logging_writer.py:48] [422000] global_step=422000, grad_norm=4.9306721687316895, loss=0.598602831363678
I0307 15:36:19.578557 139787629491968 logging_writer.py:48] [422100] global_step=422100, grad_norm=4.714629650115967, loss=0.588762640953064
I0307 15:36:53.185564 139788996814592 logging_writer.py:48] [422200] global_step=422200, grad_norm=4.538824558258057, loss=0.5546716451644897
I0307 15:37:26.844476 139787629491968 logging_writer.py:48] [422300] global_step=422300, grad_norm=4.4174909591674805, loss=0.6193403601646423
I0307 15:38:00.454391 139788996814592 logging_writer.py:48] [422400] global_step=422400, grad_norm=4.362234115600586, loss=0.5600707530975342
I0307 15:38:34.100098 139787629491968 logging_writer.py:48] [422500] global_step=422500, grad_norm=4.343873977661133, loss=0.6536940336227417
I0307 15:39:07.704108 139788996814592 logging_writer.py:48] [422600] global_step=422600, grad_norm=4.491469860076904, loss=0.6980683207511902
I0307 15:39:41.344717 139787629491968 logging_writer.py:48] [422700] global_step=422700, grad_norm=4.482973575592041, loss=0.6064603924751282
I0307 15:40:14.914353 139788996814592 logging_writer.py:48] [422800] global_step=422800, grad_norm=4.311066627502441, loss=0.609592854976654
I0307 15:40:48.580237 139787629491968 logging_writer.py:48] [422900] global_step=422900, grad_norm=4.364717960357666, loss=0.6163864731788635
I0307 15:41:22.198626 139788996814592 logging_writer.py:48] [423000] global_step=423000, grad_norm=4.521206378936768, loss=0.661165714263916
I0307 15:41:52.632274 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:41:58.667311 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:42:07.424554 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:42:09.825039 139951089751872 submission_runner.py:411] Time since start: 147333.04s, 	Step: 423092, 	{'train/accuracy': 0.9618741869926453, 'train/loss': 0.14445745944976807, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0409984588623047, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8226169347763062, 'test/num_examples': 10000, 'score': 142351.5038971901, 'total_duration': 147333.03734254837, 'accumulated_submission_time': 142351.5038971901, 'accumulated_eval_time': 4944.911732673645, 'accumulated_logging_time': 21.177175760269165}
I0307 15:42:09.897249 139789030385408 logging_writer.py:48] [423092] accumulated_eval_time=4944.911733, accumulated_logging_time=21.177176, accumulated_submission_time=142351.503897, global_step=423092, preemption_count=0, score=142351.503897, test/accuracy=0.631300, test/loss=1.822617, test/num_examples=10000, total_duration=147333.037343, train/accuracy=0.961874, train/loss=0.144457, validation/accuracy=0.757300, validation/loss=1.040998, validation/num_examples=50000
I0307 15:42:12.928887 139789038778112 logging_writer.py:48] [423100] global_step=423100, grad_norm=4.662265300750732, loss=0.5966091156005859
I0307 15:42:46.624314 139789030385408 logging_writer.py:48] [423200] global_step=423200, grad_norm=4.586383819580078, loss=0.5872758626937866
I0307 15:43:20.272590 139789038778112 logging_writer.py:48] [423300] global_step=423300, grad_norm=4.349407196044922, loss=0.6220120787620544
I0307 15:43:53.881477 139789030385408 logging_writer.py:48] [423400] global_step=423400, grad_norm=4.618719577789307, loss=0.5537051558494568
I0307 15:44:27.452961 139789038778112 logging_writer.py:48] [423500] global_step=423500, grad_norm=4.321195602416992, loss=0.5870698690414429
I0307 15:45:01.076085 139789030385408 logging_writer.py:48] [423600] global_step=423600, grad_norm=4.7723493576049805, loss=0.601489782333374
I0307 15:45:34.764902 139789038778112 logging_writer.py:48] [423700] global_step=423700, grad_norm=4.115246295928955, loss=0.5823076963424683
I0307 15:46:08.401626 139789030385408 logging_writer.py:48] [423800] global_step=423800, grad_norm=4.2664947509765625, loss=0.6189667582511902
I0307 15:46:42.010525 139789038778112 logging_writer.py:48] [423900] global_step=423900, grad_norm=4.359692573547363, loss=0.5936840772628784
I0307 15:47:15.640901 139789030385408 logging_writer.py:48] [424000] global_step=424000, grad_norm=4.6448588371276855, loss=0.6104904413223267
I0307 15:47:49.275192 139789038778112 logging_writer.py:48] [424100] global_step=424100, grad_norm=4.451296806335449, loss=0.627149224281311
I0307 15:48:22.928538 139789030385408 logging_writer.py:48] [424200] global_step=424200, grad_norm=4.092846870422363, loss=0.5795643329620361
I0307 15:48:56.556017 139789038778112 logging_writer.py:48] [424300] global_step=424300, grad_norm=4.619817733764648, loss=0.669001579284668
I0307 15:49:30.203473 139789030385408 logging_writer.py:48] [424400] global_step=424400, grad_norm=4.6294846534729, loss=0.6750184297561646
I0307 15:50:03.852460 139789038778112 logging_writer.py:48] [424500] global_step=424500, grad_norm=4.776180744171143, loss=0.6443153023719788
I0307 15:50:37.543281 139789030385408 logging_writer.py:48] [424600] global_step=424600, grad_norm=4.400266647338867, loss=0.6585167050361633
I0307 15:50:40.050553 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:50:46.679179 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:50:55.340934 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:50:57.622611 139951089751872 submission_runner.py:411] Time since start: 147860.83s, 	Step: 424609, 	{'train/accuracy': 0.9593032598495483, 'train/loss': 0.14886368811130524, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0406074523925781, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8224475383758545, 'test/num_examples': 10000, 'score': 142861.59340810776, 'total_duration': 147860.83490467072, 'accumulated_submission_time': 142861.59340810776, 'accumulated_eval_time': 4962.4837255477905, 'accumulated_logging_time': 21.25806474685669}
I0307 15:50:57.713076 139787621099264 logging_writer.py:48] [424609] accumulated_eval_time=4962.483726, accumulated_logging_time=21.258065, accumulated_submission_time=142861.593408, global_step=424609, preemption_count=0, score=142861.593408, test/accuracy=0.631300, test/loss=1.822448, test/num_examples=10000, total_duration=147860.834905, train/accuracy=0.959303, train/loss=0.148864, validation/accuracy=0.757260, validation/loss=1.040607, validation/num_examples=50000
I0307 15:51:28.635283 139787629491968 logging_writer.py:48] [424700] global_step=424700, grad_norm=4.629787445068359, loss=0.6124752163887024
I0307 15:52:02.311930 139787621099264 logging_writer.py:48] [424800] global_step=424800, grad_norm=4.366952419281006, loss=0.6207103729248047
I0307 15:52:35.905171 139787629491968 logging_writer.py:48] [424900] global_step=424900, grad_norm=4.23174524307251, loss=0.6766681671142578
I0307 15:53:09.579224 139787621099264 logging_writer.py:48] [425000] global_step=425000, grad_norm=4.486720085144043, loss=0.5777861475944519
I0307 15:53:43.183568 139787629491968 logging_writer.py:48] [425100] global_step=425100, grad_norm=4.430984020233154, loss=0.6573741436004639
I0307 15:54:16.840327 139787621099264 logging_writer.py:48] [425200] global_step=425200, grad_norm=4.651086807250977, loss=0.6329992413520813
I0307 15:54:50.464562 139787629491968 logging_writer.py:48] [425300] global_step=425300, grad_norm=4.918461799621582, loss=0.5997844934463501
I0307 15:55:24.114402 139787621099264 logging_writer.py:48] [425400] global_step=425400, grad_norm=5.084062099456787, loss=0.7117087244987488
I0307 15:55:57.782973 139787629491968 logging_writer.py:48] [425500] global_step=425500, grad_norm=4.684403896331787, loss=0.7206865549087524
I0307 15:56:31.443230 139787621099264 logging_writer.py:48] [425600] global_step=425600, grad_norm=4.3798723220825195, loss=0.5663772821426392
I0307 15:57:05.111365 139787629491968 logging_writer.py:48] [425700] global_step=425700, grad_norm=4.42109489440918, loss=0.6691573858261108
I0307 15:57:38.832627 139787621099264 logging_writer.py:48] [425800] global_step=425800, grad_norm=4.554654598236084, loss=0.6057282090187073
I0307 15:58:12.395991 139787629491968 logging_writer.py:48] [425900] global_step=425900, grad_norm=4.2703375816345215, loss=0.623765230178833
I0307 15:58:46.062438 139787621099264 logging_writer.py:48] [426000] global_step=426000, grad_norm=4.422346591949463, loss=0.6191820502281189
I0307 15:59:19.668335 139787629491968 logging_writer.py:48] [426100] global_step=426100, grad_norm=4.192402362823486, loss=0.594642162322998
I0307 15:59:27.907184 139951089751872 spec.py:321] Evaluating on the training split.
I0307 15:59:34.009636 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 15:59:42.653048 139951089751872 spec.py:349] Evaluating on the test split.
I0307 15:59:44.912580 139951089751872 submission_runner.py:411] Time since start: 148388.12s, 	Step: 426126, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14820338785648346, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.040932536125183, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8229377269744873, 'test/num_examples': 10000, 'score': 143371.71987581253, 'total_duration': 148388.12489271164, 'accumulated_submission_time': 143371.71987581253, 'accumulated_eval_time': 4979.48906993866, 'accumulated_logging_time': 21.36016345024109}
I0307 15:59:45.000352 139787612706560 logging_writer.py:48] [426126] accumulated_eval_time=4979.489070, accumulated_logging_time=21.360163, accumulated_submission_time=143371.719876, global_step=426126, preemption_count=0, score=143371.719876, test/accuracy=0.631100, test/loss=1.822938, test/num_examples=10000, total_duration=148388.124893, train/accuracy=0.960061, train/loss=0.148203, validation/accuracy=0.757360, validation/loss=1.040933, validation/num_examples=50000
I0307 16:00:10.171288 139787621099264 logging_writer.py:48] [426200] global_step=426200, grad_norm=4.054588794708252, loss=0.5992891788482666
I0307 16:00:43.739495 139787612706560 logging_writer.py:48] [426300] global_step=426300, grad_norm=4.245805740356445, loss=0.5662436485290527
I0307 16:01:17.330125 139787621099264 logging_writer.py:48] [426400] global_step=426400, grad_norm=4.487738609313965, loss=0.580107569694519
I0307 16:01:50.930471 139787612706560 logging_writer.py:48] [426500] global_step=426500, grad_norm=4.575146198272705, loss=0.666067361831665
I0307 16:02:24.522206 139787621099264 logging_writer.py:48] [426600] global_step=426600, grad_norm=4.584145545959473, loss=0.6588801741600037
I0307 16:02:58.161614 139787612706560 logging_writer.py:48] [426700] global_step=426700, grad_norm=4.789443016052246, loss=0.6472494006156921
I0307 16:03:31.786612 139787621099264 logging_writer.py:48] [426800] global_step=426800, grad_norm=4.746758937835693, loss=0.6672478914260864
I0307 16:04:05.438213 139787612706560 logging_writer.py:48] [426900] global_step=426900, grad_norm=5.028616428375244, loss=0.6812608242034912
I0307 16:04:39.053715 139787621099264 logging_writer.py:48] [427000] global_step=427000, grad_norm=4.208926677703857, loss=0.630836009979248
I0307 16:05:12.696477 139787612706560 logging_writer.py:48] [427100] global_step=427100, grad_norm=4.558252334594727, loss=0.6097266674041748
I0307 16:05:46.316621 139787621099264 logging_writer.py:48] [427200] global_step=427200, grad_norm=5.313002109527588, loss=0.6594875454902649
I0307 16:06:19.967054 139787612706560 logging_writer.py:48] [427300] global_step=427300, grad_norm=4.592819690704346, loss=0.6826602816581726
I0307 16:06:53.591467 139787621099264 logging_writer.py:48] [427400] global_step=427400, grad_norm=4.480214595794678, loss=0.5452044010162354
I0307 16:07:27.245197 139787612706560 logging_writer.py:48] [427500] global_step=427500, grad_norm=4.736193656921387, loss=0.6302176713943481
I0307 16:08:00.847157 139787621099264 logging_writer.py:48] [427600] global_step=427600, grad_norm=5.304643154144287, loss=0.6630753874778748
I0307 16:08:15.130855 139951089751872 spec.py:321] Evaluating on the training split.
I0307 16:08:21.222205 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 16:08:29.712846 139951089751872 spec.py:349] Evaluating on the test split.
I0307 16:08:32.036084 139951089751872 submission_runner.py:411] Time since start: 148915.25s, 	Step: 427644, 	{'train/accuracy': 0.9620934128761292, 'train/loss': 0.14349594712257385, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0401928424835205, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.820892095565796, 'test/num_examples': 10000, 'score': 143881.7842924595, 'total_duration': 148915.24838876724, 'accumulated_submission_time': 143881.7842924595, 'accumulated_eval_time': 4996.394257545471, 'accumulated_logging_time': 21.45875644683838}
I0307 16:08:32.127834 139789021992704 logging_writer.py:48] [427644] accumulated_eval_time=4996.394258, accumulated_logging_time=21.458756, accumulated_submission_time=143881.784292, global_step=427644, preemption_count=0, score=143881.784292, test/accuracy=0.631600, test/loss=1.820892, test/num_examples=10000, total_duration=148915.248389, train/accuracy=0.962093, train/loss=0.143496, validation/accuracy=0.757200, validation/loss=1.040193, validation/num_examples=50000
I0307 16:08:51.321814 139789030385408 logging_writer.py:48] [427700] global_step=427700, grad_norm=4.3939528465271, loss=0.6352298259735107
I0307 16:09:24.956376 139789021992704 logging_writer.py:48] [427800] global_step=427800, grad_norm=4.627760887145996, loss=0.6550132036209106
I0307 16:09:58.611707 139789030385408 logging_writer.py:48] [427900] global_step=427900, grad_norm=4.746015548706055, loss=0.6389985084533691
I0307 16:10:32.280135 139789021992704 logging_writer.py:48] [428000] global_step=428000, grad_norm=4.232754230499268, loss=0.6097135543823242
I0307 16:11:05.839811 139789030385408 logging_writer.py:48] [428100] global_step=428100, grad_norm=4.522305488586426, loss=0.6034149527549744
I0307 16:11:39.685203 139789021992704 logging_writer.py:48] [428200] global_step=428200, grad_norm=4.488326549530029, loss=0.6592149138450623
I0307 16:12:13.343974 139789030385408 logging_writer.py:48] [428300] global_step=428300, grad_norm=4.2097907066345215, loss=0.6201900839805603
I0307 16:12:47.007727 139789021992704 logging_writer.py:48] [428400] global_step=428400, grad_norm=4.487483024597168, loss=0.7157045602798462
I0307 16:13:20.609127 139789030385408 logging_writer.py:48] [428500] global_step=428500, grad_norm=4.234097957611084, loss=0.6014425754547119
I0307 16:13:54.226418 139789021992704 logging_writer.py:48] [428600] global_step=428600, grad_norm=4.188403606414795, loss=0.6150665879249573
I0307 16:14:27.892228 139789030385408 logging_writer.py:48] [428700] global_step=428700, grad_norm=4.671951770782471, loss=0.6322016716003418
I0307 16:15:01.514549 139789021992704 logging_writer.py:48] [428800] global_step=428800, grad_norm=4.3778510093688965, loss=0.6029565334320068
I0307 16:15:35.175855 139789030385408 logging_writer.py:48] [428900] global_step=428900, grad_norm=5.110056400299072, loss=0.6555896401405334
I0307 16:16:08.819057 139789021992704 logging_writer.py:48] [429000] global_step=429000, grad_norm=4.511407375335693, loss=0.5921801328659058
I0307 16:16:42.411249 139789030385408 logging_writer.py:48] [429100] global_step=429100, grad_norm=4.3850998878479, loss=0.5776143670082092
I0307 16:17:02.051778 139951089751872 spec.py:321] Evaluating on the training split.
I0307 16:17:08.087378 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 16:17:16.689731 139951089751872 spec.py:349] Evaluating on the test split.
I0307 16:17:18.969903 139951089751872 submission_runner.py:411] Time since start: 149442.18s, 	Step: 429160, 	{'train/accuracy': 0.9617147445678711, 'train/loss': 0.14298078417778015, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.041298508644104, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8239283561706543, 'test/num_examples': 10000, 'score': 144391.64415311813, 'total_duration': 149442.18219542503, 'accumulated_submission_time': 144391.64415311813, 'accumulated_eval_time': 5013.312318086624, 'accumulated_logging_time': 21.560152769088745}
I0307 16:17:19.077959 139789005207296 logging_writer.py:48] [429160] accumulated_eval_time=5013.312318, accumulated_logging_time=21.560153, accumulated_submission_time=144391.644153, global_step=429160, preemption_count=0, score=144391.644153, test/accuracy=0.630600, test/loss=1.823928, test/num_examples=10000, total_duration=149442.182195, train/accuracy=0.961715, train/loss=0.142981, validation/accuracy=0.757160, validation/loss=1.041299, validation/num_examples=50000
I0307 16:17:32.853207 139789013600000 logging_writer.py:48] [429200] global_step=429200, grad_norm=4.276082515716553, loss=0.596940279006958
I0307 16:18:06.431975 139789005207296 logging_writer.py:48] [429300] global_step=429300, grad_norm=4.426738739013672, loss=0.6021661162376404
I0307 16:18:40.061439 139789013600000 logging_writer.py:48] [429400] global_step=429400, grad_norm=4.567773818969727, loss=0.6597359776496887
I0307 16:19:13.714146 139789005207296 logging_writer.py:48] [429500] global_step=429500, grad_norm=4.862940788269043, loss=0.6974322199821472
I0307 16:19:47.399211 139789013600000 logging_writer.py:48] [429600] global_step=429600, grad_norm=4.4072957038879395, loss=0.6047071814537048
I0307 16:20:21.043077 139789005207296 logging_writer.py:48] [429700] global_step=429700, grad_norm=4.195928573608398, loss=0.5519317984580994
I0307 16:20:54.688621 139789013600000 logging_writer.py:48] [429800] global_step=429800, grad_norm=4.9958176612854, loss=0.6157139539718628
I0307 16:21:28.338282 139789005207296 logging_writer.py:48] [429900] global_step=429900, grad_norm=5.1703267097473145, loss=0.7348857522010803
I0307 16:22:01.993353 139789013600000 logging_writer.py:48] [430000] global_step=430000, grad_norm=4.7952494621276855, loss=0.6435258388519287
I0307 16:22:35.594179 139789005207296 logging_writer.py:48] [430100] global_step=430100, grad_norm=4.442286968231201, loss=0.5908880829811096
I0307 16:23:09.213698 139789013600000 logging_writer.py:48] [430200] global_step=430200, grad_norm=4.172457218170166, loss=0.637837290763855
I0307 16:23:42.827569 139789005207296 logging_writer.py:48] [430300] global_step=430300, grad_norm=4.850583076477051, loss=0.6467639207839966
I0307 16:24:16.479011 139789013600000 logging_writer.py:48] [430400] global_step=430400, grad_norm=4.111112594604492, loss=0.6112851500511169
I0307 16:24:50.093736 139789005207296 logging_writer.py:48] [430500] global_step=430500, grad_norm=4.153054237365723, loss=0.5583067536354065
I0307 16:25:23.739357 139789013600000 logging_writer.py:48] [430600] global_step=430600, grad_norm=4.235987186431885, loss=0.6676136255264282
I0307 16:25:49.101067 139951089751872 spec.py:321] Evaluating on the training split.
I0307 16:25:55.131687 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 16:26:03.690567 139951089751872 spec.py:349] Evaluating on the test split.
I0307 16:26:05.967069 139951089751872 submission_runner.py:411] Time since start: 149969.18s, 	Step: 430677, 	{'train/accuracy': 0.9617745280265808, 'train/loss': 0.1452418863773346, 'validation/accuracy': 0.757420003414154, 'validation/loss': 1.0418684482574463, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8239145278930664, 'test/num_examples': 10000, 'score': 144901.60076332092, 'total_duration': 149969.17936348915, 'accumulated_submission_time': 144901.60076332092, 'accumulated_eval_time': 5030.17825627327, 'accumulated_logging_time': 21.679065942764282}
I0307 16:26:06.058420 139787621099264 logging_writer.py:48] [430677] accumulated_eval_time=5030.178256, accumulated_logging_time=21.679066, accumulated_submission_time=144901.600763, global_step=430677, preemption_count=0, score=144901.600763, test/accuracy=0.631200, test/loss=1.823915, test/num_examples=10000, total_duration=149969.179363, train/accuracy=0.961775, train/loss=0.145242, validation/accuracy=0.757420, validation/loss=1.041868, validation/num_examples=50000
I0307 16:26:14.126685 139787629491968 logging_writer.py:48] [430700] global_step=430700, grad_norm=4.166872978210449, loss=0.6517195701599121
I0307 16:26:47.785293 139787621099264 logging_writer.py:48] [430800] global_step=430800, grad_norm=4.576183795928955, loss=0.6432332992553711
I0307 16:27:21.407312 139787629491968 logging_writer.py:48] [430900] global_step=430900, grad_norm=4.451288223266602, loss=0.5991525650024414
I0307 16:27:55.069641 139787621099264 logging_writer.py:48] [431000] global_step=431000, grad_norm=5.022737979888916, loss=0.622577965259552
I0307 16:28:28.745012 139787629491968 logging_writer.py:48] [431100] global_step=431100, grad_norm=4.5699992179870605, loss=0.6605961322784424
I0307 16:29:02.404825 139787621099264 logging_writer.py:48] [431200] global_step=431200, grad_norm=4.550466060638428, loss=0.6327865123748779
I0307 16:29:36.018846 139787629491968 logging_writer.py:48] [431300] global_step=431300, grad_norm=4.656540870666504, loss=0.6434593200683594
I0307 16:30:09.686664 139787621099264 logging_writer.py:48] [431400] global_step=431400, grad_norm=4.649921417236328, loss=0.6106981039047241
I0307 16:30:43.318554 139787629491968 logging_writer.py:48] [431500] global_step=431500, grad_norm=4.341569900512695, loss=0.577415406703949
I0307 16:31:16.994671 139787621099264 logging_writer.py:48] [431600] global_step=431600, grad_norm=4.747218608856201, loss=0.5879801511764526
I0307 16:31:50.597897 139787629491968 logging_writer.py:48] [431700] global_step=431700, grad_norm=4.707159519195557, loss=0.6884613633155823
I0307 16:32:24.264650 139787621099264 logging_writer.py:48] [431800] global_step=431800, grad_norm=4.216926574707031, loss=0.5705060362815857
I0307 16:32:57.903960 139787629491968 logging_writer.py:48] [431900] global_step=431900, grad_norm=4.838864326477051, loss=0.6571437120437622
I0307 16:33:31.553746 139787621099264 logging_writer.py:48] [432000] global_step=432000, grad_norm=4.755613327026367, loss=0.6145930290222168
I0307 16:34:05.187754 139787629491968 logging_writer.py:48] [432100] global_step=432100, grad_norm=4.718991279602051, loss=0.6615644693374634
I0307 16:34:36.121909 139951089751872 spec.py:321] Evaluating on the training split.
I0307 16:34:42.238319 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 16:34:50.812016 139951089751872 spec.py:349] Evaluating on the test split.
I0307 16:34:53.049425 139951089751872 submission_runner.py:411] Time since start: 150496.26s, 	Step: 432193, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14882323145866394, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.041712999343872, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8241660594940186, 'test/num_examples': 10000, 'score': 145411.59765625, 'total_duration': 150496.2617239952, 'accumulated_submission_time': 145411.59765625, 'accumulated_eval_time': 5047.105713605881, 'accumulated_logging_time': 21.780495166778564}
I0307 16:34:53.137946 139789030385408 logging_writer.py:48] [432193] accumulated_eval_time=5047.105714, accumulated_logging_time=21.780495, accumulated_submission_time=145411.597656, global_step=432193, preemption_count=0, score=145411.597656, test/accuracy=0.632100, test/loss=1.824166, test/num_examples=10000, total_duration=150496.261724, train/accuracy=0.960200, train/loss=0.148823, validation/accuracy=0.757500, validation/loss=1.041713, validation/num_examples=50000
I0307 16:34:55.820641 139789038778112 logging_writer.py:48] [432200] global_step=432200, grad_norm=3.9558043479919434, loss=0.5577862858772278
I0307 16:35:29.405762 139789030385408 logging_writer.py:48] [432300] global_step=432300, grad_norm=4.614846706390381, loss=0.6275259852409363
I0307 16:36:03.012129 139789038778112 logging_writer.py:48] [432400] global_step=432400, grad_norm=4.001436233520508, loss=0.5980455279350281
I0307 16:36:36.655472 139789030385408 logging_writer.py:48] [432500] global_step=432500, grad_norm=4.850667476654053, loss=0.664136528968811
I0307 16:37:10.274502 139789038778112 logging_writer.py:48] [432600] global_step=432600, grad_norm=4.105273723602295, loss=0.5481903553009033
I0307 16:37:43.923196 139789030385408 logging_writer.py:48] [432700] global_step=432700, grad_norm=4.655607223510742, loss=0.661210834980011
I0307 16:38:17.562634 139789038778112 logging_writer.py:48] [432800] global_step=432800, grad_norm=4.403717041015625, loss=0.646047830581665
I0307 16:38:51.193445 139789030385408 logging_writer.py:48] [432900] global_step=432900, grad_norm=4.898983955383301, loss=0.6090715527534485
I0307 16:39:24.808608 139789038778112 logging_writer.py:48] [433000] global_step=433000, grad_norm=4.318931579589844, loss=0.6352313160896301
I0307 16:39:58.467615 139789030385408 logging_writer.py:48] [433100] global_step=433100, grad_norm=4.683748245239258, loss=0.6307885050773621
I0307 16:40:32.194474 139789038778112 logging_writer.py:48] [433200] global_step=433200, grad_norm=4.77891731262207, loss=0.65364670753479
I0307 16:41:05.890486 139789030385408 logging_writer.py:48] [433300] global_step=433300, grad_norm=4.501499652862549, loss=0.6576379537582397
I0307 16:41:39.567303 139789038778112 logging_writer.py:48] [433400] global_step=433400, grad_norm=4.944518089294434, loss=0.695520281791687
I0307 16:42:13.256508 139789030385408 logging_writer.py:48] [433500] global_step=433500, grad_norm=4.447420120239258, loss=0.6677737832069397
I0307 16:42:46.902233 139789038778112 logging_writer.py:48] [433600] global_step=433600, grad_norm=4.660771369934082, loss=0.5571090579032898
I0307 16:43:20.591530 139789030385408 logging_writer.py:48] [433700] global_step=433700, grad_norm=4.596911907196045, loss=0.603736162185669
I0307 16:43:23.088187 139951089751872 spec.py:321] Evaluating on the training split.
I0307 16:43:29.183669 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 16:43:37.634300 139951089751872 spec.py:349] Evaluating on the test split.
I0307 16:43:39.892697 139951089751872 submission_runner.py:411] Time since start: 151023.11s, 	Step: 433709, 	{'train/accuracy': 0.9606783986091614, 'train/loss': 0.1471301168203354, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.041322112083435, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.823303461074829, 'test/num_examples': 10000, 'score': 145921.48075079918, 'total_duration': 151023.10500741005, 'accumulated_submission_time': 145921.48075079918, 'accumulated_eval_time': 5063.910180091858, 'accumulated_logging_time': 21.878495693206787}
I0307 16:43:39.981247 139787612706560 logging_writer.py:48] [433709] accumulated_eval_time=5063.910180, accumulated_logging_time=21.878496, accumulated_submission_time=145921.480751, global_step=433709, preemption_count=0, score=145921.480751, test/accuracy=0.632000, test/loss=1.823303, test/num_examples=10000, total_duration=151023.105007, train/accuracy=0.960678, train/loss=0.147130, validation/accuracy=0.757340, validation/loss=1.041322, validation/num_examples=50000
I0307 16:44:10.878867 139787621099264 logging_writer.py:48] [433800] global_step=433800, grad_norm=4.491025924682617, loss=0.6284040212631226
I0307 16:44:44.460650 139787612706560 logging_writer.py:48] [433900] global_step=433900, grad_norm=4.783045768737793, loss=0.6457555294036865
I0307 16:45:18.079452 139787621099264 logging_writer.py:48] [434000] global_step=434000, grad_norm=4.562579154968262, loss=0.6297727823257446
I0307 16:45:51.752526 139787612706560 logging_writer.py:48] [434100] global_step=434100, grad_norm=4.540247917175293, loss=0.5929247736930847
I0307 16:46:25.394659 139787621099264 logging_writer.py:48] [434200] global_step=434200, grad_norm=4.065049648284912, loss=0.5589428544044495
I0307 16:46:59.040572 139787612706560 logging_writer.py:48] [434300] global_step=434300, grad_norm=4.7667388916015625, loss=0.6858487129211426
I0307 16:47:32.662441 139787621099264 logging_writer.py:48] [434400] global_step=434400, grad_norm=4.41012716293335, loss=0.6068249344825745
I0307 16:48:06.325139 139787612706560 logging_writer.py:48] [434500] global_step=434500, grad_norm=4.522061347961426, loss=0.5776153802871704
I0307 16:48:39.978807 139787621099264 logging_writer.py:48] [434600] global_step=434600, grad_norm=4.20412540435791, loss=0.6009472608566284
I0307 16:49:13.623596 139787612706560 logging_writer.py:48] [434700] global_step=434700, grad_norm=4.424798965454102, loss=0.6518455743789673
I0307 16:49:47.255451 139787621099264 logging_writer.py:48] [434800] global_step=434800, grad_norm=4.468343257904053, loss=0.7158277034759521
I0307 16:50:20.880311 139787612706560 logging_writer.py:48] [434900] global_step=434900, grad_norm=4.950246810913086, loss=0.6485874652862549
I0307 16:50:54.502323 139787621099264 logging_writer.py:48] [435000] global_step=435000, grad_norm=4.501018524169922, loss=0.5841817855834961
I0307 16:51:28.140852 139787612706560 logging_writer.py:48] [435100] global_step=435100, grad_norm=4.575094223022461, loss=0.6866968870162964
I0307 16:52:01.762500 139787621099264 logging_writer.py:48] [435200] global_step=435200, grad_norm=4.422389984130859, loss=0.6916857957839966
I0307 16:52:09.991878 139951089751872 spec.py:321] Evaluating on the training split.
I0307 16:52:15.996511 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 16:52:24.468193 139951089751872 spec.py:349] Evaluating on the test split.
I0307 16:52:26.724715 139951089751872 submission_runner.py:411] Time since start: 151549.94s, 	Step: 435226, 	{'train/accuracy': 0.9610171914100647, 'train/loss': 0.14631707966327667, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.0406198501586914, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8207440376281738, 'test/num_examples': 10000, 'score': 146431.42701363564, 'total_duration': 151549.93702721596, 'accumulated_submission_time': 146431.42701363564, 'accumulated_eval_time': 5080.6429715156555, 'accumulated_logging_time': 21.97658133506775}
I0307 16:52:26.817243 139787621099264 logging_writer.py:48] [435226] accumulated_eval_time=5080.642972, accumulated_logging_time=21.976581, accumulated_submission_time=146431.427014, global_step=435226, preemption_count=0, score=146431.427014, test/accuracy=0.631600, test/loss=1.820744, test/num_examples=10000, total_duration=151549.937027, train/accuracy=0.961017, train/loss=0.146317, validation/accuracy=0.757100, validation/loss=1.040620, validation/num_examples=50000
I0307 16:52:52.055577 139789021992704 logging_writer.py:48] [435300] global_step=435300, grad_norm=4.976008892059326, loss=0.6905776262283325
I0307 16:53:25.630416 139787621099264 logging_writer.py:48] [435400] global_step=435400, grad_norm=4.286795616149902, loss=0.6364736557006836
I0307 16:53:59.182757 139789021992704 logging_writer.py:48] [435500] global_step=435500, grad_norm=4.573518753051758, loss=0.6155864000320435
I0307 16:54:32.846375 139787621099264 logging_writer.py:48] [435600] global_step=435600, grad_norm=4.557376861572266, loss=0.6881301403045654
I0307 16:55:06.490268 139789021992704 logging_writer.py:48] [435700] global_step=435700, grad_norm=4.692723274230957, loss=0.7176393270492554
I0307 16:55:40.137230 139787621099264 logging_writer.py:48] [435800] global_step=435800, grad_norm=4.348396301269531, loss=0.6308131814002991
I0307 16:56:13.741919 139789021992704 logging_writer.py:48] [435900] global_step=435900, grad_norm=4.0916972160339355, loss=0.5361412167549133
I0307 16:56:47.386746 139787621099264 logging_writer.py:48] [436000] global_step=436000, grad_norm=4.265120029449463, loss=0.5635659098625183
I0307 16:57:21.013186 139789021992704 logging_writer.py:48] [436100] global_step=436100, grad_norm=4.462496280670166, loss=0.5672656297683716
I0307 16:57:54.662540 139787621099264 logging_writer.py:48] [436200] global_step=436200, grad_norm=4.274846076965332, loss=0.641445517539978
I0307 16:58:28.271887 139789021992704 logging_writer.py:48] [436300] global_step=436300, grad_norm=4.526817321777344, loss=0.6273935437202454
I0307 16:59:01.940579 139787621099264 logging_writer.py:48] [436400] global_step=436400, grad_norm=4.90893030166626, loss=0.6407095193862915
I0307 16:59:35.492343 139789021992704 logging_writer.py:48] [436500] global_step=436500, grad_norm=4.375667572021484, loss=0.5765597224235535
I0307 17:00:09.161008 139787621099264 logging_writer.py:48] [436600] global_step=436600, grad_norm=4.618273735046387, loss=0.6235454678535461
I0307 17:00:42.773262 139789021992704 logging_writer.py:48] [436700] global_step=436700, grad_norm=4.31930685043335, loss=0.6060112714767456
I0307 17:00:57.046620 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:01:03.179116 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:01:11.753969 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:01:14.019510 139951089751872 submission_runner.py:411] Time since start: 152077.23s, 	Step: 436744, 	{'train/accuracy': 0.9606584906578064, 'train/loss': 0.14500433206558228, 'validation/accuracy': 0.75764000415802, 'validation/loss': 1.0415468215942383, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.82472825050354, 'test/num_examples': 10000, 'score': 146941.59220194817, 'total_duration': 152077.231808424, 'accumulated_submission_time': 146941.59220194817, 'accumulated_eval_time': 5097.61579823494, 'accumulated_logging_time': 22.0786395072937}
I0307 17:01:14.109371 139787629491968 logging_writer.py:48] [436744] accumulated_eval_time=5097.615798, accumulated_logging_time=22.078640, accumulated_submission_time=146941.592202, global_step=436744, preemption_count=0, score=146941.592202, test/accuracy=0.632100, test/loss=1.824728, test/num_examples=10000, total_duration=152077.231808, train/accuracy=0.960658, train/loss=0.145004, validation/accuracy=0.757640, validation/loss=1.041547, validation/num_examples=50000
I0307 17:01:33.252099 139788996814592 logging_writer.py:48] [436800] global_step=436800, grad_norm=4.562328815460205, loss=0.5845908522605896
I0307 17:02:06.840938 139787629491968 logging_writer.py:48] [436900] global_step=436900, grad_norm=4.4341325759887695, loss=0.6727501153945923
I0307 17:02:40.436419 139788996814592 logging_writer.py:48] [437000] global_step=437000, grad_norm=4.241461753845215, loss=0.5875203609466553
I0307 17:03:14.048605 139787629491968 logging_writer.py:48] [437100] global_step=437100, grad_norm=4.27366304397583, loss=0.5537605881690979
I0307 17:03:47.697345 139788996814592 logging_writer.py:48] [437200] global_step=437200, grad_norm=4.43499231338501, loss=0.6336439847946167
I0307 17:04:21.296617 139787629491968 logging_writer.py:48] [437300] global_step=437300, grad_norm=4.6869988441467285, loss=0.6257899403572083
I0307 17:04:54.940553 139788996814592 logging_writer.py:48] [437400] global_step=437400, grad_norm=4.874443531036377, loss=0.693956732749939
I0307 17:05:28.593576 139787629491968 logging_writer.py:48] [437500] global_step=437500, grad_norm=4.634329319000244, loss=0.6918883323669434
I0307 17:06:02.207045 139788996814592 logging_writer.py:48] [437600] global_step=437600, grad_norm=4.592390537261963, loss=0.6449297070503235
I0307 17:06:35.921173 139787629491968 logging_writer.py:48] [437700] global_step=437700, grad_norm=5.218048572540283, loss=0.7494047284126282
I0307 17:07:09.583877 139788996814592 logging_writer.py:48] [437800] global_step=437800, grad_norm=4.102544784545898, loss=0.5660801529884338
I0307 17:07:43.244231 139787629491968 logging_writer.py:48] [437900] global_step=437900, grad_norm=4.748300075531006, loss=0.6835789680480957
I0307 17:08:16.911682 139788996814592 logging_writer.py:48] [438000] global_step=438000, grad_norm=4.355910778045654, loss=0.6017536520957947
I0307 17:08:50.547312 139787629491968 logging_writer.py:48] [438100] global_step=438100, grad_norm=4.184146404266357, loss=0.583122968673706
I0307 17:09:24.189903 139788996814592 logging_writer.py:48] [438200] global_step=438200, grad_norm=4.436940670013428, loss=0.6542791128158569
I0307 17:09:44.167675 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:09:50.228993 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:09:58.922815 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:10:01.192766 139951089751872 submission_runner.py:411] Time since start: 152604.41s, 	Step: 438261, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14607051014900208, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0408618450164795, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8233457803726196, 'test/num_examples': 10000, 'score': 147451.58518648148, 'total_duration': 152604.4050400257, 'accumulated_submission_time': 147451.58518648148, 'accumulated_eval_time': 5114.640841245651, 'accumulated_logging_time': 22.17793607711792}
I0307 17:10:01.278592 139789030385408 logging_writer.py:48] [438261] accumulated_eval_time=5114.640841, accumulated_logging_time=22.177936, accumulated_submission_time=147451.585186, global_step=438261, preemption_count=0, score=147451.585186, test/accuracy=0.631800, test/loss=1.823346, test/num_examples=10000, total_duration=152604.405040, train/accuracy=0.961217, train/loss=0.146071, validation/accuracy=0.757200, validation/loss=1.040862, validation/num_examples=50000
I0307 17:10:14.710782 139789038778112 logging_writer.py:48] [438300] global_step=438300, grad_norm=4.438785076141357, loss=0.6272412538528442
I0307 17:10:48.294245 139789030385408 logging_writer.py:48] [438400] global_step=438400, grad_norm=4.85000467300415, loss=0.6839063167572021
I0307 17:11:21.936577 139789038778112 logging_writer.py:48] [438500] global_step=438500, grad_norm=4.484724044799805, loss=0.5951794385910034
I0307 17:11:55.605168 139789030385408 logging_writer.py:48] [438600] global_step=438600, grad_norm=4.418792724609375, loss=0.6573334336280823
I0307 17:12:29.255528 139789038778112 logging_writer.py:48] [438700] global_step=438700, grad_norm=4.46613883972168, loss=0.6430334448814392
I0307 17:13:02.925442 139789030385408 logging_writer.py:48] [438800] global_step=438800, grad_norm=4.654177665710449, loss=0.6169202327728271
I0307 17:13:36.611018 139789038778112 logging_writer.py:48] [438900] global_step=438900, grad_norm=4.477466583251953, loss=0.6400498151779175
I0307 17:14:10.264538 139789030385408 logging_writer.py:48] [439000] global_step=439000, grad_norm=4.352038860321045, loss=0.5970186591148376
I0307 17:14:43.932114 139789038778112 logging_writer.py:48] [439100] global_step=439100, grad_norm=4.136493682861328, loss=0.560170590877533
I0307 17:15:17.591893 139789030385408 logging_writer.py:48] [439200] global_step=439200, grad_norm=4.541018009185791, loss=0.646695613861084
I0307 17:15:51.247534 139789038778112 logging_writer.py:48] [439300] global_step=439300, grad_norm=4.834970474243164, loss=0.6379435658454895
I0307 17:16:24.906322 139789030385408 logging_writer.py:48] [439400] global_step=439400, grad_norm=5.539187431335449, loss=0.7249577045440674
I0307 17:16:58.558740 139789038778112 logging_writer.py:48] [439500] global_step=439500, grad_norm=4.562662124633789, loss=0.6431633830070496
I0307 17:17:32.227874 139789030385408 logging_writer.py:48] [439600] global_step=439600, grad_norm=4.781205177307129, loss=0.6524942517280579
I0307 17:18:05.820257 139789038778112 logging_writer.py:48] [439700] global_step=439700, grad_norm=4.449562072753906, loss=0.7108740210533142
I0307 17:18:31.506152 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:18:37.537780 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:18:46.106916 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:18:48.350478 139951089751872 submission_runner.py:411] Time since start: 153131.56s, 	Step: 439778, 	{'train/accuracy': 0.9588249325752258, 'train/loss': 0.15116292238235474, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0402222871780396, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8219047784805298, 'test/num_examples': 10000, 'score': 147961.7461400032, 'total_duration': 153131.56278252602, 'accumulated_submission_time': 147961.7461400032, 'accumulated_eval_time': 5131.485112667084, 'accumulated_logging_time': 22.27350926399231}
I0307 17:18:48.446107 139787612706560 logging_writer.py:48] [439778] accumulated_eval_time=5131.485113, accumulated_logging_time=22.273509, accumulated_submission_time=147961.746140, global_step=439778, preemption_count=0, score=147961.746140, test/accuracy=0.631400, test/loss=1.821905, test/num_examples=10000, total_duration=153131.562783, train/accuracy=0.958825, train/loss=0.151163, validation/accuracy=0.757340, validation/loss=1.040222, validation/num_examples=50000
I0307 17:18:56.161170 139787629491968 logging_writer.py:48] [439800] global_step=439800, grad_norm=4.701836109161377, loss=0.6458101868629456
I0307 17:19:29.738918 139787612706560 logging_writer.py:48] [439900] global_step=439900, grad_norm=4.859853267669678, loss=0.641305685043335
I0307 17:20:03.315809 139787629491968 logging_writer.py:48] [440000] global_step=440000, grad_norm=4.10747766494751, loss=0.5244526863098145
I0307 17:20:36.891457 139787612706560 logging_writer.py:48] [440100] global_step=440100, grad_norm=4.182212829589844, loss=0.60568767786026
I0307 17:21:10.513767 139787629491968 logging_writer.py:48] [440200] global_step=440200, grad_norm=4.54822301864624, loss=0.652231752872467
I0307 17:21:44.155302 139787612706560 logging_writer.py:48] [440300] global_step=440300, grad_norm=4.536101818084717, loss=0.628696084022522
I0307 17:22:17.789083 139787629491968 logging_writer.py:48] [440400] global_step=440400, grad_norm=4.546875953674316, loss=0.650600016117096
I0307 17:22:51.467879 139787612706560 logging_writer.py:48] [440500] global_step=440500, grad_norm=4.344667911529541, loss=0.6342703700065613
I0307 17:23:25.157292 139787629491968 logging_writer.py:48] [440600] global_step=440600, grad_norm=4.458278656005859, loss=0.5600452423095703
I0307 17:23:58.952118 139787612706560 logging_writer.py:48] [440700] global_step=440700, grad_norm=4.0235700607299805, loss=0.6130691766738892
I0307 17:24:32.550464 139787629491968 logging_writer.py:48] [440800] global_step=440800, grad_norm=4.621150016784668, loss=0.6193094849586487
I0307 17:25:06.204362 139787612706560 logging_writer.py:48] [440900] global_step=440900, grad_norm=4.392923831939697, loss=0.5845658183097839
I0307 17:25:39.838997 139787629491968 logging_writer.py:48] [441000] global_step=441000, grad_norm=4.609940528869629, loss=0.6347131729125977
I0307 17:26:13.477345 139787612706560 logging_writer.py:48] [441100] global_step=441100, grad_norm=4.799324035644531, loss=0.6755639910697937
I0307 17:26:47.101997 139787629491968 logging_writer.py:48] [441200] global_step=441200, grad_norm=4.804731369018555, loss=0.7357873320579529
I0307 17:27:18.536603 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:27:24.720934 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:27:33.246694 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:27:35.485624 139951089751872 submission_runner.py:411] Time since start: 153658.70s, 	Step: 441295, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14618828892707825, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.042002558708191, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8222293853759766, 'test/num_examples': 10000, 'score': 148471.76654148102, 'total_duration': 153658.69793319702, 'accumulated_submission_time': 148471.76654148102, 'accumulated_eval_time': 5148.434104681015, 'accumulated_logging_time': 22.382426500320435}
I0307 17:27:35.575805 139789013600000 logging_writer.py:48] [441295] accumulated_eval_time=5148.434105, accumulated_logging_time=22.382427, accumulated_submission_time=148471.766541, global_step=441295, preemption_count=0, score=148471.766541, test/accuracy=0.631800, test/loss=1.822229, test/num_examples=10000, total_duration=153658.697933, train/accuracy=0.960738, train/loss=0.146188, validation/accuracy=0.757140, validation/loss=1.042003, validation/num_examples=50000
I0307 17:27:37.591898 139789021992704 logging_writer.py:48] [441300] global_step=441300, grad_norm=4.495052814483643, loss=0.6113978624343872
I0307 17:28:11.220344 139789013600000 logging_writer.py:48] [441400] global_step=441400, grad_norm=4.545686721801758, loss=0.5951939821243286
I0307 17:28:44.899338 139789021992704 logging_writer.py:48] [441500] global_step=441500, grad_norm=4.211086273193359, loss=0.6353919506072998
I0307 17:29:18.593807 139789013600000 logging_writer.py:48] [441600] global_step=441600, grad_norm=4.876282215118408, loss=0.6448789834976196
I0307 17:29:52.333708 139789021992704 logging_writer.py:48] [441700] global_step=441700, grad_norm=4.040294647216797, loss=0.5672276020050049
I0307 17:30:26.022985 139789013600000 logging_writer.py:48] [441800] global_step=441800, grad_norm=4.6853437423706055, loss=0.6259864568710327
I0307 17:30:59.668789 139789021992704 logging_writer.py:48] [441900] global_step=441900, grad_norm=4.887327671051025, loss=0.6760775446891785
I0307 17:31:33.328067 139789013600000 logging_writer.py:48] [442000] global_step=442000, grad_norm=4.483565807342529, loss=0.5854440927505493
I0307 17:32:06.922500 139789021992704 logging_writer.py:48] [442100] global_step=442100, grad_norm=4.344954967498779, loss=0.6275784969329834
I0307 17:32:40.586314 139789013600000 logging_writer.py:48] [442200] global_step=442200, grad_norm=4.648233413696289, loss=0.66905677318573
I0307 17:33:14.211418 139789021992704 logging_writer.py:48] [442300] global_step=442300, grad_norm=4.306984901428223, loss=0.5779138803482056
I0307 17:33:47.880146 139789013600000 logging_writer.py:48] [442400] global_step=442400, grad_norm=4.339792728424072, loss=0.5873737335205078
I0307 17:34:21.491119 139789021992704 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.592447757720947, loss=0.6270236372947693
I0307 17:34:55.154291 139789013600000 logging_writer.py:48] [442600] global_step=442600, grad_norm=4.709357738494873, loss=0.6482564806938171
I0307 17:35:28.754544 139789021992704 logging_writer.py:48] [442700] global_step=442700, grad_norm=4.734707355499268, loss=0.6199093461036682
I0307 17:36:02.407113 139789013600000 logging_writer.py:48] [442800] global_step=442800, grad_norm=4.504052639007568, loss=0.527198076248169
I0307 17:36:05.580834 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:36:11.694762 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:36:20.417937 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:36:22.707339 139951089751872 submission_runner.py:411] Time since start: 154185.92s, 	Step: 442811, 	{'train/accuracy': 0.9609175324440002, 'train/loss': 0.14684396982192993, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.040229320526123, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.820679783821106, 'test/num_examples': 10000, 'score': 148981.70437383652, 'total_duration': 154185.9196381569, 'accumulated_submission_time': 148981.70437383652, 'accumulated_eval_time': 5165.56057715416, 'accumulated_logging_time': 22.4826500415802}
I0307 17:36:22.802188 139787621099264 logging_writer.py:48] [442811] accumulated_eval_time=5165.560577, accumulated_logging_time=22.482650, accumulated_submission_time=148981.704374, global_step=442811, preemption_count=0, score=148981.704374, test/accuracy=0.632100, test/loss=1.820680, test/num_examples=10000, total_duration=154185.919638, train/accuracy=0.960918, train/loss=0.146844, validation/accuracy=0.757080, validation/loss=1.040229, validation/num_examples=50000
I0307 17:36:53.075189 139787629491968 logging_writer.py:48] [442900] global_step=442900, grad_norm=4.6632513999938965, loss=0.5879688262939453
I0307 17:37:26.750332 139787621099264 logging_writer.py:48] [443000] global_step=443000, grad_norm=4.055293560028076, loss=0.5847506523132324
I0307 17:38:00.405723 139787629491968 logging_writer.py:48] [443100] global_step=443100, grad_norm=4.343021392822266, loss=0.6165955662727356
I0307 17:38:34.066059 139787621099264 logging_writer.py:48] [443200] global_step=443200, grad_norm=4.875119686126709, loss=0.6339169144630432
I0307 17:39:07.738716 139787629491968 logging_writer.py:48] [443300] global_step=443300, grad_norm=4.971558570861816, loss=0.6798877716064453
I0307 17:39:41.377145 139787621099264 logging_writer.py:48] [443400] global_step=443400, grad_norm=4.432205677032471, loss=0.6254054307937622
I0307 17:40:15.013724 139787629491968 logging_writer.py:48] [443500] global_step=443500, grad_norm=4.285264015197754, loss=0.6515054702758789
I0307 17:40:48.665946 139787621099264 logging_writer.py:48] [443600] global_step=443600, grad_norm=4.454462051391602, loss=0.6251442432403564
I0307 17:41:22.323240 139787629491968 logging_writer.py:48] [443700] global_step=443700, grad_norm=4.186855316162109, loss=0.5680725574493408
I0307 17:41:56.008738 139787621099264 logging_writer.py:48] [443800] global_step=443800, grad_norm=4.363618850708008, loss=0.6421728134155273
I0307 17:42:29.574524 139787629491968 logging_writer.py:48] [443900] global_step=443900, grad_norm=4.779926776885986, loss=0.6048907041549683
I0307 17:43:03.150094 139787621099264 logging_writer.py:48] [444000] global_step=444000, grad_norm=4.375500679016113, loss=0.6221036314964294
I0307 17:43:36.772122 139787629491968 logging_writer.py:48] [444100] global_step=444100, grad_norm=4.277336120605469, loss=0.5826117396354675
I0307 17:44:10.411131 139787621099264 logging_writer.py:48] [444200] global_step=444200, grad_norm=4.498438835144043, loss=0.5800127387046814
I0307 17:44:44.105846 139787629491968 logging_writer.py:48] [444300] global_step=444300, grad_norm=4.509886741638184, loss=0.6523295044898987
I0307 17:44:53.005199 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:44:59.024957 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:45:07.550314 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:45:10.319573 139951089751872 submission_runner.py:411] Time since start: 154713.53s, 	Step: 444328, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.15017102658748627, 'validation/accuracy': 0.7574399709701538, 'validation/loss': 1.0412341356277466, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8212313652038574, 'test/num_examples': 10000, 'score': 149491.8409090042, 'total_duration': 154713.53189253807, 'accumulated_submission_time': 149491.8409090042, 'accumulated_eval_time': 5182.874908208847, 'accumulated_logging_time': 22.5871741771698}
I0307 17:45:10.391268 139789013600000 logging_writer.py:48] [444328] accumulated_eval_time=5182.874908, accumulated_logging_time=22.587174, accumulated_submission_time=149491.840909, global_step=444328, preemption_count=0, score=149491.840909, test/accuracy=0.631700, test/loss=1.821231, test/num_examples=10000, total_duration=154713.531893, train/accuracy=0.960619, train/loss=0.150171, validation/accuracy=0.757440, validation/loss=1.041234, validation/num_examples=50000
I0307 17:45:34.863793 139789021992704 logging_writer.py:48] [444400] global_step=444400, grad_norm=4.448907375335693, loss=0.6554952263832092
I0307 17:46:08.493557 139789013600000 logging_writer.py:48] [444500] global_step=444500, grad_norm=4.264367580413818, loss=0.6097805500030518
I0307 17:46:42.133773 139789021992704 logging_writer.py:48] [444600] global_step=444600, grad_norm=4.514159202575684, loss=0.6686416864395142
I0307 17:47:15.806124 139789013600000 logging_writer.py:48] [444700] global_step=444700, grad_norm=4.699594020843506, loss=0.7192618250846863
I0307 17:47:49.460401 139789021992704 logging_writer.py:48] [444800] global_step=444800, grad_norm=4.154552936553955, loss=0.612082839012146
I0307 17:48:23.126493 139789013600000 logging_writer.py:48] [444900] global_step=444900, grad_norm=4.278243064880371, loss=0.6422085762023926
I0307 17:48:56.713655 139789021992704 logging_writer.py:48] [445000] global_step=445000, grad_norm=4.432140827178955, loss=0.6085242629051208
I0307 17:49:30.343369 139789013600000 logging_writer.py:48] [445100] global_step=445100, grad_norm=4.753661632537842, loss=0.7237223386764526
I0307 17:50:03.934474 139789021992704 logging_writer.py:48] [445200] global_step=445200, grad_norm=4.2779669761657715, loss=0.5688182711601257
I0307 17:50:37.591968 139789013600000 logging_writer.py:48] [445300] global_step=445300, grad_norm=4.465044975280762, loss=0.6206047534942627
I0307 17:51:11.241343 139789021992704 logging_writer.py:48] [445400] global_step=445400, grad_norm=4.6883955001831055, loss=0.6691441535949707
I0307 17:51:44.869497 139789013600000 logging_writer.py:48] [445500] global_step=445500, grad_norm=4.655381679534912, loss=0.6042680144309998
I0307 17:52:18.554345 139789021992704 logging_writer.py:48] [445600] global_step=445600, grad_norm=4.408265590667725, loss=0.7070755362510681
I0307 17:52:52.216055 139789013600000 logging_writer.py:48] [445700] global_step=445700, grad_norm=4.7747063636779785, loss=0.6323760151863098
I0307 17:53:25.823961 139789021992704 logging_writer.py:48] [445800] global_step=445800, grad_norm=5.017507553100586, loss=0.6165716052055359
I0307 17:53:40.451307 139951089751872 spec.py:321] Evaluating on the training split.
I0307 17:53:46.506673 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 17:53:55.026245 139951089751872 spec.py:349] Evaluating on the test split.
I0307 17:53:57.374690 139951089751872 submission_runner.py:411] Time since start: 155240.59s, 	Step: 445845, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14679908752441406, 'validation/accuracy': 0.7567799687385559, 'validation/loss': 1.0411818027496338, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.822283148765564, 'test/num_examples': 10000, 'score': 150001.83459067345, 'total_duration': 155240.5869767666, 'accumulated_submission_time': 150001.83459067345, 'accumulated_eval_time': 5199.798217058182, 'accumulated_logging_time': 22.6678466796875}
I0307 17:53:57.467333 139787612706560 logging_writer.py:48] [445845] accumulated_eval_time=5199.798217, accumulated_logging_time=22.667847, accumulated_submission_time=150001.834591, global_step=445845, preemption_count=0, score=150001.834591, test/accuracy=0.631100, test/loss=1.822283, test/num_examples=10000, total_duration=155240.586977, train/accuracy=0.960698, train/loss=0.146799, validation/accuracy=0.756780, validation/loss=1.041182, validation/num_examples=50000
I0307 17:54:16.318719 139787621099264 logging_writer.py:48] [445900] global_step=445900, grad_norm=4.613567352294922, loss=0.6081554293632507
I0307 17:54:49.944983 139787612706560 logging_writer.py:48] [446000] global_step=446000, grad_norm=4.551453590393066, loss=0.6581850647926331
I0307 17:55:23.527589 139787621099264 logging_writer.py:48] [446100] global_step=446100, grad_norm=4.266087055206299, loss=0.5913599133491516
I0307 17:55:57.096186 139787612706560 logging_writer.py:48] [446200] global_step=446200, grad_norm=4.11568021774292, loss=0.5779893398284912
I0307 17:56:30.695304 139787621099264 logging_writer.py:48] [446300] global_step=446300, grad_norm=4.79506778717041, loss=0.6523452997207642
I0307 17:57:04.339094 139787612706560 logging_writer.py:48] [446400] global_step=446400, grad_norm=4.466220855712891, loss=0.6790685653686523
I0307 17:57:37.972618 139787621099264 logging_writer.py:48] [446500] global_step=446500, grad_norm=4.262260913848877, loss=0.6017876863479614
I0307 17:58:11.619410 139787612706560 logging_writer.py:48] [446600] global_step=446600, grad_norm=4.883166790008545, loss=0.665157675743103
I0307 17:58:45.264540 139787621099264 logging_writer.py:48] [446700] global_step=446700, grad_norm=4.598290920257568, loss=0.6461647152900696
I0307 17:59:18.901067 139787612706560 logging_writer.py:48] [446800] global_step=446800, grad_norm=4.528556823730469, loss=0.560552716255188
I0307 17:59:52.562536 139787621099264 logging_writer.py:48] [446900] global_step=446900, grad_norm=4.431229591369629, loss=0.6613813638687134
I0307 18:00:26.203784 139787612706560 logging_writer.py:48] [447000] global_step=447000, grad_norm=4.898288726806641, loss=0.6396102905273438
I0307 18:00:59.819972 139787621099264 logging_writer.py:48] [447100] global_step=447100, grad_norm=4.926511764526367, loss=0.6165194511413574
I0307 18:01:33.452208 139787612706560 logging_writer.py:48] [447200] global_step=447200, grad_norm=4.747504711151123, loss=0.7605454921722412
I0307 18:02:07.109539 139787621099264 logging_writer.py:48] [447300] global_step=447300, grad_norm=4.1515913009643555, loss=0.5730255842208862
I0307 18:02:27.433960 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:02:33.490613 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:02:42.063949 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:02:44.347524 139951089751872 submission_runner.py:411] Time since start: 155767.56s, 	Step: 447362, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14714635908603668, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0414865016937256, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8231927156448364, 'test/num_examples': 10000, 'score': 150511.73395705223, 'total_duration': 155767.55983424187, 'accumulated_submission_time': 150511.73395705223, 'accumulated_eval_time': 5216.711742401123, 'accumulated_logging_time': 22.770523071289062}
I0307 18:02:44.437620 139787621099264 logging_writer.py:48] [447362] accumulated_eval_time=5216.711742, accumulated_logging_time=22.770523, accumulated_submission_time=150511.733957, global_step=447362, preemption_count=0, score=150511.733957, test/accuracy=0.631900, test/loss=1.823193, test/num_examples=10000, total_duration=155767.559834, train/accuracy=0.960280, train/loss=0.147146, validation/accuracy=0.757120, validation/loss=1.041487, validation/num_examples=50000
I0307 18:02:57.544170 139789005207296 logging_writer.py:48] [447400] global_step=447400, grad_norm=3.974357843399048, loss=0.6050289273262024
I0307 18:03:31.171739 139787621099264 logging_writer.py:48] [447500] global_step=447500, grad_norm=4.115703582763672, loss=0.5384620428085327
I0307 18:04:04.819274 139789005207296 logging_writer.py:48] [447600] global_step=447600, grad_norm=4.415750503540039, loss=0.6879824995994568
I0307 18:04:38.432935 139787621099264 logging_writer.py:48] [447700] global_step=447700, grad_norm=4.085805416107178, loss=0.6323255300521851
I0307 18:05:12.089920 139789005207296 logging_writer.py:48] [447800] global_step=447800, grad_norm=4.397345542907715, loss=0.6006572842597961
I0307 18:05:45.702562 139787621099264 logging_writer.py:48] [447900] global_step=447900, grad_norm=4.731963157653809, loss=0.6571955680847168
I0307 18:06:19.357159 139789005207296 logging_writer.py:48] [448000] global_step=448000, grad_norm=4.35762882232666, loss=0.597248911857605
I0307 18:06:53.129460 139787621099264 logging_writer.py:48] [448100] global_step=448100, grad_norm=4.152049541473389, loss=0.6214436888694763
I0307 18:07:26.777895 139789005207296 logging_writer.py:48] [448200] global_step=448200, grad_norm=4.65613317489624, loss=0.6393736004829407
I0307 18:08:00.375521 139787621099264 logging_writer.py:48] [448300] global_step=448300, grad_norm=4.53740930557251, loss=0.5964993238449097
I0307 18:08:34.026177 139789005207296 logging_writer.py:48] [448400] global_step=448400, grad_norm=4.805212497711182, loss=0.670526385307312
I0307 18:09:07.651345 139787621099264 logging_writer.py:48] [448500] global_step=448500, grad_norm=4.490766525268555, loss=0.6418555974960327
I0307 18:09:41.306662 139789005207296 logging_writer.py:48] [448600] global_step=448600, grad_norm=4.3715643882751465, loss=0.6301516890525818
I0307 18:10:14.918255 139787621099264 logging_writer.py:48] [448700] global_step=448700, grad_norm=4.526767253875732, loss=0.5828995108604431
I0307 18:10:48.576591 139789005207296 logging_writer.py:48] [448800] global_step=448800, grad_norm=4.54104471206665, loss=0.6825252771377563
I0307 18:11:14.582106 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:11:20.644923 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:11:29.150814 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:11:31.428928 139951089751872 submission_runner.py:411] Time since start: 156294.64s, 	Step: 448879, 	{'train/accuracy': 0.9606186151504517, 'train/loss': 0.14553286135196686, 'validation/accuracy': 0.7567200064659119, 'validation/loss': 1.0406856536865234, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8224512338638306, 'test/num_examples': 10000, 'score': 151021.81368637085, 'total_duration': 156294.64122962952, 'accumulated_submission_time': 151021.81368637085, 'accumulated_eval_time': 5233.558504581451, 'accumulated_logging_time': 22.870090007781982}
I0307 18:11:31.522729 139787612706560 logging_writer.py:48] [448879] accumulated_eval_time=5233.558505, accumulated_logging_time=22.870090, accumulated_submission_time=151021.813686, global_step=448879, preemption_count=0, score=151021.813686, test/accuracy=0.632000, test/loss=1.822451, test/num_examples=10000, total_duration=156294.641230, train/accuracy=0.960619, train/loss=0.145533, validation/accuracy=0.756720, validation/loss=1.040686, validation/num_examples=50000
I0307 18:11:38.924299 139787621099264 logging_writer.py:48] [448900] global_step=448900, grad_norm=4.764620780944824, loss=0.6847200989723206
I0307 18:12:12.611213 139787612706560 logging_writer.py:48] [449000] global_step=449000, grad_norm=4.2508544921875, loss=0.5044165253639221
I0307 18:12:46.313322 139787621099264 logging_writer.py:48] [449100] global_step=449100, grad_norm=4.330560207366943, loss=0.6700059175491333
I0307 18:13:19.908638 139787612706560 logging_writer.py:48] [449200] global_step=449200, grad_norm=4.233397483825684, loss=0.6012891530990601
I0307 18:13:53.485277 139787621099264 logging_writer.py:48] [449300] global_step=449300, grad_norm=4.764042854309082, loss=0.6841067671775818
I0307 18:14:27.060613 139787612706560 logging_writer.py:48] [449400] global_step=449400, grad_norm=4.402225971221924, loss=0.6313980221748352
I0307 18:15:00.707576 139787621099264 logging_writer.py:48] [449500] global_step=449500, grad_norm=4.346541404724121, loss=0.662723958492279
I0307 18:15:34.367525 139787612706560 logging_writer.py:48] [449600] global_step=449600, grad_norm=4.5287628173828125, loss=0.6894969940185547
I0307 18:16:08.007991 139787621099264 logging_writer.py:48] [449700] global_step=449700, grad_norm=4.3865742683410645, loss=0.620905876159668
I0307 18:16:41.677592 139787612706560 logging_writer.py:48] [449800] global_step=449800, grad_norm=4.291772365570068, loss=0.6184381246566772
I0307 18:17:15.342651 139787621099264 logging_writer.py:48] [449900] global_step=449900, grad_norm=4.622889041900635, loss=0.691959798336029
I0307 18:17:48.949692 139787612706560 logging_writer.py:48] [450000] global_step=450000, grad_norm=5.125851631164551, loss=0.689117431640625
I0307 18:18:22.596616 139787621099264 logging_writer.py:48] [450100] global_step=450100, grad_norm=4.580671310424805, loss=0.5690239667892456
I0307 18:18:56.414227 139787612706560 logging_writer.py:48] [450200] global_step=450200, grad_norm=4.4498419761657715, loss=0.6219803690910339
I0307 18:19:30.057012 139787621099264 logging_writer.py:48] [450300] global_step=450300, grad_norm=4.464849472045898, loss=0.6463831067085266
I0307 18:20:01.460349 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:20:07.468514 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:20:15.957740 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:20:18.239377 139951089751872 submission_runner.py:411] Time since start: 156821.45s, 	Step: 450395, 	{'train/accuracy': 0.9595423936843872, 'train/loss': 0.14797700941562653, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0416878461837769, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.824686050415039, 'test/num_examples': 10000, 'score': 151531.68371200562, 'total_duration': 156821.4516866207, 'accumulated_submission_time': 151531.68371200562, 'accumulated_eval_time': 5250.33748459816, 'accumulated_logging_time': 22.97496485710144}
I0307 18:20:18.331080 139789021992704 logging_writer.py:48] [450395] accumulated_eval_time=5250.337485, accumulated_logging_time=22.974965, accumulated_submission_time=151531.683712, global_step=450395, preemption_count=0, score=151531.683712, test/accuracy=0.632200, test/loss=1.824686, test/num_examples=10000, total_duration=156821.451687, train/accuracy=0.959542, train/loss=0.147977, validation/accuracy=0.757200, validation/loss=1.041688, validation/num_examples=50000
I0307 18:20:20.358906 139789030385408 logging_writer.py:48] [450400] global_step=450400, grad_norm=4.695828437805176, loss=0.6864463686943054
I0307 18:20:53.969858 139789021992704 logging_writer.py:48] [450500] global_step=450500, grad_norm=4.4760942459106445, loss=0.5642605423927307
I0307 18:21:27.639563 139789030385408 logging_writer.py:48] [450600] global_step=450600, grad_norm=4.297849655151367, loss=0.5306839346885681
I0307 18:22:01.320960 139789021992704 logging_writer.py:48] [450700] global_step=450700, grad_norm=4.575746536254883, loss=0.6664790511131287
I0307 18:22:34.938325 139789030385408 logging_writer.py:48] [450800] global_step=450800, grad_norm=4.618359565734863, loss=0.6311565041542053
I0307 18:23:08.568811 139789021992704 logging_writer.py:48] [450900] global_step=450900, grad_norm=4.639157772064209, loss=0.6382498145103455
I0307 18:23:42.198337 139789030385408 logging_writer.py:48] [451000] global_step=451000, grad_norm=4.916501522064209, loss=0.6649723649024963
I0307 18:24:15.850177 139789021992704 logging_writer.py:48] [451100] global_step=451100, grad_norm=4.699059963226318, loss=0.6733192205429077
I0307 18:24:49.555226 139789030385408 logging_writer.py:48] [451200] global_step=451200, grad_norm=4.561418056488037, loss=0.6476989984512329
I0307 18:25:23.138935 139789021992704 logging_writer.py:48] [451300] global_step=451300, grad_norm=4.094047546386719, loss=0.6048884391784668
I0307 18:25:56.807490 139789030385408 logging_writer.py:48] [451400] global_step=451400, grad_norm=4.467663288116455, loss=0.6036438345909119
I0307 18:26:30.501271 139789021992704 logging_writer.py:48] [451500] global_step=451500, grad_norm=5.02235221862793, loss=0.6491100788116455
I0307 18:27:04.142997 139789030385408 logging_writer.py:48] [451600] global_step=451600, grad_norm=4.312429904937744, loss=0.6084775924682617
I0307 18:27:37.813435 139789021992704 logging_writer.py:48] [451700] global_step=451700, grad_norm=4.3773956298828125, loss=0.6019006967544556
I0307 18:28:11.436096 139789030385408 logging_writer.py:48] [451800] global_step=451800, grad_norm=4.433398723602295, loss=0.5848954916000366
I0307 18:28:45.088518 139789021992704 logging_writer.py:48] [451900] global_step=451900, grad_norm=4.211429595947266, loss=0.5684486031532288
I0307 18:28:48.271967 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:28:54.303902 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:29:02.892825 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:29:05.168462 139951089751872 submission_runner.py:411] Time since start: 157348.38s, 	Step: 451911, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14697732031345367, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.0414307117462158, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8233704566955566, 'test/num_examples': 10000, 'score': 152041.5577995777, 'total_duration': 157348.38074493408, 'accumulated_submission_time': 152041.5577995777, 'accumulated_eval_time': 5267.2338972091675, 'accumulated_logging_time': 23.07772135734558}
I0307 18:29:05.259541 139788996814592 logging_writer.py:48] [451911] accumulated_eval_time=5267.233897, accumulated_logging_time=23.077721, accumulated_submission_time=152041.557800, global_step=451911, preemption_count=0, score=152041.557800, test/accuracy=0.631800, test/loss=1.823370, test/num_examples=10000, total_duration=157348.380745, train/accuracy=0.959981, train/loss=0.146977, validation/accuracy=0.757360, validation/loss=1.041431, validation/num_examples=50000
I0307 18:29:35.534672 139789038778112 logging_writer.py:48] [452000] global_step=452000, grad_norm=4.175465106964111, loss=0.5629593133926392
I0307 18:30:09.132122 139788996814592 logging_writer.py:48] [452100] global_step=452100, grad_norm=5.020328521728516, loss=0.6393750309944153
I0307 18:30:42.768043 139789038778112 logging_writer.py:48] [452200] global_step=452200, grad_norm=4.2544403076171875, loss=0.6311188340187073
I0307 18:31:16.425762 139788996814592 logging_writer.py:48] [452300] global_step=452300, grad_norm=4.654094219207764, loss=0.6513837575912476
I0307 18:31:50.093861 139789038778112 logging_writer.py:48] [452400] global_step=452400, grad_norm=4.520902156829834, loss=0.6163383722305298
I0307 18:32:23.722586 139788996814592 logging_writer.py:48] [452500] global_step=452500, grad_norm=4.142111778259277, loss=0.6646186113357544
I0307 18:32:57.396501 139789038778112 logging_writer.py:48] [452600] global_step=452600, grad_norm=4.530654430389404, loss=0.643646776676178
I0307 18:33:31.047353 139788996814592 logging_writer.py:48] [452700] global_step=452700, grad_norm=4.867631435394287, loss=0.6485790610313416
I0307 18:34:04.697555 139789038778112 logging_writer.py:48] [452800] global_step=452800, grad_norm=4.475275039672852, loss=0.6226681470870972
I0307 18:34:38.338696 139788996814592 logging_writer.py:48] [452900] global_step=452900, grad_norm=4.396275997161865, loss=0.5597928762435913
I0307 18:35:11.983241 139789038778112 logging_writer.py:48] [453000] global_step=453000, grad_norm=4.414302349090576, loss=0.6265366077423096
I0307 18:35:45.642310 139788996814592 logging_writer.py:48] [453100] global_step=453100, grad_norm=4.4803786277771, loss=0.6032180190086365
I0307 18:36:19.308660 139789038778112 logging_writer.py:48] [453200] global_step=453200, grad_norm=4.605808258056641, loss=0.663814902305603
I0307 18:36:52.914091 139788996814592 logging_writer.py:48] [453300] global_step=453300, grad_norm=4.186903476715088, loss=0.6391595602035522
I0307 18:37:26.565613 139789038778112 logging_writer.py:48] [453400] global_step=453400, grad_norm=4.634869575500488, loss=0.602961540222168
I0307 18:37:35.441742 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:37:41.489744 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:37:50.158960 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:37:52.511256 139951089751872 submission_runner.py:411] Time since start: 157875.72s, 	Step: 453428, 	{'train/accuracy': 0.9600406289100647, 'train/loss': 0.1466209590435028, 'validation/accuracy': 0.7575199604034424, 'validation/loss': 1.0417447090148926, 'validation/num_examples': 50000, 'test/accuracy': 0.632900059223175, 'test/loss': 1.82382333278656, 'test/num_examples': 10000, 'score': 152551.6747918129, 'total_duration': 157875.72356677055, 'accumulated_submission_time': 152551.6747918129, 'accumulated_eval_time': 5284.3033618927, 'accumulated_logging_time': 23.179197311401367}
I0307 18:37:52.602699 139787612706560 logging_writer.py:48] [453428] accumulated_eval_time=5284.303362, accumulated_logging_time=23.179197, accumulated_submission_time=152551.674792, global_step=453428, preemption_count=0, score=152551.674792, test/accuracy=0.632900, test/loss=1.823823, test/num_examples=10000, total_duration=157875.723567, train/accuracy=0.960041, train/loss=0.146621, validation/accuracy=0.757520, validation/loss=1.041745, validation/num_examples=50000
I0307 18:38:17.109237 139787621099264 logging_writer.py:48] [453500] global_step=453500, grad_norm=4.809030532836914, loss=0.6836980581283569
I0307 18:38:50.752322 139787612706560 logging_writer.py:48] [453600] global_step=453600, grad_norm=4.45954704284668, loss=0.6246155500411987
I0307 18:39:24.359519 139787621099264 logging_writer.py:48] [453700] global_step=453700, grad_norm=4.596841812133789, loss=0.5874046087265015
I0307 18:39:58.033173 139787612706560 logging_writer.py:48] [453800] global_step=453800, grad_norm=4.3413214683532715, loss=0.5722489356994629
I0307 18:40:31.636362 139787621099264 logging_writer.py:48] [453900] global_step=453900, grad_norm=4.560178279876709, loss=0.6301878690719604
I0307 18:41:05.303104 139787612706560 logging_writer.py:48] [454000] global_step=454000, grad_norm=4.7513251304626465, loss=0.6246291995048523
I0307 18:41:38.914355 139787621099264 logging_writer.py:48] [454100] global_step=454100, grad_norm=4.835700988769531, loss=0.6069902777671814
I0307 18:42:12.585378 139787612706560 logging_writer.py:48] [454200] global_step=454200, grad_norm=4.259754180908203, loss=0.6311073899269104
I0307 18:42:46.206418 139787621099264 logging_writer.py:48] [454300] global_step=454300, grad_norm=4.386685848236084, loss=0.5841975212097168
I0307 18:43:19.914065 139787612706560 logging_writer.py:48] [454400] global_step=454400, grad_norm=4.64243221282959, loss=0.5855990648269653
I0307 18:43:53.547431 139787621099264 logging_writer.py:48] [454500] global_step=454500, grad_norm=3.9695823192596436, loss=0.5351751446723938
I0307 18:44:27.203735 139787612706560 logging_writer.py:48] [454600] global_step=454600, grad_norm=4.332846164703369, loss=0.5811223983764648
I0307 18:45:00.876983 139787621099264 logging_writer.py:48] [454700] global_step=454700, grad_norm=4.354465007781982, loss=0.6242259740829468
I0307 18:45:34.557103 139787612706560 logging_writer.py:48] [454800] global_step=454800, grad_norm=4.258581161499023, loss=0.5889410376548767
I0307 18:46:08.149821 139787621099264 logging_writer.py:48] [454900] global_step=454900, grad_norm=4.455944061279297, loss=0.6304296255111694
I0307 18:46:22.759533 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:46:28.888830 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:46:37.471101 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:46:39.727495 139951089751872 submission_runner.py:411] Time since start: 158402.94s, 	Step: 454945, 	{'train/accuracy': 0.9618343114852905, 'train/loss': 0.14661747217178345, 'validation/accuracy': 0.7567399740219116, 'validation/loss': 1.0415546894073486, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8232858180999756, 'test/num_examples': 10000, 'score': 153061.76563978195, 'total_duration': 158402.9397919178, 'accumulated_submission_time': 153061.76563978195, 'accumulated_eval_time': 5301.271278142929, 'accumulated_logging_time': 23.28038787841797}
I0307 18:46:39.820158 139787612706560 logging_writer.py:48] [454945] accumulated_eval_time=5301.271278, accumulated_logging_time=23.280388, accumulated_submission_time=153061.765640, global_step=454945, preemption_count=0, score=153061.765640, test/accuracy=0.631700, test/loss=1.823286, test/num_examples=10000, total_duration=158402.939792, train/accuracy=0.961834, train/loss=0.146617, validation/accuracy=0.756740, validation/loss=1.041555, validation/num_examples=50000
I0307 18:46:58.631297 139789013600000 logging_writer.py:48] [455000] global_step=455000, grad_norm=4.728269100189209, loss=0.6320529580116272
I0307 18:47:32.242973 139787612706560 logging_writer.py:48] [455100] global_step=455100, grad_norm=4.456351280212402, loss=0.6246297955513
I0307 18:48:05.846764 139789013600000 logging_writer.py:48] [455200] global_step=455200, grad_norm=4.92625617980957, loss=0.6429478526115417
I0307 18:48:39.426922 139787612706560 logging_writer.py:48] [455300] global_step=455300, grad_norm=4.610991954803467, loss=0.6747356057167053
I0307 18:49:13.016103 139789013600000 logging_writer.py:48] [455400] global_step=455400, grad_norm=5.060018539428711, loss=0.6648197174072266
I0307 18:49:46.682430 139787612706560 logging_writer.py:48] [455500] global_step=455500, grad_norm=4.888474464416504, loss=0.653425931930542
I0307 18:50:20.294629 139789013600000 logging_writer.py:48] [455600] global_step=455600, grad_norm=4.4534592628479, loss=0.6604764461517334
I0307 18:50:53.954073 139787612706560 logging_writer.py:48] [455700] global_step=455700, grad_norm=4.3623785972595215, loss=0.5823508501052856
I0307 18:51:27.595873 139789013600000 logging_writer.py:48] [455800] global_step=455800, grad_norm=4.314466953277588, loss=0.5969130396842957
I0307 18:52:01.243982 139787612706560 logging_writer.py:48] [455900] global_step=455900, grad_norm=4.721677303314209, loss=0.6611282825469971
I0307 18:52:34.858369 139789013600000 logging_writer.py:48] [456000] global_step=456000, grad_norm=4.390608787536621, loss=0.656662106513977
I0307 18:53:08.499766 139787612706560 logging_writer.py:48] [456100] global_step=456100, grad_norm=4.517461776733398, loss=0.608539342880249
I0307 18:53:42.128185 139789013600000 logging_writer.py:48] [456200] global_step=456200, grad_norm=4.540380477905273, loss=0.5771851539611816
I0307 18:54:15.758148 139787612706560 logging_writer.py:48] [456300] global_step=456300, grad_norm=4.0876898765563965, loss=0.5947819352149963
I0307 18:54:49.369251 139789013600000 logging_writer.py:48] [456400] global_step=456400, grad_norm=4.527193069458008, loss=0.5972167253494263
I0307 18:55:10.021690 139951089751872 spec.py:321] Evaluating on the training split.
I0307 18:55:16.335972 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 18:55:24.623342 139951089751872 spec.py:349] Evaluating on the test split.
I0307 18:55:26.901206 139951089751872 submission_runner.py:411] Time since start: 158930.11s, 	Step: 456463, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14655934274196625, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.0404845476150513, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8218352794647217, 'test/num_examples': 10000, 'score': 153571.90072655678, 'total_duration': 158930.11351394653, 'accumulated_submission_time': 153571.90072655678, 'accumulated_eval_time': 5318.150739192963, 'accumulated_logging_time': 23.382413625717163}
I0307 18:55:26.993662 139787621099264 logging_writer.py:48] [456463] accumulated_eval_time=5318.150739, accumulated_logging_time=23.382414, accumulated_submission_time=153571.900727, global_step=456463, preemption_count=0, score=153571.900727, test/accuracy=0.630800, test/loss=1.821835, test/num_examples=10000, total_duration=158930.113514, train/accuracy=0.960977, train/loss=0.146559, validation/accuracy=0.757360, validation/loss=1.040485, validation/num_examples=50000
I0307 18:55:39.814538 139787629491968 logging_writer.py:48] [456500] global_step=456500, grad_norm=4.771224021911621, loss=0.6552225947380066
I0307 18:56:13.410635 139787621099264 logging_writer.py:48] [456600] global_step=456600, grad_norm=4.220661640167236, loss=0.5588411688804626
I0307 18:56:46.952087 139787629491968 logging_writer.py:48] [456700] global_step=456700, grad_norm=4.431352615356445, loss=0.5729835033416748
I0307 18:57:20.576353 139787621099264 logging_writer.py:48] [456800] global_step=456800, grad_norm=4.491507053375244, loss=0.6731859445571899
I0307 18:57:54.246678 139787629491968 logging_writer.py:48] [456900] global_step=456900, grad_norm=5.284447193145752, loss=0.6759889125823975
I0307 18:58:27.890528 139787621099264 logging_writer.py:48] [457000] global_step=457000, grad_norm=4.494723796844482, loss=0.543184757232666
I0307 18:59:01.555827 139787629491968 logging_writer.py:48] [457100] global_step=457100, grad_norm=4.850028038024902, loss=0.6134113669395447
I0307 18:59:35.175857 139787621099264 logging_writer.py:48] [457200] global_step=457200, grad_norm=4.3531270027160645, loss=0.5376885533332825
I0307 19:00:08.828920 139787629491968 logging_writer.py:48] [457300] global_step=457300, grad_norm=4.223686695098877, loss=0.5938618779182434
I0307 19:00:42.447217 139787621099264 logging_writer.py:48] [457400] global_step=457400, grad_norm=4.391284942626953, loss=0.5894464254379272
I0307 19:01:16.106914 139787629491968 logging_writer.py:48] [457500] global_step=457500, grad_norm=4.106419086456299, loss=0.59734708070755
I0307 19:01:49.739709 139787621099264 logging_writer.py:48] [457600] global_step=457600, grad_norm=4.889689922332764, loss=0.5951639413833618
I0307 19:02:23.347126 139787629491968 logging_writer.py:48] [457700] global_step=457700, grad_norm=4.5137176513671875, loss=0.6318223476409912
I0307 19:02:56.964791 139787621099264 logging_writer.py:48] [457800] global_step=457800, grad_norm=5.0602545738220215, loss=0.6357413530349731
I0307 19:03:30.617883 139787629491968 logging_writer.py:48] [457900] global_step=457900, grad_norm=4.33500337600708, loss=0.6325345039367676
I0307 19:03:56.987312 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:04:03.026531 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:04:11.688566 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:04:13.964409 139951089751872 submission_runner.py:411] Time since start: 159457.18s, 	Step: 457980, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14680106937885284, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0417757034301758, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.8235985040664673, 'test/num_examples': 10000, 'score': 154081.8287653923, 'total_duration': 159457.1767117977, 'accumulated_submission_time': 154081.8287653923, 'accumulated_eval_time': 5335.127779006958, 'accumulated_logging_time': 23.484861850738525}
I0307 19:04:14.062205 139789030385408 logging_writer.py:48] [457980] accumulated_eval_time=5335.127779, accumulated_logging_time=23.484862, accumulated_submission_time=154081.828765, global_step=457980, preemption_count=0, score=154081.828765, test/accuracy=0.630200, test/loss=1.823599, test/num_examples=10000, total_duration=159457.176712, train/accuracy=0.960459, train/loss=0.146801, validation/accuracy=0.757180, validation/loss=1.041776, validation/num_examples=50000
I0307 19:04:21.113208 139789038778112 logging_writer.py:48] [458000] global_step=458000, grad_norm=4.79523229598999, loss=0.6004029512405396
I0307 19:04:54.708464 139789030385408 logging_writer.py:48] [458100] global_step=458100, grad_norm=3.992588758468628, loss=0.5395612120628357
I0307 19:05:28.282905 139789038778112 logging_writer.py:48] [458200] global_step=458200, grad_norm=4.3377509117126465, loss=0.6140795350074768
I0307 19:06:01.872007 139789030385408 logging_writer.py:48] [458300] global_step=458300, grad_norm=4.451135635375977, loss=0.5567741990089417
I0307 19:06:35.523119 139789038778112 logging_writer.py:48] [458400] global_step=458400, grad_norm=4.3955793380737305, loss=0.6513879895210266
I0307 19:07:09.197437 139789030385408 logging_writer.py:48] [458500] global_step=458500, grad_norm=4.638707637786865, loss=0.6665374040603638
I0307 19:07:42.848142 139789038778112 logging_writer.py:48] [458600] global_step=458600, grad_norm=4.781702518463135, loss=0.6273029446601868
I0307 19:08:16.662020 139789030385408 logging_writer.py:48] [458700] global_step=458700, grad_norm=4.973865509033203, loss=0.606688916683197
I0307 19:08:50.351114 139789038778112 logging_writer.py:48] [458800] global_step=458800, grad_norm=4.433819770812988, loss=0.6142339110374451
I0307 19:09:23.991672 139789030385408 logging_writer.py:48] [458900] global_step=458900, grad_norm=4.493398666381836, loss=0.6505612134933472
I0307 19:09:57.669735 139789038778112 logging_writer.py:48] [459000] global_step=459000, grad_norm=4.397894382476807, loss=0.6460064053535461
I0307 19:10:31.306046 139789030385408 logging_writer.py:48] [459100] global_step=459100, grad_norm=4.485419750213623, loss=0.543403685092926
I0307 19:11:04.949442 139789038778112 logging_writer.py:48] [459200] global_step=459200, grad_norm=4.289010047912598, loss=0.6612470149993896
I0307 19:11:38.550282 139789030385408 logging_writer.py:48] [459300] global_step=459300, grad_norm=4.380360126495361, loss=0.5912832617759705
I0307 19:12:12.193366 139789038778112 logging_writer.py:48] [459400] global_step=459400, grad_norm=4.27987813949585, loss=0.6563679575920105
I0307 19:12:44.255422 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:12:50.302988 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:12:58.920192 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:13:01.233051 139951089751872 submission_runner.py:411] Time since start: 159984.45s, 	Step: 459497, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14027592539787292, 'validation/accuracy': 0.7569999694824219, 'validation/loss': 1.0417706966400146, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8234221935272217, 'test/num_examples': 10000, 'score': 154591.9551639557, 'total_duration': 159984.44529652596, 'accumulated_submission_time': 154591.9551639557, 'accumulated_eval_time': 5352.105293512344, 'accumulated_logging_time': 23.593278408050537}
I0307 19:13:01.325068 139787612706560 logging_writer.py:48] [459497] accumulated_eval_time=5352.105294, accumulated_logging_time=23.593278, accumulated_submission_time=154591.955164, global_step=459497, preemption_count=0, score=154591.955164, test/accuracy=0.632100, test/loss=1.823422, test/num_examples=10000, total_duration=159984.445297, train/accuracy=0.961456, train/loss=0.140276, validation/accuracy=0.757000, validation/loss=1.041771, validation/num_examples=50000
I0307 19:13:02.667796 139787621099264 logging_writer.py:48] [459500] global_step=459500, grad_norm=5.113008499145508, loss=0.6763601899147034
I0307 19:13:36.260223 139787612706560 logging_writer.py:48] [459600] global_step=459600, grad_norm=4.582404613494873, loss=0.5944714546203613
I0307 19:14:09.965772 139787621099264 logging_writer.py:48] [459700] global_step=459700, grad_norm=4.460463047027588, loss=0.6103445291519165
I0307 19:14:43.565342 139787612706560 logging_writer.py:48] [459800] global_step=459800, grad_norm=4.613370418548584, loss=0.6452724933624268
I0307 19:15:17.188197 139787621099264 logging_writer.py:48] [459900] global_step=459900, grad_norm=4.370387077331543, loss=0.6039158701896667
I0307 19:15:50.862295 139787612706560 logging_writer.py:48] [460000] global_step=460000, grad_norm=4.6222710609436035, loss=0.6608433723449707
I0307 19:16:24.508540 139787621099264 logging_writer.py:48] [460100] global_step=460100, grad_norm=5.042872905731201, loss=0.6953511238098145
I0307 19:16:58.140084 139787612706560 logging_writer.py:48] [460200] global_step=460200, grad_norm=4.5362653732299805, loss=0.6485008001327515
I0307 19:17:31.735221 139787621099264 logging_writer.py:48] [460300] global_step=460300, grad_norm=4.247827053070068, loss=0.6731732487678528
I0307 19:18:05.373249 139787612706560 logging_writer.py:48] [460400] global_step=460400, grad_norm=4.631473064422607, loss=0.6216545104980469
I0307 19:18:38.983947 139787621099264 logging_writer.py:48] [460500] global_step=460500, grad_norm=4.461852550506592, loss=0.6456499099731445
I0307 19:19:12.635423 139787612706560 logging_writer.py:48] [460600] global_step=460600, grad_norm=4.483802318572998, loss=0.6507394909858704
I0307 19:19:46.210247 139787621099264 logging_writer.py:48] [460700] global_step=460700, grad_norm=4.505004405975342, loss=0.6444874405860901
I0307 19:20:19.878659 139787612706560 logging_writer.py:48] [460800] global_step=460800, grad_norm=4.964654445648193, loss=0.6326661109924316
I0307 19:20:53.568310 139787621099264 logging_writer.py:48] [460900] global_step=460900, grad_norm=4.733010292053223, loss=0.6043342351913452
I0307 19:21:27.247347 139787612706560 logging_writer.py:48] [461000] global_step=461000, grad_norm=4.076089859008789, loss=0.5762525200843811
I0307 19:21:31.430430 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:21:37.542842 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:21:46.006760 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:21:48.281458 139951089751872 submission_runner.py:411] Time since start: 160511.49s, 	Step: 461014, 	{'train/accuracy': 0.9621133208274841, 'train/loss': 0.1443096399307251, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0408600568771362, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8218690156936646, 'test/num_examples': 10000, 'score': 155101.99497246742, 'total_duration': 160511.493765831, 'accumulated_submission_time': 155101.99497246742, 'accumulated_eval_time': 5368.956280946732, 'accumulated_logging_time': 23.694770097732544}
I0307 19:21:48.372545 139787621099264 logging_writer.py:48] [461014] accumulated_eval_time=5368.956281, accumulated_logging_time=23.694770, accumulated_submission_time=155101.994972, global_step=461014, preemption_count=0, score=155101.994972, test/accuracy=0.631500, test/loss=1.821869, test/num_examples=10000, total_duration=160511.493766, train/accuracy=0.962113, train/loss=0.144310, validation/accuracy=0.757280, validation/loss=1.040860, validation/num_examples=50000
I0307 19:22:17.571025 139789021992704 logging_writer.py:48] [461100] global_step=461100, grad_norm=4.220053195953369, loss=0.6097255349159241
I0307 19:22:51.148512 139787621099264 logging_writer.py:48] [461200] global_step=461200, grad_norm=4.3369245529174805, loss=0.5917765498161316
I0307 19:23:24.705338 139789021992704 logging_writer.py:48] [461300] global_step=461300, grad_norm=4.600794792175293, loss=0.5701773166656494
I0307 19:23:58.291947 139787621099264 logging_writer.py:48] [461400] global_step=461400, grad_norm=4.500633239746094, loss=0.6537876725196838
I0307 19:24:31.948720 139789021992704 logging_writer.py:48] [461500] global_step=461500, grad_norm=4.765865802764893, loss=0.7304075956344604
I0307 19:25:05.557517 139787621099264 logging_writer.py:48] [461600] global_step=461600, grad_norm=4.193657875061035, loss=0.5768588781356812
I0307 19:25:39.216184 139789021992704 logging_writer.py:48] [461700] global_step=461700, grad_norm=4.453277587890625, loss=0.5850899815559387
I0307 19:26:12.866144 139787621099264 logging_writer.py:48] [461800] global_step=461800, grad_norm=5.020251274108887, loss=0.6868489384651184
I0307 19:26:46.517982 139789021992704 logging_writer.py:48] [461900] global_step=461900, grad_norm=4.8004631996154785, loss=0.7224031686782837
I0307 19:27:20.119872 139787621099264 logging_writer.py:48] [462000] global_step=462000, grad_norm=4.699792861938477, loss=0.5915614366531372
I0307 19:27:53.848775 139789021992704 logging_writer.py:48] [462100] global_step=462100, grad_norm=4.7394537925720215, loss=0.6783586740493774
I0307 19:28:27.448127 139787621099264 logging_writer.py:48] [462200] global_step=462200, grad_norm=4.7329912185668945, loss=0.6599990725517273
I0307 19:29:01.092781 139789021992704 logging_writer.py:48] [462300] global_step=462300, grad_norm=4.440921306610107, loss=0.6028890609741211
I0307 19:29:34.729836 139787621099264 logging_writer.py:48] [462400] global_step=462400, grad_norm=4.407929420471191, loss=0.6438107490539551
I0307 19:30:08.392768 139789021992704 logging_writer.py:48] [462500] global_step=462500, grad_norm=4.057431697845459, loss=0.6317414045333862
I0307 19:30:18.303050 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:30:24.374512 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:30:33.407903 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:30:35.687069 139951089751872 submission_runner.py:411] Time since start: 161038.90s, 	Step: 462531, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.1455664485692978, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0417051315307617, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8227357864379883, 'test/num_examples': 10000, 'score': 155611.86061382294, 'total_duration': 161038.8993780613, 'accumulated_submission_time': 155611.86061382294, 'accumulated_eval_time': 5386.340245485306, 'accumulated_logging_time': 23.795379161834717}
I0307 19:30:35.781991 139787629491968 logging_writer.py:48] [462531] accumulated_eval_time=5386.340245, accumulated_logging_time=23.795379, accumulated_submission_time=155611.860614, global_step=462531, preemption_count=0, score=155611.860614, test/accuracy=0.631300, test/loss=1.822736, test/num_examples=10000, total_duration=161038.899378, train/accuracy=0.961217, train/loss=0.145566, validation/accuracy=0.757200, validation/loss=1.041705, validation/num_examples=50000
I0307 19:30:59.349582 139788996814592 logging_writer.py:48] [462600] global_step=462600, grad_norm=4.639403343200684, loss=0.6383236646652222
I0307 19:31:33.015091 139787629491968 logging_writer.py:48] [462700] global_step=462700, grad_norm=4.38087797164917, loss=0.6227928996086121
I0307 19:32:06.683638 139788996814592 logging_writer.py:48] [462800] global_step=462800, grad_norm=4.50612735748291, loss=0.6298896074295044
I0307 19:32:40.422645 139787629491968 logging_writer.py:48] [462900] global_step=462900, grad_norm=4.058755874633789, loss=0.5362788438796997
I0307 19:33:14.009269 139788996814592 logging_writer.py:48] [463000] global_step=463000, grad_norm=5.323291301727295, loss=0.6666788458824158
I0307 19:33:47.610680 139787629491968 logging_writer.py:48] [463100] global_step=463100, grad_norm=4.87966251373291, loss=0.6370940804481506
I0307 19:34:21.314737 139788996814592 logging_writer.py:48] [463200] global_step=463200, grad_norm=4.457770824432373, loss=0.679584264755249
I0307 19:34:54.985576 139787629491968 logging_writer.py:48] [463300] global_step=463300, grad_norm=4.37552547454834, loss=0.6052682399749756
I0307 19:35:28.636986 139788996814592 logging_writer.py:48] [463400] global_step=463400, grad_norm=4.531059741973877, loss=0.6566696166992188
I0307 19:36:02.264834 139787629491968 logging_writer.py:48] [463500] global_step=463500, grad_norm=4.4275312423706055, loss=0.6215724945068359
I0307 19:36:35.877489 139788996814592 logging_writer.py:48] [463600] global_step=463600, grad_norm=4.394705772399902, loss=0.6208869814872742
I0307 19:37:09.528527 139787629491968 logging_writer.py:48] [463700] global_step=463700, grad_norm=4.260789394378662, loss=0.6217979192733765
I0307 19:37:43.152555 139788996814592 logging_writer.py:48] [463800] global_step=463800, grad_norm=4.309625625610352, loss=0.6651480793952942
I0307 19:38:16.771234 139787629491968 logging_writer.py:48] [463900] global_step=463900, grad_norm=4.262105941772461, loss=0.512955367565155
I0307 19:38:50.536397 139788996814592 logging_writer.py:48] [464000] global_step=464000, grad_norm=4.095573902130127, loss=0.5688583254814148
I0307 19:39:05.831918 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:39:11.873554 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:39:20.367039 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:39:22.667124 139951089751872 submission_runner.py:411] Time since start: 161565.88s, 	Step: 464047, 	{'train/accuracy': 0.9599210619926453, 'train/loss': 0.14859415590763092, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.0420114994049072, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8246040344238281, 'test/num_examples': 10000, 'score': 156121.84329080582, 'total_duration': 161565.87942004204, 'accumulated_submission_time': 156121.84329080582, 'accumulated_eval_time': 5403.175386667252, 'accumulated_logging_time': 23.901411294937134}
I0307 19:39:22.753503 139787612706560 logging_writer.py:48] [464047] accumulated_eval_time=5403.175387, accumulated_logging_time=23.901411, accumulated_submission_time=156121.843291, global_step=464047, preemption_count=0, score=156121.843291, test/accuracy=0.631800, test/loss=1.824604, test/num_examples=10000, total_duration=161565.879420, train/accuracy=0.959921, train/loss=0.148594, validation/accuracy=0.757100, validation/loss=1.042011, validation/num_examples=50000
I0307 19:39:40.956573 139787621099264 logging_writer.py:48] [464100] global_step=464100, grad_norm=5.0515055656433105, loss=0.6679155826568604
I0307 19:40:14.574591 139787612706560 logging_writer.py:48] [464200] global_step=464200, grad_norm=4.807811737060547, loss=0.6380962133407593
I0307 19:40:48.147278 139787621099264 logging_writer.py:48] [464300] global_step=464300, grad_norm=5.022334575653076, loss=0.700094997882843
I0307 19:41:21.706881 139787612706560 logging_writer.py:48] [464400] global_step=464400, grad_norm=4.5662431716918945, loss=0.6262925863265991
I0307 19:41:55.300348 139787621099264 logging_writer.py:48] [464500] global_step=464500, grad_norm=4.4018731117248535, loss=0.6509937644004822
I0307 19:42:28.946980 139787612706560 logging_writer.py:48] [464600] global_step=464600, grad_norm=3.8745594024658203, loss=0.6048337817192078
I0307 19:43:02.603883 139787621099264 logging_writer.py:48] [464700] global_step=464700, grad_norm=3.872441530227661, loss=0.48977363109588623
I0307 19:43:36.284730 139787612706560 logging_writer.py:48] [464800] global_step=464800, grad_norm=4.942475318908691, loss=0.5842550992965698
I0307 19:44:09.925624 139787621099264 logging_writer.py:48] [464900] global_step=464900, grad_norm=4.864206790924072, loss=0.6295824646949768
I0307 19:44:43.672950 139787612706560 logging_writer.py:48] [465000] global_step=465000, grad_norm=4.425806045532227, loss=0.6125683784484863
I0307 19:45:17.255649 139787621099264 logging_writer.py:48] [465100] global_step=465100, grad_norm=5.143248081207275, loss=0.7035601139068604
I0307 19:45:50.802463 139787612706560 logging_writer.py:48] [465200] global_step=465200, grad_norm=4.502162933349609, loss=0.5657792091369629
I0307 19:46:24.384007 139787621099264 logging_writer.py:48] [465300] global_step=465300, grad_norm=4.8309478759765625, loss=0.5766116976737976
I0307 19:46:58.030896 139787612706560 logging_writer.py:48] [465400] global_step=465400, grad_norm=4.311849594116211, loss=0.5903506278991699
I0307 19:47:31.633411 139787621099264 logging_writer.py:48] [465500] global_step=465500, grad_norm=4.5547027587890625, loss=0.5848842859268188
I0307 19:47:52.968739 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:47:59.024866 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:48:07.632979 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:48:10.051063 139951089751872 submission_runner.py:411] Time since start: 162093.26s, 	Step: 465565, 	{'train/accuracy': 0.960359513759613, 'train/loss': 0.14635543525218964, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0423952341079712, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8224778175354004, 'test/num_examples': 10000, 'score': 156631.992497921, 'total_duration': 162093.26338124275, 'accumulated_submission_time': 156631.992497921, 'accumulated_eval_time': 5420.25766825676, 'accumulated_logging_time': 23.998153924942017}
I0307 19:48:10.125294 139787629491968 logging_writer.py:48] [465565] accumulated_eval_time=5420.257668, accumulated_logging_time=23.998154, accumulated_submission_time=156631.992498, global_step=465565, preemption_count=0, score=156631.992498, test/accuracy=0.631300, test/loss=1.822478, test/num_examples=10000, total_duration=162093.263381, train/accuracy=0.960360, train/loss=0.146355, validation/accuracy=0.757280, validation/loss=1.042395, validation/num_examples=50000
I0307 19:48:22.210915 139789013600000 logging_writer.py:48] [465600] global_step=465600, grad_norm=5.043511867523193, loss=0.6829959750175476
I0307 19:48:55.815389 139787629491968 logging_writer.py:48] [465700] global_step=465700, grad_norm=4.2650837898254395, loss=0.6049510836601257
I0307 19:49:29.465368 139789013600000 logging_writer.py:48] [465800] global_step=465800, grad_norm=4.85468864440918, loss=0.6522085070610046
I0307 19:50:03.122462 139787629491968 logging_writer.py:48] [465900] global_step=465900, grad_norm=4.537757396697998, loss=0.6170867681503296
I0307 19:50:36.797127 139789013600000 logging_writer.py:48] [466000] global_step=466000, grad_norm=4.413673400878906, loss=0.6386611461639404
I0307 19:51:10.483480 139787629491968 logging_writer.py:48] [466100] global_step=466100, grad_norm=4.60926628112793, loss=0.5594632625579834
I0307 19:51:44.065689 139789013600000 logging_writer.py:48] [466200] global_step=466200, grad_norm=4.567558765411377, loss=0.5861443877220154
I0307 19:52:17.677305 139787629491968 logging_writer.py:48] [466300] global_step=466300, grad_norm=4.388339042663574, loss=0.6316958665847778
I0307 19:52:51.345247 139789013600000 logging_writer.py:48] [466400] global_step=466400, grad_norm=4.616647720336914, loss=0.541866660118103
I0307 19:53:25.002185 139787629491968 logging_writer.py:48] [466500] global_step=466500, grad_norm=4.568459510803223, loss=0.6693088412284851
I0307 19:53:58.656013 139789013600000 logging_writer.py:48] [466600] global_step=466600, grad_norm=4.439530849456787, loss=0.6674602627754211
I0307 19:54:32.315035 139787629491968 logging_writer.py:48] [466700] global_step=466700, grad_norm=4.711964130401611, loss=0.705534815788269
I0307 19:55:05.970802 139789013600000 logging_writer.py:48] [466800] global_step=466800, grad_norm=4.358971118927002, loss=0.6195855140686035
I0307 19:55:39.637397 139787629491968 logging_writer.py:48] [466900] global_step=466900, grad_norm=4.113766193389893, loss=0.5919899940490723
I0307 19:56:13.308329 139789013600000 logging_writer.py:48] [467000] global_step=467000, grad_norm=4.6565632820129395, loss=0.6234186291694641
I0307 19:56:40.330355 139951089751872 spec.py:321] Evaluating on the training split.
I0307 19:56:46.326347 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 19:56:54.889600 139951089751872 spec.py:349] Evaluating on the test split.
I0307 19:56:57.211053 139951089751872 submission_runner.py:411] Time since start: 162620.42s, 	Step: 467082, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14323443174362183, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0408964157104492, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8228623867034912, 'test/num_examples': 10000, 'score': 157142.13254094124, 'total_duration': 162620.42335128784, 'accumulated_submission_time': 157142.13254094124, 'accumulated_eval_time': 5437.138303518295, 'accumulated_logging_time': 24.08114218711853}
I0307 19:56:57.306186 139787629491968 logging_writer.py:48] [467082] accumulated_eval_time=5437.138304, accumulated_logging_time=24.081142, accumulated_submission_time=157142.132541, global_step=467082, preemption_count=0, score=157142.132541, test/accuracy=0.631500, test/loss=1.822862, test/num_examples=10000, total_duration=162620.423351, train/accuracy=0.961635, train/loss=0.143234, validation/accuracy=0.757280, validation/loss=1.040896, validation/num_examples=50000
I0307 19:57:03.739843 139788996814592 logging_writer.py:48] [467100] global_step=467100, grad_norm=4.762859344482422, loss=0.6102117896080017
I0307 19:57:37.306939 139787629491968 logging_writer.py:48] [467200] global_step=467200, grad_norm=4.409711837768555, loss=0.6993314623832703
I0307 19:58:10.974310 139788996814592 logging_writer.py:48] [467300] global_step=467300, grad_norm=4.9155802726745605, loss=0.780704140663147
I0307 19:58:44.582178 139787629491968 logging_writer.py:48] [467400] global_step=467400, grad_norm=4.476949214935303, loss=0.6665563583374023
I0307 19:59:18.226246 139788996814592 logging_writer.py:48] [467500] global_step=467500, grad_norm=5.058712959289551, loss=0.6316745281219482
I0307 19:59:51.829957 139787629491968 logging_writer.py:48] [467600] global_step=467600, grad_norm=4.340231895446777, loss=0.6259341835975647
I0307 20:00:25.464265 139788996814592 logging_writer.py:48] [467700] global_step=467700, grad_norm=3.9954211711883545, loss=0.4972077012062073
I0307 20:00:59.071884 139787629491968 logging_writer.py:48] [467800] global_step=467800, grad_norm=4.247135162353516, loss=0.653691291809082
I0307 20:01:32.714466 139788996814592 logging_writer.py:48] [467900] global_step=467900, grad_norm=4.735192775726318, loss=0.675064206123352
I0307 20:02:06.339530 139787629491968 logging_writer.py:48] [468000] global_step=468000, grad_norm=4.370176792144775, loss=0.633539080619812
I0307 20:02:39.998882 139788996814592 logging_writer.py:48] [468100] global_step=468100, grad_norm=4.445068836212158, loss=0.6792750358581543
I0307 20:03:13.669819 139787629491968 logging_writer.py:48] [468200] global_step=468200, grad_norm=4.3853254318237305, loss=0.6198778748512268
I0307 20:03:47.283537 139788996814592 logging_writer.py:48] [468300] global_step=468300, grad_norm=4.437877178192139, loss=0.602276086807251
I0307 20:04:20.879868 139787629491968 logging_writer.py:48] [468400] global_step=468400, grad_norm=4.604592800140381, loss=0.7050027251243591
I0307 20:04:54.447142 139788996814592 logging_writer.py:48] [468500] global_step=468500, grad_norm=4.243216514587402, loss=0.5179142355918884
I0307 20:05:27.494723 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:05:33.531864 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:05:42.000231 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:05:44.269805 139951089751872 submission_runner.py:411] Time since start: 163147.48s, 	Step: 468600, 	{'train/accuracy': 0.9614157676696777, 'train/loss': 0.14622309803962708, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0402354001998901, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8226418495178223, 'test/num_examples': 10000, 'score': 157652.25593709946, 'total_duration': 163147.48210787773, 'accumulated_submission_time': 157652.25593709946, 'accumulated_eval_time': 5453.9133331775665, 'accumulated_logging_time': 24.18593978881836}
I0307 20:05:44.362985 139787621099264 logging_writer.py:48] [468600] accumulated_eval_time=5453.913333, accumulated_logging_time=24.185940, accumulated_submission_time=157652.255937, global_step=468600, preemption_count=0, score=157652.255937, test/accuracy=0.632500, test/loss=1.822642, test/num_examples=10000, total_duration=163147.482108, train/accuracy=0.961416, train/loss=0.146223, validation/accuracy=0.757160, validation/loss=1.040235, validation/num_examples=50000
I0307 20:05:44.712824 139789021992704 logging_writer.py:48] [468600] global_step=468600, grad_norm=4.8534722328186035, loss=0.7150750160217285
I0307 20:06:18.299485 139787621099264 logging_writer.py:48] [468700] global_step=468700, grad_norm=4.608249664306641, loss=0.64886474609375
I0307 20:06:51.861984 139789021992704 logging_writer.py:48] [468800] global_step=468800, grad_norm=4.265173435211182, loss=0.5936366319656372
I0307 20:07:25.451202 139787621099264 logging_writer.py:48] [468900] global_step=468900, grad_norm=4.558751583099365, loss=0.6786536574363708
I0307 20:07:59.060187 139789021992704 logging_writer.py:48] [469000] global_step=469000, grad_norm=4.392119884490967, loss=0.566422700881958
I0307 20:08:32.713922 139787621099264 logging_writer.py:48] [469100] global_step=469100, grad_norm=4.2992072105407715, loss=0.6244227886199951
I0307 20:09:06.354819 139789021992704 logging_writer.py:48] [469200] global_step=469200, grad_norm=4.437038421630859, loss=0.5986862182617188
I0307 20:09:40.170266 139787621099264 logging_writer.py:48] [469300] global_step=469300, grad_norm=4.502679347991943, loss=0.6074999570846558
I0307 20:10:13.839189 139789021992704 logging_writer.py:48] [469400] global_step=469400, grad_norm=4.05593729019165, loss=0.5739867687225342
I0307 20:10:47.509578 139787621099264 logging_writer.py:48] [469500] global_step=469500, grad_norm=4.813233375549316, loss=0.6004555225372314
I0307 20:11:21.191324 139789021992704 logging_writer.py:48] [469600] global_step=469600, grad_norm=4.278913497924805, loss=0.6118181943893433
I0307 20:11:54.877150 139787621099264 logging_writer.py:48] [469700] global_step=469700, grad_norm=4.4036359786987305, loss=0.574396014213562
I0307 20:12:28.556589 139789021992704 logging_writer.py:48] [469800] global_step=469800, grad_norm=4.4400739669799805, loss=0.5977814197540283
I0307 20:13:02.232101 139787621099264 logging_writer.py:48] [469900] global_step=469900, grad_norm=4.506206035614014, loss=0.6471819877624512
I0307 20:13:35.870562 139789021992704 logging_writer.py:48] [470000] global_step=470000, grad_norm=4.5459771156311035, loss=0.6331189274787903
I0307 20:14:09.498529 139787621099264 logging_writer.py:48] [470100] global_step=470100, grad_norm=4.885868072509766, loss=0.6546750664710999
I0307 20:14:14.347201 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:14:20.406106 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:14:29.055736 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:14:31.318657 139951089751872 submission_runner.py:411] Time since start: 163674.53s, 	Step: 470116, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.1469157487154007, 'validation/accuracy': 0.7570599913597107, 'validation/loss': 1.0405776500701904, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8218297958374023, 'test/num_examples': 10000, 'score': 158162.1717247963, 'total_duration': 163674.5309638977, 'accumulated_submission_time': 158162.1717247963, 'accumulated_eval_time': 5470.884748220444, 'accumulated_logging_time': 24.29122257232666}
I0307 20:14:31.414454 139787612706560 logging_writer.py:48] [470116] accumulated_eval_time=5470.884748, accumulated_logging_time=24.291223, accumulated_submission_time=158162.171725, global_step=470116, preemption_count=0, score=158162.171725, test/accuracy=0.630800, test/loss=1.821830, test/num_examples=10000, total_duration=163674.530964, train/accuracy=0.961655, train/loss=0.146916, validation/accuracy=0.757060, validation/loss=1.040578, validation/num_examples=50000
I0307 20:14:59.965023 139787621099264 logging_writer.py:48] [470200] global_step=470200, grad_norm=4.928593158721924, loss=0.6466189622879028
I0307 20:15:33.644936 139787612706560 logging_writer.py:48] [470300] global_step=470300, grad_norm=4.6151347160339355, loss=0.6302861571311951
I0307 20:16:07.313990 139787621099264 logging_writer.py:48] [470400] global_step=470400, grad_norm=4.841762542724609, loss=0.644612729549408
I0307 20:16:40.911162 139787612706560 logging_writer.py:48] [470500] global_step=470500, grad_norm=4.460119724273682, loss=0.65644770860672
I0307 20:17:14.573484 139787621099264 logging_writer.py:48] [470600] global_step=470600, grad_norm=4.474305152893066, loss=0.6249712109565735
I0307 20:17:48.207281 139787612706560 logging_writer.py:48] [470700] global_step=470700, grad_norm=4.148146629333496, loss=0.6148540377616882
I0307 20:18:21.883651 139787621099264 logging_writer.py:48] [470800] global_step=470800, grad_norm=4.279003143310547, loss=0.5993324518203735
I0307 20:18:55.499940 139787612706560 logging_writer.py:48] [470900] global_step=470900, grad_norm=4.032461643218994, loss=0.5734184980392456
I0307 20:19:29.157861 139787621099264 logging_writer.py:48] [471000] global_step=471000, grad_norm=4.056141376495361, loss=0.5830813646316528
I0307 20:20:02.767395 139787612706560 logging_writer.py:48] [471100] global_step=471100, grad_norm=4.388075351715088, loss=0.5896925330162048
I0307 20:20:36.433314 139787621099264 logging_writer.py:48] [471200] global_step=471200, grad_norm=4.594585418701172, loss=0.5641598701477051
I0307 20:21:10.038468 139787612706560 logging_writer.py:48] [471300] global_step=471300, grad_norm=4.281946659088135, loss=0.600055992603302
I0307 20:21:43.712371 139787621099264 logging_writer.py:48] [471400] global_step=471400, grad_norm=4.738805770874023, loss=0.6264517307281494
I0307 20:22:17.326022 139787612706560 logging_writer.py:48] [471500] global_step=471500, grad_norm=4.103862285614014, loss=0.5788601636886597
I0307 20:22:50.947650 139787621099264 logging_writer.py:48] [471600] global_step=471600, grad_norm=4.339022636413574, loss=0.6085841059684753
I0307 20:23:01.520104 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:23:07.642936 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:23:16.105212 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:23:18.376429 139951089751872 submission_runner.py:411] Time since start: 164201.59s, 	Step: 471633, 	{'train/accuracy': 0.960379421710968, 'train/loss': 0.1477285623550415, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0409228801727295, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8239001035690308, 'test/num_examples': 10000, 'score': 158672.21157503128, 'total_duration': 164201.5887157917, 'accumulated_submission_time': 158672.21157503128, 'accumulated_eval_time': 5487.740997314453, 'accumulated_logging_time': 24.396308660507202}
I0307 20:23:18.472783 139789021992704 logging_writer.py:48] [471633] accumulated_eval_time=5487.740997, accumulated_logging_time=24.396309, accumulated_submission_time=158672.211575, global_step=471633, preemption_count=0, score=158672.211575, test/accuracy=0.632100, test/loss=1.823900, test/num_examples=10000, total_duration=164201.588716, train/accuracy=0.960379, train/loss=0.147729, validation/accuracy=0.757340, validation/loss=1.040923, validation/num_examples=50000
I0307 20:23:41.306988 139789030385408 logging_writer.py:48] [471700] global_step=471700, grad_norm=4.990851879119873, loss=0.6588354110717773
I0307 20:24:14.893853 139789021992704 logging_writer.py:48] [471800] global_step=471800, grad_norm=4.666316032409668, loss=0.6275005340576172
I0307 20:24:48.523252 139789030385408 logging_writer.py:48] [471900] global_step=471900, grad_norm=4.417108535766602, loss=0.6147445440292358
I0307 20:25:22.148594 139789021992704 logging_writer.py:48] [472000] global_step=472000, grad_norm=4.399752616882324, loss=0.542233407497406
I0307 20:25:55.781107 139789030385408 logging_writer.py:48] [472100] global_step=472100, grad_norm=4.549747467041016, loss=0.6532686352729797
I0307 20:26:29.422826 139789021992704 logging_writer.py:48] [472200] global_step=472200, grad_norm=4.466256618499756, loss=0.6187207698822021
I0307 20:27:03.060198 139789030385408 logging_writer.py:48] [472300] global_step=472300, grad_norm=4.197723865509033, loss=0.5407477617263794
I0307 20:27:36.690030 139789021992704 logging_writer.py:48] [472400] global_step=472400, grad_norm=5.085745811462402, loss=0.6557022333145142
I0307 20:28:10.346084 139789030385408 logging_writer.py:48] [472500] global_step=472500, grad_norm=4.079816818237305, loss=0.5825047492980957
I0307 20:28:43.934063 139789021992704 logging_writer.py:48] [472600] global_step=472600, grad_norm=4.698841094970703, loss=0.6888617277145386
I0307 20:29:17.552502 139789030385408 logging_writer.py:48] [472700] global_step=472700, grad_norm=4.194586277008057, loss=0.5354059934616089
I0307 20:29:51.232715 139789021992704 logging_writer.py:48] [472800] global_step=472800, grad_norm=4.080839157104492, loss=0.58568274974823
I0307 20:30:24.906211 139789030385408 logging_writer.py:48] [472900] global_step=472900, grad_norm=4.357634544372559, loss=0.6150990128517151
I0307 20:30:58.559778 139789021992704 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.755590915679932, loss=0.6481366157531738
I0307 20:31:32.254042 139789030385408 logging_writer.py:48] [473100] global_step=473100, grad_norm=4.675108909606934, loss=0.6396399736404419
I0307 20:31:48.566661 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:31:54.614725 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:32:03.218179 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:32:05.478849 139951089751872 submission_runner.py:411] Time since start: 164728.69s, 	Step: 473150, 	{'train/accuracy': 0.9608178734779358, 'train/loss': 0.14504989981651306, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0415680408477783, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8239738941192627, 'test/num_examples': 10000, 'score': 159182.24066758156, 'total_duration': 164728.6911456585, 'accumulated_submission_time': 159182.24066758156, 'accumulated_eval_time': 5504.653124094009, 'accumulated_logging_time': 24.502280473709106}
I0307 20:32:05.571944 139787621099264 logging_writer.py:48] [473150] accumulated_eval_time=5504.653124, accumulated_logging_time=24.502280, accumulated_submission_time=159182.240668, global_step=473150, preemption_count=0, score=159182.240668, test/accuracy=0.631500, test/loss=1.823974, test/num_examples=10000, total_duration=164728.691146, train/accuracy=0.960818, train/loss=0.145050, validation/accuracy=0.757260, validation/loss=1.041568, validation/num_examples=50000
I0307 20:32:22.699447 139787629491968 logging_writer.py:48] [473200] global_step=473200, grad_norm=4.608630657196045, loss=0.6646280288696289
I0307 20:32:56.368337 139787621099264 logging_writer.py:48] [473300] global_step=473300, grad_norm=4.38613748550415, loss=0.5793659090995789
I0307 20:33:30.015370 139787629491968 logging_writer.py:48] [473400] global_step=473400, grad_norm=4.567057132720947, loss=0.6513104438781738
I0307 20:34:03.706021 139787621099264 logging_writer.py:48] [473500] global_step=473500, grad_norm=4.434995651245117, loss=0.6214036345481873
I0307 20:34:37.319714 139787629491968 logging_writer.py:48] [473600] global_step=473600, grad_norm=4.77316951751709, loss=0.6986144781112671
I0307 20:35:10.956386 139787621099264 logging_writer.py:48] [473700] global_step=473700, grad_norm=4.4353346824646, loss=0.5843357443809509
I0307 20:35:44.563939 139787629491968 logging_writer.py:48] [473800] global_step=473800, grad_norm=5.208858013153076, loss=0.6431363821029663
I0307 20:36:18.209177 139787621099264 logging_writer.py:48] [473900] global_step=473900, grad_norm=4.9057416915893555, loss=0.6304139494895935
I0307 20:36:51.832337 139787629491968 logging_writer.py:48] [474000] global_step=474000, grad_norm=4.953953742980957, loss=0.6902925968170166
I0307 20:37:25.510419 139787621099264 logging_writer.py:48] [474100] global_step=474100, grad_norm=4.371723651885986, loss=0.5981240272521973
I0307 20:37:59.179617 139787629491968 logging_writer.py:48] [474200] global_step=474200, grad_norm=4.402281761169434, loss=0.5455617308616638
I0307 20:38:32.837803 139787621099264 logging_writer.py:48] [474300] global_step=474300, grad_norm=4.645679950714111, loss=0.6952520608901978
I0307 20:39:06.458100 139787629491968 logging_writer.py:48] [474400] global_step=474400, grad_norm=4.546600341796875, loss=0.6363281011581421
I0307 20:39:40.124938 139787621099264 logging_writer.py:48] [474500] global_step=474500, grad_norm=4.811621189117432, loss=0.6515287160873413
I0307 20:40:13.762487 139787629491968 logging_writer.py:48] [474600] global_step=474600, grad_norm=4.471318244934082, loss=0.598139762878418
I0307 20:40:35.735926 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:40:41.732414 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:40:50.363890 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:40:52.647507 139951089751872 submission_runner.py:411] Time since start: 165255.86s, 	Step: 474667, 	{'train/accuracy': 0.9606385231018066, 'train/loss': 0.14740562438964844, 'validation/accuracy': 0.7566199898719788, 'validation/loss': 1.0405768156051636, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8208348751068115, 'test/num_examples': 10000, 'score': 159692.33936095238, 'total_duration': 165255.85982394218, 'accumulated_submission_time': 159692.33936095238, 'accumulated_eval_time': 5521.564659357071, 'accumulated_logging_time': 24.605119466781616}
I0307 20:40:52.741599 139787612706560 logging_writer.py:48] [474667] accumulated_eval_time=5521.564659, accumulated_logging_time=24.605119, accumulated_submission_time=159692.339361, global_step=474667, preemption_count=0, score=159692.339361, test/accuracy=0.631200, test/loss=1.820835, test/num_examples=10000, total_duration=165255.859824, train/accuracy=0.960639, train/loss=0.147406, validation/accuracy=0.756620, validation/loss=1.040577, validation/num_examples=50000
I0307 20:41:04.148796 139787621099264 logging_writer.py:48] [474700] global_step=474700, grad_norm=4.746623992919922, loss=0.6652383208274841
I0307 20:41:37.735739 139787612706560 logging_writer.py:48] [474800] global_step=474800, grad_norm=5.224112033843994, loss=0.6831680536270142
I0307 20:42:11.364248 139787621099264 logging_writer.py:48] [474900] global_step=474900, grad_norm=4.836897373199463, loss=0.6978170275688171
I0307 20:42:45.003160 139787612706560 logging_writer.py:48] [475000] global_step=475000, grad_norm=4.542320728302002, loss=0.571068525314331
I0307 20:43:18.664250 139787621099264 logging_writer.py:48] [475100] global_step=475100, grad_norm=4.2655816078186035, loss=0.6266663670539856
I0307 20:43:52.302285 139787612706560 logging_writer.py:48] [475200] global_step=475200, grad_norm=4.3131537437438965, loss=0.6028794646263123
I0307 20:44:25.905823 139787621099264 logging_writer.py:48] [475300] global_step=475300, grad_norm=4.3763837814331055, loss=0.5907309651374817
I0307 20:44:59.527671 139787612706560 logging_writer.py:48] [475400] global_step=475400, grad_norm=4.7182793617248535, loss=0.7029401063919067
I0307 20:45:33.175866 139787621099264 logging_writer.py:48] [475500] global_step=475500, grad_norm=4.663710117340088, loss=0.6165076494216919
I0307 20:46:06.880480 139787612706560 logging_writer.py:48] [475600] global_step=475600, grad_norm=4.5460357666015625, loss=0.6132940053939819
I0307 20:46:40.472355 139787621099264 logging_writer.py:48] [475700] global_step=475700, grad_norm=4.953681945800781, loss=0.6249139308929443
I0307 20:47:14.129197 139787612706560 logging_writer.py:48] [475800] global_step=475800, grad_norm=4.3773040771484375, loss=0.6575002074241638
I0307 20:47:47.771266 139787621099264 logging_writer.py:48] [475900] global_step=475900, grad_norm=4.690738677978516, loss=0.6174393892288208
I0307 20:48:21.435273 139787612706560 logging_writer.py:48] [476000] global_step=476000, grad_norm=4.33138370513916, loss=0.658690333366394
I0307 20:48:55.048117 139787621099264 logging_writer.py:48] [476100] global_step=476100, grad_norm=4.459291458129883, loss=0.6276839375495911
I0307 20:49:22.779501 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:49:28.841835 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:49:37.372804 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:49:39.644546 139951089751872 submission_runner.py:411] Time since start: 165782.86s, 	Step: 476184, 	{'train/accuracy': 0.9614556431770325, 'train/loss': 0.14442038536071777, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.0410040616989136, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8233256340026855, 'test/num_examples': 10000, 'score': 160202.31282019615, 'total_duration': 165782.85601115227, 'accumulated_submission_time': 160202.31282019615, 'accumulated_eval_time': 5538.428807735443, 'accumulated_logging_time': 24.70886778831482}
I0307 20:49:39.736603 139789021992704 logging_writer.py:48] [476184] accumulated_eval_time=5538.428808, accumulated_logging_time=24.708868, accumulated_submission_time=160202.312820, global_step=476184, preemption_count=0, score=160202.312820, test/accuracy=0.631200, test/loss=1.823326, test/num_examples=10000, total_duration=165782.856011, train/accuracy=0.961456, train/loss=0.144420, validation/accuracy=0.757240, validation/loss=1.041004, validation/num_examples=50000
I0307 20:49:45.473648 139789030385408 logging_writer.py:48] [476200] global_step=476200, grad_norm=4.850484371185303, loss=0.6143154501914978
I0307 20:50:19.131655 139789021992704 logging_writer.py:48] [476300] global_step=476300, grad_norm=4.411500453948975, loss=0.5943597555160522
I0307 20:50:52.743076 139789030385408 logging_writer.py:48] [476400] global_step=476400, grad_norm=4.480625629425049, loss=0.6830899119377136
I0307 20:51:26.330772 139789021992704 logging_writer.py:48] [476500] global_step=476500, grad_norm=4.20908260345459, loss=0.5859290361404419
I0307 20:51:59.861257 139789030385408 logging_writer.py:48] [476600] global_step=476600, grad_norm=4.265913486480713, loss=0.5916255116462708
I0307 20:52:33.493907 139789021992704 logging_writer.py:48] [476700] global_step=476700, grad_norm=4.522035121917725, loss=0.6222586035728455
I0307 20:53:07.064027 139789030385408 logging_writer.py:48] [476800] global_step=476800, grad_norm=5.163707733154297, loss=0.651273250579834
I0307 20:53:40.648748 139789021992704 logging_writer.py:48] [476900] global_step=476900, grad_norm=4.871825695037842, loss=0.6748358011245728
I0307 20:54:14.313558 139789030385408 logging_writer.py:48] [477000] global_step=477000, grad_norm=4.571259021759033, loss=0.5968416333198547
I0307 20:54:47.927550 139789021992704 logging_writer.py:48] [477100] global_step=477100, grad_norm=4.869001388549805, loss=0.620365560054779
I0307 20:55:21.586378 139789030385408 logging_writer.py:48] [477200] global_step=477200, grad_norm=4.605016231536865, loss=0.6323052644729614
I0307 20:55:55.196488 139789021992704 logging_writer.py:48] [477300] global_step=477300, grad_norm=4.797463893890381, loss=0.6794129610061646
I0307 20:56:28.854062 139789030385408 logging_writer.py:48] [477400] global_step=477400, grad_norm=4.170588970184326, loss=0.6659561991691589
I0307 20:57:02.488087 139789021992704 logging_writer.py:48] [477500] global_step=477500, grad_norm=4.744338035583496, loss=0.6108446717262268
I0307 20:57:36.153200 139789030385408 logging_writer.py:48] [477600] global_step=477600, grad_norm=5.5334296226501465, loss=0.6130045056343079
I0307 20:58:09.761120 139789021992704 logging_writer.py:48] [477700] global_step=477700, grad_norm=5.03560209274292, loss=0.6617911458015442
I0307 20:58:09.767618 139951089751872 spec.py:321] Evaluating on the training split.
I0307 20:58:16.025419 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 20:58:24.523890 139951089751872 spec.py:349] Evaluating on the test split.
I0307 20:58:26.780780 139951089751872 submission_runner.py:411] Time since start: 166309.99s, 	Step: 477701, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14844757318496704, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0417107343673706, 'validation/num_examples': 50000, 'test/accuracy': 0.6321000456809998, 'test/loss': 1.8233537673950195, 'test/num_examples': 10000, 'score': 160712.27799630165, 'total_duration': 166309.99308776855, 'accumulated_submission_time': 160712.27799630165, 'accumulated_eval_time': 5555.441893815994, 'accumulated_logging_time': 24.811065435409546}
I0307 20:58:26.875894 139787612706560 logging_writer.py:48] [477701] accumulated_eval_time=5555.441894, accumulated_logging_time=24.811065, accumulated_submission_time=160712.277996, global_step=477701, preemption_count=0, score=160712.277996, test/accuracy=0.632100, test/loss=1.823354, test/num_examples=10000, total_duration=166309.993088, train/accuracy=0.959881, train/loss=0.148448, validation/accuracy=0.757340, validation/loss=1.041711, validation/num_examples=50000
I0307 20:59:00.538347 139787621099264 logging_writer.py:48] [477800] global_step=477800, grad_norm=4.599193572998047, loss=0.6273756623268127
I0307 20:59:34.190047 139787612706560 logging_writer.py:48] [477900] global_step=477900, grad_norm=4.677163124084473, loss=0.6391720771789551
I0307 21:00:07.845826 139787621099264 logging_writer.py:48] [478000] global_step=478000, grad_norm=4.421579837799072, loss=0.6376640796661377
I0307 21:00:41.490075 139787612706560 logging_writer.py:48] [478100] global_step=478100, grad_norm=4.903130531311035, loss=0.656934380531311
I0307 21:01:15.138316 139787621099264 logging_writer.py:48] [478200] global_step=478200, grad_norm=4.700564384460449, loss=0.7012730836868286
I0307 21:01:48.953456 139787612706560 logging_writer.py:48] [478300] global_step=478300, grad_norm=4.246599197387695, loss=0.6108893156051636
I0307 21:02:22.542106 139787621099264 logging_writer.py:48] [478400] global_step=478400, grad_norm=4.399048328399658, loss=0.6424118280410767
I0307 21:02:56.172488 139787612706560 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.3694353103637695, loss=0.6665194630622864
I0307 21:03:29.797359 139787621099264 logging_writer.py:48] [478600] global_step=478600, grad_norm=4.614205837249756, loss=0.6847658753395081
I0307 21:04:03.420975 139787612706560 logging_writer.py:48] [478700] global_step=478700, grad_norm=4.883131504058838, loss=0.6517186164855957
I0307 21:04:37.093232 139787621099264 logging_writer.py:48] [478800] global_step=478800, grad_norm=5.215462684631348, loss=0.6712450981140137
I0307 21:05:10.703562 139787612706560 logging_writer.py:48] [478900] global_step=478900, grad_norm=4.78004264831543, loss=0.6832049489021301
I0307 21:05:44.311603 139787621099264 logging_writer.py:48] [479000] global_step=479000, grad_norm=4.517378330230713, loss=0.6557048559188843
I0307 21:06:17.876272 139787612706560 logging_writer.py:48] [479100] global_step=479100, grad_norm=5.121448516845703, loss=0.6719166040420532
I0307 21:06:51.464996 139787621099264 logging_writer.py:48] [479200] global_step=479200, grad_norm=4.420804977416992, loss=0.6087929010391235
I0307 21:06:56.991662 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:07:03.074839 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:07:11.705362 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:07:13.989711 139951089751872 submission_runner.py:411] Time since start: 166837.20s, 	Step: 479218, 	{'train/accuracy': 0.9594228267669678, 'train/loss': 0.14839741587638855, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0413779020309448, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8240758180618286, 'test/num_examples': 10000, 'score': 161222.3278567791, 'total_duration': 166837.20202589035, 'accumulated_submission_time': 161222.3278567791, 'accumulated_eval_time': 5572.439893722534, 'accumulated_logging_time': 24.91554069519043}
I0307 21:07:14.086917 139789005207296 logging_writer.py:48] [479218] accumulated_eval_time=5572.439894, accumulated_logging_time=24.915541, accumulated_submission_time=161222.327857, global_step=479218, preemption_count=0, score=161222.327857, test/accuracy=0.631600, test/loss=1.824076, test/num_examples=10000, total_duration=166837.202026, train/accuracy=0.959423, train/loss=0.148397, validation/accuracy=0.757040, validation/loss=1.041378, validation/num_examples=50000
I0307 21:07:41.959625 139789013600000 logging_writer.py:48] [479300] global_step=479300, grad_norm=4.058174133300781, loss=0.5795368552207947
I0307 21:08:15.505781 139789005207296 logging_writer.py:48] [479400] global_step=479400, grad_norm=4.372077465057373, loss=0.6586170792579651
I0307 21:08:49.084236 139789013600000 logging_writer.py:48] [479500] global_step=479500, grad_norm=5.046792984008789, loss=0.7206481695175171
I0307 21:09:22.681548 139789005207296 logging_writer.py:48] [479600] global_step=479600, grad_norm=4.107449531555176, loss=0.5776688456535339
I0307 21:09:56.343760 139789013600000 logging_writer.py:48] [479700] global_step=479700, grad_norm=4.741391658782959, loss=0.6569414734840393
I0307 21:10:29.965547 139789005207296 logging_writer.py:48] [479800] global_step=479800, grad_norm=4.339164733886719, loss=0.6120038628578186
I0307 21:11:03.815334 139789013600000 logging_writer.py:48] [479900] global_step=479900, grad_norm=4.79562520980835, loss=0.6466028094291687
I0307 21:11:37.452147 139789005207296 logging_writer.py:48] [480000] global_step=480000, grad_norm=3.903503656387329, loss=0.5615228414535522
I0307 21:12:11.106890 139789013600000 logging_writer.py:48] [480100] global_step=480100, grad_norm=4.17985200881958, loss=0.5538550019264221
I0307 21:12:44.701150 139789005207296 logging_writer.py:48] [480200] global_step=480200, grad_norm=5.006665229797363, loss=0.5966829657554626
I0307 21:13:18.375693 139789013600000 logging_writer.py:48] [480300] global_step=480300, grad_norm=5.271156311035156, loss=0.6477919816970825
I0307 21:13:51.997753 139789005207296 logging_writer.py:48] [480400] global_step=480400, grad_norm=4.954867362976074, loss=0.6302926540374756
I0307 21:14:25.638641 139789013600000 logging_writer.py:48] [480500] global_step=480500, grad_norm=5.098106384277344, loss=0.6563312411308289
I0307 21:14:59.262491 139789005207296 logging_writer.py:48] [480600] global_step=480600, grad_norm=4.152315616607666, loss=0.5542176961898804
I0307 21:15:32.904216 139789013600000 logging_writer.py:48] [480700] global_step=480700, grad_norm=4.573683738708496, loss=0.5641137361526489
I0307 21:15:44.125094 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:15:50.220499 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:15:58.661981 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:16:00.979421 139951089751872 submission_runner.py:411] Time since start: 167364.19s, 	Step: 480735, 	{'train/accuracy': 0.9611367583274841, 'train/loss': 0.1473008692264557, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0423572063446045, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8244268894195557, 'test/num_examples': 10000, 'score': 161732.29912495613, 'total_duration': 167364.19173169136, 'accumulated_submission_time': 161732.29912495613, 'accumulated_eval_time': 5589.2941699028015, 'accumulated_logging_time': 25.02281379699707}
I0307 21:16:01.075394 139787621099264 logging_writer.py:48] [480735] accumulated_eval_time=5589.294170, accumulated_logging_time=25.022814, accumulated_submission_time=161732.299125, global_step=480735, preemption_count=0, score=161732.299125, test/accuracy=0.630400, test/loss=1.824427, test/num_examples=10000, total_duration=167364.191732, train/accuracy=0.961137, train/loss=0.147301, validation/accuracy=0.757160, validation/loss=1.042357, validation/num_examples=50000
I0307 21:16:23.269343 139787629491968 logging_writer.py:48] [480800] global_step=480800, grad_norm=4.781407833099365, loss=0.6997988224029541
I0307 21:16:56.920183 139787621099264 logging_writer.py:48] [480900] global_step=480900, grad_norm=4.3578972816467285, loss=0.5705732703208923
I0307 21:17:30.553027 139787629491968 logging_writer.py:48] [481000] global_step=481000, grad_norm=4.573297023773193, loss=0.5961644053459167
I0307 21:18:04.148585 139787621099264 logging_writer.py:48] [481100] global_step=481100, grad_norm=4.725534439086914, loss=0.6479759216308594
I0307 21:18:37.810921 139787629491968 logging_writer.py:48] [481200] global_step=481200, grad_norm=4.8225932121276855, loss=0.6579020619392395
I0307 21:19:11.405463 139787621099264 logging_writer.py:48] [481300] global_step=481300, grad_norm=4.28577184677124, loss=0.6121315956115723
I0307 21:19:45.050196 139787629491968 logging_writer.py:48] [481400] global_step=481400, grad_norm=4.916314601898193, loss=0.678022027015686
I0307 21:20:18.664599 139787621099264 logging_writer.py:48] [481500] global_step=481500, grad_norm=4.464550495147705, loss=0.6950327157974243
I0307 21:20:52.313348 139787629491968 logging_writer.py:48] [481600] global_step=481600, grad_norm=4.933781623840332, loss=0.6403128504753113
I0307 21:21:25.925145 139787621099264 logging_writer.py:48] [481700] global_step=481700, grad_norm=4.98387336730957, loss=0.5925919413566589
I0307 21:21:59.559597 139787629491968 logging_writer.py:48] [481800] global_step=481800, grad_norm=4.3497724533081055, loss=0.564373791217804
I0307 21:22:33.165514 139787621099264 logging_writer.py:48] [481900] global_step=481900, grad_norm=4.814974784851074, loss=0.5976132750511169
I0307 21:23:06.818387 139787629491968 logging_writer.py:48] [482000] global_step=482000, grad_norm=4.619614124298096, loss=0.6573091745376587
I0307 21:23:40.407677 139787621099264 logging_writer.py:48] [482100] global_step=482100, grad_norm=4.20481014251709, loss=0.592738926410675
I0307 21:24:14.045633 139787629491968 logging_writer.py:48] [482200] global_step=482200, grad_norm=4.537265777587891, loss=0.6270934343338013
I0307 21:24:31.005472 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:24:37.062725 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:24:45.546204 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:24:47.831114 139951089751872 submission_runner.py:411] Time since start: 167891.04s, 	Step: 482252, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14672930538654327, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0419999361038208, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8233898878097534, 'test/num_examples': 10000, 'score': 162242.1644639969, 'total_duration': 167891.04339289665, 'accumulated_submission_time': 162242.1644639969, 'accumulated_eval_time': 5606.119745254517, 'accumulated_logging_time': 25.12814474105835}
I0307 21:24:47.928075 139789021992704 logging_writer.py:48] [482252] accumulated_eval_time=5606.119745, accumulated_logging_time=25.128145, accumulated_submission_time=162242.164464, global_step=482252, preemption_count=0, score=162242.164464, test/accuracy=0.632400, test/loss=1.823390, test/num_examples=10000, total_duration=167891.043393, train/accuracy=0.961356, train/loss=0.146729, validation/accuracy=0.757080, validation/loss=1.042000, validation/num_examples=50000
I0307 21:25:04.448080 139789030385408 logging_writer.py:48] [482300] global_step=482300, grad_norm=4.478602409362793, loss=0.5602957010269165
I0307 21:25:38.123752 139789021992704 logging_writer.py:48] [482400] global_step=482400, grad_norm=4.869393348693848, loss=0.6458938121795654
I0307 21:26:11.752329 139789030385408 logging_writer.py:48] [482500] global_step=482500, grad_norm=3.9303159713745117, loss=0.5497363209724426
I0307 21:26:45.369984 139789021992704 logging_writer.py:48] [482600] global_step=482600, grad_norm=4.611001014709473, loss=0.6283532977104187
I0307 21:27:18.986781 139789030385408 logging_writer.py:48] [482700] global_step=482700, grad_norm=4.1241912841796875, loss=0.565942645072937
I0307 21:27:52.637100 139789021992704 logging_writer.py:48] [482800] global_step=482800, grad_norm=4.132087707519531, loss=0.5811508893966675
I0307 21:28:26.231858 139789030385408 logging_writer.py:48] [482900] global_step=482900, grad_norm=4.526730537414551, loss=0.6435268521308899
I0307 21:29:00.057534 139789021992704 logging_writer.py:48] [483000] global_step=483000, grad_norm=4.120275020599365, loss=0.5880206823348999
I0307 21:29:33.679106 139789030385408 logging_writer.py:48] [483100] global_step=483100, grad_norm=4.227000713348389, loss=0.6092151403427124
I0307 21:30:07.329587 139789021992704 logging_writer.py:48] [483200] global_step=483200, grad_norm=4.1294026374816895, loss=0.5990983843803406
I0307 21:30:40.929169 139789030385408 logging_writer.py:48] [483300] global_step=483300, grad_norm=4.417418003082275, loss=0.5504202842712402
I0307 21:31:14.595283 139789021992704 logging_writer.py:48] [483400] global_step=483400, grad_norm=5.040191650390625, loss=0.6871094703674316
I0307 21:31:48.219265 139789030385408 logging_writer.py:48] [483500] global_step=483500, grad_norm=4.5010881423950195, loss=0.6601142883300781
I0307 21:32:21.840197 139789021992704 logging_writer.py:48] [483600] global_step=483600, grad_norm=4.095359802246094, loss=0.6084113121032715
I0307 21:32:55.454194 139789030385408 logging_writer.py:48] [483700] global_step=483700, grad_norm=4.552651405334473, loss=0.6058836579322815
I0307 21:33:18.159588 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:33:24.231445 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:33:32.842188 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:33:35.115001 139951089751872 submission_runner.py:411] Time since start: 168418.33s, 	Step: 483769, 	{'train/accuracy': 0.9601601958274841, 'train/loss': 0.1499893218278885, 'validation/accuracy': 0.7566999793052673, 'validation/loss': 1.0416303873062134, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.8239266872406006, 'test/num_examples': 10000, 'score': 162752.32794451714, 'total_duration': 168418.32730579376, 'accumulated_submission_time': 162752.32794451714, 'accumulated_eval_time': 5623.075105905533, 'accumulated_logging_time': 25.23688054084778}
I0307 21:33:35.212878 139787621099264 logging_writer.py:48] [483769] accumulated_eval_time=5623.075106, accumulated_logging_time=25.236881, accumulated_submission_time=162752.327945, global_step=483769, preemption_count=0, score=162752.327945, test/accuracy=0.632600, test/loss=1.823927, test/num_examples=10000, total_duration=168418.327306, train/accuracy=0.960160, train/loss=0.149989, validation/accuracy=0.756700, validation/loss=1.041630, validation/num_examples=50000
I0307 21:33:45.968671 139787629491968 logging_writer.py:48] [483800] global_step=483800, grad_norm=4.326820373535156, loss=0.5969467163085938
I0307 21:34:19.546280 139787621099264 logging_writer.py:48] [483900] global_step=483900, grad_norm=4.491002559661865, loss=0.5921593904495239
I0307 21:34:53.111261 139787629491968 logging_writer.py:48] [484000] global_step=484000, grad_norm=4.367812633514404, loss=0.6201509833335876
I0307 21:35:26.787653 139787621099264 logging_writer.py:48] [484100] global_step=484100, grad_norm=4.229511260986328, loss=0.6111648678779602
I0307 21:36:00.350780 139787629491968 logging_writer.py:48] [484200] global_step=484200, grad_norm=4.630763053894043, loss=0.6735246181488037
I0307 21:36:33.930630 139787621099264 logging_writer.py:48] [484300] global_step=484300, grad_norm=4.612897872924805, loss=0.675223708152771
I0307 21:37:07.490618 139787629491968 logging_writer.py:48] [484400] global_step=484400, grad_norm=4.5041913986206055, loss=0.6355891227722168
I0307 21:37:41.116697 139787621099264 logging_writer.py:48] [484500] global_step=484500, grad_norm=4.725615978240967, loss=0.6646007299423218
I0307 21:38:14.776233 139787629491968 logging_writer.py:48] [484600] global_step=484600, grad_norm=4.846676826477051, loss=0.6557149887084961
I0307 21:38:48.458520 139787621099264 logging_writer.py:48] [484700] global_step=484700, grad_norm=4.363757610321045, loss=0.6056759357452393
I0307 21:39:22.126275 139787629491968 logging_writer.py:48] [484800] global_step=484800, grad_norm=5.1347808837890625, loss=0.6719396710395813
I0307 21:39:55.768911 139787621099264 logging_writer.py:48] [484900] global_step=484900, grad_norm=4.527212619781494, loss=0.6922562122344971
I0307 21:40:29.435054 139787629491968 logging_writer.py:48] [485000] global_step=485000, grad_norm=4.547089099884033, loss=0.6317150592803955
I0307 21:41:03.089695 139787621099264 logging_writer.py:48] [485100] global_step=485100, grad_norm=4.506105899810791, loss=0.6743682622909546
I0307 21:41:36.743700 139787629491968 logging_writer.py:48] [485200] global_step=485200, grad_norm=4.306783676147461, loss=0.5663617253303528
I0307 21:42:05.156551 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:42:11.242537 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:42:19.766081 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:42:22.051937 139951089751872 submission_runner.py:411] Time since start: 168945.26s, 	Step: 485286, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.1452038437128067, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.0410614013671875, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.823925495147705, 'test/num_examples': 10000, 'score': 163262.20376372337, 'total_duration': 168945.26424503326, 'accumulated_submission_time': 163262.20376372337, 'accumulated_eval_time': 5639.970458984375, 'accumulated_logging_time': 25.34450364112854}
I0307 21:42:22.158596 139787612706560 logging_writer.py:48] [485286] accumulated_eval_time=5639.970459, accumulated_logging_time=25.344504, accumulated_submission_time=163262.203764, global_step=485286, preemption_count=0, score=163262.203764, test/accuracy=0.632800, test/loss=1.823925, test/num_examples=10000, total_duration=168945.264245, train/accuracy=0.960579, train/loss=0.145204, validation/accuracy=0.757120, validation/loss=1.041061, validation/num_examples=50000
I0307 21:42:27.196779 139787621099264 logging_writer.py:48] [485300] global_step=485300, grad_norm=4.289194107055664, loss=0.6119518280029297
I0307 21:43:00.804051 139787612706560 logging_writer.py:48] [485400] global_step=485400, grad_norm=4.152432441711426, loss=0.6275061964988708
I0307 21:43:34.462642 139787621099264 logging_writer.py:48] [485500] global_step=485500, grad_norm=4.504817962646484, loss=0.7256877422332764
I0307 21:44:08.110504 139787612706560 logging_writer.py:48] [485600] global_step=485600, grad_norm=4.6119513511657715, loss=0.6478163003921509
I0307 21:44:41.769961 139787621099264 logging_writer.py:48] [485700] global_step=485700, grad_norm=4.281400680541992, loss=0.6363082528114319
I0307 21:45:15.397500 139787612706560 logging_writer.py:48] [485800] global_step=485800, grad_norm=4.621911525726318, loss=0.5853041410446167
I0307 21:45:49.050197 139787621099264 logging_writer.py:48] [485900] global_step=485900, grad_norm=4.320075988769531, loss=0.6010491847991943
I0307 21:46:22.651848 139787612706560 logging_writer.py:48] [486000] global_step=486000, grad_norm=4.635070323944092, loss=0.5838103890419006
I0307 21:46:56.310713 139787621099264 logging_writer.py:48] [486100] global_step=486100, grad_norm=4.590819835662842, loss=0.7032657265663147
I0307 21:47:29.963903 139787612706560 logging_writer.py:48] [486200] global_step=486200, grad_norm=4.335888862609863, loss=0.6060481071472168
I0307 21:48:03.559921 139787621099264 logging_writer.py:48] [486300] global_step=486300, grad_norm=4.525886058807373, loss=0.6927057504653931
I0307 21:48:37.170435 139787612706560 logging_writer.py:48] [486400] global_step=486400, grad_norm=4.298847198486328, loss=0.6080792546272278
I0307 21:49:10.822999 139787621099264 logging_writer.py:48] [486500] global_step=486500, grad_norm=4.137526988983154, loss=0.5430184006690979
I0307 21:49:44.439213 139787612706560 logging_writer.py:48] [486600] global_step=486600, grad_norm=4.397028923034668, loss=0.6129530668258667
I0307 21:50:18.114126 139787621099264 logging_writer.py:48] [486700] global_step=486700, grad_norm=4.6363091468811035, loss=0.6632381677627563
I0307 21:50:51.745454 139787612706560 logging_writer.py:48] [486800] global_step=486800, grad_norm=4.462771892547607, loss=0.6335934400558472
I0307 21:50:52.231230 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:50:58.350875 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:51:06.908074 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:51:09.207477 139951089751872 submission_runner.py:411] Time since start: 169472.42s, 	Step: 486803, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.14673925936222076, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0423557758331299, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8246389627456665, 'test/num_examples': 10000, 'score': 163772.209897995, 'total_duration': 169472.41901779175, 'accumulated_submission_time': 163772.209897995, 'accumulated_eval_time': 5656.945882558823, 'accumulated_logging_time': 25.461092948913574}
I0307 21:51:09.355393 139787612706560 logging_writer.py:48] [486803] accumulated_eval_time=5656.945883, accumulated_logging_time=25.461093, accumulated_submission_time=163772.209898, global_step=486803, preemption_count=0, score=163772.209898, test/accuracy=0.630500, test/loss=1.824639, test/num_examples=10000, total_duration=169472.419018, train/accuracy=0.960878, train/loss=0.146739, validation/accuracy=0.757020, validation/loss=1.042356, validation/num_examples=50000
I0307 21:51:42.308779 139787621099264 logging_writer.py:48] [486900] global_step=486900, grad_norm=4.529418468475342, loss=0.5740764141082764
I0307 21:52:15.967737 139787612706560 logging_writer.py:48] [487000] global_step=487000, grad_norm=4.076714515686035, loss=0.5979098081588745
I0307 21:52:49.610441 139787621099264 logging_writer.py:48] [487100] global_step=487100, grad_norm=4.647384166717529, loss=0.6027053594589233
I0307 21:53:23.305888 139787612706560 logging_writer.py:48] [487200] global_step=487200, grad_norm=4.349058151245117, loss=0.601555347442627
I0307 21:53:56.944404 139787621099264 logging_writer.py:48] [487300] global_step=487300, grad_norm=4.750272750854492, loss=0.6510810256004333
I0307 21:54:30.566028 139787612706560 logging_writer.py:48] [487400] global_step=487400, grad_norm=4.55266809463501, loss=0.6424098014831543
I0307 21:55:04.213947 139787621099264 logging_writer.py:48] [487500] global_step=487500, grad_norm=4.165812969207764, loss=0.6049246191978455
I0307 21:55:37.901097 139787612706560 logging_writer.py:48] [487600] global_step=487600, grad_norm=4.7715959548950195, loss=0.6538771390914917
I0307 21:56:11.555746 139787621099264 logging_writer.py:48] [487700] global_step=487700, grad_norm=4.897862434387207, loss=0.6243271231651306
I0307 21:56:45.242285 139787612706560 logging_writer.py:48] [487800] global_step=487800, grad_norm=4.4895548820495605, loss=0.5591641068458557
I0307 21:57:18.897802 139787621099264 logging_writer.py:48] [487900] global_step=487900, grad_norm=4.538410663604736, loss=0.6602361798286438
I0307 21:57:52.581637 139787612706560 logging_writer.py:48] [488000] global_step=488000, grad_norm=4.483964443206787, loss=0.6760032176971436
I0307 21:58:26.187910 139787621099264 logging_writer.py:48] [488100] global_step=488100, grad_norm=4.6430511474609375, loss=0.6790211796760559
I0307 21:58:59.831941 139787612706560 logging_writer.py:48] [488200] global_step=488200, grad_norm=4.814777374267578, loss=0.6258471608161926
I0307 21:59:33.572818 139787621099264 logging_writer.py:48] [488300] global_step=488300, grad_norm=4.301487445831299, loss=0.5851407647132874
I0307 21:59:39.445212 139951089751872 spec.py:321] Evaluating on the training split.
I0307 21:59:45.566103 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 21:59:54.189704 139951089751872 spec.py:349] Evaluating on the test split.
I0307 21:59:56.485956 139951089751872 submission_runner.py:411] Time since start: 169999.70s, 	Step: 488319, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14708483219146729, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0419479608535767, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.822999119758606, 'test/num_examples': 10000, 'score': 164282.22924995422, 'total_duration': 169999.69826436043, 'accumulated_submission_time': 164282.22924995422, 'accumulated_eval_time': 5673.986569881439, 'accumulated_logging_time': 25.623329162597656}
I0307 21:59:56.583340 139787629491968 logging_writer.py:48] [488319] accumulated_eval_time=5673.986570, accumulated_logging_time=25.623329, accumulated_submission_time=164282.229250, global_step=488319, preemption_count=0, score=164282.229250, test/accuracy=0.631300, test/loss=1.822999, test/num_examples=10000, total_duration=169999.698264, train/accuracy=0.960061, train/loss=0.147085, validation/accuracy=0.757080, validation/loss=1.041948, validation/num_examples=50000
I0307 22:00:24.112171 139788996814592 logging_writer.py:48] [488400] global_step=488400, grad_norm=4.394307613372803, loss=0.5660415887832642
I0307 22:00:57.663442 139787629491968 logging_writer.py:48] [488500] global_step=488500, grad_norm=4.527512550354004, loss=0.6408857703208923
I0307 22:01:31.229749 139788996814592 logging_writer.py:48] [488600] global_step=488600, grad_norm=4.384739875793457, loss=0.6488714218139648
I0307 22:02:04.823689 139787629491968 logging_writer.py:48] [488700] global_step=488700, grad_norm=4.488208293914795, loss=0.6970992088317871
I0307 22:02:38.478184 139788996814592 logging_writer.py:48] [488800] global_step=488800, grad_norm=4.733209133148193, loss=0.681334912776947
I0307 22:03:12.104937 139787629491968 logging_writer.py:48] [488900] global_step=488900, grad_norm=4.532814025878906, loss=0.6093347072601318
I0307 22:03:45.747521 139788996814592 logging_writer.py:48] [489000] global_step=489000, grad_norm=4.173535346984863, loss=0.5089529752731323
I0307 22:04:19.368145 139787629491968 logging_writer.py:48] [489100] global_step=489100, grad_norm=4.979720115661621, loss=0.6719573736190796
I0307 22:04:53.006036 139788996814592 logging_writer.py:48] [489200] global_step=489200, grad_norm=4.3886613845825195, loss=0.6097965836524963
I0307 22:05:26.615946 139787629491968 logging_writer.py:48] [489300] global_step=489300, grad_norm=4.480949401855469, loss=0.5631510615348816
I0307 22:06:00.258876 139788996814592 logging_writer.py:48] [489400] global_step=489400, grad_norm=4.386412620544434, loss=0.6554000377655029
I0307 22:06:33.829552 139787629491968 logging_writer.py:48] [489500] global_step=489500, grad_norm=4.681280612945557, loss=0.711620032787323
I0307 22:07:07.483609 139788996814592 logging_writer.py:48] [489600] global_step=489600, grad_norm=4.622025012969971, loss=0.6723301410675049
I0307 22:07:41.098978 139787629491968 logging_writer.py:48] [489700] global_step=489700, grad_norm=4.516901016235352, loss=0.6132460832595825
I0307 22:08:14.742004 139788996814592 logging_writer.py:48] [489800] global_step=489800, grad_norm=4.5614824295043945, loss=0.6320194005966187
I0307 22:08:26.646894 139951089751872 spec.py:321] Evaluating on the training split.
I0307 22:08:32.671079 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 22:08:41.325965 139951089751872 spec.py:349] Evaluating on the test split.
I0307 22:08:43.652980 139951089751872 submission_runner.py:411] Time since start: 170526.87s, 	Step: 489837, 	{'train/accuracy': 0.958984375, 'train/loss': 0.14876966178417206, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.041583776473999, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8239089250564575, 'test/num_examples': 10000, 'score': 164792.2272515297, 'total_duration': 170526.86529254913, 'accumulated_submission_time': 164792.2272515297, 'accumulated_eval_time': 5690.992618322372, 'accumulated_logging_time': 25.730217695236206}
I0307 22:08:43.751201 139789013600000 logging_writer.py:48] [489837] accumulated_eval_time=5690.992618, accumulated_logging_time=25.730218, accumulated_submission_time=164792.227252, global_step=489837, preemption_count=0, score=164792.227252, test/accuracy=0.631300, test/loss=1.823909, test/num_examples=10000, total_duration=170526.865293, train/accuracy=0.958984, train/loss=0.148770, validation/accuracy=0.757280, validation/loss=1.041584, validation/num_examples=50000
I0307 22:09:05.324567 139789021992704 logging_writer.py:48] [489900] global_step=489900, grad_norm=4.858510971069336, loss=0.5897245407104492
I0307 22:09:38.931982 139789013600000 logging_writer.py:48] [490000] global_step=490000, grad_norm=4.404253005981445, loss=0.5893315672874451
I0307 22:10:12.573046 139789021992704 logging_writer.py:48] [490100] global_step=490100, grad_norm=4.852883815765381, loss=0.6532023549079895
I0307 22:10:46.221394 139789013600000 logging_writer.py:48] [490200] global_step=490200, grad_norm=4.157839298248291, loss=0.5658183097839355
I0307 22:11:19.850844 139789021992704 logging_writer.py:48] [490300] global_step=490300, grad_norm=4.456425666809082, loss=0.5955782532691956
I0307 22:11:53.537085 139789013600000 logging_writer.py:48] [490400] global_step=490400, grad_norm=4.365780353546143, loss=0.6006616353988647
I0307 22:12:27.214148 139789021992704 logging_writer.py:48] [490500] global_step=490500, grad_norm=4.709532260894775, loss=0.5882437825202942
I0307 22:13:00.830978 139789013600000 logging_writer.py:48] [490600] global_step=490600, grad_norm=4.648046016693115, loss=0.648191511631012
I0307 22:13:34.483663 139789021992704 logging_writer.py:48] [490700] global_step=490700, grad_norm=4.5446882247924805, loss=0.6649115085601807
I0307 22:14:08.129387 139789013600000 logging_writer.py:48] [490800] global_step=490800, grad_norm=4.283512592315674, loss=0.5814752578735352
I0307 22:14:41.781285 139789021992704 logging_writer.py:48] [490900] global_step=490900, grad_norm=3.8327858448028564, loss=0.515106201171875
I0307 22:15:15.421094 139789013600000 logging_writer.py:48] [491000] global_step=491000, grad_norm=4.902099609375, loss=0.6112831234931946
I0307 22:15:49.063785 139789021992704 logging_writer.py:48] [491100] global_step=491100, grad_norm=4.219184398651123, loss=0.5819799900054932
I0307 22:16:22.700960 139789013600000 logging_writer.py:48] [491200] global_step=491200, grad_norm=4.255230903625488, loss=0.6247949600219727
I0307 22:16:56.352941 139789021992704 logging_writer.py:48] [491300] global_step=491300, grad_norm=4.533297061920166, loss=0.620252251625061
I0307 22:17:13.663071 139951089751872 spec.py:321] Evaluating on the training split.
I0307 22:17:19.760128 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 22:17:28.290018 139951089751872 spec.py:349] Evaluating on the test split.
I0307 22:17:30.585499 139951089751872 submission_runner.py:411] Time since start: 171053.80s, 	Step: 491353, 	{'train/accuracy': 0.9603196382522583, 'train/loss': 0.14484788477420807, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.0405925512313843, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.822745442390442, 'test/num_examples': 10000, 'score': 165302.07374358177, 'total_duration': 171053.79780197144, 'accumulated_submission_time': 165302.07374358177, 'accumulated_eval_time': 5707.914986610413, 'accumulated_logging_time': 25.838003635406494}
I0307 22:17:30.680105 139788996814592 logging_writer.py:48] [491353] accumulated_eval_time=5707.914987, accumulated_logging_time=25.838004, accumulated_submission_time=165302.073744, global_step=491353, preemption_count=0, score=165302.073744, test/accuracy=0.631600, test/loss=1.822745, test/num_examples=10000, total_duration=171053.797802, train/accuracy=0.960320, train/loss=0.144848, validation/accuracy=0.757220, validation/loss=1.040593, validation/num_examples=50000
I0307 22:17:46.819132 139789005207296 logging_writer.py:48] [491400] global_step=491400, grad_norm=4.30580472946167, loss=0.6046258211135864
I0307 22:18:20.537492 139788996814592 logging_writer.py:48] [491500] global_step=491500, grad_norm=4.612462043762207, loss=0.6051251888275146
I0307 22:18:54.203707 139789005207296 logging_writer.py:48] [491600] global_step=491600, grad_norm=4.5354695320129395, loss=0.6125637888908386
I0307 22:19:27.877730 139788996814592 logging_writer.py:48] [491700] global_step=491700, grad_norm=5.264784336090088, loss=0.7499817609786987
I0307 22:20:01.493567 139789005207296 logging_writer.py:48] [491800] global_step=491800, grad_norm=4.806642055511475, loss=0.6177288293838501
I0307 22:20:35.160708 139788996814592 logging_writer.py:48] [491900] global_step=491900, grad_norm=4.527490139007568, loss=0.6102553606033325
I0307 22:21:08.786226 139789005207296 logging_writer.py:48] [492000] global_step=492000, grad_norm=4.82157564163208, loss=0.6276847124099731
I0307 22:21:42.458273 139788996814592 logging_writer.py:48] [492100] global_step=492100, grad_norm=4.631843090057373, loss=0.6151999235153198
I0307 22:22:16.078797 139789005207296 logging_writer.py:48] [492200] global_step=492200, grad_norm=4.433691501617432, loss=0.6023110151290894
I0307 22:22:49.716945 139788996814592 logging_writer.py:48] [492300] global_step=492300, grad_norm=4.915638446807861, loss=0.6275696754455566
I0307 22:23:23.320507 139789005207296 logging_writer.py:48] [492400] global_step=492400, grad_norm=4.227674961090088, loss=0.5124366283416748
I0307 22:23:57.001670 139788996814592 logging_writer.py:48] [492500] global_step=492500, grad_norm=4.334959506988525, loss=0.6279941201210022
I0307 22:24:30.637263 139789005207296 logging_writer.py:48] [492600] global_step=492600, grad_norm=4.094223976135254, loss=0.5583443641662598
I0307 22:25:04.261919 139788996814592 logging_writer.py:48] [492700] global_step=492700, grad_norm=4.455994606018066, loss=0.6058422327041626
I0307 22:25:37.943190 139789005207296 logging_writer.py:48] [492800] global_step=492800, grad_norm=4.83761739730835, loss=0.6619199514389038
I0307 22:26:00.675175 139951089751872 spec.py:321] Evaluating on the training split.
I0307 22:26:06.771491 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 22:26:15.322949 139951089751872 spec.py:349] Evaluating on the test split.
I0307 22:26:17.575178 139951089751872 submission_runner.py:411] Time since start: 171580.79s, 	Step: 492869, 	{'train/accuracy': 0.9604192972183228, 'train/loss': 0.1478891372680664, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0413697957992554, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8230407238006592, 'test/num_examples': 10000, 'score': 165812.0014333725, 'total_duration': 171580.78747987747, 'accumulated_submission_time': 165812.0014333725, 'accumulated_eval_time': 5724.814933538437, 'accumulated_logging_time': 25.942726135253906}
I0307 22:26:17.679030 139788996814592 logging_writer.py:48] [492869] accumulated_eval_time=5724.814934, accumulated_logging_time=25.942726, accumulated_submission_time=165812.001433, global_step=492869, preemption_count=0, score=165812.001433, test/accuracy=0.632000, test/loss=1.823041, test/num_examples=10000, total_duration=171580.787480, train/accuracy=0.960419, train/loss=0.147889, validation/accuracy=0.757180, validation/loss=1.041370, validation/num_examples=50000
I0307 22:26:28.427521 139789005207296 logging_writer.py:48] [492900] global_step=492900, grad_norm=4.431334018707275, loss=0.6258569955825806
I0307 22:27:02.045762 139788996814592 logging_writer.py:48] [493000] global_step=493000, grad_norm=4.769280910491943, loss=0.5728004574775696
I0307 22:27:35.616399 139789005207296 logging_writer.py:48] [493100] global_step=493100, grad_norm=4.030379295349121, loss=0.623335599899292
I0307 22:28:09.174189 139788996814592 logging_writer.py:48] [493200] global_step=493200, grad_norm=4.510000228881836, loss=0.5987247228622437
I0307 22:28:42.757622 139789005207296 logging_writer.py:48] [493300] global_step=493300, grad_norm=4.069622993469238, loss=0.5790268182754517
I0307 22:29:16.407998 139788996814592 logging_writer.py:48] [493400] global_step=493400, grad_norm=4.943199157714844, loss=0.612533450126648
I0307 22:29:50.062126 139789005207296 logging_writer.py:48] [493500] global_step=493500, grad_norm=4.745415210723877, loss=0.6427975296974182
I0307 22:30:23.728609 139788996814592 logging_writer.py:48] [493600] global_step=493600, grad_norm=4.7641921043396, loss=0.6787486672401428
I0307 22:30:57.321164 139789005207296 logging_writer.py:48] [493700] global_step=493700, grad_norm=4.304006099700928, loss=0.6371229887008667
I0307 22:31:30.912029 139788996814592 logging_writer.py:48] [493800] global_step=493800, grad_norm=4.567846775054932, loss=0.6372507810592651
I0307 22:32:04.566303 139789005207296 logging_writer.py:48] [493900] global_step=493900, grad_norm=4.384952545166016, loss=0.5517924427986145
I0307 22:32:38.213054 139788996814592 logging_writer.py:48] [494000] global_step=494000, grad_norm=4.316931247711182, loss=0.6052696704864502
I0307 22:33:11.814654 139789005207296 logging_writer.py:48] [494100] global_step=494100, grad_norm=4.331296920776367, loss=0.5719707012176514
I0307 22:33:45.491271 139788996814592 logging_writer.py:48] [494200] global_step=494200, grad_norm=5.0675435066223145, loss=0.6423183083534241
I0307 22:34:19.109753 139789005207296 logging_writer.py:48] [494300] global_step=494300, grad_norm=4.679600715637207, loss=0.6317117810249329
I0307 22:34:47.857456 139951089751872 spec.py:321] Evaluating on the training split.
I0307 22:34:54.199646 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 22:35:02.494620 139951089751872 spec.py:349] Evaluating on the test split.
I0307 22:35:04.772478 139951089751872 submission_runner.py:411] Time since start: 172107.98s, 	Step: 494387, 	{'train/accuracy': 0.9615353941917419, 'train/loss': 0.14523433148860931, 'validation/accuracy': 0.7576199769973755, 'validation/loss': 1.041755199432373, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8221620321273804, 'test/num_examples': 10000, 'score': 166322.11298918724, 'total_duration': 172107.98474049568, 'accumulated_submission_time': 166322.11298918724, 'accumulated_eval_time': 5741.729859828949, 'accumulated_logging_time': 26.05689764022827}
I0307 22:35:04.873774 139789021992704 logging_writer.py:48] [494387] accumulated_eval_time=5741.729860, accumulated_logging_time=26.056898, accumulated_submission_time=166322.112989, global_step=494387, preemption_count=0, score=166322.112989, test/accuracy=0.631200, test/loss=1.822162, test/num_examples=10000, total_duration=172107.984740, train/accuracy=0.961535, train/loss=0.145234, validation/accuracy=0.757620, validation/loss=1.041755, validation/num_examples=50000
I0307 22:35:09.577431 139789030385408 logging_writer.py:48] [494400] global_step=494400, grad_norm=4.4720563888549805, loss=0.6307039856910706
I0307 22:35:43.183132 139789021992704 logging_writer.py:48] [494500] global_step=494500, grad_norm=5.136017799377441, loss=0.6082600951194763
I0307 22:36:16.937171 139789030385408 logging_writer.py:48] [494600] global_step=494600, grad_norm=4.7847700119018555, loss=0.541352391242981
I0307 22:36:50.509567 139789021992704 logging_writer.py:48] [494700] global_step=494700, grad_norm=4.591573238372803, loss=0.5696109533309937
I0307 22:37:24.104237 139789030385408 logging_writer.py:48] [494800] global_step=494800, grad_norm=4.752784729003906, loss=0.6394118666648865
I0307 22:37:57.707229 139789021992704 logging_writer.py:48] [494900] global_step=494900, grad_norm=3.9777326583862305, loss=0.5582844614982605
I0307 22:38:31.377757 139789030385408 logging_writer.py:48] [495000] global_step=495000, grad_norm=4.785380840301514, loss=0.5895467400550842
I0307 22:39:05.012172 139789021992704 logging_writer.py:48] [495100] global_step=495100, grad_norm=4.809217929840088, loss=0.689900815486908
I0307 22:39:38.666720 139789030385408 logging_writer.py:48] [495200] global_step=495200, grad_norm=4.5494256019592285, loss=0.5967050194740295
I0307 22:40:12.320008 139789021992704 logging_writer.py:48] [495300] global_step=495300, grad_norm=4.148958206176758, loss=0.5629758834838867
I0307 22:40:45.963346 139789030385408 logging_writer.py:48] [495400] global_step=495400, grad_norm=4.685944557189941, loss=0.6185317039489746
I0307 22:41:19.588299 139789021992704 logging_writer.py:48] [495500] global_step=495500, grad_norm=4.749865531921387, loss=0.6070460081100464
I0307 22:41:53.210101 139789030385408 logging_writer.py:48] [495600] global_step=495600, grad_norm=4.5862531661987305, loss=0.6341134309768677
I0307 22:42:26.906811 139789021992704 logging_writer.py:48] [495700] global_step=495700, grad_norm=4.290696144104004, loss=0.6149479150772095
I0307 22:43:00.483717 139789030385408 logging_writer.py:48] [495800] global_step=495800, grad_norm=4.691957950592041, loss=0.6861392259597778
I0307 22:43:34.037284 139789021992704 logging_writer.py:48] [495900] global_step=495900, grad_norm=4.611957550048828, loss=0.6134005188941956
I0307 22:43:34.857084 139951089751872 spec.py:321] Evaluating on the training split.
I0307 22:43:40.887772 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 22:43:49.413133 139951089751872 spec.py:349] Evaluating on the test split.
I0307 22:43:51.653992 139951089751872 submission_runner.py:411] Time since start: 172634.87s, 	Step: 495904, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.14818249642848969, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0412178039550781, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8224083185195923, 'test/num_examples': 10000, 'score': 166832.02917313576, 'total_duration': 172634.86578130722, 'accumulated_submission_time': 166832.02917313576, 'accumulated_eval_time': 5758.526192903519, 'accumulated_logging_time': 26.16771912574768}
I0307 22:43:51.751872 139787621099264 logging_writer.py:48] [495904] accumulated_eval_time=5758.526193, accumulated_logging_time=26.167719, accumulated_submission_time=166832.029173, global_step=495904, preemption_count=0, score=166832.029173, test/accuracy=0.630800, test/loss=1.822408, test/num_examples=10000, total_duration=172634.865781, train/accuracy=0.959841, train/loss=0.148182, validation/accuracy=0.757020, validation/loss=1.041218, validation/num_examples=50000
I0307 22:44:24.370794 139787629491968 logging_writer.py:48] [496000] global_step=496000, grad_norm=4.806899547576904, loss=0.6148619651794434
I0307 22:44:57.986980 139787621099264 logging_writer.py:48] [496100] global_step=496100, grad_norm=5.001335620880127, loss=0.707298994064331
I0307 22:45:31.561829 139787629491968 logging_writer.py:48] [496200] global_step=496200, grad_norm=4.586008548736572, loss=0.6260071396827698
I0307 22:46:05.137337 139787621099264 logging_writer.py:48] [496300] global_step=496300, grad_norm=4.874530792236328, loss=0.6450001001358032
I0307 22:46:38.725543 139787629491968 logging_writer.py:48] [496400] global_step=496400, grad_norm=4.840782165527344, loss=0.6665878295898438
I0307 22:47:12.374865 139787621099264 logging_writer.py:48] [496500] global_step=496500, grad_norm=4.622998237609863, loss=0.6636531949043274
I0307 22:47:45.979124 139787629491968 logging_writer.py:48] [496600] global_step=496600, grad_norm=4.517448902130127, loss=0.635674774646759
I0307 22:48:19.634689 139787621099264 logging_writer.py:48] [496700] global_step=496700, grad_norm=4.783383369445801, loss=0.6413030028343201
I0307 22:48:53.285496 139787629491968 logging_writer.py:48] [496800] global_step=496800, grad_norm=4.443490028381348, loss=0.6484290361404419
I0307 22:49:26.897417 139787621099264 logging_writer.py:48] [496900] global_step=496900, grad_norm=4.361957550048828, loss=0.6376607418060303
I0307 22:50:00.494730 139787629491968 logging_writer.py:48] [497000] global_step=497000, grad_norm=4.86551570892334, loss=0.7102926969528198
I0307 22:50:34.115611 139787621099264 logging_writer.py:48] [497100] global_step=497100, grad_norm=4.961803913116455, loss=0.646086573600769
I0307 22:51:07.717685 139787629491968 logging_writer.py:48] [497200] global_step=497200, grad_norm=4.878756046295166, loss=0.6364380121231079
I0307 22:51:41.333935 139787621099264 logging_writer.py:48] [497300] global_step=497300, grad_norm=4.550420761108398, loss=0.6044899225234985
I0307 22:52:14.901648 139787629491968 logging_writer.py:48] [497400] global_step=497400, grad_norm=4.335893630981445, loss=0.6134746670722961
I0307 22:52:21.760965 139951089751872 spec.py:321] Evaluating on the training split.
I0307 22:52:27.794281 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 22:52:36.300117 139951089751872 spec.py:349] Evaluating on the test split.
I0307 22:52:38.517069 139951089751872 submission_runner.py:411] Time since start: 173161.73s, 	Step: 497422, 	{'train/accuracy': 0.9609375, 'train/loss': 0.14455363154411316, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 1.041479468345642, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.8234856128692627, 'test/num_examples': 10000, 'score': 167341.9730234146, 'total_duration': 173161.72937083244, 'accumulated_submission_time': 167341.9730234146, 'accumulated_eval_time': 5775.282235145569, 'accumulated_logging_time': 26.27513027191162}
I0307 22:52:38.619653 139789021992704 logging_writer.py:48] [497422] accumulated_eval_time=5775.282235, accumulated_logging_time=26.275130, accumulated_submission_time=167341.973023, global_step=497422, preemption_count=0, score=167341.973023, test/accuracy=0.631600, test/loss=1.823486, test/num_examples=10000, total_duration=173161.729371, train/accuracy=0.960938, train/loss=0.144554, validation/accuracy=0.757400, validation/loss=1.041479, validation/num_examples=50000
I0307 22:53:05.132322 139789030385408 logging_writer.py:48] [497500] global_step=497500, grad_norm=4.591017246246338, loss=0.633554995059967
I0307 22:53:38.700011 139789021992704 logging_writer.py:48] [497600] global_step=497600, grad_norm=5.198859214782715, loss=0.6727281212806702
I0307 22:54:12.272364 139789030385408 logging_writer.py:48] [497700] global_step=497700, grad_norm=5.294628143310547, loss=0.6489874720573425
I0307 22:54:45.920178 139789021992704 logging_writer.py:48] [497800] global_step=497800, grad_norm=4.363924026489258, loss=0.6331999897956848
I0307 22:55:19.509188 139789030385408 logging_writer.py:48] [497900] global_step=497900, grad_norm=4.522654056549072, loss=0.5849036574363708
I0307 22:55:53.161342 139789021992704 logging_writer.py:48] [498000] global_step=498000, grad_norm=4.080453872680664, loss=0.5759214758872986
I0307 22:56:26.788998 139789030385408 logging_writer.py:48] [498100] global_step=498100, grad_norm=4.782681465148926, loss=0.5865068435668945
I0307 22:57:00.430749 139789021992704 logging_writer.py:48] [498200] global_step=498200, grad_norm=4.071689128875732, loss=0.6458291411399841
I0307 22:57:34.082579 139789030385408 logging_writer.py:48] [498300] global_step=498300, grad_norm=4.4138264656066895, loss=0.5850308537483215
I0307 22:58:07.686638 139789021992704 logging_writer.py:48] [498400] global_step=498400, grad_norm=5.039870262145996, loss=0.7142418026924133
I0307 22:58:41.344457 139789030385408 logging_writer.py:48] [498500] global_step=498500, grad_norm=4.4132399559021, loss=0.6214659810066223
I0307 22:59:14.962235 139789021992704 logging_writer.py:48] [498600] global_step=498600, grad_norm=4.440029144287109, loss=0.6454730033874512
I0307 22:59:48.623760 139789030385408 logging_writer.py:48] [498700] global_step=498700, grad_norm=4.250566482543945, loss=0.5237534046173096
I0307 23:00:22.248176 139789021992704 logging_writer.py:48] [498800] global_step=498800, grad_norm=4.760908603668213, loss=0.6200069189071655
I0307 23:00:55.908213 139789030385408 logging_writer.py:48] [498900] global_step=498900, grad_norm=4.7252912521362305, loss=0.6049160957336426
I0307 23:01:08.824735 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:01:14.818344 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:01:23.353515 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:01:25.629762 139951089751872 submission_runner.py:411] Time since start: 173688.84s, 	Step: 498940, 	{'train/accuracy': 0.9624322056770325, 'train/loss': 0.1402934044599533, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0417726039886475, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.822174072265625, 'test/num_examples': 10000, 'score': 167852.10940527916, 'total_duration': 173688.8420562744, 'accumulated_submission_time': 167852.10940527916, 'accumulated_eval_time': 5792.087192296982, 'accumulated_logging_time': 26.389538049697876}
I0307 23:01:25.725656 139788996814592 logging_writer.py:48] [498940] accumulated_eval_time=5792.087192, accumulated_logging_time=26.389538, accumulated_submission_time=167852.109405, global_step=498940, preemption_count=0, score=167852.109405, test/accuracy=0.631900, test/loss=1.822174, test/num_examples=10000, total_duration=173688.842056, train/accuracy=0.962432, train/loss=0.140293, validation/accuracy=0.757280, validation/loss=1.041773, validation/num_examples=50000
I0307 23:01:46.203992 139789005207296 logging_writer.py:48] [499000] global_step=499000, grad_norm=4.196178913116455, loss=0.6120734214782715
I0307 23:02:19.772030 139788996814592 logging_writer.py:48] [499100] global_step=499100, grad_norm=4.553102493286133, loss=0.706074595451355
I0307 23:02:53.339545 139789005207296 logging_writer.py:48] [499200] global_step=499200, grad_norm=4.702971935272217, loss=0.6485947966575623
I0307 23:03:26.935325 139788996814592 logging_writer.py:48] [499300] global_step=499300, grad_norm=4.426472187042236, loss=0.6595617532730103
I0307 23:04:00.591010 139789005207296 logging_writer.py:48] [499400] global_step=499400, grad_norm=4.453804016113281, loss=0.6381773352622986
I0307 23:04:34.229404 139788996814592 logging_writer.py:48] [499500] global_step=499500, grad_norm=4.644367218017578, loss=0.6006507873535156
I0307 23:05:07.884551 139789005207296 logging_writer.py:48] [499600] global_step=499600, grad_norm=4.36939811706543, loss=0.608806848526001
I0307 23:05:41.514976 139788996814592 logging_writer.py:48] [499700] global_step=499700, grad_norm=4.572290897369385, loss=0.6306211948394775
I0307 23:06:15.150686 139789005207296 logging_writer.py:48] [499800] global_step=499800, grad_norm=4.381994247436523, loss=0.6412814259529114
I0307 23:06:48.896466 139788996814592 logging_writer.py:48] [499900] global_step=499900, grad_norm=4.252685546875, loss=0.6594089269638062
I0307 23:07:22.467264 139789005207296 logging_writer.py:48] [500000] global_step=500000, grad_norm=4.9884490966796875, loss=0.6520102620124817
I0307 23:07:56.045102 139788996814592 logging_writer.py:48] [500100] global_step=500100, grad_norm=3.943852663040161, loss=0.6225822567939758
I0307 23:08:29.690444 139789005207296 logging_writer.py:48] [500200] global_step=500200, grad_norm=4.287011623382568, loss=0.6153595447540283
I0307 23:09:03.344913 139788996814592 logging_writer.py:48] [500300] global_step=500300, grad_norm=4.1252264976501465, loss=0.6290223002433777
I0307 23:09:37.031773 139789005207296 logging_writer.py:48] [500400] global_step=500400, grad_norm=4.937342166900635, loss=0.6299859881401062
I0307 23:09:55.690707 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:10:01.752821 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:10:10.223964 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:10:12.505151 139951089751872 submission_runner.py:411] Time since start: 174215.72s, 	Step: 500457, 	{'train/accuracy': 0.9625119566917419, 'train/loss': 0.14442172646522522, 'validation/accuracy': 0.7576000094413757, 'validation/loss': 1.041919469833374, 'validation/num_examples': 50000, 'test/accuracy': 0.6327000260353088, 'test/loss': 1.8239601850509644, 'test/num_examples': 10000, 'score': 168362.00852704048, 'total_duration': 174215.71746110916, 'accumulated_submission_time': 168362.00852704048, 'accumulated_eval_time': 5808.90158700943, 'accumulated_logging_time': 26.495758056640625}
I0307 23:10:12.605132 139788996814592 logging_writer.py:48] [500457] accumulated_eval_time=5808.901587, accumulated_logging_time=26.495758, accumulated_submission_time=168362.008527, global_step=500457, preemption_count=0, score=168362.008527, test/accuracy=0.632700, test/loss=1.823960, test/num_examples=10000, total_duration=174215.717461, train/accuracy=0.962512, train/loss=0.144422, validation/accuracy=0.757600, validation/loss=1.041919, validation/num_examples=50000
I0307 23:10:27.393038 139789005207296 logging_writer.py:48] [500500] global_step=500500, grad_norm=4.64599084854126, loss=0.5998092293739319
I0307 23:11:00.985312 139788996814592 logging_writer.py:48] [500600] global_step=500600, grad_norm=4.031717300415039, loss=0.5650831460952759
I0307 23:11:34.559267 139789005207296 logging_writer.py:48] [500700] global_step=500700, grad_norm=4.475374698638916, loss=0.6396721005439758
I0307 23:12:08.127460 139788996814592 logging_writer.py:48] [500800] global_step=500800, grad_norm=4.370831489562988, loss=0.5796210765838623
I0307 23:12:41.761415 139789005207296 logging_writer.py:48] [500900] global_step=500900, grad_norm=4.604539394378662, loss=0.6404129862785339
I0307 23:13:15.440043 139788996814592 logging_writer.py:48] [501000] global_step=501000, grad_norm=4.528597831726074, loss=0.6292992830276489
I0307 23:13:49.055253 139789005207296 logging_writer.py:48] [501100] global_step=501100, grad_norm=5.065486907958984, loss=0.6337637901306152
I0307 23:14:22.710716 139788996814592 logging_writer.py:48] [501200] global_step=501200, grad_norm=4.294887065887451, loss=0.6647494435310364
I0307 23:14:56.373129 139789005207296 logging_writer.py:48] [501300] global_step=501300, grad_norm=4.49287223815918, loss=0.5823384523391724
I0307 23:15:30.042503 139788996814592 logging_writer.py:48] [501400] global_step=501400, grad_norm=4.425632476806641, loss=0.5668983459472656
I0307 23:16:03.687024 139789005207296 logging_writer.py:48] [501500] global_step=501500, grad_norm=4.863995552062988, loss=0.6474956274032593
I0307 23:16:37.345206 139788996814592 logging_writer.py:48] [501600] global_step=501600, grad_norm=4.689638137817383, loss=0.7096892595291138
I0307 23:17:11.008057 139789005207296 logging_writer.py:48] [501700] global_step=501700, grad_norm=4.626292705535889, loss=0.7042698860168457
I0307 23:17:44.677119 139788996814592 logging_writer.py:48] [501800] global_step=501800, grad_norm=4.59188985824585, loss=0.6822651028633118
I0307 23:18:18.324676 139789005207296 logging_writer.py:48] [501900] global_step=501900, grad_norm=4.476522922515869, loss=0.6402633190155029
I0307 23:18:42.646017 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:18:49.393524 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:18:57.918679 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:19:00.190131 139951089751872 submission_runner.py:411] Time since start: 174743.40s, 	Step: 501974, 	{'train/accuracy': 0.9595623016357422, 'train/loss': 0.1485275775194168, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0413075685501099, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8220081329345703, 'test/num_examples': 10000, 'score': 168871.98159265518, 'total_duration': 174743.40242886543, 'accumulated_submission_time': 168871.98159265518, 'accumulated_eval_time': 5826.445637702942, 'accumulated_logging_time': 26.60779643058777}
I0307 23:19:00.290719 139787621099264 logging_writer.py:48] [501974] accumulated_eval_time=5826.445638, accumulated_logging_time=26.607796, accumulated_submission_time=168871.981593, global_step=501974, preemption_count=0, score=168871.981593, test/accuracy=0.630700, test/loss=1.822008, test/num_examples=10000, total_duration=174743.402429, train/accuracy=0.959562, train/loss=0.148528, validation/accuracy=0.757180, validation/loss=1.041308, validation/num_examples=50000
I0307 23:19:09.417894 139787629491968 logging_writer.py:48] [502000] global_step=502000, grad_norm=4.037194728851318, loss=0.5772759318351746
I0307 23:19:43.097974 139787621099264 logging_writer.py:48] [502100] global_step=502100, grad_norm=4.197105407714844, loss=0.6205457448959351
I0307 23:20:16.737691 139787629491968 logging_writer.py:48] [502200] global_step=502200, grad_norm=4.86808443069458, loss=0.6176328659057617
I0307 23:20:50.427526 139787621099264 logging_writer.py:48] [502300] global_step=502300, grad_norm=4.951916217803955, loss=0.6531404256820679
I0307 23:21:24.037478 139787629491968 logging_writer.py:48] [502400] global_step=502400, grad_norm=4.357880115509033, loss=0.6119044423103333
I0307 23:21:57.701900 139787621099264 logging_writer.py:48] [502500] global_step=502500, grad_norm=4.044896602630615, loss=0.6077431440353394
I0307 23:22:31.313881 139787629491968 logging_writer.py:48] [502600] global_step=502600, grad_norm=4.438990592956543, loss=0.5877922177314758
I0307 23:23:04.964566 139787621099264 logging_writer.py:48] [502700] global_step=502700, grad_norm=4.175227642059326, loss=0.5637508034706116
I0307 23:23:38.604402 139787629491968 logging_writer.py:48] [502800] global_step=502800, grad_norm=4.313107013702393, loss=0.5851074457168579
I0307 23:24:12.288170 139787621099264 logging_writer.py:48] [502900] global_step=502900, grad_norm=4.7953715324401855, loss=0.6682538390159607
I0307 23:24:45.908516 139787629491968 logging_writer.py:48] [503000] global_step=503000, grad_norm=4.56577730178833, loss=0.6300798058509827
I0307 23:25:19.598633 139787621099264 logging_writer.py:48] [503100] global_step=503100, grad_norm=4.460818290710449, loss=0.6185240149497986
I0307 23:25:53.207703 139787629491968 logging_writer.py:48] [503200] global_step=503200, grad_norm=4.3080925941467285, loss=0.6315622329711914
I0307 23:26:26.877262 139787621099264 logging_writer.py:48] [503300] global_step=503300, grad_norm=4.529391765594482, loss=0.6419232487678528
I0307 23:27:00.499529 139787629491968 logging_writer.py:48] [503400] global_step=503400, grad_norm=4.512014865875244, loss=0.6559138894081116
I0307 23:27:30.266286 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:27:36.402399 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:27:44.916248 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:27:47.203093 139951089751872 submission_runner.py:411] Time since start: 175270.42s, 	Step: 503490, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.1469699740409851, 'validation/accuracy': 0.7571399807929993, 'validation/loss': 1.0414334535598755, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8225680589675903, 'test/num_examples': 10000, 'score': 169381.893055439, 'total_duration': 175270.41539549828, 'accumulated_submission_time': 169381.893055439, 'accumulated_eval_time': 5843.382396221161, 'accumulated_logging_time': 26.7176833152771}
I0307 23:27:47.300652 139789021992704 logging_writer.py:48] [503490] accumulated_eval_time=5843.382396, accumulated_logging_time=26.717683, accumulated_submission_time=169381.893055, global_step=503490, preemption_count=0, score=169381.893055, test/accuracy=0.631800, test/loss=1.822568, test/num_examples=10000, total_duration=175270.415395, train/accuracy=0.960539, train/loss=0.146970, validation/accuracy=0.757140, validation/loss=1.041433, validation/num_examples=50000
I0307 23:27:51.001716 139789038778112 logging_writer.py:48] [503500] global_step=503500, grad_norm=4.4622721672058105, loss=0.634259045124054
I0307 23:28:24.557740 139789021992704 logging_writer.py:48] [503600] global_step=503600, grad_norm=4.691848278045654, loss=0.632827877998352
I0307 23:28:58.160268 139789038778112 logging_writer.py:48] [503700] global_step=503700, grad_norm=4.441492080688477, loss=0.6412602663040161
I0307 23:29:31.772487 139789021992704 logging_writer.py:48] [503800] global_step=503800, grad_norm=4.554354190826416, loss=0.637286901473999
I0307 23:30:05.438922 139789038778112 logging_writer.py:48] [503900] global_step=503900, grad_norm=4.067353248596191, loss=0.5601605176925659
I0307 23:30:39.056874 139789021992704 logging_writer.py:48] [504000] global_step=504000, grad_norm=4.691594123840332, loss=0.6349084973335266
I0307 23:31:12.705007 139789038778112 logging_writer.py:48] [504100] global_step=504100, grad_norm=4.850640773773193, loss=0.7110781073570251
I0307 23:31:46.357940 139789021992704 logging_writer.py:48] [504200] global_step=504200, grad_norm=4.501023292541504, loss=0.6030089855194092
I0307 23:32:19.946841 139789038778112 logging_writer.py:48] [504300] global_step=504300, grad_norm=5.40976619720459, loss=0.6926987767219543
I0307 23:32:53.579363 139789021992704 logging_writer.py:48] [504400] global_step=504400, grad_norm=4.412285327911377, loss=0.5765663385391235
I0307 23:33:27.222543 139789038778112 logging_writer.py:48] [504500] global_step=504500, grad_norm=4.7347307205200195, loss=0.6341115832328796
I0307 23:34:00.840171 139789021992704 logging_writer.py:48] [504600] global_step=504600, grad_norm=4.529603958129883, loss=0.6540718078613281
I0307 23:34:34.483948 139789038778112 logging_writer.py:48] [504700] global_step=504700, grad_norm=4.612943172454834, loss=0.6500508785247803
I0307 23:35:08.117361 139789021992704 logging_writer.py:48] [504800] global_step=504800, grad_norm=4.714151859283447, loss=0.6606031656265259
I0307 23:35:41.757703 139789038778112 logging_writer.py:48] [504900] global_step=504900, grad_norm=4.245021343231201, loss=0.6308922171592712
I0307 23:36:15.397514 139789021992704 logging_writer.py:48] [505000] global_step=505000, grad_norm=4.569392204284668, loss=0.6379311084747314
I0307 23:36:17.220200 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:36:23.262751 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:36:31.772786 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:36:34.061958 139951089751872 submission_runner.py:411] Time since start: 175797.27s, 	Step: 505007, 	{'train/accuracy': 0.9607979655265808, 'train/loss': 0.14589370787143707, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.0407086610794067, 'validation/num_examples': 50000, 'test/accuracy': 0.631100058555603, 'test/loss': 1.8230360746383667, 'test/num_examples': 10000, 'score': 169891.74683499336, 'total_duration': 175797.27426624298, 'accumulated_submission_time': 169891.74683499336, 'accumulated_eval_time': 5860.224097967148, 'accumulated_logging_time': 26.82484793663025}
I0307 23:36:34.165166 139787612706560 logging_writer.py:48] [505007] accumulated_eval_time=5860.224098, accumulated_logging_time=26.824848, accumulated_submission_time=169891.746835, global_step=505007, preemption_count=0, score=169891.746835, test/accuracy=0.631100, test/loss=1.823036, test/num_examples=10000, total_duration=175797.274266, train/accuracy=0.960798, train/loss=0.145894, validation/accuracy=0.757500, validation/loss=1.040709, validation/num_examples=50000
I0307 23:37:05.752140 139787621099264 logging_writer.py:48] [505100] global_step=505100, grad_norm=4.566447734832764, loss=0.6112433671951294
I0307 23:37:39.484989 139787612706560 logging_writer.py:48] [505200] global_step=505200, grad_norm=4.474406719207764, loss=0.6007065176963806
I0307 23:38:13.080567 139787621099264 logging_writer.py:48] [505300] global_step=505300, grad_norm=4.619068622589111, loss=0.6482387781143188
I0307 23:38:46.742692 139787612706560 logging_writer.py:48] [505400] global_step=505400, grad_norm=4.386720180511475, loss=0.5688774585723877
I0307 23:39:20.359460 139787621099264 logging_writer.py:48] [505500] global_step=505500, grad_norm=4.4868316650390625, loss=0.5788744688034058
I0307 23:39:53.990886 139787612706560 logging_writer.py:48] [505600] global_step=505600, grad_norm=4.6852874755859375, loss=0.6230056881904602
I0307 23:40:27.604192 139787621099264 logging_writer.py:48] [505700] global_step=505700, grad_norm=4.429286956787109, loss=0.6606158018112183
I0307 23:41:01.249922 139787612706560 logging_writer.py:48] [505800] global_step=505800, grad_norm=4.2873430252075195, loss=0.671207845211029
I0307 23:41:34.843252 139787621099264 logging_writer.py:48] [505900] global_step=505900, grad_norm=4.936762809753418, loss=0.640775203704834
I0307 23:42:08.491712 139787612706560 logging_writer.py:48] [506000] global_step=506000, grad_norm=4.56958532333374, loss=0.6774559020996094
I0307 23:42:42.087720 139787621099264 logging_writer.py:48] [506100] global_step=506100, grad_norm=4.447114944458008, loss=0.5775545835494995
I0307 23:43:15.749538 139787612706560 logging_writer.py:48] [506200] global_step=506200, grad_norm=4.832435131072998, loss=0.6681879758834839
I0307 23:43:49.503831 139787621099264 logging_writer.py:48] [506300] global_step=506300, grad_norm=4.2748188972473145, loss=0.6167258620262146
I0307 23:44:23.194539 139787612706560 logging_writer.py:48] [506400] global_step=506400, grad_norm=4.664506435394287, loss=0.6198033094406128
I0307 23:44:56.818317 139787621099264 logging_writer.py:48] [506500] global_step=506500, grad_norm=4.594660758972168, loss=0.6645846366882324
I0307 23:45:04.362465 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:45:10.392212 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:45:19.027025 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:45:21.332087 139951089751872 submission_runner.py:411] Time since start: 176324.54s, 	Step: 506524, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.144617959856987, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0419772863388062, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8225206136703491, 'test/num_examples': 10000, 'score': 170401.87842082977, 'total_duration': 176324.54439496994, 'accumulated_submission_time': 170401.87842082977, 'accumulated_eval_time': 5877.193679094315, 'accumulated_logging_time': 26.938070058822632}
I0307 23:45:21.432768 139787612706560 logging_writer.py:48] [506524] accumulated_eval_time=5877.193679, accumulated_logging_time=26.938070, accumulated_submission_time=170401.878421, global_step=506524, preemption_count=0, score=170401.878421, test/accuracy=0.630800, test/loss=1.822521, test/num_examples=10000, total_duration=176324.544395, train/accuracy=0.961217, train/loss=0.144618, validation/accuracy=0.757200, validation/loss=1.041977, validation/num_examples=50000
I0307 23:45:47.387391 139787621099264 logging_writer.py:48] [506600] global_step=506600, grad_norm=4.198102951049805, loss=0.5620281100273132
I0307 23:46:21.058266 139787612706560 logging_writer.py:48] [506700] global_step=506700, grad_norm=4.400725841522217, loss=0.6337887644767761
I0307 23:46:54.723989 139787621099264 logging_writer.py:48] [506800] global_step=506800, grad_norm=4.435180187225342, loss=0.579895555973053
I0307 23:47:28.378067 139787612706560 logging_writer.py:48] [506900] global_step=506900, grad_norm=4.884934902191162, loss=0.5745438933372498
I0307 23:48:02.015431 139787621099264 logging_writer.py:48] [507000] global_step=507000, grad_norm=4.5235915184021, loss=0.6541532874107361
I0307 23:48:35.673070 139787612706560 logging_writer.py:48] [507100] global_step=507100, grad_norm=4.455867767333984, loss=0.6690329909324646
I0307 23:49:09.307968 139787621099264 logging_writer.py:48] [507200] global_step=507200, grad_norm=4.761139869689941, loss=0.6246720552444458
I0307 23:49:42.989637 139787612706560 logging_writer.py:48] [507300] global_step=507300, grad_norm=4.64615535736084, loss=0.6946595907211304
I0307 23:50:16.587799 139787621099264 logging_writer.py:48] [507400] global_step=507400, grad_norm=4.096641540527344, loss=0.5834694504737854
I0307 23:50:50.235591 139787612706560 logging_writer.py:48] [507500] global_step=507500, grad_norm=4.257207870483398, loss=0.6668611764907837
I0307 23:51:23.882804 139787621099264 logging_writer.py:48] [507600] global_step=507600, grad_norm=4.132232666015625, loss=0.6086918711662292
I0307 23:51:57.523093 139787612706560 logging_writer.py:48] [507700] global_step=507700, grad_norm=5.142343997955322, loss=0.726597785949707
I0307 23:52:31.185320 139787621099264 logging_writer.py:48] [507800] global_step=507800, grad_norm=5.251101016998291, loss=0.6447263956069946
I0307 23:53:04.839550 139787612706560 logging_writer.py:48] [507900] global_step=507900, grad_norm=4.650414943695068, loss=0.5817379355430603
I0307 23:53:38.456388 139787621099264 logging_writer.py:48] [508000] global_step=508000, grad_norm=4.648532867431641, loss=0.6315526366233826
I0307 23:53:51.366744 139951089751872 spec.py:321] Evaluating on the training split.
I0307 23:53:57.500204 139951089751872 spec.py:333] Evaluating on the validation split.
I0307 23:54:05.936493 139951089751872 spec.py:349] Evaluating on the test split.
I0307 23:54:08.200374 139951089751872 submission_runner.py:411] Time since start: 176851.41s, 	Step: 508040, 	{'train/accuracy': 0.9621930718421936, 'train/loss': 0.14290735125541687, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.04066002368927, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8211873769760132, 'test/num_examples': 10000, 'score': 170911.74804997444, 'total_duration': 176851.41265058517, 'accumulated_submission_time': 170911.74804997444, 'accumulated_eval_time': 5894.027221918106, 'accumulated_logging_time': 27.048126935958862}
I0307 23:54:08.295957 139787612706560 logging_writer.py:48] [508040] accumulated_eval_time=5894.027222, accumulated_logging_time=27.048127, accumulated_submission_time=170911.748050, global_step=508040, preemption_count=0, score=170911.748050, test/accuracy=0.632200, test/loss=1.821187, test/num_examples=10000, total_duration=176851.412651, train/accuracy=0.962193, train/loss=0.142907, validation/accuracy=0.757160, validation/loss=1.040660, validation/num_examples=50000
I0307 23:54:28.796764 139787629491968 logging_writer.py:48] [508100] global_step=508100, grad_norm=4.478267192840576, loss=0.5861411094665527
I0307 23:55:02.378484 139787612706560 logging_writer.py:48] [508200] global_step=508200, grad_norm=4.409487247467041, loss=0.5624353885650635
I0307 23:55:35.959203 139787629491968 logging_writer.py:48] [508300] global_step=508300, grad_norm=4.5345072746276855, loss=0.6028673052787781
I0307 23:56:09.605349 139787612706560 logging_writer.py:48] [508400] global_step=508400, grad_norm=4.337449550628662, loss=0.5846944451332092
I0307 23:56:43.165761 139787629491968 logging_writer.py:48] [508500] global_step=508500, grad_norm=4.682676792144775, loss=0.6910104751586914
I0307 23:57:16.800038 139787612706560 logging_writer.py:48] [508600] global_step=508600, grad_norm=4.275753021240234, loss=0.6188125610351562
I0307 23:57:50.433640 139787629491968 logging_writer.py:48] [508700] global_step=508700, grad_norm=4.693263530731201, loss=0.6714707016944885
I0307 23:58:24.029677 139787612706560 logging_writer.py:48] [508800] global_step=508800, grad_norm=4.562077522277832, loss=0.6144046187400818
I0307 23:58:57.581476 139787629491968 logging_writer.py:48] [508900] global_step=508900, grad_norm=4.43333101272583, loss=0.6901370882987976
I0307 23:59:31.136373 139787612706560 logging_writer.py:48] [509000] global_step=509000, grad_norm=4.2621002197265625, loss=0.6196506023406982
I0308 00:00:04.726135 139787629491968 logging_writer.py:48] [509100] global_step=509100, grad_norm=4.466351509094238, loss=0.6605902314186096
I0308 00:00:38.330255 139787612706560 logging_writer.py:48] [509200] global_step=509200, grad_norm=4.535189628601074, loss=0.6272758841514587
I0308 00:01:12.002794 139787629491968 logging_writer.py:48] [509300] global_step=509300, grad_norm=4.74997091293335, loss=0.6463543176651001
I0308 00:01:45.626298 139787612706560 logging_writer.py:48] [509400] global_step=509400, grad_norm=4.528132438659668, loss=0.6567474603652954
I0308 00:02:19.268111 139787629491968 logging_writer.py:48] [509500] global_step=509500, grad_norm=4.663962364196777, loss=0.6435717940330505
I0308 00:02:38.232657 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:02:44.244391 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:02:52.832266 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:02:55.104249 139951089751872 submission_runner.py:411] Time since start: 177378.32s, 	Step: 509558, 	{'train/accuracy': 0.9604392051696777, 'train/loss': 0.14881470799446106, 'validation/accuracy': 0.7572999596595764, 'validation/loss': 1.0417289733886719, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8247904777526855, 'test/num_examples': 10000, 'score': 171421.61796426773, 'total_duration': 177378.3165552616, 'accumulated_submission_time': 171421.61796426773, 'accumulated_eval_time': 5910.898756742477, 'accumulated_logging_time': 27.1546368598938}
I0308 00:02:55.206100 139789021992704 logging_writer.py:48] [509558] accumulated_eval_time=5910.898757, accumulated_logging_time=27.154637, accumulated_submission_time=171421.617964, global_step=509558, preemption_count=0, score=171421.617964, test/accuracy=0.631000, test/loss=1.824790, test/num_examples=10000, total_duration=177378.316555, train/accuracy=0.960439, train/loss=0.148815, validation/accuracy=0.757300, validation/loss=1.041729, validation/num_examples=50000
I0308 00:03:09.677420 139789030385408 logging_writer.py:48] [509600] global_step=509600, grad_norm=4.256645202636719, loss=0.6391646265983582
I0308 00:03:43.297157 139789021992704 logging_writer.py:48] [509700] global_step=509700, grad_norm=4.5572404861450195, loss=0.6174885034561157
I0308 00:04:16.936165 139789030385408 logging_writer.py:48] [509800] global_step=509800, grad_norm=4.326205253601074, loss=0.6118645668029785
I0308 00:04:50.599853 139789021992704 logging_writer.py:48] [509900] global_step=509900, grad_norm=4.568694114685059, loss=0.609519898891449
I0308 00:05:24.234379 139789030385408 logging_writer.py:48] [510000] global_step=510000, grad_norm=4.695810794830322, loss=0.7400113940238953
I0308 00:05:57.926638 139789021992704 logging_writer.py:48] [510100] global_step=510100, grad_norm=4.263613700866699, loss=0.5624103546142578
I0308 00:06:31.550297 139789030385408 logging_writer.py:48] [510200] global_step=510200, grad_norm=4.263500690460205, loss=0.5659978985786438
I0308 00:07:05.222702 139789021992704 logging_writer.py:48] [510300] global_step=510300, grad_norm=4.349322319030762, loss=0.6074094772338867
I0308 00:07:38.881809 139789030385408 logging_writer.py:48] [510400] global_step=510400, grad_norm=4.551806449890137, loss=0.6567972302436829
I0308 00:08:12.569588 139789021992704 logging_writer.py:48] [510500] global_step=510500, grad_norm=4.385012626647949, loss=0.6197254061698914
I0308 00:08:46.155105 139789030385408 logging_writer.py:48] [510600] global_step=510600, grad_norm=4.254718780517578, loss=0.5697238445281982
I0308 00:09:19.743216 139789021992704 logging_writer.py:48] [510700] global_step=510700, grad_norm=4.896013259887695, loss=0.6201196908950806
I0308 00:09:53.408483 139789030385408 logging_writer.py:48] [510800] global_step=510800, grad_norm=4.533382892608643, loss=0.5919685363769531
I0308 00:10:27.083749 139789021992704 logging_writer.py:48] [510900] global_step=510900, grad_norm=4.752800941467285, loss=0.6551198363304138
I0308 00:11:00.732550 139789030385408 logging_writer.py:48] [511000] global_step=511000, grad_norm=4.157416820526123, loss=0.549910843372345
I0308 00:11:25.438444 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:11:31.478303 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:11:39.998000 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:11:42.290992 139951089751872 submission_runner.py:411] Time since start: 177905.50s, 	Step: 511075, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14799103140830994, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0400274991989136, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.821224331855774, 'test/num_examples': 10000, 'score': 171931.7805171013, 'total_duration': 177905.50328946114, 'accumulated_submission_time': 171931.7805171013, 'accumulated_eval_time': 5927.751242399216, 'accumulated_logging_time': 27.269541263580322}
I0308 00:11:42.396516 139787629491968 logging_writer.py:48] [511075] accumulated_eval_time=5927.751242, accumulated_logging_time=27.269541, accumulated_submission_time=171931.780517, global_step=511075, preemption_count=0, score=171931.780517, test/accuracy=0.632200, test/loss=1.821224, test/num_examples=10000, total_duration=177905.503289, train/accuracy=0.960559, train/loss=0.147991, validation/accuracy=0.757200, validation/loss=1.040027, validation/num_examples=50000
I0308 00:11:51.156589 139788996814592 logging_writer.py:48] [511100] global_step=511100, grad_norm=4.499185562133789, loss=0.6414718627929688
I0308 00:12:24.839206 139787629491968 logging_writer.py:48] [511200] global_step=511200, grad_norm=4.236921787261963, loss=0.600245475769043
I0308 00:12:58.476470 139788996814592 logging_writer.py:48] [511300] global_step=511300, grad_norm=4.516513824462891, loss=0.618618369102478
I0308 00:13:32.073498 139787629491968 logging_writer.py:48] [511400] global_step=511400, grad_norm=4.977576732635498, loss=0.662327766418457
I0308 00:14:05.724945 139788996814592 logging_writer.py:48] [511500] global_step=511500, grad_norm=4.190995216369629, loss=0.5382756590843201
I0308 00:14:39.417800 139787629491968 logging_writer.py:48] [511600] global_step=511600, grad_norm=4.5655131340026855, loss=0.6436808109283447
I0308 00:15:13.076540 139788996814592 logging_writer.py:48] [511700] global_step=511700, grad_norm=4.423772811889648, loss=0.6287651062011719
I0308 00:15:46.691605 139787629491968 logging_writer.py:48] [511800] global_step=511800, grad_norm=5.11793851852417, loss=0.6495977640151978
I0308 00:16:20.287942 139788996814592 logging_writer.py:48] [511900] global_step=511900, grad_norm=4.547805309295654, loss=0.5582257509231567
I0308 00:16:53.892414 139787629491968 logging_writer.py:48] [512000] global_step=512000, grad_norm=4.670109748840332, loss=0.655670166015625
I0308 00:17:27.473721 139788996814592 logging_writer.py:48] [512100] global_step=512100, grad_norm=4.698820114135742, loss=0.6151975989341736
I0308 00:18:01.037766 139787629491968 logging_writer.py:48] [512200] global_step=512200, grad_norm=4.240811824798584, loss=0.6369301080703735
I0308 00:18:34.629072 139788996814592 logging_writer.py:48] [512300] global_step=512300, grad_norm=4.814034461975098, loss=0.671755313873291
I0308 00:19:08.315705 139787629491968 logging_writer.py:48] [512400] global_step=512400, grad_norm=4.275506019592285, loss=0.6360598802566528
I0308 00:19:41.950418 139788996814592 logging_writer.py:48] [512500] global_step=512500, grad_norm=4.475497245788574, loss=0.607694685459137
I0308 00:20:12.420863 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:20:18.442910 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:20:27.030379 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:20:29.308452 139951089751872 submission_runner.py:411] Time since start: 178432.52s, 	Step: 512592, 	{'train/accuracy': 0.9605787396430969, 'train/loss': 0.14627830684185028, 'validation/accuracy': 0.7572399973869324, 'validation/loss': 1.0417357683181763, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.8225817680358887, 'test/num_examples': 10000, 'score': 172441.7371351719, 'total_duration': 178432.52074337006, 'accumulated_submission_time': 172441.7371351719, 'accumulated_eval_time': 5944.638758897781, 'accumulated_logging_time': 27.386696577072144}
I0308 00:20:29.417400 139787629491968 logging_writer.py:48] [512592] accumulated_eval_time=5944.638759, accumulated_logging_time=27.386697, accumulated_submission_time=172441.737135, global_step=512592, preemption_count=0, score=172441.737135, test/accuracy=0.632600, test/loss=1.822582, test/num_examples=10000, total_duration=178432.520743, train/accuracy=0.960579, train/loss=0.146278, validation/accuracy=0.757240, validation/loss=1.041736, validation/num_examples=50000
I0308 00:20:32.439485 139789013600000 logging_writer.py:48] [512600] global_step=512600, grad_norm=4.7056403160095215, loss=0.7334227561950684
I0308 00:21:06.027755 139787629491968 logging_writer.py:48] [512700] global_step=512700, grad_norm=4.950279235839844, loss=0.6910189986228943
I0308 00:21:39.631405 139789013600000 logging_writer.py:48] [512800] global_step=512800, grad_norm=4.655879020690918, loss=0.5846453905105591
I0308 00:22:13.278465 139787629491968 logging_writer.py:48] [512900] global_step=512900, grad_norm=5.2304863929748535, loss=0.7022755146026611
I0308 00:22:46.923124 139789013600000 logging_writer.py:48] [513000] global_step=513000, grad_norm=4.027273178100586, loss=0.5783812403678894
I0308 00:23:20.557663 139787629491968 logging_writer.py:48] [513100] global_step=513100, grad_norm=4.767730236053467, loss=0.6485949754714966
I0308 00:23:54.173974 139789013600000 logging_writer.py:48] [513200] global_step=513200, grad_norm=4.187509059906006, loss=0.5983909964561462
I0308 00:24:27.811934 139787629491968 logging_writer.py:48] [513300] global_step=513300, grad_norm=4.671515941619873, loss=0.6172282695770264
I0308 00:25:01.436762 139789013600000 logging_writer.py:48] [513400] global_step=513400, grad_norm=4.377372741699219, loss=0.5907882452011108
I0308 00:25:35.082342 139787629491968 logging_writer.py:48] [513500] global_step=513500, grad_norm=4.019786834716797, loss=0.5612815618515015
I0308 00:26:08.707808 139789013600000 logging_writer.py:48] [513600] global_step=513600, grad_norm=4.218333721160889, loss=0.6081993579864502
I0308 00:26:42.354517 139787629491968 logging_writer.py:48] [513700] global_step=513700, grad_norm=4.51568078994751, loss=0.6172023415565491
I0308 00:27:15.967958 139789013600000 logging_writer.py:48] [513800] global_step=513800, grad_norm=4.136966228485107, loss=0.6039384603500366
I0308 00:27:49.627367 139787629491968 logging_writer.py:48] [513900] global_step=513900, grad_norm=4.217372417449951, loss=0.6264885663986206
I0308 00:28:23.242995 139789013600000 logging_writer.py:48] [514000] global_step=514000, grad_norm=4.224113941192627, loss=0.6208744645118713
I0308 00:28:56.889178 139787629491968 logging_writer.py:48] [514100] global_step=514100, grad_norm=4.35679817199707, loss=0.5860096216201782
I0308 00:28:59.389537 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:29:05.453773 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:29:14.028627 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:29:16.328813 139951089751872 submission_runner.py:411] Time since start: 178959.54s, 	Step: 514109, 	{'train/accuracy': 0.9622528553009033, 'train/loss': 0.14303383231163025, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0404475927352905, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.8226438760757446, 'test/num_examples': 10000, 'score': 172951.64312791824, 'total_duration': 178959.5411233902, 'accumulated_submission_time': 172951.64312791824, 'accumulated_eval_time': 5961.57798075676, 'accumulated_logging_time': 27.505539417266846}
I0308 00:29:16.433095 139787621099264 logging_writer.py:48] [514109] accumulated_eval_time=5961.577981, accumulated_logging_time=27.505539, accumulated_submission_time=172951.643128, global_step=514109, preemption_count=0, score=172951.643128, test/accuracy=0.631800, test/loss=1.822644, test/num_examples=10000, total_duration=178959.541123, train/accuracy=0.962253, train/loss=0.143034, validation/accuracy=0.757040, validation/loss=1.040448, validation/num_examples=50000
I0308 00:29:47.320601 139787629491968 logging_writer.py:48] [514200] global_step=514200, grad_norm=4.394960403442383, loss=0.6599144339561462
I0308 00:30:20.912682 139787621099264 logging_writer.py:48] [514300] global_step=514300, grad_norm=4.740812301635742, loss=0.6878248453140259
I0308 00:30:54.512696 139787629491968 logging_writer.py:48] [514400] global_step=514400, grad_norm=4.748629570007324, loss=0.6416152119636536
I0308 00:31:28.172056 139787621099264 logging_writer.py:48] [514500] global_step=514500, grad_norm=4.292431354522705, loss=0.6126834154129028
I0308 00:32:01.766924 139787629491968 logging_writer.py:48] [514600] global_step=514600, grad_norm=3.9663095474243164, loss=0.5716134309768677
I0308 00:32:35.483620 139787621099264 logging_writer.py:48] [514700] global_step=514700, grad_norm=4.6312456130981445, loss=0.6648437976837158
I0308 00:33:09.106709 139787629491968 logging_writer.py:48] [514800] global_step=514800, grad_norm=4.1977458000183105, loss=0.5400723218917847
I0308 00:33:42.726542 139787621099264 logging_writer.py:48] [514900] global_step=514900, grad_norm=4.258272171020508, loss=0.6128401756286621
I0308 00:34:16.308583 139787629491968 logging_writer.py:48] [515000] global_step=515000, grad_norm=4.503244876861572, loss=0.5882394313812256
I0308 00:34:49.876850 139787621099264 logging_writer.py:48] [515100] global_step=515100, grad_norm=4.2929840087890625, loss=0.5809249877929688
I0308 00:35:23.471828 139787629491968 logging_writer.py:48] [515200] global_step=515200, grad_norm=4.243258476257324, loss=0.6057760715484619
I0308 00:35:57.126878 139787621099264 logging_writer.py:48] [515300] global_step=515300, grad_norm=4.470402240753174, loss=0.6450081467628479
I0308 00:36:30.718666 139787629491968 logging_writer.py:48] [515400] global_step=515400, grad_norm=4.859902858734131, loss=0.6601419448852539
I0308 00:37:04.404479 139787621099264 logging_writer.py:48] [515500] global_step=515500, grad_norm=4.484415054321289, loss=0.575317919254303
I0308 00:37:38.015612 139787629491968 logging_writer.py:48] [515600] global_step=515600, grad_norm=4.473570823669434, loss=0.6433373689651489
I0308 00:37:46.579025 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:37:52.598683 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:38:01.093467 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:38:03.377652 139951089751872 submission_runner.py:411] Time since start: 179486.59s, 	Step: 515627, 	{'train/accuracy': 0.9596619606018066, 'train/loss': 0.14832505583763123, 'validation/accuracy': 0.757319986820221, 'validation/loss': 1.0403424501419067, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8204569816589355, 'test/num_examples': 10000, 'score': 173461.72400903702, 'total_duration': 179486.58996462822, 'accumulated_submission_time': 173461.72400903702, 'accumulated_eval_time': 5978.376556158066, 'accumulated_logging_time': 27.6191668510437}
I0308 00:38:03.481825 139787612706560 logging_writer.py:48] [515627] accumulated_eval_time=5978.376556, accumulated_logging_time=27.619167, accumulated_submission_time=173461.724009, global_step=515627, preemption_count=0, score=173461.724009, test/accuracy=0.631900, test/loss=1.820457, test/num_examples=10000, total_duration=179486.589965, train/accuracy=0.959662, train/loss=0.148325, validation/accuracy=0.757320, validation/loss=1.040342, validation/num_examples=50000
I0308 00:38:28.344973 139789013600000 logging_writer.py:48] [515700] global_step=515700, grad_norm=4.461384296417236, loss=0.6333646178245544
I0308 00:39:02.079712 139787612706560 logging_writer.py:48] [515800] global_step=515800, grad_norm=4.385596752166748, loss=0.5357687473297119
I0308 00:39:35.663506 139789013600000 logging_writer.py:48] [515900] global_step=515900, grad_norm=5.3299560546875, loss=0.6437482237815857
I0308 00:40:09.327401 139787612706560 logging_writer.py:48] [516000] global_step=516000, grad_norm=4.969326972961426, loss=0.6611568331718445
I0308 00:40:42.980159 139789013600000 logging_writer.py:48] [516100] global_step=516100, grad_norm=4.4986371994018555, loss=0.6148289442062378
I0308 00:41:16.622559 139787612706560 logging_writer.py:48] [516200] global_step=516200, grad_norm=4.657191276550293, loss=0.5963402390480042
I0308 00:41:50.270393 139789013600000 logging_writer.py:48] [516300] global_step=516300, grad_norm=5.1576995849609375, loss=0.7588373422622681
I0308 00:42:23.919039 139787612706560 logging_writer.py:48] [516400] global_step=516400, grad_norm=5.085315704345703, loss=0.7017213106155396
I0308 00:42:57.551385 139789013600000 logging_writer.py:48] [516500] global_step=516500, grad_norm=5.014885902404785, loss=0.6176286339759827
I0308 00:43:31.176244 139787612706560 logging_writer.py:48] [516600] global_step=516600, grad_norm=4.279020309448242, loss=0.5804526805877686
I0308 00:44:04.841723 139789013600000 logging_writer.py:48] [516700] global_step=516700, grad_norm=4.351078033447266, loss=0.5903646945953369
I0308 00:44:38.500081 139787612706560 logging_writer.py:48] [516800] global_step=516800, grad_norm=4.684535026550293, loss=0.6765025854110718
I0308 00:45:12.158450 139789013600000 logging_writer.py:48] [516900] global_step=516900, grad_norm=4.990137100219727, loss=0.7189627289772034
I0308 00:45:45.733510 139787612706560 logging_writer.py:48] [517000] global_step=517000, grad_norm=4.640511512756348, loss=0.6585870981216431
I0308 00:46:19.310598 139789013600000 logging_writer.py:48] [517100] global_step=517100, grad_norm=4.536096096038818, loss=0.7244303822517395
I0308 00:46:33.564931 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:46:39.549319 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:46:48.111475 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:46:50.389156 139951089751872 submission_runner.py:411] Time since start: 180013.60s, 	Step: 517144, 	{'train/accuracy': 0.9599210619926453, 'train/loss': 0.14836977422237396, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0417215824127197, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.82349693775177, 'test/num_examples': 10000, 'score': 173971.74058961868, 'total_duration': 180013.60146594048, 'accumulated_submission_time': 173971.74058961868, 'accumulated_eval_time': 5995.200742959976, 'accumulated_logging_time': 27.73391819000244}
I0308 00:46:50.500361 139788996814592 logging_writer.py:48] [517144] accumulated_eval_time=5995.200743, accumulated_logging_time=27.733918, accumulated_submission_time=173971.740590, global_step=517144, preemption_count=0, score=173971.740590, test/accuracy=0.630700, test/loss=1.823497, test/num_examples=10000, total_duration=180013.601466, train/accuracy=0.959921, train/loss=0.148370, validation/accuracy=0.757180, validation/loss=1.041722, validation/num_examples=50000
I0308 00:47:09.676716 139789005207296 logging_writer.py:48] [517200] global_step=517200, grad_norm=4.448108196258545, loss=0.5947996973991394
I0308 00:47:43.343473 139788996814592 logging_writer.py:48] [517300] global_step=517300, grad_norm=4.470851421356201, loss=0.5722804665565491
I0308 00:48:17.002815 139789005207296 logging_writer.py:48] [517400] global_step=517400, grad_norm=4.366250991821289, loss=0.5964542031288147
I0308 00:48:50.608212 139788996814592 logging_writer.py:48] [517500] global_step=517500, grad_norm=4.650385856628418, loss=0.6339676976203918
I0308 00:49:24.282522 139789005207296 logging_writer.py:48] [517600] global_step=517600, grad_norm=4.114288330078125, loss=0.5950225591659546
I0308 00:49:57.864522 139788996814592 logging_writer.py:48] [517700] global_step=517700, grad_norm=4.880114555358887, loss=0.6649855971336365
I0308 00:50:31.538072 139789005207296 logging_writer.py:48] [517800] global_step=517800, grad_norm=4.137894153594971, loss=0.6176185607910156
I0308 00:51:05.216642 139788996814592 logging_writer.py:48] [517900] global_step=517900, grad_norm=4.0904622077941895, loss=0.5647037029266357
I0308 00:51:38.882110 139789005207296 logging_writer.py:48] [518000] global_step=518000, grad_norm=4.860232830047607, loss=0.6507233381271362
I0308 00:52:12.486258 139788996814592 logging_writer.py:48] [518100] global_step=518100, grad_norm=4.536220073699951, loss=0.6372873187065125
I0308 00:52:46.153323 139789005207296 logging_writer.py:48] [518200] global_step=518200, grad_norm=4.667812347412109, loss=0.628682017326355
I0308 00:53:19.808306 139788996814592 logging_writer.py:48] [518300] global_step=518300, grad_norm=4.640459060668945, loss=0.5926871299743652
I0308 00:53:53.475116 139789005207296 logging_writer.py:48] [518400] global_step=518400, grad_norm=4.007656097412109, loss=0.5536446571350098
I0308 00:54:27.078356 139788996814592 logging_writer.py:48] [518500] global_step=518500, grad_norm=4.403571128845215, loss=0.6102876663208008
I0308 00:55:00.721001 139789005207296 logging_writer.py:48] [518600] global_step=518600, grad_norm=4.10160493850708, loss=0.5773234367370605
I0308 00:55:20.677303 139951089751872 spec.py:321] Evaluating on the training split.
I0308 00:55:26.744607 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 00:55:35.292094 139951089751872 spec.py:349] Evaluating on the test split.
I0308 00:55:37.571880 139951089751872 submission_runner.py:411] Time since start: 180540.78s, 	Step: 518661, 	{'train/accuracy': 0.959004282951355, 'train/loss': 0.14983192086219788, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0412065982818604, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8231717348098755, 'test/num_examples': 10000, 'score': 174481.84917855263, 'total_duration': 180540.78418684006, 'accumulated_submission_time': 174481.84917855263, 'accumulated_eval_time': 6012.095266580582, 'accumulated_logging_time': 27.856269359588623}
I0308 00:55:37.673642 139787621099264 logging_writer.py:48] [518661] accumulated_eval_time=6012.095267, accumulated_logging_time=27.856269, accumulated_submission_time=174481.849179, global_step=518661, preemption_count=0, score=174481.849179, test/accuracy=0.632500, test/loss=1.823172, test/num_examples=10000, total_duration=180540.784187, train/accuracy=0.959004, train/loss=0.149832, validation/accuracy=0.757160, validation/loss=1.041207, validation/num_examples=50000
I0308 00:55:51.125214 139789013600000 logging_writer.py:48] [518700] global_step=518700, grad_norm=4.433650016784668, loss=0.6252092123031616
I0308 00:56:24.765897 139787621099264 logging_writer.py:48] [518800] global_step=518800, grad_norm=4.41655969619751, loss=0.6328831315040588
I0308 00:56:58.456179 139789013600000 logging_writer.py:48] [518900] global_step=518900, grad_norm=4.472256183624268, loss=0.6118927001953125
I0308 00:57:32.053337 139787621099264 logging_writer.py:48] [519000] global_step=519000, grad_norm=4.511682510375977, loss=0.5812085866928101
I0308 00:58:05.691367 139789013600000 logging_writer.py:48] [519100] global_step=519100, grad_norm=4.297643661499023, loss=0.667265772819519
I0308 00:58:39.312576 139787621099264 logging_writer.py:48] [519200] global_step=519200, grad_norm=4.1269612312316895, loss=0.5494012832641602
I0308 00:59:12.950050 139789013600000 logging_writer.py:48] [519300] global_step=519300, grad_norm=4.817231178283691, loss=0.698947012424469
I0308 00:59:46.592495 139787621099264 logging_writer.py:48] [519400] global_step=519400, grad_norm=4.227645397186279, loss=0.5583578944206238
I0308 01:00:20.238353 139789013600000 logging_writer.py:48] [519500] global_step=519500, grad_norm=4.77671480178833, loss=0.6726586818695068
I0308 01:00:53.856584 139787621099264 logging_writer.py:48] [519600] global_step=519600, grad_norm=4.5067219734191895, loss=0.6183685660362244
I0308 01:01:27.506967 139789013600000 logging_writer.py:48] [519700] global_step=519700, grad_norm=4.307246208190918, loss=0.5673035979270935
I0308 01:02:01.160371 139787621099264 logging_writer.py:48] [519800] global_step=519800, grad_norm=4.564643383026123, loss=0.6827825903892517
I0308 01:02:34.808823 139789013600000 logging_writer.py:48] [519900] global_step=519900, grad_norm=4.490236759185791, loss=0.5960947275161743
I0308 01:03:08.456856 139787621099264 logging_writer.py:48] [520000] global_step=520000, grad_norm=4.592116355895996, loss=0.5901102423667908
I0308 01:03:42.053123 139789013600000 logging_writer.py:48] [520100] global_step=520100, grad_norm=4.3909173011779785, loss=0.6195874810218811
I0308 01:04:07.721498 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:04:13.779270 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:04:22.400490 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:04:24.695885 139951089751872 submission_runner.py:411] Time since start: 181067.91s, 	Step: 520178, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14468808472156525, 'validation/accuracy': 0.757319986820221, 'validation/loss': 1.0408968925476074, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8221988677978516, 'test/num_examples': 10000, 'score': 174991.8268210888, 'total_duration': 181067.9081850052, 'accumulated_submission_time': 174991.8268210888, 'accumulated_eval_time': 6029.069609165192, 'accumulated_logging_time': 27.970374822616577}
I0308 01:04:24.798055 139787629491968 logging_writer.py:48] [520178] accumulated_eval_time=6029.069609, accumulated_logging_time=27.970375, accumulated_submission_time=174991.826821, global_step=520178, preemption_count=0, score=174991.826821, test/accuracy=0.632200, test/loss=1.822199, test/num_examples=10000, total_duration=181067.908185, train/accuracy=0.961655, train/loss=0.144688, validation/accuracy=0.757320, validation/loss=1.040897, validation/num_examples=50000
I0308 01:04:32.512900 139788996814592 logging_writer.py:48] [520200] global_step=520200, grad_norm=4.287996292114258, loss=0.5718480348587036
I0308 01:05:06.177059 139787629491968 logging_writer.py:48] [520300] global_step=520300, grad_norm=4.700336456298828, loss=0.701711893081665
I0308 01:05:39.824677 139788996814592 logging_writer.py:48] [520400] global_step=520400, grad_norm=4.670533657073975, loss=0.6875581741333008
I0308 01:06:13.377605 139787629491968 logging_writer.py:48] [520500] global_step=520500, grad_norm=4.258574962615967, loss=0.5889236927032471
I0308 01:06:46.980122 139788996814592 logging_writer.py:48] [520600] global_step=520600, grad_norm=4.33863639831543, loss=0.5482466220855713
I0308 01:07:20.597958 139787629491968 logging_writer.py:48] [520700] global_step=520700, grad_norm=4.675243377685547, loss=0.6557537913322449
I0308 01:07:54.282298 139788996814592 logging_writer.py:48] [520800] global_step=520800, grad_norm=4.76023530960083, loss=0.6750556230545044
I0308 01:08:27.909545 139787629491968 logging_writer.py:48] [520900] global_step=520900, grad_norm=4.473408222198486, loss=0.5952075123786926
I0308 01:09:01.578445 139788996814592 logging_writer.py:48] [521000] global_step=521000, grad_norm=4.063607215881348, loss=0.5518699884414673
I0308 01:09:35.248709 139787629491968 logging_writer.py:48] [521100] global_step=521100, grad_norm=4.456473350524902, loss=0.6019928455352783
I0308 01:10:08.841650 139788996814592 logging_writer.py:48] [521200] global_step=521200, grad_norm=6.097220420837402, loss=0.7612194418907166
I0308 01:10:42.507419 139787629491968 logging_writer.py:48] [521300] global_step=521300, grad_norm=4.806921005249023, loss=0.6469515562057495
I0308 01:11:16.202634 139788996814592 logging_writer.py:48] [521400] global_step=521400, grad_norm=4.150382041931152, loss=0.6021435856819153
I0308 01:11:49.860837 139787629491968 logging_writer.py:48] [521500] global_step=521500, grad_norm=4.334189414978027, loss=0.5633394122123718
I0308 01:12:23.462211 139788996814592 logging_writer.py:48] [521600] global_step=521600, grad_norm=4.54407262802124, loss=0.6627541184425354
I0308 01:12:54.918748 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:13:01.014301 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:13:09.481950 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:13:11.758528 139951089751872 submission_runner.py:411] Time since start: 181594.97s, 	Step: 521695, 	{'train/accuracy': 0.9610371589660645, 'train/loss': 0.14848162233829498, 'validation/accuracy': 0.757319986820221, 'validation/loss': 1.0408750772476196, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8227356672286987, 'test/num_examples': 10000, 'score': 175501.88168287277, 'total_duration': 181594.9708378315, 'accumulated_submission_time': 175501.88168287277, 'accumulated_eval_time': 6045.909396409988, 'accumulated_logging_time': 28.082290172576904}
I0308 01:13:11.864114 139787629491968 logging_writer.py:48] [521695] accumulated_eval_time=6045.909396, accumulated_logging_time=28.082290, accumulated_submission_time=175501.881683, global_step=521695, preemption_count=0, score=175501.881683, test/accuracy=0.632000, test/loss=1.822736, test/num_examples=10000, total_duration=181594.970838, train/accuracy=0.961037, train/loss=0.148482, validation/accuracy=0.757320, validation/loss=1.040875, validation/num_examples=50000
I0308 01:13:13.877898 139788996814592 logging_writer.py:48] [521700] global_step=521700, grad_norm=4.417038440704346, loss=0.6750797033309937
I0308 01:13:47.512734 139787629491968 logging_writer.py:48] [521800] global_step=521800, grad_norm=4.340251922607422, loss=0.62479567527771
I0308 01:14:21.145548 139788996814592 logging_writer.py:48] [521900] global_step=521900, grad_norm=4.620279312133789, loss=0.6723150610923767
I0308 01:14:54.775761 139787629491968 logging_writer.py:48] [522000] global_step=522000, grad_norm=4.570925712585449, loss=0.6642804145812988
I0308 01:15:28.471686 139788996814592 logging_writer.py:48] [522100] global_step=522100, grad_norm=4.11891508102417, loss=0.6201545596122742
I0308 01:16:02.077669 139787629491968 logging_writer.py:48] [522200] global_step=522200, grad_norm=4.139726161956787, loss=0.573746919631958
I0308 01:16:35.675120 139788996814592 logging_writer.py:48] [522300] global_step=522300, grad_norm=4.001843452453613, loss=0.5763067007064819
I0308 01:17:09.238867 139787629491968 logging_writer.py:48] [522400] global_step=522400, grad_norm=4.102467060089111, loss=0.6095622181892395
I0308 01:17:42.790802 139788996814592 logging_writer.py:48] [522500] global_step=522500, grad_norm=4.509958267211914, loss=0.6372567415237427
I0308 01:18:16.431456 139787629491968 logging_writer.py:48] [522600] global_step=522600, grad_norm=5.101259708404541, loss=0.7248633503913879
I0308 01:18:50.087050 139788996814592 logging_writer.py:48] [522700] global_step=522700, grad_norm=4.421009063720703, loss=0.5628472566604614
I0308 01:19:23.709789 139787629491968 logging_writer.py:48] [522800] global_step=522800, grad_norm=4.722157955169678, loss=0.5394465327262878
I0308 01:19:57.383460 139788996814592 logging_writer.py:48] [522900] global_step=522900, grad_norm=4.585736274719238, loss=0.5208051204681396
I0308 01:20:31.060458 139787629491968 logging_writer.py:48] [523000] global_step=523000, grad_norm=4.811489105224609, loss=0.663352906703949
I0308 01:21:04.712303 139788996814592 logging_writer.py:48] [523100] global_step=523100, grad_norm=4.824871063232422, loss=0.7006280422210693
I0308 01:21:38.369134 139787629491968 logging_writer.py:48] [523200] global_step=523200, grad_norm=4.359567642211914, loss=0.6720083951950073
I0308 01:21:41.871574 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:21:48.054186 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:21:56.501700 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:21:58.811641 139951089751872 submission_runner.py:411] Time since start: 182122.02s, 	Step: 523212, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.14540383219718933, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0417425632476807, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8241809606552124, 'test/num_examples': 10000, 'score': 176011.82210946083, 'total_duration': 182122.0238277912, 'accumulated_submission_time': 176011.82210946083, 'accumulated_eval_time': 6062.849289655685, 'accumulated_logging_time': 28.1988844871521}
I0308 01:21:58.915251 139787621099264 logging_writer.py:48] [523212] accumulated_eval_time=6062.849290, accumulated_logging_time=28.198884, accumulated_submission_time=176011.822109, global_step=523212, preemption_count=0, score=176011.822109, test/accuracy=0.631900, test/loss=1.824181, test/num_examples=10000, total_duration=182122.023828, train/accuracy=0.960977, train/loss=0.145404, validation/accuracy=0.757160, validation/loss=1.041743, validation/num_examples=50000
I0308 01:22:28.815254 139787629491968 logging_writer.py:48] [523300] global_step=523300, grad_norm=4.4171833992004395, loss=0.6237085461616516
I0308 01:23:02.369475 139787621099264 logging_writer.py:48] [523400] global_step=523400, grad_norm=4.2729291915893555, loss=0.565904974937439
I0308 01:23:35.993729 139787629491968 logging_writer.py:48] [523500] global_step=523500, grad_norm=4.516043186187744, loss=0.6521715521812439
I0308 01:24:09.644706 139787621099264 logging_writer.py:48] [523600] global_step=523600, grad_norm=4.405228137969971, loss=0.5546339154243469
I0308 01:24:43.261916 139787629491968 logging_writer.py:48] [523700] global_step=523700, grad_norm=4.515775680541992, loss=0.7022044658660889
I0308 01:25:16.902339 139787621099264 logging_writer.py:48] [523800] global_step=523800, grad_norm=4.45746374130249, loss=0.5882828235626221
I0308 01:25:50.521333 139787629491968 logging_writer.py:48] [523900] global_step=523900, grad_norm=4.349987983703613, loss=0.5947120785713196
I0308 01:26:24.190324 139787621099264 logging_writer.py:48] [524000] global_step=524000, grad_norm=4.322576999664307, loss=0.594980001449585
I0308 01:26:57.831140 139787629491968 logging_writer.py:48] [524100] global_step=524100, grad_norm=4.421861171722412, loss=0.6619006395339966
I0308 01:27:31.493847 139787621099264 logging_writer.py:48] [524200] global_step=524200, grad_norm=4.3782243728637695, loss=0.6442657709121704
I0308 01:28:05.302441 139787629491968 logging_writer.py:48] [524300] global_step=524300, grad_norm=4.973581314086914, loss=0.6119195222854614
I0308 01:28:38.959619 139787621099264 logging_writer.py:48] [524400] global_step=524400, grad_norm=4.799365997314453, loss=0.6557407975196838
I0308 01:29:12.621966 139787629491968 logging_writer.py:48] [524500] global_step=524500, grad_norm=4.488023281097412, loss=0.5986149311065674
I0308 01:29:46.271392 139787621099264 logging_writer.py:48] [524600] global_step=524600, grad_norm=4.138200759887695, loss=0.5826641321182251
I0308 01:30:19.912509 139787629491968 logging_writer.py:48] [524700] global_step=524700, grad_norm=4.348143100738525, loss=0.5818241834640503
I0308 01:30:28.814684 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:30:34.804560 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:30:43.283916 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:30:45.565444 139951089751872 submission_runner.py:411] Time since start: 182648.78s, 	Step: 524728, 	{'train/accuracy': 0.9594427347183228, 'train/loss': 0.1490860879421234, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.0410236120224, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.823151707649231, 'test/num_examples': 10000, 'score': 176521.65455007553, 'total_duration': 182648.7777543068, 'accumulated_submission_time': 176521.65455007553, 'accumulated_eval_time': 6079.59999704361, 'accumulated_logging_time': 28.313864946365356}
I0308 01:30:45.669701 139789021992704 logging_writer.py:48] [524728] accumulated_eval_time=6079.599997, accumulated_logging_time=28.313865, accumulated_submission_time=176521.654550, global_step=524728, preemption_count=0, score=176521.654550, test/accuracy=0.631500, test/loss=1.823152, test/num_examples=10000, total_duration=182648.777754, train/accuracy=0.959443, train/loss=0.149086, validation/accuracy=0.757360, validation/loss=1.041024, validation/num_examples=50000
I0308 01:31:10.192416 139789030385408 logging_writer.py:48] [524800] global_step=524800, grad_norm=4.627499103546143, loss=0.566096305847168
I0308 01:31:43.777920 139789021992704 logging_writer.py:48] [524900] global_step=524900, grad_norm=4.596540927886963, loss=0.7086365222930908
I0308 01:32:17.390598 139789030385408 logging_writer.py:48] [525000] global_step=525000, grad_norm=4.481680870056152, loss=0.6545171737670898
I0308 01:32:51.002626 139789021992704 logging_writer.py:48] [525100] global_step=525100, grad_norm=4.568586826324463, loss=0.6055665612220764
I0308 01:33:24.681084 139789030385408 logging_writer.py:48] [525200] global_step=525200, grad_norm=4.6774797439575195, loss=0.6240761280059814
I0308 01:33:58.375500 139789021992704 logging_writer.py:48] [525300] global_step=525300, grad_norm=4.4363532066345215, loss=0.6282403469085693
I0308 01:34:31.965251 139789030385408 logging_writer.py:48] [525400] global_step=525400, grad_norm=4.4079132080078125, loss=0.617497444152832
I0308 01:35:05.542845 139789021992704 logging_writer.py:48] [525500] global_step=525500, grad_norm=4.698415756225586, loss=0.666814386844635
I0308 01:35:39.147104 139789030385408 logging_writer.py:48] [525600] global_step=525600, grad_norm=4.19728422164917, loss=0.6254807114601135
I0308 01:36:12.798295 139789021992704 logging_writer.py:48] [525700] global_step=525700, grad_norm=4.2445149421691895, loss=0.5542536973953247
I0308 01:36:46.467742 139789030385408 logging_writer.py:48] [525800] global_step=525800, grad_norm=4.238588333129883, loss=0.6253218650817871
I0308 01:37:20.126312 139789021992704 logging_writer.py:48] [525900] global_step=525900, grad_norm=4.587581157684326, loss=0.6255306005477905
I0308 01:37:53.769204 139789030385408 logging_writer.py:48] [526000] global_step=526000, grad_norm=4.928351879119873, loss=0.6552194952964783
I0308 01:38:27.422609 139789021992704 logging_writer.py:48] [526100] global_step=526100, grad_norm=4.817805767059326, loss=0.6892064213752747
I0308 01:39:01.044571 139789030385408 logging_writer.py:48] [526200] global_step=526200, grad_norm=4.3629350662231445, loss=0.560858428478241
I0308 01:39:15.652970 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:39:21.724313 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:39:30.243159 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:39:32.511396 139951089751872 submission_runner.py:411] Time since start: 183175.72s, 	Step: 526245, 	{'train/accuracy': 0.9598214030265808, 'train/loss': 0.14878027141094208, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.0420541763305664, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8240805864334106, 'test/num_examples': 10000, 'score': 177031.57284379005, 'total_duration': 183175.72368884087, 'accumulated_submission_time': 177031.57284379005, 'accumulated_eval_time': 6096.458354473114, 'accumulated_logging_time': 28.42765522003174}
I0308 01:39:32.616518 139787621099264 logging_writer.py:48] [526245] accumulated_eval_time=6096.458354, accumulated_logging_time=28.427655, accumulated_submission_time=177031.572844, global_step=526245, preemption_count=0, score=177031.572844, test/accuracy=0.630800, test/loss=1.824081, test/num_examples=10000, total_duration=183175.723689, train/accuracy=0.959821, train/loss=0.148780, validation/accuracy=0.757080, validation/loss=1.042054, validation/num_examples=50000
I0308 01:39:51.493471 139787629491968 logging_writer.py:48] [526300] global_step=526300, grad_norm=5.000080585479736, loss=0.737066924571991
I0308 01:40:25.294768 139787621099264 logging_writer.py:48] [526400] global_step=526400, grad_norm=4.74008846282959, loss=0.6638476252555847
I0308 01:40:58.949450 139787629491968 logging_writer.py:48] [526500] global_step=526500, grad_norm=4.452532768249512, loss=0.5602909922599792
I0308 01:41:32.583492 139787621099264 logging_writer.py:48] [526600] global_step=526600, grad_norm=4.573151588439941, loss=0.6008257865905762
I0308 01:42:06.225624 139787629491968 logging_writer.py:48] [526700] global_step=526700, grad_norm=4.039990425109863, loss=0.5118964910507202
I0308 01:42:39.865634 139787621099264 logging_writer.py:48] [526800] global_step=526800, grad_norm=4.580624103546143, loss=0.6570446491241455
I0308 01:43:13.504599 139787629491968 logging_writer.py:48] [526900] global_step=526900, grad_norm=4.753923416137695, loss=0.6070222854614258
I0308 01:43:47.118594 139787621099264 logging_writer.py:48] [527000] global_step=527000, grad_norm=4.231598854064941, loss=0.6304872632026672
I0308 01:44:20.729610 139787629491968 logging_writer.py:48] [527100] global_step=527100, grad_norm=4.6778998374938965, loss=0.6292994022369385
I0308 01:44:54.346911 139787621099264 logging_writer.py:48] [527200] global_step=527200, grad_norm=4.8588080406188965, loss=0.6905422806739807
I0308 01:45:27.993066 139787629491968 logging_writer.py:48] [527300] global_step=527300, grad_norm=4.486664295196533, loss=0.6076126098632812
I0308 01:46:01.691612 139787621099264 logging_writer.py:48] [527400] global_step=527400, grad_norm=4.4185028076171875, loss=0.5882697105407715
I0308 01:46:35.282350 139787629491968 logging_writer.py:48] [527500] global_step=527500, grad_norm=4.675534725189209, loss=0.6437011957168579
I0308 01:47:08.960805 139787621099264 logging_writer.py:48] [527600] global_step=527600, grad_norm=4.173679828643799, loss=0.6061259508132935
I0308 01:47:42.648766 139787629491968 logging_writer.py:48] [527700] global_step=527700, grad_norm=4.756875038146973, loss=0.6417563557624817
I0308 01:48:02.607643 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:48:08.729216 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:48:17.156682 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:48:19.419484 139951089751872 submission_runner.py:411] Time since start: 183702.63s, 	Step: 527761, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.14456722140312195, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0416818857192993, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8236315250396729, 'test/num_examples': 10000, 'score': 177541.49831581116, 'total_duration': 183702.63179159164, 'accumulated_submission_time': 177541.49831581116, 'accumulated_eval_time': 6113.270159482956, 'accumulated_logging_time': 28.543458938598633}
I0308 01:48:19.523394 139789013600000 logging_writer.py:48] [527761] accumulated_eval_time=6113.270159, accumulated_logging_time=28.543459, accumulated_submission_time=177541.498316, global_step=527761, preemption_count=0, score=177541.498316, test/accuracy=0.632200, test/loss=1.823632, test/num_examples=10000, total_duration=183702.631792, train/accuracy=0.960559, train/loss=0.144567, validation/accuracy=0.757260, validation/loss=1.041682, validation/num_examples=50000
I0308 01:48:32.959545 139789021992704 logging_writer.py:48] [527800] global_step=527800, grad_norm=4.538869857788086, loss=0.6063289642333984
I0308 01:49:06.629697 139789013600000 logging_writer.py:48] [527900] global_step=527900, grad_norm=4.4619526863098145, loss=0.5345045924186707
I0308 01:49:40.286094 139789021992704 logging_writer.py:48] [528000] global_step=528000, grad_norm=4.262069225311279, loss=0.6248149871826172
I0308 01:50:13.933978 139789013600000 logging_writer.py:48] [528100] global_step=528100, grad_norm=4.501321792602539, loss=0.5732518434524536
I0308 01:50:47.578481 139789021992704 logging_writer.py:48] [528200] global_step=528200, grad_norm=4.5168843269348145, loss=0.560313880443573
I0308 01:51:21.200680 139789013600000 logging_writer.py:48] [528300] global_step=528300, grad_norm=4.3903374671936035, loss=0.5454769134521484
I0308 01:51:54.859542 139789021992704 logging_writer.py:48] [528400] global_step=528400, grad_norm=4.288635730743408, loss=0.5566823482513428
I0308 01:52:28.798882 139789013600000 logging_writer.py:48] [528500] global_step=528500, grad_norm=4.373012065887451, loss=0.6079614758491516
I0308 01:53:02.466623 139789021992704 logging_writer.py:48] [528600] global_step=528600, grad_norm=4.580709457397461, loss=0.5562581419944763
I0308 01:53:36.072669 139789013600000 logging_writer.py:48] [528700] global_step=528700, grad_norm=4.614772319793701, loss=0.6302788257598877
I0308 01:54:09.743413 139789021992704 logging_writer.py:48] [528800] global_step=528800, grad_norm=4.546003341674805, loss=0.5961243510246277
I0308 01:54:43.340634 139789013600000 logging_writer.py:48] [528900] global_step=528900, grad_norm=4.991108417510986, loss=0.6700485944747925
I0308 01:55:17.010583 139789021992704 logging_writer.py:48] [529000] global_step=529000, grad_norm=4.732788562774658, loss=0.5855820178985596
I0308 01:55:50.613055 139789013600000 logging_writer.py:48] [529100] global_step=529100, grad_norm=4.674400329589844, loss=0.6273462772369385
I0308 01:56:24.264574 139789021992704 logging_writer.py:48] [529200] global_step=529200, grad_norm=4.489236831665039, loss=0.6054303646087646
I0308 01:56:49.629019 139951089751872 spec.py:321] Evaluating on the training split.
I0308 01:56:55.680561 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 01:57:04.182061 139951089751872 spec.py:349] Evaluating on the test split.
I0308 01:57:06.507230 139951089751872 submission_runner.py:411] Time since start: 184229.72s, 	Step: 529277, 	{'train/accuracy': 0.9598811864852905, 'train/loss': 0.14850091934204102, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0404560565948486, 'validation/num_examples': 50000, 'test/accuracy': 0.6322000026702881, 'test/loss': 1.8213852643966675, 'test/num_examples': 10000, 'score': 178051.53967666626, 'total_duration': 184229.71953058243, 'accumulated_submission_time': 178051.53967666626, 'accumulated_eval_time': 6130.148310184479, 'accumulated_logging_time': 28.656593799591064}
I0308 01:57:06.612477 139787621099264 logging_writer.py:48] [529277] accumulated_eval_time=6130.148310, accumulated_logging_time=28.656594, accumulated_submission_time=178051.539677, global_step=529277, preemption_count=0, score=178051.539677, test/accuracy=0.632200, test/loss=1.821385, test/num_examples=10000, total_duration=184229.719531, train/accuracy=0.959881, train/loss=0.148501, validation/accuracy=0.757280, validation/loss=1.040456, validation/num_examples=50000
I0308 01:57:14.695431 139787629491968 logging_writer.py:48] [529300] global_step=529300, grad_norm=4.510924339294434, loss=0.6436626315116882
I0308 01:57:48.320563 139787621099264 logging_writer.py:48] [529400] global_step=529400, grad_norm=4.0010480880737305, loss=0.6117358207702637
I0308 01:58:21.978642 139787629491968 logging_writer.py:48] [529500] global_step=529500, grad_norm=4.1337995529174805, loss=0.5979681015014648
I0308 01:58:55.557399 139787621099264 logging_writer.py:48] [529600] global_step=529600, grad_norm=4.873115062713623, loss=0.6177263855934143
I0308 01:59:29.165636 139787629491968 logging_writer.py:48] [529700] global_step=529700, grad_norm=4.9773478507995605, loss=0.6149608492851257
I0308 02:00:02.815220 139787621099264 logging_writer.py:48] [529800] global_step=529800, grad_norm=4.203293800354004, loss=0.5994621515274048
I0308 02:00:36.445564 139787629491968 logging_writer.py:48] [529900] global_step=529900, grad_norm=4.426455497741699, loss=0.6077686548233032
I0308 02:01:10.101374 139787621099264 logging_writer.py:48] [530000] global_step=530000, grad_norm=4.577376842498779, loss=0.5989924669265747
I0308 02:01:43.726790 139787629491968 logging_writer.py:48] [530100] global_step=530100, grad_norm=4.03647518157959, loss=0.5161999464035034
I0308 02:02:17.372460 139787621099264 logging_writer.py:48] [530200] global_step=530200, grad_norm=4.587692737579346, loss=0.6186931133270264
I0308 02:02:51.008201 139787629491968 logging_writer.py:48] [530300] global_step=530300, grad_norm=4.406162738800049, loss=0.5826008319854736
I0308 02:03:24.643014 139787621099264 logging_writer.py:48] [530400] global_step=530400, grad_norm=4.669396877288818, loss=0.6397038102149963
I0308 02:03:58.286734 139787629491968 logging_writer.py:48] [530500] global_step=530500, grad_norm=4.19436502456665, loss=0.6039209365844727
I0308 02:04:31.970674 139787621099264 logging_writer.py:48] [530600] global_step=530600, grad_norm=4.807929039001465, loss=0.5974988341331482
I0308 02:05:05.542899 139787629491968 logging_writer.py:48] [530700] global_step=530700, grad_norm=4.668173789978027, loss=0.6228410005569458
I0308 02:05:36.603140 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:05:42.653935 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:05:51.144785 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:05:53.431940 139951089751872 submission_runner.py:411] Time since start: 184756.64s, 	Step: 530794, 	{'train/accuracy': 0.9596819281578064, 'train/loss': 0.1456088274717331, 'validation/accuracy': 0.7568999528884888, 'validation/loss': 1.0421922206878662, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.823911190032959, 'test/num_examples': 10000, 'score': 178561.46390724182, 'total_duration': 184756.6442449093, 'accumulated_submission_time': 178561.46390724182, 'accumulated_eval_time': 6146.977098941803, 'accumulated_logging_time': 28.77240538597107}
I0308 02:05:53.535316 139787612706560 logging_writer.py:48] [530794] accumulated_eval_time=6146.977099, accumulated_logging_time=28.772405, accumulated_submission_time=178561.463907, global_step=530794, preemption_count=0, score=178561.463907, test/accuracy=0.631900, test/loss=1.823911, test/num_examples=10000, total_duration=184756.644245, train/accuracy=0.959682, train/loss=0.145609, validation/accuracy=0.756900, validation/loss=1.042192, validation/num_examples=50000
I0308 02:05:55.896724 139789013600000 logging_writer.py:48] [530800] global_step=530800, grad_norm=4.464582443237305, loss=0.6495275497436523
I0308 02:06:29.525128 139787612706560 logging_writer.py:48] [530900] global_step=530900, grad_norm=4.711764812469482, loss=0.7080450057983398
I0308 02:07:03.211462 139789013600000 logging_writer.py:48] [531000] global_step=531000, grad_norm=4.778350353240967, loss=0.6781656742095947
I0308 02:07:36.880646 139787612706560 logging_writer.py:48] [531100] global_step=531100, grad_norm=4.329938888549805, loss=0.6178238391876221
I0308 02:08:10.504418 139789013600000 logging_writer.py:48] [531200] global_step=531200, grad_norm=4.607605457305908, loss=0.6603966355323792
I0308 02:08:44.157647 139787612706560 logging_writer.py:48] [531300] global_step=531300, grad_norm=4.74557638168335, loss=0.5866878032684326
I0308 02:09:17.773018 139789013600000 logging_writer.py:48] [531400] global_step=531400, grad_norm=4.918130397796631, loss=0.6749513745307922
I0308 02:09:51.428104 139787612706560 logging_writer.py:48] [531500] global_step=531500, grad_norm=4.802231311798096, loss=0.6140304803848267
I0308 02:10:25.042644 139789013600000 logging_writer.py:48] [531600] global_step=531600, grad_norm=4.248728275299072, loss=0.5999130010604858
I0308 02:10:58.695737 139787612706560 logging_writer.py:48] [531700] global_step=531700, grad_norm=4.63701868057251, loss=0.6293664574623108
I0308 02:11:32.346652 139789013600000 logging_writer.py:48] [531800] global_step=531800, grad_norm=4.463498115539551, loss=0.6250757575035095
I0308 02:12:06.046028 139787612706560 logging_writer.py:48] [531900] global_step=531900, grad_norm=4.526907920837402, loss=0.6724346876144409
I0308 02:12:39.711469 139789013600000 logging_writer.py:48] [532000] global_step=532000, grad_norm=4.227851390838623, loss=0.6400176286697388
I0308 02:13:13.404660 139787612706560 logging_writer.py:48] [532100] global_step=532100, grad_norm=4.146103858947754, loss=0.5971294641494751
I0308 02:13:47.043127 139789013600000 logging_writer.py:48] [532200] global_step=532200, grad_norm=4.579362392425537, loss=0.6808755397796631
I0308 02:14:20.713154 139787612706560 logging_writer.py:48] [532300] global_step=532300, grad_norm=4.595669269561768, loss=0.6135302782058716
I0308 02:14:23.546141 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:14:29.563606 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:14:38.027740 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:14:40.301649 139951089751872 submission_runner.py:411] Time since start: 185283.51s, 	Step: 532310, 	{'train/accuracy': 0.9608976244926453, 'train/loss': 0.147421196103096, 'validation/accuracy': 0.7572000026702881, 'validation/loss': 1.0403904914855957, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8210970163345337, 'test/num_examples': 10000, 'score': 179071.4066681862, 'total_duration': 185283.5139591694, 'accumulated_submission_time': 179071.4066681862, 'accumulated_eval_time': 6163.73255443573, 'accumulated_logging_time': 28.88776683807373}
I0308 02:14:40.405432 139789005207296 logging_writer.py:48] [532310] accumulated_eval_time=6163.732554, accumulated_logging_time=28.887767, accumulated_submission_time=179071.406668, global_step=532310, preemption_count=0, score=179071.406668, test/accuracy=0.631700, test/loss=1.821097, test/num_examples=10000, total_duration=185283.513959, train/accuracy=0.960898, train/loss=0.147421, validation/accuracy=0.757200, validation/loss=1.040390, validation/num_examples=50000
I0308 02:15:10.978286 139789021992704 logging_writer.py:48] [532400] global_step=532400, grad_norm=4.346622943878174, loss=0.5822042226791382
I0308 02:15:44.550516 139789005207296 logging_writer.py:48] [532500] global_step=532500, grad_norm=4.606500148773193, loss=0.6428834199905396
I0308 02:16:18.162216 139789021992704 logging_writer.py:48] [532600] global_step=532600, grad_norm=4.313154697418213, loss=0.5930486917495728
I0308 02:16:51.854748 139789005207296 logging_writer.py:48] [532700] global_step=532700, grad_norm=4.23825216293335, loss=0.5675956606864929
I0308 02:17:25.495617 139789021992704 logging_writer.py:48] [532800] global_step=532800, grad_norm=5.433629035949707, loss=0.6819585561752319
I0308 02:17:59.131894 139789005207296 logging_writer.py:48] [532900] global_step=532900, grad_norm=5.166222095489502, loss=0.7049211859703064
I0308 02:18:32.768645 139789021992704 logging_writer.py:48] [533000] global_step=533000, grad_norm=4.138348579406738, loss=0.5668063759803772
I0308 02:19:06.393252 139789005207296 logging_writer.py:48] [533100] global_step=533100, grad_norm=4.629847049713135, loss=0.636383056640625
I0308 02:19:40.008821 139789021992704 logging_writer.py:48] [533200] global_step=533200, grad_norm=4.490488529205322, loss=0.6649386882781982
I0308 02:20:13.650486 139789005207296 logging_writer.py:48] [533300] global_step=533300, grad_norm=4.456908226013184, loss=0.6841643452644348
I0308 02:20:47.288647 139789021992704 logging_writer.py:48] [533400] global_step=533400, grad_norm=4.730550289154053, loss=0.6749908924102783
I0308 02:21:20.912537 139789005207296 logging_writer.py:48] [533500] global_step=533500, grad_norm=4.189425945281982, loss=0.5758272409439087
I0308 02:21:54.562582 139789021992704 logging_writer.py:48] [533600] global_step=533600, grad_norm=4.708793640136719, loss=0.5976942777633667
I0308 02:22:28.199391 139789005207296 logging_writer.py:48] [533700] global_step=533700, grad_norm=4.608577728271484, loss=0.6111351251602173
I0308 02:23:01.930996 139789021992704 logging_writer.py:48] [533800] global_step=533800, grad_norm=4.130171298980713, loss=0.6037789583206177
I0308 02:23:10.502974 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:23:16.510803 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:23:25.121703 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:23:27.421210 139951089751872 submission_runner.py:411] Time since start: 185810.63s, 	Step: 533827, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14589731395244598, 'validation/accuracy': 0.7568999528884888, 'validation/loss': 1.0407488346099854, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8223789930343628, 'test/num_examples': 10000, 'score': 179581.4375398159, 'total_duration': 185810.63350367546, 'accumulated_submission_time': 179581.4375398159, 'accumulated_eval_time': 6180.6507222652435, 'accumulated_logging_time': 29.001569032669067}
I0308 02:23:27.536636 139787629491968 logging_writer.py:48] [533827] accumulated_eval_time=6180.650722, accumulated_logging_time=29.001569, accumulated_submission_time=179581.437540, global_step=533827, preemption_count=0, score=179581.437540, test/accuracy=0.632000, test/loss=1.822379, test/num_examples=10000, total_duration=185810.633504, train/accuracy=0.961276, train/loss=0.145897, validation/accuracy=0.756900, validation/loss=1.040749, validation/num_examples=50000
I0308 02:23:52.403052 139788996814592 logging_writer.py:48] [533900] global_step=533900, grad_norm=4.9135661125183105, loss=0.6859711408615112
I0308 02:24:26.066938 139787629491968 logging_writer.py:48] [534000] global_step=534000, grad_norm=4.625044345855713, loss=0.6234149932861328
I0308 02:24:59.708415 139788996814592 logging_writer.py:48] [534100] global_step=534100, grad_norm=4.35445499420166, loss=0.5983523726463318
I0308 02:25:33.388675 139787629491968 logging_writer.py:48] [534200] global_step=534200, grad_norm=4.9644775390625, loss=0.6213881969451904
I0308 02:26:06.995066 139788996814592 logging_writer.py:48] [534300] global_step=534300, grad_norm=4.3955278396606445, loss=0.5730904340744019
I0308 02:26:40.652456 139787629491968 logging_writer.py:48] [534400] global_step=534400, grad_norm=4.340622901916504, loss=0.5852921605110168
I0308 02:27:14.255638 139788996814592 logging_writer.py:48] [534500] global_step=534500, grad_norm=4.665254592895508, loss=0.63824063539505
I0308 02:27:47.905877 139787629491968 logging_writer.py:48] [534600] global_step=534600, grad_norm=4.969385147094727, loss=0.7231390476226807
I0308 02:28:21.516004 139788996814592 logging_writer.py:48] [534700] global_step=534700, grad_norm=4.6816229820251465, loss=0.6795245409011841
I0308 02:28:55.190105 139787629491968 logging_writer.py:48] [534800] global_step=534800, grad_norm=4.477620601654053, loss=0.63215172290802
I0308 02:29:28.803997 139788996814592 logging_writer.py:48] [534900] global_step=534900, grad_norm=4.512641429901123, loss=0.6356917023658752
I0308 02:30:02.471872 139787629491968 logging_writer.py:48] [535000] global_step=535000, grad_norm=4.542858123779297, loss=0.6399853229522705
I0308 02:30:36.114811 139788996814592 logging_writer.py:48] [535100] global_step=535100, grad_norm=4.458942890167236, loss=0.65919429063797
I0308 02:31:09.806150 139787629491968 logging_writer.py:48] [535200] global_step=535200, grad_norm=4.726795196533203, loss=0.63216233253479
I0308 02:31:43.436886 139788996814592 logging_writer.py:48] [535300] global_step=535300, grad_norm=5.007966041564941, loss=0.6550851464271545
I0308 02:31:57.714181 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:32:03.687902 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:32:12.150860 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:32:14.437439 139951089751872 submission_runner.py:411] Time since start: 186337.65s, 	Step: 535344, 	{'train/accuracy': 0.9600605964660645, 'train/loss': 0.14738938212394714, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.040579080581665, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8225830793380737, 'test/num_examples': 10000, 'score': 180091.54822564125, 'total_duration': 186337.6497502327, 'accumulated_submission_time': 180091.54822564125, 'accumulated_eval_time': 6197.373930454254, 'accumulated_logging_time': 29.126783847808838}
I0308 02:32:14.540633 139787612706560 logging_writer.py:48] [535344] accumulated_eval_time=6197.373930, accumulated_logging_time=29.126784, accumulated_submission_time=180091.548226, global_step=535344, preemption_count=0, score=180091.548226, test/accuracy=0.631000, test/loss=1.822583, test/num_examples=10000, total_duration=186337.649750, train/accuracy=0.960061, train/loss=0.147389, validation/accuracy=0.756880, validation/loss=1.040579, validation/num_examples=50000
I0308 02:32:33.690989 139789013600000 logging_writer.py:48] [535400] global_step=535400, grad_norm=4.625113010406494, loss=0.6199607849121094
I0308 02:33:07.326016 139787612706560 logging_writer.py:48] [535500] global_step=535500, grad_norm=4.570626735687256, loss=0.6241496801376343
I0308 02:33:40.966429 139789013600000 logging_writer.py:48] [535600] global_step=535600, grad_norm=4.5515265464782715, loss=0.7023763060569763
I0308 02:34:14.638219 139787612706560 logging_writer.py:48] [535700] global_step=535700, grad_norm=4.490622043609619, loss=0.648647129535675
I0308 02:34:48.303229 139789013600000 logging_writer.py:48] [535800] global_step=535800, grad_norm=4.30303430557251, loss=0.6184351444244385
I0308 02:35:21.963615 139787612706560 logging_writer.py:48] [535900] global_step=535900, grad_norm=4.636393070220947, loss=0.6691707968711853
I0308 02:35:55.513353 139789013600000 logging_writer.py:48] [536000] global_step=536000, grad_norm=4.238076210021973, loss=0.6389089226722717
I0308 02:36:29.139356 139787612706560 logging_writer.py:48] [536100] global_step=536100, grad_norm=4.44681453704834, loss=0.6138074398040771
I0308 02:37:02.790211 139789013600000 logging_writer.py:48] [536200] global_step=536200, grad_norm=4.4613800048828125, loss=0.5943760871887207
I0308 02:37:36.446774 139787612706560 logging_writer.py:48] [536300] global_step=536300, grad_norm=4.163719177246094, loss=0.5804186463356018
I0308 02:38:10.091701 139789013600000 logging_writer.py:48] [536400] global_step=536400, grad_norm=4.407895088195801, loss=0.5883350372314453
I0308 02:38:43.753563 139787612706560 logging_writer.py:48] [536500] global_step=536500, grad_norm=4.445011615753174, loss=0.7035824060440063
I0308 02:39:17.388517 139789013600000 logging_writer.py:48] [536600] global_step=536600, grad_norm=4.8590989112854, loss=0.6601654291152954
I0308 02:39:51.020790 139787612706560 logging_writer.py:48] [536700] global_step=536700, grad_norm=4.691128253936768, loss=0.65286785364151
I0308 02:40:24.672036 139789013600000 logging_writer.py:48] [536800] global_step=536800, grad_norm=4.53830623626709, loss=0.6318069100379944
I0308 02:40:44.661537 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:40:50.656313 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:40:59.236266 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:41:01.504497 139951089751872 submission_runner.py:411] Time since start: 186864.72s, 	Step: 536861, 	{'train/accuracy': 0.9612762928009033, 'train/loss': 0.14305651187896729, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.0416611433029175, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.823699712753296, 'test/num_examples': 10000, 'score': 180601.60165405273, 'total_duration': 186864.71676635742, 'accumulated_submission_time': 180601.60165405273, 'accumulated_eval_time': 6214.216796398163, 'accumulated_logging_time': 29.241114854812622}
I0308 02:41:01.612143 139789005207296 logging_writer.py:48] [536861] accumulated_eval_time=6214.216796, accumulated_logging_time=29.241115, accumulated_submission_time=180601.601654, global_step=536861, preemption_count=0, score=180601.601654, test/accuracy=0.631400, test/loss=1.823700, test/num_examples=10000, total_duration=186864.716766, train/accuracy=0.961276, train/loss=0.143057, validation/accuracy=0.757100, validation/loss=1.041661, validation/num_examples=50000
I0308 02:41:15.144501 139789030385408 logging_writer.py:48] [536900] global_step=536900, grad_norm=4.858471870422363, loss=0.5905570387840271
I0308 02:41:48.730308 139789005207296 logging_writer.py:48] [537000] global_step=537000, grad_norm=4.571664333343506, loss=0.632175624370575
I0308 02:42:22.321457 139789030385408 logging_writer.py:48] [537100] global_step=537100, grad_norm=5.061956882476807, loss=0.6052468419075012
I0308 02:42:55.914757 139789005207296 logging_writer.py:48] [537200] global_step=537200, grad_norm=4.822976112365723, loss=0.6134722232818604
I0308 02:43:29.500526 139789030385408 logging_writer.py:48] [537300] global_step=537300, grad_norm=3.8919272422790527, loss=0.49235695600509644
I0308 02:44:03.091593 139789005207296 logging_writer.py:48] [537400] global_step=537400, grad_norm=4.639467716217041, loss=0.6755496263504028
I0308 02:44:36.747766 139789030385408 logging_writer.py:48] [537500] global_step=537500, grad_norm=4.378231525421143, loss=0.59938645362854
I0308 02:45:10.345386 139789005207296 logging_writer.py:48] [537600] global_step=537600, grad_norm=4.068162441253662, loss=0.5106712579727173
I0308 02:45:44.014455 139789030385408 logging_writer.py:48] [537700] global_step=537700, grad_norm=4.273539066314697, loss=0.5568058490753174
I0308 02:46:17.669919 139789005207296 logging_writer.py:48] [537800] global_step=537800, grad_norm=4.391615867614746, loss=0.6202012300491333
I0308 02:46:51.332280 139789030385408 logging_writer.py:48] [537900] global_step=537900, grad_norm=4.681354522705078, loss=0.6665862798690796
I0308 02:47:24.977309 139789005207296 logging_writer.py:48] [538000] global_step=538000, grad_norm=4.260550498962402, loss=0.6220511198043823
I0308 02:47:58.624307 139789030385408 logging_writer.py:48] [538100] global_step=538100, grad_norm=4.1268792152404785, loss=0.5613503456115723
I0308 02:48:32.271452 139789005207296 logging_writer.py:48] [538200] global_step=538200, grad_norm=4.718135356903076, loss=0.5998622179031372
I0308 02:49:05.915938 139789030385408 logging_writer.py:48] [538300] global_step=538300, grad_norm=4.431286811828613, loss=0.6436138153076172
I0308 02:49:31.637319 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:49:37.793962 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:49:46.304780 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:49:48.554385 139951089751872 submission_runner.py:411] Time since start: 187391.77s, 	Step: 538378, 	{'train/accuracy': 0.9622727632522583, 'train/loss': 0.1430283784866333, 'validation/accuracy': 0.7569199800491333, 'validation/loss': 1.041762351989746, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.822485089302063, 'test/num_examples': 10000, 'score': 181111.55942821503, 'total_duration': 187391.7662270069, 'accumulated_submission_time': 181111.55942821503, 'accumulated_eval_time': 6231.133362054825, 'accumulated_logging_time': 29.359850883483887}
I0308 02:49:48.665404 139787612706560 logging_writer.py:48] [538378] accumulated_eval_time=6231.133362, accumulated_logging_time=29.359851, accumulated_submission_time=181111.559428, global_step=538378, preemption_count=0, score=181111.559428, test/accuracy=0.632000, test/loss=1.822485, test/num_examples=10000, total_duration=187391.766227, train/accuracy=0.962273, train/loss=0.143028, validation/accuracy=0.756920, validation/loss=1.041762, validation/num_examples=50000
I0308 02:49:56.409981 139787621099264 logging_writer.py:48] [538400] global_step=538400, grad_norm=4.344235897064209, loss=0.5595678091049194
I0308 02:50:30.051345 139787612706560 logging_writer.py:48] [538500] global_step=538500, grad_norm=4.551647663116455, loss=0.6666655540466309
I0308 02:51:03.698992 139787621099264 logging_writer.py:48] [538600] global_step=538600, grad_norm=4.8133111000061035, loss=0.6726640462875366
I0308 02:51:37.391186 139787612706560 logging_writer.py:48] [538700] global_step=538700, grad_norm=4.118789196014404, loss=0.591171383857727
I0308 02:52:11.086929 139787621099264 logging_writer.py:48] [538800] global_step=538800, grad_norm=5.094172477722168, loss=0.5732378363609314
I0308 02:52:44.719222 139787612706560 logging_writer.py:48] [538900] global_step=538900, grad_norm=4.614553451538086, loss=0.6526822447776794
I0308 02:53:18.476629 139787621099264 logging_writer.py:48] [539000] global_step=539000, grad_norm=4.669123649597168, loss=0.5794917345046997
I0308 02:53:52.064188 139787612706560 logging_writer.py:48] [539100] global_step=539100, grad_norm=4.399351596832275, loss=0.6389404535293579
I0308 02:54:25.649928 139787621099264 logging_writer.py:48] [539200] global_step=539200, grad_norm=4.531144618988037, loss=0.5596606731414795
I0308 02:54:59.205491 139787612706560 logging_writer.py:48] [539300] global_step=539300, grad_norm=4.566982746124268, loss=0.5800929069519043
I0308 02:55:32.788535 139787621099264 logging_writer.py:48] [539400] global_step=539400, grad_norm=4.5294084548950195, loss=0.6347626447677612
I0308 02:56:06.422086 139787612706560 logging_writer.py:48] [539500] global_step=539500, grad_norm=3.894627332687378, loss=0.5292932987213135
I0308 02:56:40.068502 139787621099264 logging_writer.py:48] [539600] global_step=539600, grad_norm=4.466210842132568, loss=0.6349802017211914
I0308 02:57:13.716936 139787612706560 logging_writer.py:48] [539700] global_step=539700, grad_norm=5.475375652313232, loss=0.7501776218414307
I0308 02:57:47.362709 139787621099264 logging_writer.py:48] [539800] global_step=539800, grad_norm=4.331774711608887, loss=0.545322835445404
I0308 02:58:18.804322 139951089751872 spec.py:321] Evaluating on the training split.
I0308 02:58:24.912577 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 02:58:33.774802 139951089751872 spec.py:349] Evaluating on the test split.
I0308 02:58:36.060253 139951089751872 submission_runner.py:411] Time since start: 187919.27s, 	Step: 539895, 	{'train/accuracy': 0.9616350531578064, 'train/loss': 0.14553317427635193, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 1.0420688390731812, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.8241206407546997, 'test/num_examples': 10000, 'score': 181621.6319413185, 'total_duration': 187919.27256298065, 'accumulated_submission_time': 181621.6319413185, 'accumulated_eval_time': 6248.389245271683, 'accumulated_logging_time': 29.480810165405273}
I0308 02:58:36.166182 139787621099264 logging_writer.py:48] [539895] accumulated_eval_time=6248.389245, accumulated_logging_time=29.480810, accumulated_submission_time=181621.631941, global_step=539895, preemption_count=0, score=181621.631941, test/accuracy=0.632000, test/loss=1.824121, test/num_examples=10000, total_duration=187919.272563, train/accuracy=0.961635, train/loss=0.145533, validation/accuracy=0.757400, validation/loss=1.042069, validation/num_examples=50000
I0308 02:58:38.177647 139789005207296 logging_writer.py:48] [539900] global_step=539900, grad_norm=4.802553653717041, loss=0.6588091850280762
I0308 02:59:11.810527 139787621099264 logging_writer.py:48] [540000] global_step=540000, grad_norm=4.711647033691406, loss=0.6596249938011169
I0308 02:59:45.470303 139789005207296 logging_writer.py:48] [540100] global_step=540100, grad_norm=4.501461029052734, loss=0.6823036670684814
I0308 03:00:19.121665 139787621099264 logging_writer.py:48] [540200] global_step=540200, grad_norm=4.6284260749816895, loss=0.6053867340087891
I0308 03:00:52.784051 139789005207296 logging_writer.py:48] [540300] global_step=540300, grad_norm=4.559516429901123, loss=0.5390387773513794
I0308 03:01:26.458257 139787621099264 logging_writer.py:48] [540400] global_step=540400, grad_norm=4.741458892822266, loss=0.6745973825454712
I0308 03:02:00.118888 139789005207296 logging_writer.py:48] [540500] global_step=540500, grad_norm=4.5222015380859375, loss=0.6359946131706238
I0308 03:02:33.808273 139787621099264 logging_writer.py:48] [540600] global_step=540600, grad_norm=4.4196367263793945, loss=0.5477746725082397
I0308 03:03:07.460312 139789005207296 logging_writer.py:48] [540700] global_step=540700, grad_norm=4.340610980987549, loss=0.6289014220237732
I0308 03:03:41.141956 139787621099264 logging_writer.py:48] [540800] global_step=540800, grad_norm=5.092106342315674, loss=0.6028733849525452
I0308 03:04:14.743116 139789005207296 logging_writer.py:48] [540900] global_step=540900, grad_norm=4.522876262664795, loss=0.6078933477401733
I0308 03:04:48.413311 139787621099264 logging_writer.py:48] [541000] global_step=541000, grad_norm=4.186359882354736, loss=0.5428693294525146
I0308 03:05:22.060314 139789005207296 logging_writer.py:48] [541100] global_step=541100, grad_norm=4.126012325286865, loss=0.6017465591430664
I0308 03:05:55.718358 139787621099264 logging_writer.py:48] [541200] global_step=541200, grad_norm=4.569149017333984, loss=0.6090415120124817
I0308 03:06:29.330198 139789005207296 logging_writer.py:48] [541300] global_step=541300, grad_norm=4.672299861907959, loss=0.6674160957336426
I0308 03:07:02.997517 139787621099264 logging_writer.py:48] [541400] global_step=541400, grad_norm=4.206203460693359, loss=0.6088550090789795
I0308 03:07:06.167762 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:07:12.276271 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 03:07:20.730709 139951089751872 spec.py:349] Evaluating on the test split.
I0308 03:07:23.089066 139951089751872 submission_runner.py:411] Time since start: 188446.30s, 	Step: 541411, 	{'train/accuracy': 0.9597815275192261, 'train/loss': 0.1493196040391922, 'validation/accuracy': 0.7568199634552002, 'validation/loss': 1.0412883758544922, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8230903148651123, 'test/num_examples': 10000, 'score': 182131.56746602058, 'total_duration': 188446.30136823654, 'accumulated_submission_time': 182131.56746602058, 'accumulated_eval_time': 6265.310503005981, 'accumulated_logging_time': 29.59652018547058}
I0308 03:07:23.200652 139787621099264 logging_writer.py:48] [541411] accumulated_eval_time=6265.310503, accumulated_logging_time=29.596520, accumulated_submission_time=182131.567466, global_step=541411, preemption_count=0, score=182131.567466, test/accuracy=0.631700, test/loss=1.823090, test/num_examples=10000, total_duration=188446.301368, train/accuracy=0.959782, train/loss=0.149320, validation/accuracy=0.756820, validation/loss=1.041288, validation/num_examples=50000
I0308 03:07:53.414985 139787629491968 logging_writer.py:48] [541500] global_step=541500, grad_norm=4.108680725097656, loss=0.5794583559036255
I0308 03:08:26.991500 139787621099264 logging_writer.py:48] [541600] global_step=541600, grad_norm=4.368342876434326, loss=0.5771114230155945
I0308 03:09:00.621705 139787629491968 logging_writer.py:48] [541700] global_step=541700, grad_norm=4.485523700714111, loss=0.6765558123588562
I0308 03:09:34.258455 139787621099264 logging_writer.py:48] [541800] global_step=541800, grad_norm=4.622488498687744, loss=0.622458279132843
I0308 03:10:07.875467 139787629491968 logging_writer.py:48] [541900] global_step=541900, grad_norm=4.471737861633301, loss=0.6380072832107544
I0308 03:10:41.520315 139787621099264 logging_writer.py:48] [542000] global_step=542000, grad_norm=4.346522331237793, loss=0.6265210509300232
I0308 03:11:15.162479 139787629491968 logging_writer.py:48] [542100] global_step=542100, grad_norm=4.778003215789795, loss=0.7082432508468628
I0308 03:11:48.835729 139787621099264 logging_writer.py:48] [542200] global_step=542200, grad_norm=4.846271514892578, loss=0.6273539066314697
I0308 03:12:22.466739 139787629491968 logging_writer.py:48] [542300] global_step=542300, grad_norm=4.647355079650879, loss=0.6005512475967407
I0308 03:12:56.101480 139787621099264 logging_writer.py:48] [542400] global_step=542400, grad_norm=4.830227375030518, loss=0.5799064040184021
I0308 03:13:29.778557 139787629491968 logging_writer.py:48] [542500] global_step=542500, grad_norm=4.362499237060547, loss=0.6047680974006653
I0308 03:14:03.398504 139787621099264 logging_writer.py:48] [542600] global_step=542600, grad_norm=4.585731029510498, loss=0.6591344475746155
I0308 03:14:37.031750 139787629491968 logging_writer.py:48] [542700] global_step=542700, grad_norm=4.706559658050537, loss=0.5917029976844788
I0308 03:15:10.662409 139787621099264 logging_writer.py:48] [542800] global_step=542800, grad_norm=4.382000923156738, loss=0.5851200222969055
I0308 03:15:44.312753 139787629491968 logging_writer.py:48] [542900] global_step=542900, grad_norm=4.752893924713135, loss=0.6824997663497925
I0308 03:15:53.204319 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:15:59.382730 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 03:16:07.901491 139951089751872 spec.py:349] Evaluating on the test split.
I0308 03:16:10.193140 139951089751872 submission_runner.py:411] Time since start: 188973.41s, 	Step: 542928, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.14479315280914307, 'validation/accuracy': 0.7574999928474426, 'validation/loss': 1.0422248840332031, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8229801654815674, 'test/num_examples': 10000, 'score': 182641.50406050682, 'total_duration': 188973.40545129776, 'accumulated_submission_time': 182641.50406050682, 'accumulated_eval_time': 6282.299289464951, 'accumulated_logging_time': 29.718650102615356}
I0308 03:16:10.288130 139789021992704 logging_writer.py:48] [542928] accumulated_eval_time=6282.299289, accumulated_logging_time=29.718650, accumulated_submission_time=182641.504061, global_step=542928, preemption_count=0, score=182641.504061, test/accuracy=0.631200, test/loss=1.822980, test/num_examples=10000, total_duration=188973.405451, train/accuracy=0.960698, train/loss=0.144793, validation/accuracy=0.757500, validation/loss=1.042225, validation/num_examples=50000
I0308 03:16:34.797635 139789030385408 logging_writer.py:48] [543000] global_step=543000, grad_norm=5.1894731521606445, loss=0.6994009613990784
I0308 03:17:08.358714 139789021992704 logging_writer.py:48] [543100] global_step=543100, grad_norm=4.396161079406738, loss=0.6260748505592346
I0308 03:17:41.955099 139789030385408 logging_writer.py:48] [543200] global_step=543200, grad_norm=4.181575298309326, loss=0.574353814125061
I0308 03:18:15.605774 139789021992704 logging_writer.py:48] [543300] global_step=543300, grad_norm=4.504256248474121, loss=0.6462960243225098
I0308 03:18:49.219662 139789030385408 logging_writer.py:48] [543400] global_step=543400, grad_norm=4.815488815307617, loss=0.6659977436065674
I0308 03:19:22.898210 139789021992704 logging_writer.py:48] [543500] global_step=543500, grad_norm=4.089507102966309, loss=0.5408284068107605
I0308 03:19:56.555612 139789030385408 logging_writer.py:48] [543600] global_step=543600, grad_norm=4.13996696472168, loss=0.5441556572914124
I0308 03:20:30.219259 139789021992704 logging_writer.py:48] [543700] global_step=543700, grad_norm=4.404997825622559, loss=0.6280728578567505
I0308 03:21:03.820616 139789030385408 logging_writer.py:48] [543800] global_step=543800, grad_norm=4.125407695770264, loss=0.5841503143310547
I0308 03:21:37.463613 139789021992704 logging_writer.py:48] [543900] global_step=543900, grad_norm=4.283282279968262, loss=0.5484037399291992
I0308 03:22:11.056655 139789030385408 logging_writer.py:48] [544000] global_step=544000, grad_norm=3.9502358436584473, loss=0.5377315878868103
I0308 03:22:44.705859 139789021992704 logging_writer.py:48] [544100] global_step=544100, grad_norm=3.878220558166504, loss=0.5437605381011963
I0308 03:23:18.323713 139789030385408 logging_writer.py:48] [544200] global_step=544200, grad_norm=4.56419563293457, loss=0.6740392446517944
I0308 03:23:52.001839 139789021992704 logging_writer.py:48] [544300] global_step=544300, grad_norm=4.546462059020996, loss=0.6217017769813538
I0308 03:24:25.678911 139789030385408 logging_writer.py:48] [544400] global_step=544400, grad_norm=4.502359867095947, loss=0.5748434662818909
I0308 03:24:40.280880 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:24:46.304836 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 03:24:54.837002 139951089751872 spec.py:349] Evaluating on the test split.
I0308 03:24:57.098455 139951089751872 submission_runner.py:411] Time since start: 189500.31s, 	Step: 544445, 	{'train/accuracy': 0.961933970451355, 'train/loss': 0.14220908284187317, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 1.0426530838012695, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.824191689491272, 'test/num_examples': 10000, 'score': 183151.4329688549, 'total_duration': 189500.31076526642, 'accumulated_submission_time': 183151.4329688549, 'accumulated_eval_time': 6299.116809844971, 'accumulated_logging_time': 29.8223876953125}
I0308 03:24:57.203768 139787621099264 logging_writer.py:48] [544445] accumulated_eval_time=6299.116810, accumulated_logging_time=29.822388, accumulated_submission_time=183151.432969, global_step=544445, preemption_count=0, score=183151.432969, test/accuracy=0.630700, test/loss=1.824192, test/num_examples=10000, total_duration=189500.310765, train/accuracy=0.961934, train/loss=0.142209, validation/accuracy=0.757360, validation/loss=1.042653, validation/num_examples=50000
I0308 03:25:15.999637 139787629491968 logging_writer.py:48] [544500] global_step=544500, grad_norm=4.470892429351807, loss=0.6202867031097412
I0308 03:25:49.594798 139787621099264 logging_writer.py:48] [544600] global_step=544600, grad_norm=4.651529312133789, loss=0.6102202534675598
I0308 03:26:23.256483 139787629491968 logging_writer.py:48] [544700] global_step=544700, grad_norm=4.568315029144287, loss=0.6106643676757812
I0308 03:26:56.934644 139787621099264 logging_writer.py:48] [544800] global_step=544800, grad_norm=4.825321674346924, loss=0.6765124797821045
I0308 03:27:30.602973 139787629491968 logging_writer.py:48] [544900] global_step=544900, grad_norm=4.16820764541626, loss=0.593477725982666
I0308 03:28:04.252578 139787621099264 logging_writer.py:48] [545000] global_step=545000, grad_norm=4.131231784820557, loss=0.5769633650779724
I0308 03:28:37.870031 139787629491968 logging_writer.py:48] [545100] global_step=545100, grad_norm=4.772548198699951, loss=0.6726145148277283
I0308 03:29:11.518101 139787621099264 logging_writer.py:48] [545200] global_step=545200, grad_norm=4.462390899658203, loss=0.6350610256195068
I0308 03:29:45.154869 139787629491968 logging_writer.py:48] [545300] global_step=545300, grad_norm=4.703427791595459, loss=0.6670656204223633
I0308 03:30:18.803034 139787621099264 logging_writer.py:48] [545400] global_step=545400, grad_norm=4.088858604431152, loss=0.5951038599014282
I0308 03:30:52.423788 139787629491968 logging_writer.py:48] [545500] global_step=545500, grad_norm=4.270020008087158, loss=0.6402784585952759
I0308 03:31:26.032789 139787621099264 logging_writer.py:48] [545600] global_step=545600, grad_norm=4.983808994293213, loss=0.7069957852363586
I0308 03:31:59.709038 139787629491968 logging_writer.py:48] [545700] global_step=545700, grad_norm=4.349068641662598, loss=0.5773056745529175
I0308 03:32:33.346083 139787621099264 logging_writer.py:48] [545800] global_step=545800, grad_norm=4.7453413009643555, loss=0.7251752614974976
I0308 03:33:07.010322 139787629491968 logging_writer.py:48] [545900] global_step=545900, grad_norm=4.317231178283691, loss=0.6047787666320801
I0308 03:33:27.369877 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:33:33.362055 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 03:33:41.967199 139951089751872 spec.py:349] Evaluating on the test split.
I0308 03:33:44.243860 139951089751872 submission_runner.py:411] Time since start: 190027.46s, 	Step: 545962, 	{'train/accuracy': 0.9602798223495483, 'train/loss': 0.14695727825164795, 'validation/accuracy': 0.7570199966430664, 'validation/loss': 1.0411815643310547, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8221770524978638, 'test/num_examples': 10000, 'score': 183661.53258037567, 'total_duration': 190027.45615816116, 'accumulated_submission_time': 183661.53258037567, 'accumulated_eval_time': 6315.990729570389, 'accumulated_logging_time': 29.937760829925537}
I0308 03:33:44.351279 139789030385408 logging_writer.py:48] [545962] accumulated_eval_time=6315.990730, accumulated_logging_time=29.937761, accumulated_submission_time=183661.532580, global_step=545962, preemption_count=0, score=183661.532580, test/accuracy=0.630600, test/loss=1.822177, test/num_examples=10000, total_duration=190027.456158, train/accuracy=0.960280, train/loss=0.146957, validation/accuracy=0.757020, validation/loss=1.041182, validation/num_examples=50000
I0308 03:33:57.495341 139789038778112 logging_writer.py:48] [546000] global_step=546000, grad_norm=5.32988166809082, loss=0.6778884530067444
I0308 03:34:31.130554 139789030385408 logging_writer.py:48] [546100] global_step=546100, grad_norm=4.499754905700684, loss=0.5376265645027161
I0308 03:35:04.791064 139789038778112 logging_writer.py:48] [546200] global_step=546200, grad_norm=4.5571699142456055, loss=0.6187935471534729
I0308 03:35:38.438684 139789030385408 logging_writer.py:48] [546300] global_step=546300, grad_norm=4.522770404815674, loss=0.6269087195396423
I0308 03:36:12.281364 139789038778112 logging_writer.py:48] [546400] global_step=546400, grad_norm=4.220023155212402, loss=0.6598479747772217
I0308 03:36:45.935844 139789030385408 logging_writer.py:48] [546500] global_step=546500, grad_norm=4.670001983642578, loss=0.6243867874145508
I0308 03:37:19.597376 139789038778112 logging_writer.py:48] [546600] global_step=546600, grad_norm=4.665050029754639, loss=0.584355890750885
I0308 03:37:53.211179 139789030385408 logging_writer.py:48] [546700] global_step=546700, grad_norm=4.478855609893799, loss=0.6324581503868103
I0308 03:38:26.871713 139789038778112 logging_writer.py:48] [546800] global_step=546800, grad_norm=4.193747043609619, loss=0.6022758483886719
I0308 03:39:00.482838 139789030385408 logging_writer.py:48] [546900] global_step=546900, grad_norm=4.205326557159424, loss=0.5753386616706848
I0308 03:39:34.155663 139789038778112 logging_writer.py:48] [547000] global_step=547000, grad_norm=4.505578517913818, loss=0.5907342433929443
I0308 03:40:07.770365 139789030385408 logging_writer.py:48] [547100] global_step=547100, grad_norm=4.621849536895752, loss=0.667840838432312
I0308 03:40:41.434510 139789038778112 logging_writer.py:48] [547200] global_step=547200, grad_norm=4.332662582397461, loss=0.574033796787262
I0308 03:41:15.060815 139789030385408 logging_writer.py:48] [547300] global_step=547300, grad_norm=4.342605113983154, loss=0.5824614763259888
I0308 03:41:48.730106 139789038778112 logging_writer.py:48] [547400] global_step=547400, grad_norm=4.897265434265137, loss=0.6595718860626221
I0308 03:42:14.426362 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:42:20.737708 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 03:42:29.353944 139951089751872 spec.py:349] Evaluating on the test split.
I0308 03:42:31.683624 139951089751872 submission_runner.py:411] Time since start: 190554.90s, 	Step: 547478, 	{'train/accuracy': 0.9627909660339355, 'train/loss': 0.14465691149234772, 'validation/accuracy': 0.757099986076355, 'validation/loss': 1.0409207344055176, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8227473497390747, 'test/num_examples': 10000, 'score': 184171.54254198074, 'total_duration': 190554.8959350586, 'accumulated_submission_time': 184171.54254198074, 'accumulated_eval_time': 6333.247943878174, 'accumulated_logging_time': 30.054824352264404}
I0308 03:42:31.790209 139788996814592 logging_writer.py:48] [547478] accumulated_eval_time=6333.247944, accumulated_logging_time=30.054824, accumulated_submission_time=184171.542542, global_step=547478, preemption_count=0, score=184171.542542, test/accuracy=0.631000, test/loss=1.822747, test/num_examples=10000, total_duration=190554.895935, train/accuracy=0.962791, train/loss=0.144657, validation/accuracy=0.757100, validation/loss=1.040921, validation/num_examples=50000
I0308 03:42:39.510790 139789005207296 logging_writer.py:48] [547500] global_step=547500, grad_norm=4.342763900756836, loss=0.617587685585022
I0308 03:43:13.086125 139788996814592 logging_writer.py:48] [547600] global_step=547600, grad_norm=4.3845367431640625, loss=0.676544189453125
I0308 03:43:46.704623 139789005207296 logging_writer.py:48] [547700] global_step=547700, grad_norm=4.956575393676758, loss=0.5999035239219666
I0308 03:44:20.321395 139788996814592 logging_writer.py:48] [547800] global_step=547800, grad_norm=4.769676685333252, loss=0.6858163475990295
I0308 03:44:53.949935 139789005207296 logging_writer.py:48] [547900] global_step=547900, grad_norm=4.47507381439209, loss=0.640200138092041
I0308 03:45:27.597935 139788996814592 logging_writer.py:48] [548000] global_step=548000, grad_norm=4.712407112121582, loss=0.6342633962631226
I0308 03:46:01.279256 139789005207296 logging_writer.py:48] [548100] global_step=548100, grad_norm=4.133857727050781, loss=0.5492492318153381
I0308 03:46:34.915734 139788996814592 logging_writer.py:48] [548200] global_step=548200, grad_norm=4.323113918304443, loss=0.6883460283279419
I0308 03:47:08.576111 139789005207296 logging_writer.py:48] [548300] global_step=548300, grad_norm=4.51263952255249, loss=0.6157893538475037
I0308 03:47:42.185059 139788996814592 logging_writer.py:48] [548400] global_step=548400, grad_norm=4.397800445556641, loss=0.5722479820251465
I0308 03:48:15.819478 139789005207296 logging_writer.py:48] [548500] global_step=548500, grad_norm=4.4229278564453125, loss=0.6046725511550903
I0308 03:48:49.487388 139788996814592 logging_writer.py:48] [548600] global_step=548600, grad_norm=4.58060884475708, loss=0.6331720352172852
I0308 03:49:23.106630 139789005207296 logging_writer.py:48] [548700] global_step=548700, grad_norm=4.21494197845459, loss=0.5718510150909424
I0308 03:49:56.774083 139788996814592 logging_writer.py:48] [548800] global_step=548800, grad_norm=4.626389980316162, loss=0.705305278301239
I0308 03:50:30.449841 139789005207296 logging_writer.py:48] [548900] global_step=548900, grad_norm=4.45276403427124, loss=0.6467263698577881
I0308 03:51:01.848705 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:51:07.885133 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 03:51:16.340832 139951089751872 spec.py:349] Evaluating on the test split.
I0308 03:51:18.646326 139951089751872 submission_runner.py:411] Time since start: 191081.86s, 	Step: 548995, 	{'train/accuracy': 0.9591637253761292, 'train/loss': 0.14917802810668945, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.0409399271011353, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8207401037216187, 'test/num_examples': 10000, 'score': 184681.5361790657, 'total_duration': 191081.85863733292, 'accumulated_submission_time': 184681.5361790657, 'accumulated_eval_time': 6350.045515298843, 'accumulated_logging_time': 30.17049288749695}
I0308 03:51:18.753536 139787612706560 logging_writer.py:48] [548995] accumulated_eval_time=6350.045515, accumulated_logging_time=30.170493, accumulated_submission_time=184681.536179, global_step=548995, preemption_count=0, score=184681.536179, test/accuracy=0.632500, test/loss=1.820740, test/num_examples=10000, total_duration=191081.858637, train/accuracy=0.959164, train/loss=0.149178, validation/accuracy=0.757260, validation/loss=1.040940, validation/num_examples=50000
I0308 03:51:20.771885 139787621099264 logging_writer.py:48] [549000] global_step=549000, grad_norm=4.660858631134033, loss=0.6485400199890137
I0308 03:51:54.350610 139787612706560 logging_writer.py:48] [549100] global_step=549100, grad_norm=4.8331146240234375, loss=0.6877357363700867
I0308 03:52:27.913704 139787621099264 logging_writer.py:48] [549200] global_step=549200, grad_norm=4.134399890899658, loss=0.5794711112976074
I0308 03:53:01.504335 139787612706560 logging_writer.py:48] [549300] global_step=549300, grad_norm=4.108137130737305, loss=0.5808490514755249
I0308 03:53:35.116860 139787621099264 logging_writer.py:48] [549400] global_step=549400, grad_norm=4.410536289215088, loss=0.5857816934585571
I0308 03:54:08.773077 139787612706560 logging_writer.py:48] [549500] global_step=549500, grad_norm=4.353348731994629, loss=0.5802099704742432
I0308 03:54:42.604639 139787621099264 logging_writer.py:48] [549600] global_step=549600, grad_norm=5.082248687744141, loss=0.6226691603660583
I0308 03:55:16.276462 139787612706560 logging_writer.py:48] [549700] global_step=549700, grad_norm=4.4852519035339355, loss=0.650304913520813
I0308 03:55:49.899158 139787621099264 logging_writer.py:48] [549800] global_step=549800, grad_norm=4.335226058959961, loss=0.6146723628044128
I0308 03:56:23.562101 139787612706560 logging_writer.py:48] [549900] global_step=549900, grad_norm=4.7928595542907715, loss=0.6281945109367371
I0308 03:56:57.169297 139787621099264 logging_writer.py:48] [550000] global_step=550000, grad_norm=4.884193420410156, loss=0.5756003260612488
I0308 03:57:30.834582 139787612706560 logging_writer.py:48] [550100] global_step=550100, grad_norm=4.818588733673096, loss=0.6303707361221313
I0308 03:58:04.444455 139787621099264 logging_writer.py:48] [550200] global_step=550200, grad_norm=4.721418857574463, loss=0.6439536809921265
I0308 03:58:38.126272 139787612706560 logging_writer.py:48] [550300] global_step=550300, grad_norm=4.445377349853516, loss=0.6576333045959473
I0308 03:59:11.732278 139787621099264 logging_writer.py:48] [550400] global_step=550400, grad_norm=4.125936985015869, loss=0.6189243793487549
I0308 03:59:45.389286 139787612706560 logging_writer.py:48] [550500] global_step=550500, grad_norm=4.95488977432251, loss=0.6748450398445129
I0308 03:59:48.921168 139951089751872 spec.py:321] Evaluating on the training split.
I0308 03:59:54.955389 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:00:03.449137 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:00:05.723933 139951089751872 submission_runner.py:411] Time since start: 191608.94s, 	Step: 550512, 	{'train/accuracy': 0.9607381820678711, 'train/loss': 0.14717940986156464, 'validation/accuracy': 0.7575199604034424, 'validation/loss': 1.0413737297058105, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8237340450286865, 'test/num_examples': 10000, 'score': 185191.63391900063, 'total_duration': 191608.93624162674, 'accumulated_submission_time': 185191.63391900063, 'accumulated_eval_time': 6366.848225593567, 'accumulated_logging_time': 30.291536569595337}
I0308 04:00:05.836337 139787629491968 logging_writer.py:48] [550512] accumulated_eval_time=6366.848226, accumulated_logging_time=30.291537, accumulated_submission_time=185191.633919, global_step=550512, preemption_count=0, score=185191.633919, test/accuracy=0.632400, test/loss=1.823734, test/num_examples=10000, total_duration=191608.936242, train/accuracy=0.960738, train/loss=0.147179, validation/accuracy=0.757520, validation/loss=1.041374, validation/num_examples=50000
I0308 04:00:35.753781 139788996814592 logging_writer.py:48] [550600] global_step=550600, grad_norm=4.451476573944092, loss=0.6233765482902527
I0308 04:01:09.426527 139787629491968 logging_writer.py:48] [550700] global_step=550700, grad_norm=4.236715316772461, loss=0.5676528811454773
I0308 04:01:43.072922 139788996814592 logging_writer.py:48] [550800] global_step=550800, grad_norm=4.723145484924316, loss=0.6261093616485596
I0308 04:02:16.657922 139787629491968 logging_writer.py:48] [550900] global_step=550900, grad_norm=4.505339622497559, loss=0.6345624923706055
I0308 04:02:50.306988 139788996814592 logging_writer.py:48] [551000] global_step=551000, grad_norm=4.695716857910156, loss=0.6270869970321655
I0308 04:03:23.927200 139787629491968 logging_writer.py:48] [551100] global_step=551100, grad_norm=4.803142547607422, loss=0.6392545700073242
I0308 04:03:57.569390 139788996814592 logging_writer.py:48] [551200] global_step=551200, grad_norm=3.8797812461853027, loss=0.5294072031974792
I0308 04:04:31.200399 139787629491968 logging_writer.py:48] [551300] global_step=551300, grad_norm=4.399788856506348, loss=0.5858591794967651
I0308 04:05:04.847864 139788996814592 logging_writer.py:48] [551400] global_step=551400, grad_norm=4.410242080688477, loss=0.6103606820106506
I0308 04:05:38.495116 139787629491968 logging_writer.py:48] [551500] global_step=551500, grad_norm=4.6884565353393555, loss=0.6016191244125366
I0308 04:06:12.156156 139788996814592 logging_writer.py:48] [551600] global_step=551600, grad_norm=4.3635759353637695, loss=0.6073449850082397
I0308 04:06:45.796048 139787629491968 logging_writer.py:48] [551700] global_step=551700, grad_norm=4.446940898895264, loss=0.5387511253356934
I0308 04:07:19.450043 139788996814592 logging_writer.py:48] [551800] global_step=551800, grad_norm=4.534538269042969, loss=0.6254023909568787
I0308 04:07:53.039221 139787629491968 logging_writer.py:48] [551900] global_step=551900, grad_norm=4.358612537384033, loss=0.6107964515686035
I0308 04:08:26.676704 139788996814592 logging_writer.py:48] [552000] global_step=552000, grad_norm=4.259579658508301, loss=0.6373023986816406
I0308 04:08:35.902754 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:08:41.938568 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:08:50.432122 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:08:52.678464 139951089751872 submission_runner.py:411] Time since start: 192135.89s, 	Step: 552029, 	{'train/accuracy': 0.9604990482330322, 'train/loss': 0.1474171131849289, 'validation/accuracy': 0.7571600079536438, 'validation/loss': 1.0401363372802734, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8212229013442993, 'test/num_examples': 10000, 'score': 185701.63338589668, 'total_duration': 192135.89077329636, 'accumulated_submission_time': 185701.63338589668, 'accumulated_eval_time': 6383.623881816864, 'accumulated_logging_time': 30.41457509994507}
I0308 04:08:52.787214 139789030385408 logging_writer.py:48] [552029] accumulated_eval_time=6383.623882, accumulated_logging_time=30.414575, accumulated_submission_time=185701.633386, global_step=552029, preemption_count=0, score=185701.633386, test/accuracy=0.631700, test/loss=1.821223, test/num_examples=10000, total_duration=192135.890773, train/accuracy=0.960499, train/loss=0.147417, validation/accuracy=0.757160, validation/loss=1.040136, validation/num_examples=50000
I0308 04:09:16.983906 139789038778112 logging_writer.py:48] [552100] global_step=552100, grad_norm=4.220536708831787, loss=0.6176576614379883
I0308 04:09:50.529372 139789030385408 logging_writer.py:48] [552200] global_step=552200, grad_norm=4.319019317626953, loss=0.6218782663345337
I0308 04:10:24.100622 139789038778112 logging_writer.py:48] [552300] global_step=552300, grad_norm=4.2025322914123535, loss=0.6184661388397217
I0308 04:10:57.677300 139789030385408 logging_writer.py:48] [552400] global_step=552400, grad_norm=4.229992389678955, loss=0.572004497051239
I0308 04:11:31.275532 139789038778112 logging_writer.py:48] [552500] global_step=552500, grad_norm=4.800585746765137, loss=0.5884909629821777
I0308 04:12:04.942961 139789030385408 logging_writer.py:48] [552600] global_step=552600, grad_norm=4.349650859832764, loss=0.646547794342041
I0308 04:12:38.534220 139789038778112 logging_writer.py:48] [552700] global_step=552700, grad_norm=4.081258296966553, loss=0.5812218189239502
I0308 04:13:12.257222 139789030385408 logging_writer.py:48] [552800] global_step=552800, grad_norm=4.748434066772461, loss=0.5934954881668091
I0308 04:13:45.868100 139789038778112 logging_writer.py:48] [552900] global_step=552900, grad_norm=4.282973766326904, loss=0.48872110247612
I0308 04:14:19.450645 139789030385408 logging_writer.py:48] [553000] global_step=553000, grad_norm=4.34833288192749, loss=0.6113421320915222
I0308 04:14:53.041960 139789038778112 logging_writer.py:48] [553100] global_step=553100, grad_norm=4.300004959106445, loss=0.6067947745323181
I0308 04:15:26.578064 139789030385408 logging_writer.py:48] [553200] global_step=553200, grad_norm=4.477297306060791, loss=0.589275062084198
I0308 04:16:00.167075 139789038778112 logging_writer.py:48] [553300] global_step=553300, grad_norm=4.395724296569824, loss=0.5407016277313232
I0308 04:16:33.822777 139789030385408 logging_writer.py:48] [553400] global_step=553400, grad_norm=4.053890705108643, loss=0.5966053605079651
I0308 04:17:07.479862 139789038778112 logging_writer.py:48] [553500] global_step=553500, grad_norm=4.678586483001709, loss=0.6459906101226807
I0308 04:17:22.784296 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:17:28.866248 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:17:37.429149 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:17:39.688165 139951089751872 submission_runner.py:411] Time since start: 192662.90s, 	Step: 553547, 	{'train/accuracy': 0.96097731590271, 'train/loss': 0.1455092877149582, 'validation/accuracy': 0.7566999793052673, 'validation/loss': 1.0411295890808105, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.822754144668579, 'test/num_examples': 10000, 'score': 186211.56378507614, 'total_duration': 192662.90041589737, 'accumulated_submission_time': 186211.56378507614, 'accumulated_eval_time': 6400.52764081955, 'accumulated_logging_time': 30.53302764892578}
I0308 04:17:39.799277 139787612706560 logging_writer.py:48] [553547] accumulated_eval_time=6400.527641, accumulated_logging_time=30.533028, accumulated_submission_time=186211.563785, global_step=553547, preemption_count=0, score=186211.563785, test/accuracy=0.632400, test/loss=1.822754, test/num_examples=10000, total_duration=192662.900416, train/accuracy=0.960977, train/loss=0.145509, validation/accuracy=0.756700, validation/loss=1.041130, validation/num_examples=50000
I0308 04:17:57.935266 139787621099264 logging_writer.py:48] [553600] global_step=553600, grad_norm=4.300850868225098, loss=0.6074920892715454
I0308 04:18:31.576025 139787612706560 logging_writer.py:48] [553700] global_step=553700, grad_norm=4.450432777404785, loss=0.6236060857772827
I0308 04:19:05.191051 139787621099264 logging_writer.py:48] [553800] global_step=553800, grad_norm=4.900972366333008, loss=0.6319290399551392
I0308 04:19:38.865830 139787612706560 logging_writer.py:48] [553900] global_step=553900, grad_norm=4.544122695922852, loss=0.6159941554069519
I0308 04:20:12.492378 139787621099264 logging_writer.py:48] [554000] global_step=554000, grad_norm=4.63638162612915, loss=0.5973108410835266
I0308 04:20:46.140353 139787612706560 logging_writer.py:48] [554100] global_step=554100, grad_norm=4.238368988037109, loss=0.6285955905914307
I0308 04:21:19.736965 139787621099264 logging_writer.py:48] [554200] global_step=554200, grad_norm=4.9275970458984375, loss=0.6099680662155151
I0308 04:21:53.381590 139787612706560 logging_writer.py:48] [554300] global_step=554300, grad_norm=4.889271259307861, loss=0.7034512758255005
I0308 04:22:26.991178 139787621099264 logging_writer.py:48] [554400] global_step=554400, grad_norm=4.556977272033691, loss=0.6468381285667419
I0308 04:23:00.653314 139787612706560 logging_writer.py:48] [554500] global_step=554500, grad_norm=4.876699447631836, loss=0.6486368179321289
I0308 04:23:34.289386 139787621099264 logging_writer.py:48] [554600] global_step=554600, grad_norm=4.408115386962891, loss=0.6009535193443298
I0308 04:24:07.958428 139787612706560 logging_writer.py:48] [554700] global_step=554700, grad_norm=4.371257305145264, loss=0.5875065326690674
I0308 04:24:41.582853 139787621099264 logging_writer.py:48] [554800] global_step=554800, grad_norm=4.473785400390625, loss=0.6488074660301208
I0308 04:25:15.246382 139787612706560 logging_writer.py:48] [554900] global_step=554900, grad_norm=4.518925666809082, loss=0.5815094709396362
I0308 04:25:48.849683 139787621099264 logging_writer.py:48] [555000] global_step=555000, grad_norm=4.493863105773926, loss=0.6994096040725708
I0308 04:26:09.825059 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:26:15.906717 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:26:24.352357 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:26:26.631202 139951089751872 submission_runner.py:411] Time since start: 193189.84s, 	Step: 555064, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.14861686527729034, 'validation/accuracy': 0.7573399543762207, 'validation/loss': 1.0403741598129272, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.8228280544281006, 'test/num_examples': 10000, 'score': 186721.5231051445, 'total_duration': 193189.8435087204, 'accumulated_submission_time': 186721.5231051445, 'accumulated_eval_time': 6417.333734750748, 'accumulated_logging_time': 30.65377688407898}
I0308 04:26:26.739370 139787621099264 logging_writer.py:48] [555064] accumulated_eval_time=6417.333735, accumulated_logging_time=30.653777, accumulated_submission_time=186721.523105, global_step=555064, preemption_count=0, score=186721.523105, test/accuracy=0.631700, test/loss=1.822828, test/num_examples=10000, total_duration=193189.843509, train/accuracy=0.959841, train/loss=0.148617, validation/accuracy=0.757340, validation/loss=1.040374, validation/num_examples=50000
I0308 04:26:39.214005 139789021992704 logging_writer.py:48] [555100] global_step=555100, grad_norm=4.45684814453125, loss=0.634555459022522
I0308 04:27:12.869311 139787621099264 logging_writer.py:48] [555200] global_step=555200, grad_norm=4.764972686767578, loss=0.6125149130821228
I0308 04:27:46.465010 139789021992704 logging_writer.py:48] [555300] global_step=555300, grad_norm=4.642276763916016, loss=0.6668674349784851
I0308 04:28:20.096785 139787621099264 logging_writer.py:48] [555400] global_step=555400, grad_norm=4.521969318389893, loss=0.5963094830513
I0308 04:28:53.698103 139789021992704 logging_writer.py:48] [555500] global_step=555500, grad_norm=4.446842193603516, loss=0.6437786817550659
I0308 04:29:27.285729 139787621099264 logging_writer.py:48] [555600] global_step=555600, grad_norm=4.663481712341309, loss=0.699001133441925
I0308 04:30:00.896297 139789021992704 logging_writer.py:48] [555700] global_step=555700, grad_norm=4.135700225830078, loss=0.6240030527114868
I0308 04:30:34.475500 139787621099264 logging_writer.py:48] [555800] global_step=555800, grad_norm=4.678378105163574, loss=0.6592153310775757
I0308 04:31:08.021069 139789021992704 logging_writer.py:48] [555900] global_step=555900, grad_norm=4.415673732757568, loss=0.6501001119613647
I0308 04:31:41.653299 139787621099264 logging_writer.py:48] [556000] global_step=556000, grad_norm=4.68676233291626, loss=0.607384443283081
I0308 04:32:15.261972 139789021992704 logging_writer.py:48] [556100] global_step=556100, grad_norm=4.967514991760254, loss=0.7239037752151489
I0308 04:32:48.855733 139787621099264 logging_writer.py:48] [556200] global_step=556200, grad_norm=4.580172538757324, loss=0.6115123629570007
I0308 04:33:22.508301 139789021992704 logging_writer.py:48] [556300] global_step=556300, grad_norm=4.560980319976807, loss=0.5812238454818726
I0308 04:33:56.124649 139787621099264 logging_writer.py:48] [556400] global_step=556400, grad_norm=4.418639659881592, loss=0.6216232180595398
I0308 04:34:29.761326 139789021992704 logging_writer.py:48] [556500] global_step=556500, grad_norm=4.433634281158447, loss=0.668566882610321
I0308 04:34:56.773038 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:35:02.854392 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:35:11.417304 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:35:13.700896 139951089751872 submission_runner.py:411] Time since start: 193716.91s, 	Step: 556582, 	{'train/accuracy': 0.9602000713348389, 'train/loss': 0.14691311120986938, 'validation/accuracy': 0.7574599981307983, 'validation/loss': 1.0412509441375732, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8229221105575562, 'test/num_examples': 10000, 'score': 187231.49054288864, 'total_duration': 193716.91319465637, 'accumulated_submission_time': 187231.49054288864, 'accumulated_eval_time': 6434.261531352997, 'accumulated_logging_time': 30.77181887626648}
I0308 04:35:13.810286 139788996814592 logging_writer.py:48] [556582] accumulated_eval_time=6434.261531, accumulated_logging_time=30.771819, accumulated_submission_time=187231.490543, global_step=556582, preemption_count=0, score=187231.490543, test/accuracy=0.632400, test/loss=1.822922, test/num_examples=10000, total_duration=193716.913195, train/accuracy=0.960200, train/loss=0.146913, validation/accuracy=0.757460, validation/loss=1.041251, validation/num_examples=50000
I0308 04:35:20.198658 139789005207296 logging_writer.py:48] [556600] global_step=556600, grad_norm=4.584766387939453, loss=0.6613665819168091
I0308 04:35:53.783983 139788996814592 logging_writer.py:48] [556700] global_step=556700, grad_norm=4.968695163726807, loss=0.6005907654762268
I0308 04:36:27.362081 139789005207296 logging_writer.py:48] [556800] global_step=556800, grad_norm=4.428079605102539, loss=0.6194248795509338
I0308 04:37:00.949536 139788996814592 logging_writer.py:48] [556900] global_step=556900, grad_norm=4.493220329284668, loss=0.6351976990699768
I0308 04:37:34.604851 139789005207296 logging_writer.py:48] [557000] global_step=557000, grad_norm=4.6544060707092285, loss=0.6157897710800171
I0308 04:38:08.241746 139788996814592 logging_writer.py:48] [557100] global_step=557100, grad_norm=4.717132568359375, loss=0.5474584698677063
I0308 04:38:41.835681 139789005207296 logging_writer.py:48] [557200] global_step=557200, grad_norm=4.447316646575928, loss=0.6674835681915283
I0308 04:39:15.433113 139788996814592 logging_writer.py:48] [557300] global_step=557300, grad_norm=4.60836935043335, loss=0.623725414276123
I0308 04:39:49.022734 139789005207296 logging_writer.py:48] [557400] global_step=557400, grad_norm=4.2564239501953125, loss=0.6402899622917175
I0308 04:40:22.612586 139788996814592 logging_writer.py:48] [557500] global_step=557500, grad_norm=4.376110076904297, loss=0.5571603775024414
I0308 04:40:56.263608 139789005207296 logging_writer.py:48] [557600] global_step=557600, grad_norm=4.499796390533447, loss=0.5778030753135681
I0308 04:41:29.885915 139788996814592 logging_writer.py:48] [557700] global_step=557700, grad_norm=4.442383289337158, loss=0.603401780128479
I0308 04:42:03.543589 139789005207296 logging_writer.py:48] [557800] global_step=557800, grad_norm=4.332168102264404, loss=0.5408465266227722
I0308 04:42:37.155828 139788996814592 logging_writer.py:48] [557900] global_step=557900, grad_norm=4.258833885192871, loss=0.6174603700637817
I0308 04:43:10.809717 139789005207296 logging_writer.py:48] [558000] global_step=558000, grad_norm=4.446908950805664, loss=0.5867598652839661
I0308 04:43:43.818318 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:43:49.843078 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:43:58.422990 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:44:00.710855 139951089751872 submission_runner.py:411] Time since start: 194243.92s, 	Step: 558099, 	{'train/accuracy': 0.9608777165412903, 'train/loss': 0.14640024304389954, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.040795087814331, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.8225491046905518, 'test/num_examples': 10000, 'score': 187741.4332535267, 'total_duration': 194243.9229915142, 'accumulated_submission_time': 187741.4332535267, 'accumulated_eval_time': 6451.153831481934, 'accumulated_logging_time': 30.892110347747803}
I0308 04:44:00.819771 139787612706560 logging_writer.py:48] [558099] accumulated_eval_time=6451.153831, accumulated_logging_time=30.892110, accumulated_submission_time=187741.433254, global_step=558099, preemption_count=0, score=187741.433254, test/accuracy=0.631000, test/loss=1.822549, test/num_examples=10000, total_duration=194243.922992, train/accuracy=0.960878, train/loss=0.146400, validation/accuracy=0.757080, validation/loss=1.040795, validation/num_examples=50000
I0308 04:44:01.493108 139787621099264 logging_writer.py:48] [558100] global_step=558100, grad_norm=4.58980655670166, loss=0.5678364038467407
I0308 04:44:35.017238 139787612706560 logging_writer.py:48] [558200] global_step=558200, grad_norm=4.1371870040893555, loss=0.5968411564826965
I0308 04:45:08.608876 139787621099264 logging_writer.py:48] [558300] global_step=558300, grad_norm=4.656714916229248, loss=0.7001816034317017
I0308 04:45:42.137326 139787612706560 logging_writer.py:48] [558400] global_step=558400, grad_norm=4.539121150970459, loss=0.7251020669937134
I0308 04:46:15.741644 139787621099264 logging_writer.py:48] [558500] global_step=558500, grad_norm=4.789488792419434, loss=0.7058919668197632
I0308 04:46:49.392301 139787612706560 logging_writer.py:48] [558600] global_step=558600, grad_norm=4.2880682945251465, loss=0.6599491834640503
I0308 04:47:23.044953 139787621099264 logging_writer.py:48] [558700] global_step=558700, grad_norm=4.216546535491943, loss=0.5477944016456604
I0308 04:47:56.664183 139787612706560 logging_writer.py:48] [558800] global_step=558800, grad_norm=5.1216511726379395, loss=0.6360647678375244
I0308 04:48:30.331163 139787621099264 logging_writer.py:48] [558900] global_step=558900, grad_norm=4.634108543395996, loss=0.59748375415802
I0308 04:49:03.991446 139787612706560 logging_writer.py:48] [559000] global_step=559000, grad_norm=4.239597320556641, loss=0.6228389143943787
I0308 04:49:37.655438 139787621099264 logging_writer.py:48] [559100] global_step=559100, grad_norm=4.16188907623291, loss=0.5491664409637451
I0308 04:50:11.343684 139787612706560 logging_writer.py:48] [559200] global_step=559200, grad_norm=4.257157802581787, loss=0.6243233680725098
I0308 04:50:44.963841 139787621099264 logging_writer.py:48] [559300] global_step=559300, grad_norm=4.28679895401001, loss=0.5671488046646118
I0308 04:51:18.598130 139787612706560 logging_writer.py:48] [559400] global_step=559400, grad_norm=4.50422477722168, loss=0.6648483872413635
I0308 04:51:52.215867 139787621099264 logging_writer.py:48] [559500] global_step=559500, grad_norm=5.093649864196777, loss=0.6239371299743652
I0308 04:52:25.854115 139787612706560 logging_writer.py:48] [559600] global_step=559600, grad_norm=4.5134453773498535, loss=0.6198432445526123
I0308 04:52:31.036332 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:52:37.072160 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:52:45.579494 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:52:47.829602 139951089751872 submission_runner.py:411] Time since start: 194771.04s, 	Step: 559617, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14745087921619415, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.0420900583267212, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8238762617111206, 'test/num_examples': 10000, 'score': 188251.58542084694, 'total_duration': 194771.04189276695, 'accumulated_submission_time': 188251.58542084694, 'accumulated_eval_time': 6467.947027206421, 'accumulated_logging_time': 31.010435104370117}
I0308 04:52:47.942310 139789005207296 logging_writer.py:48] [559617] accumulated_eval_time=6467.947027, accumulated_logging_time=31.010435, accumulated_submission_time=188251.585421, global_step=559617, preemption_count=0, score=188251.585421, test/accuracy=0.631300, test/loss=1.823876, test/num_examples=10000, total_duration=194771.041893, train/accuracy=0.960539, train/loss=0.147451, validation/accuracy=0.757280, validation/loss=1.042090, validation/num_examples=50000
I0308 04:53:16.146239 139789013600000 logging_writer.py:48] [559700] global_step=559700, grad_norm=4.776007652282715, loss=0.5757930278778076
I0308 04:53:49.723801 139789005207296 logging_writer.py:48] [559800] global_step=559800, grad_norm=4.617053985595703, loss=0.5745424628257751
I0308 04:54:23.290253 139789013600000 logging_writer.py:48] [559900] global_step=559900, grad_norm=4.484729290008545, loss=0.5618497133255005
I0308 04:54:55.691805 139951089751872 spec.py:321] Evaluating on the training split.
I0308 04:55:01.631508 139951089751872 spec.py:333] Evaluating on the validation split.
I0308 04:55:10.133797 139951089751872 spec.py:349] Evaluating on the test split.
I0308 04:55:12.421006 139951089751872 submission_runner.py:411] Time since start: 194915.63s, 	Step: 559998, 	{'train/accuracy': 0.9612165093421936, 'train/loss': 0.14852507412433624, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 1.039791464805603, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.8217592239379883, 'test/num_examples': 10000, 'score': 188379.31034731865, 'total_duration': 194915.6333310604, 'accumulated_submission_time': 188379.31034731865, 'accumulated_eval_time': 6484.6761956214905, 'accumulated_logging_time': 31.13318109512329}
I0308 04:55:12.528543 139787621099264 logging_writer.py:48] [559998] accumulated_eval_time=6484.676196, accumulated_logging_time=31.133181, accumulated_submission_time=188379.310347, global_step=559998, preemption_count=0, score=188379.310347, test/accuracy=0.631300, test/loss=1.821759, test/num_examples=10000, total_duration=194915.633331, train/accuracy=0.961217, train/loss=0.148525, validation/accuracy=0.757120, validation/loss=1.039791, validation/num_examples=50000
I0308 04:55:12.624531 139787629491968 logging_writer.py:48] [559998] global_step=559998, preemption_count=0, score=188379.310347
I0308 04:55:13.012004 139951089751872 checkpoints.py:490] Saving checkpoint at step: 559998
I0308 04:55:14.197543 139951089751872 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax/trial_1/checkpoint_559998
I0308 04:55:14.221326 139951089751872 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_resnet_jax/trial_1/checkpoint_559998.
I0308 04:55:15.121951 139951089751872 submission_runner.py:676] Final imagenet_resnet score: 188379.31034731865
